{
    "title": "RoBERTa: language modelling in building Indonesian question-answering systems",
    "url": "https://openalex.org/W4304084366",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A204936254",
            "name": "Wiwin Suwarningsih",
            "affiliations": [
                "National Research, Development and Innovation Office",
                "National Research and Innovation Agency"
            ]
        },
        {
            "id": "https://openalex.org/A4304086992",
            "name": "Raka Aditya Pramata",
            "affiliations": [
                "University of Brawijaya"
            ]
        },
        {
            "id": "https://openalex.org/A3183919743",
            "name": "Fadhil Yusuf Rahadika",
            "affiliations": [
                "University of Brawijaya"
            ]
        },
        {
            "id": "https://openalex.org/A4201728924",
            "name": "Mochamad Havid Albar Purnomo",
            "affiliations": [
                "University of Brawijaya"
            ]
        },
        {
            "id": "https://openalex.org/A204936254",
            "name": "Wiwin Suwarningsih",
            "affiliations": [
                "National Research and Innovation Agency"
            ]
        },
        {
            "id": "https://openalex.org/A4304086992",
            "name": "Raka Aditya Pramata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3183919743",
            "name": "Fadhil Yusuf Rahadika",
            "affiliations": [
                "University of Brawijaya"
            ]
        },
        {
            "id": "https://openalex.org/A4201728924",
            "name": "Mochamad Havid Albar Purnomo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1963863728",
        "https://openalex.org/W1043462122",
        "https://openalex.org/W632432350",
        "https://openalex.org/W2522848766",
        "https://openalex.org/W2173361515",
        "https://openalex.org/W6752788575",
        "https://openalex.org/W2908607053",
        "https://openalex.org/W4288628030",
        "https://openalex.org/W6713412435",
        "https://openalex.org/W6748434257",
        "https://openalex.org/W2996503788",
        "https://openalex.org/W2907333392",
        "https://openalex.org/W2998673404",
        "https://openalex.org/W2907588548",
        "https://openalex.org/W2552027021",
        "https://openalex.org/W3105522431",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4389009369",
        "https://openalex.org/W3167369375",
        "https://openalex.org/W3103089229",
        "https://openalex.org/W3090028958",
        "https://openalex.org/W3128029819",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W4210517072",
        "https://openalex.org/W4200164199",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W2401977166",
        "https://openalex.org/W2788835992",
        "https://openalex.org/W4297823766"
    ],
    "abstract": "This research aimed to evaluate the performance of the A Lite BERT (ALBERT), efficiently learning an encoder that classifies token replacements accurately (ELECTRA) and a robust optimized BERT pretraining approach (RoBERTa) models to support the development of the Indonesian language question and answer system model. The evaluation carried out used Indonesian, Malay and Esperanto. Here, Esperanto was used as a comparison of Indonesian because it is international, which does not belong to any person or country and this then make it neutral. Compared to other foreign languages, the structure and construction of Esperanto is relatively simple. The dataset used was the result of crawling Wikipedia for Indonesian and Open Super-large Crawled ALMAnaCH coRpus (OSCAR) for Esperanto. The size of the token dictionary used in the test used approximately 30,000 sub tokens in both the SentencePiece and byte-level byte pair encoding methods (ByteLevelBPE). The test was carried out with the learning rates of 1e-5 and 5e-5 for both languages in accordance with the reference from the bidirectional encoder representations from transformers (BERT) paper. As shown in the final result of this study, the ALBERT and RoBERTa models in Esperanto showed the results of the loss calculation that were not much different. This showed that the RoBERTa model was better to implement an Indonesian question and answer system.",
    "full_text": "TELKOMNIKA Telecommunication Computing Electronics and Control \nVol. 20, No. 6, December 2022, pp. 1248~1255 \nISSN: 1693-6930, DOI: 10.12928/TELKOMNIKA.v20i6.24248      1248  \n \nJournal homepage: http://telkomnika.uad.ac.id \nRoBERTa: language modelling in building Indonesian \nquestion-answering systems \n \n \nWiwin Suwarningsih1, Raka Aditya Pratama2, Fadhil Yusuf Rahadika2, \nMochamad Havid Albar Purnomo2 \n1Research Center for Data Science and Information, National Research and Innovation Agency , Bandung, Indonesia  \n2Department of Informatics Engineering, Faculty of Computer Science, Brawijaya University, Malang, Indonesia \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Jul 01, 2021 \nRevised Sep 09, 2022 \nAccepted Sep 19, 2022 \n \n This research aimed to evaluate the performance of the A Lite BERT \n(ALBERT), efficiently learning an encoder that classifies token \nreplacements accurately (ELECTRA) and a robust optimized BERT \npretraining approach (RoBERTa) models to support the development of the \nIndonesian language question and answer system model. The evaluation  \ncarried out used Indonesian, Malay and Esperanto. Here, Esperanto was used \nas a comparison of Indonesian because it is international, which does not \nbelong to any person or country and this then make it neutral. Compared to \nother foreign languages, the st ructure and construction of Esperanto is \nrelatively simple. The dataset used was the result of crawling Wikipedia for \nIndonesian and Open Super-large Crawled ALMAnaCH coRpus ( OSCAR) \nfor Esperanto. The size of the token dictionary used in the test used \napproximately 30,000 sub tokens in both the SentencePiece and  byte-level \nbyte pair encoding methods (ByteLevelBPE). The test was carried out with \nthe learning rates of 1e-5 and 5e-5 for both languages in accordance with the \nreference from the  bidirectional encoder representations from transformers  \n(BERT) paper. As shown in the final result of this study, the ALBERT and \nRoBERTa models in Esperanto showed the results of the loss calculation \nthat were not much different. This showed that the RoBER Ta model was \nbetter to implement an Indonesian question and answer system. \nKeywords: \nALBERT \nELECTRA \nIndonesian QAS \nLanguage modelling \nRoBERTa \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nWiwin Suwarningsih  \nResearch Center for Data Science and Information, National Research and Innovation Agency  \nKomplek LIPI Jl. Sangkuriang 21 Bandung 40135, Indonesia \nEmail: wiwin.suwarningsih@brin.go.id \n \n \n1. INTRODUCTION \nThe development of a question answer system (QAS) requires the development of an appropriate \nlanguage model. This is necessary because an ideal QAS system must have four supporting processes, which \ninclude candidate document selection, answer extraction, answer validation, and response generation [1], [2]. \nThe problem that arises is which model ling technique is suitable for selecting candidate documents to \nincrease question answer (QA) performance [3], [4]. A research conducted by  Tan et al . [5] applied the \nbidirectional long-short term memory (Bi-LSTM) model for questions and answers that were connected with \nlater pooling to compare similarities. To form a better embedding, a convolutional neural network ( CNN) \nlayer was added after  Bi-LSTM. In ad dition, to better separate answer candidates based on questions, an \nembedding model was introduced for answers based on the context of the question. Similarly,  Akbik et al. [6] \ndeveloped standard Word2vec using a Bi-LSTM layer plus character embedding. Here, character embedding \nwas used to handle words that were outside the bag of word.  \n\nTELKOMNIKA Telecommun Comput El Control   \n \nRoBERTa: language modelling in building indonesian question-answering systems (Wiwin Suwarningsih) \n1249 \nHandling the validation of answers from the QA architecture is an important spotlight to improve \nQAS performance because this architecture can be developed as an improvement from the existing \narchitecture [7]. What is different about this QA architecture is that there is an addition to the answer generator. \nAfter the answer is obtained, the answer will be used as input again in the answer generator [8], [9]. \nThis answer generator will rearrange the answers obtained with additional input from the feature extraction \nresults from questions using char recurrent neural network (RNN) [10], [11]. \nThe two problems above, namely sorting candidate documents and validating answers have been \nhandled by several methods such as the  application of  long-short term memory -recurrent neural network  \n(LSTM-RNN) [12], template convolutional recurrent neural network (T-CRNN) [13], CNN-BiLSTM [14], \ndynamic co-attention networks ( DCN) [15]. In [12] the training model used seq2seq for embedding wit h a \nlearning rate of 0.001 to avoid the gradient disappearing, and to ensure high answer accuracy. In contrast [13] \napplied the T -CRNN to obtain a right correlation between answers and questions. The model training was \ncarried out using a number of feature s mapped for each layer, namely 100 features with pre -trained word \nembedding using 100 dimensions. While in [14] the system tested the collaborative learning  based answer \nselection model (QA-CL) combined with CNN and Bi -LSTM architectures as initial matrix  vector \ninitialization. This model applied weight removal (WR) for embedding question sentences and generated the \ninitial vector matrix, and used principal component analysis (PCA) for feature reduction. It not only applied a \nhybrid model of combining LSTM  with CNN, but also applied CNN behind the LSTM layer to study the \nrepresentation of the question answer sentences. In contrast [15] implemented DCN to correct possible errors \nin answers due to local maxima in the QA System. The dataset used in this study was the Stanford question \nanswering dataset (SQuAD). Basically, in SQuAD there is an intuitive method that generates an answer \nindex range by predicting the start and end of the index range. \nAnother way to study the context of a word based on the words around it, not just the words that \nprecede or follow it, allows using a language model also known as transformers’ bidirectional encoder \nrepresentation (BERT) [16]. The breakthrough of BERT is its ability to train language models based on the \nwhole set of words in a sentence or query (two-way training) rather than the traditional way of training on a \nsequenced word order. Similarly, the research conducted by [17] used a BERT-based fine-tuning approach. \nThe purpose of using BERT is to reduce the constraints when pre-training masked language (MLM) models. \nTo improve the performance in the process of training the model for longer, with larger batches and more \ndata, the variance of BERT emerged such as a robust optimized BERT pretraining approach (RoBERTa) [18] \nand ALBERT [19]. \nRoBERTa [18] is the result of a modified BERT pre-training procedure that improves the final \nproject performance. It focuses on large-scale byte and batch increments so that it can be used to train with \ndynamic masking without losing next sentence prediction  (NSP). The use of RoBERTa to explore natural \nlanguage generation in question-answering [20], focused more on producing short answers to the given \nquestions and did not require any annotated data. The proposed approach showed some very promising \nresults for English and French. Whereas in the study [21] the trees induced from RoBERTa outperformed the \ntrees provided by the parser. The experimental results showed that the language model implicitly \nincorporated the task-oriented syntactic information. \nA Lite BERT (ALBERT) for self-supervised learning of language representation model [19] has a \nmuch smaller parameter size compared to the BERT model. ALBERT combines two parameter reduction \ntechniques that remove the main bottleneck in scaling the pre-trained model. The method we used in this \nstudy was to compare ALBERT and RoBERTa. ALBERT simplifies the BERT architecture by utilizing \nparameter-sharing and using the SentencePiece tokenization technique in the pre-training stage. RoBERTa \nmade some modifications to the pre-training section of BERT and removed the NSP method in training. \nRoBERTa uses the byte-level byte pair encoding (ByteLevelBPE) tokenization method adapted from \ngenerative pre-trained transformer-2 (GPT-2). \nBased on the explanations above, there are still several things that must be addressed, including how \nto process long sequential data and collect all information from text in network memory. In this study,  \nwe proposed RoBERTa  − an artificial neural network based on transformers with 6 layers (base model). \nRoBERTa [22], [23] used the WordPiece tokenization technique in the pre -training stage and the training \nmethod used masked layer modelling ( MLM) and NSP  to support our QAS system. Our contribution are:  \n1) representing the extraction of language models at the character level for answer selection without any \nengineering features and linguistic tools ; and  2) applying an efficient self -attention model  to generate \nanswers according to context by calculating the input and output representations regardless of word order.  \nThe rest of the paper is organized as: section 2 describes research method. The research method contains \nthe stages of work we did in this paper. The result and analysis are shown in section 3, this section describes test \nresults of the method we used. Finally, section 4 presents the conclusion of the paper. In this conclusion section it \nis explained that we have solved the problem based on the method we use and testing the data we have. \n \n                ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 20, No. 6, December 2022: 1248-1255 \n1250 \n2. RESEARCH METHOD  \nThe research method in this study can be seen in Figure 1. The approach used for training data used \nALBERT [19], RoBERTa  [18], [21] and efficiently learning an encoder that classifies token replacements \naccurately (ELECTRA) [24], [25]. The goal was to obtain an  ideal model to test with our dataset. The \nprocess of evaluating the performance of the model used two languages, i.e. Indonesian and Esperanto. \nEsperanto was used as a comparison to Indonesian for several reasons, including 1) Esperanto is \ninternational, which does not belong to a person or a country. In other words, it is neutral; 2) Esperanto ’s \nstructure and construction are relatively simple compared to other foreign languages; and 3) Esperanto is fair \nand everyone has the equal rights. \n \n \n \n \nFigure 1. Proposed research method \n \n \nBased on the proposed method in Figure 1, the article about coronavirus disease 2019 (COVID ’19) \nnews (we got it from crawling results on Indonesian Wikipedia, Jakarta News , Okezone, Antara, Kumparan, \nTribune, and Open Super -large Crawled ALMAnaCH coRpus (OSCAR) ) which is the input data for our \nstudy in preprocessing and converting the format to be used as input data for our study as a knowledge base \nsystem. Then the training model for initialize the parameters whic h includes the number of layers, optimizer, \nnumber of parameters, learning rate, learning rate scheduler, weight_decay, adam_beta1, adam_beta2, \nwarmup step ratio, steps and batch size.  This initialization is needed to facilitate the process of running the \nmodel language using BERT which consists of RoBERTa, ALBERT, and ELECTRA. The training model in \nthis study using high -performance computing provided by the national research and innovation agency took \nmore than 36 hours to train on a single graphics processing unit (GPU) V100. The final result of the running \nmodel language process is the RoBERTa, ALBERT, and ELECTRA language models which will be used for \ntesting documents that are used as answer candidates in the question and answering system that will be b uilt. \nThe dataset used was the result of crawling Wikipedia for Indonesian and OSCAR  for Esperanto. \nThe size of the token dictionary used in the test used approximately 30,000 sub tokens in both the \nSentencePiece and ByteLevelBPE methods. The test was carried out with the learning rates of 1e -5 and 5e-5 \nfor both languages according to the reference from the BERT paper [17], [22], [26]. \n \n \n3. RESULTS AND ANALYSIS  \n3.1.  Long training RoBERTa \nFrom our previous research experience, RoBERTa showed the better results than ALBERT for \nIndonesian [27], but the training time tended to be long (restricted resources). Thus, we did long training for \nRoBERTa with loss results l ike the graph in the picture above. The graph showed a “trough” caused by a \nchange in the distribution of the corpus. The change in the distribution of the corpus occurred due to the shift \nfrom the Wikipedia corpus, which is generally standard language to the OSCAR corpus. The OSCAR corpus \nis the result of free web scraping by CommonCrawl, which is further classified by language. The corpus has \nthe disadvantage for containing a number of dirty or rude words generally found on gambling websites or \nfree forums. \n\nTELKOMNIKA Telecommun Comput El Control   \n \nRoBERTa: language modelling in building indonesian question-answering systems (Wiwin Suwarningsih) \n1251 \n  \n  \n(a) (b) \n  \n  \n \n  \n(c) \n \nFigure 2. Long training RoBERTa: (a) learning rate graph, (b) loss graph, and (c) performance graph \nRoBERTa language model \n \n \nIn Figure 2 performance language model , the loss value looks like a drastic drop like the trough \nphenomenon. Figure 2(a) explain learning rate graph is a hyper -parameter that controls how much we are \nadjusting the weights of our network with respect the loss gradient. Figure 2(b) loss graph is function results \nfor different models and Figure 2(c) performance graph RoBERTa language model shows that pretraining \nmultilingual language models at scale leads to significant. This was because data had the same structure and \nit made it simple for the mo del to  guess. An unstable loss value did not indicate that the neural network \ndecreased in capacity. This was because we loaded the corpus data sequentially, not randomly. The reason \nwas because of limited resources so that the corpus was not able to be lo aded entirely at one time but broken \ndown into several parts. \n \n3.2.  Comparison of ELECTRA with RoBERTa \nWe are interested in testing the performance of the ELECTRA model as a baseline for the method \nwe use because the SQuAD explorer leaderboard in [15] showed the quite good results . ELECTRA uses a \nnew pre-training task called substitution token detection that trains a bidirectional model while learning from all \ninput positions. ELECTRA trains models to distinguish between “real” and “fake” input data. In ELECTRA, \nthe input is not masked, but it is corrupted by  the approach of replacing some input tokens with plausible \nalternatives obtained from a small generator network. Figure 3 shows the results of ELECTRA and \nRoBERTa’s learning rate using Malay, which tends to be similar to Indonesian. \nBased on the learning rate in Figure 3, the next test was to conduct QA training using the ELECTRA \nmodel language of Malaysian language, which tends to be similar to Indonesian. Of 65 Indones ian QA-Pairs \ndatasets manually selected and then formatted, so that they became SQuAD version 1.1, 100 epochs of training \nwere conducted (see Figure 3(a)). The Malaysian ELECTRA model produced a satisfactory performance with \nan exact match (EM) score of 0.8 and an F1-measure score of 0.84 (see Figure 3(b)). \n\n                ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 20, No. 6, December 2022: 1248-1255 \n1252 \n  \n  \n(a) (b) \n  \nFigure 3. Learning rate RoBERTa vs ELECTRA: (a) RoBERTa with a learning rate of 1e-4 and (b) \nELECTRA with a learning rate of 1e-4 \n \n \n3.3.  Comparison of ALBERT with RoBERTa \nAt this stage, the next step was to compare Albert ’s performance with RoBERTa’s. The learning \nrate we used applied two intervals, namely 1e -5 and 5e-5 [19], [18]. The experimental results can be seen in  \nFigure 4. \n \n \n \n \nFigure 4. Learning rate 5e-5 ALBERT and RoBERTa \n \n \nAs seen in Figure 4, the ALBERT and RoBERTa models in Esperanto showed the results of the \n“loss” calculation that were not much different. The RoBERTa model showed a calculation result that tended \nto be better than ALBERT in Indonesian with a significant “ loss” calculation result. The use of learning rate \n5e-5 showed the better results than 1e-5. \n \n3.4.  Result and discussion \nThe language model training stage is the training stage where the model is trained to understand the \nstructure of the language used. T he training strategy used is to mask some of the tokens (a common \npercentage or recommended for use is 15% of masked tokens) in the input sentence, and then the model will \nbe trained to predict missing words or masked words. The data used at this stage wer e the unlabelled data. \nThe result of this training phase was a basic model that only had language skills. The language skills of the \nnext model can be used for knowledge base and retrained (fine-task tuning). \nThis training stage can be skipped by using a p ublished pre -trained model so that only fine -task \ntuning was required. When this research was started, no specific model of Indonesian based on transformers \nhas, so far, been published in general, so the researcher decided to train the language model indep endently. \nDuring this research, several types of models were published by other researchers. This model was then used \nas a comparison against the model that was trained independently in this study.  \nThe experimental results in Figure 2. Shows the occurrence of a “trough” due to the presence of the \nsame structured data so that it was simple for the model to guess. An example of the part of the body that \ncaused the trough can be seen in Figure 5. \n\nTELKOMNIKA Telecommun Comput El Control   \n \nRoBERTa: language modelling in building indonesian question-answering systems (Wiwin Suwarningsih) \n1253 \n \n \nFigure 5. Example of corpus \n \n \nAs seen in Figure 5, the language structure and word order were almost the same between each \ndocument. This appeared in such a large number that it was too simple for the model to guess 15% of the \nmasked sentences. There were 22  GB of news data taken from several sources such as t he Indonesian \nWikipedia, Jakarta News, Okezone, Antara, Kumparan, Tribune, and OSCAR. For language model training it \nused the RoBERTa (the comparison of performance model language can see on Table 1). \n \n \nTable 1. Comparison of performance model language  \nModel language Exact match (EM) F1 Accuracy \nRoBERTa 84.6% 86.2% 91.7% \nAlBERT 82.3% 84.6% 89.7 \nELECTRA 83.1% 85.4% 87.3 \n \n \nAfter conducting training 30% of the total agreed steps, the model showed a fairly good performance \nwhere, when using the same dataset from the previous model, there was an increase of about 2 % to 4% in the \nexact match (EM) and F1 matrices. As a result, in the 30% trained model, the model produced 84.6% EM and \n86.2% F1 performance. From these results, the model has been able to ou tperform the ELECTRA model, \nwhich was trained using the Malaysian language in the previous progress we used as the baseline \nperformance. This result could also continue to increase until 100% of the model was trained.  \n \n \n4. CONCLUSION  \nWe evaluated the performance of the ALBERT, RoBERTa, ELECTRA models using Indonesian, \nMalaysian and Esperanto languages. For the best result for building Indonesian QAS, the language model \nused was RoBERTa that produced 84.6 EM and 86.2% F1 performance. Th is result could also continue to \nincrease until 100% of the model was trained. \nIn the implementation of real-world cases, the application of RoBERTa alone is still not sufficient to \nget the right answer. This is because RoBERTa is trained to find answers f rom the appropriate context of the \nquestions. Our future work will carry out the search for answers in a context adapted to a very large body of \nknowledge. The search or retrieval method that we use to get the context that matches the question is Okapi \nbest matching (BM25) − a ranking system that is used to sort the results of the similarity (similarity) to the \ndocuments used based on the desired keywords. \n \n \n\n                ISSN: 1693-6930 \nTELKOMNIKA Telecommun Comput El Control, Vol. 20, No. 6, December 2022: 1248-1255 \n1254 \nACKNOWLEDGEMENTS \nThe author would like to thank all the facilities provided by the national research  and innovation \nagency (BRIN) particularly Research Center for Data Science and Informatios  and The Informatic \nDepartement, Brawijaya University during the process of making this manuscript.  \n \n \nREFERENCES  \n[1] A. Bayoudhi, L. H. Belguith, and H. Ghorbel, “Question focus extraction and answer passage retrieval,”  in 2014 IEEE/ACS 11th \nInternational Conference on Computer Systems and Applications (AICCSA), 2014, pp. 658-665, doi: 10.1109/AICCSA.2014.7073262. \n[2] A. Bayoudhi, H. Ghorbel, and L. H. Belguith, “Question answering system for dialogues: A new taxonomy of opinion questions,” \nFlexible Query Answering Systems, pp. 67–78, 2013, doi: 10.1007/978-3-642-40769-7_6. \n[3] A. B. Abacha and P. Zweigenbaum, “MEANS: A medical question -answering system combining NLP techniques and semantic \nWeb technologies,” Information Processing and Management, vol. 51, no. 5, pp. 570–594, 2015, doi: 10.1016/j.ipm.2015.04.006. \n[4] W. Suwarningsih, A. Purwarianti, and I. Supriana, “Semantic roles labeling for reuse in CBR based QA system,” 2016 4th International \nConference on Information and Communication Technology (ICoICT), 2016, pp. 1-5, doi: 10.1109/ICoICT.2016.7571935. \n[5] M. Tan, C. D. Santos, B. Xiang, and B. Zhou, “LSTM -based Deep Learning Models for Non -factoid Answer Selection,” ArXiv, \n2015, doi: 10.48550/arXiv.1511.04108. \n[6] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings for sequence labeling,” in Proc. of the 27th International \nConference on Computational Linguistics, 2018, pp. 1638–1649. [Online]. Available: https://aclanthology.org/C18-1139.pdf \n[7] B. Kratzwald and S. Feuerriegel, “Learning from on -line user feedback in neural question answering on the web,” The World \nWide Web Conference, 2019, pp. 906–916, doi: 10.1145/3308558.3313661. \n[8] M. R. Akram, C. P. Singhabahu, M. S. M. Saad, P. Deleepa, A.  Nugaliyadde, and Y. Mallawarachchi, “Adaptive Artificial \nIntelligent Q&A Platform,” ArXiv, 2019, doi: 10.48550/arXiv.1902.02162. \n[9] L. Cao, X. Qiu, and X. Huang, “Question answering for machine reading with lexical chain,” in CLEF 2011 Labs and Workshop, \n2011. [Online]. Available: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.656.7700&rep=rep1&type=pdf \n[10] L. Sha, X. Zhang, F. Qian, B. Chang, and Z. Sui, “A multi -view fusion neural network for answer selection,” The Thirty-Second \nAAAI Confere nce on Artificial Intelligence (AAAI -18), 2018 , pp. 5422 –5429. [Online]. Available:  \nhttps://ojs.aaai.org/index.php/AAAI/article/view/11989/11848 \n[11] A. C. O. Reddy and K. Madhavi, “Convolutional recurrent neural network with template based representation for complex \nquestion answering,” International Journal of Electrical and Computer Engineering (IJECE) , vol. 10, no. 3, pp. 2710 –2718, \n2020, doi: 10.11591/ijece.v10i3.pp2710-2718. \n[12] X. Zhang, M. H. Chen, and Y. Qin, “NLP -QA Framework Based on LSTM -RNN,” in 2018 2nd International Conference on \nData Science and Business Analytics (ICDSBA), 2018, pp. 307-311, doi: 10.1109/ICDSBA.2018.00065. \n[13] S. B. Jadhav, V. R. Udupi, and S. B. Patil, “Convolutional neural networks for leaf image -based plant disease classification,” \nInternational Journal of Artificial Intelligence (IJ-AI), vol. 8, no. 4, pp. 328–341, 2019, doi: 10.11591/ijai.v8.i4.pp328-341. \n[14] T. Shao, X. Kui, P. Zhang, and H. Chen, “Collaborative learning for answer selection in question answering,” IEEE Access, \nvol. 7, pp. 7337–7347, 2019 doi: 10.1109/ACCESS.2018.2890102. \n[15] C. Xiong, V. Zhong, and R. Socher, “Dynamic coattention network s for question answering,” International Conference on \nLearning Representations 2017, 2017, doi: 10.48550/arXiv.1611.01604. \n[16] L. E. -Dor et al., “Active Learning for BERT: An Empirical Study,” in Proc. of the 2020 Conference on Empirical Methods in \nNatural Language Processing (EMNLP), 2020, pp. 7949–7962, doi: 10.18653/v1/2020.emnlp-main.638. \n[17] J. Devlin, M.  -W. Chang, K. Lee, and K. Toutanova, “BERT: Pre -training of deep bidirectional transformers for language \nunderstanding,” Proceedings of the 2 019 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, 2019, vol. 1, pp. 4171–4186, doi: 10.18653/v1/N19-1423. \n[18] Y. Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining Approach,” ArXiv, 2019, doi : 10.48550/arXiv.1907.11692. \n[19] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “ALBERT: A Lite BERT for Self -supervised Learning of \nLanguage Representations,” ArXiv, 2019, doi: 10.48550/arXiv.1909.11942. \n[20] I. Akermi, J. Heinecke, and F. Herledan, “Transformer based Natural Language Generation for Question-Answering,” in Proc. of \nthe 13th International Conference on Natural Language Generation , 2020 , pp. 349 –359. [Online]. Available: \nhttps://aclanthology.org/2020.inlg-1.41.pdf \n[21] J. Dai, H. Yan, T. Sun, P. Liu, and X. Qiu, “Does syntax matter? A strong baseline for Aspect -based Sentiment Analysis with \nRoBERTa,” in Proc. of the 2021 Conference of the North American Chapter of the Association for Computational Li nguistics: \nHuman Language Technologies, 2021, pp. 1816–1829, doi: 10.18653/v1/2021.naacl-main.146. \n[22] A. Wen, M. Y. Elwazir, S. Moon, and J. Fan, “Adapting and evaluating a deep learning language model for clinical why-question \nanswering,” JAMIA Open, vol. 3, no. 1, pp. 16-20, 2020, doi: 10.1093/jamiaopen/ooz072. \n[23] J. S . Sharath and R. Banafsheh, “Question Answering over Knowledge Base using Language Model Embeddings,” in 2020 \nInternational Joint Conference on Neural Networks (IJCNN), 2020, pp. 1-8, doi: 10.1109/IJCNN48605.2020.9206698. \n[24] W. Antoun, F. Baly, and H. Hajj, “AraELECTRA: Pre -Training Text Discriminators for Arabic Language Understanding,” in \nProc. of the Sixth Arabic Natural Language Processing Workshop , 2021, pp. 191 -195. [Online]. Available: \nhttps://aclanthology.org/2021.wanlp-1.20.pdf \n[25] K. Clark, M.  -T. Luong, Q. V. Le, and C. D. Manning, “ELECTRA: Pre -training Text Encoders as Discriminators Rather Than \nGenerators,” International Conference on Learning Representations (ICLR) 2020, 2020, doi: 10.48550/arXiv.2003.10555. \n[26] A. Kavros and Y. Tzitzikas, “SoundexGR: An algorithm for phonetic matching for the Greek language ,” Natural Language \nEngineering, 2022, doi: 10.1017/S1351324922000018. \n[27] W. Suwarningsih, R. A. Pra tama, F. Y. Rahadika, and M. H. A. Purnomo, “Self -Attention Mechanism of RoBERTa to Improve \nQAS for e -health Education,” 2021 4th International Conference of Computer and Informatics Engineering (IC2IE) , 2021,  \npp. 221-225, doi: 10.1109/IC2IE53219.2021.9649363. \n \n \n \n \nTELKOMNIKA Telecommun Comput El Control   \n \nRoBERTa: language modelling in building indonesian question-answering systems (Wiwin Suwarningsih) \n1255 \nBIOGRAPHIES OF AUTHORS  \n \n  \nWiwin Suwarningsih     she was graduated from her bachelor degree at Informatics \nProgram, Adityawarman Institute of Technology Bandung in 1996. She got graduated from her \nmaster education in 2000 at the Informatics Study Program, Bandung Institute of Technology \nand doctoral degree in 2017 the School of Electrical and Informatics Engineering, Bandung \nInstitute of Technology. Since 2006 until now she has been a researcher at Research Center for \nInformation and Data Science, National Research and Innovation Agency , Indonesia. His \nresearch interests are artificial intelligence, computational linguistics, partic ularly in \nIndonesian natural language processing and Indonesian text mining, information retrieval, and \nquestion answering systems. She can be contacted at email: wiwin.suwarningsih@brin.go.id.  \n  \n \nRaka Aditya Pratama     currently He is still studying at Informatics department, \nBrawijaya University as a bachelor student. His research interests are on Image processing, \nArtificial Inteligence, Phyton programming, Text mining, and Software Engineering. H e can \nbe contacted at email: rakdit@student.ub.ac.id. \n  \n  \n \nFadhil Yusuf Rahadika     currently He is still studying at Informatics department, \nBrawijaya University as a bachelor student. His research interests are on Artificial Inteligence, \nData mining, Software engineering and Phyton programming. H e can be contacted at email: \nfadhilyusuf27@student.ub.ac.id. \n \n  \n  \n \nMochamad Havid Albar Purnomo      currently He is still studying at Informatics \ndepartment, Brawijaya University as a bachelor student. His research interests are on Text \nmining, Phyton programming, Software engineering and Artificial Inteligence. H e can be \ncontacted at email: havidalbar@student.ub.ac.id. \n  \n \n"
}