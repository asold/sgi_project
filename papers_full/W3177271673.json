{
  "title": "MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding",
  "url": "https://openalex.org/W3177271673",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4207979704",
      "name": "Jia-Chen Gu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2585202595",
      "name": "Chongyang Tao",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2156045219",
      "name": "Zhen-Hua Ling",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098334175",
      "name": "Can Xu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2167037537",
      "name": "Xiubo Geng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2123654898",
      "name": "Daxin Jiang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2952813980",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2970185413",
    "https://openalex.org/W2798456655",
    "https://openalex.org/W3102606416",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2971277071",
    "https://openalex.org/W2985258882",
    "https://openalex.org/W2908331278",
    "https://openalex.org/W2966126777",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2806367735",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3094612274",
    "https://openalex.org/W2756320212",
    "https://openalex.org/W2962854379",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W2891416139",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2561158378",
    "https://openalex.org/W3022187094",
    "https://openalex.org/W2951883832",
    "https://openalex.org/W2462831000"
  ],
  "abstract": "Jia-Chen Gu, Chongyang Tao, Zhenhua Ling, Can Xu, Xiubo Geng, Daxin Jiang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3682–3692\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3682\nMPC-BERT: A Pre-Trained Language Model for Multi-Party\nConversation Understanding\nJia-Chen Gu1∗, Chongyang Tao2, Zhen-Hua Ling1, Can Xu2, Xiubo Geng2, Daxin Jiang2†\n1National Engineering Laboratory for Speech and Language Information Processing,\nUniversity of Science and Technology of China, Hefei, China\n2Microsoft, Beijing, China\ngujc@mail.ustc.edu.cn, zhling@ustc.edu.cn,\n{chotao,caxu,xigeng,djiang}@microsoft.com\nAbstract\nRecently, various neural models for multi-\nparty conversation (MPC) have achieved\nimpressive improvements on a variety of\ntasks such as addressee recognition, speaker\nidentiﬁcation and response prediction.\nHowever, these existing methods on\nMPC usually represent interlocutors and\nutterances individually and ignore the\ninherent complicated structure in MPC which\nmay provide crucial interlocutor and utterance\nsemantics and would enhance the conversation\nunderstanding process. To this end, we present\nMPC-BERT, a pre-trained model for MPC\nunderstanding that considers learning who\nsays what to whom in a uniﬁed model with\nseveral elaborated self-supervised tasks.\nParticularly, these tasks can be generally\ncategorized into (1) interlocutor structure\nmodeling including reply-to utterance\nrecognition, identical speaker searching\nand pointer consistency distinction, and\n(2) utterance semantics modeling including\nmasked shared utterance restoration and\nshared node detection. We evaluate MPC-\nBERT on three downstream tasks including\naddressee recognition, speaker identiﬁcation\nand response selection. Experimental results\nshow that MPC-BERT outperforms previous\nmethods by large margins and achieves new\nstate-of-the-art performance on all three\ndownstream tasks at two benchmarks.\n1 Introduction\nBuilding a conversational agent with intelligence\nhas drawn signiﬁcant attention from both academia\nand industry. Most of existing methods have\nstudied understanding conversations between two\nparticipants, aiming to return an appropriate re-\nsponse either in a generation-based (Shang et al.,\n∗Work done during the internship at Microsoft.\n†Corresponding author.\nSpeaker Utterance Addressee\nI.1 How can I setup if I want add new -server at xchat?\nI.2\nFrom places, network servers, work\nI.1group, his computer, and then I\nclicked on the shared folder.\nI.3 It did not allow you to see the ﬁles? I.2\nI.2\nIt prompts for authentication and I\nI.3don’t know what to put. I tried guest\nwith no password.\nI.4 Put proper authentication in, then? I.2\nI.3 I think you had kde on suse? I.2\nTable 1: An MPC example in Ubuntu IRC channel.\nHere, “I.” is the abbreviation of “interlocutor”.\n2015; Serban et al., 2016, 2017; Zhang et al.,\n2018b, 2020) or retrieval-based manner (Lowe\net al., 2015; Wu et al., 2017; Zhou et al., 2018; Tao\net al., 2019a,b; Gu et al., 2019a,b, 2020). Recently,\nresearchers have paid more attention to a more\npractical and challenging scenario involving more\nthan two participants, which is well known as multi-\nparty conversation (MPC) (Ouchi and Tsuboi,\n2016; Zhang et al., 2018a; Le et al., 2019; Hu et al.,\n2019). Table 1 shows an MPC example in the\nUbuntu Internet Relay Chat (IRC) channel, which\nis composed of a sequence of (speaker, utterance,\naddressee) triples. In addition to returning an\nappropriate response, predicting who will be the\nnext speaker (Meng et al., 2018) and who is the\naddressee of an utterance (Ouchi and Tsuboi, 2016;\nZhang et al., 2018a; Le et al., 2019) are unique and\nimportant issues in MPC.\nAn instance of MPC always contains compli-\ncated interactions between interlocutors, between\nutterances and between an interlocutor and an\nutterance. Therefore, it is challenging to model\nthe conversation ﬂow and fully understand the\ndialogue content. Existing studies on MPC learn\nthe representations of interlocutors and utterances\nwith neural networks, and their representation\n3683\nspaces are either separate (Ouchi and Tsuboi, 2016)\nor interactive (Zhang et al., 2018a). However,\nthe semantics contained in the interlocutor and\nutterance representations may not be effectively\ncaptured as they are from two different represen-\ntation spaces. Recently, to take advantage of\nthe breakthrough in pre-training language mod-\nels (PLMs) for natural language understanding,\nsome studies proposed to integrate the speaker\n(Gu et al., 2020) or topic (Wang et al., 2020)\ninformation into PLMs. Despite of the performance\nimprovement on response selection, these models\nstill overlook the inherent relationships between\nutterances and interlocutors, such as “address-to”.\nFurthermore, most existing studies design models\nfor each individual task in MPC (e.g., addressee\nrecognition, speaker identiﬁcation and response\nprediction) separately. Intuitively, these tasks are\ncomplementary among each other. Making use\nof these tasks simultaneously may produce better\ncontextualized representations of interlocutors and\nutterances, and would enhance the conversation\nunderstanding, but is neglected in previous studies.\nOn account of above issues, we propose MPC-\nBERT which jointly learns who says what to whom\nin MPC by designing self-supervised tasks for\nPLMs, so as to improve the ability of PLMs on\nMPC understanding. Speciﬁcally, the ﬁve designed\ntasks includes reply-to utterance recognition, iden-\ntical speaker searching, pointer consistency dis-\ntinction, masked shared utterance restorationand\nshared node detection . The ﬁrst three tasks are\ndesigned to model the interlocutor structure in\nMPC in a semantics-to-structure manner. In the\noutput of MPC-BERT, an interlocutor is described\nthrough the encoded representations of the ut-\nterances it says. Thus, the representations of\nutterance semantics are utilized to construct the\nconversation structure in these three tasks. On the\nother hand, the last two tasks are designed to model\nthe utterance semantics in a structure-to-semantics\nmanner. Intuitively, the conversation structure\ninﬂuences the information ﬂow in MPC. Thus, the\nstructure information can also be used to strengthen\nthe representations of utterance semantics in return.\nIn general, these ﬁve self-supervised tasks are\nemployed to jointly train the MPC-BERT in a\nmulti-task learning framework, which helps the\nmodel to learn the complementary information\namong interlocutors and utterances, and that be-\ntween structure and semantics. By this means,\nMPC-BERT can produce better interlocutor and\nutterance representations which can be effectively\ngeneralized to multiple downstream tasks of MPC.\nTo measure the effectiveness of these self-\nsupervised tasks and to test the generalization\nability of MPC-BERT, we evaluate it on three\ndownstream tasks including addressee recognition,\nspeaker identiﬁcation and response selection ,\nwhich are three core research issues of MPC. Two\nbenchmarks based on Ubuntu IRC channel are\nemployed for evaluation. One was released by Hu\net al. (2019). The other was released by Ouchi\nand Tsuboi (2016) and has three experimental\nsettings according to session lengths. Experimental\nresults show that MPC-BERT outperforms the\ncurrent state-of-the-art models by margins of\n3.51%, 2.86%, 3.28% and 5.36% on the test sets\nof these two benchmarks respectively in terms\nof the session accuracy of addressee recognition,\nby margins of 7.66%, 2.60%, 3.38% and 4.24%\nrespectively in terms of the utterance precision of\nspeaker identiﬁcation, and by margins of 3.82%,\n2.71%, 2.55% and 3.22% respectively in terms of\nthe response recall of response selection.\nIn summary, our contributions in this paper\nare three-fold: (1) MPC-BERT, a PLM for MPC\nunderstanding, is proposed by designing ﬁve self-\nsupervised tasks based on the interactions among\nutterances and interlocutors. (2) Three downstream\ntasks are employed to comprehensively evaluate the\neffectiveness of our designed self-supervised tasks\nand the generalization ability of MPC-BERT. (3)\nOur proposed MPC-BERT achieves new state-of-\nthe-art performance on all three downstream tasks\nat two benchmarks.\n2 Related Work\nExisting methods on building dialogue systems\ncan be generally categorized into studying two-\nparty conversations and multi-party conversations\n(MPC). In this paper, we study MPC. In addition to\npredicting utterances, identifying the speaker and\nrecognizing the addressee of an utterance are also\nimportant tasks for MPC. Ouchi and Tsuboi (2016)\nﬁrst proposed the task of addressee and response\nselection and created an MPC corpus for studying\nthis task. Zhang et al. (2018a) proposed SI-RNN,\nwhich updated speaker embeddings role-sensitively\nfor addressee and response selection. Meng et al.\n(2018) proposed a task of speaker classiﬁcation as\na surrogate task for speaker modeling. Le et al.\n3684\n(2019) proposed a who-to-whom (W2W) model\nto recognize the addressees of all utterances. Hu\net al. (2019) proposed a graph-structured network\n(GSN) to model the graphical information ﬂow for\nresponse generation. Wang et al. (2020) proposed\nto track the dynamic topic for response selection.\nGenerally speaking, previous studies on MPC\ncannot unify the representations of interlocutors\nand utterances effectively. Also, they are limited to\neach individual task, ignoring the complementary\ninformation among different tasks. To the best\nof our knowledge, this paper makes the ﬁrst\nattempt to design various self-supervised tasks for\nbuilding PLMs aiming at MPC understanding, and\nto evaluate the performance of PLMs on three\ndownstream tasks as comprehensively as possible.\n3 MPC-BERT and Self-Supervised Tasks\nAn MPC instance is composed of a sequence of\n(speaker, utterance, addressee ) triples, denoted\nas {(sn, un, an)}N\nn=1, where N is the number of\nturns in the conversation. Our goal is to build\na pre-trained language model for universal MPC\nunderstanding. Given a conversation, this model\nis expected to produce embedding vectors for all\nutterances which contain not only the semantic\ninformation of each utterance, but also the speaker\nand addressee structure of the whole conversation.\nThus, it can be effectively adapted to various\ndownstream tasks by ﬁne-tuning model parameters.\n3.1 Model Overview\nIn this paper, BERT (Devlin et al., 2019) is chosen\nas the backbone of our PLM for MPC. Thus, we\nname it MPC-BERT. It is worth noting that our\nproposed self-supervised tasks for training MPC-\nBERT can also be applied to other types of PLMs.\nWe ﬁrst give an overview of the input represen-\ntations and the overall architectures of MPC-BERT.\nWhen constructing the input representations, in\norder to consider the speaker information of each\nutterance, speaker embeddings (Gu et al., 2020)\nare introduced as shown in Figure 1. Considering\nthat the set of interlocutors are inconsistent in dif-\nferent conversations, a position-based interlocutor\nembedding table is initialized randomly at ﬁrst and\nupdated during pre-training, which means each\ninterlocutor in a conversation is assigned with\nan embedding vector according to the order it\nappears in the conversation. Then, the speaker\nembeddings for each utterance can be derived by\nlooking up this embedding table. The speaker\nembeddings are combined with standard token,\nposition and segmentation embeddings and are\nthen encoded by BERT. The output embeddings\nof BERT corresponding to different input tokens\nare utilized by different self-supervised tasks for\nfurther calculation.\n3.2 Tasks of Interlocutor Structure Modeling\nThe ﬁrst three tasks follow the semantics-to-\nstructure manner. In MPC-BERT, each interlocutor\nis described through the encoded representations\nof the utterances it says. Thus, the representations\nof utterance semantics are utilized to construct\nthe conversation structure. Figure 1 shows the\ninput representations and the model architectures\nof these three tasks. A [CLS] token is inserted at\nthe start of each utterance, denoting its utterance-\nlevel representation. Then, all utterances in a\nconversation are concatenated and a [SEP] token\nis inserted at the end of the whole sequence. It is\nnotable that these three tasks share the same form\nof input data. Thus, the input only needs to be\nencoded once by BERT while the output can be\nfed into three tasks, which is computation-efﬁcient.\nAs shown in Figure 1, a task-dependent non-linear\ntransformation layer is placed on top of BERT\nin order to adapt the output of BERT to different\ntasks. We will describe the details of these tasks as\nfollows.\n3.2.1 Reply-to Utterance Recognition\nTo enable the model to recognize the addressee of\neach utterance, a self-supervised task named reply-\nto utterance recognition (RUR)is proposed to learn\nwhich preceding utterance the current utterance\nreplies to. After encoded by BERT, we extract\nthe contextualized representations for each [CLS]\ntoken representing individual utterances. Next,\na non-linear transformation followed by a layer\nnormalization are performed to derive the utterance\nrepresentations for this speciﬁc task {urur\ni }N\ni=1,\nwhere urur\ni ∈ Rd and d = 768 . Then, for a\nspeciﬁc utterance Ui, its matching scores with all\nits preceding utterances are calculated as\nmij = softmax(urur⊤\ni ·Arur ·urur\nj ), (1)\nwhere Arur ∈Rd×d is a linear transformation, mij\ndenotes the matching degree of Uj being the reply-\nto utterance of Ui, and 1 ≤j < i. We construct a\nset S by sampling a certain number of utterances\n3685\nUi’\n Ui\n UN\n [SEP]Input\nToken\nEmbeddings\nSegment\nEmbeddings\nPosition\nEmbeddings\nSpeaker\nEmbeddings\n... ...\n... ...\n... ...\n... ...\nPre-trained Language Model (BERT)\nE[CLS]\n EU_i’\n E[CLS]\n EU_i\n E[CLS]\n EU_N\n E[SEP]Output ... ...\n[CLS]\n [CLS]\n [CLS]\n(a) Reply-to Utterance Recognition\nNon-linear Transformation\n+ Layer Normalization\nui'rur\n uirur\n uNrur... ...\nmij\n...\n...\n...\n...\n...\n...\nUj’\n Uj...\n[CLS]\n [CLS]...\n...\n...\n...\n...\n...\n...\nE[CLS]\n EU_j’\n E[CLS]\n EU_j......\nuj'rur\n ujrur......\n(b) Identical Speaker Searching\nNon-linear Transformation\n+ Layer Normalization\nui'iss\n uiiss\n uNiss... ......\nuj'iss\n ujiss......\n(c) Pointer Consistency Distinction\nNon-linear Transformation\n+ Layer Normalization\nui'pcd\n uipcd\n uNpcd... ......\nuj'pcd\n ujpcd......\nPointer \n Pointer \nSimilarity\nClassifier\n...\nFigure 1: Input representations and model architectures of the three self-supervised tasks for interlocutor structure\nmodeling, including (a) reply-to utterance recognition, (b) identical speaker searching and (c) pointer consistency\ndistinction.\nin a conversation and this recognition operation\nis performed for each utterance in S. Meanwhile,\na dynamic sampling strategy is adopted so that\nmodels can see more samples. Finally, the pre-\ntraining objective of this self-supervised task is to\nminimize the cross-entropy loss as\nLrur = −\n∑\ni∈S\ni−1∑\nj=1\nyij log(mij), (2)\nwhere yij = 1 if Uj is the reply-to utterance of Ui\nand yij = 0 otherwise.\n3.2.2 Identical Speaker Searching\nHaving knowledge of who is the speaker of an\nutterance is also important for MPC. The task\nof identical speaker searching (ISS) is designed\nby masking the speaker embedding of a speciﬁc\nutterance in the input representation, and aims to\npredict its speaker given the conversation. Since\nthe set of interlocutors vary across conversations,\nthe task of predicting the speaker of an utterance\nis reformulated as searching for the utterances\nsharing the identical speaker.\nFirst, for a speciﬁc utterance, its speaker embed-\nding is masked with a special[Mask] interlocutor\nembedding to avoid information leakage. Given\nthe utterance representations for this speciﬁc task\n{uiss\ni }N\ni=1 where uiss\ni ∈Rd, the matching scores of\nUi with all its preceding utterances are calculated\nsimilarly with Eq. (1). Here, mij denotes the\nmatching degree of Uj sharing the same speaker\nwith Ui. For each instance in the dynamic sampling\nset S, there must be an utterance in previous turns\nsharing the same speaker. Otherwise, it is removed\nout of the set. Finally, the pre-training objective\nof this task is to minimize the cross-entropy loss\nsimilarly with Eq. (2). Here, yij = 1 if Uj shares\nthe same speaker with Ui and yij = 0 otherwise.\n3.2.3 Pointer Consistency Distinction\nWe design a task named pointer consistency dis-\ntinction (PCD) to jointly model speakers and\naddressees in MPC. In this task, a pair of utterances\nrepresenting the “reply-to” relationship is deﬁned\nas a speaker-to-addressee pointer . Here, we\nassume that the representations of two pointers\ndirecting from the same speaker to the same\naddressee should be consistent. As illustrated in\nFigure 2 (a), speaker Sm speaks Ui and Uj which\nreply to Ui′ and Uj′ from speaker Sn respectively.\nThus, the utterance tuples (Ui, Ui′) and (Uj, Uj′)\nboth represent the pointer of S m-to-Sn and their\npointer representations should be consistent..\nGiven the utterance representations for this\nspeciﬁc task {upcd\ni }N\ni=1 where upcd\ni ∈Rd, we ﬁrst\ncapture the pointer information contained in each\nutterance tuple. The element-wise difference and\nmultiplication between an utterance tuple (Ui, Ui′)\nare computed and are concatenated as\npii′ = [upcd\ni −upcd\ni′ ; upcd\ni ⊙upcd\ni′ ], (3)\n3686\nUi \n Ui\n ...\n Uj \n Uj\nSn\nSm\n...\n...\n: Speaker\n: Utterance\n: Utterance-to-utterance\n: Speaker-to-utterance\n(a) Pointer consistency dis-\ntinction\nU1\n U2\n U3\n U5\n U8\nU4\n U6\nU7\nU9\n(b) Shared node detection\nFigure 2: Illustrations of the self-supervised tasks of\n(a) pointer consistency distinction and (b) shared node\ndetection. Rectangles denote utterances, circles denote\ninterlocutors, a solid line denotes an utterance replying\nto an utterance, and a dashed line denotes an utterance\nfrom an interlocutor.\nwhere pii′ ∈R2d. Then, we compress pii′ and\nobtain the pointer representation ¯pii′ as\n¯pii′ = ReLU(pii′·Wpcd + bpcd), (4)\nwhere Wpcd ∈R2d×d and bpcd ∈Rd are param-\neters. Identically, a consistent pointer representa-\ntions ¯pjj′ and an inconsistent one ¯pkk′ sampled\nfrom this conversation are obtained. The similari-\nties between every two pointers are calculated as\nmij = sigmoid(¯p⊤\nii′·Apcd ·¯pjj′), (5)\nwhere mij denotes the matching degree of pointer\n¯pii′ being consistent with pointer ¯pjj′. mik can\nbe derived accordingly. Finally, the pre-training\nobjective of this task is to minimize the hinge loss\nwhich enforces mij to be larger thanmik by at least\na margin ∆ as\nLpcd = max{0, ∆ −mij + mik}. (6)\n3.3 Tasks of Utterance Semantics Modeling\nIntuitively, the conversation structure might inﬂu-\nence the information ﬂow, so that it can be used to\nstrengthen the representations of utterance seman-\ntics. Thus, two self-supervised tasks following the\nstructure-to-semantics manner are designed.\n3.3.1 Masked Shared Utterance Restoration\nThere are usually several utterances replying-to\na shared utterance in MPC. Intuitively, a shared\nutterance is semantically relevant to more utter-\nances in the context than non-shared ones. Based\non this characteristic, we design a task named\nmasked shared utterance restoration (MSUR). We\nﬁrst randomly sample an utterance from all shared\nutterances in a conversation and all tokens in this\nsampled utterance are masked with a [MASK]\ntoken. Then the model is enforced to restore the\nmasked utterance given the rest conversation.\nFormally, assuming Ui as the masked shared ut-\nterance and li as the number of tokens in Ui. Given\nthe token representations for this task {umsur\ni,t }li\nt=1\nwhere umsur\ni,t ∈Rd, the probability distribution of\neach masked token can be calculated as\npui,t = softmax(umsur\ni,t ·Wmsur + bmsur), (7)\nwhere Wmsur ∈ Rd×V is the token embedding\ntable, V denotes the vocabulary size, and bmsur ∈\nRV is a bias vector. Finally, the pre-training\nobjective of this self-supervised task is to minimize\nthe negative log-likelihood loss as\nLmsur = −1\nli\nli∑\nt=1\nlog pui,t, (8)\nwhere pui,t is the element in pui,t corresponding to\nthe original token.\n3.3.2 Shared Node Detection\nA full MPC instance can be divided into several\nsub-conversations and we assume that the repre-\nsentations of sub-conversations under the same\nparent node tend to be similar. As illustrated in\nFigure 2 (b), two sub-conversations {U3, U5, U7,\nU8}and {U4, U6, U9}share the same parent node\nU2. Thus, they should be semantically relevant.\nUnder this assumption, we design a self-supervised\ntask named shared node detection (SND), which\nutilizes the conversation structure to strengthen the\ncapability of models on measuring the semantic\nrelevance of two sub-conversations.\nWe ﬁrst construct the pre-training samples for\nthis task. Empirically, only the sub-conversations\nunder the top shared node in a conversation are\ncollected in order to ﬁlter out the sub-conversations\nwith few utterances. Given a full MPC, the two\nsub-conversations with the most utterances form\na positive pair. For each positive pair, we replace\none of its elements with another sub-conversation\nrandomly sampled from the training corpus to form\na negative pair.\nFormally, given two sub-conversations ci and\ncj, utterances in each sub-conversation are ﬁrst\nconcatenated respectively to form two segments.\nThen, the two segments are concatenated with a\n[SEP] token and a [CLS] token is inserted at the\nbeginning of the whole sequence. This sequence\nare encoded by BERT to derive the contextualized\n3687\nrepresentation for the [CLS] token. A non-linear\ntransformation with sigmoid activation is further\napplied to this representation for calculating the\nmatching score mij, i.e., the probability of ci and\ncj sharing the same parent node. Finally, the pre-\ntraining objective of this task is to minimize the\ncross-entropy loss as\nLsnd = −[yijlog(mij) + (1−yij)log(1 −mij)],\n(9)\nwhere yij = 1 if ci and cj share the same parent\nnode and yij = 0 otherwise.\n3.4 Multi-task Learning\nIn addition, we also adopt the tasks of masked\nlanguage model (MLM) and next sentence predic-\ntion (NSP) in original BERT pre-training (Devlin\net al., 2019), which have been proven effective\nfor incorporating domain knowledge (Gu et al.,\n2020; Gururangan et al., 2020). Finally, MPC-\nBERT is trained by performing multi-task learning\nthat minimizes the sum of all loss functions as\nL= Lrur + Liss + Lpcd + Lmsur\n+ Lsnd + Lmlm + Lnsp. (10)\n4 Downstream Tasks\n4.1 Addressee Recognition\nGiven a multi-party conversation where part of the\naddressees are unknown, Ouchi and Tsuboi (2016)\nand Zhang et al. (2018a) recognized an addressee\nof the last utterance. Le et al. (2019) recognized\naddressees of all utterances in a conversation. In\nthis paper, we follow the more challenging setting\nin Le et al. (2019).\nFormally, models are asked to predict {ˆan}N\nn=1\ngiven {(sn, un, an)}N\nn=1\\{an}N\nn=1, where ˆan is\nselected from the interlocutor set in this conver-\nsation and \\denotes exclusion. When applying\nMPC-BERT, this task is reformulated as ﬁnding\na preceding utterance from the same addressee.\nIts RUR matching scores with all preceding ut-\nterances are calculated following Eq. (1). Then, the\nutterance with the highest score is selected and the\nspeaker of the selected utterance is considered as\nthe recognized addressee. Finally, the ﬁne-tuning\nobjective of this task is to minimize the cross-\nentropy loss as\nLar = −\nN∑\ni=2\ni−1∑\nj=1\nyij log(mij), (11)\nwhere mij is deﬁned in Eq. (1), yij = 1 if the\nspeaker of Uj is the addressee of Ui and yij = 0\notherwise.\n4.2 Speaker Identiﬁcation\nThis task aims to identify the speaker of the last\nutterance in a conversation. Formally, models are\nasked to predict ˆsN given {(sn, un, an)}N\nn=1\\sN ,\nwhere ˆsN is selected from the interlocutor set in\nthis conversation. When applying MPC-BERT, this\ntask is reformulated as identifying the utterances\nsharing the same speaker. For the last utterance\nUN , its speaker embedding is masked and its ISS\nmatching scores mNj with all preceding utterances\nare calculated following Section 3.2.2. The ﬁne-\ntuning objective of this task is to minimize the\ncross-entropy loss as\nLsi = −\nN−1∑\nj=1\nyNj log(mNj ), (12)\nwhere yNj = 1 if Uj shares the same speaker with\nUN and yNj = 0 otherwise.\n4.3 Response Selection\nThis task asks models to select ˆuN from a set of\nresponse candidates given the conversation context\n{(sn, un, an)}N\nn=1\\uN . The key is to measure the\nsimilarity between two segments of context and\nresponse. We concatenate each response candidate\nwith the context and extract the contextualized\nrepresentation e[CLS] for the ﬁrst [CLS] token\nusing MPC-BERT. Then,e[CLS] is fed into a non-\nlinear transformation with sigmoid activation to\nobtain the matching score between the context and\nthe response. Finally, the ﬁne-tuning objective\nof this task is to minimize the cross-entropy loss\naccording to the true/false labels of responses in\nthe training set as\nLrs = −[ylog(mcr)+(1 −y)log(1−mcr)], (13)\nwhere y = 1 if the response r is a proper one for\nthe context c; otherwise y = 0.\n5 Experiments\n5.1 Datasets\nWe evaluated our proposed methods on two Ubuntu\nIRC benchmarks. One was released by Hu et al.\n(2019), in which both speaker and addressee\nlabels was provided for each utterance. The other\nbenchmark was released by Ouchi and Tsuboi\n3688\nDatasets Train Valid Test\nHu et al. (2019) 311,725 5,000 5,000\nOuchi and Tsuboi (2016)\nLen-5 461,120 28,570 32,668\nLen-10 495,226 30,974 35,638\nLen-15 489,812 30,815 35,385\nTable 2: Statistics of the two benchmarks evaluated in\nthis paper.\n(2016). Here, we adopted the version shared\nin Le et al. (2019) for fair comparison. The\nconversation sessions were separated into three\ncategories according to the session length (Len-\n5, Len-10 and Len-15) following the splitting\nstrategy of previous studies (Ouchi and Tsuboi,\n2016; Zhang et al., 2018a; Le et al., 2019). Table 2\npresents the statistics of the two benchmarks\nevaluated in our experiments.\n5.2 Baseline Models\nNon-pre-training-based models Ouchi and\nTsuboi (2016) proposed a dynamic model\nDRNN which updated speaker embeddings with\nthe conversation ﬂow. Zhang et al. (2018a)\nimproved DRNN to SI-RNN which updated\nspeaker embeddings role-sensitively. Le et al.\n(2019) proposed W2W which jointly modeled\ninterlocutors and utterances in a uniform\nframework, and predicted all addressees.\nPre-training-based models BERT (Devlin et al.,\n2019) was pre-trained to learn general language\nrepresentations with MLM and NSP tasks. SA-\nBERT (Gu et al., 2020) added speaker embeddings\nand further pre-trained BERT on a domain-speciﬁc\ncorpus to incorporate domain knowledge. We\nre-implemented SA-BERT with the pre-training\ncorpus used in this paper to ensure fair comparison.\n5.3 Implementation Details\nThe version of BERT-base-uncased was adopted\nfor all our experiments. For pre-training, GELU\n(Hendrycks and Gimpel, 2016) was employed as\nthe activation for all non-linear transformations.\nThe Adam method (Kingma and Ba, 2015) was\nemployed for optimization. The learning rate was\ninitialized as 0.00005 and the warmup proportion\nwas set to 0.1. We pre-trained BERT for 10\nepochs. The training set of the dateset used in\nHu et al. (2019) was employed for pre-training.\nThe maximum utterance number was set to 7. The\nmaximum sequence length was set to 230. The\nmaximum sampling numbers for each example\nwere set to 4 for RUR, 2 for ISS and 2 for PCD.\n∆ in Eq. (6) was set to 0.4, achieving the best\nperformance out of {0.2, 0.4, 0.6, 0.8 }on the\nvalidation set. The pre-training was performed\nusing a GeForce RTX 2080 Ti GPU and the batch\nsize was set to 4.\nFor ﬁne-tuning, some conﬁgurations were dif-\nferent according to the characteristics of these\ndatasets. For Hu et al. (2019), the maximum\nutterance number was set to 7 and the maximum\nsequence length was set to 230. For the three\nexperimental settings in Ouchi and Tsuboi (2016),\nthe maximum utterance numbers were set to 5, 10\nand 15, and the maximum sequence lengths were\nset to 120, 220 and 320. All parameters in PLMs\nwere updated. The learning rate was initialized as\n0.00002 and the warmup proportion was set to 0.1.\nFor Hu et al. (2019), the ﬁne-tuning process was\nperformed for 10 epochs for addressee recognition,\n10 epochs for speaker identiﬁcation, and 5 epochs\nfor response selection. For Ouchi and Tsuboi\n(2016), the ﬁne-tuning epochs were set to 5, 5 and\n3 respectively. The ﬁne-tuning was also performed\nusing a GeForce RTX 2080 Ti GPU. The batch\nsizes were set to 16 for Hu et al. (2019), and 40, 20,\nand 12 for the three experimental settings in Ouchi\nand Tsuboi (2016) respectively. The validation set\nwas used to select the best model for testing.\nAll codes were implemented in the TensorFlow\nframework (Abadi et al., 2016) and are published\nto help replicate our results. 1\n5.4 Metrics and Results\nAddressee recognition We followed the metrics\nof previous work (Le et al., 2019) by employing\nprecision@1 (P@1) to evaluate each utterance with\nground truth. Also, a session is marked as positive\nif the addressees of all its utterances are correctly\nrecognized, which is calculated as accuracy (Acc.).\nTable 3 presents the results of addressee recog-\nnition. It shows that MPC-BERT outperforms\nthe best performing model, i.e., SA-BERT, by\nmargins of 3.51%, 2.86%, 3.28% and 5.36%\non these test sets respectively in terms of Acc.,\nverifying the effectiveness of the proposed ﬁve self-\nsupervised tasks as a whole. To further illustrate\nthe effectiveness of each task, ablation tests were\nperformed as shown in the last ﬁve rows of Table 3.\nWe can observe that all self-supervised tasks are\nuseful as removing any of them causes performance\n1https://github.com/JasonForJoy/MPC-BERT\n3689\nHu et al. (2019) Ouchi and Tsuboi (2016)\nLen-5 Len-10 Len-15\nP@1 Acc. P@1 Acc. P@1 Acc. P@1 Acc.\nPreceding (Le et al., 2019) - - 63.50 40.46 56.84 21.06 54.97 13.08\nSubsequent (Le et al., 2019) - - 61.03 40.25 54.57 20.26 53.07 12.79\nDRNN (Ouchi and Tsuboi, 2016) - - 72.75 58.18 65.58 34.47 62.60 22.58\nSIRNN (Zhang et al., 2018a) - - 75.98 62.06 70.88 40.66 68.13 28.05\nW2W (Le et al., 2019) - - 77.55 63.81 73.52 44.14 73.42 34.23\nBERT (Devlin et al., 2019) 96.16 83.50 85.95 75.99 83.41 58.22 81.09 44.94\nSA-BERT (Gu et al., 2020) 97.12 88.91 86.81 77.45 84.46 60.30 82.84 47.23\nMPC-BERT 98.31 92.42 88.73 80.31 86.23 63.58 85.55 52.59\nMPC-BERT w/o. RUR 97.75 89.98 87.51 78.42 85.63 62.26 84.78 50.83\nMPC-BERT w/o. ISS 98.20 91.96 88.67 80.25 86.14 63.40 85.02 51.12\nMPC-BERT w/o. PCD 98.20 91.90 88.51 80.06 85.92 62.84 85.21 51.17\nMPC-BERT w/o. MSUR 98.08 91.32 88.70 80.26 86.21 63.46 85.28 51.23\nMPC-BERT w/o. SND 98.25 92.18 88.68 80.25 86.14 63.41 85.29 51.39\nTable 3: Evaluation results of addressee recognition on the test sets. Results except ours are cited from Le et al.\n(2019). Numbers in bold denote that the improvement over the best performing baseline is statistically signiﬁcant\n(t-test with p-value < 0.05).\nHu et al. (2019) Ouchi and Tsuboi (2016)\nLen-5 Len-10 Len-15\nBERT (Devlin et al., 2019) 71.81 62.24 53.17 51.58\nSA-BERT (Gu et al., 2020) 75.88 64.96 57.62 54.28\nMPC-BERT 83.54 67.56 61.00 58.52\nMPC-BERT w/o. RUR 82.48 66.88 60.12 57.33\nMPC-BERT w/o. ISS 77.95 66.77 60.03 56.73\nMPC-BERT w/o. PCD 83.39 67.12 60.62 58.00\nMPC-BERT w/o. MSUR 83.51 67.21 60.76 58.03\nMPC-BERT w/o. SND 83.47 67.04 60.44 58.12\nTable 4: Evaluation results of speaker identiﬁcation on the test sets in terms of P@1. Numbers in bold denote that\nthe improvement over the best performing baseline is statistically signiﬁcant (t-test with p-value < 0.05).\ndrop. Among the ﬁve tasks, RUR plays the most\nimportant role, and the tasks focusing on modeling\ninterlocutor structure contribute more than those\nfor utterance semantics.\nSpeaker identiﬁcation Similarly, P@1 was em-\nployed as the evaluation metric of speaker iden-\ntiﬁcation for the last utterance of a conversation\nand the results are shown in Table 4. It shows that\nMPC-BERT outperforms SA-BERT by margins of\n7.66%, 2.60%, 3.38% and 4.24% respectively in\nterms of P@1. Besides, from the ablation results\nwe ﬁnd that all tasks are useful for improving\nthe performance of speaker identiﬁcation and\nISS and RUR contribute the most. In particular,\nremoving PCD, MSUR and SND only leads to\nslight performance drop. The reason might be\nthat the information conveyed by these tasks is\nredundant.\nResponse selection The Rn@k metrics adopted\nby previous studies (Ouchi and Tsuboi, 2016;\nZhang et al., 2018a) were used here. Each model\nwas tasked with selectingk best-matched responses\nfrom n available candidates, and we calculated the\nrecall as R n@k. Two settings were followed in\nwhich k was set to 1 and n was set to 2 or 10.\nTable 5 presents the results of response selec-\ntion. It shows that MPC-BERT outperforms SA-\nBERT by margins of 3.82%, 2.71%, 2.55% and\n3.22% respectively in terms of R 10@1. Ablation\ntests show that SND is the most useful task for\nresponse selection and the two tasks focusing on\nthe utterance semantics contribute more than those\n3690\nHu et al. (2019) Ouchi and Tsuboi (2016)\nLen-5 Len-10 Len-15\nR2@1 R10@1 R2@1 R10@1 R2@1 R10@1 R2@1 R10@1\nDRNN (Ouchi and Tsuboi, 2016) - - 76.07 33.62 78.16 36.14 78.64 36.93\nSIRNN (Zhang et al., 2018a) - - 78.14 36.45 80.34 39.20 80.91 40.83\nBERT (Devlin et al., 2019) 92.48 73.42 85.52 53.95 86.93 57.41 87.19 58.92\nSA-BERT (Gu et al., 2020) 92.98 75.16 86.53 55.24 87.98 59.27 88.34 60.42\nMPC-BERT 94.90 78.98 87.63 57.95 89.14 61.82 89.70 63.64\nMPC-BERT w/o. RUR 94.48 78.16 87.20 57.56 88.96 61.47 89.07 63.24\nMPC-BERT w/o. ISS 94.58 78.82 87.54 57.77 88.98 61.76 89.58 63.51\nMPC-BERT w/o. PCD 94.66 78.70 87.50 57.51 88.75 61.62 89.45 63.46\nMPC-BERT w/o. MSUR 94.36 78.22 87.11 57.58 88.59 61.05 89.25 63.20\nMPC-BERT w/o. SND 93.92 76.96 87.30 57.54 88.77 61.54 89.27 63.34\nTable 5: Evaluation results of response selection on the test sets. Results except ours are cited from Ouchi and\nTsuboi (2016) and Zhang et al. (2018a). Numbers in bold denote that the improvement over the best performing\nbaseline is statistically signiﬁcant (t-test with p-value < 0.05).\n5 10 15\nLength\n50\n60\n70\n80Session Accuracy\nBERT\nSA-BERT\nMPC-BERT\n(a) Addressee recognition\n5 10 15\nLength\n55\n60\n65Utterance Preision\nBERT\nSA-BERT\nMPC-BERT (b) Speaker identiﬁcation\n5 10 15\nLength\n54\n56\n58\n60\n62\n64Response Recall\nBERT\nSA-BERT\nMPC-BERT (c) Response selection\nFigure 3: Performance of models under different session lengths on the test sets of Ouchi and Tsuboi (2016) on\nthe tasks of (a) addressee recognition, (b) speaker identiﬁcation and (c) response selection.\nfocusing on the interlocutor structures.\n5.5 Discussions\nFigure 3 illustrates how the performance of BERT,\nSA-BERT and MPC-BERT changed with respect\nto different session lengths on the test sets of\nOuchi and Tsuboi (2016). It can be seen that\nthe performance of addressee recognition and\nspeaker identiﬁcation dropped as the session length\nincreased. The reason might be that longer ses-\nsions always contain more interlocutors which\nincrease the difﬁculties of predicting interlocutors.\nMeanwhile, the performance of response selection\nwas signiﬁcantly improved as the session length\nincreased. It can be attributed to that longer\nsessions enrich the representations of contexts\nwith more details which beneﬁt response selection.\nFurthermore, as the session length increased, the\nperformance of MPC-BERT dropped more slightly\nthan that of SA-BERT on addressee recognition and\nspeaker identiﬁcation, and the R10@1 gap between\nMPC-BERT and SA-BERT on response selection\nenlarged from 2.71% to 3.22%. These results imply\nthe superiority of MPC-BERT over SA-BERT on\nmodeling long MPCs with complicated structures.\n6 Conclusion\nIn this paper, we present MPC-BERT, a pre-trained\nlanguage model with ﬁve self-supervised tasks for\nMPC understanding. These tasks jointly learn who\nsays what to whom in MPCs. Experimental results\non three downstream tasks show that MPC-BERT\noutperforms previous methods by large margins\nand achieves new state-of-the-art performance on\ntwo benchmarks.\nAcknowledgments\nWe thank anonymous reviewers for their valuable\ncomments.\n3691\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg, Rajat Monga,\nSherry Moore, Derek Gordon Murray, Benoit\nSteiner, Paul A. Tucker, Vijay Vasudevan, Pete\nWarden, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In 12th USENIX Symposium\non Operating Systems Design and Implementation,\nOSDI 2016, Savannah, GA, USA, November 2-4,\n2016., pages 265–283.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short\nPapers), pages 4171–4186.\nJia-Chen Gu, Tianda Li, Quan Liu, Zhen-Hua\nLing, Zhiming Su, Si Wei, and Xiaodan Zhu.\n2020. Speaker-aware BERT for multi-turn response\nselection in retrieval-based chatbots. In CIKM\n’20: The 29th ACM International Conference on\nInformation and Knowledge Management, Virtual\nEvent, Ireland, October 19-23, 2020 , pages 2041–\n2044.\nJia-Chen Gu, Zhen-Hua Ling, and Quan Liu. 2019a. In-\nteractive matching network for multi-turn response\nselection in retrieval-based chatbots. In Proceedings\nof the 28th ACM International Conference on\nInformation and Knowledge Management, CIKM\n2019, Beijing, China, November 3-7, 2019 , pages\n2321–2324.\nJia-Chen Gu, Zhen-Hua Ling, Xiaodan Zhu, and Quan\nLiu. 2019b. Dually interactive matching network for\npersonalized response selection in retrieval-based\nchatbots. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on\nNatural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019 ,\npages 1845–1854. Association for Computational\nLinguistics.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 8342–8360.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nWenpeng Hu, Zhangming Chan, Bing Liu, Dongyan\nZhao, Jinwen Ma, and Rui Yan. 2019. GSN: A\ngraph-structured network for multi-party dialogues.\nIn Proceedings of the Twenty-Eighth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI\n2019, Macao, China, August 10-16, 2019 , pages\n5010–5016.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nRan Le, Wenpeng Hu, Mingyue Shang, Zhenjun You,\nLidong Bing, Dongyan Zhao, and Rui Yan. 2019.\nWho is speaking to whom? learning to identify\nutterance addressee in multi-party conversations. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 1909–\n1919.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A\nlarge dataset for research in unstructured multi-turn\ndialogue systems. In Proceedings of the SIGDIAL\n2015 Conference, The 16th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue,\n2-4 September 2015, Prague, Czech Republic, pages\n285–294.\nZhao Meng, Lili Mou, and Zhi Jin. 2018. Towards\nneural speaker modeling in multi-party conversation:\nThe task, dataset, and models. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation, LREC 2018, Miyazaki,\nJapan, May 7-12, 2018 . European Language Re-\nsources Association (ELRA).\nHiroki Ouchi and Yuta Tsuboi. 2016. Addressee and\nresponse selection for multi-party conversation. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2016, Austin, Texas, USA, November 1-4, 2016 ,\npages 2133–2143.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua\nBengio, Aaron C. Courville, and Joelle Pineau.\n2016. Building end-to-end dialogue systems using\ngenerative hierarchical neural network models. In\nProceedings of the Thirtieth AAAI Conference\non Artiﬁcial Intelligence, February 12-17, 2016,\nPhoenix, Arizona, USA, pages 3776–3784.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron C. Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating\ndialogues. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence, February 4-9,\n2017, San Francisco, California, USA, pages 3295–\n3301. AAAI Press.\n3692\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015.\nNeural responding machine for short-text conversa-\ntion. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing of the Asian Federation of\nNatural Language Processing, ACL 2015, July 26-\n31, 2015, Beijing, China, Volume 1: Long Papers ,\npages 1577–1586.\nChongyang Tao, Wei Wu, Can Xu, Wenpeng Hu,\nDongyan Zhao, and Rui Yan. 2019a. Multi-\nrepresentation fusion network for multi-turn re-\nsponse selection in retrieval-based chatbots. In\nProceedings of the Twelfth ACM International\nConference on Web Search and Data Mining, WSDM\n2019, Melbourne, VIC, Australia, February 11-15,\n2019, pages 267–275. ACM.\nChongyang Tao, Wei Wu, Can Xu, Wenpeng Hu,\nDongyan Zhao, and Rui Yan. 2019b. One time\nof interaction may not be enough: Go deep with\nan interaction-over-interaction network for response\nselection in dialogues. In Proceedings of the 57th\nConference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28-\nAugust 2, 2019, Volume 1: Long Papers , pages 1–\n11.\nWeishi Wang, Steven C. H. Hoi, and Shaﬁq R.\nJoty. 2020. Response selection for multi-party\nconversations with dynamic topic tracking. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 6581–\n6591.\nYu Wu, Wei Wu, Chen Xing, Ming Zhou, and\nZhoujun Li. 2017. Sequential matching network:\nA new architecture for multi-turn response selection\nin retrieval-based chatbots. In Proceedings of\nthe 55th Annual Meeting of the Association for\nComputational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Papers,\npages 496–505.\nRui Zhang, Honglak Lee, Lazaros Polymenakos, and\nDragomir R. Radev. 2018a. Addressee and response\nselection in multi-party conversations with speaker\ninteraction rnns. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of\nArtiﬁcial Intelligence (IAAI-18), and the 8th AAAI\nSymposium on Educational Advances in Artiﬁcial\nIntelligence (EAAI-18), New Orleans, Louisiana,\nUSA, February 2-7, 2018, pages 5690–5697.\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,\nXiujun Li, Chris Brockett, and Bill Dolan. 2018b.\nGenerating informative and diverse conversational\nresponses via adversarial information maximization.\nIn Advances in Neural Information Processing Sys-\ntems 31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December\n3-8, 2018, Montr´eal, Canada, pages 1815–1825.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun\nChen, Chris Brockett, Xiang Gao, Jianfeng Gao,\nJingjing Liu, and Bill Dolan. 2020. DIALOGPT\n: Large-scale generative pre-training for conversa-\ntional response generation. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics: System Demonstrations,\nACL 2020, Online, July 5-10, 2020 , pages 270–278.\nAssociation for Computational Linguistics.\nXiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu,\nYing Chen, Wayne Xin Zhao, Dianhai Yu, and\nHua Wu. 2018. Multi-turn response selection for\nchatbots with deep attention matching network. In\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers, pages 1118–1127.",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.8799935579299927
    },
    {
      "name": "Chen",
      "score": 0.7419875860214233
    },
    {
      "name": "Computer science",
      "score": 0.6807880401611328
    },
    {
      "name": "Joint (building)",
      "score": 0.5526514649391174
    },
    {
      "name": "Natural language processing",
      "score": 0.537083089351654
    },
    {
      "name": "Linguistics",
      "score": 0.504953920841217
    },
    {
      "name": "Computational linguistics",
      "score": 0.4863644242286682
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.47563791275024414
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47313252091407776
    },
    {
      "name": "Natural language",
      "score": 0.4322248697280884
    },
    {
      "name": "Engineering",
      "score": 0.15033236145973206
    },
    {
      "name": "Philosophy",
      "score": 0.0931738018989563
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}