{
  "title": "Factorized Transformer for Multi-Domain Neural Machine Translation",
  "url": "https://openalex.org/W3101235629",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2138352149",
      "name": "Yongchao Deng",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2096596849",
      "name": "Hongfei Yu",
      "affiliations": [
        "Soochow University",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2127223396",
      "name": "Heng Yu",
      "affiliations": [
        "Alibaba Group (Cayman Islands)",
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2146693851",
      "name": "Xiangyu Duan",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2105392445",
      "name": "Weihua Luo",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962945654",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W4288028629",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2950760213",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2760452458",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2757592053",
    "https://openalex.org/W2963841178",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2803739890",
    "https://openalex.org/W2740718109",
    "https://openalex.org/W2750588180",
    "https://openalex.org/W2963122608",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2567571499",
    "https://openalex.org/W2963366389",
    "https://openalex.org/W2962863357",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2947187520",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2963913356",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2952200364",
    "https://openalex.org/W2892244498",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2744813330",
    "https://openalex.org/W2988394319",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "Multi-Domain Neural Machine Translation (NMT) aims at building a single system that performs well on a range of target domains. However, along with the extreme diversity of cross-domain wording and phrasing style, the imperfections of training data distribution and the inherent defects of the current sequential learning process all contribute to making the task of multi-domain NMT very challenging. To mitigate these problems, we propose the Factorized Transformer, which consists of an in-depth factorization of the parameters of an NMT model, namely Transformer in this paper, into two categories: domain-shared ones that encode common cross-domain knowledge and domain-specific ones that are private for each constituent domain. We experiment with various designs of our model and conduct extensive validations on English to French open multi-domain dataset. Our approach achieves state-of-the-art performance and opens up new perspectives for multi-domain and open-domain applications.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4221–4230\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4221\nFactorized Transformer for Multi-Domain Neural Machine Translation\nYongchao Deng1, Hongfei Yu2, Heng Yu1, Xiangyu Duan2, Weihua Luo1\n1Machine Intelligence Technology Lab, Alibaba DAMO Academy\n2Soochow University, Suzhou, China\n{yongchao.dyc, yuheng.yh, weihua.luowh}@alibaba-inc.com\nhfyutravel@gmail.com, xiangyuduan@suda.edu.cn\nAbstract\nMulti-Domain Neural Machine Translation\n(NMT) aims at building a single system that\nperforms well on a range of target domains.\nHowever, along with the extreme diversity of\ncross-domain wording and phrasing style, the\nimperfections of training data distribution and\nthe inherent defects of the current sequential\nlearning process all contribute to making the\ntask of multi-domain NMT very challenging.\nTo mitigate these problems, we propose the\nFactorized Transformer, which consists of an\nin-depth factorization of the parameters of an\nNMT model, namely Transformer in this pa-\nper, into two categories: domain-shared ones\nthat encode common cross-domain knowledge\nand domain-speciﬁc ones that are private for\neach constituent domain. We experiment with\nvarious designs of our model and conduct ex-\ntensive validations on English to French open\nmulti-domain dataset. Our approach achieves\nstate-of-the-art performance and opens up\nnew perspectives for multi-domain and open-\ndomain applications.\n1 Introduction\nRecent advances in Neural Machine Transla-\ntion (NMT) (Bahdanau et al., 2015; Vaswani et al.,\n2017) have led to signiﬁcant improvement in terms\nof translation quality (Wu et al., 2016; Hassan\net al., 2018), opening new perspectives for Ma-\nchine Translation in real-world scenarios. In order\nto deliver trust-worthy translations for end users, an\nNMT system is often required to meet expert-level\ntranslation quality in one or multiple related target\ndomains, while performing well enough on a range\nof generic subjects, just like human experts do.\nHowever, requiring a single NMT system to\nperform well on multiple distant domains simul-\ntaneously is a very challenging task. First, lan-\nguages are highly polysemous: the same words or\nexpressions may have different meanings in dif-\nferent contexts. Also wording and syntactic style\nmay signiﬁcantly vary depending on the domains.\nSecond, a multi-domain NMT system in general\nsuffers from two major issues: Domain Bias and\nCatastrophic Forgetting (Mccloskey and Cohen,\n1989; Kirkpatrick et al., 2016; Thompson et al.,\n2019) . While the former biases the model toward\nwell-represented domains to the detriment of the\nlow-resource ones, the latter makes the sequential\nlearning process difﬁcult as the model keeps forget-\nting previously learned knowledge when exposed\nto the new training examples.\nMost of the existing NMT systems rely on the\nsame network to model all domains, which means\nthe same word embedding to represent all the mean-\nings of a word and the same set of parameters to\nmodel its depending contexts. This type of conﬁg-\nuration in general maximizes the knowledge trans-\nfer, but overlooks the speciﬁcity of each domain\n(Koehn and Knowles, 2017). An obvious solution\nfor this problem is to dedicate an individual model\nto each constituent domain, which is unrealistic\nin practice as it dramatically increases the number\nof model parameters. Moreover, the recent suc-\ncess of multilingual applications (Johnson et al.,\n2017) show that a single NMT model where all\nparameters are shared can handle translation be-\ntween hundred of language pairs, suggesting that\nmodel capacity may not be the key weakness of the\ncurrent NMT models to deal with Multi-Domain\nproblems. Thus, the need for a compact architec-\nture with better parameter efﬁciency is appealing.\nWe propose the Factorized Transformerframe-\nwork to deal with the multi-domain NMT problem.\nThe Factorized Transformer consists in factorizing\npartially or fully basic components (embedding,\nattention and FFN layers) of a conventional Trans-\nformer architecture into domain-speciﬁc blocks and\ndomain-shared blocks. This dual structure has sev-\neral advantages: 1) It allows the model to leverage\nall available data, labeled or unlabeled, to build a\ngeneric model at an early stage of domain-agnostic\n4222\ntraining; 2) Domain singularities could be effec-\ntively learned by using domain-speciﬁc compo-\nnents and the respective in-domain training data\nduring the stage of domain-aware training. The\ndomain bias issue naturally disappears; 3) Domain-\nspeciﬁc components are independently optimized,\nwithout any interference between target domains.\nThe original performance of the generic model on\nun-adapted source domains is also preserved, over-\ncoming the limit of catastrophic forgetting. 4) The\ndesign of Factorized Transformer is orthogonal to\nany data-driven approach, so that the beneﬁt of\nboth approaches can be combined.\nOur contributions can be summarized as follows:\n•We address the weaknesses of existing NMT\nsystems in multi-domain scenarios by propos-\ning the Factorized Transformer, which sep-\narately model domain-shared and domain-\nspeciﬁc information via its dual structure.\n•We validate our method over a large-scale\nEnglish to French multi-domain setting. We\nstudy 3 variants of Factorized Transformer\nmeeting different requirements of perfor-\nmance and parameter space limitation, our\napproach outperforms all previously state-of-\nthe-art multi-domain systems, reaching close\nto the combined performance of individual\nﬁne-tuned models.\n•Our proposed architecture enables new per-\nspectives for open domain applications.\n2 Related Work\nMulti-domain NMT has been an active research\narea. Prior work in this area can be divided into\ntwo main categories: data-driven and model-driven,\nalthough they are usually complementary.\nData-Driven Approaches Many researches fo-\ncus on the exploration of data-driven approaches\n(van der Wees et al., 2017; Sajjad et al., 2017;\nWang et al., 2018). Chu et al. (2017) ﬁnetuned\nthe model using the mix of in-domain and out-of-\ndomain training corpora. Chen et al. (2017) scaled\nthe top-level costs of NMT system according to\neach training sentence‘s similarity to the develop-\nment set. Contrastingly, Farajian et al. (2017)\nutilized the similarity between each test sentence\nand the training instances to dynamically set the\nhyper-parameters of the learning algorithm and up-\ndate the generic model on the ﬂy. Li et al. (2018)\nwent a step further by adapting a separate model\nfor each sentence to boost the performance. While\ndata-driven approaches are very effective in alle-\nviating the domain bias issue in multi-domain sce-\nnarios, they in general require a very careful hyper-\nparameters tuning and cannot reach optimums for\nall domains simultaneously, unless resort to scaling\nup brutally the number of models.\nThe model-driven approaches can be subdivided\ninto two sub-categories:\nSoft-Constraints-Based Approaches The sub-\ncategory consists in injecting domain information\ninto the model parameters, by the means of side-\nconstraints, domain embeddings, so as to endow\nthese parameters with domain knowledge, to make\nthem domain-aware. Kobus et al. (2017) added an\nartiﬁcial token to the end of the input sequence to\nindicate the required target domain and exploited\ndomain as a tag or a feature. Britz et al. (2017) em-\nployed discriminators, training objective or GAN-\nlike techniques to incorporate domain knowledge\ninto the encoder or decoder. Chu and Dabre (2019)\ntreated text domains as distinct languages in or-\nder to use multi-lingual approaches when imple-\nmenting multi-domain NMT. Zeng et al. (2018)\ncombine source-target domain classiﬁers and adver-\nsarial domain classiﬁer during training. However,\nsince the main model parameters (embeddings, en-\ncoder, decoder) remain shared across all domains,\nthe capacity of these methods to deal with the inter-\ndomain conﬂicts might be limited.\nHard-Constraints-Based Approaches involve\ndedicating extra parameters to directly model\ndomain-speciﬁc knowledge. Michel and Neubig\n(2018) introduces speaker-speciﬁc softmax bias to\ndeal with adaptation for a large number of speak-\ners, the idea of parameter factorization is also ex-\nploited. Adapter tuning is a recently arisen ap-\nproach for transfer learning (Rebufﬁ et al., 2017,\n2018; Houlsby et al., 2019; Stickland and Murray,\n2019). Each task/domain is equipped with its own\nset of parameters in order to model and capture\ndomain speciﬁcity, which is decoupled among dif-\nferent tasks. Bapna et al. (2019) successfully adapt\nthis approach for domain adaptation and multilin-\ngual NMT models.\nOur work falls into the second sub-category of\nthe model-driven approaches and we hypothesize\nthat the idea of introducing decoupled domain-\nspeciﬁc parameters is crucial. We conduct exper-\n4223\niments and analysis in the following sections to\nvalidate this hypothesis.\n3 Approach\nAll basic components (embedding, attention and\nFFN matrices) of a conventional Transformer are\nfactorized into multiple domain-speciﬁc blocks\n(Figure 1), one for each domain (colored ones)\nand a domain-shared block (white ones), common\nacross all domains.\nIt’s worth to notice that domain information is\nnecessary for both training and inference, which\ncould be obtained via external sources. Neverthe-\nless, the domain prediction is not the main purpose\nof this work and we suppose in the whole paper, ex-\ncept otherwise mentioned, that domain information\nis known and passed as input to the model during\ntraining and inference.\n3.1 Training Curriculum\nWe ﬁrst brieﬂy explain the training curriculum be-\nfore moving to the detailed schemes of factoriza-\ntion, as the former is complementary to the latter\nand is designed to take advantage of the latter. The\ntraining curriculum can be theoretically divided\ninto two stages: an early stage of domain-agnostic\ntraining and a later stage of domain-speciﬁc train-\ning, even though in practice, it could be achieved\nin an end-to-end curriculum.\nDomain-Agnostic Training aims at building a\ngeneric model by sharing the model parameters\nacross all available training domains. Using all\navailable training data is beneﬁcial for the model’s\noverall performance as it allows the model to lever-\nage knowledge from other domains that are related\nor close to the target domains. For example, the\n“JRC Acquis” domain (a collection of legislative\ntexts of the European Union) would probably ben-\neﬁt from adding “europarl” domain (a collection\nof European Parliament texts) training data. Many\ndata weighting schemes exist in the literature, how-\never, this is beyond the scope of this paper and more\nimportantly, the design of Factorized Transformer\nis orthogonal to any data-driven approach, so that\nthe beneﬁt of both approaches can be combined.\nDomain-Speciﬁc Training Once the generic\nmodel comes to a convergence, the domain-shared\nparameters of the resulting generic model are then\nfrozen. We unfold all domain-speciﬁc components\nto the number of target domains and initialize them\nwith the same corresponding matrix trained during\nthe ﬁrst stage. The specialization step is straight-\nforward: the optimization of each set of domain-\nspeciﬁc parameters can operate independently us-\ning the respective relevant in-domain data.\nAs each domain-speciﬁc matrix is initialized\nwith the corresponding parameters from the under-\nneath pre-trained network. Therefore, no transition\nperformance degradation is observed along the ex-\ntra module integration if any. In the case where\nan additional adaptation layer is involved (Fig 1\n(F6)), we initialize it to a block identity tensor to\nmaintain the exact model performance coming off\nthe domain-agnostic training. This property is of\ngreat practical value as it allows the network to\nadapt directly on top of a set of well-optimized pa-\nrameters. Similar design can be found in adapter\nmodules: (Rebufﬁ et al., 2018; Houlsby et al.,\n2019; Stickland and Murray, 2019), which relies\non skip-connection or residual-connection in order\nto obtain a near identity initialization. Moreover,\n(Houlsby et al., 2019) observed that if the initial-\nization deviates too far from the identity function,\nthe model may fail to train with adapter modules\nfor transferring BERT style parameters across NLP\ntasks. However, our proposed Factorized Trans-\nformer does not suffer from such problem as it has\nthe exact identity initialization property.\n3.2 Factorization Schemes of Basic\nComponents\nThroughout this section, we ignore all bias terms,\nas they may or may not exist depending on the vari-\nant/block of the Transformer architecture and also\ndo not add signiﬁcantly to the parameter count. We\nﬁrst go through some notations before getting into\narchitecture description, dm refers to the dimen-\nsion of the model, which is equal to embedding\nsize de and hidden size dh in a conventional Trans-\nformer. V refers to the vocabulary size, without\nloss of generality, we suppose the source side and\ntarget side both share the same vocabulary size for\nthe theoretical considerations. dfilter refers to the\nﬁlter dimension used in the FFN layers. hdenotes\nthe number of heads used in multi-head attention.\nNd represents the number of constituent domains.\nFinally, we introduce an extra dimension dinner as\nthe inner dimension used for linear factorization\nthat we will explain in the following paragraphs.\nFactorization of Embedding Blocks A conven-\ntional Transformer network has three wide embed-\n4224\nV\nRELU\nRELU\nO\nQ\nO\nQ\nK V\n（F1）  （F3）  （F2）  \n（F4）  （F5）  （F6）  \nDF = F1  + F2  + F3  + F4 \nSF = F2  + F6  \nPA = F5\nK\n: hidden size ／ num_head\n: inner size : ﬁlter size\n: vocab size\n: hidden size \n: Domain shared matrix \n: Domain speciﬁc matrices\nDomain speciﬁc matrix i\nactive for input of domain i :\n: Domain speciﬁc matrix j \nnon-active for input of domain  i\nFactorization schemes\nEmbedding blocks Attention blocks FFN blocks\nFigure 1: Factorization schemes for basic blocks. ( F1): output embedding, ( F2): multi-query attention, ( F3):\nFFN layer, (F4): input embeddings, ( F5): multi-head attention, ( F6): FFN layer v2. DF, SF and PA respectively\nstands for Deep Factorization, Shallow Factorization and Parallel Attention. Different matrix sizes involved in the\nfactorization could be identiﬁed by segment length at the top left corner.\nding matrices We of dimensions dm ∗V, which are\noften tied or partially tied (Press and Wolf, 2016)\nto reduce model size. NMT models usually require\nthe vocabulary size V to be large, V is of the order\nof 100 ∗dm. This can easily result in an embed-\nding matrix with millions of parameters, many of\nwhich are only updated sparsely during training.\nWe follow the work of Lan et al. (2019) to fac-\ntorize these blocks (Fig 1 ( F1) and (F4)). More\nspeciﬁcally, for each embedding matrix Me, we de-\ncompose it along an inner dimension dinner (Eq 1):\nWemb = WC ×WS\ni , where WC is a shared matrix\nand Wi is a speciﬁc matrix for i ∈1 ...N d. The\nadvantage of such decomposition is two-fold: First,\ninstead of sharing the same word embedding for all\ndomains, the domain-speciﬁc sub-matrices provide\na capacity for the model to give a domain-speciﬁc\nmeaning to each word embedding. Secondly, from\na practical perspective, by using this decomposi-\ntion, we reduce the embedding parameters from\nO(V ×dm) to O(V ×dinner +Nd ×dinner ×dm).\nIf dinner ≪dm, the factorized form’s parameter\ncost remains inferior to the original embedding\nblock, resulting in better usage of model parame-\nters.\nEMB(h,Dd) =\nNd∑\nj=1\nδdjEMBDd (h)\nwhere EMBDd (h) = WS\nDd WCh\n(1)\nWhere the weight matrices are of dimension:\nWC ∈RV ×dinner , WS\nj∈1...Nd ∈Rdinner×dm\nFactorization of Attention Blocks The factor-\nization of the attention blocks operates differently\nfrom the embedding blocks, as each attention block\nis composed of four relatively small weight matri-\nces WQ,WK,WV ,WO. Within the Multi-Head\nAttention (MHA) in a conventional Transformer,\nthey are square matrices of the same dimensions\nd2\nm. In the case of Multi-Query Attention (MQA)\n(Shazeer, 2019) instead of multi-head, we share the\nsame key and value sub-matrices for all the heads,\nthe dimensions of matrices WK,WV are reduced\nto dm ∗dk = d2\nm/h.\nWe consider two schemes of introducing domain-\nspeciﬁc components. A “full” scheme (Fig 1\n4225\n(F5)) which consists in assigning different matri-\nces for each domain for each transformation of\nWQ,WK,WV ,WO in multi-head style attention,\nand a “light” scheme (Fig 1 ( F2)) which only par-\nallelizes the relatively small matrices of WK,WV\nof the multi-query style attention. Concretely, if\nwe denote the conventional attention mechanism\nas follows:\nMHA(h) = [(H1(h),...,H h(h)]WO\nHk(·) = Softmax(⟨WQ\nk ·,WK\nk ·⟩√dk\n)WV\nk ·\n(2)\nwhere [·,..., ·] stands for concatenation and ⟨·,·⟩\nfor dot product.\nThe factorization of the attention block in the\nfull scheme with multi-head style attention can be\nwritten as:\nMHADd (h) = [(HDd\n1 (h),...,H Dd\nh (h)]WO\nDd\nHDd\nk (·) = Softmax(\n⟨WQ\nDd,k·,WK\nDd,k·⟩\n√\ndm/h\n)WV\nDd,k·\n(3)\nAnd in the case of the light scheme with Multi-\nQuery Attention:\nMQADd (h) = [(HDd\n1 (h),...,H Dd\nh (h)]WO\nHDd\nk (·) = Softmax(\n⟨WQ\nk ·,WK\nDd ·⟩√dk\n)WV\nDd ·\n(4)\nWhile the latter remains parameter efﬁcient un-\nless Nd ≫h, the former signiﬁcantly increases the\nmodel parameters.\nFactorization of FFN Blocks FFN blocks are\ncomposed of coupled linear matrices joined via a\nReLU activation on their amplifying inner dimen-\nsion dfilter . We could perform twice the linear fac-\ntorization as for case of embedding matrices (Fig\n1 (F3)), or introduce an extra layer of square matri-\nces, one for each domain (Fig 1 ( F6)). In general,\nfew additional parameters are needed for the factor-\nization of the FFN blocks unless Nd ≫dfilter /dm\nFFN(h) = W2(max(0,W1h)) (5)\nwhere the weight matrices are of dimension: W1 ∈\nRdm×dfilter , W2 ∈Rdfilter ×dm\nThe ﬁrst factorization scheme (Fig 1 ( F3)) for\nthe FFN block can be written as:\nFFN(h,Dd) =\nNd∑\nj=1\nδdjFFNDd (h)\nwhere FFNDd (h) = f2,Dd (max(0,f1,Dd (h)))\nfi,Dd (h) = WS\ni,Dd WC\ni h\n(6)\nwhere the weight matrices are of dimension:\nWC\n1 ∈Rdm×dinner , WS\n1,j∈1...Nd ∈Rdinner×dfilter ,\nWC\n2 ∈Rdfilter ×dinner , WS\n2,j∈1...Nd ∈Rdinner×dm\nThe second factorization scheme (Fig 1 ( F6))\nfor the FFN block can be formulated as:\nFFNV 2(h,Dd) =\nNd∑\nj=1\nδdjWA\nDd FFN(h) (7)\nwhere WA\nj∈1...Nd ∈Rdm×dm\n3.3 Overall Architecture Designs of\nFactorized Transformer\nWe consider three architecture designs of Factor-\nized Transformer for multi-domain NMT in this pa-\nper, namely Deep Factorization (DF), Shallow Fac-\ntorization (SF) and Parallel Attention (PA). These\ndesigns have been deliberately chosen as extreme\ncases to provide insights on the limits of the Fac-\ntorized Transformer, regarding different require-\nments of performance and parameter space limita-\ntion. Other more progressive combination schemes\ncould be also interesting to be investigated depend-\ning on the ﬁnal goal and constraints of applications.\nDeep Factorization (DF) We combine the fac-\ntorization schemes (F1), (F2), (F3), (F4), and it’s\ncalled deep factorization, since factorization is ap-\nplied to all the main blocks and the combination\nof domain-shared parameters and domain-speciﬁc\nparameters occur through the whole model. We set\nthe dinner to 280 to obtain the same model capacity\nas the Transformer base setting for fair comparison.\nShallow Factorization (SF) We rely on the en-\ntire original architecture of Transformer to encode\ndomain-shared knowledge as a conventional Trans-\nformer, so that we will not suffer from the loss\nof knowledge transfer capacity compared to the\noriginal Transformer. The domain-speciﬁc com-\nponents are plugged into the main architecture as\nlight weight add-on modules. We also duplicate the\n4226\nDomain Train Scale Dev Test\nOOD 70M x1 - -\nEmea 338K x50 1K 1K\nSubtitles 34M x1 1K 1K\nNews 197K x100 1K 1K\nIwslt 223K x100 1K 1K\nJrc 483K x50 1K 1K\nTotal 105M x2 - -\nTable 1: Statistics of training corpora: “OOD” stands\nfor Out-Of-Domain, “Scale” indicates the scale factor\nfor oversampling.\nkey, value matrices as domain-speciﬁc components.\nIt corresponds to the combination of factorization\nschemes (F2) and (F6) in Figure 1.\nParallel Attention (PA) We parallelize all the at-\ntention matrices of the original multi-head attention\n(Vaswani et al., 2017) to boost the model capacity\nreserved to each domain. This conﬁguration (Fig\n1 (F5)) can be seen as a factorization of the entire\nnetwork into domain-shared non-attention blocks\nand domain-speciﬁc blocks.\n4 Experiments\n4.1 Experiment settings\nDatasets In this paper, we evaluate our proposed\nmethod on a 100 million English-French open\nmulti-domain dataset from OPUS corpus1. It con-\ntains sentences from twelve domains including\nNews, Spoken, Laws and Medical etc. We di-\nvided the corpora into training, development and\ntest sets. We select ﬁve domains of News, Iwslt,\nJrc, Emea and Subtitles as evaluation criteria, all\ndata from other domains are considered as out-\nof-domain data and used for training only. Fol-\nlowing (Sajjad et al., 2017), we oversampled the\nlow-resource domains to match the same order of\nsize for high resource domains, out-of-domain sen-\ntences are not concerned by the oversampling. All\nsentence pairs are then concatenated and shufﬂed\ninto a ﬁnal training data. We tokenize English and\nFrench sentences using MOSES script2. Byte-pair\nencoding (Sennrich et al., 2016) is employed in\nthe experiment 50,000 joint pairs, the source and\ntarget vocabulary is set to the 50,000 most frequent\ntokens . Table 1 provides the corpora statistics used\nin our experiments.\n1http://opus.nlpl.eu/\n2http://www.statmt.org/moses/\nSystems Settings We employ Transformer\n(Vaswani et al., 2017) as our basis architecture.\nSix layers are stacked in both the encoder and\ndecoder, and the dimensions of the embedding\nvectors and all hidden vectors are set to 512.\nThe inner layer of the feed-forward sublayer has\nthe dimension of 2048. We use 8 heads in the\nmulti-head or multi-query attention. The target\nembedding and the output embedding are shared in\nour experiments. We use the Adam optimizer with\nβ1 = 0.9, β2 = 0.997, ε= 10−9 during training. The\ninitial learning rate is 0.0003. The learning rate\ndecay schedule is applied for initial warm up and\nannealing (Vaswani et al., 2017). During training,\neach mini-batch contains 4096 tokens and we\nuse a dropout rate of 0.1 on all datasets including\nattention dropout. During evaluation, we employ\nlowercase token BLEU (Papineni et al., 2002) as\nour evaluation metric and use mteval-13a script. In\naddition, during decoding, we use the beam search\nalgorithm and the beam size is set to 4.\nBenchmark Systems We compare our system\nwith multi-domain systems previously reported\nin the literature, a system is considered as multi-\ndomain system if all its parameters can be con-\ntained within a uniﬁed and deployment-friendly\nframework. Such candidates are Domain Control\n(DC) (Kobus et al., 2017) and Target Token Mix-\ning (TTM) (Britz et al., 2017), which are side-\nconstraint based pioneer works of using domain\ninformation for multi-domain training; Multitask\nLearning (ML) (Britz et al., 2017) method and the\nWord-level Domain Context (WDC) (Zeng et al.,\n2018) method both add classiﬁers to the training\nso that the network can distinguish mulit-domain\ncontexts; As mentioned in the introduction, adapter-\nbased method is also considered. We use the “bot-\ntleneck” Residual Adapters (RA) reported in Bapna\net al. (2019) with an inner dimension set to 2048.\nWe re-implement all previously reported RNN-\nbased approach with the Transformer architecture\nfor fair comparison.\nWe omit any data-driven approach, as it is orthog-\nonal to our approach and can be naturally combined\ntogether. We choose a balanced scheme described\nabove as a pretty strong data-mixing baseline, the\nbest system after several preliminary experiments.\n4.2 Experimental Results and Analysis\nThe results of our system are shown at the bottom\nof Table 2. The performances of benchmark multi-\n4227\nSYSTEMS #P NEWS IWSLT JRC EMEA SUB A VG-5 ∆\nTranformer-base 1x 35.33 41.49 64.20 56.58 30.66 45.65 -\nTranformer-base+ﬁnetuning (FT) 5x 35.46 41.63 69.00 61.96 33.42 48.30 +2.65\nDomain Control (DC) 1x 36.12 41.47 63.97 56.15 30.97 45.73 +0.08\nTarget Token Mixing (TTM) 1x 35.97 41.81 64.05 56.04 30.74 45.72 +0.07\nMultitask Learning (ML) 1x 34.87 41.72 64.04 56.57 30.35 45.51 -0.14\nWord-level Domain Context (WDC) 1x 36.26 41.73 64.54 56.49 30.78 45.96 +0.31\nResidual Adapters (RA) 2.3x 35.33 41.49 65.90 59.72 32.31 46.95 +1.30\nFactorized Transformer (ours)\nDeep Factorization (DF) 1x 35.92 41.39 66.03 † 59.25 32.89 † 46.99† +1.34†\nShallow Factorization (SF) 1.1x 36.38† 42.46† 65.47 58.63 32.34 † 47.05† +1.40†\nParallel Attention (PA) 1.8x 35.39 41.69 67.21 † 61.70† 33.14† 47.78† +2.13†\nTable 2: Benchmark results on 105 million English to French multi-domain open data. “#P” denotes the scale factor\nof parameter compared to the baseline. “A VG-5” refers to the average score across the 5 domains“ †” indicates\nthe scores of our systems that outperform all other benchmark systems except the combined performance of 5\nindividual ﬁnetuned models.\ndomain systems are reported at the upper part of\nTable 2. A standard Transformer base setting is\nused as baseline for our experiments. It worth to\nnotice that the extensive use of extra out-of-domain\ngeneral data contributes for the strong performance\nof the baseline model for general domains, no sig-\nniﬁcant improvement is observed even after ﬁne-\ntuning (Luong and Manning, 2015) with in-domain\ndata for News and Iwslt domains. We refer to the\naverage score over the 5 target domains (A VG-5)\nas multi-domain performance. We also report the\ncombined performance of 5 fully ﬁne-tuned mod-\nels as the upper bound performance (+2.65 BLEU\nin average) for Multi-Domain approaches.\nOur proposed Factorized Transformer systems\nclearly outperform the baseline and other multi-\ndomain systems in terms of multi-domain perfor-\nmance (A VG-5) as well as individual performance\nfor most settings: our Deep Factorization, Shallow\nFactorization, Parallel Attention systems respec-\ntively yield +1.34, +1.40 and +2.13 BLEU gain\nover the baseline system. Substantial gains are ob-\nserved for the domains of JRC (law text), EMEA\n(medical text) and SUB (subtitles) which have ev-\nery speciﬁc terminologies and syntactic style. No\nsigniﬁcant improvement is observed for the do-\nmains of NEWS and IWSLT, which are still kinds\nof general domains.\nSurprisingly, most of the previous multi-domain\ntechniques, except adapter-based approach, yield\nvery marginal gain over the Transformer baseline in\nour experiment setting. As all these techniques are\nre-implemented under the Transformer architecture,\nwe assert that Transformer may have a stronger out-\nof-the-box expressive ability compared to its RNN-\nbased counterparts. Also, all soft-constraint-based\nParameter Efﬁciency\n0.00\n0.35\n0.70\n1.05\n1.40\nFT DC TTM ML WDC RA DF SF PA\nFigure 2: Parameter Efﬁciency (= ∆ / #P from Table 2)\nfor multi-domain benchmark systems.\nsystems perform better for domains that are closed\nto general domains (News, Iwslt) with big amount\nof out-of-domain data than the low-resource and\nover-sampled ones, which validate the assumption\nthat models with a single shared set of parameters\nare more likely to be biased toward high resource\ndomains to the detriment of the low-resource ones.\nAdapter-based system has the closest overall perfor-\nmance, demonstrating the beneﬁt of separating the\ntraining process into domain-shared and domain-\nspeciﬁc stages with the corresponding shared or\ndomain-speciﬁc parameters.\nParameter Efﬁciency All of our systems\ndemonstrate better parameter efﬁciency, measured\nby the ratio between the performance gain and the\nparameter scale factor (Fig 2).\nImpact of Catastrophic Forgetting Our Factor-\nized Transformer can also be used for domain adap-\ntation tasks. One of the main concerns of domain\n4228\nSYSTEMS In Out-Of-Domain A VG-5SUB IWSLT EMEA\nTransformer 30.66 41.49 56.58 45.65\n+ Finetune 33.42 38.22 29.49 33.70\n+ L2 reg 31.81 39.47 47.82 42.22\n+ EWC 31.96 39.25 50.41 43.10\n+ Mix-Finetune 31.98 41.23 58.01 46.32\nFactorTrans-PA 33.42 41.52 58.59 46.85\nTable 3: Benchmark for Domain Adaptation Tech-\nniques. The domain SUB is ﬁne-tuned using in-domain\ndata, the results of JRC and NEWS domains are omit-\nted for space reason, which are taken into account in\nthe average score (A VG-5). FactorTrans-PA refers to\nthe Parallel Attention design of our approach using the\nﬁne-tuned model as pre-trained model.\nadaptation is how to limit the degradation caused by\nthe catastrophic forgetting problem. Table 3 shows\nthe benchmark results between one of our Factor-\nized Transformer system (PA) and some popular do-\nmain adaptation techniques. The ﬁne-tuned system\nachieves the best in-domain performance (Subtitle),\nhowever, it suffers from severe catastrophic forget-\nting problem as its performance in the domain of\nEMEA is nearly halved. Our Factorized Trans-\nformer can operate on top of the ﬁne-tuned system\nto recover most of the performance drop while pre-\nserving the optimal in-domain performance, The\nresulting system outperforms the ﬁne-tuned sys-\ntem by +13.12 BLEU and the baseline system by\n+1.20 BLEU in overall performance. Introducing\nregularization techniques such as L2 (Barone et al.,\n2017), EWC (Kirkpatrick et al., 2016; Thompson\net al., 2019) and mix-ﬁnetuning (Chu et al., 2017)\ncan alleviate the drop in the domains of IWSLT,\nhowever it limits the performance of in-domain.\n5 Towards Open-Domain NMT\nIn many real-world scenarios, the domain infor-\nmation is unknown at inference time, and even\nworse, the test inputs may also be out-of-domain,\nwhich means the model has never seen data from\nthe same domains during training. For such un-\nknown domains, NMT systems are known to have\npoor performance, especially adapted ones (Freitag\nand Al-Onaizan, 2016; Koehn and Knowles, 2017).\nModel ensembling is a reasonable approach\nto deal with unknown domains (Freitag and Al-\nOnaizan, 2016; Saunders et al., 2019). The com-\npact and uniﬁed architecture of Factorized Trans-\nformer makes it ideal for this purpose as at each\nstep all domain-speciﬁc representations can be\nSYSTEMS Open Tag-Free\nIT EMEA SUB\nTransformer (no IT) 32.33 56.58 30.66\nFactorTrans-PA - - -\n+ use tag (oracle) 32.33 58.59 33.42\n+ ens-uniform 29.47 53.12 30.76\n+ ens-soft 31.25 58.21 33.26\n+ ens-learnable 31.10 58.38 33.41\nTable 4: Experimental results for Open-Domain set-\nting. ens-uniform refers to the ensemble system with\nﬁxed equal weights; ens-soft: weight as classiﬁer’s out-\nput distribution, over the known domains only; ens-\nlearnable: weight vector tuned over balanced train/dev\ndata from known domains all combined.\ncomputed in parallel and feed-forward to obtain\nmultiple domain-speciﬁc word prediction probabil-\nities (logits). We consider in this section the unseen\nIT domain as a new unknown test domain. The test\nset is drawn from the GNOME corpus from the\nOPUS website. Under the open domain paradigm,\nwe do not use any development or training data. We\nexperiment with 3 simple variants of model ensem-\nbling based on the Parallel Attention design of our\napproach (See Table 4 for details). We ensemble\nall of the 5 adapted domains’ output and that of the\n“general” domain, which corresponds to the base\nmodel before any domain-aware training and is\nmore likely to have good performance for unknown\ndomains than its adapted counterparts (Freitag and\nAl-Onaizan, 2016; Saunders et al., 2019).\nThe results (Table 4) demonstrate the potential\nof our Factorized Transformer for open-domain ap-\nplications: not surprisingly, a naive combination of\nadapted systems (ens-uniform) result in degrada-\ntion in all domains. The ens-soft and ens-learnable\nsystems both manage to preserve the in-domain per-\nformance for known domains while still performing\nreasonably well for the unknown IT domain.\n6 Conclusion\nIn this paper, we propose the Factorized Trans-\nformer framework to overcome the limits of tradi-\ntional multi-domain NMT approaches in modeling\nall domain knowledge within a single shared set of\nparameters. By factorizing wisely the parameters\nof the Transformer model into domain-shared and\ndomain-speciﬁc parts, we signiﬁcantly improve the\nmodel’s parameter efﬁciency and provide new per-\nspectives for open domain applications.\n4229\nAcknowledgments\nWe thank the anonymous reviewers for their de-\ntailed and constructed comments. This work is sup-\nported by National Key R&D Program of China\n(2018YFB1403202)\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In proceedings of\nICLR 2015.\nAnkur Bapna, Naveen Arivazhagan, and Orhan Fi-\nrat. 2019. Simple, Scalable Adaptation for Neu-\nral Machine Translation. arXiv e-prints , page\narXiv:1909.08478.\nAntonio Valerio Miceli Barone, Barry Haddow, Ulrich\nGermann, and Rico Sennrich. 2017. Regularization\ntechniques for ﬁne-tuning in neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1489–1494.\nDenny Britz, Quoc V . Le, and Reid Pryzant. 2017.\nEffective domain mixing for neural machine trans-\nlation. In Proceedings of the Second Conference\non Machine Translation, WMT 2017, Copenhagen,\nDenmark, September 7-8, 2017, pages 118–126.\nBoxing Chen, Colin Cherry, George Foster, and\nSamuel Larkin. 2017. Cost weighting for neural ma-\nchine translation domain adaptation. In Proceedings\nof the First Workshop on Neural Machine Transla-\ntion, pages 40–46, Vancouver. Association for Com-\nputational Linguistics.\nChenhui Chu and Raj Dabre. 2019. Multilingual multi-\ndomain adaptation approaches for neural machine\ntranslation. CoRR, abs/1906.07978.\nChenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.\nAn empirical comparison of domain adaptation\nmethods for neural machine translation. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2017, Vancou-\nver, Canada, July 30 - August 4, Volume 2: Short\nPapers, pages 385–391.\nM. Amin Farajian, Marco Turchi, Matteo Negri, and\nMarcello Federico. 2017. Multi-domain neural ma-\nchine translation through unsupervised adaptation.\nIn Proceedings of the Second Conference on Ma-\nchine Translation, WMT 2017, Copenhagen, Den-\nmark, September 7-8, 2017, pages 127–137.\nMarkus Freitag and Yaser Al-Onaizan. 2016. Fast\ndomain adaptation for neural machine translation.\nCoRR, abs/1612.06897.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt,\nWilliam Lewis, Mu Li, et al. 2018. Achieving hu-\nman parity on automatic chinese to english news\ntranslation. arXiv preprint arXiv:1803.05567.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzkeb-\nski, Bruna Morrone, Quentin de Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, pages 2790–2799.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2016. Overcoming catastrophic forgetting in neural\nnetworks. Cite arxiv:1612.00796.\nCatherine Kobus, Josep Crego, and Jean Senellart.\n2017. Domain control for neural machine transla-\ntion. In Proceedings of the International Conference\nRecent Advances in Natural Language Processing,\nRANLP 2017, pages 372–378, Varna, Bulgaria. IN-\nCOMA Ltd.\nPhilipp Koehn and Rebecca Knowles. 2017. Six\nchallenges for neural machine translation. CoRR,\nabs/1706.03872.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv e-\nprints, page arXiv:1909.11942.\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2018.\nOne sentence one model for neural machine transla-\ntion. In Proceedings of the Eleventh International\nConference on Language Resources and Evalua-\ntion (LREC-2018), Miyazaki, Japan. European Lan-\nguages Resources Association (ELRA).\nMinh-Thang Luong and Christopher D Manning. 2015.\nStanford neural machine translation systems for spo-\nken language domains. In Proceedings of the In-\nternational Workshop on Spoken Language Transla-\ntion, pages 76–79.\nMichael Mccloskey and Neil J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. The Psychology of\nLearning and Motivation, 24:104–169.\n4230\nPaul Michel and Graham Neubig. 2018. Extreme adap-\ntation for personalized neural machine translation.\nCoRR, abs/1805.01817.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic eval-\nuation of machine translation. In ACL.\nOﬁr Press and Lior Wolf. 2016. Using the output\nembedding to improve language models. ArXiv,\nabs/1608.05859.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems 30, pages 506–516. Cur-\nran Associates, Inc.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2018. Efﬁcient parametrization of multi-\ndomain deep neural networks. In 2018 IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 8119–8127.\nHassan Sajjad, Nadir Durrani, Fahim Dalvi, Yonatan\nBelinkov, and Stephan V ogel. 2017. Neural ma-\nchine translation training in a multi-domain scenario.\narXiv preprint arXiv:1708.08712.\nDanielle Saunders, Felix Stahlberg, Adri `a de Gis-\npert, and Bill Byrne. 2019. Domain adaptive in-\nference for neural machine translation. CoRR,\nabs/1906.00408.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nNoam Shazeer. 2019. Fast Transformer Decoding:\nOne Write-Head is All You Need. arXiv e-prints ,\npage arXiv:1911.02150.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand pals: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, pages 5986–5995.\nBrian Thompson, Jeremy Gwinnup, Huda Khayrallah,\nKevin Duh, and Philipp Koehn. 2019. Overcom-\ning catastrophic forgetting during domain adaptation\nof neural machine translation. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 2062–2068.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nRui Wang, Masao Utiyama, Andrew Finch, Lemao Liu,\nKehai Chen, and Eiichiro Sumita. 2018. Sentence\nselection and weighting for neural machine transla-\ntion domain adaptation. IEEE/ACM Trans. Audio,\nSpeech and Lang. Proc., 26(10):1727–1741.\nMarlies van der Wees, Arianna Bisazza, and Christof\nMonz. 2017. Dynamic data selection for neural ma-\nchine translation. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1410, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nJiali Zeng, Jinsong Su, Huating Wen, Yang Liu, Jun\nXie, Yongjing Yin, and Jianqiang Zhao. 2018. Multi-\ndomain neural machine translation with word-level\ndomain context discrimination. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, Brussels, Belgium, Octo-\nber 31 - November 4, 2018, pages 447–457.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7721517086029053
    },
    {
      "name": "Computer science",
      "score": 0.7690839767456055
    },
    {
      "name": "Machine translation",
      "score": 0.759029746055603
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5979300737380981
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5001964569091797
    },
    {
      "name": "ENCODE",
      "score": 0.48532384634017944
    },
    {
      "name": "Machine learning",
      "score": 0.4732133746147156
    },
    {
      "name": "Factorization",
      "score": 0.4200918674468994
    },
    {
      "name": "Natural language processing",
      "score": 0.3668745756149292
    },
    {
      "name": "Algorithm",
      "score": 0.19831103086471558
    },
    {
      "name": "Voltage",
      "score": 0.13188251852989197
    },
    {
      "name": "Engineering",
      "score": 0.11368963122367859
    },
    {
      "name": "Mathematics",
      "score": 0.09669700264930725
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}