{
  "title": "Large Language Models and Biorisk",
  "url": "https://openalex.org/W4387439210",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4301470673",
      "name": "William D’Alessandro",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Munich School of Philosophy"
      ]
    },
    {
      "id": "https://openalex.org/A3155670052",
      "name": "Harry R. Lloyd",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2308974998",
      "name": "Nathaniel Sharadin",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4301470673",
      "name": "William D’Alessandro",
      "affiliations": [
        "Munich School of Philosophy",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A3155670052",
      "name": "Harry R. Lloyd",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2308974998",
      "name": "Nathaniel Sharadin",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W1545248704",
    "https://openalex.org/W4220681836",
    "https://openalex.org/W4366588626"
  ],
  "abstract": "We discuss potential biorisks from large language models (LLMs). AI assistants based on LLMs such as ChatGPT have been shown to significantly reduce barriers to entry for actors wishing to synthesize dangerous, potentially novel pathogens and chemical weapons. The harms from deploying such bioagents could be further magnified by AI-assisted misinformation. We endorse several policy responses to these dangers, including prerelease evaluations of biomedical AIs by subject-matter experts, enhanced surveillance and lab screening procedures, restrictions on AI training data, and access controls on biomedical AI. We conclude with a suggestion about future research directions in bioethics.",
  "full_text": " \nLarge Language Models and Biorisk \nForthcoming in the American Journal of Bioethics \n \nWilliam D’Alessandro, Harry Lloyd and Nate Sharadin1 \n \nIntroduction \nThe use of computational tools in biology, chemistry and medicine has grown at a staggering \npace in recent decades, driven primarily by new opportunities for applying machine learning \n(ML) techniques to large data sets. These applications of ML have helped scientists understand \nprotein folding, plan chemical syntheses, predict organic reactions and design specialized drugs, \nto name just a few uses. Biochemical research has been greatly accelerated as a result.2 \nLarge language models (LLMs) like OpenAI’s ChatGPT represent a novel application of ML \ntechniques. Trained on large volumes of text, frontier LLMs demonstrate remarkably diverse \nlinguistic abilities. Their utility as multipurpose assistants can be further enhanced by domain-\nspecific training and task-specific fine-tuning. One class of biomedical ML systems integrates \nLLMs with other scientific research tools, such as Python code, expert-designed chemistry \nsoftware, and even laboratory robots (Boiko et al. 2023, Bran et al. 2023). Other more \nspecialized biomedical ML systems are trained on biological or chemical datasets rather than \nnatural language corpora, and are typically designed for specialist users (Jumper et al. 2021). \nLike many powerful technologies, biomedical ML systems are dual-use: they can be deployed as \nintended for legitimate research purposes, or misused to cause harm. Hence they face special \nethical and governance problems. The rapid pace of innovation raises the stakes on both fronts.  \nIn the remainder of this piece, we do three things. First, we highlight three risks we expect to be \nexacerbated by misuse of ML tools in bioscience. Second, we offer four ideas about policy \nresponses aimed at mitigating the foregoing risks. We close by suggesting a direction for future \npublic policy research. \nBiorisks from ML model misuse \nSince the first artificial gene synthesis in the 1970s, scientists have confronted the prospect of \ndangerous lab-created biomaterial. This danger has received renewed attention in the last decade, \nas synthesis techniques have improved and costs have fallen sharply. Two main barriers have \nstood in the way of such scenarios. The first is the technical competence required for \n \n1 Authors contributed equally and are listed in alphabetical order. \n2 DeepMind’s AlphaFold is perhaps the clearest-cut case of accelerated research. For an overview, see (Jumper et al \n2021). \n \nsynthesizing DNA. The second is the existence of laboratory security measures designed to flag \npotentially dangerous bioagents. \nUnfortunately, specialist biomedical ML systems have shown promise in generating novel \nvariants of known pathogens that may evade existing screening procedures (Madani et al. 2023). \nGiven ML tools’ remarkable ability to uncover latent information in large data sets, it’s likely \nthat future models will prove useful for finding entirely new hazardous agents. We therefore \nexpect biomedical ML models to enable bad actors to design dangerous DNA sequences which \nevade present detection procedures. \nOf course, the know-how barrier to effective misuse remains: it’s one thing to design synthetic \nDNA on a computer, but another to manufacture genetic material and insert it into a viral \ngenome. Worryingly, however, LLMs and LLM-based tools can serve as a step-ladder here, \nsystematically making barriers more surmountable by offering users personalized expert \nguidance. For instance, as documented by Soice et. al (2023), it took non-science students at \nMIT less than one hour to obtain detailed walkthroughs from popular LLM chatbots on planning \nand unleashing pandemics. The chatbots “suggested four potential pandemic pathogens, \nexplained how they can be generated from synthetic DNA using reverse genetics, supplied the \nnames of DNA synthesis companies unlikely to screen orders, …and recommended that anyone \nlacking the skills to perform reverse genetics engage a core facility or contract research \norganization” (1). \nAs biomedical AI assistants improve, this step-ladder will grow, and thus the barriers to misuse \nwill effectively shrink. They may eventually come to nothing: consider that the technical know-\nhow required to build a working firearm at home has been reduced to a wiki page.  \nSimilar misuse risks are present in the case of novel chemical agents. As with biological agents, \nnovelty will enable evasion of screening measures, and ML-based assistance will facilitate low-\nskilled access to dangerous chemical synthesis. Things are perhaps even worse in the chemical \ncase, since the equipment and know-how required for dangerous chemical synthesis are \nsomewhat less demanding than in (e.g.) virology. \nThe case of Collaborations Pharmaceuticals offers an illustration of these risks. Collaborations is \na small biotech firm offering a variety of ML-based research software, including MegaSyn, a \nmolecule generator for drug discovery. While MegaSyn normally avoids molecules predicted to \nhave harmful properties, researchers from Collaborations found that they could easily prompt \ntheir product to invent toxic compounds instead. With this modification, it took less than six \nhours for MegaSyn to generate 40,000 candidate molecules—many of which were novel and \npredicted to be more toxic than known agents. These findings were sobering for Collaborations’ \nleadership. “We were naïve in thinking about the potential misuse of our trade… Our proof-of-\nconcept highlights how a non-human autonomous creator of a deadly chemical weapon is \nentirely feasible” (Urbina et al. 2022, 189-90). \n \nA final way in which LLMs magnify the risks from deployment of harmful biochemical agents is \nby facilitating mass misinformation campaigns. The world witnessed the corrosive effects of \nsuch campaigns during the COVID-19 pandemic, and LLMs are likely to exacerbate \nmisinformation risks considerably. Language models can generate large volumes of text about \npublic health issues (Zhou et al. 2023). With appropriate prompt engineering, these outputs can \nimitate the features of highly engaging social media posts, and customized versions can be \ngenerated to elicit a particular response or to target a particular demographic group. To make \nmatters worse, current tools for detecting online misinformation cannot reliably recognize \nmachine-generated examples. Thus, LLMs make it easier for bad actors to flood online channels \nwith misinformation during biomedical emergencies. \nThese, then, are three broad ways in which ML tools exacerbate the risk of harmful biochemical \nagents being deployed. Firstly, as specialist ML tools improve, the risk of a novel, dangerous \nagent evading detection increases. Secondly, as biomedical ML assistants improve, the risk of \nsuch a dangerous agent being successfully synthesized increases. Finally, as LLM \nmisinformation capabilities improve, the risk of large-scale public health harms increases. \nPolicy responses \nThere are many strategies for policymakers to consider in responding to the challenges we’ve \nidentified. Here, we highlight four. \nFirst, model developers should be required to submit biomedical tools for capabilities \nevaluations by independent subject-matter experts before those tools are released (Sandbrink \n2023). These evaluation exercises can take different forms: one is “red-teaming”, where human \nevaluators attempt to elicit dangerous behavior in a sandboxed environment. As model \ncapabilities scale, new techniques will be required. This is an important direction of ongoing \nresearch. \nSecond, model developers should be subject to a strong product liability regime wherein they are \naccountable for damages caused by reasonably foreseeable use and misuse of their models. In \nthe case of actual harm, we think courts are well-positioned to hear developers’ arguments \nconcerning the specific reasonable efforts they undertook to mitigate misuse risks. A liability \nregime such as this correctly incentivizes investment in safety research and caution in model \ndeployment on the part of developers. \nThird, in the US context, the White House should consider issuing a new Executive Order aimed \nat strengthening both bio- and information security at laboratories governed by the CDC’s \nbiosecurity safety level requirements (BSL). Of particular concern is the fact that present BSL \nrequirements do not cover the need for labs to engage in best practices regarding information \nsecurity. Not only are the BSL requirements silent on information security, the 6th edition of the \nCDC’s guidance on Biosafety in Microbiological and Biomedical Laboratories (BMBL) was \n \nreleased in June of 2020 (Meechan and Potts, 2020). It makes just one mention of information \nsecurity (p. 126) encouraging labs to establish policies for handling sensitive information. There \nis no mention of the need to secure computational scientific tools. We think this is a serious \noversight. \nFourth, model developers should be forced to transparently report the data used to train their \nmodels at a level of detail that’s sufficiently granular to understand the specific biochemical \ntasks the model has been exposed to during training. The FTC has recently taken steps to require \nOpenAI to disclose information about its training data; we think such data disclosures to federal \nagencies charged with oversight should be the default. \nFuture research \nWe suggest that a productive topic for future research in bioethics is to discuss in detail the \nethical trade-offs involved in these differing potential policy responses. In particular: How \nshould we trade off privacy against security in choosing how to surveil those suspected of \nbiomedical malfeasance? How should we trade off a strong product liability regime against the \npotential impacts on healthcare innovation and patient well-being? And how should we trade off \nthe indubitable benefits of novel biomedical ML tools— both to patients and to researchers—\nagainst the risks of catastrophe? Bioethicists are well-placed to engage with these questions. \nReferences \nBoiko, D. A., R. MacKnight, and G. Gomes. 2023. Emergent autonomous scientific research \ncapabilities of large language models. arXiv:2304.05332. \nBran, A. M., S. Cox, A. D. White, and P. Schwaller. 2023. ChemCrow: Augmenting large-\nlanguage models with chemistry tools. arXiv:2304.05376. \nJumper, J., R. Evans, A. Pritzel et al. 2021. Highly accurate protein structure prediction with \nAlphaFold. Nature 596, 583–589. https://doi.org/10.1038/s41586-021-03819-2 \nMadani, A., B. Krause, E. R. Greene et al. 2023. Large language models generate functional \nprotein sequences across diverse families. Nature Biotechnology. Advance online publication. \nhttps://doi.org/10.1038/s41587-022-01618-2 \nMeechan, P. J., and J. Potts. 2020. Biosafety in microbiological and biomedical laboratories. \nAccessed August 3, 2023. https://www.cdc.gov/labs/pdf/SF__19_308133-A_BMBL6_00-\nBOOK-WEB-final-3.pdf.  \nSandbrink, J. B. 2023. Artificial intelligence and biological misuse: Differentiating risks of \nlanguage models and biological design tools. arXiv:2306.13952.  \n \nSoice, E. H., R. Rocha, K. Cordova, M. Specter and K. M. Esvelt. 2023. Can large language \nmodels democratize access to dual-use biotechnology? arXiv:2306.03809. \nUrbina, F., F. Lentzos, C. Invernizzi and S. Ekins. 2022. Dual use of artificial-intelligence-\npowered drug discovery. Nature Machine Intelligence 4, 189-191. \nZhou, J., Y. Zhang, Q. Luo, A. G. Parker, and M. De Choudhury. Synthetic lies: Understanding \nAI-generated misinformation and evaluating algorithmic and human solutions. 2023. In \nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 1-20. ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.47286128997802734
    },
    {
      "name": "Image (mathematics)",
      "score": 0.44087332487106323
    },
    {
      "name": "Jumper",
      "score": 0.4215850830078125
    },
    {
      "name": "Psychology",
      "score": 0.376258909702301
    },
    {
      "name": "Linguistics",
      "score": 0.36115482449531555
    },
    {
      "name": "Natural language processing",
      "score": 0.32198381423950195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3076600432395935
    },
    {
      "name": "Philosophy",
      "score": 0.19109731912612915
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I292141241",
      "name": "Munich School of Philosophy",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I889937125",
      "name": "National Patient Safety Foundation",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}