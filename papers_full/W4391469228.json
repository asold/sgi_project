{
    "title": "Performance of large language models on advocating the management of meningitis: a comparative qualitative study",
    "url": "https://openalex.org/W4391469228",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2022670767",
            "name": "Urs Fisch",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2974780892",
            "name": "Paulina Kliem",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2706745815",
            "name": "Pascale Grzonka",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2160393092",
            "name": "Raoul Sutter",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2022670767",
            "name": "Urs Fisch",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2974780892",
            "name": "Paulina Kliem",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2706745815",
            "name": "Pascale Grzonka",
            "affiliations": [
                "University Hospital of Basel"
            ]
        },
        {
            "id": "https://openalex.org/A2160393092",
            "name": "Raoul Sutter",
            "affiliations": [
                "University Hospital of Basel"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4318765555",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4362521774",
        "https://openalex.org/W2032850556",
        "https://openalex.org/W2759699778",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4321366933",
        "https://openalex.org/W4392359953",
        "https://openalex.org/W3206840963",
        "https://openalex.org/W4367626167",
        "https://openalex.org/W2326122211",
        "https://openalex.org/W3122085645",
        "https://openalex.org/W2143206150",
        "https://openalex.org/W2972099196",
        "https://openalex.org/W3174333332",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4388525110",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W3134138028",
        "https://openalex.org/W4386274036",
        "https://openalex.org/W4206410242",
        "https://openalex.org/W2170303054",
        "https://openalex.org/W4400937555"
    ],
    "abstract": "Objectives We aimed to examine the adherence of large language models (LLMs) to bacterial meningitis guidelines using a hypothetical medical case, highlighting their utility and limitations in healthcare. Methods A simulated clinical scenario of a patient with bacterial meningitis secondary to mastoiditis was presented in three independent sessions to seven publicly accessible LLMs (Bard, Bing, Claude-2, GPT-3.5, GPT-4, Llama, PaLM). Responses were evaluated for adherence to good clinical practice and two international meningitis guidelines. Results A central nervous system infection was identified in 90% of LLM sessions. All recommended imaging, while 81% suggested lumbar puncture. Blood cultures and specific mastoiditis work-up were proposed in only 62% and 38% sessions, respectively. Only 38% of sessions provided the correct empirical antibiotic treatment, while antiviral treatment and dexamethasone were advised in 33% and 24%, respectively. Misleading statements were generated in 52%. No significant correlation was found between LLMs’ text length and performance (r=0.29, p=0.20). Among all LLMs, GPT-4 demonstrated the best performance. Discussion Latest LLMs provide valuable advice on differential diagnosis and diagnostic procedures but significantly vary in treatment-specific information for bacterial meningitis when introduced to a realistic clinical scenario. Misleading statements were common, with performance differences attributed to each LLM’s unique algorithm rather than output length. Conclusions Users must be aware of such limitations and performance variability when considering LLMs as a support tool for medical decision-making. Further research is needed to refine these models' comprehension of complex medical scenarios and their ability to provide reliable information.",
    "full_text": " 1\nFisch U, et al. BMJ Health Care Inform 2024;31:e100978. doi:10.1136/bmjhci-2023-100978\nOpen access \nPerformance of large language models \non advocating the management of \nmeningitis: a comparative \nqualitative study\nUrs Fisch    ,1 Paulina Kliem,2 Pascale Grzonka,2 Raoul Sutter1,2,3\nTo cite: Fisch U, Kliem P , \nGrzonka P , et al.  Performance \nof large language models \non advocating the \nmanagement of meningitis: \na comparative qualitative \nstudy. BMJ Health Care Inform \n2024;31:e100978. doi:10.1136/\nbmjhci-2023-100978\n ► Additional supplemental \nmaterial is published online only. \nTo view, please visit the journal \nonline (http:// dx. doi. org/ 10. \n1136/ bmjhci- 2023- 100978).\nReceived 24 November 2023\nAccepted 15 January 2024\n1Department of Neurology, \nUniversity Hospital Basel, Basel, \nSwitzerland\n2Clinic for Intensive Care \nMedicine, University Hospital \nBasel, Basel, Switzerland\n3Medical Faculty, University \nBasel, Basel, Switzerland\nCorrespondence to\nDr Urs Fisch;  urs. fisch@ usb. ch\nOriginal research\n© Author(s) (or their \nemployer(s)) 2024. Re- use \npermitted under CC BY- NC. No \ncommercial re- use. See rights \nand permissions. Published by \nBMJ.\nABSTRACT\nObjectives We aimed to examine the adherence of large \nlanguage models (LLMs) to bacterial meningitis guidelines \nusing a hypothetical medical case, highlighting their utility \nand limitations in healthcare.\nMethods A simulated clinical scenario of a patient \nwith bacterial meningitis secondary to mastoiditis was \npresented in three independent sessions to seven publicly \naccessible LLMs (Bard, Bing, Claude- 2, GPT- 3.5, GPT- 4, \nLlama, PaLM). Responses were evaluated for adherence \nto good clinical practice and two international meningitis \nguidelines.\nResults A central nervous system infection was identified \nin 90% of LLM sessions. All recommended imaging, while \n81% suggested lumbar puncture. Blood cultures and \nspecific mastoiditis work- up were proposed in only 62% \nand 38% sessions, respectively. Only 38% of sessions \nprovided the correct empirical antibiotic treatment, while \nantiviral treatment and dexamethasone were advised in \n33% and 24%, respectively. Misleading statements were \ngenerated in 52%. No significant correlation was found \nbetween LLMs’ text length and performance (r=0.29, \np=0.20). Among all LLMs, GPT- 4 demonstrated the best \nperformance.\nDiscussion Latest LLMs provide valuable advice on \ndifferential diagnosis and diagnostic procedures but \nsignificantly vary in treatment- specific information for \nbacterial meningitis when introduced to a realistic clinical \nscenario. Misleading statements were common, with \nperformance differences attributed to each LLM’s unique \nalgorithm rather than output length.\nConclusions Users must be aware of such limitations \nand performance variability when considering LLMs \nas a support tool for medical decision- making. Further \nresearch is needed to refine these models' comprehension \nof complex medical scenarios and their ability to provide \nreliable information.\nINTRODUCTION\nLarge language models (LLMs) are powerful \nartificial intelligence (AI) models trained \non extensive text data to generate human- \nlike text. They can interpret user- generated \ntextual instructions (prompts) and respond \nimmediately with the contextually most \nappropriate response based on probabilistic \ncomputations learnt during their training. \nLately, several LLMs were released to the \npublic, attracting substantial attention for \ntheir chat- like interfaces requiring no tech-\nnical prerequisites.\nRecently, both trained and untrained LLMs \nhave shown proficiency in handling medical \nlicensing examination- level questions and \ndemonstrated the ability to make rapid and \naccurate judgments in medical triage and \ndiagnosing or provide helpful information to \npatients, underscoring their potential applica-\nbility in the healthcare sector.1–6 However, the \nability to perform well in knowledge- testing \nWHAT IS ALREADY KNOWN ON THIS TOPIC\n ⇒ Large language models (LLMs) have demonstrat-\ned proficiency in responding to medical licensing \nexamination- level queries and shown aptitude in \naccurate medical triage decision- making. However, \nperformance with knowledge- testing scenarios is \nnot necessarily indicative of effectiveness in real- \nworld medical contexts.\nWHAT THIS STUDY ADDS\n ⇒ This investigation presents a qualitative analysis of \nthe performance of seven publicly accessible LLMs, \nusing a stepwise presentation of a hypothetical bac-\nterial meningitis case reflecting a real- world sce-\nnario. While LLMs generally offered helpful triage \nand diagnostic advice, there were significant dis-\ncrepancies in their recommendations for treatment \nand specific diagnostic work- ups. Moreover, the \ngeneration of misleading statements and variability \nin performances between different sessions were \nobserved among individual LLMs.\nHOW THIS STUDY MIGHT AFFECT RESEARCH, \nPRACTICE OR POLICY\n ⇒ This study highlights the current capabilities of \nLLMs in handling real- world medical emergency \nsituations and identifies areas of future research, \nsuch as enhancing LLMs’ understanding of complex \nmedical scenarios and their capacity for delivering \nreliable and deterministic information.\nBMJ Health & Care Informatics: first published as 10.1136/bmjhci-2023-100978 on 2 February 2024. Downloaded from https://informatics.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n2\nFisch U, et al. BMJ Health Care Inform 2024;31:e100978. doi:10.1136/bmjhci-2023-100978\nOpen access \nvignettes does not fully reflect the needs of real- world \nmedical settings which demand parallel work- up and \nnuanced decision- making on the basis of sometimes \nincomplete information. Considering that physicians \nalready frequently use internet resources for diagnostic \ndecisions and treatment options and that not all hospitals \nmay have free access to the medical literature, it is likely \nthat LLMs will be increasingly used as potential aids in \nclinical practice.7–9 However, a deeper understanding of \ntheir potential and limitations is essential for an appro-\npriate use.10–12\nThis study explored the potentials and limitations \nof current LLMs by presenting these models with a \npredefined hypothetical but typical scenario of a patient \nwith acute bacterial meningitis. The aim was to analyse \ntheir performance and alignment with good clinical \npractice and established medical guidelines regarding \nsuggested diagnostic and treatment measures. Bacterial \nmeningitis was chosen for its life- threatening nature, \nurgency required in diagnosis and treatment and the \nrange of differential diagnoses it involves, making it ideal \nfor assessing the performance of LLMs in a realistic and \nhigh stakes medical scenario.\nMETHODS\nSeven publicly accessible LLMs were evaluated between \n5 and 8 August 2023: Bard by Google, Bing by Microsoft, \ngenerative pre- trained transformer (GPT)- 3.5 by OpenAI, \nGPT- 4 by OpenAI (accessed via Poe (Quora)), Claude- 2 \nby Anthropic PBC (accessed via Poe), pathways language \nmodel (PaLM) 2 chat- bison- 001 by Google (accessed via \nPoe) and Llama- 2- 70b by Meta Platforms (accessed via \nPoe).\nEach LLM was presented with the same hypothetical \nscenario of a patient presenting with symptoms of acute \nbacterial meningitis (as outlined below) three times \nwithin 3 days. The actual diagnosis was not provided. For \nthe LLM Bard, the settings were chosen to inhibit inter -\nsession information storage. All other LLMs claimed that \nthey are incapable of storing user information between \nsessions. Each session was initiated with a context clear -\nance of previous conversations.\nHypothetical scenario of a patient with acute bacterial \nmeningitis\nThe patient vignette described a clinical scenario of \na patient with acute symptoms due to pneumococcal \nmeningitis secondary to mastoiditis without providing \ndefinite diagnosis. The text of the inputted case vignette \nand the subsequent follow- up queries consisted of five text \nblocks that were predefined and presented unchanged to \neach LLM in every session (online supplemental table 1). \nGiven that the performance of LLMs is heavily influenced \nby prompting,13 the initial question began with a contex-\ntualisation wherein the LLM was asked to act as an ‘expe-\nrienced medical assistant’ and the user was identified as \na ‘junior medical doctor’ seeking advice for a 52- year- old \nfemale patient suffering from severe headache and confu-\nsion, followed by an open- ended question about the next \nsteps. This prompt engaged all LLMs in a conversation \nabout the hypothetical case. Second, a detailed vignette \nwas presented, depicting the medical history (notably \nacute headache and confusion, a history of diabetes type \n2 and migraine), vital signs (tachycardia and fever) and \nprominent abnormal clinical findings (ie, a Glasgow \nComa Scale (GCS) of 12 with lethargy, disorientation, fast \ndownward drift of extremities, absence of stiff neck, signs \nof inflammatory skin of the right mastoid), followed by \nthe open- ended request for a detailed step- by- step recom-\nmendation of how to proceed. Third, two closed- ended \nquestions were asked: (1) if a computer tomography (CT) \nscan of the head needs to be awaited before lumbar punc-\nture (LP) and (2) if administration of antibiotics should \nbe delayed until LP has been performed. Fourth, the \nexact dosages of antibiotics were asked. Fifth, an open- \nended question was asked about any other considerations \nregarding the treatment or work- up.\nThe case was created to reflect clinical reality and not \na medical license examination question, meaning that \ninformation was presented stepwise and reflected a real-\nistic clinical case where not all typical signs and symptoms \nare necessarily present from the beginning. For example, \nneck stiffness has shown to have a low sensitivity and as \nsuch, its absence cannot rule out meningitis. 14 A search \nfor an infectious focus is crucial and patients should be \nexamined for otitis media or mastoiditis. 15 By this design \nwe aimed to challenge the LLMs in multiple aspects, \nincluding good clinical practice, possible differential \ndiagnoses and consideration of risk factors and comor -\nbidities, such as age, diabetes and migraine, for diagnosis \nand treatment.\nEvaluation of LLM performance\nThe Infectious Diseases Society of America (IDSA) and the \nEuropean Society of Clinical Microbiology and Infectious \nDiseases (ESCMID) guidelines were chosen as references \nas they have previously both been shown in a systematic \nreview to be excellent clinical management guidelines for \nbacterial meningitis with multinational validity (online \nsupplemental table 2, right column).14 16 17\nIndividual responses from the LLMs underwent two \ntemporally separated qualitative assessments (accom-\nplished vs unaccomplished) of predefined tasks (online \nsupplemental table 2, middle column) in adherence with \ngood clinical practice and the reference guidelines. 14–18 \nAccomplished tasks were summarised to a qualitative \nperformance summary. Response consistency was defined \nas the percentage of responded tasks that were assessed \nidentically (regardless of accomplished or unaccom-\nplished) across all sessions of an individual LLM. In cases \nwhere an LLM declined to respond to a question, the \ncorresponding tasks were excluded from the assessment.\nAs the two reference guidelines differently define \ncriteria for imaging before LP (ie, according to the \nIDSA guideline, a scan of the brain would be required as \nBMJ Health & Care Informatics: first published as 10.1136/bmjhci-2023-100978 on 2 February 2024. Downloaded from https://informatics.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n3\nFisch U, et al. BMJ Health Care Inform 2024;31:e100978. doi:10.1136/bmjhci-2023-100978\nOpen access\nthe patient expresses any altered mental status and has \ndownward drift of extremities, whereas according to the \nESCMID guideline, a scan of the brain is not mandatory \nwith a GCS>10) and maximal allowed delay to start anti-\nbiotics, these aspects were not included in the qualitative \nperformance summary.14 16\nStatistics\nDescriptive statistics with numbers and percentages and \nthe two- sided Pearson correlation coefficient were used \nwhere appropriate (R, V.4.3.1). Due to the principally \nqualitative aim of this study, a statistical comparison \nbetween the LLMs was not intended.\nRESULTS\nThe individual responses of all 21 sessions of the seven \nLLMs are summarised in figure 1. We noticed marked \ndifferences in the qualitative performance summary \nbetween different LLMs and to a lesser extent also between \ndifferent sessions of individual LLMs. The response consis-\ntency ranged from 53% to 85%. LLMs with low numbers \nof accomplished tasks also had low response consistency. \nAmong all the LLMs evaluated, GPT- 4 demonstrated \nthe most consistent performance, effectively addressing \nalmost all tasks and having a high response consistency \nacross all tasks and responses. Exemplary transcripts of \nthe first conversations with Bard and GPT- 4 are shown in \nonline supplemental material.\nThe word count of individual LLMs sessions varied \nsignificantly, ranging from 325 (PaLM 2 chat- bison- 001) \nto 2045 (GPT- 3.5), with an average of 1270 words (stan-\ndard deviation 477). There was no significant correlation \n(r=0.29, p=0.20) between the total length of individual \nLLM responses and the summative performance of \naccomplished tasks, indicating that simply generating \nmore text output does not necessarily lead to improved \nperformance.\nSuggested differential diagnoses and recommended \ndiagnostic work-up\nIn 62% of the sessions, LLMs suggested an urgent work- up \nwithout direct prompting. In 57% of sessions, they recom-\nmended measuring vital parameters, taking the patient’s \nhistory and performing a physical examination as initial \nsteps. Furthermore, in 90% of the sessions, the LLMs accu-\nrately suspected a central nervous system (CNS) infection \nas a possible cause of the patient’s symptoms. However, only \n38% of the responses mentioned mastoiditis as a poten-\ntial underlying cause or suggested correspondent diag-\nnostic procedures (imaging with purpose of investigating \nmastoiditis, otoscopy, ear–nose–throat consultation). \nThe most frequently mentioned differential diagnoses \nFigure 1 Qualitative assessment of large language models (LLMs) performance on a case of bacterial meningitis. Each LLM \nwas tested three times with a standardised case vignette (individual sessions separated by dashed lines). Accomplished tasks \nare marked in green in decreasing order of agreement among all LLMs, while unaccomplished tasks are highlighted in red. \nWhite boxes represent tasks where the model either declined to respond or no additional information could be provided due \nto gaps in previous responses. Response consistency was defined as identically assessed responded tasks across different \nsessions of a single LLM. CNS, central nervous system.\nBMJ Health & Care Informatics: first published as 10.1136/bmjhci-2023-100978 on 2 February 2024. Downloaded from https://informatics.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n4\nFisch U, et al. BMJ Health Care Inform 2024;31:e100978. doi:10.1136/bmjhci-2023-100978\nOpen access \nwere stroke (86%), followed by intracranial/subarach-\nnoid haemorrhage and brain tumour (both 48%). Other \nproposed differential diagnoses were migraine (19%), \nmetabolic/endocrine disbalances (19%), medication side \neffects (10%), non- CNS infections (10%), severe hyperten-\nsion (5%), drug intoxication (5%) and neurodegenerative \ndisorders (5%).\nRegarding diagnostic work- up, cranial imaging was \nrecommended in 100% of sessions, LP in 81% and blood \ncultures in 62%. Blood glucose measurement in the \ndiabetic patient with altered mental status was suggested \nin 53%. Unrecommended tests by the IDSA and ESCMID \nguidelines (eg, electroencephalogram, electrocardiogram, \nchest radiography) were proposed in 19% of sessions as an \ninitial work- up.\nIn 43% of responses, LLMs stated that a cranial CT scan \nis necessary before LP, while 14% suggested to perform an \nLP without CT scan and another 43% gave unclear answers. \nOnly three LLMs (GPT- 3.5, Claude- 2, GPT- 4) provided a \ncase- specific rationale for their recommendation (92% \nresponses suggested CT scan before LP). Due to different \ndefinitions of criteria for cranial imaging before LP in the \nreference guidelines and maximal allowed delay to start \nantibiotics,14 16 these aspects were not included in the qual-\nitative performance summary displayed in figure 1.\nRecommended treatment\nRegarding treatment, 81% of responses stated that rapid \nadministration of antibiotics is necessary. The correct \nchoice of empirical antibiotic treatment, consisting of a \nthird- generation cephalosporin with ampicillin (alterna-\ntives: amoxicillin, penicillin G) with or without vancomycin, \nwas provided in 38%, and of those, almost 90% with correct \ndosing.14 16 Another 29% provided an incomplete choice \nof antibiotic treatment and 33% declined to comment on \nany choice of antibiotics. In 33% of the sessions, antiviral \ntreatment was considered with approximately half of them \nproviding correct dosing. Dexamethasone administration \nwas recommended in 24% of all responses.\nMisleading statements\nMisleading statements were identified in 52% of the \nsessions, such as performing an LP to relieve intracranial \npressure or carrying it out prior to imaging in order to \nfacilitate image interpretation; administering prophylactic \nantiseizure medication or giving benzodiazepines for seda-\ntion; adjusting ceftriaxone dosage based on age, weight \nand kidney function or administering dexamethasone for \nmeningococcal meningitis; the presence of a stiff neck and \nKernig’s sign (while the vignette stated that these were \nabsent); or the misinterpretation of mastoiditis as herpes \nzoster ophthalmicus.\nDISCUSSION\nThis study investigated qualitative performance charac-\nteristics of different LLMs when challenged with a hypo-\nthetical clinical case of an adult patient with bacterial \nmeningitis and revealed marked discrepancies between \nthe LLMs. This reflects both the potentials and limita-\ntions of these models when used as a guidance for medical \nwork- up and treatment. 9 The differences in qualitative \nperformances observed among the LLMs did not demon-\nstrate a correlation with the length of their respective \noutputs. This suggests that the performance variations \ncan be attributed to the unique algorithmic designs of \neach LLM rather than their quantitative output.\nCNS infection was identified as a probable cause \namong other differential diagnosis in the majority of \ncases and almost all LLMs succeeded in identifying and \nrecommending appropriate investigations, including \ncranial imaging and LP. A fair proportion underscored \nthe need for urgent diagnostics and antibiotic treatment. \nThese results align with previous findings demonstrating \na satisfactory performance of GPT- 3 (the predecessor of \nGPT- 3.5) in terms of triage and reasoning on differential \ndiagnoses and the high performance of GPT- 4 in diag-\nnostic case challenges.4 19–21 Our study expands on these \nfindings by examining an additional five LLMs which \nwere not available at the time of the previous studies.\nOur investigation also highlights limitations of most \nLLMs regarding their understanding of case complexity \nand their ability to link different disease entities. For \ninstance, the identification of mastoiditis as an underlying \ncause was mentioned infrequently, as were blood glucose \nmeasurements, drawing blood cultures, considerations \nof empirical antiviral treatment and the administration \nof dexamethasone. The considerable heterogeneity in \nthe responses of individual LLMs, despite standardised \nprompts, raises further concerns about their reliability \nand consistency. The presentation of misleading state-\nments in more than half of the LLM sessions underscores \nthe potential risk that comes along with their use for \ncritical medical decision- making, especially in complex, \nlife- threatening and time- sensitive situations, such as with \nbacterial meningitis. Such challenges must be addressed \nin future research when developing tools on the basis of \nLLMs for medical purposes.10–12\nMost LLMs’ inability to provide definitive guidance on \nwhether to conduct a cranial CT scan before an LP might \nbe due to the differences in the guidelines. 14 16 However, \nthe lack of clear direction in many LLM responses could \nalso suggest an insufficiency in handling complex clinical \nsituations where there is a need for reasoned decision- \nmaking. This finding may be viewed in the context of the \nresearch gap between healthcare AI development and the \nchallenge of its validation and implementation in real- \nworld clinical settings.22–24\nLimitations\nOur study has several limitations. Most importantly, none \nof the LLMs was designed to assist in medical diagnos-\ntics and treatment and most correctly included respec-\ntive disclaimers. However, as LLMs are powerful, new \nand easily accessible AI tools, it is highly probable that \nthey will find increasing use in the health sector, which \nBMJ Health & Care Informatics: first published as 10.1136/bmjhci-2023-100978 on 2 February 2024. Downloaded from https://informatics.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n5\nFisch U, et al. BMJ Health Care Inform 2024;31:e100978. doi:10.1136/bmjhci-2023-100978\nOpen access\njustifies studying their reliability and applicability. 1–6 \nFurther, prompting has significant influence on the \nresult.13 While our study did not explore the impact of \ndifferent prompting strategies, we used standardised \nprompts, which included contextualisation and step- by- \nstep reasoning, to ensure comparability between LLMs. \nAlthough we evaluated the LLMs’ intuitive assessment of \nthe scenario’s urgency, we did not directly inquire this in \nthe prompts. In addition, the selection of tasks for the \nqualitative assessment was unweighted and focused on \nimportant initial management steps, while other aspects, \nsuch as laboratory testing procedures or duration of anti-\nmicrobial treatment, were not investigated. Lastly, the \nstudy was limited to a single case scenario, and the results \nmay not be generalisable to other clinical scenarios. Thus, \nwe refrained from an absolute ranking of the LLMs.\nCONCLUSIONS\nThe latest versions of LLMs show potential in helping \nhealthcare professionals. Our study underscores the need \nfor cautious and informed use of most of these models \nas demonstrated by the limitations in providing specific \ninformation and potentially misleading information for \ndiagnostic work- up and treatment of adult patients with \nbacterial meningitis. Users should be aware of the vari-\nability in their performance.\nFurther research is needed to refine these models \nand enhance their understanding of complex medical \nscenarios and their ability to provide deterministic, reli-\nable information regardless of prompt nuances. Concur -\nrently, efforts are necessary to mitigate the potential for \ndisseminating erroneous content.\nCorrection notice This paper has been corrected since it was first published. The \nterm GPT models was consistently misspelled as GTP throughout the manuscript.\nContributors UF and RS planned and designed the study. UF acquired the data \nand wrote the first draft of the manuscript and is responsible for the overall content \nas guarantor. All authors interpreted the data, revised the manuscript for important \nintellectual content and approved the final submitted version.\nFunding The authors have not declared a specific grant for this research from any \nfunding agency in the public, commercial or not- for- profit sectors.\nCompeting interests None declared.\nPatient consent for publication Not applicable.\nEthics approval As no real patients were involved in this study, ethical approval \nwas not required.\nProvenance and peer review Not commissioned; externally peer reviewed.\nData availability statement Data are available upon reasonable request.\nSupplemental material This content has been supplied by the author(s). It has \nnot been vetted by BMJ Publishing Group Limited (BMJ) and may not have been \npeer- reviewed. Any opinions or recommendations discussed are solely those \nof the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and \nresponsibility arising from any reliance placed on the content. Where the content \nincludes any translated material, BMJ does not warrant the accuracy and reliability \nof the translations (including but not limited to local regulations, clinical guidelines, \nterminology, drug names and drug dosages), and is not responsible for any error \nand/or omissions arising from translation and adaptation or otherwise.\nOpen access This is an open access article distributed in accordance with the \nCreative Commons Attribution Non Commercial (CC BY- NC 4.0) license, which \npermits others to distribute, remix, adapt, build upon this work non- commercially, \nand license their derivative works on different terms, provided the original work is \nproperly cited, appropriate credit is given, any changes made indicated, and the use \nis non- commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iD\nUrs Fisch http://orcid.org/0000-0003-1557-9062\nREFERENCES\n 1 Singhal K, Azizi S, Tu T, et al. Large language models encode clinical \nknowledge. Nature 2023;620:172–80. \n 2 Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT \non USMLE: potential for AI- assisted medical education using large \nlanguage models. PLOS Digit Health 2023;2:e0000198. \n 3 Gilson A, Safranek CW, Huang T, et al. How does ChatGPT \nperform on the United States medical licensing examination? The \nimplications of large language models for medical education and \nknowledge assessment. JMIR Med Educ 2023;9:e45312. \n 4 Levine DM, Tuwani R, Kompa B, et al. The diagnostic and triage \naccuracy of the GPT- 3 artificial intelligence model. Health Informatics \n[Preprint] 2023. \n 5 Ayers JW, Poliak A, Dredze M, et al. Comparing physician and \nartificial intelligence chatbot responses to patient questions posted \nto a public social media forum. JAMA Intern Med 2023;183:589–96. \n 6 Haver HL, Ambinder EB, Bahl M, et al. Appropriateness of breast \ncancer prevention and screening recommendations provided by \nChatGPT. Radiology 2023;307:e230424. \n 7 Tang H, Ng JHK. Googling for a diagnosis--use of Google as a \ndiagnostic aid: internet based study. BMJ 2006;333:1143–5. \n 8 Russell- Rose T, Chamberlain J. Expert search strategies: \nthe information retrieval practices of healthcare information \nprofessionals. JMIR Med Inform 2017;5:e33. \n 9 Lee P , Bubeck S, Petro J. Benefits, limits, and risks of GPT- 4 as an AI \nChatbot for medicine. N Engl J Med 2023;388:1233–9. \n 10 Howard A, Hope W, Gerada A. ChatGPT and antimicrobial advice: \nthe end of the consulting infection doctor? Lancet Infect Dis \n2023;23:405–6. \n 11 Liévin V, Hother CE, Motzfeldt AG, et al. Can large language models \nreason about medical questions? arXiv:220708143 2023. Available: \nhttps://doi.org/10.48550/arXiv.2207.08143\n 12 Norori N, Hu Q, Aellen FM, et al. Addressing bias in big data and AI \nfor health care: a call for open science. Patterns (N Y) 2021;2:100347. \n 13 Wang J, Shi E, Yu S, et al. Prompt engineering for healthcare: \nmethodologies and applications. 2023. Available: https://doi.org/10. \n48550/arXiv.2304.14670\n 14 van de Beek D, Cabellos C, Dzupova O, et al. ESCMID guideline: \ndiagnosis and treatment of acute bacterial meningitis. Clin Microbiol \nInfect 2016;22 Suppl 3:S37–62. \n 15 Dyckhoff- Shen S, Koedel U, Pfister H- W, et al. SOP: emergency \nworkup in patients with suspected acute bacterial meningitis. Neurol \nRes Pract 2021;3:2. \n 16 Tunkel AR, Hartman BJ, Kaplan SL, et al. Practice guidelines for the \nmanagement of bacterial meningitis. Clin Infect Dis 2004;39:1267–84. \n 17 Sigfrid L, Perfect C, Rojek A, et al. A systematic review of clinical \nguidelines on the management of acute, community- acquired CNS \ninfections. BMC Med 2019;17:170. \n 18 Steiner I, Budka H, Chaudhuri A, et al. Viral meningoencephalitis: a \nreview of diagnostic methods and guidelines for management. Eur J \nNeurol 2010;17:999–e57. \n 19 Nori H, King N, McKinney SM, et al. Capabilities of GPT- 4 on medical \nchallenge problems. 2023. Available: https://doi.org/10.48550/arXiv. \n2303.13375\n 20 Eriksen AV, Möller S, Ryg J. Use of GPT- 4 to diagnose complex \nclinical cases. NEJM AI 2023;1. \n 21 Kanjee Z, Crowe B, Rodman A. Accuracy of a generative artificial \nintelligence model in a complex diagnostic challenge. JAMA \n2023;330:78–80. \n 22 Yin J, Ngiam KY , Teo HH. Role of artificial intelligence applications \nin real- life clinical practice: systematic review. J Med Internet Res \n2021;23:e25759. \n 23 Susanto AP , Lyell D, Widyantoro B, et al. Effects of machine learning- \nbased clinical decision support systems on decision- making, care \ndelivery, and patient outcomes: a scoping review. J Am Med Inform \nAssoc 2023;30:2050–63. \n 24 Gama F , Tyskbo D, Nygren J, et al. Implementation frameworks for \nartificial intelligence translation into health care practice: scoping \nreview. J Med Internet Res 2022;24:e32215. \nBMJ Health & Care Informatics: first published as 10.1136/bmjhci-2023-100978 on 2 February 2024. Downloaded from https://informatics.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n"
}