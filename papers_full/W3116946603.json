{
    "title": "Adapting Bidirectional Encoder Representations from Transformers (BERT) to Assess Clinical Semantic Textual Similarity: Algorithm Development and Validation Study",
    "url": "https://openalex.org/W3116946603",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3102102011",
            "name": "Klaus Kades",
            "affiliations": [
                "Deutschen Konsortium für Translationale Krebsforschung",
                "Heidelberg University",
                "German Cancer Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2554223826",
            "name": "Jan Sellner",
            "affiliations": [
                "Heidelberg University",
                "German Cancer Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2477579477",
            "name": "Gregor Koehler",
            "affiliations": [
                "German Cancer Research Center",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2726960211",
            "name": "Peter M Full",
            "affiliations": [
                "Heidelberg University",
                "German Cancer Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A3022663232",
            "name": "T Y Emmy Lai",
            "affiliations": [
                "Heidelberg University",
                "German Cancer Research Center",
                "Mannheim University of Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2228295954",
            "name": "Jens Kleesiek",
            "affiliations": [
                "German Cancer Research Center",
                "Heidelberg University",
                "Deutschen Konsortium für Translationale Krebsforschung"
            ]
        },
        {
            "id": "https://openalex.org/A4226982991",
            "name": "Klaus H. Maier-Hein",
            "affiliations": [
                "Heidelberg University",
                "Deutschen Konsortium für Translationale Krebsforschung",
                "German Cancer Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A3102102011",
            "name": "Klaus Kades",
            "affiliations": [
                "Deutschen Konsortium für Translationale Krebsforschung",
                "German Cancer Research Center",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2554223826",
            "name": "Jan Sellner",
            "affiliations": [
                "Heidelberg University",
                "German Cancer Research Center"
            ]
        },
        {
            "id": "https://openalex.org/A2477579477",
            "name": "Gregor Koehler",
            "affiliations": [
                "German Cancer Research Center",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2726960211",
            "name": "Peter M Full",
            "affiliations": [
                "German Cancer Research Center",
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A3022663232",
            "name": "T Y Emmy Lai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2228295954",
            "name": "Jens Kleesiek",
            "affiliations": [
                "Heidelberg University",
                "German Cancer Research Center",
                "Deutschen Konsortium für Translationale Krebsforschung"
            ]
        },
        {
            "id": "https://openalex.org/A4226982991",
            "name": "Klaus H. Maier-Hein",
            "affiliations": [
                "Heidelberg University",
                "Deutschen Konsortium für Translationale Krebsforschung",
                "German Cancer Research Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2889272240",
        "https://openalex.org/W2998580448",
        "https://openalex.org/W3026153896",
        "https://openalex.org/W2888285200",
        "https://openalex.org/W3094834348",
        "https://openalex.org/W2739351760",
        "https://openalex.org/W2418508864",
        "https://openalex.org/W2914966672",
        "https://openalex.org/W2585620645",
        "https://openalex.org/W795008685",
        "https://openalex.org/W2197479404",
        "https://openalex.org/W2171313960",
        "https://openalex.org/W2147733805",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2989464093",
        "https://openalex.org/W3005650752",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W3042631625",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2244501064",
        "https://openalex.org/W2100805904",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2887922452",
        "https://openalex.org/W2557519264",
        "https://openalex.org/W2159636537",
        "https://openalex.org/W4244587680",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W3082274269"
    ],
    "abstract": "Background Natural Language Understanding enables automatic extraction of relevant information from clinical text data, which are acquired every day in hospitals. In 2018, the language model Bidirectional Encoder Representations from Transformers (BERT) was introduced, generating new state-of-the-art results on several downstream tasks. The National NLP Clinical Challenges (n2c2) is an initiative that strives to tackle such downstream tasks on domain-specific clinical data. In this paper, we present the results of our participation in the 2019 n2c2 and related work completed thereafter. Objective The objective of this study was to optimally leverage BERT for the task of assessing the semantic textual similarity of clinical text data. Methods We used BERT as an initial baseline and analyzed the results, which we used as a starting point to develop 3 different approaches where we (1) added additional, handcrafted sentence similarity features to the classifier token of BERT and combined the results with more features in multiple regression estimators, (2) incorporated a built-in ensembling method, M-Heads, into BERT by duplicating the regression head and applying an adapted training strategy to facilitate the focus of the heads on different input patterns of the medical sentences, and (3) developed a graph-based similarity approach for medications, which allows extrapolating similarities across known entities from the training set. The approaches were evaluated with the Pearson correlation coefficient between the predicted scores and ground truth of the official training and test dataset. Results We improved the performance of BERT on the test dataset from a Pearson correlation coefficient of 0.859 to 0.883 using a combination of the M-Heads method and the graph-based similarity approach. We also show differences between the test and training dataset and how the two datasets influenced the results. Conclusions We found that using a graph-based similarity approach has the potential to extrapolate domain specific knowledge to unseen sentences. We observed that it is easily possible to obtain deceptive results from the test dataset, especially when the distribution of the data samples is different between training and test datasets.",
    "full_text": "Original Paper\nAdapting Bidirectional Encoder Representations from Transformers\n(BERT) to Assess Clinical Semantic Textual Similarity: Algorithm\nDevelopment and Validation Study\nKlaus Kades1,2*, MSc; Jan Sellner1,3*, MSc; Gregor Koehler1, MSc; Peter M Full1,4, BSc; T Y Emmy Lai1,5, MSc; Jens\nKleesiek1,2,3,6*, MD, PhD; Klaus H Maier-Hein1,2,3,4*, PhD\n1German Cancer Research Center (DKFZ), Heidelberg, Germany\n2Partner Site Heidelberg, German Cancer Consortium (DKTK), Heidelberg, Germany\n3Helmholtz Information and Data Science School for Health, Karlsruhe/Heidelberg, Germany\n4Heidelberg University, Heidelberg, Germany\n5Hochschule Mannheim, University of Applied Sciences, Mannheim, Germany\n6Institute for Artificial Intelligence in Medicine (IKIM), University Medicine Essen, Essen, Germany\n*these authors contributed equally\nCorresponding Author:\nKlaus Kades, MSc\nGerman Cancer Research Center (DKFZ)\nIm Neuenheimer Feld 280\nHeidelberg, 69120\nGermany\nPhone: 49 6221420\nEmail: k.kades@dkfz.de\nAbstract\nBackground: Natural Language Understanding enables automatic extraction of relevant information from clinical text data,\nwhich are acquired every day in hospitals. In 2018, the language model Bidirectional Encoder Representations from Transformers\n(BERT) was introduced, generating new state-of-the-art results on several downstream tasks. The National NLP Clinical Challenges\n(n2c2) is an initiative that strives to tackle such downstream tasks on domain-specific clinical data. In this paper, we present the\nresults of our participation in the 2019 n2c2 and related work completed thereafter.\nObjective: The objective of this study was to optimally leverage BERT for the task of assessing the semantic textual similarity\nof clinical text data.\nMethods: We used BERT as an initial baseline and analyzed the results, which we used as a starting point to develop 3 different\napproaches where we (1) added additional, handcrafted sentence similarity features to the classifier token of BERT and combined\nthe results with more features in multiple regression estimators, (2) incorporated a built-in ensembling method, M-Heads, into\nBERT by duplicating the regression head and applying an adapted training strategy to facilitate the focus of the heads on different\ninput patterns of the medical sentences, and (3) developed a graph-based similarity approach for medications, which allows\nextrapolating similarities across known entities from the training set. The approaches were evaluated with the Pearson correlation\ncoefficient between the predicted scores and ground truth of the official training and test dataset.\nResults: We improved the performance of BERT on the test dataset from a Pearson correlation coefficient of 0.859 to 0.883\nusing a combination of the M-Heads method and the graph-based similarity approach. We also show differences between the test\nand training dataset and how the two datasets influenced the results.\nConclusions: We found that using a graph-based similarity approach has the potential to extrapolate domain specific knowledge\nto unseen sentences. We observed that it is easily possible to obtain deceptive results from the test dataset, especially when the\ndistribution of the data samples is different between training and test datasets.\n(JMIR Med Inform 2021;9(2):e22795) doi: 10.2196/22795\nKEYWORDS\nNatural Language Processing; semantic textual similarity; National NLP Clinical Challenges; clinical text mining\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 1https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nIntroduction\nEvery day, hospitals acquire large amounts of textual data which\ncontain valuable information for medical decision processes,\nresearch projects, and many other medical applications [1].\nHowever, the huge quantity of reports is unsuitable for manual\nexamination, and automatic access is hindered by the\nunstructured nature of the data [2]. Natural Language\nUnderstanding can help to tackle this problem by automatically\nextracting relevant information from textual data [3,4]. In this\npaper, we will focus on a subtask of Natural Language\nUnderstanding called Semantic Textual Similarity, which\nevolved within Natural Language Understanding as a dedicated\nresearch question aiming to address tasks like question\nanswering, semantic information retrieval, and text\nsummarization [5-9].\nIn the clinical domain, Semantic Textual Similarity has the\npotential to ease clinical decision processes (eg, by highlighting\ncrucial text snippets in a report), query databases for similar\nreports, assess the quality of reports, or be used in question\nanswering applications [1]. Furthermore, clinical reports are\noften of poor quality due to time limitations or due to the fact\nthat many text snippets are simply copy-pasted from other\nreports [10,11]. This introduces low-quality data samples that\nmake it harder for Natural Language Understanding algorithms\nto extract relevant information. In this context, Semantic Textual\nSimilarity can be a key processing step when dealing with\nredundant text snippets [2].\nState-of-the-art Natural Language Processing (NLP) methods\nfor assessing the Semantic Textual Similarity of nonclinical\ndata are developed and benchmarked based on the Semantic\nTextual Similarity benchmark, which compromises the SemEval\nSemantic Textual Similarity tasks from 2012 to 2017 [5] and\nis part of the General Language Understanding Evaluation\ndataset. In order to strengthen the development of Natural\nLanguage Processing tools for clinical and biomedical text data,\nwhich are often not publicly available, the team of the National\nNLP Clinical Challenges (n2c2), formerly known as i2b2 NLP\nShared Tasks, has issued several tasks and organized challenges\nsince 2006. This paper reports our participation in track 1,\n“n2c2/OHNLP Track on Clinical Semantic Textual Similarity,”\nof the 2019 National NLP Clinical Challenges. We present the\n3 submitted systems, a further best performing variation of the\ndifferent approaches, and a statistical analysis of the dataset.\nThe aim of the track in which we participated was to predict\nthe Semantic Textual Similarity between two clinical sentences.\nA similar task was already tackled in the BioCreative/OHNLP\n2018 ClinicalSTS track [3,4].\nThe winners of Track 1: ‘n2c2/OHNLP 2018 Track on Clinical\nSemantic Textual Similarity’ [3] proposed 4 systems that\ncombined string, entity, and number similarity features with\ndeep learning features. In their best performing system, the\nwinners trained a ridge regression model based on the prediction\nscore of 8 independently trained models [3,12].  The second\nbest performing team proposed an approach using\nAttention-Based Convolutional Neural Networks and\nBidirectional Long Short Term Memory networks [3].\nIn recent years, the general Natural Language Processing domain\nmade a huge step forward with the breakthrough of transfer\nlearning which allows leveraging semantic knowledge from\nhuge amounts of unlabeled text data. That is, a model can be\npretrained on enormous unlabeled text data with multiple\nunsupervised tasks. The trained model captures a universal\nlanguage representation and can be effectively fine-tuned on\ndifferent downstream tasks. For example, the 2018 language\nmodel, Bidirectional Encoder Representations from\nTransformers (BERT), introduced a multilayer bidirectional\nTransformer that is trained on a massive amount of text in two\nunsupervised tasks: (1) next sentence prediction and (2) masked\nword prediction. To use the model for further downstream tasks,\nit is usually enough to add a linear layer on top of the pretrained\nmodel to achieve state-of-the-art performance for the desired\ndownstream tasks [13]. Since the introduction of the\nTransformer and BERT, new variations of the original models\nperform even better by (1) introducing more pretraining tasks\n[14-16], (2) employing multitask learning approaches [17], and\n(3) combining the aforementioned approaches [18].\nThe application of pretrained models like BERT on clinical data\ncomes with the question if the model can handle domain-specific\nnuances. One proposed approach to handling domain-specific\nnuances is to use transfer learning to adapt the model to clinical\ndata [19-21]. Another approach is to incorporate already existing\nmethods. To investigate the extent to which BERT can handle\ndomain-specific nuances, we examined the performance of\nBERT on subgroups of sentences and found that it performed\nmodestly with sentence types that were simply structured and\nhighly specific (eg, sentences which prescribe medications).\nBased on these findings, we created 3 approaches with the aim\nto address the diversity of clinical sentences present in the given\ndata.\nTo summarize our contributions (see Figure 1), we show that\nthe use of BERT on clinical data can be enriched by the\nfollowing:\n• a simple modification of the BERT architecture by adding\nadditional similarity features and employing a built-in\nensembling method.\n• a graph-based similarity approach for a subset of structured\nsentences in which the knowledge of the training set is\nextrapolated to unseen sentence pairs of the test set.\nAdditionally, we show that statistically analyzing the data\nreveals differences between the training and test datasets. This\nanalysis made the process of interpreting the results easier.\nThe code to reproduce the results of this paper is available online\n[22].\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 2https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 1. Overview of our pipeline for the different approaches. Blue boxes denote feature sets (see Multimedia Appendix 3) or scores which are also\nused as features, and the framed boxes denote processing steps. Bold scores correspond to our three submissions. [CLS] represents a classifier token\nwhich is used by BERT for sentence level downstream tasks [13]. The medication graph works only on a subset of the data with input from the Voting\nRegression. New scores are predicted and replaced with other scores only for this subset. BERT: Bidirectional Encoder Representations from Transformers.\nMethods\nOverview\nOur methods were developed and tested on the data of the\nClinicalSTS shared task, which consists of a collection of\nelectronic health records from the Mayo Clinic’s dataset [4]. In\ntotal, 2054 sentence pairs were independently annotated to their\ndegree of semantic textual similarity on a scale from 0 (not\nsimilar at all) to 5 (completely similar) by two medical experts.\nThe focus of semantic textual similarity is whether two sentences\nhave similar meaning and content in contrast to, for example,\nthe number of words used in both sentences [3,4]. The created\nannotations are a mixture of integer and noninteger values,\nwhereby the latter arise when averaging the result of multiple\nannotations. The training set consists of 1642 sentence pairs\nand the test set of 412. The performance of the different methods\nis measured by the Pearson correlation coefficient which aims\nto measure the linear correlation between the predicted similarity\nscores and the annotated similarity scores. More detailed\ninformation about the creation of the dataset, its properties, and\nits evaluation can be found elsewhere [1,3,4].\nWe started by applying ClinicalBERT [23] to the dataset to\nobtain a baseline. That is, we used the [CLS] token from the\nlast layer of the BERT model and fed its values to an additional\nlinear layer that consists of a single neuron performing the\nsimilarity regression task. The whole network, including BERT\nand the additional layer, were trained on the Mean Square Error.\nWe use the [CLS] token because it is designed for sentence\nclassification and regression tasks. During training of BERT,\n[CLS] tokens are used for the next sentence prediction task. The\n[CLS] token is part of every sentence pair and captures the\naggregated attention weights from each token of the sentence\npair [13]. Next, we analyzed the predictions of BERT to find\nsentences with a high deviation from the ground truth. For this,\nwe extracted InferSent embeddings for each sentence pair, as\nthey are suited to cover the semantic representation of sentences\n[24], clustered them via -means, and calculated the absolute\ndifference between the BERT scores and the ground truth for\nthe whole cluster. The cluster analysis is shown in Figure 2. To\nmake the comparison between the clusters easier, we show an\noverview of the absolute score differences per cluster in Figure\n3. From these visualizations, we see that cluster 3 has the highest\ndifference on average or, in other words, that BERT cannot\nhandle these sentences well. Looking at the sentences, we see\nthat this cluster is dominated by sentences which prescribe\nmedication, for example  “ondansetron [ZOFRAN] 4 mg tablet\n1 tablet by mouth three times a day as needed” or “furosemide\n[LASIX] 40 mg tablet 1 tablet by mouth two times a day.” This\nweakness was essentially the motivation for our third approach\n(medication graph) which focuses solely on the medication\nsentence type.\nIn the following section, we describe the approaches shown in\nour pipeline (Figure 1) in more detail. Information about the\npreprocessing steps and further implementation details can be\nfound in Multimedia Appendix 1 and Multimedia Appendix 2.\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 3https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. Clustering of the sentences to reveal BERT’s weaknesses. Each point represents a sentence pair from the training set, and the corresponding\nabsolute score difference is visualized as opacity, or, in other words, the more opaque a point is, the higher the deviation from the ground truth. The\npoints are the t-SNE projected InferSent embeddings of all sentence pairs. For each cluster, the average absolute deviation from the ground truth as well\nas the distribution of the differences is shown in the legend. Best viewed in colour. BERT: Bidirectional Encoder Representations from Transformers.\nt-SNE: t-distributed stochastic neighbor embedding.\nFigure 3. Box plot showing the absolute score differences for each cluster emphasizing the opacity information from Figure 2. The number below the\nbold cluster index is the cluster size. For each box plot, the following information is depicted: the box ranges from the lower to the upper quartiles with\nthe notch at the median position. The whiskers extend up to 1.5 times the interquartile range. Remaining points (outliers) are not shown. The white\nsquare denotes the mean value.\nApproaches\nApproach 1: Enhancing BERT With Features Based on\nSimilarity Measures\nThe motivation behind this approach is to enhance BERT with\nadditional information that BERT might not be able to capture\nin its model. On a token level, BERT uses a predefined tokenizer\nbased on a set of rules; however, it might be valuable to compare\narbitrary tokens based on character -grams. On a sentence-level,\nBERT does have a classifier token, [CLS], to compare two\nsentences. However, the [CLS] token was not designed to be a\nsentence-embedding [25,26]. Therefore, comparing embeddings\nlike InferSent. which are specifically designed to represent the\nsemantic of a whole sentence, might add additional valuable\ninformation to predict the similarity between two sentences.\nIn this approach, we used two kinds of similarity measures: (1)\ntoken-based and (2) sentence embedding–based. For a\ntoken-based similarity measure, -grams of characters are created\nand then compared with each other. For example, Jaccard\nSimilarity compares the proportion between the intersection\nand the union of -grams in two input sentences. For a sentence\nembedding-based similarity measure, the embeddings of two\nsentences are compared, for example by taking the cosine\nsimilarity between the two embeddings of the two input\nsentences. The similarity measures were inspired by Chen et al\n[12].\nWe combined BERT with two feature sets of similarity measures\nat two different positions in our pipeline (Figure 1). In a first\nstep named Enhanced BERT, we enhanced the [CLS] token of\nBERT with similarity measures from the first feature set (Feature\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 4https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nSet I) before feeding the concatenated vector to the final linear\nregression layer. In a second step named Voting Regression,\nwe fed the predicted output scores from Enhanced BERT\ntogether with a second feature set (Feature Set II) into several\nestimators (see Multimedia Appendix 3) whose predicted output\nscores were averaged with the help of a voting regressor [27].\nFeature Sets I and II were created by successively trying out\ndifferent combinations of similarity features in order to gauge\nthe best performance. A breakdown of the two features sets can\nbe found in Multimedia Appendix 3.\nApproach 2: M-Heads\nEnsembling methods have been very popular in recent machine\nlearning challenges [28,29]. The general approach is to duplicate\na model or parts of a model, repeat the prediction for each\nmodel, and aggregate the prediction results. The intuition is that\ndifferent models can focus on various aspects of the input data\n(eg, different sentence types) so that they produce different\npredictions. The aggregation over these predictions can help to\nemphasize the group opinion over the dominance of a single\nmodel, thereby mitigating the risk that a model just reacted to\nnoise in the input data [30].\nWe took up this point and decided to include a simple\nensembling method directly into the architecture of BERT. More\nconcretely, we duplicated the final linear layer (the head) which\nreceives the last [CLS] token from the BERT model and which\nis responsible for calculating the regression (score prediction).\nWe initialized each head layer with different weights to allow\nthe different solutions per head. We employed a loss scaling\nwhich enforces specialization of the different heads similar to\nmethods seen in other research [31,32]. A detailed description\nof our M-Heads updated scheme during training and how we\nperformed predictions on new samples can be found in the\nMultimedia Appendix 4.\nApproach 3: Medication Graph\nIn this approach, we focused on a subset of the sentence pairs\nwhich we named “medication sentences,” for example\n“ibuprofen 150 mg tablet 2 tablets by mouth every 7 hours as\nneeded.” Further examples are listed in the discussion. These\nsentences are fairly structured and can be compared by analyzing\nindividual entities. We used the MedEx-UIMA system [33,34]\nto extract medication related fields from the sentences and\ndecided to use the entity’s active agent (“ibuprofen”), strength\n(“150 mg”), dose (“2 tablets”), and frequency (“7”). We\nconsidered the active agent as the major contributing factor in\nterms of the similarity of medication sentences. Hence, we\nmodeled similarities between active agents that were then further\nmodified by the remaining entities to retrieve a similarity score\nfor each medication sentence pair.\nOur general idea was to determine the property of similarity\nbetween active agent pairs as compared to unknown active agent\npairs. That is, we assumed that the similarity of active agents\nA and B as well as B and C also contained information about\nthe similarity between A and C. We generalized this process\nby constructing a graph containing all active agents as nodes\nwith corresponding similarities assigned to the edges, using the\nshortest path between arbitrary active agents as a foundation to\npredict a similarity score which could then be further modified\nby the remaining entities (ie, every entity except the active\nagent).\nIn the following section, we describe how we delt with the\nremaining entities, in which way we constructed the graph of\nall active agents, and how we used this information to predict\nsimilarity scores for new sentence pairs.\nFeature Construction\nEven though we considered the active agents as the central part\nregarding sentence similarity, we still did not want to neglect\nother influences and, hence, we constructed a set of additional\nfeatures per sentence pair, which reflect the similarity of\neverything except the active agents. More concretely, we\nconstructed a set of similarity features Δk and compared the\nentity value of the first sentence ek,1 with the entity value of the\nsecond sentence ek,2. For nominally scaled entities, we calculated\nΔk as\nand for ratio-scaled entity types, we used the squared difference\nFor entities like “strength” (eg, “4 mg”), we first separated the\nunit (“mg”) from the number (“4”), used the nominal approach\nto compare the unit, and applied the squared difference equation\non the number part. This differentiation gives us k=1 , … , N\nfeatures per sentence in total (N=5 in our case, since we used\nstrength with amount and unit, and dose with amount and unit\nas well as frequency).\nGraph Construction\nWe used all medication sentences S = (a1, a2, s, Δ1, …, ΔN) from\nthe training set with the active agents a1 and a2 from the sentence\npair, the similarity score s, and the remaining entity features Δk.\nWe constructed our similarity graph G (V, E) by using the\npossible active agents Ai as nodes V={A1, A2, …} and connected\nall node pairs which occurred together in a sentence pair. More\nprecisely, we constructed an edge set E={( Ai, Aj, wij)(i,j) P with\nthe set of all possible active agent pairs and the edge weight\nwhich models the modified similarity score wij between the\nactive agents. C denotes the set of all sentence pairs with the\nsame active agents, and λ represents weights for the entity\ndifferences learned during the training process (see later text).\nλ0 can be interpreted as a bias. The tahn(x) function limits the\nchange of the similarity score, s, and the final result is clipped\nto stay in the valid range defined by smin = 0 and smax = 5.\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 5https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nThe intuition here is that the weights, wij, should model the\nsimilarities between the active agents without factoring in the\nremaining entities. However, the true similarity value is not\navailable as the similarity score, s, is also influenced by the\nremaining entities. The idea is that the weighted sum between\nthe weights, wij, and the differences, Δk, allows us to alter the\nsimilarity score, s, in a way so that wij models the true similarity\nbetween the active agents. The outer sum, responsible for\naveraging the items in the set C, is only necessary because it is\npossible that multiple sentence pairs with the same active agents\nexist.\nInference\nThe goal of the inference phase was to calculate a sentence\nsimilarity score, s, between two active agents, Ai and Aj, based\non a similarity score, \n  , obtained from the graph and the\nentity differences, Δk. This consisted of two steps: first, we\ncalculated the active agent similarity via the graph, and then we\naltered this similarity to account for the remaining entity features\nΔk.\nStep 1: In its simplest form, the similarity between two active\nagents is just the weight of the edge between the two\ncorresponding active agent nodes. For example, wij = \n .\nHowever, this is only possible when the weight already occurs\nin the training set and is not applicable in general, as G (V, E)\nis not a complete graph, and it may be the case that an edge\nbetween the two active agent nodes does not exist. As we still\nwanted to make a prediction for these cases, we proposed to\nfind the shortest path between these two active agent nodes and\naggregate all edge weights along the way. This assumes a\ntransitive relationship between the nodes or, for example, when\nthere is a connection between A1 and A2 as well as A2 and A3,\nwe can still say something about the nonexisting connection\nbetween A1 and A3. More concretely, we aggregate the\ninformation \n along the shortest path\nwhere pij(1), pij(2), …, pij(M) denote the indices of the nodes\non the shortest path between Ai and Aj. This equation resembles\nthe formula for calculating the resistance of parallel circuits\nwith the final resistance, Req, of the circuit and the resistances,\nRi, of the individual flows. We chose this formula because the\nfinal resistance, Req, is always smaller than the individual\nresistances, Ri. For instance, Req ≤ min (R1, R2, …)\n[35]. In our case, this implies that the score \n obtained from\nthe graph is always lower than any of the scores along the\nshortest path. This relies on our assumption that it is not possible\nto restore dissimilarities; for example, if there is already a score\nof 1 (low similarity) on an edge, we do not want to increase this\nvalue further by adding more connections, as we already know\nthat at least two active agents are dissimilar.\nStep 2: The weight, \n , is the prediction for the similarity of\nan active agent pair. The final goal was to retrieve a prediction\nscore, s, for a sentence pair which is also influenced by the\nremaining entity features, Δk. We accounted for this by altering\nthe predicted score again by\nto retrieve a similarity, s, for the sentence pair.\nFigure 4 shows an excerpt of the graph, which uses all the\nsentence pairs in the training set. The shortest path between the\nactive agents “calcium” and “prednisone” is highlighted to\nvisualize the prediction steps. Detailed calculations are available\nin the online version of the graph [36].\nIt may be possible that the sentences contain additional\ninformation that we do not cover in our approach, such as\nadditional words, the relation between words, etc. For this\nreason, we combined the similarity score, sg, from the graph\nwith the BERT scores, sb, in a Support Vector Regressor trained\non all sentences in the training set to retrieve a final prediction\nscore. We used Radial Basis Functions as kernel and optimized\nthe regularization parameter C as well as ε (ε-tube without\npenalty) during the learning process of λk.\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 6https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 4. Excerpt of the medication graph, which models similarities between active agent pairs. On the edges, the modified similarity score,\n<inline-graphic xlink:href=\"medinform_v9i1e22795_fig10.png\" mimetype=\"image\" xlink:type=\"simple\"/>, is shown. The full graph is available as an\nonline widget, which provides further information and shows the graph calculations between arbitrary active agent nodes.\nLearning λk\nThe parameters λk are responsible for the transformation\nbetween the active agent and sentence similarities as they reflect\nthe importance of each entity feature, which also includes\nscaling differences. We did not manually craft these weights\nbut learned them in a random walk process instead. The general\nidea was to randomly change the weights and see whether this\nimproved the graph performance and, only if it did, we kept the\nchange.\nFor a more stable evaluation, we split the training data into 10\nfolds, built a graph based on each training set, and evaluated\nthe graph performance based on the corresponding test set. For\nevaluation, we calculated the mean squared error between the\nprediction scores and the ground truth. We did not use the\nPearson correlation coefficient here because the correlation on\na subset may not be as helpful for the correlation on the\ncomplete dataset as a measure which directly enforces a\ncloseness with the ground truth.\nLet λ = (λ0, λ1, …, λN) denote the vector with the current value\nof the weights (randomly initialized in the beginning) and let\nMES(λ) denote the error when using these weights with the\npredictions from all folds. Then, we randomly selected an index,\nk, and altered the corresponding weight\nλ'k = λk + Ν(0,1)\nvia a sample from a standard normal distribution so that we\nobtained a new weight vector\nλ’= (λ0, λ1, …, λ'k, …, λN)\nwhich we evaluated again on the graphs from all folds, keeping\nthe change if\nMES(λ’) < MES(λ)\nWe repeated this process in two iterations, alternating with the\nprocess of hyperparameter tuning of the SVR model, until we\nobserved no further improvements. For each random walk\nprocess, we applied 50 update steps. During development, we\nfound that this setting was already sufficient and that the\nresulting weights tended to remain unchanged after these\nupdates. For the SVR model, we applied a grid search to find\nvalues for the hyperparameters C and ε-tube. We used the final\nweights to construct a new graph (based on all training data)\nused to predict the similarity of new sentences.\nResults\nDataset Evaluation\nIn order to help with the interpretation of our results in the next\nsection, we applied some basic statistical analysis on the training\nand test set, which revealed some imbalances. On average, the\nsimilarity score of the sentences in the training set\n(approximately 2.79) was higher than in the test set\n(approximately 1.76), whereas the standard deviation was\nslightly higher in the test set (approximately 1.52) than in the\ntraining set (approximately 1.39). This is also indicated by the\nleft histogram chart of Figure 5, which, for example, reveals\nthat sentence pairs with a score of approximately 1 are the most\nprominent ones in the test set, whereas in the training set,\nsentence pairs with a score of approximately 3 occur most often.\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 7https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 5. Histogram of the label distribution and word lengths of the training and test set.\nThe right histogram chart of Figure 5 shows the distribution of\nthe number of words of the training and test data. On average,\nthe sentences in the test set tended to be shorter than in the\ntraining set, with an average sentence length of approximately\n26 words per sentence pair (SD of approximately 7 words) in\nthe former and approximately 42 words per sentence pair (SD\nof approximately 26 words) in the latter.\nFinally, we calculated InferSent embeddings of the sentences\nin the training and test dataset and visualized them in a t-SNE\n(t-distributed stochastic neighbor embedding) plot (Figure 6).\nThis shows that the sentence types occurring in the test dataset\nrepresent only a subset of those occurring in the training dataset,\nwith many clusters of the training set being unoccupied by the\ntest set, such as the blue cluster in the bottom of Figure 6 without\nsentences of the test set in the neighborhood.\nFigure 6. t-SNE projected InferSent embeddings of the sentences in the training and test dataset. Different groups of points correspond to different\nsentence types. For example, the group on the left upper side corresponds to the medication sentences. t-SNE: t-distributed stochastic neighbor embedding.\nEvaluation Results\nWe evaluated all runs on 3 different sets. Firstly, we used the\ntraining set with k = 150 folds to reduce the influence of noise\nin the data, to increase the comparability of our models, and to\neasily employ another ensembling technique for the test set.\nWe wanted to measure the correlation of the training set and\nnot of one of the folds to get comparable results. For this, we\nconcatenated the predictions from each fold together and then\ncalculated the Pearson correlation coefficient only once based\non all scores. That is, we did not calculate a Pearson correlation\ncoefficient for each fold, but rather collected the scores from\nall folds first. The consequence of this approach is that we\ncannot provide information about the variance, because only\none Pearson correlation coefficient value is available.\nSecondly, for the evaluation of the test set, we employed an\nadditional ensembling technique by using the model for each\nfold to calculate a prediction for a sentence pair and then\naveraged all predictions.\nTable 1 gives an overview of our results for the different datasets\nand approaches. Our best result with a Pearson correlation\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 8https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\ncoefficient of 0.883 was achieved by combining enhanced BERT\nwith M-Heads and the medication graph. For comparison, the\nwinner of track 1 from IBM Research reached a Pearson\ncorrelation coefficient of 0.901 in their best submission [4].\nTable 1. Summarization of the different approaches and their results. Training and test Pearson correlation coefficient scores are rounded to 3 decimal\nplaces.\nTest setTraining setApproach\nApproach 0: baseline\n0.8590.850ClinicalBERT \nApproach 1: voting regression\n0.8590.851Enhanced BERT \n0.849a0.860Voting Regression \nApproach 2: M-Heads\n0.876a,b0.853Enhanced BERT with M-Heads \n0.883c0.853Enhanced BERT with M-Heads + Med. Graph \nApproach 3: medication graph\n0.862a0.862Voting Regression + Med. Graph \naOur submissions.\nbWe submitted a score of 0.869 for this setting because we were able to use only 10 k-folds due to a shortage of time.\ncOur best result of the test set.\nDiscussion\nOur 3 approaches performed differently on the two datasets. In\nthe following sections, we discuss the results in more detail and\ngive our thoughts.\nApproach 1: Voting Regression\nEvaluating the pure ClinicalBERT model, we see that the\nPearson correlation coefficient is slightly higher for the test set\nas compared to the training set. The Enhanced BERT\narchitecture led to an almost neglectable improvement on the\ntraining set and, in the test set, to no improvement at all. This\nindicates that, in this case, the additional features do not provide\nmore information than what is already contained in the [CLS]\ntoken from the last hidden layer of BERT.\nThe Voting Regression approach showed an improvement of\nthe Pearson correlation coefficient of the training set; however,\nfor the test set, the performance decreased. These results might\nbe traced back to overfitting of the training set. However, the\ndecrease in the test set might also be explained by the\nimbalances between the training and test set.\nApproach 2: M-Heads\nAdding M = 4 heads to BERT increased the Pearson correlation\ncoefficient of both the training and test set as compared to\nClinicalBERT. Especially for the test set, this indicates that the\ncombination of the different heads improves BERT’s\nperformance.\nApproach 3: Medication Graph\nReplacing the scores of the sentence subset which prescribes\nmedications (cluster 3) with the medication graph scores led,\nin both cases (approaches 1 and 2), to an improvement for the\ntest set. For the training set, however, we saw only marginal\nimprovements, such as 0.860 to 0.862 from approach 1 to\napproach 3. This might be due to the Pearson correlation\ncoefficient metric. In our experiments, we also evaluated our\napproaches with the Mean Squared Error between the\npredictions and the ground truth only on the subset of medication\nsentences. Without applying the medication graph (approach\n1), we obtained a Mean Squared Error of 0.70, and with the\nmedication graph (approach 3) a Mean Squared Error of 0.58.\nCombining the M-Heads approach with the medication graph\nyielded our best results for the test set, which indicates that\nBERT does have problems handling this domain-specific\nknowledge and therefore cannot cope well with these specific\ntypes of sentences.\nWhy did the medication graph perform better for the test set\nthan for the training set? First, we observe that the test set\ncontained more low-ranked sentences (see Figure 5); in\nparticular, the medication sentences had lower scores. For the\ntraining set, the mean and standard deviation of the scores was\n2.03 and 1.05, respectively, whereas the scores for the test set\nhad only a mean and standard deviation of 1.10 and 0.50,\nrespectively. We also noticed that the medication graph tended\nto dampen the prediction or, in other words, it led to lower\nscores. For example, the mean prediction score was 2.58 before\nand 1.78 after score replacement of the 94 medication sentences\nin the test set (see Table 2), which shows some example\nsentences of how the medication graph altered the scores). This\ncould be due to two reasons: (1) the scores on the edges in the\ngraph tended to be low (1.87 on average), and (2) the weight\ncombination enforced low scores when there was at least one\nedge with a low score, which could explain why the medication\ngraph achieved better predictions. This effect is facilitated by\nthe fact that the training dataset contained only 147 out of 1642\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 9https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n(8.95%) of sentences that prescribed medication, whereas the test set contained 94 out of 412 (22.82 %) medication sentences.\nTable 2. Comparison of the Pearson correlation coefficient scores predicted by the Voting Regression (approach 1, A1) and the medication graph\n(approach 3, A3) via randomly selected example sentences. T denotes the ground truth score for the corresponding sentence pair. This table shows only\nthe relevant entities from the original example sentences.\nSetA3A1TSentence bSentence a\nTraining1.701.683.0Amoxicillin, 500 mg, 2 capsules, three times a\nday\nOndansetron, 4 mg, 1 tablet, three times a day\nTraining1.682.020.5Aleve, 220 mg, 1 tablet, two times a dayProzac, 20 mg, 3 capsules, one time daily\nTraining1.701.591.5Ibuprofen, 600 mg, 1 tablet, four times a dayHydrochlorothiazide, 25 mg, one-half tablet, every\nmorning\nTest1.682.741.5Acetaminophen, 500 mg, 2 tablets, three times a\nday\nAleve, 220 mg, 1 tablet, two times a day\nTest1.692.291.0Naproxen, 500 mg, 1 tablet, two times a dayLisinopril, 10 mg, 2 tablets, one time daily\nConclusions\nTo tackle the problem of semantic textual similarity of medical\ndata, we developed 3 different approaches. We proposed to add\nadditional features to BERT and to weigh different regression\nmodels based on the BERT result and other features. Moreover,\nwe proposed the application of M-Heads and an attempt to\nautomatically extrapolate medical knowledge from the training\ndata. We observed that the success of the different methods\nstrongly depended on the underlying dataset. In future work, it\nmight be interesting to evaluate the methods on different and\nbigger datasets from other domains. The medication graph could\nbe a powerful method with the possibility to be applied to other\ndomains where it is necessary to extrapolate information from\nknown entities and where it is not possible to calculate this\ninformation directly. It may also be used to model other concepts\nwhich exist in the medical domain, such as ontologies.\nAcknowledgments\nThis research was supported by the German Cancer Consortium (DKTK), the German Cancer Research Center (DKFZ), and the\nHelmholtz Association under the joint research school HIDSS4Health (Helmholtz Information and Data Science School for\nHealth).\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nPreprocessing.\n[DOCX File , 11 KB-Multimedia Appendix 1]\nMultimedia Appendix 2\nImplementation Details.\n[DOCX File , 552 KB-Multimedia Appendix 2]\nMultimedia Appendix 3\nDetailed description of Feature Set I and Feature Set II.\n[DOCX File , 552 KB-Multimedia Appendix 3]\nMultimedia Appendix 4\nM-Heads: training and prediction.\n[DOCX File , 14 KB-Multimedia Appendix 4]\nReferences\n1. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS: a resource for clinical semantic textual\nsimilarity. Lang Resources & Evaluation 2018 Oct 24;54(1):57-72. [doi: 10.1007/s10579-018-9431-1]\n2. Liu S, Wang Y, Liu H. Selected articles from the BioCreative/OHNLP challenge 2018. BMC Med Inform Decis Mak 2019\nDec 27;19(Suppl 10):262 [FREE Full text] [doi: 10.1186/s12911-019-0994-6] [Medline: 31882003]\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 10https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n3. Wang Y, Afzal N, Liu S, Rastegar-Mojarad M, Wang L, Shen F. Overview of BioCreative/OHNLP Challenge 2018 Task\n2: Clinical Semantic Textual Similarity. In: Proceedings of the BioCreative/OHNLP Challenge. 2018. 2018 Presented at:\nBioCreative/OHNLP Challenge. 2018; August, 2018; Washington DC, USA. [doi: 10.1145/3233547.3233672]\n4. Wang Y, Fu S, Shen F, Henry S, Uzuner O, Liu H. The 2019 n2c2/OHNLP Track on Clinical Semantic Textual Similarity:\nOverview. JMIR Med Inform 2020 Nov 27;8(11):e23375 [FREE Full text] [doi: 10.2196/23375] [Medline: 33245291]\n5. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and\nCrosslingual Focused Evaluation. In: Proceedings of the 11th International Workshop on Semantic Evaluation\n(SemEval-2017).: Association for Computational Linguistics; 2017 Presented at: 11th International Workshop on Semantic\nEvaluation (SemEval-2017); 2017; Vancouver, Canada p. 1-14. [doi: 10.18653/v1/s17-2001]\n6. Fan H, Ma Z, Li H, Wang D, Liu J. Enhanced answer selection in CQA using multi-dimensional features combination.\nTinshhua Sci. Technol 2019 Jun;24(3):346-359. [doi: 10.26599/tst.2018.9010050]\n7. Majumder G, Pakray P, Gelbukh A, Pinto D. Semantic Textual Similarity Methods, Tools, and Applications: A Survey.\nCyS 2016 Dec 26;20(4). [doi: 10.13053/cys-20-4-2506]\n8. Zhang S, Zheng X, Hu C. A survey of semantic similarity and its application to social network analysis. In: 2015 IEEE\nInternational Conference on Big Data (Big Data).: IEEE Computer Society; 2015 Presented at: IEEE International Conference\non Big Data (Big Data); 2015; Santa Clara, CA, USA. [doi: 10.1109/bigdata.2015.7364028]\n9. H.Gomaa W, A. Fahmy A. A Survey of Text Similarity Approaches. IJCA 2013 Apr 18;68(13):13-18. [doi:\n10.5120/11638-7118]\n10. Embi PJ, Weir C, Efthimiadis EN, Thielke SM, Hedeen AN, Hammond KW. Computerized provider documentation:\nfindings and implications of a multisite study of clinicians and administrators. J Am Med Inform Assoc 2013;20(4):718-726\n[FREE Full text] [doi: 10.1136/amiajnl-2012-000946] [Medline: 23355462]\n11. Zhang R, Pakhomov SV, Lee JT, Melton GB. Using language models to identify relevant new information in inpatient\nclinical notes. AMIA Annu Symp Proc 2014;2014:1268-1276 [FREE Full text] [Medline: 25954438]\n12. Chen Q, Du J, Kim S, Wilbur W, lu Z. Combining rich features and deep learning for finding similar sentences in electronic\nmedical records. In: Proceedings of the BioCreative/OHNLP Challenge. 2018. 2018 Sep Presented at: BioCreative/OHNLP\nChallenge. 2018; August 2018; Washington DC, USA.\n13. Devlin J, Chang M, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers). BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. NAACL-HLT 2019; 2019 Presented at: Conference of the North American\nChapter of the Association for Computational Linguistics; 2019; Minneapolis, Minnesota p. 4171-4186.\n14. Wang W, Bi B, Yan M, Wu C, Bao Z, Xia J. StructBERT: Incorporating Language Structures into Pre-training for Deep\nLanguage Understanding. eprint arXiv:1908.04577. 2019 Aug. URL: https://arxiv.org/abs/1908.04577[accessed 2021-01-07]\n[WebCite Cache ID 2019arXiv190804577W]\n15. Sun Y, Wang S, Li Y, Feng S, Tian H, Wu H, et al. ERNIE 2.0: A Continual Pre-Training Framework for Language\nUnderstanding. In: AAAI Technical Track: Natural Language Processing. 2020 Apr 03 Presented at: AAAI Conference\non Artificial Intelligence, 34(05); February, 2020; New York, USA p. 8968-8975. [doi: 10.1609/aaai.v34i05.6428]\n16. Clark K, Luong MT, Le QV, Manning CD. ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\n2020 Presented at: International Conference on Learning Representations; April 2020; Addis Ababa, Ethiopia p. 23.\n17. Liu X, He P, Chen W, Gao J. Multi-Task Deep Neural Networks for Natural Language Understanding. In: Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics.: Association for Computational Linguistics;\n2019 Presented at: 57th Annual Meeting of the Association for Computational Linguistics; July, 2019; Florence, Italy p.\nA. [doi: 10.18653/v1/P19-1441]\n18. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M. Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer. Journal of Machine Learning Research 2010;21:1-67.\n19. Liu H, Perl Y, Geller J. Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT. AMIA\nAnnu Symp Proc 2019;2019:1129-1138 [FREE Full text] [Medline: 32308910]\n20. Sun C, Yang Z. Transfer Learning in Biomedical Named Entity Recognition: An Evaluation of BERT in the PharmaCoNER\ntask. In: Proceedings of The 5th Workshop on BioNLP Open Shared Tasks.: Association for Computational Linguistics;\n2019 Presented at: The 5th Workshop on BioNLP Open Shared Tasks; November 2019; Hong Kong, China p. 100-104.\n[doi: 10.18653/v1/d19-5715]\n21. Lin C, Bethard S, Dligach D, Sadeque F, Savova G, Miller T. Does BERT need domain adaptation for clinical negation\ndetection? J Am Med Inform Assoc 2020 Apr 01;27(4):584-591. [doi: 10.1093/jamia/ocaa001] [Medline: 32044989]\n22. MIC-DKFZ/n2c2 Challenge 2019 Source-Code. MIC-DKFZ GitHub. URL: https://github.com/MIC-DKFZ/\nn2c2-challenge-2019 [accessed 2021-07-01]\n23. Alsentzer E, Murphy J, Boag W, Weng W, Jindi D, Naumann T. Publicly Available Clinical BERT Embeddings. In:\nProceedings of the 2nd Clinical Natural Language Processing Workshop.: Association for Computational Linguistics; 2019\nPresented at: 2nd Clinical Natural Language Processing Workshop; June, 2019; Minneapolis, Minnesota, USA p. 72-78.\n[doi: 10.18653/v1/w19-1909]\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 11https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n24. Conneau A, Kiela D, Schwenk H, Barrault L, Bordes A. Supervised Learning of Universal Sentence Representations from\nNatural Language Inference Data. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing. 2017 Presented at: Conference on Empirical Methods in Natural Language Processing; September, 2017;\nCopenhagen, Denmark p. 670-680. [doi: 10.18653/v1/d17-1070]\n25. Wang B, Kuo CJ. SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models. IEEE/ACM\nTrans. Audio Speech Lang. Process 2020;28:2146-2157. [doi: 10.1109/taslp.2020.3008390]\n26. Reimers N, Gurevych I. Sentence-BERTntence Embeddings using Siamese BERT-Networks. In: Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP).: Association for Computational Linguistics; 2019 Presented at: 2019 Conference\non Empirical Methods in Natural Language Processing; November, 2019; Hong Kong, China p. A. [doi:\n10.18653/v1/d19-1410]\n27. Varoquaux G, Buitinck L, Louppe G, Grisel O, Pedregosa F, Mueller A. Scikit-learn. GetMobile: Mobile Comp. and Comm\n2015 Jun;19(1):29-33. [doi: 10.1145/2786984.2786995]\n28. Opitz D, Maclin R. Popular Ensemble Methods: An Empirical Study. jair 1999 Aug 01;11:169-198. [doi: 10.1613/jair.614]\n29. Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, et al. ImageNet Large Scale Visual Recognition Challenge. Int\nJ Comput Vis 2015 Apr 11;115(3):211-252. [doi: 10.1007/s11263-015-0816-y]\n30. Lee S, Purushwalkam S, Cogswell M, Crandall D, Batra D. Why M Heads are Better than One: Training a Diverse Ensemble\nof Deep Networks. eprint arXiv:1511.06314. 2015 Nov. URL: https://arxiv.org/abs/1511.06314 [accessed 2020-01-07]\n31. Ilg E, Çiçek  , Galesso S, Klein A, Makansi O, Hutter F. Uncertainty Estimates and Multi-hypotheses Networks for Optical\nFlow. In: European Conference on Computer Vision (ECCV). 2018 Presented at: European Conference on Computer Vision\n(ECCV); 2018; Munich, Germany. [doi: 10.1007/978-3-030-01234-2_40]\n32. Rupprecht C, Laina I, DiPietro R, Baust M, Tombari F, Navab N. Learning in an Uncertain World: Representing Ambiguity\nThrough Multiple Hypotheses. In: 2017 IEEE International Conference on Computer Vision (ICCV).: IEEE; 2017 Presented\nat: IEEE International Conference on Computer Vision (ICCV); 2017; Venice, Italy p. 3611-3620. [doi:\n10.1109/iccv.2017.388]\n33. Jiang M, Wu Y, Shah A, Priyanka P, Denny JC, Xu H. Extracting and standardizing medication information in clinical text\n- the MedEx-UIMA system. AMIA Jt Summits Transl Sci Proc 2014;2014:37-42 [FREE Full text] [Medline: 25954575]\n34. Xu H, Stenner SP, Doan S, Johnson KB, Waitman LR, Denny JC. MedEx: a medication information extraction system for\nclinical narratives. J Am Med Inform Assoc 2010;17(1):19-24 [FREE Full text] [doi: 10.1197/jamia.M3378] [Medline:\n20064797]\n35. Meschede D. Gerthsen Physik. In: Gerthsen Physik. Heidelberg, Germany: Springer Spektrum; 2015.\n36. Medication Graph Visualization. URL: https://med-graph.jansellner.net/ [accessed 2021-07-01]\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers\nNLP: Natural language Processing\nn2c2: National NLP Clinical Challenges\nOHNLP: Open Health Natural Language Processing\nEdited by Y Wang; submitted 28.07.20; peer-reviewed by M Torii, L Ferreira; comments to author 08.10.20; revised version received\n03.12.20; accepted 22.12.20; published 03.02.21\nPlease cite as:\nKades K, Sellner J, Koehler G, Full PM, Lai TYE, Kleesiek J, Maier-Hein KH\nAdapting Bidirectional Encoder Representations from Transformers (BERT) to Assess Clinical Semantic Textual Similarity: Algorithm\nDevelopment and Validation Study\nJMIR Med Inform 2021;9(2):e22795\nURL: https://medinform.jmir.org/2021/2/e22795\ndoi: 10.2196/22795\nPMID: 33533728\n©Klaus Kades, Jan Sellner, Gregor Koehler, Peter M Full, T Y Emmy Lai, Jens Kleesiek, Klaus H Maier-Hein. Originally\npublished in JMIR Medical Informatics (http://medinform.jmir.org), 03.02.2021. This is an open-access article distributed under\nthe terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted\nuse, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 12https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nproperly cited. The complete bibliographic information, a link to the original publication on http://medinform.jmir.org/, as well\nas this copyright and license information must be included.\nJMIR Med Inform 2021 | vol. 9 | iss. 2 | e22795 | p. 13https://medinform.jmir.org/2021/2/e22795\n(page number not for citation purposes)\nKades et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX"
}