{
    "title": "Iterative Transformer Network for 3D Point Cloud",
    "url": "https://openalex.org/W2902965877",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2347792871",
            "name": "Yuan, Wentao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3175998449",
            "name": "Held, David",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287436966",
            "name": "Mertz, Christoph",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3088919902",
            "name": "Hebert, Martial",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2190691619",
        "https://openalex.org/W2886499109",
        "https://openalex.org/W2949440248",
        "https://openalex.org/W2160821342",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W1920022804",
        "https://openalex.org/W2963188159",
        "https://openalex.org/W2785053089",
        "https://openalex.org/W2898360221",
        "https://openalex.org/W2083624955",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2097832352",
        "https://openalex.org/W2100657858",
        "https://openalex.org/W2963474899",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W2035379092",
        "https://openalex.org/W2962957031",
        "https://openalex.org/W2769389383",
        "https://openalex.org/W2950493473",
        "https://openalex.org/W2964109313",
        "https://openalex.org/W2200124539",
        "https://openalex.org/W2129404737",
        "https://openalex.org/W2336098239",
        "https://openalex.org/W603908379",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2091791686",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W2402144811",
        "https://openalex.org/W2151290401",
        "https://openalex.org/W2553307952",
        "https://openalex.org/W2118877769"
    ],
    "abstract": "3D point cloud is an efficient and flexible representation of 3D structures. Recently, neural networks operating on point clouds have shown superior performance on 3D understanding tasks such as shape classification and part segmentation. However, performance on such tasks is evaluated on complete shapes aligned in a canonical frame, while real world 3D data are partial and unaligned. A key challenge in learning from partial, unaligned point cloud data is to learn features that are invariant or equivariant with respect to geometric transformations. To address this challenge, we propose the Iterative Transformer Network (IT-Net), a network module that canonicalizes the pose of a partial object with a series of 3D rigid transformations predicted in an iterative fashion. We demonstrate the efficacy of IT-Net as an anytime pose estimator from partial point clouds without using complete object models. Further, we show that IT-Net achieves superior performance over alternative 3D transformer networks on various tasks, such as partial shape classification and object part segmentation.",
    "full_text": "Iterative Transformer Network for 3D Point Cloud\nWentao Yuan David Held Christoph Mertz Martial Hebert\nThe Robotics Institute\nCarnegie Mellon University\n{wyuan1, dheld, cmertz, mhebert}@cs.cmu.edu\nAbstract\n3D point cloud is an efﬁcient and ﬂexible representa-\ntion of 3D structures. Recently, neural networks operat-\ning on point clouds have shown superior performance on\n3D understanding tasks such as shape classiﬁcation and\npart segmentation. However, performance on such tasks\nis evaluated on complete shapes aligned in a canonical\nframe, while real world 3D data are partial and unaligned.\nA key challenge in learning from partial, unaligned point\ncloud data is to learn features that are invariant or equiv-\nariant with respect to geometric transformations. To ad-\ndress this challenge, we propose the Iterative Transformer\nNetwork (IT-Net), a network module that canonicalizes the\npose of a partial object with a series of 3D rigid transfor-\nmations predicted in an iterative fashion. We demonstrate\nthe efﬁcacy of IT-Net as an anytime pose estimator from\npartial point clouds without using complete object mod-\nels. Further, we show that IT-Net achieves superior per-\nformance over alternative 3D transformer networks on var-\nious tasks, such as partial shape classiﬁcation and object\npart segmentation. Our code and data are available at\nhttps://github.com/wentaoyuan/it-net.\n1. Introduction\n3D point cloud is the raw output of most 3D sensors and\nmultiview stereo pipelines [9] and a widely used represen-\ntation for 3D structures in applications such as autonomous\ndriving [10] and augmented reality [14]. Due to its efﬁ-\nciency and ﬂexibility, there is a growing interest in using\npoint clouds for high level tasks such as object recognition,\nskipping the need for meshing or other post-processing.\nThese tasks require an understanding of the semantic con-\ncept represented by the points. On other modalities like im-\nages, deep neural networks [11, 15] have proven to be a\npowerful model for extracting semantic information from\nraw sensor data, and have gradually replaced hand-crafted\nfeatures. A similar trend is happening on point clouds. With\nthe introduction of deep learning architectures like PointNet\n[19], it is possible to train powerful feature extractors that\nFigure 1: Iterative Transformer Network (IT-Net) predicts\nrigid transformations from partial point clouds in an itera-\ntive fashion. It can be used independently as a pose estima-\ntor or jointly with classiﬁcation and segmentation networks.\noutperform traditional geometric descriptors on tasks such\nas shape classiﬁcation and object part segmentation.\nHowever, existing benchmark datasets [25, 28] that are\nused to evaluate performance on these tasks make two sim-\nplifying assumptions: ﬁrst, the point clouds are sampled\nfrom complete shapes; second, the shapes are aligned in a\ncanonical coordinate system1 (see Figure 2). These assump-\ntions are rarely met in real world scenarios. First, due to\nocclusions and sensor limitations, real world 3D scans usu-\nally contain missing regions. Second, point clouds are of-\nten obtained in the sensors coordinates, which do not align\nwith the canonical coordinates of the object model. In other\nwords, real 3D point cloud data are partial and unaligned.\nIn this work, we tackle the problem of learning from par-\ntial, unaligned point cloud data. To this end, we build a\ndataset consisting of partial point clouds generated from vir-\n1In ModelNet [25], shapes are allowed to have rotations, but only along\nthe vertical axis.\narXiv:1811.11209v2  [cs.CV]  18 Oct 2019\nFigure 2: Complete point clouds aligned in a canonical\nframe from ShapeNet (top row) versus partial, unaligned\npoint clouds from our dataset (bottom row).\ntual scans of CAD models in ModelNet [25] and ShapeNet\n[7] as well as real world scans from ScanNet [8]. Our\ndataset contains challenging inputs with arbitrary 3D rota-\ntion, translation and realistic self-occlusion patterns.\nA key challenge in learning from such data is how to\nlearn features that are invariant or equivariant with respect\nto geometric transformations. For tasks like classiﬁcation,\nwe want the output to remain the same if the input is trans-\nformed. This is called invariance. For tasks like pose esti-\nmation, we want the output to vary according to the trans-\nformation applied on the input. This is called equivariance.\nOne way to achieve invariance or equivariance is via a trans-\nformer network [12], which predicts a transformation that\nis applied to the input before feature extraction. The pre-\ndicted transformation allows explicit geometric manipula-\ntion of data within the network so the inputs can be aligned\ninto a canonical space that makes subsequent tasks easier.\nT-Net [19] is a transformer network based on PointNet\nthat operates on 3D point clouds. However, T-Net outputs\nan unconstrained afﬁne transformation. This can introduce\nundesirable shearing and scaling which causes the object to\nlose its shape (see Figure 3). Moreover, T-Net is evaluated\non inputs with 2D rotations only.\nTo address the shortcomings of T-Net, we propose a\nnovel transformer network on 3D point clouds, named Iter-\native Transformer Network (IT-Net). IT-Net has two major\ndifferences from T-Net. First, it outputs a rigid transforma-\ntion instead of an afﬁne transformation. Outputting rigid\ntransformation allows the outputs to be used directly as es-\ntimates for object poses and leads to better performance on\nsubsequent tasks such as shape classiﬁcation and part seg-\nmentation. Second, instead of predicting the transformation\nin a single step, IT-Net takes advantage of an iterative re-\nﬁnement scheme which decomposes a large transformation\ninto smaller ones that are easier to predict. The multi-step\noutput not only increases accuracy of the predicted trans-\nformation, but also allows anytime prediction, i.e. the result\ncan be gradually reﬁned until the test-time computational\nbudget is depleted.\nWe demonstrate the advantage of IT-Net over alternative\n(a) Input\n (b) Transformed input\nFigure 3: T-Net [19] scales and distorts the input shape (a\nvase). Note the different scales on the plots.\ntransformer networks on three point cloud learning tasks –\npose estimation, shape classiﬁcation and part segmentation\n(Figure 1) – with partial, unaligned inputs from synthetic as\nwell as real world 3D data.\nThe key contributions of our work are as follows:\n•We propose a novel transformer network called IT-\nNet that adds geometric invariance/equivariance to net-\nworks operating on 3D point clouds;\n•We demonstrate that IT-Net can be used as an any-\ntime pose estimator which outperforms strong base-\nlines when applied to point cloud alignment;\n•We show that IT-Net outperforms existing transformer\nnetworks on point clouds when trained jointly with\nvarious classiﬁcation or segmentation networks;\n•We introduce a new dataset for pose estimation, shape\nclassiﬁcation and part segmentation consisting of par-\ntial, unaligned point clouds.\n2. Related Work\nFeature Learning on Point Clouds Traditional point fea-\nture descriptors [21, 23] rely on geometric properties of\npoints such as curvatures. They do not encode semantic\ninformation and it is non-trivial to ﬁnd the combination of\nfeatures that is optimal for speciﬁc tasks.\nQi et al. [19] introduces a way to extract semantic and\ntask-speciﬁc features from point clouds using a neural net-\nwork, which outperforms competing methods on several\nshape analysis tasks like shape classiﬁcation. Subsequent\nworks [16, 20, 24] further improves the performance of\npoint cloud-based networks by accounting for interactions\namong local neighborhoods of points.\nSpatial Transformer Network Spatial Transformer Net-\nwork (STN) [12] is a network module that performs ex-\nplicit geometric transformations on the input image in a\ndifferentiable way. STN introduces invariance to geomet-\nric transformations and can be trained jointly with various\ntask-speciﬁc networks to improve their performance.\nIC-STN [17] is an extension of STN that makes use of\nan iterative scheme inspired by the Lucas-Kanade algorithm\n[18]. Our network utilizes a similar iterative scheme to pre-\ndict accurate geometric transformations.\nIterative Error Feedback The idea of using iterative er-\nror feedback (IEF) in neural networks have been studied in\nthe context of 2D human pose estimation [6] and taxonomic\nprediction [29]. Under the IEF framework, instead of trying\nto directly predict the target in a feed-forward fashion, the\nnetwork predicts the error in the current estimate and cor-\nrects it iteratively. While our proposed network falls under\nthis general framework, unlike previous works, it does not\nuse intermediate supervision or separate stages of training.\nRather, the loss is applied at a certain iteration during train-\ning and the gradient is propagated through the composition\nof outputs from previous iterations.\n3. Iterative Transformer Network\nIterative Transformer Network (IT-Net) takes a 3D point\ncloud and produces a transformation that can be used di-\nrectly as a pose estimate or applied to the input before fea-\nture extraction for subsequent tasks. IT-Net has two key fea-\ntures that differentiate it from existing transformer networks\non 3D point clouds: 1) it predicts a 3D rigid transformation;\n2) the ﬁnal output is composed of multiple transformations\nproduced in an iterative fashion.\n3.1. Rigid Transformation Prediction\nThe output of IT-Net is a 3D rigid transformationT, con-\nsisting of a rotation R and translation t where R is a 3 ×3\nmatrix satisfying RRT = I, det(R) = 1 and t is a 3 ×1\nvector. Due to the constraints on R, it is inconvenient to\nrepresent the rotation as a 3 ×3 matrix during optimization.\nThus, many classical [3] as well as modern deep learning\nmethods [13, 26] parametrize 3D rotations with unit quater-\nnions. The quaternion parametrization allows us to map an\narbitrary 4D vector to a valid 3D rotation.\nA single iteration of IT-Net is a pose regression network\nthat takes a point cloud and outputs 7 numbers parametriz-\ning a 3D rigid transformation. The ﬁrst 4 numbers are nor-\nmalized into a unit quaternion q and the last 3 are treated\nas a 3D translation vector t. Then, q and t are assembled\ninto a 4 ×4 matrix T =\n[\nR(q) t\n0 1\n]\nwhere R(q) is the\nrotation matrix corresponding to q. The matrix representa-\ntion turns the composition of two rigid transformations into\na matrix multiplication, which is convenient for composing\nthe outputs from multiple iterations. We use PointNet [19]\nas the regression network for its simplicity, but other point\ncloud-based networks can be used as well.\nIn contrast to the afﬁne transformation produced by T-\nNet [19], the rigid transformation predicted by IT-Net can\nbe directly interpreted as a 6D pose, making it possible to\nuse IT-Net independently for pose estimation. More impor-\ntantly, rigid transformations preserve scales and angles. As\na result, the appearance of a point cloud will not vary drasti-\ncally if it is transformed by the output of IT-Net. This makes\nit possible to apply the same network iteratively to obtain a\nmore accurate estimation of the transformation.\nWe note that it is possible to add a regularization term\n∥AAT −I∥that forces an afﬁne matrix A to be orthogonal\nin order to achieve similar effects of predicting a rigid trans-\nformation2. However, the constraint, no matter how close\nto satisﬁed, cannot produce a truly rigid transformation that\nprevents the deformation of inputs. As shown in Sec. 4.2,\nthe results of the regularized network are not as good as the\nnetwork that directly outputs rigid transformations.\n3.2. Iterative Alignment\nThe idea of using an iterative scheme for predicting ge-\nometric transformations goes back to the classical Lucas-\nKanade (LK) algorithm [18] for estimating dense alignment\nbetween images. The key insight of LK is that the com-\nplex non-linear mapping from image appearance to geomet-\nric transformations can be estimated iteratively using sim-\nple linear predictors. Speciﬁcally, at each iteration, a warp\ntransformation ∆p is predicted with a linear function that\ntakes a source and a target image as inputs. Then, the source\nimage is warped by ∆p and the process is repeated. The\nﬁnal transformation is a composition of ∆p at each step.\nLater, [2] shows that the parameters used to predict ∆p can\nremain constant across iterations while achieving the same\neffect as non-constant predictors.\nThe same idea is employed in the Iterative Closest Point\n(ICP) algorithm [3] for the alignment of 3D point clouds.\nAt each iteration of ICP, a corresponding set is identiﬁed\nand a rigid transformation ∆T is produced to align the cor-\nresponding points. Then, the source point cloud is trans-\nformed by ∆T and the process is repeated. Again, the ﬁnal\noutput is a composition of ∆T at each step. The effective-\nness of ICP shows that the iterative reﬁnement framework\napplies not only to images, but also to 3D point clouds.\nThe multi-iteration IT-Net (Figure 11) can be viewed as\nan instantiation of this iterative framework. Speciﬁcally, the\nprediction of the transformation T is unfolded into multiple\niterations. At the i-th iteration, a rigid transformation ∆Ti\nis predicted as described in Sec. 3.1. Then, the input is\ntransformed by ∆Ti and the process is repeated. The ﬁnal\noutput after n iterations is a composition of the transforma-\ntions predicted at each iteration, which can be written as a\nsimple matrix product Tn = ∏n\ni=1 ∆Ti.\nWe use a ﬁxed predictor (i.e. share the network’s param-\neters) across iterations following [2]. In addition to reduc-\ntion in the number of parameters, the ﬁxed predictor allows\n2In [19], this regularization is added to the feature transformation, but\nnot to the input transformation.\nFigure 4: Illustration of the iterative scheme employed by IT-Net. At each iteration, the output of the pose regression network\nis used to transform the input for the next iteration. The parameters of the pose regression network shown in blue arrows are\nshared across iterations. The ﬁnal output is a composition of the transformations predicted at each iteration. Arrows colored\nin red indicate places where the gradient ﬂow is stopped to decorrelate the inputs at different iterations (see Sec. 3.3).\nus to use different numbers of unfolded iterations in training\nand testing. As will be shown in Sec. C, once trained, IT-\nNet can be used as an anytime predictor where increasingly\naccurate pose estimates can be obtained as the network is\napplied for more iterations.\nThe iterative scheme can be interpreted as a way to au-\ntomatically generate a curriculum, which breaks down the\noriginal task into a set of simpler pieces. In earlier iter-\nations, the network learns to predict large transformations\nthat bring the input near its canonical pose. In later iter-\nations, the network learns to predict small transformations\nthat adjusts the estimate from previous iterations. Note that\nthe curriculum is not manually deﬁned but rather generated\nby the network itself to optimize the end goal. It will be\nempirically shown in Sec. C that this curriculum emerges\nfrom the training of IT-Net.\n3.3. Implementation Details\nIn addition to the key ingredients above, there are a cou-\nple of details that are important for the training of IT-Net.\nFirst, we initialize the network to predict the identity trans-\nformation q = [1 0 0 0], t = [0 0 0]. In this way, the default\nbehavior of each iteration is to preserve the transformation\npredicted by previous iterations. This identity initialization\nhelps prevent the network from producing large transforma-\ntions which cancel each other. Second, we stop the gradi-\nents propagating through input transformations (red arrows\nin Figure 11) and let the gradients propagate through the\noutput composition only (black arrows in Figure 11). This\nremoves dependency among inputs at different iterations\nwhich leads to gradient explosion. Empirical evaluations\nfor these design choices can be found in Sec. C.\n4. Experiments\nWe evaluate IT-Net on various point cloud learning tasks.\nIn Sec. C, we demonstrate the ability of IT-Net to estimate\nthe canonical pose of an object from partial views in an\nanytime fashion. In Sec. 4.2, we show that IT-Net outper-\nforms existing transformer networks when trained jointly\nwith state-of-the-art classiﬁers on partial, unaligned shapes\nfrom both synthetic and real world data. In Sec. D, we test\nIT-Net’s capability to improve performance of state-of-the-\nart models on object part segmentation, showing that the\ninvariance learned by IT-Net can beneﬁt a variety of shape\nanalysis tasks. We implemented all our networks in Tensor-\nFlow [1]. Detailed hyperparameter settings can be found in\nthe supplement.\nDataset To evaluate the performance of point cloud learn-\ning tasks under a more realistic setting, we build a dataset of\nobject point clouds which captures the incomplete and un-\naligned nature of real world 3D data. The dataset consists\nof the following parts:\n•Partial ModelNet40 includes 81,212 object point\nclouds in 40 categories generated from ModelNet40\n[25], split into 78,744 for training and 2,468 for test-\ning. Each point cloud is generated by fusing up to 4\ndepth scans of a CAD model into a point cloud.\n•ScanNet Objects consists of 9,122 object point clouds\nin 33 categories collected from ScanNet [8], split into\n8,098 for training and 1,024 for testing. The point\nclouds are obtained by cropping indoor RGBD scans\nwith labeled bounding boxes, where realistic sensor\nnoise and clutter in the RGBD scans are kept.\n•ShapeNet Pose includes 24,000 object point clouds\nin the car and chair category, generated by back-\nprojecting depth scans of 4,800 ShapeNet [7] models\nfrom uniformly sampled viewpoints into the camera’s\ncoordinates. Each point cloud is labeled with the trans-\nformation that aligns it to the model’s coordinates. The\ndata are split into training, validation and testing with\na 10:1:1 ratio. Note that the test set and the training set\nare created with different object models.\n•ShapeNet Part contains 16,881 object point clouds in\n16 categories from ShapeNet [7], split into 13,937 for\ntraining and 2,874 for testing. Each point cloud is la-\nbeled with 2-6 parts using the labels provided in [28].\nSince the part labels are provided for point clouds and\nnot meshes, we use an approximate rendering proce-\ndure that mimics an orthographic depth camera to cre-\nate realistic-looking partial point clouds.\nMore details on data generation are in the supplement.\n4.1. Object Pose Estimation\nWe investigate the efﬁcacy of the iterative reﬁnement\nscheme on the task of estimating the canonical pose of an\nobject from a partial observation. Speciﬁcally, we use IT-\nNet to predict the transformation that aligns the input shape\nto a canonical frame deﬁned across all models in the same\ncategory (see the top row in Figure 2). Unlike most exist-\ning works on pose estimation, we do not assume knowledge\nof the complete object model and we train a single network\nthat generalizes to different objects in the same category.\nThe network architecture is described in Sec. E where\nthe pose regression network is a PointNet [19]. Details\nabout the number of layers and parameters in the pose re-\ngression network can be found in the supplement.\nAn explicit loss is applied to the output transformation.\nFor the loss function, we use a variant of PLoss proposed\nin [26], which measures the average distance between the\nsame set of points under the estimated pose and the ground\ntruth pose. Compared to the L2 loss used in earlier works\n[13], this loss has the advantage of automatically handling\nthe tradeoff between small rotations and small translations.\nThe loss can be written as\nL((R, t), ( ˜R,˜t)) = 1\n|X|\n∑\nx∈X\n∥(Rx+t)−( ˜Rx+˜t)∥2\n2, (1)\nwhere R, t are the ground truth pose and ˜R,˜t are the esti-\nmated pose and X is the set of input points.\nWe trained IT-Net under different settings and evaluated\ntheir performance on the car point clouds in ShapeNet Pose.\nSimilar experiments on the chair point clouds in ShapeNet\nPose can be found in the supplement. In what follows, we\nprovide detailed analysis of the results.\nNumber of unfolded iterations The number of unfolded\niterations during training can be treated as a hyperparameter\nthat controls the iteration at which the loss is applied. We\ntrained IT-Net with different number of unfolded iterations\nand, as shown in Table 1, IT-Net trained with 5 unfolded\niterations gives the best performance.\nUnfolded\niterations 1 2 3 4 5 6 7\nno init 17.1 0.5 18.4 7.3 6.2 41.1 13.1\nno stop 36.0 5.0 0.0 0.0 4.1 4.1 0.0\nours 36.9 41.7 47.7 61.1 67.6 62.6 48.2\nTable 1: Pose accuracy (%) with error threshold 10◦, 0.1 of\nIT-Nets with different number of unfolded iterations during\ntraining. No init means the output is not initialized as the\nidentity transformation. No stop means the gradient is not\nstopped during input transformations.\nFigure 5: Pose accuracy (%) against the number of itera-\ntions applied during inference. The dotted line corresponds\nto the number of unfolded iterations in training. Note how\nthe accuracy keeps improving even when more iterations\nare applied than the network is trained for.\nIn Figure 7a, we visualized the distribution of input poses\nat different iterations during training by measuring errors\nwith respect to the canonical pose. It can be observed that\nthe distribution of poses skews towards the canonical pose\nin later iterations. This is evidence that the network gen-\nerates a curriculum as mentioned in Sec. 3.2. We observe\nthat an appropriate distribution of examples in the gener-\nated curriculum is key to good performance. With too few\nunfolded iterations, the network does not see enough ex-\namples with small errors and thus fails to predict accurate\nreﬁnements when the shape is near its canonical pose. With\ntoo many unfolded iterations, the network sees too many\nexamples with small errors, overﬁts to them and becomes\ntoo conservative in its prediction. Empirically, 5 iterations\nturns out to be a good compromise.\nAnytime pose estimation As noted in Sec. 3.2, sharing\nweights across iterations allows us to use a different num-\nber of iterations during inference than during training. Fig-\nure 5 shows the pose accuracy of an IT-Net trained with\n5 unfolded iterations against the number of iterations ap-\nplied during inference. It can be seen that the performance\nkeeps increasing as the number of iteration increases. This\nFigure 6: Comparison with non-learning baselines on point cloud alignment. The two plots on the left show the CDF of\nrotation and translation errors over 1000 test instances. The plot on the right shows the average running time per instance.\nIteration 0 Iteration 3 Iteration 6\n(a) Distribution of rotation and translation errors at different iterations\nInput Iteration 3 Iteration 6 Ground truth Input Iteration 3 Iteration 6 Ground truth\n(b) Qualitative examples\nFigure 7: (a) The distribution (PDF) of rotation and translation error of 2,400 test instances at different iterations. Note how\nthe error distribution skews towards 0 in later iterations. The peak at 180 degrees for rotation error is caused by symmetries\nin the car models. (b) Qualitative results at corresponding iterations.\nproperty allows us to use IT-Net as an anytime pose estima-\ntor. In other words, during inference, we can keep applying\nthe trained IT-Net to obtain increasingly accurate pose es-\ntimates until the time budget runs out (each iteration takes\nabout 0.025s on a 3.60GHz Intel Core i7 CPU).\nComparison with non-learning baselines We applied\nour pose estimation network to the problem of point cloud\nalignment and compared the results with classical baselines\nthat is not learning-based. Speciﬁcally, for each pair of\nshapes in the test set, we computed their relative transfor-\nmation using the poses predicted by IT-Net. The results are\ncompared against two state-of-the-art, non-learning-based\nalignment methods, GOICP [27] and GOGMA [5]. Un-\nlike classical ICP which only works with good initializa-\ntion, these baseline methods can estimate alignment from\narbitrary initialization. The results are shown in Figure 6.\nNote that IT-Net not only produces more accurate align-\nment, but is also several orders of magnitude faster on av-\nerage. The running times are measured on a 3.60GHz Intel\nCore i7 CPU.\nAblation studies We conducted ablation studies to vali-\ndate our design choices described in Sec. 3.3, i.e. initial-\nizing the network’s prediction with the identity transforma-\ntion and stopping the gradient ﬂow through input transfor-\nmations. The results are summarized in Table 1. It can\nbe seen that the performance degrades signiﬁcantly with-\nout either identity initialization or gradient stopping, which\nindicates that both are crucial for the iterative reﬁnement\nscheme to achieve desired behavior.\n4.2. 3D Shape Classiﬁcation\nThe network used for the partial shape classiﬁcation task\nconsists of two parts – the transformer and the classiﬁer.\nThe transformer takes a point cloud and produces a transfor-\nmation T. The classiﬁer takes the point cloud transformed\nby T and outputs a score for each class. The entire network\nis trained with cross-entropy loss on the class scores and no\nexplicit supervision is applied on the transformation T.\nWe compare classiﬁers trained with three different trans-\nformers, IT-Net, T-Net and regularized T-Net (T-Net reg).\nThe transformers share the same architecture except for the\nlast output layer. IT-Net outputs 7 numbers for rotation\n(quaternion) and translation; T-Net and T-Net reg outputs\n9 numbers to form a 3 ×3 afﬁne transformation matrix A.\nFor T-Net reg, a regularization term ∥AAT −I∥is added\nto the loss with weight 0.001. Batch normalization is ap-\nplied to all except the last layer. Details about the network\narchitecture can be found in the supplement.\nWe trained the transformers with two state-of-the-art\nshape classiﬁcation networks, PointNet [19] and Dynamic\nGraph CNN (DGCNN) [24], and tested their performance\non two datasets, Partial ModelNet40 and ScanNet Objects.\nResults Table 2 and 3 show the classiﬁcation accuracy on\nPartial ModelNet40 and ScanNet Objects respectively. It\ncan be seen that IT-Net consistently outperforms baseline\ntransformers when trained with different classiﬁers. This\nis evidence that the advantage of IT-Net is agnostic to the\nclassiﬁer architecture. Further, the advantage of IT-Net over\nbaselines on real data matches that on synthetic data. This\ndemonstrates IT-Net’s ability to process point clouds with\nrealistic sensor noise and clutter and its potential to be incor-\nporated in detection/pose estimation pipelines in the wild.\nClassiﬁer PointNet\nTransformer None T-Net T-Net reg IT-Net (ours)\n# Iterations 0 1 2 1 2 1 2\nAccuracy 59.97 66.04 35.13 65.84 67.06 68.72 69.94\nClassiﬁer DGCNN\nTransformer None T-Net T-Net reg IT-Net (ours)\n# Iterations 0 1 2 1 2 1 2\nAccuracy 65.60 70.38 16.61 71.15 72.69 72.57 74.15\nTable 2: Classiﬁcation accuracy on Partial ModelNet40.\nClassiﬁer PointNet\nTransformer None T-Net T-Net reg IT-Net (ours)\n# Iterations 0 1 2 1 2 1 2\nAccuracy 62.11 63.09 30.86 62.99 61.82 63.67 66.02\nClassiﬁer DGCNN\nTransformer None T-Net T-Net reg IT-Net (ours)\n# Iterations 0 1 2 1 2 1 2\nAccuracy 66.02 72.75 18.55 74.12 70.80 76.36 76.66\nTable 3: Classiﬁcation accuracy on ScanNet Objects.\nFinally, we observe that without explicit supervision, IT-Net\nlearns to transform the inputs into a set of canonical poses\nwhich we call “pose clusters”. The learned transformations\nremoves pose variations among the inputs, which simpliﬁes\nthe classiﬁcation problem. Some examples are shown in\nFigure 8 and more visualizations are in the supplement.\nWe note that in the classiﬁcation setting, the trans-\nformer’s job is to transform the inputs into one of the pose\nclusters so that they can be recognized by the classiﬁers,\nwhich is simpler than producing precise alignments as in\nSec. C. Therefore, the performance gain diminishes as the\nnumber of unfolded iterations becomes larger than 2.\nUnlike T-Net, IT-Net preserves the shape of the input\nwithout introducing any scaling or shearing. Figure 8 shows\nthat the output of T-Net is on a very different scale than the\noriginal input. This explains why the performance of T-\nNet drops signiﬁcantly if we try to apply the same iterative\nscheme directly: as the network sees inputs on vastly dif-\nferent scales from different iterations, the training fails to\nconverge. The regularized T-Net resolves this issue, but its\nperformance is still worse than IT-Net.\n4.3. Object Part Segmentation\nThe network used for part segmentation simply replaces\nthe classiﬁer in the joint transformer-classiﬁer model from\nSec. 4.2 with a segmentation network. We use DGCNN\n[24] as the base segmentation network and compare the\nperformance gain of adding T-Net and IT-Net. Following\n[19, 24], we treat part segmentation as a per-point classiﬁ-\ncation problem and train the network with a per-point cross\nentropy loss. Similar to Sec. 4.2, no explicit supervision\nInput\nIT-Net\nIteration 1\nIT-Net\nIteration 2\nT-Net\n(scaled by 0.1) Input\nIT-Net\nIteration 1\nIT-Net\nIteration 2\nT-Net\n(scaled by 0.1)\nFigure 8: Inputs transformed by IT-Net and T-Net trained jointly with DGCNN. Note how the input pose converges with\nmore iterations and the similarity of ﬁnal poses across different categories (columns 3, 7). T-Net’s outputs are on a much\ndifferent scale (10 times bigger) than the original inputs (columns 4, 8).\nmean table chair air\nplane\nlamp car guitar laptop knife pistol motor\ncycle\nmug skate\nboard\nbag ear\nphone\nrocket cap\n# shapes 5271 3758 2690 1547 898 787 451 392 283 202 184 152 76 68 66 55\n# parts 3 4 4 4 4 3 2 2 3 6 2 3 2 3 3 2\nNone 76.9 78.8 82.6 77.3 71.3 52.3 90.1 76.8 80.0 70.1 40.4 86.1 67.6 71.0 66.7 53.1 76.9\nT-Net 77.1 79.2 82.5 78.0 70.1 55.7 89.1 73.1 81.5 73.0 39.1 81.1 69.1 74.1 71.1 51.4 74.6\nIT-Net-1 78.2 79.9 84.3 78.2 72.9 54.9 91.0 78.7 78.1 71.8 44.6 84.8 66.6 71.2 72.7 55.0 77.9\nIT-Net-2 79.1 80.2 84.7 79.9 72.1 62.6 91.1 76.4 82.8 76.9 44.0 84.4 71.8 68.1 66.8 54.2 80.4\nTable 4: Part segmentation results on ShapeNet Part. The number appending IT-Net indicates the number of iterations. The\nbase segmentation model is DGCNN [24]. The metric is mIoU(%) on points. The mean is calculated as the average of\nper-category mIoUs weighted by the number of shapes. We order the categories by number of shapes since the performance\nis more unstable for categories with fewer shapes.\nis applied on the transformations. The networks are trained\nand evaluated on ShapeNet Part.\nResults We use the mean Intersection-over-Union (mIoU)\non points as the evaluation metric following [19, 24]. The\nresults are summarized in Table 6, which shows that IT-Net\nwith 2 iterations outperforms other transformers in terms of\nmean mIoU and mIoU for most categories. Figure 9 shows\nsome qualitative examples. As in the case of classiﬁcation,\nIT-Net reduces variations in the inputs caused by geometric\ntransformations by transforming the inputs to a canonical\npose. Note that the architecture of IT-Net here is identical\nto the ones in Sec. 4.2, which demonstrates the potential\nof IT-Net as a plug-in module for any task that requires in-\nvariance to geometric transformations without task-speciﬁc\nadjustments to the model architecture.\n5. Conclusion\nIn this work, we propose a new transformer network on\n3D point clouds named Iterative Transformer Network (IT-\nNet). In an iterative fashion, IT-Net outputs a rigid transfor-\nmation that can be used to estimate object pose or transform\nthe input for subsequent tasks. The effectiveness of IT-Net\nin various tasks shows that the classical idea of iterative re-\nOriginal Transformed Original Transformed\nFigure 9: Inputs transformed by IT-Net trained with\nDGCNN on part segmentation. The colors indicate predic-\ntions of the segmentation network.\nﬁnement still applies in the context of deep learning.\nIT-Net can be easily integrated with existing deep learn-\ning architectures for shape classiﬁcation and segmentation,\nand improve the performance on these tasks with partial,\nunaligned inputs by introducing invariance to geometric\ntransformations. This opens up many avenues for future\nresearch on using neural networks to extract semantic infor-\nmation from real world point cloud data.\nReferences\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensor-\nﬂow: A system for large-scale machine learning. In 12th\n{USENIX} Symposium on Operating Systems Design and\nImplementation ({OSDI} 16), pages 265–283, 2016. 4\n[2] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-\nfying framework. International journal of computer vision ,\n56(3):221–255, 2004. 3, 4\n[3] P. J. Besl and N. D. McKay. Method for registration of 3-d\nshapes. In Sensor Fusion IV: Control Paradigms and Data\nStructures, volume 1611, pages 586–607. International Soci-\nety for Optics and Photonics, 1992. 3\n[4] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Blender Institute,\nAmsterdam, 2018. 10\n[5] D. Campbell and L. Petersson. Gogma: Globally-optimal\ngaussian mixture alignment. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 5685–5694, 2016. 7\n[6] J. Carreira, P. Agrawal, K. Fragkiadaki, and J. Malik. Human\npose estimation with iterative error feedback. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 4733–4742, 2016. 3\n[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,\nQ. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,\net al. Shapenet: An information-rich 3d model repository.\narXiv preprint arXiv:1512.03012, 2015. 2, 4, 5, 10, 11\n[8] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,\nand M. Nießner. Scannet: Richly-annotated 3d reconstruc-\ntions of indoor scenes. In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR) , volume 1, page 1,\n2017. 2, 4, 11\n[9] Y . Furukawa and J. Ponce. Accurate, dense, and robust mul-\ntiview stereopsis. IEEE transactions on pattern analysis and\nmachine intelligence, 32(8):1362–1376, 2010. 1\n[10] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-\ntonomous driving? the kitti vision benchmark suite. In Com-\nputer Vision and Pattern Recognition (CVPR), 2012 IEEE\nConference on, pages 3354–3361. IEEE, 2012. 1\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n770–778, 2016. 1\n[12] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial\ntransformer networks. In Advances in neural information\nprocessing systems, pages 2017–2025, 2015. 2\n[13] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-\ntional network for real-time 6-dof camera relocalization. In\nProceedings of the IEEE international conference on com-\nputer vision, pages 2938–2946, 2015. 3, 5\n[14] G. Klein and D. Murray. Parallel tracking and mapping for\nsmall ar workspaces. InMixed and Augmented Reality, 2007.\nISMAR 2007. 6th IEEE and ACM International Symposium\non, pages 225–234. IEEE, 2007. 1\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems , pages\n1097–1105, 2012. 1\n[16] Y . Li, R. Bu, M. Sun, and B. Chen. Pointcnn. arXiv preprint\narXiv:1801.07791, 2018. 2\n[17] C.-H. Lin and S. Lucey. Inverse compositional spatial trans-\nformer networks. arXiv preprint arXiv:1612.03897 , 2016.\n3\n[18] B. D. Lucas, T. Kanade, et al. An iterative image registration\ntechnique with an application to stereo vision. 1981. 3\n[19] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep\nlearning on point sets for 3d classiﬁcation and segmentation.\nProc. Computer Vision and Pattern Recognition (CVPR),\nIEEE, 1(2):4, 2017. 1, 2, 3, 5, 7, 8, 10, 11, 12\n[20] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hi-\nerarchical feature learning on point sets in a metric space. In\nAdvances in Neural Information Processing Systems , pages\n5099–5108, 2017. 2\n[21] R. B. Rusu, N. Blodow, and M. Beetz. Fast point feature\nhistograms (fpfh) for 3d registration. In Robotics and Au-\ntomation, 2009. ICRA’09. IEEE International Conference\non, pages 3212–3217, 2009. 2\n[22] N. Sedaghat, M. Zolfaghari, E. Amiri, and T. Brox.\nOrientation-boosted voxel nets for 3d object recognition.\narXiv preprint arXiv:1604.03351, 2016. 10, 11\n[23] J. Sun, M. Ovsjanikov, and L. Guibas. A concise and prov-\nably informative multi-scale signature based on heat diffu-\nsion. In Computer graphics forum, volume 28, pages 1383–\n1392. Wiley Online Library, 2009. 2\n[24] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and\nJ. M. Solomon. Dynamic graph cnn for learning on point\nclouds. arXiv preprint arXiv:1801.07829, 2018. 2, 7, 8, 10,\n11\n[25] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumetric\nshapes. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 1912–1920, 2015. 1,\n2, 4\n[26] Y . Xiang, T. Schmidt, V . Narayanan, and D. Fox. Posecnn:\nA convolutional neural network for 6d object pose estimation\nin cluttered scenes. arXiv preprint arXiv:1711.00199, 2017.\n3, 5\n[27] J. Yang, H. Li, and Y . Jia. Go-icp: Solving 3d registration ef-\nﬁciently and globally optimally. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 1457–\n1464, 2013. 7\n[28] L. Yi, V . G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu,\nQ. Huang, A. Sheffer, L. Guibas, et al. A scalable active\nframework for region annotation in 3d shape collections.\nACM Transactions on Graphics (TOG), 35(6):210, 2016. 1,\n5, 10\n[29] A. R. Zamir, T.-L. Wu, L. Sun, W. B. Shen, B. E. Shi, J. Ma-\nlik, and S. Savarese. Feedback networks. InComputer Vision\nand Pattern Recognition (CVPR), 2017 IEEE Conference on,\npages 1808–1817. IEEE, 2017. 3\nSuppelementary\nA. Overview\nIn this document, we provide technical details and visu-\nalizations in support of our paper. Here are the contents:\nB: details on the generation of partial point clouds;\nC: pose estimation results on chairs in ShapeNet Pose;\nD: part segmentation results with PointNet as base model;\nE: details on network architecture and training;\nF: visualizations of pose clusters learned by IT-Net.\nB. Data Generation\nIn this section, we cover details on the generation of par-\ntial, unaligned object point clouds used in our experiments.\nAs mentioned in Sec. 4 of our paper, our dataset consists\nof four parts: Partial ModelNet40, ShapeNet Part, ShapeNet\nPose and ScanNet Objects. The ﬁrst three parts are created\nfrom scans of synthetic objects and the last part is created\nfrom real world RGB-D scans.\nIn Partial ModelNet40, each point cloud is generated by\nfusing a sequence of depth scans of CAD models from the\naligned ModelNet40 dataset [22]. The fused point clouds\nare in the coordinates of the ﬁrst camera. The orientation\nof the ﬁrst camera is uniformly random and the distance\nbetween the camera center and the object center is uniform\nbetween 2 to 4 units (the models are normalized into the\nunit sphere). Subsequent camera positions are generated\nby rotating the camera around the object center for up to\n30 degrees. We use Blender [4] to render the depth scans.\nCompared to the uniformly sampled point clouds used to\nevaluate classiﬁcation in prior works [19, 24], our dataset\ncontains much more challenging inputs with various poses,\ndensities and levels of incompleteness (see Figure 10).\nFor ShapeNet Part, since the part labels [28] are associ-\nated with sampled point clouds instead of the original mesh\nmodels from ShapeNet [7], we generate partial point clouds\nby virtually scanning the point clouds. Speciﬁcally, we ran-\ndomly rotate the complete point cloud and project the points\nonto a virtual image with pixel size 0.02 in the xy-plane.\nFor each pixel of the virtual image, we keep the point with\nthe smallest z value and discard all other points that project\nonto the same pixel as they are considered as occluded\nby the selected point. This procedure mimics an ortho-\ngraphic depth camera and creates partial point clouds that\nlook much like those created from rendered depth scans.\nThe partial point clouds in ShapeNet Pose are created\nfrom ShapeNet models in a similar way as Partial Model-\nNet40, except that the label is the transformation between\nthe camera coordinates and the model coordinates instead\nof the category. Table 5 summarizes the statistics and pa-\nrameters used to generate the synthetic parts of our dataset.\nThe point clouds in ScanNet Objects are created from\n1,512 real world RGB-D scans of indoor scenes in ScanNet\nModelNet40 Partial\nModelNet40 ModelNet40 Partial\nModelNet40\nFigure 10: Comparison between ModelNet40 used in [19,\n24] and partial ModelNet40 used in our experiments.\nPartial\nModelNet\nShapeNet\nPart\nShapeNet\nPose\nTask Classiﬁcation Segmentation Pose estimation\nSource ModelNet40 [22] ShapeNet [7] ShapeNet [7]\n# classes 40 16 1\n# train 78,744 12,137 22,000\n# test 2,468 1,870 2,000\n# scans 1-4 1 1\nScan size 64 ×64 50 ×50 128 ×128\nFocal length 57 ∞ 64\nDistance to\nobject center 2-4 ∞ 1-2\nTable 5: Statistics and parameters of our partial point cloud\ndataset for classiﬁcation, segmentation and pose estimation.\n[8]. In total, we collect 9,122 object points clouds by crop-\nping the scans with labeled bounding boxes, where sensor\nnoise and clutter in the boxes are kept in the resulting point\nclouds. We normalize the point clouds into the unit sphere\nand translate their centroids to the origin.\nC. Pose Estimation on Chairs\nAs mentioned in Sec. 4.1 of our paper, we performed\nexperiments on the chair point clouds in ShapeNet Pose us-\ning the same setting as on the car point clouds. The results\nveriﬁed our claims in Sec. 4.1. First, Table 7 shows that\nIT-Net trained with 5 unfolded iterations gives the best per-\nformance, which indicates that the curriculum generated by\n5-iteration IT-Net strikes a balance between examples with\nsmall and large pose errors. Second, Figure 12 shows that\non a different category, IT-Net keeps the property that pose\naccuracy increases with the number of testing iterations.\nFigure 11: Detailed transformer architecture. Numbers in the parenthesis indicate the number of neurons in each MLP layer.\nThe output dimension M is 7 for IT-Net and 9 for T-Net and T-Net reg.\nmean table chair air\nplane\nlamp car guitar laptop knife pistol motor\ncycle\nmug skate\nboard\nbag ear\nphone\nrocket cap\n# shapes 5271 3758 2690 1547 898 787 451 392 283 202 184 152 76 68 66 55\n# parts 3 4 4 4 4 3 2 2 3 6 2 3 2 3 3 2\nNone 67.9 71.6 75.2 68.8 56.9 48.2 82.4 58.0 68.5 61.7 39.0 65.6 49.6 41.9 43.5 28.1 50.9\nT-Net 71.1 73.7 77.5 73.6 60.2 53.0 85.8 63.2 73.6 65.4 48.5 70.3 57.7 15.9 41.8 41.7 48.5\nIT-Net-1 72.3 74.5 78.7 75.9 60.6 57.7 85.1 58.3 78.6 67.9 51.5 70.3 61.6 31.6 53.9 35.2 45.3\nIT-Net-2 72.6 75.1 78.3 76.3 62.1 56.3 86.8 58.9 74.5 68.6 46.4 70.6 65.9 43.5 51.6 42.6 45.9\nTable 6: Part segmentation results on partial shapes from ShapeNet Part. The number appending IT-Net indicates the number\nof unfolded iterations during training. The base segmentation model is PointNet [19]. The metric is mIoU(%) on points. The\nmean is calculated as the average of per-category mIoUs weighted by the number of shapes.\nUnfolded\niterations 1 2 3 4 5 6 7\nPose\naccuracy 27.4 36.3 43.6 62.4 64.3 56.4 51.1\nTable 7: Pose accuracy (%) with error threshold 10◦, 0.1 of\nIT-Nets on chair point clouds in ShapeNet Pose.\nFigure 12: Pose accuracy (%) against the number of itera-\ntions applied during inference. The dotted line corresponds\nto the number of unfolded iterations in training.\nD. Part Segmentation with PointNet\nTable 6 shows part segmentation results on ShapeNet\nPart using PointNet [19] as the base segmentation model in-\nstead of DGCNN [24] used in the experiments in Sec. 4.3.\nSimilar to the classiﬁcation results in Sec. 4.2, the segmen-\ntation results show evidence that the advantage of IT-Net is\nagnostic to the architecture of the segmentation network.\nE. Architecture and Training Details\nFigure 11 shows the detailed architecture of the PointNet\n[19] used as the pose regression network in all our experi-\nments. The architecture consists of three parts. The ﬁrst part\nis a multi-layer perceptron (MLP) that is applied on each\npoint independently. It takes the N ×3 coordinate matrix\nand produces a N ×1024 feature matrix. The second part is\na max-pooling function which aggregates the features in to a\n1×1024 vector. The third part is another MLP that regresses\nM pose parameters from the 1024-dimensional global fea-\nture vector. We have M = 7 for IT-Net and M = 9 for\nT-Net and T-Net-reg.\nWe use publicly available implementations of PointNet\n[19] and DGCNN [24] for the classiﬁcation and segmenta-\ntion networks. The detailed network architectures can be\nfound in Section C of the supplementary for [19] and Sec-\ntion 5.1 and 5.4 of [24].\nThe pose estimation networks in Sec. 4.1 are trained for\n20000 steps with batch size 100. We use Adam optimizer\nwith an initial learning rate of 0.001, decayed by 0.7 every\n2000 steps. The initial decay rate for batch normalization is\n0.5 and gradually increased to 0.99.\nThe joint transformer-classiﬁcation networks in Sec. 4.2\nare trained for 50 epochs with batch size 32. We use the\nAdam optimizer with an initial learning rate of 0.001, de-\ncayed by 0.7 every 6250 steps. The initial decay rate for\nbatch normalization is 0.5 and gradually increased to 0.99.\nWe clip the gradient norm to 30.\nThe joint transformer-segmentation networks in Sec. 4.3\nare trained for 200 epochs with batch size 32. Other hyper-\nparameters are the same as in Sec. 4.2.\nF. Pose Cluster Visualizations\nFigure 13 and Figure 14 show visualizations of the “pose\nclusters” learned by IT-Net as mentioned in Sec. 4.2. To\nvisualize the pose clusters, we calculate the difference be-\ntween the canonical orientation of the input shape and the\norientation of the transformed input at different iterations.\nThen, we convert the orientation difference into axis-angle\nrepresentation, which is a 3D vector, and plot these vectors\nfor all test examples in a particular category. The learned\npose clusters for the guitar and the bottle category are shown\nin Figure 13 and Figure 14 respectively. The network being\nvisualized is a 2-iteration IT-Net trained with DGCNN for\nshape classiﬁcation on Partial ModelNet40.\nWe observe that although the object poses are uniformly\ndistributed initially, clusters of poses emerge after apply-\ning the transformations predicted by IT-Net (Figure 13a,\n14a). This is evidence that IT-Net discover a canonical\nspace to align the inputs with no explicit supervision. In-\nterestingly, there are usually more than one cluster and the\nshapes of the clusters are related to the symmetries of the\nobject (Figure 13b, 14b). Further, we note that sometimes\neven objects across different categories are aligned after be-\ning transformed by IT-Net (Figure 13d, 14d).\nInput (Iteration 0) Iteration 1 Iteration 2\n(a) Distribution of orientations at different iterations.\n(b) Reﬂection\nsymmetry of\nguitars.\n(c) Examples of original inputs (Iteration 0).\n (d) Examples of transformed inputs (Iteration 2).\nFigure 13: Pose cluster visualization for guitars. (a) Distribution of axis-angle representation of orientations of all test\nexamples at different iterations. Note how clusters emerge from uniformly distributed poses. Correctly classiﬁed examples\nare shown in blue and incorrectly classiﬁed examples are shown in red. (b) The reﬂection symmetry present in most guitars.\n(c) Examples of original inputs at iteration 0. The object orientations are uniformly distributed. (d) Examples of transformed\ninputs at iteration 2. Note that these are the inputs received by the classiﬁer. The object orientations are grouped into 4\nclusters, but visually there seems to be only 2 major orientations due to the reﬂection symmetry shown in (b). A failure case\ncaused by heavy occlusion is shown in the red box.\nInput (Iteration 0) Iteration 1 Iteration 2\n(a) Distribution of orientations at different iterations.\n(b) Rotational\nsymmetry of\nbottles.\n(c) Examples of original inputs (Iteration 0).\n (d) Examples of transformed inputs (Iteration 2).\nFigure 14: Pose cluster visualization for bottles. (a) Distribution of axis-angle representation of orientations of all test ex-\namples at different iterations. Note how clusters emerge from uniformly distributed poses. Correctly classiﬁed examples are\nshown in blue and incorrectly classiﬁed examples are shown in red. (b) The rotational symmetry present in most bottles. (c)\nExamples of original inputs at iteration 0. The object orientations are uniformly distributed. (d) Examples of transformed\ninputs at iteration 2. Note that these are the inputs received by the classiﬁer. The object orientations after transformation\nare grouped into 2 clusters. The clusters have semicircle shapes since any orientation in these semicircles are in fact indis-\ntinguishable due to the rotational symmetry shown in (b). A failure case is shown in the red box. In this case the model\nmisclassiﬁes the bottle as a vase."
}