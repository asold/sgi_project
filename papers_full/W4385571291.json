{
  "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners",
  "url": "https://openalex.org/W4385571291",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2125004599",
      "name": "Xuandong Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893179430",
      "name": "Siqi Ouyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118883612",
      "name": "Zhiguo Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105277534",
      "name": "Ming Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964071174",
    "https://openalex.org/W4287028759",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4224055980",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4286982826",
    "https://openalex.org/W2551773530",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2970254524",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3173617765",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W4385574162",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4312091568",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W4309444617",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3105625590",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3098267758"
  ],
  "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available at https://anonymous.4open. science/r/NPPrompt.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15590–15606\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPre-trained Language Models Can be Fully Zero-Shot Learners\nXuandong Zhao†, Siqi Ouyang †, Zhiguo Yu ‡, Ming Wu ‡, Lei Li †\n†UC Santa Barbara ‡Microsoft\n{xuandongzhao,siqiouyang,leili}@cs.ucsb.edu\n{zhiguo.yu,mingwu}@microsoft.com\nAbstract\nHow can we extend a pre-trained model to\nmany language understanding tasks, without la-\nbeled or additional unlabeled data? Pre-trained\nlanguage models (PLMs) have been effective\nfor a wide range of NLP tasks. However,\nexisting approaches either require fine-tuning\non downstream labeled datasets or manually\nconstructing proper prompts. In this paper,\nwe propose nonparametric prompting PLM\n(NPPrompt) for fully zero-shot language under-\nstanding. Unlike previous methods, NPPrompt\nuses only pre-trained language models and\ndoes not require any labeled data or additional\nraw corpus for further fine-tuning, nor does\nit rely on humans to construct a comprehen-\nsive set of prompt label words. We eval-\nuate NPPrompt against previous major few-\nshot and zero-shot learning methods on di-\nverse NLP tasks: text classification, text entail-\nment, similar text retrieval, paraphrasing, and\nmultiple-choice question answering. Experi-\nmental results demonstrate that our NPPrompt\noutperforms the previous best fully zero-shot\nmethod by big margins, with absolute gains\nof 12.8% in accuracy on text classification\nand 15.6% on the GLUE benchmark. Our\nsource code is available at https://github.\ncom/XuandongZhao/NPPrompt.\n1 Introduction\nNatural language understanding (NLU) has been\nimportant in many applications such as intelligent\ndialog assistants, online search, and social media\nanalysis. Recent advancement of NLU has been\ndriven by emergent pre-trained language models\n(PLMs) including BERT (Devlin et al., 2019; Liu\net al., 2019b), GPT (Radford et al., 2018, 2019;\nBrown et al., 2020), BART (Lewis et al., 2020),\nand T5 (Raffel et al., 2020). Prior studies show\nthat PLMs obtain substantial knowledge during pre-\ntraining on raw text corpus (Petroni et al., 2019;\nFeldman et al., 2019). By fine-tuning on task-\nspecific labeled data, PLMs exploit such knowl-\nedge and gain impressive accuracy on a wide range\nof NLP tasks, such as text classification (Kowsari\net al., 2019), question answering (Rajpurkar et al.,\n2016), machine reading comprehension (Campos\net al., 2016), etc.\nHowever, fine-tuning approaches are expensive.\nIt requires labeled datasets, which are rarely avail-\nable for many tasks. Significant computational\nefforts are needed to update PLMs’ parameters for\nmultiple tasks. In addition, fine-tuning results in\none distinct model for each task to maintain.\nHow can we generalize a pre-trained model to\nmany NLP tasks, without labeled or additional un-\nlabeled data? Existing few-shot and zero-shot ap-\nproaches propose to construct prompts to elicit de-\nsired predictions from PLMs (Brown et al., 2020).\nThe main idea of prompting PLMs is to convert\nan input utterance to one with masked templates.\nFor example, in text classification an input can be\n“The Warriors won the NBA championship 2022”\nand it is instead converted to “The Warriors won\nthe NBA championship 2022. This topic is about\n[MASK]”. A PLM (e.g. BERT) takes the converted\ntext and produces predictions for the masked token,\nalong with the probability. Ideally, a PLM will\ngenerate a higher probability for the word “sports\"\nthan “politics\" on the [MASK] token.\nAlthough these prompting-based methods are\neffective, they require unlabeled data for training\nor huge human efforts to construct prompts and to\nchoose designated tokens to represent class labels\n(Schick and Schütze, 2021a,b; Gao et al., 2021). In\naddition, these manually constructed verbalizers,\ni.e. mapping from words (e.g. “basketball”) to\nclass labels (e.g. SPORTS ), do not extend to new\nemerging categories after PLMs are deployed.\nIn this paper, we investigate the fully zero-shot\nlearning problem for NLU where only the target\nlabel names are available but not the extra raw\ntext. We propose nonparametric prompting PLM\n(NPPrompt), a novel method to generate predic-\n15590\ntions for semantic labels without any fine-tuning.\nNPPrompt uses PLM’s own embeddings to auto-\nmatically find relevant words to labels (e.g. “basket-\nball” and “NBA” forSPORTS ), therefore it does not\nneed humans to construct verbalizers. Our key idea\nis to search for the topk nearest neighbors to a label\nname in the embedding manifold and then generate\nand aggregate PLM’s predicted logits from masked\nprompts. In the above case, both predicted val-\nues for “basketball” and “NBA” contribute to the\nfinal prediction for the SPORTS category. In this\nway, NPPrompt can be easily generalized to any\nnew categories as long as the category names are\nsemantically meaningful.\nThe contributions of this paper are as follows. a)\nWe develop NPPrompt, a novel method for fully\nzero-shot learning with PLMs. b) We conduct\nextensive experiments on diverse language under-\nstanding tasks including text classification, text en-\ntailment, similar text retrieval, paraphrasing, and\nmultiple-choice question answering. Experimen-\ntal results show that NPPrompt outperforms the\nprevious zero-shot methods by absolute 12.8% in\naccuracy on text classification and 15.6% on the\nGLUE benchmark. Surprisingly, NPPrompt is on\na par with the best prior method that trained with\nmanual verbalizers, an additional knowledge base,\nand extra unlabeled data.\n2 Related Work\nPrompting The success of GPT-3 (Brown et al.,\n2020) has attracted much attention to prompting\nengineering, a new way to leverage pre-trained\nlanguage models. (Brown et al., 2020) concate-\nnate a few input and output pairs and feed them\nto the large-scale GPT-3 language model, which\nis an intuitive in-context learning paradigm, al-\nlowing the model to generate answers for addi-\ntional cases autoregressively. Recent works (Schick\nand Schütze, 2021a,b) show that small-scale pre-\ntrained language models such as BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019b) and AL-\nBERT (Lan et al., 2019) can also achieve decent\nperformance using prompt-tuning. Prompting has\nbeen applied to a large variety of tasks such as Text\nClassification (Schick and Schütze, 2021a), Natural\nLanguage Understanding (Xu et al., 2022), Knowl-\nedge Probing (Petroni et al., 2019), and Relation\nExtraction (Han et al., 2021). Typically, a piece of\nprompt contains a template and a verbalizer. The\nlanguage model predicts a probability distribution\nover vocabulary given the template and the verbal-\nizer transforms it into a prediction over class labels.\nIn this work, we focus on designing the verbalizers\nautomatically.\nVerbalizer Design The verbalizer plays a cru-\ncial role in prompting as it connects model outputs\nand labels, significantly influencing performance.\n(Schick and Schütze, 2021a) design human writ-\nten verbalizers for prompting, however, they are\nhighly biased towards personal vocabulary with in-\nadequate coverage. Apart from manually designed\nverbalizers, some recent studies explore automatic\nverbalizer construction. Auto-L (Gao et al., 2021)\nuses re-ranking to find the label words set by fine-\ntuning the model on the candidates searched by\nRoBERTa; AutoPrompt (Shin et al., 2020) applies\ngradient-based search to create both prompts and\nlabel words automatically with a few trigger ex-\namples. But these approaches need to update pa-\nrameters with gradient descent, which turns out to\nbe infeasible without access to the model weights\n(e.g., GPT-3). KPT (Han et al., 2021) incorporates\nexternal knowledge into the verbalizer in which the\nunlabeled dataset is needed to refine the label words\nand thus is not applicable to scenarios where only\nlabel names are known. In contrast, our approach\nNPPrompt directly finds, without any gradient up-\ndate, relevant words to label names with PLM’s\ninitial word embedding only.\nZero-shot Text Classification General zero-shot\ntext classification typically focuses on classifying\ntexts into categories that were not seen during the\ntraining process. Transferring knowledge from\nseen classes to unseen ones requires accurate and\ndiscriminative descriptions of all classes (Liu et al.,\n2019a; Xia et al., 2018) or joint embeddings of\ncategories and documents (Nam et al., 2016). How-\never, these methods rely on supervised data for\nthe known label set, making them unsuitable for\nscenarios where no labeled pairs for any category\nare available. SimPTC (Fei et al., 2022) improves\nzero-shot classification by clustering input texts\nand employing class-related prompts. LOTClass\n(Meng et al., 2020) proposes a model that utilizes\nlabel names with self-training for zero-shot clas-\nsification. Nonetheless, both SimPTC and LOT-\nClass still require an unlabeled corpus or knowl-\nedge base to extract topic-related words and per-\nform self-training. In contrast, NPPrompt achieves\ncomparable or even superior performance without\n15591\nthe need for any unlabeled dataset or knowledge\nbase.\n3 Background: Prompt-based Tuning for\nPLMs\nWe first provide standard paradigms, prompt-based\ntuning, that perform well in few-shot scenarios,\nbefore introducing our approach for the zero-shot\ncase. Take N way text classification as an example.\nWe aim to predict the labely ∈Y for each sentence,\nwhere Yis the label set with N distinct classes.\nPrompt-based tuning tunes PLM using cus-\ntomized prompts (Brown et al., 2020). The reg-\nular prompt-based tuning converts a specific task\nto a cloze-style mask language modeling problem.\nFor each input example x (single sentence or sen-\ntence pair), we first apply a task template T on\nit, converting original input x to xprompt. For\ninstance, we concatenate the template “ T(·) =\nThis topic is about [MASK]\" with the original input\n“The Warriors won the NBA championship 2022\"\nand wrap it into:\nxprompt = T(x) = x. This topic is about [MASK]\nThe verbalizer f in vanilla prompt engineering\nmaps a set of selected words Vfrom the vocabu-\nlary to the original label space Y, i.e., f : V→Y .\nInversely, we use M(yj) to denote the label words\nin V that are mapped into a specific label yj,\n∪yj∈YM(yj) = V. Then we calculate the proba-\nbility of label yj:\nP(yj |x) = g (P([MASK] = vi |xprompt) |vi ∈M(yj)) ,\nwhere g(·) is for aggregating the probability of\nlabel words into the probability of the label. Then\nPLMs can be fine-tuned by minimizing the cross-\nentropy loss with supervised examples.\n4 Proposed Method: NPPrompt\nWe inherit PLM with verbalizers framework but\nkeep PLM’s parameters frozen (Gao et al., 2021).\nThe key idea of NPPrompt is using PLM’s word\nembeddings to automatically construct verbalizers\n– mapping from words to labels – in a fully zero-\nshot way. It does not need any additional raw text\ncorpus for fine-tuning. NPPrompt consists of two\nsteps to compute predictions for any labels in a\nnonparametric form (Figure 1). 1) We search for\nall label words closely related to each class yj in\nPLM’s token embedding manifold. 2) Then we\nuse the PLM to predict values for [MASK], filter\nthem using each class’s set of label words, and\naggregate the properly weighed outputs to produce\nthe final prediction. In the following, we describe\nNPPrompt for text classification but it generalizes\nto other language understanding tasks.\nk-Nearest-Neighbor Verbalizer Construction\nFor each class label (e.g. “SPORTS”), we search\nover the whole vocabulary Vfor the top-k words\nnearest to the label name in the PLM’s embedding\nspace. Here, the distance between words and label\nnames is measured using the cosine similarity score.\nOther distance metrics work as well and are exam-\nined in Section 5. We denotek as the neighborhood\nnumber. Assuming the embeddings of word vi and\nlabel name yj are emb(vi) and emb(yj) respec-\ntively, the label words of the verbalizer for yj are\nselected by top-k ranking:\nM(yj) = Top-k\nvi∈V\n{S(emb(vi), emb(yj))}, (1)\nwhere S(·) is the cosine similarity function:\nS (emb(vi), emb(yj)) = emb(vi)\n∥emb(vi)∥ · emb(yj)\n∥emb(yj)∥.\nSince the PLM is already pre-trained on raw text\ncorpus, it acquires sensible semantic knowledge\nand relatedness of words in the vocabulary. We\nuse PLM’s embedding to search for label words\nsemantically relevant to given label names. For\nillustration, we show the found label words of two\ncategories in the AG News dataset (Zhang et al.,\n2015) and the corresponding similarity scores in\nTable 1. We also extend our verbalizer to support\nlabel names with longer expressions in Appendix\nA.2.\nWord Sim Word Sim\n“ sports\" 1.00 “ business\" 1.00\n“ Sports\" 0.77 “ Business\" 0.78\n“ sport\" 0.75 “ businesses\" 0.74\n“ sporting\" 0.68 “business\" 0.72\n“ athletics\" 0.65 “Business\" 0.67\n“sports\" 0.65 “ businessman\" 0.59\n“Sports\" 0.65 “ corporate\" 0.58\n“ Sport\" 0.62 “ company\" 0.56\n“ athletic\" 0.61 “ enterprise\" 0.55\n“ athletes\" 0.61 “ businessmen\" 0.55\nTable 1: The top 10 similar words of the RoBERTa-large\nmodel for the AG News dataset categories SPORTS and\nBUSINESS . Sim: cosine similarity scores.\nNonparametric Aggregation of Prompted Pre-\ndictions For each input text x, we construct a\nprompt-augmented sequence xprompt = T(x) with\na [MASK] token. We use the PLM to predict tokens\n15592\nis [MASK]aboutTemplate\nPLMHead\nNBAbasketballsports…physicsathleticsfootballtechnology\nThe Warriors won the NBA championship 2022.Original input\nsports, Sports, sport, sporting, athletic, athletics, SPORTS, football, soccer, basketball, tennis …, NBA, …\nSCIENCESPORTSPOLITICS…\nVerbalizer\nInitial word embeddingClosest Words to “SPORTS” \nLogits over Vocabulary\nPredict\nAggregation\nSPORTS\nThistopic\nFigure 1: The illustration of NPPrompt. We generate the label words by searching the related words from the initial\nword embedding of the pre-trained language model. By aggregating logits from the label words, we predict the\ncategory with the largest score (SPORTS ).\nfor [MASK]. In contrast to previous prompting meth-\nods which directly calculate the probability over the\nsurface labels, we use the nearest label words from\nabove to compute the probability for each output\nlabel. Only the words in a label’s top-k neighbor-\nhood will contribute to the class prediction. The\ncontribution from each label word is non-equal.\nTo be specific, with T(x), a PLM produces the\nlogit vector Θ[MASK] for all possible words at the\n[MASK] token. Notice that if the whole vocabulary\nis V, Θ[MASK] ∈R|V|. Then we compute the class\nprobability for a label yj by aggregating the logits\nfiltered by the verbalizer’s label words. We use\nkernel smoothing to aggregate as follows:\nQ(yj|x)=\n∑\nvi∈ M(yj)\nw(vi, yj)·Θ([MASK]=vi|xprompt =T(x))\n(2)\nWhere the weight between label word vi and class\nname yj is defined as:\nw(vi, yj) = exp (S(emb(vi), emb(yj)))∑\nvt∈M(yj) exp (S(emb(vt), emb(yj)))\n(3)\nFinally, the best class prediction is selected from\nthe maximum of all labels:\n˜y = argmax\nyj\nQ (yj |x) .\nNotice since we use kernel smoothing on logits\ninstead of probability,Q is also unnormalized prob-\nability.\nFor example, AG News has two classes y1 =\n{SCIENCE }, y2 = {SPORTS }. From Table 1,\nthe verbalizer for SPORTS M(y1) includes label\nwords “sports”, “athletics”, etc, and the verbal-\nizer for BUSINESS M(y2) includes label words\n“business”, “corporate”, etc. Given an input text x\n“The Warriors won the NBA championship 2022”,\nthe prompt-augmented sequence xprompt will be\n“The Warriors won the NBA championship 2022.\nThis topic is about [MASK]”. The PLM computes\nlogits for every word Θ([MASK] = v|xprompt).\nNPPrompt computes the unnormalized probabil-\nities for SPORTS and BUSINESS :\nIf the aggregated prediction Q for SPORTS is\nlarger than BUSINESS , NPPrompt outputs SPORTS .\nThere are certain conditions where one class\nhas label names containing little semantic mean-\ning or where several keywords are needed to de-\nfine a label. For instance, in the DBPedia dataset\n(Lehmann et al., 2015), one class is related to\nNATURAL PLACE , then we can use the keywords\n{“river”, “lake”, “mountain”} to represent this class.\nIn this setting, we pick out the keyword with the\nmaximum score calculated by Equation 2 to rep-\nresent each label first. Then we choose the label\nwith the largest score. We use Φ(yj) to denote all\n15593\nkeywords in class yj, and the final prediction is :\n˜y = arg max\nyj\n(\narg max\ny′∈Φ(yj)\nQ\n(\ny′ |x\n))\n. (4)\n5 Experiment\nWe conduct extensive zero-shot learning exper-\niments to demonstrate the effectiveness of our\nmethod. We provide detailed information on our\nimplementation and address several research ques-\ntions related to NPPrompt.\n5.1 Datasets, Prompt Templates, and\nExperimental Setup\nDataset Classification Type # Classes # Test\nAG News News Topic 4 7,600\nDBPedia Wikipedia Topic 14 70,000\nIMDB Movie Review Sentiment 2 25,000\nAmazon Product Review Sentiment 2 400,000\nTable 2: Dataset statistics.\nWe adopt sentiment classification tasks on two\ndatasets, IMDB (Maas et al., 2011) and Amazon\n(McAuley and Leskovec, 2013), and topic classi-\nfication tasks on another two datasets, AG News\n(Zhang et al., 2015) and DBPedia (Lehmann et al.,\n2015). All datasets are in the English language.\nFor each task, we directly use the test set to as-\nsess model performances, without incorporating\nvalidation or training sets for post-tuning or cherry-\npicking hand-crafted prompts. The statistics of\neach dataset are shown in Table 2.\nTo concentrate on the verbalizer and reduce the\ninfluence of templates, we adopt multiple fixed\nmanual templates following (Hu et al., 2022). We\nreport the best template used for the RoBERTa-\nlarge model in Table 3.\nDataset Template\nAG News A [MASK] news : x .\nDBPedia x1 x2 In this sentence, x1 is a [MASK] .\nIMDB x All in all, it was [MASK] .\nAmazon x All in all, it was [MASK] .\nTable 3: Prompt templates for NPPrompt.\nWe implement our experiments based on an\nopen-source toolkit OpenPrompt (Ding et al.,\n2021), which aims to conduct prompt learning eas-\nily. We choose RoBERTa-large (Liu et al., 2019b)\nas our pre-trained language model. We report the\nbest accuracy of classification results for all ex-\nperiments using different neighborhood numbers.\nSince we directly use the pre-trained models for\ntesting, there is no randomness (random seed) in\nthis process. All experiments are conducted on\nNvidia A6000 GPUs and more details can be found\nin Appendix A.1.\n5.2 Baselines\nWe evaluate the following baseline methods.\nSemantic Retrieval We utilize sentence embed-\nding models (Reimers and Gurevych, 2019) to ob-\ntain the embedding for each sentence and descrip-\ntions for each class. Then we calculate the cosine\nsimilarity between sentences and label descriptions.\nWe assign the most similar class labels to the sen-\ntence. Particularly, we use all-mpnet-base-v2\nfrom Hugging Face as the sentence embedding\nmodel, and the descriptions for each class can be\nfound in Appendix A.1.\nNSP-BERT (Sun et al., 2021) propose text en-\ntailment tasks to replace text classification tasks\nand then use the Next Sentence Prediction (NSP)\nhead to predict the results. We show the template\nwe use in Appendix A.1.\nManualVerb Manual verbalizers are defined by\nhuman experts with domain knowledge and we sim-\nply use the label words provided by OpenPrompt\n(Ding et al., 2021).\nLOTClass (Meng et al., 2020) employ pre-\ntrained neural language models with unlabeled data\nfor category understanding, i.e., finding words sim-\nilar to label names. They then introduce a self-\ntraining approach to the entire unlabeled corpus to\ngeneralize the model.\nGPT-3 with descriptions Following (Brown\net al., 2020), we manually write the descriptions for\neach class and query GPT-3 where the predicted\ntoken serves as the prediction. We show the de-\nscriptions in Appendix A.1.\nChatGPT with descriptions In the case of Chat-\nGPT (OpenAI, 2022), we employ the same descrip-\ntions as those used for GPT-3. We query the Chat-\nGPT model using these descriptions, and the pre-\ndicted token is considered as the corresponding\nprediction. Our experimentation is based on the\nMarch 2023 version of ChatGPT.\n15594\nMethod Human/KB Unlabeled AG News DBPedia IMDB Amazon Avg.\nManualVerb /enc-34 /enc-3779.60.6 71.71.1 92.00.7 87.30.4 82.7\nSemantic Retrieval /enc-34 /enc-3773.11.2 78.60.8 64.81.3 59.40.7 69.0\nNSP-BERT /enc-34 /enc-3777.40.6 64.75.3 72.81.1 72.73.9 71.9\nGPT-3 w. descriptions /enc-34 /enc-3783.4 82.5 88.8 89.4 86.0\nChatGPT w. descriptions /enc-34 /enc-3783.8 92.0 92.7 95.8 91.1\nSimPTC /enc-34 /enc-3786.90.3 93.21.0 91.00.0 93.90.0 91.3\nLOTClass w/o. self train /enc-37 /enc-3482.2 86.0 80.2 85.3 83.4\nLOTClass /enc-37 /enc-3486.4 91.1 86.5 91.6 88.9\nKPT /enc-34 /enc-3486.7 87.4 94.0 94.6 90.7\nNull Prompt /enc-37 /enc-3767.92.0 56.83.9 82.51.5 89.41.0 74.2\nMulti-Null Prompt /enc-37 /enc-3768.21.8 67.61.8 86.60.6 86.22.7 77.2\nNPPrompt /enc-37 /enc-3785.20.5 86.80.1 94.20.2 93.90.0 90.0\nTable 4: Classification performance on four datasets with average results and standard error. Human: with human\nefforts to write deceptions or design label words. KB: with external knowledge base; Unlabeled: with unlabeled\ncorpus. Notice that our method achieves the best performance in a fully zero-shot setting, with an absolute\nimprovement of 12.8%. Surprisingly, it even approaches the best result with human effort/knowledge base and extra\nraw data.\nMNLI MNLI-mm SST-2 QNLI RTE MRPC QQP CoLA Avg.(acc) (acc) (acc) (acc) (acc) (F1) (F1) (Matt.)\nWith human designed prompts / few-shot data\nManual Label 50.8 51.7 83.6 50.8 51.3 61.9 49.7 2.0 50.2\nIn-context learning52.00.7 53.40.6 84.81.3 53.80.4 60.41.4 45.76.0 36.15.2 −1.52.4 48.1\nAuto-L 41.65.4 42.36.2 84.33.3 57.93.9 61.97.5 67.77.9 55.55.0 1.24.8 51.6\nAMuLaP 50.82.1 52.31.8 86.91.6 53.12.8 58.97.9 56.35.0 60.22.7 2.31.4 52.6\nFew-shot fine-tuning45.86.4 47.86.8 81.43.8 60.26.5 54.43.9 76.62.5 60.74.3 33.914.3 57.6\nFully zero-shot\nMajority 32.7 33.0 50.9 49.5 52.7 81.2 0.0 0.0 37.5\nNull Prompt 33.10.4 33.80.5 79.14.0 50.70.1 47.20.6 12.97.0 1.31.0 −1.12.0 32.1\nMulti-Null Prompt38.03.5 38.54.1 70.27.7 52.21.7 53.02.2 19.98.7 25.513.4 6.22.0 37.9\nNPPrompt 45.70.6 45.90.5 86.31.2 57.60.7 55.03.4 79.81.6 52.40.4 4.94.1 53.5\nTable 5: The performance of NPPrompt with RoBERTa-large on GLUE benchmark against other methods, including\nfew-shot learning methods. Manual Label: using the human-designed prompts in (Gao et al., 2021); In-context\nlearning: using the in-context learning proposed in (Brown et al., 2020) with RoBERTa-large; Auto-L: method in\n(Gao et al., 2021); AMuLaP: method in (Wang et al., 2022); Majority: majority class.\nSimPTC Fei et al. (2022) show that zero-shot\ntext classification can be improved by leverag-\ning text clustering in the embedding spaces of\npre-trained language models. SimPTC utilizes a\nBayesian Gaussian Mixture Model to fit unlabeled\ntexts. The initialization of cluster positions and\nshapes is performed using class names.\nKPT (Hu et al., 2022) propose knowledgeable\nprompt-tuning, which expands the label words\nspace using external knowledge bases (KB). KPT\nalso refines the expanded label words based on the\nunlabeled data. We show the best results of KPT in\nthe zero-shot setting.\nNull Prompt (IV et al., 2022) insert a token at\nthe end of the text (i.e. using the prompt template\n“ [x][MASK]\" ) and then use the prediction of the\n[MASK] token to perform zero-shot classification.\nMulti-Null prompting (Wang et al., 2021) find\nthat simply introducing a few prompt [MASK]s can\nimprove the performance and robustness of the\nNull Prompt in the zero-shot settings.\n5.3 Main Results\nWe demonstrate our experimental results in Table\n4. Overall NPPrompt outperforms Null Prompt and\nMulti-Null Prompt remarkably by over 10 percent\nin a fully zero-shot setting. NPPrompt achieves an\naccuracy of over 85% on AG News and DBPedia\nand over 90% on IMDB and Amazon. We con-\njecture that topic classifications in AG News and\n15595\nDBPedia are more complicated than binary senti-\nment classifications in IMDB and Amazon, hence\nthe higher accuracy on the latter.\nNPPrompt is only slightly worse than KPT and\nSimPTC but outperforms most baseline methods\nin which human efforts/external knowledge or un-\nlabeled data are strictly required. It’s worth not-\ning that NPPrompt performs much better than\nManualVerb, suggesting that the label words gen-\nerated by our method are more comprehensive\nand unbiased than human-designed ones. Besides,\nNPPrompt can beat GPT-3 by 4% in terms of aver-\nage accuracy, a strong sign of the great potential for\nRoBERTa-large with 355M parameters compared\nto 175B parameters giant GPT-3.\nTo explore how our method NPPrompt performs\non different kinds of tasks, we also conduct experi-\nments on the GLUE benchmark (Wang et al., 2018).\nSpecifically, we test on Multi-Genre Natural Lan-\nguage Inference Matched (MNLI), Multi-Genre\nNatural Language Inference Mismatched (MNLI-\nmm)(Williams et al., 2018) , Question Natural Lan-\nguage Inference (QNLI) (Rajpurkar et al., 2016)\nand Recognizing Textual Entailment (RTE) (Ben-\ntivogli et al., 2009) for Natural Language Inference\n(NLI); Microsoft Research Paraphrase Matching\n(MRPC) (Dolan and Brockett, 2005) and Quora\nQuestion Pairs (QQP) (Chen et al., 2018) for Para-\nphrase Similarity Matching; Stanford Sentiment\nTreebank (SST-2) (Socher et al., 2013) for Sen-\ntiment Classification; The Corpus of Linguistic\nAcceptability (CoLA) (Warstadt et al., 2019) for\nLinguistic Acceptability.\nAs shown in Table 5, NPPrompt outperforms\nall other methods in fully zero-shot setting. Auto-\nL (Gao et al., 2021) and AMuLaP (Wang et al.,\n2022) are both automatic label words searching\nmethods utilizing few-shot examples. Our method\nNPPrompt can even outperform them without any\nunlabeled data or few-shot training examples.\n5.4 Effects of similarity functions in\nnonparametric aggregation\nBoth weight and similarity functions play a critical\nrole in the design of NPPrompt and we test how\nNPPrompt performs on AG News with different\nconfigurations. The “Default\" setting is as stated\nin Equation 1 and 3. We fix the similarity function\nS (emb(vi), emb(yj)) = emb(vi)\n∥emb(vi)∥ · emb(yj)\n∥emb(yj)∥,\nset w(vi, yj) = 1 for the “Same weight\" setting and\nw(vi, yj) = S(emb(vi),emb(yj))∑\nvk∈M(yj) S(emb(vk),emb(yj)) for the\n6 7 8 9 10 11 12\nNeighborhood Number: k\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84T est Accuracy\nDefault\nSame weight\nAverage weight\nEuclidean distance\nDot product\nFigure 2: Effects of different aggregation.\n“Average weight\" setting. Besides cosine similar-\nity, the Euclidean distance and the dot product are\nalso common similarity measures for embeddings.\nConsequently, we fix the weight w(vi, yj) = 1 ,\nchoose S (emb(vi), emb(yj)) = −∥emb(vi) −\nemb(yj)∥for the “Euclidean distance\" setting and\nS (emb(vi), emb(yj)) = emb(vi)·emb(yj) for\nthe “Dot product\" setting. It can be informed from\nFigure 2 that with a fixed similarity function, dif-\nferent weight calculations yield comparable results,\nbut with a fixed weight, cosine similarity is the\noptimal similarity measure.\n0 20 40 60 80 100\nNeighborhood Number: k\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84T est Accuracy\nsum logit\nsum prob\nFigure 3: Test results on AG News.\n5.5 Can we sum over probabilities?\nNPPrompt sums up all logits for a label word set as\nshown in Equation 2. Another possible approach is\nto sum up the probabilities from PLM’s prediction\nfor the label words and choose the argmax for all\ndifferent labels as the prediction: P(yj|xprompt) =∑\nvi∈M(yj) w(vi, yj) ·P([MASK] = vi|xprompt),\n˜y = arg max\nyj\nP (yj |xprompt). We conduct ex-\nperiments on AG News to compare the above two\napproaches, one that sums up logits (“sum logit\")\nand one that sums up probabilities (“sum prob\").\nFigure 3 presents the results and we find that “sum\nlogit\" performs better at small k but “sum prob\"\n15596\n8 9 10 11 12 13 14 15 16\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850T est Accuracy\nAG News\nRoBERT a-Large\nRoBERT a-Base\nBERT-Large\nBERT-Base\n3 4 5 6 7 8 9 10 11\n0.65\n0.70\n0.75\n0.80\n0.85T est Accuracy\nDBPedia\nRoBERT a-Large\nRoBERT a-Base\nBERT-Large\nBERT-Base\n100 200 300 400 500 600 700 800 900\nNeighborhood Number: k\n0.6\n0.7\n0.8\n0.9T est Accuracy\nIMDB\nRoBERT a-Large\nRoBERT a-Base\nBERT-Large\nBERT-Base\n130 140 150 160 170 180 190 200 210\nNeighborhood Number: k\n0.6\n0.7\n0.8\n0.9T est Accuracy\nAmazon\nRoBERT a-Large\nRoBERT a-Base\nBERT-Large\nBERT-Base\nFigure 4: Test results of NPPrompt for four PLMs with different neighborhood numbers.\ndelivers better results when k exceeds 30. “sum\nlogit\" achieves the best result at k = 12 among all\nexperiments.\n5.6 How many label words should we choose?\nThe number of label words impacts the perfor-\nmance of our method NPPrompt as well. In Fig-\nure 4, we display the performances of different\nmodels with varied neighborhood numbers. In gen-\neral, NPPrompt attains similar test accuracy across\ndifferent neighborhood numbers. Regardless of\nthe choice for neighborhood number, NPPrompt-\nRoBERTa-large achieves over 80% accuracy in\ntopic classification tasks on AG News and DBPe-\ndia, and it gains over 90% accuracy in sentiment\nclassification tasks on IMDB and Amazon. In real-\nworld applications, we can simply choose a fixed\nneighborhood number (e.g. 8-10) to achieve decent\nperformance.\n5.7 How does NPPrompt perform with\ndifferent PLMs?\nMethod AG DB IM AZ Avg.\nNPPrompt-T5-base 76.8 78.3 68.5 65.3 72.2\nNPPrompt-GPT2-base 81.1 78.1 83.7 85.6 82.1\nNPPrompt-BERT-base 79.4 77.8 57.7 53.5 67.1\nNPPrompt-BERT-large 82.7 80.9 81.6 80.8 81.5\nNPPrompt-RoBERTa-base 75.3 82.8 88.7 83.9 82.7\nNPPrompt-RoBERTa-large 85.0 86.8 94.1 93.9 90.0\nTable 6: The zero-shot results of different backbones.\nNPPrompt-RoBERTa-large performs the best in all\ndatasets. AG: AG News; DB: DBPeida; IM: IMDB;\nAZ: Amazon.\nThe performance of NPPrompt heavily relies on\nthe choice of the pre-trained language model. This\nis due to the variations in label words for differ-\nent categories, which stem from the distinct initial\nword embeddings and vocabularies employed by\neach PLM. Additionally, NPPrompt can be adapted\nfor text generation models such as T5 (Raffel et al.,\n2020) and GPT-2 (Radford et al., 2019)) with mi-\nnor modifications. In our approach, we utilize T5-\nbase/GPT2-base to generate the missing spans at\nthe end of the prompt text. The first predicted token\nserves as the input to the verbalizer, and we follow\nthe nonparametric aggregation steps outlined in\nAppendix A.1 to determine the category.\nTo investigate the impact of employing different\nPLMs, we conduct additional experiments using\nBERT-base-cased, BERT-large-cased, RoBERTa-\nbase, T5-base, and GPT2-base models. The re-\nsults are presented in Table 6. Notably, NPPrompt\nwith RoBERTa-large achieves the highest perfor-\nmance, which can be attributed to the model’s ex-\ntensive parameter count and the fact that it is pre-\ntrained on a large corpus. As anticipated, larger\nmodels such as RoBERTa-large and BERT-large\noutperform their base counterparts (RoBERTa-base\nand BERT-base) on average, with RoBERTa con-\nsistently exhibiting superior accuracy compared\nto BERT models. While NPPrompt-T5-base and\nNPPrompt-GPT2-base demonstrate commendable\nperformance, they do not surpass the performance\nof NPPrompt-RoBERTa-large.\n5.8 Is NPPrompt limited to text classification\ntasks\nOur research extends beyond text classification and\nencompasses experiments on multiple-choice ques-\ntion answering (QA) tasks as well. Specifically,\n15597\nMethod CQA Dev Set Accuracy\nFew-shot Direct GPT-J 20.9\nFew-shot CoT GPT-J 36.6\nFew-shot CoT LaMDA 137B 55.6\nNPPrompt-RoBERTa-large 34.2\nTable 7: Test results on CommonsenseQA dataset. Di-\nrect: directly output the final answer; CoT: prompted\nwith chain-of-thought (CoT) rationales; LaMDA:\nmethod in (Wei et al., 2022).\nwe assess the performance of NPPrompt using the\nwidely-utilized CommonsenseQA (CQA) dataset\n(Talmor et al., 2019). In this new setting, we use the\nprompt template “x The answer is [MASK].”, e.g.\n“What do animals do when an enemy is approach-\ning? The answer is [MASK].”. Subsequently, we\nsearch for the k-nearest neighbors for each target\nanswer, setting k as 15. The prediction is obtained\nby applying the same process employed for text\nclassification tasks. The results of our experiments\nare presented in Table 7 (few-shot results obtained\nfrom (Zelikman et al., 2022)). Notably, NPPrompt\nnot only achieves satisfactory performance on the\nCommonsenseQA dataset but even outperforms\nfew-shot GPT-J (Wang, 2021) as well. This demon-\nstrates the versatility and flexibility of NPPrompt\nacross various NLP scenarios.\n6 Discussion\nOur proposed method, NPPrompt, demonstrates\nexceptional performance in zero-shot text classifi-\ncation tasks. We attribute this success to two key\nfactors. Firstly, by utilizing the initial word em-\nbedding from pre-trained language models (PLMs),\nwe are able to identify cognates of the label words.\nFor instance, in Table 1, we observe variations of\nthe word \"business\" such as \"Business\" and \"busi-\nnesses\" for the BUSINESS category. Secondly, we\neffectively leverage the capabilities of pre-trained\nlanguage models by reformulating the zero-shot\nclassification problem as a masked token prediction\ntask, which aligns with the pre-training process.\nFurthermore, NPPrompt offers a promising so-\nlution for dynamic and open zero-shot classifica-\ntion problems, where new classes may arise or old\nclasses may be removed. With the use of efficient\nPLMs and category names, as well as the key word\ndesign in Equation 4, NPPrompt can also be ap-\nplied in scenarios where label names do not possess\nsemantic meaning (e.g. categories with label names\n“A”, “B”, “C”). This technique has the potential for\nwide deployment in real-world applications.\n7 Conclusion\nIn this paper, we propose NPPrompt, a novel and\neffective method for fully zero-shot learning with\npre-trained language models. We use initial word\nembedding of PLM to automatically find related\nwords for category names, which enables us to\nconstruct the verbalizers without manual design\nor unlabeled corpus. Experimental results show\nthat NPPrompt outperforms the previous zero-shot\nmethods by large margins.\nLimitations\nFor those label names without semantic meanings,\nseveral keywords are still required for NPPrompt to\nwork well. Furthermore, this study focuses exclu-\nsively on the zero-shot setting. However, there are\npotential avenues for exploration in the few-shot\nscenario, which is prevalent in practical applica-\ntions. The applicability of NPPrompt to other tasks,\nsuch as ranking and relation extraction, remains\nuncertain and warrants further investigation. De-\nsigning a refinement method to jointly search for\nlabel words and templates can be a promising di-\nrection for future research.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The sixth pascal recognizing\ntextual entailment challenge. In TAC.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS.\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\nXia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\nmarco: A human generated machine reading compre-\nhension dataset. ArXiv, abs/1611.09268.\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. Quora\nquestion pairs.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\n15598\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Hai-Tao Zheng, and Maosong Sun.\n2021. Openprompt: An open-source framework for\nprompt-learning. arXiv preprint arXiv:2111.01998.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn IJCNLP.\nYu Fei, Ping Nie, Zhao Meng, Roger Wattenhofer,\nand Mrinmaya Sachan. 2022. Beyond prompting:\nMaking pre-trained language models better zero-shot\nlearners by clustering representations. In EMNLP.\nJoshua Feldman, Joe Davison, and Alexander M. Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In EMNLP.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In ACL.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. Ptr: Prompt tuning with rules\nfor text classification. ArXiv, abs/2105.11259.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Juan-Zi Li, and Maosong Sun. 2022. Knowl-\nedgeable prompt-tuning: Incorporating knowledge\ninto prompt verbalizer for text classification. In ACL.\nRobert L Logan IV , Ivana Balavzevi’c, Eric Wallace,\nFabio Petroni, Sameer Singh, and Sebastian Riedel.\n2022. Cutting down on prompts and parameters:\nSimple few-shot learning with language models. In\nFINDINGS.\nKamran Kowsari, K. Meimandi, Mojtaba Heidarysafa,\nSanjana Mendu, Laura E. Barnes, and Donald E.\nBrown. 2019. Text classification algorithms: A sur-\nvey. Information, 10:150.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In ICLR.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N. Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick van Kleef,\nS. Auer, and Christian Bizer. 2015. Dbpedia - a\nlarge-scale, multilingual knowledge base extracted\nfrom wikipedia. Semantic Web, 6:167–195.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. In ACL.\nHan Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai\nLi, Xiao-Ming Wu, and Albert Y . S. Lam. 2019a.\nReconstructing capsule networks for zero-shot intent\nclassification. In EMNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan\nHuang, A. Ng, and Christopher Potts. 2011. Learning\nword vectors for sentiment analysis. In ACL.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: understanding rating dimen-\nsions with review text. In RecSys.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\nclassification using label names only: A language\nmodel self-training approach. In EMNLP.\nJinseok Nam, Eneldo Loza Mencía, and Johannes\nFürnkranz. 2016. All-in text: Learning document,\nlabel, and word representations jointly. In AAAI.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. OpenAI blog.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In EMNLP.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn EMNLP.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In EACL.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In NAACL.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP.\n15599\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, A. Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn EMNLP.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for better\nsemantics and faster retrieval.\nYi Sun, Yu Zheng, Chao Hao, and Hangping Qiu. 2021.\nNsp-bert: A prompt-based zero-shot learner through\nan original pre-training task-next sentence prediction.\nArXiv, abs/2109.03564.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In NAACL.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP@EMNLP.\nBen Wang. 2021. Mesh-Transformer-JAX: Model-\nParallel Implementation of Transformer Lan-\nguage Model with JAX. https://github.com/\nkingoflolz/mesh-transformer-jax.\nHan Wang, Canwen Xu, and Julian McAuley. 2022.\nAutomatic multi-label prompting: Simple and inter-\npretable few-shot classification. In NAACL.\nYue Wang, Lijun Wu, Xiaobo Liang, Juntao Li, and Min\nZhang. 2021. Are bert families zero-shot learners? a\nstudy on their potential and limitations. OpenReview,\nhttps://openreview.net/pdf?id=YLglAn-USkf.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In NeurIPS.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL.\nCongying Xia, Chenwei Zhang, Xiaohui Yan, Yi Chang,\nand Philip S. Yu. 2018. Zero-shot user intent detec-\ntion via capsule neural networks. In EMNLP.\nJingjing Xu, Qingxiu Dong, Hongyi Liu, and Lei\nLi. 2022. Go-tuning: Improving zero-shot learn-\ning abilities of smaller language models. ArXiv,\nabs/2212.10461.\nE. Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022.\nStar: Bootstrapping reasoning with reasoning. ArXiv,\nabs/2203.14465.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\n15600\nA Appendix\nA.1 Experimental Details\nTable 8 shows all the manual templates of\nNSP-BERT. We show the prompt templates for\nNPPrompt-T5 in Table 9. Table 11 summarizes\nmanual designed descriptions of each dataset for\nSemantic Retrieval. As for GPT-3, we query the\nOpenAI API1 and test with Davinci-001 model.\nThe prompts for GPT-3 are shown in Table 12. We\nlist all templates and label names for NPPrompt\nof all experiments in Table 13. We also list the\nrelated words result in sentiment classification\n(GOOD /BAD) and NLI (YES/NO)) tasks in Table\n14.\nDataset Template\nAG News News: label name.\nDBPedia News: label name.\nIMDB This text shows label name sentiment.\nAmazon The attitude of this text is label name.\nTable 8: Prompt templates of NSP-BERT (Sun et al.,\n2021) in Table 4.\nDataset Template\nAG News x In this sentence, the topic is about [MASK]\nDBPedia x1 x2 In this sentence, x1 is a [MASK]\nIMDB x In summary, the movie was [MASK]\nAmazon x All in all, it was [MASK]\nTable 9: Prompt template of NPPrompt with T5-base\n(k = 15) in Tabel 6.\nA.2 What label words do different PLMs\nchoose?\nRoBERTa-large RoBERTa-base BERT-large BERT-base\nWord Sim Word Sim Word Sim Word Sim\n“ school\" 1.00 “ school\" 1.00 “school\" 1.00 “school\" 1.00\n“ School\" 0.80 “ School\" 0.75 “School\" 0.69 “School\" 0.70\n“ schools\" 0.77 “ schools\" 0.71 “schools\" 0.63 “schools\" 0.63\n“school\" 0.74 “school\" 0.70 “college\" 0.55 “college\" 0.54\n“ SCHOOL\" 0.69 “School\" 0.70 “university\" 0.50 “university\" 0.51\n“School\" 0.68 “ SCHOOL\" 0.56 “student\" 0.42 “College\" 0.40\n“ university\" 0.66 “ college\" 0.50 “church\" 0.41 “church\" 0.40\n“ college\" 0.65 “ university\" 0.50 “house\" 0.38 “student\" 0.37\n“ Schools\" 0.65 “ Schools\" 0.49 “education\" 0.38 “students\" 0.37\n“ schooling\" 0.64 “ schooling\" 0.45 “students\" 0.37 “Schools\" 0.37\n“ preschool\" 0.63 “ preschool\" 0.44 “class\" 0.37 “academy\" 0.37\n“ kindergarten\" 0.63 “ kindergarten\" 0.41 “town\" 0.37 “class\" 0.36\n“ classroom\" 0.60 “ student\" 0.41 “College\" 0.36 “education\" 0.36\n“ student\" 0.58 “ students\" 0.39 “Schools\" 0.36 “University\" 0.35\n“ education\" 0.58 “ classroom\" 0.38 “work\" 0.35 “house\" 0.35\nTable 10: The top 15 similar words ofSCHOOL category\nin the DBPedia dataset. Sim: similarity scores.\nWe summarize the label words of different PLMs\nfor SCHOOL category in DBPedia in Table 10.\nRoBERTa-large and RoBERTa-base share similar\nsets of label words yet with a minor discrepancy\n1https://openai.com/api/\nbetween their similarity scores. RoBERTa-large\nusually produces larger similarities than RoBERTa-\nbase. In contrast, the label words in RoBERTa are\nquite different from those in BERT.\nA.3 Extension to Multi-Word Expressions\nHere we extend our method to support\nmulti-word label names like NATURAL PLACE ,\nMEAN OFTRANSPORTATION and etc. The major\npart is to obtain related words to a multi-word label\nname. Once we obtain the related words, the rest\nnon-parametric aggregation step remains identical.\nWe consider two scenarios:\nThe label name is multi-word (i.e., phrase) and\nrelated words are still single-words To model\nthe phrase, we use average contextualized embed-\nding instead of word embedding for both label\nnames and related single-words to compute cosine\nsimilarity. As suggested in (Su et al., 2021), we\nwhiten the contextualized output of RoBERTa by\na linear transformation obtained from the contex-\ntualized embedding of all words in vocabulary. To\nobtain the best result, we select the output of layer\n6 of RoBERTa. This extension achieves 61% ac-\ncuracy on the DBPedia dataset using the original\nmulti-word label names (original label names can\nbe found at https://rdrr.io/cran/textdata/\nman/dataset_dbpedia.html).\nBoth the label name and related words are\nphrases Since the search space of a related\nphrase is exponentially large in its length, we use\nanother prompt to filter candidate words. The tem-\nplate we use is “[LABEL_NAME] can also be called\n[MASK]∗n.”, where n is the length of the candidate.\nFor example, if the label name is MEAN OFTRANS -\nPORTATION and n = 2 , the template will look\nlike “Mean of transportation can also be called\n[MASK] [MASK].”. We feed it to RoBERTa and fil-\nter top-k candidate phrases of masked prediction.\nSince masked prediction is conditionally indepen-\ndent of each mask, we further re-rank the top- k\ncandidate phrases by either the contextualized em-\nbedding method mentioned above or another auto-\nregressive LM. For the latter one, we evaluate the\nperplexity of the template with [MASK] filled by\ncandidate phrases. This generates 71% accuracy\non DBPedia if the length of the phrase is two and\nthe re-ranking is performed by GPT-2 (Radford\net al., 2019).\n15601\nDescriptions\nAG News:\nThe politics category is related to politics, government, and law.\nThe sports category is related to sports, competition, and athletics.\nThe business category is related to business, portfolio, economics, and money.\nThe technology category is related to technology, software, system, and science.\nDBPedia:\nThe company category is related to company, corporation, enterprise, brand, and business.\nThe school category is related to school, academy, university, and college.\nThe artist category is related to artist, art, painter, musician, singer, and creative.\nThe athlete category is related to athletes, sports, Olympic, and gym.\nThe politics category is related to politics, government, and law.\nThe transportation category is related to transportation, transport, vehicle, and traffic.\nThe building category is related to buildings, construction, and structure.\nThe mountain category is related to river, lake, bay, and mountain.\nThe village category is related to village, town, and rural.\nThe animal category is related to animal, wildlife, and nature.\nThe plant category is related to plant, shrub, tree, and forest.\nThe album category is related to album, lyrics, cd, and song.\nThe film category is related to film, movie, cinema, and video.\nThe book category is related to book, novel, and publication.\nIMDB:\nThe bad category is related to negative and bad reviews.\nThe good category is related to positive and good reviews.\nAmazon:\nThe bad category is related to negative and bad reviews.\nThe good category is related to positive and good reviews.\nTable 11: Descriptions for Semantic Retrieval in Table 4.\nPrompts for GPT-3 and ChatGPT\nAG News :\n[Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of\nthe four different categories. The categories are: “politics”, “sports”, “business”, and “technology”. Input: [x]. Output:\nDBPedia:\n[Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of\nthe fourteen different categories. The categories are: “company”, “school”, “artist”, “athlete”, “politics”, “transportation”,\n“building”, “mountain”, “village”, “animal”, “plant”, “album”, “film”, and “book”. Input: [x]. Output:\nIMDB:\n[Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of\nthe two categories. The categories are: “bad\" and “good\". Input: [x]. Output:\nAmazon:\n[Descriptions] Definition: In this task, you are given a sentence. Your job is to classify the following sentence into one of\nthe two categories. The categories are: “bad\" and “good\". Input: [x]. Output:\nTable 12: Prompts for GPT-3 and ChatGPT with descriptions [Descriptions] from Table 11 and input text [x].\n15602\nDataset Template Label Names k\nAG News A [MASK] news : x .\ncategory 1: world, politics\n12category 2: sports\ncategory 3: business\ncategory 4: technology, science\nDBPedia\ncategory 1: company\n7\ncategory 2: school\ncategory 3: artist\ncategory 4: sports\ncategory 5: politics, office\ncategory 6: transportation, car, bus, train\nx1 x2 In this sentence, x1 category 7: building, construct, room, tower\nis a [MASK] . category 8: river, lake, mountain\ncategory 9: village\ncategory 10: animal, pet\ncategory 11: plant\ncategory 12: album\ncategory 13: film\ncategory 14: book, publication\nIMDB x All in all, it was [MASK] . positive: good 500negative: bad\nAmazon x All in all, it was [MASK] . positive: good 170negative: bad\nSST-2 x1 It was [MASK] . positive: great 9negative: terrible\nMNLI x1 ? [MASK] , x2\nentailment: yes\n4 neutral:maybe\ncontradiction: no\nMNLI-mm x1 ? [MASK] , x2\nentailment: yes\n4 neutral:maybe\ncontradiction: no\nQNLI x1 ? [MASK] , x2\nentailment: Yes, Indeed, Overall 3not_entailment: No, Well, However\nRTE x1 ? [MASK] , x2\nentailment: Yes 10not_entailment: No\nMRPC x1 [MASK] , x2\nequivalent: Yes 9not_equivalent: No\nQQP x1 [MASK] , x2\nequivalent: Yes 9not_equivalent: No\nCoLA x1 This is [MASK] . grammatical: true 7not_grammatical: wrong\nTable 13: Templates and label names for NPPrompt. k refers to the best neighborhood number for RoBERTa-large.\n15603\nGOOD BAD YES NO\nWord Sim Word Sim Word Sim Word Sim\n“ good\" 1.00 “ bad\" 1.00 “ Yes\" 1.00 “ No\" 1.00\n“ Good\" 0.73 “ Bad\" 0.71 “ yes\" 0.79 “ no\" 0.80\n“ GOOD\" 0.72 “ terrible\" 0.69 “ YES\" 0.73 “No\" 0.74\n“good\" 0.69 “ BAD\" 0.69 “Yes\" 0.72 “ NO\" 0.70\n“ great\" 0.66 “ horrible\" 0.68 “ Yeah\" 0.72 “ Nope\" 0.62\n“ excellent\" 0.66 “bad\" 0.65 “ Yep\" 0.65 “ Yes\" 0.62\n“ decent\" 0.66 “ awful\" 0.64 “ Sure\" 0.62 “no\" 0.61\n“Good\" 0.65 “Bad\" 0.64 “ No\" 0.62 “ Nobody\" 0.59\n“ nice\" 0.64 “ good\" 0.63 “ Indeed\" 0.61 “ Nos\" 0.57\n“ bad\" 0.63 “ badly\" 0.62 “ yeah\" 0.60 “ The\" 0.57\n“ better\" 0.62 “ crappy\" 0.60 “yes\" 0.59 “ Yeah\" 0.57\n“ wonderful\" 0.58 “ lousy\" 0.60 “ Wow\" 0.59 “ Nothing\" 0.56\n“ best\" 0.58 “ worst\" 0.60 “ Absolutely\" 0.58 “ Not\" 0.56\n“ terrific\" 0.57 “ horrendous\" 0.60 “ Nope\" 0.58 “ Never\" 0.56\n“ fantastic\" 0.57 “ worse\" 0.59 “ Okay\" 0.57 “ None\" 0.55\n“ mediocre\" 0.57 “ nasty\" 0.59 “ Oh\" 0.57 “ Number\" 0.55\n“ lousy\" 0.57 “ shitty\" 0.59 “ Hello\" 0.57 “ So\" 0.54\n“ satisfactory\" 0.56 “ dreadful\" 0.59 “ Hey\" 0.57 “ Any\" 0.54\n“ marvelous\" 0.56 “ rotten\" 0.58 “ Nevertheless\" 0.57 “ And\" 0.54\n“ GREAT\" 0.56 “ harmful\" 0.58 “ However\" 0.56 “NO\" 0.53\nTable 14: The top 20 similar words of label names in sentiment classification (GOOD /BAD) and NLI (YES/NO)\ntasks.\n15604\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection limitations\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nChatGPT in section 6\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 5.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15605\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n15606",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8388550281524658
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6970140933990479
    },
    {
      "name": "Natural language processing",
      "score": 0.6914011240005493
    },
    {
      "name": "Language model",
      "score": 0.6243176460266113
    },
    {
      "name": "Construct (python library)",
      "score": 0.5866826176643372
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5847396850585938
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5781462788581848
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5775967836380005
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5772719383239746
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.47666552662849426
    },
    {
      "name": "Question answering",
      "score": 0.4550609588623047
    },
    {
      "name": "Code (set theory)",
      "score": 0.4285815954208374
    },
    {
      "name": "Programming language",
      "score": 0.09036821126937866
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": []
}