{
    "title": "Text-conditioned Transformer for automatic pronunciation error detection",
    "url": "https://openalex.org/W3081817774",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2096743620",
            "name": "Zhan Zhang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2136294607",
            "name": "yuehai wang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2111635996",
            "name": "Jianyi Yang",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2096743620",
            "name": "Zhan Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2136294607",
            "name": "yuehai wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111635996",
            "name": "Jianyi Yang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6604828220",
        "https://openalex.org/W6702097088",
        "https://openalex.org/W1032614754",
        "https://openalex.org/W6623517193",
        "https://openalex.org/W6754299077",
        "https://openalex.org/W6734301670",
        "https://openalex.org/W2016114400",
        "https://openalex.org/W103312509",
        "https://openalex.org/W7027429494",
        "https://openalex.org/W2400818677",
        "https://openalex.org/W6712821132",
        "https://openalex.org/W6726265082",
        "https://openalex.org/W6602874765",
        "https://openalex.org/W6714151304",
        "https://openalex.org/W6667421958",
        "https://openalex.org/W6697583303",
        "https://openalex.org/W6642965944",
        "https://openalex.org/W6761144201",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W6742348326",
        "https://openalex.org/W6775929054",
        "https://openalex.org/W6743149223",
        "https://openalex.org/W6772991762",
        "https://openalex.org/W6776305863",
        "https://openalex.org/W6629717138",
        "https://openalex.org/W6631362777",
        "https://openalex.org/W2625519968",
        "https://openalex.org/W296713244",
        "https://openalex.org/W6763832098",
        "https://openalex.org/W3008964788",
        "https://openalex.org/W6638016470",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2766219058",
        "https://openalex.org/W2139008940",
        "https://openalex.org/W6773475747",
        "https://openalex.org/W3013336802",
        "https://openalex.org/W6760966419",
        "https://openalex.org/W6754224190",
        "https://openalex.org/W2936078256",
        "https://openalex.org/W2945613576",
        "https://openalex.org/W2964027161",
        "https://openalex.org/W2406502879",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2515121265",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W116902681",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W2946200149",
        "https://openalex.org/W2962780374",
        "https://openalex.org/W2932319281",
        "https://openalex.org/W1769175087",
        "https://openalex.org/W2951418500",
        "https://openalex.org/W334543181",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2914584698",
        "https://openalex.org/W70727784",
        "https://openalex.org/W3015974384",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2164810574",
        "https://openalex.org/W4255556797",
        "https://openalex.org/W3038172701",
        "https://openalex.org/W1561493092",
        "https://openalex.org/W2398741870",
        "https://openalex.org/W2747874407",
        "https://openalex.org/W3015719750",
        "https://openalex.org/W2067517679",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3035083561",
        "https://openalex.org/W3035035925",
        "https://openalex.org/W2122364000",
        "https://openalex.org/W1970870811",
        "https://openalex.org/W1984986173",
        "https://openalex.org/W2938359332",
        "https://openalex.org/W3152218910",
        "https://openalex.org/W2810104656",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2591575955",
        "https://openalex.org/W3016010032",
        "https://openalex.org/W2983786745",
        "https://openalex.org/W2144374888",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W4205445431",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W2890915139",
        "https://openalex.org/W2970730223",
        "https://openalex.org/W4235832433",
        "https://openalex.org/W2912083425",
        "https://openalex.org/W3024171804",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W3039049049",
        "https://openalex.org/W2614103613",
        "https://openalex.org/W3015478688",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2888954148",
        "https://openalex.org/W2963819344",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W4287746114",
        "https://openalex.org/W2295721380",
        "https://openalex.org/W4247924304",
        "https://openalex.org/W854541894",
        "https://openalex.org/W4248805241",
        "https://openalex.org/W4287777801",
        "https://openalex.org/W2990138404"
    ],
    "abstract": null,
    "full_text": "Text-Conditioned Transformer for Automatic Pronunciation Error\nDetection\nZhan Zhanga, Yuehai Wanga,∗, Jianyi Yanga\na Department of Information and Electronic Engineering, Zhejiang University, China\nAbstract\nAutomatic pronunciation error detection (APED) plays an important role in the domain of language learning.\nAs for the previous ASR-based APED methods, the decoded results need to be aligned with the target text\nso that the errors can be found out. However, since the decoding process and the alignment process are\nindependent, the prior knowledge about the target text is not fully utilized. In this paper, we propose to use\nthe target text as an extra condition for the Transformer backbone to handle the APED task. The proposed\nmethod can output the error states with consideration of the relationship between the input speech and\nthe target text in a fully end-to-end fashion. Meanwhile, as the prior target text is used as a condition\nfor the decoder input, the Transformer works in a feed-forward manner instead of autoregressive in the\ninference stage, which can signiﬁcantly boost the speed in the actual deployment. We set the ASR-based\nTransformer as the baseline APED model and conduct several experiments on the L2-Arctic dataset. The\nresults demonstrate that our approach can obtain 8.4% relative improvement on the F1 score metric.\nKeywords: automatic pronunciation error detection (APED), computer-assisted pronunciation training\n(CAPT), Transformer\n1. Introduction\nWith the quick development of globalization\nand education, the number of language learners is\nrapidly increasing. However, most learners are fac-\ning the problem of teacher shortage or ﬁnding a\nproper time to follow systematic learning. Thus,\nrecently, the computer-assisted language learning\n(CALL)[1] systems have been studied to oﬀer a ﬂex-\nible education service, which can be used to reach\nthe language learning requirement in fragmented\ntime. In particular, oral practice is an impor-\ntant part of daily communication, and computer-\nassisted pronunciation training (CAPT)[2] systems\nare designed for this task. Such systems generally\nplay the role of automatic pronunciation error de-\ntection (APED). The APED system ﬁrst gives a\npredeﬁned utterance text (and a reference speech\nof a professional teacher if needed), and the learner\n∗Corresponding author\nEmail addresses: zhan_zhang@zju.edu.cn (Zhan\nZhang), wyuehai@zju.edu.cn (Yuehai Wang),\nyangjy@zju.edu.cn (Jianyi Yang)\ntries to pronounce this target text correctly. For\nexample, a learner wants to study the pronuncia-\ntion of “apple” (its phonemes are “AE P AH L”),\nbut the learner may mispronounce it to “AE P AO\nL”. We call “AE P AO L” as the canonical pro-\nnunciation. By accurately detecting the pronun-\nciation errors and providing precise feedback that\n“AH” is mispronounced, the APED system guides\nthe learner to correct the pronunciation towards the\ntarget utterance and improve the speaking ability.\nAPED has been widely studied for decades. De-\npending on how to evaluate the matching degree\nbetween the student pronounced speech and the\nstandard pronunciation, several comparison-based\nor goodness of pronunciation (GOP) methods have\nbeen proposed to solve the APED task[3, 4, 5, 6,\n7, 8]. Recently, with the rising trend for neural\nnetworks and the development of automatic speech\nrecognition (ASR) technologies, some end-to-end\nAPED models [9, 10] have been studied to simplify\nthe workﬂow. They use ASR backbones to recog-\nnize the canonical pronunciation and obtain where\nthe errors are, based on the alignment between the\nPreprint submitted to Journal of LATEX Templates May 6, 2021\narXiv:2008.12424v2  [eess.AS]  5 May 2021\npredicted phonemes and the standard phonemes.\nThe ASR-based methods can signiﬁcantly decrease\nthe deploying eﬀorts compared with conventional\nGOP methods or comparison-based methods. In\nparticular, recently, the Transformer structure[11]\nshows a good performance for sequence-to-sequence\n(seq2seq) modelling, and gets promising perfor-\nmance in ASR tasks [12, 13, 14, 15]. Thus, we\nchoose the Transformer as the backbone for APED\ntasks in this paper.\nHowever, the main deﬁciency of the conventional\nASR-based Transformer for APED tasks is that\nthe autoregressive decoding will slow the inference\nspeed[16]. Unfortunately, the APED task generally\nrequires the system to give a quick response about\nthe errors so that the learners can adapt their pro-\nnunciations and evaluate again. Another considera-\ntion is that, for the ASR-based APED, the decoded\ntext sequence needs to be aligned with the target\ntext to detect the errors. Since the target text is\nalready known in advance, it is a waste to ignore\nthis prior knowledge during the autoregressive in-\nference. On the one hand, the length of the tar-\nget text is ﬁxed, but the autoregressive decoding is\nlength-agnostic. On the other hand, the recognized\nsequence is generally close to the prior target text in\nthis evaluation task. These two factors inspire us to\nuse the target text as extra input for the network.\nIn this paper, we propose an ASR and alignment\nuniﬁed Transformer-based APED workﬂow, which\ncan incorporate both the audio feature and the text\ninformation, and output the error states directly.\nCompared with ASR-based methods which opti-\nmize the recognition result to improve the APED\nperformance, the proposed method works in a fully\nend-to-end manner. Thus, the proposed method\ncan optimize the APED metric directly. We observe\na 8.4% relative improvement on theF1 score for the\nL2-Arctic dataset[17] with the proposed method.\nMeanwhile, by using the prior target text as an in-\nput condition, the inference process works in a feed-\nforward manner rather than autoregressive, which\ncan signiﬁcantly boost the inference speed as sug-\ngested in [18, 19].\nThe rest of this paper is organized as follows. In\nSection 2, we analyze the related works about the\nAPED task and how we are inspired to propose\nthe text-conditioned feed-forward Transformer; In\nSection 3, we compare the baseline ASR-based au-\ntoregressive APED Transformer and describe the\nproposed ASR and alignment uniﬁed feed-forward\nTransformer in detail; Next, we analyze the results\nobtained by the conventional methods and the pro-\nposed method in Section 4; Finally, we show the\nconclusion of this paper in Section 5.\n2. Related Works\nFrom the perspective of language learning, an er-\nror detected in the APED system can be described\nas that the produced pronunciation is a nonstan-\ndard one. In other words, the pronounced speech\ndeviates too far from the standard target speech.\nBased on this simple idea, comparison-based APED\nmethods [3, 4, 5, 6] have been explored. These\nmethods generally adopt dynamic time warping\n(DTW) [20] algorithms to align the extracted fea-\ntures of the input speech with the standard tar-\nget speech. Depending on the distance between\neach text unit, the pronunciation quality score can\nbe calculated. To this end, the comparison-based\nmethods need to prepare a standard speech for ref-\nerence, which are inconvenient to evaluate a new\nutterance.\nApart from directly comparing to a speciﬁc stan-\ndard speech, the input speech can also be evaluated\nby whether a standard acoustic model can recognize\neach phoneme. In particular, the likelihood of each\nphoneme has proven to be an eﬀective feature for\nindicating whether the error happens, and such a\nlikelihood-based scoring method is often referred to\nas GOP [7, 8]. In practice, this approach utilizes\nthe hidden Markov model (HMM) to model the se-\nquential phone states. The likelihood score is cal-\nculated from the force-aligned states and the open\nphone states. Since the ﬁrst proposal of GOP by [7],\nmany variants [21, 22, 23, 24, 25] have been studied\nto adapt its original equation for better measure-\nment of the goodness.\nWith the rise of deep learning, the performance\nof the ASR tasks has been greatly improved. Thus,\nby utilizing the advanced acoustic model of an ASR\nsystem and recognizing the input speech, ASR-\nbased APED can be another eﬃcient approach to\ndetect the errors. Such a method can also avoid the\ndeploying eﬀorts of conventional HMM-based GOP\nmethods or comparison-based DTW methods, and\nseveral ASR-based APED systems have been pro-\nposed [9, 10]. Currently, the ASR systems are gen-\nerally built upon CTC loss [26] or attention mecha-\nnism [27, 28] to handle the sequential features. The\nmain deﬁciency of CTC loss is the conditional in-\ndependent assumption. Such an assumption may\nnot be valid for the continuous speech. The ASR\n2\nperformance is reported to be better by combining\nthe CTC loss with the attention mechanism [29] or\nusing the Transformer structure[14, 15]. In partic-\nular, the Transformer structure, which is originally\ndesigned to handle the natural language processing\n(NLP) problems [30, 31], has been successfully uti-\nlized in several other domains, such as computer\nvision (CV)[32, 33], and speech-related tasks in-\ncluding text to speech (TTS) [34, 35, 18, 19], voice\nconversion (VC)[36], and ASR [12, 13].\nDespite the convenience of ASR-based APED\nsystems, alignment is still an inevitable process\nto obtain the ﬁnal evaluation results. The recog-\nnized phonemes should be aligned with the target\nphonemes to ﬁnd out the mispronunciations. As\nthe alignment process is not integrated into the\nbackward optimization of the ASR model, such a\nmethod is not fully end-to-end. In other words,\nthe decoding process and the evaluation process are\nindependent. However, intuitively, human raters\nwill ﬁrst keep the target text in mind, then try to\ncompare the input speech to ﬁnd out where the er-\nrors take place. Focussing on the prior target text\nlimits the search space for the decoding process.\nExtended Recognition Network (ERN) [37] utilizes\nthis idea to incorporate prior knowledge about com-\nmon mispronunciations into the HMM states. How-\never, the predeﬁned error HMM paths will lead to\nbad performance when faced with unseen mispro-\nnunciations. Despite its weakness, ERN still shows\nthat the prior knowledge is of vital importance to\nfacilitate the performance of APED tasks. This in-\nspires us to directly take the prior target text as an\nextra condition, together with the speech features\nfor input. Meanwhile, the attention mechanism can\nbe a logical approach to fuse both the speech feature\nand the text feature. Thus, the attention-based\nseq2seq models including Listen, attend and spell\n(LAS)[28] and Transformer[11] are ideal backbones\nto start with. Transformer uses the positional en-\ncoding to model the time information, instead of\na recurrent architecture in LAS. The ASR perfor-\nmance of Transformer is reported to be better in\n[15]. Thus, we use Transformer as the backbone in\nthis paper.\nHowever, the conventional attention-based\nTransformer generally adopt autoregressive de-\ncoding to predict the next entity. This will lead\nto a slow inference, which can be a deﬁciency for\nthe APED system. As analyzed in [16], for each\ndecoding step, the current prediction depends on\nthe earlier decoded output to get the conditional\nprobability. However, since the output target is al-\nready known in the training stage, the Transformer\ncan assume this target as a decoded result (this is\ncalled as “teacher-forcing”). Thus, the Transformer\ndo not need to wait for the decoded output and the\nTransformer can run in parallel. In contrast, this\nprior does not exist in the inference stage, and the\nTransformer must run sequentially to predict the\nnext entity for several decoding steps until meeting\nthe end-of-sentence-tag ( ⟨EOS⟩). On the contrary,\nTransformers which work in a feed-forward manner\ncan greatly boost the speed [16, 18, 19]. Thus, for\nthe APED task, if we can utilize the prior text to\nbe evaluated, and unify the ASR and alignment\nprocess, the conventional Transformers can decode\nin a feed-forward manner, and the aforementioned\nlimitation will no longer exist.\nBased on the analysis above, we propose the\ntext-conditioned ASR and alignment uniﬁed feed-\nforward Transformer for the APED task. We give a\ndetailed description of the proposed method in the\nnext section.\n3. Proposed Method\nIn this section, we ﬁrst show the conventional\nASR-based APED workﬂow for comparison. Next,\nwe demonstrate the proposed fully end-to-end\nworkﬂow and describe the network structure and\nits training method in detail.\n3.1. ASR-Based APED\nAudio Features\nASR Model\nPredicted\nPhonemes\nTarget \nPhonemes\nCanonical\nPhonemes\nLoss\nPredicted \nError States\nAlignment\nData outputting\nData preparing\nInference\nFigure 1: Workﬂow for the ASR-based APED method. The\nalignment process is independent from the phoneme predic-\ntion process.\n3\nA typical workﬂow for the ASR-based APED is\ndepicted in Fig.1. The training dataset is generally\nconstructed by three parts, the target text to be\nread, the collected speech, and the canonical pro-\nnounced text marked by professional annotators.\nFor example, L2-Arctic dataset[17] manually labels\nthe correct phonemes and mispronunciation error\ntags about the collected speech. Three annotators\nwho are experienced in transcribing speech samples\nof native or non-native English speakers participate\nin the annotating process to ensure the high qual-\nity. Based on such a dataset, an ASR model is\ntrained to recognize the canonical phoneme-level\ntext p = (p1,p2,...,p n,pn+1) from the extracted au-\ndio features x = ( x1,x2,...,x m). We should note\nthat the described ASR-based APED is general,\nand can be applied with any ASR systems that can\ntranslate the audio features into phonemes. How-\never, as we focus on Transformer, we limit our de-\nscription on the attention-based training and infer-\nence in the following paragraph. For the attention-\nbased models, the cross-entropy loss is used be-\ntween the predict phonemes ˆp and the canonical\nphonemes p:\nlasr = CrossEntropy(ˆp,p), (1)\nwhere pn+1 = ⟨EOS⟩.\nFor the inference stage, the Transformer works\nquite diﬀerently from the training stage. The\nTransformer uses autoregressive outputting method\nto recognize the canonical phonemes sequentially.\nThe recognized phonemes string will end with\n⟨EOS⟩. Next, Needleman-Wunscha algorithm[38] is\napplied to align the recognized sequence ˆp with the\ntarget phonemes t = (t1,t2,...,t k). After the align-\nment process, the error states e = ( e1,e2,...,e k)\nwith consideration of the target phonemes can be\nreturned to the user. An alignment example is\nshown in Table 1. We can observe that this sam-\nple includes 1 deletion and 2 substitution errors.\nThe mispronounced phonemes whose error states\nare marked as 1 can be returned to the users.\nFor better clariﬁcation, we summarize the train-\ning and the inference stage of the ASR-based model\nin Table 2. We use a 39-dim Mel frequency cep-\nstral coeﬃcients (MFCC) feature as the encoder\ninput. The start-of-sentence tag ( ⟨SOS⟩) and the\nright-shifted 1-dim label of the canonical phonemes\nare concatenated as the decoder input in the train-\ning stage. This input is replaced by ⟨SOS⟩and a re-\ngressively decoded phonemes string in the inference\nstage. The decoder tries to predict the probability\nof the next phoneme and ⟨EOS⟩for output. There\nare in total 42 tags for classiﬁcation, including 39\nphonemes and ⟨SOS⟩⟨EOS⟩⟨PAD⟩.\nWe should note that there are several lengths de-\nﬁned for the described sequences. First, the atten-\ntion mechanism is adopted to match the speech fea-\ntures (length = m) and the recognized phonemes\n(length = n+ 1). Next, the alignment operation is\napplied to ﬁnd out the error states, whose length is\nequal to that of the target phonemes ( length = k).\nHowever, such an alignment operation is performed\nin the inference stage, thus not jointly optimized\nwith the ASR model. Such a dilemma inspires us\nto integrate the alignment operation or the target\ntext into the training stage.\n3.2. Fully End-to-end APED\nAs shown in Fig.2, for the proposed method, we\nmove the alignment operation into the data prepar-\ning stage. We align the canonical phonemes and the\ntarget phonemes to obtain where the errors occur\nin advance.\nNext, we directly evaluate the relationship be-\ntween speech features and the target phonemes.\nThus, the network can be viewed as a fusion model.\nMoreover, the mother language (L1) of the speak-\ners shows to aﬀect the acoustic characteristics when\nstudying a new language (L2) [39, 40]. Meanwhile,\nthe extracted L1 features have also been proved to\nbe helpful in the APED task [41]. Thus, we intro-\nduce the accent related auxiliary task to extract the\nL1 information. As shown in Fig.3, we append an\nextra classiﬁer after the encoder and use the cross-\nentropy loss between the predicted accent ˆaand the\nground truth accent a presented in the dataset:\nla = CrossEntropy(ˆa,a). (2)\nSince the speech evaluation dataset is scarce, we\nﬁrst obtain a basic acoustic model by training the\nmodel on ASR datasets. The training process is\nsimilar to conventional ASR-based APED methods\ndiscussed in Section 3.1, and the new ASR loss func-\ntion is,\nl\n′\nasr = lasr + αla, (3)\nwhere α is the weight of the auxiliary accent task.\nWe further adapt this basic acoustic model to the\nAPED task. A training and inference summary of\nthe proposed model is shown in Table 3. We will\ndiscuss the details and the diﬀerences between the\n4\nTable 1: Alignment sample\nIF YOU ONLY COULD KNOW HOW I THANK YOU\nTarget IH F Y UW OW N L IY K UH D N OW HH AW AY TH AE NG K Y UW\nPronouncedIH F Y UW AO N L IY K UH - N AO HH AW AY TH AE NG K Y UW\nError States0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\nTable 2: Training and inference summary of the ASR-Based Transformer\nTraining Stage\nEncoderInput DecoderInput DecoderOutput\ndata SpeechFeatures⟨SOS⟩+Canonical Phonemes(Shifted) Canonical Phonemes+⟨EOS⟩\nloss - - lasr\nlen m 1+n n+1\ndim 39 1 42\nInference Stage\nEncoderInput DecoderInput DecoderOutput\ndata SpeechFeatures ⟨SOS⟩+Recognized Phonemes Next Recognized Phonemes\nlen m End with ⟨EOS⟩ End with⟨EOS⟩\ndim 39 1 42\nAudio Features\nFusion Model\nTarget \nPhonemes\nTarget \nPhonemes\nPredicted \nError States\nAlignment\nCanonical\nPhonemes\nGround Truth \nError States\nMain\nLoss\nPredicted\nAccent\nGround Truth \nAccent\nAuxiliary\nLoss 1\nData outputting\nData preparing\nMain task\nAuxiliary task\nAligned Canonical\nPhonemes\nPredicted\nPhonemes\nAuxiliary\nLoss 2\nInference\nFigure 2: Workﬂow for the proposed APED method. We move the alignment process into the preparing stage. The proposed\nmodel can directly output the error states. Meanwhile, the auxiliary accent and phoneme classiﬁcation tasks are adopted.\nTable 3: Training and inference summary of the proposed Transformer\nTraining Stage\nEncoderInput EncoderOutput DecoderInput DecoderOutput1 DecoderOutput2\ndata SpeechFeatures Accent ⟨SOS⟩+Target Phonemes Aligned Canonical Phonemes+⟨EOS⟩ ⟨SOS⟩+Error States\nloss - la - lasr leval\nlen m 1 1+k k+1 1+k\ndim 39 6 1 42 1\nInference Stage\nEncoderInput EncoderOutput DecoderInput DecoderOutput1 DecoderOutput2\ndata SpeechFeatures Accent ⟨SOS⟩+Target Phonemes Canonical Phonemes+⟨EOS⟩ ⟨ SOS⟩+Error States\nlen m 1 1+k k+1 1+k\ndim 39 6 1 42 1\n5\nMulti-\nHead\nAttention\nAdd & Norm\nInput\nEmbedding\nOutput\nEmbedding\nFeed\nForward\nAdd & Norm\nMasked\nMulti\n-\nHead\nAttention\nAdd & Norm\nMulti-\nHead\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nAudio Features\nTarget Phonemes\nPositional \nEncoding\nPositional \nEncoding\nGlobal Mean\nLinear\nLinear\nLinear\nSigmoid\nError \nStates\nSoftmax\nAccent\nAuxiliary Block\nEncoder\nDecoder\nLinear\nSoftmax\nCanonical \nPhonemes\n<SOS> AE P AH L\nAE P \nAO\nL <EOS>\n<SOS> 0 0 \n1\n0\nArabic\nFigure 3: Network architecture of the text-conditioned\nTransformer. We append an accent classiﬁer after the\nencoder to extract the L1-related information. Target\nphonemes are used as an extra condition for the decoder\ninput. The error states are obtained in a feed-forward man-\nner. Meanwhile, phoneme classiﬁcation is also performed as\nan auxiliary task. The mispronounced word “APPLE” is\nshown in this ﬁgure for demonstration.\nproposed model and the ASR-based model in the\nremaining paragraphs.\nFirstly, for the auxiliary accent classiﬁcation\ntask, while the input audio features are sequen-\ntial, the accent is a 1-dim global attribute. We try\nto process the sequential data with gated recurrent\nunits (GRU)[42] or a simple GlobalMean. Experi-\nments in Section 4 show that GlobalMean performs\na little better. Note that there are 6 kinds of accent\nincluding Arabic, Chinese, Hindi, Korean, Spanish,\nand Vietnamese, for the used dataset in our exper-\niments.\nSecondly, the prior target phonemes are used as\nan extra condition for the decoder input instead of\nthe canonical pronounced phonemes, in both the\ntraining and the inference stage. For the audio\nfeatures x and a certain target phoneme ti (target\nphoneme at step i), the decoder output is changed\nto ˆei, which indicates the matching degree of the\naudio features and ti. As we use a binary state to\njudge its goodness, we use the sigmoid activation\nat the last layer for binary classiﬁcation in Fig.3.\nAs the whole process is diﬀerentiable, we can di-\nrectly optimize the loss between the predicted er-\nror states ˆe and the ground truth error states e.\nFor now, several classiﬁcation losses can be used\nfor this model. We ﬁrst apply a basic binary cross-\nentropy (BCE) loss between the predicted error\nstates ˆe = ( ˆe0, ˆe1, ˆe2,..., ˆek) and the ground truth\nerror states e = (e0,e1,e2,...,e k) as the evaluation\nloss,\nlBCE\neval = BCE(ˆe,e), (4)\nwhere e0 = ⟨SOS⟩. A further discussion about the\nchoice of loss functions is presented in Section 4.5.\nHowever, compared with ASR-based methods, a\nbinary state only concerns about whether the tar-\nget phoneme is correct or mispronounced. Thus,\nthe model may lose information about the exact\nphoneme. To ﬁx this, we still require the proposed\nmodel to conduct the ASR task with an auxiliary\nweight of β, and the whole loss function is,\nl= leval + βlasr + αla. (5)\nThe canonical phonemes to be recognized are\naligned with the target phonemes for the pro-\nposed model using the aforementioned Needleman-\nWunscha algorithm in Section 3.1 to make these\ntwo phoneme strings have equal length k+ 1.\nLastly, we should note that the proposed model\nhas a consistent behavior in the training and infer-\nence stage, as shown in Table 3. This characteris-\ntic makes the inference in our method faster com-\npared with ASR-based autoregressive Transform-\ners, shown in Section 4.2.\n4. Experiment\nWe use the SpeechTransformer backbone pro-\nposed in [43] for experiments. The SpeechTrans-\nformer is constructed by 6 encoder and 6 decoder\nlayers in our experiments. Meanwhile, the atten-\ntion modelling dimension dmodel = 512, 4 attention\nheads, and the feed-forward dimension dff = 1024\nare adopted. We extract the MFCC features of the\naudio ﬁles by Kaldi toolkit[44]. These MFCC fea-\ntures are subsampled with a factor of n = 4, and\nstacked with m= 5 number of frames, which is the\nsame as the settings in [43]. We demonstrate the\nASR performance for phoneme recognition in the\nﬁrst subsection 4.1. Then we use this pretrained\nmodel to adapt for the APED task and show the\nlatency, metric, and results in the next three sub-\nsections. Finally, we analyze the loss functions and\nthe behavior of the proposed model in the last two\nsubsections, 4.5 and 4.6, correspondingly.\n6\n4.1. Phoneme Recognition\nWe use Librispeech [45] as the dataset for ASR\ntraining. This dataset contains approximately 1000\nhours of 16kHz read English speech. It is di-\nvided into “clean” (460 hours) and “other” (500\nhours) parts based on its recognition diﬃculty. The\n“clean” part is further divided into the training set\nof 100 hours and 360 hours, development set (dev-\nclean), and test set (test-clean). As the APED\ntask focuses on the phoneme-level error, we ﬁrst\nconvert the dataset into phoneme-level transcrip-\ntions using the Montreal Forced Aligner tool[46].\nNext, we train the Transformer on diﬀerent parts of\nthe trainset for 300 epochs, including train-clean of\n100 hours (train-100h), the whole train-clean part\n(train-460h), and the whole train part (train-960h).\nWe use dev-clean as the validation dataset to choose\nthe best model and test-clean for inference perfor-\nmance comparison. Adam optimizer, with a learn-\ning rate of 10 −3, is used. We use a CTC-based\nASR model called Jasper5x3 proposed in [47] for\ncomparison. This model is constructed by 5 re-\npeated Jasper blocks, and each Jasper block is con-\nstructed by 3 repeated Conv1D sub-blocks. The\nmodel parameters of Jasper5x3 are 44M, while the\nproposed Transformer is 32M. We show the phone\nerror rate (PER) performance in Table 4. As we can\nsee from the table, for diﬀerent amounts of training\nresources, the attention-based Transformer struc-\nture generally performs better than the CTC-based\nmethod on PER. This observation is in accord with\nthe conclusion in [14, 15], as the attention mecha-\nnism in Transformer can capture more relevant in-\nformation compared with the CTC loss which holds\nthe conditional independent assumption.\nTable 4: Performance of PER on diﬀerent subsets of Lib-\nrispeech dataset.\nCTC-Based Transformer-Based\ntrain- dev-clean test-clean dev-clean1 test-clean\n100h 8.13% 8.50% 4.55% 8.11%\n460h 4.88% 5.50% 2.32% 4.24%\n960h 4.02% 4.23% 1.70% 3.17%\n4.2. Latency Experiment\nNext, we conduct the APED task on L2-Arctic\ndataset [17]. This corpus contains 26,867 utterances\n1Since this result on the development set dev-clean is ob-\ntained by using the teacher-forcing training, PER is much\nlower.\nwith 6 diﬀerent accents, from 24 nonnative speak-\ners. The 3,599 utterances annotated on phoneme-\nlevel are used for the APED task. The train-\nset, valset, and testset are divided into 8:1:1. For\nthe testset, each sentence contains about 30 target\nphonemes on average. This suggests that the con-\nventional autoregressive ASR-based models need to\nforward about 30 times on average to get each de-\ncoded phoneme sequentially. On the contrary, the\nproposed methods only need to forward once. We\nconduct the latency evaluation on a server with In-\ntel Xeon E5-2680 CPU, and 1 NVIDIA P100 GPU.\nAs shown in Table 5, the proposed method can\nbring great speedup for the APED inference.\nTable 5: Latency comparsion. b stands for batchsize here.\nThe latency is computed as the average time to decode each\nsentence in the testset.\nLatency(ms) Speedup\nASR-Based (b= 1) 1194 ±198 1.00 ×\nASR-Based (b= 4) 966 ±106 1.24 ×\nProposed (b= 1) 88 ±13 13.6 ×\nProposed (b= 4) 67 ±6 17.8 ×\n4.3. APED Metric\nFor the APED task, the model should make a\ngood balance of detecting the wrong pronunciations\nand accepting the correct ones. Thus, F1 score is\nchosen as the main indicator for the performance.\nAs deﬁned in [48], the hierarchical evaluation struc-\nture is ﬁrst divided into correct pronunciations and\nwrong pronunciations by the canonical pronounced\nphoneme. Next, depending on whether the pre-\ndicted error state matches the ground truth label,\nthe outcomes are further divided into true accep-\ntance (TA), false rejection (FR), false acceptance\n(FA), and true rejection (TR). In other words, T/F\nsuggests whether the prediction of the model is cor-\nrect for the APED task, and A/R is the decision of\nthe model. Based on this evaluation structure, F1\nscore of the APED system is deﬁned as follows:\nPrecision = TR\nTR + FR, (6)\nRecall= TR\nTR + FA, (7)\nF1 = 2 Precision ∗Recall\nPrecision + Recall. (8)\nTo count TR,TA,FR,FA for metrics, the pre-\ndicted binary error states ( ˆe1, ˆe2,..., ˆek) are ﬁrstly\n7\nTable 6: Comparison between diﬀerent models.\nAccent\nClassiﬁcation\nPhoneme\nClassiﬁcation FAR FRR Acc Precision Recall F1\nGOP-Based\nGMM-HMM(Librispeech)2 - - - - - 0.290 0.290 0.290\nASR-Based\nInitial(Librispeech) \u0015 ✓ 0.485 0.207 0.753 0.295 0.515 0.375\nFine-tuned(L2-Arctic) \u0015 ✓ 0.375 0.103 0.858 0.504 0.625 0.558\nFine-tuned(L2-Arctic) ✓ ✓ 0.353 0.106 0.859 0.507 0.647 0.568\nProposed\nBCE Loss ✓ \u0015 0.458 0.051 0.890 0.639 0.542 0.587\nBCE Loss ✓ ✓ 0.429 0.054 0.890 0.641 0.571 0.603\nF1 Loss ✓ \u0015 0.442 0.055 0.889 0.630 0.558 0.591\nF1 Loss ✓ ✓ 0.428 0.058 0.889 0.622 0.572 0.596\nFocal Loss ✓ \u0015 0.424 0.060 0.888 0.617 0.576 0.595\nFocal Loss ✓ ✓ 0.423 0.055 0.882 0.636 0.577 0.605\nﬁltered by a threshold of θ= 0.5 to transform from\na continuous ﬂoat with the range of (0 ,1) into dis-\ncrete binary integer {0,1},\nˆe←\n{\n1, if ˆe≥θ\n0. otherwise (9)\nNext, each outcome is calculated by following equa-\ntions,\nTR =\nk∑\ni=1\n( ˆei ∗ei), (10)\nFR =\nk∑\ni=1\n( ˆei ∗(1 −ei)), (11)\nFA =\nk∑\ni=1\n((1 −ˆei) ∗ei), (12)\nTA =\nk∑\ni=1\n((1 −ˆei) ∗(1 −ei)). (13)\nApart from the conventional classiﬁcation-related\nmetrics including F1 score, accuracy, precision and\nrecall, the false rejection rate (FRR) and the false\nacceptance rate (FAR) are also of vital importance\nto the APED task. They are calculated as follows,\nFRR = FR\nTA + FR, (14)\nFAR = FA\nFA + TR. (15)\n2This result is taken from [17], Fig.4. It is trained on\nLibrispeech train-960 and tested on L2-Arctic dataset.\n4.4. APED Result\nWe ﬁrst conduct experiments to explore the aux-\niliary accent classiﬁcation task. We start from the\nmodel obtained on Librispeech dataset, and train\nfor another 200 epochs, with the learning rate de-\ncreased to 10 −4. We ﬁnd that the GlobalMean\nmethod performs a little better than the GRU, as\nshown in Fig.4. We use α= 0.7 for the ASR-based\nTransformer in Eq.3, and lower it to α = 0 .1 in\nEq.5 to balance the lasr loss for further experiments\nabout the proposed text-conditioned version.\n0.1 0.3 0.5 0.7 0.9\nλ\n0.50\n0.51\n0.52\n0.53\n0.54\n0.55\n0.56\n0.57\n0.58F1 − score\nGlobalMean\nGRU\nFigure 4: F1 score comparison of GlobalMean and GRU.\nNext, we adapt this pretrained ASR-based model\nto the proposed text-conditioned version. We still\ntrain the whole model for 200 epochs, with the\nlearning rate of 10−4. We set the ASR-based model\nwithout the auxiliary task for baseline and the pro-\nposed methods with ablation for comparison. We\nﬁnd that β = 0 .3 generally gives the best perfor-\nmance. The results are shown in Table 6. The\n8\nperformance of the GOP method tested on this\ndataset[17] and the initial Transformer model pre-\ntrained on Librispeech is also reported in this ta-\nble. First of all, as the initial model is purely\ntrained on Librispeech, which only includes stan-\ndard pronunciations, its FRR is relative high as it\ndirectly treats some unseen accents as wrong pro-\nnunciations. When adapted to handle the L2-Arctic\ndataset, the model has a signiﬁcant improvement.\nBy further employing the accent auxiliary task, the\nF1 score is increased by nearly 0.1. If we simply use\nthe target text as the condition and change the pre-\ndiction target to the error states, the basic binary\ncross-entropy loss can bring a 0.19 improvement in\nterms of the F1 score. We discuss the eﬀect of dif-\nferent loss functions for the proposed method in the\nnext subsection.\n4.5. Loss functions\nFirst of all, as the F1 score is an important met-\nric for the APED task, inspired by [49], we directly\nutilize the generalized F1 score to optimize the pre-\ndicted error states. To make it diﬀerentiable, the\nsums of probabilities are used instead of counts. We\ndo not apply Eq.9 before calculating Eq.10 - 13. We\nshould also note that Eq.9 is applied only for met-\nrics calculation. For all the loss functions discussed\nin this subsection, ˆe is continuous. As we try to\nmaximize the F1 score, the F1 evaluation loss of\nthe proposed method is,\nlF1\neval = 1 −F1. (16)\nAnother consideration is that only 14.56% of the\nlabelled phone segments are mispronounced for the\nL2-Arctic dataset3, which may cause an unbalance\nbetween correct pronunciations and mispronuncia-\ntions. Thus, we adopt focal loss[50] to mine the\nhard labels. Formally, if we deﬁne et as:\net =\n{\nˆe, if e= 1\n1 −ˆe. otherwise (17)\nThe focal loss is,\nlfocal\neval = −(1 −et)γlog(et), (18)\nwhere γ modulates how much the well-classiﬁed\nsamples are down-weighted. When γ = 0, this loss\nfunction is equivalent to Eq.4.\n3Dataset document at https://psi.engr.tamu.edu/\nl2-arctic-corpus-docs/\nWe apply F1 loss function and focal loss with\ndiﬀerent γ values to the proposed model. We can\nsee from Table 6 that, when adopting the F1 loss\nfunction instead of the basic BCE loss, the result\ncan be slightly improved. For the focal loss, we ﬁnd\nthat a small γ value ( γ=0.5 in our experiments)\nperforms the best, and a bigger value will lead to a\ndegraded F1 score. Meanwhile, the auxiliary ASR\ntask can boost the performance for all these loss\nfunctions. The focal loss version has the highest\nF1 score 0.605 for the default θ = 0.5, which is a\nrelative 8.4% improvement over the baseline ASR-\nbased method.\n4.6. Analysis\nWe further analyse the behavior of the proposed\nmethod.\nFor the APED task, we need to make a trade-\noﬀ between FAR and FRR. Meanwhile, as noted\nin [51], it is usually more unacceptable to take the\ncorrect pronunciations as wrong ones (false reject)\nthan to regard the mispronunciations as correct\nones (false acceptance). We can observe from Ta-\nble 6 that the proposed methods all have a higher\nFAR and decreased FRR compared with ASR-\nbased models, which suggests our model is behaving\nin a more acceptable way.\nFor the actual deployment, as the proﬁciency\nlevel of the target language varies among diﬀer-\nent students, the trade-oﬀ between FAR and FRR\nshould be easy to adjust. Compared with ASR-\nbased models, the proposed method can simply\nchange the threshold θ to control how strict the\nAPED system is. We further explore the eﬀect of\nchanging θ for diﬀerent loss functions. We use a\nstep of 0.1, i.e., θ ∈[0.1,0.2,..., 0.9]. The metrics\nare shown in Fig.5. By increasing θ, according to\nEq.9, more output is judged as correct, and less\noutput is judged as error. As a result, FAR (and\nprecision) increases, while FRR (and recall) drops.\nCompared with the F1 loss version, the BCE loss\nand the focal loss version have a wider range of\nFAR and FRR when adjusting θ and can be a bet-\nter choice for the actual deployment.\nSince the proposed method is related to the in-\nput text, we break the testset into diﬀerent parts\nto show the impact of how much the canonical\nphonemes diﬀer from the target phonemes, i.e., the\npronunciation error rate. A larger pronunciation\nerror rate suggests that the input text information\nbecomes less related to the input speech. We use\nthe focal loss version and the ASR-based baseline\n9\n0.500 0.525 0.550 0.575 0.600 0.625\nRecall\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675Precision\nModel\nBCE Loss\nF1 Loss\nFocal Loss ( = 0.5) \nASR-Based\n(a)\n0.375 0.400 0.425 0.450 0.475 0.500\nFAR\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10FRR\nModel\nBCE Loss\nF1 Loss\nFocal Loss ( = 0.5) \nASR-Based (b)\nFigure 5: Metrics between diﬀerent models by changing θ. We also show the performance of the ASR-based baseline. (a)\nRecall-Precision curves. (b) FAR-FRR curves.\nQuantile Error Rate ASR-Based F1 Proposed F1 Improvement\n25% [0,7.8%] 0.338 0.390 15.38%\n50% (7.8%,13.0%] 0.461 0.516 11.93%\n75% (13.0%,19.4%] 0.550 0.609 10.73%\n100% (19.4%,46.8%] 0.674 0.694 2.97%\nTable 7: Pronunciation error rate breakdown on the testset.\nfor comparison. The result is shown in Table 7.\nWe can observe that the proposed method makes\nhigher relative improvement when the error rate is\nlower.\nFinally, we try to analyze the auxiliary ASR task.\nFor the ASR-based Transformer, on the one hand,\nthe encoder extracts the speech-related features as\nembeddings; On the other hand, the decoder uses\nthe attention mechanism to query the correspond-\ning weight of each memory for the input text. Thus,\nthe attention mechanism in the decoder will con-\nduct an alignment between the text and the cor-\nresponding speech. What will the proposed text-\nconditioned Transformer do if the input text is not\nthe canonical pronunciation but the target one?\nWe plot the attention map of the proposed method\nwithout or with the auxiliary ASR task in Fig.6 to\nexplore its behavior. For simplicity, we call these\ntwo models as the simple version and the full ver-\nsion in the following discussion.\nAs shown in Fig.6, for the simple version, it still\ntries to align the speech feature with the target\nphonemes for the shallow layers. As for deeper lay-\ners, the alignment between the phonemes and the\nspeech features becomes vague. We conjecture that\nthe training target causes this phenomenon. As for\nthe ASR task, the network has to predict the next\nphoneme exactly; However, for the APED task, the\nnetwork just needs to handle the error pattern for\neach phoneme and outputs a binary state, which is\nan easier task. Under such a cosy target, the deeper\nlayers may not work hard to do the alignment, but\nchoose to focus on summarizing the error patterns.\nAs pretraining on the ASR task can be viewed as a\nsequential adaption[52], the pretrained weights per-\nform as a regularization for the APED optimization\nspace. Meanwhile, as suggested in [53], the adapted\nmodel does not deviate from the pretrained weights\nsigniﬁcantly. Thus, based on the pretrained ASR\nweight, the model still has the ability to distinguish\ndiﬀerent phonemes and match the input phonemes\nwith the audio features memory. This may be the\nreason that the simple version can still get a satis-\nfying improvement, as shown in 6. When the model\nis required to conduct ASR task, namely, the full\nversion, the attention maps appear to be regular,\nwhich are similar to those Transformers that are\napplied for ASR tasks. As a result, the full version\n10\nFigure 6: Comparison between the simple version without ASR auxiliary task (left) and the full version (right). This sample\n(arctic a0129) is spoken by speaker ZHAA, “HER FACE WAS AGAINST HIS BREAST”. The EY phoneme in “AGAINST”\nis pronounced to be EH by mistake. When an error happens, the corresponding phoneme is marked as red. The simple\nversion tries to do the alignment only for the shallow layers, whereas the behavior of the full version is more to the ASR-based\nTransformers.\ngenerally performs better than the simple version.\n5. Conclusion\nIn this study, we propose a text-conditioned\nTransformer for automatic pronunciation error de-\ntection. By conditioning the target phonemes as\nan extra input, the Transformer can directly evalu-\nate the relationship between the input speech and\nthe target phonemes. Thus, the error states are\nobtained in a fully end-to-end manner. Meanwhile,\nunlike the conventional autoregressive Transformer,\nthe proposed method works in a feed-forward man-\nner in both the training and the inference stage. We\nconduct a number of experiments to compare the\nperformance of diﬀerent methods and ﬁnd that the\nproposed text-conditioned Transformer can boost\nthe F1 score of the APED task on the L2-Arctic\ndataset. The proposed method has a more reason-\nable FAR and FRR, and the degree of strictness can\nbe easily adjusted by the threshold θ parameter.\nReferences\n[1] K. Beatty, Teaching and Researching: Computer-\nassisted Language Learning, Routledge, 2013. doi:\n10.4324/9781315833774.\n[2] N. Stenson, B. Downing, J. Smith, K. Smith, The ef-\nfectiveness of computer-assisted pronunciation training,\nCalico Journal (1992) 5–19.\n[3] A. Lee, J. Glass, Pronunciation assessment via a\ncomparison-based system, in: Speech and Language\nTechnology in Education, 2013.\n[4] A. Lee, Y. Zhang, J. Glass, Mispronunciation detec-\ntion via dynamic time warping on deep belief network-\nbased posteriorgrams, in: 2013 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, IEEE, IEEE, 2013, pp. 8227–8231. doi:10.1109/\nicassp.2013.6639269.\n[5] A. Lee, N. F. Chen, J. Glass, Personalized mispro-\nnunciation detection and diagnosis based on unsuper-\nvised error pattern discovery, in: 2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), IEEE, IEEE, 2016, pp. 6145–6149.\ndoi:10.1109/icassp.2016.7472858.\n[6] A. Lee, J. Glass, A comparison-based approach to mis-\npronunciation detection, in: 2012 IEEE Spoken Lan-\nguage Technology Workshop (SLT), IEEE, IEEE, 2012,\npp. 382–387. doi:10.1109/slt.2012.6424254.\n11\n[7] S. M. Witt, Use of speech recognition in computer-\nassisted language learning.\n[8] S. Witt, S. Young, Phone-level pronunciation scor-\ning and assessment for interactive language learning,\nSpeech Communication 30 (2-3) (2000) 95–108. doi:\n10.1016/s0167-6393(99)00044-8.\n[9] W.-K. Leung, X. Liu, H. Meng, CNN-RNN-CTC based\nend-to-end mispronunciation detection and diagnosis,\nin: ICASSP 2019 - 2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, IEEE, 2019, pp. 8132–8136. doi:\n10.1109/icassp.2019.8682654.\n[10] L. Zhang, Z. Zhao, C. Ma, L. Shan, H. Sun,\nL. Jiang, S. Deng, C. Gao, End-to-end automatic\npronunciation error detection based on improved hy-\nbrid CTC/Attention architecture, Sensors 20 (7) (2020)\n1809. doi:10.3390/s20071809.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, u. Kaiser, I. Polosukhin, Atten-\ntion is all you need, in: Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing\nSystems, NIPS’17, Curran Associates Inc., Red Hook,\nNY, USA, 2017, p. 6000–6010.\n[12] N. Moritz, T. Hori, J. Le, Streaming automatic\nspeech recognition with the transformer model, in:\nICASSP 2020 - 2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nIEEE, IEEE, 2020, pp. 6074–6078. doi:10.1109/\nicassp40776.2020.9054476.\n[13] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDer-\nmott, S. Koo, S. Kumar, Transformer transducer: A\nstreamable speech recognition model with transformer\nencoders and RNN-t loss, in: ICASSP 2020 - 2020 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), IEEE, IEEE, 2020, pp. 7829–\n7833. doi:10.1109/icassp40776.2020.9053896.\n[14] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishi-\ntoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wies-\nner, N. Chen, et al., Espnet: End-to-end speech pro-\ncessing toolkit, arXiv preprint arXiv:1804.00015.\n[15] L. Dong, S. Xu, B. Xu, Speech-transformer: A\nno-recurrence sequence-to-sequence model for speech\nrecognition, in: 2018 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\nIEEE, IEEE, 2018, pp. 5884–5888. doi:10.1109/\nicassp.2018.8462506.\n[16] J. Gu, J. Bradbury, C. Xiong, V. O. Li, R. Socher,\nNon-autoregressive neural machine translation, arXiv\npreprint arXiv:1711.02281.\n[17] G. Zhao, S. Sonsaat, A. Silpachai, I. Lucic,\nE. Chukharev-Hudilainen, J. Levis, R. Gutierrez-\nOsuna, L2-arctic: A non-native english speech cor-\npus, in: Proc. Interspeech, 2018, p. 2783–2787. doi:\n10.21437/Interspeech.2018-1110.\n[18] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, T.-\nY. Liu, Fastspeech: Fast, robust and controllable text to\nspeech, in: Advances in Neural Information Processing\nSystems, 2019, pp. 3171–3180.\n[19] K. Peng, W. Ping, Z. Song, K. Zhao, Parallel neural\ntext-to-speech, arXiv preprint arXiv:1905.08459.\n[20] D. J. Berndt, J. Cliﬀord, Using dynamic time warping\nto ﬁnd patterns in time series., in: KDD workshop,\nVol. 10, Seattle, WA, USA:, 1994, pp. 359–370.\n[21] Y. Kim, H. Franco, L. Neumeyer, Automatic pronunci-\nation scoring of speciﬁc phone segments for language\ninstruction, in: 1997 IEEE International Conference\non Acoustics, Speech, and Signal Processing, IEEE\nComput. Soc. Press, 1997. doi:10.1109/icassp.1997.\n596227.\n[22] H. Franco, L. Neumeyer, M. Ramos, H. Bratt, Au-\ntomatic detection of phone-level mispronunciation for\nlanguage learning, in: Sixth European Conference on\nSpeech Communication and Technology, 1999.\n[23] J. Proen¸ ca, C. Lopes, M. Tjalve, A. Stolcke, S. Can-\ndeias, F. Perdig˜ ao, Detection of mispronunciations and\ndisﬂuencies in children reading aloud, in: Interspeech\n2017, ISCA, 2017, pp. 1437–1441. doi:10.21437/\ninterspeech.2017-1522.\n[24] W. Hu, Y. Qian, F. K. Soong, A new DNN-based\nhigh quality pronunciation evaluation for computer-\naided language learning (CALL)., in: Interspeech, 2013,\npp. 1886–1890.\n[25] J. Cheng, X. Chen, A. Metallinou, Deep neural net-\nwork acoustic models for spoken assessment applica-\ntions, Speech Communication 73 (2015) 14–27. doi:\n10.1016/j.specom.2015.07.006.\n[26] A. Graves, S. Fern´ andez, F. Gomez, J. Schmidhuber,\nConnectionist temporal classiﬁcation: labelling unseg-\nmented sequence data with recurrent neural networks,\nin: Proceedings of the 23rd international conference on\nMachine learning - ICML ’06, ACM Press, 2006, pp.\n369–376. doi:10.1145/1143844.1143891.\n[27] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho,\nY. Bengio, Attention-based models for speech recogni-\ntion, Advances in Neural Information Processing Sys-\ntems 2015-January (2015) 577–585.\n[28] W. Chan, N. Jaitly, Q. Le, O. Vinyals, Listen, attend\nand spell: A neural network for large vocabulary con-\nversational speech recognition, in: 2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), IEEE, IEEE, 2016, pp. 4960–4964.\ndoi:10.1109/icassp.2016.7472621.\n[29] S. Watanabe, T. Hori, S. Kim, J. R. Hershey,\nT. Hayashi, Hybrid CTC/Attention architecture for\nend-to-end speech recognition, IEEE J. Sel. Top. Sig-\nnal Process. 11 (8) (2017) 1240–1253. doi:10.1109/\njstsp.2017.2763455.\n[30] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le,\nR. Salakhutdinov, Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context, arXiv preprint\narXiv:1901.02860.\n[31] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, arXiv preprint arXiv:1810.04805.\n[32] L. Sampaio Ferraz Ribeiro, T. Bui, J. Collomosse,\nM. Ponti, Sketchformer: Transformer-based represen-\ntation for sketched structure, in: 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), IEEE, 2020, pp. 14153–14162. doi:\n10.1109/cvpr42600.2020.01416.\n[33] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kir-\nillov, S. Zagoruyko, End-to-end object detection with\ntransformers, arXiv preprint arXiv:2005.12872.\n[34] T. Okamoto, T. Toda, Y. Shiga, H. Kawai,\nTransformer-based text-to-speech with weighted forced\nattention, in: ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), IEEE, IEEE, 2020, pp. 6729–6733. doi:\n10.1109/icassp40776.2020.9053915.\n[35] N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, Neural speech\n12\nsynthesis with transformer network, in: Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, Vol. 33,\n2019, pp. 6706–6713.\n[36] R. Liu, X. Chen, X. Wen, Voice conversion with trans-\nformer network, in: ICASSP 2020 - 2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), IEEE, IEEE, 2020, pp. 7759–7759.\ndoi:10.1109/icassp40776.2020.9054523.\n[37] A. M. Harrison, W.-K. Lo, X.-j. Qian, H. Meng, Im-\nplementation of an extended recognition network for\nmispronunciation detection and diagnosis in computer-\nassisted pronunciation training, in: International Work-\nshop on Speech and Language Technology in Education,\n2009.\n[38] V. Likic, The needleman-wunsch algorithm for sequence\nalignment, Lecture given at the 7th Melbourne Bioin-\nformatics Course, Bi021 Molecular Science and Biotech-\nnology Institute, University of Melbourne (2008) 1–46.\n[39] C. Chang, First language phonetic drift during second\nlanguage acquisition, Ph.D. thesis (10 2010).\n[40] Y. Jiao, M. Tu, V. Berisha, J. Liss, Accent identiﬁca-\ntion by combining deep neural networks and recurrent\nneural networks trained on long and short term fea-\ntures, in: Interspeech 2016, ISCA, 2016, pp. 2388–2392.\ndoi:10.21437/interspeech.2016-1148.\n[41] M. Tu, A. Grabek, J. Liss, V. Berisha, Investigating\nthe role of l1 in automatic pronunciation evaluation of\nl2 speech, arXiv preprint arXiv:1807.01738.\n[42] K. Cho, B. Van Merri¨ enboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, Y. Bengio, Learn-\ning phrase representations using RNN encoder-decoder\nfor statistical machine translation, arXiv preprint\narXiv:1406.1078.\n[43] Y. zhao, J. Li, X. Wang, Y. Li, The speechtrans-\nformer for large-scale mandarin Chinese speech recog-\nnition, in: ICASSP 2019 - 2019 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), IEEE, IEEE, 2019, pp. 7095–7099. doi:\n10.1109/icassp.2019.8682586.\n[44] D. Povey, A. Ghoshal, G. Boulianne, L. Burget,\nO. Glembek, N. Goel, M. Hannemann, P. Motlicek,\nY. Qian, P. Schwarz, et al., The kaldi speech recognition\ntoolkit, in: IEEE 2011 workshop on automatic speech\nrecognition and understanding, no. CONF, IEEE Signal\nProcessing Society, 2011.\n[45] V. Panayotov, G. Chen, D. Povey, S. Khudanpur,\nLibrispeech: An ASR corpus based on public do-\nmain audio books, in: 2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), IEEE, IEEE, 2015, pp. 5206–5210. doi:\n10.1109/icassp.2015.7178964.\n[46] M. McAuliﬀe, M. Socolof, S. Mihuc, M. Wagner,\nM. Sonderegger, Montreal forced aligner: Trainable\ntext-speech alignment using kaldi, in: Interspeech 2017,\nVol. 2017, ISCA, 2017, pp. 498–502. doi:10.21437/\ninterspeech.2017-1386.\n[47] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary,\nO. Kuchaiev, J. M. Cohen, H. Nguyen, R. T. Gadde,\nJasper: An end-to-end convolutional neural acoustic\nmodel, arXiv preprint arXiv:1904.03288.\n[48] X. Qian, F. K. Soong, H. Meng, Discriminative acous-\ntic model for improving mispronunciation detection\nand diagnosis in computer-aided pronunciation training\n(CAPT), in: Eleventh Annual Conference of the Inter-\nnational Speech Communication Association, 2010.\n[49] E. Eban, M. Schain, A. Mackey, A. Gordon, R. Rifkin,\nG. Elidan, Scalable learning of non-decomposable ob-\njectives, in: Artiﬁcial Intelligence and Statistics, 2017,\npp. 832–840.\n[50] T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar, Focal\nloss for dense object detection, in: 2017 IEEE Interna-\ntional Conference on Computer Vision (ICCV), IEEE,\n2017, pp. 2980–2988. doi:10.1109/iccv.2017.324.\n[51] M. Eskenazi, An overview of spoken language tech-\nnology for education, Speech Communication 51 (10)\n(2009) 832–844. doi:10.1016/j.specom.2009.04.005.\n[52] H. H. Mao, A survey on self-supervised pre-training for\nsequential transfer learning in neural networks, arXiv\npreprint arXiv:2007.00800.\n[53] V. Sanh, T. Wolf, A. M. Rush, Movement prun-\ning: Adaptive sparsity by ﬁne-tuning, arXiv preprint\narXiv:2005.07683.\n13"
}