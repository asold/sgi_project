{
  "title": "fBERT: A Neural Transformer for Identifying Offensive Content",
  "url": "https://openalex.org/W3198943295",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2992940883",
      "name": "Diptanu Sarkar",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2130891196",
      "name": "Marcos Zampieri",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2950654724",
      "name": "Tharindu Ranasinghe",
      "affiliations": [
        "University of Wolverhampton"
      ]
    },
    {
      "id": "https://openalex.org/A4208783028",
      "name": "Alexander Ororbia",
      "affiliations": [
        "Rochester Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2922580172",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3185909895",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3093699284",
    "https://openalex.org/W3000571327",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3087777115",
    "https://openalex.org/W2953646920",
    "https://openalex.org/W3004178028",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W3115245508",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2970636124",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W3140418309",
    "https://openalex.org/W3115903740",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3170547996",
    "https://openalex.org/W2912102236",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953553271",
    "https://openalex.org/W3031939012",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3100941475",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2962993339"
  ],
  "abstract": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance across various NLP tasks including the identification of offensive language and hate speech, an important problem in social media. In this paper, we present fBERT, a BERT model retrained on SOLID, the largest English offensive language identification corpus available with over 1.4 million offensive instances. We evaluate fBERT's performance on identifying offensive content on multiple English datasets and we test several thresholds for selecting instances from SOLID. The fBERT model will be made freely available to the community.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1792–1798\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n1792\nfBERT: A Neural Transformer for Identifying Offensive Content\nDiptanu Sarkar1, Marcos Zampieri1, Tharindu Ranasinghe2, Alexander Ororbia1\n1Rochester Institute of Technology, USA\n2University of Wolverhampton, UK\nds9297@rit.edu\nAbstract\nTransformer-based models such as BERT, XL-\nNET, and XLM-R have achieved state-of-the-\nart performance across various NLP tasks\nincluding the identiﬁcation of offensive lan-\nguage and hate speech, an important problem\nin social media. In this paper, we present\nfBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identi-\nﬁcation corpus available with over 1.4 million\noffensive instances. We evaluate fBERT’s per-\nformance on identifying offensive content on\nmultiple English datasets and we test several\nthresholds for selecting instances from SOLID.\nThe fBERT model will be made freely avail-\nable to the community.\n1 Introduction\nTo cope with the spread of offensive content and\nhate speech online, researchers have worked to\ndevelop automatic methods to detect such posts au-\ntomatically. Early efforts included methods that\nused various linguistic features in tandem with\nlinear classiﬁers (Malmasi and Zampieri, 2017)\nwhile, more recently, deep neural networks (DNNs)\n(Ranasinghe et al., 2019), transfer learning (Wiede-\nmann et al., 2020; Abu Farha and Magdy, 2020),\nand pre-trained language models (Liu et al., 2019a;\nRanasinghe and Zampieri, 2020, 2021) have led to\neven further advances. As evidenced in recent com-\npetitions, the performance of these models varies\nwith the sub-task that they are designed to address\nas well as the datasets used to train them. For exam-\nple, classical statistical learning models such as the\nsupport vector machine (SVM) have outperformed\nneural transformers in hate speech detection at Hat-\nEval 2019 (Basile et al., 2019) and in aggression\ndetection at TRAC 2018 (Kumar et al., 2018). How-\never, for both of these tasks in OffensEval 2019 and\n2020 (Zampieri et al., 2019b, 2020), which focused\non the identiﬁcation of more general offensive lan-\nguage identiﬁcation, pre-trained transformer-based\nmodels such as BERT (Devlin et al., 2019) out-\nperformed other neural architectures and statistical\nlearning methods.\nThe introduction of representations learned\nthrough the bidirectional encoding inherent to neu-\nral transformers (BERT) (Devlin et al., 2019) has\nbeen driven much progress in areas in NLP such\nas language understanding, named entity recogni-\ntion, and text classiﬁcation. The base model is pre-\ntrained on a large English corpus, e.g., Wikipedia,\nBookCorpus (Zhu et al., 2015), using unsuper-\nvised masked language modeling and next sentence\nprediction objectives to adjust the model weights.\nVarious other transformer-based models have also\nbeen introduced including RoBERTa (Liu et al.,\n2019b), XLNet (Yang et al., 2019), XLM-R (Con-\nneau et al., 2019). All of these models, however,\nare trained on general-purpose corpora for better\nlanguage understanding, generally lacking domain-\nspeciﬁc knowledge. To cope with this limitation,\nmore recently, domain-speciﬁc models have been\ntrained and/or ﬁne tuned to different domains such\nas ﬁnance (FinBERT) (Araci, 2019), law (LEGAL-\nBERT) (Chalkidis et al., 2020), scientiﬁc texts\n(SciBERT) (Beltagy et al., 2019), and microblog-\nging (BerTweet) (Nguyen et al., 2020).\nCaselli et al. (2021) recently released Hate-\nBERT, a BERT transformer model for abusive lan-\nguage detection trained on the Reddit Abusive\nLanguage English dataset (RAL-E). HateBERT\nachieves competitive performance on a few bench-\nmark datasets but relies heavily on manually anno-\ntated labels. Moreover, HateBERT was trained on a\ntask-speciﬁc dataset (aggression) instead of a more\ngeneral dataset that encompasses multiple types\nof offensive language (e.g. hate speech, cyberbul-\nlying, profanity) like the popular OLID (Zampieri\net al., 2019a) used in OfffensEval 2019 at SemEval.\nTo address this gap, in this study, we present\nfBERT, a pre-trained BERT model trained on\nSOLID (Rosenthal et al., 2021), a recently released\n1793\nlarge dataset crated using OLID’s general anno-\ntation model but using semi-supervised learning\ninstead of manually annotated labels. SOLID con-\ntains over 1.4 million English tweets with offensive\nscores greater than 0.5. We show that the proposed\nfBERT outperforms both a plain BERT implemen-\ntation and HateBERT on various offensive and hate\nspeech detection tasks.\nThe contributions of this paper are as follows:\n1. An empirical evaluation of transformer-based,\nsemi-supervised learning techniques applied\nto offensive language identiﬁcation with the\nclear potential application to many other text\nclassiﬁcation tasks.\n2. A comprehensive evaluation of several BERT-\nbased strategies and data selection thresholds\nfor offensive language identiﬁcation across\nmultiple datasets.\n3. The release of fBERT, a high-performing,\nstate-of-the-art pre-trained model for offen-\nsive language identiﬁcation.\n2 Related Work\nThe use of large pre-trained transformer models\nhas become widespread in NLP. This includes sev-\neral recently developed offensive language iden-\ntiﬁcation systems based on transformer architec-\ntures such as BERT (Devlin et al., 2019). These\nsystems have achieved top performance in pop-\nular competitions such as HASOC 2019 (Mandl\net al., 2019), HatEval 2019 (Basile et al., 2019),\nOffensEval 2019 and 2020 (Zampieri et al., 2019b,\n2020), and TRAC 2020 (Kumar et al., 2020). The\ngreat performance obtained by these systems pro-\nvides further evidence that pre-trained transformer\nmodels are a good ﬁt for the kind of semantic un-\nderstanding required when identifying offensive\ncontent online.\nMost of the top systems submitted to the afore-\nmentioned competitions (Ranasinghe et al., 2019;\nWiedemann et al., 2020; Liu et al., 2019a), how-\never, use models pre-trained on standard contem-\nporary texts. User generated content and offensive\nlanguage online, however, contain its own set of\ndistinctive features that models trained on standard\ntexts may fail to represent. Therefore, ﬁne-tuning\npre-trained models to this challenging domain is\na promising but under explored research direction.\nTo the best of our knowledge, a recent ﬁrst attempt\nto ﬁne-tune a BERT model to deal with offensive\nlanguage online, HateBERT (Caselli et al., 2021),\nshows promising results for English on multiple\ndatasets. In this paper, we address some of the\nlimitations of HateBERT, discussed in the intro-\nduction of this paper, and present fBERT, a new\nBERT-based offensive language model made freely\navailable to the research community.\n3 Data\nThe limited size of existing datasets has been a bot-\ntleneck for offensive language identiﬁcation. OLID\n(Zampieri et al., 2019a), the dataset used in Offen-\nsEval 2019 and arguably the most popular dataset\nfor this task, contains only 14, 100 tweets. OLID\nis annotated using a hierarchical annotation taxon-\nomy and, as a result, only a sub-set of the corpus is\nannotated in the lower levels of the taxonomy, i.e.,\nonly a few hundred instances.\nMore recently, following the OLID taxonomy,\nRosenthal et al. (2021) released a large-scale offen-\nsive language identiﬁcation dataset (SOLID) with\nover 9 million English tweets. The data is collected\nusing the Twitter streaming API. The annotations\ninclude labels learned using semi-supervised meth-\nods. One important difference between SOLID\nand OLID is that SOLID is collected using random\nseeds, which has been shown to decrease topic bias\ncompared to the target keywords used in OLID. All\nthe usernames and URLs are replaced with place-\nholders and tweets with less than two words or 18\ncharacters were discarded.\nFor retraining fBERT, we have selected over1.4\nmillion offensive instances from SOLID. We con-\nsidered multiple offensive score thresholds from\nthe SOLID dataset including all instances with\nscores between 0.5 and 1.0 arranged in ﬁve bins\nwith 0.1 increments. The number of instances in\neach range is available in Table 1. We did not con-\nsider the range 0.9 −1.0 given that only a very\nsmall number of instances (2, 771) were in this bin\n.\nThreshold Instances\n0.5 - 1.0 1,446,580\n0.6 - 1.0 1,040,525\n0.7 - 1.0 700,719\n0.8 - 1.0 348,038\n0.9 - 1.0 2,771\nTable 1: Offensive instances from the SOLID dataset,\norganized according to threshold.\n1794\n4 Model Architecture\n4.1 Input Representation\nWe take the sentence input and tokenize it using\nWordPiece embeddings (Wu et al., 2016) with a\n30, 000 token vocabulary as described in (Devlin\net al., 2019). The tokenized input is represented as:\nX = (x[CLS], x1, x2, ...,xn, x[SEP ]) (1)\nwhere xt is the V -dimensional one-hot encoding\nof the t-th token in a sequence of n symbols (vo-\ncabulary of size V ). The tokenized input is then\nprocessed via Bert(X) to generate contextualized\nembeddings as follows:\nH = Bert(X) (2)\nH = (h[CLS], h1, h2, ...,hn, h[SEP ]) (3)\nwhere ht is the d-dimensional embedding for the\nt-th token xt (resulting in n embeddings).\n4.2 Retraining Procedure\nThe goal of the study is to adapt the BERT model\nfor social media aggression detection tasks. We\nutilized a BERT base (uncased) model that consists\nof 12 bidirectional transformers encoders with 768\nhidden layers and 12 self-attention heads. To use\nthe general understanding of the English language\nand context, we initialize the transformer with pre-\ntrained weights1. We used over 1.4 million offen-\nsive texts from the SOLID dataset to retrain the\nmodel. No cleaning was applied to preserve the in-\ncoherent composition of social media posts, such as\nthe excessive use of mentions, emojis, or hashtags.\nWe retrained the model using the masked language\nmodeling objective to adapt the bidirectional repre-\nsentations of social media offensive language.\nMasked Language Modeling (MLM)In MLM,\nwe randomly mask a percentage of tokens and pre-\ndict the masked inputs. As prescribed in the orig-\ninal BERT implementation, we randomly select\n15% of the total tokens for replacement, 80% of\nthe selected tokens are replaced with [MASK ],\n10% are substituted with a random token chosen\nfrom the vocabulary, and10% remain unchanged.\nThe hidden vectors with masked tokens are fed into\na softmax activation function to generate a proba-\nbility distribution over each (masked) token xt:\np(xt|H) =softmax(W ·ht + b) (4)\n1BERT Pre-trained weights: https://github.com/\ngoogle-research/bert\nwhere ·is matrix multiplication, W ∈Rd×V , and\nb ∈RV ×1. The model is trained to predict the orig-\ninal token by minimizing the Catergorical cross-\nentropy objective as follows:\nL= −\nn∑\nt=1\nmt\n∑\nv\n(\nxt ⊗log(p(xt|H))\n)\n[v] (5)\nwhere mt is the binary scalar applied at time\nstep t (1 if the word is masked, 0 otherwise).\n[v] retrieves/indexes the vth item in the vector\nx ⊗log(p(x|H) and ⊗indicates element-wise\nmultiplication. A schematic representation of the\nBERT masked language model is presented in Fig-\nure 1.\nRetraining Setup We trained the resulting\nfBERT for 25 epochs using the MLM objective\nwith 0.15 probability to randomly mask tokens in\nthe input. The language model is trained with a\nbatch size of 32 and a 512 maximum token length\nusing the Adam optimizer with a learning rate of\n5e −5. The training time took 5 days on a single\nNvidia V100 GPU.\nFigure 1: A schematic representation of the fBERT\nmasked language model (the re-trained/tuned BERT).\n5 Experiments\nTo determine the effectiveness and portability of\nthe trained fBERT, we conducted a series of ex-\nperiments using benchmark datasets and compared\nour model with a general-purpose BERT model.\nWe used the same set of conﬁgurations for all the\ndatasets evaluated in order to ensure consistency\nbetween all the experiments. This also provides\na good starting conﬁguration for researchers who\nintend to use fBERT on a new dataset.\n1795\nWe used a batch-size of eight, Adam optimiser\nwith learning rate 1e−4, and a linear learning rate\nwarm-up over 10% of the training data. During the\ntraining process, the parameters of the transformer\nmodel, as well as the parameters of the subsequent\nlayers, were updated. The models were trained\nusing only training data. Furthermore, they were\nevaluated while training using an evaluation set that\nhad one ﬁfth of the rows in training data. We per-\nformed early stopping if the evaluation loss did not\nimprove over ten evaluation steps. All the models\nwere trained for three epochs. The rest of the pa-\nrameters are shown in Table 2. These experiments\nwere also conducted in a Nvidia V100 GPU.\nParameter Value\nlearning rate 1e-4\nadam epsilon 1e-8\nwarmup ratio 0.1\nwarmup steps 0\nmax grad norm 1.0\nmax seq. length 140\ngradient accumulation steps 1\nTable 2: Parameter Speciﬁcations.\nHatEval 2019 In the SemEval 2019, HatEval\n(Basile et al., 2019) introduced the challenge of\ndetecting multilingual hate speech against women\nand immigrants. The dataset for the task is col-\nlected from Twitter in both English and Spanish.\nIn this work, we used only the English dataset\ncomprised of 9, 000 training instances with 4, 177\nhateful tweets. The development (dev) and test\nsets contain 1, 000 (123 instances are hateful) and\n3, 000 examples (1, 380 instances are hateful) re-\nspectively. In terms of pre-processing, we removed\nextra whitespaces, usernames and URLs were re-\nplaced with placeholders, the Emoji2 package was\nused to convert the emojis to text, and the Word\nSegmentation3 package was used to segment the\nwords into hashtags. We applied the same pre-\nprocessing steps for all models to compare the test\nset macro F1 score.\nOLID We use OLID, the ofﬁcial dataset for Of-\nfensEval 2019 (Zampieri et al., 2019b), one of the\nthe most popular offensive language identiﬁcation\nshared tasks. The dataset has 13, 240 training and\n2Emoji Package: https://pypi.org/project/\nemoji/\n3Word Segmentation Package: https://pypi.org/\nproject/wordsegmentation/\n860 test instances. There are 4, 400 and 240 of-\nfensive posts in the training and test dataset, re-\nspectively. For the experiment, we chose sub-task\nA, a binary classiﬁcation task between offensive\nand non-offensive posts. We used 10% of the train-\ning data as development data and performed pre-\nprocessing and cleaning steps as described by Liu\net al. (2019a). We trained fBERT for the offensive\nlanguage detection task and compared its perfor-\nmance with other language models using the macro\nF1 score.\nHate Speech and Offensive Language Detection\n(HS & O) In ﬁne-grain aggression detection,\nclassifying offensive language and hate speech\nis challenging. Hate speech contains explicit in-\nstances targeted towards a speciﬁc group of peo-\nple intended to degrade or insult. Davidson et al.\n(2017) compiled a 24, 783 English tweets dataset\nannotated with one of three labels – “hate speech”,\n“only offensive”, and “neither”. The dataset con-\ntains 1, 430 hate speech, 19, 190 only offensive,\nand 4, 163 instances that are neither. We further\nsplit the dataset into training, dev, and test sets in\na 3:1:1 ratio. We applied the same preprocessing\nsteps we applied to the HatEval 2019 dataset.\n6 Results\nWe ﬁrst present the results for the SOLID data\nselection thresholds in Table 3 in terms of F1 Macro.\nFor the three datasets tested, the 0.5 - 1.0 threshold,\nwhich includes the largest number of instances,\nyielded the best performance.\nDatasets\nScores HatEval OLID HS & O\n0.5 - 1.0 0.596 0.813 0.878\n0.6 - 1.0 0.562 0.808 0.871\n0.7 - 1.0 0.550 0.802 0.867\n0.8 - 1.0 0.554 0.801 0.865\nTable 3: Macro F 1 scores for different SOLID thresh-\nold score values.\nWe then compare the performance of fBERT with\nBERT and HateBERT. In the HatEval Sub-task A,\nwe see that fBERT has outperformed BERT by in-\ncreasing the test macro F1 score by over 23%. This\nempirically demonstrates the advantage and gen-\neralization power of our domain-speciﬁc retrained\nBERT model. The best model (Indurthi et al., 2019)\nin this task used an SVM model with a radial ba-\nsis kernel, exploiting sentence embeddings from\n1796\nGoogle’s Universal Sentence Encoder as features.\nThe results are shown in Table 4.\nThe fBERT model also performs better than the\ngeneric BERT and abusive language HateBERT in\nOffensEval Sub-task A, achieving a test set macro\nF1 score of 0.8132. We observe that the fBERT\nis also highly effective in ﬁne-grain offensive and\nhate speech detection, obtaining a 10% increase in\nthe F1 score.\nDataset Model Macro F1\nfBERT 0.596\nHatEval HateBERT 0.525\nBERT 0.483\nfBERT 0.813\nOLID HateBERT 0.801\nBERT 0.794\nfBERT 0.878\nHS & O HateBERT 0.846\nBERT 0.806\nTable 4: The test set macro F 1 scores for all datasets\nand models. Results are ordered by performance. Best\nresults are shown in bold font.\nFinally, as observed in the experimental results\npresented above, we observe that fBERT has out-\nperformed the abusive language HateBERT model\nin all of the experiments. The proposed fBERT has\nalso performed efﬁciently in all the aggression de-\ntection tasks. This validates the effectiveness of the\nproposed domain-speciﬁc transformer model for\noffensive and hateful language classiﬁcation tasks.\nThe proposed fBERT model is effective across dif-\nferent datasets and objectives, providing a powerful\nmodel to be used for hateful/offensive content iden-\ntiﬁcation.\n7 Conclusion\nOver the years, neural transformer models have out-\nperformed previous state-of-the-art deep learning\nmodels across various NLP tasks including offen-\nsive and hate speech detection tasks. Nevertheless,\nthese transformers are usually trained on general\ncorpora which lack tweet and offensive language-\nspeciﬁc cues. Previous studies have shown that\ndomain-speciﬁc ﬁne-tuning or retraining of mod-\nels before attempting to solve downstream tasks\ncan lead to excellent results in multiple domains.\nAs discussed in this paper, ﬁne-tuning/retraining\na complex models to identify offensive language\nhas not been substantially explored before and we\naddress this gap by proposing fBERT, abert-base-\nuncased model that has been learned using over\n1.4 million offensive instances from the SOLID\ndataset. The shifted fBERT model better incorpo-\nrates domain-speciﬁc offensive language and social\nmedia features. The fBERT model achieves better\nresults in both OffensEval and HatEval tasks and\nin the HS & O dataset over BERT and HateBERT.\nIn future work, we would like to investigate the\nperformance of fBERT both at the post- and token-\nlevel identiﬁcation stages. Furthermore, we will\nexpand fBERT to multiple languages. Since our\napproach is based on a semi-supervised dataset, it\nis easily expandable to other languages as well.\nWe plan to extend this process to other trans-\nformer models such as XLNET (Yang et al., 2019),\nRoBERTa (Liu et al., 2019b) and ALBERT (Lan\net al., 2020). Finally, fBERT is publicly available\non Hugging Face model hub (Wolf et al., 2020).4\nEthics Statement\nfBERT is essentially a BERT model for offensive\nlanguage identiﬁcation which is trained on multi-\nple publicly available datasets. We used multiple\ndatasets referenced in this paper which were previ-\nously collected and annotated. No new data collec-\ntion has been carried out as part of this work. We\nhave not collected or processed writers’/users’ in-\nformation nor have we carried out any form of user\nproﬁling protecting users’ privacy and identity.\nWe understand that every dataset is subject to\nintrinsic bias and that computational models will in-\nevitably learn biased information from any dataset.\nWe believe that fBERT will help coping with bi-\nases in datasets and models as it features a freely\navailable BERT model that other researchers can\nuse to train new offensive language identiﬁcation\nmodels on other datasets.\nAcknowledgments\nWe would like to thank the HatEval and OffensE-\nval shared task organizers for making the datasets\nused in this paper available. We further thank the\nanonymous EMNLP reviewers for their insightful\nfeedback.\nThis research was partially supported by a seed\nfund award sponsored by RIT’s Global Cybersecu-\nrity Institute (GCI).\n4fBERT at HuggingFace: https://huggingface.\nco/diptanu/fBERT\n1797\nReferences\nIbrahim Abu Farha and Walid Magdy. 2020. Multi-\ntask learning for Arabic offensive language and hate-\nspeech detection. In Proceedings of OSCAT.\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models. arXiv preprint\narXiv:1908.10063.\nValerio Basile, Cristina Bosco, Elisabetta Fersini, Deb-\nora Nozza, Viviana Patti, Francisco Manuel Rangel\nPardo, Paolo Rosso, and Manuela Sanguinetti. 2019.\nSemeval-2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in twitter. In\nProceedings of SemEval.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of EMNLP.\nTommaso Caselli, Valerio Basile, Jelena Mitrovi´c, and\nMichael Granitzer. 2021. Hatebert: Retraining bert\nfor abusive language detection in english. In Pro-\nceedings of WOAH.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The Muppets straight out of\nLaw School. In Findings of the ACL.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL.\nThomas Davidson, Dana Warmsley, Michael W. Macy,\nand Ingmar Weber. 2017. Automated hate speech\ndetection and the problem of offensive language. In\nProceedings of ICWSM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nVijayasaradhi Indurthi, Bakhtiyar Syed, Manish Shri-\nvastava, Nikhil Chakravartula, Manish Gupta, and\nVasudeva Varma. 2019. FERMI at SemEval-2019\ntask 5: Using sentence embeddings to identify hate\nspeech against immigrants and women in Twitter. In\nProceedings of SemEval.\nRitesh Kumar, Atul Kr Ojha, Shervin Malmasi, and\nMarcos Zampieri. 2018. Benchmarking Aggression\nIdentiﬁcation in Social Media. In Proceedings of\nTRAC.\nRitesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and\nMarcos Zampieri. 2020. Evaluating Aggression\nIdentiﬁcation in Social Media. In Proceedings of\nTRAC.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nICLR.\nPing Liu, Wen Li, and Liang Zou. 2019a. NULI\nat SemEval-2019 task 6: Transfer learning for of-\nfensive language detection using bidirectional trans-\nformers. In Proceedings of SemEval.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nShervin Malmasi and Marcos Zampieri. 2017. Detect-\ning Hate Speech in Social Media. In Proceedings of\nRANLP.\nThomas Mandl, Sandip Modha, Prasenjit Majumder,\nDaksh Patel, Mohana Dave, Chintak Mandlia, and\nAditya Patel. 2019. Overview of the hasoc track at\nﬁre 2019: Hate speech and offensive content identi-\nﬁcation in indo-european languages. In Proceedings\nof FIRE.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model for\nEnglish tweets. In Proceedings of EMNLP.\nTharindu Ranasinghe and Marcos Zampieri. 2020.\nMultilingual Offensive Language Identiﬁcation with\nCross-lingual Embeddings. In Proceedings of\nEMNLP.\nTharindu Ranasinghe and Marcos Zampieri. 2021.\nMUDES: Multilingual Detection of Offensive Spans.\nIn Proceedings of NAACL.\nTharindu Ranasinghe, Marcos Zampieri, and Hansi\nHettiarachchi. 2019. Brums at hasoc 2019: Deep\nlearning models for multilingual hate speech and of-\nfensive language identiﬁcation. In Proceedings of\nFIRE.\nSara Rosenthal, Pepa Atanasova, Georgi Karadzhov,\nMarcos Zampieri, and Preslav Nakov. 2021. SOLID:\nA Large-Scale Weakly Supervised Dataset for Of-\nfensive Language Identiﬁcation. In Findings of the\nACL.\nGregor Wiedemann, Seid Muhie Yimam, and Chris\nBiemann. 2020. UHH-LT at SemEval-2020 task 12:\nFine-tuning of pre-trained transformer networks for\noffensive language detection. In Proceedings of Se-\nmEval.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\n1798\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of EMNLP.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized Autoregressive Pretrain-\ning for Language Understanding. In Proceedings of\nNeurIPS.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019a. Predicting the type and target of offensive\nposts in social media. In Proceedings of NAACL.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019b. SemEval-2019 Task 6: Identifying and Cat-\negorizing Offensive Language in Social Media (Of-\nfensEval). In Proceedings of SemEval.\nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa\nAtanasova, Georgi Karadzhov, Hamdy Mubarak,\nLeon Derczynski, Zeses Pitenis, and Ça˘grı Çöltekin.\n2020. SemEval-2020 Task 12: Multilingual Offen-\nsive Language Identiﬁcation in Social Media (Offen-\nsEval 2020). In Proceedings of SemEval.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of ICCV.",
  "topic": "Offensive",
  "concepts": [
    {
      "name": "Offensive",
      "score": 0.9888639450073242
    },
    {
      "name": "Transformer",
      "score": 0.7767704725265503
    },
    {
      "name": "Computer science",
      "score": 0.7184948921203613
    },
    {
      "name": "Artificial intelligence",
      "score": 0.490278035402298
    },
    {
      "name": "Natural language processing",
      "score": 0.4721239507198334
    },
    {
      "name": "Identification (biology)",
      "score": 0.46374592185020447
    },
    {
      "name": "Language identification",
      "score": 0.4601259231567383
    },
    {
      "name": "Language model",
      "score": 0.45610079169273376
    },
    {
      "name": "Speech recognition",
      "score": 0.3855987787246704
    },
    {
      "name": "Machine learning",
      "score": 0.3456844389438629
    },
    {
      "name": "Natural language",
      "score": 0.1843588650226593
    },
    {
      "name": "Engineering",
      "score": 0.12890705466270447
    },
    {
      "name": "Voltage",
      "score": 0.07698675990104675
    },
    {
      "name": "Operations research",
      "score": 0.05857780575752258
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155173764",
      "name": "Rochester Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I119664326",
      "name": "University of Wolverhampton",
      "country": "GB"
    }
  ]
}