{
  "title": "Video action recognition collaborative learning with dynamics via PSO-ConvNet Transformer",
  "url": "https://openalex.org/W4386457940",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2475420252",
      "name": "Nguyen Huu Phong",
      "affiliations": [
        "University of Coimbra"
      ]
    },
    {
      "id": null,
      "name": "Ribeiro, Bernardete M.",
      "affiliations": [
        "University of Coimbra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963795951",
    "https://openalex.org/W3041107669",
    "https://openalex.org/W3182910206",
    "https://openalex.org/W3015309989",
    "https://openalex.org/W2184544926",
    "https://openalex.org/W2106996050",
    "https://openalex.org/W2779250840",
    "https://openalex.org/W1486903288",
    "https://openalex.org/W2126574503",
    "https://openalex.org/W2105101328",
    "https://openalex.org/W2146634731",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2235034809",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W1923404803",
    "https://openalex.org/W2769581371",
    "https://openalex.org/W3107320732",
    "https://openalex.org/W3177141386",
    "https://openalex.org/W2920492823",
    "https://openalex.org/W2963526497",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4292793955",
    "https://openalex.org/W2293983223",
    "https://openalex.org/W6734593296",
    "https://openalex.org/W6764013109",
    "https://openalex.org/W6761066626",
    "https://openalex.org/W2152195021",
    "https://openalex.org/W2914657652",
    "https://openalex.org/W3083664160",
    "https://openalex.org/W2006027806",
    "https://openalex.org/W2013885787",
    "https://openalex.org/W4220980933",
    "https://openalex.org/W4281752744",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2907214745",
    "https://openalex.org/W3175528717",
    "https://openalex.org/W2981385151",
    "https://openalex.org/W4283030109",
    "https://openalex.org/W3107322323",
    "https://openalex.org/W4306168147",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214516465",
    "https://openalex.org/W3160655852",
    "https://openalex.org/W2015861736",
    "https://openalex.org/W6847397134",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3010874390",
    "https://openalex.org/W2948242301",
    "https://openalex.org/W2963814513",
    "https://openalex.org/W3018953709",
    "https://openalex.org/W3176780013",
    "https://openalex.org/W3214993537",
    "https://openalex.org/W4312797714",
    "https://openalex.org/W4312799843",
    "https://openalex.org/W2508429489"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports\nVideo action recognition \ncollaborative learning \nwith dynamics via PSO‑ConvNet \nTransformer\nHuu Phong Nguyen * & Bernardete Ribeiro \nRecognizing human actions in video sequences, known as Human Action Recognition (HAR), is a \nchallenging task in pattern recognition. While Convolutional Neural Networks (ConvNets) have shown \nremarkable success in image recognition, they are not always directly applicable to HAR, as temporal \nfeatures are critical for accurate classification. In this paper, we propose a novel dynamic PSO‑ConvNet \nmodel for learning actions in videos, building on our recent work in image recognition. Our approach \nleverages a framework where the weight vector of each neural network represents the position of \na particle in phase space, and particles share their current weight vectors and gradient estimates \nof the Loss function. To extend our approach to video, we integrate ConvNets with state‑of‑the‑art \ntemporal methods such as Transformer and Recurrent Neural Networks. Our experimental results on \nthe UCF‑101 dataset demonstrate substantial improvements of up to 9% in accuracy, which confirms \nthe effectiveness of our proposed method. In addition, we conducted experiments on larger and more \nvariety of datasets including Kinetics‑400 and HMDB‑51 and obtained preference for Collaborative \nLearning in comparison with Non‑Collaborative Learning (Individual Learning). Overall, our dynamic \nPSO‑ConvNet model provides a promising direction for improving HAR by better capturing the spatio‑\ntemporal dynamics of human actions in videos. The code is available at https:// github. com/ leonl ha/ \nVideo‑ Action‑ Recog nition‑ Colla borat ive‑ Learn ing‑ with‑ Dynam ics‑ via‑ PSO‑ ConvN et‑ Trans former.\nList of symbols\nX  Input frame in video sequences for ConvNet\nO i  Output for layer ith\nfi  Weight operation for convolution, pooling or fully connected layers at layer ith\ngi  Activation function at layer ith\nxt  Input sequence of RNN at time step t\nW h, W x, b, σ  Weight matrices, bias, sigmoid function\nht  Hidden cell state at time step t\nyt, ˆyt  Output of a cell at time step t\nft , it , o t , ct , c\n′\nt  Forget gate, input gate, output gate and cell states at time step t\nxn (t), vn (t)  Position and velocity vector of particle n at time t\nφ(n) (t), ψ(n) (t)  Intermediate position and intermediate velocity of particle n at time t\nP n (t)  Best position visited up until time t by particle n\nP n\ng (t)  Best position across all previous positions of the particle n  jointly with its nearest-neighbors \nup until time t\nL  Loss function\nc,c1 ,c2  Accelerator coefficients\nr (t)  Random uniform within the interval [0,1]\nM, β  Constants\nHuman action recognition plays a vital role for distinguishing a particular behavior of interest in the video. It has \ncritical applications including visual surveillance for detection of suspicious human activities to prevent the fatal \n accidents1,2, automation-based driving to sense and predict human behavior for safe  navigation3,4. In addition, \nOPEN\nCISUC, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal. *email: phong@dei.uc.pt\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nthere are large amount of non-trivial applications such as human–machine  interaction5,6, video  retrieval7, crowd \nscene  analysis8 and identity  recognition9.\nIn the early days, the majority of research in Human Activity Recognition was conducted using hand-crafted \n methods10–12. However, as deep learning technology evolved and gained increasing recognition in the research \ncommunity, a multitude of new techniques have been proposed, achieving remarkable results.\nAction recognition preserves a similar property of image recognition since both of the fields handle visual \ncontents. In addition, action recognition classifies not only still images but also dynamics temporal informa-\ntion from the sequence of images. Built on these intrinsic characteristics, action recognition’s methods can be \ngrouped into two main approaches namely recurrent neural networks (RNN) based approach and 3-D ConvNet \nbased approach. Besides of the main ones, there are other methods that utilize the content from both spatial and \ntemporal and coined the name two-stream 2-D ConvNet based  approach13.\nInitially, action recognition was viewed as a natural extension of image recognition, and spatial features \nfrom still frames could be extracted using ConvNet, which is one of the most efficient techniques in the image \nrecognition field. However, traditional ConvNets are only capable of processing a single 2-D image at a time. To \nexpand to multiple 2-D images, the neural network architecture needs to be re-designed, including adding an \nextra dimension to operations such as convolution and pooling to accommodate 3-D images. Examples of such \ntechniques include  C3D14,  I3D15,  R3D16,  S3D17,  T3D18,  LTC19, among others\nSimilarly, since a video primarily consists of a temporal sequence, techniques for sequential data, such as \nRecurrent Neural Networks and specifically Long Short Term Memory, can be utilized to analyze the temporal \ninformation. Despite the larger size of images, feature extraction is often employed. Long-term Recurrent Con-\nvolutional Networks (LRCN) 20 and Beyond-Short-Snippets 21 were among the first attempts to extract feature \nmaps from 2-D ConvNets and integrate them with LSTMs to make video predictions. Other works have adopted \nbi-directional  LSTMs22,23, which are composed of two separate LSTMs, to explore both forward and backward \ntemporal information.\nTo further improve performance, other researchers argue that videos usually contain repetitive frames or \neven hard-to-classify ones which makes the computation expensive. By selecting relevant frames, it can help to \nimprove action recognition performance both in terms of efficiency and  accuracy24. A similar concept based \non attention mechanisms is the main focus in recent researches to boost overall performance of the ConvNet-\nLSTM  frameworks25,26.\nWhile RNNs are superior in the field, they process data sequentially, meaning that information flows from one \nstate to the next, hindering the ability to speed up training in parallel and causing the architectures to become \nlarger in size. These issues limit the application of RNNs to longer sequences. In light of these challenges, a new \napproach, the Transformer,  emerged27–31.\nThere has been a rapid advancement in action recognition in recent years, from 3-D ConvNets to 2-D Con-\nvNets-LSTM, two-stream ConvNets, and more recently, Transformers. While these advancements have brought \nmany benefits, they have also created a critical issue as previous techniques are unable to keep up with the rapidly \nchanging pace. Although techniques such as evolutionary computation offer a crucial mechanism for architecture \nsearch in image recognition, and swarm intelligence provides a straightforward method to improve performance, \nthey remain largely unexplored in the realm of action recognition.\nIn our recent  research32, we developed a dynamic Particle Swarm Optimization (PSO) framework for image \nclassification. In this framework, each particle navigates the landscape, exchanging information with neighboring \nparticles about its current estimate of the geometry (such as the gradient of the Loss function) and its position. \nThe overall goal of this framework is to create a distributed, collaborative algorithm that improves the optimiza-\ntion performance by guiding some of the particles up to the best minimum of the loss function. We extend this \nframework to action recognition by incorporating state-of-the-art methods for temporal data (Transformer and \nRNN) with the ConvNet module in an end-to-end training setup.\nIn detail, we have made the following improvements compared to our previous publication.\nWe have supplemented a more comprehensive review of the literature on Human Action Recognition. We \nhave implemented the following enhancements and additions to our work: \n(1) We have introduced an improved and novel network architecture that extends a PSO-ConvNet to a PSO-\nConvNet Transformer (or PSO-ConvNet RNN) in an end-to-end fashion.\n(2) We have expanded the scope of Collaborative Learning as a broader concept beyond its original application \nin image classification to include action recognition.\n(3) We have conducted additional experiments on challenging datasets to validate the effectiveness of the \nmodified model.\nThese improvements and additions contribute significantly to the overall strength and novelty of our research.\nThe rest of the article is organized as follows: In Sect.  2, we discuss relevant approaches in applying Deep \nLearning and Swarm Intelligence to HAR. In addition, the proposed methods including Collaborative Learning \nwith Dynamic Neural Networks and ConvNet Transformer architecture as well as ConvNet RNN model are \nintroduced in Sects. 3.1, 3.2 and  3.3, respectively. The results of experiments, the extension of the experiments \nand discussions are presented in Sects. 4,  5 and 6. Finally, we conclude our work in Sect. 7.\nRelated works\nIn recent years, deep learning (DL) has greatly succeed in computer vision fields, e.g., object detection, image \nclassification and action  recognition24,30,33. One consequence of this success has been a sharp increase in the \nnumber of investments in searching for good neural network architectures. An emerging promising approach \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nis changing from the manual design to automatic Neural Architecture Search (NAS). As an essential part of \nautomated machine learning, NAS automatically generates neural networks which have led to state-of-the-art \n results34–36. Among various approaches for NAS already present in the literature, evolutionary search stands out \nas one of the most remarkable methods. For example, beginning with just one layer of neural network, the model \ndevelops into a competitive architecture that outperforms contemporary  counterparts34. As a result, the efficacy \nof the their proposed classification system for HAR on UCF-50 dataset was  demonstrated33 by initializing the \nweights of a convolutional neural network classifier based on solutions generated from genetic algorithms (GA).\nIn addition to Genetic Algorithms, Particle Swarm Optimization—a population-based stochastic search \nmethod influenced by the social behavior of flocking birds and schooling fish—has proven to be an efficient \ntechnique for feature  selection37,38. A novel approach that combines a modified Particle Swarm Optimization with \nBack-Propagation was put forth for image recognition, by adjusting the inertia weight, acceleration parameters, \nand  velocity39. This fusion allows for dynamic and adaptive tuning of the parameters between global and local \nsearch capability, and promotes diversity within the swarm. In catfish particle swarm optimization, the particle \nwith the worst fitness is introduced into the search space when the fitness of the global best particle has not \nimproved after a number of consecutive  iterations40. Moreover, a PSO based multi-objective for discriminative \nfeature selection was introduced to enhance classification  problems41.\nThere have been several efforts to apply swarm intelligence to action recognition from video. One such \napproach employs a combination of binary histogram, Harris corner points, and wavelet coefficients as features \nextracted from the spatiotemporal volume of the video  sequence42. To minimize computational complexity, the \nfeature space is reduced through the use of PSO with a multi-objective fitness function.\nFurthermore, another approach combining Deep Learning and swarm intelligence-based metaheuristics for \nHuman Action Recognition was  proposed43. Here, four different types of features extracted from skeletal data—\nDistance, Distance Velocity, Angle, and Angle Velocity—are optimized using the nature-inspired Ant Lion Opti-\nmizer metaheuristic to eliminate non-informative or misleading features and decrease the size of the feature set.\nThe ideas of applying pure techniques of Natural Language Processing to Computer Vision have been seen in \nrecent  years29,30,44. By using the sequences of image patches with Transformer, the  models29 can perform specially \nwell on image classification tasks. Similarly, the approach was extended to HAR with sequence of  frames30. In \n“Video Swin Transformer”31, the image was divided into regular shaped windows and utilize a Transformer block \nto each one. The approach was found to outperform the factorized models in efficiency by taking advantage of the \ninherent spatiotemporal locality of videos where pixels that are closer to each other in spatiotemporal distance \nare more likely to be relevant. In our study, we adopt a different approach by utilizing extracted features from \na ConvNet rather than using original images. This choice allows us to reduce computational expenses without \ncompromising efficiency, as detailed in Sect. 3.2.\nTemporal Correlation Module (TCM) 45 utilizes fast-tempo and slow-tempo information and adaptively \nenhances the expressive features, and a Temporal Segment Network (TSN) is introduced to further improve the \nresults of the two-stream  architecture46. Spatiotemporal vector of locally aggregated descriptor (ActionS-ST-\nVLAD) approach designs to aggregate relevant deep features during the entire video based on adaptive video \nfeature segmentation and adaptive segment feature sampling (AVFS-ASFS) in which the key-frame features are \n selected47. Moreover, the concept of using temporal difference can be found in the  works48–50. Temporal Differ-\nence Networks (TDN) approach proposes for both finer local and long-range global motion information, i.e., \nfor local motion modeling, temporal difference over consecutive frames is utilized whereas for global motion \nmodeling, temporal difference across segments is integrated to capture long-range  structure48. SpatioTemporal \nand Motion Encoding (STM) approach proposes an STM block, which contains a Channel-wise SpatioTemporal \nModule (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently \nencode motion features in which a 2D channel-wise convolution is applied to two consecutive frames and then \nsubtracts to obtain the approximate motion  representation49.\nOther related approaches that can be mentioned include Zero-Shot Learning, Few-Shot Learning, and Knowl-\nedge Distillation  Learning51–53. Zero-Shot Learning and Few-Shot Learning provide techniques for understanding \ndomains with limited data availability. Similar to humans, who can identify similar objects within a category \nafter seeing only a few examples, these approaches enable the model to generalize and recognize unseen or \nscarce classes. In our proposed approach, we introduce the concept of Collaborative Learning, where particles \ncollaboratively train in a distributed manner.\nDespite these advances, the field remains largely uncharted, especially with respect to recent and emerging \ntechniques.\nProposed methods\nCollaborative dynamic neural networks. Define N(n, t) as the set of k nearest neighbor particles of \nparticle n at time t, where k ∈ N is some predefined number. In particular,\nwhere i1 , i2 ,...i k  are the k closest particles to n and x (ik )(t) and v(ik )(t) ∈ RD  represent the position and velocity \nof particle ik  at time t. Figure 1 illustrates this concept for k = 4 particles.\nGiven a (continuous) function L : RD −→R and a (compact) subset S ⊂ RD , define\nas the subset of points that minimize L in S, i.e., L(z) ≤ L(w ) for any z ∈ Y ⊂ S and w ∈ S.\n(1)N(n, t) ={ (x(n) (t),v(n) (t)),(x(i1 )(t),v(i1 )(t)),(x(i2 ),v(i2 ))(t),... ,(x(ik )(t),v(ik )(t))}\n(2)Y = argmin\n{\nL(y) : y ∈ S\n}\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nDynamic 1 We investigate a set of neural networks that work together in a decentralized manner to minimize \na Loss function L. The training process is comprised of two phases: (1) individual training of each neural network \nusing (stochastic) gradient descent, and (2) a combined phase of SGD and PSO-based cooperation. The weight \nvector of each neural network is represented as the position of a particle in a D-dimensional phase space, where \nD is the number of weights. The evolution of the particles (or neural networks) is governed by Eq. (3), with the \nupdate rule specified by the following dynamics:\nwhere v(n) (t) ∈ RD  is the velocity vector of particle n  at time t ; ψ(n) (t) is an intermediate velocity computed \nfrom the gradient of the Loss function at x(n) (t) ; φ(n) (t) is the intermediate position computed from the inter -\nmediate velocity ψ(n) (t) ; r(t)\ni.i.d .\n∼ Uniform([0, 1 ]) is randomly drawn from the interval [0, 1 ] and we assume \nthat the sequence r (0), r(1), r(2), ...  is i.i.d.; P(n)(t) ∈ RD  represents the best  position visited up until time \nt by particle n , i.e., the position with the minimum value of the Loss function over all previous positions \nx(n) (0), x(n) (1), ..., x(n) (t) ; P (n)\ng (t) represents its nearest-neighbors’ counterpart, i.e., the best position across all \nprevious positions of the particle n jointly with its corresponding nearest-neighbors ⋃\ns≤t N(n, s) up until time t:\nThe weights wnℓ are defined as\nwith ||·|| being the Euclidean norm and f : R → R being a decreasing (or at least non-increasing) function. In \nDynamic 1, we assume that\nfor some constants M,β> 0 . This strengthens the collaboration learning between any of two particles by pushing \neach particle against each other.\nDynamic 2 An alternative to Eq. (3) is to pull back a particle instead of pushing it in the direction of the gra-\ndient. In the previous section, the assumption was that all particles were located on the same side of a valley in \n(3)\nψ(n) (t + 1) =− η∇L\n(\nx(n) (t)\n)\nφ(n) (t + 1) = x(n) (t) + ψ(n) (t + 1)\nv(n) (t + 1) = ∑\nℓ∈N(n,t)\nw nℓ ψ(ℓ)(t + 1) + c1 r(t)\n(\nP (n) (t) − φ(n) (t + 1)\n)\n+ c2 r(t)\n(\nP (n)\ng (t) − φ(n) (t + 1)\n)\nx(n) (t + 1) = x(n) (t) + v(n) (t)\n(4)\nP (n) (t + 1) ∈ argmin\n{\nL(y) : y = P (n) (t), x(n) (t)\n}\nP (n)\ng (t + 1) ∈ argmin\n{\nL(y) : y = P (n)\ng (t), x(k)(t);\nk ∈ N(n, t)\n} .\n(5)w nℓ = f\n(⏐⏐⏐\n⏐⏐⏐x(n) (t) − x(ℓ)(t)\n⏐⏐⏐\n⏐⏐⏐\n)\n,\n(6)f(z) = M\n(1 + z)β ,\nFigure 1.  A demonstration of the N(n, t) neighborhood, consisting of the positions of four closest particles \nand particle n itself, is shown. The velocities of the particles are depicted by arrows.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nthe loss function. However, if one particle is on the opposite side of the valley relative to the rest of the particles, \nit will be pulled further away from the minimum using the first dynamic. To address this issue, we introduce a \nsecond dynamic (Dynamic 2) that pulls the particle back. The formula for this dynamics is as follows:\nwhere x(i)(t) ∈ RD  is the position of particle i at time t; M, β and c are constants set up by experiments with ||·|| \nbeing the Euclidean norm; r(t)\ni.i.d .\n∼ Uniform([0, 1 ]) randomly drawn from the interval [0, 1 ] and we assume that \nthe sequence r(0), r(1), r(2), ...  is i.i.d.; Pnbest(i)(t) ∈ RD  represents nearest-neighbors’ best , i.e., the best position \nacross all previous positions of the particle n  jointly with its corresponding nearest-neighbors ⋃\ns≤t N(n, s) up \nuntil time t.\nConvNet transformer architecture for action recognition. In this section, we discuss a hybrid Con-\nvNet-Transformer architecture that replaces the traditional ConvNet-RNN block for temporal input to classify \nhuman action in videos. The architecture is composed of several components, including a feature extraction \nmodule using ConvNet, a position embedding layer, multiple transformer encoder blocks, and classification and \naggregation modules. The overall diagram of the architecture can be seen in Fig.  2. The goal of the architecture \nis to effectively capture the temporal information present in the video sequences, in order to perform accurate \nhuman action recognition. The hybrid ConvNet-Transformer design leverages the strengths of both ConvNets \nand Transformers, offering a powerful solution for this challenging task.\nFeatures extraction via ConvNet and position embedding. In the early days of using Transformer for visual \nclassification, especially for images, the frames were typically divided into smaller patches and used as the pri-\nmary  input31,54,55. However, these features were often quite large, leading to high computational requirements \nfor the Transformer. To balance efficiency and accuracy, ConvNet can be utilized to extract crucial features from \nimages, reducing the size of the input without sacrificing performance.\nWe assume that, for each frame, the extracted features from ConNet have a size of (w , h, c) where w and h \nare the width and height of a 2D feature and c is the number of filters. To further reduce the size of the features, \nglobal average pooling is applied, reducing the size from w × h × c to c.\nThe position encoding mechanism in Transformer is used to encode the position of each frame in the \nsequence. The position encoding vector, which has the same size as the feature, is summed with the feature and \nits values are computed using the following formulas. This differs from the sequential processing of data in the \nRNN block, allowing for parallel handling of all entities in the sequence.\nwhere pos, i and PE are the time step index of the input vector, the dimension and the positional encoding matrix; \ndmodel refers to the length of the position encoding vector.\n(7)x(i)(t + 1) = x(i)(t) + ∑ N\nj=1\nM ij\n(1+||xi(t)−xj(t)||2)β (xj(t) −∇ L(xj(t))) + cr\n(\nPnbest(i)(t) − xi(t)\n)\n(8)PE (pos,2i) = sin(pos/10000 2i/dmodel )\nPE (pos,2i+1) = cos(pos/10000 2i/dmodel )\nFigure 2.  Rendering end-to-end ConvNet-Transformer architecture.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nTransformer encoder. The Transformer Encoder is a key component of the hybrid ConvNet-Transformer archi-\ntecture. It consists of a stack of N identical layers, each comprising multi-head self-attention and position-wise \nfully connected feed-forward network sub-layers. To ensure the retention of important input information, resid-\nual connections are employed before each operation, followed by layer normalization.\nThe core of the module is the multi-head self-attention mechanism, which is composed of several self-\nattention blocks. This mechanism is similar to RNN, as it encodes sequential data by determining the relevance \nbetween each element in the sequence. It leverages the inherent relationships between frames in a video to \nprovide a more accurate representation. Furthermore, the self-attention operates on the entire sequence at once, \nresulting in significant improvements in runtime, as the computation can be parallelized using modern GPUs.\nOur architecture employs only the encoder component of a full transformer, as the goal is to obtain a clas -\nsification label for the video action rather than a sequence. The full transformer consists of both encoder and \ndecoder modules, however, in our case, the use of only the encoder module suffices to achieve the desired result.\nAssuming the input sequence ( X = x1 ,x2 ,..., xn ) is first projected onto these weight matrices Q = XWQ , \nK = XWK , V = XWV with WQ,WK and WV are three trainable weights, the query ( Q = q1 ,q2 ,..., qn ), key \n( K = k1 ,k2 ,..., kn ) of dimension dk , and value ( V = v1 ,v2 ,..., vn ) of dimension dv , the output of self-attention \nis computed as follows:\nAs the name suggested, multi-head attention is composed of several heads and all are concatenated and fed into \nanother linear projection to produce the final outputs as follows:\nwhere parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv and W O ∈ Rhdv×dmodel , \ni = 1, 2,... ,h with h denotes the number of heads.\nFrame selection and data pre‑processing. Input videos with varying number of frames can pose a challenge for \nthe model which requires a fixed number of inputs. Put simply, to process a video sequence, we incorporated a \ntime distributed layer that requires a predetermined number of frames. To address this issue, we employ several \nstrategies for selecting a smaller subset of frames.\nOne approach is the “shadow method, ” where a maximum sequence length is established for each video. \nWhile this method is straightforward, it can result in the cutting of longer videos and the loss of information, \nparticularly when the desired length is not reached. In the second method, we utilize a step size to skip some \nframes, allowing us to achieve the full length of the video while reducing the number of frames used. Addition-\nally, the images are center-cropped to create square images. The efficacy of each method will be evaluated in \nour experiments.\nLayers for classification. Assuming, we have a set of videos S(S1 ,S2 ,... ,Sm ) with corresponding labels \ny(y1 ,y2 ,... ,ym ) where m is the number of samples. We select l frames from the videos and obtain g features \nfrom the global average pooling 2-D layer. Each transformer encoder generates a set of representations by con-\nsuming the output from the previous block. After N transformer encoder blocks, we can obtain the multi-level \nrepresentation H N (hN\n1 ,hN\n2 ,... ,hN\nl ) where each representation is 1-D vector with the length of g (see Fig. 2 block \n(A) → (D)).\nThe classification module incorporates traditional layers, such as fully connected and softmax, and also \nemploys global max pooling to reduce network size. To prevent overfitting, we include Gaussian noise and \ndropout layers in the design. The ConvNet-Transformer model is trained using stochastic gradient descent and \nthe categorical cross entropy loss is used as the optimization criterion.\nConvNet‑RNN. Recent studies have explored the combination of ConvNets and RNNs, particularly LSTMs, \nto take into account temporal data of frame features for action recognition in  videos20–23,56,57.\nTo provide a clear understanding of the mathematical operations performed by ConvNets, the following is \na summary of the relevant formulations:\nwhere X represents the input image; O i is the output for layer ith ; W i indicates the weights of the layer; fi (·) \ndenotes weight operation for convolution, pooling or FC layers; gi(·) is an activation function, for example, \nsigmoid, tanh and rectified linear (ReLU) or more recently Leaky  ReLU58; The symbol ( ⊛ ) acts as a convolution \n(9)Attention(Q , K , V ) = softmax( QK T\n√dk\n)V .\n(10)MultiHead(Q ,K ,V ) =Concat(head1 ,head2 ,..., headh)W O .\n(11)where headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\n(12)\n{ O i = X if i= 1\nYi = fi(O i−1 ,W i) if i> 1\nO i = gi(Yi)\n(13)\n{ Yi= W i⊛ Oi−1 ith layer is a convolution\nYi= ⊞n,m Oi−1 ith layer is a pool\nYi= W i∗ Oi−1 ith layer is a FC\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\noperation which uses shared weights to reduce expensive matrix  computation59; Window ( ⊞n,m ) shows an average \nor a max pooling operation which computes average or max values over neighbor region of size n× m in each \nfeature map. Matrix multiplication of weights between layer ith and the layer (i − 1)th in FC is represented as ( ∗).\nThe last layer in the ConvNet (FC layer) acts as a classifier and is usually discarded for the purpose of using \ntransfer learning. Thereafter, the outputs of the ConvNet from frames in the video sequences are fed as inputs \nto the RNN layer.\nConsidering a standard RNN with a given input sequence x1 ,x2 ,..., xT , the hidden cell state is updated at \na time step t as follows:\nwhere W h and W x denote weight matrices, b represents the bias, and σ is a sigmoid function that outputs values \nbetween 0 and 1.\nThe output of a cell, for ease of notation, is defined as\nbut can also be shown using the softmax function, in which ˆyt is the output and yt is the target:\nA more sophisticated RNN or LSTM that includes the concept of a forget gate can be expressed as shown in the \nfollowing equations:\nwhere the ⊙ operation represents an elementwise vector product, and f, i, o and c are the forget gate, input gate, \noutput gate and cell state, respectively. Information is retained when the forget gate ft becomes 1 and eliminated \nwhen ft is set to 0.\nFor optimization purposes, an alternative to LSTMs, the gated recurrent unit (GRU), can be utilized due to \nits lower computational demands. The GRU merges the input gate and forget gate into a single update gate, and \nthe mathematical representation is given by the following equations:\nFinally, it’s worth noting that while traditional RNNs only consider previous information, bidirectional RNNs \nincorporate both past and future information in their computations:\nwhere ht−1 and ht+1 indicate hidden cell states at the previous time step ( t − 1 ) and the future time step ( t + 1).\nResults\nBenchmark datasets. The UCF-101 dataset, introduced in 2012, is one of the largest annotated video \ndatasets  available60, and an expansion of the UCF-50 dataset. It comprises 13,320 realistic video clips collected \nfrom Y ouTube and covers 101 categories of human actions, such as punching, boxing, and walking. The dataset \nhas three distinct official splits (rather than a pre-divided training set and testing set), and the final accuracy in \nour experiments is calculated as the arithmetic average of the results across all three splits.\n(14)ht = σ( W hht−1 + W xxt + b),\n(15)yt = ht,\n(16)ˆyt = softmax(W yht + by).\n(17)ft = σ( W fhht−1 + W fxxt + bf),\n(18)it = σ( W ihht−1 + W ixxt + bi),\n(19)c′\nt = tanh(W c′hht−1 + W c′xxt + b′\nc),\n(20)ct = ft ⊙ ct−1 + it ⊙ c ′\nt ,\n(21)ot = σ( W ohht−1 + W oxxt + bo),\n(22)ht = ot ⊙ tanh(ct),\n(23)rt = σ( W rhht−1 + W rxxt + br),\n(24)zt = σ( W zhht−1 + W zxxt + bz),\n(25)h′\nt = tanh(W h′h(rt ⊙ ht−1 ) + W h′xxt + bz),\n(26)ht = (1 − zt) ⊙ ht−1 + zt ⊙ h′\nt.\n(27)ht = σ( W hxxt + W hhht−1 + bh),\n(28)zt = σ( W ZX xt + W HX ht+1 + bz),\n(29)ˆyt = softmax(W yhht + W yzzt + by),\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nHMDB-5161 was released around the same time as UCF-101. The dataset contains roughly 5k videos belong-\ning to 51 distinct action classes. Each class in the dataset holds at least 100 videos. The videos are collected from \na multiple sources, for example, movies and online videos.\nKinetics-40062 was recently made available in 2017. The dataset consists of 400 human action classes with at \nleast 400 video clips for each action. The videos were assembled from realistic Y ouTube in which each clip lasts \naround 10s. In total, the dataset contains about 240k training videos and 20k validation videos and is one of the \nlargest well-labeled video datasets utilized for action recognition.\nDownloading individual videos from the Kinetics-400 dataset poses a significant challenge due to the large \nnumber of videos and the fact that the dataset only provides links to Y ouTube videos. Therefore, we utilize \n Fiftyone63, an open-source tool specifically designed for constructing high-quality datasets, to address this chal-\nlenge. In our experiment, we collected top-20 most accuracy categories according to the  work62 including “rid-\ning mechanical bull” , “presenting weather forecast” , “sled dog racing” , etc. Eventually, we obtained 7114 files for \ntraining and 773 files for validation with a significant number of files were not collected because the videos were \ndeleted or changed to private, etc. In the same manner, we gathered all categories from HMDB-51 and obtained \n3570 files for training and 1530 files for validation. The tool provides one-split for the HMDB-51, but the docu-\nment does not specify which split.\nOur experiments were conducted using Tensorflow-2.8.2 64, Keras-2.6.0, and a powerful 4-GPU system \n (GeForce® GTX 1080 Ti). We used  Hiplot65 for data visualization. Figure  3 provides snapshot of samples from \neach of the action categories.\nEvaluation metric. For evaluating our results, we employ the standard classification accuracy metric, \nwhich is defined as follows:\nFigure 3.  A snapshot of samples of all actions from UCF-101  dataset60.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nImplementation. Training our collaborative models for action recognition involves building a new, dedi-\ncated system, as these models require real-time information exchange. To the best of our knowledge, this is the \nfirst such system ever built for this purpose. To accommodate the large hardware resources required, each model \nis trained in a separate environment. After one training epoch, each model updates its current location, previous \nlocation, estimate of the gradient of the loss function, and other relevant information, which is then broadcast \nto neighboring models. To clarify the concept, we provide a diagram of the collaborative system and provide a \nbrief description in this subsection.\nOur system for distributed PSO-ConvNets is designed based on a web client-server architecture, as depicted \nin Fig. 4. The system consists of two main components: the client side, which is any computer with a web browser \ninterface, and the server side, which comprises three essential services: cloud services, app services, and data \nservices.\nThe cloud services host the models in virtual machines, while the app services run the ConvNet RNN or \nConvNet Transformer models. The information generated by each model is managed by the data services and \nstored in a data storage. In order to calculate the next positions of particles, each particle must wait for all other \nparticles to finish the training cycle in order to obtain the current information.\nThe system is designed to be operated through a web-based interface, which facilitates the advanced develop-\nment process and allows for easy interactions between users and the system.\nEffectiveness of the proposed method. Table 1 presents the results of Dynamic 1 and Dynamic \n2 on action recognition models. The experiment settings are consistent with our previous research for a fair \ncomparison. As shown in Fig.  2, we consider two different ConvNet architectures, namely DenseNet-201 and \nResNet-152, and select eight models from the  Inception66,  EfficientNet67,  DenseNet68, and  ResNet69 families. In \nthe baseline action recognition methods (DenseNet-201 RNN, ResNet-152 RNN, DenseNet-201 Transformer, \nand ResNet-152 Transformer), features are first extracted from ConvNets using transfer learning and then fine-\ntuned. However, in our proposed method, the models are retrained in an end-to-end fashion. Pretrained weights \nfrom the ImageNet  dataset70 are utilized to enhance the training speed. Our results show an improvement in \naccuracy between 1.58% and 8.72% . Notably, the Dynamics 2 for DenseNet-201 Transformer achieves the best \nresult. We also report the time taken to run each method. Fine-tuning takes less time, but the technique can lead \nto overfitting after a few epochs.\nThe experiments described above were conducted using the settings outlined in Tables  2 and 3. The batch \nsize, input image size, and number of frames were adjusted to maximize GPU memory utilization. However, it \nis worth noting that in Human Activity Recognition (HAR), the batch size is significantly reduced compared to \n(30)Accuracy= Number of correct predictions\nTotal numbers of predictions made .\nFigure 4.  Dynamic PSO-ConvNets System Design. The system is divided into two main components, client \nand server. The client side is accessed through web browser interface while the server side comprises of cloud, \napp, and data services. The cloud stores virtual machine environments where the models reside. The app service \nis where the ConvNet-RNN or ConvNet-Transformer runs, and the information generated by each model \nis managed and saved by the data service. The particles in the system update their positions based on shared \ninformation, including current and previous locations, after completing a training cycle.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nimage classification, as each video consists of multiple frames. Regarding the gradient weight M, a higher value \nindicates a stronger attractive force between particles.\nComparison with state‑of‑the‑art methods. \nThe comparison between our method (Dynamic 2 for ConvNet Transformer) and the previous approaches is \nshown in Table 4. The second method (Transfer Learning and Fusions) trains the models on a Sports-1M Y ou-\ntube dataset and uses the features for UCF-101 recognition. However, the transfer learning procedure is slightly \ndifferent as their ConvNet architectures were designed specifically for action recognition. While it may have \nbeen better to use a pretrained weight for action recognition datasets, such weights are not readily available as \nthe models differ. Also, training the video dataset with millions of samples within a reasonable time is a real \nchallenge for most research centers. Despite these limitations, the use of Transformer and RNN seem to provide \na better understanding of temporal characteristics compared to fusion methods. Shuffle &Learn tries with two \ndistinct models using 2-D images (AlexNet) and 3-D images (C3D) which essentially is series of 2-D images. The \naccuracy is improved, though, 3-D ConvNets require much more power of computing than the 2-D counterparts. \nTable 1.  Three-fold classification accuracy (%) on the UCF-101 benchmark dataset. The results of Dynamic \n1 and Dynamic 2 using DenseNet-201 Transformer and Resnet-152 Transformer models and compared to \nbaseline models, e.g., Dynamic 1 for DenseNet-201 Transformer versus DenseNet-201 Transformer. The N/A \nis the abbreviation for the phrase not applicable, for instance, the transfer learning is not applicable in the \nDynamic 2 for DenseNet-201 Transformer as the method uses the end-to-end training instead. Significant \nvalues are in bold.\nTime per fold (h)\nDynamic method Model Accuracy Improve (%) Transfer learning Fine-tune/retrain\n–\nDenseNet-201 Transformer 0.7741 N/A 26\n0.5\nResNet-152 Transformer 0.7679 N/A 24\n–\nDenseNet-201 RNN 0.8195 N/A 26\n0.1\nResNet-152 RNN 0.8118 N/A 24\nDynamic 1\nDenseNet-201 Transformer 0.8579 8.38 N/A\n5.5\nResNet-152 Transformer 0.8405 7.26 N/A\nDenseNet-201 RNN 0.8462 2.67 N/A\nResNet-152 RNN 0.8276 1.58 N/A\nDynamic 2\nDenseNet-201 Transformer 0.8613 8.72 N/A\nResNet-152 Transformer 0.8399 7.20 N/A\nTable 2.  Hyper-parameter settings for the proposed method.\nHyper-parameters Value Description Hyper-parameters Value Description\nGeneral BS 8 Batch size Encoding dense_dim 64 Dense dimension\ntraining epochs 20 Numbero fi terations layer num_heads 4 Numbero f heads\nConvNet SIZE (224, 224) Input image size RNN units 2048 Numbero f memory units\nCHANNELS 3 Numbero f image channels Classiﬁcation GaussianNoise 0.1 Standard deviation of\nlayer the noise distribution\nNBFRAME 4 Numbero f frames Dense 1024 Numbero f neurons\nNUM_FEATURES 1920 Numbero f features of Dropout 0.4 Dropout rate\nor 2048 DenseNet-201 or ResNet-512\nAugmentation zoom_range 0.1 Zoom range PSO num_neighbors 4 Total particles per group\nrotation_range 8 Rotation range c1 0.5 Coefﬁcient accelerator\nwidth_shift_range 0.2 Width shift range c2 0.5 Coefﬁcient accelerator\nheight_shift_range 0.2 Height shift range\nPreprocessing [-1,1] Preprocessing\nTable 3.  Settings of gradient weight M.\nGradient\nPSO-1 PSO-2 PSO-3 PSO-4\nGradient\nPSO-1 – 0.2 0.2 10\nPSO-2 0.2 – 0.2 10\nPSO-3 0.2 0.2 – 10\nPSO-4 0.2 0.2 0.2 –\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nThere are also attempts to redesign well-known 2-D ConvNet for 3-D data (C3D is built from scratch of a typi-\ncal  ConvNet14), e.g., DPC approach and/or pretrained on larger datasets, e.g., 3D ST-puzzle approach. Besides, \nVideoMoCo utilizes Contrastive Self-supervised Learning (CSL) based approaches to tackle with unlabeled \nimages. The method extends image-based MoCo framework for video representation by empowering temporal \nrobustness of the encoder as well as modeling temporal decay of the keys. Our Dynamic 2 method outperforms \nVideoMoCo by roughly 9% . SVT is a self-supervised method based on the TimeSformer model that employs \nvarious self-attention  schemes79. On pre-trained of the entire Kinetics-400 dataset and inference on UCF-101, \nthe SVT achieves 90.8% and 93.7% for linear evaluation and fine-tuning settings, respectively. When pre-trained \non a subset of Kinetics-400 with 60,000 videos, the accuracy reduces to 84.8%. Moreover, the TSN methods apply \nConvNet and achieves an accuracy of 84.5% using RGB images (2% less than our method) and 92.3% using a \ncombination of three networks (RGB, Optical Flow and Warped Flow). Similarly, the STM approach employs \ntwo-stream networks and pre-trained on Kinetics that enhances the performance significantly. Designing a two-\nstream networks or multi-stream networks would require a larger resource, due to the limitations, we have not \npursued this approach at this time. Furthermore, using optical  flow80 and pose  estimation81 on original images \nmay improve performance, but these techniques are computationally intensive and time consuming, especially \nduring end-to-end training. The concept of Collaborative Learning, on the other hand, is based on a general \nformula of the gradient of the loss function and could be used as a plug-and-play module for any approach. \nFinally, the bag of words method was originally used as a baseline for the dataset and achieved the lowest rec -\nognition accuracy ( 44.5%).\nHyperparameter optimization. In these experiments, we aimed to find the optimal settings for each \nmodel. Table 5 presents the results of the DenseNet-201 Transformer and ResNet-152 Transformer using trans-\nfer learning, where we varied the maximum sequence length, number of frames, number of attention heads, and \ndense size. The number of frames represents the amount of frames extracted from the sequence, calculated by \nstep = (maximum sequence length)/(number of frames)  . The results indicate that longer sequences of frames \nlead to better accuracy, but having a large number of frames is not necessarily the best strategy; a balanced \napproach yields higher accuracy. Furthermore, we discovered that models performed best with 6 attention heads \nand a dense size of either 32 or 64 neurons.\nFigures  5 and   6 show the results for ConvNet RNN models using transfer learning. In the experiments, \nwe first evaluated the performance of eight ConvNets (Inception-v3, ResNet-101, ResNet-152, DenseNet-121, \nDenseNet-201, EfficientNet-B0, EfficientNet-B4, and EfficientNet-B7). The two best performers, DenseNet-121 \nand ResNet-152 ConvNet architectures, were selected for further experimentation. The results of varying the \nnumber of frames showed a preference for longer maximum sequence lengths.\nExtension\nIn this Section, we extend our experiments to perform on more challenge datasets, i.e., Kinetics-400 and HMDB-\n51. In our methods, ConvNets are retrained to improve accuracy performance when compared to Transfer \n Learning32,44, but these processes can take a long time on the entire Kinetics-400 dataset. As a result, we decided \nto obtain only a portion of the entire dataset in order to demonstrate our concept. As shown in Table 6, our main \nfocus in this study is to compare Non-Collaborative Learning (or Individual Learning) and Collaborative Learn-\ning approaches. In each experiment, we conduct two repetitions and record both the mean accuracy and the best \naccuracy (Max) achieved. All settings are the same as in the experiments with UCF-101 dataset. The learning \nrate range is obtained by running a scan from a low to a high learning rate. As a consequence, the learning rates \nof particles PSO-1, PSO-2 and PSO-3 are set at 10−2 , 10−3 and 10−4 , respectively, whereas the learning rates of \nTable 4.  Comparisons of the proposed method and previous methods on the UCF-101 benchmark dataset. \nThe information of pretrained dataset (if any) are also displayed. Significant values are in bold.\nMethod Network Dataset Accuracy (%)\nBag of  words60 (arXiv’12) – UCF-101 44.5\nTransfer Learning and  Fusions71 (CVPR’14) ConvNet Sports-1M 65.4\nShuffle &Learn72 (ECCV’16) AlexNet UCF-101 50.2\nShuffle &Learn72 (ECCV’16) C3D 55.8\nTSN RGB  image46 (ECCV’16)\nConvNet UCF-101\n84.5\nTSN RGB + optical flow + warped  flow46 (ECCV’16) 92.3\nDPC73 (ICCV’19) 3D-ResNet34 Kinetics-400 75.7\nClip  Order74 (CVPR’19) C3D UCF-101 65.6\n3D ST-puzzle75 (AAAI’19) C3D Kinetics-400 60.6\nSTM49 (ICCV’19) ResNet-50 ImageNet+Kinetics 96.2\nP-ODN76 (SR’20) ConvNet UCF-101 78.6\nVideoMoCo77 (CVPR’21) R(2+1)D Kinetics-400 78.7\nSVT slow-fast78 (CVPR’22) TimeSformer Kinetics-400 (60K) 84.8\nOur model (Dynamics 2 for DenseNet-201 Transformer) DenseNet-201 Transformer UCF-101 86.1\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nthe wilder particle PSO-4 has a range of [10 −5 , 10−1 ] . The results show a preference for the Collaborative Learn-\ning methods as the Dynamic 1 and Dynamic 2 outperform the Individual Learning through both datasets, e.g., \nan improvement of 0.7% can be seen on Kinetics-400 using DenseNet-201 Transformer. The results obtained \nin our experiments clearly demonstrate the superiority of our proposed Collaborative Learning approach for \nvideo action recognition.\nTable 5.  Three-fold classification accuracy results (%) on the UCF-101 benchmark dataset for DenseNet-201 \ntransformer and ResNet-152 transformer with transfer learning training. Significant values are in bold.\nAccuracy (%)\n Model\nMaximum sequence \nlength Number of frames\nNumber of attention \nheads Dense size Set 1 Set 2 Set 3 Avg\nDenseNet-201 Trans-\nformer\n100\n2\n1\n4 69.94 69.42 69.21 69.52\n128 69.57 69.68 68.51 69.25\n4\n64 74.36 75.39 75.43 75.06\n128 76 75.15 74.81 75.32\n8\n128 74.57 74.75 74.65 74.66\n256 74.94 74.75 75.05 74.91\n4 1\n4 70.92 69.47 71.67 70.69\n8 70.02 69.63 70.32 69.99\n16 69.87 68.53 68.29 68.90\n32 70.37 71 69.1 70.16\n64 70.08 69.52 67.99 69.20\n128 69.26 69.42 70.54 69.74\n10\n4\n64 76.71 76 75.3 76.00\n128 77.43 75.33 76.46 76.41\n6\n8 77.08 76.57 76.95 76.87\n16 77.64 76.62 76.06 76.77\n32 77.4 76.73 77.08 77.07\n64 76.87 77.16 78.19 77.41\n1024 74.94 73.57 72.48 73.66\n20 6\n8 76 77.34 77.25 76.86\n32 76.71 75.99 75.3 76.00\n64 77.16 76 76.46 76.54\n128 77.4 75.87 77.03 76.77\n40\n2\n4\n32 74.65 74.53 74.3 74.49\n64 74.94 74.26 73.54 74.25\n128 74.46 74.29 74.46 74.40\n1024 71.03 70.73 70.45 70.74\n6\n16 73.38 74.48 73.97 73.94\n32 74.39 73.43 73.97 73.93\n64 75.34 74.37 73.57 74.43\n128 73.96 74.21 74.03 74.07\n10 6\n32 76.37 74.64 74.35 75.12\n64 75.73 75.55 74.81 75.36\n128 76.08 76 75.08 75.72\nResNet-154 Transformer 100\n2\n1\n4 71.08 70.65 68.59 70.11\n128 70.05 71.51 68.89 70.15\n4\n128 75.02 76.03 74.73 75.26\n256 74.76 75.66 75.19 75.20\n6\n64 75.39 75.52 74.65 75.19\n128 75.57 75.6 75.78 75.65\n10 6\n4 76.55 76.78 75.65 76.33\n8 77.08 76.35 76.14 76.52\n16 76.55 76.25 76.33 76.38\n32 77.24 76.49 76.65 76.79\n64 75.84 77.53 76.54 76.64\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nDiscussion\nThe performance of action recognition methods such as ConvNet Transformer and ConvNet RNN is largely \ndependent on various factors, including the number of attention heads, the number of dense neurons, the \nnumber of units in RNN, and the learning rate, among others. Collaborative learning is an effective approach \nto improve the training of neural networks, where multiple models are trained simultaneously and both their \npositions and directions, as determined by the gradients of the loss function, are shared. In our previous research, \nwe applied dynamics to ConvNets for image classification and in this study, we extend the concept to hybrid \nConvNet Transformer and ConvNet RNN models for human action recognition in sequences of images. We first \naim to identify the optimal settings that lead to the highest accuracy for the baseline models. As seen in Table 1, \nthe ConvNet Transformer models did not perform as well as the ConvNet RNN models with transfer learning, \nwhich could be due to the limited data available for training, as transformers typically require more data than \nRNN-based models. However, our proposed method, incorporating dynamics and end-to-end training, not \nonly outperforms the baseline models, but also results in the ConvNet Transformer models outperforming their \nConvNet RNN counterparts. This can be attributed to the additional data provided to the transformer models \nthrough data augmentation and additional noise.\nConclusion\nRecognizing human actions in videos is a fascinating problem in the art of recognition, and while Convolutional \nNeural Networks provide a powerful method for image classification, their application to HAR can be complex, \nas temporal features play a critical role.\nIn this study, we present a novel video action recognition framework that leverages collaborative learning \nwith dynamics. Our approach explores the hybridization of ConvNet RNN and the recent advanced method \nFigure 5.  Hyperparameter optimization results for ConvNet RNN models with transfer learning. the models \nare numbered as follows: 1. Inception-v3, 2. ResNet-101, 3. ResNet-152, 4. DenseNet-121, 5. DenseNet-201, \n6. EfficientNet-B0, 7. EfficientNet-B4, 8. EfficientNet-B7. The abbreviations acc, gn, and lr stand for accuracy, \nGaussian noise, and learning rate, respectively.\nFigure 6.  Impact of varying the number of frames on the three-fold accuracy of DenseNet-201 RNN and \nResNet-152 RNN using transfer learning on the UCF-101 benchmark dataset.\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\nTransformer, which has been adapted from Natural Language Processing for video sequences. The experi-\nments include the exploration of two dynamics models, Dynamic 1 and Dynamic 2. The results demonstrate a \nround improvement of 2–9% in accuracy over baseline methods, such as an 8.72% increase in accuracy for the \nDenseNet-201 Transformer using Dynamic 2 and a 7.26% increase in accuracy for the ResNet-152 Transformer \nusing Dynamic 1. Our approach outperforms the previous methods, offering significant improvements in video \naction recognition.\nIn summary, our work makes three key contributions: (1) We incorporate Dynamic 1 and Dynamic 2 into a \nhybrid model that combines ConvNet with two popular sequence modeling techniques—RNN and Transformer. \n(2) We extend the distributed collaborative learning framework to address the task of human action recognition. \n(3) We conducted extensive experiments on the challenging datasets including UCF-101, Dynamics-400 and \nHMDB-51 over a period of 2–3 months to thoroughly evaluate our approach. To validate its effectiveness, we \ncompared our method against state-of-the-art approaches in the field.\nData availability\nThe datasets generated and/or analysed during the current study are available in the UCF101 repository, https:// \nwww. crcv. ucf. edu/ data/ UCF101. php. All data generated or analysed during this study are included in this pub-\nlished article [and its supplementary information files].\nReceived: 17 February 2023; Accepted: 30 July 2023\nReferences\n 1. Sultani, W ., Chen, C. & Shah, M. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition 6479–6488 (2018).\n 2. Li, A. et al. Abnormal event detection in surveillance videos based on low-rank and compact coefficient dictionary learning. Pat‑\ntern Recognit. 108, 107355 (2020).\n 3. Razali, H., Mordan, T. & Alahi, A. Pedestrian intention prediction: A convolutional bottom-up multi-task approach. Transp. Res. \nPart C Emerg. Technol. 130, 103259 (2021).\n 4. Y ang, H., Liu, L., Min, W ., Y ang, X. & Xiong, X. Driver yawning detection based on subtle facial action recognition. IEEE Trans. \nMultimed. 23, 572–583 (2020).\n 5. Presti, L. L. & La Cascia, M. 3d skeleton-based human action classification: A survey. Pattern Recognit. 53, 130–147 (2016).\n 6. Poppe, R. A survey on vision-based human action recognition. Image Vis. Comput. 28, 976–990 (2010).\n 7. Zhu, H., Vial, R. & Lu, S. Tornado: A spatio-temporal convolutional regression network for video action proposal. In Proceedings \nof the IEEE International Conference on Computer Vision 5813–5821 (2017).\n 8. Curtis, S., Zafar, B., Gutub, A. & Manocha, D. Right of way. Vis. Comput. 29, 1277–1292 (2013).\n 9. Paul, S. N. & Singh, Y . J. Survey on video analysis of human walking motion. Int. J. Signal Process. Image Process. Pattern Recognit. \n7, 99–122 (2014).\n 10. Wang, H., Kläser, A., Schmid, C. & Liu, C.-L. Action recognition by dense trajectories. In CVPR 2011 3169–3176 (2011). https:// \ndoi. org/ 10. 1109/ CVPR. 2011. 59954 07.\n 11. Wang, H. & Schmid, C. Action recognition with improved trajectories. In Proceedings of the IEEE International Conference on \nComputer Vision 3551–3558 (2013).\n 12. Gorelick, L., Blank, M., Shechtman, E., Irani, M. & Basri, R. Actions as space-time shapes. IEEE Trans. Pattern Anal. Mach. Intell. \n29, 2247–2253 (2007).\n 13. Simonyan, K. & Zisserman, A. Two-stream convolutional networks for action recognition in videos. Adv. Neural Inf. Process. Syst. \n27 (2014).\n 14. Tran, D., Bourdev, L., Fergus, R., Torresani, L. & Paluri, M. Learning spatiotemporal features with 3d convolutional networks. In \nProceedings of the IEEE International Conference on Computer Vision 4489–4497 (2015).\n 15. Carreira, J. & Zisserman, A. Quo vadis, action recognition? A new model and the kinetics dataset. In proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition 6299–6308 (2017).\nTable 6.  Comparison of Collaborative Learning and Individual Learning on classification accuracy (%). The \nexperiments are performed using Kinetics-400 and HMDB-51 datasets with DenseNet-201 Transformer and \nResNet-152 Transformer models. Each experiment is repeated two times. The N/A is the abbreviation for the \nphrase not applicable, for instance, the Dynamic methods are not applicable in the Individual Learning. The \nPSO-1, PSO-2, PSO-3 have the learning rates of 10−2 , 10−3 and 10−4 , respectively whereas PSO-4’s learning \nrate is in the range [ 10−1 , 10−5]. Significant values are in bold.\nDataset Model\nIndividual learning Collaborative learning\nN/A Dynamic 1 Dynamic 2\nPSO-1 PSO-2 PSO-3 PSO-4 PSO-1 PSO-2 PSO-3 PSO-4 PSO-1 PSO-2 PSO-3 PSO-4\nHMDB-\n51\nResNet-152 \nTransformer\nAcc. 43.97 ± \n0.27\n48.09 ± \n1.94\n39.13 ± \n0.65\n37.82 ± \n8.23\n42.73 ± \n1.20\n51.43 ± \n0.92\n51.43 ± \n1.29\n49.60 ± \n0.09\n44.07 ± \n1.61\n50.84 ± \n1.39\n51.24 ± \n1.38\n51.27 ± \n1.15\nMax 44.17 49.47 39.59 43.65 43.58 52.09 52.35 49.67 45.22 51.83 52.22 52.09\nDenseNet-201 \nTransformer\nAcc. 45.18 ± \n0.78\n52.22 ± \n1.56\n41.81 ± \n0.82\n38.21 ± \n9.34\n45.74 ± \n0.46\n55.23 ± \n0.65\n55.75 ± \n0.09\n54.64 ± \n0.83\n44.56 ± \n0.83\n52.67 ± \n1.29\n52.81 ± \n1.48\n51.89 ± \n1.66\nMax 45.74 53.33 42.4 44.82 46.07 55.69 55.82 55.23 45.15 53.59 53.86 53.07\nKinet-\nics-400\nDenseNet-201 \nTransformer\nAcc 92.37 ± \n0.45\n94.59 ± \n0.27\n90.32 ± \n0.12\n86.71 ± \n9.94\n92.5 ± \n0.09 94.66 94.72 ± \n0.28\n93.42 ± \n1.5\n92.57 ± \n0.55\n94.85 ± \n0.82\n94.92 ± \n0.73\n94.85 ± \n0.64\nMax 92.7 94.79 90.41 93.75 92.57 94.66 94.92 94.53 92.96 95.44 95.44 95.31\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\n 16. Hara, K., Kataoka, H. & Satoh, Y . Can spatiotemporal 3d CNNS retrace the history of 2D CNNS and imagenet? In Proceedings of \nthe IEEE conference on Computer Vision and Pattern Recognition 6546–6555 (2018).\n 17. Xie, S., Sun, C., Huang, J., Tu, Z. & Murphy, K. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video \nclassification. In Proceedings of the European Conference on Computer Vision (ECCV) 305–321 (2018).\n 18. Diba, A. et al. Temporal 3d convnets: New architecture and transfer learning for video classification (2017). arXiv: 1711. 08200.\n 19. Varol, G., Laptev, I. & Schmid, C. Long-term temporal convolutions for action recognition. IEEE Trans. Pattern Anal. Mach. Intell. \n40, 1510–1517 (2017).\n 20. Donahue, J. et al. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition 2625–2634 (2015).\n 21. Yue-Hei Ng, J. et al. Beyond short snippets: Deep networks for video classification. In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition 4694–4702 (2015).\n 22. Ullah, A., Ahmad, J., Muhammad, K., Sajjad, M. & Baik, S. W . Action recognition in video sequences using deep bi-directional \nLSTM with CNN features. IEEE Access 6, 1155–1166 (2017).\n 23. He, J.-Y ., Wu, X., Cheng, Z.-Q., Yuan, Z. & Jiang, Y .-G. DB-LSTM: Densely-connected bi-directional LSTM for human action \nrecognition. Neurocomputing 444, 319–331 (2021).\n 24. Gowda, S. N., Rohrbach, M. & Sevilla-Lara, L. Smart frame selection for action recognition. In Proceedings of the AAAI Conference \non Artificial Intelligence 1451–1459 (2021).\n 25. Ge, H., Y an, Z., Yu, W . & Sun, L. An attention mechanism based convolutional LSTM network for video action recognition. Mul‑\ntimed. Tools Appl. 78, 20533–20556 (2019).\n 26. Wu, Z., Xiong, C., Ma, C.-Y ., Socher, R. & Davis, L. S. Adaframe: Adaptive frame selection for fast video recognition. In Proceedings \nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition 1278–1287 (2019).\n 27. Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems 5998–6008 (2017).\n 28. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G. & Jégou, H. Going deeper with image transformers. In Proceedings of the \nIEEE/CVF International Conference on Computer Vision 32–42 (2021).\n 29. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale (2020). arXiv: 2010. 11929.\n 30. Arnab, A. et al. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision \n6836–6846 (2021).\n 31. Liu, Z. et al. Video swin transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition  \n3202–3211 (2022).\n 32. Phong, N. H., Santos, A. & Ribeiro, B. PSO-convolutional neural networks with heterogeneous learning rate. IEEE Access 10, \n89970–89988. https:// doi. org/ 10. 1109/ ACCESS. 2022. 32011 42 (2022).\n 33. Ijjina, E. P . & Chalavadi, K. M. Human action recognition using genetic algorithms and convolutional neural networks. Pattern \nRecognit. 59, 199–212 (2016).\n 34. Real, E. et al. Large-scale evolution of image classifiers. In International Conference on Machine Learning 2902–2911 (PMLR, 2017).\n 35. Nayman, N. et al. Xnas: Neural architecture search with expert advice. Adv. Neural Inf. Process. Syst. 32 (2019).\n 36. Noy, A. et al. Asap: Architecture search, anneal and prune. In International Conference on Artificial Intelligence and Statistics  \n493–503 (PMLR, 2020).\n 37. Kennedy, J. & Eberhart, R. Particle swarm optimization. In: Proceedings of ICNN’95‑International Conference on Neural Networks, \nvol. 4, 1942–1948 (IEEE, 1995).\n 38. Shi, Y . & Eberhart, R. A modified particle swarm optimizer. In: 1998 IEEE International Conference on Evolutionary Computation \nProceedings. IEEE World Congress on Computational Intelligence (Cat. No. 98TH8360) 69–73 (IEEE, 1998).\n 39. Tu, S. et al. ModPSO-CNN: An evolutionary convolution neural network with application to visual recognition. Soft Comput. 25, \n2165–2176 (2021).\n 40. Chuang, L.-Y ., Tsai, S.-W . & Y ang, C.-H. Improved binary particle swarm optimization using catfish effect for feature selection. \nExpert Syst. Appl. 38, 12699–12707 (2011).\n 41. Xue, B., Zhang, M. & Browne, W . N. Particle swarm optimization for feature selection in classification: A multi-objective approach. \nIEEE Trans. Cybern. 43, 1656–1671 (2012).\n 42. Zhang, R. Sports action recognition based on particle swarm optimization neural networks. Wirel. Commun. Mob. Comput. 2022, \n1–8 (2022).\n 43. Basak, H. et al. A union of deep learning and swarm-based optimization for 3d human action recognition. Sci. Rep. 12, 1–17 (2022).\n 44. Phong, N. H. & Ribeiro, B. Rethinking recurrent neural networks and other improvements for image classification (2020). arXiv: \n2007. 15161.\n 45. Liu, Y ., Yuan, J. & Tu, Z. Motion-driven visual tempo learning for video-based action recognition. IEEE Trans. Image Process. 31, \n4104–4116 (2022).\n 46. Wang, L. et al. Temporal segment networks: Towards good practices for deep action recognition. In European Conference on \nComputer Vision 20–36 (Springer, 2016).\n 47. Tu, Z. et al. Action-stage emphasized spatiotemporal VLAD for video action recognition. IEEE Trans. Image Process. 28, 2799–2812 \n(2019).\n 48. Wang, L., Tong, Z., Ji, B. & Wu, G. TDN: Temporal difference networks for efficient action recognition. In Proceedings of the IEEE/\nCVF Conference on Computer Vision and Pattern Recognition 1895–1904 (2021).\n 49. Jiang, B., Wang, M., Gan, W ., Wu, W . & Y an, J. STM: Spatiotemporal and motion encoding for action recognition. In Proceedings \nof the IEEE/CVF International Conference on Computer Vision 2000–2009 (2019).\n 50. Phong, N. H. & Ribeiro, B. Action recognition for American sign language (2018). arXiv: 2205. 12261.\n 51. Zhang, L. et al. Tn-zstad: Transferable network for zero-shot temporal activity detection. IEEE Trans. Pattern Anal. Mach. Intell. \n45, 3848–3861 (2022).\n 52. Gao, Z. et al. A pairwise attentive adversarial spatiotemporal network for cross-domain few-shot action recognition-r2. IEEE \nTrans. Image Process. 30, 767–782 (2020).\n 53. Tu, Z., Liu, X. & Xiao, X. A general dynamic knowledge distillation method for visual analytics. IEEE Trans. Image Process. 31, \n6517–6531 (2022).\n 54. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International \nConference on Computer Vision 10012–10022 (2021).\n 55. Zhang, Y. et al. Vidtr: Video transformer without convolutions. In: Proceedings of the IEEE/CVF International Conference on \nComputer Vision 13577–13587 (2021).\n 56. Ullah, A., Ahmad, J., Muhammad, K., Sajjad, M. & Baik, S. W . Action recognition in video sequences using deep bi-directional \nLSTM with CNN features. IEEE Access 6, 1155–1166. https:// doi. org/ 10. 1109/ ACCESS. 2017. 27780 11 (2018).\n 57. Chen, J., Samuel, R. D. J. & Poovendran, P . LSTM with bio inspired algorithm for action recognition in sports videos. Image Vis. \nComput. 112, 104214. https:// doi. org/ 10. 1016/j. imavis. 2021. 104214 (2021).\n 58. Maas, A. L., Hannun, A. Y ., Ng, A. Y . et al. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml 3 (Citeseer, \n2013).\n 59. LeCun, Y ., Kavukcuoglu, K. & Farabet, C. Convolutional networks and applications in vision. In Proceedings of 2010 IEEE Inter‑\nnational Symposium on Circuits and Systems 253–256 (IEEE, 2010).\n16\nVol:.(1234567890)Scientific Reports |        (2023) 13:14624  | https://doi.org/10.1038/s41598-023-39744-9\nwww.nature.com/scientificreports/\n 60. Soomro, K., Zamir, A. R. & Shah, M. Ucf101: A dataset of 101 human actions classes from videos in the wild (2012). arXiv: 1212. \n0402.\n 61. Kuehne, H., Jhuang, H., Garrote, E., Poggio, T. & Serre, T. Hmdb: A large video database for human motion recognition. In 2011 \nInternational Conference on Computer Vision 2556–2563 (IEEE, 2011).\n 62. Kay, W . et al. The kinetics human action video dataset (2017). arXiv: 1705. 06950.\n 63. Voxel51. The open-source tool for building high-quality datasets and computer vision models (2023). https:// github. com/ voxel \n51/ fifty one.\n 64. Abadi, M. et al. Tensorflow: Large-scale machine learning on heterogeneous systems, software available from tensorflow.org (2015). \nhttps:// www. tenso rflow. org.\n 65. Haziza, D., Rapin, J. & Synnaeve, G. Hiplot, interactive high-dimensionality plots (2020). https://  github. com/ faceb ookre search/ \nhiplot.\n 66. Szegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J. & Wojna, Z. Rethinking the inception architecture for computer vision. In Proceed‑\nings of the IEEE Conference on Computer Vision and Pattern Recognition 2818–2826 (2016).\n 67. Tan, M. & Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine \nlearning 6105–6114 (PMLR, 2019).\n 68. Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition 4700–4708 (2017).\n 69. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition 770–778 (2016).\n 70. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in \nNeural Information Processing Systems 1097–1105 (2012).\n 71. Karpathy, A. et al. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition 1725–1732 (2014).\n 72. Noroozi, M. & Favaro, P . Unsupervised learning of visual representations by solving jigsaw puzzles. In Computer Vision–ECCV \n2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VI 69–84 (Springer, 2016).\n 73. Han, T., Xie, W . & Zisserman, A. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF Inter‑\nnational Conference on Computer Vision Workshops 0–0 (2019).\n 74. Xu, D. et al. Self-supervised spatiotemporal learning via video clip order prediction. In Proceedings of the IEEE/CVF Conference \non Computer Vision and Pattern Recognition 10334–10343 (2019).\n 75. Kim, D., Cho, D. & Kweon, I. S. Self-supervised video representation learning with space-time cubic puzzles. In: Proceedings of \nthe AAAI Conference on Artificial Intelligence 8545–8552 (2019).\n 76. Shu, Y ., Shi, Y ., Wang, Y ., Huang, T. & Tian, Y . P-odn: Prototype-based open deep network for open set recognition. Sci. Rep. 10, \n1–13 (2020).\n 77. Pan, T., Song, Y ., Y ang, T., Jiang, W . & Liu, W . Videomoco: Contrastive video representation learning with temporally adversarial \nexamples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 11205–11214 (2021).\n 78. Ranasinghe, K., Naseer, M., Khan, S., Khan, F . S. & Ryoo, M. S. Self-supervised video transformer. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition 2874–2884 (2022).\n 79. Bertasius, G., Wang, H. & Torresani, L. Is space-time attention all you need for video understanding? In ICML 4 (2021).\n 80. Zhao, S., Zhao, L., Zhang, Z., Zhou, E. & Metaxas, D. Global matching with overlapping attention for optical flow estimation. In \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 17592–17601 (2022).\n 81. Fang, H.-S. et al. Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time. IEEE Trans. Pattern \nAnal. Mach. Intell. 45, 7157–7173. https:// doi. org/ 10. 1109/ TPAMI. 2022. 32227 84 (2023).\nAcknowledgements\nThis research is sponsored by FEDER funds through the programs COMPETE—“Programa Operacional Factores \nde Competitividade” and Centro2020—“Centro Portugal Regional Operational Programme” , and by national \nfunds through FCT—“Fundação para a Ciência e a Tecnologia” , under the Project UIDB/00326/2020 and \nUIDP/00326/2020. The support is gratefully acknowledged.\nAuthor contributions\nN.H.P .: conceptualization, methodology, coding, experiments, validation, formal analysis, investigation, writ-\ning—original draft, writing—review and editing; B.R.: writing—review and editing, supervision. All authors \nhave reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to H.P .N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8209285736083984
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7046898603439331
    },
    {
      "name": "Transformer",
      "score": 0.5935114622116089
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5877094864845276
    },
    {
      "name": "Machine learning",
      "score": 0.5675902366638184
    },
    {
      "name": "Action recognition",
      "score": 0.5593389272689819
    },
    {
      "name": "Artificial neural network",
      "score": 0.48008424043655396
    },
    {
      "name": "Deep learning",
      "score": 0.43751460313796997
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4254212975502014
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76903346",
      "name": "University of Coimbra",
      "country": "PT"
    }
  ],
  "cited_by": 22
}