{
  "title": "Random Walks and Neural Network Language Models on Knowledge Bases",
  "url": "https://openalex.org/W2294429012",
  "year": 2015,
  "authors": [
    {
      "id": null,
      "name": "Goikoetxea Salutregi, Josu",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Agirre Bengoa, Eneko",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Soroa Echave, Aitor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2251797829",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2127289991",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2107577105",
    "https://openalex.org/W2130173309",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W1498740961",
    "https://openalex.org/W2251803266",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2159719802",
    "https://openalex.org/W2067438047",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W71795751",
    "https://openalex.org/W2026487812",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2158028897"
  ],
  "abstract": "Random walks over large knowledge bases like WordNet have been successfully used in word similarity, relatedness and disambiguation tasks. Unfortunately, those algorithms are relatively slow for large repositories, with significant memory footprints. In this paper we present a novel algorithm which encodes the structure of a knowledge base in a continuous vector space, combining random walks and neural net language models in order to produce novel word representations. Evaluation in word relatedness and similar-&#13;\\nity datasets yields equal or better results than those of a random walk algorithm, using a dense representation (300 dimensions instead of 117K). Furthermore, the word representations are complementary to those of the random walk algorithm and to corpus-based continuous representations, improving the state-&#13;\\nof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations.",
  "full_text": null,
  "topic": "WordNet",
  "concepts": [
    {
      "name": "WordNet",
      "score": 0.8412874937057495
    },
    {
      "name": "Random walk",
      "score": 0.8145225048065186
    },
    {
      "name": "Computer science",
      "score": 0.7585844993591309
    },
    {
      "name": "Word (group theory)",
      "score": 0.684631884098053
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.6588934659957886
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6139940023422241
    },
    {
      "name": "Representation (politics)",
      "score": 0.5370546579360962
    },
    {
      "name": "Natural language processing",
      "score": 0.5283644199371338
    },
    {
      "name": "Artificial neural network",
      "score": 0.518022358417511
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3408784866333008
    },
    {
      "name": "Mathematics",
      "score": 0.18147066235542297
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}