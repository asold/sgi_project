{
  "title": "Applications of Large Language Models in Psychiatry: A Systematic Review",
  "url": "https://openalex.org/W4393304897",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2682811430",
      "name": "Mahmud Omar",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2513274694",
      "name": "Shelly Soffer",
      "affiliations": [
        "Assuta Medical Center",
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A2127182135",
      "name": "Alexander W. Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2588322445",
      "name": "Isotta Landi",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2170298636",
      "name": "GIRISH N. NADKARNI",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2682811430",
      "name": "Mahmud Omar",
      "affiliations": [
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2513274694",
      "name": "Shelly Soffer",
      "affiliations": [
        "Assuta Medical Center",
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A2127182135",
      "name": "Alexander W. Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2588322445",
      "name": "Isotta Landi",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2170298636",
      "name": "GIRISH N. NADKARNI",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai",
        "Mount Sinai Health System"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4323025178",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4381435571",
    "https://openalex.org/W4385638306",
    "https://openalex.org/W4361284227",
    "https://openalex.org/W4361272589",
    "https://openalex.org/W4386117324",
    "https://openalex.org/W4389180454",
    "https://openalex.org/W4389523980",
    "https://openalex.org/W4388524014",
    "https://openalex.org/W4200615999",
    "https://openalex.org/W4387578765",
    "https://openalex.org/W4388734690",
    "https://openalex.org/W4379768613",
    "https://openalex.org/W2916945592",
    "https://openalex.org/W4367186583",
    "https://openalex.org/W2586897603",
    "https://openalex.org/W4388565177",
    "https://openalex.org/W4387744116",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W2952789948",
    "https://openalex.org/W4392164233",
    "https://openalex.org/W4390700529",
    "https://openalex.org/W4386397123",
    "https://openalex.org/W4388502894",
    "https://openalex.org/W4390585437",
    "https://openalex.org/W4386887838",
    "https://openalex.org/W4385459268",
    "https://openalex.org/W4378470708",
    "https://openalex.org/W4387677901",
    "https://openalex.org/W4388019161",
    "https://openalex.org/W4386893393",
    "https://openalex.org/W4389923012",
    "https://openalex.org/W4387157142",
    "https://openalex.org/W4385572222",
    "https://openalex.org/W4389262829",
    "https://openalex.org/W4386304195",
    "https://openalex.org/W4389934892",
    "https://openalex.org/W4389565446"
  ],
  "abstract": "Abstract Background With their unmatched ability to interpret and engage with human language and context, large language models (LLMs) hint at the potential to bridge AI and human cognitive processes. This review explores the current application of LLMs, such as ChatGPT, in the field of psychiatry. Methods We followed PRISMA guidelines and searched through PubMed, Embase, Web of Science, and Scopus, up until March 2024. Results From 771 retrieved articles, we included 16 that directly examine LLMs’ use in psychiatry. LLMs, particularly ChatGPT and GPT-4, showed diverse applications in clinical reasoning, social media, and education within psychiatry. They can assist in diagnosing mental health issues, managing depression, evaluating suicide risk, and supporting education in the field. However, our review also points out their limitations, such as difficulties with complex cases and potential underestimation of suicide risks. Conclusion Early research in psychiatry reveals LLMs’ versatile applications, from diagnostic support to educational roles. Given the rapid pace of advancement, future investigations are poised to explore the extent to which these models might redefine traditional roles in mental health care.",
  "full_text": "Applications of Large Language Models in Psychiatry: A Systematic Review. \nMahmud Omar1, Shelly Soffer2, Alexander W Charney3, Isotta Landi3, Girish N \nNadkarni4, Eyal Klang.  \n1- Tel-Aviv university, Faculty of Medicine.  \n2- Internal Medicine B, Assuta Medical Center, Ashdod, Israel, and Ben-Gurion \nUniversity of the Negev, Be'er Sheva, Israel.  \n3- Icahn School of Medicine at Mount Sinai, New York, New York. \n4- Hasso Plattner Institute for Digital Health at Mount Sinai, Icahn School of \nMedicine at Mount Sinai, New York, New York.  \nFunding: No external funding was received for the research, authorship, and/or \npublication of this article. \nConflict of Interest Statement: The authors declare that they have no competing \ninterests that might be perceived to influence the results and/or discussion reported in \nthis paper. \nAcknowledgements: The authors wish to thank the research and administrative staff \nat their respective institutions for their support and contributions to this study. \nAuthor Contributions \n• Mahmud Omar (MO): Conceptualization, Methodology, Screening and Data \nExtraction, Writing - Original Draft Preparation. \n• Shelly Soffer: Data Collection, Writing - Review & Editing. \n• Alexander W Charney: Supervision, Validation, Writing - Review & Editing. \n• Isotta Landi: Data Interpretation, Writing - Review & Editing. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n• Girish N Nadkarni: Project Administration, Resources, Writing - Review & \nEditing. \n• Eyal Klang (EK): Data Curation, Screening and Data Extraction, Writing - \nReview & Editing. \nEthical Approval: Not applicable for this systematic review. \nConsent for Publication: All authors have read and agreed to the published version \nof the manuscript. \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nAbstract  \nBackground: With their unmatched ability to interpret and engage with human \nlanguage and context, large language models (LLMs) hint at the potential to bridge AI \nand human cognitive processes. This review explores the current application of \nLLMs, such as ChatGPT, in the field of psychiatry. \nMethods: We followed PRISMA guidelines and searched through PubMed, Embase, \nWeb of Science, and Scopus, up until March 2024. \nResults: From 771 retrieved articles, we included 16 that directly examine LLMs' use \nin psychiatry. LLMs, particularly ChatGPT and GPT-4, showed diverse applications \nin clinical reasoning, social media, and education within psychiatry. They can assist in \ndiagnosing mental health issues, managing depression, evaluating suicide risk, and \nsupporting education in the field. However, our review also points out their \nlimitations, such as difficulties with complex cases and potential underestimation of \nsuicide risks. \nConclusion: Early research in psychiatry reveals LLMs' versatile applications, from \ndiagnostic support to educational roles. Given the rapid pace of advancement, future \ninvestigations are poised to explore the extent to which these models might redefine \ntraditional roles in mental health care. \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nIntroduction  \nThe integration of artificial intelligence (AI) into various healthcare sectors has \nbrought transformative changes (1–3).  Currently, Large Language Models (LLMs) \nlike Chat Generative Pre-trained Transformer (ChatGPT) are at the forefront (2,4,5). \nAdvanced LLMs, such as GPT-4 and Claude Opus, possess an uncanny ability to \nunderstand and generate human-like text. This capacity indicates their potential to act \nas intermediaries between AI functionalities and the complexities of human cognition. \nUnlike their broader application in healthcare, LLMs in psychiatry address unique \nchallenges such as the need for personalized mental health interventions and the \nmanagement of complex mental disorders\n (4,6). Their capacity for human-like \nlanguage generation and interaction is not just a technological advancement; it's a \ncritical tool in bridging the treatment gap in mental health, especially in under-\nresourced areas (4,6–8). \nIn psychiatry, LLMs like ChatGPT can provide accessible mental health services, \nbreaking down geographical, financial, or temporal barriers, which are particularly \npronounced in mental health care (4,9). For instance, ChatGPT can support therapists \nby offering tailored assistance during various treatment phases, from initial \nassessment to post-treatment recovery (10–12). This includes aiding in symptom \nmanagement and encouraging healthy lifestyle changes pertinent to psychiatric care \n(10–14). \nChatGPT’s ability to provide preliminary mental health assessments and \npsychotherapeutic support is a notable advancement (15,16). It can engage in \nmeaningful conversations, offering companionship and empathetic responses, tailored \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nto individual mental health needs (6,10,11,13), a component that is essential in \npsychiatric therapy (17).  \nDespite these capabilities, currently, LLMs do not replace human therapists (18–20). \nRather, the technology supplements existing care, enhancing the overall treatment \nprocess while acknowledging the value of human clinical judgment and therapeutic \nrelationships (4,8,13). \nAs LLM technology advances rapidly, it holds the potential to alter traditional mental \nhealth care paradigms. This review aims to assess their current role within psychiatry \nresearch. \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n \n \nMethods  \nSearch Strategy \nThe review was registered with the International Prospective Register of Systematic \nReviews - PROSPERO (Registration code: CRD42024524035) We adhered to the \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) \nguidelines (21,22).  \nA systematic search was conducted across key databases: PubMed, Embase, Web of \nScience, and Scopus, from December 2022 up until March 2024. We chose December \n2022, as it was the date of introduction of chatGPT. We complemented the search via \nreference screening for any additional papers. We chose PubMed, Embase, Web of \nScience, and Scopus for their comprehensive coverage of medical and psychiatric \nliterature.  \nOur search strategy combined specific keywords related to LLM, including \n'ChatGPT,' 'Artificial Intelligence,' 'Natural Language Processing,' and 'Large \nLanguage Models,' with psychiatric terminology such as 'Psychiatry' and 'Mental \nHealth.' Additionally, to refine our search, we incorporated keywords for the most \nrelevant psychiatric diseases. These included terms like 'Depression,' 'Anxiety \nDisorders,' 'Bipolar Disorder,' 'Schizophrenia,' and others pertinent to our study's \nscope. \nSpecific search strings for each database are detailed in the supplementary materials. \nStudy Selection\n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nThe selection of studies was rigorously conducted by two independent reviewers, MO \nand EK. Inclusion criteria were set to original research articles that specifically \nexamined the application of LLMs in psychiatric settings.  \nEligible studies were required to present measurable outcomes related to psychiatric \ncare, such as patient engagement, diagnostic accuracy, treatment adherence, or \nclinician efficiency. \nWe excluded review articles, case reports, conference abstracts without full texts, \neditorials, preprints, and studies not written in English. \nMO and EK systematically evaluated each article against these criteria. In cases of \ndisagreement or uncertainty regarding the eligibility of a particular study, the matter \nwas resolved through discussion and, if necessary, consultation with additional \nresearchers in our team to reach a consensus. \nData Extraction\n \nData extraction was performed by two independent reviewers, MO and EK, using a \nstructured template. Key information extracted included the study's title, authors, \npublication year, study design, psychiatric condition or setting, the role of the LLM \nmodel, sample size, findings related to the effectiveness and impact of the model, and \nany noted conclusions and implications. In cases of discrepancy during the extraction \nprocess, issues were resolved through discussion and consultation with other \nresearchers involved in the study. \nRisk of Bias\n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nIn our systematic review, we opted for a detailed approach instead of a standard risk \nof bias assessment, given the unique and diverse nature of the studies included. Each \nstudy is presented in a table highlighting its design and essential variables (Table 1). \nA second table catalogs the inherent limitations of each study, providing a transparent \noverview of potential biases and impacts on the results (Table 2).  \nAdditionally, a figure illustrates the quartiles and SCImago Journal Rank scores of the \njournals where these studies were published, offering insight into their academic \nsignificance (Figure 1). This method ensures a clear, concise evaluation of the varied \nincluded papers.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nResults  \nSearch Results and Study Selection \nOur systematic search across PubMed, Embase, Web of Science, and Scopus yielded a \ntotal of 771 papers. The breakdown of the initial results was as follows: PubMed \n(186), Scopus (290), Embase (133), and Web of Science (162). After applying \nautomated filters to exclude review articles, case reports, and other non-relevant \ndocument types, 454 articles remained. The removal of duplicates further reduced the \npool to 288 articles. \nSubsequent screening based on titles and abstracts led to the exclusion of 255 papers, \nprimarily due to their irrelevance or lack of discussion on LLMs, leaving 33 articles \nfor full-text evaluation. Upon detailed examination, 16 studies were found to meet our \ninclusion criteria and were thus selected for the final review (23–38). The process of \nstudy selection and the results at each stage are comprehensively illustrated in Figure \n2, the PRISMA flowchart.  \nOverview of the Included Studies\n \nMost of the studies in our review were published in Q1 journals, indicating a high \nlevel of influence in the field (Figure 1). The studies varied in their approach, using \ndata ranging from real patient interactions on online platforms to simulated scenarios, \nand in scale, from individual case studies to large datasets. \nMost research focused on various versions of ChatGPT, including ChatGPT-3.5 and \nChatGPT-4, with some comparing its performance to traditional methods or other \nLLMs. The applications of LLMs in these studies were diverse, covering aspects like \nmental health screening augmentation on social media, generating psychodynamic \nformulations, and assessing risks in psychiatric conditions.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nIn highlighting key studies, Liyanage et al. found that ChatGPT was effective in \nenhancing Reddit post analysis for wellness classification (36). Levkovich et al. \nobserved ChatGPT's unbiased approach in depression diagnosis, contrasting with \nbiases noted in primary care physicians' methods, especially due to gender and \nsocioeconomic status (31). Additionally, Li et al. demonstrated GPT-4's proficiency in \npsychiatric diagnostics, uniquely passing the Taiwanese Psychiatric Licensing \nExamination and paralleling experienced psychiatrists' diagnostic abilities (23). \nLLMs' applications and limitations in mental health \n \nWe categorized the applications of LLM in the included studies into three main \nthemes to provide a synthesized and comprehensive overview:  \nApplications (Table 1, Figure 3)  \n1. Clinical Reasoning: \n• Hwang et al. demonstrated ChatGPT's ability in generating \npsychodynamic formulations, indicating potential in clinical psychiatry \nwith statistical significance (Kendall’s W = 0.728, p = 0.012) (37). \n• Levkovich et al. (2023) compared ChatGPT's recommendations for \ndepression treatment against primary care physicians, finding ChatGPT \nmore aligned with accepted guidelines, particularly for mild depression \n(31). \n• Levkovich et al. and Elyoseph et al. (2023) assessed ChatGPT's \nperformance in assessing suicide risk. The study by Levkovich et al. \nhighlighted that ChatGPT tended to underestimate risks when \ncompared to mental health professionals, particularly in scenarios with \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nhigh perceived burdensomeness and feeling of thwarted belongingness \n(28). The study be Elyoseph, found that while GPT-4’s evaluations \nwere similar to mental health professionals, ChatGPT-3.5 often \nunderestimated suicide risk (29). \n• D’Souza et al. evaluated ChatGPT's response to psychiatric case \nvignettes, where it received high ratings, especially in generating \nmanagement strategies for conditions like anxiety and depression (33). \n• Li et al. demonstrated ChatGPT GPT-4’s capabilities in the Taiwanese \nPsychiatric Licensing Examination and psychiatric diagnostics, closely \napproximating the performance of experienced psychiatrists (23). \nChatGPT outperformed the two LLMs, Bard and Llama-2.  \n• Heston T.F. et al. Evaluated ChatGPT-3.5's responses in depression \nsimulations. AI typically recommended human support at moderate \ndepression levels (PHQ-9 score of 12) and insisted on human \nintervention at severe levels (score of 25) (34). \n• Dergaa et al. critically assessed ChatGPT's effectiveness in mental \nhealth assessments, particularly highlighting its inadequacy in dealing \nwith complex situations, such as nuanced cases of postpartum \ndepression requiring detailed clinical judgment, suggesting limitations \nin its current readiness for broader clinical use (27). \n• Elyoseph et al. (2024) provided a comparative analysis of depression \nprognosis from the perspectives of AI models, mental health \nprofessionals, and the general public. The study revealed notable \ndifferences in long-term outcome predictions. AI models, including \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nChatGPT, showed variability in prognostic outlooks, with ChatGPT-\n3.5 often presenting a more pessimistic view compared to other AI \nmodels and human evaluation (24). \n• Elyoseph et al. (2023) investigated ChatGPT's emotional awareness \nusing the Levels of Emotional Awareness Scale (LEAS). ChatGPT \nscored significantly higher than the general population, indicating a \nhigh level of emotional understanding (30).  \n2. Social Media applications: \n• Liyanage et al. used ChatGPT models to augment data from Reddit \nposts, enhancing classifier performance in identifying Wellness \nDimensions. This resulted in improvements in the F-score of up to \n13.11% (36). \n• Mazumdar et al. applied GPT-3 in classifying mental health disorders \nfrom Reddit data, achieving an accuracy of around 87% and \ndemonstrating its effectiveness in explanation generation (32). GPT-3 \ndemonstrated superior performance in classifying mental health \ndisorders and generating explanations, outperforming traditional \nmodels like LIME and SHAP.  \nFigure 4 presents the different types of data inputs for GPT in the current \napplications for the field of psychiatry.  \n3. Educational Therapeutic Interventions: \n• Spallek et al. examined ChatGPT’s application in mental health and \nsubstance use education, finding its outputs to be substandard \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \ncompared to expert materials. However, when prompts where carefully \nengineered, the outputs were better aligned with communication \nguidelines (26). \n• Hadar-Shoval D et al., explored ChatGPT's ability to understand \nmental state in personality disorders (25), and Sezgin et al. (38), \nassessed responses to postpartum depression questions Both studies \nreflect ChatGPT’s utility both as an educational resource and for \noffering preliminary therapeutic advice. Sezgin et al. (38) showed that \nGPT-4 demonstrated generally higher quality, more clinically accurate \nresponses compared to Bard and Google Search.  \n• Parker et al. ChatGPT-3.5 was used to respond to clinically relevant \nquestions about bipolar disorder and to generate songs related to \nbipolar disorder, testing both its factual knowledge and creativity. The \nstudy highlighted its utility in providing basic information, but also its \nlimitations in citing current, accurate references (35). \nAdditional data in the supplementary material includes demographics, journals of the \nincluded papers, SCImago Journal Rank (SJR) 2022 data for these publications, and \nspecific performance metrics for the models across various applications, detailed in \nS1-S2 tables in the supplementary material.  \nSafety and Limitations \nConcerns regarding safety and limitations in LLMs clinical applications emerge as \ncritical themes. For instance, Heston T.F. et al. observed that ChatGPT-3.5 \nrecommended human support at moderate depression levels but only insisted on \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nhuman intervention at severe levels, underscoring the need for cautious application in \nhigh-risk scenarios (34).  \nElyoseph et al. highlighted that ChatGPT consistently underestimated suicide risks \ncompared to mental health professionals, especially in scenarios with high perceived \nburdensomeness and thwarted belongingness (29).  \nDergaa et al. concluded that ChatGPT, as of July 2023, was not ready for mental \nhealth assessment and intervention roles, showing limitations in complex case \nmanagement (27). This suggest that while ChatGPT shows promise, it is not without \nsignificant risks and limitations, particularly in handling complex and sensitive mental \nhealth scenarios (Table 2, Figure 5).   \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nDiscussion  \nOur findings demonstrate LLMs, especially ChatGPT and GPT-4, potential as a \nvaluable tool in psychiatry, offering diverse applications from clinical support to \neducational roles. Studies like Liyanage et al. and Mazumdar et al. showcased its \nefficacy in data augmentation and mental health disorder classification (32,36). \nOthers, such as Hwang et al. and Levkovich et al. (2023), highlighted its capabilities \nin clinical settings, including diagnosis and risk assessment (31,37). Overall, GPT \nemerged as the most used and studied LLM in the field of psychiatry.  \nHowever, concerns about its limitations and safety in clinical scenarios were evident, \nas seen in studies by Elyoseph et al. (2023) and Dergaa et al., indicating that while \nChatGPT holds promise, its integration into clinical psychiatry must be approached \nwith caution (27,29).  \nThe potential and efficacy of AI, particularly LLMs, in psychiatry are highlighted by \nour review, showing its capability to streamline care, lower barriers, and reduce costs \nin mental health services (39). Studies like Liyanage et al. and Hwang et al. illustrate \nChatGPT's diverse applications, from clinical data analysis to formulating \npsychodynamic profiles, which contribute to a more efficient, accessible, and versatile \napproach in mental healthcare (19,20,36,37). Moreover, in light of the COVID-19 \npandemic's impact on mental health and the growing demand for digital interventions, \nMitsea et al.'s research highlights the significant role of AI in enhancing digitally \nassisted mindfulness training for self-regulation and mental well-being, further \nenriching the scope of AI applications in mental healthcare (40). \nWhen compared with humans and other LLMs, ChatGPT consistently adheres to \nclinical guidelines, as shown in Levkovich et al.'s study (31). Furthermore, ChatGPT \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \noften surpasses other models in tasks like psychiatric diagnostics, as demonstrated by \nLi et al (23). \nThese studies collectively underscore the utility of current LLMs, particularly \nChatGPT and GPT-4, in psychiatry, demonstrating promise across various domains. \nWhile serving as a complementary tool to human expertise, especially in complex \npsychiatric scenarios (4,6,11,20), LLMs are poised for deeper integration into mental \nhealth care. This evolution is propelled by rapid technological advancements and \nsignificant financial investments since the watershed moment of ChatGPT \nintroduction, late 2022. Future research should closely monitor this integration, \nexploring how LLMs not only supplement but also augment human expertise in \npsychiatry. \nOur review has limitations. The absence of a formal risk of bias assessment, due to \nthe unique nature of the included studies, is a notable drawback. Additionally, the \nreliance on studies that did not use real patient data as well as the heterogeneity in \nstudy designs could affect the generalizability of our findings. Moreover, the diversity \nof methods and tasks in the included studies prohibited us from performing a meta-\nanalysis. It should also be mentioned that all studies were retrospective in nature. \nFuture directions should include prospective, real-world evidence studies, that could \ncement the utility of LLM in the psychiatry field. \nIn conclusion,  \nEarly research in psychiatry reveals LLMs' versatile applications, from diagnostic \nsupport to educational roles. Given the rapid pace of advancement, future \ninvestigations are poised to explore the extent to which these models might redefine \ntraditional roles in mental health care. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nReferences  \n1. Singh O. Artificial intelligence in the era of ChatGPT - Opportunities and \nchallenges in mental health care. Indian J Psychiatry. 2023;65(3):297.  \n2. Dave T, Athaluri SA, Singh S. ChatGPT in medicine: an overview of its \napplications, advantages, limitations, future prospects, and ethical \nconsiderations. Front Artif Intell. 2023 May 4;6.  \n3. Terra M, Baklola M, Ali S, El-Bastawisy K. Opportunities, applications, \nchallenges and ethical implications of artificial intelligence in psychiatry: a \nnarrative review. Egypt J Neurol Psychiatr Neurosurg. 2023 Jun 20;59(1):80.  \n4. He Y , Liang K, Han B, Chi X. A digital ally: The potential roles of ChatGPT in \nmental health services. Asian J Psychiatr. 2023 Oct;88:103726.  \n5. Beam AL, Drazen JM, Kohane IS, Leong TY , Manrai AK, Rubin EJ. Artificial \nIntelligence in Medicine. New England Journal of Medicine. 2023 Mar \n30;388(13):1220–1.  \n6. Caliyurt O. AI and Psychiatry: The ChatGPT Perspective. ALPHA \nPSYCHIATRY . 2023 Mar 30;24(2):1–42.  \n7. Cheng S, Chang C, Chang W, Wang H, Liang C, Kishimoto T, et al. The now \nand future of <scp>ChatGPT</scp> and <scp>GPT</scp> in psychiatry. \nPsychiatry Clin Neurosci. 2023 Nov 11;77(11):592–6.  \n8. King DR, Nanda G, Stoddard J, Dempsey A, Hergert S, Shore JH, et al. An \nIntroduction to Generative Artificial Intelligence in Mental Health Care: \nConsiderations and Guidance. Curr Psychiatry Rep. 2023 Dec 30;25(12):839–\n46.  \n9. Yang K, Ji S, Zhang T, Xie Q, Kuang Z, Ananiadou S. Towards Interpretable \nMental Health Analysis with Large Language Models. In: Proceedings of the \n2023 Conference on Empirical Methods in Natural Language Processing. \nStroudsburg, PA, USA: Association for Computational Linguistics; 2023. p. \n6056–77.  \n10. Khawaja Z, Bélisle-Pipon JC. Your robot therapist is not your therapist: \nunderstanding the role of AI-powered mental health chatbots. Front Digit \nHealth. 2023 Nov 8;5.  \n11. Boucher EM, Harake NR, Ward HE, Stoeckl SE, Vargas J, Minkel J, et al. \nArtificially intelligent chatbots in digital mental health interventions: a review. \nExpert Rev Med Devices. 2021 Dec 3;18(sup1):37–49.  \n12. Sarkar S, Gaur M, Chen LK, Garg M, Srivastava B. A review of the \nexplainability and safety of conversational agents for mental health to identify \navenues for improvement. Front Artif Intell. 2023 Oct 12;6.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n13. Ettman CK, Galea S. The Potential Influence of AI on Population Mental \nHealth. JMIR Ment Health. 2023 Nov 16;10:e49936.  \n14. Haman M, Školník M, Šubrt T. Leveraging ChatGPT for Human Behavior \nAssessment: Potential Implications for Mental Health Care. Ann Biomed Eng. \n2023 Nov 8;51(11):2362–4.  \n15. Fiske A, Henningsen P, Buyx A. Your Robot Therapist Will See You Now: \nEthical Implications of Embodied Artificial Intelligence in Psychiatry, \nPsychology, and Psychotherapy. J Med Internet Res. 2019 May \n9;21(5):e13216.  \n16. Cuesta MJ. Psychopathology for the twenty-first century: Towards a ChatGPT \npsychopathology? European Neuropsychopharmacology. 2023 Aug;73:21–3.  \n17. Ross J, Watling C. Use of empathy in psychiatric practice: Constructivist \ngrounded theory study. BJPsych Open. 2017 Jan 2;3(1):26–33.  \n18. Blease C, Torous J. ChatGPT and mental healthcare: balancing benefits with \nrisks of harms. BMJ Mental Health. 2023 Nov 10;26(1):e300884.  \n19. King DR, Nanda G, Stoddard J, Dempsey A, Hergert S, Shore JH, et al. An \nIntroduction to Generative Artificial Intelligence in Mental Health Care: \nConsiderations and Guidance. Curr Psychiatry Rep. 2023 Dec 30;25(12):839–\n46.  \n20. Avula VCR, Amalakanti S. Artificial intelligence in psychiatry, present trends, \nand challenges: An updated review. Archives of Mental Health. 2023 Oct 17;  \n21. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, et \nal. The PRISMA 2020 statement: an updated guideline for reporting systematic \nreviews. BMJ. 2021 Mar 29;n71.  \n22. Schiavo JH. PROSPERO: An International Register of Systematic Review \nProtocols. Med Ref Serv Q. 2019 Apr 3;38(2):171–80.  \n23. Li D, Kao Y , Tsai S, Bai Y , Y eh T, Chu C, et al. Comparing the performance of \n<scp>ChatGPT GPT</scp> \n/i1 4, Bard, and Llama/i1 2 in the Taiwan Psychiatric \nLicensing Examination and in differential diagnosis with multi/i1 center \npsychiatrists. Psychiatry Clin Neurosci. 2024 Feb 26;  \n24. Elyoseph Z, Levkovich I, Shinan-Altman S. Assessing prognosis in depression: \ncomparing perspectives of AI models, mental health professionals and the \ngeneral public. Fam Med Community Health. 2024 Jan 9;12(Suppl 1):e002583.  \n25. Hadar-Shoval D, Elyoseph Z, Lvovsky M. The plasticity of ChatGPT’s \nmentalizing abilities: personalization for personality structures. Front \nPsychiatry. 2023 Sep 1;14.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n26. Spallek S, Birrell L, Kershaw S, Devine EK, Thornton L. Can we use ChatGPT \nfor Mental Health and Substance Use Education? Examining Its Quality and \nPotential Harms. JMIR Med Educ. 2023 Nov 30;9:e51243.  \n27. Dergaa I, Fekih-Romdhane F, Hallit S, Loch AA, Glenn JM, Fessi MS, et al. \nChatGPT is not ready yet for use in providing mental health assessment and \ninterventions. Front Psychiatry. 2024 Jan 4;14.  \n28. Levkovich I, Elyoseph Z. Suicide Risk Assessments Through the Eyes of \nChatGPT-3.5 Versus ChatGPT-4: Vignette Study. JMIR Ment Health. 2023 Sep \n20;10:e51232.  \n29. Elyoseph Z, Levkovich I. Beyond human expertise: the promise and limitations \nof ChatGPT in suicide risk assessment. Front Psychiatry. 2023 Aug 1;14.  \n30. Elyoseph Z, Hadar-Shoval D, Asraf K, Lvovsky M. ChatGPT outperforms \nhumans in emotional awareness evaluations. Front Psychol. 2023 May 26;14.  \n31. Levkovich I, Elyoseph Z. Identifying depression and its determinants upon \ninitiating treatment: ChatGPT versus primary care physicians. Fam Med \nCommunity Health. 2023 Sep 16;11(4):e002391.  \n32. Mazumdar H, Chakraborty C, Sathvik M, Mukhopadhyay sabyasachi, \nPanigrahi PK. GPTFX: A Novel GPT-3 Based Framework for Mental Health \nDetection and Explanations. IEEE J Biomed Health Inform. 2024;1–8.  \n33. Franco D’Souza R, Amanullah S, Mathew M, Surapaneni KM. Appraising the \nperformance of ChatGPT in psychiatry using 100 clinical case vignettes. Asian \nJ Psychiatr. 2023 Nov;89:103770.  \n34. Heston TF. Safety of Large Language Models in Addressing Depression. \nCureus. 2023 Dec 18;  \n35. Parker G, Spoelma MJ. A chat about bipolar disorder. Bipolar Disord. 2023 Sep \n28;  \n36. Liyanage C, Garg M, Mago V , Sohn S. Augmenting Reddit Posts to Determine \nWellness Dimensions impacting Mental Health. In: The 22nd Workshop on \nBiomedical Natural Language Processing and BioNLP Shared Tasks. \nStroudsburg, PA, USA: Association for Computational Linguistics; 2023. p. \n306–12.  \n37. Hwang G, Lee DY , Seol S, Jung J, Choi Y , Her ES, et al. Assessing the \npotential of ChatGPT for psychodynamic formulations in psychiatry: An \nexploratory study. Psychiatry Res. 2024 Jan;331:115655.  \n38. Sezgin E, Chekeni F, Lee J, Keim S. Clinical Accuracy of Large Language \nModels and Google Search Responses to Postpartum Depression Questions: \nCross-Sectional Study. J Med Internet Res. 2023 Sep 11;25:e49240.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n39. Li H, Zhang R, Lee YC, Kraut RE, Mohr DC. Systematic review and meta-\nanalysis of AI-based conversational agents for promoting mental health and \nwell-being. NPJ Digit Med. 2023 Dec 19;6(1):236.  \n40. Mitsea E, Drigas A, Skianis C. Digitally Assisted Mindfulness in Training Self-\nRegulation Skills for Sustainable Mental Health: A Systematic Review. \nBehavioral Sciences. 2023 Dec 10;13(12):1008.  \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nTables and figures \nTable 1: Summary of the included papers.  \nAuthor Year Model Application Main Results \nLiyanage et \nal.(36) \n2023 GPT-3.5  Data augmentation \nfor wellness \ndimension \nclassification \nChatGPT models \neffectively augmented \nReddit post data, \nsignificantly improving \nclassification performance \nfor wellness dimensions. \nHwang et \nal.(37) \n2024\n GPT-4 Psychodynamic \nformulations in \npsychiatry \nGPT-4 successfully created \nrelevant and accurate \npsychodynamic \nformulations based on \npatient history. \nParker et \nal.(35) \n2023 GPT-3 Information on \nbipolar disorder \nGPT-3 provided basic \nmaterial on bipolar \ndisorders and creative song \ngeneration, but lacked \ndepth for scientific review. \nHeston T.F. \net al.(34) \n2023 GPT-3.5 Simulating \ndepression \nscenarios \nChatGPT-3.5 \nconversational agents \nrecommended human \nsupport at critical points, \nhighlighting the need for AI \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nsafety in mental health. \nD’Souza et \nal.(33) \n2023 GPT-3.5 Responding to \npsychiatric case \nvignettes \nChatGPT 3.5 showed high \ncompetence in handling \npsychiatric case vignettes, \nwith strong diagnostic and \nmanagement strategy \ngeneration. \nLevkovich \net al.(31) \n2023 GPT-3.5, GPT-4 Depression \ndiagnosis and \ntreatment \nChatGPT aligned with \nguidelines for depression \nmanagement, contrasting \nwith primary care \nphysicians and showing no \ngender or socioeconomic \nbiases. \nMazumdar \net al.(32) \n2023 GPT-3, BERT-\nlarge, \nMentalBERT, \nClinicBERT, and \nPsychBERT \nMental health \ndisorder \nclassification and \nexplanation \nGPT-3 outperformed other \nmodels in classifying \nmental health disorders and \ngenerating explanations, \nshowing promise for AI-\nIoMT deployment. \nSezgin et \nal.(38) \n2023\n GPT-4, LaMDA \n(using Bard) \nResponding to \npostpartum \ndepression \nquestions \nGPT-4 provided more \nclinically accurate \nresponses to postpartum \ndepression questions, \nsurpassing other models \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nand Google Search. \nElyoseph et \nal.(30) \n2023 GPT-3.5 Emotional \nawareness \nevaluations \nChatGPT showed higher \nemotional awareness \ncompared to the general \npopulation and improved \nover time. \nElyoseph et \nal.(29) \n2023 GPT-3.5 Suicide risk \nassessment \nChatGPT underestimated \nsuicide risks compared to \nmental health professionals, \nindicating the need for \nhuman judgment in \ncomplex assessments. \nLevkovich \net al.(28) \n2023\n GPT-3.5, GPT-4 Suicide risk \nassessment \nvignettes \nGPT-4’s evaluations of \nsuicide risk were similar to \nmental health professionals, \nthough with some \noverestimations and \nunderestimations. \nDergaa et \nal.(27) \n2024\n GPT-3.5 Mental health \nassessment and \ninterventions \nChatGPT showed \nlimitations in complex \nmedical scenarios, \nunderlining its \nunpreparedness for \nstandalone use in mental \nhealth practice. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nSpallek et \nal.(26) \n2023 GPT-4 Mental health and \nsubstance use \neducation \nGPT-4's outputs were \nsubstandard compared to \nexpert materials in terms of \ndepth and adherence to \ncommunication guidelines. \nHadar-\nShoval D et \nal.(25) \n2023 GPT-3.5 Responses for \nBPD and SPD \nscenarios \nChatGPT effectively \ndifferentiated emotional \nresponses in BPD and SPD \nscenarios, showing tailored \nmentalizing abilities. \nElyoseph et \nal.(24) \n2024 GPT-3.5, GPT-4 Prognosis in \ndepression \nChatGPT-3.5 showed a \nmore pessimistic prognosis \nin depression compared to \nother LLMs and mental \nhealth professionals. \nLi et al.(23) 2024 GPT-4, Bard and \nLlama-2 \nPsychiatric \nlicensing \nexamination and \ndiagnostics \nGPT-4 outperformed other \nmodels in psychiatric \ndiagnostics, closely \nmatching the capabilities of \nhuman psychiatrists. \nAbbreviations: BPD: Borderline Personality Disorder | SPD: Schizoid Personality Disorder | GPT: Generative Pre-\ntrained Transformer | AI: Artificial Intelligence | NLP: Natural Language Processing | EDA: Easy Data \nAugmentation | BT: Back Translation | PHQ-9: Patient Health Questionnaire-9 | LEAS: Levels of Emotional \nAwareness Scale | AI-IoMT: Artificial Intelligence-Internet of Medical Things | GRADE: Grading of \nRecommendations, Assessment, Development, and Evaluations. \nTable 2: Summary of the studies designs and limitations.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nAuthor Year Study Design Sample Size Study Limitations \nLiyanage et \nal.(36) \n2023 Exploratory and \nExperimental \nStudy \n4,376 \nrecords \nLimited generalizability due to \nthe use of specific Reddit data and \npotential online discourse biases. \nHwang et \nal.(37) \n2024\n Exploratory and \nExperimental \nStudy \n1 detailed \ncase \nReliance on fictional data from \nliterature; lacks a comparator \ngroup for performance context. \nLiyanage et \nal.(36) \n2023 Exploratory and \nExperimental \nStudy - Evaluative \nResearch \nNA Clinical relevance limited by use \nof AI-generated responses and \nlack of real patient data. \nHwang et \nal.(37) \n2023 Observational \nCross-Sectional \nStudy \n25 agents Potential non-representativeness \nof simulations and small sample \nsize of ChatGPT-3.5 agents. \nLiyanage et \nal.(36) \n2023\n Experimental \nStudy \n100 \nvignettes \nFictional vignettes may not fully \nrepresent real-world psychiatric \ncomplexities; no comparator \ngroup. \nHwang et \nal.(37) \n2023\n Cross-Sectional \nAnalysis \nVignette-\nbased \nHypothetical vignettes may lack \nreal clinical scenario \napplicability; absence of patient \ndemographics. \nLiyanage et \nal.(36) \n2023\n Retrospective and \nProspective \nAnalysis \nNR Absence of demographic data and \npotential biases in retrospective \ndata selection. \nHwang et \nal.(37) \n2023\n Cross-Sectional \nStudy \n14 questions Lack of traditional participants; \nmay not reflect clinical \nconsultation complexity. \nLiyanage et \nal.(36) \n2023 Comparative \nProspective Study \n750 \nparticipants \nFictional scenarios may not \nreplicate real-world emotional \nchallenges; comparison to general \nnorms. \nHwang et \nal.(37) \n2023 Comparative \nRetrospective \nAnalysis \n379 \nprofessionals\n \nUse of fictional vignettes; limited \nby specificity and generalizability.\n \nLiyanage et \nal.(36) \n2023\n Prospective \nVignette Study \n379 \nprofessionals\n \nHypothetical vignettes; focus on \nspecific scenarios, not covering \nthe full spectrum of risk factors. \nHwang et \nal.(37) \n2024\n Prospective \nSimulated \nInteractions \n3 scenarios Fictional scenarios; lacks \ncomplex real-patient interaction \ndynamics. \nLiyanage et \nal.(36) \n2023\n View Point and \nCase Study \n10 queries Small number of real-world \nqueries; comparison to potentially \nbiased expert materials. \nHwang et \nal.(37) \n2023 Cross-Sectional \nQuantitative \nAnalysis \nAI-generated \ndata \nFictional data limits real-world \napplicability; no comparator \ngroup. \nLiyanage et \nal.(36) \n2024 Retrospective \nComparative \n2,460 \nparticipants \nUse of fictional vignettes; focus \non AI perspectives may have \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \nAnalysis inherent biases. \nHwang et \nal.(37) \n2024 Retrospective \nAnalysis \n24 \npsychiatrists \nFictional data for examination; \nlimited comparison group size. \nAbbrevations: AI: Artificial Intelligence | NA: Not Available | NR: Not Reported \n \nFigure 1: SJR scores and journal quartiles of the included studies. \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2: PRISMA flowchart.  \n \n \n \n \nTotal records identified:  771 \nPubmed (n = 186) \nScopus (n = 120) \nRecords removed before \nscreening: \nDuplicate records removed  \n(n = 166)\n \nIdentification of studies via databases and registers \nId\nen\ntifi\nca\ntio\n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3: \nApplications and Evaluations of LLMs in Diverse Domains of Psychiatry.  \n \n \nFigure 4: Data Input Spectrum for GPT in Psychiatric Applications.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n \n \n \n \n \n \n \nFigure 5: \nStrengths and limitations of GPT's current applications in psychiatry \npractice.  \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 29, 2024. ; https://doi.org/10.1101/2024.03.28.24305027doi: medRxiv preprint ",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6004661321640015
    },
    {
      "name": "Scopus",
      "score": 0.5742392539978027
    },
    {
      "name": "Pace",
      "score": 0.501882791519165
    },
    {
      "name": "Mental health",
      "score": 0.4920932650566101
    },
    {
      "name": "Psychiatry",
      "score": 0.48567092418670654
    },
    {
      "name": "Psychology",
      "score": 0.42739906907081604
    },
    {
      "name": "Medicine",
      "score": 0.3576602041721344
    },
    {
      "name": "MEDLINE",
      "score": 0.3385798931121826
    },
    {
      "name": "Political science",
      "score": 0.22333785891532898
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I2800586481",
      "name": "Assuta Medical Center",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I124227911",
      "name": "Ben-Gurion University of the Negev",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    }
  ],
  "cited_by": 7
}