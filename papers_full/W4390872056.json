{
    "title": "Pseudo-Labeling With Large Language Models for Multi-Label Emotion Classification of French Tweets",
    "url": "https://openalex.org/W4390872056",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2133316101",
            "name": "Usman Malik",
            "affiliations": [
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes",
                "Université de Rouen Normandie"
            ]
        },
        {
            "id": "https://openalex.org/A2112066445",
            "name": "Simon Bernard",
            "affiliations": [
                "Université de Rouen Normandie",
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A361282818",
            "name": "Alexandre Pauchet",
            "affiliations": [
                "Université de Rouen Normandie",
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes",
                "Institut National des Sciences Appliquées Rouen Normandie"
            ]
        },
        {
            "id": "https://openalex.org/A2041225272",
            "name": "Clément Chatelain",
            "affiliations": [
                "Institut National des Sciences Appliquées Rouen Normandie",
                "Université de Rouen Normandie",
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A2109323438",
            "name": "Romain Picot Clemente",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A740112001",
            "name": "Jérôme Cortinovis",
            "affiliations": [
                "Délégation Normandie"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2162095830",
        "https://openalex.org/W6753123197",
        "https://openalex.org/W2921907837",
        "https://openalex.org/W3095536082",
        "https://openalex.org/W2156199025",
        "https://openalex.org/W3092263709",
        "https://openalex.org/W1419568444",
        "https://openalex.org/W4285395379",
        "https://openalex.org/W4318595446",
        "https://openalex.org/W4363675415",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W4295296320",
        "https://openalex.org/W3088965898",
        "https://openalex.org/W6787640756",
        "https://openalex.org/W4300818570",
        "https://openalex.org/W6685053522",
        "https://openalex.org/W6744133147",
        "https://openalex.org/W4378907299",
        "https://openalex.org/W4321022102",
        "https://openalex.org/W6752527106",
        "https://openalex.org/W4223591627",
        "https://openalex.org/W3205012620",
        "https://openalex.org/W4365504037",
        "https://openalex.org/W3128731563",
        "https://openalex.org/W6779659972",
        "https://openalex.org/W4311161179",
        "https://openalex.org/W3200874137",
        "https://openalex.org/W2423124209",
        "https://openalex.org/W3096959502",
        "https://openalex.org/W2988566768",
        "https://openalex.org/W2903285287",
        "https://openalex.org/W3023092508",
        "https://openalex.org/W3158834934",
        "https://openalex.org/W3091880950",
        "https://openalex.org/W3015529501",
        "https://openalex.org/W2963891433",
        "https://openalex.org/W6681588610",
        "https://openalex.org/W4311847607",
        "https://openalex.org/W6675944832",
        "https://openalex.org/W2965612779",
        "https://openalex.org/W4205695064",
        "https://openalex.org/W4287322212",
        "https://openalex.org/W3174697615",
        "https://openalex.org/W6791883635",
        "https://openalex.org/W3091002423",
        "https://openalex.org/W2757763665",
        "https://openalex.org/W3217155755",
        "https://openalex.org/W3010940303",
        "https://openalex.org/W4207078300",
        "https://openalex.org/W4200154784",
        "https://openalex.org/W4225338086",
        "https://openalex.org/W3174231090",
        "https://openalex.org/W3213596592",
        "https://openalex.org/W4324378562",
        "https://openalex.org/W3021294679",
        "https://openalex.org/W6762913911",
        "https://openalex.org/W4385573570",
        "https://openalex.org/W3204892645",
        "https://openalex.org/W4283382328",
        "https://openalex.org/W2998184481",
        "https://openalex.org/W3193706068",
        "https://openalex.org/W3160554854",
        "https://openalex.org/W2011722134",
        "https://openalex.org/W3196324893",
        "https://openalex.org/W3213512781",
        "https://openalex.org/W6605449825",
        "https://openalex.org/W2150290224",
        "https://openalex.org/W4210764005",
        "https://openalex.org/W4294214983",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W6771509786",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W4229059586",
        "https://openalex.org/W2202196307",
        "https://openalex.org/W4377713204",
        "https://openalex.org/W3135613089",
        "https://openalex.org/W6770059428",
        "https://openalex.org/W4281732886",
        "https://openalex.org/W4288026527",
        "https://openalex.org/W2984353870",
        "https://openalex.org/W3098311904",
        "https://openalex.org/W2108501770",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W2978426779",
        "https://openalex.org/W2809456172",
        "https://openalex.org/W136732505",
        "https://openalex.org/W3034749675",
        "https://openalex.org/W2989347554",
        "https://openalex.org/W3115187525",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2886186188",
        "https://openalex.org/W2293363371",
        "https://openalex.org/W4302435280",
        "https://openalex.org/W2754613063"
    ],
    "abstract": "This study proposes a novel semi-supervised multi-label emotion classification approach for French tweets based on pseudo-labeling. Human subjectivity in emotional expression makes it difficult for a machine to learn. Therefore, it necessitates training supervised learning models on large datasets annotated by multiple annotators. However, creating such datasets can be costly and time-consuming. Moreover, aggregating annotations from multiple annotators to capture their collective emotional state adds complexity to the task. Semi-supervised learning techniques have shown effectiveness with limited datasets. Furthermore, Large language Models (LLMs), particularly Chat-GPT, have demonstrated superior annotation accuracy compared to annotations obtained from crowdsourcing platforms, when both are evaluated against expert-annotated data. This work introduces a novel approach for multi-label emotion classification of French tweets by leveraging pseudo-labels generated through Chat-GPT, a robust large language model. Using zero-shot, one-shot, and few-shot learning techniques, Chat-GPT annotates the unlabelled part of our dataset. These Chat-GPT-annotated pseudo-labels are then merged with manual annotations, facilitating the training of a multi-label emotion classification model via semi-supervised learning. Furthermore, within the context of our research, we present a new French tweet dataset, containing testimonials from people affected by an urban industrial incident. This dataset features 2,350 tweets, each manually annotated by three human annotators based on 8 pre-identified emotions. Benchmark results are presented for multi-label emotion classification models employing both fully supervised and semi-supervised approaches with pseudo-labeling. Our findings demonstrate the superiority of our approach for multi-label emotion classification over standard pseudo-labeling and an established baseline.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nPseudo-labeling with Large Language Models for\nMulti-label Emotion Classification of French\nTweets\nUsman Malik1, Simon Bernard1, Alexandre Pauchet1, Clement Chatelain1, Romain\nPicot-Clemente2, Jérôme Cortinovis3\n1Univ Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Univ, LITIS UR 4108, F-76000 Rouen, France\n2Saagie, Rouen, France\n3Atmo Normandie, Rouen, France\nCorresponding author: Simon Bernard (simon.bernard@univ-rouen.fr).\nABSTRACT This study proposes a novel semi-supervised multi-label emotion classification approach for\nFrench tweets based on pseudo-labeling. Human subjectivity in emotional expression makes it difficult for a\nmachine to learn. Therefore, it necessitates training supervised learning models on large datasets annotated\nby multiple annotators. However, creating such datasets can be costly and time-consuming. Moreover, ag-\ngregating annotations from multiple annotators to capture their collective emotional state adds complexity to\nthe task. Semi-supervised learning techniques have shown effectiveness with limited datasets. Furthermore,\nLarge language Models (LLMs), particularly Chat-GPT, have demonstrated superior annotation accuracy\ncompared to annotations obtained from crowdsourcing platforms, when both are evaluated against expert-\nannotated data.\nThis work introduces a novel approach for multi-label emotion classification of French tweets by leveraging\npseudo-labels generated through Chat-GPT, a robust large language model. Using zero-shot, one-shot, and\nfew-shot learning techniques, Chat-GPT annotates the unlabelled part of our dataset. These Chat-GPT-\nannotated pseudo-labels are then merged with manual annotations, facilitating the training of a multi-\nlabel emotion classification model via semi-supervised learning. Furthermore, within the context of our\nresearch, we present a new French tweet dataset, containing testimonials from people affected by an urban\nindustrial incident. This dataset features 2,350 tweets, each manually annotated by three human annotators\nbased on 8 pre-identified emotions. Benchmark results are presented for multi-label emotion classification\nmodels employing both fully supervised and semi-supervised approaches with pseudo-labeling. Our findings\ndemonstrate the superiority of our approach for multi-label emotion classification over standard pseudo-\nlabeling and an established baseline.\nINDEX TERMSMulti-label Emotion Classification, Semi-supervised Learning, Pseudo-labeling, Chat-GPT\nI. INTRODUCTION\nEmotion classification in textual data involves identifying the\nlatent emotional states, such as happiness, sadness, anger,\nfear, etc. in a text. This distinct analysis of emotions dif-\nfers from sentiment analysis, which predominantly focuses\non ascertaining the overall polarity of a text, encompassing\npositive, negative, or neutral sentiments [1]. In contrast, emo-\ntion classification aims to pinpoint and categorize the precise\nemotional expressions conveyed within a text, providing a\nmore nuanced understanding of the underlying affective states\n[2].\nEmotion classification is frequently approached as a multi-\nlabel classification problem involving two or more emotion\nlabels [3], [4]. Although this strategy facilitates the detection\nof multiple emotional states in text, it also introduces com-\nplexity to the task, due to the subjective nature of expressing\nemotions through language. In the context of emotion clas-\nsification, individuals commonly employ identical words to\nconvey distinct emotions or use different words to express\na single emotion [5], [6]. To address this subjectivity, data-\ndriven models for emotion classification necessitate datasets\nannotated by multiple annotators, thereby incurring signifi-\ncant costs and time requirements.\nFurthermore, data annotated by multiple annotators rise\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe challenge of annotation aggregation. The inter-annotator\nagreement, which quantifies the level of consensus among\nannotators for a given dataset, is frequently observed to be low\n[7], [8]. Consequently, it is imperative to employ a suitable\napproach to aggregate annotations from multiple annotators,\nconsidering the specific requirements of the problem under\nconsideration.\nSemi-supervised approaches are often employed to ad-\ndress the challenge of limited datasets. Among these ap-\nproaches, pseudo-labeling has demonstrated value to con-\nstruct emotion classification models when confronted with\nlimited datasets [9], [10]. Pseudo-labeling involves training\na supervised learning model on an annotated dataset, us-\ning this model to predict labels for an unannotated dataset\nand subsequently incorporating these predictions, known as\npseudo-labels, back into the training set to improve the model\nperformance. This technique serves as a mean to increase the\nsize of the training data and improve the overall classification\naccuracy. However, the success of pseudo-labeling depends\non the reliability of the classifier trained on the annotated data,\nwhich in turn requires a sufficiently large and well-annotated\ntraining set. The inherent complexity of multi-label emotion\nclassification further exacerbates the annotation costs.\nAn alternative method to augment the size of annotated\ndata is to leverage pre-trained models, which are machine\nlearning models trained on large datasets and are available\nfor reuse. To this end, preliminary studies on the use of pre-\ntrained large language models such as Chat-GPT for anno-\ntation, have shown promising potential [11]. This finding\npresents a prospect to leverage large language models to\npseudo-label unannotated datasets, thereby enhancing overall\nclassification performance across various tasks.\nThis article introduces a semi-supervised multi-label emo-\ntion classification approach for French tweets. The approach\nconsists in generating pseudo-labels for unannotated data us-\ning Chat-GPT and merging the pseudo-labels with manual an-\nnotations to train a multi-label emotion classification model.\nIn the applicative context of this study, a new publicly avail-\nable dataset is presented, consisting of French tweets related\nto industrial accidents. The dataset is manually annotated by\nthree annotators for eight distinct emotions and can be used\nfor various emotion classification tasks. Detailed guidelines\nfor annotation and an approach to aggregating annotations\nfrom multiple annotators are provided.\nBenchmark study is presented for the multi-label emotion\nclassification model trained on manually annotated tweets\nfrom the dataset. Additionally, results for models trained\nusing pseudo-labeling techniques with data annotated from\nChat-GPT using zero-shot, one-shot, and few-shot learning\napproaches are also included.\nThe two contributions of this article are the following.\n• We propose a novel semi-supervised multi-label emo-\ntion classification approach that exploits pseudo-\nlabelled data from Chat-GPT, annotated using zero-shot,\none-shot, and few-shot learning.\n• We present a publicly available 1, manually annotated\nFrench tweets dataset for multi-label emotion classifica-\ntion, accompanied by detailed annotation guidelines and\nan annotation aggregation procedure. Benchmark results\nfor multi-label emotion classification are also presented.\nThese contributions provide a road map to improve\nthe multi-label emotion classification model with pseudo-\nlabeling using LLM pre-trained model like chat-GPT. In this\nstudy, this is applied to AI-assisted crisis management in the\ncontext of an industrial accident in an urban environment.\nFurther details about this applicative context are given in\nSection III.\nThis article is divided into 6 sections. Section II provides a\nliterature review of the current state of the art in multi-label\nemotion classification. Section III presents the French tweet\ndataset for multi-label emotion classification. Section IV pro-\nvides detailed explanation of the proposed approach. Ex-\nperimental details and results are discussed in Section V.\nFinally, Section VI concludes this article and presents future\ndirections of the study.\nII. LITERATURE REVIEW\nMulti-label emotion classification has been the target of re-\nsearch since the inception of natural language processing\ntechniques. Over time, this area of study has experienced\na significant upsurge in attention, driven notably by the in-\ntroduction of transformer architectures and large language\nmodels. Nonetheless, the advancement of supervised learn-\ning approaches necessitates substantial annotated datasets,\na resource-intensive and time-consuming endeavor. In re-\nsponse, recent efforts within the field of semi-supervised\nlearning have been dedicated to addressing the challenges\nposed by limited datasets. This research effort resides at the\nintersection of two domains: multi-label emotion classifica-\ntion in text and semi-supervised learning, with a specific\nfocus on the concept of pseudo-labeling.\nIn this context, this section provides an overview of existing\napproaches for multi-label emotion classification in texts.\nThe challenges underlying this task, as well as the solutions\navailable in the literature, are discussed, with a particular\nfocus on semi-supervised learning and pseudo-labeling.\nA. MULTI-LABEL EMOTION CLASSIFICATION\nHistorically, multi-label emotion classification has relied on\nheuristic approaches, including the use of emotion lexicons\nand manual feature extraction techniques [12]. These meth-\nods, whereas effective to some extent, often struggle to cap-\nture the complex nuances of emotions expressed in text. With\nthe recent emergence of deep learning and the advent of\nattention mechanisms, the current state-of-the-art in multi-\nlabel emotion classification has shifted towards the adoption\nof deep learning techniques.\nMulti-label emotion classification has leveraged convolu-\ntional (CNN) and recurrent (RNN) neural networks [12], [16].\n1https://github.com/smnbrnrd/EmoDIFT\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWhile effective for short text sequences, their performance\ntends to suffer when handling longer sequences, often at-\ntributed to memory loss resulting from vanishing gradient\nproblem [17], [18].\nGraph Neural Networks (GNNs) have gained attention in\nrecent research for multi-label emotion classification due to\ntheir ability to capture dependencies and interactions among\nemotional states, by leveraging structural relationships [19].\nHowever, despite promising results, GNNs have limitations\nfor this type of task, in terms of interpretability, scalability,\ngeneralization, and incorporation of external knowledge [19].\nAttention mechanisms and transformer architectures have\nthen emerged as powerful techniques to achieve state-of-the-\nart results in various natural language processing (NLP) tasks,\nincluding multi-label emotion classification [12], [20]. Trans-\nformers operate by employing self-attention mechanisms,\ncapturing contextual relationships between tokens (words) in\na sequence. This is accomplished through a mechanism that\nallows each word to attend to relevant parts of the input,\nresulting in a contextualized representation for each word.\nThis attention-based approach has demonstrated exceptional\nperformance, surpassing traditional CNN, RNN, and GNN\nmodels, especially in tasks involving long texts [21]–[23].\nUnfortunately, despite promising results of deep learning\nfor various text classification tasks, their performance often\ndeteriorates when confronted with limited annotated data\nfor training [24]. To alleviate this issue, transfer learning\nis commonly employed, where models pre-trained on large-\nscale datasets are fine-tuned on smaller annotated datasets\n[12]. However, transfer learning may not fully address the\nchallenge of limited labelled data, as the models may still\nstruggle to generalize well in the absence of sufficient labelled\ninstances [25].\nIn the context of text classification, a huge amount of data\nis available on the internet, particularly on social media. How-\never, this data is normally not annotated, and manual annota-\ntion can be expensive. One promising approach to tackle the\nlimited annotated data challenge is the use of semi-supervised\nlearning techniques [26]. Semi-supervised learning aims to\nleverage both labelled and unlabelled data during training,\nallowing the model to learn from the additional unlabelled\ninstances. In the subsequent section, we will provide a brief\noverview of these semi-supervised learning techniques and\ntheir potential applications in addressing the challenges posed\nby limited annotated data in multi-label emotion classifica-\ntion tasks.\nB. SEMI-SUPERVISED LEARNING FOR TEXT\nCLASSIFICATION\nSemi-supervised learning is an approach that combines a\nsmall set of labelled data with a larger set of unlabelled data\nduring model training [9]. In the context of text classifica-\ntion and more particularly multi-label emotion classification,\nsemi-supervised learning techniques have shown promise in\naddressing the challenge of limited annotated data [27], [28].\nExisting works divide semi-supervised learning approaches\ninto four main categories: graph-based, unsupervised prepro-\ncessing, intrinsically semi-supervised approaches, and wrap-\nper methods. [43].\nGraph-based approaches for semi-supervised learning in-\nvolve constructing a graph that represents the labelled and\nunlabelled data points, with edges denoting their similarity.\nVarious similarity measures, such as cosine similarity or\nEuclidean distance, can be used for graph construction. By\npropagating the labels of the labelled data points along the\ngraph edges, the labels of the unlabelled data points can be\ninferred [48]. This framework allows for leveraging the graph\nstructure to enhance classification performance in a semi-\nsupervised learning setting. Graph-based approaches have\nbeen widely used for semi-supervised text classification of\nnews articles, social media texts, web pages, public sentiment,\nand scientific articles [49]–[53].\nUnsupervised preprocessing techniques employ a two-\nstep approach: unsupervised learning followed by supervised\nlearning [43]. In the first step, an unsupervised algorithm\nextracts meaningful features from input data, capturing un-\nderlying patterns without relying on labelled information.\nCommon techniques employed in this step include clustering,\ndimensionality reduction, and autoencoders. In the second\nstep, preprocessed data is fed to a supervised algorithm,\nwhich maps preprocessed features to target labels using la-\nbelled data, optimizing model performance. Unsupervised\npreprocessing techniques such as unsupervised feature ex-\ntraction [29]–[31] and cluster-then label [32]–[34] are widely\nemployed for text classification tasks.\nIntrinsically, semi-supervised approaches are designed to\nincorporate both labelled and unlabelled data during the\nlearning process [35]–[37]. These methods leverage a com-\nbination of labelled and unlabelled loss terms to optimize\nthe model performance. By jointly considering labelled and\nunlabelled data, the model can learn better representations\nand improve generalization. Techniques such as entropy min-\nimization [38], consistency regularization [39], and genera-\ntive models [40] are commonly used to effectively exploit\nunlabelled data. Intrinsically semi-supervised methods have\nproven effective in text classification scenarios with limited\nlabelled data [41], [42].\nWrapper methods in semi-supervised learning involve a\nsequential process where a subset of labelled data is used for\ninitial model training, followed by performance evaluation on\na separate validation set. Subsequently, the trained model is\nemployed to generate predictions on the unlabelled data, and\na subset of instances with high prediction confidence or low\nuncertainty scores is selected to augment the labelled set. This\ncombined labelled data, comprising the original labelled data\nalong with the newly selected instances, is then utilized to\nretrain the model [43].\nPseudo-labeling is one of the most commonly used wrap-\nper methods for semi-supervised learning [44], [45]. Pseudo-\nlabeling involves assigning temporary labels to the unlabelled\ndata based on the predictions made by a pre-trained model.\nThese pseudo-labels are then processed as additional labelled\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ndata and combined with the original labelled data for model\ntraining. The pre-trained model is often trained using only\nthe labelled data and is used to make predictions on the unla-\nbelled instances. Pseudo-labeling has emerged as a prominent\ntechnique for enhancing the performance of text classification\ntasks, particularly in scenarios with limited datasets [10],\n[54]–[56].\nThe existing semi-supervised approaches have their own\nadvantages and drawbacks. While graph-based approaches\nfor semi-supervised learning do offer advantages by utilizing\nthe relationships between data points, they do come with cer-\ntain limitations. These methods can become computationally\nintensive, which can become an obstacle when working with\nlarge datasets. Moreover, they demand careful consideration\nin terms of selecting appropriate similarity measures and\nconstructing graphs [57]. Alternatively, unsupervised prepro-\ncessing methods heavily rely on the performance of the un-\nsupervised learning algorithm for feature extraction. This can\nintroduce challenges in interpreting the outcomes effectively.\nFurthermore, intrinsically semi-supervised methods that in-\nherently integrate both labelled and unlabelled data necessi-\ntate the tuning of numerous parameters, such as loss terms\nand regularization techniques. Often, such parameter tuning\nrequires a substantial annotated dataset to yield meaningful\nresults.\nAmong the approaches discussed in this section, pseudo-\nlabeling has emerged as a highly effective and popular tech-\nnique in semi-supervised learning for several reasons. Firstly,\npseudo-labeling is known for its computational efficiency,\nas it does not require complex graph construction or fea-\nture extraction algorithms [46]. Secondly, pseudo-labeling\noffers flexibility by being compatible with various models\nand datasets. It can be seamlessly integrated with different\nmachine learning algorithms and adapted to different domains\nand problem settings [45]. Furthermore, pseudo-labeling has\nproven its effectiveness by achieving state-of-the-art perfor-\nmance on various semi-supervised learning benchmarks [47].\nOverall, pseudo-labeling stands out as a preferred choice\nin semi-supervised learning due to its simplicity, efficiency,\nflexibility, and remarkable performance in various domains.\nIn this work, we propose a semi-supervised pseudo-\nlabeling approach for multi-label emotion classification. In\nthis context, the next section reviews recent pseudo-labeling\napproaches for text classification.\nC. PSEUDO-LABELING FOR TEXT CLASSIFICATION\nPseudo-labeling involves predicting labels for unannotated\ndata using a model trained on annotated data. These predicted\nlabels, known as pseudo-labels, are then merged with the\nannotated training set to retrain the model. Pseudo-labeling\nin most cases, improves the overall classification performance\nof the model.\nExisting works on semi-supervised learning with pseudo-\nlabeling can be grouped into three categories: traditional\npseudo-labeling, lexicon-based pseudo-labeling, and pseudo-\nlabeling with data augmentation.\nTraditional pseudo-labeling approaches involve predicting\nlabels for unannotated data using a model trained on anno-\ntated data. These predicted labels, known as pseudo-labels,\nare then merged with the annotated training set to retrain\nthe model. To enhance the effectiveness of this process, tra-\nditional pseudo-labeling techniques commonly incorporate\nmechanisms designed to identify the most reliable labels.\nBy doing so, they effectively filter out noisy pseudo-labels\nthat have the potential to undermine the performance of\nmodels trained using such pseudo-labelled data. To this end,\nLi et al. propose S2TC-BDD (Semi-Supervised Text Clas-\nsification with Balanced Deep Representation Distributions)\n[54]. S2TC-BDD achieves improved accuracy in predicting\npseudo-labels by estimating variances over both labelled and\npseudo-labelled texts. Mekala et al. propose LOPS (Learn-\ning Order Inspired Pseudo-Label) for weakly supervised text\nclassification [59]. They leverage the learning order of sam-\nples to estimate the probability of incorrect annotation, based\non the hypothesis that models prioritize memorizing samples\nwith clean labels before those with noisy labels.\nLexicon-based pseudo-labeling involves a lexicon or a\npredefined set of rules to allocate labels to unlabelled data.\nThis strategy exploits the lexicon to recognize keywords or\npatterns within the text that correspond to particular labels.\nSubsequently, these identified patterns and keywords guide\nthe assignment of labels to the unannotated data. Hwang &\nLee introduce a lexicon-based pseudo-labeling method using\nexplainable AI (XAI) for sentiment analysis [55]. The method\ngenerates a lexicon of sentiment words based on explainabil-\nity scores and calculates the confidence of unlabelled data\nusing this lexicon. Jimenez et al. propose an approach that\nleverages named entity recognition (NER) based lexicon and\nsubjectivity measures to discern news from non-news content\non websites [61].\nData Augmentation-Based Pseudo-Labeling encompasses\ngenerating synthetic labelled data using data augmentation\ntechniques and then using a classifier to filter the instances\nwith high-confidence pseudo-labels. The filtered instances\nare merged with the original training data for pseudo-labeling.\nAs an example, Yang et al. present STCPC (Small-sample\nText Classification model using Pseudo-label fusion Cluster-\ning) that combines pseudo-labeling and data augmentation\ntechniques to improve text classification with limited labelled\ndata [10]. Wang & Zou combine pseudo-labeling and the\nMixMatchNL data enhancement technique [58] to predict\nlabel data and address the imbalance in short text datasets\n[56]. Anaby-Tavor et al. propose LAMBADA (Language-\nModel-Based Data Augmentation) a GAN-based approach\nfor data augmentation in text classification [62]. They refine\na pre-trained GAN to create synthetic labelled data and merge\nit with the original dataset for pseudo-labeling.\nIn addition, it is observed that while some studies have fo-\ncused on sentiment detection in public reviews (e.g., Amazon,\nYelp, and IMDB), there is a scarcity of research addressing\nmulti-label emotion classification. The limited exploration\nof pseudo-labeling for text multi-label emotion classification\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ncan be attributed to the challenge of subjective and overlap-\nping labels. Successfully applying pseudo-labeling to these\nintricate tasks necessitates larger training sets annotated by\nmultiple annotators, which may be challenging to acquire due\nto time and cost constraints, thus limiting the effectiveness of\nexisting pseudo-labeling techniques.\nThis research paper aims to address the limitations of\ncurrent semi-supervised learning approaches when applied to\ncomplex problems such as multi-label emotion classification.\nSpecifically, we propose a novel semi-supervised learning\ntechnique based on traditional pseudo-labeling that lever-\nages Large Language Models (LLMs), such as ChatGPT, to\npredict labels for unlabelled data. These pseudo-labels are\nthen integrated into the training process to enhance learning\nperformance. We refrain from utilizing the lexicon-based\npseudo-labeling approach due to its reliance on constructing\na lexicon linked to a specific domain, which may lead to\ngeneralization issues. Our intention is to introduce a more\nadaptable approach that can be employed for text classifica-\ntion tasks beyond the scope of our current study. Furthermore,\npseudo-labelling via data augmentation is not appropriate for\nour particular problem, as generating ‘‘fake yet realistic’’\ntweets that accurately capture different human emotions is an\nopen problem that may be an interesting direction for future\nresearch, but is outside the scope of this work.\nAmong traditional pseudo-labeling approaches, we have\nchosen the LOPS model by Mekala et al. [59] as the state-\nof-the-art baseline among traditional pseudo-labeling ap-\nproaches, owing to its generalization capabilities and less\ndependency on hyperparameters. We opt not to consider the\nmethod by Li et al. for benchmarking due to its substantial\nreliance on computationally expensive hyperparameters.\nThe next section provides a comprehensive overview of\nthe dataset employed in our research work for multi-label\nemotion classification in French tweets. We describe the pro-\ncess of data collection, the annotation protocol employed, the\ninter-annotator agreement, and the post-processing technique\nimplemented on the collected data.\nIII. MULTILABEL EMOTION CLASSIFICATION DATASET\nOF FRENCH TWEETS\nThis research is part of a larger project that intends to de-\nvelop a model capable of detecting public emotions from\nFrench tweets in real-time during industrial accidents. The\nultimate goal is to address both the immediate and long-\nterm impacts of the disaster and adapt crisis management\nstrategies based on public emotions. However, developing\nsuch a model requires a dataset containing French tweets from\nthe public during and after industrial accidents. To the best\nof our knowledge, no existing French dataset contains tweets\nannotated with multiple emotion labels. This motivates us\nto develop a first-of-its-kind multi-label emotion-annotated\ndataset of French tweets.\nThe Lubrizol factory fire, which occurred on September\n26, 2019, in Rouen, France, serves as a notable example of an\nindustrial accident. The incident resulted in significant dam-\nages due to the combustion of chemicals and fuel additives,\nleading to a substantial response on social media platforms\n[14], [65]–[67]. In this study, we collected tweets related\nto this factory fire incident, manually annotated them with\nemotions, and post-processed them to train a semi-supervised\nmulti-label emotion classification model. This dataset stands\nas a pioneering contribution within the realm of multi-label\nemotion classification datasets for French tweets. The next\nsection describes the data collection and annotation protocol.\nA. DATA COLLECTION & ANNOTATION\nThe dataset consists of 90,496 tweets from 21 September\n2019 to 30 December 2020, all of which mention the term\n\"Lubrizol\". These tweets originate from a diverse range of\nsources, encompassing both individuals and organizations.\nFor this work, we consider that emotions are expressed in\ntweets from the public only. So our first task is to filter\nthe tweets posted by organizations, public authorities, the\npress, and so on. This filtering task is not trivial, but presents\nfar fewer difficulties than our emotion detection task, since\nit involves binary classification for which the two classes\nare unambiguous: population or not. This is why we simply\nmanually annotated 300 tweets in order to fine-tune and test\na pre-trained CamemBERT model [75], which achieves a F1\nscore of 0.90 on the test set. From the remaining unlabelled\ndataset, predictions with a probability threshold > 0.80 are\nretained and the remaining instances are filtered out. The\nmain goal of this filtering is to exclude as many press-related\ntweets as possible from our dataset, in order to reduce the\npotentially significant imbalance between neutral tweets and\nthose that convey emotions. This is the reason why we do\nnot seek to achieve the highest possible accuracy for this\nfiltering step. At the end of this filtering stage, a set of 12,508\ntweets from individuals is obtained. From this subset, 2,350\ntweets are randomly selected and manually annotated by a\ntotal of 11 human annotators, including university professors,\nresearchers, engineers and students, all of whom are native\nFrench speakers.\nThe annotators are instructed to read and evaluate tweets,\nwith the goal of identifying a maximum of three emotions\nfrom a predefined set of emotion labels. If the emotions ex-\npressed within the tweet belong to multiple emotion registers,\nthe annotator is required to rank them in order of importance.\nThis order is based on the predominance of expressed emo-\ntions within the tweet. Specifically, the label associated with\nthe dominant emotion is assigned a value of 1, while the\nsecond and third most dominant emotions are assigned values\nof 2 and 3, respectively. The ultimate goal of the annotation is\nto identify expressions of opinion, whether they are explicit\n(I’m scared ), or implicit ( Those are hydrocarbons in there,\nthat’s dangerous), taking only the semantic information into\nconsideration.\nThe ranking approach is utilized to annotate tweets with\nemotions, thereby offering a solution to a number of problems\nsuch as 1) multi-class classification where the emotion pos-\nsessing the highest rank is treated as the target class; 2) multi-\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nlabel classification where emotion ranks can be treated as\nthe target labels, or alternatively, ranks can be converted into\none-hot encoded target labels; and 3) multi-label regression\nproblem where ranks correspond to the degree of dominance\nof emotions.\nThe negative emotion labels are selected from the second\ntier of Plutchik’s model of emotions, due to their balanced\nrepresentation between the top-tier and low-tier emotions\n[68]. This choice allows to capture the nuanced nature of\nemotions within the broader emotional framework proposed\nby Plutchik. Given the nature of our case study, 6 emotions\nare selected from this categorization model, namely anger,\ndisgust, fear, surprise, sadness, and mistrust 2. In addition,\nirony has been included as an optional label since irony has\nbeen shown to be an important clue for emotion classification\n[69]. Furthermore, the labels neutral and inexploitable have\nbeen added to allow annotators to identify tweets that do not\nexpress aany particular emotion or that are not relevant to our\ncase study (for example, a tweet from a news media that has\nnot been filtered in the pre-processing phase).\nAs discussed, emotion annotation poses a complex chal-\nlenge owing to the subjective nature of emotional expres-\nsion hindering consensus among annotators. Moreover, the\nbrevity of Tweets and the common use of colloquial language\ncontribute to the ambiguity of the emotions expressed. To\nexamine these assumptions and determine the level of agree-\nment among annotators in our dataset, the following section\npresents an analysis of inter-annotator agreement (IAA) for\nour dataset.\nB. INTER ANNOTATOR AGREEMENT\nThe inter-annotator agreement (IAA) refers to the overall\ndegree of agreement between multiple annotators for the un-\nderlying annotations. Among statistical approaches for IAA,\nwe select Krippendorff’s Alpha [71], and Fleiss’ Kappa [72].\nThe values for these measures range from 0 to 1, where 0\nsignifies no agreement and 1 represents complete agreement.\nWe select these metrics since they satisfy the two annotation\ncriteria of our annotation protocol: (1) they can be used to find\nIAA between more than two annotators, and (2) they can find\nIAA for ranked data.\nThe IAA results for Krippendorff’s Alpha and Fleiss’\nKappa are presented in Table 1. Krippendorff’s Alpha exhib-\nited a mean value of 0.1774, suggesting a notably low level\nof agreement among the annotators [73]. Similarly, Fleiss’\nkappa values aligned closely with Krippendorff’s alpha, av-\neraging at 0.1773 across all emotions, indicating minimal to\nno agreement [74].\nA close analysis of IAA reveals that specific emotions,\nnamely irony, fear , and anger, exhibit Krippendorf’s Al-\npha and Fleiss’ Kappa values ranging from 0.20 to 0.266,\nwhich suggest a moderate level of agreement, indicating a\n2The original dataset is in French language with the following emotions\nlabels: colère (anger), dégout (disgust), peur (fear), méfiance (mistrust),\ntristesse (sadness), surprise (surprise), ironie (irony), neutre (neutral)\nTABLE 1. Inter-annotator agreement results (Kripp = Krippendorffs,\nA = Alpha, K = Kappa)\nEmotion Kripp’s A Fleiss’ K\nAnger 0.2247 0.2245\nDisgust 0.1410 0.1409\nFear 0.2574 0.2572\nMistrust 0.1946 0.1945\nSadness 0.1425 0.1424\nSurprise 0.1093 0.1092\nIrony 0.2662 0.2661\nNeutral 0.1631 0.1630\nInexploitable 0.0982 0.0981\nMean 0.1774 0.1773\nfair degree of consensus among multiple annotators for these\nparticular emotions.\nIn addition to statistical measures, we also conduct heuris-\ntic tests to further investigate the IAA in our study. The\nresults indicate that only 4.83% of tweets exhibit complete\nagreement among the three annotators across all ranks and\nall emotions. For 5.48% of the tweets, the annotators agree\non the emotion labels but differ in their assigned ranks. It is\nnoteworthy that these percentages are relatively high when\nconsidering the agreement between any two annotators out\nof the three. Breaking down the results by individual ranks,\nthe annotators exhibit the highest agreement of 16.74% for\nrank 1, followed by 1.44% for rank 2. However, for rank 3,\nthere is no agreement observed among all three annotators.\nFurther investigation of the agreement for rank 1 reveals that\nanger, mistrust, and irony demonstrate the highest agreement\npercentages of 40%, 23%, and 12%, respectively. This sheds\nlight on the emotions that tend to exhibit stronger consensus\namong annotators for rank 1.\nThe statistical and heuristic analysis of IAA exposes the\nsubjective aspect of human emotion expression. This finding\nunderscores the significance of employing a reliable aggre-\ngation technique that can effectively capture the emotional\nstate reflected by multiple annotators. The following section\ndetails the post-processing approach utilized in this study for\nthat purpose.\nC. POST-PROCESSING ANNOTATIONS\nIn this study, we focus on the multi-label classification of\nemotions in tweets. Annotating tweets with emotion ranks\nenables us to estimate a degree of presence for each pre-\ndefined emotion. This also allows to reduce annotator bias,\nwhich is relatively high given the subjectivity of the task.\nThe next step is therefore to merge the three human anno-\ntations of each tweet to obtain a one-hot encoded vector,\nwhich identifies only the emotions that have reached a certain\ndegree of prominence. It is worth noting that such annotations\nwould enable us to approach the problem from other learning\nparadigms: multi-class classification, multi-output regression\nor even learning to rank. The funded project in which this\nstudy is part focuses on the multi-label classification of emo-\ntions in tweets.\nV oting and mean calculation are common approaches to\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\naggregate annotations from multiple annotators [13]. How-\never, they are unsuitable for ranked annotations due to (i)\nhigher mean values for lower ranks, and (ii) the lack of ranked\ninformation in the default voting mechanism. For example,\ntwo annotators assigning rank 1 to a text is not equivalent to\none assigning rank 1 and the other assigning rank 2 or 3.\nThis problem is addressed in [70] by introducing an ap-\nproach that transforms ranked annotations from multiple an-\nnotators into normalized degrees of class membership. These\nvalues range from 0 to 1 and sum up to 1, where higher\nvalues correspond to a more pronounced presence of emotion.\nThey employ a voting mechanism to select tweets in which a\nminimum of two annotators allocate any rank to at least one\nlabel. For the selected tweets, the normalized degrees of class\nmembership are calculated to derive the final aggregated val-\nues. The approach was evaluated on the TREMoLO dataset\n[70], which contains tweets annotated with language registers\nindicating whether a tweet is formal, neutral, or casual. These\nlabel annotations are less subjective compared to emotion\nclassification, resulting in a less complex problem setting. For\nthis reason, in our study, we have adopted and modified this\nmethod for our problem, as explained below.\nFirst, ranks are transformed into degrees of belonging using\nthe formula in [70]. For the sake of brevity, we do not give\nthe formula here, but refer the reader to [70] for the details of\nthis calculation. However, tweets are not subsequently filtered\nusing the voting mechanism, since it results in the removal\nof a large number of tweets in our dataset due to the low\nIAA. Moreover, tweets with an unexploitable label exceeding\n0.5 are eliminated from the dataset based on the consensus\nof annotators regarding their informational deficiency. Fur-\nthermore, due to the larger number of annotators and output\nlabels, as well as low IAA, the individual degrees of belonging\nfor specific labels in our dataset become significantly small.\nAs a result, the thresholding approach (>0.5) employed in\n[70] for converting degrees of belonging to one-hot encoded\nvectors is inadequate for emotions in our case. To address\nthis, we adopt an alternative approach by selecting the top\nN emotion labels with the highest degrees of belonging. Our\nchoice of N = 3 is based on two reasons: (i) annotators were\ninstructed to rank up to three emotion labels, and (ii) after\nconducting tests with different N values, we determined that\nN = 3 yields the most optimal results. It is important to note\nthat in the case of equal values for the degrees of belonging,\nboth emotion labels are selected, sometimes resulting in the\nselection of 4 labels.\nFrom the 2,350 manually annotated tweets, 2,215 are re-\ntained after excluding the tweets labelled as unexploitable.\nFigure 1 illustrates the distribution of emotions within the\nannotated tweets, providing a visual representation of their\nprominence. The analysis reveals that the two labels anger\nand mistrust are particularly present in the datasets, namely\nfor 66% and 62% of the 2,215 tweets, respectively. On the\nother hand, sadness and surprise exhibit considerably lower\nrepresentation, accounting for only 10% and 12% of the\ntweets, respectively. This highlights an additional, but ex-\nFIGURE 1. Emotion distribution by tweet percentage. The y-axis\nrepresents the number of tweets. Bar values represent percentages.\npected, difficulty inherent to our case study, namely that all\nthe selected emotions are expressed in very unequal ways in\nour datasets, resulting in highly imbalanced classes.\nThe low IAA values and the highly imbalanced nature\nof the dataset emphasize the necessity for large annotated\ndatasets to develop robust classification models. In scenarios\nlike this, pseudo-labeling approaches that rely on a model\ntrained with a limited number of manual annotations for\npseudo-label prediction exhibit poor performance [80]. The\nprimary cause of the unsatisfactory performance stems from\nthe fact that datasets with minimal IAA for multi-label emo-\ntion classification underscore the inherent subjectivity of the\ntask. When working with limited data, capturing this subjec-\ntivity becomes challenging. As a consequence, pseudo-labels\ngenerated through training on such data tend to be erroneous,\ncontributing additional noise to the training process. This, in\nturn, leads to poor performance, even when compared to a\nfully supervised model.\nA potential solution is to employ pre-trained large language\nmodels that have been trained on extensive datasets sourced\nfrom various origins. These models are better positioned to\ncapture the inherent subjectivity of human nature. This can\npotentially address the challenge of human subjectivity and\nlack of data by generating more informed pseudo-labels that\nwhen combined with the manual training set result in im-\nproved model performance.\nIn this study, we propose a semi-supervised multi-label\nemotion classification approach that uses pseudo-labels from\na pre-trained large language model (Chat-GPT). The follow-\ning section outlines the proposed approach.\nIV. PROPOSED APPROACH\nExisting works have demonstrated Chat-GPT ability for data\nannotation [11]. In the approach we propose in this study,\nwe first annotate a set of unannotated tweets from a dataset\nusing Chat-GPT and then use the Chat-GPT pseudo-labels\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nChat-GPT \nManual\nAnnotations +\nPseudo-\nlabeled\nTweets\nPseudo-\nLabeled\nTweets\nTransformer\nModels \n(Fine-tuned\nLast 4\nLayers)\nManual\nAnnotations\nUnannotated\nTweets\n100 x 50 x 8\n(Feed forward\nnetwork)\n0\n0\n1\n0\n1\n0\n0\n1\nBinary output labels\n(Using\ntop N approach)\nFIGURE 2. Semi-supervised training using pseudo labels from Chat-GPT.\ncombined with manual annotations in the training set to train\nmulti-label emotion classification models.\nChat-GPT prompts offer a high degree of flexibility, ac-\ncommodating a wide range of textual inputs. When it comes\nto annotation, the outcome is significantly influenced by the\nspecific prompt employed. Interestingly, even two prompts\nsharing the same semantic meaning may yield distinct anno-\ntations. This emphasizes the crucial role of careful prompt\nconstruction. Although dealing with the intricacies of prompt\nengineering is beyond the scope of this study, three differ-\nent prompting approaches are explored for Chat-GPT-based\npseudo-labeling, with three different levels of instructions:\n• Zero-shot: The prompt instructs Chat-GPT to select\nemotion labels from a predefined label set without\nproviding any examples from the manually annotated\ndataset.\n• One-shot: The prompt instructs Chat-GPT to select\nemotion labels from the label set, using one example for\neach emotion from the manually annotated dataset.\n• Few-shots: The prompt instructs Chat-GPT to select\nemotion labels from the label set, using two examples\nfor each emotion from the manually annotated dataset.\nFor the sake of reproducibility, we provide in Appendix A the\nexact prompts we used in our experiments for all three cases.\nUsing the above three approaches, a total of 2,800 addi-\ntional tweets are annotated using Chat-GPT. To ensure high-\nquality annotations, tweets where Chat-GPT predicted inex-\nploitable are removed from the dataset. As a result, the zero-\nshot, one-shot, and few-shot approaches retained a total of\n2,767, 2,757, and 2,775 tweets respectively. Consequently,\nfor the sake of comparison, the three pseudo-labelling ap-\nproaches have been applied on the same unlabelled tweets,\nbut have been separately merged with the manual annotations,\nresulting in three different training sets.\nFigure 2 contains an overview of the proposed approach.\nThe dataset is divided into unannotated tweets and manually\nannotated tweets. In the first step, unannotated tweets are\npseudo-labelled with Chat-GPT using one of the three afore-\nmentioned approaches. The pseudo-labelled tweets are then\nmerged with the manual annotations. The merged datasets\ncontaining pseudo-labelled and manually annotated tweets\nare then passed to transformer models for fine-tuning.\nIn this study, two variants (base and large) of the two most\ncommonly used French language transformer models are\ntested for text embedding: CamemBERT [75] and FlauBERT\n[76]. These models are preferred since they have been specif-\nically trained on French and have demonstrated state-of-the-\nart results for classification of French texts [78]. In order to\nalso compare with a multilingual transformer model, experi-\nments are complemented with RoBERTa [77]. It is known that\nfine-tuning this type of Large Language Models is complex in\na low-data regime [81]. It is known that fine-tuning this type\nof Large Language Models is complex in a low-data regime\n[81], [82]. In our experiments, we used the popular solution\nof freezing the first layers of the network and fine-tuning only\nthe last four layers, as this proved empirically to perform best\nin our context.\nThe output from the transformer models is passed to two\ndense neural network (DNN) layers of sizes 100 and 50\n(with ReLU activation functions) respectively, followed by an\noutput layer of size 8, corresponding to 8 emotion labels in the\ndataset. The sigmoid activation function is used in the output\nlayer returning values between 0 and 1 which corresponds to\nthe probability of emotions present in the input tweet. We\nopted not to employ the softmax activation function in the\noutput layer since objective does not involve identifying the\ndistribution of emotions within a tweet. Instead, our focus is to\ndetect in a tweet the presence of each emotion independently.\nSince the target emotion labels are binary values, we em-\nploy a binary cross-entropy loss function for each of the eight\nemotion categories. Formally, the binary cross-entropy loss\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfunction for the i-th emotion is expressed as:\nLi(yi, ˆyi) =−(yi ∗ log(ˆyi) + (1− yi) ∗ log(1 − ˆyi)) (1)\nwhere yi is the binary target label of the i-th emotion and\nˆyi is the corresponding prediction of our model. To assess\nthe overall performance of the model, the individual binary\nlosses for all m = 8outputs are aggregated through a simple\narithmetic mean, yielding the overarching loss metric:\nL = 1\nm\nmX\ni=1\nLi(yi, ˆyi) (2)\nFinally, to ensure the consistency between the post-\nprocessed tweet labels and the output labels, the top 3 prob-\nability values given by the model are set to 1, while the\nremaining of the values are set to 0. Since our dataset is\nimbalanced, both in terms of emotions and ranks, we utilize\nthe F1 measure as evaluation criterion, as it is suitable for\nimbalanced data classification models [79].\nThe proposed approach boasts simplicity in its implemen-\ntation and demonstrates strong potential for generalization\nacross diverse problem scenarios and case studies. Adapting\nthis approach to a different problem setting is a straightfor-\nward process, necessitating only the substitution of a distinct\ndataset and a minor adjustment in the Chat-GPT prompts.\nThe following section explains the experiments and results\nof the proposed approach.\nV. EXPERIMENTS & RESULTS\nThe four sets of experiments performed in this study are the\nfollowing ones:\n• Fully supervised learning (baseline)\n• Pseudo-labeling using Chat-GPT (proposed approach)\n• Standard pseudo-labeling\n• The state-of-the-art LOPs method from [59]\nThe above experiments are performed on the exact same\ndataset where the same set of unlabelled tweets are pseudo-\nlabelled using the proposed approach, the standard pseudo-\nlabeling approach, and the LOPS model.\nThe experimentation process for the proposed approach\nfollows the traditional machine-learning pipeline. The K-fold\ncross-validation approach (K = 5) is used to randomly split the\n2,215 post-processed manual annotations into train-test sets.\nThe pseudo-labels from Chat-GPT are merged with the manu-\nally annotated training set. The merged annotations are passed\nto transformer models (discussed in the previous section)\nwith additional dense layers. Details of the model architecture\nand training hyperparameters are given in Table 2. These\nvalues were first set based on the recommendations given in\n[60], then empirically adjusted to obtain better results on our\nproblem.\nFor a fully supervised learning baseline, a process similar\nto our proposed approach is adopted, however, in this case,\nonly manual annotations are used for training the proposed\nmodel. For the standard pseudo-labeling baseline, a fully\nsupervised model is exploited to make predictions on the\nFIGURE 3. Annotation distribution with manual and pseudo labelled Data.\nunannotated tweets, which are then merged with the manual\nannotations to make predictions on the test set. The model\narchitecture and hyper-parameters for the fully supervised\nand standard pseud-labeling approach remain the same as\nmentioned in Table 2.\nFigure 3 depicts the emotion distribution for the Chat-GPT\nannotated tweets 3. The figure shows that except concerning\nsurprise, the distribution of all the remaining emotions has in-\ncreased in the Chat-GPT annotations. This shift in distribution\ncan potentially be attributed to several factors. One possible\nexplanation is the inherent nature of Chat-GPT training data,\nwhich includes a wide range of conversational content from\nvarious sources. It is highly likely that Chat-GPT encounters\ntraining data where surprise is less prevalent compared to\nother emotion labels. As a result, the model annotates fewer\ntweets with surprise. Moreover, the discrepancy could be due\nto differences in the annotation process. Human annotators\nmight interpret and label emotions differently from how the\nmodel does, leading to variations in distribution. In addition,\nChat-GPT responses might be influenced by prompts pro-\nvided during annotation.\nTable 3 presents the overall results of the models trained\nusing the proposed pseudo-labeling approach with Chat-GPT\nusing zero-shot, one-shot, and few-shot annotations. The ta-\nble also depicts results obtained via (i) standard pseudo-\nlabeling, (ii) the LOPs method, and (iii) fully supervised\ntraining. The results indicate that the CamemBERT-Large\nmodel with Chat-GPT few-shot pseudo-labeling achieves\nthe highest F1 score (0.6662) surpassing the performance\nobtained via standard pseudo-labeling (0.6310), the LOPS\nmethod (0.6523), and the fully supervised approach (0.6538).\nIt is worth noting that the CamemBERT-Large model al-\nways yields the best performance for all the methods used\nin the experiment. Similarly, the Chat-GPT few-shot pseudo-\nlabeling method is also always the most accurate, whatever\nthe language model used.\nThe possible reason for the CamemBERT-Large model\nbetter performance compared to the FlauBERT model could\nbe that FlauBERT is larger than CamemBERT and the\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 2. Model architecture and training parameters\nModel and Parameter Types Description\nPretrained Models TensorFlow versions of the pre-trained transformed models from Hugging Face are used for training.\nOnly the last 4 layers of the models are fine-tuned. The last hidden state of the output is used as the\ninput to the downstream deeply connected layers.\nToken Sizes 512\nAdditional Layers Two deeply connected layers of sizes 100 and 50, followed by an output layer of size 8 appended to the\noutput of Hugging Face Models.\nDrop out Dropouts of 0.25 are used for the pre-trained model and the first deeply connected layer. A dropout of\n0.15 is used for the second densely connected layer.\nActivation Functions Relu activation functions are used for the first two deeply connected layers. A sigmoid function is used\nfor the final output layer.\nOptimizer & Learning Rate Adam optimizer with a learning rate of 5e- 5\nLoss Function Binary Cross Entropy Loss\nTraining Epochs & Batch Size A total of 30 epochs and a batch size of 8 with early stopping after 5 epochs if F1 score does not improve\nfurther. The best model is chosen for prediction on the test set.\nTABLE 3. Mean F1 scores and standard deviations for multi-label emotion classification with Chat-GPT annotations and with baselines and LOPS\napproach. CGPT = Chat-GPT, PL = Pseudo-labeling.\nModel CGPT Zero-shot CGPT One-shot CGPT Few-shots Standard PL LOPS Fully Supervised\nCamemBERT-Large 0.6370 ± 0.0021 0.6649 ± 0.0051 0.6662 ± 0.0042 0.6310 ± 0.0121 0.6523 ± 0.0073 0.6538 ± 0.0255\nCamemBERT-Base 0.6121 ± 0.0081 0.6289 ± 0.0281 0.6274 ± 0.0153 0.6146 ± 0.0413 0.6310 ± 0.0179 0.6201 ± 0.0092\nFlauBERT-Large 0.5920 ± 0.0241 0.6019 ± 0.0075 0.6042 ± 0.0195 0.5812 ± 0.0310 0.5890 ± 0.0012 0.5901 ± 0.0077\nFlauBERT-Base 0.5812 ± 0.0043 0.5875 ± 0.0724 0.5901 ± 0.0043 0.5798 ± 0.0015 0.5890 ± 0.0241 0.5870 ± 0.0085\nRoBERTa-Large 0.5755 ± 0.0167 0.5770 ± 0.0024 0.5846 ± 0.193 0.557 ± 0.0042 0.5810 ± 0.0031 0.5775 ± 0.0087\nRoBERTa-Base 0.5810 ± 0.0043 0.5809 ± 0.0081 0.5943 ± 0.0062 0.5802± 0.0073 0.5874 ± 0.0141 0.5821 ± 0.0110\ncomplexity of FlauBERT may require a more extensive\ndataset for optimal fine-tuning. However, the comparison\nbetween CamemBERT-large and CamemBERT-small shows\nthe opposite behavior where CamemBERT-large, despite\nbeing a larger model, results in better performance than\nCamemBERT-small. The reason for this behavior could be\nthat CamemBERT-large is a compromise between FlauBERT\nand CamemBERT-small models. While it is less complex than\nFlauBERT, it is complex enough to grasp the intricacies of\nour dataset. Furthermore, CamemBERT better performance\ncompared to RoBERTa could be attributed to RoBERTa mul-\ntilingual nature encompassing a broader range of languages,\nwhich could lead to a dilution of its capacity to grasp the\nintricacies of any particular language such as French. Nev-\nertheless, performance comparisons between various large\nlanguage for specific tasks is open to further research.\nIn addition, it is worth noting that the difference in re-\nsults between the one-shot pseudo-labeling and the few-shot\npseudo-labeling approaches is very small. However, when\ncompared to manual annotations, the results obtained through\nzero-shot pseudo-labeling exhibit inferior performance. This\nresult depicts that while a single training example might\nbe enough to achieve the best results, a Chat-GPT prompt\nwithout any example from the training set leads to inferior\nperformance on the test.\nAnother interesting result observed in Table 3 is that fully\nsupervised learning models perform better than standard\npseudo-labeling models. One possible reason could be that\nthe training dataset used for standard pseudo-labeling in this\nstudy is too small. The predicted pseudo-labels could intro-\nduce noise during the semi-supervised training step, result-\ning in reduced performance compared to fully supervised\ntraining. This observation aligns with existing research that\nclaims standard pseudo-labeling with small datasets may lead\nto poor results [80]. Furthermore, it provides support for the\nfoundation of our proposed research, where we employ large\nlanguage models instead of the standard pseudo-labeling ap-\nproach.\nTable 4 depicts the F1-scores of individual emotions\nachieved via CamemBERT-Large with Chat-GPT few-shot\npseudo-labeling. The findings indicate that, with the excep-\ntion of surprise, the classification performance of all other\nindividual emotions demonstrates improvement compared to\nfully supervised training. It is noteworthy that the relatively\nlower performance of surprise may be attributed to its re-\nduced representation in the pseudo-labelled annotations as\ndepicted in Figure 3. This finding illustrates that, similar to\nother deep learning models, there are instances where the\npseudo-labels derived from Chat-GPT annotations might not\nyield optimal performance. This is specially notable for the\npseudo-labels that have lower representation. As previously\ndiscussed, the underlying cause for the reduced occurrence\nof a specific label within Chat-GPT annotations could be\nattributed to a combination of factors, including the charac-\nteristics of data used to train Chat-GPT model from scratch,\nthe annotation process, and the specific prompts employed\nduring annotation.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. Mean F1 scores and standard deviations for individual\nemotions using semi-supervised pseudo-labeling with Chat-GPT\nfew-shots annotations and LOPS model.\nEmotion CGPT Few Shots Fully Supervised\nAnger 0.7932 ± 0.0171 0.7901 ± 0.0100\nDisgust 0.5872 ± 0.0105 0.5851 ± 0.0072\nFear 0.5732 ± 0.0087 0.5121 ± 0.0134\nMistrust 0.7710 ± 0.0174 0.7421 ± 0.0092\nSadness 0.4652 ± 0.0072 0.4142 ± 0.0172\nSurprise 0.4731 ± 0.0189 0.5105 ± 0.0127\nIrony 0.6432 ± 0.0075 0.6137 ± 0.0111\nNeutral 0.4534 ± 0.0271 0.3752 ± 0.0192\nVI. CONCLUSION & FUTURE WORK\nEmotion classification poses a formidable challenge due to\nthe inherent subjectivity of emotions expression. This com-\nplexity is further amplified in the realm of multi-label emo-\ntion classification, where the potential for conveying multiple\nemotions within a single textual unit exponentially magnifies\nsubjectivity. Furthermore, the presence of underrepresented\nemotions within specific contextual scenarios leads to im-\nbalanced datasets. Successful machine learning models for\nmulti-label emotion necessitate a substantial dataset that com-\nprehensively encapsulates the spectrum of human emotional\nexpression. However, the manual annotation of such a dataset\nrequires considerable human supervision, entailing a labori-\nous and costly process.\nThis study introduces a novel semi-supervised pseudo-\nlabeling approach to multi-label emotion classification, lever-\naging the capabilities of large language models. We employ\nChat-GPT to assign multiple emotion labels to unannotated\nFrench tweets in our dataset. Our methodology encompasses\nzero-shot, one-shot, and few-shot techniques to generate\npseudo-labels through Chat-GPT. These pseudo-labels are\nthen merged with manually annotated tweets, facilitating the\ntraining of multi-label emotion classification models. Our\nexperimental results substantiate the superiority of our pro-\nposed pseudo-labeling-driven semi-supervised learning ap-\nproach over baseline and state of the art methods. Notably, our\napproach exhibits enhanced performance on emotion labels\nthat are less frequently represented.\nThe results further demonstrate that both one-shot and few-\nshot Chat-GPT annotation techniques return better results\ncompared to baseline and the state of the art methods. Never-\ntheless, the difference in performance between the one-shot\nand few-shot learning is minimal. Consequently, we recom-\nmend adopting the one-shot approach to minimize the usage\nof Chat-GPT tokens, thus reducing the annotation cost. On the\nother hand, zero-shot pseudo-labeling is not recommended\ndue to its inferior results compared to manual annotations.\nIn addition to proposing a semi-supervised pseudo-labeling\napproach, we contribute a uniquely tailored French language\ndataset of tweets annotated with multiple emotions during\nindustrial accidents. This dataset is the first of its kind in\nFrench language research for text emotion classification and\nserves as the foundation for presenting benchmark results in\nthe domain of multi-label emotion classification of French\ntweets.\nWhile our proposed approach attains state-of-the-art re-\nsults, several future research directions follow from this study.\nFirstly, the efficacy of our approach, as demonstrated with\nChat-GPT, should be evaluated in comparison with other\nadvanced large language models to ascertain potential per-\nformance enhancements. Secondly, the utilization of a more\nextensive number of tweets per emotion in few-shot training\nwarrants investigation to determine whether it yields im-\nproved model performance. Lastly, the introduction of more\ninnovative prompts for Chat-GPT holds the potential to yield\nmore refined annotation outcomes.\nAPPENDIX A PROMPTS USED FOR PSEUDO LABELING\nWITH CHAT-GPT\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5. Prompts used for pseudo-labeling with Chat-GPT using zero-shot, one-shot, and few-shot annotations. Tweets are annotated via OpenAI’s\nGPT-3.5 Turbomodel with default settings except for the temperature attribute which is set to 0 to avoid randomness. OpenAI’s Python API is used to\nmake calls to the model. French emotion labels are used in the prompt.\nAnnotation Type Example Prompt Description\nZero-shot You are a French linguist expert who annotates French tweets with emotions. Pick 4\nor less the most likely emotions from the emotions list to annotate the input French\ntweet.\ninput french tweet= \"C’est l’équivalent d’une marée noire, mais sur terre. Le toit\nen amiante a été pulvérisé, les gens ont respiré les particules... C’est une vraie\ncatastrophe écologique et sanitaire.....\"\nemotions list= Colere, Degout, Peur, Mefiance, Tristesse, Surprise, Ironie, Neutre,\nInexploitable.\nThe response text should only contain a comma-separated list of up to 4 most likely\nemotions present in the input French tweet. The emotions should be selected from the\nemotions list. If you can’t find an emotion from the emotions list, simply return the\nvalue \"Inexploitable\"\nThe prompt entails submitting a tweet\nto Chat-GPT for annotation, with spe-\ncific instructions to select a maximum\nof 4 emotions from a predefined list of\n8 emotions used in manual annotations.\nImportantly, the prompt does not include\nany samples from the training set. Ad-\nditionally, in scenarios where none of\nthe emotions are discernible within the\ntweet, the expected outcome from Chat-\nGPT is inexploitable (unexploitable).\nTweets categorized under the label\ninexploitable are excluded from the\nsubsequent process of semi-supervised\npseudo-labeling, resulting in a total\nof 2767 tweets for zero-shot pseudo-\nlabeling.\nOne-Shot You are a French linguist expert who annotates French tweets with emotions. Pick 4 or\nless most likely emotions from the emotions list to annotate the input French tweet. As\nexamples use tweet examples and corresponding emotions from the training samples\nlist:\ninput french tweet= \"C’est l’équivalent d’une marée noire, mais sur terre. Le toit\nen amiante a été pulvérisé, les gens ont respiré les particules... C’est une vraie\ncatastrophe écologique et sanitaire.....\" https://t.co/HNNfvQmS3P. emotions list =\nColere, Degout, Peur, Mefiance, Tristesse, Surprise, Ironie, Neutre, Inexploitable.\ntraining samples list= [’(Tweet Number 1 = Text of tweet 1) (Emotions = Mefiance,\nDegout, Colere)’, ’(Tweet Number 2 = Text of tweet 2) (Emotions = Peur, Degout,\nColere)’, ’(Tweet Number 3 = Text of tweet 3) (Emotions = Tristesse, Mefiance,\nPeur)’, ’(Tweet Number 4 = Text of tweet 4) (Emotions = Mefiance, Peur, Colere)’,\n’(Tweet Number 5 = Text of tweet 5) (Emotions = Ironie, Surprise, Tristesse)’, \"(Tweet\nNumber 6 = Text of tweet 6) (Emotions = Surprise, Mefiance, Peur)\", ’(Tweet Number\n7 = Text of tweet 7) (Emotions = Ironie, Mefiance, Degout, Colere)’, \"(Tweet Number\n8 = Text of tweet 8) (Emotions = Neutre, Surprise, Colere)\"]\nThe response text should only contain a comma-separated list of up to 4 most likely\nemotions present in the input French tweet. The emotions should be selected from the\nemotions list. If you can’t find an emotion from the emotions list, simply return the\nvalue \"Inexploitable\"\nThe prompt closely resembles the one\nused in zero-shot pseudo-labeling. How-\never, a key distinction lies in the ap-\nproach: here, we randomly choose a sin-\ngle sample tweet representing each emo-\ntion from the training set. Given that a\ntweet can encompass multiple emotions,\nit is plausible that emotions beyond the\nselected one will be present. For instance,\nconsider tweet number 3, primarily cho-\nsen due to its association with the emo-\ntion peur (fear); yet, it concurrently en-\ncapsulates the emotions tristesse (sad-\nness) and mefiance (distrust). A total of\n2757 tweets are used for semi-supervised\npseudo-labeling after the exclusion of\ntweets marked as inexploitable.\nFew-shot You are a French linguist expert who annotates French tweets with emotions. Pick 4 or\nless most likely emotions from the emotions list to annotate the input French tweet. As\nexamples use tweet examples and corresponding emotions from the training samples\nlist:\ninput french tweet= \"C’est l’équivalent d’une marée noire, mais sur terre. Le toit\nen amiante a été pulvérisé, les gens ont respiré les particules... C’est une vraie\ncatastrophe écologique et sanitaire.....\" https://t.co/HNNfvQmS3P. emotions list =\nColere, Degout, Peur, Mefiance, Tristesse, Surprise, Ironie, Neutre, Inexploitable.\ntraining samples list= [’(Tweet Number 1 = Text of tweet 1) (Emotions = Mefiance,\nDegout, Colere)’, ’(Tweet Number 2 = Text of tweet 2) (Emotions = Peur, Degout,\nColere)’, ’(Tweet Number 3 = Text of tweet 3) (Emotions = Tristesse, Mefiance,\nPeur)’, ’(Tweet Number 4 = Text of tweet 4) (Emotions = Mefiance, Peur, Colere)’,\n’(Tweet Number 5 = Text of tweet 5) (Emotions = Ironie, Surprise, Tristesse)’, \"(Tweet\nNumber 6 = Text of tweet 6) (Emotions = Surprise, Mefiance, Peur)\", ’(Tweet Number\n7 = Text of tweet 7) (Emotions = Ironie, Mefiance, Degout, Colere)’, \"(Tweet Number\n8 = Text of tweet 8) (Emotions = Neutre, Surprise, Colere), ’(Tweet Number 9 = Text\nof tweet 9) (Emotions = Mefiance, Degout, Colere)’, ’(Tweet Number 10 = Text of\ntweet 10) (Emotions = Peur, Degout, Colere)’, ’(Tweet Number 11 = Text of tweet\n11) (Emotions = Tristesse, Mefiance, Peur)’, ’(Tweet Number 12 = Text of tweet\n12) (Emotions = Mefiance, Peur, Colere)’, ’(Tweet Number 13 = Text of tweet 13)\n(Emotions = Ironie, Surprise, Tristesse)’, \"(Tweet Number 14 = Text of tweet 14)\n(Emotions = Surprise, Mefiance, Peur)\", ’(Tweet Number 15 = Text of tweet 15)\n(Emotions = Ironie, Mefiance, Degout, Colere)’, \"(Tweet Number 16 = Text of tweet\n16) (Emotions = Neutre, Surprise, Colere)\"]\nThe response text should only contain a comma-separated list of up to 4 most likely\nemotions present in the input French tweet. The emotions should be selected from the\nemotions list. If you can’t find an emotion from the emotions list, simply return the\nvalue \"Inexploitable\"\nThe prompt closely mirrors the structure\nemployed in one-shot pseudo-labeling.\nHowever, the prompt incorporates two\ndistinct tweet samples per emotion la-\nbel, yielding a cumulative count of 16\ntweet samples across all emotions. The\nremaining content of the prompt remains\nentirely consistent with the format em-\nployed in one-shot pseudo-labeling. The\nexclusion of tweets labelled as inex-\nploitable leads to a remaining count of\n2775 tweets.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nACKNOWLEDGMENT\nThis work is part of the CATCH project (ANR-21-SIOM-\n0011), co-financed by the French National Research Agency\n(ANR) and by the the Normandy region through the RA-\nSIOMRI 2021 funding program.\nREFERENCES\n[1] Brynielsson, J., Johansson, F., Jonsson, C. & Westling, A. Emotion clas-\nsification of social media posts for estimating people’s reactions to com-\nmunicated alert messages during crises. Security Informatics . 3 pp. 1-11\n(2014)\n[2] Kim, E. & Klinger, R. A survey on sentiment and emotion analysis for\ncomputational literary studies. ArXiv Preprint ArXiv:1808.03137 . (2018)\n[3] Jabreel, M. & Moreno, A. A deep learning-based approach for multi-label\nemotion classification in tweets. Applied Sciences. 9, 1123 (2019)\n[4] Ameer, I., Ashraf, N., Sidorov, G. & Gómez Adorno, H. Multi-label emo-\ntion classification using content-based features in Twitter. Computación Y\nSistemas. 24, 1159-1164 (2020)\n[5] Lindquist, K., MacCormack, J. & Shablack, H. The role of language\nin emotion: Predictions from psychological constructionism. Frontiers In\nPsychology. 6 pp. 444 (2015)\n[6] Bailey, E., Matz, S., Youyou, W. & Iyengar, S. Authentic self-expression\non social media is associated with greater subjective well-being. Nature\nCommunications. 11, 4889 (2020)\n[7] Vlasenko, B. & Wendemuth, A. Annotators’ agreement and spontaneous\nemotion classification performance. Proceedings Of Interspeech ., 1546-\n1550 (2015)\n[8] Hayat, H., Ventura, C. & Lapedriza, A. Modeling Subjective Affect Anno-\ntations with Multi-Task Learning. Sensors. 22, 5245 (2022)\n[9] Duarte, J. & Berton, L. A review of semi-supervised learning for text\nclassification. Artificial Intelligence Review . pp. 1-69 (2023)\n[10] Yang, L., Huang, B., Guo, S., Lin, Y . & Zhao, T. A Small-Sample Text\nClassification Model Based on Pseudo-Label Fusion Clustering Algorithm.\nApplied Sciences. 13, 4716 (2023)\n[11] Gilardi, F., Alizadeh, M. & Kubli, M. Chatgpt outperforms crowd-workers\nfor text-annotation tasks. ArXiv Preprint ArXiv:2303.15056 . (2023)\n[12] Ameer, I., Bölücü, N., Siddiqui, M., Can, B., Sidorov, G. & Gelbukh, A.\nMulti-label emotion classification in texts using transfer learning. Expert\nSystems With Applications . 213 pp. 118534 (2023)\n[13] Kutlu, M., McDonnell, T., Elsayed, T. & Lease, M. Annotator rationales\nfor labeling tasks in crowdsourcing. Journal Of Artificial Intelligence\nResearch. 69 pp. 143-189 (2020)\n[14] Reuters Over 5,250 tonnes of chemicals burned in Rouen, France industrial\nfire. (https://tinyurl.com/58tny7rx,2019), Accessed on May 13, 2023\n[15] Dopierre, T., Gravier, C., Subercaze, J. & Logerais, W. Few-shot Pseudo-\nLabeling for Intent Detection. Proceedings Of The 28th International\nConference On Computational Linguistics . pp. 4993-5003 (2020,12),\nhttps://aclanthology.org/2020.coling-main.438\n[16] Lin, N., Fu, S., Lin, X. & Wang, L. Multi-label emotion classification based\non adversarial multi-task learning. Information Processing & Management.\n59, 103097 (2022)\n[17] Zhang, X., Zhao, J. & LeCun, Y . Character-level convolutional networks\nfor text classification. Advances In Neural Information Processing Systems .\n28 (2015)\n[18] Gao, Y ., Chen, Y ., Wang, J. & Lu, H. Reading scene text with attention con-\nvolutional sequence modeling. ArXiv Preprint ArXiv:1709.04303 . (2017)\n[19] Ameer, I., Bölücü, N., Sidorov, G. & Can, B. Emotion Classification in\nTexts over Graph Neural Networks: Semantic Representation is better than\nSyntactic. IEEE Access. (2023)\n[20] Le, H., Lee, G., Kim, S., Kim, S. & Yang, H. Multi-Label Multimodal\nEmotion Recognition With Transformer-Based Fusion and Emotion-Level\nRepresentation Learning. IEEE Access. 11 pp. 14742-14751 (2023)\n[21] Lakew, S., Cettolo, M. & Federico, M. A comparison of transformer\nand recurrent neural networks on multilingual neural machine translation.\nArXiv Preprint ArXiv:1806.06957 . (2018)\n[22] Kokab, S., Asghar, S. & Naz, S. Transformer-based deep learning models\nfor the sentiment analysis of social media data. Array. 14 pp. 100157 (2022)\n[23] Soyalp, G., Alar, A., Ozkanli, K. & Yildiz, B. Improving Text Classifica-\ntion with Transformer. 2021 6th International Conference On Computer\nScience And Engineering (UBMK) . pp. 707-712 (2021)\n[24] Alzubaidi, L., Bai, J., Al-Sabaawi, A., Santamaría, J., Albahri, A., Al-\ndabbagh, B., Fadhel, M., Manoufali, M., Zhang, J., Al-Timemy, A., & Oth-\ners. A survey on deep learning tools dealing with data scarcity: definitions,\nchallenges, solutions, tips, and applications. Journal Of Big Data . 10, 46\n(2023)\n[25] Yang, C., Cheung, Y ., Ding, J. & Tan, K. Concept drift-tolerant transfer\nlearning in dynamic environments. IEEE Transactions On Neural Net-\nworks And Learning Systems . 33, 3857-3871 (2021)\n[26] Ouali, Y ., Hudelot, C. & Tami, M. An overview of deep semi-supervised\nlearning. ArXiv Preprint ArXiv:2006.05278 . (2020)\n[27] Zhang, J., Wang, X., Zhang, D. & Lee, D. Semi-Supervised Group Emotion\nRecognition Based on Contrastive Learning. Electronics. 11, 3990 (2022)\n[28] Kang, L., Liu, J., Liu, L., Zhou, Z. & Ye, D. Semi-supervised emotion\nrecognition in textual conversation via a context-augmented auxiliary train-\ning task. Information Processing & Management . 58, 102717 (2021)\n[29] Jiang, M., Liang, Y ., Feng, X., Fan, X., Pei, Z., Xue, Y . & Guan, R. Text\nclassification based on deep belief network and softmax regression. Neural\nComputing And Applications . 29 pp. 61-70 (2018)\n[30] Pan, Y ., Chen, Z., Suzuki, Y ., Fukumoto, F. & Nishizaki, H. Sentiment\nanalysis using semi-supervised learning with few labelled data. 2020 In-\nternational Conference On Cyberworlds (CW) . pp. 231-234 (2020)\n[31] Cheng, Y ., Qian, K., Wang, Y . & Zhao, D. Missing multi-label learning with\nnon-equilibrium based on classification margin. Applied Soft Computing .\n86 pp. 105924 (2020)\n[32] Krishnamoorthy, A., Patil, A., Vasudevan, N. & Pathari, V . News article\nclassification with clustering using semi-supervised learning. 2018 Inter-\nnational Conference On Advances In Computing, Communications And\nInformatics (ICACCI). pp. 86-91 (2018)\n[33] Vilhagra, L., Fernandes, E. & Nogueira, B. Textcsn: a semi-supervised\napproach for text clustering using pairwise constraints and convolutional\nsiamese network. Proceedings Of The 35th Annual ACM Symposium On\nApplied Computing. pp. 1135-1142 (2020)\n[34] Ng, L. & Carley, K. “The coronavirus is a bioweapon”: classifying coro-\nnavirus stories on fact-checking sites. Computational And Mathematical\nOrganization Theory. 27, 179-194 (2021)\n[35] Qiu, Y ., Gong, X., Ma, Z. & Chen, X. MixLab: an informative semi-\nsupervised method for multi-label classification. Natural Language Pro-\ncessing And Chinese Computing: 9th CCF International Conference,\nNLPCC 2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part\nI 9. pp. 506-518 (2020)\n[36] Li, A. & Sethy, A. Semi-supervised learning for text classification by\nlayer partitioning. ICASSP 2020-2020 IEEE International Conference On\nAcoustics, Speech And Signal Processing (ICASSP) . pp. 6164-6168 (2020)\n[37] Gupta, R., Sahu, S., Espy-Wilson, C. & Narayanan, S. Semi-supervised\nand transfer learning approaches for low resource sentiment classification.\n2018 IEEE International Conference On Acoustics, Speech And Signal\nProcessing (ICASSP). pp. 5109-5113 (2018)\n[38] Grandvalet, Y . & Bengio, Y . Semi-supervised learning by entropy mini-\nmization. Advances In Neural Information Processing Systems . 17 (2004)\n[39] Fan, Y ., Kukleva, A., Dai, D. & Schiele, B. Revisiting consistency regular-\nization for semi-supervised learning. International Journal Of Computer\nVision. 131, 626-643 (2023)\n[40] Kingma, D., Mohamed, S., Jimenez Rezende, D. & Welling, M. Semi-\nsupervised learning with deep generative models. Advances In Neural\nInformation Processing Systems . 27 (2014)\n[41] Park, S., Lee, J. & Kim, K. Semi-supervised distributed representations\nof documents for sentiment analysis. Neural Networks . 119 pp. 139-150\n(2019)\n[42] Shehnepoor, S., Togneri, R., Liu, W. & Bennamoun, M. ScoreGAN: a\nfraud review detector based on regulated GAN with data augmentation.\nIEEE Transactions On Information Forensics And Security . 17 pp. 280-291\n(2021)\n[43] Van Engelen, J. & Hoos, H. A survey on semi-supervised learning. Machine\nLearning. 109, 373-440 (2020)\n[44] Lee, D. & Others Pseudo-label: The simple and efficient semi-supervised\nlearning method for deep neural networks. Workshop On Challenges In\nRepresentation Learning, ICML . 3, 896 (2013)\n[45] Cascante-Bonilla, P., Tan, F., Qi, Y . & Ordonez, V . Curriculum labeling:\nRevisiting pseudo-labeling for semi-supervised learning. Proceedings Of\nThe AAAI Conference On Artificial Intelligence . 35, 6912-6920 (2021)\n[46] Oliver, A., Odena, A., Raffel, C., Cubuk, E. & Goodfellow, I. Realistic\nevaluation of semi-supervised learning algortihms. International Confer-\nence On Learning Representations . pp. 1-15 (2018)\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[47] Arazo, E., Ortego, D., Albert, P., O’Connor, N. & McGuinness, K. Pseudo-\nlabeling and confirmation bias in deep semi-supervised learning. 2020\nInternational Joint Conference On Neural Networks (IJCNN) . pp. 1-8\n(2020)\n[48] Widmann, N. & Verberne, S. Graph-based semi-supervised learning for\ntext classification. Proceedings Of The ACM SIGIR International Confer-\nence On Theory Of Information Retrieval . pp. 59-66 (2017)\n[49] Souza, M., Nogueira, B., Rossi, R., Marcacini, R. & Rezende, S. A hetero-\ngeneous network-based positive and unlabelled learning approach to detect\nfake news. Brazilian Conference On Intelligent Systems . pp. 3-18 (2021)\n[50] Bose, J. & Mukherjee, S. Semi-supervised method using Gaussian random\nfields for boilerplate removal in web browsers. 2019 IEEE 16th India\nCouncil International Conference (INDICON) . pp. 1-4 (2019)\n[51] Zhao, H., Xie, J. & Wang, H. Graph convolutional network based on multi-\nhead pooling for short text classification. IEEE Access. 10 pp. 11947-11956\n(2022)\n[52] Zhu, D., Dai, X. & Chen, J. Pre-train and learn: Preserving global in-\nformation for graph neural networks. Journal Of Computer Science And\nTechnology. 36 pp. 1420-1430 (2021)\n[53] Ju, W., Yang, J., Qu, M., Song, W., Shen, J. & Zhang, M. Kgnn: Harnessing\nkernel-based networks for semi-supervised graph classification. Proceed-\nings Of The Fifteenth ACM International Conference On Web Search And\nData Mining. pp. 421-429 (2022)\n[54] Li, C., Li, X. & Ouyang, J. Semi-supervised text classification with bal-\nanced deep representation distributions. Proceedings Of The 59th Annual\nMeeting Of The Association For Computational Linguistics And The 11th\nInternational Joint Conference On Natural Language Processing (Volume\n1: Long Papers) . pp. 5044-5053 (2021)\n[55] Hwang, H. & Lee, Y . Semi-Supervised Learning based on Auto-generated\nLexicon using XAI in Sentiment Analysis. Proceedings Of The Interna-\ntional Conference On Recent Advances In Natural Language Processing\n(RANLP 2021). pp. 593-600 (2021)\n[56] Zou, H. & Wang, Z. A semi-supervised short text sentiment classification\nmethod based on improved Bert model from unlabelled data. Journal Of\nBig Data. 10, 1-19 (2023)\n[57] Chong, Y ., Ding, Y ., Yan, Q. & Pan, S. Graph-based semi-supervised\nlearning: A review. Neurocomputing. 408 pp. 216-230 (2020)\n[58] Goodfellow, B. I Papernot N Oliver A Raffel CA Mixmatch: a holistic\napproach to semi-supervised learning. Adv. Neural Inf. Process. Syst . 32,\n11 (2019)\n[59] Mekala, D., Dong, C. & Shang, J. LOPS: Learning Order Inspired Pseudo-\nLabel Selection for Weakly Supervised Text Classification. ArXiv Preprint\nArXiv:2205.12528. (2022)\n[60] Sun, C., Qiu, X., Xu, Y . & Huang, X. How to fine-tune bert for text\nclassification?. Chinese Computational Linguistics: 18th China National\nConference, CCL 2019, Kunming, China, October 18–20, 2019, Proceed-\nings 18. pp. 194-206 (2019)\n[61] Jimenez, D., Gambino, O. & Calvo, H. Pseudo-Labeling Improves News\nIdentification and Categorization with Few Annotated Data. Computación\nY Sistemas. 26, 183-193 (2022)\n[62] Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlo-\nmov, S., Tepper, N. & Zwerdling, N. Do not have enough data? Deep\nlearning to the rescue!. Proceedings Of The AAAI Conference On Artificial\nIntelligence. 34, 7383-7390 (2020)\n[63] Chandra, R. & Krishna, A. COVID-19 sentiment analysis via deep learning\nduring the rise of novel cases. PloS One. 16, e0255615 (2021)\n[64] Kabir, M. & Madria, S. EMOCOV: Machine learning for emotion detec-\ntion, analysis and visualization using COVID-19 tweets. Online Social\nNetworks And Media . 23 pp. 100135 (2021)\n[65] The Local France French chemical factory explosion: What we know so\nfar. (https://tinyurl.com/29ft4xva,2019), Accessed on May 13, 2023\n[66] France 24 French authorities probe Lubrizol factory fire in Normandy.\n(https://tinyurl.com/kdjc96k7,2019), Accessed on May 13, 2023\n[67] The New York Times Fire at French Chemical Plant Provokes Worry\nAcross the Channel. (https://tinyurl.com/3565bjyv,2019), Accessed on\nMay 13, 2023\n[68] Plutchik, R. A psychoevolutionary theory of emotions. (Sage Publica-\ntions,1982)\n[69] Nandwani, P. & Verma, R. A review on sentiment analysis and emotion\ndetection from text. Social Network Analysis And Mining . 11, 81 (2021)\n[70] Mekki, J., Lecorvé, G., Battistelli, D. & Béchet, N. TREMoLo-Tweets: a\nMulti-Label Corpus of French Tweets for Language Register Characteri-\nzation. RANLP 2021-Recent Advances In Natural Language Processing .\n(2021)\n[71] Krippendorff, K. Computing Krippendorff’s alpha-reliability. (2011)\n[72] Fleiss, J., Levin, B. & Paik, M. Statistical methods for rates and propor-\ntions. (john wiley & sons,2013)\n[73] Krippendorff, K. Content analysis: An introduction to its methodology.\n(Sage publications,2018)\n[74] McHugh, M. Interrater reliability: the kappa statistic. Biochemia Medica .\n22, 276-282 (2012)\n[75] Martin, L., Muller, B., Ortiz Suárez, P., Dupont, Y ., Romary, L.,\nClergerie, É., Seddah, D. & Sagot, B. CamemBERT: a Tasty French\nLanguage Model. Proceedings Of The 58th Annual Meeting Of The\nAssociation For Computational Linguistics . pp. 7203-7219 (2020,7),\nhttps://aclanthology.org/2020.acl-main.645\n[76] Le, H., Vial, L., Frej, J., Segonne, V ., Coavoux, M., Lecouteux, B., Al-\nlauzen, A., Crabbé, B., Besacier, L. & Schwab, D. FlauBERT: Unsuper-\nvised Language Model Pre-training for French. Proceedings Of The Twelfth\nLanguage Resources And Evaluation Conference . pp. 2479-2490 (2020,5),\nhttps://aclanthology.org/2020.lrec-1.302\n[77] Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L. & Stoyanov, V . RoBERTa: A robustly optimized bert\npretraining approach. ArXiv Preprint ArXiv:1907.11692 . (2019)\n[78] Sauvayre, R., Vernier, J. & Chauvière, C. An analysis of French-language\ntweets about COVID-19 vaccines: supervised learning approach. JMIR\nMedical Informatics. 10, e37831 (2022)\n[79] Winata, G. & Khodra, M. Handling imbalanced dataset in multi-label text\ncategorization using Bagging and Adaptive Boosting. 2015 International\nConference On Electrical Engineering And Informatics (ICEEI) . pp. 500-\n505 (2015)\n[80] Wu, H., Li, X., Lin, Y . & Cheng, K. Compete to Win: Enhancing Pseudo\nLabels for Barely-supervised Medical Image Segmentation. IEEE Trans-\nactions On Medical Imaging . (2023)\n[81] Nguyen, G., Chen, S., Jun, T., Kim, D. Explaining How Deep Neural Net-\nworks Forget by Deep Visualization. International Conference on Pattern\nRecognition Workshops. (2020)\n[82] Lee, J., Tang, R., Lin, J. What Would Elsa Do? Freezing Layers During\nTransformer Fine-Tuning. ArXiv Preprint ArXiv:1911.03090 . (2019)\nUSMAN MALIKUsman Malik received his Ph.D.\nfrom the Normandie Université, France, where his\nresearch focused on the application of machine\nlearning and deep learning techniques to enhance\nmulti-model human-agent interaction. Currently,\nhe is actively engaged in the CATCH project,\nwhere his focus lies in the development of multi-\nlabel emotion classification models tailored for\nFrench-language tweets.\nSIMON BERNARD received a Ph.D. degree in\ncomputer science from the University of Rouen\nNormandy, France in 2009. He is an Associate Pro-\nfessor (Maitre de Conférences) at the University\nof Rouen Normandy since 2013, and is a mem-\nber of the LITIS laboratory and of the Normastic\nCNRS research federation. His research interests\nfocus on machine learning, ensemble learning and\ndeep learning, in various learning contexts such as\nweakly supervised learning, one-class classifica-\ntion and/or high-dimensional, low sample size (HDLSS) classification.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nALEXANDRE PAUCHET is Associate Professor\nat INSA Rouen Normandy, France, and in the\nLITIS laboratory. He is in charge of the MIND\nteam and co-animator of the Data, Learning and\nKnowledge axis of the Normastic Federation. He\nhas defended thehis French ability to supervise\nresearches (HDR) at the University of Rouen in\n2015. His research interests focus on Human-\nAgent interaction, Affective Computing and Dia-\nlogue.\nCLÉMENT CHATELAINis an Associate Professor\nin the Department of Information Systems Engi-\nneering at INSA Rouen Normandy, France. His\nresearch interests include machine learning ap-\nplied to handwriting recognition, document image\nanalysis and medical image analysis. His teaching\ninterests include signal processing, deep learning\nand pattern recognition. In 2019, Dr. Chatelain\nreceived the French ability to supervise researches\nfrom the University of Rouen.\nROMAIN PICOT-CLÉMENTE received a Ph.D.\ndegree in computer science from the University\nof Burgundy, France in 2011. He is the Head of\nArtificial Intelligence of Saagie since 2018, and is\nco-director of the joint laboratory L-LiSa between\nLITIS and Saagie. His research interests focus on\ndeep learning, weakly supervised learning applied\nto text and time series data.\nJÉRÔME CORTINOVISis Innovation and Partner-\nship Engineer at Atmo Normandie since 2004, af-\nter receiving a PhD in the field of physico-chemical\nmodelling of the atmosphere in 2004 from the\nCNRS-Laboratoire d’Aérologie in Toulouse. He\nhas worked in particular on the development of\nnumerical modelling of air quality, as well as on\nthe implementation of open data. He is currently\ncoordinator of Incub’air, Atmo Normandie’s inno-\nvation laboratory, which aims to test and dissemi-\nnate innovative solutions for air quality (including odour issues).\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354705\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}