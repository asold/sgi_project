{
    "title": "Transformer-Based Neural Network Machine Translation Model for the Kurdish Sorani Dialect",
    "url": "https://openalex.org/W4316672576",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5064925790",
            "name": "Soran Badawi",
            "affiliations": [
                "Charmo University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2768123736",
        "https://openalex.org/W3092831871",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2184135559",
        "https://openalex.org/W6691182555",
        "https://openalex.org/W3172225000",
        "https://openalex.org/W2951165112",
        "https://openalex.org/W2970009562",
        "https://openalex.org/W3100684858",
        "https://openalex.org/W1509982784",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W4295122555",
        "https://openalex.org/W2544860310",
        "https://openalex.org/W2312893070",
        "https://openalex.org/W2173426087",
        "https://openalex.org/W2251530548",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2038721957",
        "https://openalex.org/W4293584592",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2964054038",
        "https://openalex.org/W2340598284",
        "https://openalex.org/W3211848854",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "The transformer model is one of the most recently developed models for translating texts into another language. The model uses the principle of attention mechanism, surpassing previous models, such as sequence-to-sequence, in terms of performance. It performed well with highly resourced English, French, and German languages. Using the model architecture, we investigate training the modified version of the model in a low-resourced language such as the Kurdish language. This paper presents the first-ever transformer-based neural machine translation model for the Kurdish language by utilizing vocabulary dictionary units that share vocabulary across the dataset. For this purpose, we combine all the existing parallel corpora of Kurdish – English by building a large corpus and training it on the proposed transformer model. The outcome indicated that the suggested transformer model works well with Kurdish texts by scoring (0.45) on bilingual evaluation understudy (BLEU). According to the BLEU standard, the score indicates a high-quality translation.",
    "full_text": "UHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1 15\n1. INTRODUCTION\nHuman language has a complex and irregular system that \ncan pose significant issues for machine translation. The \nkind of  morphemes, their implications, and their syntactic \nand semantic relations in the context is causing the natural \nlanguage to be complex and abnormal. The complexity of  \nthese problems has led some to believe that human translation \nis infeasible for such tasks.\nHistorically, machine translation has experienced numerous \nchanges. First, dictionary-based and rule-based translation \nmethods were developed and provided translation services \nthrough the manual specification of  rules and resources [1]. \nFollowing that, statistical translation emerged as the new \nmodel to diminish the role of  a linguist and increase the \nemphasis on language dependency [2].\nLuckily, the advancement of  neural networks and artificial \nintelligence has primarily impacted many areas of  science, \nincluding machine translation. As a result of  the neural \nmachine translation research, top-notch translations were \nproduced for texts written in resourceful languages. Therefore, \nthe need to achieve the same goal for low-resourced languages \nhas become significant and the attempts to achieve that have \nincreased [3]. Languages are considered less resourced when \nthey lack human-constructed linguistic resources, substantial \nmonolingual or parallel corpora, and general-purpose \ngrammar are the sole sources available. The research industry \nhas primarily ignored Kurdish dialects, which are practiced by \n20–30 million people across four regions [4].\nA Transformer-based Neural Network Machine \nTranslation Model for the Kurdish Sorani \nDialect\nSoran Badawi\n Language Center, Charmo Center for Scientific Research & Consulting, Charmo University, Chamchamal, Sulaimani, \nKRG, Iraq\nABSTRACT\nThe transformer model is one of the most recently developed models for translating texts into another language. The \nmodel uses the principle of attention mechanism, surpassing previous models, such as sequence-to-sequence, in terms of \nperformance. It performed well with highly resourced English, French, and German languages. Using the model architecture, \nwe investigate training the modified version of the model in a low-resourced language such as the Kurdish language. This \npaper presents the first-ever transformer-based neural machine translation model for the Kurdish language by utilizing \nvocabulary dictionary units that share vocabulary across the dataset. For this purpose, we combine all the existing parallel \ncorpora of Kurdish – English by building a large corpus and training it on the proposed transformer model. The outcome \nindicated that the suggested transformer model works well with Kurdish texts by scoring (0.45) on bilingual evaluation \nunderstudy (BLEU). According to the BLEU standard, the score indicates a high-quality translation.\nIndex Terms: Machine translation, Transformers, Dialect, Kurdish language, Bilingual evaluation understudy\nCorresponding author’s e-mail: Soran Badawi, Language Center, Charmo Center for Scientific Research & Consulting, Charmo University, \nChamchamal, Sulaimani, KRG, Iraq. E-mail: soran.sedeeq@charmouniversity.org\nReceived: 12-10-2022 Accepted: 24-12-2022 Published: 15-01-2023\nAccess this article online\nDOI: 10.21928/uhdjst.v7n1y2023.pp15-21 E-ISSN:  2521-4217\nP-ISSN: 2521-4209\nCopyright © 2022 Badawi. This is an open access article distributed \nunder the Creative Commons Attribution Non-Commercial No \nDerivatives License 4.0 (CC BY-NC-ND 4.0)\nORIGINAL RESEARCH ARTICLE UHD JOURNAL OF SCIENCE AND TECHNOLOGY\nBadawi: Kurdish Neural Network Translation \n16 UHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1\nThis study presents a transformers-based model using the \nvocabulary dictionary concept. We collect the parallel corpora \nin the language and merge them to be a large corpus for \ntraining and report the results. The resources used for the task \ninclude the Tanzil corpus [5], TED corpus [6], KurdNet–the \nKurdish wordnet [7], and the Auta corpus [8].\n2. RELATED WORKS\nFew studies have addressed the Kurdish language in the \nMachine Translation (MT) domain. The Apertium project \nis the first machine translation system for both Sorani \nand Kurmanji. The Apertium uses rules-based machine \ntranslation, which has developed various tools and resources \nfor the Kurdish language, such as bilingual and morphological \ndictionaries, structural transfer rules, and grammar [4]. \nInKurdish1 is another attempt to construct a machine \ntranslation model for Kurdish. The system applies dictionary-\nbased methods for translation. According to Taher et al. (2017), \nthis method is ineffective in translating lengthy and idiomatic \nsentences. Finally, Ahemdi and Mansoud (2020) attempted to \ntranslate Kurdish texts using neural machine translation [4]. \nTheir work was based on collecting the parallel datasets in the \nKurdish language. They used different tokenization techniques \nfor training the dataset. They eventually reported the Bilingual \nevaluation understudy (BLEU) achieved using each tokenizer. \nRegarding other low-resourced languages worldwide, Abbott \nand Martinus (2018) employed transformer models to \ntranslate texts from English to Setswana using the parallel \nAutshumato dataset [9]. The outcome of  their work indicated \nthat the transformer outperforms previous methods by 5.33 \nBLEU points. Moreover, Przystupa and Abdul-Mageed \n(2019) used transformer models with back-translation. Their \nresults demonstrate that transformer models translate texts \nbetween Spanish–Portuguese and Czech–Polish [10]. Tapo \net al. (2019) used neural machine translation to translate texts \nfrom Barbara’s language to English and French. Their work \nmainly concentrated on the challenges when performing \nneural machine translation on a low-resourced language such \nas Barbara [11].\n2.1. Dataset\nWe used a collection of  four parallel datasets. The first one \nis Tanzil, a group of  Quran translations compiled by the \nTanzil project8. The corpus has one Sorani translation aligned \nwith 11 translations, totaling 92,354 parallel texts with 3.15M \nvocabularies on the Sorani Kurdish side and 2.36M on the \nEnglish side. The corpus is available in translation memory \nexchange (TMX), where aligned verses are offered [4].\nThe second corpus, the TED corpus [6], is the collection \nof  subtitles from TED Talks, a sequence of  top-notch \ntalks on different genres, “Technology, entertainment, and \ndesign.” The only Kurdish dialect for which these subtitles \nare translated is the Sorani dialect. Even though there are only \n2358 parallel sentences, the TED collection has translations in \na broader, more comprehensive range of  subjects than Tanzil.\nThe third corpus is WordNet [12], a lexical-semantic tool \nexploited for various Natural Language Processing (NLP) \ntasks like information extraction and word disambiguation. \nWordNet offers concise definitions and uses examples for \ngroupings of  synonyms, also known as synsets, in addition \nto semantic links like synonymy, hyponymy, and meronymy. \nKurdish WordNet [7] is based on a semi-automatic technique \nthat focuses on creating a Kurdish alignment for base concepts, \na critical subset of  WordNet’s central meanings. Four thousand \nsix hundred and sixty-three definitions directly translated from \nthe Princeton WordNet are included in the most recent version \nof  KurdNet (version 3.0). We included this resource despite \nhaving fewer translated purposes than necessary for machine \ntranslation because it covers more domains.\nThe final corpus is Auta, comprising 229,222 pairs of \nphysically aligned translations [8]. The corpus is gathered \nfrom different text genres and domains to construct more \nsolid and real-world machine translation applications. The \nresearchers built this corpus and published a portion of  \nthis corpus available to promote study in this area, which \ncontains 100.000 normalized and cleaned texts ready to be \nexperimented with using the trendy machine learning models \n(Table 1).\n2.2. Transformer’s Model Architecture\nMost neural machine translation models follow an encoder-\ndecoder structure [13]. The encoder consists of  six identical \nlayers with a multi-head self-attention mechanism and \nposition-wise sublayers. These layers are fully connected to \nfeed-forward networks. The encoder aims to map an input \nsequence of  symbol representations starting from (x1;:; \nxn) to a sequence of  continuous representations, which is \nz = (z1;:; zn) [14].\nTable 1: Size of each Kurdish–English corpus\nNo Corpus Language Size\n1 Tanzil Kurdish–English 92.354 texts\n2 Ted Kurdish–English 2.358 texts\n3 KurdNet Kurdish–English 4.663 texts\n4 Auta Kurdish–English 100.000 texts\nTotal 199.375 texts\nBadawi: Kurdish Neural Network Translation \nUHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1 17\nSimilarly, the decoder is constituted of  a stack of  six identical \nlayers. Encoder layers consist of  two sublayers each, and \nthe decoder adds the third sublayer to carry out multi-head \nattention around the encoder`s output. In the same way as \nthe encoder, layer normalization uses residual connections \naround each sub-layer. The self-attention sub-layer in the \ndecoder stack has been adjusted to block positions from \nattending to the following positions, as shown in Fig. 1. The \ngoal of  the decoder is to produce a sign output sequence \n(y1;:; ym) one element at a time [14].\nMoreover, the model uses the attention function to map a \nquery and a set of  key-value pairs to an output, where the \nquestion, keys, values, and production are all vectors. The \nsum of  the weight values calculates the result. A compatibility \nfunction between the query and the relevant key determines \neach value’s weight.\nAs is shown, the transformer operates multi-head attention \non three different stages:\n1. Encoder-decoder attention lets every decoder position \nfocus on every input post.\n2. The encoder has layers for the self-attention. All the keys, \nvalues, and queries in a self-attention layer originate from \nthe same source, in this case, the encoder’s output from \nthe previous layer.\n3. The decoder’s self-attention layers enable each location \nto pay attention to all postings below and above.\nA fully connected feed-forward network is implemented to \neach position separately and uniformly in each layer of  the \nencoder and decoder. Two linear transformations and a ReLU \n(Rectified Linear Unit) activation make up this process [15].\nThe decoder output is transformed to project next-token \nprobabilities using the SoftMax function. The embedding \nis utilized to convert the input and output tokens to vectors \nof  the dimension model.\nThe positional encodings to the input embedding at the \nbottoms of  the encoder and decoder stacks. Since the \npositional encodings and the embeddings share the same \ndimension model, both can be added. Positional encodings \ncome in a variety of  discovered and fixed forms [16].\n2.3. The Proposed Model Architecture\nThe decoder and encoder for the proposed transformer-\nbased Neural Machine Translation (NMT) have a stack of  \nsix layers, as shown in Fig. 2. Every layer has two sublayers: \nThe position-wise feed-forward sub-layer and the multi-\nhead attention sub-layer (FFN). The encoder and decoder \nin the proposed Transformer NMT model architecture \nfor Kurdish texts produce variable-length sequences using \nan attention model and feed-forward net. Multi-head \nattention is the foundation for how attention operates \nacross multiple tiers. The mapping of  an input sequence \nof  symbol representations, X = (x1, x2..., xnenc)T to an \nintermediate vector. Given the intermediate vector, the \ndecoder generates the output sequence (target sentence) \nY = (y1, y2..., yndec)T. The convolutional or recurrent \nstructures are absent from the transformer design. At \nthe first layer of  both the encoder and the decoder, the \npositional encodings computed by the Equations below \nare summed to the input embeddings.\n1) PE(pos, 2i) = sin(pos100002i/dmodel)\n2) PE(pos, 2i+1) = cos(pos100002i/dmodel)\nFig. 1. The transformer model architecture [15].\nBadawi: Kurdish Neural Network Translation \n18 UHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1\nFig. 2. The Proposed model architecture.\nWhere pos stands for position, i is considered the dimension, \nand d is the dimension of  the intermediate representation. \nEvery encoding layer has a position-wise feed-forward \nsub-layer and a multi-head attention sub-layer. A residual \nconnection method [17] and a layer normalization unit \n(LayerNorm) [13] are used around each sub-layer to facilitate \ntraining and enhance performance. In contrast to the encoder, \nevery layer of  the decoder has three sub-layers: a multi-head \nattention sublayer, a position-wise feed-forward sub-layer, \nand so on. Encoder-decoder multi-head attention sub-layer \nis inserted in between them.\n3. METHODOLOGY\nThe proposed model uses the concept word dictionary \ninside the dataset to find the equivalence meaning of \neach word. Therefore, in the preprocessing stage, we \nonly tokenized the cleaned texts, which converted the \nsentences into lists of  words. Following that, we converted \nthem into an extensive dictionary of  words which has \nKurdish words and their English meanings. Next, we fed \nthe dictionary to our proposed transformer’s model. As \nshown in Fig. 3. We used the batch size of  20 and trained \non 100 epochs.\nAt first, we tried to train the model on the central processing \nunit (CPU); since the amount of  data was huge for the CPU, \nthe model trained for days without providing any results, \nand numerous ram crashes forced the computer to reboot \nand restart the process again. However, we tried to train the \nmodel for one epoch and compare its result with graphics \nprocessing unit (GPU). As it is shown in Table 2.\nAs shown in Table  2, training one epoch on the CPU \nlasted 3 h and 37 min, while it lasted <5 min for GPU. \nBecause 100 epochs are enormous to be trained on CPU \nand to avoid Ram crashes, we trained the model on Google \nColab Pro, a monthly subscription program that gives you \nhigher Ram and GPU. The whole training and test process \nlasted 5  h. The complete code and the training program \nare publicly available at/https://github.com/mbrow309/\nMachineTranslationUsingTransformers/blob/master/\nKurdishMTTransformers.ipynb/.\n4. RESULTS AND DISCUSSION\nWe train the system on 100 epochs since introducing the MT \nmodule at higher values will help guarantee a good BLEU \nscore. Neural networks are usually trained over several \nBadawi: Kurdish Neural Network Translation \nUHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1 19\nFig. 3. An outline of the methodology.\nTable 2: The difference between training on GPU \nand CPU\nMachine type Loss BLEU Time\nCPU 5.140 0.0183 3 h and 37 min\nGPU 5.109 0.0191 4 min\nGPU: Graphics processing unit, CPU: Central processing unit, BLEU: Bilingual evaluation \nunderstudy\nTable 3: Samples of the produced translation texts\nKurdish من تۆم خۆشدەویت\nEnglish I love you, and I love you.\nKurdishمن دەچم بۆ بازاڕ\nEnglish I am going to go to the marketing game.\nKurdishڕۆژێک لە ڕۆژان نەخۆشیەکی ترسناک بوو بە هەڕەشە\nEnglish Once upon a time, there was a dread disease.\nKurdishئێوە لە کوێ بوون\nEnglish Where were you, like yesterday?\nepochs. Epochs refer to cycles through a training dataset [18]. \nIt is important to note that we tried to train the module on \neach dataset, and due to the low amount of  datasets, the \nmodule yielded a significantly lower BLEU score. However, \nmerging the datasets did a perfect job. As shown in Fig. 4, \nthe amount of  BLEU improves significantly per 10 epochs \nand finally reaches the ideal score.\nWe fed the module some unseen texts to translate. Overall, \nthe module did an excellent job of  translating the texts. Below \nare samples of  translated texts shown by the module.\nThe model does a relatively good job of  translating unseen \ntexts. Even though the translation results from Table 3 \nshow some cases of  word repetition and some cases of  \nproducing ungrammatical sentences, particularly in the final \ntest example. The issue is substantially related to having a \nlow amount of  data. Therefore, if  the model is trained on \nmuch larger datasets, the translation results would be more \naccurate and flawless.\nIn the next phase of  our work, we intend to investigate the \nergative case of  our model by feeding it examples that have \nergative and compare our model`s translation with the latest \nGoogle translation for the Kurdish language. In Kurdish, \nthe word order is subject-object-verb with tense-aspect-\nmodality markings [19]. As a split-ergative language, Sorani \nKurdish marks transitive verbs in the past tenses differently \nfrom nominative verbs [20]. For ergative-absolute alignment, \nSorani Kurdish uses different pronominal enclitics [4]. To \nclarify further, we have included a few examples in Sorani \nKurdish below. The bold suffix is used for patient marking \nin Example 1 in the past tense, which uses the pronominal \nenclitic = man as an agentive marker.\n1. Kurdish/منداڵەکانمان هێنان\nTranscribe/mndalakanman hênan.\nTranslation/we brought the children.\n2. Kurdish/هێنامانن\nTranscribe/hênamanin\nTranslation/we brought them.\n3. Kurdish/دەچنە باخەکامان\nTranscribe/deçine baxakaman\nTranslation/they are going to our garden.\nBadawi: Kurdish Neural Network Translation \n20 UHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1\nUndoubtedly, the Kurdish language suffers from a lack of  \nresources, particularly in the field of  NLP . The lack of  a \ntranslation model is also part of  the problem. The work \nundertaken in this paper demonstrates that the Kurdish \nlanguage responds well to the newly developed and proposed \nneural machine translation model. It is worth noting that the \nexistence of  large corpora with more than 1 million data can \nactively work well and improve the model’s score to near-\nperfect translation. Fortunately, the results acquired from this \nwork can open many gates for the future researchers to dive \ndeeply into the transformer model and modified in a way that \ncan work specifically for the language. Finally, the transformer \nmodel’s layers remain intact, and the training and process \nstarted this way as the model modification, particularly on \nthe layers left for future researchers.\n REFERENCES\n[1] S. Tripathi and J. K. Sarkhel. “Approaches to machine translation”. \nAnnals of Library and Information Studies, vol. 57, pp. 383-393, 2010.\n[2] P. Koehn. “Statistical Machine Translation”. Cambridge University \nPress, Cambridge. 2009.\n[3] L. Bentivogli, A. Bisazza, M. Cettolo and M. Federicoa. “Neural \nversus phrase-based mt quality: An in-depth analysis on english-\ngerman and english-french”. Computer Speech and Language, \nvol. 49, pp. 52-70, 2019.\n[4] S. Ahmadi and M. Masoud. “Towards Machine Translation for the \nKurdish Language”. arXiv preprint arXiv:2010.06041, 2020.\n[5] J. Tiedemann. “Parallel data, tools and interfaces in OPUS”. \nIn: Proceedings of the Eighth International Conference on \nLanguage Resources and Evaluation (LREC’12 ). European \nLanguage Resources Association (ELRA), Istanbul, Turkey. \npp. 2214-2218, 2012.\n[6] M. Cettolo, C. Girardi and M. Federico. “Wit3: Web inventory of \ntranscribed and translated talks”. In: Conference of European \nAssociation for Machine Translation. 2012.\n[7] P. Aliabadi, M. S. Ahmadi, S. Salavati and K. S. Esmaili. “Towards \nbuilding kurdnet, the kurdish wordnet”. In:  Proceedings of the \nSeventh Global Wordnet Conference . University of Tartu Press, \nTartu, Estonia. 2014.\n[8] Z. Amini, M. Mohammadamini, H. Hosseini, M. Mansouri and D. Jaff. \n“Central Kurdish Machine Translation: First Large Scale Parallel \nCorpus and Experiments”. arXiv preprint arXiv:2106.09325, 2021.\n[9] L. Martinus and J. Z. Abbott. “A Focus on Neural Machine Translation \nfor African Languages”. arXiv preprint arXiv:1906.05685, 2019.\n[10] M. Przystupa and M. Abdul-Mageed. “Neural machine translation \nof low-resource and similar languages with backtranslation”. \nIn: Proceedings of the Fourth Conference on Machine \nTranslation. vol. 3. Association for Computational Linguistics, \nFlorence, Italy. 2019.\n[11] A. A. Tapo, B. Coulibaly, S. Diarra, C. Homan, J. Kreutzer, S. Luger, \nA. Nagashima, M. Zampieri and M. Leventhal. “Neural Machine \nTranslation for Extremely Low-Resource African Languages: \nA Case Study on Bambara”. arXiv preprint arXiv:2011.05284, 2019.\n[12] G. A. Miller. “WordNet: An Electronic Lexical Database”. MIT Press, \nTable 4: Comparison between translating \nergative sentences using Google translator and \ntransformer‑based model\nGoogle translator Our model Kurdish text\nSutandmian They burnt meسوتاندمیان\nbrought it A two into \ndifficulties \nهێنایانی\nThey took them to \nthe market.\nThey took me to \nthe market.\nبردمیان بۆ بازاڕ\nThey brought them \nhome.\nThey took me \nhome\nهێنامیان بۆ ماڵ\nI saw them in the \nmarket.\nI saw them at \nthe market.\nبینیانم لە بازاڕ\nI love you. I love you.من تۆم خۆشدەوێت\nAs shown, the pronominal (man) is translated differently \nin different examples. In examples 1 and 2, “man” was the \nsubject, and its equivalence is “we.” While in example 3, it \nfunctions as a possessive pronoun, and it is “our.” During \nmachine translation, this creates significant issues when the \nmodel tries to align the two languages.\nThe examples in Table 4 show that our model performs well \nin the ergative situation for Kurdish texts. According to the \nresults, the Google translator faces issues when the sentence \ncontains the pronominal enclitics (m), and it functions as the \nobject of  the sentence. This is because our corpus includes \nmany natural language texts that include such pronominal \npronouns, particularly in the Tanzil corpus. Thus, our \nmodel would easily detect the pronominal enclitics and their \nalignment inside the texts.\n5. CONCLUSION\nThe transformer model is a unique and highly functional \nmodel to translate texts from one language to another. \nFig. 4. The value bilingual evaluation understudy value per  \n20 epochs.\nBadawi: Kurdish Neural Network Translation \nUHD Journal of Science and Technology | Jan 2023 | Vol 7 | Issue 1 21\nMassachusetts, United States. 1998.\n[13] J. L. Ba, J. R. Kiros and G. E. Hinton. “Layer Normalization”. arXiv \npreprint arXiv:1607.06450, 2016.\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. \nGomez, Ł. Kaiser and I. Polosukhin. “Attention is all you need”. \nIn: Conference on Advances in Neural Information Processing \nSystems. 2017.\n[15] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. \nSalakhutdinov. “Dropout: A simple way to prevent neural networks \nfrom overfitting”. The Journal of Machine Learning Research , \nvol. 15, pp. 1929-1958, 2014.\n[16] J. Gehring, M. Auli, D. Grangier, D. Yarats and Y. N. Dauphin. \n“Convolutional sequence to sequence learning”. In: Proceedings \nof the 34th International Conference on Machine Learning (PMLR). \n2017.\n[17] M. Shafiq and Z. Gu, “Deep Residual Learning for Image \nRecognition: A Survey,” Applied Sciences, vol. 12, no. 18, p. 8972, \n2022.\n[18] L. N. Smith. “Cyclical learning rates for training neural networks”. \nIn: 2017 IEEE Winter Conference on Applications of Computer \nVision (WACV). IEEE, Santa Rosa, CA, USA. 2017.\n[19] G. Hai and Y. Matras. “Kurdish linguistics: A brief overview”. STUF-\nLanguage Typology and Universals, vol. 55, pp. 3-14, 2002.\n[20] M. R. Manzini, L. M. Savoia and L. Franco. “Ergative case, aspect \nand person splits: Two case studies”. Acta Linguistica Hungarica, \nvol. 52, pp. 297-351, 2015."
}