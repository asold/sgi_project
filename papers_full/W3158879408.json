{
  "title": "CAT: Cross-Attention Transformer for One-Shot Object Detection",
  "url": "https://openalex.org/W3158879408",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357653313",
      "name": "Lin Wei-dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227070711",
      "name": "Deng Yu-yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105146925",
      "name": "Gao, Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1980283440",
      "name": "Wang, Ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2238303497",
      "name": "Zhou JingHao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227707702",
      "name": "Liu, Lingqiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1842339085",
      "name": "Zhang Lei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1980315927",
      "name": "Wang Peng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799058067",
    "https://openalex.org/W3017747047",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W1976931317",
    "https://openalex.org/W2902069582",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2936707910",
    "https://openalex.org/W2962837320",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2890678738",
    "https://openalex.org/W2983156430",
    "https://openalex.org/W3011092928",
    "https://openalex.org/W3034974675",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2990139668",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963282369",
    "https://openalex.org/W2626778328"
  ],
  "abstract": "Given a query patch from a novel class, one-shot object detection aims to detect all instances of that class in a target image through the semantic similarity comparison. However, due to the extremely limited guidance in the novel class as well as the unseen appearance difference between query and target instances, it is difficult to appropriately exploit their semantic similarity and generalize well. To mitigate this problem, we present a universal Cross-Attention Transformer (CAT) module for accurate and efficient semantic similarity comparison in one-shot object detection. The proposed CAT utilizes transformer mechanism to comprehensively capture bi-directional correspondence between any paired pixels from the query and the target image, which empowers us to sufficiently exploit their semantic characteristics for accurate similarity comparison. In addition, the proposed CAT enables feature dimensionality compression for inference speedup without performance loss. Extensive experiments on COCO, VOC, and FSOD under one-shot settings demonstrate the effectiveness and efficiency of our method, e.g., it surpasses CoAE, a major baseline in this task by 1.0% in AP on COCO and runs nearly 2.5 times faster. Code will be available in the future.",
  "full_text": "CAT: Cross-Attention Transformer for One-Shot Object Detection\nWeidong Lin1,2 , Yuyan Deng1,2 , Yang Gao1,2 , Ning Wang1,2 , Jinghao Zhou1 , Lingqiao\nLiu3 , Lei Zhang1,2 , Peng Wang1,2‚àó\n1School of Computer Science, Northwestern Polytechnical University, China\n2National Engineering Lab for Integrated Aero-Space-Ground-Ocean\nBig Data Application Technology, China\n3The University of Adelaide, Australia\n{weidong.lin, dengyuyan, gy7, ningw}@mail.nwpu.edu.cn\njensen.zhoujh@gmail.com, lingqiao.liu@adelaide.edu.au\n{nwpuzhanglei, peng.wang}@nwpu.edu.cn\nAbstract\nGiven a query patch from a novel class, one-shot\nobject detection aims to detect all instances of that\nclass in a target image through the semantic similar-\nity comparison. However, due to the extremely lim-\nited guidance in the novel class as well as the un-\nseen appearance difference between query and tar-\nget instances, it is difÔ¨Åcult to appropriately exploit\ntheir semantic similarity and generalize well. To\nmitigate this problem, we present a universal Cross-\nAttention Transformer (CAT) module for accu-\nrate and efÔ¨Åcient semantic similarity comparison in\none-shot object detection. The proposed CAT uti-\nlizes transformer mechanism to comprehensively\ncapture bi-directional correspondence between any\npaired pixels from the query and the target image,\nwhich empowers us to sufÔ¨Åciently exploit their se-\nmantic characteristics for accurate similarity com-\nparison. In addition, the proposed CAT enables\nfeature dimensionality compression for inference\nspeedup without performance loss. Extensive ex-\nperiments on COCO, VOC, and FSOD under one-\nshot settings demonstrate the effectiveness and ef-\nÔ¨Åciency of our method, e.g., it surpasses CoAE, a\nmajor baseline in this task by 1.0% in AP on COCO\nand runs nearly 2.5 times faster. Code will be avail-\nable in the future.\n1 Introduction\nObject detection is a fundamental task in computer vision\ndomain, which aims to predict a bounding box with a cate-\ngory label for each instance of interest in an image. Although\ndeep convolutional neural networks (DCNN) based object de-\ntection methods have achieved great success in recent years,\ntheir success heavily relies on a huge amount of annotated\ndata, which is often difÔ¨Åcult or even infeasible to collect in\nreal applications due to the expensive annotation cost. There-\n‚àóCorresponding Author\nQuery images\n Target images Backbone outputs CAT outputs\nFigure 1: Visualization results of the intermediate feature maps. We\nvisualize the response maps of the input and output of our proposed\nCAT module in the last two columns. By capturing the bidirectional\ncorrespondence between query and target images, our CAT module\nsigniÔ¨Åcantly reÔ¨Ånes the response map and pays more attention on\nthe objects with the same category of query objects.\nfore, it is inevitable to cope with object detection for unseen\nclasses with only a few annotated examples at test phase.\nIn this study, we mainly focus on the most challenging\nproblem, i.e., one-shot object detection. Given a novel class,\nthere is only one query image with one annotated object, and\na detector is then required to Ô¨Ånd all objects of the same cate-\ngory as the annotated object in a target image. Till now, some\neffective methods have been proposed, which mainly focus\non building a two-stage paradigm [Ren et al., 2016]. SpeciÔ¨Å-\ncally, in the Ô¨Årst stage, the feature of query image and the tar-\nget image is aggregated to exploit their semantic correspon-\ndence utilizing channel attention[Hsieh et al., 2019] or corre-\narXiv:2104.14984v1  [cs.CV]  30 Apr 2021\nlation Ô¨Åltering [Fan et al., 2020]. Then, a region proposal net-\nwork is utilized to detect all candidate objects and real ones\nare ultimately located by a followed semantic similarity com-\nparison based classiÔ¨Åer. However, due to extremely limited\nguidance for the novel class ( i.e., only one annotated sam-\nple) as well as the unseen appearance difference between the\nquery object and the target one ( e.g., that is often caused by\nthe intra-class variation and different imaging endearments),\nthese existing methods still fail to appropriately generalize\nwell with pleasing performance.\nTo mitigate this problem, we revisit the one-shot object de-\ntection problem and attempt to explore the accurate seman-\ntic correspondence between the query object and the target\nimage for performance enhancement. Considering that the\ngreat appearance difference often conceal their semantic cor-\nrespondence into an unknown embedding space, we have to\nsufÔ¨Åciently exploit any detailed correspondence between two\nimages. A direct way is to explore the relation between each\nsub-region from the query image and that in the target one.\nFollowing this idea, we propose a Cross-Attention Trans-\nformer (CAT) module and embed it into the two-stage detec-\ntion paradigm for comprehensive exploration of the bidirec-\ntional correspondence between the target and query images.\nThe proposed CAT module consists of two streams of inter-\nleaved transformers [Vaswani et al., 2017 ]. Given the grid\nfeature generated from a Siamese feature extractor, the two-\nstream transformer is utilized to exploit the bi-directional cor-\nrespondence between any paired sub-regions from the query\nand the target image through computing the cross-attention\nbetween them. As shown in Figure 1, the CAT module can\nsufÔ¨Åciently exploit the semantic characteristics of each im-\nage as well as their grid-level correspondences, which will\nbe beneÔ¨Åcial for accurate similarity comparison in the sec-\nond stage. In addition, due to sufÔ¨Åcient information captured\nby the CAT module, the dimensionality of the Ô¨Ånal feature\nrepresentation of each object can be effectively compressed\nwithout performance loss. To verify the effectiveness of the\nproposed method, we compare it with state-of-the-art on three\nstandard one-shot object detection benchmarks and observe\nsigniÔ¨Åcant performance and efÔ¨Åciency improvement.\nIn summary, this study mainly contributes in the following\nthree aspects:\n‚Ä¢ We propose a CAT module which is able to sufÔ¨Åciently\nexploit the grid-level correspondence between the query\nand target image for accurate and efÔ¨Åcient one-shot ob-\nject detection. It is noticeable that the CAT module is an\nuniversal module which can be seamlessly plugged into\nother existing one-shot object detection frameworks.\n‚Ä¢ With the CAT module, we develop an effective one-shot\ndetection network, which demonstrates state-of-the-art\nperformance on three standard benchmarks for one-shot\nobject detection.\n‚Ä¢ By compressing the feature channels, the proposed\nmodel is capable of running nearly 2.5 times faster than\nthe current state-of-the-art baseline CoAE [Hsieh et al.,\n2019] without performance degradation.\n2 Related Work\nIn this section, we will brieÔ¨Çy review two lines of research\nrelated to this study.\n2.1 Few-Shot Object Detection\nThe key for few-shot object detection is to establish a sim-\nilarity metric that can be appropriately generalize to unseen\nclasses with a few labeled examples ( i.e. query set). Efforts\nhave been made recently from different perspectives, such as\ntransfer learning, metric learning and attention-based meth-\nods.\nSpeciÔ¨Åcally, for transfer learning, [Chen et al., 2018 ]\npresents the regularization techniques to relieve the over-\nÔ¨Åtting caused by directly transferring knowledge from a large\nauxiliary dataset to the novel classes. Another work [Kang\net al., 2019] develops a single-stage detector combined with\na meta-model that re-weights the importance of features from\nthe base model. For metric learning, [Karlinsky et al., 2019]\nintroduces a distance metric based classiÔ¨Åer into the RoI mod-\nule in the detector, which maps the objects into the univer-\nsal embedding space. The attention-based methods focus\non modelling the correspondence between target and query.\n[Hsieh et al., 2019] designs a co-attention based model called\nCoAE which leverages the correlated features from the tar-\nget and query for better generalization performance. [Fan et\nal., 2020] introduces depth-wise convolution to get the atten-\ntion feature map in the RPN phase and proposes the multi-\nrelation detector to model different relationships in the R-\nCNN phase. [Osokin et al., 2020 ] Ô¨Årstly performs dense\ncorrelation matching based on local features and then con-\nducts spatial alignment and bi-linear resampling to compute\nthe detection score.\nOur work lies on the third line of research, the attention-\nbased methods. Different from previous work, our proposed\nCAT module empowers us to deeply exploit the grid-level\nbidirectional correspondence between target and query, using\nstacks of cross-attention transformer layers.\n2.2 Visual Transformer\nWitnessing that transformer becomes the de-facto standard in\nNatural Language Processing (NLP), recent literature com-\nmences introducing transformer-like networks into various\ncomputer vision tasks, including image recognition [Doso-\nvitskiy et al., 2020; Touvron et al., 2020 ], object detec-\ntion [Carion et al., 2020; Zhu et al., 2020 ], segmenta-\ntion [Ye et al., 2019], visual question answering (VQA) [Tan\nand Bansal, 2019; Su et al., 2020 ]. The Vision Trans-\nformer (ViT) [Dosovitskiy et al., 2020 ] directly feeds im-\nage patches into a transformer for image classiÔ¨Åcation, which\nremoves the need of any convolution operation. [Carion et\nal., 2020] proposes DETR, a transformer encoder-decoder ar-\nchitecture that performs end-to-end object detection as set\nprediction. It does not rely on many manual components\nthat required by traditional detectors, such as non-maximum\nsuppression and anchor selection. [Ye et al., 2019 ] pro-\nposes a cross-modal self-attention model to capture the long-\nrange dependencies between language and visual features.\nLXMERT [Tan and Bansal, 2019] and VL-BERT [Su et al.,\nTarget Feature\nQuery Feature \n3x3  \nconv\n1x1  \nconv\nSiamese Feature Extractor\nreshape\nreshape\nTarget\nQuery\n3 x ùëØùíï x ùëæùíï\n3 x ùëØùíí x ùëæùíí\nùëØùíï\n‚Ä≤ùëæùíï\n‚Ä≤\nùëØùíí‚Ä≤ ùëæùíí‚Ä≤\nN x\nCross-Attention Transformer\nreshape\nreshape\n256 x ùëØùíï\n‚Ä≤ x ùëæùíï\n‚Ä≤\n256 x ùëØùíí‚Ä≤ x ùëæùíí‚Ä≤\nRPN &\nRoI Align ‚Ä¶\nShared Conv\n& GAP .\nClassify\nRegress\n...\nShared Conv \n& GAP\nRoI Feature\nConcated Feature\nMulti-Head \nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nkvq\nMulti-Head \nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nqvk\n: Positional Encoding\n: Element Add\n: Concat.\nFigure 2: The overall architecture of the proposed method for one-shot object detection. Our detector is composed of three parts. The Ô¨Årst\npart is a shared ResNet-50 [He et al., 2016] backbone used to extract features of both the target and query images. And the following part is\nour Cross-Attention Transformer (CAT) module that fuses the features from backbone and enhances the features of the regions which may\nbe the same category as query in the target image, while the last part is the detection head with a regular RPN head and a R-CNN head like\nFaster R-CNN.\n2020] are transformer-like visual-linguistic pretraining mod-\nels that achieves superior performance on several vision-\nlanguage tasks. To our best knowledge, our proposed model\nis the Ô¨Årst attempt to employing transformers for the task\nof one-shot object detection. Moreover, it relies on a two-\nstream cross-attention architecture, rather than the commonly\nadopted self-attention mechanisms.\n3 Our Approach\nWe formulate the one-shot object detection task as in [Hsieh\net al., 2019]. Given a query image patchpwith its class label,\nthe one-shot detector aims to detect all object instances of\nthe same class in a target image I, where we assume that\nat least one instance exists in the target image. We denote\nthe set of classes in the testing phase (unseen classes) as C0\nwhile those in the training phase (seen classes) is C1, and\nC0 ‚à©C1 = ‚àÖ. The model is trained with the annotated data\nof the seen classes, and generalize to unseen classes with a\nsingle query image.\n3.1 Overall Architecture\nAs shown in Figure 2, our proposed architecture is com-\nposed of three parts, including the feature extractor (back-\nbone), the cross-attention module and the similarity-based de-\ntection head. At Ô¨Årst, we adopt the ResNet- 50 to extract fea-\ntures from both query image Iq ‚ààR3√óHq√óWq and target im-\nage It ‚ààR3√óHt√óWt . Note that the backbone parameters are\nshared between query and target images. What needs to be es-\npecially explained is that we only use the Ô¨Årst three blocks of\nResNet-50 to extract feature maps with high resolutions. For\nthe ease of representations, we denote œÜ(It) ‚ààRC√óH‚Ä≤\nt√óW‚Ä≤\nt\nand œÜ(Iq) ‚ààRC√óH‚Ä≤\nq√óW‚Ä≤\nq as the feature maps of target and\nquery images respectively, where œÜrepresents the backbone,\nC = 1024, H‚Ä≤\nt = Ht\n16 , W‚Ä≤\nt = Wt\n16 , H‚Ä≤\nq = Hq\n16 and W‚Ä≤\nq = Wq\n16 .\nAfter that, we use a 3 √ó3 convolution and a 1 √ó1 convolu-\ntion to compress the number of channels ofœÜ(It),œÜ(Iq) from\n1024 to dm = 256 . Both features are Ô¨Çattened in the di-\nmension of spatial and further deeply aggregated by the CAT\nmodule with cross-attention mechanism as deÔ¨Åned in the fol-\nlowing formula:\n(Ft,Fq) = CAT(œÜ(It)‚Ä≤,œÜ(Iq)‚Ä≤), (1)\nwhere œÜ(It)‚Ä≤‚ààRdm√óH‚Ä≤\ntW‚Ä≤\nt ,œÜ(Iq)‚Ä≤‚ààRdm√óH‚Ä≤\nqW‚Ä≤\nq are the in-\nput sequences, and Ft ‚ààRdm√óH‚Ä≤\nt√óW‚Ä≤\nt ,Fq ‚ààRdm√óH‚Ä≤\nq√óW‚Ä≤\nq\nare the output feature maps after cross-attention.\nIn the end, RPN-based head takes as input the aggregated\ntarget features and generates proposals for further classiÔ¨Åca-\ntion and regression. The features of proposals p1,p2,¬∑¬∑¬∑ ,pn\nextracted from Ft by ROI align are fed into a regressor to\nobtain reÔ¨Åned bounding boxes.\nbboxi = Œ¶r(œà(Ft,pi)), (2)\nwhere Œ¶r represents the regressor andœàrepresents the opera-\ntion of ROI align. For similarity-based classiÔ¨Åcation, we Ô¨Årst\napply global average pooling on the RoI features and the ag-\ngregated query feature Fq, and then concatenate them as the\ninput of classiÔ¨Åer Œ¶c. The classÔ¨Åcation results P(bboxi),i =\n1,2,...,n can be formulated as:\nP(bboxi) = Œ¶c(Concat(GAP(œà(Ft,pi)),GAP(Fq))).\n(3)\n3.2 Cross-Attention Transformer Module\nThe cross-attention transformer (CAT) model is the key com-\nponent of our proposed framework. Based on the transformer\narchitecture, it models the bidirectional correspondences be-\ntween grids of target and query images and performs dual\nfeature aggregation for both target and query.\nThe basic building block of transformer is the ‚ÄòScaled Dot-\nProduct Attention‚Äô deÔ¨Åned as follows:\nAttention(Q,K,V ) = softmax(QKT\n‚àödk\n)V, (4)\nwhere Q,K,V represent queries, keys and values, respec-\ntively. dk is the dimension of keys.\nAs described in [Vaswani et al., 2017], Multi-Head Atten-\ntion mechanism is further employed to jointly attend to infor-\nmation from different representation subspaces:\nMultiHead(Q, K, V) = Concat(head1, ¬∑ ¬∑ ¬∑ , headM)WO,\nheadi = Attention(QWQ\ni , KWK\ni , V WV\ni ),\n(5)\nwhere WQ\ni ‚ààRdm√ód‚Ä≤\n, WK\ni ‚ààRdm√ód‚Ä≤\n, WK\ni ‚ààRdm√ód‚Ä≤\nare\nthe matrices to compute the so-called query, key and value\nembeddings respectively, andWO ‚ààRMd‚Ä≤√ódm is the projec-\ntion matrix. In our work, we set d‚Ä≤ = dm\nM , dm = 256 and\nM = 8.\nAfter the Multi-Head Attention operation, the output is sent\ninto a Feed-forward Network (FFN) module composed of two\nlinear transformation with ReLU activation, deÔ¨Åned as:\nFFN(x) = max(0,xW1 + b1)W2 + b2, (6)\nwhere W1,W2 and b1,b2 are the weight matrices and basis\nvectors respectively.\nRecently, Carion [Carion et al. , 2020 ] proposed a\ntransformer-like model (DETR) for general object detection\nand obtain competing performance. Although we also em-\nploy transformer in this work, there are still signiÔ¨Åcant dif-\nferences between DETR and our model. Firstly, the chal-\nlenges faced by the two models are different. As a general\nobject detector, DETR focuses on the discrimination between\nforeground and background, and accurate bounding box re-\ngression. On the contrary, the difÔ¨Åculty of one shot detec-\ntion is mainly on the similarity-based comparison, rather than\nproposal generation [Zhang et al., 2011 ]. Through experi-\nments, we found that in many cases, one-shot detection mod-\nels can produce accurate bounding boxes of salient objects\nbut fails to assign correct class label. To resolve their in-\ndividual challenges, DETR and our model choose different\nmodel architectures. DETR is built upon self-attention that\nexplores long-range dependencies between pixels of a single\ninput image. In contrast, our model relies on a two-stream\narchitecture which performs cross-attention (Query-to-Target\nand Target-to-Query) to exploit the similarity between sub-\nregions of query and target images.\nTo be more speciÔ¨Åc, Xt ‚ààRNt√ódm and Xq ‚ààRNq√ódm\nrepresent the input sequences that are the Ô¨Çattened feature\nmaps of target and query images respectively, as shown in\nFigure 2. Note that Nt = H‚Ä≤\nt √óW‚Ä≤\nt and Nq = H‚Ä≤\nq √óW‚Ä≤\nq\nare the lengths of the sequences. Following [Carion et al.,\n2020], we use sinefunction to generate spatial position en-\ncoding for input sequencesXt and Xq. In one stream of CAT,\nwe let Q= Xt and K = V = Xq in equation (5), and obtain\nthe aggregated target feature. This procedure can be summa-\nrized as:\nYt = Norm(ÀúXt + FFN (ÀúXt)) (7)\nÀúXt = Norm(Xt + Pt + MultiHead(Xt + Pt,\nXq + Pq, Xq))\n(8)\nwhere Pt ‚àà RNt√ódm ,Pq ‚àà RNq√ódm are the spatial posi-\ntion encodings corresponding to Xt and Xq, respectively. In\nanother stream, we set Q= Xq and K = V = Xt and gener-\nate Yq, the aggregated query feature. The above whole com-\nputation can be viewed as one layer of our proposed Cross-\nAttention Transformer, and the outputs of one layer will be\nthe inputs of the next layer. In our work, we set the number\nof layers N = 4.\nThe outputs of CAT module are then reshaped to new fea-\nture maps Ft and Fq that share the same sizes as the origin\nfeature maps, where Ft is fed into the subsequent RPN and\nFq is used in similarity-based classiÔ¨Åcation.\n4 Experiments\nOur experiments are conducted on the MS-COCO [Lin et al.,\n2014], PASCAL VOC and the recently released FSOD [Fan\net al., 2020 ] dataset. In Section 4.1, we Ô¨Årst introduce im-\nplementation details. Then we carry out ablation study and\ncomparison with SOTA in Sections 4.2 and 4.3 respectively.\n4.1 Implementation Details\nTraining Details. Our network is trained with stochastic gra-\ndient descent (SGD) over 4 NVIDIA RTX-2080Ti GPUs for\n10 epochs with the initial learning rate being0.01 and a mini-\nbatch of 16 images. The learning rate is reduced by a fac-\ntor of 10 at epoch 5 and 9, respectively. Weight decay and\nmomentum are set as 0.0001 and 0.9, respectively. As in\n[Hsieh et al., 2019], the backbone ResNet-50 model is pre-\ntrained on a reduced training set of ImageNet in which all\nthe COCO classes are removed to ensure that our model does\nnote ‚Äòforesee‚Äô any unseen class. The target images are re-\nsized to have their shorter side being 600 and their longer\nMethod FPS Unseen Seen\nAP AP 50 AP AP 50\nCAT (One stream) 18.4 15 .2 25 .3 30 .3 48 .6\nCAT (Two stream) 16.3 16.5 27 .1 31 .3 50 .5\nTable 1: Ablation study of CAT on the COCO split 1. ‚ÄòTwo stream‚Äô\nrepresents our model that performs both query-to-target and target-\nto-query attentions, while ‚ÄòOne stream‚Äô represents a model that exe-\ncutes the query-to-target side.\nLayers Unseen Seen\nAP AP 50 AP AP 50\nCAT (3 layers) 15.8 25 .9 31 .2 50 .0\nCAT (4 layers) 16.5 27 .1 31.3 50 .5\nCAT (5 layers) 16.5 27 .1 32.1 51 .6\nCAT (6 layers) 16.3 27 .1 31 .8 51 .3\nTable 2: Results of Cross-Attention Transformer with different lay-\ners on COCO split 1.\ndm Params(M) FPS Unseen Seen\nAP AP 50 AP AP 50\n128 12 .74 20 .8 14 .6 26 .0 29 .0 49 .6\n256 19 .10 16 .3 16 .5 27 .1 31 .3 50 .5\n512 37 .67 9 .5 16.6 27 .3 32.1 51 .6\n1024 110 .53 4 .9 15 .7 25 .8 32.7 52 .4\nTable 3: Results of different dimension of feature embeddings on\nCOCO split 1.\nMethod Split1 Split2 Split3 Split4 Average\nAP AP 50 AP AP 50 AP AP 50 AP AP 50 AP AP 50\nSiamMask - 15.3 - 17.6 - 17.4 - 17.0 - 16.8\nCoAE 11.8 23 .2 12 .2 23 .7 9 .3 20 .3 9 .4 20 .4 10 .7 21 .9\nCoAE (Reimp) 15.1 25 .7 15 .3 25 .4 11 .0 21 .0 12 .5 21.7 13.5 23 .5\nOurs 16.5 27 .1 16 .6 26 .6 12 .4 22 .5 12 .6 21.4 14.5 24 .4\nTable 4: Results on the COCO dataset of unseen classes, we set the results of CoAE as our baseline. For fair comparisons, we re-implement\nCoAE on our code framework and report its results, ‚ÄòReimp‚Äô represents our re-implemented model.\nMethod Split1 Split2 Split3 Split4 Average\nAP AP 50 AP AP 50 AP AP 50 AP AP 50 AP AP 50\nSiamMask - 38.9 - 37.1 - 37.8 - 36.6 - 37.6\nCoAE 22.4 42 .2 21 .3 40 .2 21 .6 39 .9 22 .0 41 .3 21 .8 40 .9\nCoAE (Reimp) 31.2 51.3 27.3 45 .3 27 .7 45 .0 28 .8 47 .3 28 .8 47 .2\nOurs 31.3 50.5 28.8 46 .1 28 .9 45 .3 29 .6 47 .5 29 .7 47 .3\nTable 5: Results on the COCO dataset of seen classes.\nMethod Seen class Unseen classplant sofa tv car bottle boat chair person bus train horse bike dog bird mbike table mAP cow sheep cat aero mAPSiamFC 3.2 22 .8 5 .0 16 .7 0 .5 8 .1 1 .2 4 .2 22 .2 22.6 35.4 14.2 25.8 11.7 19 .7 27 .8 15.1 6 .8 2 .28 31.6 12.4 13.3SiamRPN 1.9 15 .7 4 .5 12 .8 1 .0 1 .1 6 .1 8 .7 7 .9 6 .9 17 .4 17.8 20.5 7 .2 18 .5 5 .1 9 .6 15 .9 15 .7 21 .7 3 .5 14 .2CompNet 28.4 41.5 65.0 66.4 37 .1 49 .8 16.2 31 .7 69 .7 73.1 75.6 71.6 61.4 52.3 63 .4 39 .8 52.7 75.3 60 .0 47 .9 25.3 52.1CoAE 30.0 54.9 64.1 66.7 40 .1 54 .1 14.7 60 .9 77 .5 78.3 77.9 73.2 80.5 70.8 72.4 46.2 60.1 83.9 67 .1 75 .6 46.2 68.2CoAE(Reimp)47.3 61.8 72.1 83.0 56.6 63.1 40.4 80.3 81.3 80.6 79.6 77.1 83.2 75.0 69 .4 45 .5 68.5 84.3 76.5 81.5 54.6 74.2Ours 44.2 65.5 67.1 83.9 54.2 66.8 45.6 79.5 76 .8 82.3 81.4 78.5 84.0 76.7 71.0 33 .9 68.2 84.8 75.6 83.7 57.8 75.5\nTable 6: Results on the VOC dataset, we compare our model with several previous works and our baseline model CoAE.\nMethod AP AP 50 AP75\nCoAE (Reimp) 40.3 63 .8 41 .7\nOurs 42.0 64 .0 44 .2\nTable 7: Results on the FSOD dataset (unseen classes).\nside less or equal to 1000, and the query image patches are\nresized to a Ô¨Åxed size128x128. We built our model on mmde-\ntection [Chen et al., 2019 ], which is a general object de-\ntection framework based on PyTorch.Based on spatial-wise\nand channel-wise co-Attention, CoAE [Hsieh et al., 2019 ]\nachieves the best performance over existing approaches and\nserve as a major baseline in our paper. For strictly fair com-\nparison, we re-implemented the CoAE model on the same\nmmdetection framework, and achieves signiÔ¨Åcantly better re-\nsults than the original author-provided version on all the three\nevaluated datasets. The reason may be better training strate-\ngies in mmdetection, such as multiply data augmentations\nand optimized pipeline.\nInference Details. The same evaluation strategy as [Hsieh\net al., 2019] is applied for fair comparison. SpeciÔ¨Åcally, we\nÔ¨Årstly shufÔ¨Çe the query image patches of that class with a\nrandom seed of target image ID, then sample the Ô¨Årst Ô¨Åve\nquery image patches, we run our evaluations on these patches\nand take the average of these results as the stable statistics for\nevaluation.\n4.2 Ablation Study\nSince the Cross-Attention Transformer is the key component\nof our model, in this section we mainly explore the effect\nof this module with different hyper-parameters. For easy il-\nlustration, our ablation experiments are conducted on COCO\nsplit 1 which will be discussed in Section 4.3.\nTransformer Structure. Our CAT module consists of a\nstack of two-stream transformer layers, each of stream per-\nforming target-to-query or query-to-target attention and gen-\nerating the corresponding target or query features. In Table 1,\nwe compare the two-stream architecture with a one-stream\ntransformer that only performs query-to-target attention and\ngenerates aggregated target features. The results show that\nthe the one-stream model incurs 1.3 and 1.0 AP drops on un-\nseen and seen classes respectively, demonstrating the impor-\ntance of bidirectional feature aggregation.\nNumber of CAT Layers. We investigate the performance\nof CAT with different number of layers. As shown in Table 2,\nwe test the results of CAT with the number of layers ranging\nfrom 3 to 6. The CAT with 4 layers achieves the best perfor-\nmance on unseen classes, while on seen classes the best AP\nis obtained with the number of layers as 5. It can be found\nthat increasing the number of layers does not always improve\nperformance, which may be caused by the overÔ¨Åtting on seen\nclasses. Note that even using only 3 layers, our model still\noutperforms the CoAE, demonstrating the superiority of the\nproposed method. In the remaining experiments, we set the\nnumber of layers to 4 by default.\nDimension of Feature Embeddings.Table 3 shows the re-\nsults with different values of dm on COCO spit 1. We also\nreport their number of parameters and inference speed (FPS).\nFrom the results, we can Ô¨Ånd that reducing dm to 128 will\nsigniÔ¨Åcantly decrease AP by 2 points on unseen classes. The\nAPs with dm = 256 and dm = 512 are close to each other,\nbut setting dm to 512 will signiÔ¨Åcantly increase the model\nsize and slow down the inference speed. The results with\nQuery images Target images Backbone outputs Layer-1 Layer-2 Layer-3 Layer-4\nFigure 3: Visualization results of the intermediate feature maps. We visualize the outputs of each layer in our Cross-Attention Transformer\non several target-query pairs.\ndm = 1024 shows an overÔ¨Åtting on seen classes. To strike\nthe balance between accuracy and speed, we set dm to 256 in\nfollowing experiments.\n4.3 Comparison with State-of-the-Art\nMS-COCO. Following the previous work [Hsieh et al.,\n2019], we divide the 80 classes of COCO dataset [Lin et\nal., 2014 ] into four groups, alternately taking three groups\nas seen classes and one group as unseen classes. We use\nthe ‚Äòtrain 2017‚Äô (118K images) split for training and mini-\nval (5K images) split for evaluation. We compare our method\nwith SiamMask [Michaelis et al., 2018 ] and CoAE [Hsieh\net al., 2019] in Tables 4 and 5. Besides the authors‚Äô release\nof CoAE model (denoted as CoAE in the Tables), we also\nre-implement this model in the mmdetection framework (de-\nnoted as CoAE(Reimp)) for strictly fair comparison. Note\nthat CoAE (Reimp) is trained with the same strategies as our\nmodel and achieves better results than the original version, so\nit serves as a strong and fair baseline. Tables 4 and 5 show\nthe comparison on unseen and seen classes, respectively.\nCompared with the re-implemented CoAE model, our model\nachieves 1.0% and 0.9% improvements on the average AP\nand AP50 respectively. As for seen classes, our model also\nachieves better performance that outperforms CoAE (Reimp)\nby 0.9 AP point on average.\nPASCAL VOC. As for VOC [Everingham et al., 2010 ],\nwe divide the 20 classes into 16 seen classes and 4 unseen\nclasses, where the choice of seen classes and unseen classes\nis consistent with [Hsieh et al., 2019]. Note that our model\nis trained on the union set of VOC 2007 train&val sets and\nVOC2012 train&val sets, while is evaluated on VOC 2007.\nWe evaluate the average precision of each category, and cal-\nculate mean average precision (mAP) of seen classes and\nunseen classes, respectively. Table 6 shows the compari-\nson with CoAE and other several baselines [Fu et al., 2020;\nCen and Jung, 2018; Li et al., 2018], whose evaluation set-\ntings are consistent with ours. Our model outperforms the re-\nimplemented CoAE by 1.3 mAP points on unseen classes and\nperforms slightly worse ( 0.3 mAP) on seen classes, which\npresents a stronger generalization ability from seen classes to\nunseen classes.\nFSOD. The FSOD dataset [Fan et al., 2020] is speciÔ¨Åcally\ndesigned for few-shot object detection. It contains 1000 cate-\ngories, with 800 for training and 200 for test. We test the per-\nformance of our model and our re-implemented CoAE model\non this dataset, with the same one-shot setting. Table 7 shows\nthat our model outperforms CoAE by 1.7% in AP and 2.5%\nin AP 75 on novel classes.\nInference Speed. Note that our model achieves superior ac-\ncuracy with a much smaller dimension of features ( dm =\n256) than that of the previous SOTA CoAE ( 1024). On the\nother hand, the dot-product attention adopted by transformer\nis more parallelizable and space-efÔ¨Åcient. These two charac-\nteristics lead to a much faster inference speed: on an NVIDIA\nRTX-2080Ti GPU, our model achieves 16.3 FPS, while the\nspeed of CoAE is only5.9 FPS that is nearly 2.5 times slower\nthan ours.\n4.4 Visualization of CAT layers\nFor intuitively understanding our model, we visualize the\nintermediate feature maps according to the intensity of re-\nsponse. As shown in Figure 3, the Ô¨Årst and second columns\nrepresent query and target images respectively, and the re-\nmaining columns correspond to the visualization of different\nCAT layers. Without incorporating any query information,\nthe backbone outputs endow higher responses on salient ob-\njects or features. With the increase of layers and deeper ag-\ngregation of query information, the CAT outputs gradually\nfocus on the objects of the same category as query. The visu-\nalization demonstrates the importance of our proposed CAT\nmodule on exploiting the correspondence between target and\nquery.\n5 Conclusion\nIn this work, we propose a Cross-Attention Transformer mod-\nule to deeply exploit bidirectional correspondence between\nthe query and target pairs for one-shot object detection. By\ncombining the proposed CAT module with a two-stage frame-\nwork, we construct a simple yet effective one-shot detector.\nThe proposed model achieves state-of-the-art performance on\nthree one-shot detection benchmarks and meanwhile runs2.5\ntimes faster than CoAE, a major strong baseline, demonstrat-\ning a superiority over both effectiveness and efÔ¨Åciency.\nReferences\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. In ECCV, pages 213‚Äì229, 2020.\n[Cen and Jung, 2018] Miaobin Cen and Cheolkon Jung.\nFully convolutional siamese fusion networks for object\ntracking. In ICIP, pages 3718‚Äì3722, 2018.\n[Chen et al., 2018] Hao Chen, Yali Wang, Guoyou Wang,\nand Yu Qiao. Lstd: A low-shot transfer detector for ob-\nject detection. In AAAI, volume 32, 2018.\n[Chen et al., 2019] Kai Chen, Jiaqi Wang, Jiangmiao Pang,\nYuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun,\nWansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi\nCheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong\nWang, Jianping Shi, Wanli Ouyang, Chen Change Loy,\nand Dahua Lin. Mmdetection: Open mmlab detection\ntoolbox and benchmark. arXiv preprint arXiv:1906.07155,\n2019.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Everingham et al., 2010] M. Everingham, L. Van Gool,\nC. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes (voc) challenge. International Jour-\nnal of Computer Vision, 88(2):303‚Äì338, June 2010.\n[Fan et al., 2020] Qi Fan, Wei Zhuo, Chi-Keung Tang, and\nYu-Wing Tai. Few-shot object detection with attention-rpn\nand multi-relation detector. In CVPR, 2020.\n[Fu et al., 2020] Kun Fu, Tengfei Zhang, Yue Zhang, and\nXian Sun. Oscd: A one-shot conditional object detection\nframework. Neurocomputing, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, pages 770‚Äì778, 2016.\n[Hsieh et al., 2019] Ting-I Hsieh, Yi-Chen Lo, Hwann-\nTzong Chen, and Tyng-Luh Liu. One-shot object detec-\ntion with co-attention and co-excitation. In NeurIPS, vol-\nume 32, pages 2725‚Äì2734, 2019.\n[Kang et al., 2019] Bingyi Kang, Zhuang Liu, Xin Wang,\nFisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot ob-\nject detection via feature reweighting. In CVPR, pages\n8420‚Äì8429, 2019.\n[Karlinsky et al., 2019] Leonid Karlinsky, Joseph Shtok,\nSivan Harary, Eli Schwartz, Amit Aides, Rogerio\nFeris, Raja Giryes, and Alex M Bronstein. Repmet:\nRepresentative-based metric learning for classiÔ¨Åcation and\nfew-shot object detection. In CVPR, pages 5197‚Äì5206,\n2019.\n[Li et al., 2018] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu.\nHigh performance visual tracking with siamese region pro-\nposal network. In CVPR, pages 8971‚Äì8980, 2018.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge J. Be-\nlongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll¬¥ar, and C. Lawrence Zitnick. Microsoft coco: Com-\nmon objects in context. In ECCV, pages 740‚Äì755, 2014.\n[Michaelis et al., 2018] Claudio Michaelis, Ivan\nUstyuzhaninov, Matthias Bethge, and Alexander S.\nEcker. One-shot instance segmentation. arXiv preprint\narXiv:1811.11507, 2018.\n[Osokin et al., 2020] Anton Osokin, Denis Sumin, and\nVasily Lomakin. Os2d: One-stage one-shot object de-\ntection by matching anchor features. arXiv preprint\narXiv:2003.06800, 2020.\n[Ren et al., 2016] Shaoqing Ren, Kaiming He, Ross Gir-\nshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. TPAMI,\n39(6):1137‚Äì1149, 2016.\n[Su et al., 2020] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li,\nLewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-\ntraining of generic visual-linguistic representations. In\nICLR, 2020.\n[Tan and Bansal, 2019] Hao Tan and Mohit Bansal.\nLXMERT: Learning cross-modality encoder representa-\ntions from transformers. In EMNLP, 2019.\n[Touvron et al., 2020] Hugo Touvron, Matthieu Cord,\nMatthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv ¬¥e J ¬¥egou. Training data-efÔ¨Åcient image\ntransformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, volume 30, pages 5998‚Äì6008, 2017.\n[Ye et al., 2019] Linwei Ye, Mrigank Rochan, Zhi Liu, and\nYang Wang. Cross-modal self-attention network for refer-\nring image segmentation. In CVPR, pages 10502‚Äì10511,\n2019.\n[Zhang et al., 2011] Ziming Zhang, Jonathan Warrell, and\nPhilip HS Torr. Proposal generation for object detection\nusing cascaded ranking svms. InCVPR, pages 1497‚Äì1504.\nIEEE, 2011.\n[Zhu et al., 2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin\nLi, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection.\narXiv preprint arXiv:2010.04159, 2020.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.662901759147644
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4117930829524994
    },
    {
      "name": "Computer science",
      "score": 0.3542179465293884
    },
    {
      "name": "Computer vision",
      "score": 0.34201762080192566
    },
    {
      "name": "Business",
      "score": 0.32962900400161743
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32741665840148926
    },
    {
      "name": "Electrical engineering",
      "score": 0.19033530354499817
    },
    {
      "name": "Engineering",
      "score": 0.18603774905204773
    },
    {
      "name": "Materials science",
      "score": 0.1321631669998169
    },
    {
      "name": "Voltage",
      "score": 0.11292314529418945
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    }
  ],
  "institutions": []
}