{
  "title": "Making Transformers Solve Compositional Tasks",
  "url": "https://openalex.org/W3191727383",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2124070775",
      "name": "Santiago Ontañón",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2121139472",
      "name": "Zachary Fisher",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1209566617",
      "name": "Vaclav Cvicek",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2788751659",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3197009789",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W2574741565",
    "https://openalex.org/W3166703466",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3043172396",
    "https://openalex.org/W2996094825",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3175970729",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W4287755175",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2939413764",
    "https://openalex.org/W3104534595",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3154669786",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3017374003",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG).",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3591 - 3607\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMaking Transformers Solve Compositional Tasks\nSantiago Ontañón, Joshua Ainslie, Vaclav Cvicek, Zachary Fisher\nGoogle Research\n{santiontanon, jainslie, vcvicek, zachfisher}@google.com\nAbstract\nSeveral studies have reported the inability of\nTransformer models to generalize composi-\ntionally, a key type of generalization in many\nNLP tasks such as semantic parsing. In this\npaper we explore the design space of Trans-\nformer models showing that the inductive bi-\nases given to the model by several design deci-\nsions signiﬁcantly impact compositional gen-\neralization. We identiﬁed Transformer conﬁg-\nurations that generalize compositionally signif-\nicantly better than previously reported in the\nliterature in many compositional tasks. We\nachieve state-of-the-art results in a semantic\nparsing compositional generalization bench-\nmark (COGS), and a string edit operation com-\nposition benchmark (PCFG).\n1 Introduction\nAlthough modern neural network architectures\nreach state-of-the-art performance in many chal-\nlenging natural language tasks, they seem to exhibit\na low amount of “compositional generalization”,\ni.e., the ability to learn a set of basic primitives and\ncombine them in more complex ways than those\nseen during training (Hupkes et al., 2020). For ex-\nample, suppose a system has learned the meaning\nof “jump” and that “jump twice” means that the\naction “jump” has to be repeated two times. Upon\nlearning the meaning of the action “jax”, it should\nbe able to infer what “jax twice” means. Compo-\nsitional generalization is a key aspect of natural\nlanguage and many other tasks we might want ma-\nchine learning models to learn.\nWhile both humans and classical AI techniques\n(such as grammars or search-based systems) can\nhandle compositional tasks with relative ease, it\nseems that modern deep learning techniques do not\npossess this ability. A key question is thus: Can\nwe build deep learning architectures that can also\nsolve compositional tasks? In this paper we focus\non Transformers (Vaswani et al., 2017), which have\nbeen shown in the literature to exhibit poor com-\npositional generalization (see Section 2). Through\nan empirical study, we show that this can be im-\nproved. With the goal of creating general models\nthat generalize compositionally in a large range of\ntasks, we show that several design decisions, such\nas position encodings, decoder type, weight shar-\ning, model hyper-parameters, and formulation of\nthe target task result in different inductive biases,\nwith signiﬁcant impact for compositional general-\nization1. We use a collection of twelve datasets\ndesigned to measure compositional generalization.\nIn addition to six standard datasets commonly used\nin the literature (such as SCAN (Lake and Baroni,\n2018), PCFG (Hupkes et al., 2020), CFQ (Keysers\net al., 2019) and COGS (Kim and Linzen, 2020)),\nwe also use a set of basic algorithmic tasks (such\nas addition, duplication, or set intersection) that\nalthough not directly involving natural language,\nare useful to obtain insights into what can and can-\nnot be learned with different Transformer models.\nWe also include tasks where we do not see sig-\nniﬁcant improvements, to understand what types\nof compositional generalization are improved with\nour proposed modiﬁcations, and which are not.\nThe main contributions of this paper are: (1) A\nstudy of the Transformer design space, showing\nwhich design choices result in compositional learn-\ning biases across a variety of tasks. (2) state-of-the-\nart results in COGS, where we report a classiﬁca-\ntion accuracy of 0.784 using an intermediate repre-\nsentation based on sequence tagging (compared to\n0.35 for the best previously reported model (Kim\nand Linzen, 2020)), and the productivity and sys-\ntematicity splits of PCFG (Hupkes et al., 2020).\nThe rest of this paper is organized as follows.\nSection 2 provides some background on compo-\nsitional generalization and Transformers. In Sec-\n1Source code: https://github.com/\ngoogle-research/google-research/tree/\nmaster/compositional_transformers.\n3591\ntion 3, we present the datasets used in our empirical\nevaluation, which is presented in Section 4. The\npaper closes with a discussion on the implications\nof our results, and directions for future work.\n2 Background\nThis section brieﬂy provides background on com-\npositional generalization and Transformer models.\n2.1 Compositional Generalization\nCompositional generalization can manifest in dif-\nferent ways. Hupkes et al. (2020) identiﬁed ﬁve\ndifferent types, such as systematicity and produc-\ntivity (extrapolation to longer sequences than those\nseen during training). Systematicity is the ability\nof recombining known parts and rules in different\nways than seen during training. The example in the\nintroduction of knowing the meaning of “jump“,\n“jump twice“ and “jax“ and from those inferring\nthe meaning of “jax twice“ is an example of sys-\ntematicity. Productivity, on the other hand, is the\nability to extrapolate to longer sequences than those\nseen during training. For example, consider the ex-\nample of learning how to evaluate mathematical\nexpressions of the form “3 + (4−(5 ∗2))”. An\nexample of productivity would be to extrapolate to\nexpressions with a larger number of parenthesis, or\nwith deeper parenthesis nesting, than seen during\ntraining. Hupkes et al. (2020) identify other forms\nof compositionality, such as substitutivity, localism\nor overgeneralization, but we will mostly focus on\nsystematicity and productivity in this paper.\nCompositional generalization is related to the\ngeneral problem of out-of-distribution generaliza-\ntion. Hence, we can also see it as the problem of\nhow models can discoversymmetries in the domain\n(such as the existence of primitive operations or\nother regularities) that would generalize better to\nout-of-distribution samples than shortcuts (Geirhos\net al., 2020), which would only work on the same\ndistribution of examples seen during training.\nEarly work focused on showing how different\ndeep learning models do not generalize composi-\ntionally (Liška et al., 2018). For example Liška\net al. (2018) showed that while models like LSTMs\nare able to generalize compositionally, it is un-\nlikely that gradient descent converges to a solution\nthat does so (only about 2% out of 50000 train-\ning runs achieved a generalization accuracy higher\nthan 80% in a compositional task, while they had\nalmost perfect performance in training). Datasets\nlike SCAN (Lake and Baroni, 2018), PCFG (Hup-\nkes et al., 2020), Arithmetic language (Veldhoen\net al., 2016), or CFQ (Keysers et al., 2019) were\nproposed to show these effects.\nWork toward improving compositional gen-\neralization includes ideas like Syntactic at-\ntention (Russin et al., 2019), increased pre-\ntraining (Furrer et al., 2020), data augmenta-\ntion (Andreas, 2019), intermediate representa-\ntions (Herzig et al., 2021) or structure annota-\ntions (Kim et al., 2021). Specialized architectures\nthat achieve good performance in speciﬁc composi-\ntional generalization tasks also exist. For example,\nLiu et al. (2020) propose a model made up of a\n“composer” and a “solver”, achieving perfect per-\nformance on SCAN. The most related concurrent\nwork to ours is that of Csordás et al. (2021), who\nalso showed gains in compositional generalization\nvia relative attention. Additionally, in their work,\nthey show that a key problem in some tasks is the\nend of sequence detection problem (when to stop\nproducing output). Finally, they show that general-\nization accuracy keeps growing even when training\naccuracy maxes out, questioning early stopping\napproaches in compositional generalization. We\nnote that training for longer might also improve our\nresults, which we will explore in the future.\n2.2 Transformer Models\nModels based on Transformers (Vaswani et al.,\n2017), such as BERT (Devlin et al., 2018), or vari-\nants (Yang et al., 2019; Lan et al., 2019; Raffel\net al., 2019) yield state-of-the-art results in many\nNLP tasks such as language modeling (Child et al.,\n2019; Sukhbaatar et al., 2019; Rae et al., 2019;\nKitaev et al., 2020), question answering (Ainslie\net al., 2020; Lan et al., 2019; Zaheer et al., 2020;\nBeltagy et al., 2020), and summarization (Zhang\net al., 2019). However, existing studies show that\nthey do not have good compositional generaliza-\ntion. In this paper we will consider the original\nTransformer architecture and expand upon it.\nThe standard Transformer model consists of two\nmain components (see the center of Figure 2): an\nencoder and a decoder, each of which consists of\na series of layers. Each layer contains an attention\nsublayer followed by a feed-forward sublayer (the\ndecoder has two attention sublayers for decoder-\nto-decoder and decoder-to-encoder attention). The\ninput of a Transformer is a sequence of token em-\nbeddings, and the output is a sequence of tokens\n3592\nInput:    # # # 3 6 7 [SEP] # # 1 4 9 1 [END]\nOutput: # # 1 8 5 8 [END]  \nAddition:\nInput:    # # - 3 6 7 [SEP] # # 1 4 9 1 [END]\nOutput: # # 1 1 2 4 [END]  \nAdditionNegatives:\nInput:    1 3 3 7 2 [END] \nOutput: 2 7 3 3 1 [END] \nReverse:\nInput:    1 3 5 7 2 [END] \nOutput: 1 3 5 7 2 1 3 5 7 2 [END] \nDuplication:\nInput:     1 2 3 [SEP] a b [END]\nOutput:  1 a [SEP] 2 a [SEP] 3 a [SEP] \n  1 b [SEP] 2 b [SEP] 3 b [END]\nCartesian:\nInput:    a4 b1 f6 [SEP] f7 a4 c3 [END]\nOutput: true [END]\nIntersection:\nInput:     look around right and walk left twice [END]\nOutput:   I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK \n  I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_LOOK \n  I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK [END]\nSCAN-length / SCAN-add-jump:\nInput:     swap_first_last copy remove_second E18 E15 \n  Q6 , P15 L18 X10 I15 Y14 [END]\nOutput:  Q6 E15 E18 [END]\nPCFG-productivity / PCFG-systematicity\nInput:     A rose was helped by a dog . [END]\nOutput:  rose ( x _ 1 ) AND help . theme ( x _ 3 , x _ 1 )\n  AND help . agent ( x _ 3 , x _ 6 ) \n  AND dog ( x _ 6 ) [END]\nCOGS\nInput:     Did a person marry a cinematographer ,\n  influence M1 , and influence M2 [END]\nOutput:  SELECT count(*) WHERE { \n  ?x0 a ns:people.person . \n  ?x0 ns:influence.influence_node.influenced M1 .\n  ?x0 ns:influence.influence_node.influenced M2 .\n  ?x0 ns:people.person.spouse_s ?x1 . \n  ?x1 a ns:film.cinematographer . \n  FILTER ( ?x0 != ?x1 ) } [END]\nCFQ\nFigure 1: Examples from the different datasets used in our experiments.\ngenerated one at a time by predicting based on the\noutput distribution generated by the decoder. To\nprovide a notion of token “order” a set ofposition\nencodings are typically added to the embedding of\neach input token to indicate sequence order.\nWe will use l to denote the number of en-\ncoder/decoder layers, d for the dimensionality of\ntoken embeddings, f for the intermediate dimen-\nsionality used by the feed-forward sublayer, and h\nfor the number of attention-heads in the attention\nsublayers. The original Transformer model used\nl = 6, d = 512, f = 2048and h = 8, as their base\nconﬁguration. In this paper, we use parameters\nmuch smaller than that, as we are evaluating the\narchitectural decisions on relatively small datasets.\n3 Evaluation Datasets\nWe use a collection of 12 datasets that require dif-\nferent types of compositional generalization. Six\nof those dataset consist of “algorithmic” tasks\n(addition, reversing lists, etc.), and six of them\nare standard datasets used to evaluate composi-\ntional generalization (most involving natural lan-\nguage). We note that our algorithmic tasks mostly\nrequire productivity-style compositional generaliza-\ntion, while other datasets also require systematicity\nor synonimity (Hupkes et al., 2020). Speciﬁcally,\nwe used the following datasets (see Appendix E for\ndetails, and Figure 1 for examples):\nAddition (Add): A synthetic addition task,\nwhere the input contains the digits of two integers,\nand the output should be the digits of their sum.\nThe training set contains numbers with up to 8 dig-\nits, and the test set contains numbers with 9 or 10\ndigits. Numbers are padded to reach a length of 12.\nAdditionNegatives (AddNeg): The same as the\nprevious one, but 25% of the numbers are negative\n(preceded with the - symbol).\nReversing (Reverse): Where the output is ex-\npected to be the input sequence in reverse order.\nTraining contains sequences of up to 16 digits, and\nthe test set contains lengths between 17 to 24.\nDuplication (Dup): The input is a sequence of\ndigits and the output should be the same sequence,\nrepeated twice. Training contains sequences up to\n16 digits, and test from 17 to 24.\nCartesian (Cart): The input contains two se-\nquences of symbols, and the output should be their\nCartesian product. Training contains sequences of\nup to 6 symbols (7 or 8 for testing).\nIntersection (Inters): Given two sequences of\nsymbols, the output should be whether they have\na non-empty intersection. Training contains sets\nwith size 1 to 16, and testing 17 to 24.\nSCAN-length (SCAN-l): The length split of the\nSCAN dataset (Lake and Baroni, 2018).\nSCAN-add-jump (SCAN-aj): The add primi-\ntive jump split of the SCAN dataset (Lake and Ba-\nroni, 2018).\nPCFG-productivity (PCFG-p): The productiv-\nity split of the PCFG dataset (Hupkes et al., 2020)\nPCFG-sytematicity (PCFG-s: The systematic-\nity split of the PCFG dataset (Hupkes et al., 2020).\nCOGS: The generalization split of the COGS\nsemantic parsing dataset (Kim and Linzen, 2020).\nCFQ-mcd1 (CFQ): The MCD1 split of the CFQ\ndataset (Keysers et al., 2019).\nNote that most of these datasets are trivial if\n3593\nEnc2Enc\nAttention\nFeed \nForward\nInput \nEmbedding\n+\nAdd & Norm\nAbsolute \nPosition \nEncodings\nInputs\nDec2Dec\nAttention\nDec2Enc \nAttention\nAdd & Norm\nx l\nOutput \nEmbedding\nOutputs\n+\nFeed \nForward\nAdd & NormAdd & Norm\nLinear + \nSoftmax\nAdd & Norm\nOutput Probabilities\n(without copy decoder)\nLinear + \nReLu\nLinear\nd\nf\nd\nV K Q\n+\n✕\nScale\nMask+\nRelative \nPosition \nEncodings\nSoftmax\n✕\nbias\nembedding\n+\nIntermediate representation \nto final output\nx l\nMulti-head \nAttention\nx h\nFeed \nForward\nEncoder\nDecoder\n5\n3\n1\n4\nCopy Decoder \nAttention\nSoftmax\nQcone-hot\nLinear + \nSigmoid\n✕\nCopy Decoder2 ✕\np1\np2\nOutput Probabilities\n(with copy decoder)\n1-w\nw\nFigure 2: An illustration of a Transformer, extended with the additional components necessary to explore the\ndifferent dimensions we experiment with in this paper: (1) position encodings, (2) copy decoder, (3) model size\n(l, d, f, h), (4) weight sharing, and (5) intermediate representations.\nthe training and test sets come from the same dis-\ntribution, and most Transformer models achieve\nnear 100% accuracy (except a few hard tasks like\nthe Cartesian product or set intersection). Hence,\nsplitting train and test data in a way that requires\ncompositional generalization is key (e.g., having\nexamples with larger sequences in the test set than\nin the training set). We want to make sure models\ndo not just learn shortcuts (Geirhos et al., 2020)\nthat do not generalize to out-of-distribution data.\n4 Empirical Results\nIn this section we present an evaluation of the com-\npositional generalization abilities of Transformers\nwith different architectural conﬁgurations. Specif-\nically we evaluated: (1) the type of position en-\ncodings, (2) the use of copy decoders, (3) model\nsize, (4) weight sharing, and (5) the use of inter-\nmediate representations for prediction (see Figure\n2). For this systematic experimentation, we used\nsmall Transformer models, without pre-training (all\nmodels are trained from scratch). Even if previous\nwork has reported beneﬁts of pre-training in some\ncompositional tasks (e.g., in CFQ (Furrer et al.,\n2020)), we aim at disentangling the effects of each\narchitecture decision in and of itself, in the search\nfor compositional inductive biases.\nOur results show that, while these decisions do\nnot affect certain types of compositional general-\nization tasks, we see signiﬁcant gains in others.\nWe report the average of at least 3 training runs\n(for algorithmic tasks, we use at least 5 train-\ning runs, and 10 for set intersection since they\nhave a higher variance; see Appendix B). We use\nsequence-level accuracy as the evaluation metric:\nan output sequence with even just a single wrong\ntoken is considered wrong.\n4.1 Position Encodings\nWhile the original Transformer model (Vaswani\net al., 2017) and BERT (Devlin et al., 2018) used\nabsolute position encodings, later models such as\nT5 (Raffel et al., 2019) or ETC (Ainslie et al., 2020)\nuse relative position encodings (Shaw et al., 2018).\nRelative position encodings assign a label to each\npair of tokens in the input (typically representing\ntheir relative distance in the input, up to a maxi-\nmum radius). So, there is a label used for tokens\nattending to a token “two positions to the right”,\netc. One interesting thing about relative position\nencodings is that they are position invariant, i.e.\ntwo tokens that are k positions apart will attend to\neach other in the same way, regardless of where\nthey are in the sequence, and hence allowing mod-\nels to capture furthersymmetries in the domain. We\ncompare the following position encodings:\nabs: sinusoidal absolute position encodings (as\nused in the original Transformer)2.\n2We did not experiment with learnable absolute position\nencodings, as test examples are longer than anything seen\nduring training, hence containing untrained embeddings.\n3594\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQAvg.\nabs 0.005 0.042 0.000 0.000 0.000 0.500 0.000 0.003 0.174 0.434 0.177 0.304 0.137\nrel-e 0.004 0.018 0.422 0.486 0.004 0.501 0.064 0.003 0.238 0.451 0.170 0.322 0.224\nrel-b 0.002 0.005 0.277 0.362 0.054 0.501 0.049 0.007 0.042 0.102 0.126 0.276 0.150\nrel-eb 0.003 0.011 0.486 0.444 0.000 0.500 0.089 0.011 0.257 0.452 0.249 0.290 0.233\nrel2-e 0.988 0.830 0.787 0.010 0.000 0.501 0.032 0.007 0.159 0.353 0.259 0.322 0.354\nrel2-b 0.140 0.708 0.056 0.253 0.000 0.504 0.080 0.002 0.041 0.117 0.138 0.319 0.197\nrel2-eb0.978 0.779 0.737 0.017 0.000 0.504 0.091 0.010 0.194 0.374 0.159 0.311 0.346\nTable 1: Sequence-level accuracy for different position encoding methods. Bolded results represent the best results\nfor each dataset in this table.\nrel-e: relative position encodings, where the rel-\native position label deﬁnes a learnable embedding\nthat is added to the key during the attention process.\nWe used a maximum local attention radius of 16,\nwhich means that we have the following relative po-\nsition labels {l−16, l−15, ..., l−1, l0, l1, ..., l15, l16}.\nTokens that are further than 16 positions apart get\nthe l−16 or l16 labels.\nrel-b: relative positions deﬁne a learnable bias\nthat is added to the attention weight of each atten-\ntion pair. This is the attention mechanism used by\nT5 (although they use a logarithmic scheme for\nrepresenting relative positions).\nrel-eb: relative position using both a learnable\nembedding vector and a learnable bias scalar.\nWhile relative positions are straightforward for\nencoder-to-encoder and decoder-to-decoder atten-\ntion, it is unclear what the relative positions should\nbe for decoder-to-encoder. Hence, we tested three\nalternatives (rel2-e, rel2-b and rel2-eb in our result\ntables). rel-* methods do not use relative position\nlabels in decoder to encoder attention, while those\nnamed rel2-* do (where token yi in the decoder\nattending to token xj in the encoder will have label\nlj−i.\nTable 1 shows sequence-level classiﬁcation ac-\ncuracy for small Transformers (l = 2, d = 64,\nf = 256, h = 4). The right-most column shows\nthe average accuracy across all datasets, and we can\nsee that position encodings play a very signiﬁcant\nrole in the performance of the models. Going from\n0.137 accuracy of the model with absolute position\nencodings up to 0.354 for a model with relative\nposition encodings using embeddings (but no bias\nterm), as well as relative positions for decoder-to-\nencoder attention. In general almost any type of\nrelative position encodings help, but using embed-\ndings helps more than using bias terms. Moreover,\nposition encodings play a bigger role in algorith-\nmic tasks. For example, in the Add and AddNeg\ntasks, models go from 0.005 and 0.042 accuracy to\nalmost perfect accuracy (0.988 and 0.830 for the\nrel2-e model). Moreover tasks like SCAN or CFQ\ndo not seem to be affected by position encodings,\nand using relative position encodings with only a\nbias term hurts in PCFG.\n4.2 Decoder Type\nMany tasks (such as the duplication or PCFG\ndatasets used in our experiments) require models\nable to learn things like “output whatever is in po-\nsition k of the input”, rather than having to learn\nhard-coded rules for outputting the right token, de-\npending on the input, a type of symmetry that can\nbe captured with a copy decoder.\nThe copy decoder in our experiments is fairly\nsimple, and works as follows (Figure 2, top-left).\nIt assumes that the input and output vocabularies\nare the same (we use the union of input and output\nvocabularies in our experiments). For a given token\nxi in the output (with ﬁnal embedding yi), in addi-\ntion to the output probability distribution p1 over\nthe tokens in the vocabulary, the copy decoder pro-\nduces a second distribution p2, which is then mixed\nwith p1 via a weight w. p2 is obtained by attending\nto the output of the last encoder layer (the attention\nquery is calculated using a learnable weight matrix\nfrom yi, the embeddings of the last encoder layer\nare used as the keys, and the values are a one-hot\nrepresentation of the input tokens). The result is\npassed through a softmax layer, resulting in p2.\nTable 2 shows sequence-level classiﬁcation ac-\ncuracy for models with and without a copy decoder.\nAs can be seen in the last column (Avg.), having a\ncopy decoder consistently helps performance, with\nall models using a copy decoder ( abs-c, rel-eb-\nc and rel2-eb-c) outperforming their counterparts\nwithout a copy decoder. Moreover, we see that the\ncopy decoder helps the most in PCFG and COGS,\nwhile it does not seem to help in some other tasks.\nMoreover, we would like to point out that there\nare other ways to set up copy decoders. For exam-\nple Akyürek et al. (2021) propose deﬁning a lexical\n3595\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQAvg.\nabs 0.005 0.042 0.000 0.000 0.000 0.500 0.000 0.003 0.174 0.434 0.177 0.304 0.137\nrel-eb 0.003 0.011 0.486 0.444 0.000 0.500 0.089 0.011 0.257 0.452 0.249 0.290 0.233\nrel2-eb 0.978 0.779 0.737 0.017 0.000 0.504 0.091 0.010 0.194 0.374 0.159 0.311 0.346\nabs-c 0.006 0.021 0.000 0.000 0.000 0.501 0.000 0.003 0.230 0.390 0.520 0.301 0.164\nrel-eb-c 0.004 0.007 0.271 0.460 0.000 0.413 0.026 0.009 0.342 0.541 0.474 0.311 0.238\nrel2-eb-c0.977 0.791 0.540 0.283 0.000 0.528 0.043 0.010 0.336 0.527 0.511 0.295 0.403\nTable 2: Sequence-level accuracy with and without copy decoding (models with a copy decoder are marked with a\n“-c” sufﬁx). Bolded numbers are the best results for each dataset in this table.\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQAvg.\nsmall-20.977 0.791 0.540 0.283 0.000 0.528 0.043 0.010 0.336 0.527 0.511 0.295 0.403\nsmall-40.986 0.835 0.676 0.572 0.000 0.500 0.170 0.000 0.499 0.711 0.501 0.301 0.479\nsmall-60.992 0.835 0.225 0.000 0.000 0.203 0.164 0.002 0.548 0.741 0.476 0.312 0.375\nlarge-2 0.983 0.811 0.605 0.503 0.000 0.500 0.184 0.001 0.535 0.758 0.498 0.269 0.471\nlarge-4 0.957 0.786 0.684 0.523 0.000 0.400 0.164 0.004 0.513 0.770 0.462 0.310 0.464\nlarge-6 0.978 0.673 0.423 0.288 0.000 0.250 0.144 0.000 0.530 0.750 0.451 0.288 0.398\nTable 3: Sequence-level accuracy for models of different sizes. All models are variations of therel2-eb-c model in\nTable 2 (small-2 is equivalent to rel2-eb-c). Bolded results represent the best results for each dataset in this table.\ntranslation layer in the copy decoder, which allows\nmodels to translate tokens in the input to tokens in\nthe output (which is useful in tasks such as SCAN,\nwhich have disjoint vocabularies). In their work,\nthey propose to initialize this layer via a lexicon\nlearning task.\n4.3 Model Size\nNext, we compare the effect of varying both the\nnumber of layers ( l), as well as their size ( d, f,\nh). Speciﬁcally, we tested models with number\nof layers l equal to 2, 4 and 6, and layers of two\nsizes: small (d = 64, f = 256, h = 4), and large\n(d = 128, f = 512, h = 8). We denote these\nmodels small-2, small-4, small-6, large-2, large-\n4, and large-6. All of the models in this section\nare variants of rel2-eb-c, our previous best (see\nAppendix C for parameter counts of our models).\nTable 3 shows the sequence-level classiﬁcation\naccuracy, showing a few interesting facts. First,\nin most algorithmic tasks, size does not help. Our\nhypothesis is that the logic required to learn these\ntasks does not require too many parameters, and\nlarge models probably overﬁt (e.g., like in Du-\nplication)3. Some datasets, however, do beneﬁt\nfrom size. For example, most large models outper-\nform their respective small ones in both variants of\nPCFG. These results are not unexpected, as most\ncompositional generalization datasets contain ide-\nalized examples, often generated via some form of\n3Further investigation showed that lowering the learning\nrate improves performance in the larger models, preventing the\nphenomenon seen in the Duplication dataset. Systematically\nexploring this is left for future work.\ngrammar, and have very small vocabularies (see\nTable 7). Hence, models might not beneﬁt from\nsize as much as on complex natural language tasks.\n4.4 Weight Sharing\nIn this section we evaluate the effect of sharing\nweights across transformer layers. When weight\nsharing is activated, all learnable weights from all\nlayers in the encoder are shared across layers, and\nthe same is true across the layers of the decoder.\nTable 4 shows the resulting performance of the\nmodels (to be compared with Table 3). Surpris-\ningly, weight sharing signiﬁcantly boosts compo-\nsitional generalization accuracy, and almost all\nmodels achieve a higher average accuracy across\nall datasets than their equivalent models in Ta-\nble 3. In particular, datasets such as AdditionNeg-\natives see a signiﬁcant boost, with several mod-\nels achieving higher than 0.9 accuracy (0.982 for\nlarge-6s). PCFG also signiﬁcantly beneﬁts from\nweight sharing, with the large-6s model achieving\n0.634 and 0.828 in the productivity and systematic-\nity versions, respectively. These are higher than\npreviously reported results in the literature (using\nthe original Transformer, which is a much larger\nmodel): 0.50 and 0.72 (Hupkes et al., 2020). No-\ntice, moreover that achieving good results in PCFG\n(or SCAN) is easy with specialized models. The\nimportant achievement is doing so with general\npurpose models. Our hypothesis is that a model\nwith shared weights across layers might have a\nmore suited inductive bias to learn primitive opera-\ntions that are applied repeatedly to the input of the\ntransformer (copying, reversing, duplicating, etc.).\n3596\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQAvg.\nsmall-2s0.992 0.809 0.780 0.750 0.000 0.699 0.022 0.003 0.313 0.501 0.450 0.303 0.468\nsmall-4s0.991 0.955 0.708 0.580 0.000 0.500 0.172 0.017 0.534 0.723 0.445 0.292 0.493\nsmall-6s0.993 0.933 0.505 0.000 0.000 0.500 0.186 0.000 0.562 0.780 0.454 0.295 0.434\nlarge-2s0.997 0.894 0.831 0.848 0.000 0.584 0.033 0.002 0.511 0.638 0.465 0.292 0.508\nlarge-4s0.991 0.915 0.771 0.882 0.000 0.400 0.186 0.002 0.589 0.791 0.475 0.327 0.527\nlarge-6s0.985 0.982 0.241 0.000 0.000 0.500 0.196 0.000 0.634 0.828 0.454 0.303 0.427\nTable 4: Sequence-level accuracy for all the models in Table 3, but sharing weights across layers.\n4.5 Intermediate Representations\nThe key idea of an intermediate representation is\nto deﬁne a different representation of the target out-\nput that is easier to generate by the model, but that\ncan be easily mapped to the desired output. Herzig\net al. (2021) recently showed very promising re-\nsults using this technique in several tasks. Deﬁning\nuseful intermediate representations is task-speciﬁc\nand not trivial. Thus we experimented with it in\nonly two datasets: COGS and CFQ (Figure 3).\n4.5.1 Intermediate Representation for COGS\nOur intermediate representation for COGs turns\nthe task from seq2seq into a sequence tagging task.\nWe ask the model to produce 5 tags for each input\ntoken: a parent, the role of the relation between the\ntoken and its parent (if applicable), the category,\nthe noun determiner (for nouns) and the verb name\n(for verbs). With these tags, the original output can\nbe constructed deterministically. One of the main\nadvantages of this is that the model is naturally\npushed to produce outputs with the correct length\neven for longer inputs (improving productivity).\nFor the sequence tagging formulation, we used\nonly the encoder part of the Transformer and added\nﬁve prediction heads, to predict each tag. For role,\ncategory, noun determiner and verb name, we sim-\nply had a dense layer with a Sigmoid activation\nfunction. For the parent tag, we experimented with\n3 different head types: Absolute used a dense layer\nwith a Sigmoid activation to predict the absolute\nindex of the parent in the input sequence ( -1 for\nno parent). Relative predicted the relative offset of\nthe parent token with respect to the current token,\nor self for no parent. Finally, Attention used the\nattention weights from a new attention layer with 1\nhead to predict the parent.\nTable 5 shows the experimental results com-\nparing a few conﬁgurations of this new tagging\napproach to a few conﬁgurations of the seq2seq\napproach (see Appendix D for all other conﬁgu-\nrations). Examples in the structural generaliza-\ntion tasks are typically longer than in the train-\nseq2seq taggingModel abs rel2-eb-cabs rel-ebSize small-2 small-6ssmall-2 small-2sParent encoding absolute attention\nLexical Generalization: Primitives and Grammatical RolesSubject→Object (common noun)0.309 0.899 0.911 0.969Subject→Object (proper noun)0.098 0.429 0.630 0.826Object→Subject (common noun)0.790 0.936 0.982 0.978Object→Subject (proper noun)0.207 0.951 0.993 0.995Prim noun→Subject (common noun)0.240 0.913 0.993 0.988Prim noun→Subject (proper noun)0.019 0.772 0.974 0.996Prim noun→Object (common noun)0.017 0.902 0.950 0.953Prim noun→Object (proper noun)0.000 0.513 0.651 0.700Prim verb→Inﬁnitival argument0.000 0.766 0.000 0.001\nLexical Generalization: Verb Argument Structure AlternationActive→Passive 0.604 0.000 0.697 0.948Passive→Active 0.196 0.001 0.535 0.897Object-omitted transitive→Transitive0.275 0.003 0.527 0.926Unaccusative→Transitive 0.069 0.003 0.528 0.787Double object dative→PP dative 0.819 0.000 0.590 0.958PP dative→Double object dative0.404 0.004 0.771 0.850\nLexical Generalization: Verb ClassAgent NP→Unaccusative Subject0.399 0.951 0.784 1.000Theme NP→Obj-omitted trans Subj0.688 0.965 0.791 0.701Theme NP→Unergative subject0.694 0.966 0.930 0.771\nStructural Generalization: Phrases and Grammatical RolesObj-mod PP→Subj-mod PP 0.000 0.000 0.000 0.299\nStructural Generalization: Deeper RecursionDepth generalization: PP modiﬁers0.003 0.000 0.138 0.681Depth generalization: Sentential comp0.000 0.000 0.000 0.233\nOverall 0.278 0.475 0.637 0.784\nTable 5: Sequence-level accuracy in different general-\nization subsets in COGS for both seq2seq and sequence\ntagging models. PP stands for prepositional phrase.\ning set and require productivity. All the models\ntested in the original COGS paper (Kim and Linzen,\n2020) (and all of our seq2seq approaches above)\nachieved 0 accuracy in this category. The small-6s\nseq2seq model improves the overall performance\nfrom 0.278 to 0.475, but curiously has near 0 per-\nformance on Verb Argument Structure Alternation\ntasks, worse than the base abs model.\nThe intermediate representation based on tag-\nging works much better. The base abs tagging\nmodel manages to get non-zero performance on\none structural generalization task, which suggests\nthat enforcing the right output length helps. Finally,\nwhen predicting the parent directly from attention\nweights, the structural generalization tasks score\n0.2-0.7, compared to our previous near 0 scores\n(see Appendix D for common types of errors).\nOverall, the sequence tagging intermediate rep-\nresentation achieves a much higher accuracy, with\n3597\nCOGS\nInput:           Did a person marry a cinematographer ,\ninfluence M1 , and influence M2 [END]\nIntermediate Output:          \nSELECT count(*) WHERE { \n?x0 a ns:people.person . \n?x0 ns:influence.influence_node.influenced {M1,M2} .\n?x0 ns:people.person.spouse_s ?x1 . \n?x1 a ns:film.cinematographer . \nFILTER ( ?x0 != ?x1 ) } [END]\nCFQ\n...\n?x0 ns:influence.influence_node.influenced M1 .\n?x0 ns:influence.influence_node.influenced M2 .\n...\nA  rose   was  helped  by  a   dog   .  [END]\n-    3     -     -     -   -    3    -    -\n-  theme   -     -     -   -  agent  -    -\n-  CNOUN   -   VERB    -   -  CNOUN  -    -\n-  INDEF   -     -     -   -  INDEF  -    -\n-    -     -   help    -   -    -    -    -\nInput:\nIntermediate Output:\nParent:\nRole:\nCategory:\nNoun determiner:\nVerb name:\nrose ( x _ 1 ) \nAND help . theme ( x _ 3 , x _ 1 )\nAND help . agent ( x _ 3 , x _ 6 ) \nAND dog ( x _ 6 ) [END]\nFinal \nOutput\nFinal \nOutput\nFigure 3: Examples from the intermediate representations for COGs and CFQ. For COGs, we framed the task as\nsequence tagging and made the model predict 5 tags for each token; for CFQ we compressed Cartesian products.\nCFQ CFQ-im\nabs 0.304 0.541\nrel-eb 0.290 0.555\nrel2-eb 0.311 0.541\nrel-eb-c 0.311 0.541\nrel2-eb-c 0.295 0.519\nlarge-4 0.310 0.541\nlarge-4s 0.327 0.474\nTable 6: Sequence-level accuracy for different models\nfor the original CFQ, and for CFQ with intermediate\nrepresentations (CFQ-im). The top 5 models are small\nmodels with 2 layers, and the last four models are vari-\nants of rel2-eb-c (used in Tables 3 and 4).\none model reaching 0.784, higher than any previ-\nously reported performance in COGS in the litera-\nture, to the best of our knowledge. This suggests\nthat the encoder has the power to parse the input\ncorrectly, but maybe the decoder is not capable of\ngenerating the correct output sequence from the\nencoder in the full transformer.\n4.5.2 Intermediate Representation for CFQ\nOne of the difﬁculties in the CFQ dataset is that\nmodels need to learn to perform Cartesian prod-\nucts (e.g., for questions like “who directed and\nacted in M1 and M2?”, the model needs to expand\nto “directed M1”, “directed M2”, “acted in M1”\nand “acted in M2”). However, as shown in our\nexperiments above, this is a very hard task to learn.\nHence, we followed the same idea as in Herzig\net al. (2021), and deﬁned an intermediate repre-\nsentation that removes the need to learn Cartesian\nproducts by allowing triples of the form (entity list)\n- (relation list) - (entity list).\nTable 6 shows the sequence-level classiﬁcation\naccuracy for models on CFQ and on the version\nwith intermediate representations (CFQ-im). While\nthe different variations on Transformer models\nhave little affect on the performance, the use of an\nintermediate representation signiﬁcantly improves\nperformance, going from around 0.3 accuracy for\nmost Transformer models to over 0.5, and up to\n0.555 for the rel-eb model. This is consistent with\nthe results reported by Herzig et al. (2021).\n5 Discussion\nAn overall trend is that algorithmic tasks seem to\nbe greatly affected by the different architecture de-\nsign decisions we explored. In all datasets, except\nfor Cartesian product, there is at least one combina-\ntion in our experiments that achieved high perfor-\nmance (close to 0.8 accuracy or higher). Cartesian\nproducts remain an open challenge for future work,\nwhere one of the big obstacles is learning to pro-\nduce much longer outputs than seen during training\n(output is quadratic with respect to input size).\nThere are some datasets, such asSCAN-aj, where\nwe did not see large improvements in performance.\nThe main obstacle is learning to handle a symbol\n(“jump”) having seen it very few times (or even just\nonce) during training (this also happens in some\ntypes of generalization in COGS). None of the vari-\nations we experimented with were enough to han-\ndle this type of compositionality either.\nIn conclusion, we observed:\n1. relative position encodings (when both em-\nbeddings and biases are used) seem to never\nbe detrimental (they either provided gains, or\ndid not affect). Results indicate this signif-\nicantly helps in productivity. Moreover, for\ntasks where positional information is impor-\ntant (such as addition, or reversing), adding\npositional encodings to decoder2encoder at-\ntention provided signiﬁcant beneﬁts. Finally,\nas Table 1 shows, for relative position embed-\ndings to be beneﬁcial, using embeddings was\nnecessary; only using relative position biases\nwas not enough.\n2. Adding a copy decoder was generally beneﬁ-\ncial. We saw some occasional degradation in\n3598\nsome tasks (e.g., Reverse), but these are high\nvariance tasks (see Table 10 in the Appendix),\nwhere results are more uncertain.\n3. Model size in terms of embedding dimensions,\nhelped generally. Going from 2 to 4 layers\nprovided a slight beneﬁt in general. Our ex-\nperiments show going to 6 layers hurt perfor-\nmance, but as noted earlier, additional (un-\nreported preliminary) experiments indicated\nlarger models might need smaller learning\nrates, with which they also seem to improve\nperformance (systematic exploration of this is\nfuture work).\n4. Weight sharingseems to beneﬁt in tasks where\nthere are a clear set of primitives that have\nto be learned (PCFG in particular), or algo-\nrithmic tasks, but it seems to hurt in COGs.\nHence, weight sharing does not provide gen-\neral beneﬁts as the previous modiﬁcations.\n5. Intermediate representations, although\ndataset-speciﬁc, signiﬁcantly help when they\ncan be deﬁned, as expected.\n6 Conclusions\nThis paper presented an empirical study of the de-\nsign space of Transformer models, evaluated in a\ncollection of benchmarks for compositional gener-\nalization in language and algorithmic tasks. Our\nresults show that, compared to a baseline Trans-\nformer, signiﬁcant gains in compositional general-\nization can be achieved. Speciﬁcally, the baseline\nTransformer achieved an average sequence-level\naccuracy of 0.137, while we showed this can in-\ncrease to up to 0.527 with some design changes.\nAccuracy levels of up to 0.493 can be achieved\nwithout increasing the parameter count of our base-\nline model (see Appendix C for parameter counts).\nMoreover, we achieved state-of-the-art results in\nCOGS (at the time of submission), showing 0.784\naccuracy on the generalization set, and two PCFG\nsplits (0.634 and 0.828 respectively). This shows\nthat a key factor in training models that generalize\ncompositionally is to provide the right inductive\nbiases.\nAs part of our future work, we want to explore\nmore dimensions, such as pre-training and opti-\nmizer parameters, and study the implications of\nour results in compositional generalization in large\nmodels on real world tasks.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284.\nEkin Akyürek and Jacob Andreas. 2021. Lexicon learn-\ning for few-shot neural sequence modeling. arXiv\npreprint arXiv:2106.03993.\nJacob Andreas. 2019. Good-enough composi-\ntional data augmentation. arXiv preprint\narXiv:1904.09545.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150v1.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nRóbert Csordás, Kazuki Irie, and Jürgen Schmidhu-\nber. 2021. The devil is in the detail: Simple tricks\nimprove systematic generalization of transformers.\narXiv preprint arXiv:2108.12284.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDaniel Furrer, Marc van Zee, Nathan Scales, and\nNathanael Schärli. 2020. Compositional generaliza-\ntion in semantic parsing: Pre-training vs. specialized\narchitectures. arXiv preprint arXiv:2007.08970.\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence\nand Statistics, volume 9 of Proceedings of Machine\nLearning Research , pages 249–256, Chia Laguna\nResort, Sardinia, Italy. PMLR.\nJonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin\nGuu, Panupong Pasupat, and Yuan Zhang. 2021. Un-\nlocking compositional generalization in pre-trained\nmodels using intermediate representations. arXiv\npreprint arXiv:2104.07478.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and\nElia Bruni. 2020. Compositionality decomposed:\nHow do neural networks generalise? Journal of Ar-\ntiﬁcial Intelligence Research, 67:757–795.\n3599\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStaﬁniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. In International Conference on\nLearning Representations.\nJuyong Kim, Pradeep Ravikumar, Joshua Ainslie, and\nSantiago Ontañón. 2021. Improving compositional\ngeneralization in classiﬁcation tasks via structure an-\nnotations. arXiv preprint arXiv:2106.10434.\nNajoung Kim and Tal Linzen. 2020. Cogs: A compo-\nsitional generalization challenge based on semantic\ninterpretation. arXiv preprint arXiv:2010.05465.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational Conference on Machine Learning, pages\n2873–2882. PMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nAdam Liška, Germán Kruszewski, and Marco Baroni.\n2018. Memorize or generalize? searching for a\ncompositional rnn in a haystack. arXiv preprint\narXiv:1802.06467.\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen,\nZeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and\nDongmei Zhang. 2020. Compositional generaliza-\ntion by learning analytical expressions.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nand Timothy P Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling. arXiv\npreprint arXiv:1911.05507.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nJake Russin, Jason Jo, Randall C O’Reilly, and Yoshua\nBengio. 2019. Compositional generalization in a\ndeep seq2seq model by separating syntax and seman-\ntics. arXiv preprint arXiv:1904.09708.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive\nattention span in transformers. arXiv preprint\narXiv:1905.07799.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nSara Veldhoen, Dieuwke Hupkes, Willem H Zuidema,\net al. 2016. Diagnostic classiﬁers revealing how\nneural networks process hierarchical structure. In\nCoCo@ NIPS, pages 69–77.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hi-\nbert: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. arXiv preprint arXiv:1905.06566.\n3600\nA Implementation Details\nWe used a standard Transformer implementation4,\nand added all the proposed variations on top of\nit. All experiments were run on machines with\na single CPU and a single Tesla V100 GPU. All\nparameters were left to their default values from the\noriginal implementation, including the learning rate\nschedule (which could probably be further tweaked\nif state-of-the-art results are sought), as we were\njust aiming to compare inductive biases, rather than\naim for SOTA results.\nAdditionally, we would like to highlight some\nimplementation details, which surprisingly had\nlarge effects on our experimental results. Layer\nnormalization operations in our Transformer imple-\nmentation were done after each sublayer (attention\nand feed forward). Embedding layers were initial-\nized with the Keras default “uniform” Keras ini-\ntializer (uniform random distribution in the range\n[−0.05, 0.05]). Dense layers were initialized also\nwith the Keras default Glorot initializer (uniform\nrandom distribution with mean 0 and standard\ndeviation\n√\n2/(fan_in + fan_out)) (Glorot and\nBengio, 2010). While these details might not\nseem that important, we were unable to repro-\nduce some of the results reported above using a\nre-implementation of the Transformer model in\nFlax, which used different defaults (and layer nor-\nmalization before each sublayer rather than after)\nunless we changed these implementation details\nto match those of the Keras implementation. This\nindicates that these low-level details also have an\neffect on the learning bias of the models, with an\nimpact in compositional generalization, which we\nplan to study in the future.\nB Detailed Results\nTable 8 shows the average sequence-level accuracy\nfor all the models evaluated in this paper, all in one\ntable. We used the same names as used in the paper\n(as models rel2-eb-c and small-2 both refer to the\nsame model, we included the row twice, with both\nnames, for clarity).\nTable 9 shows the maximum accuracy each\nmodel achieved in each dataset out of the 3 to\n10 repetitions we did for each dataset. Recall we\nused 3 repetitions for SCAN-l, SCAN-aj, PCFG-p,\nPCFG-s, COGS and CFQ, 5 repetitions for Add,\nAddNeg, Reverse, Dup and Cart, and 10 repetitions\n4https://www.tensorflow.org/tutorials/\ntext/transformer\nfor Inters (as it was the dataset where we saw more\nextreme results). An interesting phenomenon ob-\nserved in the Inters dataset is that models tend to\nachieve either random accuracy (around 0.5), or\nperfect accuracy (1.0). Very rarely models achieve\nintermediate values. This support the needle-in-a-\nhaystack argument of Liška et al. (2018), who saw\nthat while LSTMs have the capability of general-\nize compositionally, what happens in practice is\nthat gradient descent has a very low probability of\nconverging to weights that do so (ﬁnding the “com-\npositional needle” in a haystack). We observed a\nsimilar thing in our experiments, but saw that some\nTransformer architectures resulted in an increased\nchance of ﬁnding this needle.\nTable 10 shows the standard deviation in the\nsequence-level accuracy we observed in our ex-\nperiments. As can be seen, the algorithmic tasks\nresult in a much larger standard deviation. In some\ndatasets (e.g., Add and Inters) it was common for\nmorels to either achieve near 0% accuracy (50% in\nInters) or near 100% accuracy, but few values in\nbetween.\nC Parameter Counts\nTable 11 shows the parameter count for all the mod-\nels used in this paper, notice that exact parameter\ncounts vary per dataset, as each dataset has a differ-\nent token vocabulary, and hence both the token em-\nbedding and the output layers vary. One interesting\nresult is that in our experiments, parameter count is\nnot, by itself, sufﬁcient to increase compositional\ngeneralization. Our best model overall (large-4s)\nonly had about 0.5 million parameters, and outper-\nformed signiﬁcantly larger models. Another ex-\nDataset |Train| |Test| |Vocab| Epochs\nAdd 200000 1024 14 2\nAddNeg 200000 1024 16 10\nReverse 200000 1024 14 2\nDup 200000 1024 14 4\nCart 200000 1024 24 4\nInters 200000 1024 106 8\nSCAN-l 16989 3919 25 24\nSCAN-aj 14669 7705 25 24\nPCFG-p 81011 11331 537 20\nPCFG-s 82167 10175 537 20\nCOGS 24155 21000 876 16\nCFQ 95743 11968 184 16\nTable 7: Size of the training/test sets, vocab and train-\ning epochs we used for the different datasets.\n3601\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQAvg.\nabs 0.005 0.042 0.000 0.000 0.000 0.500 0.000 0.003 0.174 0.434 0.177 0.304 0.137\nrel-e 0.004 0.018 0.422 0.486 0.004 0.501 0.064 0.003 0.238 0.451 0.170 0.322 0.224\nrel-b 0.002 0.005 0.277 0.362 0.054 0.501 0.049 0.007 0.042 0.102 0.126 0.276 0.150\nrel-eb 0.003 0.011 0.486 0.444 0.000 0.500 0.089 0.011 0.257 0.452 0.249 0.290 0.233\nrel2-e 0.988 0.830 0.787 0.010 0.000 0.501 0.032 0.007 0.159 0.353 0.259 0.322 0.354\nrel2-b 0.140 0.708 0.056 0.253 0.000 0.504 0.080 0.002 0.041 0.117 0.138 0.319 0.197\nrel2-eb 0.978 0.779 0.737 0.017 0.000 0.504 0.091 0.010 0.194 0.374 0.159 0.311 0.346\nabs-c 0.006 0.021 0.000 0.000 0.000 0.501 0.000 0.003 0.230 0.390 0.520 0.301 0.164\nrel-eb-c 0.004 0.007 0.271 0.460 0.000 0.413 0.026 0.009 0.342 0.541 0.474 0.311 0.238\nrel2-eb-c0.977 0.791 0.540 0.283 0.000 0.528 0.043 0.010 0.336 0.527 0.511 0.295 0.403\nsmall-2 0.977 0.791 0.540 0.283 0.000 0.528 0.043 0.010 0.336 0.527 0.511 0.295 0.403\nsmall-4 0.986 0.835 0.676 0.572 0.000 0.500 0.170 0.000 0.499 0.711 0.501 0.301 0.479\nsmall-6 0.992 0.835 0.225 0.000 0.000 0.203 0.164 0.002 0.548 0.741 0.476 0.312 0.375\nlarge-2 0.983 0.811 0.605 0.503 0.000 0.500 0.184 0.001 0.535 0.758 0.498 0.269 0.471\nlarge-4 0.957 0.786 0.684 0.523 0.000 0.400 0.164 0.004 0.513 0.770 0.462 0.310 0.464\nlarge-6 0.978 0.673 0.423 0.288 0.000 0.250 0.144 0.000 0.530 0.750 0.451 0.288 0.398\nsmall-2s0.992 0.809 0.780 0.750 0.000 0.699 0.022 0.003 0.313 0.501 0.450 0.303 0.468\nsmall-4s 0.991 0.955 0.708 0.580 0.000 0.500 0.172 0.017 0.534 0.723 0.445 0.292 0.493\nsmall-6s0.993 0.933 0.505 0.000 0.000 0.500 0.186 0.000 0.562 0.780 0.454 0.295 0.434\nlarge-2s 0.997 0.894 0.831 0.848 0.000 0.584 0.033 0.002 0.511 0.638 0.465 0.292 0.508\nlarge-4s 0.991 0.915 0.771 0.882 0.000 0.400 0.186 0.002 0.589 0.791 0.475 0.327 0.527\nlarge-6s 0.985 0.982 0.241 0.000 0.000 0.500 0.196 0.000 0.634 0.828 0.454 0.303 0.427\nTable 8: Average sequence-level accuracy for all the models evaluated in this paper.\nample, of this is that the models with shared layer\nparameters outperform their counterparts without\nparameter sharing, although they naturally have\nless parameters.\nD Detailed Results in COGS\nTable 12 shows the results of some of the models\nwe tested in the COGS dataset (including seq2seq\nand sequence tagging models), with the accuracy\nbroken down by the type of example in the gen-\neralization set. The COGS dataset contains four\nsplits: training, dev, test and generalization (gener-\nalization is the one used to measure compositional\ngeneralization, and the set reported in the main pa-\nper). All but one shown conﬁguration achieve more\nthan 95% sequence level accuracy on the test and\ndevelopment splits after training for 16 epochs over\nthe training data. The generalization set is split into\nseveral generalization tasks as described above, to\nbreak down performance by type of generalization\n(overall performance in the generalization set is\nshown in the bottom row).\nThe best tagging model does much better than\nthe base seq2seq model (0.784 vs. 0.278). No-\ntably the tagging model does relatively well on the\nDepth generalization: Prepositional phrase (PP)\nmodiﬁers task achieving accuracy 0.681. When the\ndepth of the model is increased from 2 to 6, the\nscore on this task increases from 0.681 to 0.819, i.e.\nthe model with more layers can parse deeper recur-\nsion. However, increasing the encoder depth at the\nsame time dramatically lowers the performance on\nVerb Argument Structure Alternationtasks.\nSince many of the tasks are solved to near per-\nfect accuracy, here we brieﬂy discuss the types of\nthe remaining errors. The one type of task where\nsequence tagging models did worse than seq2seq\nis Prim verb →Inﬁnitival argument, which mea-\nsures one shot generalization of an example with\nonly a single verb to examples where the verb is\nused in sentences. The cause of this is that the\ntagging example with only a single verb doesn’t ac-\ntually encode the type of relations the verb allows,\nso the tagging model is actually not provided the\nfull information in the only example for this one\nshot learning task. Nevertheless, this category was\nsolved in our seq2seq models with a copy decoder.\nCuriously, some errors, that the tagging model\nwith attention in the parent prediction head makes,\nare quite quite reasonable. For example in the Obj-\nmod PP →Subj-mod PP task, the model often\ngets the complete parsing tree correctly, and the\nonly error is the predicted relation of the subject to\nthe predicate (instead of agent the model predicts\ntheme as is present in all the training examples,\nwhere the prepositional phrase modiﬁes the object).\nAnother task where even the best tagging model\nachieves a low score (0.233) is Depth generaliza-\ntion: Sentential complements. The examples in this\ntask are long complex sentences chained together\nwith the conjunction that. The most common er-\nror here is to predict that the main verb depends\n3602\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQ\nabs 0.008 0.131 0.002 0.000 0.000 0.500 0.000 0.008 0.191 0.462 0.211 0.326\nrel-e 0.010 0.059 0.597 0.908 0.034 0.511 0.115 0.007 0.257 0.496 0.281 0.346\nrel-b 0.004 0.016 0.331 0.417 0.137 0.510 0.072 0.013 0.047 0.112 0.170 0.305\nrel-eb 0.006 0.018 0.658 0.795 0.001 0.502 0.129 0.023 0.268 0.528 0.306 0.333\nrel2-e 1.000 0.943 0.917 0.038 0.000 0.512 0.058 0.018 0.182 0.457 0.332 0.357\nrel2-b 0.256 0.910 0.132 0.339 0.002 0.529 0.116 0.004 0.049 0.137 0.187 0.342\nrel2-eb 1.000 0.875 0.824 0.062 0.000 0.519 0.124 0.018 0.233 0.479 0.205 0.333\nabs-c 0.021 0.037 0.000 0.000 0.000 0.506 0.000 0.005 0.250 0.420 0.550 0.312\nrel-eb-c 0.006 0.027 0.504 0.721 0.000 1.000 0.031 0.021 0.361 0.562 0.581 0.351\nrel2-eb-c0.998 0.842 0.861 0.683 0.000 1.000 0.082 0.014 0.346 0.581 0.576 0.369\nsmall-2 0.998 0.842 0.861 0.683 0.000 1.000 0.082 0.014 0.346 0.581 0.576 0.369\nsmall-4 0.992 0.877 0.939 0.805 0.000 0.500 0.197 0.001 0.509 0.734 0.520 0.342\nsmall-6 1.000 0.922 0.576 0.000 0.000 0.500 0.199 0.007 0.571 0.766 0.516 0.330\nlarge-2 0.998 0.896 0.933 0.882 0.000 0.500 0.197 0.002 0.548 0.762 0.530 0.314\nlarge-4 0.996 0.953 0.848 0.855 0.000 0.500 0.199 0.010 0.523 0.782 0.500 0.360\nlarge-6 0.994 0.887 0.619 0.856 0.000 0.500 0.195 0.000 0.549 0.766 0.483 0.317\nsmall-2s 0.998 0.871 0.979 0.972 0.000 1.000 0.044 0.006 0.328 0.519 0.487 0.348\nsmall-4s 0.998 0.986 0.870 0.871 0.000 0.500 0.175 0.039 0.540 0.742 0.515 0.362\nsmall-6s 1.000 0.984 0.821 0.000 0.000 0.500 0.199 0.000 0.569 0.788 0.486 0.344\nlarge-2s 1.000 0.945 0.952 0.955 0.000 1.000 0.054 0.003 0.526 0.641 0.563 0.304\nlarge-4s 1.000 0.959 0.923 0.959 0.000 0.500 0.195 0.004 0.604 0.810 0.481 0.362\nlarge-6s 1.000 0.998 0.489 0.000 0.000 0.500 0.198 0.000 0.642 0.832 0.469 0.361\nTable 9: Maximum sequence-level accuracy achieved in a given repetition for all the models evaluated in this\npaper.\non another verb far away in the sentence structure,\ninstead of predicting that it has no parent. The dis-\ntance to the incorrectly predicted parent is often\nmore than 16, which was the limit on our relative\nattention offsets. The attention mechanism seems\nto get confused by seeing many more tokens in this\ntest split than during training.\nE Dataset Details\nThis appendix presents more details on the datasets\nused in this paper, as well as on the type of compo-\nsitionality involved in each of them.\n•Addition (Add): This is a synthetic addition\ntask, where the input contains the digits of\ntwo integers, and the output should be the\ndigits of their sum. The training set contains\nnumbers with up to 8 digits, and the test set\ncontains numbers with 9 or 10 digits. Num-\nbers are padded to reach a length of 12 so\nthat it’s easy to align the digits that need to\nbe added. We found that without padding, the\ntask became much harder. Types of compo-\nsitionality: models need to learn that there\nis a primitive operation “adding two digits\n(with carry)” that is repeatedly applied at each\nposition. Models that learn position-speciﬁc\nshortcuts will not generalize to longer input\nlengths (as they would have learned no rules\nto produce the most signiﬁcant digits, which\nwould have never been seen during training).\nThis mostly corresponds to productivity type\nof compositional generalization.\n•AdditionNegatives (AddNeg): The same as\nthe previous one, but 25% of the numbers\nare negative (preceded with the “ -” token).\nTypes of compositionality:the type of com-\npositionality requires by this task is similar to\nthat of the previous task, except that the gen-\neral rules that need to be learned (independent\nof position) are more complex due to negative\nnumbers. So, the model needs to learn three\nbasic primitive operations that are the same\nirrespective of the position of the digits: “add\ntwo digits with carry”, “subtract ﬁrst from sec-\nond with carry”, and “subtract second from\nﬁrst with carry”, and learn when to apply each.\nThis also mostly corresponds to productivity\ntype of compositional generalization.\n•Reversing (Reverse): Where the output is ex-\npected to be the input sequence in reverse or-\nder. Training contains sequences of up to 16\ndigits, and the test set contains lengths be-\ntween 17 to 24. Types of compositionality:\nthe difﬁcult part of this task is to learn to re-\nverse position embeddings in a way that gener-\nalizes to longer inputs than seen during train-\ning, in order to attend and produce the right\n3603\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQ\nabs 0.003 0.047 0.001 0.000 0.000 0.000 0.000 0.004 0.014 0.039 0.067 0.022\nrel-e 0.003 0.017 0.169 0.271 0.012 0.003 0.045 0.004 0.023 0.078 0.103 0.027\nrel-b 0.002 0.006 0.078 0.046 0.073 0.003 0.038 0.006 0.005 0.014 0.040 0.025\nrel-eb 0.002 0.007 0.211 0.287 0.000 0.001 0.038 0.011 0.013 0.066 0.050 0.047\nrel2-e 0.009 0.074 0.167 0.014 0.000 0.004 0.023 0.009 0.016 0.111 0.104 0.035\nrel2-b 0.122 0.202 0.051 0.055 0.001 0.009 0.039 0.002 0.011 0.018 0.045 0.016\nrel2-eb 0.029 0.067 0.057 0.024 0.000 0.007 0.029 0.008 0.047 0.101 0.043 0.020\nabs-c 0.009 0.010 0.000 0.000 0.000 0.003 0.000 0.002 0.024 0.027 0.038 0.013\nrel-eb-c 0.003 0.011 0.135 0.157 0.000 0.322 0.005 0.011 0.017 0.036 0.093 0.025\nrel2-eb-c0.035 0.053 0.208 0.289 0.000 0.239 0.033 0.005 0.009 0.048 0.056 0.063\nsmall-2 0.035 0.053 0.208 0.289 0.000 0.239 0.033 0.005 0.009 0.048 0.056 0.063\nsmall-4 0.004 0.054 0.213 0.184 0.000 0.000 0.046 0.000 0.010 0.019 0.028 0.049\nsmall-6 0.007 0.120 0.233 0.000 0.000 0.256 0.056 0.004 0.024 0.026 0.047 0.022\nlarge-2 0.016 0.074 0.240 0.289 0.000 0.000 0.022 0.001 0.012 0.004 0.042 0.033\nlarge-4 0.075 0.106 0.178 0.190 0.000 0.211 0.049 0.006 0.009 0.010 0.033 0.047\nlarge-6 0.023 0.377 0.119 0.356 0.000 0.264 0.045 0.000 0.018 0.014 0.029 0.022\nsmall-2s 0.007 0.038 0.255 0.254 0.000 0.346 0.021 0.003 0.014 0.019 0.054 0.039\nsmall-4s 0.009 0.055 0.118 0.261 0.000 0.000 0.005 0.020 0.008 0.023 0.068 0.054\nsmall-6s 0.012 0.047 0.208 0.000 0.000 0.001 0.017 0.000 0.006 0.007 0.030 0.041\nlarge-2s 0.004 0.031 0.131 0.167 0.000 0.156 0.027 0.001 0.018 0.004 0.102 0.011\nlarge-4s 0.007 0.039 0.127 0.066 0.000 0.211 0.016 0.002 0.015 0.017 0.009 0.043\nlarge-6s 0.020 0.015 0.159 0.000 0.000 0.000 0.002 0.000 0.008 0.007 0.013 0.037\nTable 10: Standard deviation of the sequence level accuracy results.\noutput sequences. This mostly corresponds to\nproductivity type of compositional generaliza-\ntion, as the model needs to learn to reverse po-\nsition embeddings for longer sequences than\nseen during training.\n•Duplication (Dup): The input is a sequence\nof digits and the output should be the same\nsequence, repeated twice. Training contains\nsequences up to 16 digits, and test from 17 to\n24. Types of compositionality:Learning to\nrepeat the input several times is not a particu-\nlarly hard task for a Transformer, but we no-\nticed that the difﬁcult part was learning when\nto stop producing output (exactly after repeat-\ning the input twice in this case). This problem\nwas also noted in the work of (Csordás et al.,\n2021), and mostly corresponds to productivity\ntype of compositional generalization.\n•Cartesian (Cart): The input contains two se-\nquences of symbols, and the output should\nbe their Cartesian product. Training contains\nsequences of up to 6 symbols (7 or 8 for test-\ning). Types of compositionality: this is a\nvery challenging task that requires very de-\nmanding productivity, as the model needs to\nlearn to learn to compose the basic operation\nof pairing elements from both sets via two\nnested loops: iterating over each of the two\ninput sets.\n•Intersection (Inters): Given two sequences\nof symbols, the output should be whether they\nhave a non-empty intersection. Training con-\ntains sets with size 1 to 16, and testing 17\nto 24. Types of compositionality:the main\nchallenge in this dataset is to learn short-cut\nrules such as “if the ﬁrst set contains a4 and\nthe second set also contains a4 then the out-\nput should be true”. However, the model\nneeds to learn to ignore these token speciﬁc\nrules, and learn the general rule of ﬁnding two\nidentical tokens regardless of which speciﬁc\ntoken they are, which could be seen as a form\nof systematicity. Moreover, this needs to be\nlearned in a way that generalizes to longer\ninputs (productivity).\n•SCAN-length (SCAN-l): The length split of\nthe SCAN dataset (Lake and Baroni, 2018).\nThe SCAN dataset asks the model to learn to\ninterpret and execute natural language instruc-\ntions with a limited vocabulary. For example,\nif the input is “walk twice\", the output should\nbe “I_WALK I_WALK“. There are a set of\nprimitive actions (walk, jump, etc.), and a set\nof modiﬁers (twice, thrice, left, etc.) and com-\nposition operators (e.g., and), and the model\nneeds to learn how to compose and execute\nall of those instructions to generate the out-\nput sequence. In this speciﬁc length split, the\ntraining and test sets are split by length (the\n3604\nAdd AddNeg Reverse Dup Cart Inters SCAN-l SCAN-aj PCFG-p PCFG-s COGS CFQ\nabs 236k 236k 236k 236k 238k 253k 238k 238k 337k 337k 402k 268k\nrel-e 239k 239k 239k 239k 241k 257k 241k 241k 340k 340k 405k 272k\nrel-b 236k 236k 236k 236k 238k 254k 238k 238k 337k 337k 402k 269k\nrel-eb 239k 239k 239k 239k 241k 257k 241k 241k 340k 340k 405k 272k\nrel2-e 239k 239k 239k 239k 241k 257k 241k 241k 340k 340k 405k 272k\nrel2-b 236k 236k 236k 236k 238k 254k 238k 238k 337k 337k 402k 269k\nrel2-eb 239k 239k 239k 239k 241k 257k 241k 241k 340k 340k 405k 272k\nabs-c 241k 241k 241k 241k 242k 258k 243k 243k 341k 341k 407k 273k\nrel-eb-c 243k 244k 243k 243k 245k 261k 245k 245k 344k 344k 410k 276k\nrel2-eb-c 243k 244k 243k 243k 245k 261k 245k 245k 344k 344k 410k 276k\nsmall-2 243k 244k 243k 243k 245k 261k 245k 245k 344k 344k 410k 276k\nsmall-4 480k 480k 480k 480k 482k 498k 482k 482k 581k 581k 646k 513k\nsmall-6 717k 717k 717k 717k 719k 735k 719k 719k 818k 818k 883k 750k\nlarge-2 1.88m 1.88m 1.88m 1.88m 1.88m 1.92m 1.88m 1.88m 2.08m 2.08m 2.21m 1.95m\nlarge-4 1.88m 1.88m 1.88m 1.88m 1.88m 1.92m 1.88m 1.88m 2.08m 2.08m 2.21m 1.95m\nlarge-6 2.81m 2.81m 2.81m 2.81m 2.81m 2.84m 2.81m 2.81m 3.01m 3.01m 3.14m 2.87m\nsmall-2s 125k 125k 125k 125k 127k 143k 127k 127k 226k 226k 291k 158k\nsmall-4s 125k 125k 125k 125k 127k 143k 127k 127k 226k 226k 291k 158k\nsmall-6s 125k 125k 125k 125k 127k 143k 127k 127k 226k 226k 291k 158k\nlarge-2s 486k 487k 486k 486k 490k 521k 490k 490k 687k 687k 818k 552k\nlarge-4s 486k 487k 486k 486k 490k 521k 490k 490k 687k 687k 818k 552k\nlarge-6s 486k 487k 486k 486k 490k 521k 490k 490k 687k 687k 818k 552k\nTable 11: Parameter counts for the models used in this paper.\ntest set contains the longest sequences and\nthe training set the shortest ones). Types of\ncompositionality: Overall, SCAN requires\nsigniﬁcant systematicity to be solved, and this\nsplit in particular focuses on productivity.\n•SCAN-add-jump (SCAN-aj): The add prim-\nitive jump split of the SCAN dataset (Lake\nand Baroni, 2018). In this split, the “jump”\ninstruction is only seen during training in iso-\nlation (i.e., there is a training example “jump”\n→“I_JUMP”), but the test set contains this\ninstruction heavily, and in combination with\nother constructs. Types of compositionality:\nthis split in particular focuses more on system-\naticity.\n•PCFG-productivity (PCFG-p): The produc-\ntivity split of the PCFG dataset (Hupkes et al.,\n2020). The PCFG dataset is a synthetic dataset\nwhere each example contains a set of opera-\ntions that need to be done to one or more input\nstrings, and the model needs to learn to apply\nthese operations and produce the ﬁnal output.\nOperations include reversing, duplicating, get-\nting the ﬁrst element, etc. Types of compo-\nsitionality: this split in particular focuses on\nproductivity, as test examples contain longer\nsequences of instructions than those seen dur-\ning training.\n•PCFG-sytematicity (PCFG-s: The system-\naticity split of the PCFG dataset (Hupkes et al.,\n2020). Types of compositionality:this split\nfocuses on systematicity, by testing the model\nrecombining operations in ways never seen\nduring training.\n•COGS: The generalization split of the COGS\nsemantic parsing dataset (Kim and Linzen,\n2020). This is a semantic parsing dataset,\nwhere the input is a sentence in natural lan-\nguage, and the output should be a logical rep-\nresentation of the sentence. Types of compo-\nsitionality: The generalization split contains\ncombinations not seen during training, while\nmost of these focus onsystematicity (e.g., con-\nstructions that had only been seen as subjects,\nnow they are seen as objects), some part of the\ntest set focuses on productivity (having deeper\nnesting of propositional phrases, for example).\nThis, productivity type of generalization, is\nwhere our sequence tagging approach signiﬁ-\ncantly outperforms previous approaches.\n•CFQ-mcd1 (CFQ): The MCD1 split of the\nCFQ dataset (Keysers et al., 2019). This\ndataset asks a model to learn how to translate\ndelexicalized natural language queries into\nSPARQL. Types of compositionality: the\nMCD1 split of this dataset focuses speciﬁcally\n3605\non systematicity, but more concretely, there\nare two additional ways in which this dataset\nis hard compositionally. First, solving this\ndataset requires solving Cartesian products\n(which is the reason for which we added the\nseparate Cartesian product task), since some\nquestion contains constructions like: “Who\ndirected, played and produced movies M1,\nM2 and M3”, which get translated into 9\nSPARQL clauses (the Cartesian product). Sec-\nond, SPARQL clauses are supposed to be pro-\nduced in alphabetical order, hence the model\nneeds to learn how to sort.\nFinally, table 7 shows the size of the training\nand test sets for each dataset, as well as the size\nof their vocabularies. For the vocabulary, we used\nthe union of the input and output vocabularies as\na uniﬁed vocabulary. We also show the number\nof training epochs we performed in each dataset\n(this was chosen as the number after which perfor-\nmance stabilized with some initial models; it was\nnot tuned afterwards during the systematic evalua-\ntion presented below).\n3606\nseq2seq tagging\nModel abs abs rel2-eb rel2-eb rel2-eb-c rel2-eb-cabs abs abs abs abs abs rel-eb rel-eb rel-eb rel-eb rel-eb rel-eb\nSize small-2 small-6 small-2s small-6s small-2s small-6ssmall-2 small-6 small-2 small-6 small-2 small-6 small-2s small-6s small-2s small-6s small-2s small-6s\nParent encoding absolute absolute relative relative attention attention absolute absolute relative relative attention attention\nTest split 0.981 0.646 0.978 0.961 0.983 0.974 0.997 0.994 0.997 0.997 0.997 0.997 0.995 0.997 1.000 1.000 1.000 1.000\nDev split 0.976 0.625 0.968 0.950 0.976 0.974 0.996 0.994 0.997 0.996 0.997 0.996 0.995 0.996 1.000 1.000 1.000 0.999\nLexical Generalization: Novel Combination of Familiar Primitives and Grammatical Roles\nSubject→Object (common noun) 0.309 0.008 0.030 0.011 0.900 0.899 0.911 0.938 0.914 0.916 0.893 0.918 0.899 0.972 0.978 0.996 0.969 0.956\nSubject→Object (proper noun) 0.098 0.000 0.000 0.000 0.581 0.429 0.630 0.590 0.563 0.494 0.731 0.610 0.690 0.671 0.567 0.580 0.826 0.672\nObject→Subject (common noun) 0.790 0.091 0.304 0.175 0.959 0.936 0.982 0.973 0.974 0.994 0.965 0.935 0.945 0.988 0.992 0.999 0.978 0.769\nObject→Subject (proper noun) 0.207 0.007 0.023 0.019 0.970 0.951 0.993 0.986 0.995 0.991 0.993 0.990 0.985 0.847 0.998 0.999 0.995 0.984\nPrimitive noun→Subject (common noun)0.240 0.098 0.242 0.216 0.956 0.913 0.993 0.983 0.995 0.972 0.990 0.978 0.927 0.976 1.000 1.000 0.988 0.927\nPrimitive noun→Subject (proper noun)0.019 0.004 0.016 0.017 0.422 0.772 0.974 0.983 0.979 0.985 0.993 0.992 0.796 0.990 0.959 0.803 0.996 0.999\nPrimitive noun→Object (common noun)0.017 0.007 0.014 0.012 0.929 0.902 0.950 0.968 0.939 0.945 0.949 0.972 0.904 0.986 0.996 1.000 0.953 0.966\nPrimitive noun→Object (proper noun) 0.000 0.000 0.000 0.000 0.200 0.513 0.651 0.687 0.557 0.569 0.722 0.649 0.624 0.676 0.545 0.467 0.700 0.673\nPrimitive verb→Inﬁnitival argument 0.000 0.000 0.000 0.000 0.476 0.766 0.000 0.000 0.000 0.000 0.011 0.000 0.017 0.000 0.000 0.000 0.001 0.000\nLexical Generalization: Verb Argument Structure Alternation\nActive→Passive 0.604 0.107 0.147 0.000 0.000 0.000 0.697 0.122 0.741 0.160 0.736 0.004 0.210 0.013 0.612 0.001 0.948 0.000\nPassive→Active 0.196 0.067 0.002 0.001 0.001 0.001 0.535 0.115 0.617 0.014 0.625 0.163 0.539 0.000 0.586 0.073 0.897 0.000\nObject-omitted transitive→Transitive 0.275 0.002 0.002 0.001 0.001 0.003 0.527 0.100 0.620 0.031 0.413 0.092 0.356 0.000 0.447 0.027 0.926 0.000\nUnaccusative→Transitive 0.069 0.000 0.002 0.001 0.002 0.003 0.528 0.144 0.295 0.000 0.457 0.000 0.301 0.000 0.326 0.005 0.787 0.005\nDouble object dative→PP dative 0.819 0.005 0.095 0.001 0.002 0.000 0.590 0.279 0.778 0.345 0.732 0.037 0.424 0.000 0.853 0.018 0.958 0.000\nPP dative→Double object dative 0.404 0.002 0.053 0.003 0.006 0.004 0.771 0.542 0.617 0.169 0.895 0.042 0.616 0.008 0.717 0.069 0.850 0.139\nLexical Generalization: Verb Class\nAgent NP→Unaccusative Subject 0.399 0.000 0.003 0.002 0.958 0.951 0.784 0.661 0.951 0.911 0.952 0.949 0.982 0.980 0.995 0.999 1.000 0.999\nTheme NP→Object-omitted transitive Subject0.688 0.000 0.023 0.000 0.818 0.965 0.791 0.644 0.642 0.861 0.473 0.420 0.537 0.831 0.984 0.903 0.701 0.485\nTheme NP→Unergative subject 0.694 0.000 0.023 0.000 0.843 0.966 0.930 0.715 0.643 0.858 0.544 0.570 0.583 0.895 0.960 0.919 0.771 0.530\nStructural Generalization: Novel Combination Modiﬁed Phrases and Grammatical Roles\nObject-modifying PP→Subject-modifying PP0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.007 0.029 0.000 0.000 0.000 0.000 0.299 0.371\nStructural Generalization: Deeper Recursion\nDepth generalization: PP modiﬁers0.003 0.000 0.000 0.000 0.000 0.000 0.138 0.074 0.231 0.191 0.133 0.000 0.000 0.010 0.669 0.775 0.681 0.819\nDepth generalization: Sentential complements0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.017 0.005 0.000 0.000 0.000 0.000 0.282 0.164 0.233 0.133\nOverall 0.278 0.019 0.047 0.022 0.430 0.475 0.637 0.500 0.622 0.496 0.629 0.445 0.540 0.469 0.689 0.514 0.784 0.497\nTable 12: Sequence-level accuracy in different subsets of the generalization set in COGS for both seq2seq and sequence tagging models (averaged over 5 runs). PP stands for\nprepositional phrase.\n3607",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7743083238601685
    },
    {
      "name": "Parsing",
      "score": 0.7577565312385559
    },
    {
      "name": "Transformer",
      "score": 0.6962462067604065
    },
    {
      "name": "Generalization",
      "score": 0.5798507332801819
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5307407975196838
    },
    {
      "name": "Natural language processing",
      "score": 0.530646026134491
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5019991397857666
    },
    {
      "name": "Programming language",
      "score": 0.4023692011833191
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3200187087059021
    },
    {
      "name": "Mathematics",
      "score": 0.0939573347568512
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}