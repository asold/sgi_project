{
    "title": "Using proprietary language models in academic research requires explicit justification",
    "url": "https://openalex.org/W4390052109",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2303276807",
            "name": "Alexis Palmer",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2183947846",
            "name": "Noah A. Smith",
            "affiliations": [
                "University of Washington",
                "Allen Institute"
            ]
        },
        {
            "id": "https://openalex.org/A1985069089",
            "name": "Arthur Spirling",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2303276807",
            "name": "Alexis Palmer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183947846",
            "name": "Noah A. Smith",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1985069089",
            "name": "Arthur Spirling",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4366277658",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W4387966945"
    ],
    "abstract": null,
    "full_text": "nature computational science Volume 4 | January 2024 | 2–3 | 2\nhttps://doi.org/10.1038/s43588-023-00585-1\nCorrespondence\nUsing proprietary language models in academic \nresearch requires explicit justification\nC\nalls for scientists to develop and \nuse open AI systems are growing —  \nespecially for language models \n(LMs)1,2. Beyond concerns about \nreproducibility of results from \nclosed models, being able to audit the data \nbeing used by the system helps researchers \nunderstand its behavior. Yet despite these \nappeals, researchers continue to use closed \ntechnologies in many disciplines 3–5. If — as \nmany believe — open systems are preferable, \nthis is dispiriting. Admonishing scientists not \nto use closed models is unlikely to be immedi-\nately successful. Here we survey reasons why \nproprietary models continue to be popular, \nand suggest how we as a scientific commu -\nnity can increase uptake of open technologies. \nOur proposal is simple and low cost: we ask \nthat scientists explicitly justify their use of \nproprietary models when they employ them \nin research.\nFollowing Rogers et al.2, we define ‘open’ as \nmodels that can be downloaded, run offline \nand shared; moreover, versioning is possible \nand we know what data the model was trained \non — even if that data is not available for direct \ninspection. They may or may not permit local \nadjustment of weights. Any model that is not \nopen is closed or proprietary. By “justify their \nuse of proprietary models” we mean explain \nwhy using a closed system was preferable over \nan open system for this particular application. \nGiven that proprietary systems come with pre-\ndictable costs, we want authors to delineate \nwhy the benefits outweigh those costs in the \ncurrent use-case.\nWhy is this justification process helpful for \nscience? First, introspection and explana -\ntion may encourage scientists to alter their \nfuture behavior and improve norms. In some \ncases, they may come to see their purported \nreasons as something more like ‘excuses’ . Sec-\nond, explicit justification helps us informally \n‘version’ closed models by documenting what \nthey were capable of on a given task at a given \ntime. Third, by articulating where scientists \ndeem closed systems to be currently superior, \nwe all learn where the open developer com -\nmunity should focus attention on improving \ntheir product.\nPotential justifications\nWe can think of six circumstances where \nanalysts believe the use of proprietary LMs is \nacceptable or even preferred. These are not \nmutually exclusive nor jointly exhaustive, but \nthey deserve respect as separate ‘mainstream’ \narguments.\n(1) Object of study per se. Some models, \nproducts or algorithms are of such central \nimportance to society that they ought to be \ninvestigated in and of themselves. Failing to \nstudy how, for instance, ChatGPT ‘behaves’ \nleaves us unable to understand why it pro -\nvokes such interest in the media or how expe-\nrience of that model might affect legislators \ndesigning regulation. In this sense, studying \nproprietary LMs is akin to investigating the \nalgorithms undergirding Facebook’s time -\nlines or YouTube’s video recommendations. \nThat is, we believe that the effects of such \nclosed technologies are so profound that we \nmust investigate them directly as part of the \nbroader mission of social science.\n(2) Community of interest. Even if the model \nis not central to society as a whole, it may none-\ntheless be of particular relevance to a group we \nwish to study. For example, in 2021 the Buzz-\nFeed chief executive announced a partnership \nof the news organization with OpenAI. T o the \nextent that journalists are of interest socio -\nlogically, we may want to understand how \nLMs are being implemented and the potential \nconsequences for journalistic work. Similarly, \nwe may want to understand how generative \ntext-to-image models like DALL-E could affect \nartists, or to see how AI chat apps will change \nthe roles of customer service professionals.\n(3) Technical state of the art. Perhaps the \nmost common (implicit) argument for closed \nLMs in computer science work is that they are \nstate of the art in a performance sense. That \nis, a proprietary model is able to do some \nkey tasks — like classify a text — better, meas-\nured by some objective criterion, than any \nother model (open or closed). Consequently, \nresearchers might argue that excluding \nthat closed LM from use understates what is \nactually possible on important problems, and \npotentially misrepresents the current frontier \nof science. Ultimately, exclusion might create \nperverse results: if the performance differ -\nences are sufficiently large, other scholars will \nmisperceive the ‘true’ benchmark for contri-\nbution, and build inferior systems than in a \ncounter-factual world where they had access \nto a preferred technology.\n(4) Ethical edge. A demonstration of some use \nof an open model may inspire or enable others \nto apply the model to a nefarious end that is, \nin a technical sense, similar. By design, there is \nno authority or firm to regulate an open model \nonce released, so a researcher might prefer to \nuse a closed model, whose proprietors could, \ntheoretically, curb unintended, dangerous \napplications. This may be even more impor-\ntant for intermediate products, such as mod-\nels released openly without guardrails.\n(5) Reproducibility and ease of use. Typical \narguments against closed LMs rely heavily on \nreproducibility, or lack thereof. For example, \ncompanies can alter (even abandon and make \nunavailable) their algorithms at any time. This \nshould be less of an issue for open systems. \nStill, maintainers do make changes to open \nLMs too — including re-estimating weights, \nor no longer maintaining extant models. \nThis could affect the reproduction of earlier \nfindings. At such times open model results \nmight be less stable than closed ones, if best \npractices of open development (including ver-\nsioning and documenting) are not followed. \nRelated, usability may be a relative prob -\nlem for open efforts. That is, if the relevant \nresearch community finds it prohibitively \ncostly to use an open LM, one may prefer a \nclosed LM.\n(6) Downstream use only. Often the LM \nitself — open or closed — is not under study, \nand is being used as an intermediary for some \nother purpose. Furthermore, any data being \nproduced by the LM is available for others to \ninspect, even if they cannot produce it them-\nselves. For example, suppose a researcher \nused an LM to simply summarize a set of extant \n Check for updates\nnature computational science\n Volume 4 | January 2024 | 2–3 | 3\nCorrespondence\npublic instructions to individuals in an experi-\nment. Here the use of the LM is incidental to \nthe research question, so it arguably makes \nlittle difference whether that system is open \nor closed — it has no direct consequences for \nreproducibility of the core research.\nRecommendations\nWhat should scientists working with LMs do, \ngiven our call above? Individuals ideally would \nnot use closed LMs. If they do, they should be \nexplicit about why they use them, giving as \nmany details as possible. Preferably they would \nclarify what open models they compared \ntheir results to. They should describe what \nopen LM developers might focus on in the \nnext iteration of their designs. Institutions, \nsuch as conferences and journals, can help by \nencouraging or even requiring these steps for  \ncontributing authors.\nAlexis Palmer    1, Noah A. Smith2 & \nArthur Spirling    3 \n1New York University, New York, NY, USA. \n2University of Washington and Allen Institute \nfor AI, Seattle, WA, USA. 3Princeton University, \nPrinceton, NJ, USA.  \n e-mail: arthur.spirling@princeton.edu\nPublished online: 21 December 2023\nReferences\n1. Spirling, A. Nature 616, 413 (2023).\n2. Rogers, A. Closed AI models make  \nbad baselines. Hacking Semantics  \n(3 April 2023).\n3. Gilardi, F., Alizadeh, M. & Kubli, M. PNAS 120, \ne2305016120 (2023).\n4. Yang, C. et al. Preprint at https://arxiv.org/abs/2309.03409 \n(2023).\n5. Yiu, E., Kosoy, E. & Gopnik, A. Perspect. Psychol.  \nSci. https://doi.org/10.1177/17456916231201401  \n(2023).\nCompeting interests\nThe authors declare no competing interests."
}