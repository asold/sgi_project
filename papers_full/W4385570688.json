{
  "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks",
  "url": "https://openalex.org/W4385570688",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5080142711",
      "name": "Zhenhailong Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5022703491",
      "name": "Xiaoman Pan",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5113226487",
      "name": "Dian Yu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5034476404",
      "name": "Dong Yu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5088339142",
      "name": "Jianshu Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5075033889",
      "name": "Heng Ji",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3197876970",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W4394643672",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W4288243162",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W4303648971",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3171600947",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W4285122897",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3035731883",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W4288111630",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W3173027903",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572880",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4286903249",
    "https://openalex.org/W4224875176",
    "https://openalex.org/W4288614645"
  ],
  "abstract": "Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into the model parameters.Although existing semi-parametric language models have demonstrated promising language modeling capabilities, it remains unclear whether they can exhibit competitive zero-shot abilities as their fully-parametric counterparts. In this work, we introduce Zemi, a semi-parametric language model for zero-shot task generalization. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train Zemi with semi-parametric multitask training, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, during both training and inference, Zemi is equipped with a retrieval system based on the unlabeled pretraining corpus of our backbone model. To address the unique challenges from large-scale retrieval, we further propose a novel retrieval-augmentation fusion module that can effectively incorporate noisy retrieved documents. Finally, we show detailed analysis and ablation studies on the key ingredients towards building effective zero-shot semi-parametric language models. Notably, our proposed Zemi_Large model outperforms T0-3B by 16% across seven diverse evaluation tasks while being 3.8x smaller in scale.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 3978–4004\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nZemi: Learning Zero-Shot Semi-Parametric Language Models from\nMultiple Tasks\nZhenhailong Wang∗\nUIUC\nwangz3@illinois.edu\nXiaoman Pan\nTencent AI Lab\nxiaomanpan@global.tencent.com\nDian Yu\nTencent AI Lab\nyudian@global.tencent.com\nDong Yu\nTencent AI Lab\ndyu@global.tencent.com\nJianshu Chen\nTencent AI Lab\njianshuchen@global.tencent.com\nHeng Ji\nUIUC\nhengji@illinois.edu\nAbstract\nAlthough large language models have exhibited\nimpressive zero-shot ability, the huge model\nsize generally incurs high cost. Recently, semi-\nparametric language models, which augment\na smaller language model with retrieved re-\nlated background knowledge, alleviate the need\nfor storing everything into the model parame-\nters. Although existing semi-parametric lan-\nguage models have demonstrated promising\nlanguage modeling capabilities, it remains un-\nclear whether they can exhibit competitivezero-\nshot abilities as their fully-parametric coun-\nterparts. In this work, we introduce Zemi, a\nsemi-parametric language model for zero-shot\ntask generalization. To our best knowledge,\nthis is the first semi-parametric language\nmodel that can demonstrate strong zero-\nshot performance on a wide range of held-\nout unseen tasks. We train Zemi with semi-\nparametric multitask training, which shows\nsignificant improvement compared with the\nparametric multitask training as proposed by\nT0 (Sanh et al., 2021). Specifically, during\nboth training and inference, Zemi is equipped\nwith a retrieval system based on the unla-\nbeled pretraining corpus of our backbone\nmodel. To address the unique challenges\nfrom large-scale retrieval, we further propose\na novel retrieval-augmentation fusion mod-\nule that can effectively incorporate noisy re-\ntrieved documents. Finally, we show detailed\nanalysis and ablation studies on the key ingredi-\nents towards building effective zero-shot semi-\nparametric language models. Notably, our pro-\nposed ZemiLARGE model outperforms T0-3B\nby 16% across seven diverse evaluation tasks\nwhile being 3.8x smaller in scale.1\n1 Introduction\nAchieving strong generalization ability on unseen\ntasks while maintaining a reasonably small param-\n∗Work was done when interning at Tencent AI Lab.\n1Code and data are available for research purpose athttps:\n//github.com/MikeWangWZHL/Zemi.\neter size is a long-lasting challenge for natural lan-\nguage processing (NLP) models. Although large\nlanguage models (Brown et al., 2020; Lieber et al.,\n2021; Rae et al., 2021; Smith et al., 2022; Hoff-\nmann et al., 2022; Zhang et al., 2022; Ouyang et al.,\n2022; Chowdhery et al., 2022) have shown impres-\nsive zero-shot ability on various NLP tasks, the\nhuge model size generally incurs high cost. Al-\nternatively, instead of stuffing everything in the\nmodel parameters, recent work on semi-parametric\nlanguage models (Grave et al., 2016; Khandelwal\net al., 2019; Yogatama et al., 2021; Borgeaud et al.,\n2021; Zhong et al., 2022) demonstrated compet-\nitive language modeling performance compared\nwith much larger fully-parametric language models.\nThe intuition is to use a relatively small language\nmodel as a reasoning module and augment it with a\nretriever to retrieve related background knowledge,\nwhich effectively alleviates the need for increasing\nthe model capacity to align with the growing data\nsize.\nHowever, what really makes large language mod-\nels the focus of attention in the past two years\nis their strong zero-shot in-context learning abili-\nties. Unfortunately, it is still unclear whether semi-\nparametric language models can exhibit similar\nzero-shot ability on unseen tasks as their fully-\nparametric counterparts such as T0 (Sanh et al.,\n2021) and GPT-3 (Brown et al., 2020). Moreover,\nimprovements in language modeling metrics such\nas perplexity may not guarantee better performance\non downstream tasks especially in low-shot set-\ntings (Wei et al., 2022). Thus, in this work, we\naim to investigate this unexplored research ques-\ntion, can semi-parametric language models exhibit\nstrong zero-shot generalization abilities on various\ndownstream tasks?\nTo this end, we introduce Zemi, a zero-shot\nsemi-parametric language model. To the best of\nour knowledge, this is the first semi-parametric\nlanguage model that shows strong zero-shot perfor-\n3978\nFigure 1: Overview of the semi-parametric multitask prompted training. Each training and evaluation instance is\nformatted with unified text-to-text prompt templates (Sanh et al., 2021; Bach et al., 2022). In this work, we further\naugment the prompted instances with retrieved passages from a large-scale task-agnostic corpus, C4 (Sanh et al.,\n2021), which is the same unlabeled pretraining corpus used in T5 (Raffel et al., 2020) and T0 (Sanh et al., 2021).\nAn example of the prompted input and the retrieved documents can be found in Figure 2.\nmance on a wide range of downstream tasks. In or-\nder to effectively train Zemi, we propose to extend\nthe multitask prompted training (Sanh et al., 2021)\ninto semi-parametric settings (Section 2.1). Specif-\nically, during both the training and the inference\nstage, we augment the prompted instances with\nretrieved plain text documents. To cover a wider\nrange of unseen tasks, instead of retrieving from\nspecific corpora for certain tasks, such as exploit-\ning Wikipedia for open-domain question answer-\ning (Lee et al., 2019; Karpukhin et al., 2020; Izac-\nard and Grave, 2020), we retrieve documents from\na large-scale task-agnostic corpus, C4 (Raffel et al.,\n2020) (Section 2.2). Notably, C4 is the unlabeled\npre-training corpus of our backbone model (Raffel\net al., 2020), which means that every document\nis seen by the model and we do not require any\nannotated or curated resources. This guarantees\nfair comparison with the parametric counterpart\nT0 (Sanh et al., 2021).\nIn our preliminary experiments, we find that ex-\nisting methods (Izacard and Grave, 2020; Brown\net al., 2020) for incorporating retrieved text can-\nnot effectively handle the noise inevitably intro-\nduced by retrieving from large-scale corpora. To ad-\ndress this challenge, we propose a novel retrieval-\naugmentation fusion module that can selectively\nignore noisy retrieved text. Specifically, we in-\ntroduce a light-weight perceiver resampler and a\ngated cross-attention layer (Alayrac et al., 2022)\nto enforce the model to attend to salient informa-\ntion of each augmentation and gate out noisy ones\n(Section 2.3).\nWe train Zemi on eight multiple-choice question\nanswering (QA) tasks (4.5x fewer than T0) and\nevaluate on a diverse set of seven unseen tasks from\nfive categories (Section 3.1). In order to investigate\nthe impact of the retrieval-augmentation, we favor\nknowledge-intensive tasks over extractive tasks.\nExperimental results show that Zemi outper-\nforms both parametric and semi-parametric base-\nlines. Notably, ZemiLARGE outperforms T0-3B\nby 16% across seven evaluation tasks while be-\ning 3.8x smaller in scale (Section 3.2). We\nfurther conduct extensive analysis on why Zemi\nworks. We show that the source of the im-\nprovements comes from the interplay of our pro-\nposed retrieval-augmentation fusion architecture\nalong with the semi-parametric multitask training\nparadigm. Finally, we perform in-depth ablation\nstudies on all aspects of our model design including\nthe gated mechanism.\nTo sum up, the main contributions of this paper\nare threefold:\n• We introduce Zemi, which is to our knowl-\nedge the first semi-parametric model that\ndemonstrates strong zero-shot task general-\nization ability.\n• We propose a novel retrieval-augmentation fu-\nsion module which can effectively handle mul-\ntiple potentially noisy retrieved documents\nand is essential towards the effectiveness of\nsemi-parametric multitask training.\n• We show detailed analysis and ablation stud-\nies on why Zemi works which shed light on\nfuture work for developing large-scale uni-\nversal semi-parametric language models with\nstrong zero-shot ability.\n3979\n2 Method\n2.1 Semi-parametric multitask training\nIn this section, we introduce how we extend the\nmultitask training paradigm to semi-parametric lan-\nguage models. We follow the overall text-to-text\nframework proposed by the previous parametric\nmultitask prompted training (Sanh et al., 2021)\nwhere each input-output pair of a certain task is\nconverted into a prompted text input and a gener-\nated text output via human-written templates (Bach\net al., 2022).2 For Zemi, as illustrated in Figure 1,\nduring both training and inference, we augment\nZemi with a retrieval system. Instead of using spe-\ncific corpora for different tasks, such as Wikipedia\nfor open-domain question answering (Chen et al.,\n2017; Karpukhin et al., 2020; Izacard and Grave,\n2020) and textbooks for science question answer-\ning (Mihaylov et al., 2018), we retrieve texts from\na large-scale task-agnostic corpus, C4 (Raffel et al.,\n2020) (Section 2.2). Retrieving from a larger cor-\npus brings wider coverage but also more noisy aug-\nmentations. To address this problem we further\npropose a novel semi-parametric architecture for\nZemi that specializes in handling a large number\nof potentially noisy augmentations (Section 2.3).\nAfter semi-parametric multitask training, we per-\nform zero-shot evaluation on seven diverse held-out\nunseen tasks (Section 3).\n2.2 C4 retrieval\nTo build a universal semi-parametric language\nmodel that can generalize to various types of NLP\ntasks, we retrieve documents from Colossal Clean\nCrawled Corpus (C4) (Raffel et al., 2020). Notably,\nC4 is the unlabeled pre-training corpus of our back-\nbone model T5 (Raffel et al., 2020), which guaran-\ntees fair comparison with non-retrieval methods in\nour zero-shot evaluation settings. The C4 corpus\n(750GB in size) contains more than 364 million\ndocuments. Performing dense retrieval (Karpukhin\net al., 2020) on such a wide-coverage corpus is\nvery expensive. Thus, for efficiency considera-\ntion, we perform document-level indexing and re-\ntrieval based on BM25 (Robertson et al., 1995)\nwith ElasticSearch (ElasticSearch) and Hugging-\nface Datasets (Lhoest et al., 2021). Despite its\nsimplicity, recent work (Wang et al., 2022a) has\ndemonstrated the effectiveness of using BM25 for\nretrieving clean training data as augmentations. To\n2https://github.com/bigscience-workshop/promptsource.\nfurther improve the retrieval efficiency, we use 5%\nof the entire C4 corpus, which is still 3x larger than\nthe Wikipedia corpus (Foundation), as our retrieval\ncorpus in our experiments. For each query, we trun-\ncate the query length at 20 tokens and truncate each\nretrieved document at 256 tokens. See details on\nthe query fields for each dataset in Appendix D.\n2.3 Zemi model architecture\nOne major challenge of retrieving from a large-\nscale task-agnostic corpus is that the retrieved aug-\nmentations (documents) can be noisy and inaccu-\nrately ranked. Examples of good and noisy re-\ntrieved documents can be found in Appendix A.\nTo address this problem, intuitively, we want the\nmodel to have the following two properties: (1) be\nable to simultaneously pay attention to multiple re-\ntrieved augmentations instead of only the top-1 doc-\nument. (2) be able to identify salient information\nfrom the retrieved augmentations and selectively\nignore uninformative ones.\nTo this end, we propose the Zemi archi-\ntecture, a semi-parametric language model ca-\npable of selectively incorporating multiple po-\ntentially noisy retrieved augmentations. The\nmain idea is to jointly train a light-weight\nretrieval-augmentation fusion module between\nthe encoder and decoder, which contains two ma-\njor components, a perceiver resampler and a gated\ncross-attention, which are inspired by recent work\non vision-language fusion (Alayrac et al., 2022).\nFigure 2 shows an illustration of theZemi model\narchitecture. We consider a prompted text in-\nput I and a few retrieved textual augmentations\nA1, A2, ..., Ak. Let lI, li\nA be the length of the\nprompted input and the ith augmentation. Let d\nbe the hidden dimension of our backbone model.\nWe first independently encodeI and A1, A2, ..., Ak\nwith a shared T5 (Raffel et al., 2020) encoder Enc.\nWe then feed the latent representation of the aug-\nmentations A1, A2, ...,Ak through the perceiver\nresampler.\nI = Enc(I) (1)\nAi = Enc(Ai) (2)\nA′\ni = PerceiverResampler(Ai, Q) (3)\nwhere ∀ i ∈ {1, .., k}, I ∈ RlI×d, Ai ∈ Rli\nA×d\nand A′\ni ∈ RlQ×d.\nAs shown on the bottom right of Figure 2,\nthe perceiver resampler is a variant of Perceiver\nIO (Jaegle et al., 2021), where a cross-attention is\n3980\nFinish the following sentence with\nthe best choice: how do you taste\nsomething?\nChoices:\n- smell it enough to taste it.\n- place it in your mouth to taste.\nAnswer:\nPrompted Input\nC4 Retrieval\nTell about a time\nwhen your food\nneeded more\nflavor. What did\nyou do to ...\nwe want to taste\nsomething. And\nwe know that how\nmany different\ntastes are there ...\n... \nRetrieved Augmentations\nT5 Encoder\n... Latent\nQuery\n... Encoded\nAugmentation \nEncoded\nInput\nRetrieval-Augmentation Fusion\nFFN\nCross-Attention\nkey   \nvalue query\nTanh Gate\nTanh Gate\n... \nGated Cross-Attention \nFFN\nCross-Attention\nkey   \nvalue\nquery\nPerceiver Resampler \nGated Cross-Attention\nplace it in your mouth to taste.\nGenerated Output\nPerceiver Resampler\nT5 Decoder\nFigure 2: Zemi model architecture with an example of a prompted input and a generated output from the Piqa (Bisk\net al., 2020) task. The italic text in the prompted input I indicates the prompt template. A1 and Ak shows two\nexamples of the corresponding retrieved augmentations (documents) from the C4 corpus. To incorporate the\npotentially noisy retrieved augmentations, we introduce a light-weight retrieval-augmentation fusion module that\ncontains two major components, a single layer perceiver resampler and a single layer gated cross-attention (detailed\non the right).\nperformed between the variable-length latent repre-\nsentation of an augmentation Ai and a fixed-length\nlearnable latent query vector Q. Let lQ be the\npredefined length of the latent query, which is typi-\ncally smaller than the original length of an augmen-\ntation li\nA. The output of the perceiver resampler\nis a compressed fixed-length latent representation\nof each augmentation. This resampling mecha-\nnism not only allows the model to include longer\nand a larger number of augmentations but also en-\ncourages the model to select salient information\nfrom the original augmentations. After the resam-\npler, we concatenate the encoded augmentations\nA′= [A′\ni, ..,A′\nk] and perform gated cross-attention\nwith the encoded prompted input I. As shown in\nthe top right of Figure 2, the gated cross-attention\nlayer contains two Tanh gates controlling the in-\nformation flow from the cross-attention layer and\nthe feed-forward layer before the addition with the\nskip connections, i.e., the original encoded input\nI. Finally, the hidden states from the gated cross-\nattention module H is fed into the T5 decoder Dec\nto generate the output sequence O.\nH = GatedCrossAttn([A′\ni, ..,A′\nk], I) (4)\nO = Dec(H) (5)\nwhere [A′\ni, ..,A′\nk] ∈ R(k×lQ)×d and H ∈ RlI×d.\nFollowing (Alayrac et al., 2022), we initialize\nthe parameter of the Tanh gate to be 0, allowing\nthe forward pass of the prompted input through the\npre-trained T5 encoder-decoder to be intact at the\nbeginning of the training process. With the gated\nmechanism, the model can learn to gate out noisy\naugmentations and rely more on the skip connec-\ntions during semi-parametric multitask training.\n3 Experiments\n3.1 Experimental setup\nFollowing (Sanh et al., 2021) we partition vari-\nous types of NLP tasks into two groups, train-\ning tasks and held-out unseen tasks. In this work,\nwe are particularly interested in investigating the\nimpact of the retrieval augmentation Thus, when\nchoosing the training and evaluation tasks, we fa-\nvor knowledge-intensive tasks over extractive tasks\nsuch as summarization, where most knowledge for\nsolving the problem is already self-contained in\n3981\nMethod semi- # train # param Tasks Avg5 Avg7param tasks OBQA Piqa RT CB COPA WiC HSwag\nBART0 No 36 0.4B 34.4 36.1 - 39.6 - 46.7 39.4 39.3 -\nT0-3B No 36 3B 42.8 59.3 73.6 ⋆ 45.5 75.9 50.0 27.3 45.0 53.5\nT0-11B No 36 11B 59.1 72.5 81.8 ⋆ 70.1 91.5 55.2 33.5 58.1 66.3\nReCross Yes 36 0.4B 39.6 41.4 - 44.8 - 50.6 47.3 44.7 -\nZemiBASE (ours) Yes 8 0.2B 35.6 59.2 68.6 50.1 63.6 49.6 29.7 44.8 50.9\nZemiLARGE (ours) Yes 8 0.8B 51.5 67.9 84.1 62.1 84.5 50.4 35.8 53.5 62.3\nGPT-3 No - 175B 57.6 81.0 ⋆ 59.7 46.4 91.0 49.4 † 78.9 62.7 66.3\nTable 1: Comparison to both parametric ( BART0, T0, GPT-3 ) and semi-parametric ( ReCross) state-of-the-art.\nZemiLARGE significantly outperforms T0-3B while being 3.8x smaller in scale. ZemiBASE slightly outperforms\nReCross while being 1.7x smaller. Note that Avg7 indicates averaged performance across all seven tasks. Avg5\nindicates averaged performance on five tasks excludingRT and COPA due to unreported baseline results. ⋆ indicates\nthe task is seen during training. † indicates few-shot results with 32 examples.\nthe input. Furthermore, we avoid including large\ndatasets, such as DBPedia (Lehmann et al., 2015)\n(630K instances) and QQP (Shankar Iyer, 2017)\n(400K instances), due to limited computational re-\nsources.\nTraining Tasks We use a subset of T0’s (Sanh\net al., 2021) training mixture for our semi-\nparametric multitask prompted training. Specif-\nically, our training mixture contains eight\nmultiple-choice QA datasets, including, Common-\nsenseQA (Rajani et al., 2019), CosmosQA (Huang\net al., 2019), DREAM (Sun et al., 2019),\nQASC (Khot et al., 2020), QUARTZ (Tafjord\net al., 2019), SciQ (Johannes Welbl, 2017), Social\nIQa (Sap et al., 2019), and WIQA (Tandon et al.,\n2019). We choose the subset in multiple-choice\nQA tasks because they are diverse in domains and\noverall task formats. Ablation studies on includ-\ning more types of training tasks can be found in\nSection 3.4.\nEvaluation Tasks For evaluation tasks, we con-\nsider seven datasets from five diverse categories\nfollowing the task taxonomy of T0, including,\ntwo sentence completion tasks, COPA (Roem-\nmele et al., 2011) andHellaSwag (HSwag) (Zellers\net al., 2019), two multiple-choice QA tasks, Open-\nbookQA (OBQA) (Mihaylov et al., 2018) and\nPiqa (Bisk et al., 2020), one word sense dis-\nambiguation task, WiC (Pilehvar and Camacho-\nCollados, 2018), one sentiment task, Rotten Toma-\ntoes (RT) (Pang and Lee, 2005), and one natural\nlanguage inference task, CB (De Marneffe et al.,\n2019). All scores are reported on the validation\nset of each dataset. The detailed prompt templates\nused for training and evaluation can be found in\nAppendix C.\nPrompts We use PromptSource (Bach et al.,\n2022) with Huggingface Datasets (Lhoest et al.,\n2021) to construct prompted inputs for each train-\ning and evaluation instance. During training, we\nrandomly select two templates for each dataset.\nDuring evaluation, we follow the exact evaluation\nprocedure as in T0 (Sanh et al., 2021) and report\nthe mean accuracy across all available templates.\nAll scores are reported on the validation set of each\ndataset. The detailed templates used for training\nand evaluation can be found in Appendix C.\nModel We consider two variants of Zemi with\na different pre-trained backbone, i.e., T5-base\nand T5-large (Raffel et al., 2020). Following\nT0 (Sanh et al., 2021), we use the language mod-\neling adapted3 checkpoint, which is trained for an\nadditional 100k steps on a language modeling ob-\njective. By default, we use five retrieved passages\nas augmentations for each instance. More imple-\nmentation details can be found in Appendix B\n3.2 Main results\nWe aim to explore whether Zemi can exhibit com-\npetitive zero-shot performance against larger state-\nof-the-art language models. We compare Zemi\nwith both parametric ( T0 (Sanh et al., 2021),\nBART0 (Lin et al., 2022), GPT-3 (Brown et al.,\n2020)) and semi-parametric ( ReCross (Lin et al.,\n2022)) models on seven zero-shot tasks. Table 1\nshows the mean zero-shot accuracy across all tem-\nplates for each task. The last two columns of Ta-\nble 1 show the averaged performance across differ-\nent sets of tasks, where Avg7 is averaged across all\nseven tasks, and Avg5 considers five tasks exclud-\ning RT and COPA due to their unavailable baseline\n3https://huggingface.co/google/t5-base-lm-adapt.\n3982\nresults.\nFor BART0, ReCross, and GPT-3, we copy the\nreported scores directly from their original papers.\nFor the missing score of RT on GPT-3, we run the\noriginal text completion API 4 to get the generated\noutputs which is then mapped to the most similar\nanswer choice using SentenceBert (Reimers and\nGurevych, 2019). For T0 models, there are some\ntasks such as OBQA and Piqa that are not evalu-\nated in the original paper (Sanh et al., 2021), and\nsome tasks such as CB and WiC are evaluated with\nslightly different templates. Thus, for fair com-\nparison, we re-evaluate all seven tasks on T0-3B\nand T0-11B using the official implementation and\ncheckpoints5 with the exact same set of templates\nas our model. See details on the templates used for\neach task in Appendix C.\nResult Table 1 shows thatZemiBASE outperforms\nprevious retrieval-based method, ReCross, on the\naverage of five tasks (Avg5) while being 2x smaller\nin scale. Notably, ZemiLARGE, significantly out-\nperforms T0-3B on seven evaluation tasks (Avg7)\nby 16% with 3.8x fewer parameters. This shows\nthat Zemi scales up well with larger backbone mod-\nels. We also observe that although trained with 4.5x\nfewer training tasks (8 v.s. 36), Zemi effectively\nachieves state-of-the-art zero-shot performance. In\nSection 3.4, we show that adding more tasks into\nmultitask training does not necessarily improve\nthe performance. And the training mixture with\nmultiple-choice QA tasks seems to be highly ef-\nfective in generalizing to various kinds of unseen\ntasks.\n3.3 Analysis: semi-parametric v.s. parametric\nIn order to further analyse the source of the\nstrong performance of ZemiLARGE, we compare\nZemiLARGE with a baseline ( No Aug) trained\nwith parametric multitask training on the same\nset of training tasks and with the same back-\nbone model, T5-Large (Raffel et al., 2020).\nTo show the impact of our newly proposed\nretrieval-augmentation fusion module, we further\ncompare ZemiLARGE against two semi-parametric\nbaselines with a different fusion method for in-\ncorporating the retrieved augmentations (Concat\nand FiD). In Table 2, we show that the source\nof benefit comes from the interplay of the pro-\n4For consistency with other results, we report the RT result\nfrom the original “davinci” model.\n5https://github.com/bigscience-workshop/t-zero.\nGood Retrieval Example (Openbook QA)\nNoisy Retrieval Example (Rotten Tomatoes)\nQuestion: Eating certain foods can add fiber into a diet which helps\nthe body to stay regular, such as when eating Answer: broccoli\nRetrieved Text: ... Dietary fiber is a natural ingredient of high-fiber\nfoods, such as vegetables, salads, fruits and cereals ...\nQuestion: an opportunity missed . Did the reviewer find this movie\ngood or bad? Answer: bad \nRetrieved Text: ... Call Agent - A farmland not to be missed.  \nExcellent opportunity for growth. ...\nFigure 3: Example of good and noisy retrieved augmen-\ntations. See Appendix A for more examples.\nposed retrieval-augmentation fusion and the\nsemi-parametric multitask training.\nSpecifically, for Concat, we directly concatenate\nall retrieved augmentations with the prompted input\ntext. The concatenated input is then truncated to the\nmaximum acceptable length of 1024 tokens and fed\nto our backbone model. For FiD, we implement the\nmodel following (Izacard and Grave, 2020) where\nwe first independently encode each pair of retrieved\naugmentation and the prompted input text. Then\nwe concatenate the encoder outputs and feed them\nto the decoder. Note that we keep everything else\nidentical except the retrieval-augmentation fusion\nmodule for ZemiLARGE, Concat and FiD.\nZemi architecture improves zero-shot task gen-\neralization. In Table 2, we first notice that the\nsemi-parametric setting in itself does not necessar-\nily bring consistent positive gains compared with\nthe No Aug baseline, as shown in the results of\nConcat and FiD. This can be explained by the fact\nthat the retrieved documents are not always highly\ncorrelates with the task of interest, as shown in the\nexamples in Figure 3. The fact that FiD performs\nbetter than Concat further verifies this hypothesis,\nsince FiD preserves more input text information in\nthe encoding step and only do fusion with all the\nretrieved augmentations in the decoder, whereas\nConcat perform unified self-attention on all aug-\nmentations concatenated directly to the input.\nOn the other hand, with the proposed\nretrieval-augmentation fusion module that con-\ntains the explicit resampling and gating mechanism,\nZemiLARGE was able to achieve the best perfor-\nmance on six out of seven tasks, and brings a over-\nall gain of +5% against the No Aug baseline. This\nresult shows that the retrieval-augmentation fusion\nmodule in Zemi can effectively enable the model to\nleverage potentially noisy retrieved augmentations\nduring semi-parametric multitask training, which\n3983\nMethod # Param Tasks AvgOBQA Piqa RT CB COPA WiC HSwag\nNo Aug 0.8B 50.5 65.5 82.2 52.4 80.0 50.2 34.1 59.3\nConcat 0.8B 48.8 65.9 74.9 44.6 82.7 50.0 30.5 56.8\nFiD 0.8B 51.0 66.7 67.1 60.7 86.3 50.2 32.9 59.3\nZemiLARGE (Ours) 0.8B 51.5 67.9 84.1 62.1 84.5 50.4 35.8 62.3\nTable 2: Comparison to parametric multitask trained baseline (No Aug) and alternative augmentation fusion methods\n(Concat, FiD) with an identical backbone model, T5-large. # Param indicates the model size.\nbrings significant improvement in zero-shot task\ngeneralization. In abaltion study 3.4, we further\nverify that the gated cross-attention is an important\nfactor contributing to the effectiveness of the Zemi\narchitecture.\n3.4 Analysis: ablation studies\nIn this section, we continue investigating why Zemi\nworks by conducting comprehensive ablation stud-\nies on different aspects of the model design. As\nshown in Table 3, we consider the following five\ncategories of ablated settings on ZemiBASE 6:\n(i) Tanh gate. We replace the gated cross-\nattention module with vanilla cross-attention in the\nablated version. Specifically, we remove the two\nTanh gates as shown in Figure 2. We find that\nremoving Tanh gate hurts the zero-shot perfor-\nmance. Note that the Tanh gate is also the main dif-\nference between Zemi and FiD (Izacard and Grave,\n2020).\n(ii) Number of augmentations. We ablate on the\nnumber of augmentations. Note that for settings\nwith 20 and 30 augmentations, in order to reduce\nthe computation complexity, we propose another\nvariant of ZemiBASE where we encode augmenta-\ntions with a separate frozen augmentation encoder.\nWe find that increasing the number of augmenta-\ntions from single to multiple (five) improves the\nperformance. However, further increasing the num-\nber to 10 starts to hurt the performance, which\nagain indicates that the noise starts to overwhelm\nthe useful signals introduced by the retrieval. We\nalso observe that the performance with 30 augmen-\ntations outperforms 20 augmentations, we hypoth-\nesis that this is due to inaccurate retrieval ranking\nthat leads to some more informative documents\nbeing ranked lower. We show an example of this\n6We ablate on ZemiBASE instead of ZemiLARGE mainly to\nreduce the computation overheads of a large amount of exper-\niments.\ncase in Figure 11. Nevertheless, the fact that we\nare able to achieve positive gain with as many as 30\naugmentations shows the robustness of our model\nto very noisy augmentations.\n(iii) Perceiver resampler latent size. We ablate\non the size of the latent query vector in the per-\nceiver resampler. Note that here the latent size is\ndifferent from the hidden state size of the backbone\nmodel. The trade-off of the size of the latent query\nvector is that, a larger latent size preserves more in-\nformation from the original augmentation but also\nincludes more noise. A larger latent size can also\nincrease the computational complexity. We find\nthat Zemi is relatively robust to the change of the\nlatent size and achieves the best performance with\na latent size of 64.\n(iv) Per augmentation length. We investigate\nthe impact of different ways of constructing aug-\nmentations from the retrieved documents. Specif-\nically, we increase the maximum length of each\naugmentation from 256 to 512 and fit two retrieved\ndocuments into one augmentation. We keep the\nnumber of augmentations the same as default, i.e.,\n5. We then compare this ablated setting with the\n10-augmentation variant in (ii). We find that with\nthe same set of retrieved documents, augmenting\nthe model with longer but fewer augmentations\ngenerally outperforms using a larger number of\nshorter augmentations.\n(v) Training mixture. We investigate the impact\nof adding new types of training tasks to the original\ntraining mixture. We dub the models trained with\nthis new training mixture as No Aug+ and Zemi+.\nSpecifically, apart from the eight multiple-choice\nQA tasks, we further include four more tasks: one\nclosed-book QA task WikiQA (Yang et al., 2015),\none topic classification task TREC (Li and Roth,\n2002), one sentence completion taskCOPA(Roem-\nmele et al., 2011), and one sentiment task Rotten\n3984\nAblated Zemi Changed Tasks Avgsetting value value OBQA Piqa RT CB COPA WiC HSwag\nNo Augmentation (No AugBASE) 36.6 60.2 64.1 41.5 68.5 49.9 28.0 49.8\nZemiBASE 35.6 59.2 68.6 50.1 63.6 49.6 29.7 50.9\n(i) Tanh Gate ✓ ✗ 35.0 57.8 55.8 49.9 71.5 51.6 27.9 49.9\n(ii) 5\n1 35.6 58.9 67.1 47.6 65.2 49.5 30.1 50.6\nNum of 10 35.3 59.4 62.1 46.3 64.6 51.4 29.4 49.8\nAugs 20⋆ 34.7 58.7 60.3 46.5 61.6 50.1 28.4 48.6\n30⋆ 35.1 60.5 58.7 48.2 67.2 50.7 28.5 49.8\n(iii) Latent 64 32 34.9 58.8 64.3 44.5 67.9 51.2 28.6 50.0\nsize 128 34.7 57.6 63.1 47.8 69.8 50.4 28.1 50.2\n(iv) Aug length 256 512 35.3 58.8 58.6 52.5 68.9 50.3 28.8 50.5\n(v) Training Zemi No Aug+ 37.6 58.4 - 43.3 - 50.7 28.0 -\nmixture Zemi+ 34.5 58.7 - 42.8 - 50.1 29.3 -\nTable 3: Ablation study. Each ablated setting should be compared with the first two rows, i.e., the original No\nAugmentation (No AugBASE) setting and ZemiBASE. The superscripted “⋆” in ablated setting (ii) indicates using the\nmodel variant with a frozen augmentation encoder. See descriptions of each setting in Section 3.4.\nTomatoes (RT) (Pang and Lee, 2005) 7. We find\nthat adding new types of tasks does not necessarily\nincrease the performance. Although trained with\nonly 8 tasks (v.s. 36 tasks) we are able to achieve\nstate-of-the-art performance (Section 3.2), which\nshows that the multiple-choice QA mixture is\nhighly effective for generalizing to a wide range\nof held-out unseen tasks.\n3.5 Analysis: computation overheads\nThere are two main computation overheads com-\npared with the fully-parametric counterpart, i.e., the\nNo Aug baseline. First, retrieving from a large-scale\ncorpus can be time-consuming. As mentioned in\nSection 2.2, we apply document-level retrieval with\nBM25 and truncation on the query to reduce the\nretrieval time. We also perform the retrieval offline\nto avoid repeated time commitment. As a result,\nindexing 5% of the C4 corpus takes 1 hour. Offline\nretrieval for the entire training and evaluation mix-\nture takes 11 hours, which is approximately 0.28\nseconds per instance. Furthermore, we measure\nthe computation overhead on inference which is\ncaused by the additional retrieved inputs as well\nas a small amount of newly introduced parameters\n(+4.6%). The average computation overhead across\nall evaluation datasets during inference is around\n7We follow T0 to move tasks that are originally in the\nevaluation split, i.e. COPA and RT, into the training split in\nthis ablated setting.\n4x compared with the No Aug baseline. Notably,\nTable 2 shows that ZemiBASE achieves competitive\nperformance with T0-3B while being 15x smaller\nin scale, indicating that the benefit of the retrieval\naugmentation overwhelms the computation over-\nhead.\n4 Related Work\n4.1 Semi-parametric models\nSemi-parametric models (Sun et al., 2021; Verga\net al., 2021; Chen et al., 2017; Lee et al., 2019; Guu\net al., 2020; Wang et al., 2019; Karpukhin et al.,\n2020; Yang et al., 2019; Lewis et al., 2020; Izac-\nard and Grave, 2020), which augmenting a para-\nmetric neural network with external knowledge\nbases or text corpora, have been widely applied\nto knowledge-intensive NLP tasks such as open-\ndomain question answering. Recent advancements\nin semi-parametric language models (Khandelwal\net al., 2019; Yogatama et al., 2021; Borgeaud et al.,\n2021; Zhong et al., 2022) have demonstrated im-\nproved language modeling performance with a rela-\ntively small language model and a retrieval system\nbased on a large-scale corpus. Although the afore-\nmentioned semi-parametric language models have\nshown competitive performance on language mod-\neling, compared with fully-parametric counterparts\nsuch as GPT-3 (Brown et al., 2020), it is unclear\nwhether the superiority generally holds on down-\n3985\nstream tasks. While concurrent work (Izacard et al.,\n2022) showed initial success in few-shot settings\nrelying on Fusion-in-Decoder (FiD) (Izacard and\nGrave, 2020) framework, this work focus on the\nmore challenging zero-shot settings (Sanh et al.,\n2021; Zhou et al., 2022; Gu et al., 2022). Fur-\nthermore, instead of reusing FiD framework as in\n(Izacard et al., 2022), we show that our newly pro-\nposed fusion module is more effective than FiD\ndue to the gated mechanism, which is inspired by\nHighway Networks (Srivastava et al., 2015; Chai\net al., 2020), Gated Convolution (Dauphin et al.,\n2017) and Vision-Language Fusion(Alayrac et al.,\n2022).\n4.2 Massive multitask prompted training\nBased on the assumption that the reasonable zero-\nshot ability of large language models may come\nfrom implicit multitask learning during pretraining,\nrecent studies (Sanh et al., 2021; Wei et al., 2021;\nYe et al., 2021; Wang et al., 2022b) have demon-\nstrated that explicitly training a language model on\na mixture of diverse tasks can effectively improve\nits zero-shot performance on unseen tasks. In this\nwork, we extend T0’s multitask prompted train-\ning to a semi-parametric setting, where we fur-\nther augment the training and evaluation instances\nwith retrieved documents. Notably, our work is\ndistinguished from previous work ReCross (Lin\net al., 2022), which uses upstream training data\nfor augmentation, in twofold. First, we retrieve\ndocuments from a much larger task-agnostic cor-\npus instead of clean upstream training instances.\nSecond, in addition to directly concatenating the\naugmentation with the input just as FiD (Izac-\nard and Grave, 2020), we further propose a novel\nretrieval-augmentation fusion module to handle re-\ntrieval noise.\n4.3 Fusion of retrieved augmentations\nIn this work, the main challenge of designing the\nsemi-parametric language model architecture is\nhow to effectively leverage potentially noisy re-\ntrieved documents. Existing methods on incorporat-\ning external texts fall in two categories, direct con-\ncatenation (Lin et al., 2022; Brown et al., 2020; Liu\net al., 2021; Lewis et al., 2020; Wang et al., 2022a)\nand cross-attention (Izacard and Grave, 2020; Prab-\nhumoye et al., 2021; Borgeaud et al., 2021). How-\never, we find that prior work lacks an explicit de-\nsign for preventing the model from attending to\nnoisy augmentations. Inspired by recent visual lan-\nguage models (Alayrac et al., 2022; Yu et al., 2022;\nLi et al., 2022; Jiang et al., 2022), we find that\nwe can actually borrow ideas from vision-language\nfusion for text-text fusion. We identify two key\ndifferences from Flamingo architecture: first, we\nuse a much smaller encoder-decoder model that\nis jointly trained with the newly initialized layers\ninstead of frozen layers. Second, instead of insert-\ning the gated cross-attention module into a large\nfrozen language model (Hoffmann et al., 2022), we\nadd only one layer of gated cross-attention on top\nof the encoder to alleviate the need for extensive\npre-training.\n5 Conclusion\nIn this work, for the first time, we show that\nsemi-parametric language models have the po-\ntential to exhibit strong zero-shot task general-\nization ability by introducing Zemi. Through\nextensive analysis and ablation study, we fur-\nther demonstrate that the interplay of the pro-\nposed retrieval-augmentation fusion and the semi-\nparametric multitask training is essential towards\nZemi’s empirical success. Notably, our proposed\nZemiLARGE model outperforms T0-3B by 16%\nacross seven diverse evaluation tasks while being\n3.8x smaller in scale.\n6 Limitation\nIn Section 3.2, we show that our training mixture\nwith multiple-choice QA tasks, although small,\nis highly effective for multitask training. How-\never, it is still unclear why multiple-choice QA\ntasks are particularly effective. Identifying the key\nfactors towards positive or negative transfer from\ndifferent tasks in the multitask training mixture\nwould greatly help improve zero-shot task gener-\nalization. Future work includes investigating why\ncertain mixtures are more effective than others and\nexpanding the evaluation set to a wider range of\ntasks. Computation overhead is another noticeable\nlimitation of semi-parametric models which is dis-\ncussed in detail in Section 3.5. Moreover, future\nwork on developing more efficient and accurate\nretrieval methods for retrieving from large-scale\ntask-agnostic corpus can definitely improve semi-\nparametric language models.\nAcknowledgements\nWe would like to express our gratitude to the anony-\nmous reviewers for their insightful comments and\n3986\nsuggestions. We would also like to thank our col-\nleagues and fellow interns at Tencent AI Lab for\ntheir valuable internal discussions and feedback,\nas well as the students from Blender Lab at the\nUniversity of Illinois Urbana-Champaign for their\ninsightful feedback.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\narXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYekun Chai, Shuo Jin, and Xinwen Hou. 2020. High-\nway transformer: Self-gating enhanced self-attentive\nnetworks. arXiv preprint arXiv:2004.08178.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. arXiv preprint arXiv:1704.00051.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In International conference on\nmachine learning, pages 933–941. PMLR.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nElasticSearch. Elasticsearch.\nWikimedia Foundation. Wikimedia downloads.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a\ncontinuous cache. arXiv preprint arXiv:1612.04426.\nYuxian Gu, Pei Ke, Xiaoyan Zhu, and Minlie Huang.\n2022. Learning instructions with unlabeled data for\nzero-shot cross-task generalization. arXiv preprint\narXiv:2210.09175.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929–3938.\nPMLR.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\nIn arXiv:1909.00277v2.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste\nAlayrac, Carl Doersch, Catalin Ionescu, David Ding,\nSkanda Koppula, Daniel Zoran, Andrew Brock, Evan\nShelhamer, et al. 2021. Perceiver io: A general ar-\nchitecture for structured inputs & outputs. arXiv\npreprint arXiv:2107.14795.\nYunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi\nWang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, An-\nima Anandkumar, Yuke Zhu, and Linxi Fan. 2022.\nVima: General robot manipulation with multimodal\nprompts. arXiv preprint arXiv:2210.03094.\n3987\nMatt Gardner Johannes Welbl, Nelson F. Liu. 2017.\nCrowdsourcing multiple choice science questions.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. Qasc: A\ndataset for question answering via sentence compo-\nsition. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8082–8090.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web, 6(2):167–195.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. arXiv preprint arXiv:2201.12086.\nXin Li and Dan Roth. 2002. Learning question clas-\nsifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nWhite Paper. AI21 Labs.\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\nTian, and Xiang Ren. 2022. Unsupervised cross-\ntask generalization via retrieval augmentation. ArXiv,\nabs/2204.07937.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2021. Generated knowledge prompt-\ning for commonsense reasoning. arXiv preprint\narXiv:2110.08387.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. In Proceedings of the ACL.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2018. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. arXiv\npreprint arXiv:1808.09121.\nShrimai Prabhumoye, Kazuma Hashimoto, Yingbo\nZhou, Alan W Black, and Ruslan Salakhut-\ndinov. 2021. Focused attention improves\ndocument-grounded generation. arXiv preprint\narXiv:2104.12714.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. arXiv preprint arXiv:1906.02361.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\n3988\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at trec-3. Nist Special Publication Sp,\n109:109.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI spring symposium: logical formal-\nizations of commonsense reasoning, pages 90–95.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728.\nKornél Csernai Shankar Iyer, Nikhil Dandekar. 2017.\nFirst quora dataset release: Question pairs.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nRupesh Kumar Srivastava, Klaus Greff, and Jürgen\nSchmidhuber. 2015. Highway networks. arXiv\npreprint arXiv:1505.00387.\nHaitian Sun, Patrick Verga, Bhuwan Dhingra, Ruslan\nSalakhutdinov, and William W Cohen. 2021. Reason-\ning over virtual knowledge bases with open predicate\nrelations. In International Conference on Machine\nLearning, pages 9966–9977. PMLR.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,\nand Claire Cardie. 2019. Dream: A challenge data\nset and models for dialogue-based reading compre-\nhension. Transactions of the Association for Compu-\ntational Linguistics, 7:217–231.\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\nClark. 2019. Quartz: An open-domain dataset of\nqualitative relationship questions. arXiv preprint\narXiv:1909.03553.\nNiket Tandon, Bhavana Dalvi Mishra, Keisuke Sak-\naguchi, Antoine Bosselut, and Peter Clark. 2019.\nWiqa: A dataset for\" what if...\" reasoning over proce-\ndural text. arXiv preprint arXiv:1909.04739.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Cohen. 2021. Adaptable and interpretable\nneural memoryover symbolic knowledge. In Pro-\nceedings of the 2021 conference of the north ameri-\ncan chapter of the association for computational lin-\nguistics: human language technologies, pages 3678–\n3691.\nShuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu,\nSiqi Sun, Ruochen Xu, Chenguang Zhu, and Michael\nZeng. 2022a. Training data is more valuable than you\nthink: A simple and effective method by retrieving\nfrom training data. arXiv preprint arXiv:2203.08773.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al.\n2022b. Benchmarking generalization via in-context\ninstructions on 1,600+ language tasks. arXiv preprint\narXiv:2204.07705.\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nal-\nlapati, and Bing Xiang. 2019. Multi-passage\nbert: A globally normalized bert model for\nopen-domain question answering. arXiv preprint\narXiv:1908.08167.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. arXiv preprint arXiv:1902.01718.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiqa: A challenge dataset for open-domain ques-\ntion answering. In Proceedings of the 2015 con-\nference on empirical methods in natural language\nprocessing, pages 2013–2018.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossfit: A few-shot learning challenge for cross-\ntask generalization in nlp. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7163–7189.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics.\n3989\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\narXiv preprint arXiv:2205.12674.\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Prompt\nconsistency for zero-shot task generalization. arXiv\npreprint arXiv:2205.00049.\nA Qualitative analysis of the retrieved\ndocuments\nHere we visualize one good and one noisy example\nof the retrieved documents for each evaluation task.\nA full list of examples for each training and evalua-\ntion task can be found in the supplementary mate-\nrial under the “visualization” folder. As shown in\nFigure 4, 5, 6, 7, 9, 8, and 10, the retrieved augmen-\ntations can contain highly correlated information\nthat can be directly helpful for solving a certain\ntask, however, they can also be very noisy. As\nmentioned in Section 2.2, the retrieved documents\ncan also be inaccurately ranked, for example in\nFigure 11, we show that the 21th ranked retrieval\nresult can contain more correlated information than\nthe top ranked ones. Furthermore, as shown in the\nnoisy example of Figure 7, for some tasks such\nas sentiment analysis, even though the retrieved\ndocument is highly correlated with the input text,\ni.e., with a high BM25 score, the content can steer\nthe prediction into a wrong direction. These obser-\nvations motivate us to propose the augmentation\nfusion module with a gated mechanism.\nB Implementation details\nWe use T5-base and T5-large as backbone model\nfor ZemiBASE and ZemiLARGE, respectively. We\nfollow (Alayrac et al., 2022) to implement the per-\nceiver resampler and the gated cross-attention. For\nboth ZemiBASE and ZemiLARGE, unless otherwise\nspecified, we use one layer of gated cross-attention\nand one layer of perceiver resampler with a latent\nsize of 64. A comprehensive ablation study on the\nimpact of different aspects of our model design\nsuch as the Tanh Gate can be found in Section 3.4.\nAll models are trained on the same training mixture\nas mentioned in Section 3.1 for ten epochs with a\nbatch size of 32 and a learning rate of 1e-4. We\nreport results from the checkpoint that achieved the\nbest overall performance across all tasks. All ex-\nperiments are done on eight NVIDIA-V100 32GB\nGPUs.\nC Full list of tasks and templates\nFollowing T0 (Sanh et al., 2021), we use\ntasks from Hugginface Datasets (Lhoest et al.,\n2021) and templates from PromptSource (Bach\net al., 2022) marked as “original task\" and with\n“choices_in_prompt\". Specifically, for tasks in\nthe training mixture, we randomly sample two\n3990\ntemplates per task for semi-parametric multitask\nprompted training. For tasks in the held-out evalu-\nation mixture, we use all available templates. Ta-\nble 4, and 5 shows the full list of templates we\nused for each task during multitask training and\nzero-shot evaluation.\nD Retrieval query key for each task\nIn order to retrieve most relevant documents for\neach instance, we specify a certain field for each\ndataset which will be served as the query to the\nretrieval system. For example, for most multiple-\nchoice QA tasks, we use the “question” string as\nour query. Table 6 shows a full list of field names\nwe use as retrieval query keys for each dataset.\nNote that the field name shown in the table is what\nappears to be in the corresponding Huggingface\nDataset (Lhoest et al., 2021).\nE Broader impact\nOne major benefit of developing powerful semi-\nparametric language models is that we can reduce\nthe negative environmental impact from training\nhuge parametric models. However, since the back-\nbone language model is pretrained on massive\ninternet-scale text data, there might be unexpected\noutput that can have potential negative impact on\nthe society, such as bias against people of a certain\ngender, race or sexuality. We are fully aware of the\nrisks of potential misuses and will actively work\nwith the community to improve the responsibility\nof large NLP models.\n3991\nMixture Task Template Name\nSemi-T0 Training\ncos_e/v1.11 question_option_description_text\ndescription_question_option_id\ncosmos_qa context_description_question_answer_id\ndescription_context_question_answer_text\ndream baseline\nread_the_following_conversation_and_answer_the_question\nqasc qa_with_separated_facts_1\nqa_with_separated_facts_4\nquartz answer_question_below\nread_passage_below_choose\nsciq Multiple Choice\nMultiple Choice Question First\nsocial_i_qa Show choices and generate answer\nShow choices and generate index\nwiqa effect_with_string_answer\neffect_with_label_answer\nSemi-T0+ Training\nwiki_qa Decide_good_answer\nfound_on_google\ntrec what_category_best_describe\ntrec1\nsuper_glue/copa more likely\nbest_option\nrotten_tomatoes Sentiment with choices\nReviewer Opinion bad good choices\nTable 4: PromptSource template names used for each task (Part1).\n3992\nMixture Task Template Name\nSemi-T0 Evaluation\nopenbookqa/main choose_an_answer_with_options\nwhich_correct\npick_using_id\nchoices\nonly_options\nwhich_correct_inverse\npick_answer_with_options\npiqa what_is_the_correct_ending\npick_correct_choice_with_choice_given_before_goal\npick_correct_choice_index\nfinish_sentence_with_correct_choice\nchoose the most appropriate solution\nrotten_tomatoes Reviewer Opinion bad good choices\nSentiment with choices\nsuper_glue/cb can we infer\nbased on the previous passage\nclaim true/false/inconclusive\ndoes it follow that\njustified in saying\nalways/sometimes/never\nGPT-3 style\nconsider always/sometimes/never\nguaranteed true\nmust be true\nguaranteed/possible/impossible\ndoes this imply\nMNLI crowdsource\nshould assume\ntake the following as truth\nsuper_glue/copa exercise\n. . . What could happen next, C1 or C2?\ni_am_hesitating\nplausible_alternatives\nC1 or C2? premise, so/because. . .\n. . . As a result, C1 or C2?\nbest_option\n. . . which may be caused by\nmore likely\ncause_effect\n. . . why? C1 or C2\nchoose\nsuper_glue/wic question-context-meaning-with-label\ngrammar_homework\naffirmation_true_or_false\nsame_sense\nGPT-3-prompt-with-label\npolysemous\nhellaswag complete_first_then\nRandomized prompts template\nPredict ending with hint\nif_begins_how_continues\nTable 5: PromptSource template names used for each task (Part2).\n3993\nTask Query Key\ncos_e/v1.11 question\ncosmos_qa question\ndream question\nqasc question\nquartz question\nsciq question\nsocial_i_qa context\nwiqa question_stem\nopenbookqa/main question_stem\npiqa goal\nrotten_tomatoes text\nsuper_glue/cb hypothesis\nsuper_glue/copa premise\nsuper_glue/wic sentence1\nhellaswag ctx\nwiki_qa question\ntrec text\nTable 6: Retrieval query key used for each task.\n3994\nGood Retrieval Example\n======= Instance Index 6063 =========\nInput Text: If a description of a situation begins like this: [header]\nHow to set macgo mac blu ray player as default player [title]\nDownload mac blu-ray menu player and install it at once. [step]\nThere will be watermark on your screen if you play blu-ray with the\ntrial version. Only 39.95 dollars for the full version of mac blu-ray\nmenu player for now, please buy mac blu-ray player with\ndiscount.... Then how\ndoes it continue? \nEnding 1: [title] Click \" check file associations \" under \" tools \". [title]\nClick and macgo mac blu-ray player will be your default player.\nEnding 2: [title] Choose your video size and port size from the\ndropdown menu at the top of mac blu-ray menu. [step] Once you\nhave downloaded the blu-ray menu player and installed it, you have\nto choose your video size and port size.\nEnding 3: [title] Run the make app and then the itunes installer.\n[title] Determine the output type for each file in your mac blu-ray\nplayer.\nEnding 4: [title] Uncheck the sidebar at the bottom of \" applications\n\". [step] These are the files that are currently currently on your mac\nblu-ray player.\nTarget Text: Ending 1\n#### Retrieved Documents ####\nRank: 0\nScore: 136.6041\nRetrieved Text: Macgo Mac Blu-ray Player has added itself Auto\nPlay function, which means when you insert a disc into your Blu-ray\ndrive, the player will automatically start and play. In order to make\nthis whole process smoother, you'd better set Mac Blu-ray Player\nas default player on your Mac. Now I'll tell you how to do it.\nAfter installing Mac Blu-ray Player, you can go to \"Launchpad\" and\nclick on its icon to launch the program. The simplified main\ninterface will reduce certain misoperations. You can see a menu at\nthe top of the interface. Click \"Check File Associations\" under\n\"Tools\".\nThen it will come up with a pop up window. You can choose some\nmedia formats which you want to play with Macgo Blu-ray player,\nthen click \"Make Mac Blu-ray player my default player\".\nClick \"OK\" to continue. Then Macgo Mac Blu-ray Player will be your\ndefault player.\nAfter you set Macgo Mac Blu-ray Player as your default player, you\nalso need to enable Auto Play function to freely enjoy Blu-ray this\nplayer.\nOpen \"Preferences\" under \"Mac Blu-ray Player\".Open \"Playback\"\nand tick under \"Auto play when you insert a disc\", and then click\n\"OK\".\nInsert a Blu-ray disc into the drive and wait for the program\nautomatically start and display the Blu-ray Menu. You can make\nsome adjustments there or directly click \"Play Movie\" to enjoy some\nBlu-ray time.\n======= Instance Index 122 =========\nInput Text: If a description of a situation begins like this: A group\nof people are in a house. a man... Then how\ndoes it continue? \nEnding 1: is holding cored soap in his hand as he washes with a\nbottle.\nEnding 2: is mopping the floor with a mop.\nEnding 3: is shown wearing skis as he talks about areas he will\nlike to ski on.\nEnding 4: uses a paintball gun on his child.\nTarget Text: Ending 2\n#### Retrieved Documents ####\nRank: 0\nScore: 17.592176\nRetrieved Text: #52704883 - Red lanterns, oriental charm, the\nSpring Festival atmosphere.\n#35618548 - Silhouette of a man Happy successful raising arms\nto the sky..\n#93113276 - Backlighting portrait of a joyful mother raising her\nbaby outdoors..\n#108745447 - Backlighting portrait of a joyful mother raising her\nbaby outdoors..\n#37541508 - Group of cheerleaders performing outdoors -\nConcept of cheerleading..\n#108747918 - Back view of young backlit man looking into the\ndistance on illuminated..\n#38536931 - Working man walking near airplane wing at the\nterminal gate of..\n#73301104 - Group of urban friends walking in city skate park with\nbacklighting..\n#76682984 - People silhouettes putting puzzle pieces together on\ncity background..\n#77013901 - People silhouettes putting puzzle pieces together on\nabstract..\n#104666805 - Stylish light gray kitchen interior with modern\ncabinets with..\n#108748125 - Back view of young backlit man looking into the\ndistance on illuminated..\n#86815910 - Best friends taking selfie outdoors with backlighting -\nHappy..\n#86815909 - Best friends taking selfie outdoors with backlighting -\nHappy..\n#117963685 - Back view backlighting silhouette of a man alone on\na swing looking..\n#86815911 - Best friends taking selfie outdoors with backlighting -\nHappy..\n#118172724 - Back view backlighting silhouette of a man sitting\non swing alone..\n#96363446...\nNoisy Retrieval Example\nFigure 4: Example of retrieved documents on HellaSwag.\n3995\nGood Retrieval Example\n======= Instance Index 285 ========= \nInput Text: Decomposition occurs when a decomposer recycles\nnutrients from dead organisms back to the soil by eating them;\nwhat is an example of this?\nWhich is the correct answer?\n- flies laying eggs on a body\n- worms devouring a corpse\n- wet leaves denigrating in a pile\n- slugs digging through mulch\nTarget Text: worms devouring a corpse\n#### Retrieved Documents ####\nRank: 0\nScore: 77.088646\nRetrieved Text: In most terrestrial ecosystems the bulk of nutrient\ncycling occurs in the topmost layers of soil. The main sources of\nthe nutrient inputs to these soil layers comes from weathering,\nrainfall, fertilizers, atmospheric fallout, and organisms. Organism\nadd nutrient matter via excreted wastes, shed tissues, and from the\ndecomposition of their tissues when they die. Under most\nconditions, plants are the greatest single source of nutrients to\nsoils. Plants not only supply nutrients released by organic\ndecomposition of shed tissues and dead body parts, but also\nsubstances carried in from the plant leaves when water flows over\nthem (foliar leaching). Losses or outputs of nutrients within\necosystems are by leaching, erosion, gaseous loss (like\ndenitrification), and plant root uptake for growth purposes. Within\nthe soil, nutrients are found attached to the surface of soil particles\nby chemical bonds, stored within the chemical structure of dead\norganic matter, or in chemical compounds.\nOrganic matter decomposition is the main process that recycles\nnutrients back into the soil. Decomposition of organic matter begins\nwith large soil organisms like earthworms, arthropods (ants,\nbeetles, and termites), and gastropods (slugs and snails). These\norganisms breakdown the organic matter into smaller pieces which\ncan be decomposed by smaller organisms like fungi and\nheterotrophic bacteria (Figure 9q-1).\n======= Instance Index 361 =========\nInput Text: A scale can\nWhich is the correct answer?\n- give an estimate of a dog's age\n- measure how long a dog is\n- let you know if the dog needs to lose a few pounds\n- make an educated guess about a dog's breed\nTarget Text: let you know if the dog needs to lose a few pounds\n#### Retrieved Documents ####\nRank: 0\nScore: 9.894971\nRetrieved Text: What is the abbreviation for Vertical Scale\nMeasurement?\nA: What does Y-SCALE stand for?\nY-SCALE stands for \"Vertical Scale Measurement\".\nA: How to abbreviate \"Vertical Scale Measurement\"?\n\"Vertical Scale Measurement\" can be abbreviated as Y-SCALE.\nA: What is the meaning of Y-SCALE abbreviation?\nThe meaning of Y-SCALE abbreviation is \"Vertical Scale\nMeasurement\".\nA: What is Y-SCALE abbreviation?\nOne of the definitions of Y-SCALE is \"Vertical Scale\nMeasurement\".\nA: What does Y-SCALE mean?\nY-SCALE as abbreviation means \"Vertical Scale Measurement\".\nA: What is shorthand of Vertical Scale Measurement?\nThe most common shorthand of \"Vertical Scale Measurement\" is\nY-SCALE.\nYou can also look at abbreviations and acronyms with word Y-\nSCALE in term.\nNoisy Retrieval Example\nFigure 5: Example of retrieved documents on OpenbookQA.\n3996\nGood Retrieval Example\n======= Instance Index 653 =========\nInput Text: Sentence: To make a graham cracker crust, to turn\ngraham crackers to crumbs, you can\nChoice 1: Run the graham crackers through a food processor\nChoice 2: Run the graham crackers through a cheese grater\nWhat is the index of the correct choice for ending for the sentence?\nAnswer:\nTarget Text: 1\n#### Retrieved Documents ####\nRank: 0\nScore: 82.00445\nRetrieved Text: A graham cracker crust recipe for baked pies and\nno bake pies! We could also title this post The Anatomy of a\nGraham Cracker Crust. In other words, we’re making our own\ngraham cracker crust from scratch today and it’ll be the best\ngraham cracker crust you’ve ever had!\nYou can use it for no-bake pies or you can bake it first. That’s a\nsummer #win if you ask me!\nGraham cracker crust is one of my favorite pie crusts. I don’t think I\ncan choose an absolute favorite, because I love all of them too\nmuch. But a good graham cracker crust is a must have in your\nbaking arsenal. So many pies can be made to pair with the graham\ncracker flavor because it’s so versatile. You can fill it with creamy\ns’mores chocolate pudding or even an easy blueberry-lemon\ndessert filling.\nI think everyone needs a from-scratch graham cracker crust recipe\nin their arsenal. What if you want a pie right now and can’t get to\nthe store? And, let’s face it. As good as those store-bought crusts\nare, they sorta taste like the aluminum foil pie tin, right? Or is it just\nme?\nSo today, I’m showing you my favorite from-scratch homemade\ngraham cracker pie crust recipe. And this is even more perfect\nbecause you can use it for recipes that call for baking the crust OR\nyou can use it no-bake.\nBecause when it’s 106° like it has been this week in Sacramento,\nthe last thing you want to do is turn on your oven. A\n======= Instance Index 1111 =========\nInput Text: Sentence: water\nChoice 1: can drown a man \nChoice 2: can drown a fish \nWhat is the index of the correct choice for ending for the\nsentence?\nAnswer:\nTarget Text: 1\n#### Retrieved Documents ####\nRank: 0\nScore: 5.867839\nRetrieved Text: best water pitcher filter lead water filter pitcher\nwater filter lead best water filter for lead removal core pitcher lead\nreduction water pitcher filter 3 pk water filter aquagear water filter\npitche.\nbest water pitcher filter water pitchers that remove lead water filter\npitcher that removes fluoride fluoride water filter pitcher plus water\npitchers water pitcher filter fluoride.\nbest water pitcher filter water filter pitcher water pitcher best water\nfilter pitchers marina water filter pitcher zero water pur water filter\npitcher target.\nbest water pitcher filter water filtration pitcher reviews water\nfiltration pitchers comparison carafe water filters target water filter\npitcher reviews.\nbest water pitcher filter water filter pitcher reviews best water\npitcher filter best water filter pitchers water filter pitcher water filter\npitcher reviews consumer reports.\nbest water pitcher filter our three picks for best water filter pitcher\nwater pitcher filter cartridge.\nbest water pitcher filter the best water filter pitcher water filter\npitchers best water pitchers best water filter pitchers best water\nfiltration pitcher zero water pitcher filter replacement instruc.\nbest water pitcher filter filter pitcher target best water pitchers\ndoes zero water pitcher filter fluoride.\nbest water pitcher filter water filter best water pitcher filter water \n...\nNoisy Retrieval Example\nFigure 6: Example of retrieved documents on Piqa.\n3997\nGood Retrieval Example\n======= Instance Index 575 =========\nInput Text: every joke is repeated at least four times . every joke is\nrepeated at least four times . every joke is repeated at least--\nannoying , isn't it ? Did the reviewer find this movie good or bad?\nTarget Text: bad\n#### Retrieved Documents ####\nRank: 2\nScore: 52.421543\nRetrieved Text: Just very poor riddles in bad English and repeated\n10 times each!\nSeveral misspelled words (pretty unprofessional for a published\n\"app book\"). Also, some of the riddles are a bit morbid & makes me\nwonder what's going on in the mind of the one who came up with\nthem..?! Not very challenging or logical for my taste.\nThis book has SOME useful riddles, but most of them repeat and\ndon't make sense. Almost every riddle is misspelled and poorly\nwritten. Don't read this, find another book because this obviously\nlooked like a 5th grader typed it from a cellphone.\nWhy did you put multiple of the exact same joke like a million\ntimes?!\nThe title says \"8000+ riddles\" but it doesn't say that all the riddles\nwere DIFFERENT. It repeats the same riddles for pages after\npages. Also, some riddles don't even make sense! And so much\nmisspelling! Please update this and correct some misspelling and\ninclude more riddles so I'll rate 4 stars.\n======= Instance Index 703 =========\nInput Text: paul bettany is good at being the ultra-violent\ngangster wannabe , but the movie is certainly not number 1 . Did\nthe reviewer find this movie good or bad?\nTarget Text: bad\n#### Retrieved Documents ####\nRank: 0\nScore: 58.04047\nRetrieved Text: If you like retro crime movies this is a good one,\nits ultra-violence and unrelentingly crude language not\nwithstanding. Much of the credit goes to Paul McGuigan’s stylish\ndirection which is so good that it makes you wonder why there are\nso many pedestrian films made. A good of credit should also go to\nJohnny Ferguson’s amped-up screenplay and the fine\nperformances by the three leads, Malcolm McDowell, David\nThewlis and Paul Bettany. Although McDowell gets top billing this\nis really Paul Bettany’s film whilst David Thewlis gives a solid and\nunusually restrained performance that counterbalances the\nfamiliarly thuggish ambiance.\nThe film opens potently with a Reservoir Dogs-like round table\ndiscussion amongst a troupe of aging East End crims recalling\npast times. The subject of Freddy Mays (Thewlis) comes up and\nthis sets Malcolm McDowell’s character referred to in the credits\nas Gangster 55 to recalling his rise in Mays’ Kray-era gang. We\nthen go into flash back and follow his story with Paul Bettany\nplaying the McDowell character. Quite a few people will have\ndifficultly accepting the casting of the handsome and refined\nlooking Bettany playing a hard man, let alone McDowell's younger\nself, but he burns with the icily ambitious and sociopathic energy\nthat the character requires. Set in the mid-60s, the production\ndesign is a treat, McGuigan’s direction dynamic and the use of\nincidental music excellent.\nThe last act returns us to the starting point and now we\nunderstand why the name of Freddie Mays has derailed Gangster\n55. The film looses some of its\nNoisy Retrieval Example\nFigure 7: Example of retrieved documents on Rotten Tomatoes.\n3998\nGood Retrieval Example\n======= Instance Index 10 =========\nInput Text: The bowling ball knocked over the bowling pins. \nWhat's the best option?\n- The man rolled the bowling ball down the alley.\n- The man dropped the bowling ball on his foot.\nWe are looking for  a cause\nTarget Text: The man rolled the bowling ball down the alley.\n#### Retrieved Documents ####\nRank: 0\nScore: 54.388268\nRetrieved Text: There are different types Chesterfield Bowling\nClubs in Derbyshire.\nTen pin bowling is the most fashionable form of bowling. In ten pin\nbowling, matches consist of each player bowling a game. Each\ngame is divided into ten frames. A frame allows a bowler 2 chances\nto bang down all 10 pins. The number of pins knocked over in each\nframe is recorded, a running total is made beneath the specific\nframe score as each frame goes on, and the player with the highest\nscore in his/her game wins the match. Scores can be greater than\nthe actual number of pins knocked over if strikes or spares are\nbowled. A strike is scored when a player knocks down all pins on\nthe first roll in the frame. Rather than a score of just 10 for the\nframe, the player's score will be 10 plus the total pins knocked\ndown on the next two rolls in the next frame(s). A spare is scored\nwhen all pins are knocked down using the second roll in the frame.\nThe player's score for that frame will be 10 plus the number of pins\nknocked down on the first roll in the next frame. A player who rolls\na spare or strike in the last frame is given one (if it was a spare in\nthe previous frame) or two more rolls (if it was a strike in the\nprevious frame) to score additional points.\nAs standard in most sports there are colloquialisms for various\noccurences in a game. Two consecutive strikes is acknowledged\n======= Instance Index 2 =========\nInput Text: The woman retired. \nWhat's the best option?\n- She received her pension.\n- She paid off her mortgage.\nWe are looking for  an effect\nTarget Text: She received her pension.\n#### Retrieved Documents ####\nRank: 0\nScore: 15.455591\nRetrieved Text: What a fun and unique Valentine’s gift!!!\nCategories: Retirement, Woman, Man, Book.\nCategories: Funny Gift, Retirement, Woman, Man, Book.\nOur Name is Mud “Retired” Cuppa Doodle Porcelain Mug, 16 oz.\nCategories: Funny Gift, Retirement, Woman, Decorative Items.\nOur Name is Mud “Retirement Plan” Stoneware Mug, 16 oz.\nNoisy Retrieval Example\nFigure 8: Example of retrieved documents on COPA.\n3999\nGood Retrieval Example\n======= Instance Index 8 =========\nInput Text: Given that A: And I haven't quite figured that out, if they\nfigure they have got it won or if there's no real hurry because the\nfirst three quarters or, uh, uh, if something happens that that\nadrenalin starts flowing. They say, hey, we got to do something\nnow. And then start playing the game the way the game should be\nplayed toward the last few minutes. B: Yeah. A: So, I don't know I'm\nlooking for a good year. I guess we're always looking for a good\nyear. B: So, obviously though, do you think they're going to do\nanything in the playoffs to make it to the Super Bowl this year\nTherefore, it must be true that \"they're going to do anything in the\nplayoffs to make it to the Super Bowl this year\"? Yes, no, or\nmaybe?\nTarget Text: Maybe\n#### Retrieved Documents ####\nRank: 0\nScore: 41.988216\nRetrieved Text: Two-time super bowl champion and CNN Sport\ncontributor Hines Ward shares his Week 9 takeaways with CNN's\nJill Martin.\nWe're at the halfway point, and you start to see teams separate the\ncontenders from the pretenders. You really see what teams are\nmade of. This is a crucial month for a lot of teams in the NFL.\nLet's start with the NFC South, where the Panthers and Saints\nneed our attention.\nThe Carolina Panthers -- I don't think anyone expected them to\nhave the year that they're having.\nCam Newton is looking like he's back to his MVP form, from back in\n2015. What they're doing with running back Christian McCaffrey I\njust think is amazing. It's showing his versatility both running and\ncatching the ball.\nThey're only one game behind the New Orleans Saints, and they\nhave key matchups at the end of the year. In the last three weeks\nof the season, they play each other twice. Right now, it looks like it\nshould be for the division.\nMeanwhile, the Saints just knocked off the Rams. What, if anything\ndoes that performance show you?\nWell, it's a tough place to play. I think, right now, it's really a two-\nteam race to try to get that home field advantage for the playoffs.\nI've played in New Orleans. I've been there. I know what their fans\nare like. It's one of the toughest places to play. It's loud. They get\nrowdy, and they love their Saints.\nDefinitely having Drew Brees playing at home in the playoffs helps\nthe Saints' chances of making it to the\n======= Instance Index 7 =========\nInput Text: Given that It grew bigger with incredible speed, she\nwas whizzing towards it. She must slow down or she 'd miss it.\nShe took her foot off the accelerator and put it on the brake and\nas the car slowed she could see now that it was a child a toddler\nwith a red woolly hat on. Therefore, it must be true that \"it was a\nchild\"? Yes, no, or maybe?\nTarget Text: Yes\n#### Retrieved Documents ####\nRank: 0\nScore: 10.499066\nRetrieved Text: The Plan Has Been Executed – My Grace Is..\nThis is my love letter to you son. Forever you will remain a child\ndear to me my daughter. I know you have read and heard that I\nhave a plan to prosper you, to give you a hope and a future.\nChild oh my child, yes I had a plan for you back then, back, back\nthen. It was all true. But here is a thing today for you grab hold of\nmy child. To master and rejoice in. The plan has been executed.\nThe plan is sealed and delivered. My child, yes I had a plan for\nyou, a plan for you to live a happy life. To live a joyous life. My\nplan was great for you my child. My plan was great for you my\nprecious child.\nLike every other parent, I had a plan for you my child. The plan\nwas drawn down. Well designed, well traced and well set out. Just\nlike a cartoonist would first draw before he brings the characters\nhe has drawn to motion, I too, did that. I too my son had a plan in\nmind for you. I could not put you on earth and not have a plan at\nall. I did it and set it up my child. Worry not my son, the plan is\nexecuted. For long you heard the words that I had a plan for you,\nmy precious child, please know this, the plan has been executed.\nThe plan has come to life.\nMy plan\nNoisy Retrieval Example\nFigure 9: Example of retrieved documents on CB.\n4000\nGood Retrieval Example\n======= Instance Index 517 =========\nInput Text: The word \"knuckleball\" has multiple meanings. Does it\nhave the same meaning in sentences 1 and 2? Yes or no?\nSentence 1: Even the pitcher doesn't know where his knuckleball is\ngoing.\nSentence 2: Boston Red Sox pitcher Tim Wakefield is best known\nfor his use of the knuckleball.\nTarget Text: Yes\n#### Retrieved Documents ####\nRank: 0\nScore: 70.62252\nRetrieved Text: Tonight at approximately 5PM, Tim Wakefield will\nannounce his retirement from baseball at the age of 45. \"Wake\" will\nfinish his 19 year career with 200 wins, a feat he reached this past\nSeptember.\nHis career accomplishments also include 2 World Series rings, an\nAll-Star berth in 2009, 1995 AL Comeback Player of the Year, and\n2010 Roberto Clemente Award winner, an honor he was nominated\nfor eight times.\nTo Sox fans however, the knuckleballer will be remembered for\nbeing a world class team player who's sacrifices as a pitcher and\nan athlete in general are unparalleled. He was constantly asked to\nchange his roles from front line starter, to middle reliever, and even\na successful stint as a closer. This was something that most fans\nthought was easy since his style allowed it, but Tim has come\nforward recently as saying it was extremely difficult and\nuncomfortable.\nIn my mind, all you need to know about Wake happened in 2007.\nAfter finishing as one of the more reliable starters for Boston with a\n17-12 record that season, he volunteered his roster spot in the\nWorld Series for a healthier rookie, Jon Lester, who won the\nclinching game against the Rockies. Name the players who have\ndone that in the history of professional sports and you will\nundoubtedly come up with a very short list.\nAfter being drafted as a first baseman by the Pirates in 1988, a\nscout told Wake that he would never make it above the AA level as\na position player. Doing \"anything he could to\n======= Instance Index 406 =========\nInput Text: The word \"state\" has multiple meanings. Does it have\nthe same meaning in sentences 1 and 2? Yes or no?\nSentence 1: State your name.\nSentence 2: State your opinion.\nTarget Text: Yes\n#### Retrieved Documents ####\nRank: 0\nScore: 13.911013\nRetrieved Text: The Washington attorney general issues formal\npublished opinions in response to requests by the heads of state\nagencies, state legislators, and county prosecuting attorneys.\nWhen it appears that individuals outside the attorney general's\noffice have information or expertise that will assist in the\npreparation of a particular opinion, a summary of that opinion\nrequest will be published in the state register. If you are interested\nin commenting on a request listed in this volume of the register,\nyou should notify the attorney general's office of your interest by\nJanuary 22, 2014. This is not the due date by which comments\nmust be received. However, if you do not notify the attorney\ngeneral's office of your interest in commenting on an opinion\nrequest by this date, the opinion may be issued before your\ncomments have been received. You may notify the attorney\ngeneral's office of your intention to comment by writing to the\nOffice of the Attorney General, Solicitor General Division,\nAttention Jeffrey T. Even, Deputy Solicitor General, P.O. Box\n40100, Olympia, WA 98504-0100, or by e-mail\njeff.even@atg.wa.gov. When you notify the office of your intention\nto comment, you may be provided with a copy of the opinion\nrequest in which you are interested; information about the\nattorney general's opinion process; information on how to submit\nyour comments; and a due date by which your comments must be\nreceived to ensure that they are fully considered.\n1. Is an individual who has been convicted of aggravated assault,\nor other serious offenses, in a foreign country prohibited from\npossessing\nNoisy Retrieval Example\nFigure 10: Example of retrieved documents on WiC.\n4001\n======= Instance Index 0 =========\nInput Text: Sentence: How do I ready a guinea pig cage for it's new occupants?\nChoice 1: Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips, you will also need to supply it with\na water bottle and a food dish.\nChoice 2: Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans material, you will also need to supply it\nwith a water bottle and a food dish.\nWhat is the index of the correct choice for ending for the sentence?\nAnswer:\nTarget Text: 1\n#### Retrieved Documents ####\nRank: 1 \nScore: 46.520477\nRetrieved Text: how do I find neat names?\nHow to go about finding a vet?\nguinea pig dali apparently on mend, again?\nhamster cage Hammock pattern. . .\nhamster cage Secure your cage doors!\near infection, ear infections, inner ear infection degu sick am having rant!!!!\nDo males hump each other?...\n...\n########\nRank: 10\nScore: 41.217316\nRetrieved Text: Contact Alittlebitiffy Animal Sanctuary at Alittlebitiffy Animal Rescue to express your interest.\nAnother successful adoption - amazing work Alittlebitiffy Animal Rescue!\nMore successful adoptions - amazing work Alittlebitiffy Animal Rescue! ...\n...\n########\nRank: 21\nScore: 39.48997\nRetrieved Text: Keeping your little furry friend healthy and happy should be a priority for any owner and, along with providing the right food\nfor guinea pigs, finding an appropriate cage for them should be at the top of your priority list. Although, as you can see in this post here,\nthere are numerous options on the market when it comes to commercially available guinea pig cages, some owners have opted towards a\nmore do-it-yourself approach.\nMany guinea pig parents complain that the regular pet store-sized cages are nothing but ‘glorified litter boxes’ and therefore are looking to\nimprove the well-being of their cavies by making them a healthy and large-enough living enclosure, rather than buying one.\nIf you are one of those owners, this article will guide you through what you need to know before you start making a DIY cage for your\nguinea pig and what options you have when it comes to materials, design and features....\nExample of Inaccurate Ranking\nFigure 11: Example of the inaccurate ranking of the retrieval. Here we show the ranked retrieved documents for\ninstance 0 in Piqa. We can see that the 21th ranked document is more correlated than many of the higher ranked\nones, such as rank 1 and rank 10.\n4002\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nAppendix F Broader Impact\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nFootnote 1. See supplementary material\n□\u0013 B1. Did you cite the creators of artifacts you used?\nReference. All datasets and models used in this paper are cited.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nReference and footnotes. All the datasets and models used and created by this work are publicly\navaliable for research purpose.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 3.1\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 3.1\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSee README of the supplementary code.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3.1\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3, Appendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4003\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3, Appendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3, Appendix B, C, D\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4004",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8513766527175903
    },
    {
      "name": "Parametric statistics",
      "score": 0.7631150484085083
    },
    {
      "name": "Language model",
      "score": 0.6317804455757141
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5920248031616211
    },
    {
      "name": "Task (project management)",
      "score": 0.5870895981788635
    },
    {
      "name": "Generalization",
      "score": 0.5660475492477417
    },
    {
      "name": "Parametric model",
      "score": 0.5238718390464783
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.466127872467041
    },
    {
      "name": "Key (lock)",
      "score": 0.44965860247612
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4170882999897003
    },
    {
      "name": "Natural language processing",
      "score": 0.3733067214488983
    },
    {
      "name": "Machine learning",
      "score": 0.3649090528488159
    },
    {
      "name": "Linguistics",
      "score": 0.07385161519050598
    },
    {
      "name": "Mathematics",
      "score": 0.06324917078018188
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 6
}