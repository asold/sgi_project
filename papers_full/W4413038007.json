{
  "title": "High-level visual representations in the human brain are aligned with large language models",
  "url": "https://openalex.org/W4413038007",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4297163142",
      "name": "Doerig, Adrien",
      "affiliations": [
        "Osnabrück University",
        "Freie Universität Berlin",
        "Bernstein Center for Computational Neuroscience Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4226745127",
      "name": "Kietzmann, Tim C.",
      "affiliations": [
        "Osnabrück University"
      ]
    },
    {
      "id": "https://openalex.org/A4295293482",
      "name": "Allen, Emily",
      "affiliations": [
        "Resonance Research (United States)",
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2753794867",
      "name": "Wu, Yihan",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A2993949602",
      "name": "Naselaris, Thomas",
      "affiliations": [
        "University of Minnesota"
      ]
    },
    {
      "id": "https://openalex.org/A3186373198",
      "name": "Kay, Kendrick",
      "affiliations": [
        "University of Minnesota",
        "Resonance Research (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4297163148",
      "name": "Charest, Ian",
      "affiliations": [
        "Université de Montréal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1988545511",
    "https://openalex.org/W2115814872",
    "https://openalex.org/W3029307787",
    "https://openalex.org/W2123544472",
    "https://openalex.org/W2921125453",
    "https://openalex.org/W2176287621",
    "https://openalex.org/W2887972576",
    "https://openalex.org/W2162950292",
    "https://openalex.org/W4309167184",
    "https://openalex.org/W4378782804",
    "https://openalex.org/W2978368159",
    "https://openalex.org/W2058616551",
    "https://openalex.org/W2040036684",
    "https://openalex.org/W1715013381",
    "https://openalex.org/W2732001149",
    "https://openalex.org/W2143198623",
    "https://openalex.org/W3174137462",
    "https://openalex.org/W2060070867",
    "https://openalex.org/W2104707550",
    "https://openalex.org/W4213439066",
    "https://openalex.org/W3030316552",
    "https://openalex.org/W2134927309",
    "https://openalex.org/W2928869317",
    "https://openalex.org/W2947233752",
    "https://openalex.org/W3120199164",
    "https://openalex.org/W2950747681",
    "https://openalex.org/W4388861637",
    "https://openalex.org/W1619178479",
    "https://openalex.org/W2125935651",
    "https://openalex.org/W2106514696",
    "https://openalex.org/W2152913445",
    "https://openalex.org/W2074299407",
    "https://openalex.org/W2789332809",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W4304701298",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4286336838",
    "https://openalex.org/W4225832925",
    "https://openalex.org/W4200613736",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2052091366",
    "https://openalex.org/W2153633111",
    "https://openalex.org/W2057307785",
    "https://openalex.org/W3110534329",
    "https://openalex.org/W4312181736",
    "https://openalex.org/W2007226897",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W4300613830",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4394664678",
    "https://openalex.org/W3129717984",
    "https://openalex.org/W2611715315",
    "https://openalex.org/W4210283483",
    "https://openalex.org/W3036076479",
    "https://openalex.org/W3090234134",
    "https://openalex.org/W3100481436",
    "https://openalex.org/W2286279415",
    "https://openalex.org/W4403924340",
    "https://openalex.org/W4404985823",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2082627290",
    "https://openalex.org/W2146656425",
    "https://openalex.org/W2086593994",
    "https://openalex.org/W4220930708",
    "https://openalex.org/W2892147425",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W2964185501",
    "https://openalex.org/W2963703197",
    "https://openalex.org/W3201124393",
    "https://openalex.org/W4313893052",
    "https://openalex.org/W2411701945",
    "https://openalex.org/W1991289220",
    "https://openalex.org/W2108489623",
    "https://openalex.org/W2315537494",
    "https://openalex.org/W1986209830",
    "https://openalex.org/W4321440980",
    "https://openalex.org/W2088112754",
    "https://openalex.org/W2147328078",
    "https://openalex.org/W4402372802",
    "https://openalex.org/W2951065015",
    "https://openalex.org/W4389397315",
    "https://openalex.org/W4408279984",
    "https://openalex.org/W3210615854",
    "https://openalex.org/W4388625659",
    "https://openalex.org/W4363678535",
    "https://openalex.org/W2008193883",
    "https://openalex.org/W4393343917",
    "https://openalex.org/W3210923133",
    "https://openalex.org/W4401251180",
    "https://openalex.org/W2166206801",
    "https://openalex.org/W2048631316",
    "https://openalex.org/W2077477064",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W2979357328",
    "https://openalex.org/W2891938284",
    "https://openalex.org/W3200096343",
    "https://openalex.org/W6893250477",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W3179888904",
    "https://openalex.org/W3119948327",
    "https://openalex.org/W4377866580"
  ],
  "abstract": null,
  "full_text": "Nature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1220\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-025-01072-0\nHigh-level visual representations in  \nthe human brain are aligned with large \nlanguage models\n \nAdrien Doerig    1,2,3,9, Tim C. Kietzmann    2,9, Emily Allen    4,5, Yihan Wu6, \nThomas Naselaris7, Kendrick Kay4,10 & Ian Charest    8,10 \nThe human brain extracts complex information from visual inputs, \nincluding objects, their spatial and semantic interrelations, and their \ninteractions with the environment. However, a quantitative approach \nfor studying this information remains elusive. Here we test whether the \ncontextual information encoded in large language models (LLMs) is \nbeneficial for modelling the complex visual information extracted by the \nbrain from natural scenes. We show that LLM embeddings of scene captions \nsuccessfully characterize brain activity evoked by viewing the natural \nscenes. This mapping captures selectivities of different brain areas and \nis sufficiently robust that accurate scene captions can be reconstructed \nfrom brain activity. Using carefully controlled model comparisons, we \nthen proceed to show that the accuracy with which LLM representations \nmatch brain representations derives from the ability of LLMs to integrate \ncomplex information contained in scene captions beyond that conveyed by \nindividual words. Finally, we train deep neural network models to transform \nimage inputs into LLM representations. Remarkably, these networks learn \nrepresentations that are better aligned with brain representations than a \nlarge number of state-of-the-art alternative models, despite being trained \non orders-of-magnitude less data. Overall, our results suggest that LLM \nembeddings of scene captions provide a representational format that \naccounts for complex information extracted by the brain from visual inputs.\nThe visual system provides the brain with a wealth of information \nabout the physical environment. Much progress in understanding \nthe functional organization1–5 and computational principles6–9 of the \nvisual system has been driven by a heavy focus on the objects that are \npresent in visual scenes. In particular, exciting advances in the ability \nto quantitatively predict neural activity in the extrastriate visual cortex \nhave been achieved by training artificial neural networks (ANNs) to \nperform object recognition from raw visual inputs10–14.\nDespite this progress, it is clear that visual scenes convey more \ninformation than the identity of the objects present15. Presumably, an \neffective interpretation of a visual scene must include the context in \nwhich objects reside as well as their spatial and semantic interrelations. \nStudies of the neural basis of object context and relations have pro-\nvided insight into the role of object co-occurrence statistics16,17, spatial \nand semantic interrelations among objects18–21, the context in which \nobjects appear22 and their typical locations in scenes23–26. In addition, a \nrobust literature on scene representations in the brain has emerged27,28, \nproviding insights into scene categories27,29–33, scene grammar26 and \naction affordances34, to name a few topics. However, it remains unclear \nhow to connect and integrate the insights obtained from these studies \nReceived: 19 August 2024\nAccepted: 4 June 2025\nPublished online: 7 August 2025\n Check for updates\nA full list of affiliations appears at the end of the paper.  e-mail: ian.charest@umontreal.ca\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1221\nArticle https://doi.org/10.1038/s42256-025-01072-0\nIn this Article, we explore the hypothesis that the human brain \nprojects visual information from retinal inputs, via a series of hierar-\nchical computations, into a high-level multidimensional space that \ncan be approximated by LLM embeddings of scene captions. T o do so, \nwe combine 7 T functional magnetic resonance imaging (fMRI) data \ncollected while participants viewed thousands of natural scenes with \nmultivariate encoding and decoding analyses, as well as ANN model-\nling. We demonstrate that the visual system may indeed converge, \nacross various higher-level visual regions, towards representations \nthat are aligned with LLM embeddings.\nResults\nT o explore representational transformations across the visual sys-\ntem, we take advantage of the Natural Scenes Dataset (NSD) 46, a \nlarge-scale 7 T fMRI dataset featuring brain responses to thousands \nof complex natural scenes taken from the Microsoft Common Objects \nin Context (COCO) image database47,48. The COCO database includes \nhuman-supplied captions describing each image, as well as labels for \nwith the kind of quantitative and computational methods (including \nimage-computable models) associated with the object recognition \nliterature. A quantitative approach for studying the complex informa-\ntion extracted from visual scenes seems elusive: what representational \nformat could be used to summarize and study this information?\nExcitingly, recent advances in artificial intelligence (AI) provide \nclues into the challenge of representing scene information. First, large \nlanguage models (LLMs) have made enormous strides in natural lan-\nguage processing35. LLMs learn to encode rich contextual information \nand statistical world knowledge through training on massive amounts \nof text data36–39. Second, AI researchers have demonstrated improve-\nments in the ability of vision models to segment, recognize and gen -\nerate images by aligning visual representations with the information \nconveyed by textual image captions 40–43. Importantly, these image \ncaptions are transformed into a powerful operable format through \nembedding in the latent space of LLMs 44,45. These insights lead to an \nintriguing possibility: LLM embeddings of image captions might be an \neffective way to capture the rich information conveyed by visual scenes.\na\nScene caption\n\"a dog standing\non a boat...\"\nLLM\nActivation pattern/\nvoxel response\nBrain activity\n(7 T fMRI)\nRSA\nFig. 1b\n(pattern comparison)\nb Linear encoding model performance (LLM /uni2192 brain)\nGroup average\n(Pearson correlation)\nFigs. 1c and 2a\n(linear mapping)\nEncoding\nmodel\nRepresentational similarity analysis  (LLM ↔ brain)\nGroup average\n(Pearson correlation)\n–0.25 0.250 –0.73 0.730\nc\nLLM encoding model (Pearson)\n0 0.8\nInterparticipant\nagreement (Pearson) \n0.8d\nNon-visual\nEVC\nMidventral\nVentral\nMidlateral\nLateral\nMidparietal\nParietal\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nFig. 1 | A mapping from LLM embeddings captures visual responses to natural \nscenes. a, LLM to brain mapping methods. Each image in the NSD dataset is \nassociated with captions written by different human observers to describe \nthe scene. These captions are passed through an LLM model to generate \nembeddings. We use two approaches to quantify the match between these \nembeddings and fMRI data (RSA and encoding models). Note that, for copyright \nreasons, we cannot show the real COCO image we used; hence, it has been \nreplaced by a similar copyright-free image. b, RSA reveals an extended network \nof brain regions where LLM representations correlate with brain activities. \nSearchlight map for the group-average Pearson correlation (not noise-ceiling \ncorrected) between LLM embeddings (MPNet) and brain representations \n(significance threshold set by a two-tailed t-test across participants (N = 8) \nwith Benjamini–Hochberg false discovery rate (FDR) correction; P = 0.05). See \nSupplementary Fig. 3 for individual participants. c, A linear encoding model \nhighlights a similar network of brain regions. We performed voxel-wise linear \nregression to predict voxel activities from LLM embeddings. Shown is the \ngroup-average Pearson correlation map (not noise-ceiling corrected) between \nthe predicted and actual beta responses on the test set (significance threshold \nset by a two-tailed t-test across participants (N = 8) with Benjamini–Hochberg \nfalse discovery rate correction; P = 0.05). See Supplementary Fig. 4 for \nindividual participants. d, Encoding model performance versus interparticipant \nagreement. Each dot in the scatter plot shows the encoding model performance \nfor a given voxel versus the interparticipant agreement, computed as the mean \nPearson correlation between each participant’s (N = 8) voxel activities and the \naverage of the voxel activities of the remaining seven participants on the test \nimages. Our encoding model approaches the interparticipant agreement in all \nROIs, indicating good performance. Values below the diagonal can be explained \nby the fact that the model captures participant-specific variance not captured by \nthe mean of other participants. Calc, calcarine sulcus; CGS, cingulate sulcus; CoS, \ncollateral sulcus; CS, central sulcus; IFRS, inferior frontal sulcus; IPS, intraparietal \nsulcus; LS, lateral sulcus; OTS, occipitotemporal sulcus; PoCS, post-central \nsulcus; PrCS, precentral sulcus; SFRS, superior frontal sulcus; STS, superior \ntemporal sulcus.\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1222\nArticle https://doi.org/10.1038/s42256-025-01072-0\nobject categories present in each image (see Supplementary Fig. 1 \nfor descriptive statistics of the COCO captions). T o test whether LLM \nembeddings provide a useful representational format for modelling \nvisually evoked brain responses, we use LLM sentence encoders based \non transformer architectures49 and project the scene captions into \nthe embedding space of these LLMs (Fig. 1a). As a representative LLM, \nwe use MPNet39, a transformer that is fine-tuned for sentence-length \nembeddings. MPNet was chosen as it reaches state-of-the-art perfor-\nmance on a variety of benchmarks, including semantic textual similarity \n(STS), which measures the match with human judgements of semantic \nsimilarity between sentences50. Importantly, our LLM embeddings \nare derived entirely from text, without regard for visual features of \nthe corresponding scenes. This differs from other embeddings that \nare jointly trained on visual input and language (for example, con -\ntrastive language–image pretraining (CLIP) 43). A two-dimensional \nt-distributed stochastic neighbour embedding (t-SNE) projection of \nMPNet-embedded NSD captions confirms that the model success -\nfully captures fine-grained scene information, such as what objects \nare present, what actions are being performed and the type of scene \n(Supplementary Fig. 2).\nA linear mapping from LLM embeddings captures brain \nresponses to natural scenes\nT o quantify how well LLM embeddings of scene captions predict brain \nactivities, we used representational similarity analysis (RSA)4,51–53. We \ncorrelated representational dissimilarity matrices (RDMs) constructed \nfrom LLM embeddings of the image captions with RDMs constructed \nfrom brain activity patterns obtained while participants viewed the \ncorresponding natural scenes (Fig. 1a). Applying RSA in a searchlight \nfashion, we find that the LLM embeddings are able to predict visually \nevoked brain responses across higher level visual areas in the ventral, \nlateral and parietal streams (Fig. 1b; see Supplementary Fig. 3 for indi-\nvidual participants; see Supplementary Fig. 11 for a reproduction of \nthis result using different LLMs).\nWe then probed the mapping between LLM representations and \nbrain representations using linear encoding models. We first trained \nan encoding model to predict individual voxel activities from LLM \nembeddings using cross-validated fractional ridge regression 54. In \nline with the RSA results, we find that the encoding model successfully \npredicts variance across large parts of the visual system (Fig. 1c,d; see \nSupplementary Fig. 4 for individual participants). This suggests that \nthe LLM representations of associated captions accurately capture \nimportant features of visual processing. We verified that these features \ngeneralize across participants by using a cross-participant encoding \napproach where we train the model on one participant and test it on \nthe other participants (Supplementary Fig. 5).\nT o elaborate on this point, we tested if the model can reproduce \nwell-established tuning properties observed in cognitive neurosci -\nence. We contrasted the predictions derived from different novel \nsentences highlighting people versus scenes (for example ‘Man with \na beard smiling at the camera’ versus ‘ A view of a beautiful landscape’). \nSuch a contrast revealed classical tuning properties associated with \npeople- and place-selective areas (including the fusiform face area \n(FFA), occipital face area (OFA) and extrastriate body area (EBA) versus \nparahippocampal place area (PPA) and occipital place area (OPA)) as \nwell as food-selective areas55 (Fig. 2a; also see Supplementary Fig. 6). \nThe success of the encoding model indicates that LLM representations, \ndespite being derived purely from text, can make accurate predictions \nof region-specific tuning properties of the visual cortex.\nThe success of the LLM representations in characterizing brain activ-\nity suggests that it may be possible to accurately infer a textual description \nof what participants saw from visually evoked brain activity alone using \nsimple linear methods. T o test for this, we trained a linear decoding model \nto predict LLM embeddings from fMRI voxel activities (Fig. 2b). Then, \nto reconstruct scene captions, we used a dictionary look-up approach56 \non a large corpus of 3.1 million captions (taken from Google Conceptual \nCaptions57). As shown in Fig. 2b, we obtain remarkably accurate textual \ndescriptions of the stimuli viewed by the participants. This highlights \nthe appropriateness of LLM embeddings as a representational format \nfor higher-level brain signals evoked by visual stimuli.\nLLMs integrate complex information contained in scene \ncaptions that is important to match brain activities\nLLMs are capable of encoding and integrating complex contextual \ninformation across all words in scene captions. We hypothesized that \nthis ability can, in part, explain the match of LLM embeddings to brain \nactivities. T o test this hypothesis, we contrasted models that differ in \ntheir ability to encode contextual information in scene captions. We \nfocused our analyses on regions of interest (ROIs) across the visual \nsystem, including early visual cortex (EVC) and the ventral, parietal and \nlateral visual streams (using the NSD ‘streams’ ROI definitions). We use \nparameter-free RSA to estimate the representational agreement, and \nreport t-test statistics after Benjamini–Hochberg false discovery rate \ncorrection with a significance threshold of P < 0.05.\nFirst, we tested if the ability of LLMs to align with high-level visual \ncortex representations relies on more than just object category infor-\nmation (Fig. 3a). As a base model, we encoded the presence or absence \nof various object categories using binary multi-hot vectors (as provided \nby the COCO dataset). We then built increasingly complex models \nbased only on category information: contextually enriched single \nword embeddings (including fasttext58,59, which is based on the con -\ntext of words, as well as GloVe60, which is based on word co-occurence \nstatistics). Such word embeddings provide a richer representation \nthan multi-hot object inventories, because they contain information \nnot only about individual words but also about their typical linguis -\ntic context. One step further towards richer, more contextualized \nrepresentations, we LLM-encoded a concatenated list of all category \nwords. This provides a richer representation of category information, \nbecause LLMs can relate and encode interactions between words. LLM \nembeddings of category words showed significantly better alignment \nwith brain representations than multi-hot vectors (except in the lateral \nROI) and word embeddings (except fasttext in EVC). This shows that \nthe LLM representational format allows better predictions of brain \nactivities, even when limited to category information. However, the \nLLM embeddings of full captions better predicted brain activities in \nall ROIs by far, indicating that part of the success of LLM mapping to \nvisual brain data is due to its ability to integrate caption information \nthat goes beyond categories. T o further test this hypothesis, we con-\nducted the same encoding and decoding analyses as in Fig. 2a,b , but \nbased on LLM embeddings of category words. We found that this leads \nto worse performance in both analyses, supporting the hypothesis that \nintegrating information beyond categories is important to align LLM \nand brain representations (Supplementary Fig. 8).\nSecond, to further understand which aspects of the LLM embed-\ndings drive their agreement with the brain data, we compared LLM \nembeddings extracted from the full image caption with embeddings \nobtained from a concatenation of all caption nouns or all caption verbs \n(Fig. 3b). In agreement with our previous findings, we find that the full \ncaption embeddings significantly outperform the noun- and verb-based \nembeddings across all ROIs tested, except for noun-based embeddings \nin EVC. Note that this comparison is a stronger test than the previous \nanalysis of category words, as caption nouns include additional content \nsuch as scene locations. Again, this result supports the hypothesis that \nthe brain match of LLM embeddings is driven by the ability to integrate \ninformation across the entire captions, beyond nouns or verbs.\nWe also tested adjectives, adverbs and prepositions, which led to \nvery low alignment with brain representations (Supplementary Fig. 9). \nThis can be expected, given that prepositions, adjectives and adverbs \noften carry less specific semantic content than nouns and verbs in \nNSD captions. For example, in the caption ‘a person walking a dog on \nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1223\nArticle https://doi.org/10.1038/s42256-025-01072-0\nthe grass under a blue sky, ’ prepositions like ‘on’ and ‘under’ provide \nlimited predictive information about brain responses. Exploring data-\nsets where these word types play a more important role is an intriguing \ndirection for future research.\nThird, we asked whether contextual information between words \nof a caption is important for the representational match of LLM \nembeddings with the brain by testing if full caption embeddings pro-\nvide additional explanatory power beyond that of their constituent \nwords (Fig. 3c). T o this end, we compared the LLM caption embeddings \nwith LLM, fasttext and GloVe embeddings averaged across all individual \nwords (that is, these models see all the caption words, but each word \nis processed separately without the possibility to contextualize one \n[Human] White car parked in a lot \nwith two surfboards on top.\n[Decoder] This vehicle has been parked.\n[Nearest training] Large racing equipment \ntruck parked in the lot next to another one. \nParticipant 3\nrank 459/515 \nBrain activity\n(7 T fMRI)\nPredicted LLM \nembedding\nLinear \ndecoding\nNearest-\nneighbour\nlook-up\nb\n[1.3, 0.8, 0.1, 1.7, -3.7, ... , –0.7]\n3 million Google\nconceptual captions\n–0.6\n–1.2\n0.6\n–0.2\n–0.8\n–0.1\n–2\n0.5\n–0.6\n–1.6\n1\n–1.2\n0.3\n–0.1\n–0.9\n0.3\n1.2\n0.4\n0.7\n–0.6\n–1.1\n–0.1\n0.8\n–0.1\nv1.9\n2.2\n0.4\n0.2\nLLM embedding\nDecoded caption\n\"A woman catching a \nfrisbee...\"\nEmbedding prediction performance\n(Pearson) \nNoise ceiling\nDensity\n3.0 Participant 1\nParticipant 2\nParticipant 3\nParticipant 4\nParticipant 5\nParticipant 6\nParticipant 7\nParticipant 8\n2.5\n2.0\n–0.2 0.2 0.4 0.6 0.80\n1.5\n1.0\n0.5\n0\n[Human] two giraﬀes standing by a \ntree with a forest in the background\n[Decoder] giraﬀes standing near each \nother in the bush\n[Nearest training] Do the giraﬀes find people \nas interesting as people find giraﬀes?\nParticipant 2\nrank 0/515 \n[Human] A cat poking its head through \na small window in a stone wall.\n[Decoder] A cat looking out of the \nwindow.\n[Nearest training] A cat with its head \nbetween the window and the frame\nParticipant 1\nrank 102/515\n[Human] A group of people gathered \naround in chairs with a laptop.\n[Decoder] Individuals sitting around a table.\n[Nearest training] A group of people sitting \nat a table using laptop computers\nParticipant 4\nrank 255/515\na\nLinear decoding model (brain → LLM)\nPlaces People\n–1.52 1.52\nExample sentences\n\"Man with a beard \nsmiling at the \ncamera.\"\n\"A view of a \nbeautiful \nlandscape.\"\nROI definitions:\nAllen et al. (2021) \nPeople Food\n–0.94 0.94\nExample sentences\n\"A plate of food \nwith vegetables.\"\n\"Some children \nplaying.\"\nROI definitions:\nPennock et al. (2021)\nFunctional contrast predicted by LLM encoding model (LLM → brain)\n(diﬀerence in predicted voxel activity) (diﬀerence in predicted voxel activity)\nOPA\nEBA\nOFA\nFFA2\nFBA1/2\nPPA\npSTS face\nFFA1\nFig. 2 | LLM-based linear prediction and decoding of brain activities.  \na, The linear encoding model captures selectivities of different brain regions.  \nWe contrasted the brain activity predicted from five novel people- versus place-\nrelated sentences (left) and five food- versus people-related sentences (right; \nsignificance threshold set by a two-tailed t-test across participants (N = 8) with \nP = 0.05; without FDR correction). These contrasts highlight brain areas known to \nbe selective for people, places and food (people and place areas are localized as \npart of NSD (left); food areas described by ref. 55 shown as white outlines (right)). \nb, Decoding captions from visually evoked brain responses. T op: we fit a linear \nmodel to predict LLM embeddings (MPNet) from fMRI voxel activities. We then \nuse a nearest-neighbour look-up to generate a caption for each image. Bottom \nleft: kernel density estimate plot of the prediction score for each participant on a \nheld-out test set (see Supplementary Fig. 5 for a t-SNE projection of the training \nand testing sets), quantified using Pearson correlation between predicted and \ntarget embedding. The noise ceiling is computed as the consistency between \nthe five human-generated captions for each image. Bottom right: target (blue), \ndecoded (pink) and nearest training (green) caption examples from different \nparticipants on the held-out test set, spanning the range of prediction scores. \nThe decoder is not simply looking up the closest training item, but instead \nprovides another adequate caption. The rank refers to the prediction score of \nthe shown sample (that is, rank 0 is the best prediction for this participant, while \nrank 514 is the worst). Note that, for copyright reasons, we cannot show the real \nCOCO images we used; hence, they have been replaced by similar copyright-\nfree images. EBA, extrastriate body area; FBA1/2, posterior/anterior section of \nfusiform body area; FFA1, posterior section of fusiform face area; FFA2, anterior \nsection of fusiform face area; PPA, parahippocampal place area; pSTS face, \nposterior superior temporal sulcus face area; OFA, occipital face area; OPA, \noccipital place area. References: Allen et al.46, Pennock et al.55.\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1224\nArticle https://doi.org/10.1038/s42256-025-01072-0\nword on the basis of other words in the caption). Again, in all ROIs, the \nembeddings of whole captions aligned significantly better with brain \ndata than averaged embeddings of the individual caption words. This \nindicates that the contextual relations among the caption words are \nan important factor for the LLMs’ alignment with visual representa -\ntions in the brain.\nIn further analyses (Supplementary Fig. 10), we generated LLM \nembeddings from scrambled sentences and found them highly corre-\nlated with LLM embeddings from the original sentences (mean Person \ncorrelation across eight participants, 0.91; s.d. 0.001). This suggests \nthat the MPNet LLM is relatively insensitive to word order, thus yield-\ning comparable alignment with brain data for both sentence types. \nWhile the brain may rely on syntax in language processing, the LLM \nagreement with visually evoked responses in the brain are not driven \nby it. Note that scrambled sentences fall outside the LLM’s training \ndistribution, and it may still reconstruct the meaning of the simple \nNSD captions (for example, it can retrieve the non-scrambled meaning \nof ‘road a dirt car driving is a on’). This might not happen with more \ncomplex sentences where word order is critical. Future research will \ninvestigate this further.\nFinally, to ensure that our results are not reliant on the exact LLM \nused for embedding the captions, we tested several other LLMs from \nthe Sentence-Transformers leaderboard (https://www.sbert.net/index.\nhtml) and found that they all perform similarly to MPNet used here \n(Supplementary Fig. 11; none of the statistical comparisons among \nLLM models was significant). This finding speaks to the generality of \nour findings and aligns with previous work indicating that scale can \nmatter more than architectural differences in LLMs61,62.\nLLM-trained RCNNs outperform other models of visual \nresponses\nOur results indicate that high-level brain representations are well char-\nacterized by LLM-like representations. This leads to the hypothesis \nthe human brain projects visual information from retinal inputs, via a \ncascade of nonlinear operations across the visual system, into a mul-\ntidimensional space that can be approximated by LLM embeddings. \nUnder this hypothesis, we predicted that LLM embeddings might serve \nas a powerful target for training visual ANN models. There has been a \nhistory of success using task-optimized ANNs as models of the visual \nsystem, but, conventionally, these models are trained to classify objects \npresent in each image12,13,63,64 or, in some cases, using unsupervised \nobjectives65,66. We therefore trained ANNs to predict LLM embeddings \nfrom visual inputs and quantified the match of these task-optimized \nmodels to our brain data (Fig. 4a).\nWe used recurrent convolutional neural networks (RCNNs 67), \nbased on the vNet architecture63 that mirrors the progressive increase \nof receptive field sizes across the ventral stream. The RCNNs were \ntrained to predict LLM embeddings of the captions associated with \nthe input scenes (LLM-trained RCNNs) on the COCO dataset. T o avoid \noverfitting to the images seen by NSD participants, we excluded from \ntraining all images that were used in NSD. We trained ten network \ninstances with different random seeds to account for possible variation \ndue to network initialization68. T o compare the model response to brain \ndata, we extracted activity patterns in response to the NSD stimuli from \nthe last layer and timestep, computed RDMs and used searchlight RSA \nto quantify representational alignment with the brain. This searchlight \nanalysis revealed that the LLM-trained RCNN layer activations were \nable to significantly predict visually evoked brain responses across the \nentire visual system, similarly to the searchlight performed on the LLM \nembeddings themselves (Supplementary Fig. 13; see Supplementary \nFig. 14 for searchlight maps of all layers and timesteps in the network \nshowing that early layers better match lower visual areas, and higher \nlayers better match higher visual areas).\nWhile results from the previous sections show that high-level \nbrain representations are well captured by visually agnostic LLM \nembeddings, it is still likely that they retain some visual information \nRepresentational agreement\n(Pearson correlation, noise-normalized)\n0\n0.1\n0.2\nEarly visual Ventral\n0.4\n0\n0.1\n0.2\n0.3\nLateral\n0.4\n0\n0.1\n0.2\n0.3\nParietal\n0.4\n0\n0.1\n0.2\n0.3\nLLM caption\nMulti-hot\nLLM\nCategory\nInformation\nWord\ntypes\nSingle\nword average\nFasttext\nGlove\nLLM verbs\n LLM nouns\nFasttext\nLLM\nGlove\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\na\nb\nc\nFig. 3 | The match of LLMs to visually evoked brain activities derives from \ntheir ability to integrate complex information contained in scene captions. \nWe applied RSA in the ‘streams’ ROI definitions of the NSD dataset, shown in \nthe top-left inset. ‘LLM caption’ refers to the LLM embedding (MPNet) of the \nentire caption, and different groups denote different classes of control models, \ndetailed below. The match between each model and brain activities is quantified \nas the noise-ceiling-corrected Pearson correlations between each model and  \na given ROI (averaged across participants (N = 8), error bars reflect standard \nerror); all statistics are two-tailed t-tests across participants, with Benjamini–\nHochberg FDR correction; stars show comparisons where ‘LLM caption’ \nsignifi cantly outperforms the control model (P < 0.05); corrected P values for all \npairwise model comparisons are provided separately in Supplementary Fig. 12.  \na, LLM embeddings of category information improve match to brain data. We \ncompared multiple formats to represent category information, from binary \nmulti-hot vectors (multi-hot), through averaging fasttext (fasttext) or GloVe \n(glove) word embeddings of category words, to embedding a concatenation of all \ncategory words using MPNet (LLM). b, LLM embeddings capture brain-relevant \ninformation beyond nouns or verbs. The LLM embeddings of the concatenated \ncaption nouns (LLM nouns) or verbs (LLM verbs) both match brain data \nsignificantly less well (except LLM nouns in EVC) than the LLM embeddings of the \nfull caption (LLM caption). c, LLM embeddings capture brain-relevant contextual \ninformation. T o test if contextual information conveyed by captions is important \nto match brain data, we compared embeddings of whole captions with the \naveraged LLM, fasttext, and GloVe embeddings of individual caption words.\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1225\nArticle https://doi.org/10.1038/s42256-025-01072-0\nthat cannot be captured by the LLM embeddings alone (for example, \npositions of objects that are not explicitly mentioned in the scene \ncaptions but are available to inferior temporal cortex69). We hypoth-\nesized that this information could also be present in the late layers of \nthe LLM-trained RCNN models, which transition from visual inputs to \nLLM-like representations. In line with this hypothesis, we find that the \nLLM-trained RCNNs align significantly better with the brain data than \nthe LLM embeddings they were trained to predict (Fig. 4c; see Supple-\nmentary Fig. 15 for individual participants). Note that the representa-\ntions we extracted from our LLM-trained models have 512 features, \nwhich is lower-dimensional than the 768 features of the target LLM \nembeddings. Hence, this result cannot be explained simply by the \ndimensionality of tested representations.\nDespite the strong correlations observed between our LLM-trained \nRCNNs and the brain data, it is still possible that conventional models \ntrained to classify objects could outperform it. We therefore ran a \nhighly controlled model comparison contrasting our LLM-trained \nRCNNs with RCNNs trained to predict multi-hot category labels \n(category-trained RCNNs; again, we trained ten instances with differ-\nent random seeds). Training these networks end-to-end enabled us \nto perform a stringent test of our hypothesis: both LLM-trained and \ncategory-trained RCNNs are fed the exact same images and have the \nexact same architecture, the same dimensionality and the same random \nseeds. They differ only in their training objective (Fig. 4a). T o adjudicate \nbetween the two models, we contrasted their representational align-\nment using RSA (focusing on the last layer and timestep activities, as \nprevious work has shown that these layers perform best in predicting \nhigher-level visual regions63; searchlight contrast maps between all lay-\ners and timesteps of LLM- and category-trained RCNNs can be found in \nSupplementary Fig. 16). We chose RSA for model adjudication because \nit provides a robust and unbiased framework for comparing models \nwith varying parameter counts and dimensionalities. By avoiding \nthe need to fit parameters to neural data, RSA ensures that models \nwith more parameters, such as high-dimensional LLM embeddings, \ndo not gain an unfair advantage 70–73. In line with our hypothesis, the \nLLM-trained RCNNs significantly outperformed the category-trained \ncontrols across a wide network of higher visual areas (Fig. 4d ; see \nSupplementary Fig. 17 for individual participants). The same result \nwas replicated using a ResNet50 (ref. 74) architecture, showing that \nthe benefit of LLM training is not restricted to our particular RCNN \narchitecture (Supplementary Fig. 18).\nThese findings are still consistent with the discovery of object \ncategories as a major factor in ventral stream representations4,8,75–78. \nIndeed, because LLM embeddings capture many forms of linguistically \nexpressible content, LLM representations may encompass content \nconveyed by object category information. T o assess this hypothesis, \nwe froze the weights of our LLM-trained and category-trained RCNNs \nand quantified how well category labels and LLM embeddings could be \nlinearly read out (Fig. 4b). We found that category labels could success-\nfully be read out from LLM-trained RCNNs (that is, similar performance \nas for the category-trained RCNNs). However, the reverse was not true: \nLLM embeddings could not be read out from category-trained RCNNs \nas well as from LLM-trained RCNNs. These results suggest that the LLM \nrepresentational format encompasses categorical information while \nproviding a richer training target that improves the match to visually \nevoked brain activity.\nWe assessed our LLM-trained RCNNs in the broader landscape of \nANN modelling by comparing against 13 models previously reported \nto be good predictors of visual activity in the brain. These models have \ndiverse architectures, training datasets and objectives and include lead-\ning models on neural data prediction benchmarks such as NSD79 and \nbrainscore80, supervised category-trained models (including a larger \nversion of our RCNN architecture trained on ecoset63 and several differ-\nent models trained on Imagenet81), supervised models trained for scene \ncategorization on the Places365 (ref. 82) and taskonomy83 datasets, \nweakly supervised models trained on hundreds of millions of images84 \nor image-text pairs (CLIP43), and unsupervised models trained using \nsimCLR85 and instance-level contrastive learning65 (see the Methods  \nfor the full list of models). Notably, all of these models are trained on  \n>1 million images (ecoset/ImageNet), or hundreds of millions of  \nimages (in the case of resnext101_32x8d_wsl and CLIP), while our LLM- \ntrained RCNN is trained on orders-of-magnitude less data (the 48,000 \nimages left in COCO after removing NSD images).\nWe applied the same RSA approach as before and report the cor-\nrelation between each model’s pre-readout RDMs and brain RDMs \nobtained from higher-level ROIs of the ventral, lateral and parietal \nvisual streams (except for CLIP, for which we used the final embedding \ninstead of the pre-readout layer). We find that our LLM-trained RCNN \nmodels, trained to map from pixels to LLM embeddings, significantly \noutperform every single other model in the ventral and parietal ROIs, \nand all but one (which is worse, but not significantly) in the lateral ROI \n(Fig. 4e). T o rule out the possibility that this good alignment to brain \nrepresentations is driven by training on our subset of COCO rather \nthan by the LLM objective, we verified that RCNNs trained to predict \ncategory labels on ecoset are not outperformed by our RCNNs trained \nto predict category labels on our subset of COCO (and both are outper-\nformed by our LLM-trained RCNN; again, we reproduced this result \nusing a ResNet50 architecture; Supplementary Fig. 19). Note that, \nagain, these findings cannot be explained by the fact that LLM-trained \nmodels have a higher number of features. First, we use parameter-free \nRSA, which is not directly biased by feature dimension. Second, the rep-\nresentations extracted from our LLM-trained models have 512 features, \nFig. 4 | LLM-trained deep recurrent convolutional networks outperform other \nmodels in predicting brain activity. a, RCNNs. Our RCNNs have ten recurrent \nconvolutional layers with bottom-up (purple), lateral (green) and top-down \n(orange) connections, followed by a fully connected readout layer. The training \nobjective is to minimize the cosine distance between the network’s output and \nthe target LLM caption embeddings. Category-trained control networks are \nidentical, except that they are trained to predict multi-hot category labels. Note \nthat, for copyright reasons, we cannot show the real COCO images we used; \nhence, they have been replaced by similar copyright-free images. b, Category \nlabels can be decoded from LLM-trained RCNN activities. After freezing network \nweights, we tested how well category labels (respectively LLM embeddings) \ncan be decoded from activities in the pre-readout layer of the LLM-trained \n(respectively category-trained) network. The plot shows test performance \n(averaged across N = 10 network instances; error bars represent standard \ndeviation), quantified as the cosine similarity between predicted and target \nvectors. Dashed horizontal bars show floor performance, operationalized as the \nperformance obtained by predicting the mean training target. c, LLM-trained \nRCNNs versus LLM embeddings. Searchlight RSA contrast between llm-trained \nRCNN activities (last layer and timestep) and the LLM embeddings of scene \ncaptions. RCNN RDMs are averaged across ten network instances; correlations \nare averaged across eight participants; significance threshold set by a two-tailed \nt-test across participants with Benjamini–Hochberg FDR correction; P = 0.05. See \nSupplementary Fig. 15 for individual participants. Insert: brain-model correlation \nfor LLM-trained RCNNs versus LLM embeddings for each searchlight location. \nd, LLM-trained versus category-trained RCNNs. Similar plot as c, but showing \nthe contrast between LLM-trained and category-trained RCNNs (last layer and \ntimestep). See Supplementary Fig. 17 for individual participants, Supplementary \nFig. 16 for all other RCNN layers and timesteps, and Supplementary Fig. 18 \nfor a reproduction of this effect using the ResNet50 architecture. e, ROI-wise \ncomparison of LLM-trained RCNNs with other widely used ANNs. Noise-ceiling-\ncorrected correlations between the pre-readout layer of various models and \nROI RDMs. Our RCNN model significantly outperforms all other models (except \nCORnet-S, which is not significantly worse in the parietal ROI; two-tailed t-test \nacross participants with Benjamini–Hochberg FDR correction; P = 0.05). \nBenjamini–Hochberg FDR-corrected P values for all pairwise model comparisons \nare given in Supplementary Fig. 20.\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1226\nArticle https://doi.org/10.1038/s42256-025-01072-0\na\nTraining objectiveInput statistics\nMS COCO\nsubset\n(48,000\nimages)\nb\nDeep neural network architecture (BLT RCNN)\nc\n0.70\n0.60\n0.50\n0.40\nDecode category\n...from LLM-\ntrained\nRCNN \nDecode LLM\nCategory baseline\nLLM baseline\nValidation performance\n(cosine similarity)\n...from\ncategory \ntrained RCNN\n...from LLM-\ntrained\nRCNN ...from\ncategory \ntrained RCNN \ne\nNon-visual\nMidventral\nMidlateral\nLateral\nMidparietal\nParietal\nVentral\nEVC\nd\nVentral Lateral\nParietal\nGroup average\n(Pearson correlation)\n0 0.06–0.06\nLLM-trained RCNNLLM embedding\n0 0.07–0.07\nGroup average\n(Pearson correlation)\nCategory-trained LLM-trained\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nCSCS CSCS\nLSLS LSLS\nSFRSSFRS SFRSSFRS\nIFRSIFRS IFRSIFRS\nCGSCGS CGSCGS\nCalcCalc CalcCalcPoCSPoCS PoCSPoCS\nPrCSPrCS PrCSPrCS\nOTSOTS\nOTSOTSCoSCoS CoSCoS\nIPSIPS IPSIPS\nSTSSTS STSSTS\nLLM trained:\nLLM emb.\n\"a dog standing\non a boat\"\nCategory trained:\nCategories\n{dog, boat, ...}\nRepresentational agreement with ventral\n(Pearson correlation, normalized)\nTraining dataset size (number of training images, log scale)\n0.15\n0.25\n0.35\n0.45\n10\n4\n10\n5\n10\n6\n10\n7\n10\n8\n10\n9\nLLM trained (ours)\nCornet-s\nAlexnet nf_resnet50\nAlexnet-gn-sv\nCLIP_RN50_imgs\nCLIP_ViT_imgsAlexnetgn_ipcl\nResnext101_32x8dGoogle_simclr_rn50\nrcnn_ecoset\nResnet50\nResnet50_Places365\nTaskonomy_scene_cat\nLLM trained (ours)\nLLM trained (ours)\nLLM tr ained (ours) Supervised (category)\nSupervised (scene) Semi-supervised\nUnsupervised\n0.5\n0.1\n0.4\n0.1\n10\n4\n10\n9\n10\n4\n10\n9\nLLM-trained RCNN\n0 0.3\nCategory-trained RCNN\n0.3\nLLM-trained RCNN\n0 0.3\nLLM embedding\n0.3\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1227\nArticle https://doi.org/10.1038/s42256-025-01072-0\nwhich is smaller than most other models (for example, ResNet models \nhave 2,048 features). T ogether these results suggest that LLM training \nis a powerful objective to train brain-aligned ANNs. This is in line with \nthe hypothesis that the brain may compute LLM-aligned representa-\ntions from visual inputs through a cascade of nonlinear computations.\nDiscussion\nUsing a variety of techniques, including RSA, encoding models, lin -\near decoding and ANN modelling, we have provided evidence for \nthe hypothesis that the visual system may converge, across various \nhigher-level regions, towards representations that are aligned with \nLLM embeddings of captions describing visual inputs. This result is \nstriking, given that LLMs lack any direct visual experience. We sug-\ngest that LLM embeddings capture visually evoked brain activity by \nreflecting the statistical regularities of the world, learned through their \nextensive language training, in ways that align with sensory processing. \nIn line with this reasoning, we have shown that the success of LLMs in \nmatching brain activities comes from their ability to integrate complex \ninformation conveyed by entire captions. The robust and structured \nmapping between LLM embeddings and visually evoked activities paves \nthe way for new approaches seeking to characterize complex visual \ninformation processing in the brain.\nOur results build on, and extend, previous research showing the \nextent of features extracted by visual processing, including object6–9,86 \nand scene27,29–33 categories, aspects of linguistics 87,88, object occur-\nrence statistics 17,33, the typical location of objects in scenes 23,25 and \nmany others22,24,26,27,32,89–98. Our approach based on LLM embeddings \nshould not be seen as a competitor to these lines of work, but rather \nas synergistic. While prior work has demonstrated the importance of \nthe above features individually, no unified quantitative framework \nhas been proposed to model them collectively. One exciting avenue of \nresearch is to test to which extent LLM embeddings can provide such \na unifying quantitative framework. Our work takes initial steps in this \ndirection, as we show that LLM-trained ANNs subsume the category \ninformation present in category-trained ANNs. Future work is needed \nto assess which other known aspects of visual processing are well cap-\ntured by LLM embeddings.\nThe success of LLM caption embeddings in predicting high-level \nvisual responses to natural scenes does not imply that these embeddings \nfully account for the information present in brain responses. Adding \naccess to the actual images seen by the participants can improve pre-\ndiction performance, as shown by our finding that LLM-trained ANNs \ntaking visual inputs are better aligned with the brain than the LLM cap-\ntion embeddings. Our interpretation is that the visual system encodes \nvisual input into a representational format that aligns with LLM caption \nembeddings while retaining some visual information. This interpretation \nis supported by the good performance of our ANNs, that predict LLM \nembeddings from visual inputs, compared with a wealth of control mod-\nels (see refs. 10,11,99,100 for discussions of this approach of contrasting \nANN models to test computational hypotheses about brain processing).\nWe find that LLM-trained ANNs outperform a wealth of state-of- \nthe-art neuro-AI models in predicting visually evoked brain activ -\nity. This corroborates the hypothesis that the human brain projects \nvisual inputs, through a hierarchy of computations, into a high-level \nrepresentational format aligned with LLM embeddings of scene cap-\ntions. One notable aspect of these results is that our LLM-trained ANNs \nare trained from scratch on orders-of-magnitude fewer images than \nprevious ANN models. Hence, large-scale visual datasets may not be \nrequired, if a powerful training objective is used. In this respect, it is \nimportant to note that the LLM embeddings themselves are the result \nof training on large amounts of textual data. Whether these data need \nto be factored into the training set size estimates is an open ques -\ntion that is beyond the scope of this Article. It may be worth noting, \nhowever, that category labels, the gold-standard training objective to \nwhich we compare, also rely on substantial amounts of data that went \ninto training the human labelers. An interesting future direction of \nresearch will spell out whether the rich learning signal derived from \nlanguage might indeed provide important benefits over other train -\ning objectives, including the supervised, unsupervised, and weakly \nunsupervised approaches we tested.\nOur results do not imply that visual representations have all dis-\ntinctive attributes of language, such as recursivity and syntax. Rather, \nwhat we show is that LLM representations of pure textual input show \nstrong alignment with higher level visual representations, driven by the \nability of LLMs to integrate complex information about scenes. These \nobservations open up the possibility that LLM embeddings could be \nused to predict and decode neural activities in other species that do \nnot have language, such as macaque monkeys 101. This is in line with \nrecent work in AI, which showed that LLMs can be used to improve the \nrepresentations of visual models40–45, as well as neuroscientific work \nhighlighting similarities between linguistic and visual representations \nin the brain102 and showing that linguistic information improves the \nability of crossmodal ANNs to predict brain activities103,104.\nThe task of the NSD participants was to report if they had previ -\nously seen each presented image. It cannot be fully ruled out that, to \nperform this continuous recognition task, participants were internally \ncaptioning the scenes, and this may have benefitted the LLM caption \nembeddings as a good model of visual responses. Alternatively, the \nbrain responses may align well with LLM caption embeddings irrespec-\ntive of task demands. While data of the scale of NSD are currently not \navailable for other task settings105, it will be interesting for future work \nto investigate LLM-based codes under different tasks. For example, one \ncould use encoding models, as done here, to map from high-level LLM \nembeddings to visual responses obtained while participants engage \nin different tasks and investigate the loadings of the linear model on \ndifferent embedding dimensions106.\nA representational format that aligns with LLM caption embed -\ndings has potential computational advantages beyond being \ninformation-rich, contextual and embedded in world knowledge. \nIndeed, such rich representations may act as a suitable candidate \nfor communication between different brain systems: if, for example, \nboth visual and auditory processing project to a common (LLM-like) \nspace, information from these modalities can easily be combined and \nused by other brain processes. Given that LLMs have been shown to be \ngood models for predicting brain activities in language areas too107,108, \nanother benefit of this code would be that it may allow easy communi-\ncation with other organisms109.\nOur results suggest that the alignment between LLM embed -\ndings and brain responses to visual scenes relies on the rich infor -\nmation encoded in scene captions. This rich information is learned \nby the LLMs via a series of nonlinear computations converging in a \nhigh-dimensional representational space. Although we have begun \ninvestigating which aspects of scene captions drive our findings, inter-\npreting LLM embeddings remains a challenging task and is currently \nan active area of research in explainable AI. Moving forward, additional \nstudies will be essential to clarify which elements of LLM embeddings \nmost strongly correlate with brain representations. At the same time, \nwhile developing fully interpretable models of high-level abstract brain \nprocesses is an admirable goal, decades of neuroscience research sug-\ngest that perfect interpretability may not always be feasible. Indeed, \nfully interpretable models have historically fallen short of deep neural \nnetworks in explaining brain data.\nAltogether, our findings indicate that LLM embeddings pro -\nvide a versatile representational format for capturing the com -\nplex information the brain derives from visual inputs. By offering \na quantitative, brain-aligned framework, this work paves the way \nfor new research avenues applying modern analysis tools to highly \nabstract information processed in sensory areas. In the same way that \nadvances in category-based models spurred breakthroughs in visual \nneuroscience8,12,13,110, we anticipate that LLM embeddings—and ANN \nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1228\nArticle https://doi.org/10.1038/s42256-025-01072-0\nmodels capable of extracting such embeddings from visual inputs—will \nopen up fresh directions and yield new insights for both visual compu-\ntational neuroscience and NeuroAI.\nMethods\nNSD\nA detailed description of NSD (http://naturalscenesdataset.org) can be \nfound in ref. 46. This dataset contains measurements of fMRI responses \nfrom 8 participants who each viewed 9,000–10,000 distinct colour \nnatural scenes over the course of 30–40 scan sessions, comprising \na total of 73,000 images, with 3 repetitions per image. Scanning was \nconducted at 7 T using whole-brain, gradient-echo EPI at 1.8-mm iso-\ntropic resolution and 1.6-s repetition time. Images were taken from \nthe COCO image dataset and were presented for 3 s with 1-s gaps in \nbetween images. A special set of 1,000 images was shared across par-\nticipants; the remaining images were unique and mutually exclusive \nacross participants (note that some participants did not complete 3 \ntrials for each image; therefore, only 515 shared images were seen 3 \ntimes by all participants). Participants fixated centrally and performed \na long-term continuous recognition task on the images. The data were \npreprocessed by performing one temporal interpolation (to correct for \nslice time differences) and one spatial interpolation (to correct for head \nmotion) and then using a general linear model to estimate single-trial \nbeta weights. In this Article, we used the 1.8-mm volume preparation \nof the NSD data (betas_fithrf_GLMdenoise_RR).\nLLM embeddings for NSD stimuli\nCaptions describing the content of each natural scene were obtained \nfrom five human observers as part of the COCO dataset. For each NSD \nparticipant and for each image presented to the participant, we gath-\nered the five captions provided for that image and took the mean of \nthe resulting embeddings. This averaging was done to account for \ninterrater differences, which is especially relevant given that the COCO \ncaptions were not written by the NSD participants (we ran tests using \na single sentence per image with qualitatively similar results (data \nnot shown)). In detail, each of the five captions was passed through \nan LLM, and we take the average embedding across the captions. For \nMPNet, we used the all-mpnet-base-v2 version (https://www.sbert.\nnet/docs/pretrained_models.html). Note that this version of MPNet \nwas fine-tuned to have consistent embeddings for different sentences \ndescribing the same scene on COCO (on which NSD is based) and other \ndatasets. This ensures that captions written by different people project \nto a similar point in embedding space, amplifying the ability of the \nmodel to extract cross-observer, consistent semantic meaning from \ncaptions in the NSD dataset.\nIn Fig. 3a, we also retrieved the COCO category words for each \nimage (that is, the words associated with the COCO category labels \npresent in the image), concatenated these category words into a string \nand fed this string into the LLM (called LLM in the figure). In Fig. 3b, we \ndid the same for all nouns and verbs of the captions (using the Natural \nLanguage T oolkit (nltk) Python library111 to determine which words \nwere nouns and which were verbs, respectively called LLM nouns and \nLLM verbs in the figure). In Fig. 3c, we also used a single-word-wise LLM \nembedding (called LLM in the figure). Here, we fed each word from each \nof the five COCO captions of each image separately into the LLM, and \nretrieved the average embedding (similarly to how one would use sin-\ngle word embeddings). Finally, in Supplementary Fig. 5, we compared \nseveral different LLMs using our standard approach of averaging their \nembeddings across the five COCO captions.\nCategory labels for NSD stimuli\nFor the multi-hot control in Fig. 3 , as well as for training our \ncategory-trained ANNs, we used multi-hot binary vectors based on \nthe category labels provided by COCO for each image (that is, vectors \nof 0 s with 1 s for each category present in the image).\nWord embeddings for NSD stimuli\nFor word embedding control models (as opposed to the sentence \nembeddings described above), we used fasttext58,59 and GloVe60. Using \nthe same COCO image captions as above, we constructed several \ndistinct models, each capturing different aspects of the captions. \nWord embeddings can be combined additively (a standard example \nis ‘queen’=‘king’-’man’+’woman’), and so we average the embeddings \nacross words. In Fig. 3a, for category word embeddings, we averaged \nthe word embeddings for each COCO category label. In Fig. 3c, we \ncombined the embeddings for all words in the scene captions by taking \nthe mean embedding across all words of all five COCO captions. Some \nwords were not recognized by fasttext or GloVe because they were \neither misspelt or did not exist in the corpus. For these cases, we either \ncorrected the misspelling, found a similar word in the fasttext corpus, \nor removed them. In rare cases, a stimulus may have no category words. \nIn these cases, we used the word embedding for ‘something’ (this is \ndone because every stimulus needs an embedding for RSA, and ‘some-\nthing’ is a neutral term).\nANN activations for NSD stimuli\nFor all ANNs, we collect activities for the layer (and timestep in the \ncase of RCNNs) of interest for all NSD images. We preprocess stimuli \nto match the input range expected by each model.\nQuantifying model–brain representational agreement  \nusing RSA\nWe used RSA to quantify the match between various models described \nabove and brain representations on the entire NSD dataset. We apply \nthis analysis both ROI-wise (using the ‘streams’ ROI definitions of NSD) \nand in a searchlight fashion112,113.\nRDMs were constructed from participants’ native space single-trial \nbeta weights. Analyses were restricted to images that had been seen \nthree times by the participant, and beta weights were z-scored across \nsingle trials within each scanning session for each participant. We then \naveraged over each image’s three repetitions to get an average response \nestimate for each image. In the searchlight analysis, for each voxel v, \nwe extracted activity patterns in a sphere centred at v with a radius of \nsix voxels (keeping only spheres with more than 50% voxels inside the \nbrain; when a sphere included voxels outside the brain, these voxels \nwere excluded from the analysis). Activity patterns were compared \nbetween pairs of stimuli using Pearson correlation distances to cre -\nate RDMs.\nGiven the large scale of the NSD dataset, to relate the brain RDMs \nto the model RDMs, we devised a practical sampling procedure based \non independent subsets of images. We first randomly sampled 100 \nNSD stimuli from the participant’s 10,000 images. We indexed the \nbrain activity patterns for these 100 images and constructed the \nRDM for this subset. We also indexed the model RDMs to retrieve \nthe pairwise distances for the same 100 stimulus images. This led to \n100 × 100 symmetric RDMs, with an upper-triangular vector length \nof 4,950 pairs (one for each model/RCNN layer and timestep, and one \nfor each ROI/searchlight sphere). These upper-triangular RDMs were \nthen compared between brain and model using Pearson correlation \nin each ROI/searchlight sphere. There was one such correlation per \nROI for each participant–model comparison. The randomly sampled \n100 images were then removed from the image sampling pool, and we \nrepeated the sampling procedure until we had exhausted all 10,000 \nimages. This resulted in 100 independent correlation volumes, which \nwere averaged. Note that four participants completed the full NSD \nexperiment, while another two had seen all three repetitions of 6,234 \nimages and two participants had seen the three repetitions of 5,445 \nimages, leading to 100 splits, 62 splits or 54 splits depending on  \nthe participant.\nFor the ROI analyses, each participant’s result was noise cor -\nrected. The participant-wise noise ceiling was approximated as the \nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1229\nArticle https://doi.org/10.1038/s42256-025-01072-0\ncorrelation between this participant’s RDM and the mean RDM across \nall other participants (these RDMs were computed on the shared 515 \nNSD images seen by all participants). Intuitively, this can be seen as \npitting the model against the average of seven human participants: if \nthe model predicts the participant’s data as well as the mean of seven \nhumans, it has reached the noise ceiling. These participant-wise \nnoise-ceiling-corrected correlations were then averaged. Significance \nwas tested using two-tailed t-tests across the eight NSD participants, \nand corrected for multiple comparisons using the Benjamini–Hoch -\nberg114 procedure for controlling the false discovery rate with P = 0.05. \nFor model comparisons, we tested the significance of the difference \nbetween model correlations against 0.\nFor the searchlight analyses, group-level statistics reported in the \nmanuscript are performed using two-tailed t -tests across the eight \nNSD participants and corrected for multiple comparisons using the \nBenjamini–Hochberg procedure for controlling the false discovery \nrate with P = 0.05. In the case of individual model maps, we tested the \nmodel’s correlation against 0. In the case of model comparisons, we \ntested the significance of the difference between model correlations \nagainst 0. Average correlation maps participants, thresholded with \nour group-level statistics are then projected in freesurfer’s fsaverage \nsurface space and visualized on a flattened cortical flatmap.\nEncoding model\nWe trained a linear encoding model to predict voxel activities from \nMPNet embeddings (Fig. 1c ). We apply this analysis to the full brain. \nWe used a regularized linear regression framework that was solved for \neach participant separately. In this framework, the modelled data, y, \nconsist of the brain activity measurements (n images × p voxels) and \nthe predictors, X , consist of MPNet embeddings for each image (n \nimages × 768 MPNet_dimensions).\nWe set aside the shared 515 NSD images seen three times by all \nparticipants as a test set. We used fractional ridge regression54 to esti-\nmate the parameters, ĥ (p voxels × MPNet_dimensions) for 20 different \nregularization fractions (0.05 to 1 in increments of 0.05), using 5-fold \ncross-validation. The fraction that best predicted each embedding fea-\nture after cross-validation was identified, and used as the final model. \nT o evaluate the model, we computed the Pearson correlation for each \nvoxel between the predicted activities and the true activities on the test \nset. The group-level statistics reported in the Article are performed \nusing two-tailed t-tests across the eight NSD participants, and corrected \nfor multiple comparisons using the Benjamini–Hochberg procedure \nfor controlling the false discovery rate with P = 0.05.\nEncoding-model-based brain activity predictions\nOur encoding model allows us to predict the brain activities from any \nsentence. That is, we can predict the activities that would be evoked if \nthe participant saw an image captioned by that sentence. T o this end, \nwe simply write a sentence, project it in LLM embedding space and use \nthe resulting embedding as input to our encoding model. T o test this \napproach, we reproduced contrasts from the neuroscientific litera -\nture (Fig. 2a). In each contrast, we write five sentences for each group, \naverage the predicted activities and plot the contrast between these \nactivities on brain maps (unlike all other maps in this Article, there is no \ncorrection for false discovery rate). We did not have a precise method \nfor selecting these sentences, and simply attempted to make them \nrepresentative of the contrasts we aimed to reproduce. The sentences \nwe used for each contrast are shown below.\n – People\n –’Man with a beard smiling at the camera.’\n –’Some children playing.’\n –’Her face was beautiful.’\n –’Woman and her daughter playing.’\n –’Close up of a face of young boy.\n – Places\n –’A view of a beautiful landscape.’\n –’Houses along a street.’\n –’City skyline with blue sky.’\n –’Woodlands in the morning.’\n –’A park with bushes and trees in the distance.’\n – Food\n –’A plate of food with vegetables.’\n –’A hamburger with fries.’\n –’A bowl of fruit.’\n –’A plate of spaghetti.’\n –’A bowl of soup.\nDecoding of LLM embeddings from brain data\nWe decoded captions from visually evoked activity by learning a linear \nmapping from brain activity to the LLM caption embeddings (this \nmapping can be seen as the inverse mapping to the encoding model \ndescribed above), followed by a dictionary look-up scheme56 (Fig. 2b).\nWe apply this analysis on all voxels inside the’streams’ visual ROIs \n(provided by NSD). We used a regularized linear regression framework \nthat was solved for each participant separately. In this framework, the \nmodelled data, y, consist of the captions embeddings (n images × 768 \nMPNet_dimensions) and the predictors, X , consists of brain activity \nmeasurements (n images × p voxels).\nWe set aside a test set to test the performance of the decoding, by \nholding out the shared 515 NSD images seen three times by all partici-\npants. We used fractional ridge regression54 to estimate the parameters, \n̂h (p voxels × 768 MPNet_dimensions), that represent the optimal sets \nof weights to apply to the predictors (Xtrain) to best predict each of the \ncaptions embedding features (y). Specifically, weights were estimated \nfor 20 different regularization fractions (0.05 to 1 in increments of \n0.05), using 5-fold cross-validation. The fraction that best predicted \neach embedding feature after cross-validation was identified, and the \nresulting model was evaluated on the test set by using the correspond-\ning weights to predict the captions embeddings.\nT o quantify the accuracy of our test predictions, we compared the \nPearson correlation between the predicted embedding and the target \ntest embedding and plotted a participant-wise kernel density estimate \nof these correlations. As a noise ceiling, we use the internal consistency \nof the five human-generated captions in COCO. T o this end, we com-\npute the Pearson correlation between the LLM embeddings of each of \nthe five captions and the averaged embedding of the four others and \naverage the resulting five correlations.\nT o obtain a caption reconstruction, we used a simple dictionary \nlook-up scheme. We took the 3.1 million captions from the Google \nconceptual captions dataset 57 and embedded these captions using \nMPNet, yielding a look-up dictionary D with dimensionality 3.1 million \ncaptions × 768 MPNet_dimensions. For each embedding predicted \nfrom the brain data, we computed the Pearson correlation with each \nof the captions in the dictionary. The caption that was closest to the \npredicted embedding was chosen as the reconstructed caption.\nRCNNs\nOur RCNN models are derived from vNet, a ten-layer convolutional \ndeep neural network architecture designed to closely mirror the pro-\ngressive increase in foveal receptive field sizes found along the human \nventral stream, as estimated by population receptive fields63. In con-\ntrast to previous instances of vNet, our network is recurrent, including \nboth lateral and top-down recurrent connections following a convolu-\ntional pattern, as implemented by Kietzmann et al.115.\nWe used the COCO dataset for training. As the NSD dataset is based \non a subset of COCO, we removed the 73,000 images of the NSD dataset \nfrom the training and validation sets, and used them as our testing set \n(that is, the networks did not see any of the NSD images during training, \nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1230\nArticle https://doi.org/10.1038/s42256-025-01072-0\nnor in validation). This resulted in 48,236 COCO images for training, \n2,051 for validation and the 73,000 images part of both COCO and \nNSD for testing. For rectangle images, we took the largest possible \nsquare crop, as was done for the NSD experimental stimuli. Images \nwere resized to 128 × 128 pixels.\nWe trained our recurrent vNet to map from pixels, that is, COCO \nimages, to LLM embeddings (that is, MPNet embeddings of COCO \ncaptions extracted as described in ‘LLM embeddings for NSD stimuli’ \nsection). The readout layer therefore was 768-dimensional, to match \nMPNet embeddings (we did not apply a traditional nonlinearity softmax \nor sigmoid activation function to the readout, as MPNet embeddings \ncan be both positive and negative). The objective of the network was \nto minimize the cosine distance between the predicted and the target \nLLM embedding. T o account for possible variation due to the network \nrandomly initialized parameters, we trained ten instances with differ-\nent random seeds68.\nAs a stringently controlled comparison model, we trained a sepa-\nrate vNet with identical architecture on a category objective (that is, \nminimizing cosine distance using a multi-hot encoding of the category \nlabels provided in the COCO dataset for each image, this model has a \nsigmoid activation function, as is usual for multiclass categorization). \nAgain, we trained ten instances with different random seeds.\nT o show that the advantage of training on LLM embeddings is not \nrestricted to this current RCNN architecture, we reproduced these \nresults using a ResNet5074 architecture instead of our RCNNs (one seed \neach). We used non-pretrained ResNet50, which we trained to predict \neither LLM embeddings or category labels, as we did for our RCNNs.\nAll networks were trained using an Adam optimizer with a learning \nrate of 5 × 10−2 and an epsilon of 1 × 10−1 for 200 epochs, with a warm-up \nphase of 10 epochs where the learning rate was linearly increased, \nfollowed by a cosine decay. We used a batch size of 96 for RCNNs and \n512 for ResNets.\nRCNN fine-tuning\nT o test if category labels (respectively LLM embeddings) can be \ndecoded from LLM-trained (respectively category-trained) RCNN \nactivities, we performed fine-tuning experiments. We collected activi-\nties for the last layer and timestep from each of the ten instances of each \nnetwork on the entire NSD dataset (collecting activities in this way is \nequivalent to freezing the weights of the network but does not require \nrecomputing the activations at each epoch). We used the first 71,000 \nimages of NSD as a training set and set aside the last 2,000 as a test set. \nWe trained linear readouts to decode multi-hot category labels (respec-\ntively LLM embeddings) from the activities of LLM-trained (respec -\ntively category-trained) networks, by minimizing the cosine distance \nbetween prediction and target (as described above for training the full \nRCNN; the readout activation, optimizer and training hyperparameters \nwere also the same as for training the full RCNN). We then average the \ntest performance across the ten network instances with different seeds. \nAs a noise floor, we computed the mean LLM embedding (respectively \nmulti-hot vector) across the 48,238 images used to train the RCNNs \nand computed the mean cosine distance with the LLM embedding \n(respectively multi-hot vector) of the 2,051 validation images.\nOther ANNs\nWe tested several other ANNs. These include:\nSupervised models (object category). \n – We trained an RCNN on object classification on the ecoset data -\nset63. T o help the network deal with this larger dataset, we doubled \nthe number of channels. Otherwise this network was identical to \nthe previous RCNNs.\n – CORnet-S116 trained on imagenet81, taken from thingsvision117.\n – Alexnet118 trained on imagenet, taken from brainscore80.\n – Alexnet-gn trained on imagenet, taken from ref. 65.\n – resnet50 trained on imagenet, taken from brainscore.\n – Nf-resnet50 trained on imagenet (best-performing CNN on  \npredicting NSD data in ref. 79, taken from timm119).\nSupervised models (scene category). \n – We trained a ResnNet50 trained on scene categorization on the \nplaces365 dataset82.\n – A ResNet50 trained on scene categorization on the taskonomy \ndataset83, taken from https://github.com/StanfordVL/taskonomy.\nSemi-Supervised models. \n – Resnext101_32x8d_wsl 84, trained on 914 million public images \n(best brainscore model available to download), taken from https://\npytorch.org/hub/facebookresearch_WSL-Images_resnext/.\n – CLIP_RN50_imgs (that is, the visual stream of CLIP with a ResNet50 \nbackbone)43, trained on webimagetext43.\n – CLIP_ViT (that is, the visual stream of CLIP with a vision trans -\nformer backbone), trained on webimagetext, taken from https://\ngithub.com/openai/CLIP.\nUnsupervised models. \n – Alexnet, trained using instance-prototype contrastive learning \non imagenet, taken from ref. 65.\n – ResNet50, trained using SimCLR85 on imagenet, taken from https://\ngithub.com/google-research/simclr.\nPredicting brain activity from ANN activations\nT o compare the representations in our networks to the brain’s repre-\nsentations, we apply a similar RSA approach as described above. First, \nRDMs for all images in the NSD dataset are computed in the layer (and \ntimestep) of interest in the networks. Second, correlations between \nRCNNs and brain RDMs are computed, ROI-wise or in a searchight fash-\nion. T o quantify how well layer L at timestep T predicts brain activity, \nwe computed the Pearson correlation between the RDM for layer L at \ntimestep T and the brain data RDM at each ROI or searchlight location. \nIn the case of our RCNNs, for which we have ten instances with different \nrandom seeds, we compute individual RDMs for each seed and then \naverage correlations with brain data across seeds.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe Natural Scenes Dataset is available at http://naturalscenesdataset.\norg.\nCode availability\nCode for the analyses reported here is available via GitHub at https://\ngithub.com/adriendoerig/visuo_llm (ref. 120).\nReferences\n1. Kanwisher, N. Functional specificity in the human brain: a window \ninto the functional architecture of the mind. Proc. Natl Acad. Sci. \nUSA 107, 11163–11170 (2010).\n2. Konkle, T. & Oliva, A. A real-world size organization of object \nresponses in occipitotemporal cortex. Neuron 74, 1114–1124 (2012).\n3. Bao, P., She, L., McGill, M. & Tsao, D. Y. A map of object space in \nprimate inferotemporal cortex. Nature 583, 103–108 (2020).\n4. Kriegeskorte, N. et al. Matching categorical object \nrepresentations in inferior temporal cortex of man and monkey. \nNeuron 60, 1126–1141 (2008).\n5. Cichy, R. M., Kriegeskorte, N., Jozwik, K. M., van den Bosch, J. J. F. & \nCharest, I. The spatiotemporal neural dynamics underlying perceived \nsimilarity for real-world objects. NeuroImage 194, 12–24 (2019).\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1231\nArticle https://doi.org/10.1038/s42256-025-01072-0\n6. Kriegeskorte, N. Deep neural networks: a new framework for \nmodeling biological vision and brain information processing. \nAnnu. Rev. Vis. Sci. 1, 417–446 (2015).\n7. Kriegeskorte, N. & Douglas, P. K. Cognitive computational \nneuroscience. Nat. Neurosci. 21, 1148–1160 (2018).\n8. DiCarlo, J. J., Zoccolan, D. & Rust, N. C. How does the brain solve \nvisual object recognition? Neuron 73, 415–434 (2012).\n9. Bracci, S. & Op de Beeck, H. P. Understanding human object \nvision: a picture is worth a thousand representations. Annu. Rev. \nPsychol. 74, 113–135 (2023).\n10. Doerig, A. et al. The neuroconnectionist research programme. \nNat. Rev. Neurosci. 24, 431–450 (2023).\n11. Richards, B. A. et al. A deep learning framework for neuroscience. \nNat. Neurosci. 22, 1761–1770 (2019).\n12. Yamins, D. L. K. et al. Performance-optimized hierarchical models \npredict neural responses in higher visual cortex. Proc. Natl Acad. \nSci. USA 111, 8619–8624 (2014).\n13. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep supervised, but \nnot unsupervised, models may explain it cortical representation. \nPLoS Comput. Biol. 10, e1003915 (2014).\n14. Güçlü, U. & van Gerven, M. A. J. Deep neural networks reveal a \ngradient in the complexity of neural representations across the \nventral stream. J. Neurosci. 35, 10005–10014 (2015).\n15. Brandman, T. & Peelen, M. V. Interaction between scene and \nobject processing revealed by human fMRI and MEG decoding. J. \nNeurosci. 37, 7700–7710 (2017).\n16. Sadeghi, Z., McClelland, J. L. & Hoffman, P. You shall know an \nobject by the company it keeps: an investigation of semantic \nrepresentations derived from object co-occurrence in visual \nscenes. Neuropsychologia 76, 52–61 (2015).\n17. Bonner, M. F. & Epstein, R. A. Object representations in the human \nbrain reflect the co-occurrence statistics of vision and language. \nNat. Commun. 12, 4081 (2021).\n18. Ackerman, C. M. & Courtney, S. M. Spatial relations and spatial \nlocations are dissociated within prefrontal and parietal cortex. J. \nNeurophysiol. 108, 2419–2429 (2012).\n19. Chafee, M. V., Averbeck, B. B. & Crowe, D. A. Representing spatial \nrelationships in posterior parietal cortex: single neurons code \nobject-referenced position. Cereb. Cortex 17, 2914–2932 (2007).\n20. Graumann, M., Ciuffi, C., Dwivedi, K., Roig, G. & Cichy, R. M.  \nThe spatiotemporal neural dynamics of object location \nrepresentations in the human brain. Nat. Hum. Behav. 6,  \n796–811 (2022).\n21. Zhang, B. & Naya, Y. Medial prefrontal cortex represents the \nobject-based cognitive map when remembering an egocentric \ntarget location. Cereb. Cortex 30, 5356–5371 (2020).\n22. Bar, M. Visual objects in context. Nat. Rev. Neurosci. 5, 617–629 \n(2004).\n23. Russell, B., Torralba, A., Liu, C., Fergus, R. & Freeman, W. Object \nrecognition by scene alignment. Adv. Neural Inf. Process. Syst. 20, \n(2007).\n24. Võ, M. L.-H., Boettcher, S. E. & Draschkow, D. Reading scenes: \nhow scene grammar guides attention and aids perception in \nreal-world environments. Curr. Opin. Psychol. 29, 205–210 (2019).\n25. Kaiser, D., Quek, G. L., Cichy, R. M. & Peelen, M. V. Object vision in \na structured world. Trends Cogn. Sci. 23, 672–685 (2019).\n26. Võ, M. L.-H. The meaning and structure of scenes. Vis. Res. 181, \n10–20 (2021).\n27. Epstein, R. A. & Baker, C. I. Scene perception in the human brain. \nAnnu. Rev. Vis. Sci. 5, 373–397 (2019).\n28. Bartnik, C. G. & Groen, I. I. A. Visual perception in the human brain: \nhow the brain perceives and understands real-world scenes. In \nOxford Research Encyclopedia of Neuroscience (2023).\n29. Epstein, R. A. & Kanwisher, N. A cortical representation of the \nlocal visual environment. Nature 392, 598–601 (1998).\n30. Epstein, R., Harris, A., Stanley, D. & Kanwisher, N. The \nparahippocampal place area: recognition, navigation, or \nencoding? Neuron 23, 115–125 (1999).\n31. Epstein, R. A. Parahippocampal and retrosplenial contributions \nto human spatial navigation. Trends Cogn. Sci. 12, 388–396 \n(2008).\n32. Groen, I. I. A., Ghebreab, S., Prins, H., Lamme, V. A. F. & Scholte, \nH. S. From image statistics to scene gist: evoked neural activity \nreveals transition from low-level natural image structure to scene \ncategory. J. Neurosci. 33, 18814–18824 (2013).\n33. Stansbury, D. E., Naselaris, T. & Gallant, J. L. Natural scene \nstatistics account for the representation of scene categories in \nhuman visual cortex. Neuron 79, 1025–1034 (2013).\n34. Groen, I. I. et al. Distinct contributions of functional and deep \nneural network features to representational similarity of scenes in \nhuman brain and behavior. eLife 7, e32962 (2018).\n35. Brown, T. B. et al. Language models are few-shot learners. Adv. \nNeural Inf. Process. Syst. 33, 1877–1901 (2020).\n36. Cer, D. et al. Universal sentence encoder for English. In Proc. 2018 \nConference on Empirical Methods in Natural Language Processing: \nSystem Demonstrations 169–174 (Association for Computational \nLinguistics, 2018).\n37. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nPreprint at https://doi.org/10.48550/arXiv.1810.04805 (2018).\n38. Arora, S., Liang, Y. & Ma, T. A simple but tough-to-beat baseline for \nsen-tence embeddings. In International Conference on Learning \nRepresentations (2017).\n39. Song, K., Tan, X., Qin, T., Lu, J. & Liu, T.-Y. MPNet: masked and \npermuted pre-training for language understanding. Adv. Neural \nInf. Process. Syst. 33, 16857–16867 (2020).\n40. Lu, J., Batra, D., Parikh, D. & Lee, S. ViLBERT: pretraining \ntask-agnostic visiolinguistic representations for \nvision-and-language tasks. Adv. Neural Inf. Process. Syst. 32 \n(2019).\n41. Tan, H. & Bansal, M. LXMERT: learning cross-modality encoder \nrepresentations from transformers. Preprint at https://doi.org/ \n10.48550/arXiv.1908.07490 (2019).\n42. Pramanick, S. et al. VoLTA: vision-language transformer with weakly- \nsupervised local-feature alignment. Preprint at https://doi.org/ \n10.48550/arXiv.2210.04135 (2022).\n43. Radford, A. et al. Learning transferable visual models from natural \nlanguage supervision. In International Conference on Machine \nLearning 8748–8763 (PMLR, 2021).\n44. Du, Y., Liu, Z., Li, J. & Zhao, W. X. A survey of vision-language \npre-trained models. Preprint at https://doi.org/10.48550/\narXiv.2202.10936 (2022).\n45. Chen, F.-L. et al. VLP: a survey on vision-language pre-training. \nMach. Intell. Res. 20, 38–56 (2023).\n46. Allen, E. J. et al. A massive 7T fMRI dataset to bridge cognitive \nneuroscience and artificial intelligence. Nat. Neurosci. 25, 116–126 \n(2022).\n47. Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context. In \nComputer Vision–ECCV 2014: 13th European Conference 740–755 \n(Springer, 2014).\n48. Chen, X. et al. Microsoft COCO captions: data collection \nand evaluation server. Preprint at https://doi.org/10.48550/\narXiv.1504.00325 (2015).\n49. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. \nSyst. 30, (2017).\n50. Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I. & Specia, L. \nSemEval-2017 Task 1: semantic textual similarity multilingual \nand crosslingual focused evaluation. In Proc. 11th International \nWorkshop on Semantic Evaluation (SemEval-2017) 1–14 \n(Association for Computational Linguistics, 2017).\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1232\nArticle https://doi.org/10.1038/s42256-025-01072-0\n51. Kriegeskorte, N., Mur, M. & Bandettini, P. Representational \nsimilarity analysis—connecting the branches of systems \nneuroscience. Front. Syst. Neurosci. 2, 4 (2008).\n52. Kriegeskorte, N. & Kievit, R. A. Representational geometry: \nintegrating cognition, computation, and the brain. Trends Cogn. \nSci. 17, 401–412 (2013).\n53. Nili, H. et al. A toolbox for representational similarity analysis. \nPLoS Comput. Biol. 10, e1003553 (2014).\n54. Rokem, A. & Kay, K. Fractional ridge regression: a fast, \ninterpretable reparameterization of ridge regression. Gigascience \n9, giaa133 (2020).\n55. Pennock, I. M. L. et al. Color-biased regions in the ventral visual \npathway are food selective. Curr. Biol. 33, 134–146.e4 (2023).\n56. Kay, K. N., Naselaris, T., Prenger, R. J. & Gallant, J. L. Identifying \nnatural images from human brain activity. Nature 452, 352–355 \n(2008).\n57. Sharma, P., Ding, N., Goodman, S. & Soricut, R. Conceptual \ncaptions: a cleaned, hypernymed, image alt-text dataset for \nautomatic image captioning. In Proc. 56th Annual Meeting of \nthe Association for Computational Linguistics (Volume 1: Long \nPapers) (eds Gurevych, I. & Miyao, Y.) 2556–2565 (Association for \nComputational Linguistics, 2018).\n58. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. Enriching word \nvectors with subword information. Trans. Assoc. Comput. Linguist. \n5, 135–146 (2016).\n59. Joulin, A., Grave, E., Bojanowski, P. & Mikolov, T. Bag of tricks for \nefficient text classification. Preprint at https://doi.org/10.48550/\narXiv.1607.01759 (2016).\n60. Pennington, J., Socher, R. & Manning, C. GloVe: global vectors  \nfor word representation. In Proc. 2014 Conference on Empirical \nMethods in Natural Language Processing (EMNLP) (eds Moschitti, A.,  \nPang, B. & Daelemans, W.) 1532–1543 (Association for Computa-\ntional Linguistics, 2014).\n61. Kaplan, J. et al. Scaling laws for neural language models. Preprint \nat https://doi.org/10.48550/arXiv.2001.08361 (2020).\n62. Hernandez, D., Kaplan, J., Henighan, T. & McCandlish, S. \nScaling laws for transfer. Preprint at https://doi.org/10.48550/\narXiv.2102.01293 (2021).\n63. Mehrer, J., Spoerer, C. J., Jones, E. C., Kriegeskorte, N. & \nKietzmann, T. C. An ecologically motivated image dataset for \ndeep learning yields better models of human vision. Proc. Natl \nAcad. Sci. USA 118, e2011417118 (2021).\n64. Kietzmann, T. C., McClure, P. & Kriegeskorte, N. Deep neural \nnetworks in computational neuroscience. In Oxford Research \nEncyclopedia of Neuroscience (2019).\n65. Konkle, T. & Alvarez, G. A. A self-supervised domain-general \nlearning framework for human ventral stream representation. Nat. \nCommun. 13, 491 (2022).\n66. Zhuang, C. et al. Unsupervised neural network models of the \nventral visual stream. Proc. Natl Acad. Sci. USA 118, e2014196118 \n(2021).\n67. Spoerer, C. J., Kietzmann, T. C., Mehrer, J., Charest, I. & \nKriegeskorte, N. Recurrent neural networks can explain flexible \ntrading of speed and accuracy in biological vision. PLoS Comput. \nBiol. 16, e1008215 (2020).\n68. Mehrer, J., Spoerer, C. J., Kriegeskorte, N. & Kietzmann, T. C. \nIndividual differences among deep neural network models. Nat. \nCommun. 11, 5725 (2020).\n69. Hong, H., Yamins, D. L. K., Majaj, N. J. & DiCarlo, J. J. Explicit \ninformation for category-orthogonal object properties increases \nalong the ventral stream. Nat. Neurosci. 19, 613–622 (2016).\n70. Conwell, C., Prince, J. S., Kay, K. N., Alvarez, G. A. & Konkle, T. A \nlarge-scale examination of inductive biases shaping high-level \nvisual representation in brains and machines. Nat. Commun. 15, \n9383 (2024).\n71. Han, Y., Poggio, T. & Cheung, B. System identification of neural \nsystems: if we got it right, would we know? In International \nConference on Machine Learning 12430–12444 (PMLR, 2023).\n72. Storrs, K. R., Kietzmann, T. C., Walther, A., Mehrer, J. & \nKriegeskorte, N. Diverse deep neural networks all predict human \ninferior temporal cortex well, after training and fitting. J. Cogn. \nNeurosci. 33, 2044–2064 (2021).\n73. Bo, Y., Soni, A., Srivastava, S. & Khosla, M. Evaluating \nrepresentational similarity measures from the lens of functional \ncorrespondence. Preprint at https://doi.org/10.48550/\narXiv.2411.14633 (2024).\n74. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \nimage recognition. In Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition 770–778 (2015).\n75. Ungerleider, LG., Mishkin, L. in Analysis of Visual Behavior  \n(eds Goodale, M. et al.) Ch. 18, 549 (MIT Press, 1982).\n76. Goodale, M. A. & Milner, A. D. Separate visual pathways for \nperception and action. Trends Neurosci. 15, 20–25 (1992).\n77. Tanaka, K. Inferotemporal cortex and object vision. Annu. Rev. \nNeurosci. 19, 109–139 (1996).\n78. Ishai, A., Ungerleider, L. G., Martin, A., Schouten, J. L. & Haxby, J. V. \nDistributed representation of objects in the human ventral visual \npathway. Proc. Natl Acad. Sci. USA 96, 9379–9384 (1999).\n79. Conwell, C., Prince, J. S., Kay, K. N., Alvarez, G. A. & Konkle, T. What \ncan 1.8 billion regressions tell us about the pressures shaping \nhigh-level visual representation in brains and machines? Nat. \nCommun. 15, 9383 (2023).\n80. Schrimpf, M. et al. Brain-Score: which artificial neural network  \nfor object recognition is most brain-like? Preprint at bioRxiv \nhttps://doi.org/10.1101/407007 (2018).\n81. Russakovsky, O. et al. ImageNet large scale visual recognition \nchallenge. Int. J. Comput. Vis., 115, 211–252 (2014).\n82. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A. & Torralba, A. Places: \na 10 million image database for scene recognition. IEEE Trans. \nPattern Anal. Mach. Intell. 40, 1452–1464 (2018).\n83. Zamir, A. et al. Taskonomy: disentangling task transfer learning. \nIn Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition 3712–3722 (2018).\n84. Mahajan, D. et al. Exploring the limits of weakly supervised \npretraining. In Proc. European Conference on Computer Vision \n181–196 (2018).\n85. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple frame work \nfor contrastive learning of visual representations. In International \nconference on machine learning 1597–1607 (PMLR, 2020).\n86. Ratan Murty, N. A., Bashivan, P., Abate, A., DiCarlo, J. J. & \nKanwisher, N. Computational models of category-selective brain \nregions enable high-throughput tests of selectivity. Nat. Commun. \n12, 5540 (2021).\n87. Güçlü, U. & van Gerven, M. A. J. Semantic vector space \nmodels predict neural responses to complex visual stimuli. In \nInternational Conference on Machine Learning 1597–1607 (PMLR, \n2015).\n88. Frisby, S. L., Halai, A. D., Cox, C. R., Lambon Ralph, M. A. & Rogers, \nT. T. Decoding semantic representations in mind and brain. Trends \nCogn. Sci. 27, 258–281 (2023).\n89. Greene, M. R., Baldassano, C., Esteva, A., Beck, D. M. & Fei-Fei, L. \nVisual scenes are categorized by function. J. Exp. Psychol. Gen. \n145, 82–94 (2016).\n90. Greene, M. R. Statistics of high-level scene context. Front. \nPsychol. 4, 777 (2013).\n91. Henderson, J. M. & Ferreira, F. in The Interface of Language,  \nVision, and Action: Eye Movements and the Visual World  \n(ed. Henderson, J. M.) Vol. 399, 1–58 (Psychology Press, 2004).\n92. Greene, M. R. & Oliva, A. The briefest of glances: the time course \nof natural scene understanding. Psychol. Sci. 20, 464–472 (2009).\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234\n 1233\nArticle https://doi.org/10.1038/s42256-025-01072-0\n93. Malcolm, G. L. & Shomstein, S. Object-based attention in \nreal-world scenes. J. Exp. Psychol. Gen. 144, 257–263 (2015).\n94. Biederman, I. Perceiving real-world scenes. Science 177, 77–80 (1972).\n95. Greene, M. R. Scene perception and understanding. In Oxford \nResearch Encyclopedia of Psychology (2023).\n96. Potter, M. C. Meaning in visual search. Science 187, 965–966 (1975).\n97. Carlson, T. A., Simmons, R. A., Kriegeskorte, N. & Slevc, L. R. The \nemergence of semantic meaning in the ventral temporal pathway. \nJ. Cogn. Neurosci. 26, 120–131 (2014).\n98. Contier, O., Baker, C. I. & Hebart, M. N. Distributed representations \nof behaviorally-relevant object dimensions in the human visual \nsystem. Nat. Hum. Behav. 8, 2179–2193 (2024).\n99. Marblestone, A. H., Wayne, G. & Kording, K. P. Toward an \nintegration of deep learning and neuroscience. Front. Comput. \nNeurosci. 10, 94 (2016).\n100. Golan, T. et al. Deep neural networks are not a single hypothesis \nbut a language for expressing computational hypotheses. Behav. \nBrain Sci. 46, e392 (2023).\n101. Conwell, C. et al. Monkey see, model knew: large language \nmodels accurately predict human AND macaque visual brain \nactivity. In UniReps: 2nd Edition of the Workshop on Unifying \nRepresentations in Neural Models (2024).\n102. Popham, S. F. et al. Visual and linguistic semantic representations \nare aligned at the border of human visual cortex. Nat. Neurosci. \n24, 1628–1636 (2021).\n103. Wang, A. Y., Kay, K., Naselaris, T., Tarr, M. J. & Wehbe, L. Better \nmodels of human high-level visual cortex emerge from natural \nlanguage supervision with a large and diverse dataset. Nat. Mach. \nIntell. 5, 1415–1426 (2023).\n104. Tang, J., Du, M., Vo, V. A., Lal, V. & Huth, A. G. Brain encoding \nmodels based on multimodal transformers can transfer  \nacross language and vision. Adv. Neural Inf. Process. Syst. 36, \n29654–29666 (2023).\n105. Kay, K., Bonnen, K., Denison, R. N., Arcaro, M. J. & Barack, D. L. Tasks \nand their role in visual neuroscience. Neuron 111, 1697–1713 (2023).\n106. Çukur, T., Nishimoto, S., Huth, A. G. & Gallant, J. L. Attention \nduring natural vision warps semantic representation across the \nhuman brain. Nat. Neurosci. 16, 763–770 (2013).\n107. Goldstein, A. et al. Alignment of brain embeddings and artificial \ncontextual embeddings in natural language points to common \ngeometric patterns. Nat. Commun. 15, 2768 (2024).\n108. Schrimpf, M. et al. The neural architecture of language: \nintegrative modeling converges on predictive processing. Proc. \nNatl Acad. Sci. USA 118, e2105646118 (2021).\n109. Zada, Z. et al. A shared linguistic space for transmitting our \nthoughts from brain to brain in natural conversations. Neuron 112, \n3211–3222 (2023).\n110. Cadieu, C. F. et al. Deep neural networks rival the representation \nof primate IT cortex for core visual object recognition. PLoS \nComput. Biol. 10, e1003963 (2014).\n111. Bird, S., Klein, E., & Loper, E. Natural Language Processing with \nPython: Analyzing Text with the Natural Language Toolkit (Reilly \nMedia, 2009).\n112. Kriegeskorte, N., Goebel, R. & Bandettini, P. A. Information- \nbased functional brain mapping. Proc. Natl Acad. Sci. USA 103, \n3863–3868 (2006).\n113. Haynes, J. D. & Rees, G. Predicting the stream of consciousness \nfrom activity in human visual cortex. Curr. Biol. 15, 1301–1307 \n(2005).\n114. Benjamini, Y. & Hochberg, Y. Controlling the false discovery rate: \na practical and powerful approach to multiple testing. J. R. Stat. \nSoc. Ser. B 57, 289–300 (1995).\n115. Kietzmann, T. C. et al. Recurrence is required to capture the \nrepresentational dynamics of the human visual system. Proc. Natl \nAcad. Sci. USA 116, 21854–21863 (2019).\n116. Kubilius, J. et al. CORnet: modeling the neural mechanisms of \ncore object recognition. Preprint at bioRxiv https://doi.org/ \n10.1101/408385 (2018).\n117. Muttenthaler, L. & Hebart, M. N. THINGSvision: a Python toolbox \nfor streamlining the extraction of activations from deep neural \nnetworks. Front. Neuroinform. 15, 679838 (2021).\n118. Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet classification \nwith deep convolutional neural networks. In Advances in Neural \nInformation Processing Systems (eds Pereira, F. et al.) Vol. 25 \n(Curran Associates, Inc., 2012).\n119. timmdocs: documentation for Ross Wightman’s timm image \nmodel library. GitHub https://github.com/fastai/timmdocs (2025).\n120. Doerig, A. Visuo_llm (v1.0). Zenodo https://doi.org/10.5281/\nZENODO.15282176 (2025).\nAcknowledgements\nWe acknowledge support by SNF grant n.203018 (A.D.), the ERC \nstg grant 101039524 TIME (A.D. and T.C.K.), NIH grant R01EY034118 \n(K.K.), an ERC stg grant 759432 START (I.C.), a Courtois Chair in \ncomputational neuroscience (Charest) and an NSERC Discovery grant \n(I.C.). Collection of the NSD dataset was supported by NSF IIS-1822683 \n(K.K.) and NSF IIS-1822929 (T.N.). We thank G. Tuckute and P. Sulewski \nfor helpful discussions.\nAuthor contributions\nA.D., T.C.K., K.K. and I.C. wrote the paper. T.N. provided feedback on \nthe writing. A.D. performed the research and analysed data. T.C.K., \nK.K. and I.C. supervised the research. A.D., T.C.K., T.N., K.K. and I.C. \nconceived research. E.A. and Y.W. collected data.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s42256-025-01072-0.\nCorrespondence and requests for materials should be addressed to \nIan Charest.\nPeer review information Nature Machine Intelligence thanks the \nanonymous reviewer(s) for their contribution to the peer review  \nof this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\nNature Machine Intelligence | Volume 7 | August 2025 | 1220–1234 1234\nArticle https://doi.org/10.1038/s42256-025-01072-0\n1Department of Psychology and Education, Freie Universität Berlin, Berlin, Germany. 2Institute of Cognitive Science, University of Osnabrück, Osnabrück, \nGermany. 3Bernstein Center for Computational Neuroscience, Berlin, Germany. 4Center for Magnetic Resonance Research, Department of Radiology, \nUniversity of Minnesota, Minneapolis, MN, USA. 5Department of Psychology, University of Minnesota, Minneapolis, MN, USA. 6Graduate Program in \nCognitive Science, University of Minnesota, Minneapolis, MN, USA. 7Department of Neuroscience, University of Minnesota, Minneapolis, MN, USA. \n8cerebrUM, Département de Psychologie, Université de Montréal, Montreal, Quebec, Canada. 9These authors contributed equally: Adrien Doerig,  \nTim C. Kietzmann. 10These authors jointly supervised this work: Kendrick Kay, Ian Charest.  e-mail: ian.charest@umontreal.ca\n\n\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.704337477684021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5229055285453796
    },
    {
      "name": "Brain activity and meditation",
      "score": 0.5175349116325378
    },
    {
      "name": "Natural language processing",
      "score": 0.39694175124168396
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3272518217563629
    },
    {
      "name": "Psychology",
      "score": 0.217521071434021
    },
    {
      "name": "Neuroscience",
      "score": 0.18483173847198486
    },
    {
      "name": "Electroencephalography",
      "score": 0.18166890740394592
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210152641",
      "name": "Bernstein Center for Computational Neuroscience Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I170658231",
      "name": "Osnabrück University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210158702",
      "name": "Resonance Research (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130238516",
      "name": "University of Minnesota",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ]
}