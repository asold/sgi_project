{
  "title": "A Universal Representation Transformer Layer for Few-Shot Image Classification",
  "url": "https://openalex.org/W3036139671",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101685518",
      "name": "Liu Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2618379995",
      "name": "Hamilton, William",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352924574",
      "name": "Long, Guodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107264555",
      "name": "Jiang Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3191988255",
      "name": "Larochelle, Hugo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034587791",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3012473455",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3034312118",
    "https://openalex.org/W2472819217",
    "https://openalex.org/W2964026991",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W3035543253",
    "https://openalex.org/W2995589713",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W3012209922",
    "https://openalex.org/W2970990534",
    "https://openalex.org/W2581955877",
    "https://openalex.org/W3002740131",
    "https://openalex.org/W2753160622",
    "https://openalex.org/W2995253937",
    "https://openalex.org/W2963741406",
    "https://openalex.org/W2415204069"
  ],
  "abstract": "Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization. Our code is available at https://github.com/liulu112601/URT.",
  "full_text": "A Universal Representation Transformer Layer\nfor Few-Shot Image Classiﬁcation\nLu Liu1,2∗, William Hamilton1,3†, Guodong Long2, Jing Jiang2, Hugo Larochelle1,4†\n1 Mila, 2 Australian AI Institute, UTS, 3 McGill University, 4 Google Research, Brain Team\nCorrespondence to lu.liu.cs@icloud.com\nAbstract\nFew-shot classiﬁcation aims to recognize unseen classes when presented with only\na small number of samples. We consider the problem of multi-domain few-shot\nimage classiﬁcation, where unseen classes and examples come from diverse data\nsources. This problem has seen growing interest and has inspired the development\nof benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting\nis to effectively integrate the feature representations from the diverse set of train-\ning domains. Here, we propose a Universal Representation Transformer (URT)\nlayer, that meta-learns to leverage universal features for few-shot classiﬁcation by\ndynamically re-weighting and composing the most appropriate domain-speciﬁc\nrepresentations. In experiments, we show that URT sets a new state-of-the-art\nresult on Meta-Dataset. Speciﬁcally, it achieves top-performance on the highest\nnumber of data sources compared to competing methods. We analyze variants of\nURT and present a visualization of the attention score heatmaps that sheds light\non how the model performs cross-domain generalization. Our code is available at\nhttps://github.com/liulu112601/URT\n1 Introduction\nLearning tasks from small data remains a challenge for machine learning systems, which show a\nnoticeable gap compared to the ability of humans to understand new concepts from few examples. A\npromising direction to address this challenge is developing methods that are capable of performing\ntransfer learning across the collective data of many tasks. Since machine learning systems generally\nimprove with the availability of more data, a natural assumption is that few-shot learning systems\nshould beneﬁt from leveraging data across many different tasks and domains—even if each individual\ntask has limited training data available.\nThis research direction is well captured by the problem of multi-domain few-shot classiﬁcation. In\nthis setting, training and test data spans a number of different domains, each represented by a different\nsource dataset. A successful approach in this multi-domain setting must not only address the regular\nchallenge of few-shot classiﬁcation—i.e., the challenge of having only a handful of examples per\nclass. It must also discover how to leverage (or ignore) what is learned from different domains,\nachieving generalization and avoiding cross-domain interference.\nRecently, Triantaﬁllou et al. [22] proposed a benchmark for multi-domain few-shot classiﬁcation,\nMeta-Dataset, and highlighted some of the challenges that current methods face when training data\nis heterogeneous. Crucially, they found that methods which trained on all available domains would\nnormally obtain improved performance on some domains at the expense of others. Following on their\nwork, progress has been made, which includes the design of adapted hyper-parameter optimization\n∗This work was done while Lu Liu was a research intern with Mila.\n†Canada CIFAR AI Chair\nPreprint. Under review.\narXiv:2006.11702v4  [cs.LG]  2 Sep 2020\nstrategies [19] and more ﬂexible meta-learning algorithms [ 18]. Most notable is SUR (Selecting\nUniversal Representation) [7], a method that relies on a so-called universal representation, extracting\nfrom a collection of pre-trained and domain-speciﬁc neural network backbones. SUR prescribes a\nhand-crafted feature-selection procedure to infer how to weight each backbone for each task at hand,\nand produces an adapted representation for each task. This was shown to lead to some of the best\nperformances on Meta-Dataset.\nIn SUR, the classiﬁcation procedure for each task is ﬁxed and not learned. Thus, except for the\nunderlying universal representation, there is no transfer learning performed with regards to how\nclassiﬁcation rules are inferred across tasks and domains. Yet, cross-domain generalization might be\nbeneﬁcial in that area as well, in particular when tasks have only few examples per class.\nPresent work. To explore this question, we propose the Universal Representation Transformer\n(URT) layer, which can effectively learn to transform a universal representation into task-adapted\nrepresentations. The URT layer is inspired from Transformer networks [23] and uses an attention\nmechanism to learn to retrieve or blend the appropriate backbones to use for each task. By training\nthis layer across few-shot tasks from many domains, it can support transfer across these tasks.\nWe show that our URT layer on top of a universal representation’s pre-trained backbones sets a new\nstate-of-the-art performance on Meta-Dataset. It succeeds at outperforming SUR on 4 dataset sources\nwithout impairing accuracy on the others. This leads to top performance on 7 dataset sources when\ncomparing to a set of competing methods. To interpret the strategy that URT learns to weigh the\nbackbones from different domains, we visualize the attention scores for both seen and unseen domains\nand ﬁnd that our model generates meaningful weights for the pre-trained domains. A comprehensive\nanalysis on variants and ablations of the URT layer is provided to show the importance of various\ncomponents of URT, notably the number of attention heads.\n2 Few-Shot Classiﬁcation\n2.1 Problem Setting\nIn this section, we will introduce the problem setting for few-shot classiﬁcation and the formulation\nof meta-learning for few-shot classiﬁcation. Few-shot classiﬁcation aims to classify samples where\nonly few examples are available for each class. We describe a few-shot learning classiﬁcation task as\nthe pair of examples, comprising of a support set Sto deﬁne the classiﬁcation task and the query set\nQof samples to be classiﬁed.\nMeta-learning is a technique that aims to model the problem of few-shot classiﬁcation as learning to\nlearn from instances of few-shot classiﬁcation tasks. The most popular way to train a meta-learning\nmodel is with episodic training. Here, tasks T = (Q,S) are sampled from a larger dataset by taking\nsubsets of the dataset to build a support set Sand a query set Qfor the task. A common approach is\nto sample N-way-K-shot tasks, each time selecting a random subset of N classes from the original\ndataset and choosing only Kexamples for each class to add to the support set S.\nThe meta-learning problem can then be formulated by the following optimization:\nmin\nΘ\nE(S,Q)∼p(T) [L(S,Q, Θ)] , L(S,Q, Θ) = 1\n|Q|\n∑\n(x,y)∼Q\n−log p(y|x,S; Θ) +λΩ(Θ), (1)\nwhere p(T) is the distribution of tasks, Θ are the parameters of the model and p(y|x,S; Θ) is the\nprobability assigned by the model to label yof query example x (given the support set S), and Ω(Θ)\nis an optional regularization term on the model parameters with factor λ.\nConventional few-shot classiﬁcation targets the setting of N-way-K-shot, where the number of\nclasses and examples are ﬁxed in each episode. Popular benchmarks that follow this approach\ninclude Omniglot [10]) or benchmarks made of subsets of ImageNet, such as miniImageNet [24] and\ntieredImageNet [17]. In such benchmarks, the tasks used for training cover a set of classes that is\ndisjoint from the classes found in the test set of tasks. However, with the training and test sets tasks\ncoming from a single dataset/domain, the distribution of tasks found in either sets is similar and lacks\nvariability, which may be unrealistic in practice.\nIt is in this context that Triantaﬁllou et al. [22] proposed Meta-Dataset, as a further step towards\nlarge-scale, multi-domain few shot classiﬁcation. Meta-Dataset includes ten datasets (domains), with\n2\neight of them available for training. Additionally, each task sampled in the benchmark varies in the\nnumber of classes N, with each class also varying in the number of shots K. As in all few-shot\nlearning benchmarks, the classes used for training and testing do not overlap.\n2.2 Background and Related Work\nTransfer by ﬁne-tuning A simple and effective method for few-shot classiﬁcation is to perform\ntransfer learning by ﬁrst learning a neural network classiﬁer on all data available for training and\nusing its representation to initialize and then ﬁne-tune neural networks on the few-shot classiﬁcation\ntasks found at test time [4, 22, 6, 19]. Speciﬁcally, Saikia et al. [19] have shown that competitive\nperformance can be reached using a strong hyper-parameter optimization method applied on a\ncarefully designed validation metric appropriate for few-shot learning.\nMeta-Learning Another approach is to use meta-learning to more directly train a model to learn to\nperform few-shot classiﬁcation, in an end-to-end way. A large variety of such models have been ex-\nplored, inspired by memory-augmented networks [20], LSTMs [15] and metric-based classiﬁers [24].\nThe two most popular methods are Prototypical Networks [21] and Model Agnostic Meta-Learning\n(MAML) [8]. Prototypical Networks assume that every class can be represented as a prototype in a\nlearned embedding space (represented by a neural network). Prototypes are calculated as the average\nof the representations of the support examples of each class. A 1-nearest centroid classiﬁer is then\nadopted for classiﬁcation and the neural network representation is trained to facilitate classiﬁcation\nin few-shot tasks directly. MAML models the procedure of learning to learn as a bilevel optimization,\nwhere an outer loop backpropagates loss gradients through an optimization-based inner loop to learn\nits initialization. Triantaﬁllou et al. [22] showed that prototypical networks and MAML could be\ncombined by leveraging prototypes for the initialization of the output weights value in the inner loop.\nRequeima et al. [18] also proposed Conditional Neural Adaptive Processes (CNAPs) for few-shot\nclassiﬁcation, which can be seen as extending prototypical networks with a more sophisticated\narchitecture that allows for improved task adaptation. This architecture was later improved further\nby Bateni et al. [1] with Simple CNAPS, leading to one of the current best methods on Meta-Dataset.\nUniversal Representatons In contrast, our work instead builds on that of Dvornik et al. [7] and\ntheir method SUR (Selecting from Universal Representations). Bilen and Vedaldi [2] introduced the\nterm universal representation to refer to a representation that supports good performance in multiple\ndomains. One proposal towards such a representation is to train different neural networks backbones\nseparately on the data of each available domain, then simply to concatenate the representation learned\nby each. Another is to introduce some parameter sharing between the backbones, by having a single\nnetwork conditioned on the domain of the provenance of each batch of training data [16], e.g. using\nFeature-wise Linear Modulate (FiLM) [14]. SUR proposes to leverage a universal representation\nin few-shot learning tasks with a feature selection procedure that assigns different weights to each\nof the domain-speciﬁc subvectors of the universal representation. The objective is to assign high\nweights only to the domain-speciﬁc representations that are speciﬁcally useful for each few-shot\ntask at hand. The weights are inferred by optimizing a loss on the support set that encourages high\naccuracy of a nearest-centroid classiﬁer. As such, the method does not involve any meta-learning—a\nchoice motivated by the concern that meta-learning may struggle in generalizing to domains that are\ndissimilar to the training domains. SUR achieved some of the best performances on Meta-Dataset.\nHowever, a contribution of our work is to provide evidence that meta-learning can actually be used to\nreplace SUR’s hand-designed inference procedure and improve performance further.\nTransformer NetworksOur meta-learning approach to leverage universal representations is inspired\ndirectly from Transformer networks [23]. Transformer networks are neural network architectures\ncharacterized by the use self-attention mechanisms to solve tasks. Our model structure is inspired by\nthe structure of the dot-product self-attention in the Transformer, which we adapted here to multi-\ndomain few-shot learning by designing appropriate parametrizations for queries, keys and values.\nSelf-attention was explored in the single-domain training regime by Ye et al. [25], however for a\ndifferent purpose, where each representation of individual examples in a task support set is inﬂuenced\nby all other examples. Such a strategy is also employed by CNAPs, but with the latter using FiLM as\nthe conditioning mechanism, instead of self-attention. Regardless, the aim of this paper is to propose\na different strategy. Rather than using self-attention between individual examples in the support set,\nour model uses self-attention to select between different domain-speciﬁc backbones.\n3\nWaxcap\nWaxcap\nFly agaric\nFly agaric\nAverage\n}\nScaled \nDot-product \nAttention\nAttention scores\n( Waxcap)\nAttention scores\n(Fly agaric)\nAttention scores\nfor task\n/uni03B11,1\nAverage\n}\navg\nSupport set \nrepresentation one slot\nDot-product\nScale\nSoftmax\nAttention score\nQuery Key\nSample representation transformed by URT\nScaled \nDot-product \nAttention\n?\nScaled Dot-product Attention\nr(S2)\nr(S1)\nr(Sc) ri(Sc)\ncBackbones\n{ri}\ncBackbones\n{ri}\ncBackbones\n{ri}\ncBackbones\n{ri}\ncBackbones\n{ri} /uni03B1i,c\n/uni03B12,1 /uni03B13,1 /uni03B14,1\n/uni03B11,2 /uni03B12,2 /uni03B13,2 /uni03B14,2\n/uni03B11 /uni03B12 /uni03B13 /uni03B14\nInferring adapted representation for task\nUniversal \nRepresentations\nFigure 1: Illustration of how a single-head URT layer uses a universal representation to produce\na task-speciﬁc representation. This example assumes the use of four backbones, with each color\nillustrating their domain-speciﬁc sub-vector representation in the universal representation.\n3 Universal Representation Transformer Layer\nIn this section, we describe our proposed URT layer, which uses meta-learning episodic training\nto learn how to combine the domain-speciﬁc backbones of a universal representation for any given\nfew-shot learning classiﬁcation task.\nConceptually, the proposed model views the support set Sof a task as providing information on how\nto query and retrieve from the set {ri}of mpre-trained backbones the most appropriate backbone to\nbuild an adapted representation φfor the task.\nWe would like the model to support a variety of strategies on how to retrieve backbones. For example,\nit might be beneﬁcial for the model to retrieve a single backbone from the set, especially if the domain\nof the given task matches perfectly that of a domain found in the training set. Alternatively, if some\nof the training domains beneﬁt from much more training data than others, a better strategy might be\nto attempt some cross-domain generalization towards the few-shot learning task by blending many\nbackbones together, even if none matches the domain of the task perfectly.\nThis motivates us to use dot-product self-attention, inspired by layers of Transformer networks [23].\nFor this reason, we refer to our model as a Universal Representation Transformer (URT) layer.\nAdditionally, since each class of the support set might require a different strategy, we perform\nattention separately for each class and their support set Sc = {x|(x,y) ∈Sand y= c}.\n3.1 Single-Head URT Layer\nWe start by describing an URT layer consisting of a single attention head. An illustration of a\nsingle-head URT layer is shown in Figure 1. Let ri(x) be the output vector of the backbone for\ndomain i. We then write the universal representation as\nr(x) = concat(r1(x),...,r m(x)). (2)\nThis representation provides a natural starting point to obtain a representation of a support set class.\nSpeciﬁcally, we will note\nr(Sc) = 1\n|Sc|\n∑\nx∈Sc\nr(x) (3)\nas the representation for the set Sc. From this, we can describe the URT layer by deﬁning the queries3,\nkeys, the attention mechanism and output of the layer:\nQueries qc: For each class c, we obtain a query through qc = Wqr(Sc) + bq, where we have a\nlearnable query linear transformation represented by matrix Wq and bias bq.\n3Unable to avoid the unfortunate double usage of the term \"query\" due to conﬂicting conventions, we highlight\nthe difference between the query sets Q of few-shot tasks and the queries qc of an attention mechanism.\n4\nAlgorithm 1 Training of URT layer\nInput: Number of tasks τtotal, mpre-trained backbones ;\n1: for τ ∈{1,··· ,τtotal}do\n2: Sample a few-shot task T with support set Sand query set Q;\n3: # Infer adapted representation for task from S\n4: For each class, obtain representation using mpre-trained backbones as in Eq. (3);\n5: Obtain attention scores using Eq. (4,5) for each head using support set S;\n6: # Use adapted representation to predict labels in Qfrom support set S\n7: Compute adapted representation of examples in Sand Qas in Eq. (6,7);\n8: Compute probabilities of label of examples in Qusing Prototypical Network as in Eq. (9);\n9: Compute loss as in Eq. (1,8) and perform gradient descent step on URT parameters Θ;\n10: end for\nKeys ki,c: For each domain iand class c, we deﬁne keys as ki,c = Wkri(Sc)+ bk, using a learnable\nlinear transformation Wk and bk and where ri(Sc) = 1/|Sc|∑\nx∈Sc ri(x), using a similar notation\nas for r(Sc).\nAttention scores αi: as for regular Transformer layers, we use scaled dot-product attention\nαi,c = exp(βi,c)∑\ni′ exp(βi′,c),βi,c = qc⊤ki,c√\nl\n, (4)\nwhere lis the dimensionality of the keys and queries. Then, these per-class scores are aggregated to\nobtain scores for the full support set by averaging\nαi =\n∑\nc αi,c\nN . (5)\nEquipped with these attention scores, the URT layer can now produce an adapted representation for\nthe task (for the support and query set examples) by computing\nφ(x) =\n∑\ni\nαiri(x) . (6)\nAs we can see, this approach has the ﬂexibility of either selecting a single domain-speciﬁc backbone\n(by assigning αi = 1 for a single domain) or blending different domains together (by havingαi >>0\nfor multiple backbones).\n3.2 Multi-Head URT Layer\nThe URT layer described so far can only learn to retrieve a single backbone (or blending of backbones).\nYet, it might be beneﬁcial to retrieve multiple different (blended) backbones, especially for a few-shot\ntask that would include many classes of varying complexity.\nThus, to achieve such diversity in the adapted representation, we also consider URT layers with\nmultiple heads, i.e. where each head corresponds to the calculation of Equation 6 and each head\nhas its own set of parameters (Wq,bq,Wk,bk). Denoting each head now as φh, a multi-head URT\nlayer then produces as its output the concatenation of all of its heads:\nφ(x) = concat(φ1(x),...,φ H(x)). (7)\nEmpirically we found that the randomness in the initialization of head weights alone did not lead\nto uniqueness and being complimentary between the heads, so inspired by Lin et al. [12], we add a\nregularizer to avoid duplication of the attention scores:\nΩ(Θ) = ∥(AA⊤−I)∥F\n2\n, (8)\nwhere ∥·∥F is the Frobenius norm of a matrix and A ∈Rn×m is the matrix for attention scores, with\nAh being the vector of all scores αi for head h. The identity matrix I regularizes each set of attention\nscores to be more focused so that multiple heads can attend to different domain-speciﬁc backbones.\n5\nTable 1: Test performance (mean+CI%95) over 600 few-shot tasks. URT and the most recent methods,\nwhich are listed in the ﬁrst row, are compared on Meta-Dataset [22], which are listed in the ﬁrst row.\nThe numbers in bold have intersecting conﬁdence intervals with the most accurate method.\nILSVRC Omniglot Aircraft Birds Textures QuickDraw Fungi VGGFlowerTrafﬁcSignsMSCOCOavg. rank\nMAML[8] 37.8 ±1.0 83.9 ±1.0 76.4 ±0.7 62.4 ±1.1 64.1 ±0.8 59.7 ±1.1 33.5 ±1.1 79.9 ±0.8 42.9±1.3 29.4 ±1.1 8.0\nProtoNet[21] 44.5 ±1.1 79.6 ±1.1 71.1 ±0.9 67.0 ±1.0 65.2 ±0.8 64.9 ±0.9 40.3 ±1.1 86.9 ±0.7 46.5±1.0 39.9 ±1.1 7.3\nProtoMAML[22] 46.5 ±1.1 82.7 ±1.0 75.2 ±0.8 69.9 ±1.0 68.3 ±0.8 66.8 ±0.9 42.0 ±1.2 88.7 ±0.7 52.4±1.1 41.7 ±1.1 5.4\nCNAPs[18] 52.3 ±1.0 88.4 ±0.7 80.5 ±0.6 72.2 ±0.9 58.3 ±0.7 72.5 ±0.8 47.4 ±1.0 86.0 ±0.5 60.2±0.9 42.6 ±1.1 5.1\nBOHB-E[19] 55.4 ±1.1 77.5 ±1.1 60.9 ±0.9 73.6 ±0.8 72.8±0.7 61.2±0.9 44.5 ±1.1 90.6±0.6 57.5±1.0 51.9±1.0 4.4\nTaskNorm[3] 50.6 ±1.1 90.7 ±0.6 83.8 ±0.6 74.6 ±0.8 62.1 ±0.7 74.8 ±0.7 48.7 ±1.0 89.6 ±0.6 67.0±0.7 43.4 ±1.0 3.8\nSUR[7] 56.3 ±1.1 93.1 ±0.5 85.4±0.7 71.4±1.0 71.5±0.8 81.3±0.6 63.1±1.0 82.8±0.7 70.4±0.8 52.4±1.1 2.5\nSimpleCNAPS[1] 58.6±1.1 91.7±0.6 82.4 ±0.7 74.9 ±0.8 67.8 ±0.8 77.7 ±0.7 46.9 ±1.0 90.7±0.5 73.5±0.7 46.2±1.1 2.4\nURT 55.7±1.0 94.4±0.4 85.8 ±0.6 76.3 ±0.8 71.8 ±0.7 82.5 ±0.6 63.5 ±1.0 88.2±0.6 69.4±0.8 52.2±1.1 1.6\n3.3 Training Strategy\nWe train representations produced by the URT layer by following the approach of Prototypical\nNetworks [21], where the probability of a label yfor a query example x given the support set of a\ntask is modeled as:\np(y= c|x,S; Θ) = exp(−d(φ(x) −pc))∑N\nc′=1 exp(−d(φ(x) −pc′ ))\n, (9)\nwhere dis a distance metric and pc = 1/|Sc|∑\nx∈Sc φ(x) corresponds to the centroid of class c,\nreferred to as its prototype. We use (negative) cosine similarity as the distance. The full training\nalgorithm is presented in Algorithm 1.\n4 Experiments\nIn this section, we seek to answer three key experimental questions:\nQ1 How does URT compare with previous state-of-the-art on Meta-Dataset for multi-domain few-\nshot classiﬁcation?\nQ2 Do the URT attention heads generate interpretable and meaningful attention scores?\nQ3 Does the URT layer provide consistent beneﬁts, even when pre-trained backbones are trained in\ndifferent ways?\nIn addition, we investigate architectural choices made, such as our models for keys/queries and their\nregularization, and study their contribution to achieving strong performance with URT.\n4.1 Datasets and Setup\nWe test our methods on the large-scale few-shot learning benchmark Meta-Dataset [22]. It consists of\nten datasets with various data distributions across different domains, including natural images (Birds,\nFungi, VGG Flower), hand-written characters (Omniglot, Quick Draw), and human created objects\n(Trafﬁc Signs, Aircraft). Among the ten datasets, eight provide data that can be used during either\ntraining, validation and testing (with each class assigned to only one of those sets), while two datasets\nare solely used for testing. Following Bateni et al. [1], Requeima et al. [18], we also report results on\nMNIST [11], CIFAR10 and CIFAR100 [9] as additional unseen test datasets. Following Triantaﬁllou\net al. [22], few-shot tasks are sampled with varying number of classes N, varying number of shots K\nand class imbalance. The performance is reported as the average accuracy over 600 sampled tasks.\nMore details of Meta-Dataset can be found in Triantaﬁllou et al. [22].\nThe domain-speciﬁc backbones are pre-trained following the setup in [ 7]. Then, we freeze the\nbackbone and train the URT layer for 10,000 episodes, with an initial learning rate of 0.01 and a cosine\nlearning rate scheduler. Following Chen et al. [5], the training episodes have 50% probability coming\nfrom the ImageNet data source. Since different pre-trained backbones may produce representations\n6\nTest task domain \n Source domain for pretrained backbone\nTest task domain \n Source domain for pretrained backbone\nILSVRCOmniglotAircraftBirds\nTexturesQuickDraw\nFungi\nVGGFlower\nTraﬃcSigns\nMSCOCO\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuickDraw\nFungi\nVGGFlower\nTraﬃcSigns\nMSCOCO\nILSVRC\nOmniglot\nAircraft\nBirds\nTextures\nQuickDraw\nFungi\nVGGFlower\nILSVRCOmniglotAircraftBirds\nTexturesQuickDraw\nFungi\nVGGFlower\nFigure 2: Average attention scores generated by URT with two heads. Rows correspond to the domain\nof the test tasks and the columns correspond to the pre-trained backbones ri(x) trained on the eight\ntraining domains.\nwith different vector norms, we normalize the outputs of the backbones as in Dvornik et al. [7]. URT\nis trained with parameter weight decay of 1e-5 and with a regularization factor λ= 0.1. The number\nof heads (H in Equation 7), is set to 2 and the dimension of the keys and queries (lin Equation 4) is\nset to 1024. We choose the hyper-parameters based on the performance of the validation set. Details\nof the hyper-parameter selection and how the performance is inﬂuenced by them are outlined in\nSection 4.5.\n4.2 Comparison with Previous Approaches\nTable 1 presents a comparison of URT with SUR, as well as other baselines based on transfer\nlearning by ﬁne-tuning [19] or meta-learning (Prototypical Networks [21], ﬁrst-order MAML [8],\nProtoMAML [22], CNAPs [18]) and Simple CNAPS[1].\nTable 2: Test performance (mean+CI%95) over 600 few-shot tasks on\nadditional datasets.\nMNIST CIFAR10 CIFAR100 avg. rank\nCNAPs[18] 92.7 ±0.4 61.5 ±0.7 50.1 ±1.0 4.7\nTaskNorm[3] 92.3 ±0.4 69.3 ±0.8 54.6 ±1.1 3.3\nSUR[7] 94.3 ±0.4 66.8 ±0.9 56.6 ±1.0 2.3\nSimpleCNAPS[1] 93.9 ±0.4 74.3 ±0.7 60.5 ±1.0 1.7\nURT 94.8 ±0.4 67.3 ±0.8 56.9 ±1.0 2.0\nWe observe in Table 1 that\nURT establishes a new state-\nof-the-art on Meta-Dataset,\nby achieving the top per-\nformance on 7 out of the\n10 dataset sources. When\ncomparing to its predeces-\nsor, URT outperforms SUR\non 4 datasets without com-\npromising performance on\nothers, which is challeng-\ning to achieve in the multi-\ndomain setting. Of note, the\naverage inference time for URT is 0.04 second per task, compared to 0.43 for SUR, on a single V100.\nThus, getting rid of the optimization procedure for every episode with our meta-trained URT layer\nalso signiﬁcantly increases the latency, by more than 10×.\nWe also report performances on the MNIST, CIFAR-10 and CIFAR-100 dataset sources in Table 2,\nand compare with the subset of methods that have reported on these datasets. There, URT neither\nimproves nor gets worse performance than SUR, yeilding top performance on the MNIST domain\nbut not on the CIFAR-10/CIFAR-100 domain, on which Simple CNAPS has the best performance.\n4.3 Interpreting and Visualizing Attention by URT\nTo better understand how the URT model of Section 4.2 uses its two heads to build adapted represen-\ntations, we visualize the attention scores produced on the test tasks of Meta-Dataset in Figure 2.\n7\nThe blue (ﬁrst head) and orange (second head) heatmaps summarize the values of the attention scores\n(Equation 5), averaged across several tasks for each test domain. Speciﬁcally, the element on row\ntand column iis the averaged attention scores αi computed on test set domain tfor the backbone\nfrom domain i. Note that the last two rows are the two unseen domain datasets. We found that\nfor datasets from the seen domains, i.e. the ﬁrst eight rows, one head (right, orange) consistently\nputs most of its weight on the backbone pre-trained on the same domain, while the other head (left,\nblue) learns relatively smoother weight distributions that blends other related domains. For unseen\ndatasets, the right head puts half of its weight on ImageNet and the left head learned to blend the\nrepresentations from four backbones.\n4.4 URT using FiLM Modulated Backbones\nTable 3: Performance comparison using para-\nmetric network family (pf) backbones.\nSUR-pf [7] URT-pf VS.\nILSVRC 56.4 ±1.2 55.5 ±1.1 =\nOmniglot 88.5 ±0.8 90.2 ±0.6 +\nAircraft 79.5 ±0.8 79.8 ±0.7 =\nBirds 76.4 ±0.9 77.5 ±0.8 =\nTextures 73.1 ±0.7 73.5 ±0.7 =\nQuick Draw 75.7 ±0.7 75.8 ±0.7 =\nFungi 48.2 ±0.9 48.1 ±0.9 =\nVGG Flower 90.6 ±0.5 91.9 ±0.5 +\nTrafﬁc Signs 65.1 ±0.8 67.5 ±0.8 +\nMSCOCO 52.1 ±1.0 52.1 ±1.0 =\nMNIST 93.2 ±0.4 93.9 ±0.4 =\nCIFAR10 66.4 ±0.8 66.1 ±0.8 =\nCIFAR100 57.1 ±1.0 57.3 ±1.0 =\nAs additional evidence of the beneﬁt of URT\non universal representations, we also present\nexperiments based on a different set of backbone\narchitectures. Following SUR [ 7], we consider\nthe backbones from a parametric network family,\nobtained by training a base backbone on one\ndataset (ILSVRC) and then learning separate FiLM\nlayers [14] for each other dataset, to modulate the\nbackbone so it is adapted to the other domains.\nThese backbones collectively have only 0.5% more\nparameters than a single backbone.\nA comparison between SUR and URT using these\nbackbones (referred to as SUR-pf and URT-pf) is\npresented in Table 3. Once again, URT improves\nthe performance on three datasets without sacriﬁc-\ning performance on others. Additionally, URT-pf\nnow achieves better performance than BOHB-E on\nVGGFlower.\n4.5 Hyper-Parameter and Ablation Studies\nWe analyze the importance of the various components of URT’s attention mechanism structure and\ntraining strategy in Table 4. First we analyze the importance of using the support set to model\nqueries and/or keys. To this end, we consider setting the matrices Wq / Wk of the query / key linear\ntransformation to 0, which only leaves the bias term. We found that the support set representation\nis most crucial for building the keys (row w/o Wk in the table) and has minor beneﬁts for queries\n(row w/o Wq) in the table. This observation is possibly related to the success of attention-based\nmodels with learnable constant queries [13, 12]. We also found that adding a regularizer Ω(Θ) as in\nEquation 8 is important for some datasets, speciﬁcally VGG Flower and Birds.\nTable 4: Meta-Dataset performance variation on ablations of elements of the URT layer.\nILSVRC Omniglot Aircraft Birds Textures Draw Fungi Flower Signs MSCOCO\nw/o Wq +0.2 -0.2 -0.6 -0.1 -0.3 -0.2 0.0 -0.2 -0.8 -0.1\nw/o Wk -14.2 -2.8 -10.7 -18.1 -7.6 -9.3 -22.4 -3.6 -0.26 -10.9\nw/o r(Sc) -14.2 -2.8 -10.7 -18.1 -7.6 -9.2 -22.4 -3.6 -0.26 -10.9\nw/o Ω(Θ) 0.0 -0.9 -0.4 -3.3 -1.2 -0.2 +0.3 -9.0 -2.0 0.0\nAn important hyper-parameter in URT is the number of heads H. We chose this hyper-parameter\nbased on the performance on validation set of tasks in Meta-Dataset. In Table 5, we show the\nvalidation performance of URT for varying number of heads. As suggested by Triantaﬁllou et al.\n[22], we considered looking at the rank of the performance achieved by each choice of H for each\nvalidation domains, and taking the average across domains as a validation metric. However, since the\nperformances when using two to four heads are similar and yield the same average rank, we instead\nsimply consider the average accuracy as the selection criteria.\n8\nTable 5: Validation performance on Meta-Dataset using different number of heads\n1 2 3 4 5 6 7 8\nAverage Accuracy 74.605 77.145 76.943 76.984 76.602 75.906 75.454 74.473\nAverage Rank 2.875 1.000 1.000 1.000 2.250 2.250 2.25 2.50\nIn general, we observe a large jump in performance when using multiple heads instead of just one.\nHowever, since the number of heads controls the capacity, predictably we also observe that having\ntoo many heads leads to overﬁtting.\n5 Conclusion\nWe proposed the URT layer to effectively integrate representations from multiple domains and\ndemonstrated improved performance in multi-domain few-shot classiﬁcation. Notably, our URT\napproach was able to set a new state-of-the-art on Meta-Dataset, and never performs worse than\nits predecessor (SUR) while also being 10×more efﬁcient at inference. This work suggests that\ncombining meta-learning with pre-trained universal representations is a promising direction for new\nfew-shot learning methods. Speciﬁcally, we hope that future work can investigate the design of richer\nforms of universal representations that go beyond simply pre-training a single backbone for each\ndomain, and developing meta-learners adapted to those settings.\nBroader Impact\nOur URT model may present an interesting element of solution for applications that present difﬁculties\nin the collection and sharing of data. This could include settings where each user of an application\nhas limited private data, and as such desires that a classiﬁcation task be executed directly and solely\non their devices. Any deployment of the proposed model however should be preceded by an analysis\nof the potential biases captured by the dataset sources used for training and the correction of any such\nundesirable biases captured by the pre-trained backbones and model.\nAcknowledgement\nWe would like to thank Tianyi Zhou for paper review and suggestions. The computation support for\nthis project is provided by Compute Canada and Google Cloud. This project was supported by the\nCanada CIFAR AI Chairs program.\nReferences\n[1] Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, and Leonid Sigal. Improved\nfew-shot visual classiﬁcation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[2] Hakan Bilen and Andrea Vedaldi. Universal representations: The missing link between faces,\ntext, planktons, and cat breeds. arXiv preprint arXiv:1701.07275, 2017.\n[3] John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard E Turner.\nTasknorm: Rethinking batch normalization for meta-learning. In The International Conference\non Machine Learning (ICML), 2020.\n[4] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Liu, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\ncloser look at few-shot classiﬁcation. In International Conference on Learning Representations\n(ICLR), 2019.\n[5] Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline\nfor few-shot learning. arXiv preprint arXiv:2003.04390, 2020.\n9\n[6] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline\nfor few-shot image classiﬁcation. In International Conference on Learning Representations\n(ICLR), 2020.\n[7] Nikita Dvornik, Cordelia Schmid, and Julien Mairal. Selecting relevant features from a universal\nrepresentation for few-shot classiﬁcation. arXiv preprint arXiv:2003.09338, 2020.\n[8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-\ntation of deep networks. In The International Conference on Machine Learning (ICML) ,\n2017.\n[9] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nTechnical report, University of Toronto, 2009.\n[10] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\nlearning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.\n[11] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[12] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[13] Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference\nusing bidirectional lstm model and inner-attention. arXiv preprint arXiv:1605.09090, 2016.\n[14] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film:\nVisual reasoning with a general conditioning layer. InAAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2018.\n[15] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Interna-\ntional Conference on Learning Representations (ICLR), 2016.\n[16] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi. Efﬁcient parametrization of multi-\ndomain deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2018.\n[17] Mengye Ren, Eleni Triantaﬁllou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenen-\nbaum, Hugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot\nclassiﬁcation. In International Conference on Learning Representations (ICLR), 2018.\n[18] James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner.\nFast and ﬂexible multi-task classiﬁcation using conditional neural adaptive processes. In The\nConference on Neural Information Processing Systems (NeurIPS), pages 7957–7968, 2019.\n[19] Tonmoy Saikia, Thomas Brox, and Cordelia Schmid. Optimized generic feature learning for\nfew-shot classiﬁcation across domains. arXiv preprint arXiv:2001.07926, 2020.\n[20] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.\nMeta-learning with memory-augmented neural networks. In The International Conference on\nMachine Learning (ICML), 2016.\n[21] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nThe Conference on Neural Information Processing Systems (NeurIPS), 2017.\n[22] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross\nGoroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A\ndataset of datasets for learning to learn from few examples. In International Conference on\nLearning Representations (ICLR), 2020.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In The Conference on Neural\nInformation Processing Systems (NeurIPS), 2017.\n10\n[24] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. In The Conference on Neural Information Processing Systems (NeurIPS), 2016.\n[25] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding\nadaptation with set-to-set functions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2020.\n11",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5635637640953064
    },
    {
      "name": "Computer science",
      "score": 0.4944422245025635
    },
    {
      "name": "Representation (politics)",
      "score": 0.45871925354003906
    },
    {
      "name": "Image (mathematics)",
      "score": 0.42089709639549255
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4176555573940277
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36943259835243225
    },
    {
      "name": "Computer vision",
      "score": 0.3545915186405182
    },
    {
      "name": "Electrical engineering",
      "score": 0.13118299841880798
    },
    {
      "name": "Engineering",
      "score": 0.12804830074310303
    },
    {
      "name": "Political science",
      "score": 0.1248869001865387
    },
    {
      "name": "Voltage",
      "score": 0.05181112885475159
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}