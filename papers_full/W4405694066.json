{
  "title": "MGFusion: a multimodal large language model-guided information perception for infrared and visible image fusion",
  "url": "https://openalex.org/W4405694066",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2130757580",
      "name": "Zengyi Yang",
      "affiliations": [
        "Kunming University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2119962829",
      "name": "Yunping Li",
      "affiliations": [
        "China Tobacco"
      ]
    },
    {
      "id": "https://openalex.org/A2122679627",
      "name": "Tang Xin",
      "affiliations": [
        "China Tobacco"
      ]
    },
    {
      "id": "https://openalex.org/A2103204114",
      "name": "Minghong Xie",
      "affiliations": [
        "Kunming University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2130757580",
      "name": "Zengyi Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119962829",
      "name": "Yunping Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122679627",
      "name": "Tang Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103204114",
      "name": "Minghong Xie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1708141795",
    "https://openalex.org/W2091649119",
    "https://openalex.org/W4404035691",
    "https://openalex.org/W4394867784",
    "https://openalex.org/W4386754915",
    "https://openalex.org/W6847810607",
    "https://openalex.org/W6800300148",
    "https://openalex.org/W3083923056",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3152132512",
    "https://openalex.org/W4389195380",
    "https://openalex.org/W4389891052",
    "https://openalex.org/W2798987894",
    "https://openalex.org/W3007891240",
    "https://openalex.org/W3133700567",
    "https://openalex.org/W4366352735",
    "https://openalex.org/W4392667295",
    "https://openalex.org/W4392607771",
    "https://openalex.org/W4319969359",
    "https://openalex.org/W3030921250",
    "https://openalex.org/W4391590727",
    "https://openalex.org/W6810520470",
    "https://openalex.org/W3126855404",
    "https://openalex.org/W4389491034",
    "https://openalex.org/W6855722634",
    "https://openalex.org/W4210571497",
    "https://openalex.org/W3197456663",
    "https://openalex.org/W4393240973",
    "https://openalex.org/W2963604034",
    "https://openalex.org/W4391952683",
    "https://openalex.org/W4289654558",
    "https://openalex.org/W4285291903",
    "https://openalex.org/W4387968982",
    "https://openalex.org/W2809795042",
    "https://openalex.org/W4283732315",
    "https://openalex.org/W3011768656",
    "https://openalex.org/W2912147220",
    "https://openalex.org/W3108042295",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W4316041856",
    "https://openalex.org/W4308310215",
    "https://openalex.org/W4220893768",
    "https://openalex.org/W4286361941",
    "https://openalex.org/W4313555022",
    "https://openalex.org/W2757470902",
    "https://openalex.org/W4399526541",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W4210944420",
    "https://openalex.org/W3190785114",
    "https://openalex.org/W3046194589",
    "https://openalex.org/W4379741379",
    "https://openalex.org/W2091484864",
    "https://openalex.org/W4402715923",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W3206506760",
    "https://openalex.org/W4402753319",
    "https://openalex.org/W3036032447",
    "https://openalex.org/W2977461977",
    "https://openalex.org/W4392909612",
    "https://openalex.org/W6847461194",
    "https://openalex.org/W3216678721",
    "https://openalex.org/W4307726656",
    "https://openalex.org/W4396677248",
    "https://openalex.org/W4387682254",
    "https://openalex.org/W3105639468",
    "https://openalex.org/W2593390416",
    "https://openalex.org/W3023742835",
    "https://openalex.org/W4390873110"
  ],
  "abstract": "Existing image fusion methods primarily focus on complex network structure designs while neglecting the limitations of simple fusion strategies in complex scenarios. To address this issue, this study proposes a new method for infrared and visible image fusion based on a multimodal large language model. The method proposed in this paper fully considers the high demand for semantic information in enhancing image quality as well as the fusion strategies in complex scenes. We supplement the features in the fusion network with information from the multimodal large language model and construct a new fusion strategy. To achieve this goal, we design CLIP-driven Information Injection (CII) approach and CLIP-guided Feature Fusion (CFF) strategy. CII utilizes CLIP to extract robust image features rich in semantic information, which serve to supplement the information of infrared and visible features, thereby enhancing their representation capabilities for the scene. CFF further utilizes the robust image features extracted by CLIP to select and fuse the infrared and visible features after the injection of semantic information, addressing the challenges of image fusion in complex scenes. Compared to existing methods, the main advantage of the proposed method lies in leveraging the powerful semantic understanding capabilities of the multimodal large language model to supplement information for infrared and visible features, thus avoiding the need for complex network structure designs. Experimental results on multiple public datasets validate the effectiveness and superiority of the proposed method.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/three.tnum December /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nOPEN ACCESS\nEDITED BY\nQian Jiang,\nYunnan University, China\nREVIEWED BY\nZhiqin Zhu,\nChongqing University of Posts and\nTelecommunications, China\nShuang Li,\nChongqing University of Posts and\nTelecommunications, China\n*CORRESPONDENCE\nMingHong Xie\nminghongxie@/one.tnum/six.tnum/three.tnum.com\nRECEIVED /zero.tnum/two.tnum November /two.tnum/zero.tnum/two.tnum/four.tnum\nACCEPTED /two.tnum/nine.tnum November /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /two.tnum/three.tnum December /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nYang Z, Li Y, Tang X and Xie M (/two.tnum/zero.tnum/two.tnum/four.tnum)\nMGFusion: a multimodal large language\nmodel-guided information perception for\ninfrared and visible image fusion.\nFront. Neurorobot./one.tnum/eight.tnum:/one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Yang, Li, Tang and Xie. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nMGFusion: a multimodal large\nlanguage model-guided\ninformation perception for\ninfrared and visible image fusion\nZengyi Yang/one.tnum, Yunping Li /two.tnum, Xin Tang /two.tnumand MingHong Xie /one.tnum*\n/one.tnumFaculty of Information Engineering and Automation, Kunming Unive rsity of Science and Technology,\nKunming, Yunnan, China, /two.tnumKunming Cigarette Factory, Hongyunhonghe Tobacco Group Company\nLimited, Kunming, Yunnan, China\nExisting image fusion methods primarily focus on complex netwo rk structure\ndesigns while neglecting the limitations of simple fusion str ategies in complex\nscenarios. To address this issue, this study proposes a new me thod for infrared\nand visible image fusion based on a multimodal large languag e model. The\nmethod proposed in this paper fully considers the high demand f or semantic\ninformation in enhancing image quality as well as the fusion str ategies in\ncomplex scenes. We supplement the features in the fusion networ k with\ninformation from the multimodal large language model and cons truct a new\nfusion strategy. To achieve this goal, we design CLIP-driven Inf ormation Injection\n(CII) approach and CLIP-guided Feature Fusion (CFF) strategy. C II utilizes\nCLIP to extract robust image features rich in semantic informat ion, which\nserve to supplement the information of infrared and visible f eatures, thereby\nenhancing their representation capabilities for the scene. CF F further utilizes\nthe robust image features extracted by CLIP to select and fuse the infrared\nand visible features after the injection of semantic informa tion, addressing the\nchallenges of image fusion in complex scenes. Compared to existi ng methods,\nthe main advantage of the proposed method lies in leveraging t he powerful\nsemantic understanding capabilities of the multimodal larg e language model to\nsupplement information for infrared and visible features, thus avoiding the need\nfor complex network structure designs. Experimental results o n multiple public\ndatasets validate the eﬀectiveness and superiority of the pr oposed method.\nKEYWORDS\ninfrared and visible image fusion, CLIP, multimodal large language model, semantic\ninformation injection, image fusion\n/one.tnum Introduction\nIn recent years, image fusion technology has garnered signiﬁcant attention in the\nﬁeld of computer vision. Image fusion encompasses various types, including infrared\nand visible image fusion (\nLi and Wu, 2019 ), multi-exposure image fusion ( Liu et al.,\n2022c; Li et al., 2024b ; Tang et al., 2023a ), multi-focus image fusion ( Li et al., 2024a ,c),\nmedical image fusion ( Liu et al., 2022e ,d; Zhu et al., 2023 , 2024), and remote sensing\nimage fusion ( Zhang Y. et al., 2024 ). Among these applications, infrared and visible image\nfusion technology stands out due to its wide range of applications. The infrared and visible\nimage fusion technology aims to integrate a large amount of complementary information\nfrom both infrared and visible images to generate a single fused image, providing a\nmore comprehensive description of the scene. Due to the diﬀerences in sensor imaging\nFrontiers in Neurorobotics /zero.tnum/one.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nmechanisms, visible sensors can capture rich texture and color\ninformation (\nZhang Y. et al., 2020 ; Xie et al., 2021 ). However, their\nperformance is severely aﬀected by lighting, weather, and smoke\nconditions. In contrast, infrared sensors can eﬀectively capture\nthermal radiation information even under low-light and adverse\nweather conditions, highlighting targets such as people and vehicles\nin the images. By fusing infrared and visible images, it is possible to\nobtain information-rich scene images under all weather conditions.\nConsequently, this technology has found widespread applications\nin industrial control, autonomous driving, and aerospace ﬁelds.\nIn recent years, signiﬁcant progress has been made in the\nresearch on infrared and visible image fusion to address various\npractical application challenges. These challenges primarily include\ninconsistencies in source image resolution (\nLi et al., 2021a ; Ma et al.,\n2020; Xiao et al., 2022 ), unregistered source images ( Xu et al., 2023 ;\nLi et al., 2023a ,c; Wang et al., 2024 ), low-light environments ( Chen\net al., 2024 ; Tang et al., 2022 , 2023b), extreme weather conditions\n(Yi et al., 2024 ; Li X. et al., 2024 ), and challenges in adapting to\ndownstream task requirements ( Zhang H. et al., 2024 ; Liu et al.,\n2023b; Liu Z. et al., 2023 ). In the eﬀort to improve the quality\nof fused images, existing research primarily employs mainstream\nmethods, including CNN feature interaction-based fusion methods\n(\nLi and Wu, 2019 ; Li et al., 2021b ; Jian et al., 2021 ; Liu et al.,\n2022b, 2021; Li et al., 2023b ; Yue et al., 2023 ), multiple feature\nextraction mechanisms-based fusion methods ( Zhao et al., 2023 ;\nLi J. et al., 2021 ; Dong et al., 2024 ), and loss function-driven\nfusion methods ( Liu et al., 2023a , 2022a; Zhou et al., 2023 ). These\napproaches aim to enhance the scene representation capability of\nmultimodal features, thereby contributing to the overall quality of\nthe fused images. Initially, researchers commonly designed feature\nextraction network structures based on convolutional neural\nnetworks (CNN), injecting more information into multimodal\nfeatures through frequent information interactions to enhance\nthe quality of fused images (\nLi and Wu, 2019 ; Li et al., 2023b ;\nYue et al., 2023 ). In these methods, many studies introduced\nskip connections ( Jian et al., 2021 ), dense connections ( Li and\nWu, 2019 ), and nest connections ( Li et al., 2021b ) during feature\nextraction to enhance information exchange between features at\ndiﬀerent depths, thereby alleviating information loss caused by\ndeeper networks. Additionally, some studies (\nLi et al., 2021a ;\nHuang et al., 2022 ) employed convolutional kernels with varying\ndilation rates and sizes for feature extraction, allowing information\nfrom a larger receptive ﬁeld to be aggregated into multimodal\nfeatures. However, CNN have limitations in extracting rich global\ninformation, leading to constrained representation capability of\nthe extracted features. Consequently, many studies have integrated\nadvanced feature extraction methods with CNN to address these\ndeﬁciencies. These approaches incorporate Transformers (\nMa et al.,\n2022; Tang et al., 2023c ), Generative Adversarial Networks (GAN;\nMa et al., 2021 ; Zhang et al., 2021 ), and Mamba ( Dong et al., 2024 )\ninto the feature extraction process to assist CNN in extracting more\nglobal information, thereby enhancing the quality of fused images.\nHowever, the aforementioned methods often require\nresearchers to have extensive design experience and signiﬁcant\nmanual resources. To address this issue, some studies have\nintroduced carefully designed loss functions without the need\nfor complex network structures. These loss functions impose\nconstraints on feature extraction networks, encouraging the\nextracted features to contain more information. In representative\nworks, loss functions based on contrastive learning (\nLiu et al.,\n2023a), loss functions focusing on salient targets ( Liu et al., 2022a ),\nand loss functions guided by semantic information ( Zhou et al.,\n2023) have been introduced to enhance the quality of fused\nimages. However, these methods need to consider the balance\namong numerous hyperparameters to better utilize the carefully\ndesigned loss functions. For example, the process of balancing\nhyperparameters within the new loss functions and between\nthe new and existing loss functions can be lengthy and tedious.\nThis parameter tuning often requires a signiﬁcant amount of\ncomputational resources.To mitigate this, many researchers have\nattempted to introduce advanced ideas from other ﬁelds into\ninfrared and visible image fusion, signiﬁcantly reducing the\nworkload of network structure design and parameter tuning.\nThese approaches incorporate advanced concepts such as diﬀusion\nmodels (\nYue et al., 2023 ) and low-rank sparse decomposition ( Li\net al., 2023b , 2020) to better decompose features from diﬀerent\nmodalities and accurately capture these features. However,\ndiﬀusion models typically involve a large number of parameters\nand computational requirements, making them challenging to\ndeploy on resource-constrained platforms. Additionally, low-rank\nsparse decomposition methods may lead to information loss\nduring the extraction of low-rank and sparse features, thereby\naﬀecting fusion quality.\nTo address the shortcomings of existing methods, this paper\nreconsiders the strategies for enhancing image quality in infrared\nand visible image fusion. A careful analysis of the limitations\nof current approaches reveals that incorporating robust semantic\ninformation from outside the fusion network to supplement\nmultimodal features can eﬀectively alleviate unavoidable issues. In\nrecent years, multimodal large language models have demonstrated\nstrong semantic understanding and zero-shot learning capabilities\nthrough pre-training on large-scale multimodal datasets. Among\nthem, CLIP stands out as a powerful model trained on extensive\nimage-text data, possessing strong multimodal representation\ncapabilities and excellent generalization performance. It can extract\nhigh-dimensional semantic representations from images, which\nare not only rich in semantic information but also exhibit\nstrong robustness. These attributes make features extracted by\nmodels like CLIP particularly suitable for providing supplementary\ninformation to the features in the fusion network, thereby\nenhancing the quality of fused images. Therefore, this paper\ninnovatively proposes a multimodal large language model-based\nframework for infrared and visible image fusion, which can\nachieve high-quality fused images without the need for complex\nnetwork structures.\nTo enrich the semantic information of features in the\nfusion network, this paper proposes an information injection\nmethod based on CLIP (\nRadford et al., 2021 ). This method\nuses the multimodal features extracted by CLIP to supplement\nthe features in the fusion network, signiﬁcantly enriching the\nsemantic information of the features to be fused and enhancing\ntheir robustness. Additionally, to address the challenges posed by\nsimple fusion strategies, such as element-wise addition or channel\nconcatenation, in complex fusion scenarios, this paper introduces a\nFrontiers in Neurorobotics /zero.tnum/two.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nCLIP-guided feature fusion strategy. This strategy leverages CLIP’s\nstrong semantic understanding capabilities to select and fuse the\nfeatures, meeting the need to improve the quality of fusion results in\ncomplex situations. The proposed method deeply integrates CLIP\nwith the fusion network, providing information supplementation,\nfeature selection, and feature fusion for the multimodal features\nin the original fusion network, thereby signiﬁcantly improving the\nquality of the fused images. The main contributions of this paper\nand the advantages of the proposed method are highlighted in the\nfollowing aspects:\n(1) We propose a framework for infrared and visible\nimage fusion based on multimodal large language models. This\nframework signiﬁcantly enhances the quality of fused images\nwhile overcoming the shortcomings of existing methods, providing\nnew insights for improving the quality of infrared and visible\nimage fusion.\n(2) We introduce multimodal large language model to\nsupplement the features in the fusion network, enriching the\nsemantic information of the features to be fused and enhancing\ntheir robustness. Additionally, we embed the multimodal large\nlanguage model into the feature fusion process and propose a fusion\nstrategy. This strategy uses the multimodal large language model\nfor feature selection and fusion, eﬀectively addressing complex\nfusion scenarios.\n(3) We deploy this method on several publicly available\ninfrared and visible image fusion datasets and conduct quantitative\nand qualitative comparisons to validate its fusion performance.\nThe experimental results demonstrate that the proposed method\nsigniﬁcantly outperforms existing methods in both visual quality\nand objective evaluation metrics.\nThe remaining content of this paper is organized as follows:\nSection 2 reviews related work; Section 3 elaborates on the proposed\nmethod in detail; Section 4 presents the experimental results\nand their analysis; Section 5 summarizes the paper and draws\nsome conclusions.\n/two.tnum Related work\nIn the research of infrared and visible image fusion focused\non enhancing image quality, existing methods can be broadly\nclassiﬁed into the following categories based on their speciﬁc\nimplementation approaches: CNN feature interaction-based fusion\nmethods, multiple feature extraction mechanisms-based fusion\nmethods, and loss function-driven fusion methods.\n/two.tnum./one.tnum CNN feature interaction-based fusion\nmethods\nFusion methods based on CNN feature interaction typically\nutilize convolutional neural networks (CNN) to construct feature\nextraction networks. They enrich feature representation through\nfrequent information exchange between convolutional layers,\nthereby enhancing the quality of the fused images. In this category\nof methods, DenseFuse (\nLi and Wu, 2019 ) introduces dense\nconnections in the feature encoder, promoting the fusion of multi-\nlayer features through dense interactions between convolutional\nlayers at diﬀerent depths, ensuring that the output features contain\nas much rich information as possible from various layers. RFN-\nNest (\nLi et al., 2021b ) further fuses features of diﬀerent depths\nwithin the encoder and inputs the multiple fused features into\nthe decoder for deeper interaction and fusion. However, these\nmethods do not adequately address the potential information loss\nthat may occur between the encoder and decoder. To tackle this\nissue, SEDRFuse (\nJian et al., 2021 ) introduces skip connections\nbetween the feature encoder and decoder, leveraging long-range\ninformation supplementation to reduce information loss during the\nforward propagation process.\nAlthough the aforementioned methods enrich feature\nrepresentation through frequent information exchange, they\ndo not address the limitation of receptive ﬁelds in CNNs. To\nthis end, MLFusion (\nLi et al., 2021a ) is inspired by the human\npopulation Receptive Field (pRFs; Liu et al., 2018 ) and employs\nconvolutional kernels of varying dilation rates and sizes for feature\nextraction, aggregating features from diﬀerent receptive ﬁelds to\nobtain information from a larger receptive ﬁeld. However, these\nmethods overlook the shortcomings of CNNs in extracting global\ninformation, which limits the representational capacity of the\nextracted features.\n/two.tnum./two.tnum Multiple feature extraction\nmechanisms-based fusion methods\nMultiple feature extraction mechanisms-based fusion methods\nextract more comprehensive features by combining advanced\nfeature extraction mechanisms with CNNs, thereby enhancing the\nquality of fused images. For example, SwinFusion (\nMa et al., 2022 )\nutilizes CNNs to extract basic features and further processes these\nfeatures through Transformers to inject more global information.\nHowever, the extraction of global information in this method relies\non the basic features extracted by CNNs, inevitably leading to the\nloss of some global information. To address this issue, CDDFuse\n(\nZhao et al., 2023 ) employs Transformers and CNNs in parallel for\nfeature extraction, merging the two to create features that contain\nboth rich local and global information. In recent years, the Mamba\nmodel has gained widespread attention in the ﬁeld of deep learning\ndue to its advantages in eﬃciency, speed, scalability, and complexity\nmanagement compared to Transformers. Consequently, Fusion-\nMamba (\nDong et al., 2024 ) introduces the Mamba model into the\nfusion framework, combining it with CNNs for feature extraction\nand fusion to further enhance the quality of fused images.\nMoreover, many studies have incorporated adversarial learning\nmechanisms between the fusion results and source images\ninto fusion methods, encouraging extracted features to contain\nricher information. For example, FusionGAN (\nMa et al., 2019b )\nsupervises the fusion results using a visible image’s discriminator,\nprompting the fusion network to inject more edge detail\ninformation into the fused image. However, single-discriminator\nmethods can lead to an imbalance in modal information,\nweakening the scene representation capability of the fusion results.\nTo address this issue, DDcGAN (\nMa et al., 2020 ) introduces a\ndual-modal discriminator into the fusion process, encouraging a\nmore balanced injection of information from both infrared and\nFrontiers in Neurorobotics /zero.tnum/three.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nvisible images into the fusion results. Nevertheless, these methods\noften require researchers to possess extensive design experience and\ninvest signiﬁcant human resources.\nAdditionally, LRRNet (\nLi et al., 2023b ) employs the concept of\nlow-rank sparse decomposition, separating image features into low-\nrank and sparse components, and fuses these two parts separately\nto improve the quality of the reconstructed image. However,\nfusion methods based on low-rank sparse decomposition may lead\nto information loss during feature decomposition. resulting in\nsuboptimal fusion outcomes. In recent years, diﬀusion models have\nachieved signiﬁcant success in the ﬁeld of image generation. Dif-\nFusion (\nYue et al., 2023 ) utilizes diﬀusion models for the fusion of\ninfrared and visible images to achieve high-quality fusion results.\nHowever, the large number of parameters and computational\ndemands of diﬀusion models limit their application on platforms\nwith constrained storage and computational resources. In contrast\nto these methods, this paper introduces a multimodal large\nlanguage model to inject robust semantic information into the\nfeatures of the fusion network, enriching feature representation. To\naddress the challenges of image fusion in complex scenarios, we\nhave developed a novel fusion module based on the multimodal\nlarge language model, aiming to achieve high-ﬁdelity fused images.\n/two.tnum./three.tnum Loss function-driven fusion methods\nLoss function-driven fusion methods constrain feature\nextraction networks through carefully designed loss functions,\nencouraging the extraction of more easily overlooked information\nto enhance the quality of fused images. For example, SDDGAN\n(\nZhou et al., 2023 ) constructs a semantic-related loss function\nusing semantic segmentation results, promoting the injection\nof more semantic information into the fused image. However,\nthis method has limitations in enhancing information in the\nregions of salient objects. To address this issue, TarDAL (\nLiu\net al., 2022a ) introduces a loss function based on Saliency Degree\nWeight (SDW), focusing on enhancing the information of salient\nobjects in the fused image. However, this method overly focuses\non enhancing the information of the target objects, while the\nprocessing of background information is relatively weak. To\ncounter this, CoCoNet (\nLiu et al., 2023a ) incorporates contrastive\nlearning into the fusion process, balancing the enhancement\nof both target and background information. In the regions of\nsalient objects, the distance between the fused image and the\ninfrared image is reduced, while the distance to the visible image\nis increased; conversely, in the background regions, the fused\nimage is brought closer to the visible image, while the distance to\nthe infrared image is increased. Nevertheless, such methods often\nrequire tedious and time-consuming parameter tuning to balance\nvarious hyperparameters, thereby fully leveraging the eﬀectiveness\nof the loss function. In contrast to these methods, this paper\nintroduces a multimodal large language model to inject robust\nsemantic information into the features of the fusion network,\nenriching feature representation. To address the challenges of\nimage fusion in complex scenarios, we have developed a novel\nfusion strategy based on the multimodal large language model,\naiming to achieve high-ﬁdelity fused images.\n/three.tnum Proposed method\n/three.tnum./one.tnum Overview\nAs shown in Figure 1, the proposed method consists of\nﬁve core components: the Infrared Feature Encoder (IRE), the\nVisible Feature Encoder (VIE), the CLIP-driven Information\nInjection (CII) block, the CLIP-guided Feature Fusion (CFF)\nblock, and the Fusion Feature Decoder (FD). The IRE and VIE\nare designed to extract features from infrared images Ii and\nvisible images Iv, respectively. The CII block leverages CLIP\nto extract image features enriched with semantic information\nand injects this semantic content into the infrared and visible\nfeatures, enhancing their ability to represent the scene. The\nCFF block further employs the robust features extracted by\nCLIP to select and fuse the features with injected semantic\ninformation, producing fused features. Finally, the FD decodes\nthe fused features to reconstruct the fused image If . In the\nfollowing sections, we will provide a detailed explanation of each\ncore component.\n/three.tnum./two.tnum Feature extract and information\ninjection\nThe network architectures of the IRE and VIE are identical,\nconsisting primarily of two feature extraction layers followed by N\nRestormer Blocks (\nZamir et al., 2022 ). Each feature extraction layer\nis composed of a convolutional layer with a kernel size of 3 × 3\nand a stride of 1, stacked with a Batch Normalization layer and\na LeakyReLU activation function layer. The infrared images Ii ∈\nRH×W×1 and visible images Iv ∈ RH×W×3 are input into the IRE\nand VIE, respectively, to extract the infrared features Fi ∈ RH×W×C\nand visible features Fv ∈ RH×W×C, where H and W represent the\nheight and width of the source image, and C represents the number\nof feature channels.\nThe infrared and visible features obtained through simple\nfeature extraction often lack rich semantic information, making it\nchallenging to achieve high-quality fusion results. Therefore, we\ndirectly utilize the pre-trained weights provided by the authors of\nCLIP , without any additional retraining, to leverage its powerful\nfeature extraction capabilities. By injecting the rich semantic\ninformation extracted by CLIP into the infrared and visible\nfeatures, we eﬀectively enhance the quality of the fusion output.\nAs illustrated in\nFigure 2, the CII block primarily consists of a\nfrozen parameter pre-trained CLIP , an Adapter, and a Spatial\nExpansion Weight Prediction (SEP) block. The frozen pre-trained\nCLIP is utilized to extract image features rich in semantic content.\nThe Adapter maps the integrated CLIP features into the same\nspace as the infrared or visible features, unifying the number of\nchannels across features to ensure that the features extracted by\nCLIP can be eﬀectively utilized to enhance the quality of fused\nimages. The SEP generates a weight based on the input features,\nwhich is used to expand the CLIP features to match the spatial\ndimensions of the input features. In terms of network architecture,\nthe Adapter comprises two linear mapping layers, while the SEP\nconsists of two convolutional layers with a kernel size of 3 × 3\nFrontiers in Neurorobotics /zero.tnum/four.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nFIGURE /one.tnum\nOverview of the proposed method. The IRE and VIE are employed to ex tract features from infrared and visible images, respectively. To enhance the\nfeatures’ ability to represent the scene, the extracted infr ared and visible features are fed into the CII block, where CLIP is employed to inject rich\nsemantic information into them. Following the semantic injecti on, the features are passed into the CFF block, which leverages CL IP to perform\nselection and fusion of the multimodal features. Finally, the fus ed features are input into the FD to reconstruct the fused image. Where, Restormer\nBlock refers to a module proposed in Zamir et al. (/two.tnum/zero.tnum/two.tnum/two.tnum).\nFIGURE /two.tnum\nCLIP-driven information injection.\nand a stride of 1, and a single ReLU activation function layer.\nTaking the information injection process of the visible feature Fv\nas an example, we input the infrared image Ii and the visible\nimage Iv into the frozen parameter image encoder of CLIP to\nobtain the CLIP features Fc\ni ∈ R1×1×E and Fc\nv ∈ R1×1×E for\nthe infrared and visible images, respectively, where E represents\nthe embedding dimension of the CLIP features. Considering that\nthe features from diﬀerent modalities contain a signiﬁcant amount\nof complementary semantic information, we introduce a learnable\nweight Wi ∈ R1×1×E to integrate the semantic information from Fc\ni\nand Fc\nv. The resulting output is then fed into the Adapter to ensure\nthat the CLIP features are aligned in the same space as the visible\nFrontiers in Neurorobotics /zero.tnum/five.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nfeature Fv:\nFc\nf = A(Fc\ni ⊙ Wi + Fc\nv ⊙ Wv), (1)\nwhere, Fc\nf ∈ R1×1×C represents the integrated CLIP features, ⊙\ndenotes the Hadamard product, Wv = 1 − Wi, and A( ·) indicates\nthe Adapter block. Simultaneously, the visible feature Fv is fed\ninto the SEP , and the resulting output is passed through a Sigmoid\nactivation function to obtain the weight Ws for expanding the CLIP\nfeature space. To align Fc\nf ∈ R1×1×C with Ws ∈ RH×W×C in\nspatial dimensions, we utilize a broadcasting mechanism to achieve\nthe spatial alignment. The broadcasting mechanism is a commonly\nused operation in the ﬁeld of deep learning, which implicitly\nreplicates the shape of smaller tensors to match that of larger\ntensors. The resulting output is then element-wise multiplied with\nWs to obtain the semantic-rich feature Fc\ns ∈ RH×W×C. Finally, we\ninject the semantic information into the visible feature Fv through\nan element-wise addition:\n˜Fv = Fv + Fc\ns, (2)\nwhere, ˜Fv represents the visible feature enriched with semantic\ninformation. Similarly, we input the infrared image Ii, the visible\nimage Iv, and the infrared feature Fi into the CII block to obtain the\ninfrared feature ˜Fi, which is enriched with semantic information.\n/three.tnum./three.tnum Feature fusion and reconstruction\nIn existing fusion methods, fusion strategies typically involve\nelement-wise addition or channel dimension concatenation, which\noften struggle to address image fusion in complex scenes, resulting\nin suboptimal fusion quality. To overcome these challenges, we\nleverage the robust feature representations extracted by the pre-\ntrained CLIP to guide the fusion of infrared and visible features.\nAs illustrated in\nFigure 3, the CFF block primarily comprises a\nfrozen parameter pre-trained CLIP , an Infrared Adapter (IRA), a\nVisible Adapter (VIA), and a Spatial Attention Weight Prediction\n(SAWP) block. The IRA and VIA are responsible for generating\nattention weights that guide the fusion of infrared and visible\nfeatures, respectively. The SAWP aggregates gradient information\nfrom the feature maps at the spatial level and generates weights to\nenhance texture details. In terms of network architecture, both the\nIRA and VIA are structured identically, consisting of two linear\nmapping layers. The SAWP is composed of two convolutional\nlayers with a kernel size of 3 × 3 and a stride of 1, and a single ReLU\nactivation function layer. In the CFF block, we input the infrared\nimage Ii and the visible image Iv into the pre-trained CLIP image\nencoder, with the resulting features being fed into the IRA and VIA\nto obtain features Wf\ni ∈ R1×1×C and Wf\nv ∈ R1×1×C, respectively.\nTo guide the fusion of the infrared and visible features, we utilize\na broadcasting mechanism to perform element-wise multiplication\nof Wf\ni and Wf\nv with ˜Fi and ˜Fv, respectively, and concatenate the\nresulting outputs along the channel dimension:\nFf =\n[\nWf\ni ⊙ ˜Fi, Wf\nv ⊙ ˜Fv\n]\n, (3)\nwhere, Ff ∈ RH×W×C represents the fused features, and [ ·] denotes\nthe concatenation operation along the channel dimension.\nTo enhance the texture detail information within the fused\nfeatures, we apply the Sobel operator for gradient extraction on\nFf , and the resulting gradient map is subsequently input into the\nSAWP and a Sigmoid activation function:\nWg = Sigmoid(S(∇Ff )), (4)\nwhere, Wg represents the spatial weights used to enhance texture\ndetails, while S( ·) denotes the SAWP block, ∇ denotes the Sobel\noperator. We perform an element-wise multiplication of Wg and\nFf , and the resulting output is reinjected into Ff to enhance the\ntexture detail information within Ff :\n˜Ff = Ff + Wg ⊙ Ff , (5)\nwhere, ˜Ff represents the enhanced fused features. Finally, we\ninput ˜Ff into the FD to reconstruct the fused image If . The FD\nconsists of N Restormer Blocks (\nZamir et al., 2022 ), one feature\nextraction layer, and one image reconstruction layer. The image\nreconstruction layer is composed of a convolutional layer with a\nkernel size of 3 × 3 and a stride of 1, followed by a Tanh activation\nfunction layer.\nTo maximize the transfer of gradient information and pixel\nintensity information from the infrared and visible images to the\nfused image, we introduce a gradient loss ℓg and a pixel intensity\nloss ℓi to jointly construct the total fusion loss ℓf :\nℓf = ℓg + λℓi, (6)\nwhere, λ represents the parameters used to balance the individual\nloss components. The gradient loss ℓg :\nℓg = 1\nHW\n\n ∇If − max (∇Ii, ∇Iv)\n\n\n1. (7)\nAnd the pixel intensity loss ℓi:\nℓi = 1\nHW\n\n If − max (Ii, Iv)\n\n\n1, (8)\nwhere, H and W represent the height and width of the fused image,\nrespectively, ∥·∥1 denotes the l1-norm, and max( ·) represents the\nelement-wise maximum value.\n/four.tnum Experiments\n/four.tnum./one.tnum Datasets\nWe combined the RoadScene ( Xu et al., 2022 ), M 3FD ( Liu\net al., 2022a ), MSRS ( Tang et al., 2022 ), and LLVIP ( Jia et al.,\n2021) datasets into a uniﬁed dataset and performed end-to-end\ntraining of the fusion network on this uniﬁed dataset. This uniﬁed\ndataset includes diverse scenes from both daytime and nighttime\nas well as infrared images from diﬀerent spectral bands. Training\nthe fusion network on this uniﬁed dataset signiﬁcantly enhances its\ngeneralization ability when processing source images from varying\nscenes and spectral bands. Additionally, we validated the fusion\nperformance of our method on ﬁve datasets: RoadScene, LLVIP ,\nMSRS, M 3FD, and TNO (\nToet, 2017 ). Our experimental setup\nFrontiers in Neurorobotics /zero.tnum/six.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nFIGURE /three.tnum\nCLIP-guided feature fusion.\nstrictly follows the standard protocols in the domain. Speciﬁcally,\nwe randomly selected 200, 201, 217, and 230 pairs of infrared\nand visible images from RoadScene, LLVIP , MSRS, and M 3FD,\nrespectively, as the training set. To enhance the diversity of the\ntraining samples, we applied various data augmentation techniques,\nincluding random ﬂipping, random rotation, and random cropping\n(with a cropping size of 256 × 256). Furthermore, we randomly\nselected 20 pairs of infrared and visible images from each of the four\ndatasets as the test set to evaluate the performance of the proposed\nmethod under supervised learning. To verify the generalization\ncapability of the proposed method, we randomly selected 55 pairs\nof infrared and visible images from TNO as the test set and assessed\nthe model’s generalization performance on this dataset.\n/four.tnum./two.tnum Implementation details\nThe method proposed in this paper use the Adam Optimizer\n(\nKingma and Ba, 2015 ) to update the network parameters, with a\nbatch size of 16 and a total of 100 training epochs. During training,\na dynamic learning rate adjustment strategy is utilized: the learning\nrate gradually increases from an initial value of 1 ×10−4 to 1 ×10−3\nover the ﬁrst 20 epochs, and then decreases from 1 × 10−3 to 1 ×\n10−4 after the 20th epoch. Additionally, we set the hyperparameter\nλ to 0.2. This method is implemented using the PyTorch framework\nand trained on a single NVIDIA GeForce RTX 4090 GPU.\n/four.tnum./three.tnum Evaluation metrics\nTo quantitatively compare the method proposed in this paper\nwith existing methods, we adopted six widely used objective\nevaluation metrics in the ﬁeld of image fusion: Gradient-based\nfusion performance ( QAB/F;\nXydeas and Petrovic, 2000 ), Chen-\nVarshney metric ( QCV ; Chen and Varshney, 2007 ; Liu Y. et al.,\n2024), Structural similarity index measure ( QSSIM; Wang et al.,\n2004), Average gradient ( QAG; Zhang X. et al., 2020 ), Visual\ninformation ﬁdelity ( QVIF; Ma et al., 2019a ), and Sum of correlation\ndiﬀerences ( QSCD; Aslantas and Bendes, 2015 ). Among these\nmetrics, QAB/F measures the retention of edge information from the\nsource images in the fused image. A higher value indicates that the\nfused image contains richer edge information. QCV evaluates the\nquality of the fused image based on human visual perception, with\nsmaller values indicating better perceptual quality. QSSIM assesses\nthe similarity between the fused image and the source images in\nterms of brightness, contrast, and structure. A larger value suggests\nless information loss and lower distortion in the fused image.\nQAG quantiﬁes the texture detail information in the fused image,\nwith larger values indicating richer texture details. QVIF evaluates\nthe shared information between the fused image and the source\nimages based on human visual systems. A higher value indicates\nbetter visual ﬁdelity of the fused image. QSCD uses the diﬀerential\nimage between the source and fused images to assess the amount\nof information transfer. Larger values suggest smaller information\ndiﬀerences between the fused and source images. Among these\nmetrics, QAB/F, QSSIM, QAG, QVIF, and QSCD are positive indicators,\nmeaning that larger values indicate better fusion performance of\nthe compared methods. In contrast, QCV is a negative indicator,\nwhere smaller values represent better fusion performance of the\ncompared methods.\n/four.tnum./four.tnum Comparison experiments\nTo validate the superiority of the method proposed in this\npaper compared to existing SOTA fusion methods, we designed\ntwo experimental setups. In the ﬁrst experiment, we compared\nour method with advanced fusion methods to highlight its\nadvantages in visual quality and objective evaluation. The second\nexperiment aimed to assess the generalization capability of our\nproposed method, where we conducted qualitative and quantitative\ncomparisons on the untrained dataset TNO. Through these\nFrontiers in Neurorobotics /zero.tnum/seven.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nFIGURE /four.tnum\nCompares the visual quality of the proposed method with SOTA fusion methods. The ﬁrst and second columns show the infrared and visible images\nto be fused, respectively. The images in columns three through nin e represent the fused results from various comparison methods. The ﬁ rst two\nrows of images are from the LLVIP dataset, while the last two rows ar e from the MSRS dataset.\nFIGURE /five.tnum\nCompares the visual quality of the proposed method with SOTA fusion methods. The ﬁrst and second columns show the infrared and visible images\nto be fused, respectively. The images in columns three through nin e represent the fused results from various comparison methods. The ﬁ rst two\nrows of images are from the RoadScene dataset, while the last two row s are from the M /three.tnumFD dataset.\ntwo experimental setups, we aim to provide a comprehensive\nand precise evaluation of the fusion performance of our\nproposed method.\n/four.tnum./four.tnum./one.tnum Comparison with state-of-the-art methods\nWe compared the method proposed in this paper with six\nadvanced fusion methods: MLFusion (\nLi et al., 2021a ), DATFuse\n(Tang et al., 2023d), LRRNet ( Li et al., 2023b ), CHITNet ( Du, 2023),\nIVFWSR ( Li et al., 2023a ), and TIMFusion ( Liu R. et al., 2024 ).\nThe fusion results are shown in Figures 4, 5. The ﬁrst two columns\nof Figures 4, 5 illustrate that there is substantial complementary\ninformation between infrared and visible images. Analysis of the\noverall brightness and contrast of the fused images indicates that\nour proposed method achieves higher contrast and brightness in\nboth nighttime and daytime scenes, aligning better with human\nvisual perception. This phenomenon is particularly evident in the\nsecond and third columns of\nFigure 4 and the second column\nof Figure 5. Compared to the other methods, the fused images\ngenerated by our proposed approach exhibit higher brightness\nand contrast for features such as sidewalks, distant vehicles, and\nclouds. To further emphasize the visual advantages of our method,\nwe conducted zoom-in analysis on local regions. From these\nenlarged regions, it is evident that our proposed method achieves\na better balance in preserving thermal radiation information\nand texture details for objects like pedestrians and vehicles.\nWhile signiﬁcantly retaining thermal radiation information, the\ntexture details of these objects remain clear. For example, in\nthe zoomed-in region of the ﬁrst column in\nFigure 5, some\ncomparison methods show similar brightness for vehicles, but\nFrontiers in Neurorobotics /zero.tnum/eight.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nTABLE /one.tnum Quantitative evaluation results on the LLVIP dataset and the M/three.tnumFD dataset.\nMethods LLVIP M3FD\nQAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑ QAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑\nMLFusion 0.3275 553.87 1.2779 2.7637 0.2800 0.9936 0.4275 675.23 1.2943 4.5659 0.3719 1.2975\nDATFuse 0.4691 413.56 1.2972 3.0195 0.3883 1.3473 0.4736 555.48 1.3046 4.1460 0.3314 1.3393\nLRRNet 0.4206 572.32 1.3097 2.7998 0.2976 0.9901 0.5156 578.66 1.3634 4.5106 0.3166 1.3407\nCHITNet 0.5252 775.84 1.3158 3.5493 0.3881 1.4497 0.4735 804.87 1.3692 4.9710 0.3294 1.4884\nIVFWSR 0.2605 584.51 1.2757 2.3347 0.2070 1.2469 0.4472 718.98 1.2678 3.8479 0.2847 1.2975\nTIMFusion 0.2895 886.35 1.1581 2.4373 0.3000 0.5524 0.5153 627.03 1.2875 4.3536 0.3190 1.1215\nOurs 0.6950 272.64 1.3401 4.4622 0.4693 1.6128 0.6802 400.56 1.3089 6.0424 0.4180 1.5183\nThe top three results are highlighted using red, blue, and green.\nTABLE /two.tnum Quantitative evaluation results on the MSRS dataset and the RoadScene dataset.\nMethods MSRS RoadScene\nQAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑ QAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑\nMLFusion 0.2825 798.82 1.3674 2.4207 0.2111 1.1943 0.4581 542.13 1.3629 4.0150 0.3936 1.3728\nDATFuse 0.6299 404.84 1.2680 3.4412 0.4124 1.5900 0.4920 489.05 1.3506 4.2706 0.3523 1.3485\nLRRNet 0.4241 677.40 1.2601 2.5557 0.2849 1.0710 0.3872 655.44 1.1970 5.1535 0.3458 0.9411\nCHITNet 0.4748 783.83 1.2482 3.3012 0.3569 1.5300 0.4906 881.26 1.4049 5.1939 0.3452 1.5075\nIVFWSR 0.3527 748.67 1.3172 2.1713 0.2462 1.2424 0.3219 1088.85 1.0306 4.0258 0.2242 1.0511\nTIMFusion 0.3914 1132.22 1.1094 2.5886 0.3085 1.1499 0.3730 734.92 1.1951 4.4605 0.3914 1.0018\nOurs 0.6666 327.47 1.3823 3.5313 0.4601 1.7657 0.5911 465.75 1.3325 5.8211 0.3731 1.5180\nThe top three results are highlighted using red, blue, and green.\nour proposed method displays more prominent and clearer\ntexture details.\nTo further validate the superiority of the proposed method, we\nconducted a quantitative comparison of the fusion results using\nsix commonly used objective evaluation metrics. The quantitative\nevaluation results are presented in\nTables 1, 2. Analysis of Tables 1,\n2 reveals that our proposed method outperforms most other\nmethods in the average values of the six evaluation metrics.\nThis advantage in fusion performance is particularly evident in\nmetrics QAB/F and QCV . In these two metrics, our method ranks\nﬁrst across the RoadScene, LLVIP , MSRS, and M 3FD datasets\nand demonstrates signiﬁcant superiority over other methods. In\nsummary, our proposed method exhibits clear advantages in both\nvisual quality and objective evaluation metrics compared to existing\nadvanced methods.\n/four.tnum./four.tnum./two.tnum Veriﬁcation of generalization ability\nWe conducted qualitative and quantitative experiments on the\nuntrained dataset TNO to verify the generalization capability of the\nproposed method. Speciﬁcally, we compared our proposed method\nwith MLFusion, DATFuse, LRRNet, CHITNet, IVFWSR, and\nTIMFusion on the TNO dataset. The fusion results are presented\nin\nFigure 6. On the TNO dataset, the fused images generated by our\nproposed method maintain good brightness and contrast overall.\nAs shown in the ﬁrst row of\nFigure 6, the brightness and contrast\nof the building window areas surpass those of the other methods,\nallowing observers to quickly locate the position of the windows.\nAnother advantage in visual quality lies in the preservation of\ntexture details in local regions. For example, in the zoomed-in\narea of the second row in\nFigure 6, our proposed method retains\nbetter detail of the vehicle contours, providing a more accurate\nreﬂection of the vehicle’s condition. In contrast, the fused images\ngenerated by other methods fail to comprehensively display the\ndetails of the vehicle wheels, lacking a good balance between\nbrightness and texture, which hinders observers from quickly and\naccurately assessing the vehicle’s status. As shown in\nTable 3, we\nconducted a quantitative assessment of the generalization capability\nof our proposed method. The results indicate that our proposed\nmethod achieves optimal or near-optimal levels across all metrics,\ndemonstrating its superiority over other comparison methods.\nIn summary, the results of both qualitative and quantitative\ncomparisons indicate that our proposed method exhibits strong\ngeneralization capability.\n/four.tnum./five.tnum Ablation study\nThe method proposed in this paper mainly consists of two core\ncomponents: CLIP-driven Information Injection (CII) and CLIP-\nguided Feature Fusion (CFF). To validate the eﬀectiveness of these\ntwo components, we conducted a series of ablation experiments\non the MSRS dataset and performed qualitative and quantitative\nanalyzes on the test set.\nFrontiers in Neurorobotics /zero.tnum/nine.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nFIGURE /six.tnum\nCompares the visual quality on the TNO dataset. The ﬁrst and sec ond columns show the infrared and visible images to be fused, respec tively. The\nimages in columns three through nine represent the fused results fr om various comparison methods.\nTABLE /three.tnum Quantitative evaluation results on the TNO dataset.\nMethods TNO\nQAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑\nMLFusion 0.3909 379.92 1.3559 3.5907 0.4040 1.3990\nDATFuse 0.4946 437.10 1.3693 3.7790 0.3769 1.3508\nLRRNet 0.3588 834.27 1.3107 4.2212 0.4083 1.3431\nCHITNet 0.4393 387.36 1.3804 5.0692 0.4192 1.6149\nIVFWSR 0.3299 1381.49 1.2305 3.1430 0.2946 1.3100\nTIMFusion 0.3915 695.57 1.2797 3.4787 0.4553 1.0899\nOurs 0.5654 268.83 1.3921 4.4210 0.4352 1.5644\nThe top three results are highlighted using red, blue, and green.\n/four.tnum./five.tnum./one.tnum Eﬀectiveness of CII\nIn the method we propose, CII is a key component. It utilizes\nimage features extracted by CLIP to inject semantic information\ninto infrared and visible features, thereby enhancing the features’\nrepresentation capability for the scene. To evaluate the eﬀectiveness\nof CII, we conducted experiments by removing CII from the fusion\nframework and directly inputting the infrared and visible features\nobtained from IRE/VIE into CFF for subsequent processing. The\nresults of the ablation experiments, shown in\nFigure 7, indicate\nthat the model lacking CII exhibits a signiﬁcant deﬁciency in\ndetail information when fusing features of streetlights and distant\nbuildings. This suggests that the absence of additional semantic\ninformation injection leads to a decline in the quality of the\nfused images. In contrast, our method, with semantic information\ninjection, demonstrates richer texture details and better image\nquality. To further assess the impact of CII on image quality\nenhancement, we conducted quantitative comparisons across six\nevaluation metrics, as shown in\nTable 4. Analysis of Table 4 reveals\nthat our method outperforms the model lacking CII on most\nevaluation metrics, further validating the eﬀectiveness of CII.\n/four.tnum./five.tnum./two.tnum Eﬀectiveness of CFF\nIn the method we propose, CFF is a key component. It\nconstructs a fusion strategy based on CLIP and a multimodal\nlarge language model for feature selection and fusion, addressing\nimage fusion in complex scenes. To evaluate the eﬀectiveness\nof CFF, we removed it from the fusion framework and directly\nconcatenated the infrared and visible light features output by\nCII along the channel dimension before inputting them into FD\nfor image reconstruction. From the zoomed-in areas in\nFigure 7,\nit can be observed that the model lacking CFF experiences\nsigniﬁcant information loss when fusing features of illuminated\nstreetlights and overexposed buildings, making it diﬃcult to retain\ninformation from the source images. In contrast, our method\neﬀectively aggregates information from source images in such\ncomplex scenes, producing higher-quality fusion results. According\nto the quantitative comparison results in\nTable 4, the model without\nCFF is inferior to the complete model in the average values of all\nevaluation metrics. Combining both quantitative and qualitative\ncomparisons, CFF plays an important role in image fusion for\ncomplex scenes.\n/five.tnum Conclusion\nThis paper investigates the enhancement of image quality in\ninfrared and visible image fusion and proposes a novel fusion\nmethod. To address the limitations of existing methods that rely\non complex network architectures for improving image quality\nand to tackle the challenges of image fusion in complex scenarios,\nwe introduce a multimodal large language model-driven approach\nfor infrared and visible light image fusion. This method utilizes\nrobust image features rich in semantic information extracted by\nCLIP to supplement the infrared and visible features, thereby\nmeeting the high demand for semantic information in enhancing\nimage quality. Furthermore, to address the complexities of fusion\nscenarios, we leverage CLIP’s powerful semantic understanding\ncapabilities to select and fuse infrared and visible features. Extensive\nqualitative and quantitative experiments demonstrate a signiﬁcant\nimprovement in the eﬀectiveness and superiority of our proposed\nFrontiers in Neurorobotics /one.tnum/zero.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nFIGURE /seven.tnum\nQualitative results of the ablation study. The scene on the left h alf of the image is derived from the “/zero.tnum/zero.tnum/nine.tnum/two.tnum/seven.tnumN” image in the MSRS dataset, while the\nscene on the right half comes from the “/zero.tnum/zero.tnum/four.tnum/two.tnum/seven.tnumD” image in the MSRS dataset.\nTABLE /four.tnum Quantitative evaluation results of the ablation experiments on MSRS dataset.\nMethods QAB/F ↑ QCV ↓ QSSIM ↑ QAG ↑ QVIF ↑ QSCD ↑\nw/o CII 0.6503 343.19 1.3730 3.5350 0.4456 1.7805\nw/o CFF 0.6260 351.21 1.3659 3.5073 0.4389 1.7624\nw/o CII and CFF 0.6228 339.40 1.3806 3.4875 0.4462 1.7938\nOurs 0.6666 327.47 1.3823 3.5313 0.4601 1.7657\nThe top result is highlighted in red.\nmethod compared to existing approaches. Our method is primarily\ndesigned for the fusion of infrared and visible images. When\ndirectly applied to other image fusion tasks, such as multi-focus\nimage fusion, multi-exposure image fusion, or medical image\nfusion, its performance may decline. To address this issue, task-\nspeciﬁc loss functions need to be introduced, and the network needs\nto be retrained to maintain satisfactory fusion performance. In light\nof the limitations of the proposed method, future research will\nfocus on expanding the application of multimodal large language\nmodels to other image fusion tasks. Additionally, we will conduct\nan in-depth exploration of the commonalities among multimodal\nlarge language models and incorporate more diverse types of these\nmodels to further enhance the quality of fused images.\nData availability statement\nPublicly available datasets were analyzed in this study. The\ndatasets for this study can be found in the RoadScene dataset:\nhttps://github.com/hanna-xu/RoadScene, the MSRS dataset:\nhttps://github.com/Linfeng-Tang/MSRS, the LLVIP dataset:\nhttps://github.com/bupt-ai-cz/LLVIP, the M3FD dataset: https://\ngithub.com/dlut-dimt/TarDAL, and the TNO dataset: https://\nﬁgshare.com/articles/dataset/TNOImageFusionDataset/1008029.\nAuthor contributions\nZY: Conceptualization, Data curation, Formal analysis,\nFunding acquisition, Investigation, Methodology, Project\nadministration, Resources, Software, Supervision, Validation,\nVisualization, Writing - original draft, Writing - review &\nediting. YL: Writing - original draft, Writing - review & editing,\nSupervision. XT: Writing - original draft, Writing - review &\nediting, Supervision. MX: Writing - original draft, Writing - review\n& editing, Supervision.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This work\nwas supported in part by the National Natural Science Foundation\nof China (62161015) and the Yunnan Fundamental Research\nProjects (202301AV070004).\nConﬂict of interest\nYL and XT were employed by Hongyunhonghe Tobacco Group\nCompany Limited.\nFrontiers in Neurorobotics /one.tnum/one.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nThe remaining authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAslantas, V., and Bendes, E. (2015). A new image quality metric f or image fusion:\nthe sum of the correlations of diﬀerences. Int. J. Electr. Commun . 69, 1890–1896.\ndoi: 10.1016/j.aeue.2015.09.004\nChen, H., and Varshney, P. K. (2007). A human perception inspire d quality\nmetric for image fusion based on regional information. Inform. Fus . 8, 193–207.\ndoi: 10.1016/j.inﬀus.2005.10.001\nChen, J., Yang, L., Liu, W., Tian, X., and Ma, J. (2024). Lenfu sion: a joint low-light\nenhancement and fusion network for nighttime infrared and v isible image fusion. IEEE\nTrans. Instr. Measur . 73, 1–15. doi: 10.1109/TIM.2024.3485462\nDong, W., Zhu, H., Lin, S., Luo, X., Shen, Y., Liu, X., et al. (20 24). Fusion-mamba\nfor cross-modality object detection. arXiv. doi: 10.48550/arXiv.2404.09146\nDu, K., Li, H., Zhang, Y., and Yu, Z. (2023). ChitNet: a compleme ntary to\nharmonious information transfer network for infrared and v isible image fusion. arXiv\npreprint arXiv:2309.06118. doi: 10.48550/arXiv.2309.06118\nHuang, Z., Liu, J., Fan, X., Liu, R., Zhong, W., and Luo, Z. (20 22). “Reconet:\nrecurrent correction network for fast and eﬃcient multi-mod ality image fusion, ” in\nEuropean Conference on Computer Vision (ECCV2022) (Tel Aviv: ECCV2022; Berlin:\nSpringer), 539–555.\nJia, X., Zhu, C., Li, M., Tang, W., and Zhou, W. (2021). “Llvip: a visible-infrared\npaired dataset for low-light vision, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops (ICCVW) (Montreal, BC: ICCVW;\nPiscataway, NJ: IEEE), 3496–3504.\nJian, L., Yang, X., Liu, Z., Jeon, G., Gao, M., and Chisholm, D. ( 2021).\nSedrfuse: a symmetric encoder—decoder with residual block n etwork for infrared and\nvisible image fusion. IEEE Trans. Instr. Measur . 70, 1–15. doi: 10.1109/TIM.2020.30\n22438\nKingma, D. P., and Ba, J. (2015). “Adam: a method for stochast ic optimization, ” in\nInternational Conference on Learning Representations (ICLR) . San Diego, CA: ICLR.\nLi, H., Cen, Y., Liu, Y., Chen, X., and Yu, Z. (2021a). Diﬀeren t input\nresolutions and arbitrary output resolution: a meta learning-b ased deep framework\nfor infrared and visible image fusion. IEEE Trans. Image Process . 30, 4070–4083.\ndoi: 10.1109/TIP.2021.3069339\nLi, H., Liu, J., Zhang, Y., and Liu, Y. (2023a). A deep learning framework for infrared\nand visible image fusion without strict registration. Int. J. Comput. Vis . 132, 1625–1644.\ndoi: 10.1007/s11263-023-01948-x\nLi, H., Wang, D., Huang, Y., Zhang, Y., and Yu, Z. (2024a). Gene ration and\nrecombination for multifocus image fusion with free number o f inputs. IEEE\nTrans. Circ. Syst. Video Technol . 34, 6009–6023. doi: 10.1109/TCSVT.2023.334\n4222\nLi, H., and Wu, X.-J. (2019). DenseFuse: a fusion approach to in frared and visible\nimages. IEEE Trans. Image Process . 28, 2614–2623. doi: 10.1109/TIP.2018.2887342\nLi, H., Wu, X.-J., and Kittler, J. (2020). MDLatLRR: a novel de composition method\nfor infrared and visible image fusion. IEEE Trans. Image Process . 29, 4733–4746.\ndoi: 10.1109/TIP.2020.2975984\nLi, H., Wu, X.-J., and Kittler, J. (2021b). RFN-Nest: an end-t o-end residual\nfusion network for infrared and visible images. Inform. Fus . 73, 72–86.\ndoi: 10.1016/j.inﬀus.2021.02.023\nLi, H., Xu, T., Wu, X.-J., Lu, J., and Kittler, J. (2023b). LRRN et: a novel\nrepresentation learning guided fusion network for infrared a nd visible images. IEEE\nTrans. Pat. Anal. Machine Intell . 45, 11040–11052. doi: 10.1109/TPAMI.2023.3268209\nLi, H., Yang, Z., Zhang, Y., Tao, D., and Yu, Z. (2024b). Single -image\nhdr reconstruction assisted ghost suppression and detail pres ervation network\nfor multi-exposure hdr imaging. IEEE Trans. Comput. Imag . 10, 429–445.\ndoi: 10.1109/TCI.2024.3369396\nLi, H., Yuan, M., Li, J., Liu, Y., Lu, G., Xu, Y., et al. (2024c). F ocus aﬃnity perception\nand super-resolution embedding for multifocus image fusion. IEEE Trans. Neural\nNetw. Learn. Syst . 2024, 1–15. doi: 10.1109/TNNLS.2024.3367782\nLi, H., Zhao, J., Li, J., Yu, Z., and Lu, G. (2023c). Feature dyn amic alignment and\nreﬁnement for infrared—visible image fusion: translation ro bust fusion. Inform. Fus .\n95, 26–41. doi: 10.1016/j.inﬀus.2023.02.011\nLi, J., Huo, H., Li, C., Wang, R., and Feng, Q. (2021). Attenti onfgan: Infrared and\nvisible image fusion using attention-based generative adve rsarial networks. IEEE Trans.\nMultimed. 23, 1383–1396. doi: 10.1109/TMM.2020.2997127\nLi, X., Liu, W., Li, X., and Tan, H. (2024). Physical perception network and\nan all-weather multi-modality benchmark for adverse weather im age fusion. arXiv.\ndoi: 10.48550/arXiv.2402.02090\nLiu, J., Fan, X., Huang, Z., Wu, G., Liu, R., Zhong, W., et al. (20 22a). “Target-\naware dual adversarial learning and a multi-scenario multi-mo dality benchmark to fuse\ninfrared and visible for object detection, ” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) (New Orleans, LA: CVPR;\nPiscataway, NJ: IEEE), 5802–5811.\nLiu, J., Fan, X., Jiang, J., Liu, R., and Luo, Z. (2022b). Lear ning a deep multi-scale\nfeature ensemble and an edge-attention guidance for image fu sion. IEEE Trans. Circ.\nSyst. Video Technol. 32, 105–119. doi: 10.1109/TCSVT.2021.3056725\nLiu, J., Lin, R., Wu, G., Liu, R., Luo, Z., and Fan, X. (2023a). C oconet: Coupled\ncontrastive learning network with multi-level feature ensemb le for multi-modality\nimage fusion. Int. J. Comput. Vis . 1, 1–28. doi: 10.1007/s11263-023-01952-1\nLiu, J., Liu, Z., Wu, G., Ma, L., Liu, R., Zhong, W., et al. (2023b ). “Multi-\ninteractive feature learning and a full-time multi-modality ben chmark for image fusion\nand segmentation, ” in 2023 IEEE/CVF International Conference on Computer Vision\n(ICCV), 8081–8090.\nLiu, J., Shang, J., Liu, R., and Fan, X. (2022c). Attention-g uided global-local\nadversarial learning for detail-preserving multi-exposure ima ge fusion. IEEE Trans.\nCirc. Syst. Video Technol . 32, 5026–5040. doi: 10.1109/TCSVT.2022.3144455\nLiu, J., Wu, Y., Huang, Z., Liu, R., and Fan, X. (2021). SMOA: s earching a modality-\noriented architecture for infrared and visible image fusion . IEEE Sign. Process. Lett . 28,\n1818–1822. doi: 10.1109/LSP.2021.3109818\nLiu, R., Liu, Z., Liu, J., Fan, X., and Luo, Z. (2024). A task-g uided, implicitly-searched\nand meta-initialized deep model for image fusion. IEEE Trans. Pat. Anal. Machine\nIntell. 46, 6594–6609. doi: 10.1109/TPAMI.2024.3382308\nLiu, S., Huang, D., and Wang, Y. (2018). “Receptive ﬁeld block ne t for accurate and\nfast object detection, ” in Proceedings of the European Conference on Computer Vision\n(ECCV) (Munich: ECCV; Berlin: Springer).\nLiu, Y., Qi, Z., Cheng, J., and Chen, X. (2024). Rethinking th e eﬀectiveness\nof objective evaluation metrics in multi-focus image fusion: a statistic-\nbased approach. IEEE Trans. Pat. Anal. Machine Intell . 46, 5806–5819.\ndoi: 10.1109/TPAMI.2024.3367905\nLiu, Y., Shi, Y., Mu, F., Cheng, J., and Chen, X. (2022d). Glioma segmentation-\noriented multi-modal MR image fusion with adversarial learni ng. IEEE/CAA J.\nAutomat. Sin. 9, 1528–1531. doi: 10.1109/JAS.2022.105770\nLiu, Y., Shi, Y., Mu, F., Cheng, J., Li, C., and Chen, X. (2022e ). Multimodal MRI\nvolumetric data fusion with convolutional neural networks. IEEE Trans. Instr. Measur .\n71, 1–15. doi: 10.1109/TIM.2022.3184360\nLiu, Z., Liu, J., Zhang, B., Ma, L., Fan, X., and Liu, R. (2023) . “PAIF: perception-aware\ninfrared-visible image fusion for attack-tolerant semantic segmentation, ” inProceedings\nof the 31st ACM International Conference on Multimedia , 3706–3714.\nMa, J., Ma, Y., and Li, C. (2019a). Infrared and visible image f usion methods and\napplications: a survey. Inform. Fus. 45, 153–178. doi: 10.1016/j.inﬀus.2018.02.004\nMa, J., Tang, L., Fan, F., Huang, J., Mei, X., and Ma, Y. (2022) . Swinfusion: cross-\ndomain long-range learning for general image fusion via swin t ransformer. IEEE/CAA\nJ. Automat. Sin . 9, 1200–1217. doi: 10.1109/JAS.2022.105686\nMa, J., Xu, H., Jiang, J., Mei, X., and Zhang, X.-P. (2020). DD CGAN: a dual-\ndiscriminator conditional generative adversarial networ k for multi-resolution image\nfusion. IEEE Trans. Image Process . 29, 4980–4995. doi: 10.1109/TIP.2020.2977573\nFrontiers in Neurorobotics /one.tnum/two.tnum frontiersin.org\nYang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/five.tnum/two.tnum/one.tnum/six.tnum/zero.tnum/three.tnum\nMa, J., Yu, W., Liang, P., Li, C., and Jiang, J. (2019b). Fusio nGAN: a generative\nadversarial network for infrared and visible image fusion. Inform. Fus . 48, 11–26.\ndoi: 10.1016/j.inﬀus.2018.09.004\nMa, J., Zhang, H., Shao, Z., Liang, P., and Xu, H. (2021). GANMC C: a\ngenerative adversarial network with multiclassiﬁcation con straints for infrared and\nvisible image fusion. IEEE Trans. Instr. Measur . 70, 1–14. doi: 10.1109/TIM.2020.\n3038013\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., et al.\n(2021). “Learning transferable visual models from natural lan guage supervision, ” in\nProceedings of the 38th International Conference on Machine Learning ( ICML), Vol. 139\n(Blaxton, MA: PMLR), 8748–8763.\nTang, L., Huang, H., Zhang, Y., Qi, G., and Yu, Z. (2023a). Stru cture-\nembedded ghosting artifact suppression network for high dyna mic range image\nreconstruction. Knowl. Bas. Syst . 263:110278. doi: 10.1016/j.knosys.2023.11\n0278\nTang, L., Xiang, X., Zhang, H., Gong, M., and Ma, J. (2023b). DI Vfusion:\ndarkness-free infrared and visible image fusion. Inform. Fus . 91, 477–493.\ndoi: 10.1016/j.inﬀus.2022.10.034\nTang, L., Yuan, J., Zhang, H., Jiang, X., and Ma, J. (2022). PI Afusion: A progressive\ninfrared and visible image fusion network based on illuminatio n aware. Inform. Fus .\n83–84, 79–92. doi: 10.1016/j.inﬀus.2022.03.007\nTang, W., He, F., and Liu, Y. (2023c). YDTR: infrared and visi ble image\nfusion via Y-shape dynamic transformer. IEEE Trans. Multimed . 25, 5413–5428.\ndoi: 10.1109/TMM.2022.3192661\nTang, W., He, F., Liu, Y., Duan, Y., and Si, T. (2023d). DATFus e: infrared and visible\nimage fusion via dual attention transformer. IEEE Trans. Circ. Syst. Video Technol . 33,\n3159–3172. doi: 10.1109/TCSVT.2023.3234340\nToet, A. (2017). The TNO multiband image data collection. Data Brief 15, 249–251.\ndoi: 10.1016/j.dib.2017.09.038\nWang, D., Liu, J., Ma, L., Liu, R., and Fan, X. (2024). Improvin g misaligned\nmulti-modality image fusion with one-stage progressive dense registration.\nIEEE Trans. Circ. Syst. Video Technol . 2024:3412743. doi: 10.1109/TCSVT.2024.\n3412743\nWang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. (2004). Ima ge quality assessment:\nfrom error visibility to structural similarity. IEEE Trans. Image Process . 13, 600–612.\ndoi: 10.1109/TIP.2003.819861\nXiao, W., Zhang, Y., Wang, H., Li, F., and Jin, H. (2022). Hete rogeneous\nknowledge distillation for simultaneous infrared-visible imag e fusion and\nsuper-resolution. IEEE Trans. Instr. Measur . 71, 1–15. doi: 10.1109/TIM.2022.\n3149101\nXie, M., Wang, J., and Zhang, Y. (2021). A uniﬁed framework fo r damaged image\nfusion and completion based on low-rank and sparse decomposition . Sign. Process.\nImage Commun. 98:116400. doi: 10.1016/j.image.2021.116400\nXu, H., Ma, J., Jiang, J., Guo, X., and Ling, H. (2022). U2Fusio n: a uniﬁed\nunsupervised image fusion network. IEEE Trans. Pat. Anal. Machine Intell . 44, 502–\n518. doi: 10.1109/TPAMI.2020.3012548\nXu, H., Yuan, J., and Ma, J. (2023). MURF: mutually reinforcing multi-modal\nimage registration and fusion. IEEE Trans. Pat. Anal. Machine Intell . 45, 12148–12166.\ndoi: 10.1109/TPAMI.2023.3283682\nXydeas, C. S., and Petrovic, V. (2000). Objective image fusi on performance measure.\nElectr. Lett. 36, 308–309. doi: 10.1049/el:20000267\nYi, X., Xu, H., Zhang, H., Tang, L., and Ma, J. (2024). “TEXT-I F: leveraging semantic\ntext guidance for degradation-aware and interactive image fusion, ” in2024 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) (Seattle, WA: CVPR;\nPiscataway, NJ: IEEE), 27016–27025.\nYue, J., Fang, L., Xia, S., Deng, Y., and Ma, J. (2023). DIF-fusion: Toward High Color\nFidelity in Infrared and Visible Image Fusion With Diﬀusion Models , 32. Piscataway, NJ:\nIEEE.\nZamir, S. W., Arora, A., Khan, S., Hayat, M., Khan, F. S., and Y ang, M. (2022).\n“RestorMer: eﬃcient transformer for high-resolution image restoration, ” in 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 5718–\n5729.\nZhang, H., Yuan, J., Tian, X., and Ma, J. (2021). GAN-FM: infra red and visible image\nfusion using gan with full-scale skip connection and dual marko vian discriminators.\nIEEE Trans. Comput. Imag . 7, 1134–1147. doi: 10.1109/TCI.2021.3119954\nZhang, H., Zuo, X., Jiang, J., Guo, C., and Ma, J. (2024). “MRFS : mutually\nreinforcing image fusion and segmentation, ” in 2024 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) (Seattle, WA: CVPR; Piscataway, NJ:\nIEEE), 26964–26973.\nZhang, X., Ye, P., and Xiao, G. (2020). “VIFB: a visible and infr ared image\nfusion benchmark, ” in 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW) (Seattle, WA: CVPR; Piscataway, NJ: IEEE), 468–\n478.\nZhang, Y., Yang, M., Li, N., and Yu, Z. (2020). Analysis-synth esis dictionary pair\nlearning and patch saliency measure for image fusion. Sign. Process . 167:107327.\ndoi: 10.1016/j.sigpro.2019.107327\nZhang, Y., Yang, X., Li, H., Xie, M., and Yu, Z. (2024). DCPNet : a dual-task\ncollaborative promotion network for pansharpening. IEEE Trans. Geosci. Rem. Sens .\n62, 1–16. doi: 10.1109/TGRS.2024.3377635\nZhao, Z., Bai, H., Zhang, J., Zhang, Y., Xu, S., Lin, Z., et al. ( 2023). “CDDFuse:\ncorrelation-driven dual-branch feature decomposition for mu lti-modality image\nfusion, ” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (Vancouver, BC: CVPR; Piscataway, NJ: IEEE), 5906–5916.\nZhou, H., Wu, W., Zhang, Y., Ma, J., and Ling, H. (2023). Seman tic-supervised\ninfrared and visible image fusion via a dual-discriminator ge nerative adversarial\nnetwork. IEEE Trans. Multimed . 25, 635–648. doi: 10.1109/TMM.2021.3129609\nZhu, Z., He, X., Qi, G., Li, Y., Cong, B., and Liu, Y. (2023). Bra in tumor segmentation\nbased on the fusion of deep semantics and edge information in multimodal MRI.\nInform. Fus. 91, 376–387. doi: 10.1016/j.inﬀus.2022.10.022\nZhu, Z., Wang, Z., Qi, G., Mazur, N., Yang, P., and Liu, Y. (2024 ). Brain tumor\nsegmentation in MRI with multi-modality spatial information e nhancement and\nboundary shape correction. Pat. Recogn. 153:110553. doi: 10.1016/j.patcog.2024.110553\nFrontiers in Neurorobotics /one.tnum/three.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8474880456924438
    },
    {
      "name": "Image fusion",
      "score": 0.6909188032150269
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5874578356742859
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.5577935576438904
    },
    {
      "name": "Focus (optics)",
      "score": 0.4979102611541748
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.49240267276763916
    },
    {
      "name": "Fusion",
      "score": 0.4643308222293854
    },
    {
      "name": "Image (mathematics)",
      "score": 0.44513756036758423
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4363346993923187
    },
    {
      "name": "Representation (politics)",
      "score": 0.43381911516189575
    },
    {
      "name": "Computer vision",
      "score": 0.41787219047546387
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3297688961029053
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}