{
  "title": "Understanding when spatial transformer networks do not support invariance, and what to do about it",
  "url": "https://openalex.org/W3018218353",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2999397432",
      "name": "Lukas Finnveden",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1876871654",
      "name": "Ylva Jansson",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A47817726",
      "name": "Tony Lindeberg",
      "affiliations": [
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2999397432",
      "name": "Lukas Finnveden",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1876871654",
      "name": "Ylva Jansson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A47817726",
      "name": "Tony Lindeberg",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2118877769",
    "https://openalex.org/W1938714998",
    "https://openalex.org/W1970269179",
    "https://openalex.org/W2165497495",
    "https://openalex.org/W2172188317",
    "https://openalex.org/W1980911747",
    "https://openalex.org/W2167383966",
    "https://openalex.org/W6694954494",
    "https://openalex.org/W6748579715",
    "https://openalex.org/W2981512247",
    "https://openalex.org/W6754783729",
    "https://openalex.org/W1912570122",
    "https://openalex.org/W6755816839",
    "https://openalex.org/W6765877273",
    "https://openalex.org/W6703116779",
    "https://openalex.org/W6764386301",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2792806930",
    "https://openalex.org/W6676338569",
    "https://openalex.org/W2912259665",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W6696085341",
    "https://openalex.org/W2562066862",
    "https://openalex.org/W2576989276"
  ],
  "abstract": "Spatial transformer networks (STNs) were designed to enable convolutional\\nneural networks (CNNs) to learn invariance to image transformations. STNs were\\noriginally proposed to transform CNN feature maps as well as input images. This\\nenables the use of more complex features when predicting transformation\\nparameters. However, since STNs perform a purely spatial transformation, they\\ndo not, in the general case, have the ability to align the feature maps of a\\ntransformed image with those of its original. STNs are therefore unable to\\nsupport invariance when transforming CNN feature maps. We present a simple\\nproof for this and study the practical implications, showing that this\\ninability is coupled with decreased classification accuracy. We therefore\\ninvestigate alternative STN architectures that make use of complex features. We\\nfind that while deeper localization networks are difficult to train,\\nlocalization networks that share parameters with the classification network\\nremain stable as they grow deeper, which allows for higher classification\\naccuracy on difficult datasets. Finally, we explore the interaction between\\nlocalization network complexity and iterative image alignment.\\n",
  "full_text": "Understanding when spatial transformer networks\ndo not support invariance, and what to do about it\nLukas Finnveden∗, Ylva Jansson ∗ and Tony Lindeberg\nComputational Brain Science Lab, Division of Computational Science and Technology\nKTH Royal Institute of Technology, Stockholm, Sweden\nAbstract—Spatial transformer networks (STNs) were designed\nto enable convolutional neural networks (CNNs) to learn invari-\nance to image transformations. STNs were originally proposed\nto transform CNN feature maps as well as input images. This\nenables the use of more complex features when predicting\ntransformation parameters. However, since STNs perform a\npurely spatial transformation, they do not, in the general case,\nhave the ability to align the feature maps of a transformed image\nwith those of its original. STNs are therefore unable to support\ninvariance when transforming CNN feature maps. We present\na simple proof for this and study the practical implications,\nshowing that this inability is coupled with decreased classiﬁcation\naccuracy. We therefore investigate alternative STN architectures\nthat make use of complex features. We ﬁnd that while deeper\nlocalization networks are difﬁcult to train, localization networks\nthat share parameters with the classiﬁcation network remain\nstable as they grow deeper, which allows for higher classiﬁcation\naccuracy on difﬁcult datasets. Finally, we explore the interaction\nbetween localization network complexity and iterative image\nalignment.\nI. I NTRODUCTION\nSpatial transformer networks (STNs) [1] constitute a widely\nused end-to-end trainable solution for CNNs to learn in-\nvariance to image transformations. This makes it part of a\ngrowing body of work concerned with developing CNNs that\nare invariant or robust to image transformations. The key\nidea behind STNs is to introduce a trainable module – the\nspatial transformer (ST) – that applies a data dependent spatial\ntransformation of input images or CNN feature maps before\nfurther processing. If such a module successfully learns to\nalign images to a canonical pose, it can enable invariant\nrecognition. However, when transforming CNN feature maps ,\nsuch alignment is, in general, not possible, and STNs can\ntherefore not enable invariant recognition. The reasons for\nthis are that: (i) STs perform a purely spatial transformation,\nwhereas transforming an image typically also results in a shift\nin the channel dimension of the feature activations (Figure 1),\n(ii) The shapes of the receptive ﬁelds of the individual neurons\nare not invariant. (Figure 2). These problems have, to our\nknowledge, not been discussed in the STN literature. In the\noriginal paper and a number of subsequent works, STNs are\npresented as an option for achieving invariance also when\napplied to intermediate feature maps [1]–[5].\n*The ﬁrst and second authors contributed equally to this work.\nShortened version in International Conference on Pattern Recognition\n(ICPR 2020), pages 3427–3434. Jan 2021. The support from the Swedish\nResearch Council (contract 2018-03586) is gratefully acknowledged.\nFig. 1. A spatial transformation of a CNN feature map cannot, in general,\nalign the feature maps of a transformed image with those of its original . Here,\nthe network Γ has two feature channels ”W” and ”M”, and Tg corresponds to\na 180◦ rotation. Since different feature channels respond to the rotated image\nas compared to the original image, it is not possible to align the respective\nfeature maps by applying the inverse spatial rotation to the feature maps.\nThis implies that spatially transforming feature maps cannot enable invariant\nrecognition by the means of aligning a set of feature maps to a common pose.\nOur ﬁrst contribution is to present a simple proof that STNs\ndo not enable invariant recognition when transforming CNN\nfeature maps. We do not claim mathematical novelty of this\nfact, which is in some sense intuitive and can be inferred from\nmore general results (see e.g. [6]), but we present a simple\nalternative proof directly applicable to STNs and accessible\nwith knowledge about basic analysis and some group theory.\nWe believe this is important since the idea that transforming\nfeature maps can achieve invariant recognition is often either\nproposed explicitly or the question about the ability for invari-\nance is ignored for a range of different methods transforming\nCNN feature maps or ﬁlters although often misunderstood\nor not taken into account, Our second contribution is to\nexplore the practical implications of this result. Is there a\npoint in transforming intermediate feature maps if this cannot\nenable invariant recognition? To investigate this, we compare\ndifferent STN architectures on the MNIST [7], SVHN [8] and\nPlankton [9] datasets. We show that STNs that transform the\nfeature maps are, indeed, worse at compensating for rotations\nand scaling transformations, while they – in accordance with\ntheory – handle pure translations well. We also show that\nthe inability of STNs to fully align CNN feature maps is\ncoupled with decreased classiﬁcation performance. Our third\ncontribution is to explore alternative STN architectures that\nmake use of more complex features when predicting im-\nage transformations. We show that: (i) Using more complex\nfeatures can signiﬁcantly improve performance, but most of\narXiv:2004.11678v5  [cs.CV]  18 May 2021\nthis advantage is lost if transforming CNN feature maps,\n(ii) sharing parameters between the localization network and\nthe classiﬁcation network makes training of deeper localization\nnetworks more stable and (iii) iterative image alignment can\nbe complimentary to, but is no replacement for, using more\ncomplex features.\nIn summary, this work clariﬁes the role and functioning\nof STNs that transform input images vs. CNN feature maps\nand illustrates important tradeoffs between different STN\narchitectures that make use of deeper features.\nA. Related work\nSuccessful image alignment can considerably simplify a\nrange of computer vision tasks by reducing variability related\nto differences in object pose. In classical computer vision,\nLukas and Kanade [10] developed a methodology for image\nalignment, which estimates translations iteratively. This ap-\nproach was later generalized to more general parameterized\ndeformation models [11]. Afﬁne shape adaptation of afﬁne\nGaussian kernels to local image structures – or equivalently,\nnormalizing image structures to canonical afﬁne invariant ref-\nerence frames [12] – has been an integrated part in frameworks\nfor invariant image-based matching and recognition [13]–[15].\nSuch classical approaches always align the input images .\nLately, the idea to combine structure and learning has\ngiven rise to the subﬁeld of invariant neural networks, which\nadd structural constraints to deep neural networks to enable\ne.g. scale or rotation invariant recognition [16]–[19]. Spa-\ntial transformer networks [1] are based on a similar idea\nof combining the knowledge about the structure of image\ntransformations with learning. An STN, however, does not\nhard code invariance to any speciﬁc transformation group\nbut learns input dependent image alignment from data. The\noriginal work [1] simultaneously claims the ability to learn\ninvariance from data and that ST modules can be inserted\nat “any depth” . This seems to have left some confusion\nabout whether STNs can enable invariance when transforming\nCNN feature maps. A number of subsequent works advocate\nto perform image alignment by transforming CNN feature\nmaps [2]–[5] including e.g. pose alignment of pedestrians [5]\nand to use a spatial transformer to mimic the kind of patch\nnormalization done in SIFT [2]. Additional works transform\nCNN feature maps without giving any speciﬁc motivation\n[20], [21]. As we will show, transforming CNN feature maps\nis not equivalent to extracting features from a transformed\ninput image. Other approaches dealing with pose variability\nby transforming neural network feature maps or ﬁlters are e.g.\nspatial pyramid pooling [22] and dilated [23] or deformable\nconvolutions [24]. Our results imply that these approaches\nhave limited ability to enable e.g. scale invariance.\nWeight sharing between the classiﬁcation and the localiza-\ntion network has previously been considered in [25], primarily\nas a way of regularizing CNNs. Here, we are instead interested\nin it as a way to make use of deeper features when predicting\nimage transformations. [26] combines STNs with iterative\nimage alignment. In this paper, we investigate whether such\niterative alignment is complimentary to using deeper features.\nPrevious theoretical work [6] characterizes all equivariant\n(covariant) maps between homogeneous spaces using the\ntheory of ﬁbers and ﬁelds. We, here, aim for a different\nperspective on the same theory. We present a simple proof\nfor the special case of purely spatial transformations of CNN\nfeature maps together with an experimental evaluation of\ndifferent STN architectures. A practical study [27] indicated\nthat approximate alignment of CNN feature maps can be\npossible if allowing for a full transformation, as opposed to the\npurely spatial transformations that we analyze in this paper.\nII. T HEORETICAL ANALYSIS OF INVARIANCE PROPERTIES\nSpatial transformer networks [1] were introduced as an\noption for CNNs to learn invariance to image transformations\nby transforming input images or convolutional feature maps\nbefore further processing. A spatial transformer (ST) module\nis composed of a localization network that predicts transfor-\nmation parameters and a transformer that transforms an image\nor a feature map using these parameters. An STN is a CNN\nwith one or several ST modules inserted at arbitrary depths.\nA. How STNs can enable invariance\nWe will here work with a continuous model of the image\nspace. We model an image as a measurable function f : Rn →\nR and denote this space of images as V. Let {Th}h∈H be a\nfamily of image transformations corresponding to a group H.\nTh transforms an image by acting on the underlying space\n(Thf)(x) = f(T−1\nh x) (1)\nwhere Th : Rn → Rn is a linear map. We here consider\nafﬁne image transformations, but the general argument is also\nvalid for non-linear invertible transformations such as e.g.\ndiffeomorphisms. Let Γ : V →Vk be a (possibly non-linear)\ntranslation covariant feature extractor with k feature channels.\nΓ could e.g. correspond to a sequence of convolutions and\npointwise non-linearities. Γ is invariant to Th if the feature\nresponse for a transformed image is equal to that of its original\n(ΓThf)c(x) = (Γf)c(x), (2)\nwhere c∈[1,2,···k] corresponds to the feature channel. An\nST module can support invariance by learning to transform\ninput images to a canonical pose, before feature extraction, by\napplying the inverse transformation\n(Γ ST(Thf))c(x) = (ΓT−1\nh Thf)c(x) = (Γf)c(x). (3)\nWe will in the following assume such a perfect ST that always\nmanages to predict the correct pose of an object. 1 We now\nshow that even a perfect ST cannot support invariance if\ninstead applied to CNN feature maps .\n1There is no hard-coding of invariance in an STN and no guarantee that the\npredicted object pose is correct or itself invariant. We here assume the ideal\ncase where the localization network does learn to predict the transformations\nthat would align a relevant subset of all images (e.g. all images of the same\nclass) to a common pose.\n2\nFig. 2. For any transformation that includes a scaling component, the ﬁeld\nof view of a feature extractor with respect to an object will differ between an\noriginal and a rescaled image. Consider a simple linear model that performs\ntemplate matching with a single ﬁlter. When applied to the original image,\nthe ﬁlter matches the size of the object that it has been trained to recognize\nand thus responds strongly. When applied to a rescaled image, the ﬁlter never\ncovers the full object of interest. Thus, the response cannot be guaranteed to\ntake even the same set of values for a rescaled image and its original.\nB. The problems of transforming CNN feature maps\nAn advantage of inserting an ST deeper into the network is\nthat the ST can make use of more complex features shared with\nthe classiﬁcation network. When using STNs that transform\nfeature maps, as opposed to input images, the key question is\nwhether it is possible to undo a transformation of an image\nafter feature extraction . Is there a Tg dependent on Th such\nthat (applying the same transformation in each feature channel)\n(TgΓThf)c(x) = (ΓThf)c(T−1\ng x)\n?\n= (Γf)c(x) (4)\nholds for all f,Γ and h? If this is possible, we refer to it as\nfeature map alignment . An ST that transforms CNN feature\nmaps could then support invariance by the same mechanism\nas for input images. We here present the key intuitions and\nthe outline of a proof that this is, in the general case, not\npossible. We refer to [28] for a mathematically rigorous proof.\nNote that for any translation covariant feature extractor, such\nas a continuous or discrete CNN, feature map alignment for\ntranslated images is, however, possible by means of a simple\ntranslation of the feature maps.\n1) Using Tg = T−1\nh is a necessary condition to align CNN\nfeature maps with an ST: The natural way to align the feature\nmaps of a transformed image to those of its original would\nbe to apply the inverse spatial transformation to the feature\nmaps of the transformed image i.e.\nT−1\nh (ΓThf)c(x) = (ΓThf)c(Thx). (5)\nFor example, to align the feature maps of an original and a\nrescaled image, we would, after feature extraction, apply the\ninverse scaling to the feature maps. Using T−1\nh is, in fact,\na necessary condition for (4) to hold [28]. To see this, note\nthat the value for each spatial position x must be computed\nfrom the same region in the original image for the right hand\nand left hand side. Clearly, features extracted from different\nimage regions cannot be guaranteed to be equal. Assume that\n(Γf)c(x) is computed from a region Ω centered at x in the\noriginal image. Using the deﬁnition of Th and the fact that Γ\nis translation covariant, we get that (TgΓThf)(x) is computed\nfrom a region Ω′ centered at T−1\ng T−1\nh x. Now,\nΩ = Ω′ =⇒ T−1\ng T−1\nh x= x =⇒ Tg = T−1\nh (6)\nand thus the only candidate transformation to align CNN\nfeature maps with an ST is Tg = T−1\nh . We, next, show that\nusing Tg = T−1\nh is, however, not a sufﬁcient condition to\nenable feature map alignment for two key reasons.\n2) Transforming an image typically implies a shift in the\nchannel dimension of the feature map: When transforming\nan input image, this typically causes not only a spatial shift\nin its feature representation but also a shift in the channel\ndimension. This problem is illustrated in Figure 1. Since an\nST performs a purely spatial transformation, it cannot correct\nfor this. A similar reasoning is applicable to a wide range of\nimage transformations. An exception would be if the features\nextracted at a speciﬁc layer are themselves invariant to H.\nAn example of this would be a network built from rotation\ninvariant ﬁlters λ, where λ(x) = λ(Thx) for all λ. For\nsuch a network, or a network with more complex (learned or\nhardcoded) rotation invariant features at a certain layer, feature\nmap alignment of rotated images would be possible.\nIt should be noted, however, that to have invariant features in\nintermediate layers is in many cases not desirable (especially\nnot early in the network), since they discard too much informa-\ntion about the object pose. For example, rotation invariant edge\ndetectors would lose information about the edge orientations\nwhich tend to be important for subsequent tasks.\n3) Receptive ﬁeld shapes of neural networks are not in-\nvariant: A second problem which in most cases prevents\nfeature map alignment also by means of learning invariant\nfeatures is that the receptive ﬁelds of neural networks are\ntypically not invariant to the relevant transformation group.\nConsider e.g. a ﬁlter of ﬁnite support together with a scaling\ntransformation. In that case, T−1\nh (Γ Thf)c(x) will not only\ndiffer from (Γf)c(x) because it might be computed from\ndifferently oriented image patches, but also because the scaling\nimplies it will be computed from not fully overlapping image\npatches. This problem is illustrated in Figure 2. For a scaling\ntransformation, a convolutional ﬁlter, adapted to detecting a\ncertain feature at one scale, will for a larger scale never fully\ncover the relevant object. Since a non-trivial CNN feature\nextractor can not be guaranteed to take the same output for\ndifferent inputs, this implies that the set of values in the two\nfeature maps can not be guaranteed to be equal. Naturally if\ntwo feature maps do not contain the same values they can not\nbe aligned.\nIt is not hard to show that invariant receptive ﬁelds of\nﬁnite support only exist for transformations that correspond\nto reﬂections or rotations in some basis [28]. Intuitively this\ncan be understood by considering how a scaling or shear\ntransformation will always change the area covered by any\nﬁnite sized template. Thus there are no non-trivial afﬁne-,\n3\nFig. 3. Depiction of four different ways to build STNs. LOC denotes the\nlocalization network, which predicts the parameters of a transformation. ST\ndenotes the spatial transformer, which takes these parameters and transforms\nan image or feature map according to them. In STN-C0, the ST transforms the\ninput image. In STN-CX, the ST transforms a feature map, which prevents\nproper invariance. STN-DLX transforms the input image, but makes use of\ndeeper features by including copies of the ﬁrst X convolutional layers in the\nlocalization network. This is not fundamentally different from (1) but acts\nas a useful comparison point. STN-SLX is similar to STN-DLX, but shares\nparameters between the classiﬁcation and localization networks.\nscale- or shear-invariant ﬁlters with compact support 2.\n4) Conclusion: Our arguments show that a purely spatial\ntransformation cannot align the feature maps of a transformed\nimage with those of its original for general afﬁne transfor-\nmations. This implies that while an STN transforming feature\nmaps will support invariance to translations, it will not enable\ninvariant recognition for more general afﬁne transformations .\nThe exception is if the features in the speciﬁc network layer\nare themselves invariant to the relevant transformation group.\nIII. STN ARCHITECTURES\nWe test four different ways to structure STNs, all depicted\nin Figure 3. By comparing these four architectures, we can\nseparate out the effects of (i) whether it is good or bad to\ntransform feature maps, (ii) whether it is useful for localization\nnetworks to use deep features when predicting transformation\nparameters, and (iii) whether it is better for a localization\nnetwork to make use of representations from the classiﬁcation\nnetwork, or to train a large localization network from scratch.\n1) ST transforming the input: The localization network\nand the ST are placed in the beginning of the network\nand transform the input image. This approach is denoted by\nSTN-C0.\n2) ST transforming a feature map: The localization net-\nwork takes a CNN feature map from the classiﬁcation network\n2A CNN might under certain conditions learn features approximately\ninvariant over a limited transformation rate, e.g. by average pooling over a set\nof ﬁlters with effectively covariant receptive ﬁelds (e.g. learning zero weights\noutside an effective receptive ﬁeld of varying size/shape)\nas input, and the ST transforms the feature map. This archi-\ntecture does not support invariance. A network with the ST\nafter X convolutional layers is denoted by STN-CX.\n3) Deeper localization: The localization network is placed\nin the beginning, but it is made deeper by including copies of\nsome layers from the classiﬁcation network. In particular, an\nSTN where the localization network includes copies of the ﬁrst\nX convolutional layers is denoted by STN-DLX. STN-DLX is\nnot fundamentally different from STN-C0, since both architec-\ntures place the ST before any other transformations, but it acts\nas a useful comparison point to STN-CX: Both networks can\nmake use of equally deep representations, but STN-DLX does\nnot suffer any problems with achieving invariance. In addi-\ntion, the deep representations of STN-DLX are independently\ntrained from the classiﬁcation network. This is beneﬁcial if\ndifferent ﬁlters are useful to ﬁnd transformation parameters\nthan to classify the image, but requires more parameters. If the\ntraining signal becomes less clear when propagated through\nthe ST, it could also make the network harder to train. These\ndifferences motivate our fourth architecture.\n4) Shared localization: As with STN-DLX, we place a\ndeeper localization network in the beginning. However, for\neach of the copied layers, we share parameters between\nthe classiﬁcation network and the localization network. An\nSTN where the ﬁrst X layers are shared will be denoted\nby STN-SLX. STN-SLX solves the theoretical problem, uses\nno more parameters than STN-CX, and like STN-CX, the\nlocalization network makes use of layers trained directly on\nthe classiﬁcation loss.\nA. Iterative methods\nAn iterative version of STNs known as IC-STN was pro-\nposed in [26]. It starts from an architecture that has multiple\nsuccessive STs in the beginning of the network, and develops\nit in two crucial ways: (i) Instead of letting subsequent STs\ntransform an already-transformed image, all predicted transfor-\nmation parameters are remembered and composed, after which\nthe composition is used to transform the original image. Note\nthat each localization network still uses the transformed image\nwhen predicting transformation parameters: the composition is\nonly done to preserve image quality and to remove artefacts at\nthe edges of the image. (ii) All STs use localization networks\nthat share parameters with each other.\nBoth of these improvements can be generalized to work\nwith STN-SLX. The simplest extension is to use several STs\nat the same depth, where all STs share localization network\nparameters with each other in addition to sharing parameters\nwith the classiﬁcation network. Moreover, the ﬁrst improve-\nment can be used with STN-SLX even when there are multiple\nSTs at different depths , since all of them will transform the\ninput image, regardless. In this case, the ﬁnal layers of their\nlocalization networks remain separate, but whenever they pre-\ndict transformation parameters, the parameters are composed\nwith the previously predicted transformations, and used to\ntransform the input image. This is illustrated in Figure 4.\n4\nFig. 4. Depiction of how an STN transforming CNN feature maps at different depths can be transformed into an iterative STN with shared layers. STN-C0123\ntransforms feature maps by placing STs at multiple depths [1]. STN-SL0123 instead iteratively transforms the input image and, in addition, shares parameters\nbetween the localisation networks and the classiﬁcation network. The image is fed multiple times through the ﬁrst layers of the network, each time producing\nan update to the transformation parameters. Thus, the transformation is, similarly to STN-C0123, iteratively ﬁnetuned based on more and more complex\nfeatures but, at the same time, the ability to support invariant recognition is preserved.\nIV. E XPERIMENTS\nA. MNIST\n1) Datasets: MNIST is a simple dataset containing\ngrayscale, handwritten digits of size 28x28 [7]. To see how\ndifferent STN architectures compensate for different transfor-\nmations, we compare them on 3 different variations of MNIST\n(the ﬁrst two constructed as in [1]): In Rotation (R), the digits\nare rotated a random amount between ±90◦. In Translation\n(T), each digit is placed at a random location on a 60x60-pixel\nbackground; to make the task more difﬁcult, the background\ncontains clutter generated from random fragments of other\nMNIST-images. In Scale (S), the digits are scaled a random\namount between 0.5x and 4x, and placed in the middle of\na 112x112-pixel background cluttered in a similar way to\n(T). Additional details about the experiments, networks and\ndatasets are given in the Appendix.\n2) Networks: We use network architectures similar to those\nin [1]. On all three datasets, the baseline classiﬁcation network\nis a CNN that comprises two convolutional layers with max\npooling. On (R), the localization network is a FCN with 3\nlayers. Since the images in (T) and (S) are much larger,\ntheir localization networks instead comprise two convolutional\nlayers with max pooling and a single fully connected layer.\nLike in [1], the ST used with (T) produces an image with half\nthe pixel width of its input. This is done because a perfect\nlocalization network should be able to locate the digit and\nscale it 2x, so all 60x60 pixels would not be needed.\nMNIST is an easy dataset, so there is a risk that strong net-\nworks could learn the variations without using the ST. To make\nthe classiﬁcation accuracy dependent on the ST’s performance,\nwe intentionally use quite small networks. The networks of\n(R) and (T) have 50 000-70 000 learnable parameters in total,\nwhile those of (S) have around 135 000. All networks are\ntrained using SGD, with cross-entropy loss. Networks trained\non the same dataset have approximately the same number of\nparameters and use the same hyperparameters.\nThe tested architectures are STN-C0, STN-C1 (which places\nthe ST after the ﬁrst convolutional layer and max pooling),\nSTN-DL1, and STN-SL1. A baseline CNN is also tested. Note\nthat because of the transformations present in the training\ndataset, this equals a standard CNN trained with data aug-\nmentation. All networks are trained with 10 different random\nseeds, and each architecture is evaluated on 100 000 images\ngenerated by random transformations of the MNIST test set.\n3) Results: As a ﬁrst investigation of the different architec-\ntures, we study how well the STs learn to transform the digits\nto a canonical pose, when the networks are trained to predict\nthe MNIST labels.\nFigure 5 shows examples of how STN-C1 and STN-SL1\nperform on (R), (T), and (S). As predicted by theory, STN-C1\ncan successfully localize a translated digit, but STN-SL1 is\nbetter at compensating for differences in rotation and scale.\nThe difference between the networks’ abilities to compensate\nfor rotations is especially striking. Figure 6 shows that STN-\nSL1 compensates for rotations well, while STN-C1 barely\nrotates the images at all. The reason for this is that a rotation\nis not enough to align deeper layer feature maps.\nTo quantify the STs’ abilities to ﬁnd a canonical pose, we\nmeasure the standard deviation of the digits’ ﬁnal poses after\nthey have been perturbed and the ST has transformed them.\nFor the rotated, translated, and scaled dataset, the ﬁnal pose is\nmeasured in units of degrees, pixels translated, and logarithm\nof the scaling factor, respectively. 3 Table I displays this value\nfor each network.\nTABLE I\nAVERAGE STANDARD DEVIATION OF THE FINAL ANGLE (R), FINAL\nDISTANCE (T), AND FINAL SCALING (S).\nNetwork R (degrees) T (pixels) S ( log2(det))\nSTN-C0 23.2 1.16 0.319\nSTN-C1 47.2 1.15 0.508\nSTN-DL1 26.8 1.08 0.291\nSTN-SL1 18.7 1.32 0.330\nAs can be seen, STN-SL1 is the best network on (R)\n3As measure of the rotation of an afﬁne transformation matrix, we compute\narctan((a21 − a12)/(a11 + a22)) determined from a least-squares ﬁt to a\nsimilarity transformation. a13 and a23 are used to measure the translation. As\na measure of the scaling factor, we use the log2 of the determinant a11a22 −\na12a21. The standard deviation (or in the case of (T), the standard distance\ndeviation) of the ﬁnal pose is measured separately for each label, with Table I\nreporting the average across all labels and across 10 different random seeds.\n5\nand STN-DL1 is the best network on (T) and (S). Both\nthese architectures use deeper localization networks, which\ngives them the potential to predict transformations better than\nSTN-C0. As opposed to STN-C1, which also uses deeper\nrepresentations for predicting transformation parameters, STN-\nSL1, STN-DL1, and STN-C0 all transform the input. This\nallows them to perform better on (R) and (S), as predicted by\ntheory. However, STN-C1 performs adequately on (T).\nSTN-SL1 performing well on (R) can be explained by\nit sharing parameters between the localization network and\nclassiﬁcation network. The localization network processes\nimages before the ST transforms them, and the classiﬁcation\nnetwork processes them after the transformation. Thus, we\nshould expect parameter sharing to be helpful if the ﬁlters\nneeded before and after the transformation are similar. This\nis true for (R), since edge- and corner-detectors of multiple\ndifferent orientations are likely to be helpful for classifying\nMNIST-digits independently of digit orientation. In contrast,\non both (T) and (S), the scale and resolution of the digits\nvary signiﬁcantly before and after transformation. On (T), this\nis partly because the ST zooms in on the identiﬁed digit,\nand partly because the transformation produces an image with\nlower resolution, as described in Section IV-A2. 4 On (S), the\nscale is intentionally varied widely. Since the images contain\nscales and resolutions before the transformation that are not\npresent after the transformation, the localization network re-\nquires ﬁlters that the classiﬁcation network does not need.\nSTN-DL1 allows for the networks to use different ﬁlters, and\nconsequently, it does better than STN-SL1 on (T) and (S).\nDo these differences in ST-performance affect the classiﬁ-\ncation performance? Table II shows that they do. STN-SL1\nremains the best network on (R), while STN-DL1 remains\nbest on (T) and (S). The architectures that transform the input\nremain better than STN-C1 on (R) and (S). One difference\nis that STN-SL1 is better than STN-C1 on (T), which is the\nopposite from their relation in Table I. Since the differences\nin both Table I and Table II are small, this could be caused by\nSTN-SL1 being better at compensating for rotation and scale\ndifferences inherent in the original MNIST dataset.\nIt is notable that all STs do improve performance. STN-\nC1 signiﬁcantly improves on the CNN baseline even for (R),\ndespite not compensating for rotations at all, as shown in\nFigure 6. So what is STN-C1 doing? One noticeably fact\nabout its transformation matrices is that it typically scales\ndown the image, as can be seen in Figure 5. On average,\ntransformation matrices from STN-C1 have a determinant of\n0.74 (which corresponds to scaling down the image), while the\ndeterminant of all other STNs are greater than 1. We do not\nhave an explanation of why this should improve performance,\nbut it is an example of how networks that spatially transform\n4Ideally, these effects would cancel out, since a zoomed-in image with\nfewer pixels could have the same resolution as the original. In practice, the\nSTN learns to scale the image by less than 2x, which means that the digits are\nlower resolution after transformation. In addition, since the STN only learns\nto zoom in after a while, the network unavoidably processes digits at two\ndifferent resolutions in the beginning.\nFig. 5. Illustration of how STN-C1 and STN-SL1 compensate for different\nperturbations. The top row shows three digits rotated (ﬁrst image), translated\n(second image), or scaled (third image) in three different ways. The middle\nrow and bottom row show how STN-C1 and STN-SL1 transform the digits\nin the top row. STN-C1 does not compensate for rotations at all, but it\nsuccessfully localizes and zooms in on translated digits. It only compensates\nsomewhat for scaling. STN-SL1 ﬁnds a canonical pose for all perturbations.\nNote that STN-C1 does not transform the input image, so the middle row is\njust an illustration of the transformation parameters that are normally used to\ntransform its CNN feature map.\nCNN feature maps may behave in unpredictable ways.\nTABLE II\nAVERAGE CLASSIFICATION ERROR AND STANDARD DEVIATION ON THE\nMNIST TEST -SET, ACROSS 10 DIFFERENT RUNS .\nNetwork R (std) T (std) S (std)\nCNN 1.71%(0.07) 1 .61%(0.06) 1 .38%(0.04)\nSTN-C0 1.08%(0.05) 1 .10%(0.11) 0 .85%(0.06)\nSTN-C1 1.32%(0.04) 1 .16%(0.03) 0 .96%(0.04)\nSTN-DL1 1.05%(0.02) 1.08%(0.07) 0.77%(0.04)\nSTN-SL1 0.98%(0.06) 1 .13%(0.04) 0 .82%(0.06)\n4) Concluding remarks: As predicted by theory, it is signif-\nicantly better to transform the input image when compensating\nfor differences in rotation and scale, while transforming inter-\nmediate feature maps works reasonably well when compen-\nsating for differences in translation. STN-SL1 beneﬁts from\nsharing parameters on the rotation task, but in the presence of\nlarge scale variations, it is better for the localization and the\nclassiﬁcation network to be independent of each other.\nB. Street View House Numbers\n1) Dataset: In order to learn how the different STN archi-\ntectures perform on a more challenging dataset, we evaluate\nthem on the Street View House Numbers (SVHN) [8]. The\nphotographed house numbers contain 1-5 digits each, and we\npreprocess them by taking 64x64 pixel crops around each\nsequence, as done in [1]. This allows us to compare our results\nwith [1], who achieved good results on SVHN with an iterative\nST that transforms feature maps. Since the dataset beneﬁts\nfrom the use of a deeper network, it also allows us to test\nhow the architectures’ performance varies with their depth.\n2) Comparison with [1]: We use the same baseline CNN\nas [1]. The classiﬁcation network comprises 8 convolutional\nlayers followed by 3 fully connected layers. Since the images\ncan contain up to 5 digits, the output consists of 5 parallel\nsoftmax layers. Each predicts the label of a single digit.\nAs variations of the baseline CNN, [1] considers STNs\nwith two different localization networks: One with a large\n6\nFig. 6. The rotation angle predicted by the ST module (y-axis) as a function\nof the rotation angle applied to the input image (x-axis) . The black points\nconstitute a heatmap of 100 000 datapoints generated by random rotations of\nthe MNIST test set, and reports the rotations done by a single ST. The red\nline corresponds to the best ﬁt with orthogonal regression. STN-C1 cannot\ncompensate for the rotations in a useful way, since it transforms CNN feature\nmaps, while STN-SL1 directly counteracts the rotations by rotating the input.\nlocalization network in the beginning, here denoted by STN-\nC0-large, and one with a small localization network before\neach of the ﬁrst 4 convolutional layers, here denoted by STN-\nC0123. The large localization network uses two convolutional\nlayers and two fully connected layers, while each of STN-\nC0123’s localization networks uses two fully connected layers.\nAs a further variation, we consider the network STN-SL0123,\nwhich is similar to STN-C0123 but always transforms the\ninput, as described in Section III-A. STN-C0123 and STN-\nSL0123 are illustrated in Figure 4.\nThe average classiﬁcation errors of these networks are\nshown in Table III. STN-SL0123 achieves the lowest error,\nwhich shows that it is better to transform the input than to\ntransform CNN feature maps, also on the SVHN dataset.\nTABLE III\nCLASSIFICATION ERRORS ON THE SVHN DATASET, AVERAGED OVER 3\nRUNS , COMPARING OUR IMPLEMENTATION WITH [1]\nNetwork Error [1] Error (ours)\nCNN 4.0% 3.88%\nSTN-C0-large 3.7% 3.69%\nSTN-C0123 3.6% 3.61%\nSTN-SL0123 - 3.49%\n3) Comparing STs at different depths: If an ST is placed\ndeeper into the network, it can use deeper features to pre-\ndict transformation parameters, but the problem with spatial\ntransformations of CNN feature maps may get worse. STN-\nDLX and STN-SLX would not suffer from the latter, but might\nbeneﬁt from the former. We test this by placing STs at depths\n0, 3, 6, or 8 in our base classiﬁcation network, using the small\nlocalization network from the previous section.\nThe results are shown in Table IV. STN-C3, STN-DL3, and\nSTN-SL3 perform better than STN-C0, indicating that STN-\nC0’s inability to ﬁnd the correct transformation parameters\ncauses more problems than STN-C3 causes by transforming\nthe feature map at depth 3. However, at depths 6 and 8, STN-\nCX becomes worse than not using any ST at all (see Table III)\nand STN-DLX performs worse than at depth 3. This stands\nin sharp contrast to STN-SLX, where the classiﬁcation error\nkeeps decreasing, reaching 3.26% for STN-SL8.\nTABLE IV\nMEAN SVHN CLASSIFICATION ERROR OF STN-CX, STN-DLX AND\nSTN-SLX AT 4 DIFFERENT DEPTHS , ACROSS 3 RUNS .\nDepth STN-CX STN-DLX STN-SLX\nX = 0 3.81% - -\nX = 3 3.70% 3.48% 3.54%\nX = 6 3.91% 3.75% 3.29%\nX = 8 4.00%* 3.76% 3.26%\n∗One run diverged to > 99% classiﬁcation error. The error is the average of three\nruns where that did not happen.\nThis shows that localization networks beneﬁt from using\ndeep representations, if they use ﬁlters from the classiﬁca-\ntion network, while still transforming the input. It is not\nachievable by spatially transforming CNN feature maps, since\ntransforming deep feature maps causes too much distortion.\nJust using deeper localization networks does not always work,\neither, as these results show that they fail to ﬁnd appropriate\ntransformation parameters at greater depths.\nNote that the larger localization networks of STN-DLX and\nSTN-SLX take more time to train. STN-SL0123 takes 1.8\ntimes as long to train as STN-C0123, while STN-SL8 takes\n1.6 times as long as STN-C0123.\n4) Comparing iterative STNs: Given that the use of deeper\nlocalization networks helps performance, and that [26] showed\nthat iterative methods improve performance, a natural ques-\ntion is whether using iterative methods is a replacement for\nusing deeper features or if iterations and deeper features are\ncomplimentary. To answer this, we train STN-C0, STN-DL3,\nSTN-SL3, STN-SL6, and STN-SL8 with two iterations.\nThe second iteration does not change STN-C0’s perfor-\nmance (mean error 3.80%). However, STN-SL3 improves\nsigniﬁcantly (mean error 3.38%), and STN-SL6 improves\nslightly (mean error 3.26%). STN-SL8 and STN-DL3 both\nperform worse with a second iteration. If a third iteration is\nadded to STN-SL3 and STN-SL6 during testing , the errors\nfurther decrease to 3.33% and 3.18%, respectively.\nThis shows that multiple iterations do not improve perfor-\nmance when the localization network is too weak (as with\nSTN-C0). Thus, multiple iterations is not a replacement for\nusing deeper features, but can confer additional advantages.\nHere, parameter-shared networks are the only ones that beneﬁt.\nC. Plankton\n1) Dataset: Finally, we test the STN architectures on Plank-\ntonSet [9]. This dataset is challenging, because it contains 121\nclasses and only about 30 000 training samples.\n2) Networks: We use network architectures inspired by\n[29]. As base classiﬁcation network, we use a CNN consisting\nof 10 convolutional layers with maxpooling after layers 2, 4,\n7, and 10; followed by two fully connected layers before the\nﬁnal softmax output. As base localization network, two fully\nconnected layers are used. All hyperparameters were chosen\nthrough experiments on the validation set.\n7\nFig. 7. A rotated plankton transformed by various networks. The top row\ncontains the input images, the middle row contains the top images transformed\nby STN-C2, STN-DL2, and STN-SL2, and the bottom row contains the top\nimages transformed by STN-C7, STN-DL7, and STN-SL7. In the middle row,\nall networks ﬁnd a canonical angle. However, in the bottom row, only STN-\nSL7 ﬁnds a canonical angle, while STN-C7 suffer from not being able to\ntransform the input directly, and STN-DL7’s localization network is too deep.\n3) Results: Table V displays the results, evaluated on the\ntest set. The best architectures are STN-SL2 and STN-SL4.\nAs on SVHN and MNIST, networks that transform the input\nare better than STN-CX. Also similar to the results on SVHN\nis that STN-DLX is signiﬁcantly better for low values of X.\nTABLE V\nMEAN PLANKTON SET CLASSIFICATION ERROR OF STN-CX, STN-DLX,\nAND STN-SLX AT 5 DIFFERENT DEPTHS , ACROSS 4 RUNS .\nDepth STN-CX STN-DLX STN-SLX\nX = 0 22.1% - -\nX = 2 22.3% 21.6% 21.5%\nX = 4 22.5% 21.7% 21.5%\nX = 7 22.9% 22.7% 21.6%\nX = 10 22.7% 22.7% 21.9%\nIncreasing the depth of STN-SLX does not decrease per-\nformance as much as it does for STN-DLX, but neither does\nit increase performance, as it does on SVHN. This is likely\ncaused by differences between the datasets. Identifying the\ninteresting objects in SVHN is not a trivial task, because\nthe digits must be distinguished from the background. Since\nthe classiﬁcation network must be able to identify the digits,\nthe localization network can be expected to beneﬁt from\nsharing more layers. However, on PlanktonSet, the interesting\nobjects are easily identiﬁable against the black background,\nwhich makes fewer layers sufﬁcient. In addition, the easy\nseparability of planktons and background allows for large-\nscale data augmentation on PlanktonSet, which makes the STs\ntask both easier and less important for classiﬁcation accuracy.\n4) Rotational invariance: Figure 7 shows how the archi-\ntectures transform a plankton rotated in different ways. In\ncontrast to STN-C1 on MNIST (see Figure 5 and Figure 6),\nSTN-C2 has learned to transform the example image to a\ncanonical angle. Despite this, STN-C0, STN-DL2 and STN-\nSL2 perform substantially better in Table V. This shows\nthat STN-CX’s problem is not that it is difﬁcult to ﬁnd\ncorrect transformation parameters; it is the problem that even\na “correct” transformation does not yield proper invariance.\nIn this case, it is nonetheless beneﬁcial for STN-C2 to rotate\nthe plankton to a canonical angle, while transformations made\nby the deeper STN-C7 introduce enough distortions that it is\nbetter to do nothing.\n5) Comparing iterative STNs: Training STN-C0 with a\nsecond iteration improves it somewhat, yielding mean error\n21.8%, but training it with a third iteration decreases perfor-\nmance. STN-SL2, STN-SL7, and STN-DL2 are all slightly\nimproved by a second iteration (mean error 21.4%, 21.5%,\nand 21.5%), while the performance of STN-SL4 and STN-\nDL4 decreases.\nWe, again, observe that multiple iterations is not a substitute\nfor deeper features when predicting transformations, as the\ndeeper networks remain better than STN-C0.\nWe also observed that the training error decreases much\nmore than the test error, when a second iteration is added.\nFor example, STN-SL2, STN-DL4, and STN-SL7 all reduce\ntheir training error with 1.7% or more. This suggests that the\nsecond iterations make the STs substantially more effective,\nbut that this mostly leads to overﬁtting.\nV. S UMMARY AND CONCLUSIONS\nWe have presented a theoretical argument why STNs cannot\ntransform CNN feature maps in a way that enables invariant\nrecognition for other image transformations than translations.\nInvestigating the practical implications of this result, we have\nshown that this inability is clearly visible in experiments and\nnegatively impacts classiﬁcation performance. In particular,\nwhile STs transforming feature maps perform adequately when\ncompensating for differences in translation, STs transforming\nthe input perform better when compensating for differences\nin rotation and scale. Our results also have implications for\nother approaches that spatially transform CNN feature maps,\npooling regions, or ﬁlters. We do not argue that such methods\ncannot be beneﬁcial, but we do show that these approaches\nhave limited ability to achieve invariance.\nFurthermore, we have shown that STs beneﬁt from using\ndeep representations when predicting transformation param-\neters, but that localization networks become harder to train\nas they grow deeper. In order to use representations from\nthe classiﬁcation network, while still transforming the input,\nwe have investigated the beneﬁts of sharing parameters be-\ntween the localization and the classiﬁcation network. Our\nexperiments show that parameter-shared localization networks\nremain stable better as they grow deeper, which has signiﬁcant\nbeneﬁts on complex datasets. Finally, we ﬁnd that iterative\nimage alignment is not a substitute for using deeper features,\nbut that it can serve a complimentary role.\nREFERENCES\n[1] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n“Spatial transformer networks,” in Advances in Neural Information\nProcessing Systems (NIPS) , 2015, pp. 2017–2025.\n[2] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker, “Universal\ncorrespondence network,” inAdvances in Neural Information Processing\nSystems (NIPS), 2016, pp. 2414–2422.\n[3] J. Li, Y . Chen, L. Cai, I. Davidson, and S. Ji, “Dense transformer\nnetworks,” arXiv preprint arXiv:1705.08881 , 2017.\n[4] S. Kim, S. Lin, S. R. JEON, D. Min, and K. Sohn, “Recurrent\ntransformer networks for semantic correspondence,” in Advances in\nNeural Information Processing Systems (NIPS) , 2018, pp. 6126–6136.\n8\n[5] Z. Zheng, L. Zheng, and Y . Yang, “Pedestrian alignment network for\nlarge-scale person re-identiﬁcation,” IEEE Transactions on Circuits and\nSystems for Video Technology, 2018.\n[6] T. S. Cohen, M. Geiger, and M. Weiler, “A general theory of equivariant\nCNNs on homogeneous spaces,” in Advances in Neural Information\nProcessing Systems (NIPS) , 2019, pp. 9142–9153.\n[7] Y . LeCun and C. Cortes, “MNIST handwritten digit database,” 2010.\n[Online]. Available: http://yann.lecun.com/exdb/mnist/\n[8] Y . Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y . Ng,\n“Reading digits in natural images with unsupervised feature learning,” in\nNIPS Workshop on Deep Learning and Unsupervised Feature Learning\n2011, 2011.\n[9] R. K. Cowen, S. Sponaugle, K. Robinson, J. Luo, O. S. University,\nand H. M. S. Center, “PlanktonSet 1.0: Plankton imagery data\ncollected from F.G. Walton Smith in Straits of Florida from\n2014-06-03 to 2014-06-06 and used in the 2015 National Data\nScience Bowl (NCEI accession 0127422).” [Online]. Available:\nhttps://accession.nodc.noaa.gov/0127422\n[10] B. D. Lukas and T. Kanade, “An iterative image registration technique\nwith an application to stereo vision,” in Image Understanding Workshop,\n1981.\n[11] J. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani, “Hierarchical\nmodel-based motion estimation,” in European Conference on Computer\nVision (ECCV). Springer, 1992, pp. 237–252.\n[12] T. Lindeberg and J. G ˚arding, “Shape-adapted smoothing in estimation\nof 3-D depth cues from afﬁne distortions of local 2-D structure,” Image\nand Vision Computing , vol. 15, pp. 415–434, 1997.\n[13] A. Baumberg, “Reliable feature matching across widely separated\nviews,” in Proc. Computer Vision and Pattern Recognition (CVPR) ,\nHilton Head, SC, 2000, pp. I:1774–1781.\n[14] K. Mikolajczyk and C. Schmid, “Scale & afﬁne invariant interest point\ndetectors,” International Journal of Computer Vision , vol. 60, no. 1, pp.\n63–86, 2004.\n[15] K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas,\nF. Schaffalitzky, T. Kadir, and L. van Gool, “A comparison of afﬁne\nregion detectors,” International Journal of Computer Vision , vol. 65,\nno. 1–2, pp. 43–72, 2005.\n[16] L. Sifre and S. Mallat, “Rotation, scaling and deformation invariant\nscattering for texture discrimination,” in Proc. Conference on Computer\nVision and Pattern Recognition (CVPR) , 2013, pp. 1233–1240.\n[17] T. Cohen and M. Welling, “Group equivariant convolutional networks,”\nin International Conference on Machine Learning (ICML) , 2016, pp.\n2990–2999.\n[18] R. Kondor and S. Trivedi, “On the generalization of equivariance and\nconvolution in neural networks to the action of compact groups,” in\nInternational Conference on Machine Learning (ICML), 2018, pp. 2752–\n2760.\n[19] T. Lindeberg, “Provably scale-covariant continuous hierarchical net-\nworks based on scale-normalized differential expressions coupled in\ncascade,” Journal of Mathematical Imaging and Vision , vol. 62, no. 1,\npp. 120–148, 2020.\n[20] ´A. Arcos-Garc´ıa, J. A. Alvarez-Garcia, and L. M. Soria-Morillo, “Deep\nneural network for trafﬁc sign recognition systems: An analysis of spatial\ntransformers and stochastic optimisation methods,” Neural Networks ,\nvol. 99, pp. 158–165, 2018.\n[21] T. V . Souza and C. Zanchettin, “Improving deep image clustering with\nspatial transformer layers,” in International Conference on Artiﬁcial\nNeural Networks (ICANN) . Springer, 2019, pp. 641–654.\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\nconvolutional networks for visual recognition,” in European Conference\non Computer Vision (ECCV) . Springer, 2014, pp. 346–361.\n[23] F. Yu and V . Koltun, “Multi-scale context aggregation by dilated\nconvolutions,” in Int. Conf. on Learning Representations (ICLR) , 2016.\n[24] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei, “Deformable\nconvolutional networks,” inProc. International Conference on Computer\nVision (ICCV), 2017, pp. 764–773.\n[25] B.-I. C ˆırstea and L. Likforman-Sulem, “Tied spatial transformer net-\nworks for digit recognition,” in 2016 15th International Conference on\nFrontiers in Handwriting Recognition (ICFHR) . IEEE, 2016, pp. 524–\n529.\n[26] C.-H. Lin and S. Lucey, “Inverse compositional spatial transformer\nnetworks,” in Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017, pp. 2568–2576.\n[27] K. Lenc and A. Vedaldi, “Understanding image representations by\nmeasuring their equivariance and equivalence,” in Proc. Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2015, pp. 991–999.\n[28] Y . Jansson, M. Maydanskiy, L. Finnveden, and T. Lindeberg, “Inability\nof spatial transformations of CNN feature maps to support invariant\nrecognition,” arXiv preprint:2004.14716, 2020.\n[29] J. Burms, P. Buteneers, J. Degrave, S. Dieleman, I. Korshunova,\nA. van den Oord, and L. Pigou. Classifying plankton with deep\nneural networks. [Online]. Available: https://benanne.github.io/2015/03/\n17/plankton.html\nAPPENDIX\nA.1 D ETAILS OF MNIST EXPERIMENTS\nPerturbed variants\nThe rotated variant (R) is generated by random rotations of\nthe MNIST training set, uniformly chosen between -90 ◦ and\n90◦. Bilinear interpolation is used for resampling.\nThe translated variant (T) is generated by placing a digit\nfrom the MNIST training set at a random location in a 60x60\nimage. In addition, noise is added by choosing 6 random\nimages from the MNIST training set, choosing a random 6x6\nsquare in each of them, and adding those squares to random\nlocations in the 60x60 image.\nThe scaled variant (S) is generated by scaling images from\nthe MNIST training set with a random scaling factor between\n0.5 and 4, uniformly sampled on a logarithmic scale. The\nresulting image is placed at the center of a 112x112 image. In\naddition, noise is added by choosing 6 random images from\nthe MNIST training set, choosing a random 6x6 square in\neach of them, scaling the square a random amount between\n0.5 and 4 (again uniformly sampled on a logarithmic scale),\nand adding those squares to random locations in the 112x112\nimage. Bilinear interpolation is used for resampling.\nAll variants are normalized to mean 0 and standard deviation\n1 after perturbation.\nArchitectures\nWe use the same classiﬁcation network for (R), (T), and\n(S), but vary the localization network. In addition, we vary\nthe number of ﬁlters and neurons in the classiﬁcation and\nlocalization networks, to keep the number of learnable param-\neters approximately constant. The network architectures and\nthe number of learnable parameters are presented in Table VI.\nC(N) denotes a convolutional layer with N ﬁlters, and F( N)\ndenotes a fully connected layer with N neurons. Note that\nthe ﬁrst layers in STN-SL1’s localization and classiﬁcation\nnetworks shares parameters, so STN-SL1 uses somewhat fewer\nparameters than STN-DL1.\nThe ﬁnal layer in the classiﬁcation-architecture (not in-\ncluded in the table) is a 10-neuron softmax output layer, while\nthe ﬁnal layer in the localization network is a 6-neuron output\ndescribing an afﬁne transformation. All layers use ReLU as\nactivation function. The classiﬁcation network’s ﬁrst convolu-\ntional layer has ﬁlter-size 9x9, and the second has ﬁlter-size\n7x7. Convolutional layers in localization networks use ﬁlter-\nsize 5x5. Each convolutional layer is followed by a 2x2 max-\npooling with stride 2, except for the last convolutional layer in\n9\nTABLE VI\nTHE TYPES OF LAYERS AND NUMBER OF PARAMETERS IN EACH OF THE NETWORKS USED ON MNIST. C(N) DENOTES A CONVOLUTIONAL LAYER WITH\nN FILTERS , AND F(N) DENOTES A FULLY CONNECTED LAYER WITH N NEURONS .\nDataset Rotation Translation Scale\nNetwork Classiﬁcation Localization Params Classiﬁcation Localization Params Classiﬁcation Localization Params\nCNN C(32),C(32) - 54122 C(21),C(32) - 66692 C(23),C(21) - 136674\nSTN-C0 C(16),C(32) F(32),F(16)x2 53744 C(16),C(32) C(20)x2,F(10) 67138 C(16),C(16) C(16)x2,F(16) 136448\nSTN-C1 C(16),C(32) F(16)x3 53984 C(16),C(32) C(20)x2,F(20) 67088 C(16),C(16) C(16)x2,F(16) 137072\nSTN-DL1 C(16),C(32) C(16),F(16)x3 55296 C(16),C(32) C(16),C(20)x2,F(20) 66800 C(16),C(16) C(16)x3,F(16) 138384\nSTN-SL1 C(16),C(32) C(16),F(16)x3 53984 C(16),C(32) C(16),C(20)x2,F(20) 65488 C(16),C(16) C(16)x3,F(16) 137072\nthe localization network of STN-C1, STN-DL1, and STN-SL1\non (T).\nFor STN-C1, the ST is placed after the ﬁrst convolution’s\nmax-pooling layer, while STN-DL1 and STN-SL1 includes a\ncopy of the max-pooling layer in their localization networks.\nIn order to keep the number of parameters similar across all\nnetworks, the 112x112 image is downsampled by 2x before it\nis processed by STN-C0’s localization network, on (S).\nThe ST on (T) produces an image with half the pixel width\nof its input, as is done in [1]. This leads to STN-C1 having\nslightly more parameters than STN-DL1 and STN-SL1, as the\nimage becomes downsampled further into the classiﬁcation\nnetwork.\nOn all variants, the ST uses bilinear interpolation. When the\nST samples values outside the image, the value of the closest\npixel is used.\nTraining process\nThe bias of the localization network’s ﬁnal layer is initial-\nized to predict the identity transformation, while the weights\nare initialized to 0. All other learnable parameters are initial-\nized uniformly, with the default bounds in Pytorch 1.3.0.\nDuring training, we use a batch size of 256. All networks\nare trained for 50 000 iterations with the initial learning\nrate, before lowering it 10x and training for another 20 000\niterations. We use initial learning rate 0.02 on (R) and (S),\nand initial learning rate 0.01 on (T). During training, we\ncontinuously generate new, random transformations of the\n60 000 MNIST training images. During testing, we randomly\ngenerate 10 different transformations of each of the 10 000\nMNIST test images, and report the percentage error across all\nof them.\nA.2 Q UANTIFYING ST P ERFORMANCE\nTo measure the ST’s ability to align perturbed images to\na common pose, we need a measure of how consistent the\npose of a set of perturbed digits are after alignment . We\nmeasure this using the standard deviation of the digits’ ﬁnal\npose (i.e. ﬁnal orientation, scale or translation) for each label,\nusing some measure of the initial perturbation, θ and the\nST’s compensation, θ′. The compensation is extracted from\nthe afﬁne transformation matrix\n\n\na11 a12 a13\na21 a22 a23\n0 0 1\n\n (7)\noutput by the localization network. The standard deviation is\nmeasured for each label separately, since each class might\nhave a unique canonical pose. Since the translation data is\n2-dimensional, we use the standard distance deviation on (T).\nAll reported results are the average standard deviation, across\nall 10 class labels and across 10 networks trained with different\nrandom seeds.\nEstimating rotations from afﬁne transformation matrix\nFirst, we describe a general method of estimating rotations\nfrom afﬁne transformation matrices. Given a predicted afﬁne\ntransformation matrix (excluding translations)\nA=\n( a11 a12\na21 a22\n)\n(8)\nwe want to ﬁnd the combined rotation and scaling transfor-\nmation\nR=\n( Scos ϕ −Ssin ϕ\nSsin ϕ S cos ϕ\n)\n(9)\nthat minimises the Frobenius norm of the difference between\nthe model and the data\nmin\nS,ϕ\n∥A−R∥F . (10)\nIntroducing the variables u = Scos ϕ and v = Ssin ϕ, this\ncondition can be written\nmin\nu,v\n(a11 −u)2 + (a12 + v)2 + (a21 −v)2 + (a22 −u)2. (11)\nDifferentiating with respect to u and v and solving the\nresulting equations gives\nS =\n√\nu2 + v2\n= 1\n2\n√\na2\n11 + a2\n12 + a2\n21 + a2\n22 + 2(a11a22 −a12a21),\n(12)\ntan ϕ= v\nu = a21 −a12\na11 + a22\n. (13)\nThus, we can estimate the amount of rotation from an afﬁne\ntransformation matrix in a least-squares sense from\nϕ= arctan\n(a21 −a12\na11 + a22\n)\n+ nπ. (14)\n10\nMeasuring rotations on (R)\nOn (R), the perturbation θ is the angle with which the digit\nis initially rotated. To estimate the rotation θ′ made by the\nST, we apply the method described in the previous section.\nChoosing the sign of θ and θ′so that a positive rotation is in\nthe same direction, the ﬁnal pose (rotation) is deﬁned to be\nθ+ θ′.\nNote that, in Figure 6, the x-axis is θ and the y-axis\n−θ′. These graphs were generated from the median models\n(speciﬁcally, the 5th best) among the 10 trained models, as\nmeasured by the standard deviation of the ﬁnal pose.\nMeasuring translation on (T)\nOn (T), the perturbation θ = ( x,y) is the distance in\npixels between the middle of the 28x28 MNIST image and the\nmiddle of the 60x60 image that it is inserted in, horizontally\nand vertically. The ST’s horizontal and vertical translation is\nextracted as θ′ = ( x′,y′) = m(a13,a23), where the sign of\nm is chosen so that θ and θ′ deﬁne a positive translation\nin the same direction. In order to have θ′ measure distance\nin pixels (in the original image), we choose |m|= 29.5 for\nSTN-C0, STN-DL1, and STN-SL1, and |m|= 25 for STN-C1\n(see next paragraph for a more detailed explanation). Since θ\nand θ′ are 2-dimensional, we measure the standard distance\ndeviation of the ﬁnal pose θ+ θ′, as follows: For a dataset of\nn images of a particular label, with perturbations (θi)1≤i≤n\nand ST transformations (θ′\ni)1≤i≤n, the mean translation is\n¯θ = (¯x,¯y) = Σ n\ni=1(θi + θ′\ni). Then, the standard distance\ndeviation of the ﬁnal pose is\n√\nΣn\ni=1((xi + x′\ni −¯x)2 + (yi + y′\ni −¯y)2)\nAs mentioned, the matrix’s translational elements are mul-\ntiplied by 29.5 for networks that translate the input image,\nand 25 for STN-C1. This is because we want to measure\nthe distance in units of pixels. Pytorch’s spatial transformer\nmodule interprets a translation parameter of 1 as a translation\nof half the image-width. Since the image is 60x60 pixels wide,\nand pixels on the edge are assumed to be at the very end of the\nimage,5 the image-width is 60 −1 = 59 . Thus, a translation\nparameter of 1 corresponds to 29.5 pixels, for networks that\ntransform the input. However, STN-C1 transforms an image\nthat has been processed by a 9x9 convolutional layer and a\nmax pooling layer. Since the 9x9 convolution is done without\npadding, it shrinks the image to be 52x52, after which the\nmax pooling shrinks it to be 26x26. Thus, the image-width\nis 26 −1 = 25, and a translation parameter of 1 corresponds\nto a translation of 12.5 feature-map neurons. However, two\nadjacent neurons after max pooling are on average computed\nfrom regions that are twice as distant from each other as\ntwo adjacent pixels in the original image. Thus, a translation\nparameter of 1 applied after the max pooling corresponds to\na translation of 2 ·12.5 = 25 pixels in the input image.\n5This description applies to the behavior in Pytorch 1.3.0 and earlier.\nFrom version 1.4.0, the default behavior is changed such that edge-pixels\nare considered to be half a pixel away from the end of the image.\nThis adjustment needs to be applied whenever STN-C1’s\ntransformations are compared with transformations of the in-\nput image. In particular, in Figure 5, the translational elements\nof STN-C1’s predicted matrix was multiplied by 25\n29.5 before\ntransforming the input image.\nMeasuring scaling on (S)\nOn (S), the perturbation θ is measured as the log2 of the\nscaling factor squared. The ST’s transformation is measured\nas θ′= log2(|A|) = log2(a11a22 −a12a21). Choosing the sign\nof θ and θ′ so that a positive value scales up the image, for\nboth, the ﬁnal pose (scale) is measured as θ+ θ′.\nNote on the predicted transformation matrix\nWhen using spatial transformer networks, the transforma-\ntion predicted by the localization network is used to transform\nthe points from which the image is resampled . This corre-\nsponds to the inverse transformation of the image itself .\nFor the methods used to extract the rotation and scaling, this\nchanges the sign of θ′, which must be accounted for before\nsumming θand θ′. For the translation, it does not only change\nthe sign, but it also allows us to directly extract the translation\nas m(a13,a23). For the inverse transformation matrix, that\ntransforms the image directly,\n\n\na11 a12 a13\na21 a22 a23\n0 0 1\n\n\n−1\n=\n\n\nb11 b12 b13\nb21 b22 b23\n0 0 1\n\n (15)\nthe correct translation parameters would be\nθ′= −m\n(\nb11 b12\nb21 b22\n)−1 (\nb13\nb23\n)\n. (16)\nThis is because (b13,b23) corresponds to a translation after the\npurely linear transformation has been applied, which means\nthat it may act on a different scale and in a different direction\nthan the original perturbation θ. This is accounted for by ﬁrst\napplying the inverse transformation in (16). In addition, the\nsame factor m must be applied to convert to units of pixels,\nbut with the reverse sign.\nA.3 D ETAILS OF SVHN EXPERIMENTS\nThe SVHN data set contains 235 754 training images and\n13 068 test images, where each image contains a digit se-\nquence obtained from house numbers in natural scene images.\nWe follow [1] in how we generate the dataset, using 64x64\ncrops around each digit sequence. Each color channel is\nnormalized to mean 0 and standard deviation 1.\nArchitectures\nThe CNN, STN-C0-large, and STN-C0123 architectures\nexactly correspond to the CNN, STN-Single, and STN-Multi\narchitectures, respectively, in [1]. Using C(N) to denote a\nconvolutional layer with N ﬁlters of size 5x5, MP to denote\n2x2 max pooling layer with stride 2, and F(N) to denote a fully\nconnected layer with N neurons, the classiﬁcation network\nis C(48)-MP- C(64)-C(128)-MP- C(160)-C(192)-MP- C(192)-\nC(192)-MP- C(192)-F(3072)-F(3072)-F(3072), followed by 5\n11\nparallel F(11) softmax output layers. STN-C0-large uses a\nlocalization network with architecture C(32)-MP-C(32)-F(32)-\nF(32). STN-C0123 and all architectures in Section IV-B3\nuse localization networks with architecture F(32)-F(32). Each\nlayer in the classiﬁcation network except for the ﬁrst uses\ndropout with probability 0.5. Localization networks do not use\ndropout, except for the layers that STN-DLX and STN-SLX\ncopy from the classiﬁcation network. All layers use ReLU as\nactivation function.\nThe ST uses bilinear interpolation. When the ST samples\nvalues from outside the image, (0,0,0) is used. When inserting\nSTs at depth X, they are placed after the ﬁrst X convolutional\nlayers and after any max pooling or dropout layers that follow\nthe last of those convolutional layers (i.e., we always place\nthe ST right before the next convolutional or fully connected\nlayer).\nInitialization\nThe last layer of the localization network is initialized to\npredict the identity transformation. We do this by setting both\nweights and biases to zero, but when transforming images, we\ncalculate the afﬁne transformation matrix from the 6 output\nparameters o1,...,o 6 as\n\n\no1 + 1 o2 o3\no4 o5 + 1 o6\n0 0 1\n\n. (17)\nBy letting o1,...,o 6 = 0,..., 0 represent the identity transform,\nL2-regularization pushes the localization network towards the\nidentity transformation, which improves classiﬁcation perfor-\nmance.\nAll other learnable parameters are initialized uniformly, with\nthe default bounds in Pytorch 1.3.0.\nTraining process\nOur training process differs somewhat from [1]. We train\nall networks for 120 000 iterations with learning rate 0.03,\n120 000 iterations with learning rate 0.003, and ﬁnally 60 000\niterations with learning rate 0.0003. We use batch size 128.\nThe localization learning rate is always 0.01 times the base\nlearning rate, except for STN-DLX, which signiﬁcantly beneﬁt\nfrom a higher learning rate multiplier. STN-DL3 uses multi-\nplier 0.3, while STN-DL6 and STN-DL8 uses 0.1. In SL-STN,\nthe shared layers use the classiﬁcation network’s learning rate.\nWe use L2-regularization 0.0002 to regularize all layers.\nWhen training the networks with two iterations, we reduce\nthe localization networks’ learning rate by factors of approxi-\nmately 3 until further reductions do not increase performance.\nOn STN-C0, the localization network’s learning rate is 0.001\nof the classiﬁcation network’s; on STN-SL3 it is 0.01; on\nSTN-SL6 it is 0.003. On STN-SL8 and STN-DL3, the best\nresults (mean error 3.39% and 3.57%) are achieved when the\nlocalization network’s learning rate multipliers are 0.0003 and\n0.03, respectively, but this is worse than what the networks\nachieve with a single iteration.\nAll reported results are the mean classiﬁcation error across\n3 networks trained with different random seeds.\nA.4 D ETAILS OF PLANKTON SET EXPERIMENTS\nPlanktonSet was originally used in a competition on the\nwebsite Kaggle.6 In the competition, 30% of the test set was\nused for public leaderboards, and 70% was used for ﬁnal\nevaluation. With the same split, we use the ﬁrst 30% of the test\nset as a validation set, while all reported results are evaluated\non the ﬁnal 70%. All such results are mean classiﬁcation error\nacross 4 models trained with different random seeds.\nArchitectures\nThe classiﬁcation network architecture and the training\nprocess are inspired by those used in [29], but they are not\nidentical. For example, we do not use ensemble learning, since\nthis would not give any additional insight into the questions\nwe investigate in this study.\nUsing C(N) to denote a convolutional layer with N\nﬁlters of size 3x3, MP to denote a 3x3 max pooling\nlayer with stride 2, and F(N) to denote a fully con-\nnected layer with N neurons, the classiﬁcation network\nis C(32)-C(32)-MP-C(64)-C(64)-MP-C(128)-C(128)-C(128)-\nMP-C(256)-C(256)-C(256)-MP-F(512)-F(512) followed by a\nF(121) softmax output layer. Dropout layers with dropout\nprobability 0.5 are placed before each of the ﬁnal 3 fully\nconnected layers. The base localization network is F(64)-\nF(64). All layers use leaky ReLUs with negative slope 1\n3 as\nactivation functions, except for the base localization network,\nwhich uses non-leaky ReLUs.\nThe ST uses bilinear interpolation. When the ST samples\nvalues from outside the image, the value of the closest pixel\nis used. When inserting STs at depth X, they are placed after\nany max-pooling layer at depth X.\nTraining process\nThe networks are initialized as on SVHN (Section A.3).\nAll networks are trained for 215 000 iterations using a batch\nsize of 64. The initial learning rate of 0.003 is divided by\n10 after iteration 180 000, and divided by 10 again after\niteration 205 000. All networks use L2-regularization 0.0002,\nand Nesterov momentum 0.8.\nDuring training, each image is rescaled so that its longest\nside is 192 pixels, and placed in a 192x192 image without\nchanging its aspect ratio. We apply data augmentation with\nrotations ±180◦, translation ±20 pixels, shear ±20◦, and scale\nfactor [ 1\n1.3 ,1.3]. Each amount is sampled uniformly, except for\nscaling, which is sampled uniformly from a logarithmic scale.\nFinally, the image has a 50% chance of being horizontally\nﬂipped, before it is rescaled to 95x95 pixels. The images were\nnot normalized.\nFor all STN architectures, we searched for localization\nlearning rate multipliers between 0.01 and 1, with factors\nof approximately 3 between tested multipliers. For each ar-\nchitecture, we chose the learning rate multiplier with the\nsmallest validation error, and evaluated the models trained\nwith that multiplier on the test set (i.e., we did not retrain\n6The competition website is at https://kaggle.com/c/datasciencebowl\n12\nthe models). STN-C0, STN-C2, and STN-C4 were all trained\nwith multiplier 0.3, while STN-C7 was trained with 0.03 and\nSTN-C10 used 0.1. STN-SL2 and STN-SL4 were trained with\n0.1, while STN-SL7 and STN-SL10 were trained with 0.03.\nSTN-DL2 was trained with 0.3, while STN-DL4, STN-DL7,\nand STN-DL10 used 1, i.e. used the same learning rate in the\nlocalization network and classiﬁcation network.\nWe used a similar process when choosing multipliers for\nthe iterative STN architectures. For each architecture, we\nreduced the localization learning rate multiplier by factors\nof approximately 3 until further reductions did not decrease\nvalidation error. The models that were trained with the best\nmultiplier were then evaluated on the test-set. This was 0.3 for\nSTN-DL2, 0.01 for STN-SL2, and 0.003 for STN-SL7. STN-\nDL4 and STN-SL4 performed best when trained with 0.1 and\n0.03, respectively, but the mean test errors (21.8% and 21.6%)\nwere higher than when trained with a single iteration. When\ntraining STN-C0 with two iterations, the best multiplier was\n0.1; when training it with three iterations, the best multiplier\nwas 0.03, although the mean test error (22.1%) was higher\nthan for 2 iterations.\n13",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I86987016",
      "name": "KTH Royal Institute of Technology",
      "country": "SE"
    }
  ],
  "cited_by": 18
}