{
    "title": "Harnessing large language models to develop research-based learning assistants for formative feedback",
    "url": "https://openalex.org/W4405531030",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2127310737",
            "name": "Steffen Steinert",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2127186382",
            "name": "Karina E. Avila",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "Rheinland-Pfälzische Technische Universität Kaiserslautern-Landau"
            ]
        },
        {
            "id": "https://openalex.org/A213390772",
            "name": "Stefan Ruzika",
            "affiliations": [
                "Rheinland-Pfälzische Technische Universität Kaiserslautern-Landau"
            ]
        },
        {
            "id": "https://openalex.org/A2114114149",
            "name": "Jochen Kuhn",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A220724609",
            "name": "Stefan Küchemann",
            "affiliations": [
                "Ludwig-Maximilians-Universität München"
            ]
        },
        {
            "id": "https://openalex.org/A2127310737",
            "name": "Steffen Steinert",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2127186382",
            "name": "Karina E. Avila",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A213390772",
            "name": "Stefan Ruzika",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114114149",
            "name": "Jochen Kuhn",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A220724609",
            "name": "Stefan Küchemann",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4401941694",
        "https://openalex.org/W2039424564",
        "https://openalex.org/W2095907159",
        "https://openalex.org/W3022260523",
        "https://openalex.org/W3113108648",
        "https://openalex.org/W2032791684",
        "https://openalex.org/W2401631853",
        "https://openalex.org/W4281827230",
        "https://openalex.org/W2560140854",
        "https://openalex.org/W1429178785",
        "https://openalex.org/W4229442974",
        "https://openalex.org/W2790608840",
        "https://openalex.org/W2121704561",
        "https://openalex.org/W4367678106",
        "https://openalex.org/W6892950523",
        "https://openalex.org/W4221142858",
        "https://openalex.org/W4252250239",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W2030441548",
        "https://openalex.org/W4399390300",
        "https://openalex.org/W4404969638",
        "https://openalex.org/W4396511042",
        "https://openalex.org/W4366733179",
        "https://openalex.org/W2074386705",
        "https://openalex.org/W1975625245",
        "https://openalex.org/W4387812493",
        "https://openalex.org/W6733345909",
        "https://openalex.org/W4390414972",
        "https://openalex.org/W2142995122",
        "https://openalex.org/W2056950139",
        "https://openalex.org/W4390542627",
        "https://openalex.org/W2400552980",
        "https://openalex.org/W4400187437",
        "https://openalex.org/W4402813403",
        "https://openalex.org/W4402826794",
        "https://openalex.org/W4321499901",
        "https://openalex.org/W3002947507",
        "https://openalex.org/W2117766479",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W3012303672",
        "https://openalex.org/W4247666167"
    ],
    "abstract": "Abstract Effectively supporting students in mastering all facets of self-regulated learning is a central aim of teachers and educational researchers. Prior research could demonstrate that formative feedback is an effective way to support students during self-regulated learning. In this light, we propose the application of Large Language Models (LLMs) to guide students towards problem-solving through formative feedback. We present LEAP, a novel platform that utilizes advanced LLMs, such as GPT-4o. LEAP empowers teachers with the ability to effectively pre-prompt and assign tasks to the LLM, resulting in formative feedback that stimulates students’ cognitive and metacognitive processes, thereby enhancing self-regulated learning. We demonstrate that a systematic prompt design can provide a wide range of types of scaffolds to students. These scaffolds, which are rooted in educational research, include sense-making, elaboration, self-explanation, partial task-solution scaffolds, as well as metacognitive and motivational scaffolds. Through this approach, we emphasize the critical importance of synchronizing educational technological advances with empirical research and theoretical frameworks. This alignment could potentially ensures the positive and effective application of LLMs in the educational landscape.",
    "full_text": "Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nCOMMENTARY\nSteinert et al. Smart Learning Environments           (2024) 11:62  \nhttps://doi.org/10.1186/s40561-024-00354-1\nSmart Learning Environments\nHarnessing large language models \nto develop research-based learning assistants \nfor formative feedback\nSteffen Steinert1†, Karina E. Avila1,2*†  , Stefan Ruzika2, Jochen Kuhn1 and Stefan Küchemann1 \nAbstract \nEffectively supporting students in mastering all facets of self-regulated learning \nis a central aim of teachers and educational researchers. Prior research could dem-\nonstrate that formative feedback is an effective way to support students during self-\nregulated learning. In this light, we propose the application of Large Language \nModels (LLMs) to guide students towards problem-solving through formative feed-\nback. We present LEAP , a novel platform that utilizes advanced LLMs, such as GPT-\n4o. LEAP empowers teachers with the ability to effectively pre-prompt and assign \ntasks to the LLM, resulting in formative feedback that stimulates students’ cognitive \nand metacognitive processes, thereby enhancing self-regulated learning. We demon-\nstrate that a systematic prompt design can provide a wide range of types of scaffolds \nto students. These scaffolds, which are rooted in educational research, include sense-\nmaking, elaboration, self-explanation, partial task-solution scaffolds, as well as meta-\ncognitive and motivational scaffolds. Through this approach, we emphasize the criti-\ncal importance of synchronizing educational technological advances with empirical \nresearch and theoretical frameworks. This alignment could potentially ensures the posi-\ntive and effective application of LLMs in the educational landscape.\nKeywords: LLM, Formative feedback, Self-regulated learning, LEAP\nIntroduction\nFormative feedback, a pedagogical approach that facilitates the bridging of gaps in \nunderstanding and fosters metacognitive processes in students, has a rich historical \ncontext and theoretical framework (Butler & Winne, 1995; Hattie & Timperley, 2007; \nNarciss, 2008; Wisniewski et  al., 2020). Designed to stimulate cognitive processes and \ndeepen understanding, formative feedback and various learning scaffolds enhance self-\nregulated learning (SRL) (Hattie & Timperley, 2007; Wisniewski et al., 2020), a subject \nof intensive study in educational psychology (Ifenthaler, 2012; Lehmann & I. Hähnlein \net al., 2014; Zimmerman, 2002). In light of the efficacy of appropriately structured feed -\nback, digital learning platforms have progressively incorporated automated feedback \nsystems to simulate the formative feedback inherent in traditional pedagogical contexts \n†Steffen Steinert and Karina E. \nAvila have equally contributed to \nthis work.\n*Correspondence:   \nkavila@rhrk.uni-kl.de\n1 Faculty of Physics, Chair \nof Physics Education, Ludwig-\nMaximilians-University Munich, \n80539 Munich, Germany\n2 Department of Mathematics, \nRPTU Kaiserslautern-Landau, \n67663 Kaiserslautern, Germany\nPage 2 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \n(Deeva et  al., 2021; Steinert et  al., 2024a). In the contemporary era, the emergence of \nLLMs, distinguished by their superior linguistic performance, offers a promising trajec -\ntory for the development of intelligent tutoring systems. These advanced systems, tran -\nscending the capabilities of conventional automated feedback mechanisms, have the \npotential to stimulate SRL through the strategic implementation of formative feedback.\nDespite their potential, LLMs have demonstrated certain biases (Gallegos et al., 2309; \nJones & Steinhardt, 2022) and are prone to errors (Tlili et al., 2023), particularly in the \nrealms of physics and mathematics (Krupp et al., 2024, 2312; S. Küchemann et al., 2023). \nEnsuring the controllability, explainability, and lawful use of LLMs presents a signifi -\ncant challenge. Additionally, ethical considerations such as the potential for bias and \nthe broader impact of these models must be carefully addressed (Kasneci et  al., 2023; \nS. Küchemann, K. E. Avila, Y. Dinc, C. Hortmann, N. Revenga, V. Ruf, N. Stausberg, S. \nSteinert, F. Fischer, M. Fischer, et al., 2024). Nevertheless, the continuous evolution and \nrefinement of LLMs are gradually addressing these issues, bringing us closer to the real -\nity of AI-powered tutors in educational settings (Hemachandran et al., 2022; Jeon & Lee, \n2023).\nBeyond the acknowledged challenges that LLMs present in the educational sector, \nit is known that simply providing information via an interface like ChatGPT is insuf -\nficient for learning due to the non-linear process of knowledge acquisition (Everwijn \net  al., 1993; Wittrock, 2000). To develop effective learning tools, educational research \nmust keep pace with advancements, as effective learning necessitates the application of \nempirically validated theories from pedagogy, didactics, and educational psychology. \nAdditionally, the simultaneous development of open-source platforms by both research-\ners and corporations is vital to ensure the economic viability of these technologies, pre -\nvent a digital divide, and offer equal opportunities for learners of diverse backgrounds. \nSuch advancements could transform education by removing barriers like geographical \nconstraints, limited interaction, language barriers, and the fear of asking questions, pav -\ning the way for a more personalized learning experience.\nWhile formative feedback has been widely studied in educational research, its appli -\ncation in advanced technologies such as artificial intelligence (AI) (specifically LLMs) \nis still in its early stages. The novelty of LLMs means that their potential to support \nSRL through feedback mechanisms is not yet fully recognized or explored. This paper \naddresses this emerging gap by introducing LEAP (https:// ai4ph  ysed. physik. uni- muenc \nhen. de/ LEAP/ engli sh_ examp le/ Newto ns_ pendu lum) (Learning with AI about Phys -\nics), a platform that streamlines the creation of learning content and fosters interaction \nwith a LLM. LEAP has already been briefly introduced in various educational contexts \n(Avila et al., 2024; S. Küchemann et al., 2024; Steinert et al., 2024b); however, the aim \nof this manuscript is to explore the potential of LEAP in enhancing the learning experi -\nence and to discuss the future implications of such tools for educational practices. The \ndevelopment of LEAP addresses a significant research gap in the effective integration \nof Large Language Models (LLMs) in educational settings, particularly in the provision \nof formative feedback. Recent studies have highlighted the importance of faithful and \nproject-specific feedback (Jia et al., 2024), as well as the potential of interactive feedback \nmechanisms (Jia et al., 2024). Moreover, empirical evidence has demonstrated the effec -\ntiveness of LLM-generated feedback on student outcomes in the writing domain (Meyer \nPage 3 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \net al., 2024). In line with these findings, LEAP offers a platform that streamlines the crea-\ntion of learning content and fosters interaction with a LLM, providing a promising solu -\ntion for the provision of effective formative feedback. As new generative AI technologies \ncontinue to emerge, our work contributes to the ongoing discussion on the principle \nand technology sides of feedback modalities in different learning contexts (Stamper \net al., 2024). By exploring the potential of LEAP in enhancing the learning experience, \nthis paper aims to contribute to the theoretical and practical implications of LLM-based \nfeedback systems, ultimately informing the development of more effective and person -\nalized learning environments. LEAP also enables educators to provide individualized \nfeedback without technical expertise, making AI tools accessible to more teachers. The \nsystem allows educators to develop more effective learning interventions with targeted, \npre-trained AI tasks, improving the precision and relevance of feedback. Overall, LEAP \nhas the potential to revolutionize the way feedback is provided in education, making \nit more efficient, effective, and accessible, particularly in under-resourced institutions \nwhere personalized feedback is often needed but exceeds teaching staff capacity.\nWe provide a detailed method of how specific prompts to the LLM can yield research-\nbased formative feedback, thereby bolstering various learning mechanisms to promote \nSRL. LEAP offers educators the flexibility to tailor these prompts to stimulate desired \ncognitive processes in students. Furthermore, LEAP equips educators with the ability \nto design problems through a simplified four-step process, eliminating the necessity for \nprogramming skills. This methodology ensures a controlled learning environment, effec-\ntively minimizing bias and hallucinations from the LLM. The system proposed here is a \nstarting point for investigating the automated use of formative feedback. It can be used \nin the future to verify the proposed methods and iteratively improve their effectiveness \nand suitability for a variety of learning situations.\nTheoretical background\nSelf‑regulated learning and formative feedback\nSelf-regulated learning (SRL) is a mental activity that enables individuals to control their \nthoughts, actions, and feelings to effectively steer their learning journeys (Zimmerman, \n2002). Possessing SRL skills equips a student with the ability to monitor their behav -\nior in alignment with their goals and to self-reflect on their growing effectiveness. This \nself-monitoring and reflection are key to success, as they enhance the learners’ ability \nto adapt and improve their strategies over time (Zimmerman, 2002). To evolve into a \nself-regulated learner, an individual needs to cultivate the key processes: cognition, \nmetacognition, and motivation (Ifenthaler, 2012; Lehmann & I. Hähnlein et  al., 2014; \nZimmerman, 2002).\nCognition is the mental act of acquiring knowledge and understanding (Ifenthaler, \n2012; Zimmerman, 2002). It encompasses processes like thinking, knowing, remember -\ning, judging, and problem-solving. These are sophisticated functions of the brain that \ninclude language, imagination, perception, and planning. Metacognition, in contrast, \nis the consciousness and comprehension of one’s own cognitive processes (Mahdavi, \n2014). It is a level above cognition and involves “thinking about thinking” (Mahdavi, \n2014). Metacognition empowers individuals to control their own learning by planning \nand monitoring their cognitive processes, checking progress, and adjusting strategies \nPage 4 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \nas necessary. Motivation is the third element of SRL and is arguably its driving force \n(Lehmann & I. Hähnlein et  al., 2014). It is the eagerness or readiness to exert effort \ntowards achieving objectives. Motivation in learning can originate from curiosity, inter -\nest in the subject matter, or the pleasure of mastering a new skill (Mamlok-Naaman, \n2011). These elements work together in a dynamic system to facilitate self-regulated \nlearning. This process enables learners to take an active role in their learning, using \nstrategies to acquire, comprehend, and remember information, as well as to evaluate and \nadjust their learning approaches as needed.\nA crucial component to support SRL strategies among learners is formative feedback \n(Nicol & Macfarlane-Dick, 2006). Formative feedback, which transpires during the \nlearning process (Narciss, 2008), is instrumental in enhancing learners’ understanding \nand refining their skills by stimulating learners’ metacognitive processes such as self-\nimprovement and self-monitoring (Butler & Winne, 1995). This feedback helps learners \nidentify their strengths and weaknesses and target areas they need work on. It plays an \nimportant role in the learning process as it provides students with insights into their \nareas of proficiency and where additional effort is required (Butler & Winne, 1995).\nKluger and colleagues highlight that formative feedback needs to be specifically \nrelated to the task or the learning process to be truly effective (Kluger & DeNisi, 1996). \nThis involves providing information about the current level of performance or under -\nstanding, as well as guidance on how to close the gap between the present understanding \nand the targeted level of comprehension (Candel et al., 2020). Formative feedback may \nalso include different types of information such as cognitive (organizing elements of task \nrequirements, elaboration of content), metacognitive (e.g., monitoring and self-regula -\ntion) or motivational aspects (e.g., relevance of addressing the task) (Candel et al., 2020).\nCognitively activating activities\nCognitively activating activities are tasks that stimulate students’ cognitive processes (C. \nFörtsch et al., 2017; Hofer et al., 1175). These processes include problem-solving, critical \nthinking, and understand- ing (Groß-Mlynek et al., 2022). The goal of these activities is \nto engage students’ minds in a way that promotes active learning and encourages them \nto think deeply about the material (C. Förtsch et al., 2017).\nIn physics, Hofer et al (Hofer et al., 1175) have devised cognitively stimulating instruc -\ntions to aid in the acquisition of a robust base of conceptual knowledge. They have \ndeveloped five distinct types of instructions: generating solutions to novel problems, \ninventing with contrasting cases, comparing and contrasting, self-explanation prompts, \nand metacognitive questions. These methods have proven to be effective in enhancing \nthe learning process.\nAdditionally, Smy and colleagues (Smy et al., 2016) highlight that novices often over -\nlook rigorous sense-making activities while solving a task and instead, directly proceed \nto formulate a solution. These sense-making activities, which are integral to problem-\nsolving and learning, encompass defining the problem, structuring, representing the \nproblem, and generating potential alternatives along with the preferred solution. In this \ncontext, Berthold and colleagues also point out that organization and elaboration are \ntwo relevant cognitive strategies during learning (Berthold & M. Nückles et al., 2007). \nFundamentally, formative feedback or scaffolds can be employed to stimulate cognitively \nPage 5 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nactivating activities for students, thereby creating a dynamic and effective learning envi -\nronment. Together, they encourage students to be active participants in their learning \njourney, fostering a deeper understanding and mastery of the subject matter.\nImplementation of principles of self‑regulated learning and formative feedback using \nLLMs into learning technologies\nBy integrating LLMs into learning platforms, one can leverage their ability to provide \neffective formative feedback. This approach promotes the advanced learning principle \nof SRL, thereby enriching the overall learning experience and leading to improved learn-\ning outcomes. LLMs have the potential to provide real-time formative feedback across \nall three dimensions of SRL-cognition, metacognition, and motivation-depending on the \nlearners inputs and progress. This leads to a personalized learning experience that is tai -\nlored to learners’ specific prior knowledge, competence levels, and attitudes.\nTo foster SRL in students, strategic prompting can be employed to generate feedback \nin the form of cognitively activating instructions as suggested by Hofer and colleagues \n(Hofer et al., 1175). For instance, by posing questions about students’ understanding, we \ncan generate metacognitive scaffolds that encourage monitoring and self-reflection, as \nthese content-specific scaffolds play a pivotal role in enhancing comprehension. Addi -\ntionally, motivational prompts can be used to inspire and motivate students, thereby cul-\ntivating a conducive learning environment.\nSecond, in addition to personalizing the different levels of formative feedback, LLMs \nallow verbal interaction and stimulate learners to ask questions when they lack under -\nstanding or competencies.\nCombining both advantages, we introduce LEAP , a user-friendly interface designed \nto empower educators to create content and formulate prompts for the LLM. This ena -\nbles the provision of specific types of formative feedback and enhances SRL. In this way, \nLEAP eliminates the need for programming skills, enabling educators to guide the feed -\nback provided to students more effectively.\nFig. 1 Systematic prompt generation in three parts based on theoretical principles\nPage 6 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \nMaterials and methods\nPrompt design for LLMs based on theoretical principles\nIn this work, we systematically designed the prompts for the LLMs by splitting them up \ninto three segments (Fig.  1) based on theoretical considerations and prior work. Here, \na prompt is defined as a sequence of instructions given to the LLM to shape the out -\nput (here: the formative feedback) provided by the LLM to the students. Each prompt \nbegins with a Condition that specifies the circumstances under which the LLM should \nprovide learning support to the student. Here, we used a statement such as “If the stu -\ndents’ answer is incomplete or incorrect” . This segment ensures that learning support is \nonly provided when necessary, as research has shown that adaptive learning support can \nbe counterproductive if it merely reiterates the student’s existing knowledge, lead- ing \nto an expertise reversal effect (Kalyuga, 2009). This is because it consumes additional \nresources of the working memory leading to a decrease in learners’ performance (Kaly -\nuga, 2009).\nThe second part is the Consequence, which outlines the expected outcome in case the \ncondition is fulfilled. It comprises two elements: an adaptable one, chosen by the teacher \nor researcher, which requests a specific output (e.g., “encourage the student to . . . ”), and \nan adaptive one, selected by the LLM, that is customized to the unique aspects of the \nstudent’s answer (e.g., “. . . the concepts that they misunderstood or that were missing”). \nIn this way, the LLM is supposed to follow principles that are known from cognitive \ntheory and prior educational research. The principles specifically incorporated here per -\ntain to formative feedback and SRL (see Section II). These principles include strategies \nsuch as sense-making, elaboration, self-explanation, partial task solution, and the use of \nmetacognitive and motivational scaffolds. These strategies are usually employed as part \nof cognitively activating activities (see Section II B).\nThe third part consists of a Refinement, which includes aspects that are known to com-\npromise the learning effectiveness of the considered theoretical principles in the second \npart. Here, we included the restriction “without giving the correct answer” . This restric -\ntion is motivated by prior research indicating that providing elaborated feedback in \ncombination with the correct answer leads to a low efficiency of the feedback (Kulhavy \net al., 1985; Phye & Bender, 1989) and may lead to superficial learning (Schimmel, 1988).\nThis approach to prompt design allows a systematic manipulation and empirical inves-\ntigation of adaptive and adaptable segments of the prompt for the LLM. It ensures the \nLLM receives adequate information while eliminating superfluous elements.\nFunctions and features of LEAP\nTo utilize the LEAP application, teachers are required to create an account by providing \nan email address and creating a password. Upon successful registration, users gain the \nability to generate content. At the moment, users can use GPT-4o mini for free. GPT \n(Generative Pre-trained Transformer) is a large language model developed by OpenAI, \ndesigned to understand and generate human-like text based on input. The GPT models \nare what powers the interactive platform ChatGPT that many are familiar with. Addi -\ntionally, if users enter their personal OpenAI key, they can access GPT 4.0-Turbo and, in \nprinciple, nearly any other text-based model provided by OpenAI. Later, various LLMs \nwith different fine-tunings will be provided to the teachers to choose from.\nPage 7 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nTo create a task with formative feedback or other educational content, teachers need \nto complete the following four fields (as shown in Fig.  2, which provides an example of \nhow to add content to each field):\n1. Task Name: This refers to the title of the problem that the user intends to create.\n2. Question for Students: In this section, teachers can incorporate the question text, \npotentially accompanied by images, videos, mathematical equations or internet links.\n3. Instructions for the LLM: This section allows teachers to specify the prompt type \nto support different mechanisms of self-regulated learning, such as cognitive sense-\nmaking or metacognitive scaffolds (see Section IV for details). Also, this field should \ninclude a description of the images and videos incorporated in the question, provid -\ning the LLM with an understanding of what the students are viewing.\n4. Correct answer: In this field, teachers provide the correct answer to the task. This \ncould be a detailed explanation delineating what constitutes an acceptable response \nfrom the student.\nUpon the creation of the content, the teacher can share a link with the students, \nensuring the anonymity of the students is maintained. However, the teacher has the \nability to review and download the history of the answers and the feedback provided \nby the LLM, as well as the history of the prompts used for the specific feedback.\nWhen students click on the provided link, they are directed to a window showcasing \nthe physics task created by the teacher. Fig.  3 shows the feedback generated as a result \nof the teacher’s input from Fig.  2. The task, student question, and instructions entered \nby the teacher in Fig.  2 directly influence the personalized feedback displayed to the \nstudent, demonstrating how LEAP adapts to individual student responses. Read -\ners are invited to visit the website showcasing the example presented in this manu -\nscript at (https:// ai4ph  ysed. physik. uni- muenc hen. de/ LEAP/ engli sh_ examp le/ Newto \nFig. 2 The LEAP window displays an example of how to assign a task to students. This process includes \nseveral steps: selecting the GPT version, naming the task, formulating the question for students, setting \ninstructions for the LLM, and providing the correct answer\nPage 8 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \nns_ pendu lum). In Fig. 3, the ‘ Answer’ box allows students to submit their responses, \nwhich are then sent to the LLM to receive formative feedback. Students have the flex -\nibility to interact with LLM as many times as they wish. Furthermore, they can choose \nthe complexity level of the feedback (as explained in Section III D) for each submitted \nresponse, based on their individual preferences.\nArchitecture of LEAP\nLEAP is a web-based application that employs Flask (https:// jinja. palle  tspro jects. com/) \nfor backend development and SQLAlchemy (https:// www. sqlal chemy. org/) for database \naccess. The user interface is enhanced with the integration of frontend technologies such \nas HTML, CSS, Jinja2 (https:// jinja. palle  tspro jects. com/), and JavaScript, ensuring an \nengaging and interactive user experience.\nOur application’s database is constructed using MariaDB (https:// maria db. org/), a \nwidely recognized open-source relational database management system, ensuring reli -\nable data management. To serve the application on the web, we use Gunicorn (https:// \ngunic orn. org/), a WSGI HTTP server, ensuring efficient handling of client requests.\nAdditionally, we employ Nginx (https:// nginx. org/) as a reverse proxy, directing client \nrequests to the Gunicorn server effectively. This comprehensive architecture guarantees \nan optimized user experience by efficiently managing client requests.\nFig. 3 The LEAP window displays a physics task related to Newton’s cradle. Students can submit their \nresponses in the “Answer” box and send it to the LLM for feedback. The complexity of the feedback -“low” , \n“middle” , or “high”- can be selected by the student based on their preference\nPage 9 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nLLM implementation\nThe LEAP application integrates LLMs to facilitate seamless interaction between the \nsystem and the AI models. As mentioned before, while the language models offered \nby OpenAI are currently the only LLMs available for users, the platform is designed \nto support additional LLMs in the future. The communication between the applica -\ntion and the LLM happens in the background, and users (students or teachers) are \nnot exposed to the technical details of these exchanges. The system processes each \nprompt and interaction internally, following a structured format that instructs the \nLLM on how to generate feedback.We use the API (Application Programming Inter -\nface) provided by OpenAI to enable the interaction between the LEAP system and the \nLLM. An API is a set of protocols that allows different software applications to com -\nmunicate with each other. Through the API, LEAP sends instructions to the LLM, and \nreturns feedback based on the inputs it receives.\nThe messages sent to the LLM are organized as a list of dictionaries (i.e., specific \nprompts), with each dictionary representing a message. These messages consist of two \nkey-value pairs:\n1. Role: Identifies the sender of the message, which can be:\n2. System: Sets the assistant’s behavior and has priority over user instructions.\n3. Assistant: Signifies the AI model interacting with the user.\n4. User: Denotes the person communicating with the assistant (in this case, the stu -\ndent).\n5. Content: Holds the actual content of the message, which includes the specific instruc-\ntions or responses.\n1. A “system” message is sent to set the initial conditions for the LLM. This message \nincludes the following:\n• A scenario description: “You are a friendly physics teacher having a conversation \nwith a student. You are on first-name terms. ”\n• Instructions for the LLM: These are provided by the educator and are explained in \nSection III B.\n• Complexity: By default, the complexity of the feedback is set to medium, but stu -\ndents have the option to change it to low or high based on their needs. The spe -\ncific prompts given to the LLM for each complexity level are as follows:\n – “Low” instructs the LLM to “Write a very easy to understand feedback \nin five sentences for a secondary school student” .\n– “Middle” instructs it to “Write your feedback for a high school student” .\n– “High” instructs it to “Write your feedback very elaborately and use physical for -\nmulas in MathJax format: $$ < Formel > $$” . MathJax (https:// www. mathj ax. org/) \nis an open-source JavaScript display engine that renders LaTeX, MathML, and \nAsciiMath notation in all modern browsers.\nPage 10 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \n2. An “assistant” message presents the question asked to the student.\n3. A “user” message provides the student’s answer to the question.\n4. Another “assistant” message provides the correct answer to the question and a place-\nholder for the correction of the student’s answer.\nIn LEAP , every time a student sends an interaction through the interface, the system pro-\ncesses the input and communicates with the LLM to generate an appropriate response. \nThese behind-the-scenes interactions are structured as follows:\nTable 1 This table provides information on the type of prompts, the instructions given to the LLM, \nand the responses generated for the physics problem related to Newton’s Cradle\nThe feedback provided by the LLM is in relation to the answer statement “The metal balls collide with each other”\nTheoretical aspect Prompt for the LLM The LLM feedback\nCognitive Sense-\nmaking (Smy et al., \n2016)\nIf the student’s answer is incomplete or \nincorrect, encourage the student to organ-\nize and make sense of the concepts that \nthey misunderstood or that was missing \nwithout giving the correct answer\nYou’re right, the metal balls hit each other. \nBut that is only one part of the whole. Think \nabout what happens when the balls hit \neach other. What happens to the energy and \nmomentum? And why does only the last ball \nmove and not the others? Try to answer these \nquestions to get a better understanding of \nNewton’s cradle\nCognitive Elabora-\ntion (Berthold & M. \nNückles et al., 2007)\nIf the student’s answer is incomplete, \nencourage the student to elaborate on \naspects that were missing without giving \nthe correct answer\nYou’re right, the metal balls hit each other. \nBut that’s only part of the story. Can you \nperhaps say a little more about this? For \nexample, what happens when one ball hits \nthe others? And why does the last ball move \neven though it wasn’t hit directly by the first \nball? Think about the laws of physics that we \ndiscussed in class\nCognitive Self-\nexplanation (Hofer \net al., 1175)\nIf the answer of the student is incomplete \nor incorrect, give an example of a simpler \nsituation in which the incorrect or missing \nconcept plays a central role and ask the stu-\ndent how he would explain the situation. \nDo not give the correct answer\nYou’re right about the metal balls hitting each \nother. But what happens after that? Think of \na row of dominoes. What happens when you \nknock over the first domino?\nCognitive Partial \ntask solution\nIf the answer of the student is incomplete \nor incorrect, provide the next step towards \nthe complete solution without giving the \ncomplete answer\nThat’s a good start! You have correctly recog-\nnized that the metal balls hit each other. But \nwhat exactly happens when they do? Think \nabout the laws of physics that could play a \nrole here. What can you say about the law of \nconservation of energy and momentum?\nMetacognitive \n(Hofer et al., 1175; \nBerthold & M. \nNückles et al., 2007)\nIf the student’s answer is incomplete or \nincorrect, ask specifically if the student \nun- derstands the role of the concepts that \nthey misunderstood or that were missing \nwithout giving the correct answer\nYou’re right about the metal balls hitting each \nother. That’s part of the process, but there \nare more physical concepts that play a role \nhere. Have you understood the principles \nof conservation of momentum and energy? \nThese are crucial to understanding what is \ngoing on in Newton’s cradle. It would be \nhelpful if you could include these concepts in \nyour explanation\nMotivational (Haw-\nthorne et al., 2015)\nIf the student’s answer is incomplete or \nincorrect, motivate them by reminding \nthem that every attempt to understand \nsomething is a step in their learning \njourney. Encourage them to keep going \nand stay curious. Do not give the correct \nanswer or any hints\nThat’s a good start! You have correctly recog-\nnized that the metal balls hit each other. Now \ntry to delve deeper into the topic and explain \nwhat happens after the impact and why. \nKeep at it and stay curious!\nPage 11 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nResults\nIn this section, we examine the physics problem related to Newton’s cradle, as repre -\nsented in Fig.  3. In this particular task, students are shown a video that carries out an \nexperiment involving Newton’s cradle. They are asked, “The video demonstrates an \nexperiment involving Newton’s cradle. Can you explain the underlying physics that gov -\nern the observations made in the video?”\nTable 1 presents a series of prompt examples and the corresponding feedback gener -\nated by the GPT-4 turbo model in response to the student’s statement “The metal balls \nhit each other” . The prompts shown in this table correspond to the “Instructions for the \nLLM” part presented in Section III B. In addition to what is displayed in Table  1, we \ninstruct the LLM on the student’s visual context, for instance, “The video presents a \ndemonstration of Newton’s cradle, or Newton’s cradle, in motion” . Subsequently, we pro-\nvide the special prompt type shown in the table and finally instruct the LLM, “If the stu -\ndent’s response contains information unrelated to the concepts presented in the video or \nif the student greets you, kindly request the student to focus on the video content” . This \nfinal instruction is designed to prevent the generation of responses when the student \ninputs something unrelated to the task.\nThe feedback provided by the LLM, as shown in Table  1, demonstrate how strategic \nprompting can be formatted as cognitively activating instructions to potentially assist \nstudents in various self-regulating processes. By leveraging LLM, students can receive \ncognitively activating instructions, including metacognitive, self-explanatory, or motiva -\ntional feedback, as shown here. Moreover, these types of prompts can be combined to \ncreate a more tailored and effective feedback for the students.\nIt is well-known that the quality of responses generated by the ChatGPT interface is \nsignificantly influenced by the nature of the prompts provided. Within the LEAP inter -\nface, however, teachers are granted the flexibility to customize and test these prompts to \noptimize the AI’s responses. As expected, we found that more complex prompts tend to \nelicit superior results with advanced versions of LLMs.\nDiscussion\nWe introduced LEAP , a research prototyping platform designed to harness the great \nopportunities of LLMs for personalized learning and interaction in educational settings. \nThe primary objective of LEAP is to implement methods that support students by pro -\nviding formative feedback during problem-solving tasks, thereby promoting the concept \nof self-regulated learning. Empirical ev- idence to date suggests that formative feedback, \nwhen combined with cognitively activating activities, enhances learning gains. We pro -\npose that such methodologies can be extrapolated beyond traditional classroom settings \nand integrated into educational platforms.\nA distinctive advantage of LLMs is their proficiency in generating coherent and con -\ntextually relevant text, which is instrumental in delivering effective, personalized feed -\nback. The strength of LEAP lies in its ability to deliver real-time, personalized formative \nfeedback immediately following each student’s response, thereby guiding the student \ntowards the correct answer. Our demonstration in a controlled environment showed \nPage 12 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \nthat specific prompting enables the provision of cognitively activating scaffolds that are \ngrounded in theoretical principles.\nWhile several studies have explored the use of ChatGPT in physics education (Krupp \net al., 2024, 2312; Liang et al., 2023; S. Küchemann et al., 2023), LEAP takes this applica -\ntion further by allowing teachers to pre-prompt the AI for more targeted support. This \nempowers educators to design feedback tailored to individual student input, ensuring \nthat cognitive and metacognitive scaffolds are aligned with research-backed strategies.\nUnlike chatbot versions of ChatGPT and other LLMs used in many research studies, \nLEAP does not require students to prompt the LLMs for learning. Instead, specific pre-\nprompting is handled by the teacher. However, thanks to the intuitive structure of LEAP , \nthis process is streamlined and not burdensome for educators, thereby freeing them \nfrom several time-consuming yet crucial tasks. At the same time, it strongly reduces the \nnumber of erroneous output and hallucinations because the teacher provides the cor -\nrect solution to the task, and it does not allow the student to get side tracked as it kindly \nguides the student back to the task. It has also the advantage that learners do not need to \ncritically reflect as in a chatbot as the output is aligned with the correct solution.\nIn addition to integrating personalized formative feedback for students, the inclusion \nof such platforms in teachers’ technological toolkit provides invaluable resources to aug-\nment their instructional practices. The deployment of this technology has the potential \nto liberate teachers’ time, enabling them to concentrate on other crucial facets of teach -\ning, all while delivering personalized, targeted support.\nLEAP also serves as a valuable platform for conducting empirical studies by enabling \neducators and researchers to efficiently extract and analyze data, providing valuable \ninsights into the effectiveness of AI-driven feedback. This does not only facilitate aca -\ndemic research but also offers teachers valuable insights into how students interact with \nthe content. These insights can guide interventions or adjustments to the prompting \nstrategy, thereby enhancing the effectiveness of the learning process. This underscores \nthe adaptability and potential of our system in both educational and research contexts.\nFurthermore, with the advent of new technological advances, such as the release of \nGPT-4 Turbo, we can anticipate a future where LLMs become more accurate and pro -\nvide an enhanced user experience. These advances will likely introduce new functionali -\nties, paving the way for more sophisticated and effective AI tutoring systems.\nHere, we are free to incorporate any newly developed LLM into LEAP being able \nto have a platform that is easily to maintain up to date. As we continue to refine and \ndevelop these technologies, the potential for AI to revolutionize education becomes \nincreasingly tangible.\nAt the moment, it is up to the teacher or researcher to decide which type of formative \nfeedback LEAP should provide. However, the importance of additional research cannot \nbe overstated. Such research is essential to investigate whether a single type of formative \nfeedback would yield equivalent benefits for all learners. It is rather likely that specific \nlearner characteristics would influence the effectiveness of the feedback. Furthermore, \nempirical research is indispensable to verify the learning gains and the effectiveness of \nfeedback provided by LLMs. This underscores the critical role of continued research in \nthis domain.\nPage 13 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nFinally, the affordability, usability and ease-of-use of such systems are critical consid -\nerations. While our ap- plication currently utilizes models offered by OpenAI, LEAP \nwill soon be adapted to incorporate LLMs from other providers, particularly free LLMs, \nthereby increasing accessibility. Although LEAP is currently accessible to everyone and \nallows free use of GPT-4o mini within the platform, there are still ongoing expenses \nassociated with using other versions of OpenAI’s API. To ensure that everyone can ben -\nefit from the ongoing technological revolution with LLMs in education, it is necessary to \nextent the options of LLMs within LEAP to include more free versions. In this regard, it \ncould be beneficial to include LLMs that have been specifically adapted or “fine-tuned” \nto accommodate certain fields of study or areas of interest.\nConclusion\nIn this work, we introduced a platform named LEAP that utilizes LLMs, currently \nmodels offered by OpenAI, to provide research-based feedback know to support stu -\ndents in various aspects of SRL, including cognition, metacognition, and motivation. \nThis platform simplifies the interaction between teachers and LLMs, enabling them to \neasily create tasks that should stimulate SRL in students via fine prompting. Another \nuse is the creation of various task and feedback types to research methods of provid -\ning formative feedback to verify their effectiveness for SLR. We hold the conviction \nthat the integration of LLMs into education is a promising avenue with the potential \nto revolutionize teaching and learning practices. By making education more personal -\nized, effective, and accessible, platforms like LEAP are paving the way for a new era in \neducational technology.\nAlthough LEAP shows great potential, certain limitations remain. A key challenge \nis to ensure the accuracy and reliability of AI-generated feedback for a wide range \nof tasks, as LLMs can occasionally produce biased or erroneous results. In addition, \nthe overall effectiveness of formative feedback provided by LLMs remains to be con -\nclusively clarified. Future research should aim to fill these gaps by providing more \npractical insights and validating the impact of AI-driven feedback on student learning \noutcomes.\nAuthor contributions\nConceptualization, S.K.; methodology, S.S., K.A.; software, S.S.,K.A.; validation, S.S., K.A.; formal analysis, S.S., K.A. and S.K.; \ninvestigation, S.S., K.A.; resources, S.R., J.K.; data curation, S.S., K.A. and S.K.; writing—original draft preparation, K.A.; writ-\ning—review and editing, S.S., K.A., S.R., J.K., S.K.; visualization, K.A.; supervision, S.R., J.K., S.K.; project administration, S.K.; \nfunding acquisition, J.K. All authors have read and agreed to the published version of the manuscript.\nAvailability of data and materials\nData sharing not applicable.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing of interest.\nReceived: 3 April 2024   Accepted: 26 November 2024\n\nPage 14 of 15Steinert et al. Smart Learning Environments           (2024) 11:62 \nReferences\nAvila, K. E., Steinert, S., Ruzika, S., Kuhn, J., & Küchemann, S. (2024). Using chatgpt for teaching physics. The Physics Teacher, \n62(6), 536–537. https:// doi. org/ 10. 1119/5. 02271 32\nBerthold, K., Nückles, M., & Renkl, A. (2007). Do learning protocols support learning strategies and outcomes? the role of \ncognitive and metacognitive prompts. Learning and Instruction, 17(5), 564–577. https:// doi. org/ 10. 1016/j. learn instr \nuc. 2007. 09. 007\nButler, D. L., & Winne, P . H. (1995). Feedback and self-regulated learning: A theoretical synthesis. Review of Educational \nResearch, 65, 245.\nCandel, C., Vidal-Abarca, E., Cerdán, R., Lippmann, M., & Narciss, S. (2020). Effects of timing of formative feedback in \ncomputer-assisted learning environments. Journal of Computer Assisted Learning., 36(5), 718–728.\nDeeva, G., Bogdanova, D., Serral, E., Snoeck, M., & De Weerdt, J. (2021). A review of automated feedback systems for learn-\ners: Classification framework, challenges and op- portunities. Computers & Education, 162, 104094.\nEverwijn, S., Bomers, G., & Knubben, J. (1993). Ability-or competence-based education: Bridging the gap between knowl-\nedge acquisition and ability to apply. Higher Educa- Tion, 25, 425.\nFörtsch, C., Werner, S., Dorfner, T., von Kotzebue, L., & Neuhaus, B. J. (2017). Effects of cognitive activation in biology les-\nsons on students’ situational interest and achievement. Research in Science Education, 47(3), 559–578. https:// doi. org/ \n10. 1007/ s11165- 016- 9517-y\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., & Ahmed, N. K.(2023), Bias and fairness in large \nlanguage models: A survey, arXiv preprint arXiv:2309.00770 (2023).\nGroß-Mlynek, L., Graf, T., Harring, M., Gabriel-Busse, K., & Feldhoff, T. (2022). Cognitive activation in a close-up view: Trig-\ngers of high cognitive activity in students during group work phases. Frontiers in Education. https:// doi. org/ 10. 3389/ \nfeduc. 2022. 873340\nHattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77, 81.\nHawthorne, K. A., Bol, L., Pribesh, S., & Suh, Y. (2015). Effects of motivational prompts on motivation, effort, and perfor-\nmance on a low-stakes standardized test. Research & Practice in Assessmen, 10.\nHemachandran, K., Verma, P ., Pareek, P ., Arora, N., Rajesh, K. V., Kumar, T. A., Ahanger, A. A., & Pise, R. R. (2022). Artificial intel-\nligence: A universal virtual tool to augment tutoring in higher education. Computational Intelligence and Neurosci-\nence, 2022, 1–8. https:// doi. org/ 10. 1155/ 2022/ 14104 48\nHofer, S. I., Schumacher, R., Rubin, H., & Stern, E. (2018). Enhancing physics learning with cognitively activating instruction: \nA quasi-experimental classroom intervention study. Journal of Educational Psychology, 110(8), 1175.\nhttps:// ai4ph ysed. physik. uni- muenc hen. de/ LEAP/ engli sh_ examp le/ Newto ns_ pendu lum.\nhttps:// flask. palle tspro jects. com.\nhttps:// gunic orn. org/.\nhttps:// jinja. palle tspro jects. com/.\nhttps:// maria db. org/.\nhttps:// nginx. org/.\nhttps:// www. mathj ax. org/.\nhttps:// www. sqlal chemy. org/.\nIfenthaler, D. (2012). Determining the effectiveness of prompts for self-regulated learning in problem-solving scenarios. \nJournal of Educational Technology & Society, 15, 38.\nJeon, J., & Lee, S. (2023). Large language models in education: A focus on the complementary relationship between \nhuman teachers and ChatGPT. Education and Information Technologies., 28(12), 15873–15892.\nJia, Q., Cui, J., Du, H., Rashid, P ., Xi, R., Li, R., & Gehringer, E. (2024). LLM-generated Feedback in Real Classes and Beyond: \nPerspectives from Students and Instructors. In: Proceedings of the 17th International Conference on Educational Data \nMining (pp. 862-867).\nJones, E., & Steinhardt, J. (2022). Capturing failures of large language models via human cognitive biases. Advances in \nNeural Information Processing Systems, 35, 11785.\nKalyuga, S. (2009). The Expertise Reversal Effect: In S. Kalyuga (Ed.), Managing Cognitive Load in Adaptive Multimedia Learn-\ning: (pp. 58–80). IGI Global. https:// doi. org/ 10. 4018/ 978-1- 60566- 048-6. ch003\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., Günnemann, S., Hül-\nlermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., … \nKasneci, G. (2023). Chatgpt for good? on opportunities and challenges of large language models for education. \nLearning and Individual Differences, 103, 102274. https:// doi. org/ 10. 1016/j. lindif. 2023. 102274\nKluger, A. N., & DeNisi, A. (1996). The effects of feedback interventions on performance: a historical review a meta-analy-\nsis, and a preliminary feedback intervention theory. Psychological Bulletin, 119, 254.\nKrupp, L., Steinert, S., Kiefer-Emmanouilidis, M., Avila, K. E., Lukowicz, P ., Kuhn, J., & Karolus, J. (2024). Unreflected accept-\nance–investigating the negative consequences of chatgpt-assisted problem solving in physics education. In HHAI \n2024: Hybrid Human AI Systems for the Social Good (pp. 199-212). IOS Press.\nKrupp, L., Steinert, S., Kiefer-Emmanouilidis, M., Avila, K. E., Lukowicz, P ., Kuhn, J., & Karolus, J. (2023). Challenges and \nOpportunities of Moderating Usage of Large Language Models in Education. arXiv preprint arXiv:2312.14969..\nKüchemann, S., Steinert, S., Kuhn, J., Avila, K., & Ruzika, S. (2024). Large language models—valuable tools that require \na sensitive integration into teaching and learning physics. The Physics Teacher, 62(5), 400–402. https:// doi. org/ 10. \n1119/5. 02123 74\nKüchemann, S., Steinert, S., Revenga, N., Schweinberger, M., Dinc, Y., Avila, K. E., & Kuhn, J. (2023). Can chatgpt support \nprospective teachers in physics task development? Physical Review Physics Education Research. https:// doi. org/ 10. \n1103/ PhysR evPhy sEduc Res. 19. 020128\nKulhavy, R. W., White, M. T., Topp, B. W., Chan, A. L., & Adams, J. (1985). Feedback complexity and corrective efficiency. \nContemporary Educational Psychology, 10, 285.\nLehmann, T., Hähnlein, I., & Ifenthaler, D. (2014). Cognitive, metacognitive and motivational perspectives on preflection in \nself-regulated online learning. Computers in Human Behavior, 32, 313–323. https:// doi. org/ 10. 1016/j. chb. 2013. 07. 051\nPage 15 of 15\nSteinert et al. Smart Learning Environments           (2024) 11:62 \n \nLiang, Y., Zou, D., Xie, H., & Wang, F. L. (2023). Exploring the potential of using chatgpt in physics education. Smart Learning \nEnvironments, 10, 52.\nMahdavi, M. (2014). An overview: Metacognition in education. International Journal of Multidisciplinary and Current \nResearch, 2, 529.\nMamlok-Naaman, R. (2011). How can we motivate high school students to study science? Science Education Interna- \nTional, 22, 5.\nMeyer, J., Jansen, T., Schiller, R., Liebenow, L. W., Steinbach, M., Horbach, A., & Fleckenstein, J. (2024). Using LLMs to bring \nevidence-based feedback into the classroom: AI-generated feedback increases secondary students’ text revision, \nmotivation, and positive emotions. Computers and Education: Artificial Intelligence., 6, 100199.\nS. Narciss, Feedback strategies for interactive learning tasks, In: Handbook of research on educational communications and \ntechnology (Routledge, 2008) pp. 125–143.\nNicol, D. J., & Macfarlane-Dick, D. (2006). Formative assessment and self-regulated learning: A model and seven principles \nof good feedback practice. Studies in Higher Education, 31, 199.\nPhye, G. D., & Bender, T. (1989). Feedback complexity and practice: Response pattern analysis in retention and transfer. \nContemporary Educational Psychology, 14, 97.\nKüchemann, S., Avila, K. E., Dinc, Y., Hortmann, C., Revenga, N., Ruf, V. & Kuhn, J. (2024). Are large multimodal foundation \nmodels all we need? On opportunities and challenges of these models in education. https:// doi. org/ 10. 35542/ osf. \nio/ n7dvf\nSchimmel B J., Patterns in students’ selection of feedback in computer-based instruction (1988).\nSmy, V., Cahillane, M., & MacLean, P . (2016). Sensemaking and metacognitive prompting in ill-structured problems. The \nInternational Journal of Information and Learning Technology, 33, 186.\nStamper, J., Xiao, R., & Hou, X. (2024). Enhancing llm-based feedback: Insights from intelligent tutoring systems and the \nlearning sciences, in Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, \nIndustry and Innovation Tracks, Practi- tioners, Doctoral Consortium and Blue Sky, edited by M. Olney, I.-A. Chounta, \nZ. Liu, O. C. Santos, and I. I. Bittencourt (Springer Nature Switzerland, Cham, 2024)pp. 32–43.\nSteinert, S., Avila, K. E., Kuhn, J., & Küchemann, S. (2024b). Using gpt-4 as a guide during inquiry-based learning. The Phys-\nics Teacher, 62(7), 618–619. https:// doi. org/ 10. 1119/5. 02357 00\nSteinert, S., Krupp, L., Avila, K. E., Janssen, A. S., Ruf, V., Dzsotjan, D., De Schryver, C., Karolus, J., Ruzika, S., Joisten, K., \nLukowicz, P ., Kuhn, J., Wehn, N., & Küchemann, S. (2024a). Lessons learned from designing an open-source auto-\nmated feedback system for stem education. Education and Information Technologies. https:// doi. org/ 10. 1007/ \ns10639- 024- 13025-y\nTlili, A., Shehata, B., Adarkwah, M. A., Bozkurt, A., Hickey, D. T., Huang, R., & Agyemang, B. (2023). What if the devil is my \nguardian angel: Chatgpt as a case study of using chatbots in education. Smart Learning Environments, 10, 15.\nWisniewski, B., Zierer, K., & Hattie, J. (2020). The power of feedback revisited: A meta-analysis of educational feedback \nresearch. Frontiers in Psychology, 10, 3087.\nWittrock, M. C. (2000). Knowledge acquisition and education. The Journal of Mind and Behavior, 205-212.\nZimmerman, B. J. (2002). Becoming a self-regulated learner: An overview. Theory into Practice, 41, 64.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}