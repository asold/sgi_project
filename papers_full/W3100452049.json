{
    "title": "Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art",
    "url": "https://openalex.org/W3100452049",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5063377058",
            "name": "Patrick Lewis",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A5076248976",
            "name": "Myle Ott",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5006191011",
            "name": "Jingfei Du",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A5091317839",
            "name": "Veselin Stoyanov",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2949176808",
        "https://openalex.org/W3020786614",
        "https://openalex.org/W2801930304",
        "https://openalex.org/W2136437513",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970557265",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2071879021",
        "https://openalex.org/W2955483668",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2149369282",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W4289366653",
        "https://openalex.org/W1034374084",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W2005430761",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2884668708",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2137407193",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W2963923670",
        "https://openalex.org/W2515248967",
        "https://openalex.org/W3105491236",
        "https://openalex.org/W2890830728",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035068109",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2888120268",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W2100627415",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2174775663",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W3037063616",
        "https://openalex.org/W2052217781",
        "https://openalex.org/W2170189740",
        "https://openalex.org/W2944400536",
        "https://openalex.org/W2190333735"
    ],
    "abstract": "A large array of pretrained models are available to the biomedical NLP (BioNLP) community. Finding the best model for a particular task can be difficult and time-consuming. For many applications in the biomedical and clinical domains, it is crucial that models can be built quickly and are highly accurate. We present a large-scale study across 18 established biomedical and clinical NLP tasks to determine which of several popular open-source biomedical and clinical NLP models work well in different settings. Furthermore, we apply recent advances in pretraining to train new biomedical language models, and carefully investigate the effect of various design choices on downstream performance. Our best models perform well in all of our benchmarks, and set new State-of-the-Art in 9 tasks. We release these models in the hope that they can help the community to speed up and increase the accuracy of BioNLP and text mining applications.",
    "full_text": "Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 146–157\nNovember 19, 2020.c⃝2020 Association for Computational Linguistics\n146\nPretrained Language Models for Biomedical and Clinical Tasks:\nUnderstanding and Extending the State-of-the-Art\nPatrick Lewis†‡ , Myle Ott†, Jingfei Du†, Veslin Stoyanov†\n†Facebook AI Research;‡University College London\n{plewis,myleott,jingfeidu,ves}@fb.com\nAbstract\nA large array of pretrained models are avail-\nable to the biomedical NLP (BioNLP) com-\nmunity. Finding the best model for a partic-\nular task can be difﬁcult and time-consuming.\nFor many applications in the biomedical and\nclinical domains, it is crucial that models can\nbe built quickly and are highly accurate. We\npresent a large-scale study across 18 estab-\nlished biomedical and clinical NLP tasks to de-\ntermine which of several popular open-source\nbiomedical and clinical NLP models work\nwell in different settings. Furthermore, we ap-\nply recent advances in pretraining to train new\nbiomedical language models, and carefully in-\nvestigate the effect of various design choices\non downstream performance. Our best mod-\nels perform well in all of our benchmarks, and\nset new State-of-the-Art in 9 tasks. We release\nthese models in the hope that they can help the\ncommunity to speed up and increase the accu-\nracy of BioNLP and text mining applications.\n1 Introduction\nThe pretrain-and-ﬁnetune approach has become the\ndominant paradigm for NLP applications in the last\nfew years (Peters et al., 2018; Devlin et al., 2019;\nYang et al., 2019; Conneau et al., 2020, inter alia.),\nbringing signiﬁcant performance gains in many\nareas of NLP. Models trained on Wikipedia and\nWebText (Radford et al., 2019) generally perform\nwell on a variety of target domains, but various\nworks have noted that pretraining on in-domain\ntext is an effective method for boosting downstream\nperformance further (Peters et al., 2018; Beltagy\net al., 2019; Li et al., 2019; Gururangan et al.,\n2020). Several pretrained models are available\nspeciﬁcally in the domain of biomedical and clini-\ncal NLP driving forward the state of the art includ-\ning BioBERT (Lee et al., 2019), SciBERT (Beltagy\net al., 2019), ClinicalBERT (Alsentzer et al., 2019)\nand BioMedRoBERTa (Gururangan et al., 2020).\nWhile it is great to have multiple options, it can\nbe difﬁcult to make sense of what model to use\nin what case — different models are often com-\npared on different tasks. To further complicate mat-\nters, more powerful general-purpose models are\nbeing released continuously. It is unclear whether\nit is better to use a more powerful general-purpose\nmodel like RoBERTa, or a domain-speciﬁc model\nderived from an earlier model such as BioBERT.\nAnd given the opportunity to pretrain a new model,\nit is unclear what are the best practices to do that\nefﬁciently.\nOur goal is to understand better the landscape of\npretrained biomedical and clinical NLP models. To\nthat effect, we perform a large-scale study across\n18 established biomedical and clinical NLP tasks.\nWe evaluate four popular bioNLP models using\nthe same experimental setup. We compare them to\ngeneral purpose RoBERTa checkpoints. We ﬁnd\nthat BioBERT performs best overall on biomedi-\ncal tasks, but the general-purpose RoBERTA-large\nmodel performs best on clinical tasks. We then\ntake advantage of recent advances in pretraining by\nadapting RoBERTa (Liu et al., 2019) to biomedi-\ncal and clinical text. We investigate what choices\nare important in pretraining for strong downstream\nbioNLP performance, including model size, vo-\ncabulary/tokenization choices and training corpora.\nOur best models perform well across all of the\ntasks, establishing a new state of the art on 9 tasks.\nFinally, we apply knowledge distillation to train a\nsmaller model that outperforms all other models\nwith similar computational requirements. We will\nrelease our pretrained models and the code used to\nrun our experiments.1\n1Models and code are available athttps://github.\ncom/facebookresearch/bio-lm\n147\n2 Tasks and Datasets\nWe select a broad range of datasets to cover\nboth scientiﬁc and clinical textual domains, and\ncommon modelling tasks – namely i) Sequence\nlabelling tasks, covering Named Entity Recog-\nnition (NER) and de-identiﬁcation (De-id) and\nii) Classiﬁcation tasks, covering relation extrac-\ntion, multi-class and multi-label classiﬁcation and\nNatural Language Inference (NLI)-style tasks.\nThese tasks were also selected to optimize over-\nlap with previous work in the space, drawing tasks\nfrom the BLUE benchmark (Peng et al., 2019),\nBioBERT (Lee et al., 2019), SciBERT (Beltagy\net al., 2019) and ClinicalBERT (Alsentzer et al.,\n2019). The tasks are summarized in Table1 and\ndescribed in the following subsections.\n2.1 Sequence Labelling Tasks\nBC5-CDR (Li et al., 2016) is an NER task re-\nquiring the identiﬁcation of Chemical and Disease\nconcepts from 1,500 PubMed articles. There are\n5,203 and 4,182 training instances for chemicals\nand diseases respectively.\nJNLPBA (Collier and Kim, 2004) is an NER\ntask requiring the identiﬁcation of entities of inter-\nest in micro-biology, with 2,000 training PubMed\nabstracts.\nNCBI-Disease (Do˘gan et al., 2014) requires\nidentiﬁcation of disease mentions in PubMed ab-\nstracts. There are 6,892 annotations from 793 ab-\nstracts.\nBC4CHEMD (Krallinger et al., 2015) requires\nthe identiﬁcation of chemical and drug mentions\nfrom PubMed abstracts. There are 84,310 annota-\ntions from 10,000 abstracts.\nBC2GM (Smith et al., 2008) requires the identi-\nﬁcation of 24,583 protein and gene mentions from\n20,000 sentences from PubMed.\nLINNAEUS (Gerner et al., 2010) is a collection\nof 4,077 species annotations from 153 PubMed\narticles.\nSpecies-800 (Paﬁlis et al., 2013) is a collection\n3,708 species annotations in 800 PubMed abstracts.\nI2B2-2010/V A (Uzuner et al., 2011) is made up\nof 871 de-identiﬁed clinical reports. The task re-\nquires labelling a variety of medical concepts in\nclinical text.\nI2B2-2012 (Sun et al., 2013b,a) is made up\nof 310 de-identiﬁed clinical discharge summaries.\nThe task requires the identiﬁcation of temporal\nevents within these summaries.\nI2B2-2014 (Stubbs and Uzuner, 2015; Stubbs\net al., 2015) is made up of 1,304 de-identiﬁed lon-\ngitudinal medical records. The task requires the\nlabelling of spans of text of private health informa-\ntion.\n2.2 Classiﬁcation Tasks\nHOC (Baker et al., 2016) is a multi-label classi-\nﬁcation task requiring the classiﬁcation of cancer\nconcepts for PubMed Articles. We follow (Peng\net al., 2019) and report abstract-level F1 score.\nMedNLI (Romanov and Shivade, 2018) is a 3-\nclass NLI dataset built from 14K pairs of sentences\nin the clinical domain.\nChemProt (Krallinger et al., 2017) requires clas-\nsifying chemical-protein interactions from 1,820\nPubMed articles. We follow the standard practice\nof evaluating over the 5 most common classes.\nGAD (Bravo et al., 2015) is a binary relation ex-\ntraction task for 5330 annotated gene-disease inter-\nactions from PubMed. We use the cross-validation\nsplits fromLee et al.(2019).\nEU-ADR (van Mulligen et al., 2012) is a small\ndata binary relation extraction task with 355 an-\nnotated gene-disease interactions from PubMed.\nWe use the cross-validation splits fromLee et al.\n(2019).\nDDI-2013 (Herrero-Zazo et al., 2013) is a rela-\ntion extraction task requiring recognition of drug-\ndrug interactions. There are 4 classes to extract\nfrom 4920 sentences from PubMed, as well as\nmany sentences which do not contain relations.\nI2B2-2010-RE (Uzuner et al., 2011) in this set-\nting of I2B2-2010, we focus on the relation extrac-\ntion task to detect 8 clinical events.\n3 Pretraining Corpora\nThere is a wide range of text corpora in the biomed-\nical and clinical domains. We limit our options to\ndata that is freely available to the public so that\nmodels can be open-sourced.\n148\nTask Name Domain Task Metric Task Name Domain Task Metric\nBC5-CDR-Chemical PubMed N.E.R. F1 I2B2-2012 Clinical N.E.R. F1\nBC5-CDR-Disease PubMed N.E.R. F1 I2B2-2014 Clinical De-ID F1\nJNLPBA PubMed N.E.R. F1 HOC PubMed Multi-label classif. Macro-F1\nNCBI-D PubMed N.E.R. F1 ChemProt PubMed Rel. extract. Macro-F1\nBC4CHEMD PubMed N.E.R. F1 GAD PubMed Binary Rel. Extract. F1\nBC2GM PubMed N.E.R. F1 EU-ADR PubMed Binary Rel. Extract. F1\nLINNEAEUS PubMed N.E.R. F1 DDI-2013 PubMed Rel. Extract. Micro-F1\nSpecies-800 PubMed N.E.R. F1 I2B2-2010-RE Clinical Rel. extract. F1\nI2B2-2010 Clinical N.E.R. F1 MedNLI Clinical NLI Acc\nTable 1: Summary of our considered tasks\nPubMed abstracts PubMed2 is a free resource\ncontaining over 30 million citations and abstracts\nof biomedical literature. PubMed abstracts are a\npopular choice for pretraining biomedical language\nmodels (Lee et al., 2019; Peng et al., 2020) because\nof the collection’s large size and broad coverage.\nFollowing past work, we obtained all PubMed ab-\nstracts published as of March 2020. After removing\nempty abstracts we retained 27GB of text from 22\nmillion abstracts, consisting of approximately 4.2\nbillion words.\nPubMed Central full-text PubMed Central3\n(PMC) is an open access collection of over 5 mil-\nlion full-text articles from biomedical and life sci-\nence research, which has been used in past scien-\ntiﬁc language modeling work (Beltagy et al., 2019).\nFollowing past work, we obtained all PubMed Cen-\ntral full-text articles published as of March 2020.\nWe use thepubmed parser package4 to extract\nplain text from each article. After removing empty\nparagraphs and articles with parsing failures we\nretained 60GB of text from 3.4 million articles,\nconsisting of approximately 9.6 billion words.\nMIMIC-III The Medical Information Mart for\nIntensive Care, third update (MIMIC-III) consists\nof deidentiﬁed clinical data from approximately\n60k intensive care unit admissions. Following re-\nlated work (Zhu et al., 2018; Peng et al., 2019), we\nextract all physician notes resulting in 3.3GB of\ntext and approximately 0.5 billion words.\nOther corpora Other authors have used subsets\nof papers on Semantic Scholar (Gururangan et al.,\n2020; Ammar et al., 2018), but these corpora are\nnot generally publicly available. The CORD-19\ndataset (Wang et al., 2020) is a publicly-available\n2https://pubmed.ncbi.nlm.nih.gov\n3https://www.ncbi.nlm.nih.gov/pmc\n4https://github.com/titipata/pubmed_\nparser\ncorpus of articles focusing on COVID-19, but is\nlargely subsumed by PMC, so we do not directly\ninclude it in our work.\n4 Pretrained Models\nWe compare ﬁve publicly-available language mod-\nels which together form a representative picture of\nthe state-of-the-art in biomedical and clinical NLP.\nWe use the HuggingFace Transformers library to\naccess the model checkpoints (Wolf et al., 2019).\nSciBERT (Beltagy et al., 2019) is a masked\nlanguage model (MLM) pretrained from scratch\non a corpus of 1.14M papers from Semantic\nScholar (Ammar et al., 2018), of which 82% are in\nthe biomedical domain. SciBERT uses a special-\nized vocabulary built using Sentence-Piece (Sen-\nnrich et al., 2016; Kudo, 2018)5 on their pretraining\ncorpus. We use the uncased SciBERT variant.\nBioBERT (Lee et al., 2019) is based on the\nBERT-base model (Devlin et al., 2019), with addi-\ntional pretraining in the biomedical domain. We\nuse BioBERT-v1.1. This model was was trained for\n200K steps on PubMed and PMC for 270K steps,\nfollowed by an additional 1M steps of training on\nPubMed, using the same hyperparameter settings\nas BERT-base.\nClinicalBERT (Alsentzer et al., 2019) is also\nbased on BERT-base, but with a focus on clinical\ntasks. We use the “Bio+Clinical BERT” check-\npoint, which is initialized from BioBERT, and then\ntrained using texts from MIMIC-III for 150K steps\nusing a batch size of 32.\nRoBERTa (Liu et al., 2019) is a state-of-the-\nart general purpose model. We experiment with\nRoBERTa-base and RoBERTa-large to understand\n5https://github.com/google/\nsentencepiece\n149\nhow general domain models perform on biomedi-\ncal tasks. Both models are pretrained with much\nlarger batch sizes than BERT, and use dynamic\nmasking strategies to prevent the model from over-\nmemorization of the training corpus. RoBERTa\noutperforms BERT on general-domain tasks (Liu\net al., 2019).\nBioMed-RoBERTa (Gururangan et al., 2020) is\na recent model based on RoBERTa-base. BioMed-\nRoBERTa is initialized from RoBERTa-base, with\nan additional pretraining of 12.5K steps with a\nbatch size of 2048, using a corpus of 2.7M scien-\ntiﬁc papers from Semantic Scholar (Ammar et al.,\n2018).\n4.1 Pretraining New Models\nIn addition to these publicly available models, we\nalso pretrain new models on the corpora in Sec-\ntion 3 and examine which design criteria are im-\nportant for strong downstream performance on Bio-\nNLP tasks. We have three criteria we are interested\nin studying: i) The effect of model size on down-\nstream performance; ii) the effect of pretraining cor-\npus on downstream performance; and, iii) whether\ntokenizing with a domain-speciﬁc vocabulary has\na strong effect on downstream performance.\nWe pretrain a variety of models based on the\nRoBERTa-base and RoBERTa-large architectures,\nwith detailed ablations discussed in section6.1.\nWe use the PubMed data, and optionally include\nMIMIC-III. We initialize our models with the\nRoBERTa checkpoints, except when we use a\ndomain-speciﬁc vocabulary, then we retrain the\nmodel from a random initialization. Our domain-\nspeciﬁc vocabulary is a byte-level byte-pair en-\ncoding (BPE) dictionary learned over our PubMed\npretraining corpus (Radford et al., 2019; Sennrich\net al., 2016). Both the general-purpose (RoBERTa)\nand domain-speciﬁc vocabularies contain 50k sub-\nword units. Our best performing models use\nPubMed abstracts, PMC and MIMIC-III pretrain-\ning and a domain-speciﬁc vocabulary, and are re-\nferred to as “ours-base” and “ours-large” in the\nfollowing sections.\n5 Experimental Setup\n5.1 Pretraining\nWe largely follow the pretraining methodology\nof Liu et al.(2019). We pretrain models using\nFAIRSEQ (Ott et al., 2019) on input sequences of\n512 tokens, of which 15% are masked and later\npredicted.6 We pretrain with batches of 8,192 se-\nquences and use the AdamW optimizer (Loshchilov\nand Hutter, 2019) with\u00001 =0 .9,\u0000 2 =0 .98,✏ =\n1e \u0000 6. We regularize the model with dropout\n(p =0 .1) and weight decay (\u0000 =0 .01). We pre-\ntrain all models for 500k steps using mixed pre-\ncision on V100 GPUs. We linearly warmup the\nlearning for the ﬁrst 5% of steps and linearly decay\nthe learning rate to 0 over the remaining steps. We\nuse a learning rate of 6e-4 for base models and 4e-4\nfor large models.\n5.2 Fine-tuning\nWe ﬁne-tune models using 5 different seeds and\nreport the median result on the test sets.\nFor sequence labelling tasks, we use learning\nrate of 1e-5 and a batch size of 32. For all sequence\nlabelling tasks, we train for 20 epochs in total and\nchoose the best checkpoint based on validation set\nperformance (evaluating every 500 optimization\nsteps). We ﬁne-tuned the models with 5 seeds and\nreport the median test results across these seeds.\nFor classiﬁcation tasks, we use a learning rate of\n0.002 and a batch size of 16. For HOC, ChemProt,\nMedNLI and I2B2-2010-RE, we run for a maxi-\nmum of 10 epochs, and perform early stopping,\nevaluating performance on validation data every\n200 optimization steps. As GAD and EU-ADR are\nsplit into 10 train/test cross-validation partitions,\nwe choose early-stopping hyperparameters using\none fold, and report the median test results on the\nother 9 folds.\n6 Results\nTable 2 shows our main results. The ﬁrst columns\nshow results for the general-purpose RoBERTa-\nbase checkpoint, the next four show results for\nthe specialized models mentioned in Section4.\nThe Roberta-large column shows results for the\ngeneral-purpose RoBERTa-large checkpoint. The\n“ours-base” and “ours-large” columns refers to\nour proposed RoBERTa-base and RoBERTa-large\nsized models respectively, which were trained us-\ning PubMed and MIMIC-III data and a domain-\nspeciﬁc vocabulary. We observe the following: i)\nRoBERTa-large outperforms RoBERTa-base con-\nsistently, despite having access to the same training\n6Following Devlin et al.(2019) andLiu et al.(2019), with\n10% probability we randomly unmask a masked token or\nreplace it with a random token.\n150\nTask Name RoBERTa-\nbase SciBERT BioBERT Clinical-\nBERT\nBioMed-\nRoBERTa\nOurs-\nbase\nRoBERTa-\nlarge\nOurs-\nlarge\nBC5CDR-C. 87.3 91.9 91.9 90.6 90.3 92.9 90.8 93.7\nBC5CDR-D. 77.6 83.6 83.3 81.3 80.6 83.8 82.3 85.2\nJNLPBA 79.5 80.3 80.4 79.3 80.2 80.6 80.1 81.0\nNCBI-disease 84.7 86.9 87.6 86.1 86.1 87.7 87.1 89.0\nBC4CHEMD 88.6 91.8 92.2 90.3 89.7 92.7 90.6 93.7\nBC2GM 82.7 85.7 85.6 83.9 84.2 87.0 85.3 88.0\nLINNEAEUS 79.8 84.1 86.2 84.8 84.2 85.3 87.8 88.4\nSpecies-800 75.8 77.8 79.2 77.4 77.3 79.6 78.3 81.1\nI2B2-2010 83.5 86.3 86.0 86.3 85.0 88.1 87.3 89.7\nI2B2-2012 74.9 77.6 77.6 78.0 76.4 79.5 78.3 80.8\nI2B2-2014 95.6 95.2 94.7 94.6 95.2 95.5 95.8 96.3\nHOC 86.0 84.7 86.6 86.2 86.7 86.5 85.2 86.6\nChemProt 69.6 69.7 73.9 68.5 75.7 75.4 71.7 76.2\nGAD 79.4 78.7 81.2 79.2 81.6 82.2 73.4 81.1\nEU-ADR 85.0 85.5 85.0 85.1 85.0 85.0 85.0 85.0\nDDI-2013 79.0 79.1 79.9 77.3 80.7 81.0 80.5 82.1\nI2B2-2010-RE 72.4 69.8 74.4 74.0 75.0 75.0 75.2 78.6\nMedNLI 81.4 79.7 82.5 81.8 85.1 87.1 83.3 88.5\nMean (Seq. Lab.) 82.7 85.6 85.9 84.8 84.5 86.6 85.8 87.9\nMean (Classif.) 79.0 78.2 80.5 78.9 81.4 81.7 79.2 82.6\nMean (PubMed) 81.1 83.1 84.1 82.3 83.3 84.6 82.9 85.5\nMean (Clinical) 81.6 81.7 83.0 82.9 83.3 85.1 84.0 86.8\nMean (all) 81.3 82.7 83.8 82.5 83.3 84.7 83.2 85.8\nTable 2: Test results on all tasks for our RoBERTa baselines, publicly available models and our best Large and\nBase-sized models. All results are the median of 5 runs with different seeds\ncorpora; ii) We ﬁnd that BioBERT performs best\nfrom the publicly available models that we exper-\niment with; and iii) our newly introduced models\nperform well, achieving the best results for 17 out\nof the 18 tasks in our experiments, often by a large\nmargin. The exception is EU-ADR, which has a\nsmall test set where all models achieve essentially\nthe same classiﬁcation accuracy.\nDigging deeper, we note that standard RoBERTa-\nlarge is competitive with the four specialized mod-\nels on sequence labelling tasks (85.8 vs 85.9) and\noutperforms them on clinical tasks (84.0 vs 83.3),\ndespite having no specialized biomedical or clin-\nical pretraining. This suggests that larger, more\npowerful general-purpose models could be a good\ndefault choice compared to smaller, less powerful\ndomain-speciﬁc models.\nNevertheless, applying domain-speciﬁc training\nto otherwise-comparable models results in signif-\nicant performance gains in our experiments, as\nshown by comparing ours-base and ours-large to\nRoBERTa-base and RoBERTa-large in Table2,\n(+3.5% and +2.6% mean improvement), consis-\ntent with ﬁndings from previous work (Gururangan\net al., 2020).\n6.1 Ablations\nThe “ours-base” and “ours-large” models shown in\nTable 2 refer to the best language models that we\ntrained in our experiments described in Section4.1.\nThese models use the RoBERTa architectures, are\ninitialized with random weights, use a BPE vocab-\nulary learnt from PubMed, and are pretrained on\nboth our PubMed and MIMIC-III corpora. We per-\nformed a detailed ablation study to arrive at these\nmodels, and in what follows, we analyse the design\ndecisions in detail. A summary of these results\nare shown in Table3, a description of task group-\nings in Table4, and full results can be found in\nAppendix A.2.\n6.1.1 Effect of vocabulary\nThe effect of learning a dedicated biomedical vo-\ncabulary for base and large models can be analysed\nby comparing row 2 to row 3, row 4 to 5, and\nrow 7 to 8 in Table3. A dedicated vocabulary\nconsistently improves sequence labelling tasks, im-\nproving results for base models by 0.7% and our\nlarge model by 0.6% on average. The difference\nis less consistent for classiﬁcation tasks, improv-\ning the large model by 0.5%, but reducing perfor-\nmance on the small model by 0.7%. A specialized\ndomain-speciﬁc vocabulary was also shown to be\n151\nModel\nMean Clin-\nical\nPub-\nMed\nSeq.\nLab. Classif. All\n(1) RoBERTa-base 81.6 81.1 82.7 79.0 81.3\n(2) +PM 83.5 84.1 85.7 81.1 83.9\n(3) +PM+V oc. 83.4 84.4 86.5 80.4 84.1\n(4) +PM+M3 85.0 84.0 85.9 81.6 84.2\n(5) +PM+M3+V oc. 85.1 84.6 86.6 81.8 84.7\n(6) RoBERTa-large 84.0 82.9 85.8 79.2 83.2\n(7) +PM+M3 85.7 85.1 87.3 82.1 85.3\n(8) +PM+M3+V oc. 86.8 85.5 87.9 82.6 85.8\nTable 3: Ablation test set results. Rows 5 and 8 cor-\nrespond to “ours-base”’ and “ours-large” in Table2\nrespectively. Bold indicates the best model overall,\nUnderlined indicates the best base model. “PM” in-\ndicates training with PubMed and PMC corpora and\n“M3” refers to the MIMIC-III corpus. “V oc” indicates\nusing a dedicated biomedical vocabulary. Details of the\ntasks incuded in each column are given in Table4\nTask group Tasks in group\nClinical I2B2-2010, I2B2-2012, I2B2-2014, I2B2-2010-\nRE, MedNLI\nPubMed\nBC5CDR-C, BC5CDR-D, JNLPBA, NCBI-D,\nBC4CHEMD, BC2GM, Linneaus, Species-800,\nHOC, ChemProt, GAD, EU-ADR, DDI-2013\nSeq. Lab.\nBC5CDR-C, BC5CDR-D, JNLPBA, NCBI-D,\nBC4CHEMD, BC2GM, Linneaus, Species-800,\nI2B2-2010, I2B2-2012, I2B2-2014\nClassif. HOC, ChemProt, GAD, EU-ADR, DDI-2013,\nI2B2-2010-RE, MedNLI\nTable 4: High-level task groupings. “Clinical” indi-\ncates clinical tasks, “PubMed” indicates tasks based on\nPubMed, “Seq. Lab.” refers to sequence labelling, i.e.\nN.E.R. and De-ID. “Classif.” refers to classiﬁcation, i.e.\nrelation extraction, multi-label classiﬁcation and NLI.\nuseful inBeltagy et al.(2019). Since our special-\nized vocabulary models are trained from scratch\nonly on biomedical data, we see that Wikipedia and\nWebText (Radford et al., 2019) pretraining is not\nnecessary for strong performance.\n6.1.2 Effect of training corpora\nTable 3 also shows the results of text corpora.\nRows 1 and 2 show that, unsurprisingly, includ-\ning PubMed pretraining improves results over a\nRoBERTa-only model, by 2.6%. Comparing row\n2 to row 4 and row 3 to 5 shows that including\nMIMIC-III in pretraining results in a large improve-\nment on clinical tasks over PubMed-only models\n(+1.5% and +1.7%) but has little effect on PubMed-\nbased tasks (-0.1% and +0.1%).\n6.1.3 Effect of model size\nConsistent with ﬁndings from the recent litera-\nture (Devlin et al., 2019; Liu et al., 2019; Rad-\nford et al., 2019; Brown et al., 2020), we ﬁnd that\nlarge models perform consistently better than com-\nparable smaller ones. Comparing row 1 to row\n6, row 4 to 7, and row 5 to 8 in Table3 shows\naverage improvements of 2%, 1.6% and 0.9% re-\nspectively. These improvements are mostly driven\nby improved sequence labelling performance for\nlarge models.\n6.2 Comparisons to the state-of-the-art\nThe focus of this paper was not to set the state-of-\nthe-art on speciﬁc downstream tasks, but rather to\nevaluate which models consistently perform well.\nAs such, we prioritized consistent hyperparameter\nsearch and did not consider task-speciﬁc tuning.\nNevertheless, the models that we trained compare\nfavorably to the state-of-the-art. Table5 shows\nthe best results obtained for each task in our ex-\nperiments. In some cases, models used in our ex-\nperiments have been reported with higher results\nin the literature. We attribute such difference to\nvariance in test performance, small differences in\npre-processing and differing levels of hyperparame-\nter optimization and tuning. We control for test-set\nvariance by running each model 5 times with dif-\nferent random seeds and reporting median results.\nWe also use standard hyperparameter settings as\nreported in the literature. Table5 compares our\nresults to numbers reported in the literature. The\nbest model in our experiments sets a new State-of-\nthe-Art in 9 out of 18 tasks, and comes within 0.1%\nof the best reported result in another 3 tasks.\n7 Distillation\nIn Section6.1.3, we noted that larger models re-\nsult in better accuracy. However, they also require\nmore computational resources to run, limiting their\napplicability. Recent work addresses this issue by\ndistilling larger models into smaller ones while re-\ntaining performance. Next, we investigate whether\ndistillation works well in the BioNLP space.\n7.1 Distillation Technique\nKnowledge distillation (Hinton et al., 2015) aims to\ntransfer the performance from a more accurate and\ncomputationally expensive teacher model into a\nmore efﬁcientstudent model. Typically, the student\nnetwork is trained to mimic the output distribution\n152\nTask Name State-of-the-Art Our\nbest Task Name State-of-the-Art Our\nbestMethod Score Method Score\nBC5CDR-C. Lee et al.(2019) 93.5 93.7 I2B2-2012 Si et al.(2019) 80.9 80.8\nBC5CDR-D. Lee et al.(2019) 87.2 85.2 I2B2-2014 Lee et al.(2019) 93.0 96.3\nJNLPBA Yoon et al.(2019) 78.6 81.0 HOC Peng et al.(2019) 87.3 87.2*\nNCBI-disease Lee et al.(2019) 89.4 89.0 ChemProt Lee et al.(2019) 76.5 76.4*\nBC4CHEMD Lee et al.(2019) 92.4 93.7 GAD Bhasuran et al.(2018) 83.9 82.2‡\nBC2GM Lee et al.(2019) 84.7 88.0 EU-ADR Lee et al.(2019) 86.5 85.5†\nLINNEAEUS Giorgi and Bader(2018) 93.5 88.4 DDI-2013 Peng et al.(2020) 81.0 82.1‡\nSpecies-800 Lee et al.(2019) 75.3 81.1 I2B2-2010-RE Peng et al.(2019) 76.4 78.6\nI2B2-2010 Si et al.(2019) 90.3 89.7 MedNLI Peng et al.(2020) 84.2 88.5\nTable 5: Our best models compared to best reported results in the literature. The best model in our experiments\nunless otherwise stated is RoBERTa-large with PubMed, MIMIC-III and specialized vocabulary (“ours-large” in\nTable 2). Other models are indicated by: (*) RoBERTa-large + PubMed + MIMIC-III; (†) SciBERT; (‡) RoBERTa-\nbase + PubMed + MIMIC-III + vocab.\nor internal activations of the teacher network, while\nkeeping the teacher network’s weights ﬁxed.\nIn NLP, prior work has exploring distilling larger\nBERT-like models into smaller ones. Most of this\nwork trains the student network to mimic a teacher\nthat has already been ﬁnetuned for a speciﬁc task,\ni.e., task-speciﬁc distillation(Tsai et al., 2019; Turc\net al., 2019; Sun et al., 2020). Recently, Sanh\net al.(2020) showed that it is also possible to dis-\ntill BERT-like models in a task-agnostic way by\ntraining the student to mimic the teacher’s outputs\nand activations on the pretraining objective, i.e.,\nmasked language modeling (MLM). Task-agnostic\ndistillation is appealing because it enables the dis-\ntilled student model to be applied to a variety of\ndownstream tasks. Accordingly, we primarily ex-\nplore task-agnostic distillationin this work.\nRecent work has also shown the importance of\nstudent network initialization. For example,Sanh\net al.(2020) ﬁnd that initializing the student net-\nwork with a subset of layers from the teacher net-\nwork outperforms random initialization; unfortu-\nnately this approach constrains the student network\nto the same embedding and hidden dimension as\nthe teacher. Turc et al.(2019) instead advocate\ninitializing the student model via standard MLM\npretraining, ﬁnding that it outperforms the layer\nsubset approach. Unfortunately, they only consider\ntask-speciﬁc distillation, where the teacher network\nhas already been ﬁnetuned to the end task, reducing\nthe generality of the resulting student network.\nWe combine the approaches fromSanh et al.\n(2020) and Turc et al.(2019) by initializing the\nstudent network via standard MLM pretraining and\nthen performing task-agnostic distillation by train-\ning the student to mimic a pretrained teacher on\nthe MLM objective. We use our pretrained base\nmodel as the student network and large model as\nthe teacher network. We also experiment with\naligning the hidden states of the teacher’s and stu-\ndent’s last layer via a cosine embedding loss (Sanh\net al., 2020). Since our student and teacher net-\nworks have different hidden state sizes, we learn a\nlinear projection from the student’s hidden states to\nthe dimension of the teacher’s hidden states prior\nto computing this loss.\nWe distill each student for 50k steps. Similar to\npretraining (Section5.1), we distill with a batch\nsize of 8,192 and linearly warmup the learning rate\nfor the ﬁrst 5% of steps. We use a learning rate\nof 5e-4 and largely follow the distillation hyperpa-\nrameter choices ofSanh et al.(2020). In particular,\nour loss function is a weighted combination of the\noriginal MLM cross entropy loss (with a weight\n↵MLM =5 .0), a KL divergence loss term encour-\naging the student to match the teacher’s outputs\n(with a weight↵KL =2 .0) and optionally a co-\nsine embedding loss term to align the student’s and\nteacher’s last layer hidden states (with a weight\n↵cos =1 .0). For the KL loss we additionally em-\nploy a temperature of 2.0 to smooth the teacher’s\noutput distribution, followingSanh et al.(2020)\nand originally advocated byHinton et al.(2015).\n7.2 Distillation Results\nResults for distillation are shown in Table6. Since\ndistillation trains the student for an additional 50k\nsteps, we also include a baseline that just trains\nthe student (base) model for longer without any\ndistillation loss terms (“ours-base + train longer”).\nWe ﬁnd that distillation only slightly outper-\nforms the original base model (+0.2% on average)\nand the original base model trained longer (+0.1%\non average). Aligning the student and teacher hid-\n153\nModel\nMean Clin\n-ical\nPub\nMed\nSeq.\nLab. Classif. All\nours-base 85.1 84.6 86.6 81.8 84.7\n+ train longer 85.0 84.7 86.6 81.8 84.8\nours-large 86.8 85.5 87.9 82.6 85.8\nDistillation results (teacher = large; student = base)\ndistill 85.1 84.8 86.8 81.9 84.9\ndistill + align 85.2 84.9 86.9 81.9 85.0\nTable 6: Distillation results in context with our base\nand large models. Distillation outperforms both the\noriginal base model and the base model trained longer.\nAligning the student and teacher’s hidden states further\nimproves performance, but the best student underper-\nforms the large (teacher) model.\nden states via a cosine embedding loss brings addi-\ntional albeit slight gains (+0.1% on average relative\nto the “distill” model). This result is consistent\nwith ﬁndings fromTurc et al.(2019) showing that\npretrained student models are a competitive base-\nline. The best student (“distill + align”) improves\nupon the base model (+0.3% on average) but un-\nderperforms the large teacher (-0.8% on average).\n8 Related Work\nPretrained word representations have been used\nin NLP modelling for many years (Mikolov et al.,\n2013; Pennington et al., 2014; Bojanowski et al.,\n2016), and have been specialised for BioNLP ap-\nplications (Chiu et al., 2016; Wang et al., 2018b;\nZhang et al., 2019). More recently, contextual em-\nbeddings have led to robust improvements across\nmost NLP tasks, notably, ELMo (Peters et al.,\n2018) and BERT (Devlin et al., 2019), followed\nmore recently by models such as XLNet (Yang\net al., 2019), RoBERTa (Liu et al., 2019), XLM\nand XLM-RoBERTa (Lample and Conneau, 2019;\nConneau et al., 2020) amongst others.\nSeveral works adapt such models to scientiﬁc\nand biomedical domains. Four such models – SciB-\nERT (Beltagy et al., 2019), BioBERT (Lee et al.,\n2019), ClinicalBERT (Alsentzer et al., 2019) and\nBioMed-RoBERTA (Gururangan et al., 2020) – are\nextensively covered in Section4. Others include\nBlueBERT (Peng et al., 2019), which continues\nto pretrain BERT with data from PubMed and\nMIMIC-III. Zhu et al.(2018) andSi et al.(2019)\ntrain ELMo and BERT models on clinical data. In\nconcurrent work,Gu et al.(2020) train models for\nPubMed-like text, but do not consider clinical text.\nMethods for training or ﬁnetuning models on\ndownstream tasks is also an active area of research.\nWe focus on well-established single-task ﬁnetuning\ntechniques for BERT-like models using standard\nhyperparameter settings.Si et al.(2019) use com-\nplex task-speciﬁc models to yield strong results on\nclinical tasks, andPeng et al.(2020) investigate\nSTILTS methods (Phang et al., 2019) on a suite of\nBioNLP tasks, achieving gains over baselines.\nIn this work, we build a suite of 18 tasks to eval-\nuate our models. Aggregated benchmarks have\nbecome a common tool in NLP research, popular-\nized by the GLUE benchmark (Wang et al., 2018a)\nfor language understanding and its successor Super-\nGLUE (Wang et al., 2019). Evaluating on a suite of\ntasks is common in BioNLP too.Lee et al.(2019)\nevaluate on a set of 15 tasks,Peng et al.(2019) eval-\nuate on 10 tasks referred to as “BLUE”,Beltagy\net al.(2019) andGururangan et al.(2020) evaluate\non 7 and 2 biomedical tasks respectively. Unfortu-\nnately, often there is little overlap between efforts,\nand different metrics and dataset splits are often\nused, making cross-model comparisons challeng-\ning, hence our efforts to evaluate all models on a\nsingle testbed. In concurrent work,Gu et al.(2020)\nalso note this problem, and release a similar suite\nof tasks, referred to as BLURB, but do not include\nclinical tasks. We plan to evaluate our models on\nthe “BLURB” benchmarks in future work.\n9 Conclusion\nWe have thoroughly evaluated 6 open-source lan-\nguage models on 18 biomedical and clinical tasks.\nOf these models, we found that BioBERT was\nthe best on biomedical tasks, but general-purpose\nRoBERTa-large performed best on clinical tasks.\nWe then pretrained 6 of our own large-scale spe-\ncialized biomedical and clinical language models.\nWe determined that the most effective models were\nlarger, used a dedicated biomedical vocabulary and\nincluded both biomedical and clinical pretraining.\nThese models outperform all the other models in\nour experiments. Finally, we demonstrate that our\nbase model can be further improved by knowledge\ndistillation from our large model, although there\nremains a gap between the distillation-improved\nbase model and our large model.\nAcknowledgments\nThe authors would like to thank Jinhyuk Lee, Kyle\nLo, Yannis Papanikolaou, Andrea Pierleoni, Daniel\nO’Donovan and Sampo Pyysalo for their feedback\nand comments.\n154\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available\nClinical BERT Embeddings. In Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78, Minneapolis, Minnesota, USA.\nAssociation for Computational Linguistics.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Wang,\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\nand Oren Etzioni. 2018.Construction of the Litera-\nture Graph in Semantic Scholar. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 84–91, New Orleans - Louisiana. As-\nsociation for Computational Linguistics.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Jo-\nhan H¨ogberg, Ulla Stenius, and Anna Korhonen.\n2016. Automatic semantic classiﬁcation of scien-\ntiﬁc literature according to the hallmarks of cancer.\nBioinformatics, 32(3):432–440. Publisher: Oxford\nAcademic.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019.SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3615–3620, Hong Kong, China. Association for\nComputational Linguistics.\nBalu Bhasuran, Jeyakumar Natarajan, and . . 2018.Au-\ntomatic extraction of gene-disease associations from\nliterature using joint ensemble learning. PloS One,\n13(7):e0200699.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2016. Enriching Word Vectors\nwith Subword Information. arXiv:1607.04606 [cs].\nArXiv: 1607.04606.\n`Alex Bravo, Janet Pi ˜nero, N ´uria Queralt-Rosinach,\nMichael Rautschka, and Laura I. Furlong. 2015.Ex-\ntraction of relations between genes and diseases\nfrom text and large-scale data analysis: implica-\ntions for translational research. BMC bioinformat-\nics, 16:55.\nTom B. Brown, Benjamin Pickman Mann, Nick Ryder,\nMelanie Subbiah, Jean Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, G. Kr¨uger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric J Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nBilly Chiu, Gamal Crichton, Anna Korhonen, and\nSampo Pyysalo. 2016. How to Train good Word\nEmbeddings for Biomedical NLP. In Proceedings\nof the 15th Workshop on Biomedical Natural Lan-\nguage Processing, pages 166–174, Berlin, Germany.\nAssociation for Computational Linguistics.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the Bio-entity Recognition Task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. InProceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. NCBI Disease Corpus: A Resource for\nDisease Name Recognition and Concept Normaliza-\ntion. Journal of biomedical informatics, 47:1–10.\nMartin Gerner, Goran Nenadic, and Casey M. Bergman.\n2010. LINNAEUS: a species name identiﬁcation\nsystem for biomedical literature. BMC bioinformat-\nics, 11:85.\nJohn M. Giorgi and Gary D. Bader. 2018.Transfer\nlearning for biomedical named entity recognition\nwith neural networks. Bioinformatics (Oxford, Eng-\nland), 34(23):4087–4094.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lu-\ncas, Naoto Usuyama, Xiaodong Liu, Tristan\nNaumann, Jianfeng Gao, and Hoifung Poon.\n2020. Domain-Speciﬁc Language Model Pretrain-\ning for Biomedical Natural Language Processing.\narXiv:2007.15779 [cs]. ArXiv: 2007.15779.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020.Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks .\narXiv:2004.10964 [cs]. ArXiv: 2004.10964.\n155\nMar´ıa Herrero-Zazo, Isabel Segura-Bedmar, Paloma\nMart´ınez, and Thierry Declerck. 2013. The DDI\ncorpus: An annotated corpus with pharmacological\nsubstances and drug–drug interactions. Journal of\nBiomedical Informatics, 46(5):914–920.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Deep Learning and Representation Learn-\ning Workshop.\nMartin Krallinger, Obdulia Rabal, Saber Ahmad\nAkhondi, Mart´ın P´erez P´erez, J´es´us L´opez Santa-\nmar´ıa, Gael P´erez Rodr´ıguez, Georgios Tsatsaro-\nnis, Ander Intxaurrondo, Jos´e Antonio Baso L´opez,\nUmesh Nandal, Erin M. van Buel, A. Poorna Chan-\ndrasekhar, Marleen Rodenburg, Astrid Lægreid,\nMarius A. Doornenbal, Julen Oyarz ´abal, An ´alia\nLourenc ¸o, and Alfonso Valencia. 2017. Overview\nof the BioCreative VI chemical-protein interaction\nTrack.\nMartin Krallinger, Obdulia Rabal, Florian Leit-\nner, Miguel Vazquez, David Salgado, Zhiyong\nLu, Robert Leaman, Yanan Lu, Donghong Ji,\nDaniel M. Lowe, Roger A. Sayle, Riza Theresa\nBatista-Navarro, Rafal Rak, Torsten Huber, Tim\nRockt¨aschel, S´ergio Matos, David Campos, Buzhou\nTang, Hua Xu, Tsendsuren Munkhdalai, Keun Ho\nRyu, SV Ramanan, Senthil Nathan, SlavkoˇZitnik,\nMarko Bajec, Lutz Weber, Matthias Irmer, Saber A.\nAkhondi, Jan A. Kors, Shuo Xu, Xin An, Ut-\npal Kumar Sikdar, Asif Ekbal, Masaharu Yoshioka,\nThaer M. Dieb, Miji Choi, Karin Verspoor, Ma-\ndian Khabsa, C. Lee Giles, Hongfang Liu, Koman-\ndur Elayavilli Ravikumar, Andre Lamurias, Fran-\ncisco M. Couto, Hong-Jie Dai, Richard Tzong-\nHan Tsai, Caglar Ata, Tolga Can, Anabel Usi ´e,\nRui Alves, Isabel Segura-Bedmar, Paloma Mart´ınez,\nJulen Oyarzabal, and Alfonso Valencia. 2015.The\nCHEMDNER corpus of chemicals and drugs and its\nannotation principles. Journal of Cheminformatics,\n7(1):S2.\nTaku Kudo. 2018. Subword Regularization: Improv-\ning Neural Network Translation Models with Mul-\ntiple Subword Candidates. arXiv:1804.10959 [cs].\nArXiv: 1804.10959.\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual Language Model Pretraining .\narXiv:1901.07291 [cs]. ArXiv: 1901.07291.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho\nSo, and Jaewoo Kang. 2019. BioBERT: a\npre-trained biomedical language represen-\ntation model for biomedical text mining .\nBioinformatics, 36(4):1234–1240. eprint:\nhttps://academic.oup.com/bioinformatics/article-\npdf/36/4/1234/32527770/btz682.pdf.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016. BioCreative V CDR task\ncorpus: a resource for chemical disease relation\nextraction. Database: The Journal of Biological\nDatabases and Curation, 2016.\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean\nWelleck, Y .-Lan Boureau, Kyunghyun Cho, and Ja-\nson Weston. 2019.Don’t Say That! Making Incon-\nsistent Dialogue Unlikely with Unlikelihood Train-\ning. arXiv:1911.03860 [cs]. ArXiv: 1911.03860.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013.Distributed Represen-\ntations of Words and Phrases and Their Composi-\ntionality. In Proceedings of the 26th International\nConference on Neural Information Processing Sys-\ntems - Volume 2, NIPS’13, pages 3111–3119, USA.\nCurran Associates Inc. Event-place: Lake Tahoe,\nNevada.\nErik M. van Mulligen, Annie Fourrier-Reglat, David\nGurwitz, Mariam Molokhia, Ainhoa Nieto, Gian-\nluca Triﬁro, Jan A. Kors, and Laura I. Furlong. 2012.\nThe EU-ADR corpus: annotated drugs, diseases, tar-\ngets, and their relationships. Journal of Biomedical\nInformatics, 45(5):879–884.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nEvangelos Paﬁlis, Sune P. Frankild, Lucia Fanini,\nSarah Faulwetter, Christina Pavloudi, Aikaterini\nVasileiadou, Christos Arvanitidis, and Lars Juhl\nJensen. 2013.The SPECIES and ORGANISMS Re-\nsources for Fast and Accurate Identiﬁcation of Taxo-\nnomic Names in Text. PloS One, 8(6):e65390.\nYifan Peng, Qingyu Chen, and Zhiyong Lu. 2020.An\nEmpirical Study of Multi-Task Learning on BERT\nfor Biomedical Text Mining. arXiv:2005.02799\n[cs]. ArXiv: 2005.02799.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer Learning in Biomedical Natural Language\nProcessing: An Evaluation of BERT and ELMo on\nTen Benchmarking Datasets. In Proceedings of the\n18th BioNLP Workshop and Shared Task, pages 58–\n65, Florence, Italy. Association for Computational\nLinguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global Vectors for Word\n156\nRepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. 2018.Deep contextualized word\nrepresentations. arXiv:1802.05365 [cs]. ArXiv:\n1802.05365.\nJason Phang, Thibault F´evry, and Samuel R. Bowman.\n2019. Sentence Encoders on STILTs: Supplemen-\ntary Training on Intermediate Labeled-data Tasks.\narXiv:1811.01088 [cs]. ArXiv: 1811.01088.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from Natural Language Inference in the\nClinical Domain. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1586–1596, Brussels, Belgium.\nAssociation for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\narXiv:1910.01108 [cs]. ArXiv: 1910.01108.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nYuqi Si, Jingqi Wang, Hua Xu, and Kirk Roberts. 2019.\nEnhancing Clinical Concept Extraction with Contex-\ntual Embeddings. Journal of the American Medical\nInformatics Association, 26(11):1297–1304. ArXiv:\n1902.08691.\nLarry Smith, Lorraine K. Tanabe, Rie Johnson nee\nAndo, Cheng-Ju Kuo, I.-Fang Chung, Chun-Nan\nHsu, Yu-Shi Lin, Roman Klinger, Christoph M.\nFriedrich, Kuzman Ganchev, Manabu Torii, Hong-\nfang Liu, Barry Haddow, Craig A. Struble, Richard J.\nPovinelli, Andreas Vlachos, William A. Baumgart-\nner, Lawrence Hunter, Bob Carpenter, Richard\nTzong-Han Tsai, Hong-Jie Dai, Feng Liu, Yifei\nChen, Chengjie Sun, Sophia Katrenko, Pieter Adri-\naans, Christian Blaschke, Rafael Torres, Mariana\nNeves, Preslav Nakov, Anna Divoli, Manuel Ma˜na-\nL´opez, Jacinto Mata, and W. John Wilbur. 2008.\nOverview of BioCreative II gene mention recogni-\ntion. Genome Biology, 9 Suppl 2:S2.\nAmber Stubbs, Christopher Kotﬁla, and Ozlem Uzuner.\n2015. Automated systems for the de-identiﬁcation\nof longitudinal clinical narratives: Overview of 2014\ni2b2/UTHealth shared task Track 1. Journal of\nbiomedical informatics, 58(Suppl):S11–S19.\nAmber Stubbs and Ozlem Uzuner. 2015.Annotating\nlongitudinal clinical narratives for de-identiﬁcation:\nthe 2014 i2b2/UTHealth Corpus. Journal of biomed-\nical informatics, 58(Suppl):S20–S29.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner.\n2013a. Annotating temporal information in clinical\nnarratives. Journal of Biomedical Informatics, 46\nSuppl:S5–12.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner.\n2013b. Evaluating temporal relations in clinical text:\n2012 i2b2 Challenge. Journal of the American Med-\nical Informatics Association : JAMIA, 20(5):806–\n813.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020.MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019.Small\nand practical BERT models for sequence labeling.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3632–\n3636, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019.Well-Read Students Learn Better:\nOn the Importance of Pre-training Compact Models.\narXiv:1908.08962 [cs]. ArXiv: 1908.08962.\n¨Ozlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011.2010 i2b2/V A challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Asso-\nciation : JAMIA, 18(5):552–556.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems . In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d\\textquotesingle\nAlch´e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 3261–3275. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding . In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\n157\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Doug Burdick, Dar-\nrin Eide, Kathryn Funk, Yannis Katsis, Rodney\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\nPaul Mooney, Dewey Murdick, Devvret Rishi, Jerry\nSheehan, Zhihong Shen, Brandon Stilson, Alex\nWade, Kuansan Wang, Nancy Xin Ru Wang, Chris\nWilhelm, Boya Xie, Douglas Raymond, Daniel S.\nWeld, Oren Etzioni, and Sebastian Kohlmeier. 2020.\nCORD-19: The COVID-19 Open Research Dataset.\narXiv:2004.10706 [cs]. ArXiv: 2004.10706.\nYanshan Wang, Sijia Liu, Naveed Afzal, Majid\nRastegar-Mojarad, Liwei Wang, Feichen Shen, Paul\nKingsbury, and Hongfang Liu. 2018b.A compari-\nson of word embeddings for the biomedical natural\nlanguage processing. Journal of Biomedical Infor-\nmatics, 87:12–20.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nWonjin Yoon, Chan Ho So, Jinhyuk Lee, and Jaewoo\nKang. 2019.CollaboNet: collaboration of deep neu-\nral networks for biomedical named entity recogni-\ntion. BMC Bioinformatics, 20(10):249.\nYijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin,\nand Zhiyong Lu. 2019. BioWordVec, improving\nbiomedical word embeddings with subword infor-\nmation and MeSH. Scientiﬁc Data, 6(1):52. Num-\nber: 1 Publisher: Nature Publishing Group.\nHenghui Zhu, Ioannis Ch Paschalidis, and Amir Tah-\nmasebi. 2018. Clinical Concept Extraction with\nContextual Word Embedding. arXiv:1810.10566\n[cs]. ArXiv: 1810.10566."
}