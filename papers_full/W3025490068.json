{
  "title": "Parallel Corpus Filtering via Pre-trained Language Models",
  "url": "https://openalex.org/W3025490068",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2350979515",
      "name": "Zhang, Boliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287060943",
      "name": "Nagesh, Ajay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2518222383",
      "name": "Knight, Kevin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2763856713",
    "https://openalex.org/W2903182367",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W1834000468",
    "https://openalex.org/W2960374072",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W2902918014",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2970686691",
    "https://openalex.org/W3203064768",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2963281280",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2147262247",
    "https://openalex.org/W2496235729",
    "https://openalex.org/W2903297715",
    "https://openalex.org/W3037465386",
    "https://openalex.org/W2798389157",
    "https://openalex.org/W2155607551",
    "https://openalex.org/W2773493195",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2902949028",
    "https://openalex.org/W2963919854",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2962735107",
    "https://openalex.org/W2211796614",
    "https://openalex.org/W2419539795",
    "https://openalex.org/W630532510"
  ],
  "abstract": "Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains. We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus. Our method significantly outperforms baselines and achieves a new state-of-the-art. In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method. We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.",
  "full_text": "Parallel Corpus Filtering via Pre-trained Language Models\nBoliang Zhang, Ajay Nagesh, and Kevin Knight\nDiDi Labs\n{boliangzhang, ajaynagesh, kevinknight}@didiglobal.com\nAbstract\nWeb-crawled data provides a good source of\nparallel corpora for training machine transla-\ntion models. It is automatically obtained, but\nextremely noisy, and recent work shows that\nneural machine translation systems are more\nsensitive to noise than traditional statistical ma-\nchine translation methods. In this paper, we\npropose a novel approach to ﬁlter out noisy\nsentence pairs from web-crawled corpora via\npre-trained language models. We measure sen-\ntence parallelism by leveraging the multilin-\ngual capability of BERT and use the Genera-\ntive Pre-training (GPT) language model as a\ndomain ﬁlter to balance data domains. We\nevaluate the proposed method on the WMT\n2018 Parallel Corpus Filtering shared task, and\non our own web-crawled Japanese-Chinese\nparallel corpus. Our method signiﬁcantly out-\nperforms baselines and achieves a new state-\nof-the-art. In an unsupervised setting, our\nmethod achieves comparable performance to\nthe top-1 supervised method. We also evalu-\nate on a web-crawled Japanese-Chinese paral-\nlel corpus that we make publicly available.\n1 Introduction\nTraining modern neural machine translation (NMT)\nsystems requires large parallel-text resources.\nPublicly-available parallel corpora are mostly\npaired with English, such as German-English,\nFrench-English, Chinese-English, etc., and their\ndomains are limited. For building machine transla-\ntion systems between non-English language pairs,\nsuch as Chinese and Japanese, existing parallel\ncorpora are insufﬁcient and often low quality. To\naddress this problem, system builders have trained\nNMT systems on web-crawled data and achieved\npromising results (Xu and Koehn, 2017; Junczys-\nDowmunt, 2018; Schwenk, 2018; Schwenk et al.,\n2019). However, data automatically crawled from\nthe web is extremely noisy. Khayrallah and Koehn\n(2018) and Belinkov and Bisk (2018) show that\nneural translation models are far more sensitive to\nnoisy parallel training data than statistical machine\ntranslation. Data selection methods that can ﬁl-\nter noisy parallel sentences from large-scale web\ncrawled resources are in demand.\nIn this paper, we study the problem in a real-\nworld scenario where we crawl a large Japanese-\nChinese parallel corpus from various websites and\nbuild open-domain machine translation systems\nbetween Japanese and Chinese, by ﬁltering the\nweb crawled parallel corpus. In addition, a small\namount of clean parallel data is available, in the\nsoftware domain. In order to conﬁrm our results\non a public data, we also apply our ﬁlter to the\nWMT 2018 German-English Parallel Corpus Fil-\ntering shared task.\nPrevious work on parallel corpus ﬁltering per-\nforms poorly in our scenario as it either requires\nlarge clean parallel corpora or dictionaries (Xu\nand Koehn, 2017; Artetxe and Schwenk, 2019;\nJunczys-Dowmunt, 2018; Chaudhary et al., 2019),\nor relies on multilingual word embeddings and ne-\nglects context when measuring translation paral-\nlelism (Hangya and Fraser, 2018).\nIn this paper, we propose a simple but effec-\ntive parallel corpus ﬁltering method. Multilingual\nBERT (Devlin et al., 2019) projects multilingual\nsentences into a shared space and has shown a great\npotential for cross-lingual model transfer (Pires\net al., 2019). We use pre-trained multilingual\nBERT as prior knowledge and ﬁne-tune it on a\nsynthetic dataset. This multilingual BERT-based\nclassiﬁer forms an acceptability ﬁlter that deter-\nmines whether or not a sentence pair consists of a\nbona-ﬁde translation.\nAs the domain of training data largely affects\nmachine translation model performance, we also in-\ntroduce a domain ﬁlter. It uses the pre-trained Gen-\nerative Pre-training (GPT) as in-domain language\narXiv:2005.06166v1  [cs.CL]  13 May 2020\nmodel and is an extension of the existing cross-\nentropy difference based domain ﬁlter (Moore and\nLewis, 2010; Junczys-Dowmunt, 2018).\nWe evaluate our proposed method on the WMT\n2018 German-English Parallel Corpus Filtering\nshared task and achieve a new state-of-the-art. Our\nunsupervised method achieves comparable perfor-\nmance to the top system that is trained on mil-\nlions of clean parallel sentence pairs. Our proposed\nmethods also signiﬁcantly outperform baselines in\nour own Japanese-Chinese parallel corpus ﬁltering\ntask.\nWe make the following contributions:\n•We propose a novel approach to ﬁlter noisy\nparallel corpora by using pre-trained language\nmodels. Our approach outperforms strong\nbaselines and achieves a new state-of-the-art.\n•We devise an unsupervised ﬁltering approach\nthat does not require an identiﬁable clean sub-\nset of parallel segments. Our unsupervised\nmethod matches the results of previous super-\nvised methods.\n•We release a large web-crawled Japanese-\nChinese parallel corpus which can be a useful\nresource for machine translation research on\nnon-English language pairs.1\n2 Related Work\nSeveral recent works address parallel corpus ﬁlter-\ning. Denkowski et al. (2012), Dyer et al. (2010)\nand Heaﬁeld (2011) use language models and word\nalignments to determine how likely sentences are\nto be a good translation of another. Xu and Koehn\n(2017) introduce a noise ﬁltering tool, Zipporah,\nthat discriminates parallel and non-parallel sen-\ntences based on word-frequency vectors and a dic-\ntionary. Junczys-Dowmunt (2018) proposes a dual\nconditional cross-entropy ﬁltering method, which\nachieved ﬁrst place in the WMT 2018 German-\nEnglish Parallel Corpus Filtering shared task. They\ntrain two translation models in inverse directions on\nmillions of parallel sentences and score sentence\npairs based on the word-normalized conditional\ncross-entropy from the translation models. Artetxe\nand Schwenk (2019) and Schwenk (2018) propose\na margin-based scoring method that compares the\n1http://iwslt.org/doku.php?id=open_\ndomain_translation\nsimilarity of the source and target sentence repre-\nsentations. The sentence representations are pro-\nduced by a sentence encoder trained on clean paral-\nlel data via a neural encoder-decoder architecture.\nOther works based on sentence embeddings include\nHangya and Fraser (2018) and Littell et al. (2018),\nas well as Schwenk et al. (2019), which mines mil-\nlions of parallel sentences in 1620 language pairs\nfrom Wikipedia. These encoder-decoder based\nmethods require large amounts of clean parallel\ntraining data and are not applicable in our sce-\nnario where available data is noisy. Ondrej Bojar\n(2020) organize an open domain translation chal-\nlenge where participants are provided a large, noisy\nset of Japanese-Chinese segment pairs built from\nweb data, and the task is to clean the noisy data and\nbuild an end-to-end machine translation system.\nWork on data selection is also related. Moore\nand Lewis (2010); Junczys-Dowmunt (2018) se-\nlect domain-related data by computing the cross-\nentropy difference between in-domain and out-\ndomain language models. Duh et al. (2013) use\nneural language models for data selection. Axel-\nrod et al. (2011) and Axelrod et al. (2015) expand\ncross-entropy difference ﬁltering to both sides of\nthe parallel corpus. Since we aim to build a general\nmachine translation system, instead of selecting\ndata that are relevant to a speciﬁc domain, we se-\nlect data whose domains are as general as possible,\nby using Generative Pre-training (GPT) models\ntrained on large and diverse corpora.\n3 Method\nIn this section we introduce a language detection\nﬁlter, a translation-acceptability ﬁlter, and a do-\nmain ﬁlter. Each ﬁlter produces a score for every\ncandidate source/target sentence pair. The partial\nscore produced by each ﬁlter ranges from 0 to 1.\nValues beyond this range are normalized by min-\nmax normalization: ˆy= (y−min)/(max −min).\nThe ﬁnal score is the product of the partial scores.\n3.1 Language Detection Filter\nTargeting a web-crawler at a given language pair\nstill results in many pages written in the wrong\nlanguage. For example, while a URL pair may\nclearly indicate translation (e.g., “.jp” and “.zh”), it\nmay happen that the text content is simply copied\nrather than translated. We observe this in both\nour Japanese-Chinese data and the German-English\nParacrawl data set. It is necessary to ﬁlter out sen-\ntence pairs with undesired languages.\nWe adopt the fastText (Joulin et al., 2017, 2016)\nlanguage identiﬁcation toolkit in our language de-\ntection ﬁlter. For each sentence, the toolkit pro-\nduces a list of language candidates and their cor-\nresponding conﬁdence scores. We select the lan-\nguage that has the highest conﬁdence score from\nfastText as the language of the sentence. Sentence\npairs that have both of the elements detected as the\ndesired language are assigned score 1 and other-\nwise 0. By discarding sentence pairs with undesired\nlanguage IDs, we ﬁlter out 27% of our Chinese-\nJapanese parallel sentences and nearly 70% of the\nGerman-English parallel sentences from Paracrawl\ndata set.\n3.2 Acceptability Filter\nIn this section, we introduce our translation accept-\nability ﬁlter, one of the main contributions in the\npaper. It aims to measure the parallelism of sen-\ntence pairs and ﬁlter out sentence pairs that are not\nmutual translations.\nThe pre-trained language model BERT (Devlin\net al., 2019) has been shown to be effective in\nmany NLP tasks as it produces better and meaning-\nful contextualized word representations. Multilin-\ngual BERT, a transformer Masked Language Model\npre-trained on Wikipedia dumps of 104 languages,\nshows remarkable multilingual capability, given\nthat it is not exposed to any multilingual signals,\nsuch as parallel data or dictionaries. A thorough\nstudy by Pires et al. (2019) shows the promising\nzero-shot cross-lingual model transfer ability of\nmultilingual BERT on named entity recognition\nand part-of-speech tagging tasks. They hypothesize\nthat having language-universal word pieces, such\nas numbers and URLs, mapped to a shared space\nforces the co-occurring pieces to also be mapped\nto a shared space, thus spreading the effect to other\nword pieces, until different languages are close in\nthe shared space.\nWe use pre-trained multilingual BERT to encode\na sentence pair (s,t) and create the sentence em-\nbeddings vs and vt by using the representations of\nthe [CLS] token of sand t. We ﬁnd that the cosine\nsimilarity between vs and vt does not necessarily\nreﬂect the parallelism of sentence s and t. We\nsuspect that the word representations from multilin-\ngual BERT are loosely aligned across languages as\nthere is no parallel data or dictionary used during\nthe pre-training. A similar observation was made in\nLample et al. (2018), where the cross-lingual word\nembeddings learned in an unsupervised manner are\nloosely aligned. However, after ﬁne-tuning on a\nfew anchor pairs (word translations), they become\nmore aligned.\nSimilarly, we use an unsupervised synthetic\ntraining set as anchors to ﬁne-tune multilingual\nBERT with a binary classiﬁcation objective. Xu\nand Koehn (2017) did similar work to train a ﬁl-\ntering classiﬁer on synthetic data, but via bag-of-\nwords translation features.\nSynthetic Training Set. In cases where a small\nnumber of clean parallel sentence pairs are avail-\nable, we use them as positive training samples\nfor our classiﬁer. In Japanese-Chinese ﬁltering,\nwe use around 300k sentence pairs, mostly from\nopen-source software documentation,2 as our pos-\nitive samples. In extreme cases where no identiﬁ-\nable, clean parallel data is available, we sub-select\nhigh quality parallel sentences, which are used as\npositive samples, from the noisy parallel corpus\nbased on the Hunalign (Varga et al., 2007) sentence-\nalignment score. We sample negative instances by\nsimulating the noise produced by web crawling and\nalignment. Given a positive pair (s,t), we create a\nnegative sample by randomly choosing one of the\nfollowing options:\n•Randomly select a target sentence from its\nadjacent sentences within a window size of k\n(where k= 2in our experiments).\n•Randomly truncate 30%-70% of the source or\ntarget sentence.\n•Swap the order of 30%-70% words of the\nsource or target sentence.\nTo balance the training set, we create the same\nnumber of positive instances and sampled negative\ninstances.\nBinary Classiﬁcation Objective. We feed the\nsentence pair (s,t) into multilingual BERT, which\naccepts two-sentence input due to its next-sentence\nprediction objective (Devlin et al., 2019). Instead\nof using the [CLS] token representation, we use a\nConvolutional Network (CNN) layer that takes the\nBERT output and generates the ﬁnal representation\nof the pair. Our experiments show that using CNN\nlayer pooling achieves marginal gains over [CLS]\npooling. The ﬁnal layer is a feed-forward network\n2GNOME, Ubuntu, OpenOfﬁce, and KDE data set, from\nhttp://opus.nlpl.eu/\nwith a softmax activation function to produce label\nprobabilities. We use the softmax probability as\nthe degree of parallelism.\n3.3 Domain Filter\nWeb-crawled data contains noise of various types,\ndue to the complicated structure of web pages. By\ninspecting the training data generated by the above\nmethods, we notice much of the content is not\nwell-formed, e.g., concatenated lists of months and\ndates, randomly mixed content from tables, series\nof emojis and punctuation marks, etc. These are\ncertainly written in the desired language, thus not\nﬁltered out by language detection. The translation\nacceptability ﬁlter also accepts them. However,\nsuch malformatted data is not helpful to machine\ntranslation models, and we prefer a training corpus\nto contain meaningful content.\nFor our domain ﬁlter, we adopt the cross-entropy\ndifference scoring method proposed by Moore and\nLewis (2010) and Junczys-Dowmunt (2018). More\nspeciﬁcally, we treat a general domain monolingual\ncorpus as our in-domain data set I, and the noisy\nparallel corpus without any ﬁltering as our non-\ndomain data set N. We train two language models\nLI and LN and measure how the target sentence t\nis domain-related to I and less domain-related to N\nby a perplexity ratio, which is a transformation of\ncross-entropy difference:\nˆfdom (s,t ) = PPL N (t)\nPPL I (t)\nwhere PPLM (x) is the word-normalized perplexity\nof the sentence xdeﬁned by the language model\nLM :\nPPL M (x) = exp(1\n|x|\n|x|∑\ni=1\nlog PM (xi|x<i ))\nThe intuition is fairly straightforward: the higher\nthe perplexity of the sentence to the non-domain\ncorpus and the lower the perplexity of the sentence\nto the in-domain corpus, the more likely the sen-\ntence is meaningful.\nOur contribution is to use GPT (Radford et al.,\n2019) as our in-domain language model, instead\nof news domain text (Junczys-Dowmunt, 2018).\nThis minor yet crucial change yields non-trivial\nperformance gains in our experiments for German-\nEnglish parallel corpus ﬁltering. As GPT is trained\non data from various sources, such as Wikipedia,\nReddit, news websites, etc., it covers a wide range\nof domains, so our ﬁltered data is more diverse and\nperforms better on multi-domain test sets, as well\nas in the real world application.\nFor our in-domain language model, we use\npre-trained Chinese GPT 3 for Japanese-Chinese\nand pre-trained GPT-2 4 for German-English.\nWe randomly sample 4 million sentences from\nthe unﬁltered noisy parallel corpus and use\nKenLM (Heaﬁeld, 2011) to train the non-domain\nlanguage model. Perplexity scores from different\nlanguage models are compatible.\nFollowing Junczys-Dowmunt (2018), we in-\ntroduce two operations, clip and cutoff, to post-\nprocess the domain ﬁlter score ˆfdom(s,t). The clip\noperation clips the maximum value of the domain\nscore to a threshold τclip:\nfclip (x,τclip ) = min (x,τclip )\nand the cutoff operation modiﬁes scores below a\nthreshold τcutoff and changes them to 0:\nfcutoff (x,τ cutoff ) =\n{\nx, if x>τ cutoff\n0, otherwise\nτclip prevents a high monolingual in-domain score\nfrom overwriting scores from other ﬁlters. τcutoff\neliminates out-domain sentence pairs and ensures\nthat highly parallel sentence pairs are at least some-\nwhat in-domain. We tune τclip and τcutoff on the\ndevelopment set.\nThe scoring method of our ﬁnal domain ﬁlter\nbecomes:\nfdom(s,t) =fclip(fcutoff( ˆfdom(s,t),τcutoff),τclip)\n4 Experiments and Results\n4.1 WMT 2018 Parallel Corpus Filtering\nWe use the WMT 2018 Parallel Corpus Filtering\nshared task (Koehn et al., 2018) as a benchmark\nto evaluate our methods. Participants in the shared\ntask are provided a very noisy 1 billion word (En-\nglish token count) German-English corpus crawled\nfrom the web by the Paracrawl project.5 The task\nis to sub-select clean sentence pairs amounting to\n(a) 10 million words, and (b) 100 million words,\ncounted on the English side. The quality of the\n3https://github.com/dbiir/UER-py\n4https://github.com/huggingface/transformers\n5https://paracrawl.eu\nresulting subsets is determined by training a neu-\nral machine translation system (Marian)6 (Junczys-\nDowmunt et al., 2018) on this data. The quality\nof the machine translation system is measured by\nBLEU score on six test sets from various domains.\nAs the task is to address the challenge of the data\nquality and not domain-relatedness of the data for\na particular use, sub-sampling the corpus for rel-\nevance to the news domain is not encouraged by\nthe shared task organizers. All parameters used for\ntraining Marian machine translation models are the\nsame as described in Koehn et al. (2018). We use\nCLIP = 5and CUTOFF = 1.5 in the experiments.\nWe use 4 GPUs for training.\n4.2 Web-Crawled Japanese-Chinese Parallel\nCorpus Filtering\nDue to the lack of publicly available Japanese-\nChinese parallel corpus, we build a data harvest-\ning pipeline to fetch Japanese-Chinese parallel text\nfrom the Internet. The crawled bi-text are ex-\ntremely noisy, but we rely on the proposed parallel\ncorpus ﬁltering method to clean up the data and\neventually train a satisfactory machine translation\nsystem. In this paper, we use these crawled data as\nanother test bed to evaluate our proposed method.\nA single run of the of the data harvesting\npipeline is the following. We ﬁrst identify\nJapanese-Chinese parallel webpages by program-\nmatically analyzing the URL structure of the 5\nbillion URLs from CommonCrawl, 7 for exam-\nple, https://www.gotokyo.org/jp/ and https:\n//www.gotokyo.org/cn/ only differ by jp and cn.\nThen we download the webpages and conduct a se-\nries of cascaded data cleaning methods, including\nremoving HTML markups, sentence segmentation,\netc. Finally we perform segment alignment and\nﬁltering. Our workﬂow consists of several runs\nof the data harvesting pipeline with entry points\nat different modules (for instance, a more targeted\ncrawling of higher quality material from a previous\nrun).\nWe also integrate existing Japanese-Chinese par-\nallel datasets from other publicly available sources\nfor a ﬁnal parallel data size of 527m characters in\n20.9M parallel segments.\nWe include all details of our data harvesting\n6https://github.com/marian-nmt/marian\n(We do not evaluate our method using Moses, the statistical\nmachine translation system provided by WMT, as neural\nmachine translation better ﬁts our real world scenario.)\n7https://commoncrawl.org/\npipeline, as well as the statistics of the obtained\ndataset, in Appendix A.\nTest and Development Dataset. We curate two\nparallel test sets by manually processing web data\ninvolving daily expressions (337 parallel segments)\nand news (437 parallel segments). For our devel-\nopment set, we use 5304 Japanese-Chinese basic\nexpressions.\n4.3 Results and Analysis\nWMT 2018 Parallel Corpus Filtering. Table 1\npresents the BLEU scores of neural machine trans-\nlation systems trained on 10 million and 100 mil-\nlion words of training data, selected by different\nﬁltering methods. In the table, we list the top three\nperformers from the shared task, as well as an-\nother two work that are similar to ours. Junczys-\nDowmunt (2018) has a dual conditional cross-\nentropy adequacy ﬁlter and a domain ﬁlter trained\non news corpora. Hangya and Fraser (2018) gener-\nate sentence embeddings by using unsupervised\nword embedding alignment and measure paral-\nlelism via multilingual sentence embedding similar-\nity. Chaudhary et al. (2019) leverage massive pub-\nlicly available English-German parallel corpora to\ntrain multilingual sentence embeddings via bidirec-\ntional Long Short Term Memory (LSTM) encoder-\ndecoder network.\nWe replicate the adequacy and domain-news ﬁl-\nters from Junczys-Dowmunt (2018) and obtain sim-\nilar results. By replacing the domain-news ﬁlter\nwith our domain-GPT ﬁlter, we achieve new state-\nof-the-art scores on 10M and 100M word data sets\n(bold scores in the table). Given the very compact\nscore range in the shared task (Koehn et al., 2018),\nwe consider this gain very successful. It is stated in\nthe shared task that the test sets are from multiple\ndomains. Domain-news ﬁlter in Junczys-Dowmunt\n(2018) tends to select sentence pairs from news\ndomain as the ﬁlter is trained on news domain data,\nand this leads to a biased parallel corpus for training\nmachine translation system. Our proposed domain-\nGPT ﬁlter is trained from various sources and thus\ncovers a wide range of domains, so our ﬁltered\ndata is more diverse and performs better on multi-\ndomain test sets.\nFor our supervised acceptability ﬁlter, we train a\nmulitlingual BERT classiﬁer on clean parallel sen-\ntences as positive examples and randomly sampling\nnegative instances, using the method described\nin Section 3.2. For our unsupervised acceptabil-\nMethod Supervised Unsupervised 10M 100M\nJunczys-Dowmunt (2018) top-1 x 28.62 32.05\nLu et al. (2018) top-2 x 27.60 31.93\nLo et al. (2018) top-3 x 27.41 31.88\nHangya and Fraser (2018) x 22.96 30.54\nChaudhary et al. (2019) x 26.98 30.77\nadequacy (our replication of J-D 2018) x 27.12 31.20\n+ domain-news (our replication of J-D 2018) x 28.66 32.01\n+ domain-GPT x †29.09 †32.11\nsupervised acceptability x 27.09 31.56\n+ domain-GPT x 28.94 32.03\nunsupervised acceptability x 27.03 30.65\n+ domain-GPT x ‡28.68 ‡32.02\n- all methods above apply language detection ﬁlter beforehand.\n† our new state-of-the-art combines adequacy (Junczys-Dowmunt, 2018) + our proposed domain-GPT.\n‡ our unsupervised acceptability + domain-GPT is comparable to top supervised method.\nTable 1: BLEU scores of German-English neural MT systems trained on 10 million and 100 million word training\ndata selected by different methods. The scores are averaged BLEU scores across the six test sets from WMT 2018\nparallel corpus ﬁltering task. domain-news trains an in-domain language model on news corpus, while domain-\nGPT uses the pre-trained GPT language model.\nMethods JA-ZH % ∗ ZH-JA % ∗\nunﬁltered 22.92 100 22.27 100\nChaudhary et al. (2019) 23.46 75 26.22 70\nadequacy (our replication of J-D 2018) 23.91 90 24.51 90\n+ domain-GPT 24.00 65 - -\nacceptability 25.53 75 28.54 50\n+ domain-GPT 25.49 50 - -\n- all methods above apply language detection ﬁlter beforehand.\n* percentage of raw parallel sentences used for MT training.\nTable 2: BLEU scores of Japanese-Chinese and Chinese-Japanese MT systems trained on data sets generated by\nvarious ﬁltering methods. We rank sentence pairs by ﬁltering scores and train an MT system on N percent of the\ntop ranked data. N is selected based on the development set and we report the best BLEU score. domain-GPT is\nthe domain ﬁlter whose in-domain language model is the pre-trained GPT language model; note that for ZH-JA,\nwe do not have access to pre-trained Japanese GPT.\nity ﬁlter, we rank noisy parallel sentences by (a)\nthe alignment score from Hunalign, and (b) the\nGPT domain ﬁlter score. We then select the top\n10M words (counted on English side) worth of\nsentence pairs as positive examples. This makes\nthe method completely unsupervised, not requiring\nany identiﬁable clean parallel data. With ﬁnetuning\nmultilingual BERT on sentences pairs aligned by\nHunalign, the unsupervised acceptability already\nachieves comparable performance to Chaudhary\net al. (2019) which use massive public parallel data.\nAfter applying the unsupervised domain-GPT ﬁlter,\nwe achieve a surprisingly good result (underlined\nscores in the table), comparable to the best super-\nvised method.\nJapanese-Chinese Parallel Corpus Filtering.\nIn Table 2, we evaluate machine translation sys-\ntems trained on data generated by different ﬁl-\ntering methods. Unﬁltered refers to data gener-\nated by Hunalign without any ﬁltering. Chaud-\nhary et al. (2019) refer to LASER, the top per-\nforming ﬁltering system in WMT 2019 Parallel\nCorpus Filtering shared task. We use the pre-\ntrained 93-language LASER model to generate\nsentence pair scores. The model is trained on a\nlarge parallel corpus that contains 3.2M English-\nJapanese and 8.2M English-Chinese sentence pairs\n(English is used as pivot to connect Japanese and\nChinese during their training). Adequacy refers to\nthe dual conditional cross-entropy ﬁltering method\nthat we replicate from Junczys-Dowmunt (2018).\nIt is trained on around 300k high quality software-\ndomain parallel sentences from Microsoft Devel-\noper Network (MSDN) and Ubuntu. The GPT do-\nmain ﬁlter uses a pre-trained Chinese GPT8 as the\nin-domain language model and trains a four-gram\nKenLM (Heaﬁeld, 2011) language model on the\nChinese side of our 4 million unﬁltered noisy par-\nallel sentences as a non-domain language model.\nAcceptability is our proposed multilingual BERT\nbased ﬁltering method, which is trained on a syn-\nthetic dataset, where we use 300k high-quality\nsoftware domain parallel sentences as positive ex-\namples and sample equal-sized negative sentence\npairs, using the sampling methods described in Sec-\ntion 3.2.\nChaudhary et al. (2019) train a multilin-\ngual sentence encoder on various English-\nForeign Language parallel corpus and prove the\nzero-shot cross-lingual transfer capability between\nnon-English pairs, such as Japanese and Chinese.\nHowever, when English is used as the pivot, the dis-\ntance between Japanese and Chinese become larger,\nresulting in not effectively capturing the correla-\ntion between them. The conditional cross-entropy\nmetric in adequacy relies on the quality of machine\ntranslation system. Due to the difﬁculty of training\nhigh-quality machine translation systems on 300k\nsentence pairs, the adequacy ﬁlter cannot produce\naccurate conditional cross-entropy. The GPT do-\nmain ﬁlter assigns higher score to sentences that are\nmore like human natural language and downgrades\nmalformatted sentence pairs. It is effective in the\nGerman-English ﬁltering task, where a ﬁxed-size\nsubset is selected and we want to ﬁll the subset with\nas much domain relevant data as possible. However,\nto best ﬁt the real world scenario where the goal is\nto have the best machine translation system, we do\nnot limit the amount of data to select for training\nmachine translation system and let the system de-\ncide the amount of the data to select, according to\neach ﬁltering method. We rank sentence pairs by\ntheir ﬁltering scores and train a MT system on N\npercentage of the top ranked data. N is selected\nbased on the development set and we report the best\nBLEU score. Under this setting, adding a domain\nﬁlter makes the model use less data ( N = 50%\n8pre-trained Mixedlarge corpus + GptEncoder + LmTarget\nModel in https://github.com/dbiir/UER-py\nFiltering Probability Threshold\nQuality of Pairs (P/R)\n0.2\n0.4\n0.6\n0.8\n1\n0.2 0.4 0.6 0.8\nPrecision Recall\nFigure 1: Precision and recall curves of the acceptabil-\nity ﬁlter on our internal JA-ZH ﬁltering test set. The\nthreshold is based on the classiﬁer probability produced\nby the softmax layer. When threshold set to 0.9, we ob-\ntain 97.7% precision parallel sentence pairs at 66.9%\nrecall.\nvs N = 75%), but we do not observe any perfor-\nmance gain, as we suspect that the malformatted\nbut parallel sentence pairs are neither harmful or\nhelpful to the model, and ﬁltering them out makes\nno difference in performance of the model.\nHigh Precision Parallel Corpus Filtering. For\nanalysis purposes, we manually annotate a small\nset of 320 sentence pairs randomly selected from\nour original web crawled Japanese-Chinese data\nset. 24% of the sentence pairs are labeled “not\nmutual translations.” As stated in Khayrallah and\nKoehn (2018), neural machine translation models\nare more sensitive to noise than statistical machine\ntranslation models, so having high precision ﬁlter-\ning results as training data is necessary. In Fig-\nure 1, we show precision and recall curves for our\nproposed ﬁltering method on this labeled test set,\nunder different threshold settings. The threshold is\nselected based on the ﬁltering classiﬁer probabil-\nity produced by the softmax layer. By setting the\nthreshold to 0.9, we are able to obtain 97.7% pre-\ncision high-quality parallel sentences, while still\nhaving 66.9% recall.\n5 Conclusions\nIn this paper, we address the parallel corpus ﬁlter-\ning problem in machine translation. We propose a\nnovel ﬁltering method using pre-trained language\nmodels. Our method outperforms strong baselines\nand achieves a new state-of-the-art. We release a\nlarge Japanese-Chinese web crawled parallel cor-\npus for the research purposes. Because it is artiﬁ-\ncial to use synthetic data for training a ﬁlter classi-\nﬁer, future work can focus on a better objective that\nmodels parallelism more smoothly. Future work\nalso includes extending the method to low-resource\nlanguages not covered by multilingual BERT.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nfor their constructive feedback.\nReferences\nMikel Artetxe and Holger Schwenk. 2019. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao.\n2011. Domain adaptation via pseudo in-domain\ndata selection. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nAmittai Axelrod, Yogarshi Vyas, Marianna Martindale,\nand Marine Carpuat. 2015. Class-based n-gram lan-\nguage difference models for data selection. In Pro-\nceedings of the International Workshop on Spoken\nLanguage Translation (IWSLT).\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In Proceedings of the Sixth International Con-\nference on Learning Representations (ICLR).\nVishrav Chaudhary, Yuqing Tang, Francisco Guzm ´an,\nHolger Schwenk, and Philipp Koehn. 2019. Low-\nresource corpus ﬁltering using multilingual sentence\nembeddings. In Proceedings of the Fourth Confer-\nence on Machine Translation.\nChenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-\nhashi. 2015. Integrated parallel sentence and frag-\nment extraction from comparable corpora: A case\nstudy on Chinese–Japanese Wikipedia. ACM Trans-\nactions on Asian and Low-Resource Language Infor-\nmation Processing.\nRaj Dabre and Sadao Kurohashi. 2017. MMCR4NLP:\nmultilingual multiway corpora repository for natural\nlanguage processing. CoRR, abs/1710.01025.\nMichael Denkowski, Greg Hanneman, and Alon Lavie.\n2012. The CMU-Avenue French-English translation\nsystem. In Proceedings of the Seventh Workshop on\nStatistical Machine Translation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (NAACL-HLT).\nKevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-\njime Tsukada. 2013. Adaptation data selection us-\ning neural language models: Experiments in ma-\nchine translation. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nChris Dyer, Jonathan Weese, Hendra Setiawan, Adam\nLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-\nitkevitch, Phil Blunsom, and Philip Resnik. 2010.\ncdec: A decoder, alignment, and learning framework\nfor ﬁnite-state and context-free translation models.\nIn Proceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics (ACL), System\nDemonstrations.\nViktor Hangya and Alexander Fraser. 2018. An unsu-\npervised system for parallel corpus ﬁltering. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, H ´erve J ´egou, and Tomas Mikolov.\n2016. FastText.zip: Compressing text classiﬁcation\nmodels. arXiv preprint arXiv:1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient text\nclassiﬁcation. In Proceedings of the Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics (EACL).\nMarcin Junczys-Dowmunt. 2018. Dual conditional\ncross-entropy ﬁltering of noisy parallel corpora. In\nProceedings of the Third Conference on Machine\nTranslation: Shared Task Papers.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann, Al-\nham Fikri Aji, Nikolay Bogoychev, Andr´e F. T. Mar-\ntins, and Alexandra Birch. 2018. Marian: Fast neu-\nral machine translation in C++. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL), System Demonstrations.\nHuda Khayrallah and Philipp Koehn. 2018. On the\nimpact of various types of noise on neural machine\ntranslation. In Proceedings of the Workshop on Neu-\nral Machine Translation and Generation.\nPhilipp Koehn, Huda Khayrallah, Kenneth Heaﬁeld,\nand Mikel L Forcada. 2018. Findings of the WMT\n2018 shared task on parallel corpus ﬁltering. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only. In\nProceedings of the 6th International Conference on\nLearning Representations (ICLR).\nPierre Lison and J ¨org Tiedemann. 2016. OpenSub-\ntitles2016: Extracting large parallel corpora from\nmovie and TV subtitles. In Proceedings of the Tenth\nInternational Conference on Language Resources\nand Evaluation (LREC).\nPatrick Littell, Samuel Larkin, Darlene Stewart, Michel\nSimard, Cyril Goutte, and Chi-kiu Lo. 2018. Mea-\nsuring sentence parallelism using Mahalanobis dis-\ntances: The NRC unsupervised submissions to the\nWMT18 parallel corpus ﬁltering shared task. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers.\nChi-kiu Lo, Michel Simard, Darlene Stewart, Samuel\nLarkin, Cyril Goutte, and Patrick Littell. 2018. Ac-\ncurate semantic textual similarity for cleaning noisy\nparallel corpora using semantic machine translation\nevaluation metric: The NRC supervised submissions\nto the parallel corpus ﬁltering task. In Proceedings\nof the Third Conference on Machine Translation:\nShared Task Papers.\nJun Lu, Xiaoyu Lv, Yangbin Shi, and Boxing Chen.\n2018. Alibaba submission to the WMT18 parallel\ncorpus ﬁltering task. In Proceedings of the Third\nConference on Machine Translation: Shared Task\nPapers.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nChristian Federmann Jiatao Gu Fei Huang Ajay\nNagesh Jan Niehues Elizabeth Salesky Sebastian\nSt uker Marco Turchi Ondrej Bojar, Marcello Fed-\nerico. 2020. Findings of the iwslt 2020 evalua-\ntion campaign. In Proceedings of the 2020 Interna-\ntional Conference on Spoken Language Translation\n(IWSLT).\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nHolger Schwenk. 2018. Filtering and mining parallel\ndata in a joint multilingual space. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm ´an. 2019. Wiki-\nmatrix: Mining 135m parallel sentences in 1620\nlanguage pairs from Wikipedia. arXiv preprint\narXiv:1907.05791.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eight Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC).\nD´aniel Varga, P ´eter Hal ´acsy, Andr ´as Kornai, Viktor\nNagy, L´aszl´o N´emeth, and Viktor Tr´on. 2007. Paral-\nlel corpora for medium density languages. Amster-\ndam Studies in the Theory and History of Linguistic\nScience.\nHainan Xu and Philipp Koehn. 2017. Zipporah: a\nfast and scalable data cleaning system for noisy web-\ncrawled parallel corpora. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nA Web-Crawled Parallel Data for Japanese-Chinese\nFigure 2: Our Japanese-Chinese parallel data harvesting pipeline. It consists of several modules, each of them\nnumbered. The inputs to and outputs from each module are depicted in orange. The example entry points to the\ndata pipeline are shown at the bottom of the diagram.\nSource # Segment-pairs # Characters (zh side) Reference\nWeb-crawled (pipeline) 18,966,595 493,902,539 -\nLinux documentation 92,250 1,549,964 Tiedemann (2012)\nOpen Subtitiles 914,355 10,932,722 Lison and Tiedemann (2016)\nTED 376,441 5,345,867 Dabre and Kurohashi (2017)\nGlobal V oices 16,848 337,194 Tiedemann (2012)\nWikipedia 228,565 5,067,489 Chu et al. (2015)\nWiktionary 62,557 222,562 wiktionary.org\nNews Commentary 570 65,038 Tiedemann (2012)\nTatoeba 4,243 50,846 tatoeba.org\nFacebook 267,409 9,950,657 Schwenk et al. (2019)\nTotal 20,929,833 527,424,878 -\nTable 3: Japanese-Chinese parallel data assembled for our experiments.\nThis appendix describes our pipeline to extract parallel Japanese-Chinese parallel sentence fragments\nfrom the Internet (Figure 2). We start with 5 billion URLs from CommonCrawl.9 We identify Japanese-\nChinese parallel webpages by looking at URL structure (step 2). For example, https://www.gotokyo.\norg/jp/ and https://www.gotokyo.org/cn/ only differ by jp and cn. We download these potentially\nparallel page pairs (step 3), remove HTML and other markup metadata (step 4),10 and split into sentence\nsegments. We use off-the-shelf Hunalign11 for segment alignment (step 5). We ﬁlter segment pairs by\nrough language ID and length ratio (step 6). We obtain 227k URL pairs, 1.4m segment pairs, and 28.7m\ncharacters of parallel data (measured on the Chinese side).\nFrom the 227k URL pairs above, we trace which site pairs yielded the most parallel data. We then\nrun a deep-crawling module on each of the 6000 most-promising sites,12 and we process the resulting\nURLs using the rest of the pipeline. Concatenating parallel data from all runs (step 7) and running a\nsimple post-processing ﬁlter to remove objectionable content in the text gathered, we obtain around 494m\ncharacters of parallel data (measured on the Chinese side).\nWe also integrate existing Japanese-Chinese parallel datasets from other publicly available sources\nfor a ﬁnal parallel data size 527m characters in 20.9m parallel segments. Table 3 describes the various\ncomponents of this dataset.\n9https://commoncrawl.org/\n10Using Python module BeautifulSoup\n11http://mokk.bme.hu/en/resources/hunalign/\n12Using the Python-based scrapy tool",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8991907835006714
    },
    {
      "name": "Machine translation",
      "score": 0.85288405418396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6937777400016785
    },
    {
      "name": "Natural language processing",
      "score": 0.6642770767211914
    },
    {
      "name": "Sentence",
      "score": 0.5778555274009705
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.5489691495895386
    },
    {
      "name": "Parallel corpora",
      "score": 0.5471150875091553
    },
    {
      "name": "Language model",
      "score": 0.5413399338722229
    },
    {
      "name": "Translation (biology)",
      "score": 0.51556396484375
    },
    {
      "name": "Task (project management)",
      "score": 0.497621089220047
    },
    {
      "name": "Generative grammar",
      "score": 0.43773338198661804
    },
    {
      "name": "Machine learning",
      "score": 0.32192739844322205
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}