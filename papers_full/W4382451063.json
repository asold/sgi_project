{
  "title": "Text to Point Cloud Localization with Relation-Enhanced Transformer",
  "url": "https://openalex.org/W4382451063",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2132220699",
      "name": "Guangzhi Wang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2807829587",
      "name": "Hehe Fan",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2502133576",
      "name": "Mohan Kankanhalli",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2132220699",
      "name": "Guangzhi Wang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2807829587",
      "name": "Hehe Fan",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2502133576",
      "name": "Mohan Kankanhalli",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095974555",
    "https://openalex.org/W2179042386",
    "https://openalex.org/W2556455135",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2995439012",
    "https://openalex.org/W3180659539",
    "https://openalex.org/W4221003815",
    "https://openalex.org/W3168718178",
    "https://openalex.org/W3140398265",
    "https://openalex.org/W3123547918",
    "https://openalex.org/W3135162481",
    "https://openalex.org/W6746975906",
    "https://openalex.org/W4221164177",
    "https://openalex.org/W6770800577",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W3035242260",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W2905255369",
    "https://openalex.org/W2990957769",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W6668982098",
    "https://openalex.org/W3186681406",
    "https://openalex.org/W3133833192",
    "https://openalex.org/W3161771873",
    "https://openalex.org/W2965124455",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4300614726",
    "https://openalex.org/W3203534862",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3173736705",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4320168098",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W3107521863",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2962705366",
    "https://openalex.org/W3203701986",
    "https://openalex.org/W3091598621",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4312636849",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4287375617",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W4288049158",
    "https://openalex.org/W2073761981",
    "https://openalex.org/W2963121255"
  ],
  "abstract": "Automatically localizing a position based on a few natural language instructions is essential for future robots to communicate and collaborate with humans. To approach this goal, we focus on a text-to-point-cloud cross-modal localization problem. Given a textual query, it aims to identify the described location from city-scale point clouds. The task involves two challenges. 1) In city-scale point clouds, similar ambient instances may exist in several locations. Searching each location in a huge point cloud with only instances as guidance may lead to less discriminative signals and incorrect results. 2) In textual descriptions, the hints are provided separately. In this case, the relations among those hints are not explicitly described, leaving the difficulties of learning relations to the agent itself. To alleviate the two challenges, we propose a unified Relation-Enhanced Transformer (RET) to improve representation discriminability for both point cloud and nature language queries. The core of the proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism, which explicitly encodes instance (hint)-wise relations for the two modalities. Moreover, we propose a fine-grained cross-modal matching method to further refine the location predictions in a subsequent instance-hint matching stage. Experimental results on the KITTI360Pose dataset demonstrate that our approach surpasses the previous state-of-the-art method by large margins.",
  "full_text": "Text to Point Cloud Localization with Relation-Enhanced Transformer\nGuangzhi Wang1, Hehe Fan2, Mohan Kankanhalli2\n1Institute of Data Science, National University of Singapore\n2School of Computing, National University of Singapore\nguangzhi.wang@u.nus.edu, hehe.fan@nus.edu.sg, mohan@comp.nus.edu.sg\nAbstract\nAutomatically localizing a position based on a few natural\nlanguage instructions is essential for future robots to commu-\nnicate and collaborate with humans. To approach this goal,\nwe focus on the text-to-point-cloud cross-modal localization\nproblem. Given a textual query, it aims to identify the de-\nscribed location from city-scale point clouds. The task in-\nvolves two challenges. 1) In city-scale point clouds, similar\nambient instances may exist in several locations. Searching\neach location in a huge point cloud with only instances as\nguidance may lead to less discriminative signals and incor-\nrect results. 2) In textual descriptions, the hints are provided\nseparately. In this case, the relations among those hints are\nnot explicitly described, leading to the difficulties of learn-\ning relations. To overcome these two challenges, we propose\na unified Relation-Enhanced Transformer (RET) to improve\nrepresentation discriminability for both point cloud and nat-\nural language queries. The core of the proposed RET is a\nnovel Relation-enhanced Self-Attention (RSA) mechanism,\nwhich explicitly encodes instance (hint)-wise relations for the\ntwo modalities. Moreover, we propose a fine-grained cross-\nmodal matching method to further refine the location predic-\ntions in a subsequent instance-hint matching stage. Experi-\nmental results on the KITTI360Pose dataset demonstrate that\nour approach surpasses the previous state-of-the-art method\nby large margins.\nIntroduction\nUnderstanding natural language instructions in the 3D real\nworld is a fundamental skill for future artificial intelligence\nassistants to collaborate with humans. In this paper, we fo-\ncus on the outdoor environment and study the task of natural\nlanguage-based localization from city-scale point clouds. As\nshown in Figure 1, given a linguistic description of a posi-\ntion, which contains several hints, the goal of the task is to\nfind out the target location from a large-scale point cloud.\nThis task can effectively help mobile robots, such as self-\ndriving cars and autonomous drones, cooperate with humans\nto coordinate actions and plan their trajectories. By under-\nstanding the destination from natural language instructions,\nit reduces the human effort required for manual operation.\nHowever, this task is intrinsically challenging. Precise lo-\ncalization requires both correct language interpretation and\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nHeading to a place:\n[hint1] east of a dark-green terrain.\n[hint2] south of a gray road.\n[hint3] west of a dark-green traffic sign.\n[hint4] south of a green terrain.\nTextual Query\nLocalization\nFigure 1: Illustration of the text to point cloud localization\ntask. Given a textual query, which usually contains several\nindependent hints, the goal is to localize the point of interest\nin a huge city-scale point cloud.\neffective large-scale point cloud understanding. Considering\nthe difficulties, an existing method (Kolmet et al. 2022) first\ndivides a city-wide point cloud into several cells, and then\nsolves this task in a Coarse-to-Fine manner.\nThe goal of the ‘coarse’ stage is to find out the target\ncell that contains the queried location according to the given\nnatural language descriptions. In this stage, the instances\nincluded in point cloud cells and those mentioned in lan-\nguage descriptions are mainly used for text-to-point-cloud\nretrieval based on their types, without considering their rela-\ntions. In the ‘fine’ stage, each object in the textual query is\nmatched with an in-cell point cloud instance, whereby a tar-\nget location will be predicted from each hint. This pioneer-\ning method sets up a significant starting point for tackling\nthe challenging task. However, it fails to consider the intrin-\nsic relations in both stages, resulting in sub-optimal perfor-\nmance.\nFor the coarse stage, because similar ambient instances\nmay exist in several cells, performing retrieval based on only\nthe cell-contained and query-related instance types without\nconsidering their relations may lead to low discriminabil-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2501\nity for both cell and query representations, which inevitably\nleads to ambiguity. Based on those low-discriminability rep-\nresentations, it is difficult to find out the correct cell. In the\nfine stage, we observe that insufficient cross-modal collabo-\nration leads to difficulties in location refinement. Given the\nretrieved cell, precise location prediction requires joint un-\nderstanding of both point clouds and textual queries. How-\never, in the previous method (Kolmet et al. 2022), the cross-\nmodal collaboration is only performed from textual queries\nto point clouds in a single step, which results in optimization\ndifficulty for multi-task learning.\nIn this work, we aim to solve the aforementioned short-\ncomings in both stages. For the coarse stage, we pro-\npose to encode pairwise instance relations to improve rep-\nresentation discriminability for both modalities, which is\nachieved through a novel Relation-Enhanced Transformer\n(RET) architecture. In particular, the in-cell point cloud in-\nstance relations are modeled as their geometric displace-\nments, while computed as the fusion of hint representations\nin the linguistic domain. These relations from two modali-\nties are respectively incorporated into their representation in\na unified manner, which is achieved through the proposed\nRelation-enhanced Self-Attention (RSA) mechanism. For\nthe fine stage, we perform Cascaded Matching and Refine-\nment (CMR) to enhance cross-modal collaboration. In par-\nticular, different from (Kolmet et al. 2022) which achieves\nthis objective in a single step, we perform description-\ninstance matching and position refinement in two sequential\nsteps. Such formulation allows us to minimize the optimiza-\ntion difficulty of multi-objective learning and noisy interme-\ndiate results, thereby improving cross-modal collaboration.\nWe validated the effectiveness of our method on the\nKITTI360Pose benchmark (Kolmet et al. 2022). Extensive\nexperiments demonstrate that the proposed method can sur-\npass the previous approach by a large margin, leading to new\nstate-of-the-art results. Our contributions are three-fold:\n• We propose a novel Relation-Enhanced Transformer\n(RET) to improve representation discriminability for\nboth point clouds and textual queries. The core com-\nponent of RET is the Relation-enhanced Self-Attention\n(RSA) mechanism, which encodes instance (hint) rela-\ntions for the two modalities in a unified manner.\n• We propose to perform cross-modal instance matching\nand position refinement in two sequential steps. This for-\nmulation allows us to minimize the optimization diffi-\nculty of multi-task learning and the influence of noisy\nintermediate results, thereby improving cross-modal col-\nlaboration for fine-grained location prediction.\n• We perform extensive experiments on the KITTI360Pose\ndataset (Kolmet et al. 2022). The results show that our\napproach can surpass previous method by a large margin,\nresulting in new state-of-the-art performance. Additional\nablation studies further demonstrate the effectiveness of\neach component in the proposed method.\nRelated Work\nTransformer and Attention Mechanism.Transformer and\nself-attention mechanism (Vaswani et al. 2017; Fan, Yang,\nand Kankanhalli 2021) has become increasingly popular in\nrecent years. Although first proposed for natural language\nprocessing, with architectural adaptation, Transformer has\nbeen widely applied to many vision tasks including visual\nrecognition (Dosovitskiy et al. 2020; Liu et al. 2021), object\ndetection (Carion et al. 2020; Zhu et al. 2020) and seman-\ntic segmentation (Cheng, Schwing, and Kirillov 2021). Be-\nsides, the transformer-based architectures are also utilized to\nmodel cross-modal (e.g., vision and language) relations (Tan\nand Bansal 2019; Lu et al. 2019; Li et al. 2019; Zhang et al.\n2021; Li et al. 2022). In these architectures, the attention\nmechanism is widely employed to implicitly learn relations\namong the input tokens. Nevertheless, without explicit rela-\ntion encoding, the vanilla Transformer can only encode rela-\ntions implicitly with the help of positional encoding (Doso-\nvitskiy et al. 2020). To facilitate better relation modeling,\nsome works modulate the attention computation process\nby explicitly incorporating element relations. For example,\n(Wu et al. 2021) modified the attention mechanism via uni-\nfied relative position bias to improve visual recognition. For\nobject detection, spatial relations between bounding boxes\nare introduced to modulate the attention weights (Liu et al.\n2022; Gao et al. 2021). For dynamic point cloud analy-\nsis, displacement between points (Fan, Yang, and Kankan-\nhalli 2022) is utilized for point-specific attention computa-\ntion. In this work, we propose to model relations for both\npoint clouds and language queries by explicitly incorporat-\ning intra-modality relations in a unified manner.\nVisual Localization. The task that is most related to ours is\nvision-based localization (Arandjelovic et al. 2016; Brach-\nmann et al. 2017; Hausler et al. 2021), which is to estimate a\npose based on an image or image sequence. Existing meth-\nods mostly solve this task in two stages (Sarlin et al. 2019;\nSattler, Leibe, and Kobbelt 2016; Zhou et al. 2020). The first\nstage finds a subset of all images using image retrieval-based\ntechniques (Arandjelovic et al. 2016; Hausler et al. 2021;\nTorii et al. 2015), while the second stage establishes pixel-\nwise correspondence between the query image and the re-\ntrieved one to predict the precise pose. In this work, we also\nstudy the task of localization in a coarse-to-fine manner, but\ndiffer from visual localization in that: 1) we try to infer the\nlocation from city-wide point clouds instead of images. 2)\nwe try to estimate the pose from textual query rather than\nimages. Compared to visual localization, our task requires\nmulti-modal understanding and is more challenging to solve.\n3D Language Grounding. As we humans live in a 3D\nworld and communicate through natural language, recent\nwork has begun to investigate the tasks on the cross-modal\nunderstanding of 3D vision and natural language. Among\nthese tasks, the one that is most related to ours is 3D lan-\nguage grounding, which aims at localizing an object in\npoint clouds from a given natural language query. For ex-\nample, ScanRefer (Chen, Chang, and Nießner 2020) stud-\nies 3D language grounding from real-life in-door scenes.\nReferIt3D (Achlioptas et al. 2020) studies a related task un-\nder a simpler setting, which assumes the object instances\nare segmented in advance. InstanceRefer (Yuan et al. 2021)\nimproves previous methods by adopting a 3D panoptic seg-\nmentation backbone, utilizing multi-level visual context. Re-\n2502\neast of a dark-green terrain.\nsouth of a gray road.\nsouth of a green terrain. \nwest of a dark-green traffic sign. \nSplit\n...\nCells\nTextual Query  \nHint Encodernorth of a dark-green smallpole . \neast of a green pole. \n...\nInstance Encoder\nHints  \nInstances \nRelation-Enhanced \nSelf-Attention \nAdd & LayerNorm\nFeed Foward Network\nAdd & LayerNorm\nRelation-Enhanced \nSelf-Attention \nAdd & LayerNorm\nFeed Foward Network\nAdd & LayerNorm\nInstance-wise\nRelation  \nx\nx\n...\n(a) Hint-Instance Matching\n...\n...\n...\n...\nFeature\nPooling\n(b) Offset Prediction\nOffsets\nMatching\nCoarse Stage Fine Stage\nCross-modal\nFusion\nMulti-Layer\nPerceptron\nFigure 2: Framework of the proposed method. The city-scale point cloud is first divided into individual cells. Then, in the\ncoarse stage, the cells and the textual query are respectively encoded with the proposed Relation-Enhanced Transformer (RET),\nwhich are later used for query-cell matching. In the fine stage, each hint is matched with an in-cell instance. Then, cross-modal\nfusion dynamically aggregates hints and instance representations for offset prediction. The target location is predicted based on\nmatching results and offset predictions.\ncently, graph structure (Feng et al. 2021) is also utilized to\nimprove the representation learning qualities.\nMethodology\nPreliminaries\nGiven a textual query, our goal is to identify the position it\ndescribes from a city-scale point cloud. To handle the large-\nscale point cloud, we divide each scene into a set of cubic\ncells of fixed size by a preset stride. Each cell C contains a\nset of p point cloud instances, which are encoded by Point-\nNet++ (Qi et al. 2017) into vector representations {pi}p\ni=1.\nFollowing (Kolmet et al. 2022), the textual queryT is repre-\nsented as a set of hints{hj}h\nj=1, each encoding the direction\nrelation between the target location and an instance.\nInspired by the existing work (Kolmet et al. 2022), given\nthe cell splits, we solve this task in a coarse-to-fine manner\nwith two stages. The coarse stage is formulated as textual\nquery based cell retrieval. The goal of this stage is to train\na model that encodes C and T into a joint embedding space\nwhereby matched query-cell pairs are close while those un-\nmatched are pulled apart (Kiros, Salakhutdinov, and Zemel\n2014). In the fine stage, given a retrieved cell, we aim to\nrefine the position prediction by utilizing fine-grained cross-\nmodal information. In particular, we first match each hint\nin the query with an in-cell instance by formulating it as an\noptimal transport problem (Liu et al. 2020). After that, with\nthe matching results, we predict the target location through\na cross-modal fusion of point cloud instance and hint repre-\nsentations. Based on the fused representation, we predict the\ntarget location for each matched instance. Finally, we obtain\nthe target location prediction based on a weighted combi-\nnation of the matching and location prediction results. The\nframework of our method is shown in Figure 2. In the fol-\nlowing of this section, we will explain the proposed method\nfor coarse stage and fine stage. After that, our training and\ninference procedure will be detailed.\nCoarse Stage: Relation-Enhanced Transformer\nAfter the cell split, the goal of the coarse stage is to suc-\ncessfully retrieve the cell C given a textual query T . To ap-\nproach this objective, we need to encodeC and T into a joint\nembedding space. An intuitive solution is to encode both\nC and T based on the instances they contained as is done\nin (Kolmet et al. 2022). However, with such representations,\nthe low discriminability for cells and textual queries results\nin poor retrieval performance. We argue that this can be at-\ntributed to the following two reasons. On the one hand, the\noutdoor scenes are often of low diversity, whereby a group\nof mentioned instances can appear at multiple different lo-\ncations. Thus, simply describing a cell with its contained in-\nstances can result in less discriminative representations. On\nthe other hand, the textual queries often contain limited clues\ncompared to the point clouds, making this cross-modality re-\ntrieval especially challenging. To this end, we propose to ex-\nplicitly encode instance-relations to provide more discrimi-\nnative representations for both modalities.\nThe Transformer (Vaswani et al. 2017) has been widely\nutilized for relation-based representation learning in vari-\nous tasks (Hu et al. 2018; Liu et al. 2021; Fan, Yang, and\nKankanhalli 2022). The key component of the Transformer\nis the Self-Attention (SA) operation:\nAttn(Q, K, V ) =Softmax(QKT /\n√\nd)V , (1)\n2503\nPooling \nMatmul \nAdd \nFigure 3: Illustration of the proposed Relation-enhanced\nSelf-Attention (RSA) mechanism. Pairwise relations are ex-\nplicitly encoded into the value computation process.\nwhere d is the representation dimension and Q, K, V ∈\nRN×d are the query, key and value matrices by transform-\ning in-cell instances (or hints for textual queries) with corre-\nsponding linear transformations:\nQ = WQX, K = WKX, V = WV X, (2)\nwith W∗ ∈ Rd×d are learnable matrices and X = P ∈\nRp×d or H ∈ Rh×d represents stacked instances1.\nDespite its generality, the vanilla SA lacks explicit rela-\ntions in both modalities, thus is less informative to represent\nthe cell and query. To this end, we propose a novel Relation-\nEnhanced Transformer (RET) to model explicit instance re-\nlations in both point clouds and textual descriptions. Our\nRET is a stack of multiple Transformer encoder layers, ex-\ncept that, in place of SA, we propose a Relation-enhanced\nSelf-Attention (RSA) to explicitly incorporate relation in-\nformation into value computation. The computation process\nis shown as follows and illustrated in Figure 3.\nRSA(Q, K, V , R) =Softmax(QKT /\n√\nd)(V +Pool(R, 1)),\n(3)\nwhere R ∈ RN×N×d captures pairwise relations with\nRij ∈ Rd representing the relation between the i-th and j-\nth instance (hint). Pool(R, 1) indicates pooling tensor R\nalong dimension 1. In this way, our model can explicitly\nencode instance relations through this computation process,\nleading to more informative representations.\nThe definition of relation varies flexibly with task objec-\ntive and input modality. For point cloud data, we take the\ngeometric displacement of two instances as their relations,\nas direction is often mentioned in textual queries and thus\ninformative for retrieval:2\nRV\nij = WV (ci − cj), (4)\nwhere ci ∈ R3 represents the center coordinate of the i-th\ninstance and Wv ∈ Rd×3 transforms the displacement into\n1Note that the attention operation is often performed in different\nsubspaces with multiple heads, which is omitted for simplicity.\n2We have also tried other features such as number of points\nand bounding boxes of instances but didn’t observe performance\nimprovement.\nembedding space. For the linguistic description, we compute\nthe hint relation as the concatenation of their embeddings:\nRL\nij = WL[hi; hj], (5)\nwhere WL ∈ Rd×2d transforms the linguistic feature into\nrepresentation space. With the computation of RSA, the\ninstance-wise relations for different modalities can be uni-\nformly incorporated into query or cell representations\nFinally, the cell (description) representationsCm (Tm) are\nobtained via a pooling operation over all instances (hints)\noutput from the RET for cross-modal retrieval.\nFine Stage: Cascaded Matching and Refinement\nFollowing the coarse stage, we aim to refine the location pre-\ndiction within the retrieved cell in the fine stage. Inspired\nby (Kolmet et al. 2022), we perform instance matching and\nlocation refinement to utilize the fine-grained visual and lin-\nguistic information, which involves the following two objec-\ntives: (1) For each hint, we find the in-cell instance it refers\nto via a matching process. (2) For each matched pair (i, j),\na regressor predicts an offset ˆti ∈ R2 for each matched hint\nhj, which represents the offset from the instance center ci\nto the target location.3\nPrevious method (Kolmet et al. 2022) achieves the two\nobjectives within a single step. However, given the objec-\ntive of both hint-instance matching and offset prediction,\nthe multi-task learning process introduces optimization dif-\nficulty. Furthermore, in the early training steps, the matcher\nis only partially trained, which produces noisy matching re-\nsults. The regressor learns and makes predictions based on\nthis noisy results, leading to unstable learning process and\nsub-optimal performance.\nTo this end, we propose a Cascaded Matching and Refine-\nment (CMR) strategy for the fine stage, where hint-instance\nmatching and offset regression are sequentially performed.\nSpecifically, following (Kolmet et al. 2022), we first train\nthe SuperGlue (Sarlin et al. 2020) matcher for hint-instance\nmatching, which is formulated as an optimal-transport prob-\nlem. Given the trained matcher, we obtain a set of hint-\ninstance matching results {pi, hj, wi}h\nj=1, where wi repre-\nsents the confidence of the match. Then, to reduce the noise\nfor regression, we predict the target location according to\nmatched instances only.\nPrecise location prediction requires proper understand-\ning on both point cloud (what and where the referred in-\nstance is, e.g., dark-green terrain) and language de-\nscription (what is the relation between the matched instance\nand the target location, e.g., east of). For this, we pro-\npose to facilitate cross-modal collaboration via the Cross-\nAttention (CA) mechanism, which is commonly used for\ncross-modality information fusion.\nCA(H, P) =Attn(WQH, WKP, WV P), (6)\nwhere H, P represent hints and instances, respectively, and\nW∗ are learnable transformation matrices. Shortcut connec-\ntion and layer normalization (Ba, Kiros, and Hinton 2016)\n3For position prediction, we ignore the height information and\nconsiders 2D coordinates only.\n2504\nMethod\nLocalization Recall (ϵ <5/10/15m) ↑\nValidation Set Test Set\nk = 1 k = 5 k = 10 k = 1 k = 5 k = 10\nText2Pos (Kolmet et al. 2022) 0.14/0.25/0.31 0.36/0.55/0.61 0.48/0.68/0.74 0.13/0.21/0.25 0.33/0.48/0.52 0.43/0.61/0.65\nRET (Ours) 0.19/0.30/0.37 0.44/0.62/0.67 0.52/0.72/0.78 0.16/0.25/0.29 0.35/0.51/0.56 0.46/0.65/0.71\nTable 1: Performance comparison on the KITTI360Pose.\nfollows the cross-attention operation. With these operations,\nthe hint representation hi is accordingly updated to ˜hi by\ndynamically fusing visual information. As such, the infor-\nmation in the two modalities are joint utilized with the help\nof cross-modal collaboration.\nThen, we predict the offset (the direction vector from in-\nstance center to target location) from the updated hint:\nˆti = MLP(˜hj). (7)\nTo utilize the matching results, the final prediction is ob-\ntained via a weighted combination of each hint’s prediction:\nˆg =\nX\ni\nwiP\nm wm\n(ci + ˆti), (8)\nwhere wi ∈ [0, 1] is the confidence score of the match\n(pi, hj, wi) and is set to 0 for non-matched instances. To\nfilter out noisy matches, we consider only matches with con-\nfidence score greater than 0.2.\nTraining and Inference\nTraining. For the coarse stage, we train the proposed RET\nfor cross-modal retrieval with pairwise ranking loss (Kiros,\nSalakhutdinov, and Zemel 2014):\nLcoarse =\nNbX\nm=1\nNbX\nn̸=m\n[α − ⟨Cm, Tm⟩ + ⟨Cm, Tn⟩]+\n+\nNbX\nm=1\nNbX\nn̸=m\n[α − ⟨Tm, Cm⟩ + ⟨Tm, Cn⟩]+,\n(9)\nwhere Nb is the batch size, α is a hyper-parameter to con-\ntrol the separation strength and⟨·, ·⟩ represents inner product\nbetween vectors. This loss function encourages the represen-\ntation of matched description-cell pair to be by a margin α\ncloser than those unmatched. For the fine stage, we employ\nthe loss in (Sarlin et al. 2020) to train the matcher, while L2\nloss is applied to train the offset regressor.\nInference. We first encode all cells and queries into a joint\nembedding space with the proposed Relation-Enhanced\nTransformer. Then, for each query representation, we re-\ntrieve top-k cells with highest similarity. For each retrieved\ncell, we use the SuperGlue matcher trained in the fine stage\nto match each hint with an in-cell instance, which is fol-\nlowed by offset prediction based on the fused representa-\ntions. Finally, the position prediction is given by Eq. 8.\nExperiments\nDataset and Implementation Details\nDataset Details. We evaluate our method on the recently\nproposed KITTI360Pose dataset (Kolmet et al. 2022), which\nis built upon the KITTI360 dataset (Liao, Xie, and Geiger\n2021) with sampled locations and generated hints. It con-\ntains point clouds of a total of 9 scenes, covering 14,934\npositions with a total area of 15.51km 2. We follow (Kol-\nmet et al. 2022) to use five scenes for training, one for val-\nidation, and the remaining three for testing. We sample the\ncells of size 30m with a stride of 10m. For more details on\nthe dataset preprocessing, please refer to our supplementary\nmaterial.\nImplementation Details For the coarse stage, we trained\nthe model with AdamW optimizer (Loshchilov and Hutter\n2018) with a learning rate of 2e-4. The models are trained\nfor a total of 18 epochs while the learning rate is decayed\nby 10 at the 9-th epoch. The α is set to 0.35. For the fine\nstage, we first train the matcher with a learning rate of 5e-\n4 for a total of 16 epochs. Afterwards, we fix the matcher\nand train the regressor based on the matching results for 10\nepochs with a learning rate of 1e-4. The regressor is for-\nmulated as a 3 layer Multi-Layer Perceptron. Both of the\ntwo steps adopt an Adam (Kingma and Ba 2014) optimizer.\nThe RET has 2 encoder layers for both point cloud part and\nlinguistic part, each utilizing the Relation-enhanced Atten-\ntion (RSA) mechanism with 4 heads and hidden dimension\n2048. For the two stages, we encode each instance in the cell\nwith PointNet++ (Qi et al. 2017) provided by Text2Pos (Kol-\nmet et al. 2022) for a fair comparison. The hint representa-\ntions are obtained by concatenating learned word embed-\ndings. More details are provided in our appendix.4\nComparison with the State-of-the-art\nWe compared our method with Text2Pos (Kolmet et al.\n2022) on the KITTI360Pose dataset. Following (Kolmet\net al. 2022), we report top-k (k = 1/5/10) recall rate of dif-\nferent error ranges ϵ <5/10/15m for comprehensive com-\nparison. The results are shown in Table 1. Text2Pos gives\na recall of 0.14 when k = 1 and ϵ < 5m. In contrast,\nour method can significantly improve the recall rate to 0.19,\nwhich amounts to 35.7% relative improvement upon the\nbaseline. Furthermore, when we relax the localization error\nconstraints or increase k, consistent improvements upon the\nbaseline can also be observed. For example, with ϵ <5m,\nour method achieves top-5 recall rate of 0.44, which is 8%\nhigher than previous state-of-the-art. Similar improvements\ncan also be seen on the test set, showing our method is su-\nperior to the baseline method.\n4Code available at: https://github.com/daoyuan98/text2pos-ret\n2505\nMethod k = 1↑ k = 3↑ k = 5↑\nw/o both relations 0.11 0.24 0.32\nw/o linguistic relation 0.14 0.28 0.37\nw/o visual relation 0.16 0.30 0.40\nFull (Ours) 0.18 0.34 0.44\nTable 2: Ablation study of the Relation-Enhanced Trans-\nformer (RET) on KITTI360Pose validation set. ”wo X rela-\ntion” indicates replacing the proposed RSA with the vanilla\nSelf-Attention in the corresponding modality.\nAblation Studies\nIn this section, we perform ablation studies for both stages\nto investigate the effectiveness of each proposed component\nin our method. The ablation studies for coarse stage and fine\nstage are provided separately for clear investigation.\nCoarse Stage. We study the importance of explicit relation\nincorporation in the coarse stage. Since the coarse stage is\nformulated as a retrieval task, we use top-1/3/5 recall rate as\nevaluation metric, whereby the cell that contains the ground\ntruth location is defined as positive.\nRelation Incorporation.We first study the necessity of ex-\nplicit relation modeling for both point cloud and textual\nqueries. The results are shown in Table 2. It can be observed\nthat relation modeling contributes significantly to successful\nretrieval. In particular, without any relation incorporation,\nthe top-5 recall rate is 0.32. With the explicit fusion of lin-\nguistic relation, we observe an increase of 0.05 recall rate\nunder same condition. Besides, with the incorporation of vi-\nsual (point cloud instance) relations only, the top-5 recall\nrate can be improved by 0.08, indicating explicit relations\nin the point clouds play a more important role. Finally, with\nboth relations, we achieve an improvement of 0.12 at top-5\nrecall rate upon that without any relation, showing that both\nvisual and linguistic relations are necessary and complemen-\ntary to improve the cell retrieval performance.\nRET Hyper-parameters.We also studied the importance of\nthe hyper-parameters involved in RET, namely the number\nof layers of RET and the number of heads of RSA. The re-\nsults are shown in Table 3. It can be observed that, thanks to\nthe strong relation modeling capacity of the proposed RET,\nwe can obtain the best performance with 2 layers and 4 heads\nin the RSA. Decreasing and increasing the number of layers\nboth lead to worse performance, which may be attributed to\nunderfitting and overfitting, respectively.\nFine Stage. The objective of the fine stage is to correctly\nmatch linguistic hints and point cloud instances and regress\nthe target location. Thus, we study the performance of the\nmatcher and regressor, respectively.\nMatcher. Following (Sarlin et al. 2020), we take precision\nand recall as the the evaluation metric of the matcher. With\nan identical matcher architecture, we investigate the impact\nof training strategy on the matcher performance. The results\nare shown in Table 4. It can be seen that compared with joint\ntraining (Kolmet et al. 2022), our cascaded training achieves\nnot only high precision and recall in the training set, but\nalso stronger generalization on the validation set. The re-\n#Layers #Heads k = 1↑ k = 3↑ k = 5↑\n1 4 0.16 0.31 0.40\n1 8 0.16 0.30 0.40\n2 2 0.17 0.32 0.42\n2 4 0.18 0.34 0.44\n2 8 0.16 0.31 0.40\n3 4 0.16 0.32 0.39\n3 8 0.15 0.29 0.37\nTable 3: The effects of #layers of RET and #heads of RSA.\nStrategy Train Validation\nPrecision ↑ Recall ↑ Precision ↑ Recall ↑\njoint 98.12 98.16 86.67 87.59\ncascade(ours) 98.89 99.04 92.18 93.01\nTable 4: Comparison of training strategy and matcher per-\nformance on the KITTI360Pose dataset.\nsults demonstrate that the cascade training strategy is able to\nmitigate the multi-task optimization difficulty.\nRegressor. The regressor predicts the target location based\non the the matching results. We study the effects of cas-\ncaded training, cross-attention based cross-modal fusion and\nconfidence weighting for final location prediction. We use\nregression error as evaluation metric and compare different\nversions on both KITTI360Pose training and validation set.\nThe results are shown in Table. 5. Without cascaded training\nstrategy, the regressor achieves an error of 10.24 and 10.01\non the training and validation set, respectively, which is 1.72\nand 0.86 higher than that with cascaded training. This re-\nsult suggests that our cascaded training strategy also allevi-\nates the optimization difficulty of the regressor, which was\ncaused by the noisy intermediate results. Furthermore, with-\nout cross-attention mechanism, the regression error also in-\ncreases by a considerable margin, showing that cross-modal\ncollaboration is important for precise location prediction. Fi-\nnally, with confidence-based weighting, we can further re-\nduce the regression error on both the training and validation\nset, suggesting this information from the trained matcher can\nbe further utilized to improve performance.\nVisualizations\nEmbedding Space Visualization. We visualize the learned\nembedding space via T-SNE (Van der Maaten and Hin-\nMethod Train Error ↓ Validation Error ↓\nw/o cascade training 10.24 (+1.72) 10.01 (+0.86)\nw/o cross-attention 9.57 (+1.05) 9.56 (+0.41)\nw/o confidence weighting 9.02 (+0.50) 9.23 (+0.08)\nOurs 8.52 9.15\nTable 5: Ablation study on the regression error of the fine-\nstage on the KITTI360Pose dataset.\n2506\nGround Truth Top-1 Top-2 Top-3 Ground Truth Top-1 Top-2 Top-3\n(a) (b) \n(c) \n(e) \n(d) \n(f) \n557.85 10.00 20.00 \n0.00 10.00 50.99 \n10.00 0.0 819.08 \n10.00 0.00 64.03 \n14.14 21 1.90 221.36 \n455.41 1 150.00 218.40 \nBuilding Pole Traffic Light Traffic Sign Parking Sidewalk Vegetation Terrain Road Wall Garage\nFigure 4: Qualitative retrieval results on KITTI360Pose validation set. The red dot in the ground truth cell indicates the target\nlocation. In each retrieved cell, the number in the lower right indicates the center distance between this cell and the ground\ntruth. Green box indicates positive cell which contains the target location, while red box indicates negative cells.\nton 2008) in Figure 5. It can be observed that the base-\nline method Text2Pos (Kolmet et al. 2022) results in a less\ndiscriminative space, where positive cells are relatively far\naway from the query and sometimes separated across the\nembedding space. In contrast, our method draw positive cell\nand query representations closer in the embedding space, re-\nsulting in a more informative embedding space for retrieval.\nText2PosOurs\nTextual Query Negative CellPositive Cell\nFigure 5: T-SNE visualization of embedding space for the\ncoarse stage. A cell is considered as positive if it contains\nthe location described by the query. Compared with baseline\nmethod (Kolmet et al. 2022), our method can produce better\nrepresentation where positive cells are closer to the target.\nQualitative Cell Retrieval Results. We show some exam-\nple text to point cloud retrieval results in Figure. 4. For a\ngiven query, we visualize the top-3 retrieved cells. A re-\ntrieved cell is defined as positive if it contains the target lo-\ncation. It can be observed that, our method can retrieve the\nground truth cell or those close in most cases. Sometimes,\nnegative cells can also be retrieved, e.g., top-1 in (a) and\ntop-3 in (e). It can be seen that these retrieved negative cells\nexhibit high semantic similarity with the ground truth cell,\neven though far away from it. We also show a failure case (f),\nwhere the retrieved cells are all negative. It can be seen that\neven though far away from the target location, all these neg-\native cells have instances similar to the ground truth. These\nobservations suggest that outdoor scenes are indeed of low\ndiversity, indicating that successful retrieval requires highly\ndiscriminative representations to disambiguate the cells.\nConclusion\nIn this work, we proposed a novel method for precise\ntext-based localization from large-scale point clouds. Our\nmethod employs a coarse-to-fine principle and pipelines this\nprocess into two stages. For the coarse stage which is formu-\nlated as a textual query based cell retrieval task, we aim to\nimprove representation discriminability for both point cloud\nand query representations. This is achieved through explicit\nmodeling of instance relations and implemented via a newly\nproposed Relation-Enhanced Transformer (RET). The core\nof RET is a novel Relation-enhanced Self-Attention (RSA)\nmechanism, whereby the instance relations for the two\nmodalities are explicitly incorporated into the value com-\nputation process in a unified manner. For the fine stage,\nour method performs description-instance matching and\nposition refinement in a cascaded way, whereby cross-\nmodal information collaboration is enhanced through the\ncross-attention mechanism. Extensive experiments on the\nKITTI360Pose dataset validated the effectiveness of the pro-\nposed method, which achieves new state-of-the-art perfor-\nmance. Additional ablation studies further corroborate the\neffectiveness of each component in the proposed method.\n2507\nAcknowledgements\nThis research is supported by the National Research Foun-\ndation, Singapore under its Strategic Capability Research\nCentres Funding Initiative. Any opinions, findings and con-\nclusions or recommendations expressed in this material are\nthose of the author(s) and do not reflect the views of National\nResearch Foundation, Singapore.\nReferences\nAchlioptas, P.; Abdelreheem, A.; Xia, F.; Elhoseiny, M.; and\nGuibas, L. 2020. Referit3d: Neural listeners for fine-grained\n3d object identification in real-world scenes. In ECCV.\nSpringer.\nArandjelovic, R.; Gronat, P.; Torii, A.; Pajdla, T.; and Sivic,\nJ. 2016. NetVLAD: CNN architecture for weakly supervised\nplace recognition. In CVPR.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBrachmann, E.; Krull, A.; Nowozin, S.; Shotton, J.; Michel,\nF.; Gumhold, S.; and Rother, C. 2017. Dsac-differentiable\nransac for camera localization. In CVPR.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV. Springer.\nChen, D. Z.; Chang, A. X.; and Nießner, M. 2020. Scanrefer:\n3d object localization in rgb-d scans using natural language.\nIn ECCV.\nCheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel\nclassification is not all you need for semantic segmentation.\nNeurIPS.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In\nICLR.\nFan, H.; Yang, Y .; and Kankanhalli, M. 2022. Point spatio-\ntemporal transformer networks for point cloud video model-\ning. TPAMI.\nFan, H.; Yang, Y .; and Kankanhalli, M. S. 2021. Point\n4D Transformer Networks for Spatio-Temporal Modeling in\nPoint Cloud Videos. In CVPR.\nFeng, M.; Li, Z.; Li, Q.; Zhang, L.; Zhang, X.; Zhu, G.;\nZhang, H.; Wang, Y .; and Mian, A. 2021. Free-form descrip-\ntion guided 3d visual graph network for object grounding in\npoint cloud. In ICCV.\nGao, P.; Zheng, M.; Wang, X.; Dai, J.; and Li, H. 2021.\nFast Convergence of DETR With Spatially Modulated Co-\nAttention. In ICCV.\nHausler, S.; Garg, S.; Xu, M.; Milford, M.; and Fischer, T.\n2021. Patch-netvlad: Multi-scale fusion of locally-global de-\nscriptors for place recognition. In CVPR.\nHu, H.; Gu, J.; Zhang, Z.; Dai, J.; and Wei, Y . 2018. Relation\nNetworks for Object Detection. In CVPR.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKiros, R.; Salakhutdinov, R.; and Zemel, R. S. 2014. Uni-\nfying visual-semantic embeddings with multimodal neural\nlanguage models. arXiv preprint arXiv:1411.2539.\nKolmet, M.; Zhou, Q.; Osep, A.; and Leal-Taixe, L. 2022.\nText2Pos: Text-to-Point-Cloud Cross-Modal Localization.\nIn CVPR.\nLi, G.; Zhu, L.; Liu, P.; and Yang, Y . 2019. Entangled Trans-\nformer for Image Captioning. In ICCV.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Boot-\nstrapping Language-Image Pre-training for Unified Vision-\nLanguage Understanding and Generation. In ICML.\nLiao, Y .; Xie, J.; and Geiger, A. 2021. KITTI-360: A Novel\nDataset and Benchmarks for Urban Scene Understanding in\n2D and 3D. arXiv preprint arXiv:2109.13410.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2022. DAB-DETR: Dynamic Anchor Boxes\nare Better Queries for DETR. In ICLR.\nLiu, Y .; Zhu, L.; Yamada, M.; and Yang, Y . 2020. Seman-\ntic Correspondence as an Optimal Transport Problem. In\nCVPR.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight De-\ncay Regularization. In ICLR.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. NeurIPS.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017. Pointnet++:\nDeep hierarchical feature learning on point sets in a metric\nspace. NeurIPS.\nSarlin, P.-E.; Cadena, C.; Siegwart, R.; and Dymczyk, M.\n2019. From coarse to fine: Robust hierarchical localization\nat large scale. In CVPR.\nSarlin, P.-E.; DeTone, D.; Malisiewicz, T.; and Rabinovich,\nA. 2020. Superglue: Learning feature matching with graph\nneural networks. In CVPR.\nSattler, T.; Leibe, B.; and Kobbelt, L. 2016. Efficient & ef-\nfective prioritized matching for large-scale image-based lo-\ncalization. TPAMI.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nEMNLP-IJCNLP.\nTorii, A.; Arandjelovic, R.; Sivic, J.; Okutomi, M.; and Pa-\njdla, T. 2015. 24/7 place recognition by view synthesis. In\nCVPR.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. JMLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. NeurIPS.\nWu, K.; Peng, H.; Chen, M.; Fu, J.; and Chao, H. 2021. Re-\nthinking and improving relative position encoding for vision\ntransformer. In ICCV.\n2508\nYuan, Z.; Yan, X.; Liao, Y .; Zhang, R.; Wang, S.; Li, Z.;\nand Cui, S. 2021. Instancerefer: Cooperative holistic under-\nstanding for visual grounding on point clouds through in-\nstance multi-level contextual referring. In ICCV.\nZhang, H.; Sun, A.; Jing, W.; Nan, G.; Zhen, L.; Zhou, J. T.;\nand Goh, R. S. M. 2021. Video Corpus Moment Retrieval\nwith Contrastive Learning. In SIGIR.\nZhou, Q.; Sattler, T.; Pollefeys, M.; and Leal-Taixe, L. 2020.\nTo learn or not to learn: Visual localization from essential\nmatrices. In ICRA.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR.\n2509",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7733081579208374
    },
    {
      "name": "Discriminative model",
      "score": 0.6718272566795349
    },
    {
      "name": "Point cloud",
      "score": 0.6180904507637024
    },
    {
      "name": "Matching (statistics)",
      "score": 0.517022967338562
    },
    {
      "name": "Transformer",
      "score": 0.49593719840049744
    },
    {
      "name": "Relation (database)",
      "score": 0.46711620688438416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45909246802330017
    },
    {
      "name": "Cloud computing",
      "score": 0.4412469267845154
    },
    {
      "name": "Representation (politics)",
      "score": 0.4281364977359772
    },
    {
      "name": "Modal",
      "score": 0.4205835461616516
    },
    {
      "name": "Point (geometry)",
      "score": 0.41622117161750793
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3672410249710083
    },
    {
      "name": "Natural language processing",
      "score": 0.3416312038898468
    },
    {
      "name": "Data mining",
      "score": 0.2287648618221283
    },
    {
      "name": "Mathematics",
      "score": 0.09751477837562561
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}