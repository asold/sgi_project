{
  "title": "Cognitive bias in clinical large language models",
  "url": "https://openalex.org/W4412164899",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2551604318",
      "name": "Arjun Mahajan",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A847559205",
      "name": "Ziad Obermeyer",
      "affiliations": [
        "Berkeley Public Health Division",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2121270349",
      "name": "Jenna Lester",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2184942774",
      "name": "Dylan Powell",
      "affiliations": [
        "University of Stirling"
      ]
    },
    {
      "id": "https://openalex.org/A2551604318",
      "name": "Arjun Mahajan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A847559205",
      "name": "Ziad Obermeyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121270349",
      "name": "Jenna Lester",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184942774",
      "name": "Dylan Powell",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384524772",
    "https://openalex.org/W4406307660",
    "https://openalex.org/W4408314709",
    "https://openalex.org/W4403601954",
    "https://openalex.org/W4407570061",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4404350936",
    "https://openalex.org/W4404136902",
    "https://openalex.org/W4404518047",
    "https://openalex.org/W4392490008",
    "https://openalex.org/W4402186969",
    "https://openalex.org/W4408165783",
    "https://openalex.org/W4403757297",
    "https://openalex.org/W4293998609",
    "https://openalex.org/W6910899885",
    "https://openalex.org/W4405173927",
    "https://openalex.org/W4407691270",
    "https://openalex.org/W6967157550",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4407960574"
  ],
  "abstract": "Cognitive bias accounts for a significant portion of preventable errors in healthcare, contributing to significant patient morbidity and mortality each year. As large language models (LLMs) are introduced into healthcare and clinical decision-making, these systems are at risk of inheriting – and even amplifying – these existing biases. This article explores both the cognitive biases impacting LLM-assisted medicine and the countervailing strengths these technologies bring to addressing these limitations.",
  "full_text": "npj |digital medicine News & Views\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01790-0\nCognitive bias in clinical large\nlanguage models\nArjun Mahajan, Ziad Obermeyer, Roxana Daneshjou, Jenna Lester & Dylan Powell\n Check for updates\nCognitive bias accounts for a signiﬁcant portion of\npreventable errors in healthcare, contributing to\nsigniﬁcant patient morbidity and mortality each year.\nAs large language models (LLMs) are introduced into\nhealthcare and clinical decision-making, these\nsystems are at risk of inheriting– and even amplifying\n– these existing biases. This article explores both the\ncognitive biases impacting LLM-assisted medicine\nand the countervailing strengths these technologies\nbring to addressing these limitations.\nIn modern healthcare, cognitive biases represent a persistent and per-\nvasive challenge to effective clinical decision-making. These unconscious\npatterns of thinking can lead physicians astray, resulting in misdiagnoses,\ndelayed treatments, and ultimately, compromised patient outcomes\n1.\nDiagnostic and medical errors account for up to 40,000–80,000 pre-\nventable deaths in the United States yearly, with cognitive biases impli-\ncated in as many as 40–80% of these cases\n1,2. As large language models\n(LLMs) are introduced into healthcare and clinical decision-making,\nthese systems are at risk of inheriting—and even amplifying—these\nexisting biases\n3. At the same time, their capabilities in contextual and\nadaptive reasoning may offer unique opportunities to detect and mitigate\ncognitive biases\n4. This article explores both the cognitive biases impacting\nLLM-assisted medicine and the countervailing strengths these technol-\nogies bring to addressing these limitations—a critical examination for\nensuring these technologies ultimately improve rather than undermine\npatient care.\nKey causes of cognitive biases in clinical LLMs\nBiases affecting clinical LLM systems can arise at multiple stages, including\ndata-related biases from collection andrepresentation, model-related biases\nfrom algorithm design and training, and deployment-related biases stem-\nming from real-world use and feedback\n5.C o g n i t i v eb i a s e s—here referring to\nsystematic deviations from rational reasoning that affect clinical decision-\nmaking—can interact with and emerge at each of these stages3,5.F o r\ninstance, these biases can enter LLM systems through incomplete or skewed\ntraining data (e.g., datasets that underrepresent certain patient populations),\nincorporation ofﬂawed heuristics into algorithms (e.g., diagnostic rules that\noverlook atypical symptom presentations in certain groups), or deployment\nin contexts that amplify existing healthcare disparities\n5,6. The results are\ntools that not only inherit these clinical reasoningﬂaws but potentially\nmagnify them through automation3,6. Clinical LLM applications may\ncommonly encounter several notable cognitive biases, though many oth-\ners exist:\n• Suggestibility bias (prioritizing user agreement over independent rea-\nsoning) can lead LLMs to adopt incorrect answers when confronted\nwith persuasive but inaccurate prompts. This can emerge from rein-\nforcement learning methods that optimize for user satisfaction metrics\nor from training approaches where agreement with user inputs is\ninadvertently rewarded more than factual correctness\n6. For example,\nwhen presented with conﬁdent-sounding rebuttals—particularly those\nciting external sources—LLMs frequently revised correct diagnostic\nanswers to align with user suggestions, even when doing so meant\nsacriﬁcing accuracy\n7.\n• Availability bias(relying on the most easily recalled or commonly seen\ninformation) can inﬂuence LLM-driven decision support when\ntraining data contains overrepresented clinical patterns or patient\nproﬁles, causing models to give disproportionate weight to common\nexamples in their corpus6. In an experiment with four commercial\nLLMs, each model recommended outdated race-adjusted equations for\nestimating eGFR and lung capacity—guidance no longer supported by\nevidence—demonstrating how the prevalence and recallability of race-\nbased formulas in the training corpus became the models’ default\nadvice8.\n• Conﬁrmation bias (seeking evidence that supports initial hypotheses)\ncan emerge in clinical LLMs in both development and deployment\nstages. During development, conﬁrmation bias can be encoded when\ntraining labels, such as those used in supervisedﬁne-tuning, reinforce\nprevailing clinical assumptions or when model evaluation metrics\nfavor agreement with existing diagnostic patterns\n5. At deployment, the\nbias can manifest in human-model interactions when interfaces are\ndesigned to highlight outputs that match clinicians ’ initial\nimpressions5. In one study, pathology experts were signiﬁcantly more\nlikely to keep an erroneous tumor‑cell‑percentage estimate when an\nequivalently incorrect LLM recommendation aligned with their\npreliminary judgment, illustrating how human and model errors can\nco‑reinforce rather than correct one another\n9.\n• Framing bias (inﬂuence of presentation or wording on decision-\nmaking) can affect LLM-enabled systems when the same clinical\ninformation presented in different ways leads to different model\noutputs. This may occur when LLMs learn language patterns where\ncertain descriptive words or presentation formats are statistically\nassociated with particular clinicalconclusions in their training data\n10.\nOne study found that GPT-4’s diagnostic accuracy declined when\nclinical cases were reframed with disruptive behaviors or other salient\nbut irrelevant details—mirroring the effects of framing on human\nclinicians and highlighting the model’s susceptibility to the same\ncognitive distortion\n10.\n• Anchoring bias (relying on early information in making decisions) can\nsurface in LLM-enabled diagnosisw h e ne a r l yi n p u to ro u t p u td a t a\nbecomes the LLM’sc o g n i t i v e“anchor” for subsequent reasoning. This\neffect can emerge when LLMs predominantly process information\nnpj Digital Medicine|           (2025) 8:428 1\n1234567890():,;\n1234567890():,;\nsequentially (autoregressive processing), generating each part of their\nresponse based on what came before, giving more weight to earlier-\nformed hypotheses when interpreting new information\n11. In a study of\nchallenging clinical vignettes, GPT-4 generated incorrect initial\ndiagnoses that consistently inﬂuenced its later reasoning, until a\nstructured multi-agent setup was introduced to challenge that anchor\nand improve diagnostic accuracy\n11.\nWhen medical experts transfer their cognitive biases to AI systems\nthrough training data, validation processes, deployment strategies, or\nreal-time interactions during prompting, these systems risk amplifying\nrather than reducing clinical errors, potentially embedding human\ncognitive limitations into ostensibly objective computational tools. Yet,\nit is also worth considering if certain forms of cognitive bias—such as\nsuggestibility—might support the adaptability and responsiveness that\ngive large language models clinical utility, raising the question of whe-\nther zero bias is always optimal. At the same time, it is important to\nrecognize that not all model failures are reﬂections of cognitive bias—for\ninstance, large language models may also generate content that departs\nentirely from clinical fact, an effect better described as hallucination\n(Fig. 1).\nExploring the potential of LLMs as tools to mitigate bias\nWhile LLMs are susceptible to several important cognitive biases, they may\nalso offer enhancedﬂexibility compared to traditional AI systems through\ntheir ability to examine their own outputs, apply structured assessment\ncriteria, and adjust recommendations accordingly\n4,12. Their capacity for self-\ncorrection, integration of clinical context, and transparent step-by-step\nreasoning may create opportunities to detect and reduce cognitive bias in\nclinical decision making.\nSelf-reﬂection\nThrough a structured internal review process for each recommendation\n(which may be speciﬁed for the task or generalized), LLMs have demon-\nstrated the potential to systematically detect when their assessments may be\nskewed by common reasoning errors12. For instance, in clinical simulation, a\nsequential‑prompting framework that asked GPT‑4 to debate and revise its\ndiagnostic impressions cut anchoring errors and improved accuracy on\nmisdiagnosis‑prone cases compared with a single‑pass approach\n10.I no n e\nradiology-diagnosis experiment, a two-step prompt thatﬁrst required the\nmodel to summarize keyﬁndings and only then generate a differential\noutperformed a single-pass prompt and reduced anchoring on the initial\nimpression\n13.T h i sr eﬂective and iterative approach may allow LLMs to re-\nevaluate initial conclusions and highlight potential blind spots in medical\nreasoning that might otherwise go unnoticed - whether triggered autono-\nmously or through interactive prompting by the clinician.\nContextual reasoning\nAn LLM’s contextual reasoning ability—its capacity to read an entire case\nnote, a patient’s longitudinal history, and relevant guidelines in one pass and\ninterpret each detail in relation to the others—may help it surface and\ncorrect cognitive biases6,14. By analyzing sentiment, content structure, tra-\ncing how conclusions evolve across encounters, and benchmarking clinical\ndecisions against evidence-based practices, LLMs may help uncover subtly\nbiased language, anchoring when anearly hunch dominates later doc-\numentation, and highlight omissions that signal premature closure6,14.F o r\ninstance, in a cross‑sectional analysis of emergency‑department and dis-\ncharge notes, GPT‑4 accuratelyﬂagged stigmatizing and judgmental lan-\nguage and revealed that such documentation was disproportionately\ndirected at Black, transgender, and unstably housed patients15. Utilizing this\ncontextual reasoning within clinical decision‑support pipelines may\nFig. 1 |Select cognitive biases in clinical large lan-\nguage models.\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:428 2\npromote more balanced, evidence‑aligned reasoning and may offer practical\nsafeguards against the spectrum of cognitive biases that can distort clin-\nical care.\nTransparent insights\nLLMs are often capable of producing a“reasoning trace” for each answer,\npurporting to give reviewers a step-by-step map of how a conclusion was\nreached\n16. These transparent traces may serve as audit hooks—text\nsnapshots that other tools (or clinicians) can scan for factual accuracy,\nlogical soundness and bias\n16,17. Although studies have demonstrated that\nsuch traces may sometimes represent plausible post-hoc explanations\nrather than a literal account of the model’s internal reasoning, they may\nstill offer value:ﬂaws in these narratives—such as contradictions, omis-\nsions, or unsupported logic—can correlate with incorrect or biased out-\nputs and provide a practical entry point for external audit or clinician\noversight\n17,18. This added layer of transparency may support more\ninformed review and error detection before model outputs inﬂuence\nclinical decision making.\nChallenges and limitations\nDespite their potential, self-reﬂective LLMs face challenges to imple-\nmentation. Generalized self-evaluation frameworks may not be compre-\nhensive enough to mitigate all typesof reasoning errors, and developing\neffective frameworks for individualized clinical tasks may be resource\nintensive\n19. Further, LLMs’ biases may exist as subtle contextual or statistical\npatterns that may be difﬁcult to detect with conventional auditing\napproaches3,19. Compounding these methodological concerns, much of the\ncurrent literature evaluating LLM biasand performance relies on standar-\ndized or simulated clinical vignettes rather than real-world clinical inter-\nactions or data, raising important questions about generalizability to\npractical clinical environments11,20. As LLMs continue to develop, there is a\ncritical need for ongoing human oversight by diverse clinical experts who\ncan validate LLM outputs and reasoning against established clinical stan-\ndards and emerging best practices\n20. Further research is also needed to\nevaluate the effectiveness and reliability of clinician oversight in auditing\nLLM reasoning and outputs in real-world clinical settings. Additionally,\nregulatory frameworks must evolve to address the unique challenges of\nimplementing continuously evolving and self-reﬂective AI in clinical\nenvironments\n21,22. Most importantly, models must maintain rigorous safe-\nguards to protect patient safety and engender a sense of trust in patients and\nclinicians to ensure adoption.\nConclusion\nLLMs offer unique approaches to cognitive bias detection in clinical settings\nthrough self-reﬂection, structured reasoning frameworks, and context-\naware analysis, though questions remain about their practical imple-\nmentation within established healthcare workﬂows and decision processes.\nThe intersection of AI and clinical reasoning creates an evolving landscape\nwhere traditional biases may be transformed rather than eliminated,\nopening avenues for research on novelcognitive reasoning strategies that\nneither uncritically embrace technology nor dismiss its potential beneﬁts.\nData availability\nNo datasets were generated or analyzed during the current study.\nArjun Mahajan1,Z i a dO b e r m e y e r2, Roxana Daneshjou3, Jenna Lester4 &\nDylan Powell5\n1Harvard Medical School, Boston, MA, USA.2School of Public Health,\nUniversity of California, Berkeley, Berkeley, CA, USA.3Department of\nBiomedical Data Science, Stanford University, Stanford, CA, USA.\n4Department of Dermatology, University of California, San Francisco, San\nFrancisco, CA, USA.5Faculty of Health Sciences & Sport, University of\nStirling, Stirling, UK. e-mail: dylan.powell@stir.ac.uk\nReceived: 7 May 2025; Accepted: 10 June 2025;\nReferences\n1. Newman-Toker, D. E. et al. Burden of serious harms from diagnostic error in the USA.BMJ\nQual. Saf.33, 109– 120 (2024).\n2. Rodziewicz, T. L., Houseman, B. & Vaqar Sarosh and Hipskind, J. E. Medical error reduction\nand prevention. InStatPearls (StatPearls Publishing, 2025).\n3. Laura, Z. Cognitive bias in large language models: implications for research and practice.\nNEJM AI1, AIe2400961 (2024).\n4. Emma, P. et al. Using large language models to promote health equity.NEJM AI2, AIp2400889\n(2025).\n5. Hasanzadeh, F. et al. Bias recognition and mitigation strategies in artiﬁcial intelligence\nhealthcare applications.NPJ Digit Med.8, 154 (2025).\n6. Schmidgall, S. et al. Evaluation and mitigation of cognitive biases in medical language models.\nNPJ Digit Med.7, 295 (2024).\n7. Fanous, A. et al. SycEval: evaluating LLM sycophancy. Preprint athttps://doi.org/10.48550/\narXiv.2502.08177 (2025).\n8. Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R. Large language models\npropagate race-based medicine.NPJ Digit Med.6, 195 (2023).\n9. Rosbach, E. et al. When two wrongs don’t make a right”– examining conﬁrmation bias and the\nrole of time pressure during human-AI collaboration in computational pathology. Preprint at\nhttps://doi.org/10.48550/arXiv.2411.01007 (2025).\n10. Schmidt, H. G., Rotgans, J. I. & Mamede, S. Bias sensitivity in diagnostic decision-making:\ncomparing ChatGPT with residents.J. Gen. Intern. Med.40, 790– 795 (2025).\n11. Ke, Y. et al. Mitigating cognitive biases in clinical decision-making through multi-agent\nconversations using large language models: simulation study.J. Med. Internet Res.26, e59439\n(2024).\n12. Echterhoff, J., Liu, Y., Alessa, A., McAuley, J. & He, Z. Cognitive bias in decision-making with\nLLMs. Preprint athttps://doi.org/10.48550/arXiv.2403.00811 (2025).\n13. Sonoda, Y. et al. Structured clinical reasoning prompt enhances LLM’s diagnostic capabilities\nin diagnosis please quiz cases. Preprint athttps://doi.org/10.1101/2024.09.01.24312894\n(2024).\n14. Xie, Q. et al. Medical foundation large language models for comprehensive text analysis and\nbeyond. NPJ Digit Med.8, 141 (2025).\n15. Apakama, D. U. et al. Identifying and characterizing bias at scale in clinical notes using large\nlanguage models. Preprint athttps://doi.org/10.1101/2024.10.24.24316073 (2024).\n16. Creswell, A. & Shanahan, M. Faithful reasoning using large language models. Preprint at\nhttps://doi.org/10.48550/arXiv.2208.14271 (2025).\n17. Moell, B., Aronsson, F. S. & Akbar, S. Medical reasoning in LLMs: an in-depth analysis of\nDeepSeek R1. Preprint athttps://doi.org/10.48550/arXiv.2504.00016 (2025).\n18. Meinke, A. et al. Frontier models are capable of in-context scheming. Preprint athttps://doi.\norg/10.48550/arXiv.2412.04984 (2025).\n19. Wang, W. et al. A survey of LLM-based agents in medicine: how far are we from Baymax?\nPreprint athttps://doi.org/10.48550/arXiv.2502.11211 (2025).\n20. Al-Garadi, M. et al. Large language models in healthcare. Preprint athttps://doi.org/10.48550/\narXiv.2503.04748 (2025).\n21. Meskó, B. & Topol, E. J. The imperative for regulatory oversight of large language models (or\ngenerative AI) in healthcare.NPJ Digit Med.6, 120 (2023).\n22. Mahajan, A. & Powell, D. Generalist medical AI reimbursement challenges and opportunities.\nNPJ Digit Med.8, 125 (2025).\nAuthor contributions\nA.M. developed the concept and wrote theﬁrst draft and amended theﬁnal version. Z.O., R.D., J.L.,\nand D.P. provided oversight in drafting and editing of the manuscript. All authors read and approved\nthe ﬁnal manuscript.\nCompeting interests\nThe authors declare no competing interests. D.P. is News & Views editor atnpj Digital Medicinebut\nplayed no role in the internal review or decision to publish this News & Views piece.\nAdditional information\nCorrespondenceand requests for materials should be addressed to Dylan Powell.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:428 3\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:428 4",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.44483184814453125
    },
    {
      "name": "Cognitive psychology",
      "score": 0.43062523007392883
    },
    {
      "name": "Psychology",
      "score": 0.41863077878952026
    },
    {
      "name": "Natural language processing",
      "score": 0.3783588409423828
    },
    {
      "name": "Computer science",
      "score": 0.3738776445388794
    },
    {
      "name": "Linguistics",
      "score": 0.3525756895542145
    },
    {
      "name": "Philosophy",
      "score": 0.1465303599834442
    },
    {
      "name": "Neuroscience",
      "score": 0.13982638716697693
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210147807",
      "name": "Berkeley Public Health Division",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I12093191",
      "name": "University of Stirling",
      "country": "GB"
    }
  ]
}