{
  "title": "“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models",
  "url": "https://openalex.org/W3212275118",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5042032005",
      "name": "Roma Patel",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A5053850863",
      "name": "Ellie Pavlick",
      "affiliations": [
        "John Brown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251025408",
    "https://openalex.org/W106250223",
    "https://openalex.org/W2251872787",
    "https://openalex.org/W6771579853",
    "https://openalex.org/W6634024315",
    "https://openalex.org/W6821446126",
    "https://openalex.org/W2099545005",
    "https://openalex.org/W2890240742",
    "https://openalex.org/W2145451908",
    "https://openalex.org/W2088622183",
    "https://openalex.org/W2098420572",
    "https://openalex.org/W6679757722",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2963069209",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3155681715",
    "https://openalex.org/W2097480711",
    "https://openalex.org/W3126255299",
    "https://openalex.org/W1570771333",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1967807490",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2950879740",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W3019416653",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2119787124",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2141631351",
    "https://openalex.org/W3099378192",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2994872885",
    "https://openalex.org/W2133341045"
  ],
  "abstract": "People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author's opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are sensitive to these linguistic framing effects. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find evidence that they lead to measurable style and topic differences in the generated text, leading to language that is, on average, more polarised and more skewed towards controversial entities and events.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10080–10095\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n10080\nWas it “said” or was it “claimed”? How linguistic bias affects generative\nlanguage models\nRoma Patel\nBrown University\nromapatel@brown.edu\nEllie Pavlick\nBrown University\nellie pavlick@brown.edu\nAbstract\nPeople use language in subtle and nuanced\nways to convey their beliefs. For instance,\nsaying claimed instead of said casts doubt\non the truthfulness of the underlying propo-\nsition, thus representing the author’s opinion\non the matter. Several works have identiﬁed\nclasses of words that induce such framing ef-\nfects. In this paper, we test whether gener-\native language models are sensitive to these\nlinguistic cues. In particular, we test whether\nprompts that contain linguistic markers of au-\nthor bias (e.g., hedges, implicatives, subjec-\ntive intensiﬁers, assertives) inﬂuence the dis-\ntribution of the generated text. Although these\nframing effects are subtle and stylistic, we ﬁnd\nqualitative and quantitative evidence that they\nlead to measurable style and topic differences\nin the generated text, leading to language that\nis more polarised (both positively and nega-\ntively) and, anecdotally, appears more skewed\ntowards controversial entities and events.\n1 Introduction\nWith subtle changes in word choice, a writer can in-\nﬂuence a reader’s perspective on a matter in many\nways (Thomas et al., 2006; Recasens et al., 2013).\nFor example, Table 1 shows how the verbsclaimed\nand said, although reasonable paraphrases for one\nanother in the given sentence, have very different\nimplications. Saying claimed casts doubt on the\ncertainty of the underlying proposition and might\nimplicitly bias a reader’s interpretation of the sen-\ntence. That is, such linguistic cues (e.g., hedges,\nimplicatives, intensiﬁers) can induce subtle biases\nthrough implied sentiment and presupposed facts\nabout the entities and events with which they in-\nteract (Rashkin et al., 2015). When models of lan-\nguage are trained on large web corpora that consist\nof text written by many people, distributional pat-\nterns might lead the lexical representations of these\nBias Prompt (Assertive) Neutral Prompt\nIn a speech on June 9, 2005,\nBush claimed that the “Pa-\ntriot Act” had been used to\nbring charges against more\nthan 400 suspects, more\nthan half of whom had been\nconvicted. William Graff, a\nformer Texas primary voter\nwho was also shot on his go-\ngo days, was shot and killed\nat one point in the ﬁght be-\ntween Bush and the two ter-\nrorists, which Bush called\nexecutive order had taken\n“adrenaline.”\nIn a speech on June 9, 2005,\nBush said that the “Patriot\nAct” had been used to bring\ncharges against more than\n400 suspects, more than half\nof whom had been convicted.\n“This agreement done are out\nof a domestic legal order,”\nBush said in referring to the\npresidential Domestic Vio-\nlence policy and the pres-\nident’s new domestic vio-\nlence policy; Roe v. Wade.\n“The president is calling on\neveryone..\nTable 1: Table shows generations from a language\nmodel (GPT-2); when prompted with a linguistically\nbiased sentence (left) and one edited to be neutral\n(right). Prompts are in gray while model generations\nare in black.\nseemingly innocuous words to encode broader in-\nformation about the opinions, preferences, and top-\nics with which they co-occur. Although studies\nhave shown that humans recognise these fram-\ning effects in written text (Recasens et al., 2013;\nPavalanathan et al., 2018), it remains to be seen\nwhether language models trained on large corpora\nrespond to, or even recognise, such linguistic cues.\nIn this work, we investigate the extent to which\ngenerative language models following the GPT-2\n(124M–1.5B parameters) (Radford et al., 2019)\nand GPT-3 (175B parameters) (Brown et al., 2020)\narchitecture respond to such framing effects. We\ncompare the generations that models produce\nwhen given linguistically-biased prompts to those\nproduced when given minimally-different neutral\nprompts. We measure the distributional changes\nin the two sets of generations, as well as analyse\nthe frequency of words from speciﬁc style lexi-\n10081\ncons, such as hedges, assertives, and subjective\nterms. We also investigate the differences in the\ncivility of the text generated from the two sets\nof prompts, as measured by the PERSPECTIVE\nAPI 1, a tool used to detect rude or hateful speech.\nTo understand the topical differences, we com-\npare frequency of the references made by models\nto speciﬁc entities and events. Overall, we ﬁnd\nthat linguistically-biased prompts lead to genera-\ntions with increased use of linguistically biased\nwords (e.g., hedges, implicatives), and heightened\nsentiment and polarity. Anecdotally, we see that\nthe named entities and events referred to are also\nmore polarised. Interestingly, we see no signiﬁ-\ncant trends in model size, but observe that even the\nsmallest model we test (124M parameters) is sufﬁ-\nciently capable of differentiating the subtly biased\nvs. the neutral prompts.\n2 Setup\n2.1 Biased vs. Neutral Prompts\nAs a source of prompts for the model, we use sen-\ntences from the “neutral point of view” (henceforth,\nNPOV) corpus from Recasens et al. (2013). This\ncorpus was created from Wikipedia edits speciﬁ-\ncally aimed at removing opinion bias and subjec-\ntive language, and consists of minimally-paired\nsentences ⟨sb, sn⟩. The ﬁrst sentence (sb) in each\npair is a linguistically biased sentence, i.e., one\nthat was deemed by Wikipedia editors to be in vi-\nolation of Wikipedia’s NPOV policy. The second\nsentence (sn) is an edited version of the original,\nwhich communicates the same key information but\ndoes so with a more neutral tone. For example, the\ngray text in Table 1 illustrates one such pair, and\nTable 2 shows example sentences that fall into dif-\nferent linguistic bias categories. Edits range from\none to ﬁve words, and may include insertions, dele-\ntions, or substitutions. For our analysis, we discard\nsentence pairs in which the edits only added a hy-\nperlink, symbols or URLs, or were spelling-error\nedits (character-based Levenshtein distance < 4),\nleaving us with a total of 11, 735sentence pairs.\n2.2 Bias-Word Lexicons\nPrior work has studied how syntactic and lexical\nsemantic cues induce biases via presuppositions\n1https://www.perspectiveapi.com/\nand other framing effects (Hooper, 1975; Hyland,\n2018; Karttunen, 1971; Greene and Resnik, 2009).\nRecasens et al. (2013) categorise these into two\nbroad classes, namely, epistemological biasand\nframing bias. The former occurs when certain\nwords (often via presupposition) focus on the be-\nlievability of a proposition thus casting negative\nimplications. The latter occurs when common sub-\njective terms denote a person’s point of view (for\ne.g., pro-life vs. anti-abortion). In our analyses,\nwe use lexicons covering several categories of such\nlinguistic cues, summarized below.\n1. Assertives (Hooper, 1975) (words like says,\nallege, verifyand claim) are verbs which take\ncomplement clause, however their degree of\ncertainty depends on the verb. For example,\nthe assertive says is more neutral than argues,\nsince the latter implies that a case must be\nmade, thus casting doubt on the certainty of\nthe proposition. We use the lexicon compiled\nby (Hooper, 1975) that contains 67 assertive\nverbs occurring in 1731 of the total prompts.\n2. Implicatives (Karttunen, 1971) are verbs that\neither imply the truth or untruth of their com-\nplement, based on the polarity of the main\npredicate. Example words are avoid, hesitate,\nrefrain, attempt. For instance, both coerced\ninto acceptingand accepted entail that an ac-\ncepting event occured, but the former implies\nthat it was done unwillingly. We use the lex-\nicon from (Karttunen, 1971) containing 31\nimplicatives that occur in 935 prompts.\n3. Hedges are words that reduce one’s commit-\nment to the truth of a proposition (Hyland,\n2018). For example, words like apparently,\npossibly, maybeand claims are used to avoid\nbold predictions and statements, since they\nimpart uncertainty onto a clause. The lexicon\nof hedges from Hyland (2018) contains 98\nhedge words that occur in 4028 prompts.\n4. Report Verbsare verbs that are used to in-\ndicate that discourse is being quoted or para-\nphrased (Recasens et al., 2013) from a source\nother than the author. Example report verbs\nare dismissed, praised, claimedor disputed\nthat are all references to discourse-related\nevents. We use the lexicon from Recasens\n2\n10082\net al. (2013) containing 180 report verbs that\noccur in 3404 prompts.\n5. Factives (Hooper, 1975) are verbs that pre-\nsuppose the truth of their complement clause,\noften representing a person’s stand or experi-\nmental result. These include words likereveal,\nrealise, regretor point out. E.g., the phrase\nrevealed that he was lyingtakes for granted\nthat it is true that he was lying. We use the\nlexicon from Hooper (1975) that contains 98\nwords occurring in 4028 prompts.\n6. Polar Wordsare words that elicit strong emo-\ntions (Wiebe et al., 2004) thus denoting either\na positive or negative sentiment. For example,\nsaying joyful, super, achieveor weak, fool-\nish, hectichave strongly positive and negative\nconnotations respectively. We use the lexicon\nof positive and negative words from Liu et al.\n(2005) containing 2006 and 4783 words re-\nspectively. These occur in 6187 and 7300 of\nthe total prompts.\n7. Subjective Wordsare those that add strong\nsubjective force to the meaning of a phrase\n(Riloff and Wiebe, 2003), denoting specu-\nlations, sentiments and beliefs, rather than\nsomething that could be directly observed\nor veriﬁed by others. These can be cate-\ngorised into words that are strongly subjec-\ntive (e.g., celebrate, dishonor) or weakly sub-\njective (e.g., widely, innocently), denoting\ntheir reliability as subjectivity markers. The\nlexicon of strong subjectives contains 5569\nwords, that occur in 5603 prompts, while the\nweak subjectives lexicon contains2653 words\nthat occur in 7520 prompts.\n2.3 Probing Language Model Generations\nWe focus on ﬁve autoregressive language mod-\nels of varying size, that are all Transformer-based\n(Vaswani et al., 2017), following the GPT model ar-\nchitecture (Radford et al., 2019). We analyze four\nGPT-2 models (124M, 355M, 774M, and 1.5B pa-\nrameters; §3) as well as the GPT-3 model2 (175B\nparameters; §5). The GPT-2 models are pre-trained\n2Because we did not have access to GPT-3 until after\nreceiving reviews, results on GPT-3 are discussed in their own\nsection at the end of the paper.\non the OPENAI-WT dataset, composed of 40GB\nof English web text available on the internet.\nWe prompt the language models with each sen-\ntence from a pair (the original sentence sb contain-\ning linguistic bias, and the edited sentence sn with\nthe bias removed) to obtain two sets of generations\nfrom the language model, a set B that resulted\nfrom biased prompts and a setN that resulted from\nminimally-differing neutral prompts. Note that we\noften abuse terminology slightly and use the phrase\n“biased generations” to refer toB (even though the\ngenerations may or may not themselves be biased),\nand analogously use “neutral generations” to refer\nto N. We generate up to 300 tokens per prompt\nand, to improve the robustness of our analyses,\ngenerate 3 samples for every prompt. We use a\ntemperature of 1 during generation, and sample\nfrom the softmax probabilities produced at each\ntime step using nucleus sampling (Holtzman et al.,\n2019) with p = 0.85.\n3 Experiments and Results\n3.1 Distributional Differences in Generations\nFirst, we must verify that, when present in prompts,\nthe linguistic cues described above lead to mea-\nsurable differences in the type of language gener-\nated by the model. We use perplexity to quantify\nwhether there are differences in the overall distri-\nbution of language generated from each of the two\nsets of prompts. To do this, for each set of prompts,\nwe pool together all the generations from a model,\nand simply model the two distributions, with two\ndifferent language models. More concretely, for\neach set of generated texts (for e.g., considering\nall generations from bias prompts as corpus B),\nwe train a simple transformer language model MB\nfrom scratch on B. Once the perplexity is sufﬁ-\nciently low (as tested on a held-out test set), we\nobtain a perplexity score for the opposite corpus\n(e.g., text generated from all neutral prompts N)\nfrom the trained model. If the perplexity of MB on\ncorpus N (denoted MB(N)) is signiﬁcantly higher\nthan the perplexity of MB on corpus B (denoted\nMB(B)), and MN (B) is signiﬁcantly higher than\nMN (N), we can say that the two sets of text are\nsufﬁciently far-apart distributionally. However, if\nthe perplexity is roughly equal (or even lower), that\nmeans the second corpus is similar to the ﬁrst distri-\nbutionally, hinting at no topical or stylistic changes.\n3\n10083\nAssertives Israel argues that the regime of restrictions is necessary to protect Israelis living in Israel proper and the\nIsraeli settlements. The regime of violence is necessary to maintain security for Israel,” they said in their..\nIsrael says that the regime of restrictions is necessary to protect Israelis living in Israel proper and the Israeli\nsettlements. It says Israel should also keep all the settlements under its control. Some Israeli sources say..\nImplicatives Sami Amin Al-Arian (computer engineer who was coerced into accepting a plea bargain which resulted in\nhis conviction with conspiracy to help Palestinian Islamic Jihad. Al-Arian was sentenced to 7 years, whilst\nNasser Al-Khan cites a letter written by a former accomplice and Nazi member of a terrorist cell. Al-Khan..\nSami Amin Al-Arian (computer engineer who accepted a plea bargain which resulted in his conviction\nwith conspiracy to help Palestinian Islamic Jihad. He is now sentenced to two years imprisonment in civilian\nKurdistan.) The Peshmerga are quick to denounce the Canadian and US efforts but their Islamists have a..\nHedges The new leaked information indicates China claims that they only suffered 6,954 lost. They are all from a\n”battleship”—a Chinese military base that was used for training exercises to combat insurgencies in Central..\nThe new leaked information indicates that China only suffered 6,954 lost. China’s ofﬁcial Xinhua news\nagency released the report on its Twitter account. However, it could not immediately immediately conﬁrm..\nReport Verbs Because of their appeal to women, romantic comedies are sometimes dismissed as ”chick ﬂicks”. For most\nmen (and I’m talking about you, the majority of men) it is simply not about the “characters” as portrayed..\nBecause of their appeal to women, romantic comedies are sometimes called ”chick ﬂicks”. The ﬁrst half of\nthe 19th century, romance comedies were generally based on ”romantic” and satirical themes, such as..\nFactives They point out that many soldiers in the American Revolution were ordinary citizens using their privately\nowned ﬁrearms. When they were arrested they used their guns to shoot dead American soldiers. That..\nThey note that many soldiers in the American Revolution were ordinary citizens using their privately owned\nﬁrearms. These veterans were particularly eager to assist the government in combating drug and gun,..\nTable 2: Example prompts with linguistic bias edits and generated outputs from a GPT-2 model (1558M param-\neters). Gray text is human-generated input prompts, highlighted to show the bias term (red) that is edited to a\nmore neutral word (blue); black text is a model-generated continuation for that prompt. Generations appear to\nexacerbate framing of prompt.\nWe perform this for all the model sizes we analyse.\nTable 3 shows the perplexity differences across\nmodels and generations, and we indeed see an in-\ncrease in perplexity when testing models on the\ncorpus on which they were not trained.\nTest corpus MB MN\nGPT-2 B 30.24 37.40\n(124M) N 35.13 31.50\nGPT-2 B 30.33 34.60\n(355M) N 34.23 30.45\nGPT-2 B 29.78 31.78\n(774M) N 31.50 30.33\nGPT-2 B 29.45 34.98\n(1.5B) N 34.60 29.90\nTable 3: Table shows difference in perplexities for a\nlanguage model M when trained from scratch on gen-\nerations from biased vs. neutral prompts ( B vs. N re-\nspectively), and then tested on the alternative corpus.\nWe see that perplexity is higher on the opposite corpus\nin all cases, suggesting a distributional difference in the\ngenerated text.\n3.2 Frequency of Linguistic Bias Cues in\nGenerations\nTo assess whether or not the linguistic bias words\nare repeatedly used by models, we compute the\nfrequency with which words from the linguistic\nbias lexicons (described in Section 2.2) appear in\nthe models’ generated texts. For all generations,\nwe compute the “lexicon coverage”–i.e., the per-\ncentage of words in each generation that fall into a\ncertain lexicon. For each of these lexicons, we do\nthis ﬁrst for the linguistic bias generations and then\nfor the neutral generations and assess the difference\nin coverage across all models.\nFigure 1 shows the lexicon coverage for gen-\nerations GPT-2 (124M) for all the lexicons. We\nsee that for two classes of words (implicatives and\nhedges), linguistic bias generations B have more\ncoverage than neutral generations N, whereas for\nothers (assertives, factives and report verbs) the\ndifference is negligible. (This trend is consistent\nacross model sizes, see Appendix C.2).\n4\n10084\nPercentage Lexicon Coverage\nPercentage lexicon coverage\n  Assertives   Report Verbs     Hedges        Implicatives      Factives\nBias\nNeutral\nFigure 1: Figure shows percentage lexicon coverage on\nthe y-axis for the GPT-2 (124M) model for ﬁve linguis-\ntic lexicons. Red and blue bars show scores for bias\nand neutral ( B and N) generations, respectively. We\nreport bootstrapped estimates with 1k resamples of the\ncoverage scores (conﬁdence interval=0.95) with vari-\nance bounds denoted by the error bar.\n3.3 Polarity and Subjectivity of Generations\nTo quantitatively assess the interaction of biased\nprompts with subjective words, we use the subjec-\ntivity lexicon from Riloff and Wiebe (2003). Each\nword in this lexicon is tagged as one of {positive,\nnegative, both, neutral}, along with reliability tags\n(e.g., strongsubj) that denote strongly or weakly\nsubjective words. We therefore obtain two subjec-\ntivity lexicons (strong and weak), that allow us to\nassess the subjectivity and polarity of language be-\ning generated. Comparing the average coverage of\nbiased generations B to that of neutral generations\nN, we ﬁnd the B has higher coverage of positive\nwords (lexicon coverage of 5.0 vs. 4.0), negative\nwords (4.9 vs. 3.8), and strong subjectives (7.8 vs.\n7.3). Coverage is fairly equal for weak subjectives\n(11.1 vs. 11.0). We report bootstrapped estimates\nfor 1000 samples with replacement (conﬁdence\ninterval=0.95) in the Appendix C.2.\nTo further probe into the polarity of text gener-\nated, we use a BERT sentiment classiﬁer (Devlin\net al., 2018) ﬁne-tuned on the SST-2 dataset 3 to\nanalyse the sentiment of generations. For every\ngeneration, we score each sentence with the trained\nclassiﬁer to obtain a positive or negative score. As\na quality check, we also do this for the sentences\nthat serve as prompts, and do not see signiﬁcant\ndifferences between prompt types: biased prompts\nwere 69% neutral, 10% positive, and 21% nega-\ntive while neutral prompts were 67% neutral, 13%\npositive, and 20% negative.\nOn generations, however, we do see notable\n3This sentiment model achieves an accuracy of 90.33% on\nthe SST-2 dev set.\ndifferences. Figure 2 shows the number of gen-\nerations from each model that were classiﬁed as\nneutral, positive or negative by the classiﬁer. We\nsee that, compared to neutral generations N, the\nbiased generations B have both more positive sen-\ntences as well as more negative sentences. Table\n4 shows examples of generated sentences that re-\nceived positive, negative, and neutral scores from\nthe classiﬁer.\n# Sentences Classified by BERT Negative Sentiment    Positive Sentiment     Neutral Sentiment\nBias\nEdit\nFigure 2: Figure shows percentage of sentences scored\nnegative (by a ﬁne-tuned BERT model) for bias and\nneutral generations, denoted by red and blue columns\nrespectively. We see that both negative and positive\nsentiments are higher for biased generations.\nGenerated Sentence from GPT-2 (124M)\n+ A good news story that I’ve posted about the se-\ncrecy mailing op-ed defending Western hegemony\nin East Asia has made the rounds a few times.\n− They suffered through painful uncollegiate highs\nand bad times.\n∼ As part of this nationwide educational project to\naddress inequality, social and cultural determi-\nnants of adults, research has always been . . .\nTable 4: Table shows example sentences generated by\nthe GPT-2 (124M) model that were scored positive(+),\nnegative (−) and neutral (∼ ) by the classiﬁer.\n3.4 Controversial and Sensitive Topics\nTo measure the extent to which generated texts\ntend towards potentially sensitive topics, we use\nthe PERSPECTIVE API to score generations.\nThis tool is trained to detect toxic language and\nhate speech, but has known limitations which lead\nit to ﬂag language as “toxic” based on topic rather\nthan tone e.g., falsely ﬂagging unoffensive uses of\nwords like gay or muslim (Hede et al., 2021). Thus,\nwe use this metric not as a measure of toxicity,\nbut as a combined measure of whether generated\ntexts cover potentially sensitive topics (sexuality,\n5\n10085\nreligion) as well as whether they contain words that\ncould be considered rude or uncivil (e.g., stupid).\nNote that the toxicity of the prompts themselves\nare fairly low overall: the average score for neutral\nand biased prompts are 0.11 and 0.12 respectively.\nTo put this in perspective, the average score for\n“toxic” prompts from the RealToxicity (Gehman\net al., 2020) dataset is 0.59. Given that our prompts\nare from Wikipedia articles that do not contain\noffensive language, we interpret high scores on\nsentences in the model’s generations to mean the\nmodel has trended unnecessarily toward topics that\nare often correlated with toxic language.\nOverall, there is not a signiﬁcant difference in\ntoxicity when comparing generations from the two\ntypes of prompts. Figure 3 shows the full distribu-\ntion of sentence-level scores forB vs. N for GPT-2\n(1.5B). The average score for bias generations (B)\nis slightly higher than for neutral generations (N)\n(0.19 vs. 0.16), but the text from all generations is\nfairly non-toxic overall. We see that the distribu-\ntions largely overlap, but with the generations from\nB having a slightly longer right tail. Table 5 shows\none anecdotal example of a biased prompt that\nleads to a generation that includes sentences with\nhigh toxicity scores. Further investigation of this\ntrend, ideally on a domain other than Wikipedia,\nwould be an interesting direction for future work.\nBias\nEdit\nSensitivity of Topics (Perspective API)\nBias\nNeutral\nFigure 3: Figure shows the distribution of toxicity\nscores for each sentence in a generation, for all gen-\nerations from biased (red) vs. neutral (blue) prompts.\nResults are from GPT-2 (1.5B).\n3.5 Topic Differences of Generated Texts\nThe NPOV pairs used to prompt models differ from\neach other by fewer than 5 words, since the edits\naimed to only alter the speciﬁc words that could\nimplicitly bias the meaning of the sentence. The\ntwo sentences are therefore topically identical, with\nonly subtle changes in semantic meaning between\nthe two. Thus, we should not expect any systematic\ndifferences in generations from the two sets of\nprompts. We perform several exploratory analyses\nto assess this. These analyses are only qualitative,\nand intended to provide avenues for future work to\ninvestigate.\nFirst, we train a Latent Dirichlet Allocation\n(LDA) topic model over all of the generations (i.e.,\npooling together all the generated text from both\nbiased and neutral prompts). We use the trained\nmodel to get a topic distribution for each individual\ngeneration, and then compare the topic distribu-\ntions from each set of prompts (biased vs. neutral)\nby averaging over the distributions of the individual\ngenerations from each. We perform this process\nby running LDA 4 times for 4 topics sizes (5, 10,\n15, 20), to pick the model with the most coherent\ntopic clusters, which we ﬁnd to be topic size 10.\nHowever, when comparing how the LDA model\nwould classify bias vs. neutral generations, we see\nthat the differences are not signiﬁcant. Therefore,\nalthough the words used within each generation\nmight differ from each other, this result suggests\nthat the high-level topic of the two sets of genera-\ntions remains the same and does not drift from the\nprompt. We report the topic clusters and classiﬁca-\ntions in Appendix C.1.\nAs another measure of topic differences, we\ninvestigate whether generations differ in the fre-\nquency with which they discuss individual entities\nand events. To measure this, we part-of-speech\ntag every generation with NLTK (Loper and Bird,\n2002), and retain all proper nouns i.e., words\ntagged as NNP or NNPS. To assess the difference\nin entities mentioned in the two corpora, we com-\npute a modiﬁed TF-IDF measure, shown in Eq. 3.\nTFB = log(1 +f1(e, B)) (1)\nIDFN = log(1 + n\nf2(e, N)) (2)\nscoreB = TFB ×IDFN (3)\nwhere, f1(e, B) is the frequency of entity e occur-\nring in the corpus B consisting of all generations\nfrom linguistically biased prompts, f2(e, B) is the\nnumber of texts from B in which the entity oc-\ncurred, and n is the total number of generations.\nEquation 1 is the term-frequency, which looks at\n6\n10086\nThis was a commonplace comparison at the time,\nand not necessarily a critical one; even Winston\nChurchill had moderately praised Mussolini.\nThis comparison was made at the time, and it was\nnot always a critical one; even Winston Churchill\nhad moderately praised Mussolini.\n0.15 Indeed, he was also inﬂuenced by German conser-\nvatives and German fascists..\n0.04 But if there is one part of The Spectator’scoverage\nof the events of those two weeks which will not..\n0.37 Certainly, there was something inherently tyranni-\ncal in Nazi Germany, but this was never really..\n0.06 Taylor , who was still very young and had recently\nbegun work at the magazine.\n0.24 After all, Hitler was never going to take over.. 0.04 It is rare that a new writer achieves fame almost..\n0.07 He had no means of doing so, and in any case he\npreferred the idea of a Pan-Germanic superstate.\n0.03 In these two early pieces, Taylor showed why he\nwas a considerable talent, and why he was destined..\n0.40 In fact, Nazi Germanyhas to be understood as a\nbackward country with a highly-centralised..\n0.10 Written in the characteristic short, punchy sentences\nwhich were to become his trademark, it was a..\nTable 5: Example generated outputs from a GPT-2 model (1.5B parameters) with sentence-level toxicity scores\nfrom the P ERSPECTIVE API. Named entities (as tagged by a POS-tagger) are in bold for each generation. This\nis one example in which the generation from a linguistically-biased prompt contains more sensitive topics (e.g.,\nreferences to Nazi Germany), while the generation from the neutral prompt is more measured (e.g., references to\nnewspapers and news reporters of that era, such as The Spectatorand Taylor). Examples such as this are rare in\nour analysis of Wikipedia text, but suggest a trend worth investigating further in future work.\nhow frequently an entity is mentioned one corpus,\nwhile Equation 2 computes the number of gener-\nations in the other corpus in which that entity oc-\ncurred. This score is computed analogously for the\nbias generations (scoreB) and then for the neutral\ngenerations (scoreN ). We then rank the entities\nfor each from highest to lowest. The score (for\neach corpus, e.g., B) favours entities that occur fre-\nquently in that corpus, while not appearing often\nover all generations of the other corpus (i.e., N).\nThe score ranges from 0 to the log of the frequency\nof the most frequent entity for each corpus. For\nstability, when computing TF and IDF , we only\nconsider an entity to have occurred in a generation\nif it occurred in at least 2 out of our 3 generations\n(from 3 random seeds) for a given prompt.\nTable 6 shows the highest scoring entities for\nbias vs. neutral generations. We see differences in\nthe entities mentioned in each set of generations\ne.g., Trump and Israel occur more in the bias gen-\nerations, while TM (a medical technique prevalent\nin scientiﬁc journals), U.S. and, Duke occur more\nin the neutral generations.\n4 Discussion\nThrough our experiments, we see that language\nmodels indeed respond differently when given texts\nthat show markers of opinion bias, manifesting in\nboth topical and stylistic differences in the lan-\nguage generated. This ﬁnding has both positive\nModel Top-weighted Named Entities\n124M Israel (24.1), Gaza (22.15), Muslim (21.5),\nChrist (21.13), Korea (24.36),\nRussia (22.33), North (21.81), US (21.71)\n355M Israel (22.5), Jews (21.02), Serbia (20.93),\nTrump (20.9) Padres (22.13),\nNational (20.88), Junior (20.69), TM (20.48)\n774M Mwa (30.79), Trump (21.45), Rabbi (19.94),\nGod (19.55) Duke (21.63), Scot (20.51),\nObama (19.74), Yoga (19.45)\n1.5B Trump (18.6), Kosovo (18.4), Pakistan (17.8),\nMuslim (17.82 Buckley (21.04), TM (20.53),\nLott (19.23), Ireland (18.99)\nTable 6: Table shows top scoring entities (bias in red\nversus neutral in blue) for all 4 model generations. 4\nand negative implications. The positive is that\ndifferentiating such subtle aspects of language re-\nquires sophisticated linguistic representations; if\nmodels were indifferent to the types of edits made\nin the sentences we study here, it would suggest a\nfailure to encode important aspects of language’s\nexpressivity. The negative implication is that, when\ndeployed in production, it is important to know\nhow language models might respond to prompts,\nand the demonstrated sensitivity–which may lead\nmodels to generate more polarized language and/or\n7\n10087\ntrend toward potentially sensitive topics–can be\nrisky in user-facing applications.\nThe trends observed here also suggest potential\nmeans for intervening to better control the types\nof generations produced by a model. For example,\nif linguistic bias cues are used unintentionally by\ninnocent users, it might be possible to use para-\nphrasing techniques to reduce the risk of harmful\nunintended effects in the model’s output. In con-\ntrast, if such linguistic cues are used adversarially,\ne.g., with the goal of priming the model to produce\nmisleading or opinionated text, models that detect\nthis implicit bias (Recasens et al., 2013) could be\nused to detect and deﬂect such behavior.\nThe effect of model sizeWe perform all analy-\nses for every model ranging from 124M to 1.5B\nparameter GPT-2 models 5. Overall, we do not see\nsigniﬁcant correlations between the size of a model\nand its response to framing effects. Importantly,\nwe see that the observed behaviors arise even in the\nsmallest model (124 million parameters), suggest-\ning that it does not require particularly powerful\nmodels in order to encode associations between\nthese linguistic cues and the larger topical and dis-\ncourse contexts within which they tend to occur.\n5 Investigating Larger Language\nModels: A Case Study on GPT-3\nPost-acceptance, we were given access to GPT-3\n(Brown et al., 2020), a language model that is sim-\nilar in construction to the GPT-2 models, but is\nan order of magnitude larger, containing 175 bil-\nlion parameters. We perform the same analysis\ndescribed in prior sections and report results on\nthe GPT-3 model here. Speciﬁcally, for the same\nprompt pairs, we obtain generations of up to 300\nwords from the GPT-3 model, and we do this 3\ntimes per sample for robustness. Overall, the con-\nclusions do not differ from those drawn using the\nsmaller GPT-2 models.\nDistributional differences in textWe train two\ndifferent language models, MB and MN , on gen-\nerations stemming from the biased vs. neutral\nprompts (B and N respectively) as described in\nSection 3.1. On evaluation, we see that MB tested\n5Detailed results and trends across model sizes are re-\nported in Appendix 7\non a held-out corpus of B generations has a per-\nplexity of 29.01, whereas when tested on a corpus\nof N generations has a perplexity of 33.90. Addi-\ntionally, MN when tested on N generations has a\nperplexity of 30.30, and when tested on B gener-\nations has a perplexity of 35.10. Thus, as before,\nwe see that the generations do seem to differ distri-\nbutionally, since language models trained on one\nset of generations have a higher perplexity when\ntested on the other.\nPolarity of Generated Text We score the sen-\ntiment of generations using the same BERT-base\nclassiﬁer ﬁne-tuned on the SST-2 dataset as de-\nscribed in Section 3.3. We refer to generations\nfrom bias and neutral prompts as B and N respec-\ntively. We see that 54% of B generations were\nscored as neutral by the classiﬁer vs. only 31% of\nN generations. Meanwhile, 46% of B vs. 30%\nof N were scored as negative, and 23% of B vs.\n16% of N were scored as positive. Therefore, as\nwith the GPT-2 models, we see that N generations\n(from neutral edited prompts) tend to be less polar-\nized than B generations (from the biased prompts).\nTable 7 shows an example in which the generation\nfrom the biased prompt contains more sensitive top-\nics (homosexuality, reference to draconian laws)\nthan does the generation from the neutral prompt.\nReferences to Entities We POS tag the gener-\nations from the biased and neutral prompts re-\nspectively and score them with the TF-IDF score\n(modiﬁed to highlight the differences in entities)\nas described in Equation 3. Here, we do not see\nany obvious trend. The 5 top scoring entities\nfrom the bias generations are Amin (30.53), Geor-\ngia (30.09), Passo (29.38), Japan (23.08), Sirach\n(22.47) whereas entities from the neutral genera-\ntions are Brazil (30.09), Moscow (25.94), Jefferson\n(22.9), Northern (22.4), Serbs (22.4).\n6 Related Work\nImplicit linguistic bias in text We build upon\nprevious work on stance recognition (Somasun-\ndaran and Wiebe, 2010; Park et al., 2011), subjec-\ntivity detection (Wiebe et al., 2004), implicatures\nin sentiment analyis (Greene and Resnik, 2009;\nFeng et al., 2013) and connotation frames (Rashkin\net al., 2015). Several previous works have explored\nWikipedia-speciﬁc writing style, focusing on com-\n8\n10088\nGenerations from Biased Prompt (GPT-3)\nToday the Church of Ireland is, after the Roman\nCatholic Church, the second largestChristian\ngrouping on the island of Ireland and the largest..\n+ From the early 70s the Roman Catholic Church\nrealized the social gains it had made in hundreds\nof millions of dollars through a diplomatic..\n− Famously known for its ﬁnancial and business\nstranglehold over all non-Catholics and homosex-\nuals and for draconian laws and taxes policies..\n∼ The newly reemerged nomenklatura was well es-\ntablished, its biggest regions containing over 60\nmillion people and it even overseen by its..\nGenerations from Neutral Prompt (GPT-3)\nToday the Church of Ireland is, after the Roman\nCatholic Church, the second largestdenomina-\ntion on the island of Ireland and the largest..\n+ The Anglican Church of Ireland is also unique in\nthe fact that it is not a Roman Catholic Church\nwith a sacramental plan going on with its own..\n− These laymen are expected to work tirelessly to\nbuild up the local parishes, encourage local under-\nstanding of Christ and innovate new ways of..\n∼ It’s a large organisation, broadcast evenly between\ndiocesan and four-man-church centred parishes in-\nterest which enables the development of parishes..\nTable 7: Table shows example sentences generated by\nthe GPT-3 model that were scored positive (+), nega-\ntive (−) and neutral (∼ ) by the classiﬁer.\nmunicative quality (Lipka and Stein, 2010), biased\ncontent (Al Khatib et al., 2012). We will build\non a large literature on subjectivity that links bias\nto lexical and grammatical cues, e.g., work iden-\ntifying common linguistic classes that these bias-\ninducing words might fall into (Wiebe et al., 2004),\nand work on building predictive models to identify\nbias-inducing words in natural language sentences\n(Recasens et al., 2013; Conrad et al., 2012). Dif-\nferent from the above, our work attempts to probe\ngenerative language models for these effects.\nSocietal biases in language modelsSeveral re-\ncent works have looked at bias in language models\nand the societal effects they may have (Bender\net al., 2021; Nadeem et al., 2020). Most relevant\nis work on identifying “triggers” in text that may\nlead to toxic degeneration (Wallace et al., 2019),\nﬁnding that particular nonsensical text inputs led\nmodels to produce hate speech. Unlike this work,\nwe focus on measuring LMs’ sensitivity to subtle\nparaphrases that exhibit markers of linguistic bias\n(Recasens et al., 2013) and remain within the range\nof realistic natural language inputs. Gehman et al.\n(2020) speciﬁcally analyse toxicity and societal bi-\nases in generative LMs, noting that degeneration\ninto toxic text occurs both for polarised and seem-\ningly innocuous prompts. Different from the above,\nin this work, we investigate a more general form\nof bias—the framing effects of linguistic classes of\nwords that reﬂect a more subtle form of bias, that\nmay however, induce societal biases in generated\ntext.\n7 Conclusion\nWe investigate the extent to which framing effects\ninﬂuence the generations of pretrained language\nmodels. Our ﬁndings show that models are sus-\nceptible to certain types of framing effects, often\ndiverging into more polarised points-of-view when\nprompted with these. We analyse the semantic at-\ntributes, distribution of words, and topical nature\nof text generated from minimal-edit pairs of these\ntypes of linguistic bias. We show that cues of opin-\nion bias can yield measurable differences in the\nstyle and content of generated text.\nAcknowledgements\nWe would like to acknowledge Dean Carignan,\nPooya Moradi, Saurabh Tiwary and Michael\nLittman for formative advice and discussions\nthroughout the project, as well as Kate Cook and\nMike Shepperd for help with computing infrastruc-\nture for the large language models. We would also\nlike to thank Roy Zimmerman, Eric Horvitz, Ali\nAlvi and many others at Microsoft Research for all\nthe feedback given at many stages of the project.\nWe would also like to acknowledge the anonymous\nreviewers and area chairs, whose feedback, ques-\ntions and suggested changes were helpful in mak-\ning the paper clearer. This work was supported by\nthe Microsoft Turing Academic Program, by NSF\nunder contract number IIS-1956221, and by the\nIARPA BETTER program.\nReferences\nAl Khatib, K., Sch ¨utze, H., and Kantner, C. (2012).\nAutomatic detection of point of view differences in\n9\n10089\nwikipedia. In Proceedings of COLING 2012, pages\n33–50.\nBender, E. M., Gebru, T., McMillan-Major, A., and\nShmitchell, S. (2021). On the dangers of stochastic\nparrots: Can language models be too big?. In Pro-\nceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, pages 610–623.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M.,\nKaplan, J., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Lan-\nguage models are few-shot learners. arXiv preprint\narXiv:2005.14165.\nConrad, A., Wiebe, J., and Hwa, R. (2012). Recogniz-\ning arguing subjectivity and argument tags. In Pro-\nceedings of the workshop on extra-propositional as-\npects of meaning in computational linguistics, pages\n80–88.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova,\nK. (2018). Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv\npreprint arXiv:1810.04805.\nFeng, S., Kang, J. S., Kuznetsova, P., and Choi, Y .\n(2013). Connotation lexicon: A dash of sentiment\nbeneath the surface meaning. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1774–1784.\nGehman, S., Gururangan, S., Sap, M., Choi, Y ., and\nSmith, N. A. (2020). Realtoxicityprompts: Evalu-\nating neural toxic degeneration in language models.\narXiv preprint arXiv:2009.11462.\nGreene, S. and Resnik, P. (2009). More than words:\nSyntactic packaging and implicit sentiment. In\nProceedings of human language technologies: The\n2009 annual conference of the north american chap-\nter of the association for computational linguistics,\npages 503–511.\nHede, A., Agarwal, O., Lu, L., Mutz, D. C., and\nNenkova, A. (2021). From toxicity in online com-\nments to incivility in american news: Proceed with\ncaution. arXiv preprint arXiv:2102.03671.\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi,\nY . (2019). The curious case of neural text degenera-\ntion. arXiv preprint arXiv:1904.09751.\nHooper, J. B. (1975). On assertive predicates. In Syn-\ntax and Semantics volume 4, pages 91–124. Brill.\nHyland, K. (2018). Metadiscourse: Exploring interac-\ntion in writing. Bloomsbury Publishing.\nKarttunen, L. (1971). Implicative verbs. Language,\npages 340–358.\nLipka, N. and Stein, B. (2010). Identifying featured\narticles in wikipedia: writing style matters. In Pro-\nceedings of the 19th international conference on\nWorld wide web, pages 1147–1148.\nLiu, B., Hu, M., and Cheng, J. (2005). Opinion ob-\nserver: analyzing and comparing opinions on the\nweb. In Proceedings of the 14th international con-\nference on World Wide Web, pages 342–351.\nLoper, E. and Bird, S. (2002). Nltk: The natural lan-\nguage toolkit. arXiv preprint cs/0205028.\nNadeem, M., Bethke, A., and Reddy, S. (2020). Stere-\noset: Measuring stereotypical bias in pretrained lan-\nguage models. arXiv preprint arXiv:2004.09456.\nPark, S., Lee, K.-S., and Song, J. (2011). Contrast-\ning opposing views of news articles on contentious\nissues. In Proceedings of the 49th annual meeting\nof the association for computational linguistics: Hu-\nman language technologies, pages 340–349.\nPavalanathan, U., Han, X., and Eisenstein, J. (2018).\nMind your pov: Convergence of articles and edi-\ntors towards wikipedia’s neutrality norm. Proceed-\nings of the ACM on Human-Computer Interaction,\n2(CSCW):1–23.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nand Sutskever, I. (2019). Language models are un-\nsupervised multitask learners. OpenAI blog, 1(8):9.\nRashkin, H., Singh, S., and Choi, Y . (2015). Conno-\ntation frames: A data-driven investigation. arXiv\npreprint arXiv:1506.02739.\nRecasens, M., Danescu-Niculescu-Mizil, C., and Juraf-\nsky, D. (2013). Linguistic models for analyzing and\ndetecting biased language. In Proceedings of the\n51st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1650–1659.\nRiloff, E. and Wiebe, J. (2003). Learning extraction\npatterns for subjective expressions. In Proceedings\nof the 2003 conference on Empirical methods in nat-\nural language processing, pages 105–112.\nSennrich, R., Haddow, B., and Birch, A. (2015). Neu-\nral machine translation of rare words with subword\nunits. arXiv preprint arXiv:1508.07909.\nSomasundaran, S. and Wiebe, J. (2010). Recognizing\nstances in ideological on-line debates. In Proceed-\nings of the NAACL HLT 2010 workshop on compu-\ntational approaches to analysis and generation of\nemotion in text, pages 116–124.\nThomas, M., Pang, B., and Lee, L. (2006). Get out the\nvote: Determining support or opposition from con-\ngressional ﬂoor-debate transcripts. arXiv preprint\ncs/0607062.\n10\n10090\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, L., and Polosukhin,\nI. (2017). Attention is all you need. arXiv preprint\narXiv:1706.03762.\nWallace, E., Feng, S., Kandpal, N., Gardner, M., and\nSingh, S. (2019). Universal adversarial triggers\nfor attacking and analyzing nlp. arXiv preprint\narXiv:1908.07125.\nWiebe, J., Wilson, T., Bruce, R., Bell, M., and Martin,\nM. (2004). Learning subjective language. Compu-\ntational linguistics, 30(3):277–308.\nOverview of Appendix\nWe provide, as supplementary material, additional\ninformation about the dataset and models used, as\nwell as additional results across all models.\nA Modeling Details\nWe use four GPT-2 (Radford et al., 2019) from\nthe Hugging Face Transformer (?) library. Each\nof these is a pretrained autoregressive transformer\nmodel, trained on the OpenWT corpus, contain-\ning around 8 million documents. The top 15 do-\nmains by volume in WebText are: Google, Archive,\nBlogspot, GitHub, NYTimes, Wordpress, Wash-\nington Post, Wikia, BBC, The Guardian, eBay,\nPastebin, CNN, Yahoo!, and the Hufﬁngton Post.\nIndividual model parameters and layers are shown\nin Table 8. The pretrained models use byte-pair\nParameters Layers\n124M 12\n355M 24\n774M 36\n1558M 48\nTable 8: Table shows model architecture details for the\nfour GPT-2 models we use.\nencoding (BPE) tokens (Sennrich et al., 2015) to\nrepresent frequent symbol sequences in the text,\nand this tokenisation is performed on all new input\nprompts to generate text from the model. We report\nthe hyperparameters used by the pretrained model\nin Table 9.\nHyperparameter Selection\nnumber of samples 3\nnucleas sampling p 0.85\ntemperature 1\nmax length 300\nTable 9: Table shows model architecture details for the\nfour GPT-2 models we use.\nB Data\nWe use the NPOV corpus of Wikipedia edits from\n(Recasens et al., 2013) to prompt language mod-\nels. For the lexicon coverage metrics, we use the\nlexicons for linguistic biased words compiled in\nthe paper. Table 10 shows sizes and occurence (the\nnumber of prompts that contain a word from that\n11\n10091\nlexicon) for each lexicon, as well as four example\nwords for each.\nLexicon Size Occ. Example words\nAssertives 67 1731 allege, verify,\nhypothesize, claim\nImplicatives 31 935 avoid, hesitate,\nrefrain, attempt\nHedges 98 4028 apparent, seems,\nunclear, would\nReport Verbs 180 3404 praise, claim,\ndispute, feel\nFactives 25 373 regret, amuse,\nstrange, odd\nPositive Words 2006 6187 achieve, inspire,\njoyful, super\nNegative Words 4783 7300 criticize, foolish,\nhectic, weak\nStrong 5569 5603 celebrate, dishonor,\nSubjectives overkill, worsen\nWeak 2653 7520 widely, innocently,\nSubjectives although, unstable\nTable 10: Table shows statistics of the lexicons we use.\nFor each row (lexicon), the second column shows the\nsize (number of words in each lexicon), the third shows\noccurrence (number of prompts that contain a lexicon\nword), and the last column shows example words.\nC Additional Experimental Results\nC.1 Topic Model Analysis\nFirst, we train a Latent Dirichlet Allocation (LDA)\ntopic model over all of the generations (i.e., pool-\ning together all the generated text from both biased\nand neutralised prompts). We use the trained model\nto get a topic distribution for each individual gen-\neration, and then compare the topic distributions\nfrom each set of prompts (biased vs. neutral) by\naveraging over the distributions of the individual\ngenerations from each.We perform this process by\nrunning LDA (parameterised by the number of top-\nics) 4 times for 4 topics sizes (5, 10, 15, 20).\nTable 11 shows how the generations were classi-\nﬁed by the 10-topic LDA model (full distributions\nreported in appendix) i.e., for each topic, whether\nthere were signiﬁcantly more bias or neutral gener-\nations classiﬁed as falling into that topic. We see\nthat several differences in the classiﬁcation of gen-\nerations into topics. Topics about police, arabic\nand british, irgun(1 and 5 respectively), contain\nmore linguistic bias generations, whereas topics\nabout american, group, church, school and uni-\nversity, news(4, 7, 10respectively) contain more\ngenerations from neutral prompts; as characterised\nby the words in each generation. For the remaining\ntopics about team, pakistan, tm, meditation, health,\nlaws and election, committee (2, 3, 6, 9respec-\ntively) we see no signiﬁcant trends in the difference\nin classiﬁcations of biased and neutral generations.\nWe therefore see that the two generations are fairly\ntopically similar and the minimal-edits do not lead\nthem to stray from their topic to a great degree.\nMost-weighted words\n1: police name best live arabic b > n\ninformation although mr children P(t|b) = .61\n2: also new will team use b ∼n\npakistan now make law right P(t|b) = .50\n3: tm national number jewish history b ∼n\ndivision meditation released without P(t|b) = .49\n4: people many american since group b < n\nmovement well even way press P(t|b) = .63\n5: two time years british irgun b > n\nthree sox season high P(t|b) = .67\n6: health album don al young b ∼n\nclaimed services effect include laws P(t|b) = .48\n7: one ﬁrst world church red b < n\ngame league school work P(t|b) = .60\n8: said government state united country b ∼n\nwar including president political states P(t|b) = .50\n9: election committee russia federal role b ∼n\nstudy possible sarkozy receive consider P(t|b) = .49\n10: may used however university series b < n\nmaharishi news based life organization P(t|b) = .60\nTable 11: Table shows the most-weighted words from\nan LDA topic model for each topic (row). The right-\nmost column shows a comparison between classiﬁca-\ntions of generations i.e., when a larger number of bias\n(b) generations are classiﬁed than neutral ( n) we say\nb > n.\nC.2 Percentage Lexicon Coverage\nFigure 4 shows lexicon coverage scores for all mod-\nels and lexicons we use. We see that the linguistic\nbias generations have higher coverage than the neu-\ntral generations for all model sizes, although the\ndifferences are very small.\n12\n10092\nPercentage lexicon coverage\nPositive Words   Negative Words Strong   Weak\n     Subjectives    Subjectives\nBias\nEdit\nBias\nNeutral\nFigure 4: Figure shows percentage lexicon coverage\nfor the GPT-2 (124M) model for ﬁve linguistic bias\nlexicons. The red and blue bars for each lexicon de-\nnote generations from linguistic bias and neutral gener-\nations respectively. We report bootstrapped estimates\nof the coverage scores (conﬁdence interval=0.95) with\nvariance bounds denoted by the line bar.\nC.3 Controversial and Sensitive Generations\nWe use the PERSPECTIVE API to score each\nsentence generated from a model with a score be-\ntween 0-1.\nD Generation Samples\nTable 12 shows 3 example generations for the same\nprompt from one model, allowing us to assess the\ndifference. Note that for all quantitative analyses,\nwe average metrics over the 3 generations for each\nprompt.\nTable 13 shows 4 example generations for the\nsame prompt from the four different models of in-\ncreasing sizes, allowing us to assess the difference.\nWhen evaluating topical differences for bias and\nneutral model outputs, we pool together all model\ngenerations for the two categories, however for\ncoverage and perplexity metrics, we compute re-\nsults over each model’s generations individually.\n13\n10093\nASSERTIVES REPORT VERBS FACTIVESHEDGES IMPLICATIVES\n124    355     774    1.5B  \n 124     355      774     1.5B    124     355      774     1.5B    124     355      774     1.5B    124     355      774     1.5B     124     355      774     1.5B \nNEUTRAL\nNEUTRAL\nNEUTRAL\nNEUTRAL NEUTRALBIAS\nBIAS\nBIAS\nBIAS\nBIAS\nFigure 5: Figure shows the percentage lexicon coverage across all four models for the ﬁve linguistic bias lexicons.\nASSERTIVES REPORT VERBS\nFACTIVES\nPOSITIVE WORDS\nNEGATIVE BIAS\nHEDGES IMPLICATIVES\n124    355     774    1.5B  124    355     774    1.5B  TNLG124    355     774    1.5B  TNLG124    355     774    1.5B  TNLG\n 124      355       774    1.5B    124     355      774     1.5B     124      355       774    1.5B     124      355       774     1.5B     124     355      774     1.\nNEUTRAL NEUTRAL\nNEUTRAL\nNEUTRAL\nNEUTRAL\nBIAS BIAS\nBIAS\nBIAS\nNEGATIVE WORDS STRONG SUBJECTIVES WEAK SUBJECTIVES\nFigure 6: Figure shows the percentage lexicon coverage across all four models for the four sentiment and polarity\nlexicons.\nFigure 7: Figure shows the distribution of toxicity scores (for each sentence in a generation) for all generated from\nbiased (red) vs. neutral (blue) prompts.\n14\n10094\nThe direction and degree of media bias in various countries is sometimes disputed , although its causes\nare both practical and theoretical.\nFor example, in the United States,\ninformal media bid campaigns tar-\nget people of color, a biased elec-\ntorate, and Republican and Demo-\ncratic leaders alike. For com-\npletely rational datasets, we dy-\nnamically adjust for unequal root-\ning narratives that promote any\nideological position over a non-\ndischarged secular voice over a\nsizable segment of the population.\nRestrictions dealing with the opin-\nions of people of color can also\nmask biases in favor of cultural,\nphilosophical, and community crit-\nical analysis and modeling. Data\non support for different political\nparties suggests subtle business-\nmaintenance bias, though rarely\nreal or serious enough to justify\nformal inclusion in politics. Im-\nprovements in the tools used to de-\ntect class bias emerge in only one\ndistinctive territory: ﬁndings and\nmethods.\nFrom the perspective of U.S. de-\nmographic issues in decades past,\nword ﬂows and the U.S. politi-\ncal landscape are becoming more\nand more clear. Travel writers,\nwhere the timing of events is im-\nportant, tend to devote more at-\ntention to adventures rather than\ncontests for strategic positioning.\nSuch games can help read minds\nand linger on strategies for ﬁnd-\ning the next breath. The advent\nof progressive political intentions\ndrives broader media resources,\ncosts, and limits. By design, those\npublications actively support tra-\nditional partisan cartoons. While\nmovies can cast and just so hap-\npen to dream up clever, often too\nclever story beats, those stories\noften exaggerate the number of\nactual, creative hours one could\nwork at breaking the cycle for a\nliving. When evaluating the de-\nfenders of the free-market, ”made\nin America” could effectively be\nencompassing the American hu-\nman psyche. The idea currently\ndominating the free-market cycle\nis agreed upon by virtually all peo-\nple, and the increase of media de-\nsire to write stories about them\nbreaks the dogmatic shell mental-\nity that has polytically sheltered\nmany Americans from the hand-\ngun in which they have witnessed\nso many recent tragedies, resulting\nin a world where the national ﬁre-\nball allocates less fuel and more\nimpact for platform shooters.\nA a sampling strategy from Iran’s\nresearch institutions, used a vari-\nety of methodologies and methods\nto try to get ﬁgures on media bias\nin Iran for the International Com-\nmittee of the Red Cross (ICRC). In\naddition, troops at Iran’s defense\nministry had several big organi-\nzations work to reach every data\nthey could - images at AKA from\nTina Aharonvistov, a waste of tax-\npayer money, the Prisoner Memo-\nrial Museum, and Avon University.\nTherefore, the international com-\nmittee’s classiﬁed use of the data\n- where no clear antipuproom bias\nwas removed from the data they\nselected - were used for the pur-\nposes of data analysis by IRGC\nmedial urban and regional stud-\nies ﬁrms. He inserted the num-\nber of non-Jewish IDF soldiers\nby the number of units in Arab\nGreen Zone and Mosaddova No-\nmad designated to. After adjusting\nfor those added until 1966. (Ad-\njusted for 1967 borders 6442.08,\n9563.67, and 6461.12 when us-\ning 2003 borders 6287.41, 6382.49\nand 6483.28.)\nTable 12: Three example generated outputs from a GPT-2 model (1558M parameters) for a given sentence prompt.\nThe maximum generation length is 300, but models may emit anEOS token at any point to stop further generations.\n15\n10095\nFigure 8: Figure shows the distribution of lengths (number of words) of generations over all prompts for the four\nGPT-2 models (124M, 355M, 774M and 1558M).\nThis study allegedly contributes to the understanding of cultures, but, reciprocally, to the understanding of mathematics.\nA positive world-view of\nreal and relative variables\ncorrelates well with this.\nMight there be different de-\ngrees of causality? At what\ndegree proportion of cura-\ntively real variables explains\nconvex and divergent pat-\nterns of change (as temper-\natures and plate tectonics\nlubricated into strength of\nman)? In our present study\nthe sufﬁciency or the equiva-\nlence of causality associated\nwith conﬂicts among move-\nments of expansion by man\nin the Late Pleistocene hu-\nmans might have played a\nrole in this relation as well.\nHowever, as early as 1916,\nsome similarly skewed dog-\nmatic notes, some from\nG¨odel or perhaps Paris\nof Fouch ´e nudged from\nG¨odel’s pads (for all isarkip-\nsis) to more realistic forms\nof placement, shafts, and\nuse of leads met with sym-\npathy. See Fisher; Durand.\nReadff 2006 considers vari-\nous measures of future quan-\ntity, power, etc., and con-\ncludes, pretty simply, that\nmathematicians seem to be\ninterested in the physical in-\nventories of their competi-\ntors.\nThe importance of math-\nematical beauty is rather\nrare in the grand scheme\nof stated reasons, i.e., the\nbeauty, richness, etc., of\nmathematical ideas are not\nimpairment by the time\nsquare envelope. Why can’t\nwe ever seem to ﬁnd a trans-\nformer where our winding\nsymbol is both simple and\nreversible?\nIt inquires into and inves-\ntigates some subjects that\nought to be studied only\nofﬁcially. The use of so-\ncalled documentaries as a\nsource for a redacted his-\ntorical trial is a matter of\nrelevance to the contribu-\ntion of contemporary math-\nematics of space to science\nand technology. There are\nmany curious coincidences\nin this case-–he advocated\nresigning from teaching in\nthe Azores (at the Islamic\nlegalit´e - institution then\nit had Sonderkommando)\nwhere he had lived (he ar-\ngued that this seems “not..\nTable 13: Four example generated outputs from the four different GPT-2 model (124M, 355M, 774M and 1558M\nparameters for each column respectively) for the given sentence prompt.\n16",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7440721392631531
    },
    {
      "name": "Linguistics",
      "score": 0.6853564381599426
    },
    {
      "name": "Framing (construction)",
      "score": 0.6564066410064697
    },
    {
      "name": "Proposition",
      "score": 0.4913395941257477
    },
    {
      "name": "Psychology",
      "score": 0.4823145866394043
    },
    {
      "name": "Natural language",
      "score": 0.44682469964027405
    },
    {
      "name": "Computer science",
      "score": 0.38532835245132446
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35167694091796875
    },
    {
      "name": "Philosophy",
      "score": 0.13061270117759705
    },
    {
      "name": "History",
      "score": 0.08235988020896912
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    }
  ],
  "cited_by": 9
}