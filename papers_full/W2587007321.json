{
  "title": "Detection and Localization of Robotic Tools in Robot-Assisted Surgery Videos Using Deep Neural Networks for Region Proposal and Detection",
  "url": "https://openalex.org/W2587007321",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A1787511002",
      "name": "Duygu Sarikaya",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2073062517",
      "name": "Jason J. Corso",
      "affiliations": [
        "University of Michigan–Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A711309458",
      "name": "Khurshid A. Guru",
      "affiliations": [
        "Roswell Park Comprehensive Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A1787511002",
      "name": "Duygu Sarikaya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2073062517",
      "name": "Jason J. Corso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A711309458",
      "name": "Khurshid A. Guru",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2155893237",
    "https://openalex.org/W2179352600",
    "https://openalex.org/W2162888803",
    "https://openalex.org/W1867429401",
    "https://openalex.org/W1483408659",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W6600313631",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W1892716672",
    "https://openalex.org/W2098701449",
    "https://openalex.org/W2009912861",
    "https://openalex.org/W2030414913",
    "https://openalex.org/W1994854671",
    "https://openalex.org/W2322287462",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W22745672",
    "https://openalex.org/W2401482382",
    "https://openalex.org/W1989460737",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2266464013",
    "https://openalex.org/W1607976681",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2107775979",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6748710392",
    "https://openalex.org/W2122667561",
    "https://openalex.org/W6720378446",
    "https://openalex.org/W6684339883",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2120419212",
    "https://openalex.org/W6639204139",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W1536680647"
  ],
  "abstract": "Video understanding of robot-assisted surgery (RAS) videos is an active research area. Modeling the gestures and skill level of surgeons presents an interesting problem. The insights drawn may be applied in effective skill acquisition, objective skill assessment, real-time feedback, and human-robot collaborative surgeries. We propose a solution to the tool detection and localization open problem in RAS video understanding, using a strictly computer vision approach and the recent advances of deep learning. We propose an architecture using multimodal convolutional neural networks for fast detection and localization of tools in RAS videos. To the best of our knowledge, this approach will be the first to incorporate deep neural networks for tool detection and localization in RAS videos. Our architecture applies a region proposal network (RPN) and a multimodal two stream convolutional network for object detection to jointly predict objectness and localization on a fusion of image and temporal motion cues. Our results with an average precision of 91% and a mean computation time of 0.1 s per test frame detection indicate that our study is superior to conventionally used methods for medical imaging while also emphasizing the benefits of using RPN for precision and efficiency. We also introduce a new data set, ATLAS Dione, for RAS video understanding. Our data set provides video data of ten surgeons from Roswell Park Cancer Institute, Buffalo, NY, USA, performing six different surgical tasks on the daVinci Surgical System (dVSS) with annotations of robotic tools per frame.",
  "full_text": "1\nDetection and Localization of Robotic Tools in\nRobot-Assisted Surgery Videos Using Deep Neural\nNetworks for Region Proposal and Detection\nDuygu Sarikaya, Jason J. Corso and Khurshid A. Guru\nAbstract—Video understanding of robot-assisted surgery\n(RAS) videos is an active research area. Modeling the gestures\nand skill level of surgeons presents an interesting problem. The\ninsights drawn may be applied in effective skill acquisition,\nobjective skill assessment, real-time feedback, and human-robot\ncollaborative surgeries. We propose a solution to the tool detec-\ntion and localization open problem in RAS video understanding,\nusing a strictly computer vision approach and the recent advances\nof deep learning. We propose an architecture using multimodal\nconvolutional neural networks for fast detection and localization\nof tools in RAS videos. To our knowledge, this approach will be\nthe ﬁrst to incorporate deep neural networks for tool detection\nand localization in RAS videos. Our architecture applies a Region\nProposal Network (RPN), and a multi-modal two stream convo-\nlutional network for object detection, to jointly predict objectness\nand localization on a fusion of image and temporal motion cues.\nOur results with an Average Precision (AP) of 91% and a mean\ncomputation time of 0.1 seconds per test frame detection indicate\nthat our study is superior to conventionally used methods for\nmedical imaging while also emphasizing the beneﬁts of using\nRPN for precision and efﬁciency. We also introduce a new\ndataset, ATLAS Dione, for RAS video understanding. Our dataset\nprovides video data of ten surgeons from Roswell Park Cancer\nInstitute (RPCI) (Buffalo, NY) performing six different surgical\ntasks on the daVinci Surgical System (dVSS\nR⃝) with annotations\nof robotic tools per frame.\nIndex Terms—Object detection, Multi-layer neural network,\nImage classiﬁcation, Laparoscopes, Telerobotics\nI. I NTRODUCTION\nR\nOBOT-ASSISTED surgery (RAS) is the latest form of\ndevelopment in today’s minimally invasive surgical tech-\nnology. The robotic tools help the surgeons complete complex\nmotion tasks during procedures with ease by translating the\nsurgeons’ real-time hand movements and force on the tissue\ninto small scale ones. Despite its advances in minimally\ninvasive surgery, the steep learning curve of the robot-assisted\nsurgery devices remains a disadvantage [1]. Translation of the\nsurgeons’ movements via the robotic device is challenged by\na loss of haptic sensation. Surgeons usually feel comfortable\nwith the procedures only after they have completed proce-\ndures on 12-18 patients [1]. The traditional mode of surgical\nD. Sarikaya is a PhD candidate at the Department of Computer Sci-\nence and Engineering, SUNY Buffalo, NY , 14260-1660 USA e-mail: duy-\ngusar@buffalo.edu\nJ. J. Corso is an Associate Professor at the Department of Electrical\nEngineering and Computer Science, University of Michigan, Ann Arbor, MI,\nUSA 48109\nK. A. Guru is the director of Applied Technology Laboratory for Advanced\nSurgery (ATLAS), Roswell Park Cancer Institute, Buffalo, NY , USA, 14263\nManuscript received — 2017.\ntraining (apprenticeship) fails to answer the needs of today’s\nRAS training. The conventional approach relies heavily on\nobservational learning and operative practice [2]. Moreover,\nthe evaluation of the training also relies on the subjective\nobservance of an experienced surgeon, ideally measured by\na senior surgeon with a scoring system [3]. The need for\nuniversally-accepted and validated metrics and quantitative\nskill assessment via automation is addressed in the community\n[2]. Early identiﬁcation of technical competence in surgical\nskills is expected to help tailor training to personalized needs\nof surgeons in training [2], [3]. Moreover, we believe that auto-\nmated feedback or human-robot collaborative surgeries could\ngreatly beneﬁt novice surgeons and their patients. As such,\nwe have investigated the development of such systems with\nvideo understanding via computer vision using the video data\nrecorded during surgical tasks on the daVinci Surgical System\n(dVSS R⃝). Figure 2 shows sample frames from recorded video\ndata of surgeons performing on training sets.\nWe approach the problem of video understanding of RAS\nvideos as modeling the motions of surgeons. Modeling ges-\ntures and skills depends highly on the motion information and\nthe precise trajectories of the motion. We argue that, detecting\nthe tools and capturing their motion in a precise manner is an\nimportant step in RAS video understanding. The ﬁrst step for\ntracking the tools in the video is to robustly detect the presence\nof a tool and then localize it in the image. Tool detection and\nlocalization is hence the focus of our study.\nAdvances in object detection had plateaued until the recent\nreintroduction of deep neural networks into the computer\nvision community with large-scale data object classiﬁcation\ntasks. The deep neural network trained via back-propagation\nthrough layers of convolutional ﬁlters by LeCun et al. [4],\n[5] had had an outstanding performance on large amounts\nof training data. On large-scale recognition challenges like\nImageNet [6], similar approaches have now proven that deep\nneural networks could be used with object recognition and\nclassiﬁcation tasks [6], [7]. However, in the medical ﬁeld these\nadvances are yet unused, to the best of our knowledge; so, we\npropose a novel approach of using deep convolutional neural\nnetworks (CNN) for fast detection and localization for video\nunderstanding of RAS videos. No study we are aware of has\naddressed the use of CNN in tool detection and localization\nin RAS video understanding and our study will be the ﬁrst.\nIn this paper, we address the problem of object detection\nand localization, speciﬁcally the surgical robotic tool, in RAS\nvideos as a deep learning problem. We propose complete\narXiv:2008.00936v1  [cs.CV]  29 Jul 2020\n2\nFig. 1. We propose an end-to-end deep learning approach for tool detection\nand localization in RAS videos. Our architecture has two separate CNN\nprocessing streams on two modalities: the RGB video frame and the RGB\nrepresentation of the optical ﬂow information of the same frame. We convolve\nthe two separate input modalities and get their convolutional feature maps.\nUsing the RGB image convolutional features, we train a Region Proposal\nNetwork (RPN) to generate object proposals. We use the region proposals\nand the feature maps as input to our object classiﬁer network. The last layer\nfeatures of these streams are later fused together before classiﬁer.\nsolutions for detection and localization using CNNs. We apply\nRegion Proposal Networks (RPN) [8] jointly with a multi-\nmodal object detection network for localization; as such, we\nsimultaneously predict object region proposals and objectness\nscores. Figure 1 shows an overview of our system. Our results\nindicate that our study is superior to conventionally used\nmethods for medical imaging with an Average Precision (AP)\nof 91% and a mean computation time of 0.1 seconds per test\nframe.\nII. R ELATED WORK\nMany early approaches to detecting and tracking robotic\ntools in surgery videos use markers and landmarks to reduce\nthe problem to a simple computer vision problem of color\nsegmentation or thresholding [9], [10]. Using color markers or\ncolor coding the tools are examples of this approach. Another\nexample of a marker is a laser-pointing instrument holder used\nto project laser spots on the scene [11]. The works of Zhang\net al. [12] and Zhao et al. [13] introduce a barcode marker.\nHowever, these methods require additional manufacturing and\nraise concerns on bio-compatibility. Moreover, having to use\nadditional instruments in minimally-invasive surgical settings\ncould be challenging [14].\nMore recent approaches focus on tracking the tool via a per\nvideo initialization and/or using additional modalities such as\nkinematic data. Du et al. develop a 2D tracker based on a\nGeneralized Hough Transform using SIFT features and use this\nto initialize a 3D tracker at each frame to recover instrument\npose [15]–[17]. This method assumes that they have the 3D\npose of the instrument in the ﬁrst frame and they use this\ninformation to initialize a 2D bounding box which is then used\nas a reference point for tracking. Allan et al. uses Random\nForests to fuse region based constraints based on multi-label\nprobabilistic region classiﬁcations with low level optical ﬂow\ninformation [18]. Instead of an ofﬂine learning approach, they\ntrain their Random Forest using a manual segmentation of a\nsingle frame with a tool positioned in front of the scene. This\napproach poses a drawback as it does not allow their system\nto operate on different surgical setups without re-training.\nCapturing kinematic data of the robotic console as an addi-\ntional feature to help with tool detection and localization could\nbe an alternative, however, it requires additional instruments,\nrecording, and preprocessing in order to be used as motion\ninformation. Recent approaches such as the work presented in\nReiter et al.’s [19], creates templates using different kinematic\nconﬁgurations of the virtual renderings of a CAD model of\nthe tool. They further reﬁne their conﬁguration estimation by\nmatching gradient orientation templates to the real image.\nThere are strictly vision based approaches as well. In their\nwork, Sznitman et al. [20] learn a multiclass classiﬁer based on\ntool-parts detectors. During training they use image windows\nand evaluate the image features that compute the proportions\nof edges in different locations and orientations in relation to\nthat window. At test time, to localize the tool, they evaluate the\nclassiﬁer in a sliding window fashion; at each image location\nand at multiple image scales. Although they use an early\nstopping algorithm to reduce the time, we believe that it is a\npriority to offer a solution that is more effective and efﬁcient.\nOne of the common computer vision approaches is using shape\nmatching. The challenge of detection by ﬁtting shape models is\nthat the objects in the RAS videos in study show a high-degree\nof variation in shape, pose, and articulation. The parts of the\nobjects might even be occluded or missing in the frame as seen\nin Figure 3. So the approaches that follow a rigid appearance\nand global shape model are usually not sufﬁcient. In contrast,\nthe Deformable Parts Model (DPM) by Felzenszwalb et al.\n[21], [22] consists of a star-structured pictorial model linking\nthe root of an object to its parts using deformable springs.\nThis model has proven to be successful in object detection\nas it captures the occlusions and articulations. One of the\nrecent, surgical tool detection applications of DPM [21], [22]\nis the product of experts tool detector by Kumar et al. [14].\nWe compare our results to DPM [21], [22] as it is often\nsuccessfully used in recent successful applications in similar\ndomains.\n3\nFig. 2. Sample frames of the video data recorded during surgical training tasks on the daVinci Surgical System (dVSS R⃝).\nIn this paper, we propose a novel end-to-end approach\nof using deep convolutional neural networks (CNN) for fast\ndetection and localization for video understanding of RAS\nvideos. We apply Region Proposal Networks (RPN) [8] jointly\nwith a multimodal object detection network for localization.\nOur architecture, based on the work of Zeiler et al. [24] and\ndeveloped to support multimodality, starts with two separate\nCNN processing streams on two modalities: the RGB video\nframe and the RGB representation of the optical ﬂow infor-\nmation of the same frame as temporal motion cues. RPN\nis a deep, fully convolutional network that outputs a set of\nregion proposals based on the convolutional feature maps of\nthe RGB image inputs. We adopt Fast R-CNN [25] for the\nobject detection task, we use the region proposal boxes of\nRPN with the convolutional features as input for the detection\nnetwork streams on both modalities. The last layer features of\nthese streams are fused together.\nOur study differs from the former approaches as we focus on\nproposing a solution using strictly computer vision instead of\nusing additional modalities that requires additional equipment\nto capture; such as kinematic data. With our study, we propose\nto make use of recent advances of Convolutional Neural\nNetworks with the hope that it will serve as a benchmark for\ntool tracking to move towards deep learning. We also prioritize\nan efﬁcient and effective way of localizing the tools in the\nimages, our study demonstrates the invaluable contribution\nof RPNs to the problem of tool detection and localization\nin RAS videos. With our new architecture that incorporates\ntwo modalities, we make use of not only the visual object\nfeatures but also the temporal motion cues. This approach\nhelps us improve the detection of the tools by decreasing false\npositives. Our experiments show that our method is able to\ndetect and localize the tools fast with superior accuracy. Our\nproposed method does not require initialization, so it can be\nused on new video data without re-training. It could be used\nto automatically initialize tracking algorithms in RAS videos.\nIII. D ATASET\nRAS video understanding may be particularly challenging\nas the view of the operating site is limited and recorded via\nendoscopic cameras. Tracking the free movement of the sur-\ngeons in unconstrained scenarios requires camera movement\nand zoom. The tools in RAS videos vary in pose, articulation\nand their parts might be occluded or missing in the frame\n(refer to Figure 3 for visual examples). There are frequently\nother objects in use and in motion such as the needle, suture,\nFig. 3. The tools in RAS videos vary in pose, articulation and parts of the\ntool might be occluded or missing in the frame.\nclamps or other objects used in training. The tissue that the\nsurgeon operates on might move or deform, show variation in\nshape and occlude the tool.\nWe have noticed that the RAS datasets for public use are\nlimited. The only comprehensive RAS datasets we know of\nthat is open for public use are JIGSAWS by Gao et al.\n[26] and the very recently released m2cai16-workﬂow and\nm2cai16-tool datasets [27]. Neither of these datasets provide\ntool annotations. JIGSAWS is quite restricted as it does not\ninclude artifacts, camera movement and zoom, or a wide range\nof free movement. For this reason, we have built a new, more\nchallenging dataset; ATLAS Dione, that leverages the data\ngathered for earlier works of Guru et al. [3] and Stegemann\net al. [28]. We have used the video data and prepared manual\nannotations for RAS video understanding problems. This new\nvideo dataset is a core contribution of our work and we will\nrelease it for tool detection and localization purposes upon\npublication. Despite being a phantom setting, our dataset is\nquite challenging as it has camera movement and zoom, free\nmovement of surgeons, a wider range of expertise levels,\nbackground objects with high deformation, and annotations\ninclude tools with occlusion, change in pose and articulation\nor when they are only partially visible in the scene.\nATLAS Dione provides video data of ten subjects perform-\ning six different surgical tasks on the dVSS R⃝. The ten sur-\ngeons who participated in this IRB-approved study (I 228012)\nwork at Roswell Park Cancer Institute (RPCI) (Buffalo, NY).\nThe difﬁculty and complexity of the tasks vary. These tasks in-\nclude basic RAS skill tasks which are part of the Fundamental\n4\nTABLE I\nPROPERTIES OF THE DATASET\nMain Dataset\nSkill Level Task Subtask Number of Videos Number of Frames\nBasic Skills\nBall Placement Task Using 1 Arm 7 videos\nUsing 2 Arms 7 videos\nThe Ring Peg Transfer Task Using 1 Arm 7 videos\nUsing 2 Arms 7 videos\nSuture Pass Task Put Through 7 videos\nPull Through 7 videos\nIntermediate Skills Suture and Knot Tie Task\nSuture Pick Up 7 videos\nSuture Pull Through 7 videos\nSuture Tie 7 videos\nAdvanced Skills Urethrovesical Anastomosis(UV A)\nUV A Pick Up 7 videos\nUV A Pull Through 7 videos\nUV A Tie 7 videos\nTotal 84 videos 18782 frames\nAdditional Test Set\nof compiled subtasks 15 videos 3685 frames\nTotal 99 videos 22467 frames\nSkills of Robotic Surgery (FSRS) curriculum [28] and also the\nprodecure-speciﬁc skills required for the Robotic Anastomosis\nCompetency Evaluation (RACE) [3]. The subjects, surgeons\nfrom RPCI, are classiﬁed by different expertise levels of\nbeginner (BG), combined competent and proﬁcient (CPG), and\nexpert (EG) groups based on the Dreyfuss model [31]. For tool\ndetection and localization purposes, we manually label both\nthe left and right tools in use by providing exact bounding\nboxes of tool locations. In addition to these, following our\nfuture works, expertise levels of the subjects, task videos with\nthe beginning and ending timestamps for each subtask will\nalso be released for surgical activity recognition research.\nThe robotic skill groups based on the difﬁculty and complexity\nof the tasks are as follows (Refer to Figure 4 for sample frames\nof these skill groups):\n1) Basic Skills: Ball Placement Task, Suture Pass Task and\nRing Peg Transfer Task.\n2) Intermediate Skills: Placement of Simple Suture with\nKnot Tying.\n3) Advanced Skills: Performance of Urethrovesical Anas-\ntomosis (UV A) on an Inanimate Model.\nPlease refer to Table I to see properties of the dataset.\nA. Tool Annotation\nFor the tool detection and localization, we annotate bound-\ning boxes for both left and right tools seen in the videos.\nBounding boxes are manually annotated with the guidance\nof an expert RAS surgeon. For efﬁcient annotation, we have\ncustomized the tools provided by Caltech Pedestrian Detection\nBenchmark by Doll ´ar et al. [29] to our problem and used it\nto annotate each frame in the video clips.\nWe provide each frame (each of size 854x480 pixels) of the\nRAS videos in JPEG format. The annotations are provided in\nxml templates in VOC format [30].\nB. Subject Demographics\nWe recorded 10 surgeons from RPCI performing the given\ntasks on (dVSS R⃝). Out of these 10 subjects, 2 of them are\n(a) Basic Skills: Ball Placement Task, Suture Pass Task and Ring Peg Transfer\nTask.\n(b) Intermediate Skills: Placement of Simple Suture with Knot Tying. Ad-\nvanced Skills: Performance of Urethrovesical Anastomosis (UV A) on an\nInanimate Model.\nFig. 4. Sample frames of tasks under different robotic skill groups based on\nthe difﬁculty and complexity.\nresidents, 3 are fellows and 5 are practicing robotic surgeons.\nWhile 3 of them have more than 10 years of experience, 2\nof them have between 2 to 5 years of experience and the\nremaining 5 subjects are still in training. Two of our subjects\nhave performed over 500 robot-assisted procedures. The ten\nsubjects are assigned to beginner (BG), combined competent\nand proﬁcient (CPG), and expert (EG) groups based on the\nDreyfuss model [31].\nThe dataset with tool annotations is available for download\nto encourage further research in RAS video understanding.\nIV. M ETHOD\nWe propose an end-to-end deep learning approach for tool\ndetection and localization in RAS videos. Our architecture,\nbased on the work of Zeiler et al. [24], has two separate\nCNN processing streams on two modalities: the RGB video\nframe and the RGB representation of the optical ﬂow [40]\ninformation of the same frame. The last layer features of these\nstreams are later fused together. We ﬁrst convolve the two\nseparate input modalities and get their convolutional feature\nmaps. Using the RGB image convolutional features, we train\n5\na Region Proposal Network (RPN) [8] to generate object\nproposals. Figure 1 shows an overview of our system.\nFigure 5 provides a visual overview of the region proposal\nnetwork. RPN uses the convolutional feature maps of the RGB\ninput for generating region proposals. Each of these proposals\nhave an objectness score. In our study, we use an architecture\nbased on the one proposed by Zeiler et al. [24] with ﬁve\nconvolutional layers followed by fully connected layers. As\nproposed by the work of Girschick et al. [8], we slide a\nnetwork over the convolutional feature map of the conv5 layer\nin a sliding-window fashion. This network is fully connected\nto a spatial window of the convolutional feature map with\nan 3 ×3 convolutional layer. Then each 3 ×3 window is\nmapped to a lower-dimensional, ﬁxed 256 dimensional feature\nvector. This feature vector is then used as input for a box\nregression layer and a box classiﬁcation layer. The regression\nlayer outputs 4k values: the bounding box coordinates for\neach of the kregion proposals. The classiﬁcation layer outputs\n2k scores that estimate probability of each of the k regions\nbeing of object class or not. Region proposals are relative\nreference boxes to anchors centered at each sliding window.\nEach anchor is related with a scale of size 128, 256, and 512\npixels and aspect ratios of 1 : 1 , 1 : 2 , and 2 : 1 resulting\nin 9 anchors at each sliding window and WHk translation\ninvariant anchors in total where the convolutional feature map\nsize is W ×H. At the training step, each anchor is given a\nbinary class label according to Intersection-over-Union (IoU)\noverlap with a ground-truth box. If the IoU is higher than 0.7,\nthe anchor is given a positive object class label whereas if the\nIoU is smaller than 0.3 the anchor is given a negative label.\nThe remaining anchors are considered neutral and are not used\nfor training purposes.\nWe use these region proposal boxes from the RPN and\nthe convolutional features of both modalities, as input to the\nROI pooling layer, as introduced by Girschick et al. [25],\nfor each stream. After the last fully connected layers, the\nfeatures of both streams are concatenated and fused together in\na fully connected layer before the loss layers. We train region\nproposal network and the multimodal object detection network\njointly, with reference to the approximate joint training; that\nis, we ignore the derivate with respect to the proposal boxes\ncoordinates as explained by Girschick et al. [8]. We also\ninitiate our region proposal network with the RGB modal\nimage only, while we convolve the two modality pair of\nimages and then fuse their high level features for the detection\nnetwork. We think of the optical ﬂow as a function on the\ninput of RGB image features. Our experiments show that this\napproach is able to produce competitive results with a modest\ntraining time.\nA. Loss function for Learning\nWe use a multitask objective function [33], [34] to enable\nlearning parameters over our full network concurrently.\nIn our work, as proposed in Faster R-CNN [8], we minimize\nan objective function following the multi-task loss:\nL({pi},{ti}) = 1\nNcls\n∑\ni Lcls(pi,p∗i) +λ 1\nNreg\n∑\ni p∗iLreg(ti,t∗i) (1)\nFig. 5. We use a Region Proposal Network that proposes regions, which are\nused by a detector network. We slide a small network over the convolutional\nfeature map. Each spatial window over the feature map is mapped to a 256\ndimensional feature which is fed into the system. RPN outputs object region\nproposals, each with an objectness score on whether the region is of a tool\nor not. (Please refer to IV for details of our Method.)\nRecall that the regression layer outputs 4k values; the\nbounding box coordinates for each of the k region pro-\nposals while the classiﬁcation layer outputs 2k scores that\nestimate probability of each of the k region proposals be-\ning of object class or not. The classiﬁcation layer outputs\na discrete probability {pi} p = ( p0,...,p K), over K +\n1(background/non −object) categories and the regression\nlayer outputs {ti}bounding-box regression offsets; a predicted\ntuple tu = ( tu\nx,tu\ny,tu\nw,tu\nh) for class u. In Equation (1), i is\nthe index of an anchor and pi is the predicted probability of\nanchor i being an object. According to the IoU overlap, p∗\ni ,\nthe ground truth label, is 1 if the anchor is positive, and is\n0 if the anchor is negative. ti is the offset of the predicted\nbounding box where t∗\ni is the offset of the ground-truth box\nassociated with a positive anchor. The classiﬁcation loss Lcls\nis log loss over classes for whether it is an object or not:\nLcls(p,u) = −log pu (2)\nfor true class u.\nFor the regression loss ( Lreg), we use Lreg(ti,t∗\ni ) = R(ti −\nt∗\ni ), where R is the robust loss (smooth L1) function shown\nbelow:\nsmoothL1(x) =\n{\n0.5x2 if |x|<1\n|x|−0.5 otherwise (3)\nRegression loss is activated only for positive anchors and\ndoes not contribute otherwise. The two terms are normalized\nwith Ncls and Nreg and a balancing weight λ, set to 10,\nthat controls the balance between the two task losses. cls is\nnormalized by the mini-batch size (i.e., Ncls = 256) and the\nreg is normalized by the number of anchor locations. For a\n6\nconvolutional feature map of a size W ×H, there are WHk\nanchors in total.\nWe use the bounding-box regression to predict more pre-\ncise boundaries and to improve localization. At each object\nproposal bounding box, predictions are reﬁned with the help\nof the anchors and regression. The features are of a ﬁxed size.\nHowever, a set ofkbounding-box regressors for each proposed\nbox, called anchors, are learned. Each of these anchors are\nresponsible for a scale and an aspect ratio (Figure 5).\nA transformation that maps an anchor of a proposed box P\nto a nearby ground-truth box Gis learned using the regression\nsuggested by Girschick et al. [35]. Pk = (Pi\nx,Pi\ny,Pi\nw,Pi\nh) is\ndeﬁned to be the pixel coordinates of the center of an anchor\nbox of a proposal P with P’s width and height. Similiarly, the\nground truth pair is G= (Gx,Gy,Gw,Gh). The transforma-\ntion between the pairs of P and the nearest ground-truth box\nGis parameterized as a scale-invariant translation of the center\nof Ps bounding box dx(P),dy(P) and log-space translations\nof the width and height of Ps bounding box.\nWe apply the transformations below from P into G:\nˆGx = Pwdx(P) + Px\nˆGy = Phdy(P) + Py\nˆGw = Pw exp(dw(P))\nˆGh = Ph exp(dh(P))\nEach function d⋆(P) is a linear function of the pool5\nfeatures of P. Assuming d⋆(P) = wT Φ5(P)\nWe learn w⋆ by ridge regression:\nThe regression targets t⋆ for the training pair ( P, G) are\nthen deﬁned as:\ntx = (GxPx)/Pw\nty = (GyPy)/Ph\ntw = log(Gw/Pw)\nth = log(Gh/Ph)\nwhich we solve as a standard regularized least squares prob-\nlem.\nB. Optimization\nThe RPN is a fully-convolutional network trained end-to-\nend with back-propogation and stochastic gradient descent\n(SGD). To train this network efﬁciently, we ﬁrst sample N\nimages and then sample R/N anchors from each image pair.\nWe also randomly sample a top-ranking ﬁxed number of\nanchors in each modality image to compute the loss function\nof a mini-batch and keep a ratio of 1 : 1 of positive and\nnegative anchors.\nDuring the optimization we initialize the new layers by\nweights from a zero-mean Gaussian distribution with standard\ndeviation 0.01, while using the pre-trained ImageNet [6] model\nto initialize the rest. Transferring weights from pre-trained\nImagenet model has helped greatly with initialization of RPN\nnetworks and also with the RGB image convolutional features.\nIt has also shown great beneﬁts by addressing the problem of\noverﬁtting and has decreased the ﬂuctuations while our model\nconverged. Unfortunately, we couldn’t ﬁnd a suitable dataset\nmodel to train the newly introduced layers for the optical\nﬂow modality, we have initialized these layers by weights\nfrom a zero-mean Gaussian distribution. This has led to some\nﬂuctuations while the model converged, however, in the end,\nhas shown improvement with accuracy.\nWe set the learning rate to 0.001, with momentum of 0.9\nand a weight decay of 0.0005. We set the number of iterations\nto 70 k, we have decided on this number to be optimal after\nexperimenting with a range of 30 kto 120kiterations. 70khas\nshown comparable results to 120kwhile taking much less time\nfor training.\nC. Detection at test time\nDuring testing we apply the fully-convolutional RPN to\nthe entire image. To reduce the redundancy of overlapping\nproposals, we use non-maximum suppression (NMS) with the\nthreshold of 0.7 on the proposals based on their objectness\nscores. Then we use the top ranking ﬁxed number of proposals\nfor detection at test.\nV. E XPERIMENTS AND EVALUATION\nWe evaluate our architecture on the ATLAS Dione dataset\nusing all the 99 videos for either training or testing. In order to\nexperiment how stable the average precision results are, we do\na ten fold experiment. We split 90 of the videos for training and\nthe rest 9 for testing for each experiment setup. The videos\nare randomized for each experiment and we never use the\nframes extracted from the same video for both training and\ntesting for the reason they might be too similar. Our dataset\nhas only one class object; that is, the robotic tool and the\nbackground class. We use a setting of multiple 2.62GHz CPU\nprocessors and Geforce GTX1080 with computation capability\nof 6.1, which have allowed us to run our experiments faster\nwith less memory consumption with cuDNN library. We use\nPASCAL VOC evaluation [30] to evaluate the accuracy of our\ndetections.\nOur experimental results reach an Average Precision (AP)\nof 91% ( 90.65%). It takes 7.22 hours to train a model with\n70k iterations and a mean computation time of 0.103 seconds\nto detect the tools in each test frame, given that we are working\nwith a set of already computed optical ﬂow images. We\ncompare our experimental results with the original architecture\nproposed by Girschick et al. [8], Fast RCNN using EdgeBoxes\nproposals [25], [36] and Deformable Parts Model (DPM)\nsuggested by Felzenszwalb et al. [21], [22], which is a proven\nstate of the art method in medical domain and tool detection\nin RAS videos [14]. The architecture proposed by Girschick\net al. [8] scores 90% (0.9039%), takes only 4.21 hours to train\na model with 70k iterations and has a mean computation time\nof 0.59 seconds per each test frame. While the improvement\nwith our multimodal approach over FasterRCNN seems small,\nour architecture has repeatedly and consistently scored higher\naccuracy for each experiment set. Thus, we believe using\na multimodal approach has a stable improvement over the\n7\nTABLE II\nEXPERIMENTS AND EVALUATION\nMethod Mean Average Precision Detection Time (per frame)\nRPN+Fast R-CNN Detection, Multimodal 91% (90.65%) 0.103 seconds\n+ optical ﬂow computation (a few seconds [40])\nRPN+Fast R-CNN Detection (Faster R-CNN) 90% (0.9039%) 0.059 seconds\nEdge Boxes+Fast R-CNN Detection (Fast R-CNN) 20% 0 .134 seconds for detection\n+ 2 seconds for region proposal\nDeformable Parts Model (DPM) 76% (83% with bounding box regression) 2.3 seconds\narchitecture proposed by Girschick et al. [8], however there\nmight be room for improvement such as pre-training on similar\nﬂow data and transferring its weights to initialize our model.\nThis could help our architecture reﬁne its improvement over\nsingle modality approach. In order to test the efﬁciency of\nusing Region Proposal Network to generate object region\nproposals, we experiment with an alternative region proposal\nmethod. Among the most popular region proposal methods;\nSelective Search takes about two seconds per image and\ngenerates higher quality proposals while Edge-Boxes takes\nonly 0.2 seconds per image; however, it compromises the\nquality of the proposals. With initial experimentation, we\nhave observed that the Selective Search did not produce\ndrastically superior results compared to EdgeBoxes on our\ndataset. We chose to run our experiments with EdgeBoxes with\nthe time consumption concern. The experimantal results of\nusing EdgeBoxes with Fast RCNN network reach an Average\nPrecision of only 20% while it takes 0.134 for detection on\ntop of the 2 seconds to compute the region proposals per\nframe. We ﬁnd 40k iterations to give better results instead\nof the 70k. This approach takes 1.48 hours to train with 40k\nmaximum iterations. We use a subset of our training images\n( each experiment set above 2k and on average 2064 images)\nto train a Deformable Parts Model (DPM) (voc-release4) with\nmemory concerns. It scores an Average Precision of 76% and\nup to an average of 83% with bounding box regression. The\nmean time of object detection by DPM in a test frame is 2.3\nseconds. Each of these experiments are carried with a ten-fold\napproach, using the same randomized video sets for training\nand testing. We show sample results in Figure 6.\nVI. C ONCLUSION\nVideo understanding in RAS has not yet taken advantage\nof the recent advances in deep neural networks. In our paper,\nwe propose an end-to-end deep learning approach for fast\ndetection and localization for RAS video understanding. Our\narchitecture applies a Region Proposal Network (RPN), and\na multimodal convolutional network for object detection, to\njointly predict objectness and localization on a fusion of image\nand temporal motion cues. We also introduce our dataset\nATLAS Dione which provides video data of ten subjects\nperforming six different surgical tasks on the dVSS R⃝ with\nproper tool annotations. Our experimental results demonstrate\nthat our multimodal architecture is superior to similar methods\napproaches and also the conventionally used object detection\n(a) Sample detection results of regular scenes with their scores.\n(b) Sample detection results of challenging scenes with their scores. Here we\ncan see conﬁdent detections with precise boundaries even when the tool is\noccluded or partially out of screen.\nFig. 6. Sample detection results.\nmethods in medical domain with an Average Precision (AP)\nof 91% and a mean computation time of 0.1 seconds per\ntest frame. With our new architecture that supports multi-\nmodality, we improve the results of the architecture proposed\nby Girschick et al. [8]. Although the improvement is small,\nour architecture has repeatedly and consistently scored higher\naccuracy for each experiment set. We believe that making use\nof the temporal motion cues; optic ﬂow, improves the accuracy\nby decreasing false positives. Using a fusion of both RGB\nimage and ﬂow modalities make our system more stable and\nour detections more conﬁdent. These ﬁndings encourage using\nadditional modalities in detection and localization of tools in\nRAS videos. Using the architecture proposed by Girschick et\nal. [8], it is possible to achieve comparable results to ours\nin less time for training and testing, this is mainly because\nour architecture has two different streams of convolutions\nfor RGB input image and ﬂow input image. Although the\nconvolutions are shared for RGB stream and Region Proposal\nNetwork, the ﬂow input is convolved in a separate stream\nbefore fused, almost doubling the time spent for training and\ntesting, that is excluding the time spent to compute ﬂow\nimages as part for preprocessing. We believe we could further\n8\nimprove our accuracy following the multimodal approach; pre-\ntraining on similar ﬂow data and transferring its weights to\ninitialize our model could help us further reﬁne our model.\nOur results show that using Region Proposal Network jointly\nwith detection network, whether it is the new multimodal\narchitecture we propose or the one proposed by Girschick et\nal. [8], dramatically improves the accuracy and reduces the\ncomputation time for detection in each frame. We believe our\nstudy and dataset will form a benchmark for future studies.\nACKNOWLEDGMENT\nWe would like to thank our interns: Lauren Samar, who\nis a Biomedical Engineering student at Rochester Institute of\nTechnology, and Basel Ahmad, who is a Biomedical Sciences\nstudent at SUNY Buffalo, for manual annotation of the data.\nWe would like to acknowledge the ten surgeons working at\nRPCI who participated this IRB approved study (I 228012).\nWe use the Caffe framework by Jia et al. [39] for our\nexperiments and the optical ﬂow estimation code by Brox et al.\n[40], for estimating the temporal motion cues between video\nframes.\nREFERENCES\n[1] M. Meadows. Robots Lend a Helping Hand to Surgeons, U.S. Food and\nDrug Administration (FDA) Consumer Magazine 36(3), 2002.\n[2] S. Kumar, N. Ahmidi, G. Hager, P. Singhal, J. J. Corso and V . Krovi.\nSurgical performance assessment. ASME Dynamics Systems and Control\nMagazine, 3(3):7–10, 2015.\n[3] K. A. Guru et al., Cognitive Skills Assessment during Robot-Assisted\nSurgery: Separating Wheat from Chaff, British Journal of Urology (BJU)\nInternational, 2014.\n[4] Y . LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard\nand L. Jackel, Backpropagation Applied to Handwritten Zip Code Recog-\nnition, Neural Computation, 1989.\n[5] Y . LeCun, L. Bottou, Y . Bengio and P. Haffner, Gradient-based Learning\nApplied to Document Recognition, Proceedings of Institute of Electrical\nand Electronics Engineers (IEEE), 1998.\n[6] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z.\nHuang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg and L. Fei-\nFei, ImageNet Large Scale Visual Recognition Challenge, International\nJournal of Computer Vision (IJCV), 2015.\n[7] A. Krizhevsky, I. Sutskever and G.E. Hinton, ImageNet Classiﬁcation\nwith Deep Convolutional Neural Networks, Neural Information Process-\ning Systems (NIPS), 2012.\n[8] S. Ren, K. He, R. Girshick, J. Sun, Faster R-CNN: Towards Real-Time\nObject Detection with Region Proposal Networks, Neural Information\nProcessing Systems (NIPS), 2015.\n[9] M. Groeger, K. Arbter, and G. Hirzinger, Motion Tracking for Minimally\nInvasive Robotic Surgery, Medical Robotics, I-Tech Education and Pub-\nlishing, pages 117148, 2008.\n[10] G. Q. Wei, K. Arbter, and G. Hirzinger, Automatic Tracking of Laparo-\nscopic Instruments by Color Coding, First Joint Conference, Computer\nVision, Virtual Reality and Robotics in Medicine and Medical Robotics\nand Computer-Assisted Surgery (CVRMed-MRCAS), pages 357366.\nSpringer, 1997.\n[11] A. Krupa, J. Gangloff, C. Doignon, M.F. de Mathelin, G. Morel, J. Leroy,\nL. Soler, and J. Marescaux, Autonomous 3D Positioning of Surgical\nInstruments in Robotized Laparoscopic Surgery Using Visual Servoing,\nInstitute of Electrical and Electronics Engineers (IEEE) Transactions on\nRobotics and Automation, 19(5):842853, 2003.\n[12] X. Zhang and S. Payandeh, Application of Visual Tracking for Robot-\nAssisted Laparoscopic Surgery, Journal of Robotic Systems, vol. 19, no.\n7, pp. 315328, 2002.\n[13] A. Reiter, T. Zhao, and P.K Allen, Appearance Learning for 3D Tracking\nof Robotic Surgical Tools, International Journal of Robotics Research\n(IJRR), 2014.\n[14] S. Kumar, M.S. Narayanan, P. Singhal, J. Corso and V . Krovi, Product of\nTracking Experts for Surgical Tool Visual Tracking, Institute of Electrical\nand Electronics Engineers (IEEE) Conference on Automation Science\nand Engineering, 2013. tistical Physics and Matching Problems, Neural\nComputation, 2, 1990, pp 1-24.\n[15] X. Du, M. Allan et al., Combined 2D and 3D Tracking of Surgical\nInstruments for Minimally Invasive and Robotic Assisted Surgery, Infor-\nmation Processing in Computer-Assisted Interventions (IPCAI), 2016.\n[16] D. Lowe, Distinctive Image Features from Scale-Invariant Keypoints,\nInternational Journal of Computer Vision (IJCV), 2004.\n[17] D.H. Ballard, Generalizing the Hough Transform to Detect Arbitrary\nShapes, Pattern Recognition, V ol.13, No.2, p.111-122, 1981.\n[18] M. Allan, P. Chang et. al., Image Based Surgical Instrument Pose\nEstimation with Multi-Class Labelling and Optical Flow, Medical Image\nComputing and Computer Assisted Intervention (MICCAI), 2015.\n[19] A. Reiter, P.K. Allen and T. Zhao, Marker-less Articulated Surgical Tool\nDetection, Computer Assisted Radiology and Surgery, 2012.\n[20] R. Sznitman, C. Becker and P. Fua, Fast Part-Based Classiﬁcation\nfor Instrument Detection in Minimally Invasive Surgery, Medical Image\nComputing and Computer Assisted Intervention (MICCAI), 2014.\n[21] P. Felzenszwalb, D. McAllester, and D. Ramanan, A Discriminatively\nTrained, Multiscale, Deformable Part Model, Conference on Computer\nVision and Pattern Recognition (CVPR), 2008.\n[22] P. Felzenszwalb, R. Girshick, D. McAllester, D. Ramanan, Object\nDetection with Discriminatively Trained Part Based Models, Pattern\nAnalysis and Machine Intelligence, Institute of Electrical and Electronics\nEngineers (IEEE) Transactions, 2010.\n[23] N. Dalal and B. Triggs, Histograms of Oriented Gradients for Human\nDetection, Conference on Computer Vision and Pattern Recognition\n(CVPR), 2005.\n[24] M. D. Zeiler, R. Fergus, Visualizing and Understanding Convolutional\nNetworks, European Conference on Computer Vision (ECCV), 2014.\n[25] Ross Girshick, Fast R-CNN, International Conference on Computer\nVision (ICCV), 2015.\n[26] Gao, Yixin et al., The JHU-ISI Gesture and Skill Assessment Working\nSet (JIGSAWS): A Surgical Activity Dataset for Human Motion Mod-\neling, In Modeling and Monitoring of Computer Assisted Interventions\n(M2CAI) MICCAI Workshop, 2014.\n[27] Workshop and Challenges on Modeling and Monitoring of Computer\nAssisted Interventions, http://camma.u-strasbg.fr/m2cai2016\n[28] A. P. Stegemann et al., Fundamental Skills of Robotic Surgery: A Multi-\nInstitutional Randomized Controlled Trial for Validation of a Simulation-\nBased Curriculum, Urology 81.4 (2013) : 767-774, 2013.\n[29] P. Doll ´ar, C. Wojek, B. Schiele and P. Perona, Pedestrian Detection:\nA Benchmark, Conference on Computer Vision and Pattern Recognition\n(CVPR), 2009.\n[30] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn and A.\nZisserman, International Journal of Computer Vision (IJCV), 2010.\n[31] S. E. Dreyfuss and H.L. Dreyfuss, A Five-Stage Model of the Mental\nActivities Involved in Directed Skill Acquisition, Operations Research\nCenter 1-18, 1980.\n[32] B. Thomas et al., High accuracy optical ﬂow estimation based on a\ntheory for warping, European Conference on Computer Vision (ECCV),\n2004.\n[33] J. A. Baxter, Model for Inductive Bias Learning, Journal of Artiﬁcial\nIntelligence Research, 12:149198, 2000.\n[34] R. Caruana, Multi-task Learning: A Knowledge-based Source of Induc-\ntive Bias, Machine Learning, 28:4175, 1997.\n[35] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich Feature Hierarchies\nfor Accurate Object Detection and Semantic Segmentation, Conference\non Computer Vision and Pattern Recognition (CVPR), 2014.\n[36] C. L. Zitnick and P. Dollar, Edge boxes: Locating object proposals from\nedges, European Conference on Computer Vision (ECCV), 2014.\n[37] J. R. R. Uijlings, K.E.A. van de Sande, T. Gevers, A.W.M. Smeulders,\nSelective Search for Object Recognition, International Journal of Com-\nputer Vision (IJCV), 2013.\n[38] K. He, X. Zhang, S. Ren, J. Sun, Spatial Pyramid Pooling in Deep\nConvolutional Networks for Visual Recognition, European Conference\non Computer Vision (ECCV), 2014.\n[39] Y . Jia, E. Shelhamer et al., Caffe: Convolutional Architecture for Fast\nFeature Embedding, Proceedings of the ACM International Conference\non Multimedia, 2014.\n[40] B. Thomas et al., High accuracy optical ﬂow estimation based on a\ntheory for warping, European Conference on Computer Vision (ECCV),\n2004.",
  "topic": null,
  "concepts": []
}