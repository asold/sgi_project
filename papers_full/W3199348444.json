{
  "title": "Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers",
  "url": "https://openalex.org/W3199348444",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2891256592",
      "name": "Jason Phang",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2328060875",
      "name": "Haokun Liu",
      "affiliations": [
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A1967404238",
      "name": "Samuel R. Bowman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W3022717752",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2408074187",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3115586018",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3034487470",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2949558627"
  ],
  "abstract": "Despite the success of fine-tuning pretrained language encoders like BERT for downstream natural language understanding (NLU) tasks, it is still poorly understood how neural networks change after fine-tuning. In this work, we use centered kernel alignment (CKA), a method for comparing learned representations, to measure the similarity of representations in task-tuned models across layers. In experiments across twelve NLU tasks, we discover a consistent block diagonal structure in the similarity of representations within fine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of earlier and later layers, but not between them. The similarity of later layer representations implies that later layers only marginally contribute to task performance, and we verify in experiments that the top few layers of fine-tuned Transformers can be discarded without hurting performance, even with no further tuning.",
  "full_text": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538\nOnline, November 11, 2021. ©2021 Association for Computational Linguistics\n529\nFine-Tuned Transformers Show Clusters of\nSimilar Representations Across Layers\nJason Phang1, Haokun Liu2, Samuel R. Bowman134\n1Center for Data Science, New York University\n2Dept. of Computer Science, University of North Carolina at Chapel Hill\n3Dept. of Linguistics, New York University\n4Dept. of Computer Science, New York University\nCorrespondence: jasonphang@nyu.edu\nAbstract\nDespite the success of ﬁne-tuning pretrained\nlanguage encoders like BERT for downstream\nnatural language understanding (NLU) tasks,\nit is still poorly understood how neural net-\nworks change after ﬁne-tuning. In this work,\nwe use centered kernel alignment (CKA), a\nmethod for comparing learned representations,\nto measure the similarity of representations in\ntask-tuned models across layers. In experi-\nments across twelve NLU tasks, we discover\na consistent block diagonal structure in the\nsimilarity of representations within ﬁne-tuned\nRoBERTa and ALBERT models, with strong\nsimilarity within clusters of earlier and later\nlayers, but not between them. The similarity\nof later layer representations implies that later\nlayers only marginally contribute to task per-\nformance, and we verify in experiments that\nthe top few layers of ﬁne-tuned Transformers\ncan be discarded without hurting performance,\neven with no further tuning.\n1 Introduction\nFine-tuning pretrained language encoders such as\nBERT (Devlin et al., 2019) and its successors (Liu\net al., 2019b; Lan et al., 2020; Clark et al., 2020;\nHe et al., 2020) has proven to be highly success-\nful, attaining state-of-the-art performance on many\nlanguage tasks, but how do these models internally\nrepresent task-speciﬁc knowledge?\nIn this work, we study how learned representa-\ntions change through ﬁne-tuning by studying the\nsimilarity of representations between layers of un-\ntuned and task-tuned models. We use centered\nkernel alignment (CKA; Kornblith et al., 2019)\nto measure representation similarity and conduct\nextensive experiments across three pretrained en-\ncoders and twelve language understanding tasks.\nWe discover a consistent, block diagonal struc-\nture (Figure 1c,d) in the similarity of learned rep-\nresentations for almost all task-tuned RoBERTa\nOrig layers\n0\n8\n16\n24Orig layers\n(a) ORIG ORIG\nOrig layers\nFT layers\n(b) FT ORIG\n0 8 16 24\nFT layers\n0\n8\n16\n24FT layers\n(c) FT FT\n0 8 16 24\nFT (run 2) layers\nFT layers\n(d) FT[1] FT[2]\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 1: CKA similarity scores of CLS (classiﬁer to-\nken) representations of ORIG (untuned ALBERT) and\nFT (ﬁne-tuned) models on RTE, across different layers\nof the model. FT[1]–FT[2] compares two RTE models\nwith different random restarts. ORIG–ORIG and FT–\nFT are symmetric by construction. Fine-tuned models\nexhibit a block-diagonal structure in the representation\nsimilarities. The same color scale is used in all plots.\nand ALBERT models, where early layer represen-\ntations and later layer representations form two\ndistinct clusters, with high intra-cluster and low\ninter-cluster similarity.\nGiven the strong representation similarity of later\nmodel layers, we hypothesize that many of the\nlater layers only marginally contribute to task per-\nformance. We show in experiments that the later\nlayers of task-tuned RoBERTa and ALBERT can\nindeed be discarded with minimal impact to perfor-\nmance, even without any further ﬁne-tuning.\n2 Experimental Setup\nModels For the majority of our experiments, we\nconsider three commonly used language-encoding\nmodels: RoBERTa (Liu et al., 2019b), ALBERT\n(Lan et al., 2020) and ELECTRA (Clark et al.,\n2020). Because of the large number of exper-\n530\niments being performed, we use RoBERTa BASE ,\nALBERTLARGE V2 and ELECTRABASE rather than\nthe largest available versions of these models.\nTasks We use the tasks included in the GLUE\nbenchmark (Wang et al., 2018) excluding the data-\npoor WNLI, namely: CoLA (Warstadt et al., 2019),\nMNLI (Williams et al., 2018), MRPC (Dolan and\nBrockett, 2005), QNLI (Rajpurkar et al., 2016),\nQQP,1 RTE (Dagan et al., 2005), SST-2 (Socher\net al., 2013), and STS-B (Cer et al., 2017). We in-\nclude four additional tasks to cover a more diverse\nset of task formats and difﬁculties: BoolQ (Clark\net al., 2019) and Yelp Review Polarity (Zhang et al.,\n2015) classiﬁcation tasks, and HellaSwag (Zellers\net al., 2019) and CosmosQA (Huang et al., 2019)\nmultiple-choice tasks.\nOptimization The representations learned over\nthe course of training and similarity of representa-\ntions may be sensitive to the number of steps used\nin training. To control for this, and to avoid task-\nspeciﬁc hyperparameter tuning, we ﬁne-tune on\neach task for up to 10,000 steps. We use the Adam\n(Kingma and Ba, 2014) optimizer with batch size\nof 4, a learning rate of 1e-5, and 1,000 warmup\noptimization steps.\nWe use the jiant (Phang et al., 2020) library,\nbuilt on Transformers (Wolf et al., 2020) and Py-\nTorch (Paszke et al., 2019), to run our experiments.\n3 Representation Similarity with CKA\nTo analyze how learned representations change\nvia ﬁne-tuning, we use centered kernel alignment\n(CKA; Kornblith et al., 2019) to measure represen-\ntation similarity. CKA is invariant to both orthog-\nonal transformation and isotropic scaling of the\ncompared representations, making it ideal for mea-\nsuring the similarity of neural network representa-\ntions, and has applied to BERT-type models in prior\nwork (Wu et al., 2020; Sridhar and Sarah, 2020).\nGiven two sets of representations X ∈RN×d1 and\nY ∈RN×d1 where N is the number of examples\nand d1, d2 the hidden dimensions, CKA computes\na similarity score between 0 and 1, where a higher\nscore indicates greater similarity. Further details\non CKA are provided in Appendix A.\nUsing CKA, we can compare the similarity of\nrepresentations between different layers of the\n1https://quoradata.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\nsame model or even different models. For our anal-\nysis, we use the representations of the CLS token,\ni.e. the token whose ﬁnal layer representation is\nfed to the task output head. 2 We compute CKA\nover the validation examples of each task.\nTo provide intuition for CKA scores, we ﬁrst\nshow in Figure 1 an example of the comparison\nformats using ALBERT ﬁne-tuned on RTE.\nORIG–ORIG The top left plot shows the sim-\nilarity of representations across the layers of the\nuntuned ALBERT model on RTE inputs. Adjacent\nlayers have high similarity scores, only gradually\ndecreasing as more distant layers are compared.\nFT–ORIG We show layers of the task-tuned\nmodel on the Y-axis and untuned model on the\nX-axis. The CLS representations of the later layers\nin the task-tuned model appear highly dissimilar\nto any of the untuned model: In other words, the\nrepresentations differ starkly from those used for\nALBERT’s masked language modeling (MLM) and\nsentence order prediction (SOP) pretraining. This\ncoheres with prior work showing that representa-\ntions of later layers are most likely to change during\nﬁne-tuning (Kovaleva et al., 2019; Wu et al., 2020).\nFT–FT Next, we compare layers within a sin-\ngle ﬁne-tuned model. We observe a block-diagonal\nstructure in the representation similarities—two dis-\ntinct clusters of earlier (approx. ﬁrst 10) and later\n(approx. last 14) layers that have high inter-cluster\nbut low intra-cluster similarity. When considered\ntogether with FT-ORIG, we can infer that the ear-\nlier layer representations resemble those used for\npretraining, whereas the later layers encode a rep-\nresentation suitable for tackling the task. The high\ninternal similarity between the top few layers and\nthe sharp block diagonal structure of the similarity\nmatrix imply that the representations starkly differ.\nFT[1]–FT[2] Finally, we compare ﬁne-tuned\nALBERT models across two random restarts. We\nobserve a similar block diagonal structure. In par-\nticular, the similarity of the CLS representations in\nthe later layers indicates that CKA is able recover\nthe similarity of representations for tackling the\nsame task across random restarts. This likely arises\nas the models are ﬁne-tuned from the same initial\npretrained parameters.\n2RoBERTa uses a<s> token instead, but for brevity and\nconsistency, we will refer to it as CLS as well.\n531\n0 4 8 12\nBoolQ CoLA MNLI\n0 8 16 24\nBoolQ CoLA MNLI\n0 4 8 12\nBoolQ CoLA MNLI\n0 4 8 12\nMRPC QQP QNLI\n0 8 16 24\nMRPC QQP QNLI\n0 4 8 12\nMRPC QQP QNLI\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 8 16 24\nRTE STS-B Yelp Polarity\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\n0 8 16 24\n0 8 16 24\nSST-2\n0 8 16 24\nCosmos QA\n0 8 16 24\nHellaSwag\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\nRoBERTa ALBERT ELECTRA\nFigure 2: Representation similarity between layers for task-tuned models (FT–FT). RoBERTa and ALBERT task\nmodels exhibit a ‘block diagonal‘ structure in the representation similarity ofCLS tokens across nearly all tasks.\n3.1 Results\nWe extend our CKA analysis to all twelve tasks\nand all three pretrained models, showing the FT-FT\nresults in Figure 2. We observe that the block diag-\nonal structure of representation similarity identiﬁed\nin Section 3 appears in almost every RoBERTa and\nALBERT model, sharply delineating the earlier and\nlater clusters of representations. In fact, RoBERTa\noften has even more distinct clusters than ALBERT.\nWe hypothesize that since ALBERT shares param-\neters across layers, it is more difﬁcult for repre-\nsentations to sharply change across a single layer,\nwhereas RoBERTa, which has no parameter shar-\ning, has no such constraint.\nThe signiﬁcant similarity of the later layers sug-\ngests that many of the later layers may not con-\ntribute much to the task. Given residual con-\nnections between Transformer layers, later layers\ncould learn a ‘no-op’ or only slightly adjust the\noutput representation if the task can be adequately\n‘solved’ at an earlier layer. If this is true, we should\nbe able to feed an intermediate representation from\nlater layers to the output head with no further ﬁne-\ntuning and retain most of the task performance. We\ninvestigate this hypothesis in Section 4.\nIn contrast, we do not see the same pattern in\nthe ELECTRA models. The representations of the\nlater layers are generally highly dissimilar even up\nto the penultimate layer in many tasks. A few tasks\ndo exhibit a minor block diagonal structure, such\nas STS-B, Yelp Polarity and SST-2, but it is far less\napparent compared to the other two models. ELEC-\nTRA has a very different pretraining task from the\nother two models (replaced token detection), which\nmay explain this difference.\nWe see complementary results for FT–ORIG\nand FT[1]–FT[2] in Figure 5 and Figure 6. For\nRoBERTa and ALBERT, while the earlier layers of\nthe task models have similar CLS representations\nto the untuned models, the later layers are largely\ndissimilar to any layer in the base model.\n4 Truncating Fine-tuned Models\nTo test our hypothesis that the later layers of tuned\ntask-models only marginally contribute to task per-\nformance, we propose a simple experiment where\nwe feed the representations from an intermediate\nlayer directly to the task output head, effectively\ndiscarding the later layers. We refer to these as\ntruncated models. We test three different conﬁg-\nurations: (a) UNTUNED , where we feed interme-\ndiate representations from a ﬁne-tuned model to\nthe tuned task output head without any further ﬁne-\ntuning, (b) TUNED , where we ﬁne-tune only the\noutput head, and (c) TUNED ORIG , where we use\nrepresentations from the base model (not ﬁne-tuned\non the task), but we ﬁne-tune the output head. Per-\nformance of the UNTUNED trunated models indi-\ncates the extent to which an intermediate represen-\ntation can be directly substituted for the ﬁnal layer’s\nrepresentation; the TUNED and TUNED ORIG mod-\nels provide an upper-bound of performance using\nthe CLS representation of a given layer of a ﬁne-\ntuned and non-ﬁne-tuned encoder respectively.\nOur results are shown in Figure 3. For RoBERTa\nand ALBERT, we ﬁnd that theUNTUNED truncated\nmodels perform comparably to the Tuned truncated\nand full ﬁne-tuned models3 at the later layers. For\n3An UNTUNED model using the ﬁnal layer representation\n532\n0.6\n0.7\n0.8\nBoolQ\n0.0\n0.3\n0.6\nCoLA\n0.4\n0.6\n0.8\nMNLI\n0.4\n0.6\n0.8\nBoolQ\n0.0\n0.3\n0.6\nCoLA\n0.4\n0.6\n0.8\nMNLI\n0.4\n0.6\n0.8\nBoolQ\n0.1\n0.4\n0.7\n CoLA\n0.3\n0.6\n0.9\n MNLI\n0.3\n0.6\n0.9\nMRPC\n0.3\n0.6\n0.9\n QQP\n0.5\n0.7\n0.9\n QNLI\n0.4\n0.7\n1.0\n MRPC\n0.3\n0.6\n0.9\n QQP\n0.5\n0.7\n0.9\nQNLI\n0.3\n0.6\n0.9\nMRPC\n0.3\n0.6\n0.9\n QQP\n0.5\n0.7\n0.9\n QNLI\n0.6\n0.8\n RTE\n-0.4\n0.9\nSTS-B\n0.6\n0.8\n1.0\n Yelp Polarity\n0.6\n0.8\nRTE\n0.3\n0.6\n0.9\nSTS-B\n0.6\n0.8\n1.0\n Yelp Polarity\n0.6\n0.8\nRTE\n0.3\n0.6\n0.9\nSTS-B\n0.6\n0.8\n1.0\n Yelp Polarity\n0 4 8 12\n0.6\n0.8\n1.0\n SST-2\n0 4 8 12\n0.4\n0.6\n Cosmos QA\n0 4 8 12\n0.3\n0.4\n0.5\n HellaSwag\n0 8 16 24\n0.6\n0.8\n1.0\n SST-2\n0 8 16 24\n0.4\n0.6\n Cosmos QA\n0 8 16 24\n0.3\n0.4\n0.5\n HellaSwag\n0 4 8 12\n0.6\n0.8\n1.0\n SST-2\n0 4 8 12\n0.3\n0.5\n0.7\n Cosmos QA\n0 4 8 12\n0.4\n0.6\n0.8\n HellaSwag\nRoBERTa ALBERT ELECTRA\nUntuned Tuned TunedOrig\nFigure 3: Model Truncation Experiments: Task performance (Y-axis) when feeding representation from an in-\ntermediate layer (X-axis) directly to the task output head, equivalent to discarding the top layers of the model.\nUNTUNED (green), uses a task-tuned encoder, but no further ﬁne-tuning of the task-tuned output head. T UNED\n(blue), involves further ﬁne-tuning the output head on the intermediate representation. TUNED ORIG (yellow) uses\nthe pretrained encoder, but the output head is ﬁne-tuned. For RoBERTa and ALBERT, the top few layers bcan e\ndiscarded for many tasks in either TUNED or UNTUNED conﬁgurations without hurting performance. The majority\nclass baseline is shown with a red dotted line, while the rightmost data-point corresponds to a full model with no\ntruncation.\ninstance, the top 4 layers of the RoBERTa for Yelp\nPolarity model can be discarded with no further\ntuning and minimal impact to performance (95.5\nvs 96.1). On the other hand, TUNED ORIG mod-\nels perform very poorly compared to the TUNED\nmodels across all layers, showing that task-tuned in-\ntermediate representations are crucial for good per-\nformance, even when ﬁne-tuning the output head.\nFor ALBERT, which shares parameters between\nlayers, a larger fraction of layers can be discarded\nwith minimal impact to performance for both UN-\nTUNED and TUNED truncated models.\nOn the other hand, we do not ﬁnd a similar pat-\ntern in ELECTRA models. The UNTUNED trun-\ncated models perform extremely poorly when dis-\ncarding almost any number of layers, and even\nthe TUNED truncated models quickly drop in per-\nformance with even one or two layers discarded.\nThese results are consistent with our CKA analyses\nthat showed that the learned and task-tuned repre-\nsentations for ELECTRA do not share the same\nstructure as those of RoBERTa and ALBERT. We\nspeculate that this differences stems from the dif-\nferent pretraining objectives—replaced token de-\ntection is a binary prediction problem, whereas\nmasked language modeling involves predicting a\ndistribution over a large number of tokens—leading\nto differences in learned representations that prop-\nagate even to ﬁne-tuned models. We leave further\nis equivalent to a regular ﬁne-tuned model.\ninvestigation these differences to future work.\n4.1 Skipping Layers\nWe perform a smaller set of experiments on skip-\nping intermediate layers in a model and measuring\nthe impact on performance. We use fully ﬁne-tuned\nRoBERTa models on a subset of the tasks we con-\nsidered above, and evaluate task performance of\nthe tuned models when we skip over contiguous\nspans of layers in the model without any further\nﬁne-tuning. We show the results for skipping ev-\nery possible span of layers in Fig 4. Performance\ntends to drop as larger spans of layers are skipped,\nalthough in many cases skipping any single layer\nseems to make little to no impact to performance.\nThe primary exception to this is the very ﬁrst layer,\nwhere we observe that skipping just the ﬁrst layer\ncan heavily impact task performance, such as in\nCoLA, STS-B and Cosmos QA. On the other hand,\nwe ﬁnd that skipping multiple of the later layers\ncan have minimal impact on performance, consis-\ntent with our results above. The proﬁle of per-\nformance drops given the number of intermediate\nlayers skipped also differs greatly across tasks: For\ninstance, dropping more than two contiguous lay-\ners in the middle of the model seems to heavily\nimpact MNLI and RTE performance, whereas for\nSST-2 the impact is not as large until 3-4 layers are\nskipped.\n533\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\nFirst skipped layer\n0.63 0.62 0.61 0.62 0.62 0.62 0.61 0.62 0.62 0.62 0.62 0.62 0.62\n0.77 0.72 0.67 0.63 0.63 0.58 0.62 0.62 0.62 0.62 0.62 0.62\n0.78 0.73 0.67 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62\n0.78 0.71 0.67 0.64 0.62 0.62 0.62 0.62 0.62 0.62\n0.76 0.67 0.62 0.64 0.62 0.62 0.62 0.62 0.62\n0.73 0.67 0.66 0.63 0.62 0.62 0.62 0.62\n0.76 0.70 0.65 0.62 0.62 0.62 0.62\n0.77 0.71 0.63 0.62 0.62 0.62\n0.80 0.67 0.63 0.62 0.62\n0.81 0.74 0.63 0.63\n0.81 0.80 0.80\n0.81 0.81\n0.80\nBoolQ\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\n0.01 -0.01 0.07 -0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n0.57 0.40 0.20 0.13 0.16 0.12 -0.00 0.00 0.00 -0.02 0.00 0.00\n0.58 0.47 0.32 0.35 0.30 0.07 0.00 0.00 -0.01 0.00 0.00\n0.59 0.46 0.46 0.43 0.15 -0.02 0.00 0.03 0.00 0.00\n0.54 0.51 0.52 0.22 0.04 0.00 0.09 0.00 0.00\n0.61 0.61 0.35 0.10 0.00 0.13 0.00 0.00\n0.60 0.46 0.33 0.05 0.13 0.00 0.00\n0.51 0.48 0.07 0.05 0.00 0.00\n0.53 0.22 0.05 0.00 0.00\n0.41 0.26 0.00 0.00\n0.59 0.05 0.05\n0.58 0.58\n0.60\nCoLA\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\nFirst skipped layer\n0.54 0.36 0.36 0.34 0.35 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32\n0.76 0.66 0.43 0.35 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32\n0.76 0.59 0.43 0.35 0.32 0.32 0.32 0.32 0.32 0.32 0.32\n0.77 0.66 0.43 0.32 0.32 0.32 0.32 0.32 0.32 0.32\n0.76 0.57 0.36 0.32 0.32 0.32 0.32 0.32 0.32\n0.73 0.55 0.33 0.32 0.32 0.32 0.32 0.32\n0.76 0.39 0.33 0.32 0.32 0.32 0.32\n0.59 0.35 0.32 0.32 0.32 0.32\n0.71 0.41 0.32 0.32 0.32\n0.80 0.77 0.38 0.38\n0.81 0.79 0.79\n0.81 0.81\n0.81\nMNLI\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\n0.76 0.75 0.74 0.75 0.31 0.44 0.16 0.16 0.16 0.16 0.75 0.16 0.16\n0.83 0.78 0.20 0.16 0.16 0.16 0.17 0.16 0.16 0.16 0.16 0.16\n0.84 0.38 0.20 0.16 0.16 0.16 0.16 0.16 0.16 0.16 0.16\n0.83 0.50 0.16 0.16 0.17 0.17 0.16 0.16 0.16 0.16\n0.87 0.28 0.19 0.17 0.17 0.16 0.16 0.16 0.16\n0.80 0.76 0.49 0.24 0.16 0.16 0.16 0.16\n0.88 0.86 0.68 0.24 0.16 0.16 0.16\n0.89 0.87 0.57 0.16 0.16 0.16\n0.89 0.89 0.89 0.21 0.21\n0.89 0.90 0.89 0.89\n0.90 0.89 0.89\n0.90 0.90\n0.90\nMRPC\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\nFirst skipped layer\n0.48 0.47 0.46 0.48 0.48 0.49 0.48 0.47 0.47 0.47 0.47 0.47 0.47\n0.73 0.66 0.47 0.47 0.47 0.47 0.47 0.48 0.47 0.47 0.47 0.47\n0.74 0.53 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47\n0.73 0.52 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47\n0.73 0.48 0.47 0.47 0.47 0.47 0.47 0.47 0.47\n0.71 0.48 0.47 0.47 0.47 0.47 0.47 0.47\n0.70 0.47 0.47 0.47 0.47 0.47 0.47\n0.71 0.49 0.48 0.47 0.47 0.47\n0.77 0.72 0.52 0.47 0.47\n0.78 0.78 0.75 0.75\n0.78 0.78 0.78\n0.78 0.78\n0.77\nRTE\n0 1 2 3 4 5 6 7 8 9 10 11 12\n0123456789101112\n0.50 0.05 0.15 0.05 -0.02 0.03 0.01 0.01 -0.02 -0.05 -0.01 0.00 0.00\n0.87 0.74 0.50 0.31 0.14 -0.10 -0.16 0.00 -0.00 0.02 -0.11 -0.11\n0.87 0.74 0.43 0.21 0.14 -0.03 -0.03 -0.01 -0.01 -0.13 -0.13\n0.89 0.81 0.52 0.29 0.01 -0.03 -0.02 0.01 -0.13 -0.13\n0.89 0.79 0.65 0.28 0.07 0.05 0.05 -0.01 -0.01\n0.87 0.83 0.64 0.20 0.05 0.08 -0.20 -0.20\n0.88 0.85 0.54 0.28 -0.17 0.24 0.24\n0.90 0.83 0.35 -0.49 -0.46 -0.46\n0.90 0.67 0.06 0.36 0.36\n0.90 0.66 0.76 0.76\n0.89 0.87 0.87\n0.90 0.90\n0.91\nSTS-B\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLast skipped layer (inclusive)\n0123456789101112\nFirst skipped layer\n0.60 0.51 0.51 0.62 0.51 0.51 0.51 0.51 0.51 0.51 0.50 0.49 0.49\n0.91 0.88 0.78 0.64 0.67 0.58 0.54 0.52 0.51 0.51 0.51 0.51\n0.92 0.88 0.77 0.82 0.69 0.61 0.51 0.51 0.51 0.51 0.51\n0.92 0.87 0.87 0.77 0.57 0.53 0.51 0.51 0.51 0.51\n0.91 0.90 0.86 0.80 0.55 0.51 0.51 0.51 0.51\n0.92 0.90 0.85 0.58 0.51 0.51 0.51 0.51\n0.91 0.90 0.80 0.51 0.51 0.51 0.51\n0.91 0.90 0.75 0.51 0.51 0.51\n0.93 0.93 0.88 0.51 0.51\n0.93 0.93 0.90 0.90\n0.93 0.93 0.93\n0.93 0.93\n0.93\nSST-2\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLast skipped layer (inclusive)\n0123456789101112\n0.27 0.21 0.22 0.30 0.31 0.26 0.32 0.31 0.33 0.34 0.29 0.25 0.25\n0.51 0.42 0.42 0.39 0.38 0.36 0.33 0.33 0.32 0.31 0.24 0.24\n0.53 0.47 0.43 0.44 0.39 0.35 0.31 0.32 0.31 0.24 0.24\n0.52 0.47 0.47 0.44 0.35 0.34 0.31 0.30 0.24 0.24\n0.54 0.52 0.47 0.36 0.34 0.33 0.33 0.27 0.27\n0.56 0.52 0.38 0.41 0.41 0.39 0.28 0.28\n0.54 0.50 0.48 0.49 0.43 0.28 0.28\n0.55 0.49 0.53 0.50 0.41 0.41\n0.56 0.54 0.54 0.48 0.48\n0.57 0.56 0.55 0.55\n0.56 0.56 0.56\n0.57 0.57\n0.56\nCosmos QA\nFigure 4: Layer Experiments: Task performance when skipping contiguous spans of Transformer layers, with the\nY-axis and X-axis indicating the ﬁrst and last (inclusive) skipped layers, with no further ﬁne-tuning. Performance\ntends to drop as more layers are skipped, but in many cases skipping any single layer makes little to no impact to\nperformance, except for the ﬁrst layer. Consistent with results above, many of the higher layers can be skipped\nwith minimal impact to performance.\n534\n5 Related Work\nWhile CKA (Kornblith et al., 2019) was initially\nproposed as an interpretability method for com-\nputer vision models, it has more recently seen ap-\nplication to NLP models. Wu et al. (2020) ap-\nplied CKA to pretrained Transformers models such\nas BERT and GPT-2, focusing on cross-model\ncomparison—our analysis builds on their ﬁndings,\nwith greater focus on layer-wise comparisons and\nimplications for ﬁne-tuning and discarding layers.\nSridhar and Sarah (2020) use CKA to measure the\nimpact of a proposed model architecture change on\nthe learned representations. V oita et al. (2019) and\nMerchant et al. (2020) apply similar representation\nsimilarity analyses to Transformers, with the lat-\nter also investigating freezing and dropping layers\nfrom models.\nMore broadly, signiﬁcant work has been done on\nbetter understanding and interpreting the capabil-\nities of BERT-type models—Rogers et al. (2020)\noffers a thorough survey of this line of work. Of\nparticular relevance to our work: Work on model\nprobing (Tenney et al., 2019b; Liu et al., 2019a;\nTenney et al., 2019a) has studied the extent to syn-\ntactic and semantic features are represented at dif-\nferent layers of BERT-type models.\nOur results on model truncation also cohere with\nexisting work on early exit in BERT models(Xin\net al., 2020a,b; Zhou et al., 2020), wherein models\nare explicitly ﬁne-tuned to dynamically skip the\nlater layers of a BERT encoder and directly to the\noutput head, often to reduce inference times of\nmodels. Our results somewhat differ as we show\nthat models can also be truncated or exited early\nwithout any explicit tuning. It has also been shown\nin the computer vision domain that models with\nresidual networks work akin to an ensemble of deep\nand shallow models (Veit et al., 2016).\n6 Conclusion\nWe show a consistent pattern to the structure of\nrepresentation similarity in task-tuned RoBERTa\nand ALBERT models, with strong representation\nsimilarity within clusters of earlier and later lay-\ners, but not between them. We further show that\nthe later layers of task-tuned RoBERTa and AL-\nBERT models can often be discarded without hurt-\ning task performance, verifying that the later layers\nof these models truly have similar representations.\nHowever, we ﬁnd that ELECTRA models exhibit\nstarkly different properties from the other two mod-\nels, which prompts further investigation into how\nand why these models differ.\nAcknowledgments\nWe would like to thank Kyunghyun Cho for his\ninvaluable feedback on this work. This project\nhas beneﬁted from ﬁnancial support to SB by Eric\nand Wendy Schmidt (made by recommendation of\nthe Schmidt Futures program), Samsung Research\n(under the project Improving Deep Learning us-\ning Latent Structure), Apple, and Intuit, and from\nin-kind support by the NYU High-Performance\nComputing Center and by NVIDIA Corporation\n(with the donation of a Titan V GPU). This mate-\nrial is based upon work supported by the National\nScience Foundation under Grant Nos. 1922658 and\n2046556. Any opinions, ﬁndings, and conclusions\nor recommendations expressed in this material are\nthose of the author(s) and do not necessarily reﬂect\nthe views of the National Science Foundation.\nReferences\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\n535\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn International Workshop on Paraphrasing.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2391–2401, Hong Kong, China. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference for Learning Representations,\nSan Diego, 2015.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey E. Hinton. 2019. Similarity of\nneural network representations revisited. CoRR,\nabs/1905.00414.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. Unpublished manuscript available on arXiv.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT em-\nbeddings during ﬁne-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP , pages 33–44,\nOnline. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn Imperative Style, High-Performance Deep Learn-\ning Library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nJason Phang, Phil Yeres, Jesse Swanson, Haokun\nLiu, Ian F. Tenney, Phu Mon Htut, Clara Va-\nnia, Alex Wang, and Samuel R. Bowman. 2020.\njiant 2.0: A software toolkit for research on\ngeneral-purpose text understanding models. http:\n//jiant.info/.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nSharath Nittur Sridhar and Anthony Sarah. 2020. Un-\ndivided attention: Are intermediate layers necessary\nfor bert?\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\n536\nAndreas Veit, Michael J. Wilber, and Serge J. Be-\nlongie. 2016. Residual networks are exponential\nensembles of relatively shallow networks. volume\nabs/1605.06431.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics (TACL), 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJohn Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Dur-\nrani, Fahim Dalvi, and James Glass. 2020. Similar-\nity analysis of contextual word representation mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 4638–4655, Online. Association for Compu-\ntational Linguistics.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020a. Early exiting BERT for efﬁcient document\nranking. In Proceedings of SustaiNLP: Workshop on\nSimple and Efﬁcient Natural Language Processing ,\npages 83–88, Online. Association for Computational\nLinguistics.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020b. DeeBERT: Dynamic early ex-\niting for accelerating BERT inference. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2246–2251,\nOnline. Association for Computational Linguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can\na machine really ﬁnish your sentence? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4791–\n4800, Florence, Italy. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. Bert loses pa-\ntience: Fast and robust inference with early exit. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 18330–18341. Curran Associates,\nInc.\n537\nA Centered Kernel Alignment\nGiven two sets of representations X ∈RN×d and\nY ∈RN×d where N is the number of examples\nand d the hidden dimension (for instance the CLS\nvector representations of a set of examples from\ntwo different layers of the same model), CKA com-\nputes a similarity score between 0 and 1. :\nCKA(K, L) = HSIC(K, L)√\nHSIC(K, K)HSIC(L, L)\nwith\nHSIC(K, L) = 1\n(n −1)2 tr(KHLH )\nand H = In −1\nb 11T K = XXT , L = Y YT\nwhen using a linear kernel. We refer the reader to\nthe original work (Kornblith et al., 2019) for more\ndetails and properties of CKA.\nB Additional Results\nFigure 5 shows the FT–ORIG plots for all tasks\nand models.\nFigure 6 shows the FT[1]–FT[2] plots for all\ntasks and models.\nFigure 7 computes representation similarity be-\ntween models.\n538\n0 4 8 12\nBoolQ CoLA MNLI\n0 8 16 24\nBoolQ CoLA MNLI\n0 4 8 12\nBoolQ CoLA MNLI\n0 4 8 12\nMRPC QQP QNLI\n0 8 16 24\nMRPC QQP QNLI\n0 4 8 12\nMRPC QQP QNLI\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 8 16 24\nRTE STS-B Yelp Polarity\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\n0 8 16 24\n0 8 16 24\nSST-2\n0 8 16 24\nCosmos QA\n0 8 16 24\nHellaSwag\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\nRoBERTa ALBERT ELECTRA\nFigure 5: CKA representation similarity for FT–ORIG. Task-tuned layers are on the Y-axis, untuned layers in the\nX-axis. CLS representations of the top few layers RoBERTa and ALBERT models are highly dissimilar to those\nof the pretrained model at any layer.\n0 4 8 12\nBoolQ CoLA MNLI\n0 8 16 24\nBoolQ CoLA MNLI\n0 4 8 12\nBoolQ CoLA MNLI\n0 4 8 12\nMRPC QQP QNLI\n0 8 16 24\nMRPC QQP QNLI\n0 4 8 12\nMRPC QQP QNLI\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 8 16 24\nRTE STS-B Yelp Polarity\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\n0 8 16 24\n0 8 16 24\nSST-2\n0 8 16 24\nCosmos QA\n0 8 16 24\nHellaSwag\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\nRoBERTa ALBERT ELECTRA\nFigure 6: CKA representation similarity for FT[1]–FT[2]. RoBERTa and ALBERT task models exhibit a ‘block\ndiagonal‘ structure to representation similarity of CLS tokens, indicating in particular that the representations of\nthe top few layers are highly similar. Plots for tasks that do not use the CLS token are dimmed.\n0 4 8 12\nBoolQ CoLA MNLI\n0 4 8 12\nBoolQ CoLA MNLI\n0 8 16 24\nBoolQ CoLA MNLI\n0 4 8 12\nMRPC QQP QNLI\n0 4 8 12\nMRPC QQP QNLI\n0 8 16 24\nMRPC QQP QNLI\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 4 8 12\nRTE STS-B Yelp Polarity\n0 8 16 24\nRTE STS-B Yelp Polarity\n0 8 16 24\n0 4 8 12\nSST-2\n0 8 16 24\nCosmos QA\n0 8 16 24\nHellaSwag\n0 4 8 12\n0 4 8 12\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\n0 4 8 12\n0 8 16 24\nSST-2\n0 4 8 12\nCosmos QA\n0 4 8 12\nHellaSwag\nALBERT Layers\nRoBERTa Layers\nRoBERTa vs ALBERT\nELECTRA Layers\nRoBERTa Layers\nRoBERTa vs ELECTRA\nELECTRA Layers\nALBERT Layers\nALBERT vs ELECTRA\nFigure 7: CKA representation similarity comparing CLS representations cross models. The upper right blocks\nindicate the representations in the earlier and the later layers are similar even across models.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7128349542617798
    },
    {
      "name": "Computer science",
      "score": 0.6922558546066284
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5751115083694458
    },
    {
      "name": "Encoder",
      "score": 0.5490206480026245
    },
    {
      "name": "Kernel (algebra)",
      "score": 0.48610901832580566
    },
    {
      "name": "Natural language understanding",
      "score": 0.4738484025001526
    },
    {
      "name": "Task (project management)",
      "score": 0.44895729422569275
    },
    {
      "name": "Diagonal",
      "score": 0.44091424345970154
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4234035611152649
    },
    {
      "name": "Natural language processing",
      "score": 0.34885936975479126
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3222755789756775
    },
    {
      "name": "Natural language",
      "score": 0.2557428479194641
    },
    {
      "name": "Voltage",
      "score": 0.1299925148487091
    },
    {
      "name": "Mathematics",
      "score": 0.11231294274330139
    },
    {
      "name": "Engineering",
      "score": 0.09554168581962585
    },
    {
      "name": "Electrical engineering",
      "score": 0.0675736665725708
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}