{
  "title": "Synthesizer: Rethinking Self-Attention in Transformer Models",
  "url": "https://openalex.org/W3021293129",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222427123",
      "name": "Tay, Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578404",
      "name": "Bahri, Dara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177605537",
      "name": "Metzler, Donald",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287876895",
      "name": "Juan, Da-Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126900286",
      "name": "Zhao Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355471923",
      "name": "Zheng Che",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2413794162",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W2740747242",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W2963967365",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2964308564"
  ],
  "abstract": "The dot product self-attention is known to be central and indispensable to state-of-the-art Transformer models. But is it really required? This paper investigates the true importance and contribution of the dot product-based self-attention mechanism on the performance of Transformer models. Via extensive experiments, we find that (1) random alignment matrices surprisingly perform quite competitively and (2) learning attention weights from token-token (query-key) interactions is useful but not that important after all. To this end, we propose \\textsc{Synthesizer}, a model that learns synthetic attention weights without token-token interactions. In our experiments, we first show that simple Synthesizers achieve highly competitive performance when compared against vanilla Transformer models across a range of tasks, including machine translation, language modeling, text generation and GLUE/SuperGLUE benchmarks. When composed with dot product attention, we find that Synthesizers consistently outperform Transformers. Moreover, we conduct additional comparisons of Synthesizers against Dynamic Convolutions, showing that simple Random Synthesizer is not only $60\\%$ faster but also improves perplexity by a relative $3.5\\%$. Finally, we show that simple factorized Synthesizers can outperform Linformers on encoding only tasks.",
  "full_text": "Synthesizer: Rethinking Self-Attention for Transformer Models\nYi Tay1 Dara Bahri1 Donald Metzler1 Da-Cheng Juan1 Zhe Zhao1 Che Zheng1\nAbstract\nThe dot product self-attention is known to be cen-\ntral and indispensable to state-of-the-art Trans-\nformer models. But is it really required? This\npaper investigates the true importance and con-\ntribution of the dot product-based self-attention\nmechanism on the performance of Transformer\nmodels. Via extensive experiments, we ﬁnd that\n(1) random alignment matrices surprisingly per-\nform quite competitively and (2) learning atten-\ntion weights from token-token (query-key) in-\nteractions is useful but not that important after\nall. To this end, we propose SYNTHESIZER ,\na model that learns synthetic attention weights\nwithout token-token interactions. In our exper-\niments, we ﬁrst show that simple Synthesizers\nachieve highly competitive performance when\ncompared against vanilla Transformer models\nacross a range of tasks, including machine trans-\nlation, language modeling, text generation and\nGLUE/SuperGLUE benchmarks. When com-\nposed with dot product attention, we ﬁnd that Syn-\nthesizers consistently outperform Transformers.\nMoreover, we conduct additional comparisons\nof Synthesizers against Dynamic Convolutions,\nshowing that simple Random Synthesizer is not\nonly 60% faster but also improves perplexity by a\nrelative 3.5%. Finally, we show that simple fac-\ntorized Synthesizers can outperform Linformers\non encoding only tasks.\n1. introduction\nTransformer models (Vaswani et al., 2017) have demon-\nstrated success across a wide range of tasks. This has\nresulted in Transformers largely displacing once popular\nauto-regressive and recurrent models in recent years. At the\nheart of Transformer models lies the query-key-value dot\n1Google Research, Mountain View, California. Correspondence\nto: Yi Tay <yitay@google.com>.\nProceedings of the37 th International Conference on Machine\nLearning, Online, PMLR 119, 2020. Copyright 2020 by the au-\nthor(s).\nproduct attention. The success of Transformer models is\nwidely attributed to this self-attention mechanism since fully\nconnected token graphs, which are able to model long-range\ndependencies, provide a robust inductive bias.\nBut is the dot product self-attention really so important? Do\nwe need it? Is it necessary to learn attention weights via\npairwise dot products? This paper seeks to develop a deeper\nunderstanding of the role that the dot product self-attention\nmechanism plays in Transformer models.\nThe fundamental role of dot product self-attention is to learn\nself-alignment, i.e., to determine the relative importance\nof a single token with respect to all other tokens in the\nsequence. To this end, there have been memory metaphors\nand analogies constructed to support this claim. Indeed,\nthe terms query, keys, and values imply that self-attention\nemulates a content-based retrieval process which leverages\npairwise interactions at its very core.\nMoving against convention, this paper postulates that we\ncannot only do without dot product self-attention but also\ncontent-based memory-like self-attention altogether. Tra-\nditionally, attention weights are learned at the instance or\nsample level, where weights are produced by instance-level\npairwise interactions. As a result, these instance-speciﬁc\ninteractions often ﬂuctuate freely across different instances\nas they lack a consistent global context.\nThis paper proposes SYNTHESIZER , a new model that learns\nto synthesize the self-alignment matrix instead of manually\ncomputing pairwise dot products. We propose a diverse suite\nof synthesizing functions and extensively evaluate them. We\ncharacterize the source information that these synthesizing\nfunctions receive, i.e., whether they receive information\nfrom individual tokens, token-token interactions, and/or\nglobal task information. Intuitively, different source inputs\nto the synthesizing functions should capture diverse views,\nwhich may be useful when employed in conjunction.\nAside from generalizing the standard Transformer model,\nwe show that it is possible to achieve competitive results\nwith fully global attention weights that do not consider\ntoken-token interactions or any instance-level (local) infor-\nmation at all. More speciﬁcally, a random matrix SYNTHE -\nSIZER model achieves a 27.27 BLEU score on WMT 2014\narXiv:2005.00743v3  [cs.CL]  24 May 2021\nSynthesizer: Rethinking Self-Attention for Transformer Models\nEnglish-German1. Via a set of rigorous experiments, we\nobserve that the popular and well-established dot-product\ncontent-based attention can be approximated with simpler\nvariants such as random matrices or dense layers without\nsacriﬁcing much performance in some cases.\nIn our experiments, we also show that our relatively simple\nSynthesizer models also outperform Dynamic Convolutions\n(Wu et al., 2019) with a +3.5 % relative improvement in\nperplexity while being 60% faster. On encoding tasks, our\nfactorized Synthesizers can outperform other low-rank efﬁ-\ncient Transformer models such as Linformers (Wang et al.,\n2020).\nWhile simple Synthesizer models are able to perform com-\npetitively, our experiments show that the pairwise dot prod-\nuct is still ultimately helpful. When composing our syn-\nthesizing functions with dot products, we ﬁnd that they\nconsistently improve the performance of Transformers. In\ngeneral, we believe our ﬁndings will spur further investi-\ngation and discussion about the true role and utility of the\nself-attention mechanism in Transformer models.\nOur Contributions Our key contributions are described\nas follows:\n• We propose Synthetic Attention, a new way of learn-\ning to attend without explicitly attending (i.e., without\ndot product attention or content-based attention). In-\nstead, we generate the alignment matrix independent\nof token-token dependencies and explore a potpourri\nof parameterized functions for synthesizing attention\nmatrices.\n• We propose SYNTHESIZER , a new model that lever-\nages Synthetic Attention. The model performs compet-\nitive to state-of-the-art Transformer models on a wide\nrange of language tasks, including machine translation\nand language modeling.\n• Moreover, we show that (1) random learnable align-\nment matrices perform competitively and (2) token-\ntoken dependencies are not necessary to achieve good\nperformance with Transformer models on certain tasks.\n• On large-scale masked language modeling on the\nC4 dataset (Raffel et al., 2019) and ﬁnetuning on\nSuperGLUE and GLUE benchmarks, we show that\nsimple random Synthesizers can outperform/match\nLightweight Dynamic convolutions (Wu et al., 2019)\nalong with outperforming Transformers and Universal\nTransformers (Dehghani et al., 2018). On two encod-\ning tasks, factorized random Synthesizers outperform\nlow-rank Linformers (Wang et al., 2020).\n1The originally reported result is 27.30.\n2. Related Work\nAttention-based models are used across a wide spectrum\nof problem domains. Such models are especially popular,\ndue to their effectiveness, in the language and vision do-\nmains. Attention models can be traced back to the machine\ntranslation models of (Bahdanau et al., 2014) and (Luong\net al., 2015), where attention is employed to learn soft word\nalignments between language pairs. The intuition behind\nthe attention mechanism is deeply-rooted in the notion of\nmemory-based retrieval (Graves et al., 2014; Weston et al.,\n2014), in which soft differentiable addressing of memory\nwas initially proposed.\nThe paradigm of learning self-alignments, also known as\nself-attention, has been largely popularized by Transformer\nmodels (Vaswani et al., 2017). This technical narrative\nhas also been explored by a number of other recent stud-\nies, including those on intra-attention (Parikh et al., 2016),\nself-matching networks (Wang et al., 2017), and LSTMN\n(Cheng et al., 2016). To this end, Transformer models,\nwhich function primarily based on self-attention and feed-\nforward layers, generally serve as a reliable replacement for\nautoregressive recurrent models.\nThe self-attention layer itself has been the subject of many\nrecent technical innovations. For example, recent studies\nhave investigated improving the layer’s overall efﬁciency\nvia sparsiﬁcation and reducing the complexity of computing\nthe alignment matrix (Child et al., 2019; Kitaev et al., 2020;\nHuang et al., 2018; Tay et al., 2020; Beltagy et al., 2020).\nThese methods are tightly coupled with the query-key-value\nparadigm, employing a form of memory-based content re-\ntrieval as an attention mechanism. On the other end of the\nspectrum, there have been studies that advocate for replac-\ning self-attention with convolution (Wu et al., 2019). The\nrecent surge in interest in simplifying the attention mech-\nanism raises important questions about the role and utility\nof the pairwise dot products, which are one the deﬁning\ncharacteristics of self-attention models. Meanwhile, in the\nimage domain, (Cordonnier et al., 2019) shows connection\nof Transformers with CNNs.\nOur work is a new take on the self-attention mechanism in\nTransformer models. We delve deeper, starting with replac-\ning the pairwise dot products with what we call synthesizing\nfunctions that learn attention matrices that may or may not\ndepend on the input tokens. The most closely related work\nis ((Raganato et al., 2020)), in which the authors propose us-\ning ﬁxed (i.e., not learned) attention patterns in Transformer\nencoders. However, the scope of their work is limited to en-\ncoders and relies on manually deﬁned handcrafted patterns\nthat seem to work well. Our work takes this intuition further\nand expands on this narrative.\nSynthesizer: Rethinking Self-Attention for Transformer Models\nMLP-Mixers are Random Synthesizers This is an up-\ndate2 discussing the relationship between Random Syn-\nthesizers and recent MLP-Mixers (Tolstikhin et al., 2021).\nThere have been recent work (April 2021) that proposed All-\nMLP architectures for vision. Although, this work made it’s\nappearance ﬁrst in May 2020, a year before the MLP-Mixer\nwas proposed, we show that Random Synthesizers are a\nform of MLP-Mixers. Random Synthesizers apply a weight\nmatrix Ron the length dimension. Ris a L×Lmatrix and\ncan be seen as a form of projection across the length dimen-\nsion. This is equivalent to transposing the axis before linear\nprojection in the token-mixer in the MLP-Mixer model. The\nkey difference here is that (1) we use a softmax normaliza-\ntion on the kernel (weights) and (2) Random Synthesizers\nare a form of multi-headed MLP-Mixers.\n3. The Proposed Method\nThis section introduces our proposed SYNTHESIZER model.\nAt its core, our model is essentially a Transformer model\nwith self-attention modules replaced with our Synthetic At-\ntention modules. Figure 3.1 illustrates the key ideas behind\n(a) Transformer (b) Dense Synthesizers and (c) Random\nSynthesizers.\n3.1. Synthesizer Model\nThis section introduces Synthetic Attention, our proposed\nself-attention module. Our model removes the notion of\nquery-key-values in the self-attention module and directly\nsynthesizes the alignment matrix instead. For simplicity, we\ndescribe the per head and per layer computation, which is\ndenoted by hand ℓrespectively in most cases.\nDense Synthesizer Let us consider the simplest varia-\ntion of the SYNTHESIZER model which is conditioned on\neach input token. Overall, our method accepts an input\nXh,ℓ ∈RN×d and produces an output of Yh,ℓ ∈RN×d.\nHere, ℓrefers to the sequence length and drefers to the di-\nmensionality of the model. We ﬁrst adopt Fh,ℓ(.), a parame-\nterized function, for projecting input Xi from ddimensions\nto N dimensions.\nBi,h,ℓ = Fh,ℓ(Xi,h,ℓ) (1)\nwhere Fh,ℓ(.) is a parameterized function that maps Rd to\nRℓ and iis the i-th token of Xh,ℓ and is applied position-\nwise (to each vector in the sequence of lengthN). Intuitively,\nthis can be interpreted as learning a token-wise projection\nto the sequence length N. Essentially, with this model, each\ntoken predicts weights for each token in the input sequence.\nIn practice, we adopt a simple two layered feed-forward\n2This paper’s draft ﬁrst went out a year ago, on May 2020.\nlayer with ReLU activations for Fh,ℓ(.):\nFh,ℓ(Xi,h,ℓ) =W2,h,ℓ(σR(W1,h,ℓ(Xi,h,ℓ)) (2)\nwhere σR is the ReLU activation function and W1,h,ℓ ∈\nRd×d and W2,h,ℓ ∈ Rd×ℓ. Hence, Bi,h,ℓ is now of Rℓ.\nGiven Bi,h,ℓ ∈RN×N, we now compute:\nYh,ℓ = softmax(Bh,ℓ)Gh,ℓ(Xh,ℓ) (3)\nwhere Gh,ℓ(.) is another parameterized function ofXthat is\nanalogous to Vh,ℓ (value) in the standard Transformer model.\nThis approach eliminates the dot product attention Y =\nsoftmax(Qh,ℓK⊤\nh,ℓ)Vh,ℓ altogether by replacing Qh,ℓK⊤\nh,ℓ\nin standard Transformers with the synthesizing function\nFh,ℓ(.).\nRandom Synthesizer The previous variant learns syn-\nthetic attention by conditioning on each input of X and\nprojecting to N dimensions. Hence, the Dense Synthesizer\nconditions on each token independently, as opposed to pair-\nwise token interactions in the vanilla Transformer model.\nWe consider another variation of SYNTHESIZER where the\nattention weights are not conditioned on any input tokens.\nInstead, the attention weights are initialized to random val-\nues. These values can then either be trainable or kept ﬁxed\n(denoted as Fixed).\nLet Rh,ℓ be a randomly initialized matrix. The Random\nSynthesizer is deﬁned as:\nYh,ℓ = softmax(Rh,ℓ)Gh,ℓ(Xh,ℓ). (4)\nwhere Rh,ℓ ∈RN×N. Notably, each head adds N2 pa-\nrameters to the network. The basic idea 3 of the Random\nSynthesizer is to not rely on pairwise token interactions or\nany information from individual token but rather to learn\na task-speciﬁc alignment that works well globally across\nmany samples. This is a direct generalization of the re-\ncently proposed ﬁxed self-attention patterns (Raganato et al.,\n2020).\nFactorized Models The Dense Synthesizer adds d×N\nparameters to the network. On the other hand, the Random\nSynthesizer adds N×Nparameters. Here, note that we omit\nthe Q,K projections in the standard Transformer which\nresults in further parameter savings. Despite these savings,\nsynthesized models can be cumbersome to learn when ℓ\nis large. Hence, we propose factorized variations of the\nSYNTHESIZER models and show that these variants perform\ncomparably in practice.\nFactorized Dense Synthesizer Factorized outputs not\nonly slightly reduce the parameter cost of the SYNTHE -\nSIZER but also aid in preventing overﬁtting. The factorized\n3We were not expecting this variation to work at all, but it turns\nout to be a strong baseline.\nSynthesizer: Rethinking Self-Attention for Transformer Models\nInput \nX\nQuery\n \nKey\nValue\nDot \nProduct \nAttention\nOutput\nInput \nX\nValue\nOutput\nInput \nX\nValue\nOutput\nRandom \nSynthesizer\nDense \nSynthesizer\n(a) Transformer (b) Synthesizer (Dense)  \n(c) Synthesizer (Random)\nFigure 1.Our proposed SYNTHESIZER model architecture.\nvariant of the dense synthesizer can be expressed as follows:\nAh,ℓ,Bh,ℓ = FA,h,ℓ(Xi,h,ℓ),FB,h,ℓ(Xi,h,ℓ) (5)\nwhere FA,h,ℓ(.) projects input Xi,h,ℓ into a dimensions,\nFB,h,ℓ(.) projects Xi,h,ℓ to bdimensions, and a×b= N.\nThe output of the factorized module is now written as:\nYh,ℓ = softmax(Ch,ℓ)Gh,ℓ(Xh,ℓ). (6)\nwhere Ch,ℓ = HA(Ah,ℓ) ∗HB(Bh,ℓ) where HA,HB are\ntiling functions and Ch,ℓ ∈RN×N. The tiling function\nsimply duplicates the vector ktimes, i.e., RN →RN×k. In\nthis case, HA(·) is a projection ofRa →Ra×b and HB(·) is\na projection of Rb →Rb×a. To avoid having similar values\nwithin the same block, we compose the outputs of HA and\nHB.\nFactorized Random Synthesizer Similar to Factorized\nSynthesizers, we are also able to factorize Rh,ℓ into low\nrank matrices R1,h,ℓ,R2,h,ℓ ∈RN×k.\nYh,ℓ = softmax(R1,h,ℓR⊤\n2,h,ℓ)Gh,ℓ(Xh,ℓ). (7)\nTherefore, it is easy to see that, for each head, this reduces\nthe parameter costs from N2 to 2(Nk) where k <<Nand\nhence helps prevent overﬁtting. In practice, we use a small\nvalue of k= 8.\nMixture of Synthesizers Finally, we note that all of the\nproposed synthetic attention variants can be mixed in an\nadditive fashion. This can be expressed as:\nYh,ℓ = softmax(α1,h,ℓS1,h,ℓ(Xh,ℓ)+\n···αN,h,ℓSN,h,ell(Xh,ℓ))Gh,ℓ(Xh,ℓ).\nwhere S(.) is a parameterized synthesizing function and the\nα(where ∑α = 1) are learnable weights. In the case of\nmixing Random Factorized with standard Dense Synthesiz-\ners, this is expressed as:\nYh,ℓ = softmax(α1,h,ℓR1,h,ℓR⊤\n2,h,ℓ+\nα2,h,ℓFh,ℓ(Xh,ℓ))Gh,ℓ(X).\nWe investigate several Mixture of Synthesizers variants in\nour experiments.\nOn Parameters Depending on Sequence LengthRan-\ndom and dense Synthesizers both rely on parameters that\ndepend on length ℓ. In general, we deﬁne a maximum length\nand dynamically truncate to the actual length of each batch.\nWe note that this is in similar spirit to trainable positional en-\ncodings which have been common practice in Transformer\nmodels. Hence, we do not forsee any issue here. In the\ncase that this is really a problem, one potential solution is\nto project to a smaller value band tile bto the maximum\nsequence length. We leave this exploration to future work.\n3.2. Discussion\nThis paper asks fundamental questions about the attention\nmatrix Aand whether it is possible to synthesize Aby al-\nternate means other than pairwise attention. It is worth\nnoting that the regular dot product attention can also be\nsubsumed by our SYNTHESIZER framework, i.e., SYNTHE -\nSIZER generalizes the Transformer model. In the case of\nthe Transformer, the synthesizing function in question is\nS(X) =FQ(X)FK(X)⊤. Table 1 lists the different model\nvariants explored within ourSYNTHESIZER framework. The\n’condition on’ column refers to whether the synthesized out-\nput is produced as a function ofXi or everyXi,Xj pair. The\n‘sample‘ column indicates whether a given variant leverages\nSynthesizer: Rethinking Self-Attention for Transformer Models\nModel S(X) Condition On Sample Interact |θ|\nDot Product FQ(X)FK(Xi)⊤ Xj ∀j Local Yes 2d2\nRandom R N/A Global No N2\nFac. Random R1R⊤\n2 N/A Global No 2Nk\nDense F1σ(F2(Xi)) Xi Local No d2 + dN\nFac. Dense HA(FA(Xi))) ∗HB(FB(Xi))) Xi Local No d2 + d(k1 + k2)\nTable 1.Overview of all Synthesizing Functions.\nlocal or global context. Random Synthesizers are global be-\ncause they share the same global alignment patterns across\nall samples. Dense Synthesizers are considered to be local\nas they are conditioned on Xi, which makes the alignment\npattern dependent on each individual sample. To this end, it\nis imperative for synthesized models to have multiple heads\nto be effective.\n4. Experiments\nThis section outlines our experimental setup and results. We\nﬁrst conduct experiments on ﬁve tasks to evaluate the ef-\nfectiveness4 of different Synthesizer variants along with\nhow they compare to the vanilla Transformer. Specif-\nically, we conduct experiments on (1) machine transla-\ntion (EnDe, EnFr) (2) autoregressive language modeling\n(LM1B) (3) text generation (summarization and dialogue\nmodeling and (4) multi-task natural language processing\n(GLUE/SuperGLUE). Details of each experiments can be\nfound in the appendix.\nNotation of Variants We use R to denote Random, D to\ndenote Dense and V to denote vanilla dot product attention.\nFix to represent Fixed Random, FR to represent Factorized\nRandom and FD to represent Factorized random. For Mix-\nture Synthesizers, we use + to denote that two methods are\nmixed.\n4.1. Comparing Synthesizer Variants and Transformer\nModels\nThis section dives into a detailed study of multiple Synthe-\nsizer variants and the base Transformer model.\nExperimental Results on MT/LMFirst, we observe that\nour Random Synthesizer baseline achieves 27.27 on EnDe\nand 41.12 on EnFr. The non-trainable (i.e., ﬁxed) variant\nperforms substantially worse, but still yields surprisingly\nstrong ≈24 BLEU with ﬁxed random attention weights.\nMost other SYNTHESIZER variants achieve competitive per-\nformance, although with slight performance degradation\n4Note that we are primarily interested in making controlled\ncomparisons instead of going for the state-of-the-art result on each\ntask.\ncompared to Transformers. An interesting ﬁnding is that\nthe Mixture model of Random + Dense synthesizer per-\nforms comparably to vanilla Transformers on EnDe. When\nmixing the standard dot product attention, performance fur-\nther increases by +0.8 BLEU points (EnDe). In general,\nthe performance of SYNTHESIZER variants are competitive\nwith Transformers for this task. On LM1b, We ﬁnd that the\nRandom Synthesizers perform within 1-2 PPL points away\nfrom the vanilla Transformer model. The best performing\nmodel is the Synthesizer (D+V), which achieves the best\nperformance on this setting.\nResults on Text GenerationFor summarization, we ﬁnd\nthat the (R) and (D) variants do not outperform Transform-\ners. The performance of the (D) model is ≈2 Rouge-L\npoints below Transformers. Hence, we postulate that the\nlocal sample-wise pairwise interactions are important for\nthe summarization task. On the other hand, the utility of\nsynthesized attention can also be observed, i.e., the (R+V)\nand (R+D) models both outperform Transformers. On the\ndialogue task, Synthesizers (R) and (D) both outperform\nvanilla Transformers by a reasonable margin (≈1-3) points\nacross most/all metrics. The best performing model here\nis the (D) variant. Surprisingly, unlike most other tasks,\nthe (+V) variants do not perform well, signifying that dot\nproduct self-attention may actually be harmful for this task.\nSum. Dialogue\nModel RL B4 RL Met. CIDr\nTrans. 35.77 3.20 13.38 5.89 18.94\nSynthesizer Models\nR 33.10 2.25 15.00 6.42 19.57\nD 33.70 4.02 15.22 6.61 20.54\nD+V 36.02 3.57 14.22 6.32 18.87\nR+V 35.95 2.28 14.79 6.39 19.09\nTable 3.Experimental results on Abstractive Summarization\n(CNN/Dailymail) and Dialogue Generation (PersonaChat). We\nreport on RL (Rouge-L), B4 (Bleu-4), Met. (Meteor) and CIDr.\nComparing Synthesizers with Dynamic Convolutions\nTo ascertain the competitiveness of Synthesizers, we also\ncompare them with Dynamic convolutions (Wu et al., 2019).\nSynthesizer: Rethinking Self-Attention for Transformer Models\nNMT (BLEU) LM (PPL)\nModel |θ| EnDe EnFr |θ| LM\nTransformer† 67M 27.30 38.10 - -\nTransformer 67M 27.67 41.57 70M 38.21\nSynthesizer (Fixed Random) 61M 23.89 38.31 53M 50.52\nSynthesizer (Random) 67M 27.27 41.12 58M 40.60\nSynthesizer (Factorized Random) 61M 27.30 41.12 53M 42.40\nSynthesizer (Dense) 62M 27.43 41.39 53M 40.88\nSynthesizer (Factorized Dense) 61M 27.32 41.57 53M 41.20\nSynthesizer (Random + Dense) 67M 27.68 41.21 58M 42.35\nSynthesizer (Dense + Vanilla) 74M 27.57 41.38 70M 37.27\nSynthesizer (Random + Vanilla) 73M 28.47 41.85 70M 40.05\nTable 2.Experimental Results on WMT’14 English-German, WMT’14 English-French Machine Translation tasks and Language Modeling\nOne Billion (LM1B). †denotes original reported results in (Vaswani et al., 2017).\nModel Log PPL Steps/Sec Params TFLOPS\nTrans. 1.865 3.90 223M 3.70\nDyConv 2.040 2.65 257M 3.93\nLightConv 1.972 4.05 224M 3.50\nSyn (D) 1.965 3.61 224M 3.80\nSyn (R) 1.972 4.26 254M 3.36\nSyn (R+V) 1.849 3.79 292M 4.03\nSyn (D+V) 1.832 3.34 243M 4.20\nTable 4.Validation perplexity scores on C4 dataset (Raffel et al.,\n2019). All models are at approximately similar parameterization.\nWe compare them on (1) pretraining perplexity using the\nmasked language modeling objective on C4 and (2) down-\ntream ﬁnetuning results on GLUE and SuperGLUE.\nResults on Masked Language ModelingWe also bench-\nmark the speed of these models. In order to do so, we con-\nduct additional experiments on the T5 adaptation of masked\nlanguage modeling on the C4 dataset (Raffel et al., 2019) by\ncomparing against lightweight dynamic convolutions (Wu\net al., 2019) on a masked language modeling task. We also\ntake this chance to benchmark the speed of Synthesizers\ncompared with Transformers. Experiments are conducted\non Mesh Tensorﬂow (Shazeer et al., 2018) and ran on 2x2\nTPU V3 Chips for approximately 524Ksteps.\nResults on MLM Table 4 reports the validation set log\nperplexity on masked language modeling 5. We observe\nthat Synthesizers (R) can outperform Dynamic Convolu-\ntions by a relative +3.5% while being +60% faster. Against\nLightweight Dynamic Convolutions, we match the perfor-\nmance while being +5% faster. Given that this is the simple\nrandom Synthesizer baseline, we ﬁnd this extremely inter-\nesting how it is able to outperform dynamic convolutions,\na relatively complex model. The Random Synthesizer also\nhas less FLOPS compared to both convolution models. On\n5Note that this follows the sequence transduction style in T5.\nthe other hand, the Mixture Synthesizer models that use the\ndot product attention improves the performance of the base\nTransformer model with relatively an equal model speed.\nFinally, similar to the earlier results, we see a consistent\nperformance gain of Synthesizer (D+V) and Synthesizer\n(R+V) outperforming the base Transformer model.\nResults on GLUE and SuperGLUETables 5 and 6 re-\nport results on the GLUE and SuperGLUE benchmarks.\nWe note that the (R) and (D) variants of SYNTHESIZER do\nnot achieve reasonable performance. This can be largely\nattributed to the fact that the encoder self-attention in the\nT5 setting also functions as a cross-sentence attention. For\nexample, in the entailment or reading comprehension tasks,\nthe premise and hypothesis are concatenated together and\nself-attention effectively acts as cross-sentence attention6.\nOn datasets like SST, a straightforward sentiment classiﬁ-\ncation task, this cross sentence attention is not necessary\nand therefore Syn (R) and Syn (D) both perform competi-\ntively. To this end, Dynamic Convolutions (Wu et al., 2019)\nalso do not have this encoder ”cross-attention” and there-\nfore also suffer on many of these pairwise matching tasks.\nNotably, in this ‘no cross attention’ setting, the Random\nSynthesizers are are 4 to 5 percentage points higher in\nGLUE/SuperGLUE score compared to Dynamic Convo-\nlutions.\nOptimistically, we observe that the mixture model Syn\n(R+V) outperforms the T5 model by a substantial margin\n(+1.9 points on SuperGLUE and +0.6 points on GLUE).\nNaturally, the hybrid mixture model also very substantially\noutperforms Dynamic Convolution. Finally to ensure that\nthe Syn (+V) variations are not outperforming Transformers\ndue to simply having more parameters, we also compared\n6On a related note, the perceived success of pairwise self-\nattention might also be attributed to the fact that these public\nbenchmarks are bias towards pairwise matching tasks. In reality,\nthis is computationally prohibitive for many practical real-world\napplications (Seo et al., 2018).\nSynthesizer: Rethinking Self-Attention for Transformer Models\nModel Glue CoLA SST MRPC STSB QQP MNLI QNLI RTE\nT5 (Base) 83.5 53.1 92.2 92.0/88.7 89.1/88.9 88.2/91.2 84.7/ 85.0 91.7 76.9\nT5 (Base+) 82.8 54.3 92.9 88.0/83.8 85.2/85.4 88.3/91.2 84.2/84.3 91.4 79.1\nDyConv 69.4 33.9 90.6 82.6/72.5 60.7/63.1 84.2/88.2 73.8/75.1 84.4 58.1\nSyn (R) 75.1 41.2 91.2 85.9/79.4 74.0/74.3 85.5/89.0 77.6/78.1 87.6 59.2\nSyn (D) 72.0 18.9 89.9 86.4/79.4 75.3/75.5 85.2/88.3 77.4/78.1 86.9 57.4\nSyn (D+V) 82.6 48.6 92.4 91.2/87.7 88.9/89.0 88.6/91.5 84.3/84.8 91.7 75.1\nSyn (R+V) 84.1 53.3 92.2 91.2/87.7 89.3/88.9 88.6/91.4 85.0 /84.6 92.3 81.2\nTable 5.Experimental results (dev scores) on multi-task language understanding (GLUE benchmark) for small model and en-mix\nmixture. Note: This task has been co-trained with SuperGLUE.\nModel SGlue BoolQ CB CoPA MultiRC ReCoRD RTE WiC WSC\nT5 (Base) 70.3 78.2 72.1/83.9 59.0 73.1/32.1 71.1/70.3 77.3 65.8 80.8\nT5 (Base+) 70.7 79.3 81.1/87.5 60.0 75.1/34.4 71.7/70.7 80.5 64.6 71.2\nDyConv 57.8 66.7 65.9/73.2 58.0 57.9/8.71 58.4/57.4 69.0 58.6 73.1\nSyn (R) 61.1 69.5 54.6/73.2 60.0 63.0/15.7 58.4/57.4 67.5 64.4 66.3\nSyn (D) 58.5 69.5 51.7/71.4 51.0 66.0/15.8 54.1/53.0 67.5 65.2 58.7\nSyn (D+V) 69.7 79.3 74.3/85.7 64.0 73.8/33.7 69.9/69.2 78.7 64.3 68.3\nSyn (R+V) 72.2 79.3 82.7/91.1 64.0 74.3/34.9 70.8/69.9 82.7 64.6 75.0\nTable 6.Experimental results (dev scores) on multi-task language understanding (SuperGLUE benchmark) for small model and en-mix\nmixture. Note: This task has been co-trained with GLUE.\nwith T5 (Base+) which has equal number of parameters to\nSyn (+V) variants (approximately≈10Mmore parameters).\nOur results show that Synthesizers (+V) still outperform T5\n(Base+).\n4.2. Comparing Synthesizers with Linformers\nWe conduct more experiments comparing factorized random\nSynthesizers with Linformers. Since Linformer cannot be\nused to decode, we compare them on two encoding tasks\nfrom tensorﬂow datasets (AGnews (Zhang et al., 2015) and\nmovie reviews (Maas et al., 2011)). We use k=32 for both\nfactorized models. We also benchmark Transformers on\nthis task. Note we do not use contextualized embeddings so\nresults are not comparable with other work.\nModel News Reviews Steps/Sec\nTransformer 88.83 81.34 1.09\nLinformer 86.50 82.86 1.09\nSyn (FR) 86.53 83.39 1.10\nSyn (FR+V) 89.13 84.61 0.80\nTable 7.Results on Encoding only tasks (accuracy).\nResults We notice that factorized Synthesizers (FR) are\ncompetitive with Linformers and Transformers on this task.\nThe accuracy of Syn (FR) is competitive with Linformers\nwhile Syn (FR+V) outperforms both Transformers and Lin-\nformers.\nFigure 2.Init Decoder weights (Reference)\n5. Qualitative Analysis\nDistribution of Weights We are interested in investigat-\ning how the synthetically generated attention weights differ\nfrom the dot product attention weights. Figure 3 shows the\nattention histograms on trained Transformer and SYNTHE -\nSIZER models. We report histograms at layers 1, 3, and 5\nof a 6 layered (Transformer or SYNTHESIZER ) model at\n50Ksteps. We found that the weight distributions remain\nrelatively identical thereafter. Figure 2 shows the initializa-\ntion state. We observe that there are distinct differences in\nthe weight distribution of SYNTHESIZER and Transformer\nmodels. The variance of the SYNTHESIZER weights tends\nto be higher. On the other hand, the weights on the Trans-\nformer model tends to gravitate near 0 and have smaller\nvariance. There are also notable differences across the (R)\nand (D) SYNTHESIZER variants. Speciﬁcally, the (D) model\nin general has greater max values with more values in the\n0.1-0.2 range while the values of the Rmodel tends to stay\ncloser to 0.\nSynthesizer: Rethinking Self-Attention for Transformer Models\nFigure 6.Synthesizer weights\non LM1B.\nFigure 7.Transformer weights\non LM1B.\nEnc L1\n Enc L3\n Enc L5\nDec L1\n Dec L3\n Dec L5\nFigure 3.Histogram of Encoder and Decoder Attention Weights\non MT (WMT EnDe). L denotes the layer number and Enc/Dec\ndenotes encoder or decoder.\n5.1. What patterns do Synthesizers learn?\nIn this section, we perform a deeper analysis of the SYN-\nTHESIZER model.\nVanilla\n Random\n Fr\n Dense\n FD\nFigure 4.Visual analysis of Synthetic Attention (encoder) on\nWMT EnDe.\nVanilla\n Random\n FR\n Dense\n FD\nFigure 5.Visual analysis of Synthetic Attention (decoder) on\nWMT EnDe.\nAnalysis Finally, we are interested to understand what\nthese Synthesizer models are learning. We inspect the ran-\ndom synthetic attention weights for language modeling task\nLM1B and visualise the differences compared to the vanilla\nattention. We ﬁnd that, for the LM task, Synthesizers are\ncapable of learning a local window, emulating the vanilla\nTransformer quite closely despite starting from completely\nrandom. The weights, however, seem smoother and less\ncoarse as compared to the Transformer. This seems to reﬂect\nwhat we expect since the Synthesizer does not beneﬁt from\ntoken speciﬁc information. We provide additional analysis\nand visualisation of weights for the Machine Translation\ntask in the supplementary material.\n5.2. Overall Summary of Quantitative Results\nThis section summarizes our overall ﬁndings.\nSynthetic Attention is competitive even without Dot\nProduct Attention On all evaluated tasks, we showed\nthat synthesized attention functions competitively, i.e., it\nachieves performance reasonably close to the dot product\nself-attention. On one task (dialogue generation), the dot\nproduct self-attention is found to actually degrade perfor-\nmance. Amongst the other tasks, machine translation is\nthe least affected by the removal of the vanilla dot prod-\nuct. These ﬁndings allow us to introspect about whether\npairwise comparisons for self-attention are even necessary.\nOn the multi-task language understanding benchmark, the\nself-attention functions as a form of cross-attention by con-\ncatenating sentence pairs. Hence, synthesize attention per-\nformance is considerably worse than vanilla Transformers.\nSynthetic Attention and Dot Product Attention are\nhighly complementary Overall, we also observe that the\ndot product attention is very helpful. To this end, synthetic\nattention is highly complementary to the pairwise dot prod-\nuct attention. While Synthetic Attention can usually achieve\ncompetitive and fast performance on its own, synthetic at-\ntention boosts performs, composing multiple synthetic at-\ntention (and dot product attention) together shows gains\non almost all tasks that we have investigated. Hence, we\nbelieve this to be a robust ﬁnding.\nThe simplest Synthesizers such as Random Synthesiz-\ners are fast competitive baselinesFinally, we note that\nsimple random Synthesizers are competitive with dynamic\nconvolutions and Linformers, which are recently proposed\nmodels. On two encoding task and a large-scale masked\nlanguage modeling task, we show that random (or factorized\nrandom) Synthesizers remain competitive to other fast or\nefﬁcient Transformer models.\n6. Conclusion\nThis paper proposed SYNTHESIZER , a new Transformer\nmodel that employs Synthetic Attention. We conducted\nSynthesizer: Rethinking Self-Attention for Transformer Models\na principled study to better understand and evaluate the\nutility of global alignment and local, instance-wise align-\nment (e.g., independent token and token-token based) in\nself-attention. We show that, on multiple tasks such as ma-\nchine translation, language modeling, dialogue generation,\nmasked language modeling and document classiﬁcation,\nsynthetic attention demonstrates competitive performance\ncompared to vanilla self-attention. Moreover, for the dia-\nlogue generation task, pairwise interactions actually hurt\nperformance. Notably, we reemphasize that this study refers\nto self-attention. We found that we are not able to replace\ncross-attention with simpler variants in most cases. Via a\nset of additional large-scale experiments, also ﬁnd that Syn-\nthesizers can outperform or match Dynamic Convolutions\nand Factorized Synthesizers can outperform other low rank\nLinformer models.\nReferences\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.arXiv\npreprint arXiv:1409.0473, 2014.\nBeltagy, I., Peters, M. E., and Cohan, A. Longformer: The\nlong-document transformer. arXiv:2004.05150, 2020.\nCheng, J., Dong, L., and Lapata, M. Long short-term\nmemory-networks for machine reading. arXiv preprint\narXiv:1601.06733, 2016.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nCordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-\ntionship between self-attention and convolutional layers.\narXiv preprint arXiv:1911.03584, 2019.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, Ł. Universal transformers. arXiv preprint\narXiv:1807.03819, 2018.\nGraves, A., Wayne, G., and Danihelka, I. Neural turing\nmachines. arXiv preprint arXiv:1410.5401, 2014.\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N.,\nSimon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D.,\nDinculescu, M., and Eck, D. Music transformer. arXiv\npreprint arXiv:1809.04281, 2018.\nKitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\nefﬁcient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nLuong, M.-T., Pham, H., and Manning, C. D. Effective\napproaches to attention-based neural machine translation.\narXiv preprint arXiv:1508.04025, 2015.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,\nA. Y ., and Potts, C. Learning word vectors for sen-\ntiment analysis. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Linguis-\ntics: Human Language Technologies, pp. 142–150, Port-\nland, Oregon, USA, June 2011. Association for Com-\nputational Linguistics. URL http://www.aclweb.\norg/anthology/P11-1015.\nParikh, A. P., T ¨ackstr¨om, O., Das, D., and Uszkoreit, J.\nA decomposable attention model for natural language\ninference. arXiv preprint arXiv:1606.01933, 2016.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRaganato, A., Scherrer, Y ., and Tiedemann, J. Fixed encoder\nself-attention patterns in transformer-based machine trans-\nlation. arXiv preprint arXiv:2002.10260, 2020.\nSeo, M., Kwiatkowski, T., Parikh, A. P., Farhadi, A., and\nHajishirzi, H. Phrase-indexed question answering: A new\nchallenge for scalable document comprehension. arXiv\npreprint arXiv:1804.07726, 2018.\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., et al. Mesh-tensorﬂow: Deep learning for supercom-\nputers. In Advances in Neural Information Processing\nSystems, pp. 10414–10423, 2018.\nTay, Y ., Bahri, D., Yang, L., Metzler, D., and Juan,\nD.-C. Sparse sinkhorn attention. arXiv preprint\narXiv:2002.11296, 2020.\nTolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,\nX., Unterthiner, T., Yung, J., Keysers, D., Uszkoreit, J.,\nLucic, M., et al. Mlp-mixer: An all-mlp architecture for\nvision. arXiv preprint arXiv:2105.01601, 2021.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020.\nWang, W., Yang, N., Wei, F., Chang, B., and Zhou, M. Gated\nself-matching networks for reading comprehension and\nquestion answering. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 189–198, 2017.\nSynthesizer: Rethinking Self-Attention for Transformer Models\nWeston, J., Chopra, S., and Bordes, A. Memory networks.\narXiv preprint arXiv:1410.3916, 2014.\nWu, F., Fan, A., Baevski, A., Dauphin, Y . N., and Auli, M.\nPay less attention with lightweight and dynamic convolu-\ntions. arXiv preprint arXiv:1901.10430, 2019.\nZhang, X., Zhao, J., and LeCun, Y . Character-level con-\nvolutional networks for text classiﬁcation. In Advances\nin neural information processing systems, pp. 649–657,\n2015.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6173349022865295
    },
    {
      "name": "Computer science",
      "score": 0.3358680307865143
    },
    {
      "name": "Psychology",
      "score": 0.32803863286972046
    },
    {
      "name": "Electrical engineering",
      "score": 0.2107926309108734
    },
    {
      "name": "Engineering",
      "score": 0.16880127787590027
    },
    {
      "name": "Voltage",
      "score": 0.12099093198776245
    }
  ]
}