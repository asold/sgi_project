{
  "title": "Adaptable Multi-Domain Language Model for Transformer ASR",
  "url": "https://openalex.org/W3049591882",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2126389896",
      "name": "Lee Tae-Woo",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622957",
      "name": "Lee, Min-Joong",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622958",
      "name": "Kang, Tae Gyoon",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622959",
      "name": "Jung, Seokyeoung",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2252323649",
      "name": "Kwon Minseok",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622961",
      "name": "Hong, Yeona",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A705114869",
      "name": "Lee Jung-In",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4228041955",
      "name": "Woo, Kyoung-Gu",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622964",
      "name": "Kim, Ho-Gyeong",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A4288622965",
      "name": "Jeong, Jiseung",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2145871355",
      "name": "Lee Jihyun",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2243168859",
      "name": "Lee HoSik",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A3202707961",
      "name": "Choi, Young Sang",
      "affiliations": [
        "Samsung (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3021931813",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W6849896277",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W2962945654",
    "https://openalex.org/W6758026734",
    "https://openalex.org/W3015565442",
    "https://openalex.org/W2938974662",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W6640059789",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2971840980",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W6749669830",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2426267443",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2913946806"
  ],
  "abstract": "We propose an adapter based multi-domain Transformer based language model (LM) for Transformer ASR. The model consists of a big size common LM and small size adapters. The model can perform multi-domain adaptation with only the small size adapters and its related layers. The proposed model can reuse the full fine-tuned LM which is fine-tuned using all layers of an original model. The proposed LM can be expanded to new domains by adding about 2% of parameters for a first domain and 13% parameters for after second domain. The proposed model is also effective in reducing the model maintenance cost because it is possible to omit the costly and time-consuming common LM pre-training process. Using proposed adapter based approach, we observed that a general LM with adapter can outperform a dedicated music domain LM in terms of word error rate (WER).",
  "full_text": "ADAPTABLE MULTI-DOMAIN LANGUAGE MODEL FOR TRANSFORMER \nASR \n \nTaewoo Lee1, Min-Joong Lee2, Tae Gyoon Kang2, Seokyeoung Jung1, Minseok Kwon1, Yeona Hong1,  \nJungin Lee1, Kyoung-Gu Woo1, Ho-Gyeong Kim2, Jiseung Jeong2, Jihyun Lee2, Hosik Lee2, Young Sang Choi2 \n1 AI R&D Group, Samsung Electronics, South Korea \n2 Samsung Advanced Institute of Technology, Samsung Electronics, South Korea \n{tw1.lee, minjoong.lee, taeg.kang, jihyun.s.lee}@samsung.com \n \nABSTRACT \nWe propose an adapter based multi -domain Transformer based \nlanguage model (LM) for Transformer ASR. The model consists of \na big size common LM and small size adapters. The model can \nperform multi-domain adaptation with only the small size adapters \nand its related layers. The proposed model can reuse the full fine -\ntuned LM which is fine-tuned using all layers of an original model. \nThe proposed LM can be expanded to new domains by adding about \n2% of parameter s for a first domain and 13% parameters for after \nsecond domain. The proposed model is also effective in reducing the \nmodel maintenance cost because it is possible to omit the costly and \ntime-consuming common LM pre-training process. Using proposed \nadapter based approach, we observed that a general LM with adapter \ncan outperform a dedicated music domain LM in terms of word error \nrate (WER). \nIndex Termsâ€”End-to-end (E2E) automatic speech recognition \n(ASR), language model (LM), multi-domain adaptation \n1. INTRODUCTION \nIn recent years, virtual voice assistants have been widely spread to \nreal-world applications. End -to-end (E2E) automatic speech \nrecognition (ASR) has become one of the key elements of virtual \nvoice assistant services. As new domains continue to be added, ASR \nmodels need to be adapted quickly to the new domains. Furthermore, \ndomain specific proper nouns must be recognized such as new song \ntitles and singer names. This means that it is necessary to maintain \nthe recognition accuracy of the existin g supported domains while \nsecuring the recognition accuracy for new words in the new domain. \nIn addition, in order to provide a good user experience, such a \nresponse must be done very quickly. \nTransformer was first introduced as a model for translation [1]. \nThen, it has also been successfully applied to ASR [2]. This is \nbecause Transformer has an advantage in terms of computation and \nparallelism over recurrent neural network (RNN) based models. In \naddition, knowledge distillation has been studied to create parameter \nefficient models [3,4]. Shallow fusion of the E2E ASR models and \nexternal language models (LM) also showed a further improvement \nin WER [5,6], because external LMs are able to learn more \ncontextual information from abundant text-only data.  \nIn natural language processing (NLP), several methods of pre-\ntraining neural language models have led to major advances in NLP \nsubtasks. BERT, ELMO, GPT, RoBERTa, and XLNet are typical \n[7-11]. These methods find dependencies between words and their \ncombinations by pre-training neural networks on large amounts of \ndata. Also, by fine-tuning the model on training data in target tasks, \nthese models could be easily applied to solving other NLP tasks. \nHowever, it is difficult to continuously update these models because \ndeep networks tend to forget previous knowledge when it is \nsequentially re -trained [12]. To solve such a problem, continual \nlearning approaches have been studied. To preserve previous \nknowledge, learning without forgetting (LWF) [13] adds output \nlogits of previous stage networks to logits of current stage networks. \nElastic weight consolidation (EWC) [14] constrains weight updates \nby valuing which weight are important for a task. Progressive neural \nnetworks [15] avoid forgetting by preserving task specific networks. \nHowever, those approaches are imperfect in memory and parameter \nefficiency [16].  \nIn computer vision, residual adapter modules have been \nintroduced to make a multi -task and multi -domain model [17]. In \nthe paper, a large common model is used as a  base model. Then \nsmall adapter modules are added in front of each batch \nnormalization layer in series or in parallel manner. In the \nexperiments, both methods showed better accuracy than a full fine-\ntuned model. Similar approaches have been explored for BE RT in \nNLP [18]. In the paper, the authors proposed a model (called \nprojected attention layers or PALs) that can resolve multi -domain \nNLP tasks by adding only adjustable 13% parameters compared to \nthe original model. Meanwhile,  in [16], a method to fine -tune \nmodels by adding only adjustable 3.6% of parameters has been \nproposed. The method adds small size adapters to the self -attention \n(SA) and feed forward network (FFN) layers of Transformer, \nrespectively. In [19], the authors compared PALs and adapters. In \nthe paper, fine-tuning adapters with norm layer showed better results \ncompared to the PALs when almost similar number of parameters is \nused. For multilingual ASR, a structure is introduced so that only \nadapter layers can be switched [20]. In the study, the  experiments \nhave been conducted on recurrent neural network transducer (RNN-\nT) based streaming E2E ASR models. \nIn this paper, we study an external LM structure for \nTransformer based ASR model that can be adapted for multi-domain \nwith only 2% or 13% parameter addition per domain. To the best of \nour knowledge, this is a first attempt applying adapters to \nTransformer LM in ASR. The effects of our model are: 1) Our \nadapter-based adaptation can be used on top of the full fine -tuned \nmodel, and it further reduces word error rate (WER) from the model. \n2) Multi -domain LM can be supported with fewer parameters. 3) \nOur approach provides cost efficient way to maintain existing \nmodels. \n2. SA-BASED MULTI â€“DOMAIN LM WITH \nADAPTER \n2.1. Transformer-based E2E ASR \nFigure 1 shows a Transformer based E2E ASR models with an \nexternal LM. As in [2], the encoder module, which is similar to an \nacoustic model, takes the input features, ğ’™, and transforms them to \na higher-level feature representation with self -attention layers. The \noutputs of the encoder key ğ‘²ğ‘’ğ‘›ğ‘ and value ğ‘½ğ‘’ğ‘›ğ‘ are passed to \nencoder-decoder attention layers of E2E decoder. Using the ğ‘²ğ‘’ğ‘›ğ‘ \nand ğ‘½ğ‘’ğ‘›ğ‘, the E2E de coder iteratively predicts output probabilities \nğ‘ƒ(ğ‘¦ğ‘¡|ğ‘¦0,â‹¯,ğ‘¦ğ‘¡âˆ’1,ğ’™) of next output symbol ğ‘¦ğ‘¡  until maximum \nsequence length or EOS (end -of-sequence) is met. An external LM  \n[25], where encoder -decoder attention layers are removed, can be \nincorporated a t each step of beam search to improve accuracy. \nHereafter, we focus on an external LM decoder with adapters. \n2.2. SA-based LM Decoder with Adapter \nSA-based LM decoder consists of  three parts: an input embedding, \nğ‘ğ¿ LM SA layers, and a linear transform following Softmax (Figure \n2 left). For simplicity we set batch size and the number of domains \nto one in the followings. \n2.2.1 Input Embedding \nLet word-piece [21] vocabulary size be ğ‘ğ‘¤, an input one-hot vector \nbe ğ‘¥ğ‘¡ âˆˆâ„1Ã—ğ‘ğ‘¤, hidden size be â„. The output of embedding matrix \nis computed as (1): \n \nğ‘Šğ‘’ğ‘œ =ğ‘¥ğ‘¡ğ‘¾ğ‘’ (1) \n \nwhere ğ‘¾ğ‘’ âˆˆâ„ğ‘ğ‘¤Ã—â„ and ğ‘Šğ‘’ğ‘œ âˆˆâ„1Ã—â„. Then a positional encoding \nvector ğ‘ƒğ¸ âˆˆâ„1Ã—â„ is added to ğ‘Šğ‘’ğ‘œ [1]. \n2.2.2. SA layer in LM Decoder with Adapter \nA SA layer of a LM decoder with adapters consists of four layers: \nlayer norm [22], multi-head attention (MHA), FFN, and adapters. \n2.2.2.1 Multi-Head Attention \nLet the number of heads be ğ‘â„ğ‘’ğ‘ğ‘‘. Previous output is projected to a \nquery, a key, and a value simultaneously for multi -head attention \n(Figure 2 left). Instead of performing a single attention function \nusing â„ dimentional ğ‘„, ğ¾, and ğ‘‰, MHA performs the attention \nfunction ğ‘â„ğ‘’ğ‘ğ‘‘ times in parallel with differently learned â„/ğ‘â„ğ‘’ğ‘ğ‘‘ \ndimentional ğ‘„, ğ¾ , and ğ‘‰. Then ğ‘â„ğ‘’ğ‘ğ‘‘  numbers outputs are \nconcatenated and projected into a single representation. The detailed \nequation is as follows: \nMultiHead(ğ‘„,ğ¾,ğ‘‰)=Concat(â„ğ‘’ğ‘ğ‘‘1,â‹¯,â„ğ‘’ğ‘ğ‘‘ğ‘â„ğ‘’ğ‘ğ‘‘)ğ‘¾ğ‘‚ (2) \nwhere     â„ğ‘’ğ‘ğ‘‘ğ‘– =ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„ğ‘¾ğ‘–\nğ‘„,ğ¾ğ‘¾ğ‘–\nğ¾,ğ‘‰ğ‘¾ğ‘–\nğ‘‰) \n=softmax\n(\n (ğ‘„ğ‘¾ğ‘–\nğ‘„)(ğ¾ğ‘¾ğ‘–\nğ¾)\nğ‘»\nâˆš â„\nğ‘â„ğ‘’ğ‘ğ‘‘ )\n (ğ‘‰ğ‘¾ğ‘–\nğ‘‰), \n(3) \nğ‘¾ğ‘–\nğ‘„ âˆˆâ„â„Ã—ğ‘‘ğ‘ , ğ‘¾ğ‘–\nğ¾ âˆˆâ„â„Ã—ğ‘‘ğ‘˜ , ğ‘¾ğ‘–\nğ‘‰ âˆˆâ„â„Ã—ğ‘‘ğ‘£ , and ğ‘¾ğ‘‚ âˆˆâ„â„Ã—â„ \nare trainable parameters.  Note ğ‘‘ğ‘ =ğ‘‘ğ‘˜ =ğ‘‘ğ‘£ =â„ ğ‘â„ğ‘’ğ‘ğ‘‘â„  \nthroughout the paper. \n2.2.2.2 Position-wise Feed-Forward Network \nLet an inner filter size ğ‘“. Position -wise feed forward network \nconsists of two FFNs with ReLU activation in between. An output \nof position-wise FFN is calculated as (4) where the input vector ğ‘–1 âˆˆ\nâ„1Ã—â„, the weight matrices and bias vectors ğ‘¾1\n âˆˆâ„â„Ã—ğ‘“, ğ‘1 âˆˆâ„1Ã—ğ‘“, \nğ‘¾2 âˆˆâ„ğ‘“Ã—â„, and ğ‘2 âˆˆâ„1Ã—â„. \n \n \nFig. 1. The dotted line box shows transformer -based E2E ASR \nmodel, including encoder and decoder. An external LM is \nincorporated at each step of beam search. \n \n \nFig. 2. (Left) is an architecture of transformer multi -domain LM. \nIn a LM decoder, the adapter module (right) is added on top of \nmulti-head attention and feed-forward layers [16]. Only green layers \n(including layer norms or LN) are fine -tuned on the downstream \ndata and expanded for  ğ‘ğ‘‘ multi-domain. Dotted red lines shows a \nswitchable decoding path for a first domain. \nTransformer\nEncoder\nTransformer\nDecoder\nTransformer\nLM\n \nE2E\nğ’™\nğ‘²   ,ğ‘½   \nğ‘¦ğ‘¡\nğ‘¦ğ‘¡âˆ’1 ğ‘¦ğ‘¡âˆ’1\nFeed-forward \ndown-project\nNonlinearity\nFeed-forward \nup-project\n \nAdapter \nLayer\n \nÃ—ğ‘ğ¿ \n2x\nFeed-forward\nğ´ğ‘‘ğ‘ ğ‘¡ğ‘’ ğ‘ğ‘‘\n \n ğ‘–ğ‘›ğ‘’ğ‘ 1\nSoftmax\nMulti-Head \nAttention\nInput\nEmbedding\nPositional \nEncoding\n ğ‘1\nğ´ğ‘‘ğ‘ ğ‘¡ğ‘’ 1\n ğ‘ğ‘ğ‘‘\nğ‘„ğ¾ğ‘‰\nâ‹¯\nâ‹¯\n ğ‘1  ğ‘ğ‘ğ‘‘â‹¯\nğ´ğ‘‘ğ‘ ğ‘¡ğ‘’ ğ‘ğ‘‘ğ´ğ‘‘ğ‘ ğ‘¡ğ‘’ 1 â‹¯\n ğ‘1  ğ‘ğ‘ğ‘‘â‹¯\n ğ‘–ğ‘›ğ‘’ğ‘ ğ‘ğ‘‘â‹¯\nTransformer \nLM\nFFN(ğ‘–1)=max(0,ğ‘–1ğ‘¾1\n +ğ‘1)ğ‘¾2 +ğ‘2 (4) \n2.2.2.3 Adapter \nAdapter modules proposed in [16] are inserted on top of MHA and \nFFN layers as in Figure 2 (left). An adapter module (Figure 2 right) \nconsists of two linear transforms and ReLU activation in between. \nA residual connection is added to the output. The outputs of adapters \nğ´1 and ğ´2 are calculated as follows: \n \nğ´1 (ğ‘–2)=ğ‘–2 +max(0,ğ‘–2ğ‘¾3 +ğ‘3)ğ‘¾4 +ğ‘4 (5) \nğ´2(ğ‘–3)=ğ‘–3 +max(0,ğ‘–3ğ‘¾5 +ğ‘5)ğ‘¾6 +ğ‘6, (6) \nwhere ğ‘–2 =MultiHead(ğ‘„,ğ¾,ğ‘‰), ğ‘–3 =FFN(ğ‘–1), adapter filter size is \nğ‘“ğ´, ğ‘¾3,ğ‘¾5 âˆˆâ„â„Ã—ğ‘“ğ´ , ğ‘3,ğ‘5 âˆˆâ„1Ã—ğ‘“ğ´ , ğ‘¾4,ğ‘¾6 âˆˆâ„ğ‘“ğ´Ã—â„, ğ‘4,ğ‘6 âˆˆ\nâ„1Ã—â„. \n2.2.3 Softmax \nThe outputs of decoder are transformed to the probabilities of output \nclasses by a linear projection ğ‘¾7 âˆˆâ„â„Ã—ğ‘ğ‘¤  and a subsequent \nsoftmax function. \n3. EXPERIMENTS \nTable 1 shows overall model architectures and model sizes used in \nthe experiments. In the experiments, a general domain LM (G-LM), \na music specialized domain LM (M-LM), and adapter added general \nand music LMs (G-LM-A, M-LM-A) are used. For single precision \nfloating point, model sizes are increased about 2% when adapters \nare added for a first domain. \nThe G-LM is trained on 24GiB normalized Korean text data \nconsisting of 353M utterances. All data were anonymized. The data \nconsists of representative utterances of Samsungâ€™s  Bixby scenario \nand general domain corpus. The M -LM is trained on normalized \nKorean text data consisting of 45M utterances, in which general and \nmusic domain (song title and singer name related commands) corpus \nare mixed. To train our models, we used Tensor2Tensor framework \n[23]. \nFor G-LM experiments, we recorded test cases (TCs) in three \ncategories: In-Domain, Out-Domain, and Open-Domain. In-domain \nTCs are having a similar data distribution with training cases but \nout-domain TCs are not. To ensure the difference, Out-Domain TCs \nare not only from completely different domain (e.g. doctor -patient \nconversation) but also include unique proper nouns. Open domain \nTCs ha ve been added to confirm that there is no performance \ndegradation before and after adaptation.  In-Domain TCs includes \n50K Bixby use -case scenario utterances such as phone and device \ncontrol commands and daily conversational question and answering. \nOut-Domain TCs includes 8K domain specific utterances which is \nnot included in In -Domain training corpus. Especially, we selected \ndomains having its own unique proper nouns such as hospital or \ndoctorâ€™s names. Open -Domain TCs are included to test noisy \nenvironment, on which cafe, city, office, highway noises are added \nto clean speech. The content of the utterances is in arbitrary domain \nand do not include unknown unique proper nouns. All TC s are \nrecorded in male and female voices. For M -LM experiments, In -\nDomain and Out-Domain TCs are recorded. In-Domain TC includes \n610 utterances. It represents well known son g titles and singer \nnames. On the other hand, Out-Domain TC includes 3709 utterances. \nThe content is newly added song titles and singer names. \nWe initialize d weights of each adapter layer  to the values \nfollowing a normal distribution having zero mean and 10eâˆ’4 \nvariance. We tested variance values of { 0, 10eâˆ’7, 10ğ‘’âˆ’6, 10ğ‘’âˆ’5, \n10ğ‘’âˆ’4, 10ğ‘’âˆ’3, 10ğ‘’âˆ’2} and selected a largest stable value. Since an \nadapter module internally has a residual connection, zero variance \ncan be inserted to test output of the adapter module is bypassed \nproperly. All runs are trained on eight P40 GPUs to build models \nfrom scratch and on one P40 GPU for ad aptations. We used Adam \noptimizer with ğ›½1 =0.9, ğ›½2 =0.98, ğœ– =1ğ‘’âˆ’9. Bat ch sizes tested \nfrom {32, 64, 128, 512, 1024, 4096, 8192}. 8192 is used for all our \nadaptation experiments. Unlike [16], small batch size made our \ntraining unstable, failing to converge. Learning rate is selected as \n0.03 from {0.1, 0.03, 0.001, 0.0003, 0.0 001}. When we train our \nmodels from scratch or adapt without adapter, we applied Noam \nlearning rate decay scheme with 1000 warmup steps. On the other \nhand, when we train our adapter related layers, learning rate decay \nscheme did not used. \nTable 1. The architectures and sizes of SA E2E, general LM (G-\nLM), music LM (M-LM), and adapter added LMs \n  \nE2E \nEnc. \nE2E \nDec. G-LM G-LM-\nA M-LM M-LM-\nA \n# Layers 6 4 3 3 2 2 \nâ„ 512 512 512 512 512 512 \nğ‘“ 3072 3072 4096 4096 2048 2048 \nğ‘“ğ´ - - - 64 - 64 \nğ‘â„ğ‘’ğ‘ğ‘‘ 16 4 8 8 8 8 \nSize (MiB) 96.7 80.3 76.4 77.9 40.3 41.3 \n \nTable 2. WERs of E2E, E2E-G-LM, and E2E-G-LM-A on General \nDomain TCs \nTC E2E E2E-G-LM E2E-G-LM-A \nIn-Domain 2.42 1.82 1.69 \nOut-Domain 10.62 8.18 2.84 \nOpen-Domain 12.8 5.08 4.55 \n \nTable 3. WERs of E2E, E2E-M-LM, and E2E-M-LM-A on Music \nDomain TCs \nTC E2E E2E-M-LM E2E-M-LM-A \nIn-Domain 8.2 2.68 2.46 \nOut-Domain 12.66 5.43 4.13 \n \nTable 4. WERs of iterative adapter fine-tuning with M-LM-A on \nMusic Domain TCs \nTC E2E-M-LM M1iter1 M1iter2 M1iter3 \nIn-Domain 2.68 2.46 1.97 1.81 \nOut-Domain 5.43 4.13 3.96 3.87 \n \nWe used 4096 word -pieces as output token units. For E2E \nmodel training, we used same hyper -parameters in [ 3]. All \nexperiments used the identical input feature processing to that of \n[24]. The decoding hyper-parameters (beam size = 4, length-penalty \n= 1.2, and maximum decoding length = 80) were tuned to minimize \nWER. Known proper nouns and number are converted with an \ninverse text normalization (ITN) module. We assumed we already \nknew proper domain names before inferencing. \n4. RESULTS \nTable 2 shows WERs measured with only E2E models (E2E), E2E \nmodels with a full fine-tuned G-LM (E2E-G-LM), and E2E models \nwith an adapter fine-tuned G-LM (E2E-G- LM-A). Compared to the \nresults decoded with only E2E models (E2E), in E2E-G-LM, WERs \nwere reduced 0.6, 2.44, and 7.72%p for in, out, and open domain \nTCs, respectively. When the full fine-tuned G-LM was additionally \nadapter fine-tuned (E2E-G-LM-A), WERs were further reduced by \n0.73, 7.78, and 8.25%p for in, out, and open domain TCs \nrespectively. In particular, in domains having unusual proper nouns, \nwe got higher improvement in accuracies. This means adapter fine-\ntuning can bias output probability properly for unusual proper nouns. \nIn addition, despite this strong biasing, the accuracy of existing \ndomain TCs did not deteriorated. \nTable 3 shows WERs measured with only E2E models (E2E), \nE2E models with a fu ll fine-tuned M-LM (E2E-M-LM), and E2E \nmodels with an adapter fine -tuned M -LM (E2E -M- LM-A). The \nresults of using E2E models with a full fine -tuned M-LM (E2E-M-\nLM) showed improved WERs than the results decoded with the E2E \nmodels alone. The WERs of in and out domain TCs were reduced \nby 5.52 and 7.23%p, respectively. When the full fine -tuned M-LM \nwas additionally adapter fine -tuned (E2E -M-LM-A), WERs were \nfurther reduced by 0.22, 1.3%p for in and out domain TCs \nrespectively. Like G -LM experiments, adapter fi ne-tunings \nimproves the proper noun recognition accuracy without \ncompromising the accuracy of existing domains, even for smaller \nmodels. \nIn Table 4, we see how far WERs can be reduced by iterative \nadapter fine-tuning. The model M1 refers to a model that an adapter \nfine-tuned M -LM using error sentences from the E2E -M-LM \ndecoding result as training data.  We considered decoding, error \nsentence extraction, and re-training a model as one iteration. From a \nmodel training point of view, an iteration also can be defined to find \na point where loss values are minimal when we train with a small \namount of iterative adaptation data.  In the experiment, accuracy \nimproved until iterations were repeated three times. Although the \nloss value was a minimum in 4th iterative ad aptation, the WER  \nincrease was observed. Therefore, the training has been stopped at \nthe point based on WER. \nTable 5 compares the case of using a G-LM as a common base \nLM with an iterative adapter fine -tuned G-LM (E2E-G-LMiter) and \nthe case of creating a dedicated M-LM and full fine-tune or adapter \nfine-tune it (E2E -M-LM, E2E -M-LM-A). Intuitively, when we \ndecode music domain TCs with the E2E model and G -LM without \nany adaptation (E2E-G-LM) as a baseline, it showed a higher error \nrates than E2E -M-LM and E2E -M-LM-A. Last two columns in \nTable 5 show word error rate reduction (WERR). When we iterative \nadapter fine-tuned the G-LM three times (E2E-G-LM-Aiter3), WERs \nwere reduced by 0.49 and 0.83%p in both in and out domain  TCs, \nrespectively, compared to E2E -M-LM. Also, the WERs of E2E -G-\nLM-Aiter3 were almost close to the results of E2E -M-LM-A. This \nmeans that a common G -LM with adapters can be used as a \ndedicated domain LM, and we can switch only adapter related layers \nto fit our model on each domain. Therefore, a  multi-domain LM \nconfiguration with the structure shown in Figure 2 is possible. \nSince ğ‘“ğ´ is a relatively small value , the increasing number of \nparameters per domain is about 2% for the first domain and about \n13% for after the second domain. Specifically, 2ğ‘ğ¿(2ğ‘“ğ´â„+ğ‘“ğ´ +â„) \nfor the first domain, because norms and Softmax linear layers can \nbe reused. 2ğ‘ğ¿(2ğ‘“ğ´â„+ğ‘“ğ´ +3â„)+(2â„+â„ğ‘ğ‘¤ +ğ‘ğ‘¤) for after the \nsecond domain. This slow increasing property is important because \nmemory size is limited for GPU or on-device applications. \nWe built our base LMs from scratch on eight P40 GPUs and on \nv3-8 tensor processing units (TPU). It took t hree days on eight P40 \nGPUs and 4 hours and 30 minutes on TPU. Iterative adapter fine -\ntuning proposed in the paper can train G-LM in 60 minutes on a P40 \nGPU and 25 minutes for M-LM. Since a P40 GPUs may be available \nin on premise servers, we expect that cl oud computing cost will be \nsaved. \n5. CONCLUSIONS  \nIn this paper, adapter based multi -domain LM structure has \nbeen proposed. The structure is a combination of two architectures: \nan adapter module proposed for BERT in NLP area and a switchable \nadapter archit ecture proposed for RNN -T streaming ASR model. \nThe proposed architecture allows LMs to expand multi -domain, \nsuppressing the increase of the number of parameters. The proposed \narchitecture can reduce WERs of target domains without WER \ndecrease of existing d omains. Also we observed that applying \nadapter module on Transformer LM has an effect on WER \nimprovement especially for proper nouns that is hard to be handled \nwith a common base LM. Finally, the proposed architecture can \nreuse standard full fine-tuned LMs. So, the full fine-tuned LMs can \nbe easily reused (or transferred) without any changes. \n  \nTable 5. Iterative fine-tuning performance (WER). The results show a G-LM with iterative fine-tuned adapters can be used as a dedicated \nmusic LM. \nTC E2E-M-LM E2E-M-LM-\nA E2E-G-LM E2E-G-LM-\nAiter1 \nE2E-G-LM-\nAiter2 \nE2E-G-LM-\nAiter3 \nWERR  \n(E2E-G-LM-Aiter3 -  \nE2E-M-LM) \nWERR  \n( E2E-G-LM-Aiter3 \n- E2E-M-LM-A) \nIn-Domain 2.68 2.46 4.65 3.82 2.38 2.19 -0.49 -0.27 \nOut-Domain 5.43 4.13 11.27 5.75 4.75 4.60 -0.83 0.47 \n \n \n6. REFERENCES \n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. \nN. Gomez, L. Kaiser, and I. Polosukhin, â€œAttention Is All You \nNeed,â€ in NIPS, 2017. \n[2] L. Dong, S. Xu, and B. Xu, â€œSpeech -transformer: a No -\nrecurrence Sequence -to-Sequence Model for Speech \nRecognition,â€ in ICASSP, 2018. \n[3] H. Kim, H. Na, H. Lee, J. Lee, T. Kang, M. Lee, and Y. Choi, \nâ€œKnowledge Distillation Using Output Errors for Self -\nAttention End-To-End Models,â€ in ICASSP, 2019. \n[4] K. Kwon, H. Na, H. Lee, and N. Kim, â€œAdaptive Knowledge \nDistillation Based On Entropy,â€ in ICASSP, 2020. \n[5] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. C. Lin, F. \nBougares, H. Schwenk, and Y. Bengio, â€œOn Using \nMonolingual Corpora in Neural Machine Translation,â€ arXiv \npreprint arXiv:1503.03535, 2015. \n[6] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and R. \nPrabhavalkar, â€œAn Analysis of Incorporating an External \nLanguage Model into a Sequence -to-Sequence Model,â€  in \nICASSP, 2018. \n[7] J. Devlin, M. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre -\ntraining of Deep Bidirectional Transformers for Language \nUnderstanding,â€ arXiv preprint arXiv:1810.04805, 2018. \n[8] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. \nLee, an d L. Zettlemoyer, â€œDeep contextualized word \nrepresentations,â€ in NAACL, 2018. \n[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. \nSutskever, â€œLanguage models are unsupervised multitask \nlearners,â€ Accessed on: May 7 2018. [Online]. Available: \nhttps://openai.com/blog/better-language-models \n[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, \nM. Lewis, L. Zettlemoyer, and V. Stoyanov, â€œRoBERTa: A \nRobustly Optimized BERT Pretraining Approach,â€ arXiv \npreprint arXiv:1907.11692, 2019. \n[11] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and \nQ. V. Le, â€œXLNet: Generalized Autoregressive Pretraining for \nLanguage Understanding,â€ in NeurIPS, 2019. \n[12] M. McCloskey and N. J. Cohen, â€œCatastrophic Interference in \nConnectionist Networks: The Sequential Learning  Problem,â€ \nin Psychology of Learning and Motivation, 1989. \n[13] Z. Li and D. Hoiem, â€œLearning without Forgetting,â€ in ECCV, \n2016 \n[14] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. \nDesjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. \nGrabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and \nR. Hadsell, â€œOvercoming catastrophic forgetting in neural \nnetworks,â€ in PNAS, 2017. \n[15] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. \nKirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell, \nâ€œProgressive N eural Networks,â€ arXiv preprint \narXiv:1606.04671, 2016. \n[16] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de \nLaroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \nâ€œParameter-Efficient Transfer Learning for NLP,â€ in ICML, \n2019. \n[17] S. A. Rebuffi, H. Bilen,  and A. Vedaldi, â€œEfficient \nParametrization of Multi -Domain Deep Neural Networks,â€ in \nCVPR, 2018. \n[18] A. C. Stickland and I. Murray, â€œBERT and PALs: Projected \nAttention Layers for Efficient Adaptation in Multi -Task \nLearning,â€ in PMLR, 2019. \n[19] S. J. Semnani, K. R. Sadagopan, and F. Tlili, â€œBERT-A: Fine-\nTuning BERT with Adapters and Data Augmentation,â€ \n[Online]. Available: \nhttp://web.stanford.edu/class/cs224n/reports/default/15848417\n.pdf \n[20] A. Kannan, A. Datta, T. N. Sainath, E. Weinstein, B. \nRamabhadran, Y. Wu, A. Bapna, Z. Chen, and S. Lee, â€œLarge-\nScale Multilingual Speech Recognition with a Streaming End-\nto-End Model,â€ in INTERSPEECH, 2019. \n[21] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. \nMacherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. \nKlingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, \nY. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, \nW. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, \nG. Corrado, M. Hughes, and J. Dean, â€œGoogleâ€™s Neural \nMachine Translation System: Bridging the Gap between \nHuman and Machine Translation,â€ arXiv preprint \narXiv:1609.08144, 2016. \n[22] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer Normalization,â€ \narXiv preprint arXiv:1607.06450, 2016. \n[23] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet, A. N. Gomez, S. \nGouws, L. Jones, L. Kaiser, N. Kalchbrenner, N. Parmar, R. \nSepassi, N. Shazeer, and J. Uszkoreit, â€œTensor2Tensor for \nNeural Machine Translation,â€ arXiv preprint \narXiv:1803.07416, 2018. \n[24] C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. \nChen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina, N. Jaitly, B. \nLi, J. Chorowski, and M. Bacchiani â€œState -of-the-art Speech \nRecognition with Sequence-to-Sequence Models,â€ in ICASSP, \n2018. \n[25] K. Irie, A. Zeyer, R. Schluter, and H. Ney, â€œLanguage \nModeling with Deep Transformers,â€ in INTERSPEECH, 2019. \n ",
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.8183936476707458
    },
    {
      "name": "Transformer",
      "score": 0.80696702003479
    },
    {
      "name": "Computer science",
      "score": 0.746406078338623
    },
    {
      "name": "Reuse",
      "score": 0.6456862688064575
    },
    {
      "name": "Language model",
      "score": 0.631094217300415
    },
    {
      "name": "Domain model",
      "score": 0.46021783351898193
    },
    {
      "name": "Speech recognition",
      "score": 0.4470231831073761
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3152954578399658
    },
    {
      "name": "Computer hardware",
      "score": 0.19885969161987305
    },
    {
      "name": "Electrical engineering",
      "score": 0.13732966780662537
    },
    {
      "name": "Engineering",
      "score": 0.13235902786254883
    },
    {
      "name": "Domain knowledge",
      "score": 0.11426153779029846
    },
    {
      "name": "Voltage",
      "score": 0.0820571780204773
    },
    {
      "name": "Waste management",
      "score": 0.0
    }
  ]
}