{
  "title": "Composer Style Classification of Piano Sheet Music Images Using Language Model Pretraining",
  "url": "https://openalex.org/W3045676752",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3147026978",
      "name": "Tsai Tj",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ji, Kevin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2110629808",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2990466364",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W1569598860",
    "https://openalex.org/W2043701535",
    "https://openalex.org/W2989817308",
    "https://openalex.org/W2575595060",
    "https://openalex.org/W2767106145",
    "https://openalex.org/W46679369",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2891469296",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W2963208801",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2534129698",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W139961733",
    "https://openalex.org/W2797805271",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2889118961",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W133467950",
    "https://openalex.org/W2802792548"
  ],
  "abstract": "This paper studies composer style classification of piano sheet music images. Previous approaches to the composer classification task have been limited by a scarcity of data. We address this issue in two ways: (1) we recast the problem to be based on raw sheet music images rather than a symbolic music format, and (2) we propose an approach that can be trained on unlabeled data. Our approach first converts the sheet music image into a sequence of musical \"words\" based on the bootleg feature representation, and then feeds the sequence into a text classifier. We show that it is possible to significantly improve classifier performance by first training a language model on a set of unlabeled data, initializing the classifier with the pretrained language model weights, and then finetuning the classifier on a small amount of labeled data. We train AWD-LSTM, GPT-2, and RoBERTa language models on all piano sheet music images in IMSLP. We find that transformer-based architectures outperform CNN and LSTM models, and pretraining boosts classification accuracy for the GPT-2 model from 46\\% to 70\\% on a 9-way classification task. The trained model can also be used as a feature extractor that projects piano sheet music into a feature space that characterizes compositional style.",
  "full_text": "COMPOSER STYLE CLASSIFICATION OF PIANO SHEET MUSIC\nIMAGES USING LANGUAGE MODEL PRETRAINING\nTJ Tsai\nHarvey Mudd College\nttsai@g.hmc.edu\nKevin Ji\nHarvey Mudd College\nkji@g.hmc.edu\nABSTRACT\nThis paper studies composer style classiﬁcation of piano\nsheet music images. Previous approaches to the composer\nclassiﬁcation task have been limited by a scarcity of data.\nWe address this issue in two ways: (1) we recast the prob-\nlem to be based on raw sheet music images rather than a\nsymbolic music format, and (2) we propose an approach\nthat can be trained on unlabeled data. Our approach ﬁrst\nconverts the sheet music image into a sequence of musi-\ncal “words\" based on the bootleg feature representation,\nand then feeds the sequence into a text classiﬁer. We show\nthat it is possible to signiﬁcantly improve classiﬁer perfor-\nmance by ﬁrst training a language model on a set of un-\nlabeled data, initializing the classiﬁer with the pretrained\nlanguage model weights, and then ﬁnetuning the classiﬁer\non a small amount of labeled data. We train AWD-LSTM,\nGPT-2, and RoBERTa language models on all piano sheet\nmusic images in IMSLP. We ﬁnd that transformer-based ar-\nchitectures outperform CNN and LSTM models, and pre-\ntraining boosts classiﬁcation accuracy for the GPT-2 model\nfrom 46% to 70% on a 9-way classiﬁcation task. The\ntrained model can also be used as a feature extractor that\nprojects piano sheet music into a feature space that charac-\nterizes compositional style.\n1. INTRODUCTION\nWe’ve all had the experience of hearing a piece of music\nthat we’ve never heard before, but immediately recogniz-\ning the composer based on the piece’s style. This paper\nexplores this phenomenon in the context of sheet music.\nThe question that we want to answer is: “Can we predict\nthe composer of a previously unseen page of piano sheet\nmusic based on its compositional style?\"\nMany previous works have studied the composer clas-\nsiﬁcation problem. These works generally fall into one of\ntwo categories. The ﬁrst category of approach is to con-\nstruct a set of features from the music, and then feed the\nfeatures into a classiﬁer. Many works use manually de-\nsigned features that capture musically meaningful informa-\ntion (e.g. [15] [27] [1] [9]). Other works feed minimally\nc⃝ TJ Tsai, Kevin Ji. Licensed under a Creative Commons\nAttribution 4.0 International License (CC BY 4.0).Attribution: TJ Tsai,\nKevin Ji. “Composer Style Classiﬁcation of Piano Sheet Music Images\nUsing Language Model Pretraining”, 21st International Society for Music\nInformation Retrieval Conference, Montréal, Canada, 2020.\npreprocessed representations of the data (e.g. 2-D piano\nrolls [33] [34] or tensors encoding note pitch & duration\ninformation [35] [3]) into a convolutional model, and al-\nlow the model to learn a useful feature representation. The\nsecond category of approach is to train one model for each\ncomposer, and then select the model that has the highest\nlikelihood of generating a given sequence of music. Com-\nmon approaches in this category include N-gram language\nmodels [36] [11] [10] and Markov models [14] [23].\nOur approach to the composer classiﬁcation task ad-\ndresses what we perceive to be the biggest common ob-\nstacle to the above approaches: lack of data. All of the\nabove approaches assume that the input is in the form of a\nsymbolic music ﬁle (e.g. MIDI or **kern). Because sym-\nbolic music formats are much less widely used than audio,\nvideo, and image formats, the amount of training data that\nis available is quite limited. We address this issue of data\nscarcity in two ways: (1) we re-deﬁne the composer classi-\nﬁcation task to be based on sheet music images, for which\nthere is a lot of data available online, and (2) we propose\nan approach that can be trained on unlabeled data.\nOur work takes advantage of recent developments in\ntransfer learning in the natural language processing (NLP)\ncommunity. Prior to 2017, transfer learning in NLP was\ndone in a limited way. Typically, one would use pre-\ntrained word embeddings such as word2vec [20] [19] or\nGloVe [22] vectors as the ﬁrst layer in a model. The prob-\nlem with this paradigm of transfer learning is that the en-\ntire model except the ﬁrst layer needs to be trained from\nscratch, which requires a large amount of labeled data.\nThis is in contrast to the paradigm of transfer learning in\ncomputer vision, where a model is trained on the ImageNet\nclassiﬁcation task [26], the ﬁnal layer is replaced with a\ndifferent linear classiﬁer, and the model is ﬁnetuned for a\ndifferent task. The beneﬁt of this latter paradigm of trans-\nfer learning is that the entire model except the last layer is\npretrained, so it can be ﬁnetuned with only a small amount\nof labeled data. This paradigm of transfer learning has\nbeen widely used in computer vision in the last decade\n[39] using pretrained models like VGG [29], ResNet [8],\nDensenet [13], etc. The switch to ImageNet-style transfer\nlearning in the NLP community occurred in 2017, when\nHoward et al. [12] proposed a way to pretrain an LSTM-\nbased language model on a large set of unlabeled data,\nadd a classiﬁcation head on top of the language model,\nand then ﬁnetune the classiﬁer on a new task with a small\namount of labeled data. This was quickly followed by sev-\neral other similar language model pretraining approaches\narXiv:2007.14587v1  [cs.CV]  29 Jul 2020\nFigure 1. Overview of proxy classiﬁer training. A lan-\nguage model is ﬁrst trained on a set of unlabeled data, the\nclassiﬁer is initialized with the pretrained language model\nweights, and then the classiﬁer is ﬁnetuned on a small set\nof labeled data.\nthat replaced the LSTM with transformer-based architec-\ntures (e.g. GPT [24], GPT-2 [25], BERT [5]). These\npretrained language models have provided the basis for\nachieving state-of-the-art results on a variety of NLP tasks,\nand have been extended in various ways (e.g. Transformer-\nXL [4], XLNet [38]).\nOur approach is similarly based on language model pre-\ntraining. We ﬁrst convert each sheet music image into a\nsequence of words based on the bootleg score feature rep-\nresentation [37]. We then feed this sequence of words into\na text classiﬁer. We show that it is possible to signiﬁcantly\nimprove the performance of the classiﬁer by training a lan-\nguage model on a large set of unlabeled data, initialize the\nclassiﬁer with the pretrained language model weights, and\nﬁnetune the classiﬁer on a small amount of labeled data.\nIn our experiments, we train language models on all pi-\nano sheet music images in the International Music Score\nLibrary Project (IMSLP) 1 using the AWD-LSTM [18],\nGPT-2 [25], and RoBERTa [16] language model architec-\ntures. By using pretraining, we are able to improve the ac-\ncuracy of our GPT-2 model from 46% to 70% on a 9-way\nclassiﬁcation task. 2\n2. SYSTEM DESCRIPTION\nWe will describe our system in the next four subsections.\nIn the ﬁrst subsection, we give a high-level overview and\nrationale behind our approach. In the following three sub-\nsections, we describe the three main stages of system de-\nvelopment: language model pretraining, classiﬁer ﬁnetun-\ning, and inference.\n2.1 Overview\nFigure 1 summarizes our training approach. In the ﬁrst\nstage, we convert each sheet music image into a sequence\nof words based on the bootleg score representation [37],\nand then train a language model on these words. Since\nthis task does not require labels, we can train our language\nmodel on a large set of unlabeled data. In this work, we\ntrain our language model on all piano sheet music images\n1 http://imslp.org/\n2 Code can be found at https://github.com/tjtsai/\nPianoStyleEmbedding.\nFigure 2. A short section of sheet music and its corre-\nsponding bootleg score. Staff lines in the bootleg score\nare shown for reference, but are not present in the actual\nfeature representation.\nin the IMSLP dataset. In the second stage, we train a clas-\nsiﬁer that predicts the composer of a short fragment of mu-\nsic, where the fragment is a ﬁxed-length sequence of sym-\nbolic words. We do this by adding one or more dense lay-\ners on top of the language model, initializing the weights\nof the classiﬁer with the language model weights, and then\nﬁnetuning the model on a set of labeled data. In the third\nstage, we use the classiﬁer to predict the composer of an\nunseen scanned page of piano sheet music. We do this by\nconverting the sheet music image to a sequence of sym-\nbolic words, and then either (a) applying the classiﬁer to a\nsingle variable length input sequence, or (b) averaging the\npredictions of ﬁxed-length crops sampled from the input\nsequence. We will describe each of these three stages in\nmore detail in the following three subsections.\nThe guiding principle behind our approach is to max-\nimize the amount of data. This impacts our approach in\nthree signiﬁcant ways. First, it informs our choice of data\nformat. Rather than using symbolic scores (as in previ-\nous approaches), we instead choose to use raw sheet music\nimages. While this arguably makes the task much more\nchallenging, it has the beneﬁt of having much more data\navailable online. Second, we choose an approach that can\nutilize unlabeled data. Whereas labeled data is usually ex-\npensive to annotate and limited in quantity, unlabeled data\nis often extremely cheap and available in abundance. By\nadopting an approach that can use unlabeled data, we can\ndrastically increase the amount of data available to train\nour models. Third, we use data augmentation to make\nthe most of the limited quantity of labeled data that we do\nhave. Rather than ﬁxating on the page classiﬁcation task,\nwe instead deﬁne a proxy task where the goal is to predict\nthe composer given a ﬁxed-length sequence of symbolic\nwords. By deﬁning the proxy task in this way, we can\naggressively subsample fragments from the labeled data,\nresulting in a much larger number of unique training data\npoints than there are actual pages of sheet music. Once the\nproxy task classiﬁer has been trained, we can apply it to the\nfull page classiﬁcation task in a straightforward manner.\n2.2 Language Model Pretraining\nThe language model pretraining consists of three steps, as\nshown in the upper half of Figure 1. These three steps will\nFigure 3 . Overview of AWD-LSTM, GPT-2, and\nRoBERTa language models (top) and classiﬁers (bottom).\nBoxes in blue are trained during the language modeling\nphase and used to initialize the classiﬁer.\nbe described in the next three paragraphs.\nThe ﬁrst step is to convert the sheet music image into a\nbootleg score. The bootleg score is a low-dimensional fea-\nture representation of piano sheet music that encodes the\nposition of ﬁlled noteheads relative to the staff lines [37].\nFigure 2 shows an example of a section of sheet music and\nits corresponding bootleg score representation. The boot-\nleg score itself is a 62 × N binary matrix, where 62 in-\ndicates the total number of possible staff line positions in\nboth the left and right hands, and where N indicates the\ntotal number of estimated simultaneous note onset events.\nNote that the representation discards a signiﬁcant amount\nof information: it does not encode note duration, key signa-\nture, time signature, measure boundaries, accidentals, clef\nchanges, or octave markings, and it simply ignores non-\nﬁlled noteheads (e.g. half or whole notes). Nonetheless, it\nhas been shown to be effective in aligning sheet music and\nMIDI [37], and we hypothesize that it may also be useful\nin characterizing piano style. The main beneﬁt of using\nthe bootleg score representation over a full optical music\nrecognition (OMR) pipeline is processing time: computing\na bootleg score only takes about 1 second per page using\na CPU, which makes it suitable for computing features on\nthe entire IMSLP dataset. 3 We use the code from [37] as\na ﬁxed feature extractor to compute the bootleg scores.\nThe second step is to tokenize the bootleg score into a\nsequence of word or subword units. We do this differently\nfor different language models. For word-based language\nmodels (e.g. AWD-LSTM [18]), we consider each bootleg\nscore column as a single word consisting of a 62-character\nstring of 0s and 1s. We limit the vocabulary to the 30, 000\nmost frequent words, and map infrequent words to a spe-\ncial unknown word token <unk>. For subword-based lan-\nguage models (e.g. GPT-2 [25], RoBERTa [16]), we use a\nbyte pair encoding (BPE) algorithm [6] to learn a vocab-\nulary of subword units in an unsupervised manner. The\nBPE algorithm starts with an initial set of subword units\n(e.g. the set of unique characters [28] or the 28 = 256\nunique byte values that comprise unicode characters [7]),\nand it iteratively merges the most frequently occurring pair\nof adjacent subword units until a desired vocabulary size\n3 In contrast, the best performing music object detectors take 40-80\nseconds to process each page at inference time using a GPU [21].\nhas been reached. We experimented with both character-\nlevel and byte-level encoding schemes (i.e. representing\neach word as a string of 62 characters vs. a sequence of 8\nbytes), and we found that the byte-level encoding scheme\nperforms much better. We only report results with the byte-\nlevel BPE tokenizer. For both subword-based language\nmodels explored in this work, we use the same shared BPE\ntokenizer with a vocabulary size of 30, 000 (which is the\nvocabularly size used in the RoBERTa model). At the end\nof the second step, we have represented the sheet music\nimage as a sequence of words or subword units.\nThe third step is to train a language model on a set of\nunlabeled data. In this work, we explore three different\nlanguage models, which are representative of state-of-the-\nart models in the last 3-4 years. The top half of Figure 3\nshows a high-level overview of these three language mod-\nels. The ﬁrst model is AWD-LSTM [18]. This is a 3-layer\nLSTM architecture that makes heavy use of regularization\ntechniques throughout the model, including four different\ntypes of dropout. The output of the ﬁnal LSTM layer is\nfed to a linear decoder whose weights are tied to the input\nembedding matrix. This produces an output distribution\nacross the tokens in the vocabulary. The model is then\ntrained to predict the next token at each time step. We\nuse the fastai implementation of the AWD-LSTM model\nwith default parameters. The second model is openAI’s\nGPT-2 [25]. This architecture consists of multiple trans-\nformer decoder layers [32]. Each transformer decoder\nlayer consists of a masked self-attention, along with feed-\nforwards layers, layer normalizations, and residual connec-\ntions. While transformer encoder layers allow each token\nto attend to all other tokens in the input, the transformer\ndecoder layers only allow a token to attend to previous to-\nkens. 4 Similar to the AWD-LSTM model, the outputs of\nthe last transformer layer are fed to a linear decoder whose\nweights are tied to the input embeddings, and the model\nis trained to predict the next token at each time step. We\nuse the huggingface implementation of the GPT-2 model\nwith default parameters, except that we reduce the vocabu-\nlary size from 50, 000 to 30, 000 (to use the same tokenizer\nas the RoBERTa model), the amount of context from 1024\nto 512, and the number of layers from 12 to 6. The third\nmodel is RoBERTa [16], which is based on Google’s BERT\nlanguage model [5]. This architecture consists of multiple\ntransformer encoder layers. Unlike GPT-2, each token can\nattend to all other tokens in the input and the goal is not to\npredict the next token. Instead, a certain fraction of the in-\nput tokens are randomly converted to a special <mask> to-\nken, and the model is trained to predict the masked tokens.\nWe use the huggingface implementation of RoBERTa with\ndefault parameter settings, except that we reduce the num-\nber of layers from 12 to 6.\n2.3 Classiﬁer Finetuning\nIn the second main stage, we ﬁnetune a classiﬁer based\non a set of labeled data. The labeled data consists of a\n4 This is because, in the original machine translation task [32], the\ndecoder generates the output sentence autoregressively.\nset of sheet music images along with their corresponding\ncomposer labels. The process of training the classiﬁer is\ncomprised of four steps (lower half of Figure 1).\nThe ﬁrst two steps are to compute and tokenize a boot-\nleg score into a sequence of symbolic words. We use the\nsame ﬁxed feature extractor and the same tokenizer that\nwere used in the language model pretraining stage.\nThe third step is to sample short, ﬁxed-length fragments\nof words from the labeled data. As mentioned in Section\n2.1, we deﬁne a proxy task where the goal is to predict the\ncomposer given a short, ﬁxed-length fragment of words.\nDeﬁning the proxy task in this way has three signiﬁcant\nbeneﬁts: (1) we can use sampling to generate many more\nunique training data points than there are actual pages of\nsheet music in our dataset, (2) we can sample the data in\nsuch a way that the classes are balanced, which avoids\nproblems during training, and (3) using ﬁxed-length in-\nputs allows us to train more efﬁciently in batches. Our\napproach follows the general recommendations of a recent\nstudy on best practices for training a classiﬁer with imbal-\nanced data [2]. Each sampled fragment and its correspond-\ning composer label constitute a single(Xi, yi) training pair\nfor the proxy task.\nThe fourth step is to train the classiﬁer model. The bot-\ntom half of Figure 3 shows how this is done with our three\nmodels. Our general approach is to add a classiﬁer head\non top of the language model, initialize the weights of the\nclassiﬁer with the pretrained language model weights, and\nthen ﬁnetune the classiﬁer on the proxy task data. For\nthe AWD-LSTM, we take the outputs from the last LSTM\nlayer and construct a ﬁxed-size representation by concate-\nnating three things: (a) the output at the last time step, (b)\nthe result of max pooling the outputs across the sequence\ndimension, and (c) the result of average pooling the out-\nputs across the sequence dimension. This ﬁxed-size repre-\nsentation (which is three times the hidden dimension size)\nis then fed into the classiﬁer head, which consists of two\ndense layers with batch normalization and dropout. For\nthe GPT-2 model, we take the output from the last trans-\nformer layer at the last time step, and then feed it into a\nsingle dense (classiﬁcation) layer. Because the GPT-2 and\nRoBERTa models require special tokens during training,\nwe insert special symbols <s> and </s> at the beginning\nand end of every training input, respectively. Because of\nthe masked self-attention, we must use the output of the\nlast token in order to access all of the information in the\ninput sequence. For the RoBERTa model, we take the out-\nput from the last transformer layer corresponding to the\n<s> token, and feed it into a single dense (classiﬁcation)\nlayer. The <s> takes the place of the special [CLS] token\ndescribed in the original paper.\nWe integrated all models into the fastai framework and\nﬁnetuned the classiﬁer in the following manner. We ﬁrst\nselect an appropriate learning rate using a range test, in\nwhich we sweep the learning rate across a wide range of\nvalues and observe the impact on training loss. We initially\nfreeze all parameters in the model except for the untrained\nclassiﬁcation head, and we gradually unfreeze more and\nmore layers in the model as the training converges. To\nFigure 4 . Statistics on the target dataset. The top two\nhistograms show the distribution of the number of pages\n(top left) and number of bootleg score features (top right)\nper composer. The bottom ﬁgure shows the distribution of\nthe number of bootleg score features per page.\navoid overly aggressive changes to the pretrained language\nmodel weights, we use discriminative ﬁnetuning, in which\nearlier layers of the model use exponentially smaller learn-\ning rates compared to later layers in the model. All train-\ning is done with (multiple cycles of) the one cycle training\npolicy [30], in which learning rate and momentum are var-\nied cyclically over each cycle. The above practices were\nproposed in [12] and found to be effective in ﬁnetuning\nlanguage models for text classiﬁcation.\n2.4 Inference\nThe third main stage is to apply the proxy classiﬁer to the\noriginal full page classiﬁcation task. We explore two dif-\nferent ways to do this. The ﬁrst method is to convert the\nsheet music image into a bootleg score, tokenize the boot-\nleg score into a sequence of word or subword units, and\nthen apply the proxy classiﬁer to a single variable-length\ninput. Note that all of the models can handle variable-\nlength inputs up to a maximum context length. The second\nmethod is identical to the ﬁrst, except that it averages the\npredictions from multiple ﬁxed-length crops taken from\nthe input sequence. The ﬁxed-length crops are the same\nsize as is used during classiﬁer training, and the crops are\nsampled uniformly with 50% overlap. 5\n3. EXPERIMENTAL SETUP\nIn this section we describe the data collection process and\nthe metrics used to evaluate our approach.\nThe data comes from IMSLP. We ﬁrst scraped the web-\nsite and downloaded all PDF scores and accompanying\nmetadata. 6 We ﬁltered the data based on its instrumen-\ntation in order to identify a list of solo piano scores. We\nthen computed bootleg score features for all of the piano\nsheet music images using the XSEDE supercomputing in-\nfrastructure [31], and discarded any pages that had less\nthan a minimum threshold of features. This latter step\nis designed to remove non-music pages such as the title\n5 We also experimented with applying a Bayesian prior to the classiﬁer\nsoftmax outputs, as recommended in [2], but found that the results were\nnot consistently better.\n6 We downloaded the data over a span of several weeks in May of\n2018.\npage, foreword, or table of contents. The resulting set of\ndata contained 29, 310 PDFs, 7 255, 539 pages and a total\nof 48.5 million bootleg score features. This set of data is\nwhat we refer to as the IMSLP dataset in this work (e.g. the\nIMSLP pretrained language model). For language model\ntraining, we split the IMSLP data by piece, using 90% for\ntraining and 10% for validation.\nThe classiﬁcation task uses a subset of the IMSLP data.\nWe ﬁrst identiﬁed a list of composers with a signiﬁcant\namount of data (composers shown in Figure 4). We limited\nthe list to nine composers in order to avoid extreme class\nimbalance. Because popular pieces tend to have many\nsheet music versions in the dataset, we select one version\nper piece in order to avoid over-representation of a small\nsubset of pieces. Next, we manually labeled and discarded\nall ﬁller pages, and then computed bootleg score features\non the remaining sheet music images. This cleaned dataset\nis what we refer to as the target data in this work (e.g.\nthe target pretrained language model). Figure 4 shows\nthe total number of pages and bootleg score features per\ncomposer for the target dataset, along with the distribu-\ntion of the number of bootleg score features per page. For\ntraining and testing, we split the data by piece, using 60%\nof the pieces for training ( 4347 pages), 20% for valida-\ntion (1500 pages), and 20% for testing ( 1304 pages). To\ngenerate data for the proxy task, we randomly sampled\nﬁxed-length fragments from the target data. We sample\nthe same number of fragments for each composer to en-\nsure class balance. We experimented with fragment sizes\nof 64/128/256 and sampled 32400/16200/8100 fragments\nfor training and 10800/5400/2700 fragments for valida-\ntion/test, respectively. This sampling scheme ensures the\nsame data coverage regardless of fragment length. Note\nthat the classiﬁcation data is carefully curated, while the\nIMSLP data requires minimal processing.\nWe use two different metrics to evaluate our systems.\nFor the proxy task, accuracy is an appropriate metric since\nthe data is balanced. For the full page classiﬁcation task\n– which has imbalanced data – we report results in macro\nF1 score. Macro F1 is a generalization of F1 score to a\nmulti-class setting, in which each class is treated as a one-\nversus-all binary classiﬁcation task and the F1 scores from\nall classes are averaged.\n4. RESULTS & ANALYSIS\nIn this section we present our experimental results and con-\nduct various analyses to answer key questions of interest.\nWhile the proxy task is an artiﬁcially created task, it pro-\nvides a more reliable indicator of classiﬁer performance\nthan the full page classiﬁcation. This is because the test set\nof the full page classiﬁcation task is both imbalanced and\nvery small (1304 data points). Accordingly, we will report\nresults on both the proxy task and full page classiﬁcation\ntask.\n7 Note that a PDF may contain multiple pieces (e.g. the complete set\nof Chopin etudes).\nFigure 5. Model performance on the proxy classiﬁcation\ntask. This comparison shows the effect of different pre-\ntraining conditions and fragment sizes.\n4.1 Proxy Task\nWe ﬁrst consider the performance of our models on the\nproxy classiﬁcation task. We would like to understand the\neffect of (a) model architecture, (b) pretraining condition,\nand (c) fragment size.\nWe evaluate four different model architectures. In ad-\ndition to the AWD-LSTM, GPT-2, and RoBERTa models\npreviously described, we also measure the performance of\na CNN-based approach recently proposed in [35]. Note\nthat we cannot use the exact same model in [35] since\nwe do not have symbolic score information. Nonetheless,\nwe can use the same general approach of computing local\nfeatures, aggregating feature statistics across time, and ap-\nplying a linear classiﬁer. The design of our 2-layer CNN\nmodel roughly matches the architecture proposed in [35].\nWe consider three different language model pretrain-\ning conditions. The ﬁrst condition is with no pretraining,\nwhere we train the classiﬁer from scratch only on the proxy\ntask. The second condition is with target language model\npretraining, where we ﬁrst train a language model on the\ntarget data, and then ﬁnetune the classiﬁer on the proxy\ntask. The third condition is with IMSLP language model\npretraining. Here, we train a language model on the full\nIMSLP dataset, ﬁnetune the language model on the target\ndata, and then ﬁnetune the classiﬁer on the proxy task.\nFigure 5 shows the performance of all models on the\nproxy task. There are three things to notice. First, re-\ngarding (a), the transformer-based models generally out-\nperform the LSTM and CNN models. Second, regarding\n(b), language model pretraining improves performance sig-\nniﬁcantly across the board. Regardless of architecture, we\nsee a large improvement going from no pretraining (con-\ndition 1) to target pretraining (condition 2), and another\nlarge improvement going from target pretraining (condi-\ntion 2) to IMSLP pretraining (condition 3). For exam-\nple, the performance of the GPT-2 model increases from\n37.3% to 45.2% to 57.5% across the three pretraining con-\nditions. Because the data in conditions 1 & 2 is exactly the\nFigure 6. Results on the full page classiﬁcation task.\nsame, the improvement in performance must be coming\nfrom more effective use of the data. We can interpret this\nfrom an information theory perspective by noting that the\nclassiﬁcation task provides the model log29 = 3.17 bits of\ninformation per fragment, whereas the language modeling\ntask provides log2V bits of information per bootleg score\nfeature where V is the vocabulary size. The performance\ngap between condition 2 and condition 3 can also be inter-\npreted as the result of providing more information to the\nmodel, but here the information is coming from having ad-\nditional data. Third, regarding (c), larger fragments result\nin better performance, as we might expect.\n4.2 Full Page Classiﬁcation\nNext, we consider performance of our models on the full\npage classiﬁcation task. We would like to understand the\neffect of (a) model architecture, (b) pretraining condition,\n(c) fragment size, and (d) inference type (single vs. multi-\ncrop). Regarding (d), we found that taking multiple crops\nimproved results with all models except the CNN. This\nsuggests that this type of test time augmentation does not\nbeneﬁt approaches that simply average feature statistics\nover time. In the results presented below, we only show\nthe optimal inference type for each model architecture (i.e.\nCNN with single crop, all others with multi-crop).\nFigure 6 shows model performance on the full page\nclassiﬁcation task. There are two things to notice. First, we\nsee the same general trends as in Figure 5 for model archi-\ntecture and pretraining condition: the transformer-based\nmodels generally outperform the CNN and LSTM mod-\nels, and pretraining helps substantially in every case. The\nmacro F1 score of our best model (GPT-2 with fragment\nsize 64) increases from0.41 to 0.51 to 0.67 across the three\npretraining conditions. Second, we see the opposite trend\nas the proxy task for fragment size: smaller fragments have\nbetter page classiﬁcation performance. This strongly indi-\ncates a data distribution mismatch. Indeed, when we look\nat the distribution of the number of bootleg score features\nin a single page (Figure 4), we see that a signiﬁcant frac-\ntion of pages have less than 256 features. Because we only\nFigure 7. t-SNE plot of the RoBERTa model activations\nfor ﬁve novel composers. Each data point corresponds to\na single page of sheet music for a composer that was not\nconsidered in the classiﬁcation task.\nsample fragments that contain a complete set of 256 words,\nour proxy task data is biased towards longer inputs. This\nleads to poor performance when the classiﬁer is faced with\nshort inputs, which are never seen in training. Using a frag-\nment size of 64 minimizes this bias.\n4.3 t-SNE Plots\nAnother key question of interest is, “Can we use our model\nto characterize the style of any page of piano sheet music?\"\nThe classiﬁcation task forces the model to project the sheet\nmusic into a feature space where the compositional style of\nthe nine composers can be differentiated. We hypothesize\nthat this feature space might be useful in characterizing the\nstyle of any page of piano sheet music, even from com-\nposers not in the classiﬁcation task.\nTo test this hypothesis, we fed data from 5 novel com-\nposers into our models and constructed t-SNE plots [17] of\nthe activations at the second-to-last layer. Figure 7 shows\nsuch a plot for the RoBERTa model. Each data point corre-\nsponds to a single page of sheet music from a novel com-\nposer. Even though we have not trained the classiﬁer to\ndistinguish between these ﬁve composers, we can see that\nthe data points are still clustered, suggesting that the fea-\nture space can describe the style of new composers in a\nuseful manner.\n5. CONCLUSION\nWe propose a method for predicting the composer of a sin-\ngle page of piano sheet music. Our method ﬁrst converts\nthe raw sheet music image into a bootleg score, tokenizes\nthe bootleg score into a sequence of musical words, and\nthen feeds the sequence into a text classiﬁer. We show that\nby pretraining a language model on a large set of unlabeled\ndata, it is possible to signiﬁcantly improve the performance\nof the classiﬁer. We also show that our trained model can\nbe used as a feature extractor to characterize the style of\nany page of piano sheet music. For future work, we would\nlike to explore other forms of data augmentation and other\nmodel architectures that explicitly encode musical knowl-\nedge.\n6. ACKNOWLEDGMENTS\nThis work used the Extreme Science and Engineering Dis-\ncovery Environment (XSEDE), which is supported by Na-\ntional Science Foundation grant number ACI-1548562.\nLarge-scale computations on IMSLP data were performed\nwith XSEDE Bridges at the Pittsburgh Supercomputing\nCenter through allocation TG-IRI190019. We also grate-\nfully acknowledge the support of NVIDIA Corporation\nwith the donation of the GPU used for training the mod-\nels.\n7. REFERENCES\n[1] Andrew Brinkman, Daniel Shanahan, and Craig Sapp.\nMusical stylometry, machine learning and attribution\nstudies: A semi-supervised approach to the works of\njosquin. In Proc. of the Biennial Int. Conf. on Music\nPerception and Cognition, pages 91–97, 2016.\n[2] Mateusz Buda, Atsuto Maki, and Maciej A\nMazurowski. A systematic study of the class im-\nbalance problem in convolutional neural networks.\nNeural Networks, 106:249–259, 2018.\n[3] Giuseppe Buzzanca. A supervised learning approach\nto musical style recognition. In Proc. of the interna-\ntional conference on music and artiﬁcial intelligence\n(ICMAI), volume 2002, page 167, 2002.\n[4] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a\nﬁxed-length context. In Proc. of the 57th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 2978–2988, 2019.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding.\narXiv preprint arXiv:1810.04805, 2018.\n[6] Philip Gage. A new algorithm for data compression. C\nUsers Journal, 12(2):23–38, 1994.\n[7] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag\nSubramanya. Multilingual language processing from\nbytes. In Proc. of the Conference of the North Ameri-\ncan Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, pages 1296–\n1306, 2016.\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. In\nProc. of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[9] Dorien Herremans, David Martens, and Kenneth\nSörensen. Composer classiﬁcation models for music-\ntheory building. In Computational Music Analysis ,\npages 369–392. 2016.\n[10] Ruben Hillewaere, Bernard Manderick, and Darrell\nConklin. String quartet classiﬁcation with monophonic\nmodels. In Proc. of the International Society for Mu-\nsic Information Retrieval Conference (ISMIR) , pages\n537–542, 2010.\n[11] María Hontanilla, Carlos Pérez-Sancho, and Jose M In-\nesta. Modeling musical style with language models for\ncomposer recognition. In Iberian Conference on Pat-\ntern Recognition and Image Analysis , pages 740–748,\n2013.\n[12] Jeremy Howard and Sebastian Ruder. Universal lan-\nguage model ﬁne-tuning for text classiﬁcation. InProc.\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics, pages 328–339, 2018.\n[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten,\nand Kilian Q Weinberger. Densely connected convo-\nlutional networks. In Proc. of the IEEE conference on\ncomputer vision and pattern recognition, pages 4700–\n4708, 2017.\n[14] Maximos A Kaliakatsos-Papakostas, Michael G\nEpitropakis, and Michael N Vrahatis. Weighted\nmarkov chain model for musical composer identiﬁca-\ntion. In European Conference on the Applications of\nEvolutionary Computation, pages 334–343, 2011.\n[15] Katherine C Kempfert and Samuel WK Wong.\nWhere does haydn end and mozart begin? com-\nposer classiﬁcation of string quartets. arXiv preprint\narXiv:1809.05075, 2018.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized BERT pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[17] Laurens van der Maaten and Geoffrey Hinton. Visual-\nizing data using t-SNE. Journal of machine learning\nresearch, 9(Nov):2579–2605, 2008.\n[18] Stephen Merity, Nitish Shirish Keskar, and Richard\nSocher. Regularizing and optimizing LSTM language\nmodels. arXiv preprint arXiv:1708.02182, 2017.\n[19] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781, 2013.\n[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. Distributed representations\nof words and phrases and their compositionality. In\nAdvances in neural information processing systems ,\npages 3111–3119, 2013.\n[21] Alexander Pacha, Jan Haji ˇc, and Jorge Calvo-\nZaragoza. A baseline for general music object detec-\ntion with deep learning. Applied Sciences, 8(9):1488,\n2018.\n[22] Jeffrey Pennington, Richard Socher, and Christopher D\nManning. GloVe: Global vectors for word representa-\ntion. In Proc. of the conference on empirical methods\nin natural language processing (EMNLP), pages 1532–\n1543, 2014.\n[23] Emanuele Pollastri and Giuliano Simoncelli. Classiﬁ-\ncation of melodies by composer with hidden markov\nmodels. In Proc. of the First International Conference\non WEB Delivering of Music, pages 88–95, 2001.\n[24] Alec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding by\ngenerative pre-training. OpenAI Blog, 2018.\n[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners. OpenAI Blog,\n1(8):9, 2019.\n[26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan\nKrause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C Berg, and Fei-Fei Li. ImageNet large\nscale visual recognition challenge. International jour-\nnal of computer vision, 115(3):211–252, 2015.\n[27] Pasha Sadeghian, Casey Wilson, Stephen Goeddel, and\nAspen Olmsted. Classiﬁcation of music by composer\nusing fuzzy min-max neural networks. In Proc. of the\n12th International Conference for Internet Technology\nand Secured Transactions (ICITST) , pages 189–192,\n2017.\n[28] Rico Sennrich, Barry Haddow, and Alexandra Birch.\nNeural machine translation of rare words with subword\nunits. In Proc. of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics , pages 1715–\n1725, 2016.\n[29] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556, 2014.\n[30] Leslie N Smith. A disciplined approach to neural net-\nwork hyper-parameters: Part 1–learning rate, batch\nsize, momentum, and weight decay. arXiv preprint\narXiv:1803.09820, 2018.\n[31] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither,\nA. Grimshaw, V . Hazlewood, S. Lathrop, D. Lifka,\nG. D. Peterson, R. Roskies, J. R. Scott, and N. Wilkins-\nDiehr. XSEDE: Accelerating scientiﬁc discovery.\nComputing in Science & Engineering , 16(5):62–74,\nSept.-Oct. 2014.\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nIn Advances in neural information processing systems,\npages 5998–6008, 2017.\n[33] Gissel Velarde, Carlos Cancino Chacón, David\nMeredith, Tillman Weyde, and Maarten Grachten.\nConvolution-based classiﬁcation of audio and sym-\nbolic representations of music. Journal of New Music\nResearch, 47(3):191–205, 2018.\n[34] Gissel Velarde, Tillman Weyde, Carlos Eduardo Can-\ncino Chacón, David Meredith, and Maarten Grachten.\nComposer recognition based on 2d-ﬁltered piano-rolls.\nIn Proc. of the International Society for Music Infor-\nmation Retrieval Conference (ISMIR), pages 115–121,\n2016.\n[35] Harsh Verma and John Thickstun. Convolutional com-\nposer classiﬁcation. In Proc. of the International So-\nciety for Music Information Retrieval Conference (IS-\nMIR), pages 549–556, 2019.\n[36] Jacek Wołkowicz and Vlado Kešelj. Evaluation of N-\ngram-based classiﬁcation approaches on classical mu-\nsic corpora. In International Conference on Mathemat-\nics and Computation in Music, pages 213–225, 2013.\n[37] Daniel Yang, Thitaree Tanprasert, Teerapat Jenrungrot,\nMengyi Shan, and TJ Tsai. MIDI passage retrieval us-\ning cell phone pictures of sheet music. In Proc. of the\nInternational Society for Music Information Retrieval\nConference (ISMIR), pages 916–923, 2019.\n[38] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. XL-\nNet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in neural informa-\ntion processing systems, pages 5754–5764, 2019.\n[39] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. How transferable are features in deep neural\nnetworks? In Advances in neural information process-\ning systems, pages 3320–3328, 2014.",
  "topic": "Piano",
  "concepts": [
    {
      "name": "Piano",
      "score": 0.8415374755859375
    },
    {
      "name": "Style (visual arts)",
      "score": 0.7452405095100403
    },
    {
      "name": "Linguistics",
      "score": 0.49698641896247864
    },
    {
      "name": "Speech recognition",
      "score": 0.44304168224334717
    },
    {
      "name": "Psychology",
      "score": 0.42749661207199097
    },
    {
      "name": "Sheet music",
      "score": 0.4242340326309204
    },
    {
      "name": "Art",
      "score": 0.38989630341529846
    },
    {
      "name": "Communication",
      "score": 0.35836559534072876
    },
    {
      "name": "Computer science",
      "score": 0.3400420546531677
    },
    {
      "name": "Visual arts",
      "score": 0.33525925874710083
    },
    {
      "name": "Art history",
      "score": 0.1417943835258484
    },
    {
      "name": "Philosophy",
      "score": 0.10461878776550293
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133543626",
      "name": "Harvey Mudd College",
      "country": "US"
    }
  ]
}