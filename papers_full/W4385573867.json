{
  "title": "Benchmarking Language Models for Code Syntax Understanding",
  "url": "https://openalex.org/W4385573867",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102638684",
      "name": "Da Shen",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A2556594351",
      "name": "Xinyun Chen",
      "affiliations": [
        "Google (United States)",
        "Brain (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2137691572",
      "name": "Chenguang Wang",
      "affiliations": [
        "Washington University in St. Louis"
      ]
    },
    {
      "id": "https://openalex.org/A2157836386",
      "name": "koushik sen",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2131252044",
      "name": "Dawn Song",
      "affiliations": [
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2799051177",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4205596491",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4286904762",
    "https://openalex.org/W589103562",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3105247453",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W3098044990",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2094061585",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2912206855",
    "https://openalex.org/W3201244947",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4296540990",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4281488715",
    "https://openalex.org/W2912126238",
    "https://openalex.org/W3093464584",
    "https://openalex.org/W4288375898",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pre-trained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce CodeSyntax, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that pre-training on massive code data does not result in decent code syntax understanding. In fact, these pre-trained programming language models fail to match the performance of naive baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of understanding corresponding syntactic structures. Our findings point out key limitations of existing pre-training methods and suggest the importance of modeling syntactic structures for the programming language.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3071–3093\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nBenchmarking Language Models for Code Syntax Understanding\nDa Shen1, Xinyun Chen2†, Chenguang Wang3†, Koushik Sen4, Dawn Song4\n1University of Maryland, College Park, 2Google Research, Brain Team\n3Washington University in St. Louis, 4University of California, Berkeley\ndashen@terpmail.umd.edu, xinyunchen@google.com, chenguangwang@wustl.edu,\n{ksen,dawnsong}@cs.berkeley.edu\nAbstract\nPre-trained language models have demon-\nstrated impressive performance in both natu-\nral language processing and program under-\nstanding, which represent the input as a to-\nken sequence without explicitly modeling its\nstructure. Some prior works show that pre-\ntrained language models can capture the syn-\ntactic rules of natural languages without ﬁne-\ntuning on syntax understanding tasks. How-\never, there is limited understanding of how\nwell pre-trained models understand the code\nstructure so far. In this work, we perform\nthe ﬁrst thorough benchmarking of the state-\nof-the-art pre-trained models for identifying\nthe syntactic structures of programs. Speciﬁ-\ncally, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syn-\ntactic relationships in their corresponding ab-\nstract syntax trees. Our key observation is\nthat existing language models pretrained on\ncode still lack the understanding of code syn-\ntax. In fact, these pre-trained programming\nlanguage models fail to match the performance\nof simple baselines based on positional offsets\nand keywords. We also present a natural lan-\nguage benchmark to highlight the differences\nbetween natural languages and programming\nlanguages in terms of syntactic structure un-\nderstanding. Our ﬁndings point out key limita-\ntions of existing pre-training methods for pro-\ngramming languages, and suggest the impor-\ntance of modeling code syntactic structures.1\n1 Introduction\nLarge-scale pre-training of language models has\nbecome the de-facto paradigm for a variety of natu-\nral language processing tasks. Furthermore, recent\nstudies show that models pre-trained on a massive\namount of code also achieve competitive perfor-\nmance on many tasks, e.g., code generation and\n†\nCorresponding authors.\n1Our code and dataset are available at https://github.\ncom/dashends/CodeSyntax.\nThere were many pioneer PC contributors.\nresult = object.function(argument)\nroot\nexpl nn\nnn\namod\nnsubj\nAssign Attribute Call\n(a)\n(b)\nFigure 1: Examples of syntactic relations for (a) natural\nlanguages (NL) and (b) programming languages (PL).\nEach relation is represented by an arrow. The relations\nin PL represent the syntax of code in a way similar to\nthose in NL.\nOffset BERT RoBERT aCodeBERT\nEnglish Syntax Understanding\n47.1\n64.6 62.7\n59.3\nOffset CuBERT CodeBERT RoBERT a\nPython Syntax Understanding\n43.6\n39.2\n33.1 34.5\nFigure 2: A preview of the model performance com-\nparison on NL and PL syntax understanding tasks. Pre-\ntrained models capture NL syntax relatively well, but\nperform worse in understanding PL syntax. The Offset\nbaseline picks the token using a ﬁxed positional offset.\nWe use BERT-large and RoBERTa-base conﬁgurations\n(corresponding to the conﬁgurations of CuBERT and\nCodeBERT). The plot shows top-1 scores. See Tables 3\nand 4 for the full results.\ncode classiﬁcation. These tasks are closely related\nto natural language (NL) tasks in their problem\nformulation. Nowadays, the common practice for\nsolving these coding tasks is to utilize the language\nmodel architectures and training schemes that are\noriginally designed for NL. The design principle of\nthese neural language models is signiﬁcantly differ-\nent from the classic rule-based program generation\n3071\nsystems. Speciﬁcally, neural language models take\nthe program as a token sequence, while classic\nprogram generation systems utilize the language\ngrammar and code structure. Despite the advanced\nperformance of pre-trained language models on\ncode understanding tasks, what these models have\nlearned from the code corpus remains unclear.\nIn this work, we investigate whether large-scale\npre-training is all we need for code representation\nlearning. In particular, we conduct the ﬁrst system-\natic study to analyze how the pre-trained language\nmodels understand the syntactic structures of pro-\ngrams. To this end, we introduce CodeSyntax, a\nlarge-scale benchmark consisting of programs an-\nnotated with the syntactic relationships between\ndifferent tokens. The ground truth syntactic rela-\ntionships are extracted from edges in the abstract\nsyntax trees (AST) of the programs. Figure 1\nshows some examples. These syntactic relations\nare function-wise similar to dependency relations\nfor NL, where prior work has demonstrated that\nthe attention heads of pre-trained language models\ncan help to identify NL relation types (Clark et al.,\n2019; Raganato et al., 2018). To measure how well\nthe pre-trained language models capture the code\nsyntactic structures, we adopt the approach to the\nPL domain. We focus on investigating the zero-\nshot capability of existing pre-training methods in\nour experiments, and we evaluate these pre-trained\nmodels without ﬁnetuning them on our benchmark.\nWe evaluate the state-of-the-art pre-trained lan-\nguage models for code representation learning, in-\ncluding CuBERT (Kanade et al., 2020) and Code-\nBERT (Feng et al., 2020). A common character-\nistic of these models is that they share the same\nTransformer-based architectural design as NL mod-\nels (Vaswani et al., 2017; Devlin et al., 2019). This\nallows us to directly compare their performance\nin capturing the syntax structure. We present a\npreview of our key results in Figure 2. Our main\nobservation is that pre-training is insufﬁcient for\nlearning the syntactic relations in code. First, we\nﬁnd that the models pre-trained on code do not al-\nways outperform models pre-trained on NL corpus\nalone. Surprisingly, compared to CodeBERT which\nis trained on both text and code corpora, RoBERTa\nachieves better performance without training on\nany code with identical model architecture. This\nindicates that pre-training on programs as token\nsequences does not help learn the syntactic rela-\ntions. On the contrary, without dependency rela-\ntions, pre-training still enables language models to\nunderstand the NL syntax to some extent.\nMoreover, for code syntax understanding, the\npre-trained models even perform worse than simple\nbaselines that pick the tokens with a ﬁxed offset.\nFor example, always selecting the (p+2)-th token as\nthe p-th token’s dependency yields higher accuracy\nthan any attention head for several relation types.\nOn the other hand, the same model architectures\npre-trained on text corpora achieve decent accuracy\nin identifying the dependency relations in the NL\ndomain, where the performance of the same simple\nbaselines is far behind.\nOur analysis reveals several key differences be-\ntween NL and PL that lead to different capabilities\nof understanding the syntax for pre-trained mod-\nels. First, programs are more structured than NL\nsentences. Programs usually contain hierarchical\nstructures representing long-term dependencies be-\ntween code tokens. Consequently, a large num-\nber of syntactic relation types are between distant\ntokens, which can be difﬁcult to recognize for at-\ntention heads. On the contrary, the dependency\nrelations in NL sentences mostly connect nearby\ntoken pairs, and in this case the attention heads are\nmore capable of identifying the correct relations.\nMeanwhile, language models are good at recog-\nnizing keyword-based relations, such as picking\nthe corresponding else keyword for an if token.\nInterestingly, we ﬁnd that the inclusion of tokens\nsuch as newlines and semicolons notably affects\nthe performance in the code domain.\nOur ﬁndings suggest that existing pre-trained\nmodels perform quite differently in PL and NL do-\nmains in terms of the ability to understand syntax.\nThus, directly applying training paradigms devel-\noped for NL could be suboptimal for program learn-\ning, and we consider designing better approaches\nto model the code structure as future work.\n2 CodeSyntax: Benchmarking Code\nSyntax Understanding\nWe construct the CodeSyntax benchmark to eval-\nuate the performance of language models on code\nsyntax understanding. We focus on Python and\nJava languages, on which the publicly released\nmodel checkpoints of both CuBERT (Kanade et al.,\n2020) and CodeBERT (Feng et al., 2020) are pre-\ntrained. We obtain the code samples from Code-\nSearchNet (Husain et al., 2019), which is a large-\nscale dataset consisting of code in different pro-\n3072\nRelation Count Explanation Code Example\nhead→dependent Python Java Python Java\nAssign:\ntarget→value\n78,482 13,384 Assigning a value to a target\nvariable.\ntarget= 10 inttarget= 10;\nCall:\nfunc→args\n110,949 50,890 Calling a function with some\narguments.\nfunction(arg) function(arg);\nFor:\nfor→body\n8,704 1,864 A for loop repeatedly executes\nthe body block for some itera-\ntions.\nfortarget in iter:\nbody\nfor(initializers;\ntest; updaters) {\nbody;\n}\nIf:\nif→else\n11,024 5,038 An if statement conditionally\nexecutes a body based upon\nsome criteria. The dependent\nis theelsekeyword.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else{\nbody2;\n}\nIf:\nif→body\n34,250 22,392 An if statement. The depen-\ndent is the body block.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nIf:\nbody→orelse\n11,024 4,976 An if statement. The head is\nthe body block and the depen-\ndent is the body of the else\nblock.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nWhile:\ntest→body\n743 975 The while loop repeatedly exe-\ncutes the body block as long as\nthe speciﬁed condition is true.\nwhilecondition:\nbody\nwhile (condition) {\nbody;\n}\nTable 1: Dataset statistics of selected relation types in CodeSyntax. For each relation type, we highlight the head\nand dependent nodes in the examples in bold, with the head in blue and the dependent in red. We defer the full\nstatistics of all relation types to Table 8 in the appendix.\ngramming languages. Its training set is also part\nof the pre-training data of CodeBERT, so we re-\nmove the data samples that are included in the\npre-training data of either CuBERT or CodeBERT.\nThus, none of the programs in CodeSyntax has\nbeen seen by CuBERT or CodeBERT in the pre-\ntraining phase.\nIn total, CodeSyntax contains 18,701 code sam-\nples annotated with 1,342,050 relation edges in\n43 relation types for Python, and 13,711 code\nsamples annotated with 864,411 relation edges\nin 39 relation types for Java. Each code sam-\nple is an entire function consisting of multiple\nstatements, which is analogous to a paragraph\nin NL. Each relation corresponds to an edge in\nthe program AST; speciﬁcally, we utilize the\nPython ast module (Foundation, 2021) and the Java\norg.eclipse.jdt.core.dom.ASTParser class (Contrib-\nutors, 2014) to parse a code sample into an AST.\nWe present some examples of relation types in Ta-\nble 1, and we defer the description of all relation\ntypes to Table 8 in the appendix. More details about\nrelation extraction are discussed in Appendix A.\nNote that we can easily extend the dataset to cover\nmore languages since the workﬂow for extracting\nrelations is automated and AST parsers are avail-\nable for most popular programming languages.\nWe observe several characteristics of relations\nin CodeSyntax. First, the keywords in PL play an\nimportant role in recognizing the code structure.\nSpeciﬁcally, some relation types have ﬁxed key-\nwords as the edge nodes, such as the If:if→else\nrelation. Meanwhile, compared to the dependency\nrelations in NL, the relation edges in the program\nAST tend to connect nodes that are much farther\naway from each other. As shown in Figure 3, the\naverage offset between head and dependent nodes\nis no more than 10 for dependency relations in NL,\nwhile the average offset for a relation type can be\nmore than 100 tokens in programs. Speciﬁcally, in\nCodeSyntax, there are 22 near dependency types\nwhose average offsets are less than 10, and 12 far\n3073\n0 20 40 60 80 100 120 140 160 180\noffset\n0\n5\n10\n15\n20Count\npython\njava\n(a) CodeSyntax.\n10\n 8\n 6\n 4\n 2\n 0 2 4 6 8 10\noffset\n0\n2\n4\n6\n8\n10Count\nEnglish\nGerman\n(b) Natural language corpus.\nFigure 3: Offset distribution of relation types in (a)\nCodeSyntax and (b) NL corpus. The x axis is the av-\nerage positional offset distance between heads and de-\npendents for each relation. The y axis is the number\nof relations that has the average offset value. See Sec-\ntion 3 for more details on the NL corpus.\ndependency types whose average offsets are above\n10.\n3 Evaluation Setup\nDo pre-trained language models capture the code\nstructure without direct supervision of the syntac-\ntic information? To investigate this question, we\nevaluate several pre-trained language models with-\nout ﬁnetuning, and compare their performance in\nunderstanding the syntax for NL and PL.\nNatural language benchmark. To compare the\nperformance on CodeSyntax to NL syntax under-\nstanding, we construct the NL benchmark that\nincludes English and German. Speciﬁcally, we\nuse the English News Text Treebank: Penn Tree-\nbank Revised (Bies et al., 2015) labeled with Stan-\nford Dependencies (de Marneffe and Manning,\n2008a,b), and German Hamburg Dependency Tree-\nbank (Foth et al., 2014) labeled with Universal De-\npendencies (de Marneffe et al., 2021). In total, the\nEnglish dataset has 48,883 sentences, 43 relation\ntypes, and 1,147,526 relation edges; the German\ndataset has 18,459 sentences, 35 relation types, and\n307,791 relation edges.\nAttention probing approach. Some prior\nworks demonstrate that a Transformer archi-\ntecture (Vaswani et al., 2017) pre-trained on a\ntext corpus, such as BERT (Devlin et al., 2019),\ncontains attention heads that specialize in certain\ndependency relations in NL (Raganato et al., 2018;\nClark et al., 2019). Speciﬁcally, in the Transformer\narchitecture, each vector ei for an input token\nis transformed into the query and key vectors qi\nand ki via some linear transformations, and the\ntransformations vary among different attention\nheads. For the i-th token, the attention weight\nassigned to the j-th token is\nαi,j = exp(qT\ni kj)∑\nl exp(qT\ni kl)\nThe attention weight indicates how important\nthe j-th token is with respect to the i-th token.\nTypically, different attention heads learn differ-\nent weights between input tokens. Therefore, to\nmeasure the correctness of recognizing a relation\ntype r, for each edge <h, t, r> in the program\nAST where h is the head node and t is the de-\npendent node, we enumerate all attention heads to\ncompute the attention weight αh,t. If an attention\nhead tends to assign high attention weights that\nconnect the pair of tokens belonging to the relation\ntype r, we consider the relation type to be captured.\nWe defer more implementation details of attention\nmap extraction to Appendix B.\nMetrics. We use the unlabeled attachment score\n(UAS) to measure the syntax understanding perfor-\nmance, and we consider top-k scores with different\nvalues of k. To compute top-k scores for language\nmodels, for each attention head, given the head to-\nken h in a relation edge <h, t, r>, we compute\nthe attention weight over all tokens in the input\ncode, and we consider the prediction to be correct\nif the attention weight over the dependent token\nt is among the top-k tokens with the highest at-\ntention weights. For each relation, we select the\nbest-performing attention head and use its score as\nthe model’s score for that relation. We calculate a\nmodel’s average score over all relations as the ﬁnal\nscore of the model.\nIn NL dependency parsing problems, the depen-\ndent node t usually corresponds to a single word.\nHowever, in PL, the dependent can be a block that\n3074\ncontains multiple code tokens. For example, in the\nIf:if→body relation, the head is the keyword if,\nwhile the dependent is the entire body block. There-\nfore, we measure three metrics. First-token metric\nand last-token metric: the prediction is deemed\ncorrect if it successfully predicts the ﬁrst and last\ntoken of the dependent block, respectively; Any-\ntoken metric: the prediction is considered correct\nif it can predict any token within the dependent\nblock. While we agree that these are not perfect\nmetrics and one single metric may be incomplete,\nwe observe that our ﬁndings generally hold for all\nthe three metrics we evaluated. Note that the ﬁrst-\ntoken metric is stricter than the any-token metric by\ndesign. Unless otherwise speciﬁed, we report the\ntop-k scores using the ﬁrst-token metric by default.\nModel architectures. Table 2 summarizes the\nmodels evaluated in this work. For language\nmodels over code, we consider CuBERT (Kanade\net al., 2020) and CodeBERT (Feng et al., 2020),\nand we evaluate their released pre-trained check-\npoints. Both of them are based on architectures\ninitially designed for NL. Speciﬁcally, CuBERT\nutilizes the BERT (Devlin et al., 2019) architec-\nture, and CodeBERT (Feng et al., 2020) utilizes\nthe RoBERTa (Liu et al., 2019) architecture. For\nNL models, we also evaluate multilingual variants\nof BERT and RoBERTa on the German dataset,\ni.e., Multilingual BERT (Pires et al., 2019) and\nXLM-RoBERTa (Conneau et al., 2020). Both of\nthe two code language models are cased, so we also\nevaluate the cased versions of the NL models.\nProgramming Languages Natural Languages\nCuBERT BERT\nMultilingual BERT\nCodeBERT RoBERTa\nXLM-RoBERTa\nTable 2: Model architectures evaluated on PL and NL\nbenchmarks. Models in the same row share the same\narchitecture, but are pre-trained on different corpora.\nBaselines. To examine how well the attention\nperforms through comparisons, we design a sim-\nple offset baseline and a simple keyword baseline.\nThe offset baseline with an offset value of ialways\nselects the token after ipositions of the input to-\nken as its prediction when i > 0, and selects i\npositions before the input token when i< 0. The\nkeyword baseline with a keyword of key always\npredicts the next keytoken as its prediction. In our\nexperiments, we evaluate offset baselines with each\npossible offset value between 0 and 512 for PL, and\n-512 to 512 for NL. We use all Python and Java key-\nwords for the keyword baselines on Python and\nJava datasets respectively, including tokens such\nas if, for, in, etc. To evaluate the top-k scores\nfor baselines where k ≥2, we combine k simple\nbaselines with different offset (keyword) values to\ngive k predictions. To select k offset (keyword)\nvalues, we repeatedly and greedily include the next\nvalue that yields the highest performance increase\nfor the relation type under consideration.\n4 Experiments\nIn this section, we present the results of pre-trained\nlanguage models for both PL and NL syntax un-\nderstanding tasks, and discuss the key observations\nthat distinguish PL from NL.\n4.1 Main Results\nLanguage Model Top-k Score\nk=1 k=3 k=10 k=20\nPython\nOffset 43.6 63.7 87.3 94.9\nKeyword 15.7 21.9 23.6 23.8\nCombined 49.4 69.7 90.1 96.3\nCuBERT 39.2 58.4 81.3 91.4\nCodeBERT 33.1 51.8 78.6 89.2\nRoBERTa 34.5 56.9 82.5 91.3\nDiff (Model - Baseline) -10.2 -11.3 -8.8 -4.9\nJava\nOffset 52.7 71.5 87.1 94.3\nKeyword 22.4 27.3 30.2 30.6\nCombined 60.4 77.2 90.0 96.1\nCuBERT 39.7 59.8 80.0 90.2\nCodeBERT 36.3 57.1 78.3 88.8\nRoBERTa 34.7 57.8 80.3 90.5\nDiff (Model - Baseline) -20.7 -17.4 -10.0 -5.9\nTable 3: Top-k scores for code syntax understanding.\nFor each language, the upper block contains the re-\nsults of baselines, including: (1) Offset: always picking\nthe token with a ﬁxed positional offset; (2) Keyword:\nmatching a ﬁxed keyword nearby; and (3) Combined:\ncombining the best option from Offset and Keyword.\nScore differences are calculated as the best attention\nscore - best baseline score for each language, where\na positive value indicates that the language model sur-\npasses the baseline.\nWe present our main results to compare the per-\nformance in syntactic relation understanding on PL\nand NL in Tables 3 and 4, respectively. First, on\nCodeSyntax, language models generally perform\nworse than simple offset baseline and its combi-\nnation with the keyword baseline, which indicates\n3075\nLanguage Model Top-k Score\nk=1 k=3 k=10 k=20\nEnglish\nOffset 47.1 72.7 91.0 96.6\nBERT-large 64.6 83.2 96.3 99.3\nRoBERTa-base 62.7 84.3 96.9 99.4\nCodeBERT 59.3 79.7 95.2 99.1\nDiff (Model - Baseline) 17.5 11.6 5.9 2.8\nGerman\nOffset 36.3 58.0 83.1 95.1\nMultilingual BERT 62.6 81.9 96.5 99.6\nXLM-RoBERTa-base67.4 85.5 97.1 99.7\nDiff (Model - Baseline) 31.1 27.5 14.0 4.6\nTable 4: Top-k scores for NL syntax understanding.\nNote that BERT-large and CuBERT share the same\nmodel conﬁguration, and CodeBERT and RoBERTa-\nbase have the same model architecture. Unlike Table 3,\nwe exclude Keyword and Combined baselines because\nthey do not add upon theOffset baseline in terms of the\nperformance.\nLanguage Model Top-k Score (Any-token Metric)\nk=1 k=3 k=10 k=20\nPython\nOffset 63.6 85.4 96.7 98.9\nKeyword 22.2 31.3 34.9 35.2\nCombined 66.8 88.4 98.2 99.6\nCuBERT 64.3 82.7 96.1 99.2\nCodeBERT 56.0 76.5 93.5 97.9\nRoBERTa 49.4 74.7 94.4 98.5\nDiff (Model - Baseline) -2.5 -5.7 -2.1 -0.4\nJava\nOffset 69.4 86.5 96.8 99.0\nKeyword 40.9 44.9 46.7 47.0\nCombined 75.7 90.0 98.2 99.6\nCuBERT 72.1 87.4 97.5 99.5\nCodeBERT 62.7 81.1 93.9 97.6\nRoBERTa 59.8 81.4 94.9 98.4\nDiff (Model - Baseline) -3.6 -2.6 -0.7 -0.1\nTable 5: Top-k scores for code syntax understanding\nusing the any-token metric.\nthat the attention heads of PL pre-trained models do\nnot effectively capture the syntactic relations in pro-\ngrams. The comparison between CodeBERT and\nRoBERTa further shows that pre-training on a large-\nscale code corpus, in addition to the text corpus for\nRoBERTa pre-training, does not yield a notably bet-\nter understanding of code syntax. In comparison,\nlanguage models substantially outperform offset\nbaselines in recognizing the dependency relations\nin NL, demonstrating that the attention heads learn\nto be specialized for different relation types via\nlarge-scale pre-training on text.\nMeanwhile, we present the any-token results on\nCodeSyntax in Table 5. Although the best com-\nbined baseline still outperforms language models,\nthe performance gap shrinks drastically. In par-\nticular, CuBERT achieves better scores than the\noffset baseline, and the improvement on Java is\nmore notable. We defer the full results of different\ntop-k scores on both PL and NL benchmarks to\nAppendix D. In the following sections, we discuss\nthe key factors that affect prediction performance.\n4.2 Case Studies: The Effect of Keywords\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.6\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) With semicolons (default).\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) Without semicolons.\nFigure 4: Top-k scores for Java syntax understanding\nusing the last-token metric.\nTo examine why the offset baseline outperforms\nCodeBERT and CuBERT, and why the relative per-\nformance differences get smaller when using the\nany-token metric, we conducted case studies and\nerror analysis in Section 4.2 and Section 4.3, which\nboth quantitatively and qualitatively categorize the\nerror patterns.\nFirstly, we investigate the most frequently at-\ntended code tokens, and we observe that the atten-\ntion heads tend to recognize the reserved tokens\nand keywords in PL. For example, CuBERT and\nCodeBERT get an improved score on Java because\nthe semicolon token is part of the ground truth de-\npendent node, which is a popular token attended\nto by language models. Based on this observation,\nwe perform an ablation study on the presence of\nthe semicolon in ground truth annotations. When\nthe semicolon tokens are removed from ground\n3076\ntruth dependent nodes, we also disable the lan-\nguage models to attend to semicolons in the in-\nput code. Since the semicolon appears at the end\nof each Java statement, here we compute the last-\ntoken score which may be signiﬁcantly affected by\nsemicolons. As shown in Figure 4, CuBERT sub-\nstantially outperforms baselines when semicolons\nare included in the ground truth labels. On the other\nhand, CuBERT reaches lower scores than baselines\nwhen semicolons are excluded from ground truth\nlabels and predictions. The comparison suggests\nthat attention heads are more capable of identifying\nfrequent keywords in the model input. We defer\nthe full ablation study on both Python and Java to\nAppendix F.\nWe further discuss the breakdown results with\nrespect to relation types, and we select some rep-\nresentative relations for Python that highlight the\nperformance differences between CuBERT and the\noffset baseline in Table 6. First, the attention is\nhighly capable of performing keyword matching,\nwhich leads to decent accuracy on relations that\nconnect popular keywords, such as If:if→else.\nHowever, when the head and dependent tokens\nare diverse, it becomes challenging for the lan-\nguage model to recognize the relation. For ex-\nample, in relation types Assign:target→value\nand Call:func→args, both head and dependent\nnodes can take various identiﬁer names deﬁned by\ndifferent programmers. In particular, CuBERT can\nnot effectively utilize the relative positions of to-\nkens to learn the relations, even if the dependent\nnode is near the head node. In such situations, the\noffset baseline with a ﬁxed offset value of 2 already\nsurpasses the pre-trained model. The full break-\ndown results of all relation types on both Python\nand Java can be found in Appendix G.\nRelation Score Offset DiffCuBERT Offset\nIf:if→else 92.7 5.7 17 87.1\nIf:body→orelse 29.2 7.1 12 22.0\nIf:if→body 31.5 23.1 7 8.4\nFor:for→body 30.4 32.7 7 -2.3\nAssign:target→value 39.8 71.2 2 -31.4\nWhile:test→body 16.2 48.5 4 -32.4\nCall:func→args 59.3 93.2 2 -33.9\nTable 6: The comparison of top-1 ﬁrst-token scores\nbetween CuBERT and the offset baseline with the\nbest ﬁxed offset for selected relation types on Python\ndataset.\n4.3 Error Analysis\nRelation Error Situation Count\nPython Java\nIf:\nif→else\nNested if statements or multiple\nif statements close to each other.\n34 42\nPredicts other keywords inside\nbody block, e.g.,ifandwhile.\n11 4\nOther. 5 4\nIf:\nbody→orelse\nPredicts another token with the\nsame name as head token itself.\n38 21\nPredicts keywords inside body\nblock, e.g.,if, ;andwhile.\n0 14\nPredicts a long string or doc-\nstring.\n7 7\nOther. 5 8\nIf:\nif→body\nPredicts blank space or tab. 32 0\nPredicts{or}. 0 27\nPredictsreturn. 0 19\nPredicts\\nor:. 15 0\nOther. 3 4\nFor:\nfor→body\nPredicts blank space or tab. 46 0\nPredicts{or}. 0 29\nOther. 4 21\nAssign:\ntarget→value\nPredicts a token that comes be-\nfore=, e.g.a[0]anda.b\n16 30\nPredicts=. 22 5\nOther. 12 15\nWhile:\ntest→body\nPredicts a token in the test block.48 36\nOther. 2 14\nCall:\nfunc→args\nPredicts(or). 45 37\nPredicts another token with the\nsame name as head token itself.\n0 10\nOther. 5 3\nTable 7: Error analysis using CuBERT.\nTo categorize the wrong predictions of the at-\ntention, we manually examine 50 error cases for\neach relation selected in Table 6, and present the\nerror situations in Table 7. Again, we observe that\nthe attention often incorrectly selects frequently\noccurring tokens such as brackets. Moreover, the\nmodel has difﬁculty capturing the hierarchical code\nstructure, thus it often attends to nearby keywords\nregardless of logical code blocks.\nTake the relation If:if→else as an example,\non which the language model generally achieves\nthe best performance. Shown in Figure 5 are two\nsample if-statements, where the ﬁrst one does not\ncontain nested ﬂow control blocks while the second\none contains a keyword while inside the if-body.\n\"...\" denotes that some code is omitted. Visualiz-\ning their corresponding attention weights of the\nattention head that performs the best on the relation\nIf:if→else, we observe that the attention head\n3077\n(a) Python code.\n'if' 'if'\n'e' 'e'\n'.' '.'\n'errno' 'errno'\n'==' '=='\n'errno' 'errno'\n'.' '.'\n'EEXIST' 'EEXIST'\n':' ':'\n'\\n' '\\n'\n'            ' '            '\n'pass' 'pass'\n'\\n' '\\n'\n'' ''\n'else' 'else'\n':' ':'\n'\\n' '\\n'\n'if' 'if'\n'(' '('\n'len' 'len'\n'(' '('\n'self' 'self'\n'.' '.'\n'stack' 'stack'\n')' ')'\n'>' '>'\n'0' '0'\n')' ')'\n':' ':'\n'\\n' '\\n'\n'            ' '            '\n'while' 'while'\n'...' '...'\n'else' 'else'\n':' ':'\n(b) Attention weights.\nFigure 5: Two sample cases for the relation\nIf:if→else and corresponding attention weights of\nCuBERT’s head 17-2.\ncorrectly attends to the else token in the ﬁrst ex-\nample, while it wrongly attends to the while token\ninside the if-body in the second example. More\nexamples like these can be found in Appendix E.\n5 Related Work\nTransformer-based language models have been\nwidely used for natural language processing (De-\nvlin et al., 2019; Liu et al., 2019; Wang et al., 2020,\n2021; Shen et al., 2022; Wang et al., 2022). Hewitt\nand Manning (2019) show that syntax trees are im-\nplicitly embedded in BERT’s word representation\nspace via a structural probe. Another line of work\nstudies what is learned by the attention in language\nmodels (Clark et al., 2019; Raganato et al., 2018;\nV oita et al., 2019; Michel et al., 2019; Vig, 2019;\nBurns et al., 2018; Marecek and Rosa, 2018; V oita\net al., 2018). In particular, Clark et al. (2019) eval-\nuate the attention heads of BERT on dependency\nparsing tasks using the English Penn Treebank cor-\npus, where the attention signiﬁcantly outperforms\noffset baselines. On the contrary, we demonstrate\nthat attention-based models largely perform worse\nthan offset baselines on code syntax understanding.\nThe success of Transformer-based models for\nnatural language processing leads to their applica-\ntion in the PL domain (Kanade et al., 2020; Feng\net al., 2020; Rozière et al., 2020, 2021; Clement\net al., 2020; Dehghani et al., 2019). Chen et al.\n(2021) evaluate the model performance by mea-\nsuring the functional correctness on unit tests.\nChirkova and Troshin (2021) empirically shows\nthat Transformers can utilize syntactic information\nto make predictions in some code processing tasks,\nwhile we analyze attention’s ability to understand\nsyntactic relations. Karmakar and Robbes (2021)\nprobe pre-trained models on four code understand-\ning tasks. They focus more on code classiﬁcation,\ne.g., they train a classiﬁer for predicting the AST\nnode tag and the code length. On the contrary,\nwe probe the attention heads for syntactic relation\nunderstanding, and we aim to present a comprehen-\nsive study of the differences between pre-trained\nlanguage models on NL and PL for capturing the\nsyntax structures.\nThere have been some efforts that try to take\ncode structure into account during pre-training\nof Transformer-based models for code. For ex-\nample, GraphCodeBERT (Guo et al., 2021) uti-\nlizes data ﬂow for pretraining; i.e., the relation of\n\"where-the-value-comes-from\" for variables. On\nour Python benchmark, GraphCodeBERT achieves\na top-1 ﬁrst-token score of 39.3, which is better\nthan 33.1 of CodeBERT, and comparable to 39.2 of\nCuBERT. However, such a score is still worse than\n43.6 of the offset baseline. This trend is consistent\nwhen evaluating with other metrics. These results\nshow that pre-training on data ﬂow helps improve\nthe model’s ability to understand code syntax, but\nthere is still large room for improvement.\n6 Conclusion\nIn this work, we introduce CodeSyntax, a large-\nscale benchmark for measuring the performance of\ncode syntax understanding. Based on CodeSyntax,\nwe conduct the ﬁrst comprehensive study to ana-\nlyze the capability of pre-trained language models\non understanding the code syntactic structures with-\nout further ﬁnetuning. We demonstrate that while\nthe attention heads of pre-trained language models\nare able to identify dependency relations in NL to\nsome extent, they have difﬁculty recognizing the\nsyntactic relations in programs. Pre-trained models\neven generally perform worse than simple offset\nbaselines, and they tend to attend to frequently\noccurring nearby tokens without taking the hierar-\nchical code structure into consideration.\nWe also analyze the differences between NL and\n3078\nPL from the perspectives of pre-trained models.\nOur evaluation suggests that PL has unique char-\nacteristics that distinguish them from NL, such as\nthe long-term dependency between code tokens,\nand the hierarchy in the syntactic structures. There-\nfore, simply taking a program as a token sequence\nis insufﬁcient for modeling the program structure,\nwhich could eventually limit the potential of lan-\nguage models for code understanding tasks. We\nconsider developing new model architectures and\npre-training algorithms to leverage and represent\nthe code structure and dependency graph as impor-\ntant future work.\n7 Limitations\nFor the limitations of our benchmark, the gold an-\nnotations are based on the AST parsers. Adding\nnew programming languages whose parsers are un-\navailable will require additional labeling efforts. A\nlimitation in our experimental setup is that we have\nonly benchmarked six models across two kinds\nof natural languages and programming languages.\nFinally, the main focus of our study is to probe\nthe language models for code understanding. As\na result, we have not proposed models that could\ndeal with the code syntax in natural language and\nprogramming language applications. Future work\ncould include developing such models that capture\nboth semantics and structures.\n8 Ethical Considerations\nWe hereby acknowledge that all of the co-authors of\nthis work are aware of the provided ACM Code of\nEthics and honor the code of conduct. The follow-\nings give the aspects of both our ethical considera-\ntions and our potential impacts to the community.\nThis work creates a benchmark to test the code syn-\ntax understanding of pre-trained language models.\nInstead of natural language, the programming lan-\nguage is used for pre-training. We do not anticipate\nthe production of harmful outputs after using our\nbenchmark and existing models, especially towards\nvulnerable populations.\n9 Environmental Considerations\nWe use several pre-trained language models. Ac-\ncording to the estimation in (Strubell et al., 2019),\npre-training a model with a similar size as used in\nthe work costs 1,507 kWh·PUE and emits 1,438 lb\nCO2. This work focuses on inference. Therefore,\nour energy cost and CO2 emissions are relatively\nsmall.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their suggestions and comments. This material\nis in part based upon work supported by Berke-\nley DeepDrive and Berkeley Artiﬁcial Intelligence\nResearch.\nReferences\nAnn Bies, Justin Mott, and Colin Warner. 2015. En-\nglish news text treebank: Penn treebank revised.\nPhiladelphia: Linguistic Data Consortium.\nKaylee Burns, Aida Nematzadeh, Erin Grant, Alison\nGopnik, and Thomas L. Grifﬁths. 2018. Exploit-\ning attention to reveal shortcomings in memory mod-\nels. In Proceedings of the Workshop: Analyzing\nand Interpreting Neural Networks for NLP , Black-\nboxNLP@EMNLP 2018, Brussels, Belgium, Novem-\nber 1, 2018, pages 378–380. Association for Com-\nputational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott\nGray, Nick Ryder, Mikhail Pavlov, Alethea Power,\nLukasz Kaiser, Mohammad Bavarian, Clemens Win-\nter, Philippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating large language models trained on code. CoRR,\nabs/2107.03374.\nNadezhda Chirkova and Sergey Troshin. 2021. Em-\npirical study of transformers for source code. In\nProceedings of the 29th ACM Joint Meeting on Eu-\nropean Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineer-\ning, ESEC/FSE 2021, page 703–715, New York, NY ,\nUSA. Association for Computing Machinery.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP. The\nAssociation for Computational Linguistics.\n3079\nColin B. Clement, Dawn Drain, Jonathan Timcheck,\nAlexey Svyatkovskiy, and Neel Sundaresan. 2020.\nPymt5: multi-mode translation of natural language\nand python code with transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020, pages 9052–9065. As-\nsociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 8440–8451. Associa-\ntion for Computational Linguistics.\nEclipse Contributors. 2014. Rational software archi-\ntect realtime edition 9.5.0. https://www.ibm.com/\ndocs/en/rsar/9.5?topic=SS5JSH_9.5.0/org.\neclipse.jdt.doc.isv/reference/api/org/\neclipse/jdt/core/dom/package-use.html.\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008a. Stanford dependencies manual.\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008b. The stanford typed dependencies rep-\nresentation. In Proceedings of the workshop on\nCross-Framework and Cross-Domain Parser Evalu-\nation@COLING 2008, Manchester, UK, August 23,\n2008, pages 1–8. Coling 2008 Organizing Commit-\ntee.\nMarie-Catherine de Marneffe, Christopher D. Manning,\nJoakim Nivre, and Daniel Zeman. 2021. Universal\ndependencies. Comput. Linguistics, 47(2):255–308.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, Online Event, 16-\n20 November 2020.\nKilian A. Foth, Arne Köhn, Niels Beuck, and Wolf-\ngang Menzel. 2014. Because size does matter: The\nhamburg dependency treebank. In Proceedings of\nthe Ninth International Conference on Language Re-\nsources and Evaluation, LREC 2014, Reykjavik, Ice-\nland, May 26-31, 2014, pages 2326–2333. European\nLanguage Resources Association (ELRA).\nPython Software Foundation. 2021. Python 3.10.0 doc-\numentation, ast — abstract syntax trees. https:\n//docs.python.org/3/library/ast.html.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng,\nDuyu Tang, Shujie Liu, Long Zhou, Nan Duan,\nAlexey Svyatkovskiy, Shengyu Fu, Michele Tu-\nfano, Shao Kun Deng, Colin B. Clement, Dawn\nDrain, Neel Sundaresan, Jian Yin, Daxin Jiang, and\nMing Zhou. 2021. Graphcodebert: Pre-training\ncode representations with data ﬂow. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021.\nOpenReview.net.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4129–4138. Association for Computa-\ntional Linguistics.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2020. Learning and evaluating con-\ntextual embedding of source code. In International\nConference on Machine Learning, pages 5110–5121.\nPMLR.\nAnjan Karmakar and Romain Robbes. 2021. What\ndo pre-trained code models know about code? In\n36th IEEE/ACM International Conference on Auto-\nmated Software Engineering, ASE 2021, Melbourne,\nAustralia, November 15-19, 2021, pages 1332–1336.\nIEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nDavid Marecek and Rudolf Rosa. 2018. Extract-\ning syntactic trees from transformer encoder self-\nattentions. In Proceedings of the Workshop: An-\nalyzing and Interpreting Neural Networks for NLP ,\nBlackboxNLP@EMNLP 2018, Brussels, Belgium,\nNovember 1, 2018, pages 347–349. Association for\nComputational Linguistics.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems\n3080\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 14014–\n14024.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4996–5001. Association for Computa-\ntional Linguistics.\nAlessandro Raganato, Jörg Tiedemann, et al. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP. The As-\nsociation for Computational Linguistics.\nBaptiste Rozière, Marie-Anne Lachaux, Lowik\nChanussot, and Guillaume Lample. 2020. Unsu-\npervised translation of programming languages. In\nAdvances in Neural Information Processing Systems\n33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nBaptiste Rozière, Jie M. Zhang, François Char-\nton, Mark Harman, Gabriel Synnaeve, and Guil-\nlaume Lample. 2021. Leveraging automated unit\ntests for unsupervised code translation. CoRR,\nabs/2110.06773.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nJianhao Shen, Chenguang Wang, Linyuan Gong, and\nDawn Song. 2022. Joint language semantic and\nstructure embedding for knowledge graph comple-\ntion. In COLING.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In ACL, pages 3645–3650.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models. CoRR,\nabs/1904.02679.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Pa-\npers, pages 1264–1274. Association for Computa-\ntional Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5797–5808. Association for Computational Linguis-\ntics.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2021. Zero-shot informa-\ntion extraction as a uniﬁed text-to-triple translation.\nIn EMNLP.\nChenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong,\nJie Tang, and Dawn Song. 2022. DeepStruct: Pre-\ntraining of language models for structure prediction.\nIn ACL.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs.\narXiv preprint arXiv:2010.11967.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nA More Details on CodeSyntax\nConstruction\nSince the code search net dataset does not come\nwith syntactic relation labels, we come up with\na way of extracting syntactic relations. We ﬁrst\nutilize python’s tokenize module and javalang mod-\nule to produce code tokens from source code, and\nthen label these code tokens with syntactic relations\nby using AST parsers on source code. We utilize\nPython ast module (Foundation, 2021) and Java\norg.eclipse.jdt.core.dom.ASTParser class (Contrib-\nutors, 2014) to parse source code into ast nodes.\nThe AST structure captures syntactical relations.\nAn AST node has children AST nodes and a name\nthat denotes its class. We use the class of the\nnode as label and children nodes as dependents\nand heads when generating annotations. For exam-\nple, the source code A = B, which means assign-\ning value B to target variable A, is parsed into the\n3081\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Python (First Token Metric)\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Average Score\nCuBERT\nCodeBERT\nCombined\nOffset (b) Java (First Token Metric)\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.6\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Python (Any Token Metric)\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset (d) Java (Any Token Metric)\nFigure 6: PL Top-k Scores On Test Set\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Average Score\nBERT-base\nBERT-large\nRoBERTa-base\nRoBERTa-large\nOffset\nFigure 7: English Top-k Scores\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.6\n0.8\n1.0Average Score\nMultilingual BERT\nXLM-RoBERTa-base\nXLM-RoBERTa-large\nOffset\nFigure 8: German Top-k Scores\nAST node Assign(targets=[Name(id=’A’)],\nvalue=Name(id=’B’)). It gives us a syntactic rela-\ntion whose head is A and dependent is B, annotated\nwith the relation type label Assign. Full statistics\nof CodeSyntax are displayed in Table 8.\nB More Details on Attention Map\nExtraction for Code Language Models\nOur experiments follow the work of Clark et al.\n(2019). They evaluate the attention heads of BERT\non dependency parsing tasks on an English dataset,\nwhile we extend the work to the PL domain. We\nadopt and extend some of their code, such as the\nfunctions for extracting attention from BERT and\nplotting attention weights. The main differences\nbetween our work and theirs are that we construct\na novel dataset for syntax understanding tasks for\nPL and come up with related evaluation metrics to\naccommodate the characteristics of PL.\nB.1 Model Input\nEach of our code samples is an entire Python or\nJava function. To prepare the input to be fed to the\nmodels, we run CuBERT and CodeBERT tokeniza-\n3082\ntion to obtain sequences of input ids for each code\nsample. We insert a [CLS] token at the beginning\nand append a [SEP] token at the end. If the input\nlength is longer than 512 tokens (the maximum\nnumber of tokens allowed), we discard that code\nsample. We never split a long code sample into\nseveral input sentences because the span of some\ndependency relations is very long within a function.\nFor example, for an if statement, the else block\nmay be far away from the keyword if. If we split\nthem into two input sentences, then attention will\nnot be able to understand and predict the relation\nbetween them. To avoid uncommon data points,\nwe remove a code sample from both CuBERT and\nCodeBERT’s input if it is longer than 512 tokens\nafter either one of CuBERT or CodeBERT’s tok-\nenization.\nB.2 Token Alignment And Word-level\nAttention\nBERT uses WordPiece tokenization (Wu et al.,\n2016) and RoBERTa uses byte-level Byte-Pair En-\ncoding (BPE) (Sennrich et al., 2016), which may\nsplit a word into several subtokens. Additionally,\nCuBERT imposes some special rules when produc-\ning program vocabulary. However, our dataset’s\nlabels use code tokens generated by the tokenize\nmodule and the javalang module. Therefore, there\nexists a need to align CuBERT/CodeBERT subto-\nkens with code tokens in order to evaluate the mod-\nels on our dataset. We ﬁrst generate such an align-\nment that maps each code token to a set of Cu-\nBERT/CodeBERT subtokens, and then convert the\noriginal subtoken-level attention to word-level at-\ntention. We follow (Clark et al., 2019) to combine\nthe attention weights of subtokens, i.e., we sum up\ntheir attention weights.\nC More Reproducibility Information\nHere we provide more information according to the\nEMNLP 2022 Reproducibility Criteria.\n• Train/validation/test splits for datasets used:\nWe do not ﬁnetune the pre-trained models\non our benchmark. The validation set of\nCodeSyntax contains the code samples that\ncome from the validation set of CodeSearch-\nNet, and our test set contains the samples from\nCodeSearchNet’s test set. We use our test par-\ntition to probe the pre-trained attention heads\nwhile the validation set is not used.\n• Number of parameters in each model: Cu-\nbert and BERT-large have 340M parameters.\nCodeBert and RoBERTa-base have 125M pa-\nrameters. XLM-RoBERTa-base has 250M pa-\nrameters. Multilingual BERT-base has 110M\nparameters.\n• The average runtime for each model or algo-\nrithm: Running the pipeline to construct the\nCodeSyntax dataset takes about four hours\nassuming that dependencies and required\ndatasets have been downloaded. The algo-\nrithm to probe a pre-trained model on one\nprogramming language of CodeSyntax takes\nabout twelve hours on our machine using one\nNvidia 1080Ti GPU.\nD More Results on Top-k Scores\nPL top-k scores are plotted in ﬁgure 6. NL scores\nare plotted in ﬁgure 7 (English) and ﬁgure 8 (Ger-\nman).\nE Examples of Correct and Incorrect\nPredictions\nIn this section, we present some visualization ex-\namples where attention correctly or incorrectly pre-\ndicts the dependents. The heads chosen in these ex-\namples are the best-performing heads of CuBERT\nevaluated using the ﬁrst-token metric. We feed the\nentire function as input to the transformer, however,\nwe only present relevant snippets here for simplic-\nity. In the source code displayed, \"...\" denotes that\nthe remaining part of the code is omitted. As a re-\nsult, the attention from a token may not sum up to\none in these ﬁgures because the rest of the function\nis omitted.\nRelation Call: func →args. The correspond-\ning attention weights are visualized in Table 9 for\nPython and 10 for Java.\n• Python correct case.\nn = len(x)\nn_fft = len(win_sq)\nAttention correctly predicts the arguments x\nand win_sq, respectively.\n• Python error case.\nre.findall(pattern,text)\nThe function findall is called. The correct\nprediction should be the ﬁrst argument, which\n3083\nis pattern; however, attention incorrectly\npredicts the parenthesis (.\n• Java correct case.\nsubscriber.onError(ex);\nThe token ex has the largest weight in atten-\ntion, which is a correct prediction.\n• Java error case.\nisBug(error)\nThe function isBug is called. The correct pre-\ndiction should be the argument, error; how-\never, attention incorrectly predicts ).\n'n' 'n'\n'=' '='\n'len' 'len'\n'(' '('\n'x' 'x'\n')' ')'\n'\\n' '\\n'\n'n_fft' 'n_fft'\n'=' '='\n'len' 'len'\n'(' '('\n'win_sq' 'win_sq'\n')' ')'\n're' 're'\n'.' '.'\n'findall' 'findall'\n'(' '('\n'pattern' 'pattern'\n',' ','\n'text' 'text'\n')' ')'\nFigure 9: Python Head 15-11 Call: func →args.\n'subscriber' 'subscriber'\n'.' '.'\n'onError' 'onError'\n'(' '('\n'ex' 'ex'\n')' ')'\n';' ';'\n'isBug' 'isBug'\n'(' '('\n'error' 'error'\n')' ')'\nFigure 10: Java Head 19-9 Call: func →args\nRelation Assign: target → value. The cor-\nresponding attention weights are visualized in Ta-\nble 11 for Python and 12 for Java.\n• Python correct case.\nvalue = round(value,\nprecision)\nThe assigned value round is correctly pre-\ndicted.\n• Python error case.\nd[\"_text\"] = r.text\nThe value assigned is r.text, but attention\nincorrectly predicts [.\n• Java correct case.\nint p = parallelism();\nAttention has the largest weight for the head\ntoken parallelism, which correctly predicts\nthe relation.\n• Java error case.\nthis.defaultProcessor\n= processor;\nThe value assigned is processor, but atten-\ntion incorrectly predicts ;.\n'value' 'value'\n'=' '='\n'round' 'round'\n'(' '('\n'value' 'value'\n',' ','\n'precision' 'precision'\n')' ')'\n'\\n' '\\n'\n'd' 'd'\n'[' '['\n'\"_text\"' '\"_text\"'\n']' ']'\n'=' '='\n'r' 'r'\n'.' '.'\n'text' 'text'\nFigure 11: Python Head 15-10 Assign: target →value\n'int' 'int'\n'p' 'p'\n'=' '='\n'parallelism' 'parallelism'\n'(' '('\n')' ')'\n';' ';'\n'this' 'this'\n'.' '.'\n'defaultProcessor' 'defaultProcessor'\n'=' '='\n'processor' 'processor'\n';' ';'\nFigure 12: Java Head 20-10 Assign: target →value\nRelation If: if → else. The corresponding\nattention weights are visualized in Table 13 for\nJava.\n• Java correct case.\nif (t instanceof Error) {\nthrow (Error) t;\n} else {\n...\nIt correctly identiﬁes the keyword else.\n• Java error case.\nif(error.addThrowable(ex)) {\nif ...\n} else {\n...\nThere is another if statement inside the body\nof the ﬁrst if statement. The correct prediction\nshould be keyword else, but it predicts the\ninner if.\nRelation For: for →body. The corresponding\nattention weights are visualized in Table 14 for\nPython and 15 for Java.\n• Python correct case.\n3084\n'if' 'if'\n'(' '('\n't' 't'\n'instanceof' 'instanceof'\n'Error' 'Error'\n')' ')'\n'{' '{'\n'\\n' '\\n'\n'throw' 'throw'\n'(' '('\n'Error' 'Error'\n')' ')'\n't' 't'\n';' ';'\n'\\n' '\\n'\n'}' '}'\n'else' 'else'\n'{' '{'\n'if' 'if'\n'(' '('\n'error' 'error'\n'.' '.'\n'addThrowable' 'addThrowable'\n'(' '('\n'ex' 'ex'\n')' ')'\n')' ')'\n'{' '{'\n'\\n' '\\n'\n'if' 'if'\n'...' '...'\n'\\n' '\\n'\n'}' '}'\n'else' 'else'\nFigure 13: Java Head 9-10 If: if →else\nfor el in predictions:\nif 0 in el:\n...\n• Python error case.\nfor pass_ in\nself.working_list:\nret.append(...\nThe correct prediction should be the ﬁrst to-\nken within the body, which is ret; however,\nattention incorrectly predicts the blank space\n\" \" before ret.\n• Java correct case.\nfor(BehaviorSubscription<T>\ns : array) {\nif (...\n• Java error case.\nfor (;;) {\nCacheSubscription ...\nThe correct prediction should be the\nﬁrst token within the body, which is\nCacheSubscription; however, attention\nincorrectly predicts {.\n'for' 'for'\n'el' 'el'\n'in' 'in'\n'predictions' 'predictions'\n':' ':'\n'\\n' '\\n'\n'        ' '        '\n'if' 'if'\n'0' '0'\n'in' 'in'\n'el' 'el'\n':' ':'\n'\\n' '\\n'\n'for' 'for'\n'pass_' 'pass_'\n'in' 'in'\n'self' 'self'\n'.' '.'\n'working_list' 'working_list'\n':' ':'\n'\\n' '\\n'\n'            ' '            '\n'ret' 'ret'\n'.' '.'\n'append' 'append'\n'(' '('\nFigure 14: Python Head 18-4 For: for →body\n'for' 'for'\n'(' '('\n'BehaviorSubscription' 'BehaviorSubscription'\n'<' '<'\n'T' 'T'\n'>' '>'\n's' 's'\n':' ':'\n'array' 'array'\n')' ')'\n'{' '{'\n'\\n' '\\n'\n'if' 'if'\n'(' '('\n'for' 'for'\n'(' '('\n';' ';'\n';' ';'\n')' ')'\n'{' '{'\n'\\n' '\\n'\n'CacheSubscription' 'CacheSubscription'\nFigure 15: Java Head 16-5 For: for →body\nF More Results on the Ablation Study of\nDelimiter Tokens\nThe ablation study on java dataset is shown in Fig-\nure 17 (any-token metric) and Figure 16 (last-token\nmetric). Results with ﬁrst-token metric are not af-\nfected at all because semicolons and newlines are\nnever used as the ﬁrst token of dependents. We\nfound that attention performs very well with last-\ntoken metric because it can ﬁnd semicolons and\nnewlines.\nThe ablation study on python dataset is shown\nin Figure 18 (any-token metric) and Figure 19 (last-\ntoken metric).\nG More Breakdown Results on Different\nRelation Types\nMore results on comparisons of top-1 scores be-\ntween CuBERT and the offset baseline are pre-\nsented in Tables 9, 10, 11, and 12.\n3085\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.6\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.4\n0.6\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Without Semicolons\nFigure 16: Top-k scores for Java syntax understanding\nusing the last-token metric.\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(c) Without Semicolons\nFigure 17: Ablation Study (Java Any-Token Metric).\n3086\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.6\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.6\n0.7\n0.8\n0.9\n1.0Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\nFigure 18: Ablation Study (Python Any-Token Metric).\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(a) Original\n1 3 5 7 9 11 13 15 17 19 21\nk\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Average Score\nCuBERT\nCodeBERT\nCombined\nOffset\n(b) With Newline\nFigure 19: Ablation Study (Python Last-Token Metric).\n3087\nRelation:head→dependent Count Explanation Code Example\nPython Java Python Java\nAssign:target→value 78482 13384 Assigning a value to a target\nvariable.\ntarget= 10 int target= 10;\nAttribute:value→attr 158797 84215\nAccessing the attribute (member\nﬁeld or member function) of an\nvalue.\nvalue.attribute line .setLength(2);\nAugAssign:target→value 3150 /\nAn assignment augmented\nwith an operation. For Java,\nthis case is included in\nAssign:target→value.\nx += 2 /\nBinOp:left→right 26035 / A binary operation. a + b /\nBoolOp:value→value 5783 / A boolean operation Trueor False /\nCall:args→keywords 9256 / Calling a function with some\narguments (and keywords). function(arg, key=1) function(arg);Call:func→args 110949 50890\nCall:func→keywords 16274 /\nCompare:left→comparator 25852 / A comparison between values. a < b /\nDict:key→value 7787 / Initializing a dictionary. { count: 10 } /\nDictComp:key→generator 359 /\nDictComp:key→value 359 / Dictionary comprehension. { i: 2*i for i in list} /\nDictComp:value→generator 359 /\nDo:body→test / 38 The do loop repeatedly executes\nthe body block as long as the\ncondition is true.\ndo\nstatement;\nwhile (condition);\nDo:do→body / 45 /\nDo:do→test / 38\nFor:for→body 8704 1864\nA for loop repeatedly executes\nthe body block for some\niterations.\nfor targetin iter:\nbody\nfor (initializers;\ntest; updaters) {\nbody;\n}\nFor:for→initializers / 1650\nFor:for→iter 8704 /\nFor:for→target 8704 /\nFor:for→test / 1296\nFor:for→updaters / 1682\nFor:initializers→body / 1781\nFor:initializers→test / 1286\nFor:initializers→updaters / 1670\nFor:iter→body 8704 /\nFor:target→body 8704 /\nFor:target→iter 8704 /\nFor:test→body / 1789\nFor:test→updaters / 1678\nFor:updaters→body / 1685\nGeneratorExp:elt→generator 685 / A generator expression. ( 2*i for i in list) /\nIf:body→orelse 11024 4976\nAn if statement conditionally\nexecutes a body based upon\nsome criteria.\nif condition:\nbody1\nelse:\nbody2\nif (condition) {\nbody1;\n} else {\nbody2;\n}\nIf:if→body 34250 22392\nIf:if→else 11024 5038\nIf:if→test 34250 19323\nIf:test→body 34250 22392\nIf:test→orelse 11024 5007\nIfExp:body→orelse 1262 1173\nAn if expression (conditional\nexpression). x if conditionelse y (condition) ? x : yIfExp:body→test 1262 /\nIfExp:test→body / 1218\nIfExp:test→orelse 1262 1173\nInﬁxExpr:left→right / 35170 Inﬁx expression of the form\nleftOperand InﬁxOperator right-\nOperand.\n/ a + b\nInstanceofExpr:expr→type / 1367 Checking whether an expression\nis some type.\n/ inputinstanceofString\nLabeledStatement:label→body / 10 A statement labeled with an\nidentiﬁer.\n/ Identiﬁer: Statement\nListComp:elt→generator 2691 / List comprehension. [ x for x in list1] /\nSetComp:elt→generator 67 / Set comprehension. { x for x in list1} /\nSlice:lower→upper 731 / A slice used in subscript of lists.A[2:6] /\nSubscript:value→slice 39271 4555 Accessing parts of an array or\ndata structure through subscript.\nA[2:6] A[0]\nContinued on next page.\n3088\nContinued from previous page.\nRelation:head→dependent Count Explanation Code Example\nPython Java Python Java\nSwitch:expr→statement / 385 A switch statement chooses a\nbranch to execute based upon\nconditions.\nswitch(inputExpr) {\nStatement;\n}\nSwitch:switch→expr / 320 /\nSwitch:switch→statement / 385\nTry:body→ﬁnalbody 135 474\nA try statement for handling\nexceptions.\ntry:\nbody1\nexceptException:\nbody2\nelse:\nbody3\nﬁnally:\nbody4\ntry {\nbody1;\n} catch (Exception e){\nbody2;\n} ﬁnally {\nbody3;\n}\nTry:body→handler 3020 2011\nTry:body→orelse 181 /\nTry:handler→ﬁnalbody 48 186\nTry:handler→orelse 181 /\nWhile:test→body 743 975 The while loop repeatedly\nexecutes the body block as long\nas the condition is true.\nwhile condition:\nbody\nwhile(condition) {\nbody;\n}\nWhile:while→body 743 975\nWhile:while→test 743 416\nWith:item→body 1239 / A with statement with built-in\ncontext manager.\nwith open(\"ﬁle\") as f:\ncontent = f.read()\n/\nchildren:parent→child 652417 569499 Any pair of AST nodes that are\nparent and child in the parse\ntree.\n/ /\ncomprehension:target→iter 3881 / A for clause to iterate over some\nsequences.\n[x for x in list1] /\nTable 8: Full dataset statistics table of CodeSyntax. For each relation type, we highlight the head and dependent\nnodes in the examples in bold, with the head in blue and the dependent in red. If a node can be either head or\ndependent in different relations, we color it in green. For more explanation about the syntax, please refer to the\ndocumentation of Python ast module (Foundation, 2021) and Java org.eclipse.jdt.core.dom.ASTParser (Contribu-\ntors, 2014)\n3089\nRelation Score Offset DifferenceCuBERT Offset\nIf:if→else 92.7 5.7 17 87.1\nIfExp:body→orelse 46.4 13.6 6 32.8\nTry:body→handler 39.1 7.9 8 31.3\nIf:body→orelse 29.2 7.1 12 22.0\nBoolOp:value→value 33.3 22.4 2 10.9\nTry:body→ﬁnalbody 20.5 10.3 7 10.3\nIf:if→body 31.5 23.1 7 8.4\nCall:func→keywords 38.0 34.1 2 4.0\nIf:test→orelse 7.5 5.1 18 2.4\nCompare:left→comparator 46.5 45.7 2 0.8\nIf:if→test 98.8 98.8 1 -0.0\nFor:for→target 99.4 99.4 1 0.0\nWhile:while→test 99.3 99.3 1 0.0\nTry:body→orelse 10.5 10.5 33 0.0\nFor:for→body 30.4 32.7 7 -2.3\nTry:handler→orelse 13.2 15.8 16 -2.6\nIfExp:test→orelse 23.2 26.7 2 -3.5\nchildren:parent→child 26.3 30.7 2 -4.4\nAttribute:value→attr 76.7 82.3 2 -5.6\nFor:target→body 26.4 32.7 6 -6.3\nIf:test→body 16.3 23.2 6 -6.9\nSubscript:value→slice 59.4 66.4 2 -7.1\nBinOp:left→right 31.5 45.2 2 -13.7\nTry:handler→ﬁnalbody 8.3 25.0 21 -16.7\nFor:target→iter 58.3 77.4 2 -19.1\nAugAssign:target→value 49.4 70.3 2 -21.0\nFor:iter→body 11.7 34.8 4 -23.1\nIfExp:body→test 17.2 42.1 2 -24.9\nWhile:while→body 22.1 48.5 5 -26.5\nAssign:target→value 39.8 71.2 2 -31.4\nWhile:test→body 16.2 48.5 4 -32.4\nCall:func→args 59.3 93.2 2 -33.9\nCall:args→keywords 20.6 54.9 2 -34.4\nFor:for→iter 34.1 77.4 3 -43.3\nTable 9: Attention vs. offset baseline with ﬁxed offset for each relation onPython dataset using ﬁrst-token metric.\nIn the score column, we present the accuracy score for CuBERT and offset baseline. In the offset column, the\nchosen offset is shown. Score differences are calculated as CuBERT score - offset baseline score for each relation,\nwhere a positive value indicates that the language model surpasses the baseline performance. Since CuBERT\nalways outperforms Codebert, we only include results for CuBERT.\n3090\nRelation Score Offset DifferenceCuBERT Offset\nIf:if→else 92.7 5.7 17 87.1\nIfExp:body→orelse 52.4 21.3 10 31.1\nFor:target→iter 98.8 77.4 2 21.4\nFor:target→body 97.2 81.4 14 15.8\nIf:if→body 77.0 61.8 11 15.2\nIf:body→orelse 60.7 48.7 18 12.0\nFor:for→body 93.2 81.3 15 11.8\nCompare:left→comparator 56.4 45.7 2 10.7\nTry:body→orelse 57.9 52.6 62 5.3\nWhile:while→body 95.6 92.6 14 2.9\nchildren:parent→child 45.3 42.6 2 2.7\nIf:if→test 100.0 98.8 1 1.1\nBinOp:left→right 46.0 45.2 2 0.8\nFor:for→target 99.4 99.4 1 0.0\nWhile:while→test 99.3 99.3 1 0.0\nTry:body→ﬁnalbody 25.6 25.6 56 0.0\nAssign:target→value 78.2 79.7 4 -1.5\nCall:func→keywords 62.4 64.7 4 -2.3\nIf:test→body 59.2 61.7 10 -2.5\nBoolOp:value→value 47.8 50.6 8 -2.8\nAugAssign:target→value 66.8 70.3 2 -3.6\nSubscript:value→slice 61.7 66.4 2 -4.8\nFor:for→iter 72.2 77.4 3 -5.2\nAttribute:value→attr 76.7 82.3 2 -5.6\nIfExp:body→test 34.8 42.1 2 -7.3\nWhile:test→body 85.3 92.6 13 -7.4\nIf:test→orelse 36.5 44.9 31 -8.4\nTry:body→handler 41.6 51.2 17 -9.7\nIfExp:test→orelse 30.0 40.7 4 -10.7\nTry:handler→orelse 31.6 47.4 25 -15.8\nCall:func→args 75.3 93.2 2 -17.8\nFor:iter→body 63.6 83.5 12 -19.8\nCall:args→keywords 49.2 74.4 4 -25.2\nTry:handler→ﬁnalbody 16.7 58.3 21 -41.7\nTable 10: Attention vs. offset baseline with ﬁxed offset for each relation onPython dataset using any-token metric.\n3091\nRelation Score Offset DifferenceCuBERT Offset\nIf:if→else 87.0 7.0 15 80.0\nSwitch:switch→statement 100.0 75.2 5 24.8\nIf:if→body 48.8 26.2 7 22.5\nFor:test→updaters 76.0 53.4 4 22.5\nIf:body→orelse 28.7 9.4 10 19.3\nTry:body→handler 24.4 5.5 8 18.9\nDo:body→test 13.3 6.7 74 6.7\nTry:body→ﬁnalbody 7.0 4.3 9 2.6\nDo:do→test 6.7 6.7 34 0.0\nIfExp:test→orelse 24.3 24.5 7 -0.2\nInstanceofExpr:expr→type 89.8 91.9 2 -2.1\nFor:for→initializers 97.5 100.0 2 -2.5\nAttribute:value→attr 81.2 83.9 2 -2.7\nchildren:parent→child 34.2 36.8 2 -2.7\nIf:test→orelse 2.6 6.8 15 -4.2\nFor:initializers→updaters 39.8 45.7 9 -6.0\nSubscript:value→slice 72.3 78.8 2 -6.5\nIfExp:body→orelse 44.2 52.5 2 -8.2\nFor:for→updaters 36.2 45.5 11 -9.3\nTry:handler→ﬁnalbody 19.5 29.9 16 -10.4\nInﬁxExpr:left→right 50.7 61.6 2 -10.9\nSwitch:switch→expr 89.0 100.0 2 -11.0\nIf:test→body 11.3 26.2 5 -14.9\nIfExp:test→body 21.2 37.5 5 -16.3\nFor:initializers→body 21.0 37.5 13 -16.6\nSwitch:expr→statement 58.1 75.2 3 -17.1\nFor:for→body 16.1 36.0 15 -19.9\nWhile:while→body 13.2 42.7 9 -29.6\nWhile:test→body 9.4 42.7 7 -33.3\nAssign:target→value 35.3 68.7 2 -33.4\nCall:func→args 63.6 98.7 2 -35.1\nFor:test→body 13.9 49.2 8 -35.3\nIf:if→test 58.2 96.5 2 -38.3\nWhile:while→test 41.9 82.0 2 -40.1\nFor:updaters→body 19.4 88.9 4 -69.5\nFor:initializers→test 15.1 85.4 5 -70.3\nDo:do→body 26.7 100.0 2 -73.3\nFor:for→test 10.5 84.8 7 -74.3\nTable 11: Attention vs. offset baseline with ﬁxed offset for each relation on Java dataset using ﬁrst-token metric.\n3092\nRelation Score Offset DifferenceCuBERT Offset\nIf:if→else 87.0 7.0 15 80.0\nFor:test→updaters 86.9 53.6 5 33.3\nIf:if→body 79.6 62.1 11 17.5\nIf:body→orelse 69.1 52.1 16 17.0\nWhile:while→test 98.8 82.0 2 16.9\nAssign:target→value 81.4 68.7 2 12.6\nTry:body→ﬁnalbody 20.0 13.0 37 7.0\nIfExp:body→orelse 59.5 52.5 2 7.0\nFor:for→updaters 53.6 46.8 11 6.7\nDo:body→test 33.3 26.7 81 6.7\nDo:do→test 33.3 26.7 81 6.7\nFor:for→test 95.3 89.5 9 5.8\nIfExp:test→orelse 45.9 42.4 10 3.5\nWhile:while→body 91.4 89.0 17 2.4\nIf:if→test 98.7 96.5 2 2.3\nchildren:parent→child 44.0 42.4 2 1.6\nInstanceofExpr:expr→type 93.1 91.9 2 1.1\nFor:initializers→updaters 47.2 47.1 9 0.1\nSwitch:switch→statement 100.0 100.0 21 0.0\nFor:for→initializers 99.4 100.0 2 -0.6\nTry:body→handler 44.0 44.7 25 -0.7\nSubscript:value→slice 77.9 78.8 2 -0.9\nSwitch:switch→expr 99.0 100.0 2 -1.0\nWhile:test→body 87.4 89.5 15 -2.2\nAttribute:value→attr 81.2 83.9 2 -2.7\nFor:updaters→body 96.8 99.6 9 -2.8\nSwitch:expr→statement 96.6 100.0 12 -3.4\nInﬁxExpr:left→right 58.2 61.6 2 -3.4\nFor:test→body 92.3 96.1 14 -3.9\nIf:test→body 57.4 62.1 9 -4.7\nFor:for→body 90.3 94.9 23 -4.7\nIf:test→orelse 42.3 47.7 27 -5.4\nDo:do→body 93.3 100.0 2 -6.7\nFor:initializers→body 87.7 94.9 21 -7.2\nCall:func→args 91.1 98.7 2 -7.6\nFor:initializers→test 74.5 90.2 7 -15.6\nIfExp:test→body 25.9 50.0 5 -24.1\nTry:handler→ﬁnalbody 26.0 53.2 20 -27.3\nTable 12: Attention vs. offset baseline with ﬁxed offset for each relation on Java dataset using any-token metric.\n3093",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8892923593521118
    },
    {
      "name": "Syntax",
      "score": 0.7940316200256348
    },
    {
      "name": "Natural language processing",
      "score": 0.6573906540870667
    },
    {
      "name": "Abstract syntax tree",
      "score": 0.6416151523590088
    },
    {
      "name": "Artificial intelligence",
      "score": 0.608923614025116
    },
    {
      "name": "Abstract syntax",
      "score": 0.5605348348617554
    },
    {
      "name": "Programming language",
      "score": 0.5334404110908508
    },
    {
      "name": "Natural language",
      "score": 0.5073615908622742
    },
    {
      "name": "Language model",
      "score": 0.500603437423706
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4919707477092743
    },
    {
      "name": "Benchmarking",
      "score": 0.4841206967830658
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113520",
      "name": "Brain (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204465549",
      "name": "Washington University in St. Louis",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ],
  "cited_by": 10
}