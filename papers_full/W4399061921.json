{
  "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning",
  "url": "https://openalex.org/W4399061921",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2359841906",
      "name": "Zhao Yuyue",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A4226781938",
      "name": "Wu, Jiancan",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1945709380",
      "name": "Wang Xiang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2109996506",
      "name": "Tang Wei",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A214919626",
      "name": "Wang, Dingxian",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2742303006",
      "name": "de Rijke Maarten",
      "affiliations": [
        "Amsterdam University of the Arts",
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4392669753",
    "https://openalex.org/W4386728933",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4367319708",
    "https://openalex.org/W4296591867",
    "https://openalex.org/W4389520443",
    "https://openalex.org/W3091993229",
    "https://openalex.org/W3045200674",
    "https://openalex.org/W4376312036",
    "https://openalex.org/W4400525124",
    "https://openalex.org/W4404594861",
    "https://openalex.org/W3175529606",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W4390872747",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W2945623882",
    "https://openalex.org/W3155368131",
    "https://openalex.org/W3094605801",
    "https://openalex.org/W4389675217",
    "https://openalex.org/W6601141708",
    "https://openalex.org/W4400525295",
    "https://openalex.org/W6600120041",
    "https://openalex.org/W4393147158",
    "https://openalex.org/W6600042225",
    "https://openalex.org/W3170841641",
    "https://openalex.org/W3153325943",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Conventional recommender systems (RSs) face challenges in precisely capturing users' fine-grained preferences. Large language models (LLMs) have shown capabilities in commonsense reasoning and leveraging external tools that may help address these challenges. However, existing LLM-based RSs suffer from hallucinations, misalignment between the semantic space of items and the behavior space of users, or overly simplistic control strategies (e.g., whether to rank or directly present existing results). To bridge these gap, we introduce ToolRec, a framework for LLM-empowered recommendations via tool learning that uses LLMs as surrogate users, thereby guiding the recommendation process and invoking external tools to generate a recommendation list that aligns closely with users' nuanced preferences. We formulate the recommendation process as a process aimed at exploring user interests in attribute granularity. The process factors in the nuances of the context and user preferences. The LLM then invokes external tools based on a user's attribute instructions and probes different segments of the item pool. We consider two types of attribute-oriented tools: rank tools and retrieval tools. Through the integration of LLMs, ToolRec enables conventional recommender systems to become external tools with a natural language interface. Extensive experiments verify the effectiveness of ToolRec, particularly in scenarios that are rich in semantic content.",
  "full_text": "Let Me Do It For You: Towards LLM Empowered\nRecommendation via Tool Learning\nYuyue Zhao\nyyzha0@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nUniversity of Amsterdam\nHefei, Anhui, China\nJiancan Wuâˆ—\nwujcan@gmail.com\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nXiang Wangâˆ—â€ \nxiangwang1223@gmail.com\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nWei Tang\nweitang@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nDingxian Wang\ndingxian.wang@student.uts.edu.au\nUniversity of Technology Sydney\nUltimo, New South Wales, Australia\nMaarten de Rijke\nm.derijke@uva.nl\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nABSTRACT\nConventional recommender systems (RSs) face challenges in pre-\ncisely capturing usersâ€™ fine-grained preferences. Large language\nmodels (LLMs) have shown capabilities in commonsense reasoning\nand leveraging external tools that may help address these challenges.\nHowever, existing LLM-based RSs suffer from hallucinations, mis-\nalignment between the semantic space of items and the behavior\nspace of users, or overly simplistic control strategies (e.g., whether\nto rank or directly present existing results). To bridge these gap, we\nintroduce ToolRec, a framework for LLM-empowered recommen-\ndations via tool learning that uses LLMs as surrogate users, thereby\nguiding the recommendation process and invoking external tools\nto generate a recommendation list that aligns closely with usersâ€™\nnuanced preferences.\nWe formulate the recommendation process as a process aimed\nat exploring user interests in attribute granularity. The process\nfactors in the nuances of the context and user preferences. The\nLLM then invokes external tools based on a userâ€™s attribute instruc-\ntions and probes different segments of the item pool. We consider\ntwo types of attribute-oriented tools: rank tools and retrieval tools.\nThrough the integration of LLMs, ToolRec enables conventional\nrecommender systems to become external tools with a natural lan-\nguage interface. Extensive experiments verify the effectiveness of\nToolRec, particularly in scenarios that are rich in semantic content.\nâˆ— Jiancan Wu and Xiang Wang are corresponding authors.\nâ€  Xiang Wang is also affiliated with Institute of Dataspace, Hefei Comprehensive\nNational Science Center.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07.\nhttps://doi.org/10.1145/3626772.3657828\nCCS CONCEPTS\nâ€¢ Information systems â†’Recommender systems.\nKEYWORDS\nRecommender system, Large language models, Tool learning\nACM Reference Format:\nYuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, and\nMaarten de Rijke. 2024. Let Me Do It For You: Towards LLM Empowered\nRecommendation via Tool Learning. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR â€™24), July 14â€“18, 2024, Washington, DC, USA. ACM, New York, NY,\nUSA, 11 pages. https://doi.org/10.1145/3626772.3657828\n1 INTRODUCTION\nRecommender systems (RSs) are typically designed to identify\nuser preferences and subsequently suggest potential items of in-\nterest [9, 15, 22, 34, 43, 47, 48, 54]. This strategy has two important\nlimitations. First, the capabilities of existing RSs to accurately cap-\nture a userâ€™s true preferences are limited when relying solely on\nhistorical interaction data. Second, conventional RSs are often â€œnar-\nrow experts, â€ lacking commonsense knowledge about users and\nitems, which leads to a restricted scope of recommendations [14].\nInspired by the commonsense reasoning and knowledge utiliza-\ntion capabilities of large language models (LLMs), there have been\nseveral attempts to integrate LLMs with RSs and mitigate their\ninherent limitations [26, 49, 62]:\nâ€¢LLMs as RSs : Here, LLMs, whether initially trained or further\nfine-tuned using user-item interaction data, are adapted to serve\nas RSs [27, 41, 50] and directly generate candidate items in text.\nThis approach easily suffers from the hallucination problem [2],\nespecially given large item catalogue sizes and extensive item\nnames [21].\nâ€¢LLMs enhance RSs: Here, RSs are enhanced with world knowl-\nedge and reasoning abilities of LLMs [25, 27, 32, 41, 50, 59]. This\ncategory limits LLMs to offering semantic information within a\nconventional recommendation paradigm, sometimes leading to\ninconsistencies between the semantic and behavior spaces.\narXiv:2405.15114v1  [cs.IR]  24 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Yuyue Zhao et al.\n:The actorsin the retrieved movies are not as famous as the actorsin the userâ€™s previous films. My task is to rank these movies based on the actors and output the top 4movies.Action 3: Rank[actor, 4]Observation 3:[ ]\n:Based on the inconsistency in release_yearbetween the retrieved movies and the user's previous films, I need to retrieve 3movies based on the release_yearof the movie.Action 2: Retrieval[release_year, 3]Observation 2:[ ]\nUserProfileMultiplerounds\n:Based on the userâ€™s profile and his historicalinteracteditems, he might have a preference foractionmovies. I need to retrieve 5movies according to the movie genre.Action 1: Retrieval[genre, 5]Observation 1:[ ]\nPlease help theuserğ’–find movies he wants to watch next.\n:The recommended list is good enough, so the answer is [].Action4:Finish.\nItempooltargetitem\nsurrogateuser attribute-orienteditems\nFigure 1: Illustrating how ToolRec works. The LLM-based\nsurrogate userlearns the real userâ€™s preferences and decides\nto employ attribute-oriented tools to explore areas of items.\nThis process leads to a broad view of items, which, in turn,\nleads to the successful retrieval of the target item. It is impor-\ntant to note that the areas representing different attribute-\noriented items that are retrieved according to a specific at-\ntribute may contain overlapping elements.\nâ€¢LLMs control RSs: Here, LLMs are used to monitor and con-\ntrol the recommendation pipeline. Existing controllers either\nhave simple control strategies [10, 16] or necessitate active user\ninvolvement [8]. Their decisions are rarely human-like, which\nhinders their effectiveness in applications.\nTool learning for recommendations. Motivated by recent ad-\nvancements in tool learning with foundation models [e.g.,35, 36, 55],\nwe propose to use LLMs as a surrogate user to emulate her/his\ndecision-making process along with the utilization of tools. At the\ncore of our proposal is the task of learning to adaptively select\nappropriate recommender tools and curate a user-centric item list\nthat is aligned with the userâ€™s preferences. Figure 1 illustrates a\nfour-round example of how ToolRec works. The LLM starts the\nsimulation by focusing on the movie genre and selects an initial set\nof 5 movies; satisfied with the genre of the returned movies, it then\naims to complement the set based on the release_year, and retrieves\n3 additional movies; subsequently, the LLM refines its focus towards\nthe actors, leading to an adjustment of four movies in the list. Such\niterative refinements continue until the simulator deems the movie\nlist satisfactory enough to include the item of interest, thus finish-\ning the recommendation process. Notice how the LLM directs the\nrecommendation through multiple decision-making rounds with\nattribute signals, contrasting with conventional RSs. By adopting\nthis approach, we aim to move beyond relying solely on usersâ€™ his-\ntorical interactions, resulting in a more tailored set of recommended\nitems (cf. the red area on the right of Figure 1).\nUsing LLM tool learning to simulate usersâ€™ decision-making pro-\ncesses presents distinct challenges. The first challenge concerns the\nrecommendation ability of LLMs. Although LLMs are pretrained on\nextensive datasets [1, 28], improving their ability to produce qual-\nity recommendations remains a challenge, particularly in domain-\nspecific scenarios. The second challenge is developing appropriate\nattribute-oriented tools. Attribute-oriented tools should not only\nbe capable of exploring facets of the item pool (e.g., genre, release\nyear of movies) but also need to be effective in handling differ-\nent attribute choices during decision simulation. Lastly, leveraging\nLLMs to refine the set of candidate items in each round presents a\nsignificant challenge. This step could ensure that the final results\nbenefit from an LLMâ€™s open-world knowledge and are not limited\nby a single â€œnarrow expert. â€\nA new proposal for tool learning for recommendations. To\naddress the challenges listed above, we introduce ToolRec, for LLM-\nempowered recommendations via tool learning, which is aimed\nat aligning the emergent abilities of LLMs with the demands of\nrecommender systems. ToolRec comprises three key components:\n(i) A user decision simulation module : We use LLMs initialized\nwith user behavior history, acting as a surrogate user to evaluate\nuser preferences against the current scenario. (ii) Attribute-ori-\nented tools: We develop two distinct sets of attribute-based tools:\nrank tools and retrieval tools. The ranking tools are operationalized\nby LLMs with attribute-oriented ranking instructions, while the\nretrieval tools are operationalized by merging a frozen backbone\nwith additional fine-tuned attribute encoders. These tools are acti-\nvated when the surrogate user identifies unsatisfactory attributes,\nand then fetches corresponding candidate items. (iii) A memory\nstrategy: This component checks the presence of intermediate re-\nsults and stores them with associated tool marks, aiding the LLM in\nleveraging open-world knowledge to refine the final recommenda-\ntion list. The latter two components are governed by the surrogate\nuserâ€™s decisions, and conclude the recommendation process once\na satisfactory candidate item list has been found. ToolRecâ€™s itera-\ntive framework integrates LLMs into recommender systems while\nenhancing the quality of recommendations.\nContributions. Our main contributions are as follows:\nâ€¢We propose ToolRec, a framework that deploys LLMs to enhance\nrecommendations via tool learning. It employs LLMs to closely\nemulate user preferences, thereby improving the accuracy of\nrecommendations generated during user decision simulation.\nâ€¢To better meet the surrogate userâ€™s needs, we incorporate at-\ntribute-oriented tools and a memory strategy. Those components\naddress the challenge of effective item retrieval based on identi-\nfied attributes, ensuring the recommendations are well-aligned\nwith user preferences.\nâ€¢Experimental results on three real-world datasets demonstrate\nthe effectiveness of ToolRec, especially in domains enriched by\nworld knowledge.\n2 RELATED WORK\nWe review tool learning with LLMs and the use of LLMs for recom-\nmendation.\n2.1 LLMs with Tool Learning\nThere is a growing trend to employ LLMs to construct autonomous\nagents to achieve decision-making capabilities [ 4, 31, 35, 36, 61].\nLet Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nThese LLM-based agents often fall short in domains that demand ex-\ntensive expert knowledge and suffer from hallucination issues [42].\nTo alleviate these problems, these agents are enhanced with the abil-\nity to invoke external tools for action execution. Previous external\ntools can be grouped into three types: APIs [ 24, 30, 31, 35, 36],\nDatabases & Knowledge Bases [ 11, 17, 23], and External Mod-\nels [38, 52, 60].\nThe use of APIs as tools has become a popular approach. E.g.,\nHuggingGPT [36] employs models on HuggingFace to accomplish\ncomplex user tasks. API-Bank [ 24] serves as an LLM-based API\nrecommendation agent, and autonomously searches and generates\nsuitable API calls across various programming languages and do-\nmains. ToolBench [30] is an LLM-based tool generation system\nthat creates various tools based on natural language requirements.\nConnecting LLMs to external databases or knowledge bases en-\nables access to domain-specific information, thus generating more\nrealistic actions. E.g., ChatDB [17] uses SQL statements to query\ndatabases, enabling logical action execution by agents. Similarly, in\nthe recommendation scenario, RecMind [44] and InteRecAgent [21]\nengage various expert systems such as MySQL and planners to re-\ntrieve detailed item information. Employing external models can ex-\npand the range of feasible actions. E.g., MemoryBank [60] employs\ntwo language models to enhance text retrieval capabilities: one for\nencoding input text and the other for matching query statements.\nMM-REACT [52] integrates various vision models to improve its\nperformance in visual understanding tasks.\nOur attribute-oriented rank tools can be summarized into APIs,\nwhile retrieval tools can be summarized into external models. This\napproach helps our surrogate user to explore different interest areas\nand return a better user preference-aligned item set.\n2.2 LLMs for Recommendation\nMethods using LLMs in RSs come in three groups, based on the\nrole of the LLMs: LLMs as RSs [ 3, 12, 13, 19, 51, 53, 58], LLMs as\nassistants [27, 41, 50], and LLMs as pipeline controllers [8, 10, 21,\n44].\nLLMs as RSs involve using LLMs to generate candidate items. For\ninstance, P5 [12] is fine-tuned on T5 [33] with a collection of per-\nsonalized prompts to achieve zero-shot generalization. Following\nthis, UP5 [19] and VIP5 [13] extend the paradigm to fairness and\nmultimodal tasks, respectively. InstructRec [58] views recommen-\ndation as an instruction following task for LLMs, tuning T5 with\nuser-specific instructions. TALLRec [ 3] trains LLaMA [ 39] with\nLoRA [18] to follow instructions and respond to a binary query\nprovided within the contextual information. LLaRA [ 25] utilizes\nhybrid prompting to bridge the modality gap between traditional\nrecommendation systems and LLMs.\nWhen LLMs serve as Assistants, LLMs are leveraged for their\nfactual knowledge or reasoning capabilities to generate or encode\nauxiliary textual features, assisting conventional RSs with semantic\ninformation. For example, Various studies use auxiliary textual\nfeatures from BERT [7] for document ranking [63], while others\naim to leverage these features for news recommendation [32, 46, 59].\nKAR [50] extracts reasoning and factual knowledge from LLMs to\nserve as augmented features, enhancing recommendation models\nin a model-agnostic manner.\nUserdecisionsimulation\nAttribute-orientedtools\nuserprofileitems attribute,quantity\nitems\nrecordsMemorystrategy\nFigure 2: An overview of the proposed LLM-based recommen-\ndation method via tool learning.\nIn the setting where LLMs act as Pipeline Controllers, models\nsuch as Chat-REC [10] use ChatGPT to understand user preferences,\ndecide whether to use the backend recommender system, and refine\nthe suggested items before showing them to the user. RecLLM [8]\nand InteRecAgent [21] propose frameworks for integrated conver-\nsational recommender systems with LLMs managing dialogues,\nunderstanding user preferences. InteRecAgent further determines\nthe usage of various tools (i.e., information query tools, retrieval\ntools, and rank tools) to support the candidate items. RecMind [44],\ninspired by P5â€™s various-task experiment setup, manages tool usage\nthrough a Self-Inspiring mechanism. Agent4Rec [56] simulates and\ninfers user-personalized preferences and behavior patterns using a\nLLM-based movie recommendation simulator.\nWhile these pipeline controller efforts address challenges similar\nto ours, there are several aspects that make our approach differ-\nent from theirs. First, we focus on the sequential recommendation\ntask and top-N recommendation setting. Second, we introduce\nattribute-oriented tools designed specifically for the recommenda-\ntion exploration journey. Lastly, we propose a proactive analysis of\npreference mismatches by using an LLM as a surrogate user.\n3 METHODOLOGY\nWe propose ToolRec, as illustrated in Figure 2. We first formalize\nthe recommendation process as a process exploring usersâ€™ interests.\nThen, we introduce a general framework that adapts LLMs as sur-\nrogate users to enhance the recommendation mechanism through\nattribute-oriented tools.\n3.1 Problem Formulation\nIn the context of sequential recommendation, as illustrated in Fig-\nure 1, given user ğ‘¢ with a historical interaction sequence H =\n{ğ‘–1,ğ‘–2,...,ğ‘– ğ‘›âˆ’1}, the goal is to predict the next item of interest\nğ‘–ğ‘› âˆˆI, where Iis the complete item pool. Conventional RSs re-\ntrieve a subset Iğ‘ âŠ†I based on the predicted preferences of user\nğ‘¢. However, this easily results in the desired itemğ‘–ğ‘› being absent\nfrom Iğ‘.\nIn this work, we position LLMs as the central controller for rec-\nommendation, simulating the user exploration w.r.t. item attributes\nin a multi-round manner. The interaction history Hof user ğ‘¢ is\nfed into the LLM, so as to initialize the profile of surrogate user Ë†ğ‘¢.\nIn the first round, Ë†ğ‘¢identifies a key attribute ğ‘1 and uses external\ntools to fetch the related item set Iğ‘1\nË†ğ‘¢ . For clarity and brevity, we\nomit the subscript Ë†ğ‘¢ when the user is clear from the context. In\nthe second round, Ë†ğ‘¢ contrasts the preferences between Iğ‘1 and\nH, selects another attribute ğ‘2, and retrieves Iğ‘2 . Such iterative\nrefinements continue until Ë†ğ‘¢is satisfied with the retrieved items,\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Yuyue Zhao et al.\nand outputs the final set IË†ğ‘¢ ranked by preference. Each derived\nitem set reflects the interest emerging from the respective round.\n3.2 User Decision Simulation\nTo validate LLMsâ€™ capability in simulating user preferences and uti-\nlizing external tools, we draw inspiration from prior tool-learning\nstudies [30, 31, 35, 55] and propose a user decision simulation pro-\ncess. Here is an example of the simulation prompt in the movie\nrecommendation scenario:\nâ€œAssuming you are an online movie recommender, your task is to help users find movies they wouldliketo watch next based on their interests. To effectively recommend movies, you should follow three steps: Thought, Action, Observation. During the Thought step, your objective is to consider how to make the final movie list match the userâ€™s preferences,and decide the best course of action<ğ’Ÿ>. If the movie list is good enough or no better actiontotake, you will finish early with the candidate movie list. For the Action step, you have three options: Retrieval,Rank,andFinish.<ğ’¯>â€\nUserDecisionSimulationPromptExample\nwhere <D> represents the collection of demonstrations that show\nLLMs what a good movie list looks like, and <T> denotes the\ndetailed description of the tools. The foundational elements of the\nuser decision simulation methodology are twofold:\nChain-of-thought prompting (CoT). We use CoT [45] to syner-\ngize reasoning and action. Here, â€œreasoningâ€ refers to the â€œthinking\nprocedureâ€ for how to recommend suitable items to users, and\ndecide the details of the subsequent action. Meanwhile, â€œactionâ€\nentails executing the directives derived from reasoning, making\nâ€œobservations, â€ and yielding the corresponding outcome. At each\nstep ğ‘¡, the LLM-driven surrogate user Ë†ğ‘¢ receives an observation\nğ‘œğ‘¡âˆ’1 from the last step and derives a thought ğ‘”ğ‘¡ to take an ac-\ntion Ağ‘¡ following some policy ğœ‹(ğ‘”ğ‘¡,Ağ‘¡|ğ‘ğ‘¡), where ğœ‹ could be im-\nplemented by any LLMs, specifically ChatGPT, in our ToolRec,\nand ğ‘ğ‘¡ = (ğ‘œ0,ğ‘”1,A1,ğ‘œ1,...,ğ‘” ğ‘¡âˆ’1,Ağ‘¡âˆ’1,ğ‘œğ‘¡âˆ’1)is the context to the\nË†ğ‘¢. The primary objective is to deduce the policy and mapping\nğ‘ğ‘¡ â†’(ğ‘”ğ‘¡,Ağ‘¡). As shown in the left of Figure 1, within the context\nğ‘ğ‘¡, Ë†ğ‘¢ observes a discrepancy in the movie actors between the re-\ntrieved movies and the userâ€™s previous movies. Consequently, it\ndecides to rank the movie list based on their actors (i.e., Ağ‘¡) and\nobtain ğ‘œğ‘¡. If Ë†ğ‘¢is satisfied within the context (i.e.,ğ‘ğ‘), by comparing\nthe candidate items to the userâ€™s history, then Ë†ğ‘¢will conclude the\nprocess and present the final set of candidate items, denoted as IË†ğ‘¢.\nTool learning. Tool learning technology refers to combine the\nstrength of LLMs and specialized tools, as discussed in previous\nstudies [30]. Here, our tools are activated by the generated ac-\ntion Ağ‘¡ [31, 35, 55]. For each context ğ‘ğ‘¡, the initial observation\nğ‘œ0 = (H,T,D)is composed of the user ğ‘¢â€™s historical interactions\nH, tool description T, and demonstration D. The tools can be\ncategorized into two types based on Tğ‘¡ğ‘¦ğ‘ğ‘’, which can be either\nâ€œretrieve toolsâ€ or â€œrank tools. â€ The description Telucidates the\nimpact of using the tools, and Doffers practical demonstrations\nof their application. Together, these components promote the ef-\nfective utilization of external tools. Tool actions are formulated as\nTğ‘¡ğ‘¦ğ‘ğ‘’ [ğ‘ğ‘¡,$ğ¾], where Tğ‘¡ğ‘¦ğ‘ğ‘’ âˆˆ{ğ‘…ğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘’ğ‘£ğ‘ğ‘™,ğ‘…ğ‘ğ‘›ğ‘˜ }, ğ‘ğ‘¡ indicates the\nchosen attribute based on Ë†ğ‘¢â€™s decision at timeğ‘¡, and $ğ¾specifies the\nnumber of items to be returned. For instance, as shown in Figure\n1, at the first step, Ë†ğ‘¢opts to use the retrieval tools conditioned on\nthe attribute â€œgenreâ€ and retrieves 5 items. Meanwhile, at step 3, Ë†ğ‘¢\ndecides to employ rank tools conditioned on the attribute â€œactorâ€,\nreturning the top 4 items. This phase is constructed to emulate a\nhuman-like approach of leveraging tools to broaden their choices\nthrough linguistic reasoning.\nThe CoT prompting phase controls the iterative decision process,\ndetermining when to employ external tools or finalize the recom-\nmendations. Concurrently, feedback from these tools enhances item\nexploration and further refines the recommendation process.\n3.3 Attribute-oriented Tools\nBy empowering an LLM to use tools, we can explore different parts\nof the item pool, uncovering target items that remain latent. To\nachieve this, we have designed two types of tools.\n3.3.1 Rank tools. For attribute-oriented rank tools, we incorpo-\nrate a ranking instruction template and employ LLMs to order the\ncandidate items. Given that LLMs have demonstrated proficiency\nin both zero-shot and few-shot scenarios [ 16], their capabilities\nare essential for returning item sets that align more closely with\nthe userâ€™s latent intent. Our instruction for ranking is framed as:\nâ€œ{User Historical Record } {Prior Retrieved Item Set }. Please rank the\nabove recommended movies by measuring the possibilities that the\nuser would like to watch next most according to the movie [Attribute\nPattern ğ‘ğ‘¡] attribute and the given movie history records, and output\ntop [Output Size Pattern $ğ¾] movies except userâ€™s historical movies . â€\n3.3.2 Retrieval tools. For each user, our attribute-oriented retrieval\ntools accept an attribute pattern ğ‘ğ‘¡ and a specified item set size\n$ğ¾, subsequently returning the matching candidate item set. An\nintuitive way is creating dedicated models tailored to each attribute\npattern. However, it is markedly inefficient to fully train and store\nseparate models for every possible attribute permutation. To address\nthis, we introduce a two-stage method for managing attribute-\nspecific variations:\nPre-training. The pre-training stage is for the original model with\nno attribute-specific consideration. Without loss of generality, we\nincorporate the attribute-specific switches into the popular sequen-\ntial recommendation model, SASRec [22]. For a historical behav-\nior sequence H, the ğ‘™-th layer behavior representation matrix is\ndenoted as Hğ‘™ = {hğ‘™\n1,hğ‘™\n2,..., hğ‘™\n|H|}, where hğ‘™\nğ‘– is the ğ‘™-th layerâ€™s\nrepresentation of the ğ‘–-th behavior within the sequence H. Then\nin the final layer ğ¿, the pre-training behavior representation H is\nderived:\nH = ğ‘“ğ‘ ğ‘’ğ‘(H|ğ›½)= hğ¿\n|H|, Hğ‘™+1 = Transformerğ‘™\nâ„(Hğ‘™), (1)\nwhere ğ‘“ğ‘ ğ‘’ğ‘ represents the sequential model, ğ›½ is the pre-training\nparameter, Transformer denotes the Transformer architecture en-\ncoder, and H is considered as the user representation at the pre-\ntraining phase for sequential recommendation.\nAttribute-specific encoder in tuning. As illustrated in Figure 3,\nafter learning the pretrained model, we freeze the pre-training\nparameter ğ›½. Our goal is to fine-tune the attribute-specific encoder.\nTo achieve this, we construct an additional attribute encoder ğ‘“ğ‘ğ‘¡ğ‘¡ğ‘Ÿ.\nThis encoder takes the userâ€™s historical item attribute sequence, ğ‘ğ‘¢,\nLet Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nSequential Model(pre-trained)\nAttribute-SpecificEncoder\nBehavior Seq \nAttribute Seq\nÃ—\nDense layer\nğ’Š ğ’–â„’!\"#\n+\nFigure 3: Fine-tuning stage of attribute-oriented retrieval\ntools. The parameters in the blue â€˜iceâ€™ section remain fixed,\nwhile those in the red â€˜flameâ€™ section are exclusively fine-\ntuned.\nas input. Importantly, Hand ğ‘ğ‘¢ are aligned based on items in the\nhistorical sequence. Subsequently, we define theğ‘™-th layer attribute\nrepresentation matrix as ğ’‚ğ‘™ğ‘¢ = {ğ’‚ğ‘™\n1,ğ’‚ğ‘™\n2,Â·Â·Â· ,ğ’‚ğ‘™\n|ğ‘ğ‘¢ |}. Similar to the\npre-training approach, the attribute sequence representation ğ’‚ğ‘¢ is\nlearned as:\nğ’‚ğ‘¢ = ğ‘“ğ‘ğ‘¡ğ‘¡ğ‘Ÿ (ğ‘ğ‘¢|ğ›¾)= ğ’‚ğ¿\n|ğ‘ğ‘¢ |, ğ’‚ğ‘™+1\nğ‘¢ = Transformerğ‘™\nğ‘(ğ’‚ğ‘™\nğ‘¢). (2)\nWe then incorporate a dense layer to encode the combination of\nattribute representation ğ’‚ğ‘¢ and the frozen behavior representation\nH, resulting in a new user representation:\nu = Dense(ğ’‚ğ‘¢ âŠ•H,ğœƒ), (3)\nwhere ğ›¾ and ğœƒ are the trainable parameters in the tuning phase.\nThe sequential recommender is trained by minimizing the BPR loss\nfunction:\nLğµğ‘ƒğ‘… = âˆ’\nâˆ‘ï¸\n(H,ğ‘£)âˆˆO+,(H,ğ‘¤)âˆˆOâˆ’,\nlog ğœ(ğœ™(u,ğ‘£)âˆ’ğœ™(u,ğ‘¤)), (4)\nwhere O+and Oâˆ’denote the positive samples and negative sam-\nples, ğœ™(Â·)represents the inner-product layer, and ğœ(Â·)refers to the\nSigmoid activation function. During the pre-training phase, u is\nrepresented by H. The finetuned user embedding u is designed to\nbe sensitive to the specific attribute, while maintaining the per-\nsonalized sequential recommendation learned in the pre-training\nphase.\n3.4 Memory Strategy\nThe vast number of items and the complex item names&IDs pose\na challenge for LLMs when generating control commands or tool\nusages. Additionally, items retrieved from various tools should be\nsystematically ordered to aid the LLM-based surrogate user Ë†ğ‘¢ in\nmaking decisions. Therefore, we introduce a memory strategy, en-\nsuring the correctness of generated items and cataloging candidate\nitems with their respective tool annotations.\nThe memory strategy is initialized with the item pool directory.\nWhenever external tools return candidate items, particularly from\nattribute-oriented rank tools, the strategy verifies the presence of\nthese items in the initial directory. If there are any discrepancies,\nthe tools are prompted to re-run with additional incorrect details\nattached behind. Once validated, the candidate items are recorded\nalongside their associated tool marks, in order to serve the sub-\nsequent tool calls. As an illustration, a typical prompt might be\nTable 1: The statistics of the datasets used.\nDatasets #Users #Items #Interactions Sparsity (%)\nML-1M 6,041 3,884 1,000,209 95.74\nAmazon-Book 158,349 97,566 3,876,663 99.97\nYelp2018 77,278 45,582 2,102,836 99.94\nâ€œHereâ€™s the top [Output Size Pattern $ğ¾] movie ID, movie name, and\nthe recommendation confidence score from the recommender system\nwith [Attribute Pattern ğ‘Ë†ğ‘¢] type. {Candidate Item Set }. â€\n4 EXPERIMENTS\nIn this section, we report on extensive experiments aimed at evalu-\nating the performance of our proposed ToolRec. Our experiments\nfocus on answering the following research questions: (RQ1) How\ndoes ToolRec compare to conventional RSs and LLM-based RSs in\nthe sequential recommendation setting? (RQ2) How do different\ncomponents (i.e., user decision simulation, termination round, at-\ntribute-oriented retrieval tools ) influence our ToolRec? And(RQ3) Are\nLLMs capable of using their inherent knowledge to cater to the\nrecommendation taskâ€™s needs?\n4.1 Experimental Settings\n4.1.1 Datasets. To evaluate the effectiveness of our methods, we\nconduct experiments on three real-world datasets: Ml-1M, Amazon-\nBook, and Yelp2018.\nâ€¢ML-1M. This dataset is derived from the MovieLens-1M1 bench-\nmark, which contains user ratings for movies with timestamps.\nWe take the moviesâ€™ genre and release year as the attribute infor-\nmation.\nâ€¢Amazon-Book. This dataset 2 is extracted from Amazon.com\nplatform. We adopt the Book category to evaluate our method. We\ntake the booksâ€™ Price and Sales rank as the attribute information.\nA 10-core setting is applied to maintain dataset quality.\nâ€¢Yelp2018. This dataset is collected from the 2018 edition of the\nYelp Challenge.3 We utilize local businessesâ€™ Categories, City and\nStars as attributes. We employ the 10-core setting to ensure a\nminimum of ten interactions for each user and item.\nFor each dataset, we organize usersâ€™ interactions chronologically\nbased on timestamps, allowing us to create the corresponding histor-\nical interaction sequences. Items are described using their product\nIDs&Names. We summarize the statistics of our datasets in Table 1.\n4.1.2 Evaluation protocols. We apply the leave-one-out strategy [22,\n37] and employ timestamps to set the sequence order, dividing the\ninteraction data into training, validation, and test sets. The attribute-\noriented retrieval tools are trained on the training and validation\nsets. To measure the recommendation performance, we adopt two\nwidely used metricsNDCG@N and Recall@N to evaluate the results\nwithin the top-N positions, with ğ‘ = 10 in our experiments. Due to\nbudget constraints, following prior work [10, 16], we randomly sam-\nple 200 users and their historical behaviors from the test set for each\ndataset. A similar, and similarly-sized, setting has been adopted in\nother recent LLM-related recommendation researches [56, 57]. To\nenhance the robustness and credibility of our results, we repeated\n1 https://grouplens.org/datasets/movielens/ 2 https://nijianmo.github.io/amazon/\n3 https://www.yelp.com/dataset\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Yuyue Zhao et al.\nthe experiments three times, each with a different sample of 200\nusers. The average results and standard deviations from these trials\nare presented in Table 2.\n4.1.3 Baselines. We compare ToolRec4 against two traditional ap-\nproaches (the first two below), one that uses LLMs as RSs (the\nfollowing one), two that enhance RSs with LLMs (the next two),\nand two that use LLMs to control RSs (the remaining two).\nâ€¢SASRec [22]. A self-attention-based sequential recommender,\nwhich employs the encoder of the Transformer architecture to\ngenerate representations of usersâ€™ behavior sequences.\nâ€¢BERT4Rec [37]. A bidirectional self-attention-based sequential\nrecommender. It uses the Transformer encoder to predict ran-\ndomly masked items in a sequence by conditioning on both their\nleft and right context, thereby capturing user historical behaviors.\nâ€¢P5 [12]. An encoderâ€“decoder Transformer-based approach that\nunifies different recommendation related tasks into a single gen-\nerative LLM. For our sequential recommendation downstream\ntask, we adopt the personalized prompts from OpenP5 [51] and\napply the same random indexing method to items as used in our\nToolRec. This model is fully fine-tuned using these personalized\nprompts on the pre-trained T5-small [33].\nâ€¢SASRecğµğ¸ğ‘…ğ‘‡ [7]. An attention-based method that modifies the\nsingle interaction sequence encoder by adding a feature encoder\n(structured as shown in Figure 3). This model is fully fine-tuned\nusing semantic representations pre-trained with BERT.\nâ€¢BERT4Recğµğ¸ğ‘…ğ‘‡. A variation of the BERT4Rec sequential rec-\nommender enhanced with BERTâ€™s pre-trained representations.\nâ€¢Chat-REC [10]. The first work was on using an LLM as a con-\ntroller. For our sequential recommendation task, we adopt the\nrecommendation prompt based on the original paper, choose\nSASRec to supply the candidate items, and modify the output\nformat for parsing.\nâ€¢LLMRank [16]. A LLM-based ranking model. In our full-ranking\nexperimental setup, we retrieve thirty candidate items (according\nto the original paper) from SASRec and adjust the output format\nfor parsing.\nâ€¢ToolRec. The method that we propose. We primarily evaluate\ntwo versions of ToolRec: ToolRec that is implemented using\nexternal attribute-oriented retrieval tools with frozen ğ‘†ğ´ğ‘†ğ‘…ğ‘’ğ‘\nparameters; and ToolRecğµ that is developed using BERT4Rec as\nthe backbone for external attribute-oriented retrieval tools.\n4.1.4 Implementation details. We use the gpt-3.5-turbo-16k (Chat-\nGPT for short) model as our primary LLM within ToolRec. This\nmodel is responsible for parsing user preferences and assisting in\ntool learning. We retain the default hyperparameters of ChatGPT\nwithout modifications. To enable ToolRec to emulate user decisions,\nwe incorporate decision demonstrations into the prompt for in-\ncontext-learning. For attribute-oriented tools, retrieval tools are\nenhanced with corresponding additional item attributes specific to\neach dataset. Attributes are represented using word embeddings\nfrom GloVe [29]. These tools are developed on two widely recog-\nnized backbones: SASRec and BERT4Rec. Additionally, our rank\ntools are constructed with instructions on ChatGPT. We limit our\ndecision processes to a maximum of eight rounds.\n4 Our codes are available at https://github.com/Go0day/ToolRec-Code.\n4.2 Performance Comparison (RQ1)\nWe compare our proposal with conventional sequential recom-\nmenders and LLM-enhanced sequential recommenders, as detailed\nin Section 4.1.3. The results are reported in Table 2, from which we\nobserve:\nâ€¢In general, ToolRec outperforms all baselines on the ML-1M and\nAmazon-Book datasets. The performance improvement can be\nattributed to the efficacy of our designed ToolRec framework. By\ndelving into various facets of the item pool and utilizing LLMs to\nguide the exploration process, it can more effectively align with\nthe userâ€™s intent.\nâ€¢ToolRec and ToolRecğµ consistently demonstrate improved perfor-\nmance compared to their respective underlying models (i.e., SAS-\nRec and BERT4Rec). This confirms the superiority of the ToolRec\nframework and underscores its adaptability. The results suggest\nthat our approach, which harnesses LLMs for recommendation\nthrough tool learning, has the potential to be integrated as a\nsupplementary module in various recommendation systems.\nâ€¢ToolRec exhibits subpar performance on the Yelp2018 dataset.\nWe attribute this to the LLMâ€™s limited knowledge of local busi-\nnesses. Since LLMs are primarily trained on widely available web\ndata, they might possess a more robust understanding of topics\nlike movies and books than local (niche) businesses. In more spe-\ncialized domains, ToolRec, which involves multiple interactions\nbetween the LLM and external tools, has the potential to make\nmore incorrect decisions and may exhibit heightened deficien-\ncies when compared to other LLM-based approaches such as\nLLMRank and SASRecBERT.\nâ€¢In the approach where LLMs serve as RSs, P5 demonstrates strong\nperformance on the ML-1M. This is exciting, considering we use\ndatasets on the scale of millions (Table 1), and accurately gener-\nating items for users is challenging. However, P5 shows weaker\nperformance on the Amazon-Book and Yelp2018 datasets, poten-\ntially due to the fact that the sparsity of those datasets is higher\nthan ML-1M, thus leading to more severe hallucination issues.\nThe development of more carefully designed item indexing meth-\nods [20] may help alleviate this problem, but this falls beyond\nthe scope of our current research.\nâ€¢For LLM-enhanced RS approaches, such as SASRec ğµğ¸ğ‘…ğ‘‡ and\nBERT4Recğµğ¸ğ‘…ğ‘‡, it is not surprising to see improvements over\ntheir backbone models in most cases. However, on the ML-1M\ndataset, SASRecğµğ¸ğ‘…ğ‘‡ performs much worse than SASRec. Mean-\nwhile, on the Amazon-Book dataset, BERT4Rec ğµğ¸ğ‘…ğ‘‡â€™s perfor-\nmance is comparable to its baseline version. While language\nmodels can generally augment recommendation tasks, addressing\nthe disparity between semantic and behavioral spaces remains\nchallenging.\nâ€¢For LLM-controlled RS approaches, such as Chat-REC and LLM-\nRank, they couldnâ€™t achieve consistent improvements over SAS-\nRec in the same way as ToolRec. This suggests that merely using\nbasic control strategies or employing LLM as a ranker might not\nbe the most effective way forward for recommendation tasks.\nA more comprehensive strategy, as demonstrated by ToolRec,\nappears better suited to harnessing LLMs to enhance recommen-\ndations.\nLet Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 2: The test performance comparison on three real-world datasets. The bold font denotes the winner in that column. The\nrow â€œImprovementâ€ indicates the relative performance gain of our ToolRec and the suboptimal method.\nML-1M Amazon-Book Yelp2018\nRecall NDCG Recall NDCG Recall NDCG\nSASRec 0.203 Â±0.047 0.1017Â±0.016 0.047Â±0.015 0.0205Â±0.006 0.030Â±0.005 0.0165Â±0.006\nBERT4Rec 0.158 Â±0.024 0.0729Â±0.008 0.042Â±0.015 0.0212Â±0.009 0.033Â±0.021 0.0218Â±0.016\nP5 0.208 Â±0.021 0.0962Â±0.009 0.006Â±0.003 0.0026Â±0.002 0.012Â±0.005 0.005Â±0.001\nSASRecğµğ¸ğ‘…ğ‘‡ 0.192Â±0.015 0.0967Â±0.006 0.042Â±0.003 0.0194Â±0.002 0.032Â±0.016 0.0131Â±0.007\nBERT4Recğµğ¸ğ‘…ğ‘‡ 0.202Â±0.013 0.0961Â±0.009 0.045Â±0.023 0.0233Â±0.012 0.040Â±0.028 0.0208Â±0.015\nChat-REC 0.185 Â±0.044 0.1012Â±0.016 0.033Â±0.015 0.0171Â±0.007 0.022Â±0.003 0.0121Â±0.001\nLLMRank 0.183 Â±0.049 0.0991Â±0.020 0.047Â±0.013 0.0246Â±0.004 0.030Â±0.005 0.0140Â±0.004\nToolRec 0.215Â±0.044 0.1171Â±0.018 0.053Â±0.013 0.0259Â±0.005 0.028Â±0.003 0.0159Â±0.001\nToolRecğµ 0.185Â±0.018 0.0895Â±0.002 0.043Â±0.013 0.0223Â±0.008 0.025Â±0.005 0.0136Â±0.009\nImprovement 3.36% 15.10% 14.28% 5.14% â€“29.16% â€“27.32%\n4.3 Decomposing ToolRec (RQ2)\nNext, we delve deeper into ToolRec. We examine the user deci-\nsion simulation, revealing how multi-round interactions enhance\nrecommendation quality. We evaluate the efficiency of our attribute-\noriented retrieval tools, highlighting the balance between rich in-\nformation and computational practicality.\n4.3.1 Effectiveness of user decision simulation. To verify the contri-\nbution of the user decision simulation component (cf. Section 3.2),\nwe conducted an ablation study considering three variants of Tool-\nRec: (i) Disabling the CoT and tool learning components of ToolRec,\nwe forced the LLM to rank the candidate items from SASRec and\noutput the result. This variant is denoted as â€œw/ singleâ€; (ii) Unlike\nâ€œw/ singleâ€, we had the LLM rank candidate items using both SAS-\nRec and the attribute-oriented retrieval tools, termed â€œw/ multiâ€;\n(iii) We disable the CoT component, and instead, the LLM was in-\nstructed to generate all the steps of tool-calling at once and then\nstrictly follow the execution plan. This setup is termed â€œw/ Planâ€.\nTo ensure fairness in our comparisons, both the â€œw/ singleâ€ and\nâ€œw/ multiâ€ variants use the same number of candidate items: thirty\nitems in total.\nWe demonstrate the experimental results in Figure 4 and have\nthe following findings:\nâ€¢Removing the CoT and tool learning component degrades the\nmodelâ€™s performance. The â€œw/ singleâ€ variant consistently un-\nderperforms both â€œw/ multiâ€ and â€œw/ Planâ€, and its performance\nis even subpar compared to SASRec. This decline in performance\ncan be attributed to â€œw/ singleâ€ relying exclusively on the LLMâ€™s\nzero-shot ranking without leveraging additional information to\nrefine the results.\nâ€¢It is important to note that the performance of attribute-oriented\nretrieval tools, namely â€œfrozen + ğ‘1â€ and â€œfrozen + ğ‘2â€, is inferior\nto SASRec in dataset ML-1M. However, results refined by â€™w/\nmultiâ€™ not only surpass â€™w/ singleâ€™ but also outperform SASRec.\nThis improvement suggests that the strength of our approach is\nnot solely due to the broader item size, but also derives from the\nassistance of the additional attribute information.\nâ€¢When comparing â€œw/ Planâ€ with â€œw/ multiâ€, the latter consis-\ntently achieves superior results. One potential reason might be\nthat the â€œgenerate-then-executeâ€ approach in â€œw/ Planâ€ lacks\nSASRecforzen+a1forzen+a2w/ singlew/ multiw/ Plan T oolRec\n0.16\n0.18\n0.20\n0.22\n0.24Recall@10\nRecall@10\n0.06\n0.08\n0.10\n0.12\n0.14\nNDCG@10\nNDCG@10\n(a) ML-1M.\nSASRecforzen+a1forzen+a2w/ singlew/ multiw/ PlanT oolRec\n0.025\n0.030\n0.035\n0.040Recall@10\nRecall@10\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\nNDCG@10\nNDCG@10\n(b) Amazon-Book.\nFigure 4: Performance of ToolRec and its variants. The right\nside of the dividing line indicating the methods involving\nLLMs.\nexplicit performance guidance, causing it to miss the target. This\nfurther underscores the importance of the user decision simula-\ntion in our approach.\n4.3.2 Analysis of round termination in ToolRec. Figure 5 illustrates\nthe distribution of the number of termination rounds,ğ‘, in ToolRec.\nBased on the data, we make the following observations: (i) The\nmajority of processes conclude within three or four rounds. This\nsuggests that after a few iterations, our LLM-based surrogate user,\ndenoted as Ë†ğ‘¢, develops a good understanding of whether the userâ€™s\npreferences have been adequately addressed. (ii) While the major-\nity of successful hits also occur within three or four rounds, an\ninteresting trend emerges in the ML-1M dataset: both shorter and\nlonger processes tend to be more successful in reaching the target\nitem. One interpretation is that shorter rounds signify tasks that are\nmore straightforward for the surrogate user Ë†ğ‘¢. But for users with\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Yuyue Zhao et al.\n1 2 3 4 5 6\nRound\n0\n25\n50\n75\n100\n125Count\nHis Round\nHit Round\nHit/His Ratio\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nHit/His Ratio\n(a) ML-1M.\n1 2 3 4 5 6\nRound\n0\n20\n40\n60\n80\n100Count\nHis Round\nHit Round\nHit/His Ratio\n0.0\n0.1\n0.2\n0.3\nHit/His Ratio\n(b) Amazon-Book.\nFigure 5: Distribution of termination rounds for ToolRec.\nâ€œHis Roundâ€ indicates the distribution of termination rounds\nfor all users, while â€œHit Roundâ€ highlights the termination\nround where the recommended list accurately contains the\nuserâ€™s target item.\ndiverse interests or nuanced tastes, Ë†ğ‘¢might need additional rounds\nto gather more information and determine if the recommendation\nprocess has been satisfactorily completed.\n4.3.3 Efficiency and scalability of attribute-oriented retrieval tools.\nAs elaborated in Section 3.3, the attribute-oriented retrieval tools\nare designed to adeptly follow diverse attribute choices. Figure 6\ncompares the number of trainable parameters and FLOPs on the ML-\n1M and Amazon-Book datasets. Here, â€˜forzen+ğ‘1â€™ represents our\nattribute-oriented retrieval tool with frozen backbone parameters,\nwhile â€˜full+ğ‘1â€™ denotes its full fine-tuning variant.\nWe have the following observations: (i) The SASRecğµğ¸ğ‘…ğ‘‡ model\nis considerably larger than the original SASRec. This increase in\nsize can be attributed to the enriched semantic information present\nin the BERT embedding. (ii) The trainable parameter count for\nâ€˜full+ğ‘1â€™ aligns closely with that of SASRec. Essentially, this is akin\nto training an entirely new model. Such an approach becomes\nimpractical as the number of attributes escalates. (iii) While both\nâ€˜forzen+ğ‘1â€™ and â€˜full+ğ‘1â€™ exhibit identical FLOPs, the former boasts\na significantly reduced count of trainable parameters compared\nto the latter. Moreover, since our â€˜forzen+ğ‘1â€™ employs the shared\nbackbone of SASRec, storage is primarily reserved for the additive\nparameters from the attribute-specific encoder and the dense layer.\n4.4 Surprises and Limitations (RQ3)\nToolRec benefits from LLMs to reason about usersâ€™ preferences\nacross attributes, enabling multi-round interaction processes and\nimproved recommendations.\nSASRec SASRecBERT full+a1 forzen+a1\n0\n2\n4\n6Trainable Parameters\n1e6\nTrainable Parameters\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFLOPs\n1e8\nFLOPs\n(a) ML-1M.\nSASRec SASRecBERT full+a1 forzen+a1\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25Trainable Parameters\n1e7\nTrainable Parameters\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFLOPs\n1e8\nFLOPs\n(b) Amazon-Book.\nFigure 6: Comparison of trainable parameters and FLOPs for\nvarious retrieval model configurations.\nHowever, language models are not inherently designed for rec-\nommendation tasks. Below, we discuss the failures and limitations\nthis gives rise to.\n4.4.1 Unanticipated outcomes: Beyond conventional failures. For\na rigorous comparison with existing baselines, we employed the\nMemory strategy (cf. Section 3.4). This strategy ensures that re-\nturned items align with the dataset directory, prompting a re-run if\ndiscrepancies arise.\nHowever, beyond the traditional recommendation paradigm,\nsome samples that are typically classified as â€œfailuresâ€ deserve a\nsecond look.\nFigure 7 shows an example. At Thought 5 , when our LLM-based\nsurrogate user, Ë†ğ‘¢, is unsatisfied with the retrieved movies, it typi-\ncally evaluates the unsatisfactory attributes and uses external tools\nto uncover additional options, thereby refining the recommenda-\ntion.\nYet, in this scenario, rather than using external tools or settling\nfor the current list of candidates,Ë†ğ‘¢decides to suggest â€œsome popular\nmoviesâ€, providing a top-ğ‘ recommendation on its own. Since we\nnever trained the LLM on any of our recommendation datasets, it is\nevident that not all of the movies it suggests are found in the ML-1M\ndataset. As a result, this action is labeled as a â€œfailureâ€, prompting a\nre-run. However, from another perspective, this could be viewed as\na successful recommendation; the only limitation is our inability to\nevaluate it within the current dataset constraints.\n4.4.2 Influence of LLM selection on recommendation performance.\nFollowing the experimental setup described in Section 4.1, we re-\nplace the base LLM with two alternative LLMs: Vicuna1.5-13B-\n16k [5] (Vicuna in short), an open-source chatbot fine-tuned on\nLet Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nThought ğŸ: To recommend movies to the user, I need to use a conventional recommender system and retrieve 15items based on genre condition. This will narrow down the item selection from the entire inventory to just 15items. Action ğŸ: Retrieval[genre,15]Observation ğŸ: [ItemList]\nThought ğŸ“: The retrieved movies based on popularitystill do not align closelywith the userâ€˜s historical films. Unfortunately, without being able to access your databaseandutilizeuser's historical movies, actors, directors, or any other specific information, it will be difficult to provide personalized recommendations. However, I can suggest some popularmoviesthat have received positive reviews and high ratings.Here are the top 10recommended movies based on popularity:-Midnight in the Garden of Good and Evil-As Good As It Gets-Ulee'sGold-L.A. Confidential\nUID_276\nâ€¦â€¦\nâ€¦â€¦\nFigure 7: Case study of parsing errors in output samples.\nHighlighted samples have been manually verified to exist in\nthe real world.\nSASRec Vicuna PaLM ChatGPT\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30Recall\nRecall\nNDCG\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nNDCG\n(a) ML-1M.\nSASRec Vicuna PaLM ChatGPT\n0.00\n0.01\n0.01\n0.01\n0.02\n0.03\n0.03Recall\nRecall\nNDCG\n0.000\n0.005\n0.010\n0.015\n0.020\nNDCG\n(b) Yelp2018.\nFigure 8: Ranking performance across different LLMs.\nLlama2 [40]; and PaLM [6], a commercial LLM developed by Google\nAI. However, both variants yielded subpar outcomes, either fail-\ning to adhere to task instructions, misconstruing tool usage, or\nmisinterpreting user preferences (some representative failures are\ncataloged in the Appendix). Notably, even though Vicuna and PaLM\nuse the same prompt template as ChatGPT, they are unable to con-\nsistently generate recommendations across all test users, despite\nseveral attempts. This suggests that ChatGPT has better reasoning\ncapabilities than both Vicuna and PaLM.\nBeyond reasoning, a ranking experiment was conducted (cf. Sec-\ntion 4.3.1, w/ multi) to examine the open-world knowledge of var-\nious LLMs. As revealed in Figure 8, on datasets like ML-1M (and\nsimilar outcomes on Amazon-Book), both Vicuna and PaLM under-\nperformed SASRec, whereas ChatGPT exhibited superior results.\nThese findings align with insights presented in LLMRank [16]. How-\never, all three models lagged behind SASRec on the Yelp2018 dataset,\nsuggesting a possible limitation of LLMs in contexts like local busi-\nnesses, where open-world knowledge is limited and of more limited\nuse than on the other datasets.\n5 CONCLUSION\nIn this work, we zoomed in on the tool-learning capacities of LLMs,\nusing them as controllers to guide the exploration of item spaces\nin a recommendation scenario. Specifically, by treating LLMs as\nsurrogate users, they can adeptly capture the nuances of a cur-\nrent context alongside user preferences. Subsequently, we employ\nattribute-oriented tools for precise item retrieval. We developed\ntwo types of attribute-oriented tools: rank tools and retrieval tools,\neach fetching the corresponding candidate items. To enhance accu-\nrate item retrieval, items that appear in the process are verified and\nstored using the memory strategy. Extensive experiments on real-\nworld datasets rich in knowledge demonstrate the effectiveness and\nrationality of ToolRec.\nThe idea of using LLMs for simulation, either under the hood as\nin our approach or for counterfactual explorations while interacting\nwith users, holds great potential for combining the strengths of\nLLMs and recommendation models. In the short term, companies\noperating their own recommender systems may find it impractical\nto switch to LLM-based RSs. However, ToolRec could enhance rec-\nommendation performance by integrating LLMs with their current\nsystems. In the long-term, users are increasingly relying on LLMs\nfor various daily tasks, including recommendations. ToolRec does\nnot require extensive fine-tuning of the LLM, which can lead to\nadditional costs and potential delays due to outdated information.\nFurthermore, the results from ToolRec are more reliable than those\nfrom zero-shot or few-shot LLM recommendations, as they are\naugmented by traditional recommendation system outputs.\nAchieving strong recommendation performance hinges on a\ndataset with rich semantic knowledge and the robust capabilities of\nthe LLM. In future work, we plan to incorporate recommendation\nknowledge into LLMs to enhance domain-specific tool learning,\nsuch as retrieval-augmented generation or fine-tuning LLMs, po-\ntentially reducing the reliance on rich semantic knowledge. Ad-\nditionally, we intend to explore different types of tools, including\nsearch engines and databases, along with a self-reflection strategy,\nto achieve even more personalized recommendations.\nAcknowledgments. This research is supported by the National\nScience and Technology Major Project (2023ZD0121102), National\nNatural Science Foundation of China (92270114, 62302321), the\nDutch Research Council (NWO), under project numbers 024.004.022,\nNWA.1389.20.183, and KICH3.LTP.20.006, and the European Unionâ€™s\nHorizon Europe program under grant agreement No 101070212.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Yuyue Zhao et al.\nREFERENCES\n[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova\nDasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas\nJoseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson\nElhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston,\nShauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared\nKaplan. 2022. Training a Helpful and Harmless Assistant with Reinforcement\nLearning from Human Feedback.\n[2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan\nWilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of\nChatGPT on Reasoning, Hallucination, and Interactivity. In IJCNLP (1) . 675â€“718.\n[3] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He.\n2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large\nLanguage Model with Recommendation. In RecSys. 1007â€“1014.\n[4] Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le Song. 2019.\nGenerative Adversarial User Model for Reinforcement Learning Based Recom-\nmendation System. In ICML. 1052â€“1061.\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,\nand Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\n90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling\nwith Pathways. J. Mach. Learn. Res. 24 (2023), 240:1â€“240:113.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT (1). 4171â€“4186.\n[8] Luke Friedman, Sameer Ahuja, David Allen, Zhenning Tan, Hakim Sidahmed,\nChangbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, Brian Chu,\nZexi Chen, and Manoj Tiwari. 2023. Leveraging Large Language Models in\nConversational Recommender Systems. CoRR abs/2305.07961 (2023).\n[9] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang\nLei, Biao Li, Yuan Zhang, and Peng Jiang. 2024. CIRS: Bursting Filter Bubbles\nby Counterfactual Interactive Recommender System. ACM Trans. Inf. Syst. 42, 1\n(2024), 14:1â€“14:27.\n[10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei\nZhang. 2023. Chat-REC: Towards Interactive and Explainable LLMs-Augmented\nRecommender System. CoRR abs/2303.14524 (2023).\n[11] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong\nLi, and Yongfeng Zhang. 2023. OpenAGI: When LLM Meets Domain Experts. In\nNeurIPS.\n[12] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as Language Processing (RLP): A Unified Pretrain, Personalized\nPrompt & Predict Paradigm (P5). In RecSys. 299â€“315.\n[13] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.\nVIP5: Towards Multimodal Foundation Models for Recommendation. In EMNLP\n(Findings). 9606â€“9620.\n[14] Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong,\nand Qing He. 2020. A survey on Knowledge Graph-based Recommender Systems.\nIEEE Trans. Knowl. Data Eng. 34, 8 (2020), 3549â€“3568.\n[15] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network\nfor Recommendation. In SIGIR. 639â€“648.\n[16] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley,\nand Wayne Xin Zhao. 2024. Large Language Models are Zero-Shot Rankers for\nRecommender Systems. In ECIR (2) , Vol. 14609. 364â€“381.\n[17] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.\n2023. ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.\nCoRR abs/2306.03901 (2023).\n[18] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In ICLR.\n[19] Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, Zelong Li, and Yongfeng\nZhang. 2024. UP5: Unbiased Foundation Model for Fairness-aware Recommenda-\ntion. In EACL (1). 1899â€“1912.\n[20] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How to\nIndex Item IDs for Recommendation Foundation Models. In SIGIR-AP. 195â€“204.\n[21] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. 2023.\nRecommender AI Agent: Integrating Large Language Models for Interactive\nRecommendations. CoRR abs/2308.16505 (2023).\n[22] Wang-Cheng Kang and Julian J. McAuley. 2018. Self-Attentive Sequential Rec-\nommendation. In ICDM. 197â€“206.\n[23] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Rat-\nner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay,\nNoam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua,\nand Moshe Tennenholtz. 2022. MRKL Systems: A modular, neuro-symbolic archi-\ntecture that combines large language models, external knowledge sources and\ndiscrete reasoning. CoRR abs/2205.00445 (2022).\n[24] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive\nBenchmark for Tool-Augmented LLMs. In EMNLP. 3102â€“3116.\n[25] Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang,\nand Xiangnan He. 2024. Large Language-Recommendation Assistant. In SIGIR.\n[26] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu\nZhu, Huifeng Guo, Yong Yu, Ruiming Tang, and Weinan Zhang. 2023. How Can\nRecommender Systems Benefit from Large Language Models: A Survey. CoRR\nabs/2306.05817 (2023).\n[27] Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large Language\nModel Augmented Narrative Driven Recommendations. RecSys (2023), 777â€“783.\n[28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Train-\ning language models to follow instructions with human feedback. In NeurIPS.\n[29] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove:\nGlobal Vectors for Word Representation. InEMNLP. ACL, 1532â€“1543.\n[30] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni\nZeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong\nWang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen,\nBokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu,\nZhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi\nYan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu,\nHeng Ji, Zhiyuan Liu, and Maosong Sun. 2023. Tool Learning with Foundation\nModels.\n[31] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin,\nXin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie\nZhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM:\nFacilitating Large Language Models to Master 16000+ Real-world APIs. CoRR\nabs/2307.16789 (2023).\n[32] Zhaopeng Qiu, Xian Wu, Jingyue Gao, and Wei Fan. 2021. U-BERT: Pre-training\nUser Representations for Improved Recommendation. In AAAI, Vol. 35. 4320â€“\n4327.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer.J. Mach.\nLearn. Res. 21 (2020), 140:1â€“140:67.\n[34] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI. 452â€“\n461.\n[35] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli,\nEric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\nToolformer: Language Models Can Teach Themselves to Use Tools. InNeurIPS.\n[36] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting\nZhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in\nHugging Face. In NeurIPS.\n[37] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Repre-\nsentations from Transformer. In CIKM. ACM, 1441â€“1450.\n[38] DÃ­dac SurÃ­s, Sachit Menon, and Carl Vondrick. 2023. ViperGPT: Visual Inference\nvia Python Execution for Reasoning. In ICCV. 11854â€“11864.\n[39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lam-\nple. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR\nabs/2302.13971 (2023).\n[40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucu-\nrull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLet Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023).\n[41] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using\nLarge Pretrained Language Models. CoRR abs/2304.03153 (2023).\n[42] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei,\nand Jirong Wen. 2024. A survey on large language model based autonomous\nagents. Frontiers Comput. Sci. 18, 6 (2024), 186345.\n[43] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:\nKnowledge Graph Attention Network for Recommendation. InSIGKDD. 950â€“958.\n[44] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho,\nXing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. 2023. RecMind: Large\nLanguage Model Powered Agent For Recommendation. CoRR abs/2308.14296\n(2023).\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei\nXia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting\nElicits Reasoning in Large Language Models. In NeurIPS.\n[46] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering\nNews Recommendation with Pre-trained Language Models. InSIGIR. 1652â€“1656.\n[47] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun Lian,\nand Xing Xie. 2021. Self-supervised Graph Learning for Recommendation. In\nSIGIR. 726â€“735.\n[48] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,\nand Xiangnan He. 2024. On the Effectiveness of Sampled Softmax Loss for Item\nRecommendation. ACM Trans. Inf. Syst. 42, 4 (2024).\n[49] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen,\nChuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. 2023.\nA Survey on Large Language Models for Recommendation. CoRR abs/2305.19860\n(2023).\n[50] Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang,\nWeinan Zhang, Rui Zhang, and Yong Yu. 2023. Towards Open-World Recom-\nmendation with Knowledge Augmentation from Large Language Models. CoRR\nabs/2306.10933 (2023).\n[51] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023. OpenP5: Benchmarking\nFoundation Models for Recommendation. CoRR abs/2306.11134 (2023).\n[52] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal\nAhmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. MM-REACT:\nPrompting ChatGPT for Multimodal Reasoning and Action. CoRR abs/2303.11381\n(2023).\n[53] Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang,\nXiang Wang, and Xiangnan He. 2023. Large Language Model Can Interpret\nLatent Space of Sequential Recommender. CoRR abs/2310.20487 (2023).\n[54] Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and\nXiangnan He. 2023. Generate What You Prefer: Reshaping Sequential Recom-\nmendation via Guided Diffusion. In NeurIPS.\n[55] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\nModels. In ICLR.\n[56] An Zhang, Yuxin Chen, Leheng Sheng, Xiang Wang, and Tat-Seng Chua. 2024.\nOn Generative Agents in Recommendation. In SIGIR.\n[57] Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian J. McAuley, Wayne Xin\nZhao, Leyu Lin, and Ji-Rong Wen. 2023. AgentCF: Collaborative Learning with\nAutonomous Language Agents for Recommender Systems. CoRR abs/2310.09233\n(2023).\n[58] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong\nWen. 2023. Recommendation as Instruction Following: A Large Language Model\nEmpowered Recommendation Approach. CoRR abs/2305.07001 (2023).\n[59] Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang,\nand Xiuqiang He. 2021. UNBERT: User-News Matching BERT for News Recom-\nmendation.. In IJCAI. 3356â€“3362.\n[60] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Mem-\noryBank: Enhancing Large Language Models with Long-Term Memory. In AAAI.\n19724â€“19731.\n[61] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao\nHuang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023. Ghost in the Minecraft:\nGenerally Capable Agents for Open-World Enviroments via Large Language\nModels with Text-based Knowledge and Memory. CoRR abs/2305.17144 (2023).\n[62] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-\nlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large Language Models for\nInformation Retrieval: A Survey. CoRR abs/2308.07107 (2023).\n[63] Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, Shuaiqiang\nWang, Daiting Shi, Zhicong Cheng, and Dawei Yin. 2021. Pre-trained Language\nModel based Ranking in Baidu Search. In KDD. ACM, 4014â€“4022.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6662686467170715
    },
    {
      "name": "Knowledge management",
      "score": 0.3604409098625183
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.32465723156929016
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114017466",
      "name": "University of Technology Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I4210137491",
      "name": "National Science Centre",
      "country": "PL"
    }
  ],
  "cited_by": 26
}