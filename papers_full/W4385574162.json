{
  "title": "Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations",
  "url": "https://openalex.org/W4385574162",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100331867",
      "name": "Yu Fei",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5113748101",
      "name": "Zhao Meng",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5053562090",
      "name": "Ping Nie",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5078339613",
      "name": "Roger Wattenhofer",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A5002316432",
      "name": "Mrinmaya Sachan",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W3171153522",
    "https://openalex.org/W2108281845",
    "https://openalex.org/W3166913490",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W3215552966",
    "https://openalex.org/W4385567015",
    "https://openalex.org/W3173233831",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2891575196",
    "https://openalex.org/W3197810337",
    "https://openalex.org/W2014902591",
    "https://openalex.org/W2576754561",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2152886525",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3202082017",
    "https://openalex.org/W2061873838",
    "https://openalex.org/W4285286749",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W3099045991",
    "https://openalex.org/W2094644779",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287028759",
    "https://openalex.org/W3034588688",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W4309444617",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W1601795611",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286953959"
  ],
  "abstract": "Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian Gaussian Mixture Model after initializing cluster positions and shapes using class names. Despite its simplicity, this approach achieves superior or comparable performance on both topic and sentiment classification datasets and outperforms prior works significantly on unbalanced datasets. We further explore the applicability of our clustering approach by evaluating it on 14 datasets with more diverse topics, text lengths, and numbers of classes. Our approach achieves an average of 20% absolute improvement over prompt-based zero-shot learning. Finally, we compare different PLM embedding spaces and find that texts are well-clustered by topics even if the PLM is not explicitly pre-trained to generate meaningful sentence embeddings. This work indicates that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero-shot learning ability.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8560–8579\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nBeyond prompting: Making Pre-trained Language Models\nBetter Zero-shot Learners by Clustering Representations\nYu Fei1, Zhao Meng∗1, Ping Nie∗2, Roger Wattenhofer1, Mrinmaya Sachan1\n1ETH Zurich, 2Peking University\nfeiyuwalter@gmail.com, {zhmeng, wattenhofer}@ethz.ch,\nping.nie@pku.edu.cn, mrinmaya.sachan@inf.ethz.ch\nAbstract\nRecent work has demonstrated that pre-trained\nlanguage models (PLMs) are zero-shot learn-\ners. However, most existing zero-shot methods\ninvolve heavy human engineering or compli-\ncated self-training pipelines, hindering their ap-\nplication to new situations. In this work, we\nshow that zero-shot text classification can be\nimproved simply by clustering texts in the em-\nbedding spaces of PLMs. Specifically, we fit\nthe unlabeled texts with a Bayesian Gaussian\nMixture Model after initializing cluster posi-\ntions and shapes using class names. Despite\nits simplicity, this approach achieves superior\nor comparable performance on both topic and\nsentiment classification datasets and outper-\nforms prior works significantly on unbalanced\ndatasets. We further explore the applicability of\nour clustering approach by evaluating it on 14\ndatasets with more diverse topics, text lengths,\nand numbers of classes. Our approach achieves\nan average of 20% absolute improvement over\nprompt-based zero-shot learning. Finally, we\ncompare different PLM embedding spaces and\nfind that texts are well-clustered by topics even\nif the PLM is not explicitly pre-trained to gen-\nerate meaningful sentence embeddings. This\nwork indicates that PLM embeddings can cat-\negorize texts without task-specific fine-tuning,\nthus providing a new way to analyze and utilize\ntheir knowledge and zero-shot learning ability1.\n1 Introduction\nRecent developments in large pre-trained language\nmodels (PLMs) (Devlin et al., 2019; Liu et al.,\n2019; Raffel et al., 2020a) open up the possibil-\nity of classifying texts without massive in-task data\nannotation. Such a zero-shot setting is receiving\nincreasing attention as it is a good way to eval-\nuate the generalizability of knowledge in PLMs.\n*Equal contribution.\n1Code and datasets available at: https://github.com/\nfywalter/simptc\nCurrently, most existing methods either utilize key-\nwords for self-training (Chang et al., 2008; Meng\net al., 2018; Wang et al., 2021) or reformulate the\nclassification task into a cloze task using prompts\n(Brown et al., 2020; Schick and Schütze, 2021; Gao\net al., 2021a). Keyword-based methods usually\ntrain multiple modules sequentially (Meng et al.,\n2020b), while prompting methods depend heavily\non human engineering (Liu et al., 2021) or external\nknowledge (Hu et al., 2021). Such task-specific\ntraining or engineering is inefficient and usually\ndoes not generalize well to new applications.\nIn this work, we show that we can better elicit the\nzero-shot text classification abilities of PLMs sim-\nply by clustering texts in their embedding spaces.\nWe draw inspiration from recent findings (Aha-\nroni and Goldberg, 2020) that texts in the same\ndomain (e.g., legal or medical texts) tend to be clus-\ntered together in the PLM embedding spaces. This\nindicates that PLMs already have the knowledge\nto distinguish texts with different meanings. Fol-\nlowing this idea, we propose SimPTC: A Simple\nProbabilistic Text Classification framework build-\ning upon state-of-the-art sentence embeddings Sim-\nCSE (Gao et al., 2021b). Given an unlabeled\ndataset and the corresponding class names, SimPTC\nmodels the texts in each class with a Gaussian\ndistribution and fits the text embeddings with a\nBayesian Gaussian Mixture Model (BGMM). To\ninitialize the clusters, we first use the class names\nto generate class-related anchor sentences. Then\nthe initial cluster assignment of a text is determined\naccording to its similarity to the class anchors in\nthe embedding space.\nDespite the simplicity of SimPTC, it achieves\nstate-of-the-art performance while avoiding many\npreviously mentioned drawbacks of existing meth-\nods: 1) Without self-training of the PLM, SimPTC\nachieves superior or comparable performance on\nboth topic and sentiment classification datasets; 2)\nUnlike prompt-based methods, SimPTC works well\n8560\nwithout human engineering or access to external\nknowledge; 3) SimPTC outperforms previous meth-\nods when the dataset is unbalanced. Finally, once\nwe obtain the sentence embeddings, we no longer\nuse the PLM, and SimPTC clusters the embeddings\nin a fixed dimensional space. Thus, one can easily\napply SimPTC to new and large datasets.\nTo explore the applications and limitations of\nSimPTC, we compare it with prompt-based zero-\nshot learning (Schick and Schütze, 2021) on 14\ndatasets with more diverse topics, text lengths, and\nnumbers of classes. SimPTC gives consistently bet-\nter performance with a 20% absolute improvement\nin macro-F1 score on average. We find thatSimPTC\nhandles domain-specific rare class names and large\nclass numbers better, while both the prompt-based\nmethod and SimPTC suffer when the class names\nare abstract concepts, e.g., subjective v.s. objective.\nFinally, we analyze the embedding spaces of dif-\nferent PLMs using SimPTC. Surprisingly, although\nRoBERTalarge (Liu et al., 2019) is not explicitly\npre-trained to generate meaningful sentence em-\nbeddings, texts of the same topic are clustered with\nstate-of-the-art zero-shot accuracy. A Larger PLM\nlike T5 (Raffel et al., 2020b) is able to achieve\nbetter zero-shot results, even matching the fully\nsupervised performance of BERT (Devlin et al.,\n2019) on some datasets. On the other hand, Sim-\nCSE embeddings separate topics better, and texts of\nsub-topics can form sub-clusters. On some datasets,\nwe can even observe a linear semantic structure.\nTo conclude, the strong performance of such a\nsimple clustering-based algorithm suggests that the\nzero-shot learning ability of PLMs is still under-\nexplored. With SimPTC, we provide a new starting\npoint to utilize and analyze the implicit knowledge\nand zero-shot learning ability of PLMs.\n2 Related Work\nIn this section, we review three types of zero-shot\ntext classification approaches. Zero-shot text clas-\nsification aims at classifying texts without any an-\nnotated data. This is also referred to as weakly-\nsupervised text classification as it can use various\nweak supervision signals, such as the names or\ndescriptions of the classes, to make predictions.\nKeyword-driven methods The most common\nsupervision signal is keywords (Chang et al., 2008;\nMekala and Shang, 2020). Meng et al. (2018,\n2020b) use iterative self-training on unlabeled in-\ntask data to refine the model or keyword sets. Wang\net al. (2021) learn document representations that\nalign with the classes. Zhang et al. (2021b) build\na keyword graph to take the connections between\nkeywords into account. Unlike these approaches,\nSimPTC contains no model training or keyword re-\nfinement process and depends solely on the sen-\ntence embedding spaces of PLMs.\nClustering-based methods Early clustering-\nbased methods work with discrete text represen-\ntations such as TF-IDF (Zeng et al., 2003) or bag-\nof-words (Kyriakopoulou and Kalamboukis, 2006).\nRecently, ULR (Chu et al., 2021) has explored\nclustering-based text classification with contextual-\nized sentence embeddings. However, ULR requires\nfine-tuning the PLM on extra task-related data and\nuses a heuristic regularization. The K-Means-based\napproach also places a strong spherical assumption\non the cluster shapes. In this work, we show that\nneither the task-relevant pre-training nor the heuris-\ntic designs are necessary. The original embedding\nspaces of PLMs are sufficient to give strong results\nwith a more flexible clustering algorithm. Neverthe-\nless, it is possible to utilize unsupervised learning\nto further improve the clustering quality of text rep-\nresentations like in Gupta et al. (2022) and Zhang\net al. (2021a). We leave this as a future direction.\nPrompt-based methods Prompt-based methods\nperform zero-shot learning in a natural way by\nmimicking human behaviors when solving NLP\ntasks (Brown et al., 2020). Many existing works\non prompts focus on text classification, where a\ntemplate is used to transform the classification task\ninto a cloze task, and a verbalizer maps the pre-\ndicted words into classification labels (Schick and\nSchütze, 2021). With carefully designed templates\nand verbalizers, prompt-based methods can per-\nform comparably to supervised methods in text\nclassification. Various methods have been explored\nfor designing templates (Gao et al., 2021a; Qin and\nEisner, 2021) and verbalizers (Cui et al., 2022).\nOther researchers leverage external knowledge. Hu\net al. (2021) expand label names with knowledge\nbases, and Chen et al. (2022) re-train PLMs by\nadaptively retrieving extra data.\nSimPTC shares the idea of utilizing natural lan-\nguage templates and class names. Nevertheless,\ninstead of reformulating the classification task,\nSimPTC uses natural language templates and class\nnames to construct class-related texts, which are\nused to compute initial cluster positions and shapes\n8561\nSentence embedding space\nAverage\nE\nThe news is about politics. \nThe news is about society. \nThe news is about business. \nThe news is about company.\nAnchor sentences\nThe news is about sports. \nThe news is about swimming.\nAnchors\nE\nEU wants U.S. aid to ... \nIn China's changing society ... \nDollar rises vs Euro on ... \nGoogle shares bounce up ...\nTexts to be classified\nOlympic history for India ... \nFormer Florida swimming ...\nEmbeddings\nInitialize predictions Estimate means and covariance Update predictions\nShape indicates true label\nColor represents predictions\nAnchor vectors\nE Fixed pre-trained encoder\nEncode\nMatch Update\nFigure 1: An overview of SimPTC. Top: In the Encode step, all unlabeled texts and anchor sentences of each class\nare encoded using a PLM. Anchor sentences are constructed by combining a template with class names. The anchor\nsentence embeddings of the same class are averaged to get the final anchor vector. Bottom left: In the Match step,\nthe initial cluster assignments are determined based on the cosine similarity between text embeddings and anchor\nvectors. Bottom right: In the update step, we fit the unlabeled data with a BGMM starting from the initial clusters.\nfor the subsequent probabilistic clustering step.\n3 SimPTC\nAs illustrated in Figure 1,SimPTC formalizes a zero-\nshot text classification task into a clustering prob-\nlem and solves it in three steps: Encode, Match,\nand Update. We start by modeling each class with\na Gaussian cluster in the embedding space. Next,\nthe Encode step and Match step provide a coarse\ninitialization of the cluster means and covariances\nusing the class names. Finally, starting from the ini-\ntialization, we fit the unlabeled data with a BGMM.\nWe elaborate on the three steps of SimPTC below.\n3.1 Encode\nThe first step of SimPTC is to construct class anchor\nsentences by filling the class names expanded based\non external knowledge bases into natural language\ntemplates. Then we encode both the unlabeled\ntexts and the class anchor sentences into the PLM\nembedding space (Figure 1 top).\nExpanding class names To make the anchor sen-\ntences more class-indicative and less dependent\non the exact textual forms of the class names, we\nexpand the class names using external knowledge\nbases. Specifically, we use ConceptNet Number-\nbatch (Speer et al., 2017), a set of word embeddings\nwith semi-structured, common sense knowledge\nfrom ConceptNet (Speer et al., 2017) combining\nword2vec (Mikolov et al., 2013) and GloVe (Pen-\nnington et al., 2014). To extract M related words\ngiven a class name si, we simply choose the words\nwhose embeddings have top-M largest inner prod-\nucts with the embedding of si:\nSi = top-M\nx∈V\n(x⊤si),\nwhere Si is the expanded class name set of si; V\nis the vocabulary; bold font denotes word embed-\ndings. Words that appeared in multiple Si’s are\ndeleted. If m >1 class names are given for one\nclass, for each name we extract M/mwords. See\nAppendix A for extracted word examples.\nConstructing anchor sentences We take the\nidea of using natural language templates from\nprompt-based methods (Schick and Schütze, 2021)\nto construct anchor sentences. A template is a piece\nof text containing one or multiple special tokens\nto be filled in, such as “The text is about ⟨mask⟩.”.\nBy replacing the ⟨mask⟩token with the expanded\nclass names si ∈Si, we get a set of class-related\nsentences. Unlike prompt-based methods, we al-\nlow class names with multiple tokens. The anchor\nembeddings of the same class are averaged to give\nthe final anchor vector (Figure 1 top middle).\n8562\nFigure 2: 2D PCA visualization of Amazon dataset in\nthe SimCSE embedding space. The texts of different\nclasses are well-clustered, and the class anchors from\nthe Encode step reflect the relative positions of text\nclusters. (Figure 1 bottom left).\n3.2 Match\nLet {xi,x2,..., xN}be the embeddings of a set\nof unlabeled texts of size N, and {a1,a2,..., aK}\nbe the averaged anchor vectors for Kclasses. The\npseudo-label ˆyi of xi are determined by:\nˆyi = arg max\nj∈[K]\ncos-sim(xi,aj). (1)\n{ˆyi}are then used to compute the initial cluster\nmeans and covariances. We call this pseudo-label-\ngenerating process Encode&Match (E&M).\nFigure 2 illustrates the encoded anchor vec-\ntors ai’s and example vectors xi’s after perform-\ning PCA. The anchor vectors ai’s indeed reflect\nthe relative positions of the clusters. To pro-\nvide more insights, we conduct a pilot experi-\nment on AG’s News (Zhang et al., 2015) dataset .\nWe show the zero-shot performance of E&M and\nVanilla Prompting , the vanilla prompt-based\nzero-shot text classification method used in Schick\nand Schütze (2021), in Table 1 2. For a fair com-\nparison, we use the original class names directly to\nconstruct anchor sentences for E&M. E&M provides a\ncompetitive initialization and is more stable across\ndifferent choices of natural language templates.\n3.3 Update\nModel classes with Gaussian clusters To cap-\nture the position and shape characteristics of text\nclusters, we model the texts of the same class with\na Gaussian in the embedding space and define a\nGaussian Mixture Model (GMM). Then the likeli-\nhood of the dataset is given by\np(X|θ) =\nN∏\nn=1\nK∑\ni=1\nπiN(xn|µi,Σi),\n2Vanilla Prompting results are based on BERT large,\nand E&M uses SimCSE supervised BERTlarge\nTemplate Acc\nVanilla Prompting\n⟨text⟩A ⟨mask⟩news . 31.5\n⟨text⟩[class: ⟨mask⟩] 70.3\n⟨text⟩This text is about ⟨mask⟩. 68.7\nEncode&Match\nA ⟨mask⟩news. 78.9\n[class: ⟨mask⟩] 76.8\nThis text is about ⟨mask⟩. 78.2\nTable 1: Accuracy of Vanilla Prompting and\nEncode&Match with different templates on AG’s News\ntest set. Encode&Match depends less on the choice of\ntemplate and gives better performance.\nwhere θ = (π,µ,Σ) denotes the model parame-\nters; π = {π1,...,π K}, µ = {µ1,..., µK}, and\nΣ = {Σ1,..., ΣK}are the priors, means, and\ncovariances of each component respectively. We\ncan further require all components to share the\nsame covariance matrix to add extra regularization\nwhen the data is sparse, or we have additional prior\nknowledge that the clusters have similar shapes.\nVariational update Clustering in a high dimen-\nsional space can be challenging, for instance, when\nthe data is limited, or the initialization is poor. One\nsimple solution is to inject prior knowledge, such\nas assuming a uniform prior on the classes as in\nseveral prompt-based methods (Zhao et al., 2021;\nHu et al., 2021). However, debiasing model ex-\nplicitly can be harmful when the prior is incorrect.\nTo balance injecting prior knowledge and fitting\nthe data, we turn to the Bayesian approaches and\nintroduce prior distributions on model parameters.\nWe choose a Dirichlet distribution as the prior for\nmixture weights π to favor balanced weights:\np(π) = Dir(π|α0) = C(α0)\n∏\nk\nπα0−1\nk\nwhere C(α0) is a normalizing constant, and α0\ncan be interpreted as the prior number of observa-\ntions associated with each class. We simply choose\nα0 = N/K. For the means and covariances, we\nchoose a non-informative Gaussian-Wishart prior\n(see Appendix C for details). Then we update the\nmodel with the standard variational optimization\n(Bishop and Nasrabadi, 2006). As BGMM is guar-\nanteed to converge (Boyd et al., 2004), we stop up-\ndating when the model predictions stop changing\nor the maximum number of iterations is reached.\nThe overall SimPTC algorithm is summarized\nin Algorithm 1. Note that, in general, we can\n8563\nreplace Encode&Match with any initialization\nmethod and Bayesian GMM with any clustering\nalgorithm (see discussion in §4.2).\nAlgorithm 1: SimPTC\nInput: unlabeled texts U; test texts Utest;\nclass names S; sentence encoder E;\nmax iteration T\nOutput: The prediction of Utest\nX ←E(U);\nXtest ←E(Utest);\n{ˆyi}← Encode&Match (§3.1 and §3.2);\nM ←BayesianGMM(\ninitial predictions ←{ˆyi},\nweight prior α0 ←|U|/|S|,\nmean & cov prior ←Eq. (2) in App. C,\nmax iter ←T,\n);\nFit M with X;\n{ytest\ni }← prediction of M on Xtest;\nReturn {ytest\ni }\n4 Experiments\nWe conduct extensive experiments to understand\nSimPTC. We compare SimPTC with state-of-the-art\nzero-shot text classification methods in §4.1, study\nthe effect of its components in §4.2, and explore\nits applications and limitations on a wide range\nof tasks in §4.3. For all experiments, we use the\nSimCSE supervised RoBERTalarge embeddings,\nwhich are in R1024 and trained using NLI datasets\nvia contrastive learning starting from the original\nRoBERTalarge model. We discuss and analyze\nother PLMs, such as T5 in §4.4.\n4.1 Comparison with State-of-the-art\nWe evaluate the zero-shot text classification perfor-\nmance of SimPTC on five benchmark datasets.\nDatasets We use three topic datasets: AG’s News\n(Zhang et al., 2015), DBpedia (Lehmann et al.,\n2015), and Yahoo (Zhang et al., 2015), and two\nsentiment datasets: IMDb (Maas et al., 2011) and\nAmazon (McAuley and Leskovec, 2013). The full\ndataset statistics can be found in Appendix D.\nImplementations Following Hu et al. (2021),\nwe manually design four templates (Appendix B)\nfor every dataset. The number of extracted class-\nrelated words for each class is 1000. We fit the\nBGMM with both the unlabeled train and test data.\nFor topic datasets, each Gaussian has its individual\ncovariance. For sentiment datasets, all Gaussians\nshare the same covariance to provide extra regular-\nization as the data is relatively sparse. The max-\nimum iterations are set empirically based on the\nsize of unlabeled data. See Appendix D for details.\nBaselines We compare SimPTC with the follow-\ning methods. Vanilla Prompting is the vanilla\nprompt-based zero-shot text classification without\nself-training used in Schick and Schütze (2021).\nWe use the original class names and templates de-\nsigned by Hu et al. (2021) for predicting. ULR\n(Chu et al., 2021) performs zero-shot text classi-\nfication by clustering data using K-Means with\na heuristic regularization. Since ULR originally\nuses an encoder pre-trained with extra in-domain\ndata, we evaluate ULR with the same embeddings\nused by SimPTC. LOTCLass (Meng et al., 2020b)\nis a state-of-the-art keyword-based method that in-\nvolves training multiple models with multiple tasks\nsequentially. KPT (Hu et al., 2021) is the state-of-\nthe-art prompt-based method that utilizes external\nknowledge bases and contextualized calibration to\nproduce stable zero-shot predictions.\nExperimental Design We conduct experiments\nto evaluate the following three claims:\nC1: SimPTC achieves superior or compara-\nble performance on both topic and sentiment\ndatasets. Table 2 reports the accuracy on the test\nsets. We report the average scores with standard\ndeviations for methods using multiple natural lan-\nguage templates. Without fine-tuning the PLM,\nSimPTC presents superior or comparable perfor-\nmance on all datasets. On IMDb, KPT gets slightly\nbetter results (91.6 v.s. 91.0) but has a much larger\nstandard deviation (2.7 v.s. 0). Moreover, KPT im-\nproperly poses a balanced dataset assumption (Ap-\npendix H), which hurts model performance when\nthe dataset is unbalanced (see C3).\nC2: SimPTC gives stable predictions across\ndifferent templates. Compared to Vanilla\nPrompting, E&M gives a better or comparable per-\nformance on all datasets with much lower standard\ndeviations across different natural language tem-\nplates (Table 2). The observation holds even when\nwe compare E&M with the prompt-based method en-\nhanced with external knowledge (KPT). SimPTC fur-\nther reduces the standard deviations and improves\nperformance.\n8564\nMethod AG’s News DBPedia Yahoo Amazon IMDb\nVanilla Prompting 72.1 ±10.4 80 .9 ±2.3 40 .4 ±3.1 79 .7 ±10.8 81 .5 ±4.1\nULR (Chu et al., 2021) 80.1 79 .8 59 .6 92 .6 82 .4\nLOTCLass†(Meng et al., 2020b) 86.4 91 .1 fail 91.6 86 .5\nKPT†(Hu et al., 2021) 84.8 ±1.2 82 .2 ±5.4 61 .6 ±2.2 92 .8 ±1.2 91.6 ±2.7\nEncode&Match (E&M) 78.2 ±0.3 74 .4 ±1.6 58 .3 ±0.1 91 .2 ±0.1 85 .6 ±0.4\nSimPTC 86.9 ±0.3 93 .2 ±1.0 63 .9 ±0.1 93 .9 ±0.0 91 .0 ±0.0\n-class name expansion 87.6 ±0.5 92 .9 ±0.1 63 .7 ±0.1 93 .9 ±0.0 91 .0 ±0.0\n-manual templates 86.5 93 .3 62 .9 93 .9 91 .1\nTable 2: Zero-shot test accuracy on five benchmark datasets. †: We use the number reported in the original papers.\nIndentation means the configuration is modified based on the up-level indentation. The keyword-extracting module\nof LOTCLass fails on Yahoo.\n0.0 0.2 0.4 0.6 0.8\nRatio of two classes\n40\n60\n80Micro F1\n SimPTC\nEncode & Match\nKPT\nLOTCLass\n(a) Micro-F1 score\n0.0 0.2 0.4 0.6 0.8\nRatio of two classes\n40\n60\n80Macro F1\n SimPTC\nEncode & Match\nKPT\nLOTCLass (b) Macro-F1 score\nFigure 3: The micro-F1 and macro-F1 score plots of different methods on unbalanced IMDb datasets with different\nclass ratios. When the dataset is unbalanced, SimPTC consistently performs better, and the gain is substantial\nin extreme cases.\nC3: SimPTC consistently outperforms prior work\nwhen the classes in the dataset are unbalanced.\nCurrently, most benchmark datasets are balanced.\nOverfitting to this balanced bias reduces the gener-\nalizability of the method. To illustrate this problem,\nwe conduct the following experiment on IMDb.\nWe keep the texts of one class with a ratio vary-\ning from 0.01 to 0.9 to generate different unbal-\nanced settings, and we compare SimPTC with KPT\nand LOTCLass. KPT injects a balanced dataset as-\nsumption directly into its design (Appendix H), and\nLOTCLass is a self-training keyword-based method\nwithout an explicit balanced assumption. As shown\nin Figure 4, the performance of KPT and LOTClass\ndrops significantly as the dataset becomes more\nunbalanced, whereas SimPTC achieves consistently\nbetter performance. As the class ratio approaches\nzero, the micro-F1 score of KPT goes to 50 since\nthe balanced prior forces the model to make a bal-\nance prediction. Although LOTClass is purely data-\ndriven, the data imbalance still dramatically affects\nits self-training process. On the other hand, E&M\nprovides a strong starting point for SimPTC, and\nSimPTC further improves its performance.\nWe discuss the convergence ofSimPTC, the effect\nof unlabeled dataset size, and sharing covariance\nmatrix in Appendix E, F and G respectively.\n4.2 Ablations\nWe try to understand what contributes to the com-\npetitive performance of SimPTC by studying the im-\nportance of 1) the choice of natural language tem-\nplate and class names, 2) the initialization method,\nand 3) the clustering algorithm.\n4.2.1 Templates and Class Names\nSimPTC gives state-of-the-art results even with-\nout carefully designed templates or class names\nextracted using external knowledge. We first\nevaluate SimPTC using only the original class\nnames for constructing class anchor sentences (-\nclass name expansion in Table 2). SimPTC still\ngives a comparable performance on all datasets.\nThen we further test SimPTC with the naive tem-\nplate “ ⟨mask⟩” (-manual templates in Table 2).\nThe performance is again only slightly affected.\nUnlike prompt-based methods, which are sensi-\ntive to the quality of class names and templates,\n8565\nMethod AG DB YH AM IM\nVP 72.1 80 .9 40 .4 79 .7 81 .5\nE&M 78.2 74 .4 58 .3 91 .2 85 .6\nSimPTC+VP 86.7 92 .7 63 .4 93.9 91.0\nSimPTC+E&M 86.9 93.2 63.9 93.9 91.0\nTable 3: Comparison of different initialization methods.\nSimPTC is fairly robust to the quality of initialization.\nClustering Algo. AG DB YH AM IM\nK-Means 75.3 90 .5 61 .7 92 .1 88 .3\nGMM 76.4 82 .9 51 .6 93.9 89.4\nBGMM 86.9 93.2 63.9 93.9 91.0\nTable 4: Comparison of different clustering algorithms.\nBGMM outperforms K-Means, while GMM fails to\nwork on many-class tasks like DBpedia and Yahoo.\nSimPTC gives strong performance even without ex-\nternal knowledge or human engineering.\n4.2.2 Initialization Method\nSimPTC is robust to the quality of initialization.\nWe use E&M to initialize the clusters mainly be-\ncause E&M works directly with the text embeddings\ncomputed for later clustering, adding only mini-\nmal additional computations. In general, SimPTC\nworks with any initialization method (see Algo-\nrithm 1). As a comparison, we test using Vanilla\nPrompting(VP) as the initialization. We report the\nresults averaged over four templates on five bench-\nmarks in Table 3. Although VP gives a slightly\nworse initialization performance, SimPTC achieves\na similar performance after clustering, showing the\nrobustness of SimPTC to the initialization method.\n4.2.3 Clustering Algorithm\nIn this section, we aim to show what makes a good\nchoice of clustering algorithm for SimPTC by com-\nparing BGMM with K-Means and GMM.\nModeling cluster shapes is beneficial. As\nshown in Table 4, BGMM outperforms K-Means\non all five balanced benchmark datasets. This\nshows that putting a strong assumption on the clus-\nter shapes like K-Means limits the clustering step’s\nperformance. Since the SimCSE embedding space\nis rather well-structured, we further test SimPTC +\nK-Means with the original RoBERTalarge embed-\ndings. The performance on IMDb drops from 92.3\nto 54.1, indicating that BGMM is a more robust\nchoice for clustering PLM embeddings in general.\n0.0 0.2 0.4 0.6 0.8\nRatio of two classes\n40\n60\n80Macro F1\n Encode & Match\nBGMM\nGMM\nKMeans\nFigure 4: The macro-F1 score plots of different clus-\ntering algorithms on unbalanced IMDb datasets with\ndifferent class ratios. K-Means cannot handle unbal-\nanced datasets. BGMM and GMM perform better\nby allowing the cluster weights to adapt to the data,\nbut GMM is less stable in extreme cases.\nAdding prior on cluster weights helps on many-\nclass tasks. Following the previous observation,\nGMM outperforms K-Means on AG News, IMDb,\nand Amazon by allowing to model the cluster\nshapes using data. However, GMM fails on many-\nclass tasks like DBpedia and Yahoo (Table 4),\nshowing the benefits of adding prior on cluster\nweights as extra regularization.\nLearnable cluster weights handle class imbal-\nance. The learnable mixing weights of BGMM\n(and GMM) model the proportion of classes and\ntherefore handle unbalanced clusters. To test this,\nwe again compare three clustering algorithms on\nIMDb dataset with different class ratios. Figure\n4 shows that K-means fails completely when the\ndataset is unbalanced. BGMM and GMM perform\nbetter by allowing the cluster weights to adapt to\nthe data, but GMM is less stable in extreme cases.\n4.3 TC14 Datasets\nTo further study the potential applications and limi-\ntations of SimPTC, we collect 14 publicly available\ntext classification datasets with various topics, text\nlengths, and numbers of classes (Table 5). For sim-\nplicity, we refer to these datasets as TC14. For\nmore dataset information, see Appendix I.1.\nSetup To simulate the most basic scenario, we\nevaluate SimPTC with the naive template “⟨mask⟩”\nand the original class names without expansion. We\nchoose Vanilla Prompting as the baseline since\nit is the most widely used zero-shot prompt-based\nmethod. For a fair comparison, we do not engineer\ntemplates or verbalizers and use the original class\nnames with templates adopted from Hu et al. (2021)\n(see Appendix I.2 for implementation details).\n8566\n0\n20\n40\n36.636.6\n42.0\n36.6\n42.0\n53.1\n20 News\n0\n20\n40\n60\n55.555.5 53.955.5 53.9\n63.5\nNYT-T opic\n0\n20\n40\n60\n80\n62.062.0 66.462.0 66.4\n77.7\nNYT-Location\n0\n20\n40\n60\n80\n73.873.8 80.473.8 80.4\n89.7\nBBC-News\n0\n20\n40\n60\n80 80.080.0\n93.3\n80.0\n93.3 94.3\nYelp\n0\n10\n20\n30\n40\n19.319.3\n46.2\n19.3\n46.2 46.6\nEmotion\n0\n20\n40\n60\n16.516.5\n55.6\n16.5\n55.6\n66.7\nBanking77\n0\n20\n40\n60\n80\n72.072.0\n81.9\n72.0\n81.9 86.8\nSST-2\n0\n10\n20\n30\n40\n28.428.4\n38.7\n28.4\n38.7\n42.2\nSST-5\n0\n20\n40\n60\n80\n48.648.6\n82.4\n48.6\n82.4 83.4\nMPQA\n0\n20\n40\n47.847.8 47.247.8 47.2\n52.0\nSubj\n0\n10\n20\n30\n28.128.1 28.928.1 28.9 30.8\nTREC\n0\n10\n20\n30\n40\n22.422.4 24.722.4 24.7\n40.6\nBiomedical\n0\n20\n40\n60\n80\n21.021.0\n49.4\n21.0\n49.4\n77.9\nStackOverflow\nVanilla Prompting Encode & Match SimPTC\nFigure 5: Macro-F1 scores on TC14. SimPTC outperforms Vanilla Prompting on all 14 datasets.\nDatasets # Texts # Cls. Ave. Len. Unb.\n20 News 18391 20 186 ✓\nNYT-Topic 31997 9 783 ✓\nNYT-Location 31997 10 783 ✓\nBBC News 2225 5 390 ✓\nYelp 38000 2 132 ✗\nEmotion 20000 6 19 ✓\nBanking77 13083 77 12 ✓\nSST-2 9613 2 19 ✓\nSST-5 11855 5 19 ✓\nMPQA 10606 2 3 ✓\nSubj 10000 2 23 ✗\nTREC 5952 6 10 ✓\nBiomedical 20000 20 13 ✗\nStackOverflow 20000 20 8 ✗\nTable 5: TC14 datasets (Cls.: class, Unb.: unbalanced).\nResults We report the macro-F1 scores on TC14\nin Figure 5 and put micro-F1 scores in Appendix\nI.3. E&M outperforms Vanilla Prompting on 12\nout of 14 datasets. SimPTC further boosts the per-\nformance and gives a superior performance on all\n14 datasets, showing the strong generalizability of\nour approach. SimPTC achieves the most gain when\n1) the class names contain multiple tokens (e.g.,\nBanking77); 2) the number of classes is large (e.g.,\nStackOverflow); 3) the class names contain rare or\ndomain-specific words (e.g., Biomedical).\nWhen does SimPTC not work very well? Both\nVanilla Promptingand E&M suffer when the class\nnames are abstract concepts, e.g., subjective and\nobjective in the Subj dataset. This suggests that\nprompting and current text embeddings are still\npoor at linking texts to class names describing ab-\nstract properties. But interestingly, the two classes\nof Subj separate well in the SimCSE embedding\nspace (Figure 6), indicating the ability of PLM\nembedding spaces to capture abstract semantic con-\ncepts. Additionally, both methods underperform\nFigure 6: 2D t-SNE visualization of Subj dataset in\nthe SimCSE embedding space. Although E&M cannot\nprovide a meaningful initial prediction given the ab-\nstract class names: subjective and objective, the two\nclasses are well separated in the embedding space.\nself-training keyword-based methods in long docu-\nment tasks (see Appendix I.4 for more details).\n4.4 Different Encoders\nIn this section, we utilize SimPTC to analyze dif-\nferent PLM embedding spaces. Specifically, we\nask two questions: 1) Are the texts also clus-\ntered by topics in the embedding spaces of PLMs\nthat are not explicitly trained to generate mean-\ningful embeddings? 2) Are the embeddings\nof larger PLMs more informative? To answer\nthese questions, we compare RoBERTalarge (RL)\n(Liu et al., 2019), Sentence RoBERTalarge (SRL)\n(Reimers and Gurevych, 2019), SimCSE super-\nvised RoBERTalarge (SimCSE), and T5-3B (Raffel\net al., 2020b) embedding spaces. For sentence em-\nbeddings, we average the embeddings of all tokens\nin a text for RL, and use the embeddings of the last\nhidden states from the encoder for T5-3B.\n4.4.1 Quantitative Results\nPLM embeddings can categorize text with-\nout task-specific fine-tuning. As RL is not pre-\ntrained to generate meaningful sentence embed-\n8567\n7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n6\n4\n2\n0\n2\n4\n6\nCluster center\nMovie & Music\nBook\nProduct\n(a) SimCSE-RoBERTalarge.\n3\n 2\n 1\n 0 1 2 3 4\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2\n1\n0\n1\n2\n3\nCluster center\n (b) RoBERTalarge.\n15\n 10\n 5\n 0 5 10 15 10\n5\n0\n5\n10\n15\n15\n10\n5\n0\n5\n10\n15\nCluster center\n (c) Sentence-RoBERTalarge.\nFigure 7: 3D PCA visualization of Amazon dataset in different sentence embedding spaces. (a) Two classes are\nclearly separated, and we can even find sub-topics by clustering texts in the same class. We can even observe a clear\nlinear semantic structure. (b) Data sub-structures are somewhat kept, but the two sentiment classes are not separated\ndistinctly. (c) The two classes are better distinguished, but the detailed data structures are lost.\nEncoder Size AG DB YH AM IM\nSimCSE 350M 86.9 93.2 63.9 93.9 91 .0\nRL† 350M 86.1 96 .0 54 .2 93 .5 92 .3\nSRL† 350M 85.8 93 .4 55 .4 92 .9 90 .9\nT5-3B† 3B 86.7 96.7 55.1 95.3 94.5\nBERT(sup.) 110M 94.4 99 .4 75 .0 97 .2 94 .5\nTable 6: Comparison of different encoders. †: Clusters\nare initialized using Vanilla Prompting (§4.4.1).\ndings, E&M does not work with RL. So we initialize\nSimPTC using Vanilla Prompting . We do the\nsame for SRL as it provides a better initialization.\nWe share the covariance matrices to offer extra\nregularization. Surprisingly, as shown in Table 6,\nthe original RL achieves comparable performance\nto SimCSE and outperforms the more sophisticated\nsentence encoder SRL on 4 out of 5 datasets.\nLarger PLMs tend to have more informative\nembedding spaces. With a larger model T5-3B,\nSimPTC gives even better results. Initialized using\nVP, T5-3B achieves comparable or better perfor-\nmance on 4 out of 5 datasets than the state-of-the-\nart sentence encoder SimCSE, matching even the\nsupervised BERT performance on IMDb. This indi-\ncates that embedding spaces of larger PLMs might\nhave even better clustering properties, which agrees\nwith their stronger zero-/few-shot learning ability.\n4.4.2 Qualitative Analysis\nTo explain the first finding in §4.4.1, we analyze\nthe 3D PCA visualization of the Amazon dataset\nin three embedding spaces (Figure 7). We observe\nthat: 1) RL preserves the dataset sub-structures, but\nthe two sentiment clusters do not separate very well.\n2) SRL pushes semantically close texts together by\nintroducing an extra training objective, which leads\nto more separable clusters, but the detailed struc-\ntures of data are lost. 3) The SimCSE embeddings\nseparate the two classes distinctively, and the texts\nare further clustered together by sub-topics, such\nas books or products. Very interestingly, a clear\nlinear semantic sub-structure can be observed:\n¯vbook\npos −¯vbook\nneg ≈¯vprod\npos −¯vprod\nneg ≈¯vpos −¯vneg,\nwhere ¯vprod\nneg is the cluster center vector of all nega-\ntive product reviews; ¯vpos and ¯vneg are the centers\nof two sentiment classes. ThereforeRL outperforms\nSRL possibly because it is more descriptive of texts.\nWith a good separability of topics and the ability\nto capture data sub-structures, SimCSE achieves the\nbest overall zero-shot classification performance.\n5 Conclusion\nIn this work, we show that a simple clustering-\nbased approach, SimPTC, can achieve state-of-the-\nart zero-shot text classification performance on\na wide range of tasks. With extensive experi-\nments, we identify the keys to cluster texts in the\nPLM embedding spaces and also the limitations of\nSimPTC. Further analysis of different PLMs shows\nthat PLMs can categorize texts in their embedding\nspaces without being trained to derive semanti-\ncally meaningful sentence embeddings, and Larger\nPLMs tend to have more informative embeddings.\nWe hope our exploration into the embedding spaces\nof PLMs can provide insights into understanding\nand developing new methods to elicit the zero-/few-\nshot learning ability of PLMs.\n8568\nLimitations\nWe identify three limitations of SimPTC as well as\nthis work: 1) Due to the nature of clustering and\nsentence embeddings, SimPTC still suffers at many-\nclass tasks with long documents and tasks with\nabstract class names (e.g., subjective v.s. objective);\n2) Currently how to apply SimPTCto other NLP\ntasks like NLI is not straightforward. 3) Due to\ncomputational resource constraints, our analysis is\nlimited to PLMs with parameters up to 3 Billion.\nIt would be interesting to see if our observations\ngeneralize to the largest models like GPT-3 (175B)\n(Brown et al., 2020) and PaLM (540B) (Chowdhery\net al., 2022), which show the strongest zero-/few-\nshot ability.\nEthics Statement\nThis work aims to analyze how to use PLM\nknowledge in their embedding spaces to catego-\nrize texts on different topics. Unlike many other\ndeep-learning-based models, SimPTC involves no\nlarge neural model pre-training, re-training, or fine-\ntuning throughout the entire development of the\nmethod. Once we get the embeddings of the un-\nlabeled texts, the PLMs are not used anymore.\nThus developing and applying our approach re-\nquires only minimal computational resources and\ncause fewer carbon emissions than methods that\nrequire dataset-specific fine-tuning or engineering.\nBesides, we do not anticipate any significant eth-\nical issues introduced by our approach. We use\nonly off-the-shelf PLMs, and the datasets involved\nare all publicly available topic or sentiment classi-\nfication datasets. Nevertheless, we urge anyone to\nevaluate the robustness of the method before using\nSimPTC in sensitive contexts such as healthcare or\nlegal scenarios.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7747–\n7763, Online. Association for Computational Lin-\nguistics.\nNeel Alex, Eli Lifland, Lewis Tunstall, Abhishek\nThakur, Pegah Maham, C Jess Riedel, Emmie Hine,\nCarolyn Ashurst, Paul Sedille, Alexis Carlier, et al.\n2021. Raft: A real-world few-shot text classification\nbenchmark. arXiv preprint arXiv:2109.14076.\nChristopher M Bishop and Nasser M Nasrabadi. 2006.\nPattern recognition and machine learning, volume 4.\nSpringer.\nStephen Boyd, Stephen P Boyd, and Lieven Vanden-\nberghe. 2004. Convex optimization. Cambridge uni-\nversity press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nIñigo Casanueva, Tadas Temcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vulic. 2020. Ef-\nficient intent detection with dual sentence en-\ncoders. In Proceedings of the 2nd Workshop\non NLP for ConvAI - ACL 2020 . Data avail-\nable at https://github.com/PolyAI-LDN/task-specific-\ndatasets.\nMing-Wei Chang, Lev-Arie Ratinov, Dan Roth, and\nVivek Srikumar. 2008. Importance of semantic repre-\nsentation: Dataless classification. In Aaai, volume 2,\npages 830–835.\nYulong Chen, Yang Liu, Li Dong, Shuohang Wang,\nChenguang Zhu, Michael Zeng, and Yue Zhang.\n2022. Adaprompt: Adaptive model training for\nprompt-based nlp. arXiv preprint arXiv:2202.04824.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nZewei Chu, Karl Stratos, and Kevin Gimpel. 2021.\nUnsupervised label refinement improves dataless\ntext classification. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4165–4178, Online. Association for Computa-\ntional Linguistics.\nGanqu Cui, Shengding Hu, Ning Ding, Longtao Huang,\nand Zhiyuan Liu. 2022. Prototypical verbalizer\nfor prompt-based few-shot tuning. arXiv preprint\narXiv:2203.09770.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\n8569\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Hai-Tao Zheng, and Maosong Sun.\n2021. Openprompt: An open-source framework for\nprompt-learning. arXiv preprint arXiv:2111.01998.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDerek Greene and Pádraig Cunningham. 2006. Practi-\ncal solutions to the problem of diagonal dominance\nin kernel document clustering. In Proceedings of the\n23rd International Conference on Machine Learn-\ning, ICML ’06, page 377–384, New York, NY , USA.\nAssociation for Computing Machinery.\nVikram Gupta, Haoyue Shi, Kevin Gimpel, and Mrin-\nmaya Sachan. 2022. Deep clustering of text repre-\nsentations for supervision-free probing of syntax. In\nAssociation for the Advancement of Artificial Intelli-\ngence.\nIsmail Harrando and Raphaël Troncy. 2021. Explain-\nable zero-shot topic extraction using a common-sense\nknowledge graph. In LDK 2021, 3rd Conference on\nLanguage, Data and Knowledge.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Juanzi Li, and Maosong Sun. 2021. Knowl-\nedgeable prompt-tuning: Incorporating knowledge\ninto prompt verbalizer for text classification. arXiv\npreprint arXiv:2108.02035.\nAntonia Kyriakopoulou and Theodore Kalamboukis.\n2006. Text classification using clustering. In Pro-\nceedings of the Discovery Challenge Workshop at\nECML/PKDD 2006, pages 28–38.\nKen Lang. 1995. Newsweeder: Learning to filter net-\nnews. In Machine Learning Proceedings 1995, pages\n331–339. Elsevier.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web, 6(2):167–195.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: Understanding rating dimen-\nsions with review text. In Proceedings of the 7th\nACM Conference on Recommender Systems, RecSys\n’13, page 165–172, New York, NY , USA. Association\nfor Computing Machinery.\nDheeraj Mekala and Jingbo Shang. 2020. Contextu-\nalized weak supervision for text classification. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 323–333,\nOnline. Association for Computational Linguistics.\nYu Meng, Jiaxin Huang, Guangyuan Wang, Zihan Wang,\nChao Zhang, Yu Zhang, and Jiawei Han. 2020a. Dis-\ncriminative Topic Mining via Category-Name Guided\nText Embedding, page 2121–2132. Association for\nComputing Machinery, New York, NY , USA.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2018. Weakly-supervised neural text classification.\nIn proceedings of the 27th ACM International Con-\nference on information and knowledge management,\npages 983–992.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020b. Text\nclassification using label names only: A language\nmodel self-training approach. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9006–9017,\nOnline. Association for Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nKevin P Murphy. 2007. Conjugate bayesian analysis of\nthe gaussian distribution. def, 1(2σ2):16.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. arXiv preprint cs/0409058.\n8570\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020b. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3687–3697, Brussels, Belgium. Association\nfor Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nRoman Vershynin. 2012. How close is the sample\ncovariance matrix to the actual covariance matrix?\nJournal of Theoretical Probability, 25(3):655–686.\nEllen M V oorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 200–207.\nZihan Wang, Dheeraj Mekala, and Jingbo Shang. 2021.\nX-class: Text classification with extremely weak su-\npervision. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3043–3053, Online. Association for\nComputational Linguistics.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.\nAnnotating expressions of opinions and emotions\nin language. Language resources and evaluation ,\n39(2):165–210.\nJiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guan-\nhua Tian, and Jun Zhao. 2017. Self-taught convolu-\ntional neural networks for short text clustering. Neu-\nral Networks, 88:22–31.\nHua-Jun Zeng, Xuan-Hui Wang, Zheng Chen, Hongjun\nLu, and Wei-Ying Ma. 2003. Cbc: Clustering based\ntext classification requiring minimal labeled data. In\nThird IEEE International Conference on Data Min-\ning, pages 443–450. IEEE.\nDejiao Zhang, Feng Nan, Xiaokai Wei, Shangwen Li,\nHenghui Zhu, Kathleen McKeown, Ramesh Nalla-\npati, Andrew Arnold, and Bing Xiang. 2021a. Sup-\nporting clustering with contrastive learning. arXiv\npreprint arXiv:2103.12953.\nLu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, and\nShuigeng Zhou. 2021b. Weakly-supervised text clas-\nsification based on keyword graph. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2803–2813, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\n8571\nA Expanded Class Name Examples for\nAll Datasets\nSome examples of the original and extracted class\nnames are shown in Table 11 - 14.\nB Templates Used for All Datasets\nAG’s News:\nThe news is about ⟨mask⟩.\nThe news is related to ⟨mask⟩.\n⟨mask⟩is the topic of the news.\nThis week’s news is about ⟨mask⟩.\nDBpedia:\nThe object is about ⟨mask⟩.\nThe object is related to ⟨mask⟩.\n⟨mask⟩is the topic of the object.\n⟨mask⟩is the subject of the object.\nYahoo:\nThe answer is about ⟨mask⟩.\nThe answer is related to ⟨mask⟩.\n⟨mask⟩is the topic of the answer.\n⟨mask⟩is involved in the answer.\nAmazon:\nA ⟨mask⟩product review.\nThe product review is ⟨mask⟩.\nThe reviewer found the product ⟨mask⟩.\nThe product is ⟨mask⟩.\nIMDb:\nA ⟨mask⟩movie review.\nThe movie review is ⟨mask⟩.\nThe reviewer found the movie ⟨mask⟩.\nThe movie is ⟨mask⟩.\nC Math foundation of the SimPTCUpdate\nStep\nBayesian approaches inject prior knowledge by\nintroducing prior distribution on model parameters\nwhile still allowing the model to fit the data. In this\nsection we first discuss the prior distributions we\nchoose. Then we show how these choices affect\nthe model prediction by analyzing the maximum\na posteriori probability (MAP) solution of model\nparameters.\nPrior distributions Following Bishop and\nNasrabadi (2006), we choose a Dirichlet distri-\nbution as the prior for mixture weights π, and a\nGaussian-Wishart prior for the mean and precisions,\ni.e., the inverse of covariance Λ = Σ−1:\np(π) = Dir(π|α0) = C(α0)\n∏\nk\nπα0−1\nk\np(µ,Λ) = p(µ|Λ)p(Λ)\n=\n∏\nk\nN(µk|m0,(β0Λk)−1)·\nW(Λk|W0,ν0),\nwhere C(α0) is a normalizing constant, and α0 can\nbe interpreted as the prior number of observations\nassociated with each mixture. We simply choose\nα0 = N\nK to favor balanced weights. For the means\nand covariances, we offer the model maximum free-\ndom to fit the data by choosing a non-informative\nprior (Murphy, 2007). Specifically, we set:\nm0 = 0, β0 →0, W0 = 1\ndΣ−1\ninit, ν0 = d, (2)\nwhere Σinit is some initial guess of the covariance\nmatrix, which can be set as the empirical covari-\nance of the data. Then we update the model with\nthe standard variational optimization (Bishop and\nNasrabadi, 2006) for Bayesian GMM.\nMAP solution Here, we show the MAP solution\nafter one update step to give some intuition about\nhow our choice of prior model parameters (2) in-\nfluences the update of model parameters. As the\nstandard EM update of maximum likelihood meth-\nods, the variational update also contains two steps.\nIn the variational E step, we evaluate the respon-\nsibilities using the current variational distribution\nparameters:\nrnk := Eπ,µ,Σ[znk],\nwhere znk is the binary latent variable indicating\nwhether data xn belongs to cluster k; and in the\n8572\nName Type # Class Training Size Test Size Max Iter Covariance Setting\nAG’s News Topic 4 120000 7600 50 Full\nDBpedia Topic 14 560000 70000 40 Full\nYahoo Topic 10 1400000 60000 20 Full\nAmazon Sentiment 2 200000 10000 50 Tied\nIMDb Sentiment 2 25000 25000 150 Tied\nTable 7: Statistics of datasets used to compare with state-of-the-art methods in §4.1 and extra model settings.\nSimPTC stops when the model prediction stops changing, or the maximum number of iteration is achieved. Full:\neach Gaussian mixture has its own covariance Σk. Tied: all Gaussians share the same covariance Σ.\nvariational M step, we update the variational distri-\nbution parameters. For simplicity, we introduce the\nfollowing statistics:\nNk =\n∑\nn\nrnk\n¯xk = 1\nNk\n∑\nn\nrnkxn\nSk = 1\nNk\n∑\nn\nrnk(xn −¯xk)(xn −¯xk)⊤.\nThen the MAP solution of π,µ,Σ given the re-\nsponsibilities rnk’s after a variational M steps is\ngiven by\nπ∗\nk = α0 −1 + Nk\nK(α0 −1) + N\nµ∗\nk = ¯xk\nΣ∗\nk = dΣinit + NkSk\nNk −1 ,\n(3)\nwhere dis the number of feature dimensions and K\nis the number of classes. We can see that by choos-\ning non-informative prior (2) of (µ,Σ), we allow\nthe model to fit the data with maximum freedom.\nBy choosing a large α0, we can push the mixing\nweights towards uniform but still allow the model\nto fit the data.\nD Datasets Statistics and Model Settings\nThe statistics of the five datasets used in §4.1 and\nmax iteration numbers can be found in Table 7. For\nAmazon, we use the same test set sampled by Hu\net al. (2021) and randomly sample 200,000 texts\nfrom the original training set for the unlabeled train-\ning data. Since SimCSE only handles texts with a\nmaximum length of 512, we crop texts with lengths\nexceeding 512. We choose the maximum number\nof iterations empirically according to the size of\nthe unlabeled data which is equal to the training set\n0 10 20 30 40\nIteration\n60\n70\n80\n90Test accuracy\nAgnews\nDbpeda\nYahoo\nAmazon\nIMDB\nFigure 8: The performance v.s. update iteration plot of\nSimPTC on all five datasets. The solid line shows the\naverage accuracy at each iteration, whereas the blurred\narea indicates the standard deviation of using different\ntemplates. SimPTC converges to a good-quality pre-\ndiction as the clustering process converges.\nsize plus the test set size. For topic datasets, each\nGaussian has its individual covariance matrix. For\nsentiment datasets, all Gaussians share the same\ncovariance matrix to provide extra regularization as\nthe data is relatively sparse. The effect of sharing\nthe covariance matrix is discussed in Appendix G.\nE Convergence Analysis\nAlthough SimPTC is guaranteed to converge, it is\nunclear whether it will converge to a good solu-\ntion when the algorithm stops. Therefore we study\nhow the model performance varies as the updating\nprocess proceeds. We plot the test accuracy of in-\ntermediate update steps on all datasets in Figure\n8, where the standard deviations caused by using\ndifferent templates are illustrated with blurred ar-\neas. We observe that the performance gradually\nimproves and converge in all dataset except Yahoo,\nwhere SimPTC still converges to a result much bet-\nter than the initialization. Also, as shown in the\nblurred areas in Figure 8, the update step is sta-\n8573\n0.0 0.2 0.4 0.6 0.8 1.0\nTraining set ratio\n60\n70\n80\n90Test accuracy\nAgnews\nDbpeda\nYahoo\nAmazon\nIMDB\nFigure 9: The performance v.s. unlabeled dataset size\nplot of SimPTC on all five datasets. More unlabeled data,\nin general, tends to improve the prediction of SimPTC.\nble when different templates are used. Moreover,\nSimPTC almost converges on all five datasets under\nour setting of the maximum number of iterations.\nF Effect of Unlabeled Dataset Size\nIn the standard setting, we use both the train and\ntest set for fitting the Bayesian GMM. To study\nthe effect of the unbalanced dataset size, we keep\nthe unlabeled test data and use the training data\nwith ratios varying from 0 to 1. As illustrated in\nFigure 9, on almost all datasets, more unlabeled\ndata brings more improvement.\nOne possible explanation is: to model the shape\nof all clusters with a certain error threshold, one\nneeds samples of a number at least linear to the\nnumber of dimensions (Vershynin, 2012) and lin-\near to the number of classes. Therefore a large\nunlabeled dataset helps the model to fit data with\nmany classes in a high-dimensional space better\n(for RoBERTalarge , the number is 1024). By\nsharing the covariance matrix (Amazon and IMDb),\nwe reduce the number of model parameters. Thus\nSimPTC works better than fitting individual covari-\nance for each cluster (Agnews, Dbpedia, and Ya-\nhoo) when the data is sparse. Since for many tasks\ncollecting unlabeled data is considered to be much\neasier than collecting annotated data, we can im-\nprove the performance of SimPTC in real-world ap-\nplications at a low cost.\nG Effect of Sharing Covariance Matrix\nWe explore two covariance settings inSimPTC. Full:\neach Gaussian mixture has its own covariance Σk,\nand tied: all Gaussians share the same covariance\nΣ. Note that the sharing the covariance matrices\nSetting Topic Sentiment\nAG DB YH AM IM\nE&M 78.2 74 .4 58 .3 91 .2 85 .6\nFull 86.9↑ 93.2↑ 63.9↑ 92.4↑ 86.2↑\nTied 86.5↑ 90.8↑ 56.9↓ 93.9↑ 91.0↑\nTable 8: Average test accuracy of all templates with\ndifferent covariance settings. Tied: all Gaussians share\nthe same covariance matrix. Full: every Gaussian has\nits own covariance matrix.\n(the full and tied setting) is a standard hyperparam-\neter of GMM. The full setting is more flexible, and\nas Table 8 shows it improves the initial E&M pre-\ndictions on all datasets. By sharing the covariance\nmatrices (the tied setting) we 1) reduce model pa-\nrameters to provide extra regularization and 2) add\nstronger assumptions on the cluster shapes. There-\nfore it is useful when\n• the data is relatively sparse (e.g., IMDb in\nTable 8 and TC14 datasets in §4.3),\n• the embedding space of PLM is less structured\n(T5 and RoBERTalarge embeddings (§4.4)),\n• the texts of different classes describe similar\nobjects (e.g., sentiment tasks).\nOtherwise, we recommend allowing clusters to\nhave different covariances.\nH Implicit Balanced Assumption of KPT\nHu et al. (2021) proposed a data-dependent\nContextualized Calibration (CC). They mo-\ntivate CC by observing that some label words are\nless likely to be predicted than others, regardless of\nthe label of input sentences. To solve the problem,\nCC works in the following steps: First, to estimate\na contextualized prior distribution of label words\nusing some sampled unlabeled data:\nPD(v) = Ex∼DPM([MASK] = v|x)\n≈ 1\n|C|\n∑\nx∈C\nPM([MASK] = v|x), (4)\nwhere vstands for a particular label word, Dis the\ndata distribution, PMis the model prediction, Cis\na sampled subset of the dataset. Then they use the\ncontextualized prior of label words to calibrate the\npredicted distribution:\n˜PM([MASK] = v|x) ∝PM([MASK] = v|x)\nPD(v) .\n(5)\n8574\nName Type Class name examples Template for prompting\n20 News Topic comp.graphics; sci.space [ Category : ⟨mask⟩] ⟨text⟩\nNYT-Topic Topic business; politics; sports [ Category : ⟨mask⟩] ⟨text⟩\nNYT-Location location united_states; iraq; japan [ Category : ⟨mask⟩] ⟨text⟩\nBBC News Topic sport; business; entertainment [ Category : ⟨mask⟩] ⟨text⟩\nEmotion Emotion sad; joy; anger [ Category : ⟨mask⟩] ⟨text⟩\nBanking77 Intent activate_my_card; age_limit [ Category : ⟨mask⟩] ⟨text⟩\nTREC Question abbr.; entity; description [ Category : ⟨mask⟩] ⟨text⟩\nBiomedical Paper title aging; chemistry; erythrocytes [ Category : ⟨mask⟩] ⟨text⟩\nStackOverflow Question svn; oracle; bash [ Category : ⟨mask⟩] ⟨text⟩\nYelp Sentiment positive; negative It is ⟨mask⟩. ⟨text⟩\nSST-2 Sentiment positive; negative It is ⟨mask⟩. ⟨text⟩\nSST-5 Sentiment very positive; positive; negative It is ⟨mask⟩. ⟨text⟩\nMPQA Opinion polarity positive; negative It is ⟨mask⟩. ⟨text⟩\nSubj Subjectivity subjective; objective It is ⟨mask⟩. ⟨text⟩\nTable 9: Extra information about TC14 datasets. Template for prompting is the template we used to perform\nprompt-based zero-shot learning, i.e., Vanilla Prompting. We use the same template for all sentiment tasks and\nanother for all other datasets.\nThe final probability is normalized to 1.\nThe contextualized prior can be interpreted as\na marginal distribution. Consider we have one la-\nbel word for each class. The contextualized prior\nmeasures the portion of each class in the dataset\nbased on the model’s predictions. Then CC penal-\nizes the probability of predicting one class if the\nmodel thinks it assigns too many samples to this\nclass (PD(v) is large). Intuitively this is to force\nthe model to assign equal numbers of samples to\neach class, which is to force a uniform marginal\ndistribution. The underlying implicit assumption is\nthat the dataset is balanced. Although CC improves\nthe zero-shot performance of KPT, we argue that\nthis is because the evaluation datasets happen to be\nbalanced, and CC becomes problematic when the\ndataset is unbalanced (see C2 in §4.1).\nI TC14 Datasets\nTo study the applications and limitations ofSimPTC,\nwe collect the following 14 datasets with diverse\ntopics, text lengths, and class numbers. Specifi-\ncally, we did a literature search in zero-shot text\nclassification and collected datasets that best fit\nour text classification setting with label names that\nhave class-info. We first introduce the details of the\nTC14 datasets (§I.1). Then we discuss the imple-\nmentation details in §I.2. We show the full results\nin §I.3 and provide extra analysis in §I.4.\nI.1 Dataset Information\nThe datasets we used are:\n• 20 News (Lang, 1995) is a news classification\ndataset. It has a relatively long average text\nlength and many classes.\n• NYT-Topic(Meng et al., 2020a) is a long docu-\nment topic classification dataset that is very un-\nbalanced.\n• NYT-Location (Meng et al., 2020a) uses the\nsame corpus as NYT-Topic but categorizes the\ntexts according to locations. The dataset is very\nunbalanced.\n• BBC News (Greene and Cunningham, 2006) is a\nnews dataset containing 2225 articles.\n• Yelp (Zhang et al., 2015) is a review sentiment\ndataset.\n• Emotion (Saravia et al., 2018) is a dataset of En-\nglish Twitter messages with six basic emotions,\nand the dataset is very unbalanced.\n• Banking77 (Casanueva et al., 2020) is a dataset\ncomposed of online banking queries annotated\nwith their corresponding intents. It has a very\nfine-grained set of intents in the banking domain.\n13,083 customer service queries are categorized\ninto 77 intents.\n• SST-2 (Socher et al., 2013) is a sentence senti-\nment classification dataset.\n• SST-5 (Socher et al., 2013) is a fine-grained sen-\ntiment classification dataset. Texts are classified\ninto five sentiment classes: very negative, nega-\n8575\nMethod 20News NYT-T NYT-L BBC Yelp Emotion Banking77\nVP 41.0/36.6 72.1/55.5 66.3/62.0 75.8/73.8 80.6/80.0 21.7/19.3 21.2/16.5\nEncode&Match 42.9/42.0 59.6/53.9 65.9/66.4 80.6/80.4 93.3/93.3 52.3/46.2 57.0/55.6\nSimPTC 51.2/53.1 66.0/63.5 72.1 /77.7 89.5 /89.7 94.3 /94.3 51.0/46.6 66.6 /66.7\nZero-shot SOTA 78.6/77.8 a 79.0/68.6a 91.8/92.0a 84.0 (acc)b 90.0/90.0a -/- -/33.2 c\nSST-2 SST-5 MPQA Subj TREC Biomed. StackOF\nVP 73.7/72.0 32.4/28.4 49.0/48.6 56.2/47.8 37.7/28.1 25.0/22.4 26.6/21.0\nEncode&Match 82.0/81.9 42.7/38.7 83.8/82.4 51.7/47.2 35.4/28.9 26.8/24.7 49.0/49.4\nSimPTC 86.8/86.8 46.2 /42.2 84.8 /83.4 53.9/52.0 37.3/30.8 38.4 /40.6 74.2 /77.9\nZero-shot SOTA 83.6 (acc) d 35.0 (acc)d 67.6 (acc)d 51.4 (acc)d 32.0 (acc)d 46.2 (acc)e 75.5 (acc)e\nTable 10: Zero-shot micro-/macro-F1 scores on other datasets. VP: vanilla prompting (§4). We collect publicly\navailable zero-shot state-of-the-art (SOTA) method performance as a reference. a: X-Class, (Wang et al., 2021) a\nSOTA keyword-based method. b: (Harrando and Troncy, 2021). c: Crowdsourced human performance from Alex\net al. (2021) (they used a selected portion of Banking77). d: zero-shot prompt-based zero-shot learning provided\nby Gao et al. (2021a). e: SCCL, a contrastive-learning-based unsupervised text clustering method by Zhang et al.\n(2021a). SCCL forces on clustering texts of different topics. When calculating accuracy, the labels of clusters are\ndetermined by solving a min-cost perfect matching problem based on the predicting accuracy.\ntive, neutral, positive, and very positive.\n• MPQA (Wiebe et al., 2005) is an opinion polarity\nanalysis dataset.\n• Subj (Pang and Lee, 2004) is a subjectivity anal-\nysis dataset.\n• TREC (V oorhees and Tice, 2000) is an unbal-\nanced question classification dataset.\n• Biomedical (Xu et al., 2017) is a paper title clas-\nsification dataset, where 20,000 titles are catego-\nrized into 20 groups.\n• StackOverflow (Xu et al., 2017) is a dataset con-\ntaining 20,000 questions with 20 classes.\nSince we are evaluating zero-shot methods, we\nreport scores on the full datasets (dataset sizes are\nshown in Table 5).\nI.2 Additional Implementation Details\nWe compare with Vanilla Prompting rather than\nKPT because KPT has an improper balanced dataset\nassumption (§4.1 C3), and KPT cannot handle class\nnames containing multiple words.\nFor the 20 News dataset, we use class names\nfrom Mekala and Shang (2020) as the original\nclass names are not complete English. We im-\nplement Vanilla Prompting using OpenPrompt\n(Ding et al., 2021). When a class name contains\nmultiple words, we use the average probability\nof predicting each word as implemented in Open-\nPrompt. BBC News contains only 2225 texts and is\ntoo small to fit a 1024-by-1024 covariance matrix\neven if we share the covariance matrices of clusters.\nBanking77 has too many classes compared with\nthe dataset size, and as a result, Encode&Match ass-\ning zero samples to some classes. To fix these two\nproblems, we perform a PCA to reduce the feature\ndimension such that the reconstruction error is 3%\nbefore Encode&Match.\nI.3 Full Results\nWe report the micro-macro F1 scores on TC14 in\nTable 10. For comparison, we also collect publicly\navailable state-of-the-art results on these datasets.\nSome papers only report the accuracy of their mod-\nels, and we report these numbers instead.\nI.4 Additional Analysis\nAs discussed in §4.3, both prompting and E&M suf-\nfer on the Subj dataset where the class names are\nabstract concepts (subjective v.s. objective). As a\nresult, SimPTC also does not go very far from ran-\ndom guessing (50%). However, despite E&M failing\nto link the texts correctly with the abstract class\nnames, the texts themselves are well-separated in\nthe embedding space (Figure 6). This suggests that\ntexts with abstract classes can also be clustered to-\ngether in the PLM embedding spaces. A 10-shot\nsetting (averaged over 5 seeds) improves SimPTC\nfrom 52.0 to 89.2 on Subj, outperforming GPT-3\n175B in-context learning (76.4).\nIn terms of limitations, another important ob-\nservation is that: on long document classification\ntasks (20 News, NYT-Topic, NYT-Location), both\nSimPTC and Vanilla Prompting underperform\nthe state-of-the-art keyword-based method X-Class\n(Wang et al., 2021), showing an information loss\nwhen PLMs encodes long documents into the em-\n8576\nbedding spaces. This indicates that in terms of\nextracting information from long documents, self-\ntraining keyword-based approaches still perform\nbetter than zero-shot our clustering-based approach\nand prompting methods.\n8577\nClass Name Expanded Class Names\npolitics alt rightist, social fascism, psychopolitical, leader of opposition, junior minister,\nwhipped vote, political, regressive leftism, policy making, dollar democracy, ...\nsports professional baseball, game set match, banana ball, empty bench, first touch,\nfootball, sportsman, visiting team, athletic, exhibition game, super cup, ...\nbusiness account name, commerciality, making money, sprinkler strategy, web company,\nconsumer good, business economics, maintained markup, commercial enterprise, ...\ntechnology cryoengineering, aeronautical engineering, geotechnology, cwm silicon, nuclearism,\ndigital technology, cryotechnology, xenotechnology, applied science, deepfake, ...\nTable 11: Original class names and expanded class names of AG’s News.\nClass Name Expanded Class Names\ncompany hook stock, private corporation, large company, big company, business organization,\nfurniture company, companies, sprinkler strategy, corp, livery company, ...\nschool elementary schooler, undergraduates, university student, dual school, antiuniversity,\nschoolless, overschooled, secondary modern, science room, state school, ...\nartist arte povera, ernstian, art show, da vincian, polystylist, gallery opening, pricasso,\nartworks, artistdom, superrealist, artists, clean brushes, post impressionist ...\nathlete olga korbut, athleticism, pull muscle, walking sports event, pancratical,nongymnast,\nsportswomen, athletic contest, weightlifter, winter olympics competition, ...\npolitics alt rightist, social fascism, psychopolitical, leader of opposition, junior minister,\nwhipped vote, political, regressive leftism, policy making, dollar democracy, ...\ntransportation antirail, air freight logistics, delivered ex ship, road rail, transmodal,\nwater bailage, transportive, cargon, vecturist, multiride, transfer to hospital, ...\nbuilding tower block, nonbuilding, inbond, interior door, interiorscaper, split level,\nelectrical wiring, seismic retrofit, house raising, sevenplex, office complex, ...\nriver mountainlike, talav, mountainside, mount sharp, river, lake albert nyanza,\nsubapennine, khabur, transmountain, longs peak, riverling, land form, monticulus, ...\nvillage koprivnica, khutor, intown, b road, mini mall, oppidan, cybervillage, gaothan,\nlawley, shillingstone, shakespeare play, claygate, goosnargh, hamlets, northcott, ...\nanimal gambian pouched rat, cattle beast, wild game, cymothoa exigua, farm animal,\nbestiarian, stylophora, brazilian wandering spider, western black rhinoceros, ...\nplant anthoxanthum odoratum, harpulla, calochortus amabilis, brazilian pepper tree,\ntree roots, cuphea, lespedeza bicolor, phoenix tree, akeake, rauli beech, nontree,...\nalbum studio album, lyrics, space cakes, guitar drums, song, chiodos, american life,\ndance pop, keys of kingdom, record deal, rock opera, songsheet, songcraft, ...\nfilm star actor, filmically, company men, moving pictures, stfilm, getting acquainted,\nsound film, photographic film, collage film, cinematology, filmize, ...\nbook megabook, pilgrim’s progress, neophiliac, forebook, young adult fiction, clipsheet,\nnovels, novel, book, novelle, reading material, booklessness, e novel, ...\nTable 12: Original class names and expanded class names of DBpedia.\n8578\nClass Name Expanded Class Names\nsociety, culture crowd elevator, cybersociety, macroculture, intersocietal, islandness,\ndesocialize, cultureshed, overculture, preculture, ghost skin, antisociety, ...\nscience, mathematics inequality sign,ur science, odd function, common antilog, hydroscience,\nknown quantity, find out truth, science, commutative law, aetherometry, ...\nhealth being well, dietetist, hale and hearty, healthcare delivery, healthful, health,\ncountry doctor, geomedical ,health centre, nutritionwise, patient contact,...\neducation, reference postsecondary school, uneducation, special educator, secondary education,\ncross index, tertiary education, forward reference, exophora,...\ncomputers, internet allows null sessions, dynamic ip address, friendly url, data processor,\nlaptops, deadlink, web diving, dictionary attacker, nt account system, ...\nsports professional baseball, game set match, banana ball, empty bench,\nfootball, sportsman, visiting team, athletic, exhibition game, super cup, ...\nbusiness, finance adhocratic, net operating loss, business organization, capital structure,\nsystematic risk, manufacturers rep, web company, garmento, ...\nentertainment, music bigophonic, good fun, entertaintment, natabhairavi, eating popcorn,\nallegro non troppo, semihemidemisemiquaver, musicaholic, ...\nfamily, relationships mother father, enicocephalid, profamily, close friendship, salpidae,\nvisual proximity, relations, lac scale, sexual relationship, ...\npolitics, government governmentalise, ruling party, westminster system, antiindependence,\nleader of opposition, cryptarchy, macropolitical, antipopulist,...\nTable 13: Original class names and expanded class names on AG’s News.\nClass Name Expanded Class Names\nbad overawful, crappy, uglysome, not good, suck balls, do badder, blow chunks,\nshitly, godawful, sucktastic, worsts, horridsome, fucky, god awful, terrible, ...\ngood correct answer, have good day, better job, clean apartment, double plus good, nice,\ntalk with friends, goodish, supernice, like million bucks, healthy environment, ...\nTable 14: Original class names and expanded class names on AG’s News.\n8579",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7803152799606323
    },
    {
      "name": "Cluster analysis",
      "score": 0.7324793338775635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.611265242099762
    },
    {
      "name": "Embedding",
      "score": 0.610766589641571
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5754507780075073
    },
    {
      "name": "Sentence",
      "score": 0.575352668762207
    },
    {
      "name": "Natural language processing",
      "score": 0.5403620600700378
    },
    {
      "name": "Initialization",
      "score": 0.5401934385299683
    },
    {
      "name": "Language model",
      "score": 0.5117809176445007
    },
    {
      "name": "Task (project management)",
      "score": 0.5105977654457092
    },
    {
      "name": "Shot (pellet)",
      "score": 0.48037606477737427
    },
    {
      "name": "Categorization",
      "score": 0.41899752616882324
    },
    {
      "name": "Machine learning",
      "score": 0.4155842959880829
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33200007677078247
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}