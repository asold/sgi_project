{
    "title": "INFORM : Information eNtropy based multi-step reasoning FOR large language Models",
    "url": "https://openalex.org/W4389524181",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3177204714",
            "name": "Chuyue Zhou",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A5113392993",
            "name": "Wangjie You",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A2126452540",
            "name": "Juntao Li",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A2101356739",
            "name": "Jing Ye",
            "affiliations": [
                "Soochow University"
            ]
        },
        {
            "id": "https://openalex.org/A2513203147",
            "name": "Kehai Chen",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2005452123",
            "name": "Min Zhang",
            "affiliations": [
                "Soochow University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4389518664",
        "https://openalex.org/W4304194220",
        "https://openalex.org/W4287207937",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W4302011807",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4367701181",
        "https://openalex.org/W4321855256",
        "https://openalex.org/W2105717194",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4303648904",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4310829078",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4366565380",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W4389520273",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4360890239",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W2251935656"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks with dedicated Chain-of-Thought (CoT) prompts. Further enhancing CoT prompts with exquisite exemplars can significantly improve reasoning performance.However, the effectiveness of CoT prompts may fluctuate dramatically with different choices of in-context examples. Additionally, manual construction of rationale steps can be time-consuming, presenting challenges for the widespread adoption of CoT prompting. In this work, we propose a novel approach by introducing information entropy (IE) as a criteria on for CoT prompt selection. We extend this criterion to the CoT generation and inference stages, automatically generating CoT prompts with higher information entropy scores and adaptively determining the number of samples. These three stages together form our proposed information- entropy-based multi-step reasoning for large language models, named INFORM. Our experiments across seven reasoning benchmarks utilizing two language models(GPT-3.5-Turbo and text-davinci-003) demonstrate the superiority of INFORM both in performance and efficiency.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3565‚Äì3576\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nINFORM : Information eNtropy based multi-step reasoning FOR large\nlanguage Models\nChuyue Zhou1‚àó, Wangjie You1‚àó, Juntao Li1‚Ä†, Jing Ye1, Kehai Chen2, Min Zhang1\n1Institute of Computer Science and Technology, Soochow University, China\n2 Harbin Institute of Technology, Shenzhen\n{cyzhou, wjyouu, jyeyj}@stu.suda.edu.cn\n{ljt, minzhang}@suda.edu.cn\nchenkehai@hit.edu.cn\nAbstract\nLarge language models (LLMs) have demon-\nstrated exceptional performance in reasoning\ntasks with dedicated Chain-of-Thought (CoT)\nprompts. Further enhancing CoT prompts with\nexquisite exemplars can significantly improve\nreasoning performance. However, the effec-\ntiveness of CoT prompts may fluctuate dramat-\nically with different choices of in-context ex-\namples. Additionally, manual construction of\nrationale steps can be time-consuming, present-\ning challenges for the widespread adoption of\nCoT prompting. In this work, we propose a\nnovel approach by introducing information en-\ntropy (IE) as a criteria on for CoT prompt selec-\ntion. We extend this criterion to the CoT gener-\nation and inference stages, automatically gen-\nerating CoT prompts with higher information\nentropy scores and adaptively determining the\nnumber of samples. These three stages together\nform our proposed information entropy based\nmulti-step reasoning for large language mod-\nels, named INFORM. Our experiments across\nseven reasoning benchmarks utilizing two lan-\nguage models(GPT-3.5-Turbo and text-davinci-\n003) demonstrate the superiority of INFORM\nboth in performance and efficiency.1\n1 Introduction\nLarge language models (LLMs) (Brown et al.,\n2020; Chowdhery et al., 2022; Thoppilan et al.,\n2022; Le Scao et al., 2022; Touvron et al.,\n2023) have achieved great success in recent years.\nThese models are commonly employed through\nin-context learning (Brown et al., 2020), where\ninstructions and exemplars are provided to enhance\nlanguage understanding and generation. However,\nthe widely-used in-context learning methods might\nperform poorly for complex reasoning tasks (Liang\net al., 2022; Wei et al., 2022). Recent studies (Wei\n‚àó Equal Contribution\n‚Ä†Juntao Li is the corresponding author.\n1https://github.com/oneningt/INFORM\net al., 2022; Wang et al., 2023) have highlighted\nthe importance of elaborating the reasoning steps\nin exemplars, leading to the emergence of chain-\nof-thought (CoT) prompting. CoT has shown\npromising results in improving the reasoning abil-\nities of LLMs. Various strategies like self-notes\n(Lanchantin et al., 2023), progressive-hint prompt-\ning (Narang et al., 2023) and Least-to-Most prompt-\ning (Chowdhery et al., 2023), have been proposed\nto enhance CoT further.\nWhile these works have focused on leveraging\nthe inherent capabilities of models to augment the\noriginal prompts, recent research has shown that\nthe performance of CoT heavily relies on the choice\nof exemplars (Lu et al., 2022; Liu et al., 2022; Wei\net al., 2022; Zhang et al., 2023; Fu et al., 2023).\nThis has led to investigations into identifying exem-\nplars that maximize LLMs‚Äô reasoning abilities. No-\ntably, complexity-based prompting (Fu et al., 2023)\nhas introduced a selection strategy based on the\nnumber of rationale steps in annotated data. They\nhave observed that more complex CoT prompts\neffectively stimulate LLMs‚Äô multi-step reasoning\ncapabilities. However, this approach is limited by\nits dependency on carefully annotated data. In the\nabsence of annotations, it degrades to a querylen-\nbased strategy, which is more fragile to input noise.\nThis limitation highlights a fundamental draw-\nback of CoT prompting, i.e., the heavy reliance\non human engineering and the scarcity of anno-\ntated datasets, which are time-consuming to create.\nTo address this, previous works such as zero-shot-\ncot (Kojima et al., 2022) and Auto-CoT (Zhang\net al., 2023) attempted to alleviate the reliance on\nhuman effort in constructing CoT prompts. How-\never, they faced challenges such as low-quality gen-\neration and high computational cost.\nTo overcome the aforementioned issues, we pro-\nposed a comprehensive CoT prompting process\nthat bypasses the need for extensive human effort\nwhile improving the effectiveness of CoT. Our first\n3565\n0.5\n0.6\n0.7\n0.8\n0.9\n0 .2 0 .4 0 .6 0 .8 1\nACC  RATEÔºà%Ôºâ\nGPT-3.5-Turbo\n0\n0.05\n0.1\n0.15\n0.2\n0 .2 0 .4 0 .6 0 .8 1\nACC  RATEÔºà%Ôºâ\ntext-davinci-003\n0.5\n0.6\n0.7\n0.8\n0.9\n3 4 5 6 7\nACC  RATEÔºà%Ôºâ\nGPT-3.5-Turbo\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n3 4 5 6 7\nACC  RATEÔºà%Ôºâ\ntext-davinci-003\nINFORMATION ENTROPY SCORE COMPLEX STEPS\nFigure 1: We compared the performance (ACC rate) of GSM8K dataset along with the increase of complex\nreasoning step and the information entropy scores on two models: GPT-3.5-Turbo and text-davinci-003.\nand foremost objective is to identify a pre-selection\nstrategy that achieves comparable effectiveness to\ncomplexity-based prompting while remaining ap-\nplicable in all scenarios. Through extensive ex-\nperiments, we have discovered that information\nentropy can serve as an effective and accurate cri-\nterion. Figure 1 depicts the similar trends between\nperformance and the number of rationale steps and\ninformation entropy scores of queries, respectively,\non the GSM8K dataset using GPT-3.5-Turbo and\ntext-davinci-003. We consider information entropy\nto be a suitable criterion for several reasons: (1)\nInformation entropy can be applied to queries, mak-\ning it applicable to all datasets. (2) The information\nentropy score of a query provides insights into its\ncomplexity, which is related to the information con-\ntained in the annotated or generated prompt. (3) In-\nformation entropy is well-suited for both the gener-\nation and inference stages, enabling a coherent and\nunified framework, namely INFORM. Our experi-\nments further validate the effectiveness of informa-\ntion entropy. Besides, we combined our informa-\ntion entropy criteria with self-consistency (Wang\net al., 2023), extending our strategy from the input\nspace (question selection) to the output space (CoT\ngeneration and IE inference).\nIn a nutshell, the contributions of our work are:\n‚Ä¢ We introduce information entropy as a new cri-\nterion for CoT prompts selection, improving\nLLMs‚Äô performance on reasoning tasks.\n‚Ä¢ We further apply information entropy criteria\nto the inference stage, generate reliable out-\nputs automatically and save computation costs\nin the meantime\n‚Ä¢ We propose INFORM, a comprehensive and\neffective CoT prompting framework con-\nsisting of three stages: question selection,\nCoT generation and information entropy self-\nconsistency inference.\n‚Ä¢ Experimental results demonstrate the effec-\ntiveness of INFORM on different reasoning\ntasks utilizing different models.\n2 Related Work\n2.1 In-context Learning\nRecent studies have shown that with the ever-\nincreasing scale of language models, they show\na remarkable ability to perform in-context learning\n(ICL) on downstream tasks (Brown et al., 2020;\nKaplan et al., 2020). ICL enables LLMs to tackle\na target task by using a few prompted examples\nas part of the input, allowing them to solve uni-\nversal tasks without the need for gradient updates.\nHowever, it has been demonstrated that the perfor-\nmance of ICL is influenced by the prompts used\n(Liu et al., 2022, 2023). Therefore, determining the\noptimal prompt is a crucial and fundamental area\nof research.\n2.2 Chain-of-Thought Prompt\nChain-of-Thought(CoT) is a novel prompting\nparadigm proposed by Wei et al. (2022), which\ninvolves a series of rationale steps leading to the\nfinal answer. CoT has been shown to signifi-\ncantly enhance performance on complex reasoning\ntasks such as arithmetic and commonsense reason-\ning. This success has spurred several subsequent\nworks that adapt different strategies to improve\nCoT, including self-consistency (Wang et al., 2023),\nLeast-to-Most prompting (Chowdhery et al., 2023),\n3566\nself-notes (Lanchantin et al., 2023), Progressive-\nHint Prompting (Narang et al., 2023), and self-\npolish (Wei et al., 2023).\nDespite the remarkable success of CoT prompt-\ning, previous studies have primarily focused on\nhow to use CoT to achieve the best results while\nignoring how to construct prompting examples. Re-\nsearch by Liu et al. (2023) and Lu et al. (2022) has\nshown that the effectiveness of CoT prompting can\nvary widely depending on the choice of CoT exam-\nples. Given that data with manually annotated rea-\nsoning chains are scarce, and it is time-consuming\nto annotate rational chains manually in addition to\nthe selection of questions and the accuracy of an-\nnotations that need to be considered, constructing\nappropriate prompts has been identified as a critical\naspect of CoT prompting.\n2.3 Example Construction For Prompting\nPrevious works on example construction for\nprompting can be viewed from two perspectives:\nrule-based selection and automated generation.\nRule-based Selection Rule-based selection\nmethods employ various criteria to select optimal\nprompts from the original space. One intuitive strat-\negy is to choose the most similar examples to the\ntest question as its prompts. Nie et al. (2022) use\nnearest neighbor calibration to adjust the similarity\nscores between the input and the few-shot exam-\nples. KATE (Liu et al., 2022) shared the same strat-\negy but scored the similarity based on the distance\namong embeddings. However, similarity-based\nstrategies are usually computationally expensive.\nAs an alternative, fairness-guided prompting (Ma\net al., 2023) employs a content-free strategy that\nuses fairness as a metric to evaluate the predictive\nbias of a fixed prompt and shows that model per-\nformance is highly consistent with fairness. For\nCoT prompting, fairness-based methods require\nreasoning chain annotations for the whole training\nset, which compromises their advantage of being\nfew-shot. Fu et al. (2023) propose that the rea-\nsoning ability of LLMs can be elicited by more\ncomplex CoT prompts, and the number of CoT\nsteps can determine the complexity of the prompt.\nTheir experiments demonstrate that selecting more\ncomplex CoT examples helps LLMs solve complex\nreasoning tasks. However, their complexity-based\ncriteria heavily rely on labeled CoT data and degen-\nerate to querylen-based when there is no labeled\nCoT data. In addition, Diao et al. (Diao et al.,\n2023) proposes an uncertainty-based active selec-\ntion strategy to determine which questions are the\nmost important and helpful to annotate from a pool\nof task-specific queries.\nIn the realm of rule-based selection, most current\nmethods tend to focus on searching prompts along\na single dimension and either excessively rely on\nlabeled data or require manual prompt construc-\ntion. Our work sits in CoT prompting and proposes\na novel IE-based selection strategy that evaluates\nqueries from multi-dimension and constructs the\nCoT prompt automatically.\nAutomated Generation It is observed that data\nwith annotated reasoning chains are scarce, and\nit‚Äôs time-consuming to annotate manually. Some\nresearchers have committed to automating the gen-\neration of CoT prompts for language models.\nAuto-CoT (Zhang et al., 2023) classify questions\ninto different clusters, select representative ques-\ntions with diversity, and generate reasoning chains\nby zero-shot CoT prompt to construct demonstra-\ntions. Huang et al. (2022) demonstrate that LLM is\nalso capable of self-improving with only unlabeled\ndatasets. They use a pre-trained LLM to gener-\nate high-confidence rationale-augmented answers\nfor unlabeled questions using Chain-of-Thought\nprompting and self-consistency and fine-tune the\nLLM using those self-generated solutions as tar-\nget outputs. Automate-CoT (Shum et al., 2023)\nfirst augment rational chains from a small labeled\ndataset, then prune low-quality chains to construct\na candidate pool of machine-generated rationale\nchains based on the labels, and finally apply a\nvariance-reduced policy gradient strategy to esti-\nmate the gradients and optimize the latent variables\nwith estimated gradients to select CoT.\nThese methods have extremely dispensed hu-\nman efforts in generating CoT examples; however,\nmost of them suffer from extra training costs, de-\npendency on annotated data, and ignorance of the\nselection of questions. Our work proposes a com-\nprehensive process of CoT prompting, covering\nrule-based selection of query, automatical genera-\ntion of rationale steps and improvement of infer-\nence results.\n3 INFORM\nThe overall schematic of our proposed INFORM\nare illustrated in Figure 2. Our approach is devel-\noped based on Information Entropy criteria, which\nwe integrate into three different stages of standard\n3567\nQuestion: The man had spent all day collecting old appliance‚Ä¶‚Ä¶\nAnswer: Let‚Äôs think step by step.\nDiversity Pruning\nQuestion: Sarah wanted to by a new\nrug Ink spilled onto her old one‚Ä¶‚Ä¶\nQuestion: Billy didn't know what to\ndo. He had just bought a huge ficus,\nbut he has no where to put it. It was\njust too gid for his house‚Ä¶‚Ä¶.\nLow\nHigh\nOrder Augment\nIE-Score:5.20\nIE-Score:5.15\nIE Ranking\nQuestion:Danny was meeting his friend James ‚Ä¶‚Ä¶\nIE-Score:4.74\nQuestion: Billy didn't know what to do . He had ‚Ä¶‚Ä¶\nIE-Score:5.20\nUnlabeled Questions\nQuestion:20 marbles were pulled out of a\nbag of only white marbles , painted black ,\nand then put back in . Then , another‚Ä¶‚Ä¶\nAnswer: Let‚Äôs think step by step.\nCoT Generation\nAs explained in the rationale, we know\nthat there are 20 black marbles in the\nbag‚Ä¶‚Ä¶when we consider the original 20\nmarbles that were pulled out and painted\nblack and then put back in the bag. So the\ncorrect answer is 400.\nFirst, we know that there are 20 black\nmarbles in the bag. We also know that\nwhen 20 marbles ‚Ä¶‚Ä¶Therefore, the bag\ncurrently holds 2000 marbles in total. So\nthe final answer is 2000.\nWrong Answer\nWe know that there are 20 black marbles\nin the bag, and this represents 1/20th of\nthe total number of marbles in the\nbag. ‚Ä¶‚Ä¶ Solving for x, we get: x = 20/1 *\n400/20 = 400.Therefore,the bag currently\nholds 400 marbles in total. So the correct\nanswer is 400.\nHighest IE\nRight Answer\nInput\nQuestion:20 marbles were pulled out of a bag of only white‚Ä¶‚Ä¶\nAnswer: Let‚Äôs think step by step.As explained in the rationale, we\nknow that there are 20 black ‚Ä¶‚Ä¶So the correct answer is 400.\nThe man had spent all day \ncollecting old appliance\nTest question\nùêª ùë• = ‚àí‡∑ç\n·à∂ùëñ=1\nùëõ\nùëÉ ùë•ùëñ log2 ùëÉ ùë•ùëñIE-Score:\nAdjusts the Number of Samples\nIE-Score\n5\n20\n6\n19\nSelf-Consistency with Adjusted Number\nanswer 1\nanswer n\nMajority Vote\nQuestion Selection IE-SC Inference\nFigure 2: Overview of our proposed INFORM. Question Selection: Comprises three sub-steps: IE ranking, diversity\npruning, and order augmentation, aimed at selecting informative questions. CoT Generation: Automatically\ngenerates rationale steps for the selected questions and identifies the one with the highest IE score.IE-SC Inference:\nAdjusts the number of samples based on the IE score of the query, followed by applying the original self-consistency\nmethod to conduct a majority vote.\nfew-shot CoT prompting. Specifically, our pro-\nposed approach consists of three sequential stages:\nquestion selection, CoT generation and IE-SC in-\nference. We will introduce every stage in detail in\nthe following sections.\n3.1 Information Entropy\nInformation entropy describes the uncertainty or\nrandomness of information. The definition of infor-\nmation entropy is as follows:\nH(X) =‚àí\nn‚àë\ni=1\np(xi) log2 p(xi) (1)\nThe term p(xi) represents the probability of each\noutcome xi. Therefore, as the information entropy\nincreases, the sentence becomes more informative.\nAs demonstrated in the complexity-based strategy,\nthe more complex exemplars can significantly im-\nprove the performance of large language models.\n3.2 Question Selection\nPrevious research consistently demonstrates that\nselecting exemplars based on specific criteria can\nsignificantly improve the performance of LLMs in\nmulti-step reasoning tasks. We emphasize the im-\nportance of question selection instead of exemplar\nselection. Specifically, our question selection strat-\negy comprises three steps: IE ranking, diversity\npruning and order augment.\nIE Ranking Given a set of unlabeled ques-\ntions Q = ( q1,q2,q3...qn), we calculate their\ninformation entropy score denoted by H =\n(h1,h2,h3...hn), using Equation 1. We then select\nthe top-kquestions with the highest IE scores, as\nwe believe they will generate more informative ra-\ntionale steps. In our experiments, we set kto three\ntimes the number of candidates in the dataset.\nDiversity Pruning After obtaining a set of ques-\ntions with high IE scores, we consider incorporat-\ning a filtering step to enhance diversity. Specifi-\ncally, we calculate the cosine-similarity among the\nquestion set with:\ncos(Œ∏) =\n‚àën\ni=1 (xi √ó yi)‚àën\ni=1 (xi)2 √ó ‚àën\ni=1 (yi)2 (2)\nFor examples with high similarity (>80%), we\nonly keep the one with the highest IE score, dis-\ncarding the rest.\nOrder Augment As widely recognized, in-\ncontext learning is sensitive to the order of demon-\nstrations within prompts (Lu et al., 2022; Liu et al.,\n3568\n2022). Intuitively, we sort the order of questions\nbased on their information entropy score, effec-\ntively arranging them from easy to hard, aiming to\nexploit LLMs‚Äô rational reasoning ability gradually,\nwhich is similar to curriculum learning (Bengio\net al., 2009) in spirit.\n3.3 CoT Generation\nAfter employing our query selection strategy to\nacquire a specific set of queries, we proceed to au-\ntomatically generate the corresponding rationale\nsteps using the zero-shot-CoT technique (Kojima\net al., 2022). Note that we generate rationale steps\nonly when the data lacks annotated CoT; otherwise,\nwe utilize the original rationale steps. Taking in-\nspiration from previous work that leveraged multi-\nsampling to enhance LLMs‚Äô performance during\nthe inference stage (Wang et al., 2023), we extend\nthis approach to our CoT generation stage. Specif-\nically, given a selected query q, we append \"Let‚Äôs\nthink step by step\" to the end of the query and ask\nthe model to answer it k times. We set k = 10\nin our experiments. Once we obtain 10 distinct\nCoT sequences, we select the CoTs that have cor-\nrect answers based on the ground truth provided\nin the dataset. If the correct answer is not given,\nwe adhere to the original self-consistency approach\nto employ a majority voting mechanism. Finally,\nwe select the CoT sequence with the highest infor-\nmation entropy score among the selected correct\nanswers.\n3.4 Information Entropy Self-Consistency\nInference\nWang et al. (2023) proposed Self-Consistency,\nwhich improves performance for CoT reasoning by\nemploying multiple diverse reasoning chains and\naggregating their outputs using a simple majority\nvoting technique. However, this approach comes\nwith a trade-off of the increased computational cost\nthat the model must be prompted multiple times for\nthe same question. As LLMs continue to grow in\nsize, the cost becomes increasingly unacceptable.\nOur empirical observations revealed that having\nmore candidates does not always yield better per-\nformance, and using a fixed number of samples for\nevery question in a dataset can be inefficient, par-\nticularly for straightforward questions. To address\nthese issues, we introduce adaptive Information\nEntropy Self-consistency(IE-SC). Unlike the con-\nventional approach, IE-SC dynamically adjusts the\nnumber of samples n for each query instead of\nusing a fixed budget for the entire dataset. The ad-\njustment is based on the information entropy score\nof the query and can be expressed as follows:\nNi = Nmin + Nmax ‚àí Nmin\nHmax ‚àí Hmin\n‚àó (H (qi) ‚àí Hmin) (3)\nwhere H(qi) represents the IE-score of i-th query.\nHmin and Hmax denote minimum and maximum\nIE scores, which we set according to the dataset,\ntypically to 3 and 8. Nmin and Nmax indicate\nthe minimum and maximum number of samples,\nrespectively, which we set to 5 and 20 in our exper-\niment. Our experiment demonstrates that IE-SC, in\ncontrast to the original SC approach, is well-suited\nfor different datasets and different language mod-\nels, and effectively reduces the computational costs\nwithout compromising output quality.\n4 Experiments\n4.1 Experimental Settings\nDatasets We evaluate our approach across seven\nbenchmarks, covering three categories of reasoning\ntasks: (i) Arithmetic Reasoning: GSM8K (Cobbe\net al., 2021), MultiArith (Roy and Roth, 2015),\nAddsub (Hosseini et al., 2014), AQuA (Ling\net al., 2017) (ii) Commonsense Reasoning: Com-\nmonsenseQA (Talmor et al., 2019) and Strate-\ngyQA (Geva et al., 2021) (iii) Temporal Reasoning:\nDate-Understanding (Zhang et al., 2021). Among\nthese datasets, GSM8K, AQuA, and StrategyQA\nprovide their own rationale steps within their train-\ning splits. Hence, for these datasets, we seam-\nlessly integrate our strategy into the query selec-\ntion and inference stages, leveraging the original\nrationales. For the remaining datasets, we first se-\nlect the queries and then automatically generate the\nChains of Thought (CoT) prompts. By default, we\nemploy the test split for evaluation. In cases where\ntest sets are not provided, we assess performance\non the validation set instead.\nImplementation We utilize the text-davinci-003\nand GPT-3.5-Turbo version of the public ChatGPT\nmodel from the OpenAI API with 175 billion pa-\nrameters (Brown et al., 2020; Ouyang et al., 2022)\nfor most of our experiments. We selected these two\nLLMs because they are the latest and most robust\nmodels available. For most datasets, queries are\nselected from the official training split. Given that\nsome datasets only have the test split, we select\nqueries from the validation split or from the test\n3569\nsplit itself, excluding them from subsequent experi-\nments. Following Wei et al. (2022), we set the num-\nber of demonstrations k to 8, except for AQuA(4),\nStrategyQA (6), and CSQA (7). In datasets lack-\ning annotated reasoning steps, we automatically\ngenerate CoTs and select the one with the highest\nIE score among the correct answers. Following\nKojima et al. (2022), we add ‚ÄúLet‚Äôs think step by\nstep‚Äù before the reasoning chains for all prompting\nschemes to improve the performance. During the\nstandard decoding process, we set the temperature\nto 0 and employ a greedy search algorithm to ob-\ntain results. Under the IE-SC setting, we set the\ntemperature to 0.7 and determine the number of\ngenerations adaptively based on the IE score of the\nquery, ranging from 5 to 20.\nBaselines We compare our IE-CoT with three\nbaselines: manual-CoT, complex-CoT, and Auto-\nCoT. To ensure fairness between our method with\nother baselines, we use the same number of CoT ex-\namples in all methods and datasets. We implement\nthese methods on our own, following the original\nsettings in the corresponding paper.\n‚Ä¢ Manual-CoT (Wei et al., 2022) is a widely\nused naive method for CoT prompting con-\nstruction, which provides 4 to 8 manual-\nwritten exemplars consisting of a series of\nmanual-written intermediate reasoning steps.\n‚Ä¢ Complex-CoT (Fu et al., 2023) utilizes the\nnumber of reasoning steps as criteria to se-\nlect more complex exemplars in the annotated\ndata.\n‚Ä¢ Auto-CoT (Zhang et al., 2023) is an automatic\nCoT exemplars construction method that clus-\nters questions with diversity and generates\nCoT prompts by zero-shot-CoT.\n4.2 Results\nThe main results are displayed in Table 1. Overall,\nour method consistently matches or exceeds the per-\nformance of baselines on most datasets, utilizing\ntwo different large language models. Our IE-CoT\nachieves an average improvement of 3.28% and\n2.87% compared to manual-CoT with GPT-3.5-\nTurbo and text-davinci-003, respectively. More-\nover, under the IE-SC settings, the improvement\nfurther increased to 7.38% and 6.51%. These re-\nsults unequivocally demonstrate the effectiveness\nof our proposed approach. In the subsequent sec-\ntions, we categorize the datasets based on the pres-\nence of annotated rationale steps and provide a\ndetailed discussion of our findings.\nAnnotated Dataset For datasets obtaining rea-\nsoning annotations themselves, such as GSM8K,\nAQuA, and StrategyQA, we simply apply our strat-\negy to the question selection and inference stage,\nutilizing the original rationale steps from the train-\ning set. In comparison to complexity-based prompt-\ning, dedicated to selecting examples in the an-\nnotated dataset, our approach outperforms it in\nmost datasets using two models. One exception\nis GSM8K dataset with text-davinci-003, where\nour method slightly lags behind the complexity-\nbased strategy by 0.68%. Upon conducting a thor-\nough analysis of the reasons, we have discovered\na plausible explanation that the GSM8K dataset is\nmeticulously annotated and the questions exhibit\na significant diversity, allowing the complexity-\nbased selection strategy to achieve surprising re-\nsults. Furthermore, under the IE-SC setting, our\nmethod demonstrates additional improvements of\n7.12%, 9.37% and 4.17%, respectively, with GPT-\n3.5-Turbo. Compared to methods automatically\ngenerating the CoT prompt, like Auto-CoT, our ap-\nproach effectively leverages the labeled data, elim-\ninating the need for augmenting extra CoT exem-\nplars. In Appendix A, we supplement the experi-\nmental evidence that using original rationale steps\nleads to better results compared to generating it\nvia LLMs, while also saving costs. Meanwhile,\nour strategy outperforms Auto-CoT by 3.03% and\n2.15% on average with these three datasets with\nGPT-3.5-Turbo and text-davinci-003, respectively.\nThis discrepancy can be attributed to our strategy‚Äôs\nselection of queries based on predefined rules and\nthe original manually constructed rationale steps,\nwhich is more reliable compared to the zero-shot\nCoT generated in Auto-CoT.\nUnlabeled Dataset Our method consistently\nshowcases improvements across the remaining\nunlabeled datasets, namely MultiArith, Addsub,\nCSQA, and Date-Understanding. In contrast with\nthe competitive baseline, Auto-CoT, which auto-\nmatically generates CoT prompt using zero-shot-\nCoT, the same as our approach, our approach sur-\npasses it on all datasets utilizing GPT-3.5-Turbo\nand text-davinci-003. The most notable advance-\nments are observed in the Addsub and Date-\n3570\nGSM8K MultiArith Addsub AQuA CSQA StrategyQA DATE\nLaMDA-137B\nManual 14.30 44.90 51.90 20.60 57.90 65.40 26.80\nSelf-consistency 27.70 75.70 63.50 26.80 63.10 67.80 -\nPaLM-540B\nManual 56.90 94.70 91.90 35.80 79.90 77.80 65.30\nSelf-consistency 74.40 99.30 93.70 48.30 80.70 81.60 -\nGPT-3.5-Turbo\nManual 79.59 97.33 89.11 55.51 71.08 60.21 77.60\nComplex 80.89 97.66 92.65 56.29 73.46 64.73 75.20\nAuto-CoT 76.62 96.28 88.22 54.53 72.84 62.28 79.50\nIE-CoT(ours) 81.65 98.66 92.91 58.59 74.69 64.86 82.00\n+ IE-SC 88.77 99.16 94.68 67.96 76.50 69.03 86.00\ntext-davinci-003\nManual 56.55 94.16 85.75 43.70 77.55 68.16 78.40\nComplex 60.42 95.16 88.35 44.88 77.64 70.08 75.60\nAuto-CoT 57.63 95.08 87.94 42.60 76.80 70.68 78.00\nIE-CoT(ours) 59.74 96.00 89.10 45.66 78.10 70.95 84.80\n+ IE-SC 65.20 97.33 91.13 51.90 80.09 74.58 89.60\nTable 1: The overall performance of INFORM and the comparison against existing methods under different\nmodels on seven reasoning tasks. Manual-CoT, Complex and Auto-CoT denote chain-of-thought (Wei et al., 2022),\ncomplexity-based prompting (Fu et al., 2023) and Auto-CoT (Zhang et al., 2023), respectively. IE-SC denotes our\nperformance under the IE-SC setting. Bold denotes the best performance in performed methods. Underline denotes\nthe second performance. LaMDA (Thoppilan et al., 2022) and PaLM (Chowdhery et al., 2022) are not accessible to\nthe public, so their numbers are from corresponding papers. The remaining methods are our own implementation.\nunderstanding datasets, where our proposed strat-\negy outperforms Auto-CoT by 3.85% and 3.98% on\naverage with GPT-3.5-Turbo and text-davinci-003.\nThis can be attributed to the relatively simplified\nnature of queries in these two datasets. While au-\ntomatically generated CoT prompts may provide\nlimited information, our strategy diligently selects\nthe most informative queries and prompts during\nthe selection and generation stages, resulting in\na more effective prompting effect than Auto-CoT.\nNoting that complexity-based strategy degrades to\nquerylen-based when handling unlabeled datasets,\nleading to the selection of highly similar questions.\nIt lagged behind 2.32% and 2.82% on average by\nour approach with GPT-3.5-Turbo and text-davinci-\n003, respectively. This serves as further evidence\nof the effectiveness and versatility of our approach.\n5 Analysis\nWe conducted additional experiments to assess the\neffectiveness of INFORM and analyze the contri-\nbutions of each component of INFORM. Due to\nthe high cost associated with using the text-davinci-\n003 API, we primarily utilized GPT-3.5-Turbo for\nthe additional experiments.\nEffects of Question SelectionAs shown in Fig 2,\nthe query selection stage involves three steps: IE\nDatasets GSM8K AQuA\nRandom 79.59 55.51\nIE-low 78.22 53.93\nIE-high 80.42 56.69\nIE-high + div 81.50 58.26\nIE-high + div + order 81.65 58.59\nTable 2: Effect of every step in the query selection stage.\nMultiA Addsub CSQA\nRandom 98.33 92.31 72.76\nIE-low 97.83 91.17 73.49\nIE-high 98.66 92.91 74.69\nTable 3: Effect of information entropy score in CoT\ngeneration stage.\nranking, diversity pruning, and order augment. To\nevaluate the effectiveness of each step, we con-\nducted an ablation study by analyzing their in-\ndividual contributions. As presented in Table 2,\nour method consistently achieved improvements\non two datasets. Specifically, IE-low and IE-high\nrefer to the selection of queries with low informa-\ntion entropy scores and high information entropy\nscores, respectively. A substantial average gap of\n2.48% is observed between these two extreme con-\ntrasts. Additionally, IE-low lags behind randomly\nselected exemplars by an average of 1.47%, high-\n3571\n86\n87\n88\n89\n90\n91\nGSM8K\n5 10 adaptive-ie 15 20\n60\n62\n64\n66\n68\n70\nAQuA\n5 10 15 adaptive-ie 20\n72\n73\n74\n75\n76\n77\n78\nCSQA\n5 10 adaptive-ie 15 20\n5 10 12.9 15 20 5 10 15 15.2 20 5 10 11.8 15 20\nIE-SC\nSC\nIE-SC\nSC\nIE-SC\nSC\nFigure 3: The performance of self-consistency with original self-consistency and our proposed IE-SC. The X-axis\nmeans the average number of candidates and the Y-axis means the exact accuracy.\nModels BLOOM-7b OPT-6.7b Alpaca-7b Alpaca-13b Davinci-002\nManual-CoT 3.77 4.70 8.64 11.06 48.06\nIE-CoT 4.25 5.48 9.70 17.21 50.49\nTable 4: The performance of INFORM on five language models on GSM8K dataset.\nlighting that queries with low information entropy\nlead to prompts lack of information, hampering\nthe reasoning ability of LLMs. Moreover, the ap-\nplication of diversity pruning brings an additional\n1.08% improvement for IE-high queries. This im-\nprovement can be attributed to the decreased simi-\nlarity between queries, which provides diverse ide-\nology to LLMs, enabling them to handle differ-\nent types of challenging tasks. As widely recog-\nnized, in-context learning is sensitive to the order of\ndemonstrations within prompts (Lu et al., 2022; Liu\net al., 2022). Indeed, the result reveals that order-\naugment brings improvement; however, the mag-\nnitude of this improvement is very slight(0.24%).\nWe leave further study on how to determine the op-\ntimal ordering of exemplars within prompts to fully\nexploit the capabilities of LLMs in future work.\nEffects of IE in CoT generationAfter apply-\ning our query selection strategy to obtain a spe-\ncific set of queries, we proceed to automatically\ngenerate the corresponding rationale steps using\nthe zero-shot-CoT technique (Kojima et al., 2022).\nNoting that we generate rationale steps only when\nthe data lacks annotated CoT. Therefore, we select\nthree datasets, namely MultiA, Addsub, and CSQA,\nwhich do not have labeled rationales, to evaluate\nthe effectiveness of choosing CoT prompts with\nhigh information entropy scores. As depicted in Ta-\nble 3, selecting high information entropy (IE) score\nprompts consistently outperforms the other two\nstrategies: random selection and selecting prompts\nwith low IE scores. This observation suggests that\nCoT prompts with high IE scores are more effective\nin stimulating LLMs‚Äô reasoning ability, leading to\nimproved performance.\nEffects of IE-SC We conducted further anal-\nysis of our proposed Information Entropy Self-\nConsistency (IE-SC). As depicted in Fig 3, while\nself-consistency undeniably yields significant im-\nprovements for reasoning tasks, it is noteworthy\nthat better performance does not necessarily rely\non a larger candidate pool, as evidenced by the\nresults on the CSQA dataset. Our proposed IE-\nSC approach achieves a remarkable 33% reduc-\ntion in computational cost (equivalent to reducing\n6.7 instances per query) compared to the original\nSC method, which samples 20 candidates. This\nreduction in cost comes with a minimal average\ndecrease in accuracy of less than 0.2%. These\nfindings demonstrate that our proposed approach\nstrikes a favorable balance between effectiveness\nand efficiency.\nRobustness of INFORM We performed addi-\ntional experiments on various large language mod-\nels to assess the robustness of our proposed IN-\nFORM approach. As illustrated in Table 4, our\nmethod consistently achieves improvements across\ndifferent models, including the previous version of\nGPT-3.5 (text-davinci-002) as well as smaller-scale\nlanguage models such as Alpaca-7b and Alpaca-\n13b. Since some larger-scale models are unavail-\nable, i.e., Palm and LaMDA , we have not con-\nducted experiments on them. Even so, these results\nhave clearly demonstrated the versatility and effec-\ntiveness of INFORM, indicating its applicability\nto a wide range of language models. We further\ndiscuss the impact of linguistic characteristics on\nour approach and and provide some examples for\nfurther clarification in the appendix B and C.\n3572\nAcknowledgements\nThis work is supported by the National Science\nFoundation of China (NSFC No. 62276077 and\n62206194), Shenzhen College Stability Support\nPlan under Grants GXWD20220811170358002\nand GXWD20220817123150002, and the Natu-\nral Science Foundation of Jiangsu Province (Grant\nNo. BK20220488).\n6 Conclusion\nThis paper presents INFORM, a novel framework\nfor CoT prompting that consists of a comprehen-\nsive process encompassing query selection, CoT\ngeneration, and IE-SC inference. INFORM is de-\nsigned to be adaptable to various datasets and mod-\nels, effectively improving the performance of CoT\nprompts and mitigating the requirement for addi-\ntional human involvement. Our experimental re-\nsults demonstrate the efficacy and versatility of\nINFORM, showcasing its ability to significantly\nenhance CoT performance in various scenarios.\nLimitations\nDespite the promising results of our proposed\nINFORM, it comes with several limitations that\nshould be addressed in future work:\n‚Ä¢ While queries are typically manually con-\nstructed, our query selection strategy may be\nsusceptible to noise when the queries contain\nirrelevant information. Integrating additional\ncriteria could help mitigate this issue.\n‚Ä¢ Our experiments focused solely on reason-\ning tasks, showcasing the effectiveness of IN-\nFORM with CoT prompts. However, the per-\nformance of INFORM on non-reasoning tasks\nremains unknown.\n‚Ä¢ Although we have evaluated the robustness of\nINFORM on several different LLMs of vary-\ning sizes, there may still be models, such as\nvery large-scale models like Palm, where IN-\nFORM may not be as effective.\nThese limitations highlight areas for further inves-\ntigation and refinement in order to enhance the\napplicability and performance of INFORM in a\nwider range of scenarios.\nReferences\nYoshua Bengio, J√©r√¥me Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th annual international confer-\nence on machine learning, pages 41‚Äì48.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAakanksha Chowdhery, Sharan Narang, Xuezhi Wang,\nJason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi,\nand Denny Zhou. 2023. Least-to-most prompting\nenables complex reasoning in large language models.\nIn International Conference on Learning Representa-\ntions.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. arXiv preprint\narXiv:2302.12246.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning. In International Conference on\nLearning Representations.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346‚Äì\n361.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn EMNLP, pages 523‚Äì533.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv e-\nprints, pages arXiv‚Äì2001.\n3573\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nJack Lanchantin, Shubham Toshniwal, Jason Weston,\nArthur Szlam, and Sainbayar Sukhbaatar. 2023.\nLearning to reason and memorize with self-notes.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ¬¥c, Daniel Hesslow, Roman\nCastagn√©, Alexandra Sasha Luccioni, Fran√ßois Yvon,\nMatthias Gall√©, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158‚Äì167, Vancouver,\nCanada. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (Dee-\nLIO 2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100‚Äì114.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1‚Äì35.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086‚Äì8098.\nHuan Ma, Changqing Zhang, Yatao Bian, Lemao Liu,\nZhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu,\nQinghua Hu, and Bingzhe Wu. 2023. Fairness-\nguided few-shot prompting for large language mod-\nels. arXiv preprint arXiv:2303.13217.\nSharan Narang, Aakanksha Chowdhery, Xuezhi Wang,\nJason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi,\nand Denny Zhou. 2023. Progressive-hint prompting\nimproves reasoning in large language models. In In-\nternational Conference on Learning Representations.\nFeng Nie, Meixi Chen, Zhirui Zhang, and Xu Cheng.\n2022. Improving few-shot performance of language\nmodels via nearest neighbor calibration.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743‚Äì1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. In The Eleventh International Conference\non Learning Representations.\nKaShun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data. arXiv preprint\narXiv:2302.12822.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of NAACL-HLT, pages 4149‚Äì\n4158.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nInternational Conference on Learning Representa-\ntions.\nJason Wei, Xuezhi Wang, Sharan Narang, Aakanksha\nChowdhery, Dale Schuurmans, Quoc V Le, Ed H\nChi, and Denny Zhou. 2023. Self-polish: Enhance\nreasoning in large language models via problem re-\nfinement. In International Conference on Learning\nRepresentations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Advances in\nNeural Information Processing Systems.\n3574\nZheng Zhang, Yuanhe Zhang, and Zhihong Zhou. 2021.\nWebgqa: A new benchmark for web-based multilin-\ngual question answering. In Proceedings of the 2021\nConference of the Association for Computational Lin-\nguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023. Automatic chain of thought prompting\nin large language models. In International Confer-\nence on Learning Representations.\n3575\nA Compared With Original CoT\nTo elucidate the implications of disregarding the\nincluded CoT strings and relying on INFORM in-\nstead, we conducted supplementary experiments\nusing GPT-3.5-Turbo. The experimental setup in-\nvolved three conditions: \"INFORM-Without-CoT,\"\nwhich involved providing only the selected queries\nand their corresponding direct answers as examples\nwithout including the rationale steps; \"INFORM-\nGenerated-CoT,\" which entailed generating CoTs\nusing INFORM for the selected questions, specif-\nically through the CoT-generation operation; and\n\"INFORM-Original-CoT,\" which employed the\noriginal CoTs included in the datasets as per our\noriginal strategy. The obtained results are presented\nbelow:\nGSM8K AQuA StrategyQA\nWithout-CoT 63.47 55.69 52.14\nGenerated-CoT 75.18 56.71 62.58\nOrginal-CoT 81.65 58.59 64.86\nTable 5: Compare Generated-CoT with Orginal-CoT.\nBased on the obtained results, it can be inferred\nthat the Original-CoT strategy exhibited the most\nfavorable outcome, indicating that a manually con-\nstructed CoT outperforms a zero-shot-CoT gen-\nerated by the model in the majority of scenarios.\nNevertheless, when compared to examples with-\nout CoTs, the automatically generated CoT still\ndemonstrated a significant improvement.\nB Linguistic Characteristics\nIt is important to notice that language/discourse-\nbased reasoning, different from reasoning in gen-\neral, may be affected by the linguistic character-\nistics of the language used. Our work has not ex-\nplicitly addressed the syntactic, semantic, and prag-\nmatic aspects of the language. The focus of our\nwork, however, is on how to select or construct\nbetter CoT examples through simple analysis of\nthe data in the training set rather than how linguis-\ntic characteristics would affect the performance of\nlarge models on reasoning tasks. Our findings can\nserve as a starting point for researchers interested in\nexploring the impact of language-specific features\non reasoning tasks.\nIn addition, shi et al( 2022) have demonstrated\nthat reasoning in English (EN-CoT) consistently\nachieves competitive or better performance than\nreasoning in the native language of the question,\nwhich implies that our method can generalize to\nother languages. We have conducted some addi-\ntional experiments in other languages; we built\nthree Chinese CoT datasets that were translated\nfrom English. The results are as follows:\nMultiA_zh Strategyqa_cn Addsub_cn\nManual-CoT 96.43 68.14 81.20\nIE-CoT 98.55 68.85 90.35\nTable 6: Performance of IE-CoT in Chinese-CoT\ndatasets.\nC Case Study\nWhen compared to alternative methodologies, such\nas the selection of queries based on complexity, the\nutilization of query-len in datasets lacking ratio-\nnales may lead to the occurrence of invalid char-\nacter placeholders, exemplified by the inclusion\nof\n\"Question: Shipment - - - No . of Defective\nChips / shipment - - - Total Chips in shipment\nS 1- - - - - - - 3- - 8,000\nS 2- - - - - - - 5- - 12,000\nS 3- - - - - - - 6- - 18,000\nS 4- - - - - - - 4- - 16,000\nA computer chip manufacturer expects the ratio\nof the number of defective chips to the total num-\nber of chips in all future shipments to equal the\ncorresponding ratio for shipments S 1 , S 2 , S 3 ,\nand S 4 combined , as shown in the table above .\nWhat ‚Äô s the expected number of defective chips in\na shipment of 60,000 chips ?\nOptions: (A) 14 (B) 22 (C) 20 (D) 24 (E) 25\nA: Let‚Äôs think step by step. for a total of 51000\nchips ( adding S 1 , S 2 , S 3 , S 4) total number of\ndefective chips is 17 ( adding defective chips of S\n1 , S 2 , S 3 , S 4) so ratio is 18 / 54000 or 1 every\n3000 chips . Keeping this ratio constant for 60000\nchips number of defective chips will be ( 1 / 3000)\n* 60000 = 20 The answer is (C), 20.\"\nFor complex detection methods, special charac-\nters are used to define the steps of rationale, it is sen-\nsitive to different characters for different datasets.\n3576"
}