{
  "title": "Personalized Language Model for Query Auto-Completion",
  "url": "https://openalex.org/W2949628266",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5082970015",
      "name": "Aaron Jaech",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5087215613",
      "name": "Mari Ostendorf",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2740195281",
    "https://openalex.org/W2294684023",
    "https://openalex.org/W1982858363",
    "https://openalex.org/W2964352131",
    "https://openalex.org/W2522306626",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2787178450",
    "https://openalex.org/W2093245971",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2252235528",
    "https://openalex.org/W2951962790",
    "https://openalex.org/W2093646604",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963748792"
  ],
  "abstract": "Query auto-completion is a search engine feature whereby the system suggests completed queries as the user types. Recently, the use of a recurrent neural network language model was suggested as a method of generating query completions. We show how an adaptable language model can be used to generate personalized completions and how the model can use online updating to make predictions for users not seen during training. The personalized predictions are significantly better than a baseline that uses no user information.",
  "full_text": "Personalized Language Model for Query Auto-Completion\nAaron Jaech and Mari Ostendorf\nUniversity of Washington\n{ajaech, ostendor}uw.edu\nAbstract\nQuery auto-completion is a search engine\nfeature whereby the system suggests com-\npleted queries as the user types. Recently,\nthe use of a recurrent neural network lan-\nguage model was suggested as a method of\ngenerating query completions. We show\nhow an adaptable language model can be\nused to generate personalized completions\nand how the model can use online updat-\ning to make predictions for users not seen\nduring training. The personalized predic-\ntions are signiﬁcantly better than a base-\nline that uses no user information.\n1 Introduction\nQuery auto-completion (QAC) is a feature used by\nsearch engines that provides a list of suggested\nqueries for the user as they are typing. For in-\nstance, if the user types the preﬁx “mete” then the\nsystem might suggest “meters” or “meteorite” as\ncompletions. This feature can save the user time\nand reduce cognitive load (Cai et al., 2016).\nMost approaches to QAC are extensions of the\nMost Popular Completion (MPC) algorithm (Bar-\nYossef and Kraus, 2011). MPC suggests com-\npletions based on the most popular queries in the\ntraining data that match the speciﬁed preﬁx. One\nway to improve MPC is to consider additional sig-\nnals such as temporal information (Shokouhi and\nRadinsky, 2012; Whiting and Jose, 2014) or infor-\nmation gleaned from a users’ past queries (Shok-\nouhi, 2013). This paper deals with the latter of\nthose two signals, i.e. personalization. Personal-\nization relies on the fact that query likelihoods are\ndrastically different among different people de-\npending on their needs and interests.\nRecently, Park and Chiba (2017) suggested a\nsigniﬁcantly different approach to QAC. In their\nCold Start Warm Start\n1 bank of america bank of america\n2 barnes and noble basketball\n3 babiesrus baseball\n4 baby names barnes and noble\n5 bank one baltimore\nTable 1: Top ﬁve completions for the preﬁx “ba”\nfor a cold start model with no user knowledge\nand a warm model that has seen the queries espn,\nsports news, nascar, yankees, and nba.\nwork, completions are generated from a charac-\nter LSTM language model instead of by ranking\ncompletions retrieved from a database, as in the\nMPC algorithm. This approach is able to com-\nplete queries whose preﬁxes were not seen during\ntraining and has signiﬁcant memory savings over\nhaving to store a large query database.\nBuilding on this work, we consider the task of\npersonalized QAC, advancing current methods by\ncombining the obvious advantages of personaliza-\ntion with the effectiveness of a language model in\nhandling rare and previously unseen preﬁxes. The\nmodel must learn how to extract information from\na user’s past queries and use it to adapt the gen-\nerative model for that person’s future queries. To\ndo this, we leverage recent advances in context-\nadaptive neural language modeling. In particular,\nwe make use of the recently introduced FactorCell\nmodel that uses an embedding vector to additively\ntransform the weights of the language model’s re-\ncurrent layer with a low-rank matrix (Jaech and\nOstendorf, 2017). By allowing a greater fraction\nof the weights to change during personalization,\nthe FactorCell model has advantages over the tra-\nditional approach to adaptation of concatenating a\ncontext vector to the input of the LSTM (Mikolov\nand Zweig, 2012).\nTable 1 provides an anecdotal example from\narXiv:1804.09661v1  [cs.CL]  25 Apr 2018\nthe trained FactorCell model to demonstrate the\nintended behavior. The table shows the top ﬁve\ncompletions for the preﬁx “ba” in a cold start sce-\nnario and again after the user has completed ﬁve\nsports related queries. In the warm start scenario,\nthe “baby names” and “babiesrus” completions no\nlonger appear in the top ﬁve and have been re-\nplaced with “basketball” and “baseball”.\nThe novel aspects of this work are the appli-\ncation of an adaptive language model to the task\nof QAC personalization and the demonstration of\nhow RNN language models can be adapted to con-\ntexts (users) not seen during training. An addi-\ntional contribution is showing that a richer adapta-\ntion framework gives added gains with added data.\n2 Model\nAdaptation depends on learning an embedding for\neach user, which we discuss in Section 2.1, and\nthen using that embedding to adjust the weights of\nthe recurrent layer, discussed in Section 2.2.\n2.1 Learning User Embeddings\nDuring training, we learn an embedding for each\nof the users. We think of these embeddings as\nholding latent demographic factors for each user.\nUsers who have less than 15 queries in the train-\ning data (around half the users but less than 13% of\nthe queries) are grouped together as a single entity,\nuser1, leaving kusers. The user embeddings ma-\ntrix Uk×m, where mis the user embedding size, is\nlearned via back-propagation as part of the end-to-\nend model. The embedding for an individual user\nis the ith row of U and is denoted by ui.\nIt is important to be able to apply the model to\nusers that are not seen during training. This is\ndone by online updating of the user embeddings\nduring evaluation. When a new person, userk+1\nis seen, a new row is added to U and initialized to\nu1. Each person’s user embedding is updated via\nback-propagation every time they select a query.\nWhen doing online updating of the user embed-\ndings, the rest of the model parameters (everything\nexcept U) are frozen.\n2.2 Recurrent Layer Adaptation\nWe consider three model architectures which dif-\nfer only in the method for adapting the recurrent\nlayer. First is the unadapted LM, analogous to the\nmodel from Park and Chiba (2017), which does\nno personalization. The second architecture was\nintroduced by Mikolov and Zweig (2012) and has\nbeen used multiple times for LM personalization\n(Wen et al., 2013; Huang et al., 2014; Li et al.,\n2016). It works by concatenating a user embed-\nding to the character embedding at every step of\nthe input to the recurrent layer. Jaech and Osten-\ndorf (2017) refer to this model as the ConcatCell\nand show that it is equivalent to adding a termVu\nto adjust the bias of the recurrent layer. The hidden\nstate of a ConcatCell with embedding size e and\nhidden state size h is given in Equation 1 where\nσ is the activation function, wt is the character\nembedding, ht−1 is the previous hidden state, and\nW ∈Re+h×h and b ∈Rh are the recurrent layer\nweight matrix and bias vector.\nht = σ([wt,ht−1]W + b+ Vu) (1)\nAdapting just the bias vector is a signiﬁcant lim-\nitation. The FactorCell model, (Jaech and Os-\ntendorf, 2017), remedies this by letting the user\nembedding transform the weights of the recurrent\nlayer via the use of a low-rank adaptation ma-\ntrix. The FactorCell uses a weight matrix W′ =\nW + A that has been additively transformed by a\npersonalized low-rank matrixA. Because the Fac-\ntorCell weight matrix W′is different for each user\n(See Equation 2), it allows for a much stronger\nadaptation than what is possible using the more\nstandard ConcatCell model.1\nht = σ([wt,ht−1]W′+ b) (2)\nThe low-rank adaptation matrix A is generated\nby taking the product between a user’s mdimen-\nsional embedding and left and right bases tensors,\nZL ∈Rm×e+h×r and ZR ∈Rr×h×m as so,\nA = (ui ×1 ZL)(ZR ×3 ui) (3)\nwhere ×i denotes the mode-i tensor product. The\nabove product selects a user speciﬁc adaptation\nmatrix by taking a weighted combination of the\nmrank rmatrices held between ZL and ZR. The\nrank, r, is a hyperparameter which controls the de-\ngree of personalization.\n3 Data\nOur experiments make use of the AOL Query data\ncollected over three months in 2006 (Pass et al.,\n2006). The ﬁrst six of the ten ﬁles were used for\n1In the case of an LSTM, W′ is extended to incorporate\nall of the gates.\ntraining. This contains approximately 12 million\nqueries from 173,000 users for an average of 70\nqueries per user (median 15). A set of 240,000\nqueries from those same users (2% of the data)\nwas reserved for tuning and validation. From the\nremaining ﬁles, one million queries from 30,000\nusers are used to test the models on a disjoint set\nof users.\n4 Experiments\n4.1 Implementation Details\nThe vocabulary consists of 79 characters including\nspecial start and stop tokens. Models were trained\nfor six epochs. The Adam optimizer is used dur-\ning training with a learning rate of 10−3 (Kingma\nand Ba, 2014). When updating the user embed-\ndings during evaluation, we found that it is easier\nto use an optimizer without momentum. We use\nAdadelta (Zeiler, 2012) and tune the online learn-\ning rate to give the best perplexity on a held-out\nset of 12,000 queries, having previously veriﬁed\nthat perplexity is a good indicator of performance\non the QAC task.2\nThe language model is a single-layer character-\nlevel LSTM with coupled input and forget gates\nand layer normalization (Melis et al., 2018; Ba\net al., 2016). We do experiments on two model\nconﬁgurations: small and large. The small mod-\nels use an LSTM hidden state size of 300 and 20\ndimensional user embeddings. The large models\nuse a hidden state size of 600 and 40 dimensional\nuser embeddings. Both sizes use 24 dimensional\ncharacter embeddings. For the small sized mod-\nels, we experimented with different values of the\nFactorCell rank hyperparameter between 30 and\n50 dimensions ﬁnding that bigger rank is better.\nThe large sized models used a ﬁxed value of 60 for\nthe rank hyperparemeter. During training only and\ndue to limited computational resources, queries\nare truncated to a length of 40 characters.\nPreﬁxes are selected uniformly at random with\nthe constraint that they contain at least two charac-\nters in the preﬁx and that there is at least one char-\nacter in the completion. To generate completions\nusing beam search, we use a beam width of 100\nand a branching factor of 4. Results are reported\nusing mean reciprocal rank (MRR), the standard\nmethod of evaluating QAC systems. It is the mean\nof the reciprocal rank of the true completion in the\n2Code at http://github.com/ajaech/query completion\nSize Model Seen Unseen All\nMPC .292 .000 .203\nUnadapted .292 .256 .267\n(S) ConcatCell .296 .263 .273\nFactorCell .300 .264 .275\nUnadapted .324 .286 .297\n(B) ConcatCell .330 .298 .308\nFactorCell .335 .298 .309\nTable 2: MRR reported for seen and unseen pre-\nﬁxes for small (S) and big (B) models.\nFigure 1: Relative improvement in MRR over the\nunpersonalized model versus queries seen using\nthe large size models. Plot uses a moving average\nof width 9 to reduce noise.\ntop ten proposed completions. The reciprocal rank\nis zero if the true completion is not in the top ten.\nNeural models are compared against an MPC\nbaseline. Following Park and Chiba (2017), we\nremove queries seen less than three times from the\nMPC training data.\n4.2 Results\nTable 2 compares the performance of the differ-\nent models against the MPC baseline on a test set\nof one million queries from a user population that\nis disjoint with the training set. Results are pre-\nsented separately for preﬁxes that are seen or un-\nseen in the training data. Consistent with prior\nwork, the neural models do better than the MPC\nbaseline. The personalized models are both bet-\nter than the unadapted one. The FactorCell model\nis the best overall in both the big and small sized\nexperiments, but the gain is mainly for the seen\npreﬁxes.\nFigure 1 shows the relative improvement in\nMRR over an unpersonalized model versus the\nnumber of queries seen per user. Both the Factor-\nFigure 2: MRR by preﬁx and query lengths for the\nlarge FactorCell and unadapted models with the\nﬁrst 50 queries per user excluded.\nCell and the ConcatCell show continued improve-\nment as more queries from each user are seen, and\nthe FactorCell outperforms the ConcatCell by an\nincreasing margin over time. In the long run, we\nexpect that the system will have seen many queries\nfrom most users. Therefore, the right side of Fig-\nure 1, where the relative gain of FactorCell is up\nto 2% better than that of the ConcatCell, is more\nindicative of the potential of these models for ac-\ntive users. Since the data was collected over a lim-\nited time frame and half of all users have ﬁfteen or\nfewer queries, the results in Table 2 do not reﬂect\nthe full beneﬁt of personalization.\nFigure 2 shows the MRR for different preﬁx and\nquery lengths. We ﬁnd that longer preﬁxes help\nthe model make longer completions and (more ob-\nviously) shorter completions have higher MRR.\nComparing the personalized model against the\nunpersonalized baseline, we see that the biggest\ngains are for short queries and preﬁxes of length\none or two.\nWe found that one reason why the FactorCell\noutperforms the ConcatCell is that it is able to pick\nup sooner on the repetitive search behaviors that\nsome users have. This commonly happens for nav-\nigational queries where someone searches for the\nname of their favorite website once or more per\nday. At the extreme tail there are users who search\nfor nothing but free online poker. Both models do\nwell on these highly predictable users but the Fac-\ntorCell is generally a bit quicker to adapt.\nWe conducted case studies to better understand\nwhat information is represented in the user em-\nbeddings and what makes the FactorCell different\nfrom the ConcatCell. From a cold start user em-\nbedding we ran two queries and allowed the model\nto update the user embedding. Then, we ranked\nFactorCell ConcatCell\n1 high school musical horoscope\n2 chris brown high school musical\n3 funnyjunk.com homes for sale\n4 funbrain.com modular homes\n5 chat room hair styles\nTable 3: The ﬁve queries that have the great-\nest adapted vs. unadapted likelihood ratio after\nsearching for “high school softball” and “math\nhomework help”.\nthe most frequent 1,500 queries based on the ratio\nof their likelihood from before and after updating\nthe user embeddings.\nTables 3 and 4 show the queries with the high-\nest relative likelihood of the adapted vs. unadapted\nmodels after two related search queries: “high\nschool softball” and “math homework help” for\nTable 3, and “Prada handbags” and “Versace eye-\nwear” for Table 4. In both cases, the Factor-\nCell model examples are more semantically co-\nherent than the ConcatCell examples. In the ﬁrst\ncase, the FactorCell model identiﬁes queries that a\nhigh school student might make, including enter-\ntainment sources and a celebrity entertainer pop-\nular with that demographic. In the second case,\nthe FactorCell model chooses retailers that carry\nwoman’s apparel and those that sell home goods.\nWhile these companies’ brands are not as luxu-\nrious as Prada or Versace, most of the top luxury\nbrand names do not appear in the top 1,500 queries\nand our model may not be capable of being that\nspeciﬁc. There is no obvious semantic connec-\ntion between the highest likelihood ratio phrases\nfor the ConcatCell; it seems to be focusing more\non orthography than semantics (e.g. “home” in\nthe ﬁrst example).. Not shown are the queries\nwhich experienced the greatest decrease in like-\nlihood. For the “high school” case, these included\nsearches for travel agencies and airline tickets—\nwebsites not targeted towards the high school age\ndemographic.\n5 Related Work\nWhile the standard implementation of MPC can\nnot handle unseen preﬁxes, there are variants\nwhich do have that ability. Park and Chiba (2017)\nﬁnd that the neural LM outperforms MPC even\nwhen MPC has been augmented with the approach\nfrom Mitra and Craswell (2015) for handling rare\nFactorCell ConcatCell\n1 neiman marcus craigslist nyc\n2 pottery barn myspace layouts\n3 jc penney verizon wireless\n4 verizon wireless jensen ackles\n5 bed bath and beyond webster dictionary\nTable 4: The ﬁve queries that have the great-\nest adapted vs. unadapted likelihood ratio after\nsearching for “prada handbags” and “versace eye-\nwear”.\npreﬁxes. There has also been work on personaliz-\ning MPC (Shokouhi, 2013; Cai et al., 2014). We\ndid not compare against these speciﬁc models be-\ncause our goal was to show how personalization\ncan improve the already-proven generative neural\nmodel approach. RNN’s have also previously been\nused for the related task of next query suggestion\n(Sordoni et al., 2015).\nOur results are not directly comparable to Park\nand Chiba (2017) or Mitra and Craswell (2015)\ndue to differences in the partitioning of the data\nand the method for selecting random preﬁxes.\nPrior work partitions the data by time instead of\nby user. Splitting by users is necessary in order\nto properly test personalization over longer time\nranges.\nWang et al. (2018) show how spelling correction\ncan be integrated into an RNN language model\nquery auto-completion system and how the com-\npletions can be generated in real time using a\nGPU. Our method of updating the model during\nevaluation resembles work on dynamic evaluation\nfor language modeling (Krause et al., 2017), but\ndiffers in that only the user embeddings (latent de-\nmographic factors) are updated.\n6 Conclusion and Future Work\nOur experiments show that the LSTM model can\nbe improved using personalization. The method\nof adapting the recurrent layer clearly matters and\nwe obtained an advantage by using the FactorCell\nmodel. The reason the FactorCell does better is\nin part attributable to having two to three times as\nmany parameters in the recurrent layer as either\nthe ConcatCell or the unadapted models. By de-\nsign, the adapted weight matrix W′only needs to\nbe computed at most once per query and is reused\nmany thousands of times during beam search. As\na result, for a given latency budget, the FactorCell\nmodel outperforms the Mikolov and Zweig (2012)\nmodel for LSTM adaptation.\nThe cost for updating the user embeddings is\nsimilar to the cost of the forward pass and depends\non the size of the user embedding, hidden state\nsize, FactorCell rank, and query length. In most\ncases there will be time between queries for up-\ndates, but updates can be less frequent to reduce\ncomputational costs.\nWe also showed that language model person-\nalization can be effective even on users who are\nnot seen during training. The beneﬁts of person-\nalization are immediate and increase over time as\nthe system continues to leverage the incoming data\nto build better user representations. The approach\ncan easily be extended to include time as an addi-\ntional conditioning factor. We leave the question\nof whether the results can be improved by com-\nbining the language model with MPC for future\nwork.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nZiv Bar-Yossef and Naama Kraus. 2011. Context-\nsensitive query auto-completion. In WWW, pages\n107–116. ACM.\nFei Cai, Maarten De Rijke, et al. 2016. A survey\nof query auto completion in information retrieval.\nFoundations and Trends in Information Retrieval,\n10(4):273–363.\nFei Cai, Shangsong Liang, and Maarten De Rijke.\n2014. Time-sensitive personalized query auto-\ncompletion. In CIKM, pages 1599–1608. ACM.\nYu-Yang Huang, Rui Yan, Tsung-Ting Kuo, and Shou-\nDe Lin. 2014. Enriching cold start personalized lan-\nguage model using social network information. In\nACL, pages 611–617.\nAaron Jaech and Mari Ostendorf. 2017. Low-rank\nRNN adaptation for context-aware language model-\ning. arXiv preprint arXiv:1710.02603.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2017. Dynamic evaluation of neural\nsequence models. arXiv preprint arXiv:1709.07432.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A persona-based neural con-\nversation model. ACL.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. ICLR.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT, pages 234–239.\nBhaskar Mitra and Nick Craswell. 2015. Query auto-\ncompletion for rare preﬁxes. In CIKM, pages 1755–\n1758. ACM.\nDae Hoon Park and Rikio Chiba. 2017. A neural lan-\nguage model for query auto-completion. In SIGIR,\npages 1189–1192. ACM.\nGreg Pass, Abdur Chowdhury, and Cayley Torgeson.\n2006. A picture of search. In InfoScale, volume\n152, page 1.\nMilad Shokouhi. 2013. Learning to personalize query\nauto-completion. In SIGIR, pages 103–112. ACM.\nMilad Shokouhi and Kira Radinsky. 2012. Time-\nsensitive query auto-completion. In SIGIR, pages\n601–610. ACM.\nAlessandro Sordoni, Yoshua Bengio, Hossein Vahabi,\nChristina Lioma, Jakob Grue Simonsen, and Jian-\nYun Nie. 2015. A hierarchical recurrent encoder-\ndecoder for generative context-aware query sugges-\ntion. In CIKM, pages 553–562. ACM.\nPo-Wei Wang, J. Zico Kolter, Vijai Mohan, and Inder-\njit S. Dhillon. 2018. Realtime query completion via\ndeep language models. ICLR.\nTsung-Hsien Wen, Aaron Heidel, Hung-yi Lee,\nYu Tsao, and Lin-Shan Lee. 2013. Recurrent neural\nnetwork based language model personalization by\nsocial network crowdsourcing. In INTERSPEECH,\npages 2703–2707.\nStewart Whiting and Joemon M Jose. 2014. Recent\nand robust query auto-completion. In WWW, pages\n971–982. ACM.\nMatthew D. Zeiler. 2012. ADADELTA: an adaptive\nlearning rate method. CoRR, abs/1212.5701.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.840411901473999
    },
    {
      "name": "Language model",
      "score": 0.6903972625732422
    },
    {
      "name": "Query language",
      "score": 0.6532973647117615
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.6094326972961426
    },
    {
      "name": "Query expansion",
      "score": 0.6065639853477478
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6059664487838745
    },
    {
      "name": "User information",
      "score": 0.5366188287734985
    },
    {
      "name": "Web search query",
      "score": 0.5247911810874939
    },
    {
      "name": "Search engine",
      "score": 0.5143030881881714
    },
    {
      "name": "RDF query language",
      "score": 0.4974997341632843
    },
    {
      "name": "Information retrieval",
      "score": 0.49421384930610657
    },
    {
      "name": "Web query classification",
      "score": 0.39675775170326233
    },
    {
      "name": "Artificial intelligence",
      "score": 0.389430433511734
    },
    {
      "name": "Natural language processing",
      "score": 0.32883602380752563
    },
    {
      "name": "Information system",
      "score": 0.18172988295555115
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ]
}