{
  "title": "Transformer-Based Models for Automatic Identification of Argument Relations: A Cross-Domain Evaluation",
  "url": "https://openalex.org/W3108757211",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5013979714",
      "name": "Ramon Ruiz-Dolz",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5039408594",
      "name": "José Alemany",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5113712353",
      "name": "Stella María Heras Barberá",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5082431810",
      "name": "Ana García‐Fornes",
      "affiliations": [
        "Artificial Intelligence Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6748493786",
    "https://openalex.org/W2609722168",
    "https://openalex.org/W2144232471",
    "https://openalex.org/W2808366770",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W2343649478",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2911433384",
    "https://openalex.org/W581684831",
    "https://openalex.org/W3014117701",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2970929006",
    "https://openalex.org/W2979666134",
    "https://openalex.org/W4407877659",
    "https://openalex.org/W2759690420",
    "https://openalex.org/W6775109978",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3011707786",
    "https://openalex.org/W2949928613",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2557475746",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2995418984",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2788370157"
  ],
  "abstract": "© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other\\nuses, in any current or future media, including reprinting/republishing this material for advertising or promotional\\npurposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted\\ncomponent of this work in other works.",
  "full_text": "Transformer-Based Models for\nAutomatic Identiﬁcation of\nArgument Relations: A\nCross-Domain Evaluation\nRamon Ruiz-Dolz\nUniversitat Polit`ecnica de Val`encia, Valencian Research Institute for Artiﬁcial Intelligence (VRAIN)\nJose Alemany\nUniversitat Polit`ecnica de Val`encia, Valencian Research Institute for Artiﬁcial Intelligence (VRAIN)\nStella Heras\nUniversitat Polit`ecnica de Val`encia, Valencian Research Institute for Artiﬁcial Intelligence (VRAIN)\nAna Garc´ıa-Fornes\nUniversitat Polit`ecnica de Val`encia, Valencian Research Institute for Artiﬁcial Intelligence (VRAIN)\nAbstract—Argument Mining is deﬁned as the task of automatically identifying and extracting\nargumentative components (e.g., premises, claims, etc.) and detecting the existing relations\namong them (i.e., support, attack, rephrase, no relation). One of the main issues when\napproaching this problem is the lack of data, and the size of the publicly available corpora. In this\nwork, we use the recently annotated US2016 debate corpus. US2016 is the largest existing\nargument annotated corpus, which allows exploring the beneﬁts of the most recent advances in\nNatural Language Processing in a complex domain like Argument (relation) Mining. We present\nan exhaustive analysis of the behavior of transformer-based models (i.e., BERT, XLNET,\nRoBERTa, DistilBERT and ALBERT) when predicting argument relations. Finally, we evaluate the\nmodels in ﬁve different domains, with the objective of ﬁnding the less domain dependent model.\nWe obtain a macro F1-score of 0.70 with the US2016 evaluation corpus, and a macro F1-score of\n0.61 with the Moral Mazecross-domain corpus.\nCOMPUTATIONAL ARGUMENTATION has\nproved to be a very solid way to approach several\nproblems such as fake news detection [5], rec-\nommendation systems [14] or debate analysis [4]\n© 2021 IEEE. Personal use of this material is permit-\nted. Permission from IEEE must be obtained for all other\nuses, in any current or future media, including reprint-\ning/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or re-\ndistribution to servers or lists, or reuse of any copyrighted\ncomponent of this work in other works.\namong others. However, in almost every domain,\nit is of great importance to be able to automat-\nically extract the arguments and their relations\nfrom the input source. Argument Mining (AM) is\nthe Natural Language Processing (NLP) task by\nwhich this problem is addressed. The Transformer\nmodel architecture [17] and its subsequent pre-\ntraining approaches have been a turning point\nin the NLP research area. Thanks to its archi-\ntecture, it has been possible to capture longer-\nIEEE Intelligent Systems Published by the IEEE Computer Society © 2021 IEEE\n1\narXiv:2011.13187v2  [cs.CL]  15 Apr 2021\nDepartment Head\nrange dependencies between input structures, and\nthus the performance of systems developed for\nthe most general NLP tasks (i.e., translation, text\ngeneration or language understanding) improved\nsigniﬁcantly. Therefore, the Transformer architec-\nture has laid the foundations on which newer\nmodels and pre-training approaches have been\nproposed, deﬁning the state-of-the-art in NLP.\nIn this work, we analyze the behavior of BERT\n[3], XLNET [20], RoBERTa [9], DistilBERT [15]\nand ALBERT [6] when facing the hardest AM\ntask: identifying relational properties between ar-\nguments.\nArgument Mining was formally deﬁned in\n[13] as the task that aims to automatically detect\narguments, relations and their internal structure.\nAs pointed out in [8], due to the complexity of\nAM, the whole task can be decomposed into three\nmain sub-tasks depending on their argumentative\ncomplexity. First, the identiﬁcation of argument\ncomponents consists in distinguishing argumenta-\ntive propositions from non-argumentative propo-\nsitions. This allows to segment the input text\ninto arguments, making it possible to carry out\nthe subsequent sub-tasks. Second, the identiﬁ-\ncation of clausal properties is the part of AM\nthat focuses on ﬁnding premises or conclusions\namong the argumentative propositions. Third, the\nlast sub-task is the identiﬁcation of relational\nproperties. Two different argumentative propo-\nsitions are considered at a time, and the main\nobjective is to identify which type of relation\nlinks both propositions. Different relations can be\nobserved in argument analysis, from the classical\nattack/support binary analysis [2], to the identi-\nﬁcation of complex patterns of human reasoning\n(i.e., argumentation schemes [19]). Therefore, the\nidentiﬁcation of argumentative relations is the\nmost complex part of AM [8], but its complexity\nmay vary depending on how the problem is\ninstantiated.\nOne of the main problems when addressing\nany AM task is the lack of high quality annotated\ndata. In fact, as the argumentative complexity of\nthe task increases, it gets harder to ﬁnd large\nenough corpus to do experiments that match\nthe latest NLP advances. An important feature\nthat characterizes the transformer-based models\nis that large corpora are needed to achieve the\nperformance improvement mentioned above. Re-\ncently, in [18], a new argument annotated cor-\npus of the United States 2016 debate ( US2016)\nwas published. This corpus contains data from\nthe transcripts of the televised political debates\nand from internet debates generated around the\nsame context. This is the ﬁrst publicly available\ncorpus with enough data to begin exploring the\nbeneﬁts of the most recent contributions in NLP,\nwhen applied to the identiﬁcation of argumenta-\ntive relations. Additionally, the US2016 corpus\nhas been annotated using Inference Anchoring\nTheory (IAT 1), a standard argument annotation\nguideline that provides more information than the\nclassic attack/support binary annotation. Learning\na model to automatically annotate with the use of\nIAT, makes it possible to evaluate its performance\nnot only with the test samples of the corpus, but\nalso with other different corpus already analyzed\nand tagged using this standard (e.g., Moral Maze\ncorpus). This way, it is our objective to both:\nevaluate the performance of these new models in\nthe identiﬁcation of argument relations task; and\nto ﬁnd out which one is more robust to variations\nin the application domain.\nIn this work, we explore the beneﬁts of the\nmost recent advances in NLP applied to relation\nprediction in the AM domain. For this purpose\nwe use the recently published US2016 corpus,\nsince it is to the best of our knowledge, the\nlargest annotated corpus containing information\nabout argumentative relations, and the Moral\nMaze cross-domain corpus. Then we do: (i) a\npre-processing of the corpus in order to clean\nand structure the data for the requirements of our\nexperiments; (ii) an analysis of the performance\nof the most relevant transformer-based models\n(i.e., BERT, XLNET, RoBERTa, DistilBERT and\nALBERT) when learning to predict the relations\nbetween argumentative propositions deﬁned by\nthe IAT standard; and (iii) an evaluation of the\nobtained models in ﬁve different domains ( Moral\nMaze corpus) with the objective of analyzing\nthe domain dependency of the transformer-based\nmodels when facing this AM task.\nRELATED WORK\nArgument Mining is one of the main research\nareas in Computational Argumentation. AM has\n1https://typo.uni-konstanz.de/add-up/wp-content/uploads/\n2018/04/IAT-CI-Guidelines.pdf\n2\nIEEE Intelligent Systems\ncaught the attention of many researchers since\nit is considered to be the ﬁrst step towards au-\ntonomous argumentative systems. We identiﬁed\nmany different approaches to the Argument (re-\nlation) Mining problem, which depend on the pro-\nposed methods (i.e., Parsing algorithms, Textual\nEntailment Suites, Logistic Regression, Support\nVector Machines and Neural Networks), and the\navailable corpus at each moment. Initial research\non automatic identiﬁcation of argument relations\nwas done in [13] where parsing algorithms were\nused to determine the type of relation existing\nbetween two argument propositions. Some years\nlater, AM started to gain relevance in the NLP\ncommunity. We can observe the popularization\nof machine learning techniques for NLP purposes\nin [11], [16] and [10]. Support Vector Machines\n(SVMs) seemed to be the best performing ma-\nchine learning technique for the purpose of ar-\ngument relation identiﬁcation. With the advent\nof Neural Networks (NNs), a performance gap\nbetween previous works and this new approach\ncould be observed. In [2] the empirical results\nobtained by Recurrent Neural Network (RNN)\nmodels for AM were signiﬁcantly better. How-\never, there is an interesting observation to make\nemphasis on, which makes it hard to compare\nAM works. As it can be pointed out after look-\ning at the results depicted in works like [12]\nor [2], the corpus used in each work has a\nlot of inﬂuence in the results. This is due to\nmany different factors such as class distributions,\nvariable language complexity (e.g., use of irony,\nenthymemes, etc.) or the own size of the corpus.\nTherefore, misleading results may be observed if\nthe generalization of the model is not properly\nevaluated. On the other hand, deep learning algo-\nrithms require much more data to signiﬁcantly\nincrease its performance compared to classic\nneural, machine learning or statistical methods.\nTherefore, from all these past years of argument\nrelation identiﬁcation works, the performance has\nbeen improved not only with the use of new\nmodels or techniques, but also with the creation\nof better corpora. In [18], a new argument an-\nnotated corpus ( US2016) was published, with\nenough data to begin exploiting the beneﬁts of the\nmost recent advances in NLP (i.e., transformer-\nbased models) in the AM domain. To the best of\nour knowledge, this is the ﬁrst work addressing\nthe Argument (relation) Mining problem using\nTransformer-based models in more than a unique\ndomain.\nDATA\nTwo different corpora have been used in this\nwork: the US2016 debate corpus and the Moral\nMaze multi-domain corpus. Both corpora can\nbe downloaded from the Argument Interchange\nFormat Database (AIFdb), an initiative of re-\nsearchers from the ARG-tech 2 with the objec-\ntive of creating a standard formatted argument\ncorpus database [7]. This database contains 193\ndifferent argumentative corpora structured using\nthe AIF standard. Each corpus is divided into\nseveral argument maps ( Figure 1 ), and each\nargument map contains a set of Argument Dis-\ncourse Units (ADUs) with its argumentative re-\nlations annotated using the Inference Anchoring\nTheory (IAT). This annotation method considers\nthe most important three argumentative relations:\ninference (RA), conﬂict (CA) and rephrase (MA).\nAn inference relation between two propositions\ndetermines that one is used to support or justify\nthe other; a conﬂict relation indicates that two\npropositions have contradictory information; and\na rephrase between two propositions means that\nthey are equivalent from an argumentative point\nof view.\nIn order to adapt the AIFdb corpus to the\nneeds of this task, we did some pre-processing.\nEach argument map is stored in a JSON ﬁle,\nand represented as a graph following the AIF\nstandard. We generate a unique tab-separated\nvalues ﬁle per corpus containing three different\nvalues: proposition1, proposition2 and label. In\naddition to the existing IAT relation labels, we\ndecided to generate an additional relation: the no\nrelation (NO) label. Since most of the pairs of\npropositions found in a debate are not related, we\ndecided to generate a 65% of samples belonging\nto this new class. For this purpose, we mixed up\nthe propositions that were not annotated with any\nof the IAT relation classes. This way, the resulting\nmodel will also be able to discriminate between\nrelated or not related propositions.\n2https://www.arg-tech.org/\n- 2021\n3\nDepartment Head\nFigure 1. US2016 Argument Map Sample. ADUs are bounded by rectangles. Relation types are contained in\nthe rhombuses.\nTable 1. Class distribution of the US2016 corpus, Train\nand Test partitions.\nUS2016 Train Test\nRA 2744 2195 549\nCA 888 710 178\nMA 705 564 141\nNO 8055 6444 1611\nTotal 12392 9913 2479\nUS2016 Debate Corpus\nThe US20163 corpus is an argument annotated\ncorpus of the electoral debate carried out in 2016\nin the United States. It contains both, transcrip-\ntions of the different rounds of TV debate, and\ndiscussions from the Reddit forums as detailed\nin [18]. The class distribution of the processed\nUS2016 corpus is depicted in Table 1. Since it is\nthe largest publicly available argument annotated\ncorpus in the literature, we used it to train the\nmodels. We decided to split the corpus with the\n80% of the proposition pairs for training, and the\nremaining 20% for the evaluation.\nMoral Maze Multi-Domain Corpus\nThe Moral Maze4 multi-domain corpus is an\nargument annotated corpus obtained from the\ntranscriptions of the 2012 Moral Maze BBC\ndiscussion show. This corpus has been built from\nsamples collected in ﬁve different broadcasts. The\nclass distribution of the processed Moral Maze\ncorpus is depicted in Table 2. This corpus is used\nto evaluate the domain robustness of the trained\nmodels across ﬁve different domain corpus: Bank\n(B), Empire (E), Money (M), Problem (P) and\n3http://corpora.aifdb.org/US2016\n4http://corpora.aifdb.org/mm2012\nTable 2. Multi-domain evaluation corpus (Moral Maze)\nclass distribution.\nMM2012 B E M P W\nRA 833 128 121 205 192 187\nCA 200 26 36 30 45 63\nMA 156 3 25 48 41 39\nNO 2209 292 339 526 517 537\nTotal 3398 449 521 810 795 826\nWelfare (W); each one focused on a speciﬁc\ndebate topic and with a different distribution of\nclasses.\nAUTOMATIC IDENTIFICATION OF\nRELATIONAL PROPERTIES\nThe problem addressed in this paper can be\nseen as an instance of the sentence pair classiﬁ-\ncation problem. The sentence pair classiﬁcation\nproblem consists of assigning the most likely\nclass to two text inputs at a time. In Argument\nMining, after segmenting the text and deﬁning\nthe argument components, the argument graph\nmust be built by identifying the relational prop-\nerties between every two argument components.\nTherefore, given two argument components (i.e.,\nsentences): xN\n1 = x1, x2, . . . , xN of length N\nwhere xn is each word of the ﬁrst component;\nand yM\n1 = y1, y2, . . . , yM of length M where\nym is each word of the second component, the\nclassiﬁcation problem can be modeled as deﬁned\nin Equation 1,\nˆc = arg max\nc∈C\np(c|xN\n1 , yM\n1 ) (1)\nwhere C = [RA, CA, MA, NO], so the four\ndifferent relation types existing in the IAT label-\ning are considered: inference (RA), conﬂict (CA),\n4\nIEEE Intelligent Systems\nrephrase (MA) and no relation (NO). To approach\nthis problem, we decided to use transformer-\nbased neural architectures. The most recent works\nin the literature tackle the Argument Mining\nproblem using Recurrent Neural Networks (e.g.,\nLSTMs, BiLSTMs, etc.). However, the Trans-\nformer architecture presents several interesting\nimprovements with respect to the RNNs. The\nTransformer architecture uses multiple attention\nmodules, which allow to capture longer range\ndependencies between words in a sentence. Given\nthe nature of this work’s task, we expect to have\nlong input sentences since argumentative text is,\ngenerally, more complex than others. Therefore,\nwe think attention mechanisms can be very use-\nful for the identiﬁcation of relational properties\nbetween argument components.\nIn this work, we apply Inductive Transfer\nLearning combined with different Transformer\npre-training methods that allow us to learn our\ntask not from scratch but using previously calcu-\nlated weights. We decided to use the pre-training\nmethods that performed the best in other NLP\ntasks such as Natural Language Understanding,\nQuestion Answering or Text Generation: BERT\n[3], XLNET [20], RoBERTa [9], DistilBERT [15]\nand ALBERT [6]. All these models have in\ncommon that they are based on the Transformer\narchitecture, however different approaches have\nbeen considered in order to compute the ini-\ntial weights. BERT, also known as Bidirectional\nEncoder Representations from Transformers, is\npre-trained on masked language model and next\nsentence prediction tasks. The model is designed\nto be able to ﬁne-tune its weights on other dif-\nferent tasks by adding an additional output layer.\nXLNet is proposed after identifying a potential\nproblem in BERT: the language modeling of\nthe existing dependencies between the masked\npositions. XLNet combines both auto-regressive\nlanguage modeling and auto-encoding techniques\nin order to overcome the detected potential is-\nsues. RoBERTa is an strong optimization of the\nBERT pre-training approach. After researchers\ndid a thorough analysis on the impact of the\nmost important hyper-parameters, this new model\nwas able to obtain interesting results in most of\nthe evaluated tasks. Finally, both DistilBERT and\nALBERT were proposed as smaller and faster\nTable 3. Transformer-based architectures conﬁguration.\nModel TBlocks HSize AH Params.\nBERT-base[3] 12 768 12 110M\nBERT-large[3] 24 1024 16 340M\nXLNet-base[20] 12 768 12 110M\nXLNet-large[20] 24 1024 16 340M\nRoBERTa-base[9] 12 768 12 125M\nRoBERTa-large[9] 24 1024 16 335M\nDistilBERT-base[15] 6 768 12 66M\nALBERT-base[6] 12 768 12 11M\nALBERT-xxlarge[6] 12 4096 64 223M\nversions of the previous approaches. We ﬁnd it\ninteresting to also analyze and evaluate the be-\nhavior of these smaller versions, which have been\ndesigned to democratize the use of transformer-\nbased pre-training methods without signiﬁcant\nloss of performance.\nEVALUATION\nExperimental Setup\nAll the experiments carried out in this work\nhave been run in a double NVIDIA Titan V com-\nputer with an Intel Xeon W-2123 CPU and 62Gb\nof RAM. This way, we can evaluate not only the\nperformance of the models in the classiﬁcation\ntask, but also their training computational cost\nin our speciﬁc task. The number of parameters\nof each model is directly related to the train-\ning computational cost. Table 3 summarises the\nmost relevant features that deﬁne each Trans-\nformer architecture considered in this research.\nThe Transformer blocks (TBlocks) stand for the\nnumber of layers; the hidden size (HSize) repre-\nsents the number of hidden states in each layer;\nthe attention heads (AH) indicate the number of\npointers used by the attention layers; ﬁnally, the\nlast feature is the total number of parameters\n(Params.) of each architecture.\nIn our experiments, we explore the beneﬁts\nof transfer learning applied to the argument re-\nlation mining task. For that purpose, during our\ntraining phase, we use the pre-trained encoder of\neach model with a linear layer on its top. The\noutput size of the linear layer coincides with the\nnumber of classes considered in our instance of\nthe problem (i.e., 4). With the softmax function,\nwe are able to model the probability of belonging\nto one class or another for each pair of arguments\n(Equation 1).\n- 2021\n5\nDepartment Head\nWe adapted the maximum sequence length\nand the batch size of our inputs in each ex-\nperiment. These parameters were conﬁgured in\norder to use the whole available GPU memory.\nWhen training BERT-base models, we deﬁned a\nmaximum sequence length of 256 and a batch\nsize of 64. When training BERT-large models,\nwe halved those values to a maximum sequence\nlength of 128 and a batch size of 32. We trained\nXLNet-base with a maximum sequence length of\n256 and a batch size of 32, and XLNet-large\nwith a maximum sequence length of 256 and a\nbatch size of 8. RoBERTa-base was trained with\na maximum sequence length of 256 and batch\nsize of 32, and for training RoBERTa-large we\nused the same maximum sequence length but\na batch size of 16. For DistilBERT we used a\nmaximum sequence length of 256 and a batch\nsize of 128. Finally, ALBERT-base was trained\ndeﬁning a maximum sequence length of 256 and\nbatch size of 64, but in order to ﬁt ALBERT-\nxxlarge in our available memory we had to deﬁne\na maximum sequence length of 128 and a batch\nsize of 4. We trained all these models for 50\nepochs in our corpus. The best results (depicted\nin the following section) were obtained with a\n1e-5 learning rate.\nResults\nIn this section we present the empirical results\nobtained after running the experiments on all\nthe previously deﬁned models. In addition to\nthe Transformer-based architectures, we have also\ntrained a Recurrent Neural Network (RNN) as a\nbaseline in our task. We used the best performing\nRNN architecture in argument relation mining\nproposed in [2], consisting of two Long Short-\nTerm Memory (LSTM) networks working in par-\nallel with each pair of arguments. We trained\nthe baseline model for 50 epochs in our data,\nas the authors did in the original publication. In\norder to measure the performance of the different\nmodels, we have evaluated them using the macro\nF1-score metric. Due to the huge class imbalance\nin our corpora, the use of the macro F1-score\nmakes possible to avoid misleading results during\nthe evaluation. Additionally, we also measured\nthe training time required by each model when\nlearning the task proposed in this work, in order\nto analyze if it can be worthwhile to sacriﬁce their\nperformance in pursuit of faster training times or\navailability in lower resource environments.\nThe macro F1-scores obtained by each model\nare depicted in Table 4. In the ﬁrst column, we\ncan see every trained model. The second column\nrepresents the macro F1 obtained by each model\nwhen evaluated with the test partition of the same\ncorpus used for training (i.e., US2016). The third\ncolumn contains the scores obtained when the\nevaluation is performed on a different corpus (i.e.,\nMM2012) containing a mixture of ﬁve domains.\nFinally, the last ﬁve columns are the macro F1-\nscores of the models when using each one of the\nﬁve domain speciﬁc corpora (i.e., Bank, Empire,\nMoney, Problem and Welfare) for evaluation.\nWith most of the models, we achieved state-\nof-the-art macro F1-scores for relation identiﬁca-\ntion in Argument Mining [1]. Here, it is important\nto make emphasis that the way we considered\nto represent argumentative relations (i.e., IAT\nlabelling) make this task harder than most of\nthe previous work (i.e., attack/support) in this\narea. We obtained a 0.70 macro F1-score with\nRoBERTa-large, outperforming the LSTM base-\nline used as a reference of previous research in\nargument relation identiﬁcation. Furthermore, in\norder to have a more strong reference to compare\nwith previous published results, we carried out\nan experiment using the same parameters but\nconsidering a binary instance of the problem\n(i.e., only attack and support relations). This way,\nRoBERTa-large achieved a macro F1-score of\n0.81 highlighting the mentioned complexity gap\nbetween the two instances of the same problem.\nIn general, we can observe that RoBERTa has\nperformed very well in this task. When looking\nat the cross-domain evaluation, RoBERTa-large\nhas also performed the best. We obtained a 0.61\nmacro F1-score when doing the evaluation with\na different domain corpus. Moreover, the model\nhas been able to keep a good performance with\neach one of the ﬁve domain speciﬁc corpora,\neven having different class distributions. With\nALBERT-xxlarge-v2 it has been possible to obtain\na slightly better performance when evaluating\nwith the Empire corpus. It is possible to observe\nhow the scores obtained on the Bank and Empire\ncorpus are slightly lower than the rest. This is\nmainly due to their smaller size, combined with\nthe strong imbalance between classes. We also\n6\nIEEE Intelligent Systems\nTable 4. Performance of the models in the automatic identiﬁcation of argument relations, given in macro F1-scores.\nExperiment US2016-test MM2012 Bank Empire Money Problem Welfare\nLSTM (baseline) .26 .24 .25 .22 .24 .25 .23\nBERT-base-cased .62 .53 .40 .45 .54 .47 .53\nBERT-base-uncased .65 .56 .42 .48 .54 .50 .54\nBERT-large-cased .61 .55 .45 .49 .53 .47 .51\nBERT-large-uncased .66 .57 .47 .49 .56 .49 .57\nXLNet-base .65 .56 .44 .49 .51 .54 .55\nXLNet-large .69 .57 .44 .51 .53 .53 .54\nRoBERTa-base .68 .58 .51 .52 .54 .52 .58\nRoBERTa-large .70 .61 .53 .53 .59 .56 .59\nDistilBERT .55 .42 .33 .39 .40 .43 .39\nALBERT-base-v2 .60 .54 .49 .45 .53 .47 .51\nALBERT-xxlarge-v2 .67 .59 .50 .54 .56 .48 .59\ndid experiments with cased and uncased models,\nin order to see the relevance of cased text in the\nrelation identiﬁcation task. As we can observe,\nthe uncased models performed signiﬁcantly better\nthan the cased models, so we can point out that\ncased text did not help to improve the perfor-\nmance of the models in our task.\nOn the other hand, we obtained the worst\nresults with DistilBERT and ALBERT-base-v2,\nas one might expect. We decided to use these\nmodels in order to see if the observed perfor-\nmance sacriﬁce was worthwhile in exchange for\nmore feasible training times. Table 5 contains the\ntraining times required by each model under our\nexperimental setup. With DistilBERT, it was pos-\nsible to achieve a signiﬁcant reduction of training\ntime in exchange for a huge drop in performance.\nHowever, with ALBERT-base-v2 we could not\nobserve a signiﬁcant reduction of training time.\nFrom our experiments, we have not seen any\nsigniﬁcant advantage in using these lite models.\nWe also observed that the computational cost of\ntraining XLNet-large and ALBERT-xxlarge-v2 in\nour task was very expensive. XLNet-large was\n5.1 times slower to train than BERT-large. As\nfor ALBERT-xxlarge-v2, the training time was 7.1\ntimes higher than BERT-large. This is due to its\nhidden size of 4096 with respect to the 1024 sized\nlarge models. Thus, observing the performance\nof the models in means of their macro F1-score\nand the required training time, we still think that\nRoBERTa is the best approach to tackle both,\nTable 5. Training time of 50 epochs running in a double\nNVIDIA Titan V computer.\nExperiment Time\nBERT-base 39m 11s\nBERT-large 2h 19m 57s\nXLNet-base 1h 52m 38s\nXLNet-large 11h 51m 09s\nRoBERTa-base 43m 17s\nRoBERTa-large 4h 44m 33s\nDistilBERT 16m 15s\nALBERT-base-v2 38m 04s\nALBERT-xxlarge-v2 16h 20m 22s\nTable 6. Distribution of the misclassiﬁed samples per class\nusing the RoBERTa-large model. Each column indicates\nthe real class of the samples, each row indicates the\nassigned class by our model.\nPred. \\ Real RA CA MA NO\nRA - 0.512 0.603 0.730\nCA 0.200 - 0.138 0.226\nMA 0.100 0.075 - 0.044\nNO 0.700 0.412 0.259 -\ndomain-speciﬁc and cross-domain identiﬁcation\nof relational properties between arguments. Even\nthe RoBERTa-base version performed well in this\ntask and it was 6.6 times faster than its large\nversion on training.\nError Analysis\nIn an effort to conduct a thorough evalu-\nation, we decided to analyze the errors made\n- 2021\n7\nDepartment Head\nby RoBERTA-large, the best performing model.\nFor this purpose we measured the volume of\nmisclassiﬁcations found on each one of the four\nclasses considered in this work. Table 6 shows\nthe error distribution detected when analyzing the\nresults. Two important remarks can be pointed\nout when looking at the obtained error distribu-\ntions. First of all, it is possible to observe how\nmost of the misclassiﬁed argument pairs labeled\nwith an inference relation were assigned the no\nrelation class. Similarly, most of the misclassiﬁed\nargument pairs without relation were assigned the\ninference class by our model. We observed that\nmany of these errors were due to a loss of contex-\ntual information. In an argumentative discourse, it\nis very common to refer to past concepts without\nexplicitly mentioning them (i.e., enthymemes) or\nsimplifying them with the use of pronouns. The\nlack of dialogical context can make the automatic\nidentiﬁcation of argument relations a harder task.\nFor a better understanding of this problem we\npresent the following example with two argument\ncomponents labeled with inference relation:\nP1: I thinkit’s not going to help change the culture\nP2: In banking we’ve a totally different situation\nOur system classiﬁed the pair as no related\nsamples. In fact, by only reading the sentence\npair, one may think there is not any argument\nrelation between them. In these situations, it is\nevident that the key to avoid any possible er-\nror is to give additional information about the\nuttered propositions. In this case, depending on\nthe background meaning of the “ it” and “ we”\npronouns, the sentences may be related or not.\nThe only way of considering this proposition pair\nrelated as an inference, is assuming that the it\npronoun refers to the banking system. We also\ndetected that in these situations the softmax out-\nputs of our model gives very close probabilities\nfor both, RA and NO classes. Another indicator of\nthe existing model misunderstandings presented\nbefore, are the similar error distributions that\nconﬂicting arguments show with both inference\nand no relation classes. On the other hand, we\nalso pointed out that the rephrased argument pairs\nwere mainly misclassiﬁed as inference related\narguments. However, when analyzing them we\nobserved that most of the relations could also\nbe considered as inference related arguments de-\npending on the interpretation. For example:\nP1: We do need curriculum reform\nP2: RUBIO too believes in curriculum reform\nIn this case, the sentence pair can be inter-\npreted as a rephrase, assuming that “ We” and\n“RUBIO too” are equivalent subjects. But it can\nalso be interpreted as an argument from authority,\nwith P2 supporting (inference relation) P1. In\nsome situations the line that differences rephrase\nfrom inference may not be as clear as desired, and\nboth types of relation can be considered correct.\nAdditionally, with these second type of signiﬁcant\ndetected errors, it is also possible to observe the\nproblem mentioned before. Therefore, the loss of\ninformation caused by the use of pronouns or\nenthymemes in the discourse can be determinant\nwhen approaching a task of this complexity.\nCONCLUSION\nThe automatic identiﬁcation of argument re-\nlations is an essential task in the whole com-\nputational argumentation process. It allows to\nautomatically generate the argumentative struc-\nture from argument discourse units. In this work,\nwe present how the automatic identiﬁcation of\nargument relations, based on Inference Anchoring\nTheory labeling, can be approached using the\nlatest advances in natural language processing.\nTo the best of our knowledge, this is the ﬁrst\nwork using transformer-based pre-trained models\nto learn this task. For this purpose, we have used\nthe largest publicly available argument annotated\ncorpus to the date. Most of the trained models\nhave been able to outperform the state-of-the-\nart baselines in argument relation mining ([1]),\neven with a more complex instance of the task.\nWe observed a signiﬁcant better performance with\nRoBERTa than other models, the best results were\nachieved with RoBERTa-large. We also made a\ncross-domain evaluation of the models, in order\nto ﬁnd out their domain robustness. Even there\nwas a small drop in performance (most probably\nbecause of the signiﬁcant variations of linguistic\nand class distributions between different domain\ncorpora), the scores on different domains were\nstill close to previous AM reports on a unique\ndomain. This way, it is our objective to contribute\non paving the way for ﬁnding models that do\na better generalisation of this task. Finally, we\n8\nIEEE Intelligent Systems\nanalyzed the errors made by our best performing\nmodel. We have seen that two important groups\nof errors are caused by the loss of contextual\ninformation. We also pointed out that another\nimportant group of errors made by the model\nwas due to possible multiple interpretations of the\nrelations. We think that signiﬁcant improvements\nin model performance can be achieved after an-\nalyzing the most common errors detected in this\nwork. As future work, we propose the following\nmodiﬁcations to the automatic identiﬁcation of\nargument relations task: (i) pronoun replacement,\nto solve the loss of contextual information in\nsome propositions; (ii) consider the possible clas-\nsiﬁcation ambiguity, in some cases, by accept-\ning multiple correct relations if the interpretation\nleads to this conclusion; and (iii) incorporation\nof external information. In argumentation theory,\nan enthymeme is known as the omission of a\nclaim or a support of an argument. In order to\nmake the discourse more ﬂuid, it is very com-\nmon to use enthymemes in situations where the\nomitted information is considered to be known by\nall the participants. Therefore, without external\ninformation, the model may not be able to fully\nunderstand relations between enthymemes.\nACKNOWLEDGMENT\nThis work is partially supported by the Span-\nish Government project TIN2017-89156-R, the\nFPI grant BES-2015-074498, and the Valencian\nGovernment project PROMETEO/2018/002. We\ngratefully acknowledge the support of NVIDIA\nCorporation with the donation of the Titan V\nGPUs used for this research.\nREFERENCES\n1. Oana Cocarascu, Elena Cabrio, Serena Villata, and\nFrancesca Toni. A dataset independent set of baselines\nfor relation prediction in argument mining.arXiv preprint\narXiv:2003.04970, 2020.\n2. Oana Cocarascu and Francesca Toni. Identifying at-\ntack and support argumentative relations using deep\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 1374–1379, 2017.\n3. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n4. Ria Jha, Francesco Belardinelli, and Francesca Toni.\nFormal veriﬁcation of debates in argumentation theory.\narXiv preprint arXiv:1912.05828, 2019.\n5. Neema Kotonya and Francesca Toni. Gradual argumen-\ntation evaluation for stance aggregation in automated\nfake news detection. In Proceedings of the 6th Work-\nshop on Argument Mining, pages 156–166, 2019.\n6. Zhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut. Al-\nbert: A lite bert for self-supervised learning of language\nrepresentations. arXiv preprint arXiv:1909.11942, 2019.\n7. John Lawrence and Chris Reed. Aifdb corpora. 2014.\n8. John Lawrence and Chris Reed. Argument mining: A\nsurvey. Computational Linguistics, 0(0):1–54, 2019.\n9. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n10. Stefano Menini, Elena Cabrio, Sara Tonelli, and Serena\nVillata. Never retreat, never retract: Argumentation\nanalysis for political speeches. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence, 2018.\n11. Nona Naderi and Graeme Hirst. Argumentation mining\nin parliamentary discourse. In Principles and Practice\nof Multi-Agent Systems, pages 16–25. Springer, 2015.\n12. Vlad Niculae, Joonsuk Park, and Claire Cardie. Ar-\ngument mining with structured svms and rnns. arXiv\npreprint arXiv:1704.06869, 2017.\n13. Raquel Mochales Palau and Marie-Francine Moens.\nArgumentation mining: the detection, classiﬁcation and\nstructure of arguments in text. In Proceedings of the\n12th international conference on artiﬁcial intelligence\nand law, pages 98–107. ACM, 2009.\n14. Antonio Rago, Oana Cocarascu, and Francesca Toni.\nArgumentation-based recommendations: Fantastic ex-\nplanations and how to ﬁnd them. 2018.\n15. Victor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint\narXiv:1910.01108, 2019.\n16. Christian Stab and Iryna Gurevych. Parsing argumen-\ntation structures in persuasive essays. Computational\nLinguistics, 43(3):619–659, 2017.\n17. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In\nAdvances in neural information processing systems ,\npages 5998–6008, 2017.\n18. Jacky Visser, Barbara Konat, Rory Duthie, Marcin Kos-\n- 2021\n9\nDepartment Head\nzowy, Katarzyna Budzynska, and Chris Reed. Argu-\nmentation in the 2016 us presidential elections: anno-\ntated corpora of television debates and social media\nreaction. Language Resources and Evaluation , pages\n1–32, 2019.\n19. Douglas Walton, Christopher Reed, and Fabrizio\nMacagno. Argumentation schemes . Cambridge Uni-\nversity Press, 2008.\n20. Zhilin Y ang, Zihang Dai, Yiming Y ang, Jaime Carbonell,\nRuss R Salakhutdinov, and Quoc V Le. Xlnet: Gen-\neralized autoregressive pretraining for language under-\nstanding. In Advances in neural information processing\nsystems, pages 5754–5764, 2019.\nRamon Ruiz-Dolz received the Master’s Degree in\nArtiﬁcial Intelligence, Pattern Recognition and Digital\nImaging from the Universitat Polit `ecnica de Val `encia\n(UPV) in 2019. He currently works as a researcher\nat the Valencian Research Institute for Artiﬁcial In-\ntelligence (VRAIN) and is pursuing a Ph.D. degree\nin Computer Science. His research interests are fo-\ncused on Argument Mining, Computational Argumen-\ntation and Persuasion Technologies. Contact him at\nthe Universitat Polit`ecnica de Val`encia, 46022, Valen-\ncia, Spain, raruidol@dsic.upv.es.\nJose Alemany received his PhD degree in Com-\nputer Science from the Universitat Polit `ecnica de\nVal`encia (UPV) in 2020. He is currently working as a\nPostdoctoral Researcher at the Valencian Research\nInstitute for Artiﬁcial Intelligence (VRAIN). He is also\nworking as an Assistant Professor at Florida Uni-\nversit`aria. His research interests include information\ndissemination, privacy-preserving, content analysis,\nand complex networks. Contact him at the Universi-\ntat Polit `ecnica de Val `encia, 46022, Valencia, Spain,\njalemany1@dsic.upv.es.\nStella M. Heras Barber ´a holds a PhD in Computer\nScience (extraordinary prize Cum Laude) from the\nPolytechnic University of Valencia (UPV, 2011). She\nobtained the Executive Master in Project Manage-\nment by the University of Valencia (2013) (certiﬁed\nPMP-1558995) and has the title of University Special-\nist in University Pedagogy (UPV, 2011). She currently\nworks as a researcher at the Valencian Research\nInstitute for Artiﬁcial Intelligence (VRAIN) and as an\nassociate professor in the Department of Languages\nand Computer Systems at the UPV, where she has\nbeen teaching since 2007. Her research area is fo-\ncused on the development of artiﬁcial intelligence sys-\ntems (computational argumentation, persuasion tech-\nnologies, educational recommender systems). Con-\ntact her at the Universitat Polit `ecnica de Val `encia,\n46022, Valencia, Spain, stehebar@upv.es.\nAna Garc´ ıa-Fornes received her PhD degree in\nComputer Science from the Universitat Polit `ecnica\nde Val `encia (UPV, 1996). She currently works as\na Full Professor at the Department of Information\nSystems and Computation at the UPV, Spain and\nas a researcher at the Valencian Research Institute\nfor Artiﬁcial Intelligence (VRAIN). Her research inter-\nests focus on real-time systems, multi-agent systems,\nagreement technologies and privacy in social media.\nContact her at the Universitat Polit`ecnica de Val`encia,\n46022, Valencia, Spain, agarcia@dsic.upv.es.\n10\nIEEE Intelligent Systems",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7521729469299316
    },
    {
      "name": "Transformer",
      "score": 0.592130720615387
    },
    {
      "name": "Identification (biology)",
      "score": 0.5359538197517395
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.47175461053848267
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.427750825881958
    },
    {
      "name": "Data mining",
      "score": 0.3591650426387787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35873448848724365
    },
    {
      "name": "Voltage",
      "score": 0.12156736850738525
    },
    {
      "name": "Electrical engineering",
      "score": 0.11885139346122742
    },
    {
      "name": "Engineering",
      "score": 0.10264992713928223
    },
    {
      "name": "Mathematics",
      "score": 0.0835832953453064
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}