{
    "title": "TransTrack: Multiple Object Tracking with Transformer",
    "url": "https://openalex.org/W3115390238",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221641988",
            "name": "Sun, Peize",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226211340",
            "name": "Cao, Jinkun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099657629",
            "name": "Jiang Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2372615449",
            "name": "Zhang Rufeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2355899877",
            "name": "Xie, Enze",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221641989",
            "name": "Yuan, Zehuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2376739287",
            "name": "Wang, Changhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117169576",
            "name": "Luo, Ping",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3165926952",
        "https://openalex.org/W3093245983",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2252355370",
        "https://openalex.org/W2798875622",
        "https://openalex.org/W3035727180",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2739491435",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3149936330",
        "https://openalex.org/W2981393651",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2739374836",
        "https://openalex.org/W2963574614",
        "https://openalex.org/W2895071559",
        "https://openalex.org/W2895150009",
        "https://openalex.org/W3027919498",
        "https://openalex.org/W2603203130",
        "https://openalex.org/W3176403636",
        "https://openalex.org/W2897582990",
        "https://openalex.org/W3119686997",
        "https://openalex.org/W3171162369",
        "https://openalex.org/W2291627510",
        "https://openalex.org/W3110900159",
        "https://openalex.org/W2798542761",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W2031454541",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W3014582150",
        "https://openalex.org/W3036415994",
        "https://openalex.org/W3145269263",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2145938889",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2799058067",
        "https://openalex.org/W3167949052",
        "https://openalex.org/W2594507094",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3012922853",
        "https://openalex.org/W3094000868",
        "https://openalex.org/W3106763294",
        "https://openalex.org/W2107775979",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2964286567",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W2150298366",
        "https://openalex.org/W2963901085",
        "https://openalex.org/W3096068180",
        "https://openalex.org/W2336589871",
        "https://openalex.org/W3110402800",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W2534578893",
        "https://openalex.org/W2470394683",
        "https://openalex.org/W3099887740",
        "https://openalex.org/W2124781496",
        "https://openalex.org/W1533861849"
    ],
    "abstract": "In this work, we propose TransTrack, a simple but efficient scheme to solve the multiple object tracking problems. TransTrack leverages the transformer architecture, which is an attention-based query-key mechanism. It applies object features from the previous frame as a query of the current frame and introduces a set of learned object queries to enable detecting new-coming objects. It builds up a novel joint-detection-and-tracking paradigm by accomplishing object detection and object association in a single shot, simplifying complicated multi-step settings in tracking-by-detection methods. On MOT17 and MOT20 benchmark, TransTrack achieves 74.5\\% and 64.5\\% MOTA, respectively, competitive to the state-of-the-art methods. We expect TransTrack to provide a novel perspective for multiple object tracking. The code is available at: \\url{https://github.com/PeizeSun/TransTrack}.",
    "full_text": "TransTrack: Multiple Object Tracking with Transformer\nPeize Sun1, Jinkun Cao 2, Yi Jiang 3, Rufeng Zhang 4, Enze Xie 1,\nZehuan Yuan3, Changhu Wang 3, Ping Luo 1\n1The University of Hong Kong 2Carnegie Mellon University\n3ByteDance AI Lab 4Tongji University\nAbstract\nIn this work, we propose TransTrack, a simple but ef-\nÔ¨Åcient scheme to solve the multiple object tracking prob-\nlems. TransTrack leverages the transformer architecture,\nwhich is an attention-based query-key mechanism. It ap-\nplies object features from the previous frame as a query\nof the current frame and introduces a set of learned ob-\nject queries to enable detecting new-coming objects. It\nbuilds up a novel joint-detection-and-tracking paradigm by\naccomplishing object detection and object association in\na single shot, simplifying complicated multi-step settings\nin tracking-by-detection methods. On MOT17 and MOT20\nbenchmark, TransTrack achieves 74.5% and 64.5% MOTA,\nrespectively, competitive to the state-of-the-art methods. We\nexpect TransTrack to provide a novel perspective for mul-\ntiple object tracking. The code is available at: https:\n//github.com/PeizeSun/TransTrack.\n1. Introduction\nVisual object tracking is a vital problem in many prac-\ntical applications, such as visual surveillance, public secu-\nrity, video analysis, and human-computer interaction. Ac-\ncording to the number of objects to track, the task of ob-\nject tracking is divided intoSingle Object Tracking (SOT)\nand Multiple Object Tracking (MOT). In recent years, the\nemerging of deep siamese networks [3, 37, 20, 19] have\nmade great progress in solving SOT tasks. However, the\nexisting MOT methods are still suffering from the model\ncomplexity and computational cost due to the multi-stage\npipeline [50, 36, 43] as shown in Figure 1a.\nA critical dilemma in many existing MOT solutions is\nwhen object detection and re-identiÔ¨Åcation are performed\nseparately, they can not beneÔ¨Åt each other. To tackle the\nproblem in MOT, a joint-detection-and-tracking framework\nis needed to share knowledge between detection and ob-\nject association. By reviewing SOT solutions, we empha-\nsize that Query-Key mechanism is promising in this direc-\n(a) Complex tracking-by-detection MOT pipeline.\n(b) Simple query-key SOT pipeline.\n(c) Query-key pipeline has great potential to setup a simple MOT\nmethod. However, it will miss new-coming objects.\nFigure 1: Motivation of TransTrack. The dominant MOT\nmethod is the complex multi-step tracking-by-detection\npipeline. Directly migrating the query-key mechanism from\nSOT to MOT will cause severe missing of new-coming ob-\njects. TransTrack is aimed to take advantage of query-key\nmechanism and to detect new-coming objects. The pipeline\nis shown in Figure 2.\n1\narXiv:2012.15460v2  [cs.CV]  4 May 2021\nùúô\nùêπ!\nkey\n‚Ä¶\nobject query\n detection box\nmatching\ntracking box\ntrack queryùêπ!\nobject feature\n(         track query )ùêπ!\"#\nFigure 2: Pipeline of TransTrack. Both object feature query from the previous frame and learned object queries are taken\nas input. The image feature maps are a shared key. The learned object query detects objects in the current frame. The track\nquery from the previous frame associates objects of the current frame with the previous ones. This process is performed\nsequentially over all adjacent frames and Ô¨Ånally completes the multiple object tracking tasks.\ntion. In existing works, the object target is the query and the\nimage regions are the keys as shown in Figure 1b. For the\nsame object, its feature in different frames is highly similar,\nwhich enables the query-key mechanism to output ordered\nobject sets. This inspiration should also be beneÔ¨Åcial to the\nMOT task.\nHowever, merely transferring the vanilla query-key\nmechanism from SOT into the MOT task leads to poor per-\nformance, signiÔ¨Åcantly causing much more false negatives.\nIt is because when an new object comes into birth, there is\nno corresponding features for it. This defect causes severe\nobject missing, as shown in Figure 1c. So what is a suitable\nquery-key mechanism for MOT remains a critical question.\nA desirable solution should be able to well capture new-\ncoming objects and propagate previously detected objects\nto the following frames at the same time.\nIn this paper, we make efforts in this direction by build-\ning an MOT framework based on transformer [38], which\nis an attention-based query-key mechanism. We term it as\nTransTrack. It leverages set prediction for detection [5]\nand the knowledge passed from the previous frame to gain\nreliable object association at the same time. There are two\nsets of keys (following previous works [5], they are confus-\ningly termed as ‚Äúobject query‚Äù in transformer). One set con-\ntains the object queries learned as in existing transformer-\nbased detector [5] and the other contains those generated\nfrom the features of objects on the previous frame, which\nare also termed as ‚Äútrack query‚Äù for clariÔ¨Åcation. The Ô¨Årst\nset of queries provides a sense of new-coming objects and\nthe track queries provide consistent object information to\nmaintain tracklets. Two sets of bounding boxes are pre-\ndicted respectively and TransTrack uses simple IoU match-\ning to generate the Ô¨Ånal ordered object set from them.\nIn TransTrack, the two sets of boxes can be output from\na uniform decoder architecture with only different queries\nas input. Our model even removes the traditional NMS\nstage in detection. Therefore, our method is simple and\nstraightforward where all components of the model can be\ntrained at the same time. We evaluate TransTrack on the\ntwo real-world benchmarks MOT17 and MOT20 [26, 7]. It\nachieves 74.5 and 64.5 MOTA on the test set of MOT17 and\nMOT20 respectively. To the best of our knowledge, we are\nthe Ô¨Årst to introduce the transformer in the MOT task. As\nit has achieved comparable performance with state-of-the-\nart models, we hope it could provide a new perspective and\nefÔ¨Åcient baseline for multi-object tracking tasks.\n2. Related Work\nIn this section, we Ô¨Årst review previous transformer ap-\nplications in vision tasks. Then we introduce the two main\nMOT paradigms, namely tracking-by-detection and joint-\ndetection-and-tracking methods.\nTransformer in vision tasks. Recently, there is a popu-\nlarity of using transformer architecture [38] in vision tasks,\nwhere it has been proven powerful and inspiring. As a spe-\ncial query-key mechanism, the transformer heavily relies on\nthe attention mechanism to process extracted deep features.\nIt Ô¨Årst shows great efÔ¨Åciency in natural language process-\ning [38] and later migrated to visual perception tasks [5]\nachieving remarkable success. Transformer appeals to the\nvision community with elegant structure and good perfor-\nmance. It has shown great potential in detection [5, 60], seg-\nmentation [57], 3D data processing [55] and even backbone\nconstruction [11]. Lately, the good efforts of using a trans-\nformer in processing sequential visual data also make re-\nmarkable shots in video segmentation [42]. With the natural\nstrength of passing features along the temporal dimension,\n2\nthe transformer shows the ability to contribute to diverse\ntemporal-spatial processing tasks on visual data and even\nreplaces the role of traditional RNN models [16]. How-\never, to the best of our knowledge, there are still no pub-\nlished transformer-based solutions for object tracking while\nit is intuitive to leverage its demonstrated good capacity in\nvisual perception and temporal processing there. Hence, in\nthis paper, we follow the insight to propose a transformer-\nbased model for MOT. It shows convincingly high perfor-\nmance on the popular MOT benchmark.\nTracking-by-detection. State-of-the-art multiple object\ntrackers are mostly dominated by the tracking-by-detection\nparadigm. It Ô¨Årstly uses the object detectors [23, 30, 22]\nto localize all objects of interest, then associates these de-\ntected objects according to their Re-ID features and/or other\ninformation, e.g., Intersection over Unions (IoU) between\neach other. SORT [4] tracks bounding boxes using the\nKalman Filter [44] and associates to the current frame by\nthe Hungarian algorithm [18]. DeepSORT [45] replaces\nthe association cost in SORT with the appearance fea-\ntures from deep convolutional networks. POI [50] achieves\nstate-of-the-art tracking performance based on the high-\nperformance detection and deep learning-based appearance\nfeature. Lifted-Multicut [36] combines the deep representa-\ntions and body pose feature obtained by the pose estimation\nmodel. STRN [48] presents a similarity learning frame-\nwork between tracks and objects, which encodes various\nSpatial-Temporal relations. Tracking-by-detection pipeline\nachieves leading performance, but its model complexity and\ncomputational cost are not satisfying.\nJoint-detection-and-tracking. The joint-detection-and-\ntracking pipeline aims to achieve detection and tracking\nsimultaneously in a single stage. D&T [13] proposes a\nmulti-task architecture for frame-based object detection and\nacross-frame track regression. Integrated-Detection [54]\nboosts the detection performance by combining the detec-\ntion bounding boxes in the current frame and tracks in pre-\nvious frames. More recently, Tracktor [1] directly uses\nthe previous frame tracking boxes as region proposals and\nthen applies the bounding box regression to provide track-\ning boxes on the current step, thus eliminating the box as-\nsociation procedure. JDE [43] and FairMOT [51] learn\nthe object detection task and appearance embedding task\nfrom a shared backbone. CenterTrack [58] localizes ob-\njects by tracking-conditioned detection and predicts their\noffsets to the previous frame. ChainedTracker [29] chains\npaired bounding boxes estimated from overlapping nodes,\nin which each node covers two adjacent frames. Our pro-\nposed TransTrack falls into the joint-detection-and-tracking\ncategory. Previous works adopt anchor-based [30] or point-\nbased [59] detection framework. Instead, we build the\npipeline based on a query-key mechanism and the tracked\nSelf-\nAttention\nCross-\nAttention\nFt  image     \nCNN\nAdd & Norm\nM x\nN \nx\nSelf-\nAttention\nFeed \nForward\nAdd & Norm\nAdd & Norm\nAdd & Norm\nAdd & Norm\nFeed \nForward\nobject query Ft  track query\nmatching\nFeed \nForward\nFeed \nForward\ndetection box tracking box\nFt object box\ntrack feature\nFt feature \nmap\nFt-1 feature \nmap\nSelf-\nAttention\nCross-\nAttention\nM x\nAdd & Norm\nAdd & Norm\nAdd & Norm\nFeed \nForward\nMatching\nDecoderDecoder\nEncoder\nobject feature\n(Ft+1 track query)\nBackbone\nFigure 3: The architecture details of TransTrack. First,\nthe current frame image is input to CNN backbone to ex-\ntract feature map. Then, both the current frame feature map\nand the previous one are fed into encoder to generate com-\nposite feature. Next, learned object query is decoded into\ndetection boxes and object feature of the previous frame is\ndecoded into tracking boxes. Finally, IoU matching is em-\nployed to associate detection boxes to tracking boxes.\nobject feature is used as the query.\n3. TransTrack\nIn MOT task, the desirable output is acomplete and cor-\nrectly ordered set of objects on each frame in a video. To\nthese two ends, TransTrack uses queries from two sources\nto gain adaptive cues. On the one hand, similar to usual\ntransformer-based detectors [5, 60], TransTrack takes an\nobject query as input to provide common object detec-\ntion results. On the other hand, TransTrack leverages\nfeatures from previously detected objects to form another\n‚Äútrack query‚Äù to discover associated objects on the follow-\ning frames. Under this scheme, TransTrack generates in\nparallel two sets of bounding boxes, termed as ‚Äúdetection\n3\nboxes‚Äù and ‚Äútracking boxes‚Äù. Last, TransTrack uses the\nHungarian algorithm, where the cost is IoU area among\nboxes, to achieve the Ô¨Ånal ordered box set from the two\nbounding box sets. The pipeline is illustrated in Figure 3.\n3.1. Pipeline\nIn this section, we introduce the encoder-decoder archi-\ntecture of TransTrack for object detection and object prop-\nagation. Given the detection boxes and tracking boxes from\ntwo decoders, box IoU matching is used to obtain the Ô¨Ånal\ntracking result. We also introduce the training and inference\nprocess of TransTrack.\nArchitecture. TransTrack is based on transformer, an\nencoder-decoder framework. It replies on stacked multi-\nhead attention layers and feed-forward networks. Multi-\nhead attention is called self-attention if the input query and\nthe input key are the same, otherwise, cross-attention. In\ntransformer architecture, The encoder generates keys and\nthe decoder takes as input task-speciÔ¨Åc queries. The archi-\ntecture overview is shown in Figure 3.\nThe encoder of TransTrack takes the composed feature\nmaps of two consecutive frames as input. To avoid du-\nplicated computation, the extracted features of the current\nframe are temporarily saved and then re-used for the next\nframe. Two parallel decoders are employed in TransTrack.\nFeature maps generated from the encoder are used as com-\nmon keys by the two decoders. The two decoders are de-\nsigned to perform object detection and object propagation,\nrespectively. SpeciÔ¨Åcally, a decoder takes learned object\nquery as input and predicts detection boxes. The other de-\ncoder takes the object feature from previous frames, namely\n‚Äútrack query‚Äù, as input and predicts the locations of the cor-\nresponding objects on the current frame, whose bounding\nboxes are termed as tracking boxes.\nObject Detection. Following DETR [5], TransTrack lever-\nages learned object query for object detection. The object\nquery is a set of learnable parameters, trained together with\nall other parameters in the network. During detection, the\nkey is the global feature maps generated from the input im-\nage and the object query looks up objects of interest in the\nimage and outputs the Ô¨Ånal detection predictions, termed as\n‚Äúdetection boxes‚Äù. This stage is performed by the left-hand\ndecoder block in Figure 3.\nObject Propagation. Given detected objects in the previ-\nous frame, TransTrack propagates these objects by passing\ntheir features to the next frame as the track query. The stage\nis performed by the right-hand decoder block in Figure 3.\nThe decoder has the same architecture as the left-hand one\nbut takes queries from different sources. This inherited ob-\nject feature conveys the appearance and location informa-\ntion of previously seen objects, so this decoder could well\nlocate the position of the corresponding object on the cur-\nrent frame and output ‚Äútracking boxes‚Äù.\nBox Association. Provided the detection boxes and track-\ning boxes, TransTrack uses the box IoU matching method\nto get the Ô¨Ånal tracking result, as shown in Figure 3. Apply-\ning the Kuhn-Munkres (KM) algorithm [18] to IoU similar-\nity of detection boxes and tracking boxes, detection boxes\nare matched to tracking boxes. Those unmatched detection\nboxes are kept to create new tracklets.\n3.2. Training\nTraining Data. We build training dataset from two sources.\nAs usual, the training data of could be two consecutive\nframes or two randomly selected frames from a real video\nclip. Furthermore, training data could also be the static\nimage [58], where the adjacent frame is simulated by ran-\ndomly scaling and translating the static image.\nTraining Loss. In TransTrack, tracking boxes and detec-\ntion boxes are the predictions of object boxes in the same\nimage. It allows us to simultaneously train two decoders by\nthe same training loss.\nTransTrack applies set prediction loss to supervise detec-\ntion boxes and tracking boxes of classiÔ¨Åcation and box coor-\ndinates. Set-based loss produces an optimal bipartite match-\ning between predictions and ground truth objects. Follow-\ning [5, 60, 35, 34, 39], the matching cost is deÔ¨Åned as\nL= Œªcls ¬∑Lcls + ŒªL1 ¬∑LL1 + Œªgiou ¬∑Lgiou (1)\nwhere Lcls is focal loss [23] of predicted classiÔ¨Åcations and\nground truth category labels,LL1 and Lgiou are L1 loss and\ngeneralized IoU loss [31] between normalized center coor-\ndinates and height and width of predicted boxes and ground\ntruth box, respectively. Œªcls, ŒªL1 and Œªgiou are coefÔ¨Åcients\nof each component. The training loss is the same as the\nmatching cost except that only performed on matched pairs.\nThe Ô¨Ånal loss is the sum of all pairs normalized by the num-\nber of objects inside the training batch.\n3.3. Inference\nIn the inference stage, TransTrack Ô¨Årst detects objects on\nthe Ô¨Årst frame, where the feature maps are from two copies\nof the Ô¨Årst frame. Then TransTrack operates object prop-\nagation and box association for the following frames and\nÔ¨Ånally completes tracklets over the entire video sequence.\nWe use track rebirth in the inference procedure of\nTransTrack to enhance robustness to occlusions and short-\nterm disappearing [1, 58, 29]. SpeciÔ¨Åcally, if a tracking box\nis unmatched, it keeps as an ‚Äúinactive‚Äù tracking box until\nit remains unmatched for K consecutive frames. Inactive\ntracking boxes can be matched to detection boxes and re-\ngain their ID. Following [58], we choose K = 32.\n4\nBenchmark Method Data MOTA‚Üë IDF1‚Üë MOTP‚Üë MT‚Üë ML‚Üì FP‚Üì FN‚Üì IDS‚Üì\nMOT17\nTubeTK [27] No 63.0 58.6 78.3 31.2 19.9 27060 177483 4137\nChainedTracker [29] No 66.6 57.4 78.2 32.2 24.2 22284 160491 5529\nQuasiDense [28] No 68.7 66.3 79.0 40.6 21.9 26589 146643 3378\nGSDT [41] 5D2 73.2 66.5 80.7 41.7 17.5 26397 120666 3891\nCSTrack [21] 5D1 74.9 72.6 80.9 41.5 17.5 23847 114303 3567\nFairMOT [51] 5D1 73.7 72.3 81.3 43.2 17.3 27507 117477 3303\nFUFET [32] 5D1 76.2 68.0 81.1 51.1 13.6 32796 98475 3237\nMLT [53] 5D1 75.3 75.5 81.7 49.3 19.5 27879 109836 1719\nCorrTracker [40] 5D1 76.5 73.6 81.2 47.6 12.7 29808 99510 3369\nCenterTrack [58] CH 67.8 64.7 78.4 34.6 24.6 18489 160332 3039\nTraDeS [46] CH 69.1 63.9 78.9 36.4 21.5 20892 150060 3555\nTransMOT [6] CH 76.7 75.1 82.0 51.0 16.4 36231 93150 2346\nTransCenter [49] CH 73.2 62.2 81.1 40.8 18.5 23112 123738 4614\nTransTrack(ours) CH 74.5 63.9 80.6 46.8 11.3 28323 112137 3663\nMOT20\nGSDT [41] 5D2 67.1 67.5 79.1 53.1 13.2 31507 135395 3230\nCSTrack [21] 5D1 66.6 68.6 78.8 50.4 15.5 25404 144358 3196\nFairMOT [51] 5D1 61.8 67.3 78.6 68.8 7.6 103440 88901 5243\nCorrTracker [40] 5D1 65.2 73.6 - 47.6 12.7 29808 99510 3369\nTransCenter [49] CH 58.3 46.8 79.7 35.7 18.6 35959 174893 4947\nTransTrack(ours) CH 64.5 59.2 80.0 49.1 13.6 28566 151377 3565\nTable 1: Evaluation on MOT17 and MOT20 test sets. We compare TransTrack with recent methods in private protocol,\nwhere external data can be used: CH for CrowdHuman [33], 5D1 for the use of 5 extra datasets, including CrowdHuman [33],\nCaltech Pedestrian [9, 10], CityPersons [52], CUHK-SYS [47], and PRW [56], 5D2 is the same as 5D1 replacing CroudHu-\nman by ETH [12], NO for using no extra dataset.\n4. Experiments\nTo measure the performance of our proposed method,\nwe conduct experiments on the pedestrian-tracking dataset\nMOT17 [26] and MOT20 [7]. In the ablation study, we fol-\nlow previous practice [58] to split the MOT17 training set\ninto two parts, one for training and the other for validation.\nWe adopt the widely-used MOT metrics set [2] for quan-\ntitative evaluation where multiple object tracking accuracy\n(MOTA) is the primary metric to measure the overall per-\nformance.\n4.1. Implementation details\nWe use ResNet-50 [15] as the network backbone. The\noptimizer is AdamW [24] and the batch size is set to be\n16. The initial learning rate is 2e-4 for the transformer and\n2e-5 for the backbone. The weight decay is 1e-4 All trans-\nformer weights are initialized with Xavier-init [14], and the\nbackbone model is pretrained on ImageNet [8] with frozen\nbatch-norm layers [17]. We use data augmentation includ-\ning random horizontal, random crop, scale augmentation,\nresizing the input images whose shorter side is by 480 -\n800 pixels while the longer side is by at most 1333 pixels.\nWe train the model for 150 epochs and the learning rate\ndrops by a factor of 10 at the 100th epoch. In the ablation\nstudy, the model is Ô¨Årst pre-trained on CrowdHuman [33]\nand then Ô¨Åne-tuned on MOT. When evaluating on the test\nset, we train our network on combination of CrowdHuman\nand MOT. More details are discussed in Appendix.\n4.2. MOT17 benchmark\nWe evaluate models on MOT17 under the private detec-\ntor setting. The results We evaluate models on MOT17 un-\nder the private detector setting. The results are shown in Ta-\nble 1. TransTrack achieves comparable results with the cur-\nrent state-of-the-art methods, especially in terms of MOTP\nand FN. The excellent MOTP demonstrates TransTrack\ncan precisely locate objects in the image. The good FN\nscore represents that most objects are successfully detected.\nThose prove the success of introducing learned object query\ninto the pipeline. As for ID-switch, TransTrack is compara-\nble with the popular trackers, e.g., FairMOT [51] and Cen-\nterTrack [58], which proves the effectiveness of object fea-\nture query to associate adjacent frames. Although the ID-\nswitch score of TransTrack is inferior to SOTA methods, it\nis a promising direction to further improve the overall per-\nformance of TransTrack.\n5\n4.3. MOT20 benchmark\nWe evaluate models on MOT20 under the private de-\ntector setting. The results are shown in Table 1. MOT20\nincludes more crowded scenes than MOT17. Its more se-\nvere object occlusion and smaller object size bring more\nchallenges for object detection and tracking. Therefore,\nall methods show lower performance on MOT20 than on\nMOT17. But still, TransTrack achieves comparable re-\nsults with the current state-of-the-art methods on MOT20,\nin terms of detection metrics and association metrics.\n4.4. Ablation study\n4.4.1 Transformer Architecture\nWe ablate the effect of Transformer architecture. Four\ntransformer structures are put into comparison. Trans-\nformer follows the settings of DETR [5] detector, where\ntransformer is built on top of the feature maps of res5\nstage [15]. Transformer-DC5 increases the feature maps\nresolution. To be precise, we apply dilation convolution to\nres5 stage and remove a stride from the Ô¨Årst convolution of\nthis stage. Transformer-P3 adopts FPN [22] on the input\nfeature maps. The encoder of the Transformer is directly\nremoved from the whole pipeline for memory limitation.\nAfter removing the encoder, the learning rate of the back-\nbone could be raised to the same as transformers. Finally,\nwe also tried Deformable Transformer [60], which is a\nrecently proposed architecture to solve the issue of limited\nresolution in the transformer. Within plausible memory us-\nage, it fuses multiple-scale features into the whole encoder-\ndecoder pipeline and achieves excellent performance in the\ngeneral object detection dataset.\nThe quantitative results are shown in Table 2. The\nÔ¨Ånal performance of Transformer is only 55.4 MOTA.\nWith higher feature resolution, Transformer-DC5 yields\n3.6 MOTA improvement. However, it also leads to the\ndrawback of dilation convolution, such as big memory us-\nage. Transformer-P3 only outputs close performance as\nTransformer-DC5, saying that higher resolution than DC5\nfails to bring further performance gain. And the reason\nbehind this might be the absence of encoder blocks. At\nlast, Deformable Transformerfuses multiple-scale feature\ninto the whole encoder-decoder pipeline and achieves ex-\ncellent performance, up to 65.0 MOTA. Therefore, we use\nDeformable Transformer as the default architecture choice\nof TransTrack.\n4.4.2 Query in Decoder\nWe study the effect of what the input query is used. In the\ndetection task, an input query is generated from the input\nimage only [5, 60]. But in tracking, the knowledge of pre-\nviously detected objects is expected to be helpful, so we set\nArchitecture MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nTransformer [5] 55.4 7.4% 35.2% 2.0%\nTransformer-DC5 [5] 59.0 5.2% 34.0% 1.8%\nTransformer-P3 59.3 5.1% 33.8% 1.8%\nDeformable Transformer [60] 65.0 4.3% 30.3% 0.4%\nTable 2: Ablation study on Transformer architecture.\nOriginal transformer suffers from low feature resolution.\nDeformable DETR with multi-scale feature input achieves\nbest performance.\nQuery MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nObejct query 58.3 4.0% 29.7% 8.0%\nTrack query - 15.6% 93.8% 0.3%\nTrack query + Object query 65.0 4.3% 30.3% 0.4%\nTable 3: Ablation study on input query. Using only ob-\nject query obtains limited association performance. Using\nonly track query leads to numerous FN since it misses new-\ncoming objects. By using both object query and track query,\nthe detection and tracking performance are improved.\nexperiments to compare the model performance when ob-\nject query and track query are used or absent respectively.\nThe results are reported in Table 3.\nOnly object query. When only learned object query is in-\nput as decoder query, we adopt a naive pipeline where the\noutput detection boxes are associated according to their in-\ndex in the output set. Surprisingly, this solution achieves a\nnot bad performance by 58.3 MOTA. This is because each\nobject query predicts the object in a certain area on images,\nand most objects just move around a small distance in the\nvideo sequence. However, solely relying on the index in\nthe output set leads to non-negligible wrong matching, es-\npecially when the object moves through a long distance.\nWhen the object moves around a wide range, this pattern\nfails easily as visualized in Figure 4.\nOnly track query. When only the track query, which is\ngenerated from the previous frame, is input to the decoder,\nwe have no common detection results on the image. The\nvisualization in the second row of Figure 4 shows that this\nmethod is capable to associate the object with a large range\nof motion. Nevertheless, only the object that appears in the\nÔ¨Årst frame can be tracked successively. For the whole video\nsequence, most of the objects will be missed and the FN\nmetric collapses as shown in the second row of Table 3.\nObject query + track query. As the default setting of\nTransTrack, both object query and track query are input to\nthe decoder. Now it can handle most failure cases in the\nprevious two cases with the help of the other query. Visu-\nalization in Figure 4 and performance reported in Table 3\nprove the giant improvement.\n6\nFigure 4: Visualization of TransTrack with different input query.1st row is only learned object query. 2nd row is only\nobject feature query from the previous frame. 3rd row is both learned object query and object feature query from the\nprevious frame. Only learned object query or object feature query from the previous frame causes ID switch case or missing\nobject case. TransTrack takes both as input query and exhibits best detection and tracking performance.\nMatching MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nPrevious 64.8 4.8% 29.8% 0.6%\nCurrent 65.0 4.3% 30.3% 0.4%\nTable 4: Ablation study of matching strategy of tracking\nboxes. Previous indicates directly inheriting the index of\ntrack query for box matching on the previous frame. Cur-\nrent indicates using optimal bipartite matching with current\nobject boxes.\n4.4.3 Matching strategy of tracking boxes\nTransTrack builds tracklets based on two sets of detection\nresults and box matching. To emphasize temporal corre-\nlation in tracking tasks, it is natural to consider match-\ning tracking boxes with previous frame objects. To ab-\nlate the inÔ¨Çuence of tracking boxes matching, we conduct\ntwo strategies. One way is to match initial tracking boxes\nto previous object boxes by optimal bipartite matching\n(Previous), in other words, the matching index is directly\nfrom the matching index of corresponding track queries.\nThe other strategy is to supervise the output of tracking\nAssociation MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nHungarian 65.0 4.3% 30.3% 0.4%\nNMS 65.0 4.3% 30.3% 0.4%\nTable 5: Ablation study of box association. Two sets\nof bounding boxes, track boxes and detection boxes, are\nmerged into the desired ordered object set. The results show\nthat Hungarian algorithm and NMS actually have the same\neffect in this stage.\nboxes with optimal bipartite matching to current object\nboxes (Current). The results are shown in Table 4. The\nresults show that bipartite matching with previous frame ob-\njects does not help to void ID switch (0.6% v.s. 0.4%). This\nshows that the inherit the property of the query-key mech-\nanism could well locate the position of the corresponding\nobject on the current frame already.\n4.4.4 Bounding Box Association\nWe study the effect of different box association post-\nprocessing strategies. We choose the classic Hungarian al-\n7\nMotion model MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì 4xIDs‚Üì\nNone 64.4 4.3% 30.3% 1.0% 1.2%\nKalman Ô¨Ålter 64.9 4.3% 30.4% 0.4% 1.0%\nTrack query(Ours) 65.0 4.3% 30.3% 0.4% 0.5%\nTable 6: The effect of motion model . All models use\nDETR as detectors. For None, object box is associated\nby IoU similarity. For Kalman Ô¨Ålter , the output bound-\ning boxes are processed by Kalman Ô¨Ålter. Ours follows the\ntwo-query-set setting where track query is used to associate\nacross-frame objects.\ngorithm [18] and the NMS merging method used in [58, 25].\nResults are shown in Table 5. It suggests both two strategies\nshow equivalent effect in the box association stage.\n4.5. Comparisons with other trackers\nTwo commonly used signals to upgrade a detector to\na tracker are motion and appearance features. So ‚Äúdetec-\ntor + motion model‚Äù and ‚Äúdetector + Re-ID‚Äù are widely-\nused and intuitive methods, thus it is necessary to compare\nTransTrack with these two models to have a clear idea about\nhow much TransTrack gains from its design except for im-\nprovement from the detector it replies on.\n4.5.1 Motion model\nWe combine the widely-used Kalman Ô¨Ålter and DETR to\nbuild a ‚Äúdetector + motion model‚Äù tracker. The results\nare shown in Table 6. Kalman Ô¨Ålter and our method pro-\nvide similar IDs performance. We explain that the MOT17\ndataset is the video sequence of high frame rate (14-30\nFPS), where the object motion between two adjacent frames\nis minor. Therefore, different association methods make no\nbig difference. However, when we sample one frame every\n4 frames, the object motion becomes larger, then the im-\nprovement brought by the feature query is obvious (0.5%\nvs. 1.0%), shown in the last column of Table 6. Similar\nphenomenons are discussed in [58].\n4.5.2 Re-ID features\nTo maintain the joint-detection-and-tracking paradigm, we\ndo not implement an independent Re-ID model but use the\nRe-ID branch to formulate a ‚Äúdetector + Re-ID‚Äù tracker.\nAs features generated in the detector have conÔ¨Çicts with\nappearance-based Re-ID features [51], we study the inÔ¨Çu-\nence of using an independent Re-ID passway, e.g., a cross-\nattention layer in the decoder. The two patterns are illus-\ntrated in Figure 5. The results are included in Table 7.\nIt agrees that when passed through an independent pass-\nway, the Re-ID feature brings better results than using a\nobject query\nCross-Attention\nobject \nfeature\ndetection \nbox\nre-ID \nfeature\nCross-\nAttention\ndetection \nbox\nre-ID \nfeature\nCross-\nAttention\nobject query\n(a) shared feature (b) independent feature\nFigure 5: Two designs to introduce Re-ID into DETR.\nThe left one uses a shared feature from a single cross-\nattention layer to train detection and re-identiÔ¨Åcation. The\nright scheme uses two cross-attention layers to generate in-\ndependent Re-ID features and detection features for the two\nsources of supervision.\nRe-ID feature MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nShared 61.1 5.9% 32.3% 0.7%\nIndependent 64.7 3.2% 31.7% 0.4%\nNone (Ours) 65.0 4.3% 30.3% 0.4%\nTable 7: The effect of Re-ID features. When passing Re-\nID features to an independent cross-attention layer, the per-\nformance is better than using shared cross-attention layer\nfor detection features and Re-ID features. However, this\nalso results in degradation of detector, so the overall perfor-\nmance does not beat default TransTrack.\nshared module with detection features. However, the overall\nMOTA score is not improved against default TransTrack.\n5. Conclusion\nIn this work, we set up a joint-detection-and-tracking\nMOT pipeline, TransTrack, based on the transformer. It\nuses the learned object query as input to detects objects\nand track query, which is the features the from previous\nframe, to propagate previously detected objects to the fol-\nlowing frames. TransTrack is the Ô¨Årst work solving MOT in\nsuch a paradigm. It achieves a competitive 74.5 MOTA on\nthe MOT17 dataset and 64.5 MOTA on a more challenging\nMOT20 dataset. We expect it to provide a novel perspective\nand insight to the MOT community.\n8\nAppendix\nA. Training Data\nWe follow the common practice of the state-of-the-\nart MOT methods [58] to train TransTrack on CrowdHu-\nman [33] Ô¨Årst and then Ô¨Åne-tune the model on MOT17.\nWe conduct a comparison to study the effect of the ex-\nternal CrowdHuman data. The result is reported in Ta-\nble 8. Only using the training set of MOT17 merely obtains\n61.6 MOTA. When Ô¨Årst pre-trained on CrowdHuman then\ntrained on MOT17, the performance achieves 64.8 MOTA.\nIt suggests adding external data boosts the model perfor-\nmance signiÔ¨Åcantly.\nPre-train Fine-tune MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nCH - 53.8 13.0% 32.3% 1.0%\n- MOT17 61.6 3.4% 34.2% 0.9%\nCH MOT17 65.0 4.3% 30.3% 0.4%\nTable 8: Ablation study on pre-training data. The Ô¨Årst\nrow is the model trained only on CrowdHuman dataset. The\nsecond row indicates model trained on the training set of\nMOT dataset only. The third shows the performance when\nthe model is trained on CrowdHuman Ô¨Årst and then Ô¨Åne-\ntuned on MOT dataset. All models are evaluated on the\nvalidation set of MOT17 dataset.\nBesides the pre-training data settings, we Ô¨Ånd the data\nused for Ô¨Åne-tuning is also critical. We conduct an abla-\ntion study on it and the results are shown in Table 9. Inter-\nestingly, Ô¨Åne-tuning on the combination of CrowdHuman\nand MOT shows better performance than Ô¨Åne-tuning on the\nMOT dataset only.\nDataset Pre-train Fine-tune MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì\nMOT17 CH MOT17 68.4 22137 152064 3942\nCH CH+MOT17 74.5 28323 112137 3663\nMOT20 CH MOT20 57.4 32921 184047 3705\nCH CH+MOT20 64.5 28566 151377 3565\nTable 9: Ablation study on Ô¨Åne-tuning data. For each\nbenchmark, the Ô¨Årst row is the model Ô¨Åne-tuned only on\nMOT train dataset. The second row indicates the model\nÔ¨Åne-tuned on the combination of CrowdHuman and MOT\ntraining set. All models are evaluated on the test set of MOT\nbenchmark.\nB. Accuracy vs. Speed\nWe analyze the inference speed of TransTrack. The time\ncost is measured using a single Tesla V100 GPU. Table 10\nshows the effect of number of decoders. Increasing de-\ncoders hurts inference speed, for example, from 1 decoder\nto 6, FPS decreases from 15FPS to 10FPS. However, more\ndecoders signiÔ¨Åcantly boost MOTA performance. There-\nfore, we choose 6 as the default decoder number. Table 11\nshows the effect of input image size. Gradually increasing\ninput image size, MOTA performance is saturated when the\nshort-side of the input image is by 800 pixels so we set it as\nthe default setting in TransTrack.\nDecoders MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì FPS\n1 47.0 10.0% 40.0% 3.0% 15\n3 64.3 3.3% 31.4% 1.0% 12\n6 65.0 4.3% 30.3% 0.4% 10\nTable 10: Ablation study on number of decoders. In-\ncreasing decoders has minor impact on inference time while\nsigniÔ¨Åcantly improves MOTA performance. Therefore, we\nchoose 6 decoders as default.\nShort-side MOTA ‚Üë FP‚Üì FN‚Üì IDs‚Üì FPS\n540 pix 62.4 3.9% 32.8% 0.9% 14\n800 pix 65.0 4.3% 30.3% 0.4% 10\n1080 pix 59.2 4.7% 35.0% 1.1% 7\nTable 11: Ablation study on input image size. Gradually\nincreasing input image size, MOTA performance is satu-\nrated when the short-side of image is 800 pixels.\n9\nReferences\n[1] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe.\nTracking without bells and whistles. In ICCV, pages 941‚Äì\n951, 2019. 3, 4\n[2] Keni Bernardin and Rainer Stiefelhagen. Evaluating mul-\ntiple object tracking performance: the clear mot metrics.\nEURASIP Journal on Image and Video Processing, 2008:1‚Äì\n10, 2008. 5\n[3] Luca Bertinetto, Jack Valmadre, Jo Àúao F. Henriques, Andrea\nVedaldi, and Philip H. S. Torr. Fully-convolutional siamese\nnetworks for object tracking, 2016. 1\n[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In 2016\nIEEE International Conference on Image Processing (ICIP),\npages 3464‚Äì3468, 2016. 3\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nEnd object detection with transformers. In ECCV, 2020. 2,\n3, 4, 6\n[6] Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling,\nand Zicheng Liu. Transmot: Spatial-temporal graph\ntransformer for multiple object tracking. arXiv preprint\narXiv:2104.00194, 2021. 5\n[7] P. Dendorfer, H. RezatoÔ¨Åghi, A. Milan, J. Shi, D. Cremers,\nI. Reid, S. Roth, K. Schindler, and L. Leal-Taix ¬¥e. Mot20:\nA benchmark for multi object tracking in crowded scenes.\narXiv:2003.09003[cs], Mar. 2020. arXiv: 2003.09003. 2, 5\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248‚Äì255. Ieee, 2009. 5\n[9] Piotr Doll ¬¥ar, Christian Wojek, Bernt Schiele, and Pietro Per-\nona. Pedestrian detection: A benchmark. In 2009 IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n304‚Äì311. IEEE, 2009. 5\n[10] Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Per-\nona. Pedestrian detection: An evaluation of the state of the\nart. IEEE transactions on pattern analysis and machine in-\ntelligence, 34(4):743‚Äì761, 2011. 5\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2020. 2\n[12] Andreas Ess, Bastian Leibe, Konrad Schindler, and Luc\nVan Gool. A mobile vision system for robust multi-person\ntracking. In 2008 IEEE Conference on Computer Vision and\nPattern Recognition, pages 1‚Äì8. IEEE, 2008. 5\n[13] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nDetect to track and track to detect, 2018. 3\n[14] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Å-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiÔ¨Å-\ncial intelligence and statistics, pages 249‚Äì256, 2010. 5\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 5, 6\n[16] Sepp Hochreiter and J ¬®urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735‚Äì1780, 1997. 3\n[17] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 5\n[18] Harold W Kuhn. The hungarian method for the assignment\nproblem. Naval research logistics quarterly , 2(1-2):83‚Äì97,\n1955. 3, 4, 8\n[19] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\nand Junjie Yan. Siamrpn++: Evolution of siamese visual\ntracking with very deep networks, 2018. 1\n[20] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu. High performance\nvisual tracking with siamese region proposal network. In\n2018 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8971‚Äì8980, 2018. 1\n[21] Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li,\nXiyong Ye, and Jianxiao Zou. Rethinking the competition\nbetween detection and reid in multi-object tracking. arXiv\npreprint arXiv:2010.12138, 2020. 5\n[22] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017. 3, 6\n[23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll¬¥ar. Focal loss for dense object detection, 2018. 3,\n4\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[25] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. Trackformer: Multi-object track-\ning with transformers. arXiv preprint arXiv:2101.02702 ,\n2021. 8\n[26] Anton Milan, Laura Leal-Taix ¬¥e, Ian Reid, Stefan Roth, and\nKonrad Schindler. Mot16: A benchmark for multi-object\ntracking. arXiv preprint arXiv:1603.00831, 2016. 2, 5\n[27] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu.\nTubetk: Adopting tubes to track multi-object in a one-step\ntraining model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6308‚Äì\n6318, 2020. 5\n[28] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi\nLi, Trevor Darrell, and Fisher Yu. Quasi-dense similar-\nity learning for multiple object tracking. arXiv preprint\narXiv:2006.06664, 2020. 5\n[29] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu,\nYabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue\nHuang, and Yanwei Fu. Chained-tracker: Chaining paired at-\ntentive regression results for end-to-end joint multiple-object\ndetection and tracking. arXiv preprint arXiv:2007.14557 ,\n2020. 3, 4, 5\n[30] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks, 2016. 3\n[31] Hamid RezatoÔ¨Åghi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In CVPR, 2019. 4\n10\n[32] Chaobing Shan, Chunbo Wei, Bing Deng, Jianqiang Huang,\nXian-Sheng Hua, Xiaoliang Cheng, and Kewei Liang.\nFgagt: Flow-guided adaptive graph tracking. arXiv preprint\narXiv:2010.09015, 2020. 5\n[33] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,\nXiangyu Zhang, and Jian Sun. Crowdhuman: A bench-\nmark for detecting human in a crowd. arXiv preprint\narXiv:1805.00123, 2018. 5, 9\n[34] Peize Sun, Yi Jiang, Enze Xie, Zehuan Yuan, Changhu\nWang, and Ping Luo. Onenet: Towards end-to-end one-stage\nobject detection. arXiv preprint arXiv:2012.05780, 2020. 4\n[35] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, and Ping Luo. Sparse r-cnn: End-to-end\nobject detection with learnable proposals. arXiv preprint\narXiv:2011.12450, 2020. 4\n[36] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt\nSchiele. Multiple people tracking by lifted multicut and per-\nson re-identiÔ¨Åcation. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , July\n2017. 1, 3\n[37] Ran Tao, Efstratios Gavves, and Arnold W. M. Smeulders.\nSiamese instance search for tracking, 2016. 1\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017. 2\n[39] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun,\nJian Sun, and Nanning Zheng. End-to-end object de-\ntection with fully convolutional network. arXiv preprint\narXiv:2012.03544, 2020. 4\n[40] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multi-\nple object tracking with correlation learning. arXiv preprint\narXiv:2104.03541, 2021. 5\n[41] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object\ndetection and multi-object tracking with graph neural net-\nworks. arXiv preprint arXiv:2006.13164, 5, 2020. 5\n[42] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers, 2020. 2\n[43] Zhongdao Wang, Liang Zheng, Yixuan Liu, and Shengjin\nWang. Towards real-time multi-object tracking. arXiv\npreprint arXiv:1909.12605, 2019. 1, 3\n[44] Greg Welch, Gary Bishop, et al. An introduction to the\nkalman Ô¨Ålter, 1995. 3\n[45] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple\nonline and realtime tracking with a deep association metric.\nIn ICIP, pages 3645‚Äì3649. IEEE, 2017. 3\n[46] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming\nYang, and Junsong Yuan. Track to detect and seg-\nment: An online multi-object tracker. arXiv preprint\narXiv:2103.08808, 2021. 5\n[47] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiao-\ngang Wang. Joint detection and identiÔ¨Åcation feature learn-\ning for person search. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n3415‚Äì3424, 2017. 5\n[48] Jiarui Xu, Yue Cao, Zheng Zhang, and Han Hu. Spatial-\ntemporal relation networks for multi-object tracking, 2019.\n3\n[49] Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan,\nDaniela Rus, and Xavier Alameda-Pineda. Transcenter:\nTransformers with dense queries for multiple-object track-\ning. arXiv preprint arXiv:2103.15145, 2021. 5\n[50] Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi,\nand Junjie Yan. Poi: Multiple object tracking with high per-\nformance detection and appearance feature. In European\nConference on Computer Vision , pages 36‚Äì42. Springer,\n2016. 1, 3\n[51] Yifu Zhan, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\nand Wenyu Liu. A simple baseline for multi-object tracking.\narXiv preprint arXiv:2004.01888, 2020. 3, 5, 8\n[52] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele.\nCitypersons: A diverse dataset for pedestrian detection. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3213‚Äì3221, 2017. 5\n[53] Yang Zhang, Hao Sheng, Yubin Wu, Shuai Wang, Wei Ke,\nand Zhang Xiong. Multiplex labeling graph for near-online\ntracking in crowded scenes.IEEE Internet of Things Journal,\n7(9):7892‚Äì7902, 2020. 5\n[54] Zheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, and\nJifeng Dai. Integrated object detection and tracking with\ntracklet-conditioned detection, 2018. 3\n[55] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer, 2020. 2\n[56] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan\nChandraker, Yi Yang, and Qi Tian. Person re-identiÔ¨Åcation\nin the wild. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1367‚Äì1376,\n2017. 5\n[57] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers, 2020. 2\n[58] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¬®ahenb¬®uhl.\nTracking objects as points, 2020. 3, 4, 5, 8, 9\n[59] Xingyi Zhou, Dequan Wang, and Philipp Kr ¬®ahenb¬®uhl. Ob-\njects as points, 2019. 3\n[60] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2, 3, 4, 6\n11"
}