{
  "title": "Large Scale Legal Text Classification Using Transformer Models",
  "url": "https://openalex.org/W3094571063",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227521637",
      "name": "Shaheen, Zein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3174434650",
      "name": "Wohlgenannt Gerhard",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288504662",
      "name": "Filtz, Erwin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2899313146",
    "https://openalex.org/W3080739421",
    "https://openalex.org/W3011296786",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2971106339",
    "https://openalex.org/W2787132559",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963967134",
    "https://openalex.org/W2116878278",
    "https://openalex.org/W2362855512",
    "https://openalex.org/W2183087644",
    "https://openalex.org/W2118020653",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2952695757",
    "https://openalex.org/W2077260783",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W54887220",
    "https://openalex.org/W2962910668",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2889577585",
    "https://openalex.org/W2992422826",
    "https://openalex.org/W2980073373",
    "https://openalex.org/W756166754",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2987098737"
  ],
  "abstract": "Large multi-label text classification is a challenging Natural Language Processing (NLP) problem that is concerned with text classification for datasets with thousands of labels. We tackle this problem in the legal domain, where datasets, such as JRC-Acquis and EURLEX57K labeled with the EuroVoc vocabulary were created within the legal information systems of the European Union. The EuroVoc taxonomy includes around 7000 concepts. In this work, we study the performance of various recent transformer-based models in combination with strategies such as generative pretraining, gradual unfreezing and discriminative learning rates in order to reach competitive classification performance, and present new state-of-the-art results of 0.661 (F1) for JRC-Acquis and 0.754 for EURLEX57K. Furthermore, we quantify the impact of individual steps, such as language model fine-tuning or gradual unfreezing in an ablation study, and provide reference dataset splits created with an iterative stratification algorithm.",
  "full_text": "Large Scale Legal Text Classiﬁcation Using Transformer Models\nZein Shaheen\nITMO University\nSt. Petersburg, Russia\nshaheen@itmo.ru\nGerhard Wohlgenannt\nITMO University\nSt. Petersburg, Russia\ngwohlg@corp.ifmo.ru\nErwin Filtz\nVienna University of Economics and Business (WU)\nVienna, Austria\nerwin.ﬁltz@wu.ac.at\nAbstract—Large multi-label text classiﬁcation is a challenging\nNatural Language Processing (NLP) problem that is concerned\nwith text classiﬁcation for datasets with thousands of labels.\nWe tackle this problem in the legal domain, where datasets,\nsuch as JRC-Acquis and EURLEX57K labeled with the EuroVoc\nvocabulary were created within the legal information systems\nof the European Union. The EuroVoc taxonomy includes around\n7000 concepts. In this work, we study the performance of various\nrecent transformer-based models in combination with strategies\nsuch as generative pretraining, gradual unfreezing and discrim-\ninative learning rates in order to reach competitive classiﬁcation\nperformance, and present new state-of-the-art results of 0.661\n(F1) for JRC-Acquis and 0.754 for EURLEX57K. Furthermore,\nwe quantify the impact of individual steps, such as language\nmodel ﬁne-tuning or gradual unfreezing in an ablation study,\nand provide reference dataset splits created with an iterative\nstratiﬁcation algorithm.\nKeywords–multi-label text classiﬁcation; legal document\ndatasets; transformer models; EuroVoc.\nI. INTRODUCTION\nText classiﬁcation, i.e., the process of assigning one or\nmultiple categories from a set of options to a document [1],\nis a prominent and well-researched task in Natural Language\nProcessing (NLP) and text mining. Text classiﬁcation variants\ninclude simple binary classiﬁcation (for example, decide if\na document is spam or not spam), multi-class classiﬁcation\n(selection of one from a number of classes), and multi-label\nclassiﬁcation. In the latter, multiple labels can be assigned to\na single document. In Large Multi-Label Text Classiﬁcation\n(LMTC), the label space is typically comprised of thousands\nof labels, which obviously raises task complexity. The work\npresented here tackles an LMTC problem in the legal domain.\nLMTC tasks often occur when large taxonomies or formal\nontologies are used as document labels, for example in the\nmedical domain [2] [3], or when using large open domain\ntaxonomies for labelling, such as annotating Wikipedia with\nlabels [4]. A common feature of many LMTC tasks is that\nsome labels are used frequently, while others are used very\nrarely (few-shot learning) or are never used (zero-shot learn-\ning). This situation is also referred to by power-law or long-tail\nfrequency distribution of labels, which also characterizes our\ndatasets and which is a setting that is largely unexplored for\ntext classiﬁcation [3]. Another difﬁculty often faced in LMTC\ndatasets [3] are long documents, where ﬁnding the relevant\nareas to correctly classify documents is a needle in a haystack\nsituation.\nIn this work, we focus on LMTC in the legal domain,\nbased on two datasets, the well-known JRC-Acquis dataset [5]\nand the new EURLEX57K dataset [6]. Both datasets contain\nlegal documents from Eur-Lex [7], the legal database of the\nEuropean Union (EU). The usage of language in the given\ndocuments is highly domain speciﬁc, and includes many legal\ntext artifacts such as case numbers. Modern neural NLP\nalgorithms often tackle domain speciﬁc text by ﬁne-tuning\npretrained language models on the type of text at hand [8].\nBoth datasets are labelled with terms from the the European\nUnion’s multilingual and multidisciplinary thesaurus EuroVoc\n[9].\nThe goal of this work is to advance the state-of-the-art in\nLMTC based on these two datasets which exhibit many of\nthe characteristics often found in LMTC datasets: power-law\nlabel distribution, highly domain speciﬁc language and a large\nand hierarchically organized set of labels. We apply current\nNLP transformer models, namely BERT [10], RoBERTa [11],\nDistilBERT [12], XLNet [13] and M-BERT [10], and combine\nthem with a number of training strategies such as gradual un-\nfreezing, slanted triangular learning rates and language model\nﬁne-tuning. In the process, we create new standard dataset\nsplits for JRC-Acquis and EURLEX57 using an iterative strat-\niﬁcation approach [14]. Providing a high-quality standardized\ndataset split is very important, as previous work was typically\ndone on different random splits, which makes results hard to\ncompare [15]. Further, we make use of the semantic relations\ninside the EuroV oc taxonomy to infer reduced label sets for the\ndatasets. Some of our main evaluation results are the Micro-F1\nscore of 0.661 for JRC-Acquis and 0.754 for EURLEX57K,\nwhich sets new states-of-the-art to the best of our knowledge.\nThe main ﬁndings and contributions of this work are: (i)\nthe experiments with BERT, RoBERTa, DistilBERT, XLNet,\nM-BERT (trained on three languages), and AWD-LSTM in\ncombination with the training tricks to evaluate and compare\nthe performance of the models, (ii) providing new standardized\ndatasets for further investigation, (iii) ablation studies to mea-\nsure the impact and beneﬁts of various training strategies, and\n(iv) leveraging the EuroV oc term hierarchy to generate variants\nof the datasets for which higher classiﬁcation performance can\nbe achieved.\nThe remainder of the paper is organized as follows: After\na discussion of related work in Section II, we introduce\nthe EuroV oc vocabulary and the two datasets (Section III),\nand then present the main methods (AWD-LSTM, BERT,\nRoBERTa, DistilBERT, XLNet) in Section IV. Section V\ncontains extensive evaluations of the methods on both datasets\nas well as ablation studies, and after a discussion of results\n(Section VI) we conclude the paper in Section VII.\narXiv:2010.12871v1  [cs.CL]  24 Oct 2020\nII. R ELATED WORK\nIn connection with the JRC-Acquis dataset, Steinberger\net al. [16] present the “JRC EuroV oc Indexer JEX”, by the\nJoint Research Centre (JRC) of the European Commission.\nThe tool categorizes documents using the EuroV oc taxonomy\nby employing a proﬁle-based ranking task; the authors report\nan F-score between 0.44 and 0.54 depending on the document\nlanguage. Boella et al. [17] manage to apply a support vector\nmachine approach to the problem by transforming the multi-\nlabel classiﬁcation problem into a single-label problem. Liu et\nal. [18] present a new family of Convolutional Neural Network\n(CNN) models tailored for multi-label text classiﬁcation. They\ncompare their method to a large number of existing approaches\non various datasets; for the EurLex/JRC dataset however,\nanother method (SLEEC), provided the best results. SLEEC\n(Sparse Local Embeddings for Extreme Classiﬁcation) [19],\ncreates local distance preserving embeddings which are able\nto accurately predict infrequently occurring (tail) labels. The\nresults on precision for SLEEC applied in Liu et al. [18] are\nP@1: 0.78, P@3: 0.64 and P@5: 0.52 – however, they use a\nprevious version of the JRC-Acquis dataset with only 15.4K\ndocuments.\nChalkidis et al. [6] recently published their work on the\nnew EURLEX57K dataset. The dataset will be described\nin more detail (incl. dataset statistics) in the next sections.\nChalkidis et al. also provide a strong baseline for LMTC on\nthis dataset. Among the tested neural architectures operating on\nthe full documents, they have best results with BIGRUs with\nlabel-wise attention. As input representation they use either\nGloVe [20] embeddings trained on domain text, or ELMO\nembeddings [21]. The authors investigated using only the ﬁrst\nzones of the (long) documents for classiﬁcation, and show that\nthe title and recitals part of each document leads to almost the\nsame performance as considering the full document [6]. This\nhelps to alleviate BERT’s limitation of having a maximum of\n512 tokens as input. Using only the ﬁrst 512 tokens of each\ndocument as input, BERT [10] archives the best performance\noverall. The work of Chalkidis et al. is inspired by You et\nal. [22] who experimented with RNN-based methods with self\nattention on ﬁve LMTC datasets (RCV1, Amazon-13K, Wiki-\n30K, Wiki-500K, and EUR-Lex-4K). Similar work has been\ndone in the medical domain, Mullenbach et al. [2] investigate\nlabel-wise attention in LMTC for medical code prediction (on\nthe MIMIC-II and MIMIC-III datasets).\nIn this work, we experiment with BERT, RoBERTa, Dis-\ntilBERT, XLNet, M-BERT and AWD-LSTM. We provide ab-\nlation studies to measure the impact of various training strate-\ngies and heuristics. Moreover, we provide new standardized\ndatasets for further investigation by the research community,\nand leverage the EuroV oc term hierarchy to generate variants\nof the datasets.\nIII. D ATASETS AND EUROVOC VOCABULARY\nIn this section, we ﬁrst introduce the multilingual EuroV oc\nthesaurus which is used to classify legal documents published\nby the institutions of the European Union. The EuroV oc\nthesaurus is also used as a classiﬁcation schema for the\ndocuments contained in the two legal datasets we use for our\nexperiments, the JRC-Acquis V3 and EURLEX57K datasets\nwhich are described in this section.\n@prefix r d f : <h t t p : / / www. w3 . org / 1 9 9 9 / 0 2 / 2 2 − r d f − s y n t a x − ns\n# t y p e> .\n@prefix s k o s : <h t t p : / / www. w3 . org / 2 0 0 4 / 0 2 / s k o s / c o r e #> .\n@prefix d c t e r m s : <h t t p : / / p u r l . org / dc / t e r m s /> .\n@prefix e v : <h t t p : / / e u r o v o c . e u r o p a . eu /> .\n@prefix e v s : <h t t p : / / e u r o v o c . e u r o p a . eu / schema #> .\n<h t t p : / / e u r o v o c . e u r o p a . eu /100142>\nr d f : t y p e evs:Domain ;\ns k o s : p r e f L a b e l ” 04 POLITICS ”@en .\n<h t t p : / / e u r o v o c . e u r o p a . eu /100166>\nr d f : t y p e e v s : M i c r o T h e s a u r u s ;\ns k o s : p r e f L a b e l ” 0421 p a r l i a m e n t ”@en ;\nd c t e r m s : s u b j e c t ev:100142 ;\ns k o s : h a s T o p C o n c e p t e v : 4 1 .\n<h t t p : / / e u r o v o c . e u r o p a . eu / 4 1>\nr d f : t y p e e v s : T h e s a u r u s C o n c e p t ;\ns k o s : p r e f L a b e l ” powers o f p a r l i a m e n t ”@en ;\ns k o s : i n S c h e m e ev:100166 .\n<h t t p : / / e u r o v o c . e u r o p a . eu / 1 5 9 9>\nr d f : t y p e e v s : T h e s a u r u s C o n c e p t ;\ns k o s : p r e f L a b e l ” l e g i s l a t i v e p e r i o d ”@en ;\ns k o s : i n S c h e m e ev:100166\ns k o s : b r o a d e r e v : 4 1 .\nFigure 1. EuroV oc example\nA. EuroVoc\nThe datasets we use for our experiments contain legal\ndocuments from the legal information system of the European\nUnion (Eur-Lex) and are classiﬁed into a common classi-\nﬁcation schema, the EuroV oc [9] thesaurus published and\nmaintained by the Publications Ofﬁce of the European Union\nsince 1982. The EuroV oc thesaurus has been introduced to\nharmonize the classiﬁcation of documents in the communi-\ncations across EU institutions and to enable a multilingual\nsearch as the thesaurus provides all its terms in the ofﬁcial\nlanguage of the EU member states. It is organized based on\nthe Simple Knowledge Organization System (SKOS) [23] ,\nwhich encodes data using the Resource Description Format\n(RDF) [24] and is well-suited to represent hierarchical relations\nbetween terms in a thesaurus like EuroV oc. EuroV oc uses\nSKOS to hierarchically organize its concepts into 21 domains,\nfor instance Law, Tradeor Politics, to name a few. Each domain\ncontains multiple microthesauri (127 in total), which in turn\nhave in total around 600 top terms. About 7K terms (also called\ndescriptors, concepts or labels) are assigned to one or multiple\nmicrothesauri and connected to top terms using the predicate\nskos:broader.\nAll concepts in EuroV oc have a preferred (skos:\nprefLabel) label and non-preferred (skos:altLabel)\nlabel for each language; the label language is indicated with\nlanguage tags. Figure 1 illustrates with an example serialized\nin Turtle (TTL) [25] format how the terms are organized in the\nEuroV oc thesaurus. Our example is from the domain04 POLI-\nTICS and we show only the English labels of the concepts. The\ndomain 04 POLITICS has the EuroV oc IDev:100142 and is\nof rdf:type evs:Domain. Each domain has microthesauri\nas the next lower level in the hierarchy. In this example,\nwe can see that a evs:Microthesaurus named 0421\nparliament is assigned to the 04 POLITICS domain using\n(dcterms:subject ev:100142) and is also connected\nto the next lower level of top terms. The top term powers\nof parliament (ev:41) is linked to the microthesaurus using\nskos:inScheme. Finally, the lowest level in this example is\nthe concept legislative period (ev:1599) which is linked to its\n(skos:broader) top term powers of parliament (ev:41),\nand is also directly linked to the microthesaurus 0421 parlia-\nment to which it belongs to using skos:inScheme.\nThe legal documents are annotated with multiple EuroV oc\nclasses typically on the lowest level which results in a huge\namount of available classes a document can be potentially\nclassiﬁed in. In addition, this also comes with the disadvantage\nof the power-law distribution of labels such that some labels\nare assigned to many documents whereas others are only\nassigned to a few documents or to no documents at all. The\nadvantages of using a multilingual and multi-domain thesaurus\nfor document classiﬁcation are manifold. Most importantly, it\nallows us to reduce the numbers of potential classes by going\nup the hierarchy, which does not make classiﬁcation incorrect\nbut only more general. Reducing the number of labels allows\nto compare the efﬁciency of the model for different label sets,\nwhich vary in size and sparsity. In this line, we use a class\nreduction method to generate datasets with a reduced number\nof classes by replacing the original labels with the top terms,\nmicrothesauri or domains they belong to. For the top terms\ndataset, we leverage the skos:broader relations of the\noriginal descriptors, for the microthesauri dataset we follow\nskos:inScheme links to the microthesauri, and the domains\ndataset is inferred via the dcterms:subject links of the\nmicrothesauri. This process creates three additional datasets\n(top terms, microthesauri, domains ) [26]. Furthermore, such\na thesaurus would also allow to incorporate potentially more\nﬁne-grained national thesauri of member states which could be\naligned with EuroV oc and therefore enable multilingual search\nin an extended thesarus.\nB. Legal Text Datasets\nIn this work we focus on legal documents collected from\nthe Eur-Lex [7] database serving as the ofﬁcial site for re-\ntrieving European Union law, such as Treaties, International\nagreements and Legislation, and case law of the European\nUnion (EU). Eur-Lex provides the documents in the ofﬁcial\nlanguages of the EU member states. As discussed in previous\nwork [26] the documents are well structured and written\nin domain speciﬁc language. Furthermore, legal documents\nare typically longer compared to texts often taken for text\nclassiﬁcation task such as the Reuters-21578 dataset containing\nnews articles.\nIn this paper, we use the English versions of the two legal\ndatasets JRC-AcquisV3 [27] and EURLEX57K [28]. The JRC-\nAcquis V3 dataset has been compiled by the Joint Research\nCentre (JRC) of the European Union with the Acquis Commu-\nnautaire being the applicable EU law and contains documents\nin XML format. Each JRC document is divided into body,\nsignature, annex and descriptors. The EURLEX57K dataset has\nbeen prepared by academia [6] and is provided in JSON format\nstructured into several parts, namely the header including title\nand legal body, recitals (legal background references), the main\nbody (organized in articles) and the attachments (appendices,\nannexes). Furthermore and in contrast to JRC-Acquis, the\nEURLEX57K dataset is already provided with a split into train\nand test sets.\nTable I shows a comparison of the dataset characteristics.\nEURLEX57K contains almost three times as many documents\nTABLE I. D ATASET STATISTICS FOR JRC-A CQUIS AND EURLEX57K.\nJRC-Acquis EURLEX57K\n#Documents 20382 57000\nMax #Tokens/Doc 469820 3934\nMin #Tokens/Doc 21 119\nMean #Tokens/Doc 2243.43 758.46\nStdDev #Tokens/Doc 7075.94 542.86\nMedian #Tokens/Doc 651.0 544\nMode #Tokens/Doc 275 275\nas the JRC-Acquis V3 dataset, but the documents are compa-\nrable in their minimum number of tokens, median and mode\nof tokens per document. The large difference in the maximum\nnumber of tokens per document impacts the standard deviation\nand the mean number of tokens. The reason for this difference\nis that JRC-Acquis also includes documents dealing with the\nbudget of the European Union, comprised of many tables. As\nboth datasets originate from the same source, but with different\nproviders, we analyzed the number of documents contained in\nboth datasets and found an overlap of approx. 12%.\nTable II provides an overview of label statistics for both\ndatasets. We created different versions based on the original\ndescriptors (DE), top terms (TT), microthesauri (MT) and\ndomains (DO) and present the numbers for all versions. The\nmaximum number of labels assigned to a single document\nis similar for both datasets. The average number of labels\nper document in the original (DE) version is 5.46 (JRC-\nAcquis) and 5.07 (EURLEX57). Due to the polyhierarchy in\nthe geography domain a label may be assigned to multiple Top\nTerms, therefore the number of Top Term labels is higher than\nthat of the original descriptors.\nFigure 2 visualizes the power-law (long tail) label distri-\nbution, where a large portion of EuroV oc descriptors is used\nrarely (or never) as document annotations. In the JRC-Acquis\ndataset only 50% of the labels available in EuroV oc are used\nto classify documents. Only 417 labels are used frequently\n(used on more than 50 documents) and 3,3147 labels have\na frequency between 1–50 (few-short). The numbers for the\nEURLEX57K dataset are similar [6], with 59.31% of all\nEuroV oc labels being actually present in EURLEX57K. From\nthose labels, 746 are frequent, 3,362 have a frequency between\n1–50, and 163 are only in the testing, but not in the training,\ndataset split (zero-shot). The high number of infrequent la-\nbels obviously is a challenge when using supervised learning\napproaches.\nIV. M ETHODS\nIn this section we describe the methods used in the\nLMTC experiments presented in the evaluation section, and the\ngeneral training process. Furthermore, we discuss important\nrelated points such as language model pretraining and ﬁne-\ntuning, and discriminative learning rates, and other important\nfoundations for the evaluation section like dataset splitting and\nmultilingual training.\nA. General Training Strategy and Implementation\nIn accordance with common NLP practice, as ﬁrst intro-\nduced by Howard and Ruder for text classiﬁcation [29], we\nTABLE II. D ATASET STATISTICS – NUMBER OF LABELS PER DOCUMENT .\nJRC-Acquis EURLEX57K\nLabel DE TT MT DO DE TT MT DO\nMax 24 30 14 10 26 30 15 9\nMin 1 1 1 1 1 1 1 1\nMean 5.46 6.04 4.74 3.39 5.07 5.94 4.55 3.24\nStdDev 1.73 3.14 1.92 1.17 1.7 3.06 1.82 1.04\nMedian 6 5 5 3 5 5 4 3\nMode 6 4 4 3 6 4 4 3\nFigure 2. Power-law distribution of descriptors in the JRC-Acquis dataset.\ntrain our models in two steps: ﬁrst we ﬁne-tune the language\nmodeling part of the model to the target corpus (JRC-Acquis or\nEURLEX57K), and then we train the classiﬁer on the training-\nsplit of the dataset.\nThe baseline model (AWD-LSTM) and the transformer\nmodels are available with pretrained weights, trained with lan-\nguage modelling objectives on large corpora such as Wikitext\nor Webtext – a process that is computationally very expensive.\nFine-tuning allows to transfer the language modeling capabil-\nities to a new domain [29].\nOur implementation makes use of the FastAI library [30],\nwhich includes the basic infrastructure to apply training strate-\ngies like gradual unfreezing or slanted triangular learning\nrates (see below). Moreover, for the transformer models, we\nintegrate the Hugging Face transformers package [31] with\nFastAI.\nOur implementation including the evaluation results, is\navailable on GitHub [32]. The repository also includes the\nreference datasets created with iterative splitting, which can\nbe used by other researchers as reference datasets – in order\nto have a fair comparison of different approaches in the future.\nB. Tricks for Performance Improvement (within FastAI)\nIn their Universal Language Model Fine-tuning for Text\nClassiﬁcation (ULMFiT) approach, Howard and Ruder [29]\npropose a number of training strategies and tricks to improve\nmodel performance, which are available within the FastAI\nlibary. Firstly, based on the idea that early layers in a deep\nneural network capture more general and basic features of\nlanguage, which need little domain adaption, discriminative\nﬁne-tuning applies different learning rates depending on the\nlayer; earlier layers use smaller learning rates compared to later\nlayers. Secondly, slanted triangular learning rates quickly\nincrease the learning rate at the beginning of a training epoch\nup to the maximal learning rate in order to ﬁnd a suitable\nregion of the parameter space, and then slowly reduce the\nlearning rate to reﬁne the parameters. And ﬁnally, in gradual\nunfreezing the training process is divided into multiple cycles,\nwhere each cycle consists of several training epochs. Training\nstarts after freezing all layers except for the last few layers\nin cycle one, during later cycles more layers are unfrozen\ngradually (from last to ﬁrst layers). The intuition is that, in ﬁne-\ntuning a deep learning model (similar to discriminative ﬁne-\ntuning), that later layers are more task and domain speciﬁc and\nneed more ﬁne-tuning. In the evaluation section, we provide\ndetails about our unfreezing strategy (Table IV).\nC. Baseline Model\nWe use A WD-LSTM[33] as a baseline model. Merity et\nal. [33] investigate different strategies for regularizing word-\nlevel LSTM language models, including the weight-dropped\nLSTM with its recurrent regularization, and they introduce NT-\nASGD as a new version of average stochastic gradient descent\nin AWD-LSTM.\nIn the ULMFiT approach [29] of FastAI, AWD-LSTM\nis used as encoder, with extra layers added on top for the\nclassiﬁcation task.\nFor any of the models (AWD-LSTM and transformers)\nwe apply the basic method discussed above: a) ﬁne-tune the\nlanguage model on all documents (ignoring the labels) of the\ndataset (JRC-Acquis or EURLEX57K), and then b) ﬁne-tune\nthe classiﬁer using the training-split of the dataset.\nD. Transformer Models\nIn the experiments we study the performance of BERT,\nRoBERTa, DistilBERT and XLNet on the given text classiﬁ-\ncation tasks. BERT is an early, and very popular, transformer\nmodel, RoBERTa is a modiﬁed version of BERT trained on a\nlarger corpus, DistilBERT is a distilled version of BERT and\nthereby with lower computational cost, and ﬁnally, XLNet can\nbe fed with larger input token sequences.\nBERT: BERT [10] is a bidirectional language model which\naims to learn contextual relations between words using the\ntransformer architecture [34]. We use an ofﬁcial release of the\npre-trained models, details about the speciﬁc hyperparameters\nare found in Section V-A.\nThe input to BERT is either a single text (a sentence or\ndocument), or a text pair. The ﬁrst token of each sequence is\nthe special classiﬁcation token [CLS], followed by WordPiece\ntokens of the ﬁrst text A, then a separator token [SEP], and\n(optionally) after that WordPiece tokens for the second text B.\nIn addition to token embeddings, BERT uses positional\nembeddings to represent the position of tokens in the se-\nquence. For training, BERT applies Masked Language Model-\ning (MLM) and Next Sentence Prediction (NSP) objectives. In\nMLM, BERT randomly masks 15% of all WordPiece tokens\nin each sequence and learns to predict these masked tokens.\nFor NSP, BERT is fed in 50% of cases with the actual next\nsentence B, in the other cases with a random sentence B from\nthe corpus.\nRoBERTa: RoBERTa, introduced by Liu et al. [11], re-\ntrains BERT with an improved methodology, much more data,\nlarger batch size and longer training times. In RoBERTa the\ntraining strategy of BERT is modiﬁed by removing the NSP\nobjective. Further, RoBERTa uses byte pair encoding (BPE) as\na tokenization algorithm instead of WordPiece tokenization in\nBERT.\nDistilBERT: We use a distilled version of BERT released\nby Sanh et al. [12]. DistilBERT provides a lighter and faster\nversion of BERT, reducing the size of the model by 40% while\nretaining 97% of its capabilities on language understanding\ntasks [12]. The distillation process includes training a complete\nBERT model (the teacher) using the improved methodology\nproposed by Liu et al. [11], then DistilBERT (the student)\nis trained to reproduce the behaviour of the teacher by using\ncosine embedding loss.\nXLNet: The previously discussed transformer-based mod-\nels are limited to a ﬁxed context length (such as 512 tokens),\nwhile legal documents are often long and exceed this context\nlength limit. XLNet [13] includes segments recurrence, intro-\nduced in Transformer-XL [35], allowing it to digest longer\ndocuments. XLNet follows RoBERTa in removing the NSP\nobjective, while introducing a novel permutation language\nmodel objective. In our work with XLNet, we ﬁne-tune the\nclassiﬁer directly without LM ﬁne-tuning (as LM ﬁne-tuning\nof XLNet was computationally not possible on the hardware\navailable for our experiments).\nE. Dataset Splitting\nStratiﬁcation of classiﬁcation data aims at splitting the data\nin a way that in all dataset splits (training, validation, test) the\ntarget classes appear in similar proportions. In multi-label text\nclassiﬁcation stratiﬁcation becomes harder, because the target\nis a combination of multiple labels. In random splitting, it is\npossible that most instances of a speciﬁc class end up either\nin the training or test split (esp. for low frequency classes),\nand therefore the split can be unrepresentative with respect to\nthe original data set. Moreover, random splitting and different\ntrain/validation/test ratios create the problem that results from\ndifferent approaches are hard to compare [15].\nDepending on the dataset, other criteria can be used for\ndataset splitting, for example Azarbonyad et al. [36] split JRC-\nAcquis documents according to document’s year, where older\ndocuments could be used in training, and newer in testing.\nFor splitting both JRC-Acquis and EURLEX57K, we use\nthe iterative stratiﬁcation algorithm proposed by Sechidis et\nal. [14], ie. its implementation provided by the scikit-multilearn\nlibrary [37]. Applying this algorithm leads to a better document\nsplit with respect to the target labels, and in turn, helps with\ngeneralization of the results and allows for a fair comparison\nof different approaches. The reference splits of the dataset are\navailable online [32].\nIn the experiments in Section V we use these dataset splits,\nbut in addition for EURLEX57K also the dataset split of the\ndataset creators [6], in order to compare to their evaluation\nresults.\nF . Multilingual Training\nJRC-Acquis is a collection of parallel texts in 22 languages\n– we make use of this property to train multilingual BERT\n[38] on an extended version of JRC-Acquis in 3 languages.\nMultilingual BERT provides support for 104 languages and\nit is useful for zero-shot learning tasks in which a model is\ntrained using data from one language and then used to make\ninference on data in other languages.\nWe extend the English JRC-Acquis dataset with parallel\ndata in German and French. The additional data has the\nsame dataset split as in the English version, ie. if an English\ndocument is in the training set then the German and French\nversions will be in the same split as well.\nV. E VALUATION\nThis section ﬁrst discusses evaluation setup (for example\nmodel hyperparameters) and then evaluation results for JRC-\nAcquis and EURLEX57K.\nA. Evaluation Setup\nEvaluation setup includes important aspects such as dataset\nsplits, preprocessing, the speciﬁc model architectures and\nvariants, and major hyperparameters used in training.\na) Dataset Splits:: The ofﬁcial JRC-Acquis dataset\ndoes not include a standard train-validation-test split, and as\ndiscussed in Section IV-E a random split exhibits unfavorable\ncharacteristics. We apply iterative splitting [14] to ensure that\neach split has the same label distribution as the original\ndata. We split with an 80%/10%/10% ratio for training/valida-\ntion/test sets. For the EURLEX57K the dataset creators already\nprovide a split and a strong baseline evaluation. We run our\nmodels on the given split in order to compare results, and also\ncreate our own split with iterative splitting (dataset available\nin the mentioned GitHub repository [32]).\nb) Text Preprocessing:: All described models have their\nown preprocessing included (e.g. WordPiece tokenization in\nBERT), we do not apply extra preprocessing to the text.\nc) Neural Network Architectures:: For A WD-LSTM,\nwe use the standard setup of the pretrained model included in\nFastAI, which has an input embedding layer with embedding\nsize of 400, followed by three LSTM layers with hidden sizes\nof 1152 and weight dropout probability of 0.1.\nTABLE III. A RCHITECTURE HYPERPARAMETERS OF TRANSFORMER\nMODELS\nModel\nName\n# Layers\n# Heads\nContext\nLength\nIs Cased\nbatch-\nsize\nBERT 12 12 512 False 4\nRoberta 12 12 512 False 4\nDistilBERT 6 12 512 False 4\nXLNet 12 12 1024 True 2\nFor the transformer models, we start from pretrained mod-\nels, the uncased BERT model [39], the RoBERTa model [40],\nDistilBERT [41], and the XLNET model [42].\nIn Table III, we see that many architectural details are\nsimilar for the different model types. The transformer models\nall have 12 network layers, except DistilBERT with 6 layers,\nand 12 attention heads. XLNet allows for longer input contexts,\nbut for performance reasons we limited the context to 1024\ntokens, and it was necessary to reduce the batch size to 2 to ﬁt\nthe model into GPU memory, and also we could not unfreeze\nthe whole pretrained model (see below).\nTo create the text classiﬁers, we take the representation of\nthe text generated by the transformer model or AWD-LSTM,\nand add two fully connected layers of size 1200 and 50,\nrespectively, with a dropout probability of 0.2, and an output\nlayer. We apply batch normalization on the fully connected\nlayers.\nd) Gradual Unfreezing:: Gradual unfreezing is one of\nthe ULMFiT strategies discussed in Section IV-B, where the\nneural network layers are grouped, and trained starting with the\nlast group, then incrementally unfrozen and trained further.\nTABLE IV. G RADUAL UNFREEZING DETAILS : LEARNING RATES (LR),\nNUMBER OF EPOCHS (ITERS ), AND LAYER GROUPS THAT ARE UNFROZEN .\n# Unfrozen Layers\nCycle Max LR # Iters\nBERT\nRoBERTa\nDistilBERT\nXLNet\n1 2e-4 12 4 2 4\n2 5e-5 12 8 4 6\n3 5e-5 12 12 6 8\n4 5e-5 36 12 6 8\n5 5e-5 36 12 6 8\nExcept for DistilBERT, which has only 2 layers per layer\ngroup, all transformer models have 3 groups of 4 layers used\nin the unfreezing process. Table IV gives an overview of\nthe training setup for the transformer models. We trained the\nclassiﬁer for 5 cycles, starting in cycle 1 with 4 layers and\na LR = 2e − 4, and 12 training epochs (Iters). The setup of\nthe other cycles is shown in the table. Overall, we used the\nsame setup for all transformer models with a goal of better\ncomparison between models. (Remark: hand-picking LRs and\ntraining epochs might lead to slightly better results.)\nTable V shows the main hyperparameters of AWD-LSTM\ntraining, we trained the model in 6 cycles, with LRs, epochs\nTABLE V. G RADUAL UNFREEZING SETTINGS FOR AWD-LSTM\nCycle # Max LR # Unfrozen Layers # Iterations\n1 2e-1 1 2\n2 1e-2 2 5\n3 1e-3 3 5\n4 5e-3 all 20\n5 1e-4 all 32\n6 1e-4 all 32\nper cycle, and unfrozen layers as shown in the table.\ne) LM Fine-tuning:: For the transformer models we do\nLM ﬁne-tuning for 5 iterations, with a batch size of 4 and\nLR of 5e − 5. Transformer ﬁne-tuning is done with a script 1\nprovided by Hugging Face. For the AWD-LSTM model we\nﬁrst ﬁne-tune the frozen LM for 2 epochs, and then in cycle\ntwo ﬁne-tune the unfrozen model for another 5 epochs.\nf) Hardware speciﬁcations: We trained the models on a\nsingle GPU device (NVIDIA GeForce GTX 1080 with 11 GB\nof GDDR5X memory). For inference, we use an Intel i7-\n8700K CPU @ 3.70GHz and 16GB RAM.\nB. Evaluation Metrics\nIn the evaluations, in line with Chalkidis et al. [6], we\napply the following evaluation metrics: micro-averaged F1,\nR-Precision@K (RP@K), and Normalized Discounted Cumu-\nlative Gain (nDCG@K) . Precision@K (P@K) and Recall@K\n(R@K) are popular measures in LTMC, too, but they unfairly\npenalize in situations where the number of gold labels is\nunequal to K, which is the typical situation in our datasets.\nThis problem led to the introduction of more suitable metrics\nlike RP@K and nDCG@K. In the following, we brieﬂy discuss\nthe metrics.\nThe F1-score is a common metric in information retrieval\nsystems, and it is calculated as the harmonic mean between\nprecision and recall. If we have a label L, Precision, Recall,\nand F1-score with respect to L are calculated as follows:\nPrecision L = TruePositives L\nTruePositives L+FalsePositives L\nRecallL = TruePositives L\nTruePositives L+FalseNegatives L\nF1L = 2∗ Precision ∗Recall\nPrecision +Recall\nMicro-F1 is an extension of the F1-score for multi-label\nclassiﬁcation tasks, and it treats the entire set of predictions\nas one vector and then calculates the F1. We use grid search\nto pick the threshold on the output probabilities of the models\nthat gives the best Micro-F1 score on the validation set. The\nthreshold determines which labels we assign to the documents.\nPropensity scores prioritize predicting a few relevant labels\nover the large number of irrelevant ones [43]. R-Precision@K\n(RP@K) calculates precision for the top K ranked labels, if\nthe number of ground truth labels for a document is less than\nK, K is set to this number for this document.\nRP@K = 1\nN\n∑N\nn=1\n∑K\nk=1\nRel(n,k)\nmin(K,Rn)\nWhere N is the number of documents, Rel(n, k) is set\nto 1 if the k-th retrieved label in the top-K labels of the n-th\n1https://github.com/huggingface/transformers/blob/master\n/examples/language-modeling/run language modeling.py\ndocument is correct, otherwise it is set to 0 . Rn is the number\nof ground truth labels for the n-th document.\nNormalized Discounted Cumulative Gain nDCG@k for the\nlist of top K ranked labels measures ranking quality. It is based\non the assumption that highly relevant documents are more\nuseful than moderately relevant documents.\nnDCG@K = 1\nN\n∑N\nn=1 Zkn\n∑K\nk=1\n2Rel(n,k)−1\nlog2(1+k)\nN is the number of documents, Rel(n, k) is set to 1 if the\nk-th retrieved label in the top-K labels of the n-th document is\ncorrect, otherwise it is set to 0. Zkn is a normalization factor\nto ensure nDCG@K = 1 for a perfect ranking.\nC. Evaluation Results\nThe evaluation results are organized into three subsec-\ntions, results for the JRC-Acquis dataset, results for the EU-\nRLEX57K dataset, and ﬁnally results from ablation studies.\n1) JRC-Acquis: Table VI presents an overview of the\nresults on the JRC-Acquis dataset for the transformer models\nand the AWD-LSTM baseline, and initial results from the\nmultilingual model.\nThe observations here are as follows: Firstly, transformer-\nbased models outperform the LSTM baseline by a large\nmargin. Further, within the transformer models RoBERTa and\nBERT yield best results, the scores are almost the same. As\nexpected, the distilled version of BERT is a bit lower in most\nmetrics like Micro-F1, but the difference is small.\nIn this set of experiments, XLNet is behind DistilBERT,\nwhich we attribute to two main causes: (i) for computational\nreasons (given the available GPU hardware), we couldnot ﬁne-\ntune the LM on XLNet, and in classiﬁer training we could\nnot unfreeze the full model. (ii) We used the same LR on all\nmodels; the choice of LR was inﬂuenced by a recommendation\non BERT learning rates in Devlin et al. [10], and may not be\noptimal for XLNet. Overall, we could not properly test XLNet\ndue to its high computational requirements, and did therefore\nnot include it in the set of experiments on the EURLEX57K\ndataset.\nThe initial set of experiments with multilingual BERT (M-\nBERT) provides very promising results, on par with RoBERT\nand BERT. This is remarkable given the fact that we use the\nsame amount of global training steps – which means, because\nour multilingual dataset is 3 times larger, that on individual\ndocuments we train only a 1/3 of the time. We expect even\nbetter results with more training epochs. LM ﬁne-tuning of the\nM-BERT model was done on the text from all three languages\n(en, de, fr).\nRegarding comparisons to existing baseline results, ﬁrstly\nbecause of the problem of different dataset splits (see Sec-\ntion IV-E) results are hard to compare. However, Steinberger\net al. [16] report an F1-score of 0.48, Esuli et al. [44] report\nan F1 of 0.589 and Chang et al. [15] do not provide F1, but\nonly P@5 (62.64) and R@5 (61.59).\nFor Table VII, we picked one transformer-based method,\nnamely BERT, and analyzed its performance on the various\nJRC datasets resulting from class reduction described in Sec-\ntion III-A. By using inference on the EuroV oc hierarchy, we\ncreated, additionally to the default descriptors dataset, datasets\nfor EuroV oc Top Terms (TT), Micro-Thesauri (MT), and\nEuroV oc Domains (DO). With the reduced number of classes,\nclassiﬁcation performance is clearly rising, for example from a\nMicro-F1 of 0.661 (descriptors) to 0.839 (EuroV oc domains).\nWe argue that the results with the inferred labels show that our\napproach might be well-suitable for real-world applications\nin scenarios like automatic legal document classiﬁcation or\nkeyword/label suggestion – for example the RP@5 for domains\n(DO) is at 0.928, so the classiﬁcation performance (depending\non the use case requirements) may be sufﬁcient.\nFigure 3. A visualization of RP@K and nDCG@K for all transformer\nmodels for JRC-Acquis.\nFigure 3 contains a visual representation of RP@K and\nnDCG@K for the transformer models applied to the JRC-\nAcquis dataset. We can see how similar the performance\nof BERT and RoBERTa is for different values of K, and\nRoBERTa scores are consistently marginally better.\n2) EURLEX57K: In this subsection we report the evalu-\nation results on the new EURLEX57K dataset by Chalkidis\net al. [6]. In order to compare to the results of the dataset\ncreators, we ran the experiments on the dataset and dataset split\n(45K training, 6K validation, 6K testing) provided by Chalkidis\net al. [6]. Below, we also show evaluation results on our\ndataset split (created with the iterative stratiﬁcation approach).\nTable VIII gives an overview of results for our transformer\nmodels, and compares them to the strong baselines in existing\nwork. Chalkidis et al. [6] evaluate various architectures, the\nresults of the three best models presented here: BERT-BASE,\nBIGRU-LW AN-ELMO and BIGRU-LW AN-L2V . BERT-BASE\nis a BERT model with an extra classiﬁcation layer on top,\nBIGRU-LW AN combines a BIGRU encoder with Label-Wise\nAttention Networks (LW AN), and uses either Elmo (ELMO)\nor word2vec (L2V) embeddings as inputs. Table VIII shows\nthat our models outperform the previous baseline, the best\nresults are delivered by RoBERTa and DistilBERT. The good\nperformance of DistilBERT in these experiments is surprising\n(We need further future experiments to explain the results\nsufﬁciently. One intuition might be that the random weight\ninitialization of the added layers was very suitable.).\nOverall, the results are much better than for the smaller\nTABLE VI. C OMPARISON BETWEEN DIFFERENT TRANSFORMER MODELS , FINE -TUNED USING THE SAME NUMBER OF ITERATIONS ON JRC-A CQUIS .\nBERT RoBERTa XLNet DistilBERT AWD-LSTM Multilingual BERT\nMicro-F1 0.661 0.659 0.605 0.652 0.493 0.663\nRP@1 0.867 0.873 0.845 0.884 0.762 0.873\nRP@3 0.784 0.788 0.736 0.78 0.619 0.783\nRP@5 0.715 0.716 0.661 0.711 0.548 0.717\nRP@10 0.775 0.778 0.733 0.775 0.627 0.777\nnDCG@1 0.867 0.873 0.845 0.884 0.762 0.873\nnDCG@3 0.803 0.807 0.762 0.805 0.651 0.804\nnDCG@5 0.750 0.753 0.703 0.75 0.594 0.752\nnDCG@10 0.778 0.781 0.746 0.779 0.630 0.780\nTABLE VII. BERT RESULTS FOR JRC-A CQUIS WITH class reduction\nMETHODS APPLIED , WHICH LEAD TO 4 DATASETS : DE ( DESCRIPTORS ), TT\n(TOP -TERMS ), MT ( MICROTHESAURI , DO ( DOMAINS )\nDE TT MT DO\nMicro-F1 0.661 0.745 0.778 0.839\nRP@1 0.867 0.922 0.943 0.967\nRP@3 0.784 0.838 0.871 0.905\nRP@5 0.715 0.804 0.844 0.928\nRP@10 0.775 0.857 0.908 0.974\nnDCG@1 0.867 0.922 0.943 0.967\nnDCG@3 0.803 0.858 0.888 0.919\nnDCG@5 0.750 0.829 0.864 0.929\nnDCG@10 0.778 0.852 0.896 0.952\nJRC dataset, with the best Micro-F1 for JRC being 0.661\n(BERT), while for EURLEX57K we reach 0.758 (RoBERTa).\nTable IX presents the results for BERT on the additional\ndatasets with Top Terms (TT), Micro-Thesauri (MT) and\nDomains (DO) labels inferred from the EuroV oc taxonomy\n(similar to Table VII, which presents the scores of JRC-\nAcquis). As expected from the general results on the EU-\nRLEX57 dataset, the values on the derived datasets are better\nthan for JRC-Acquis, for example RP@5 is now at 0.956 for\nthe domains (DO).\nFIGURE 4. RP@K AND N DCG@K FOR THE TRANSFORMER MODELS\nTRAINED ON EURLEX57K.\nSimilar to Figure 3, Figure 4 shows RP@K and nDCG@K\nfor BERT, RoBERTa and DistilBERT depending on the value\nof K. RoBERTa and DistilBERT are almost identical in\ntheir performance, BERT lags behind a little in this set of\nexperiments.\nFinally, in Table X, we trained a BERT model on our\niterative split of the EURLEX57K dataset in order to provide a\nstrong baseline for future work on a standardized and arguably\nimproved version of the EURLEX57K dataset.\n3) Ablation Studies: In this section, we want to study the\ncontributions of various training process components – by\nexcluding some of those components individually (or reducing\nthe number of training epochs). We focus on three important\naspects: (i) the use of Language Model (LM) ﬁne-tuning, (ii)\ngradual unfreezing, (iii) and a reduction of the number of\ntraining cycles.\nIn Table XI, we compare the evaluation metrics when\nremoving the LM ﬁne-tuning (on the legal target corpus) step\nbefore classiﬁcation model training to the original version\nincluding LM ﬁne-tuning (in parenthesis). For all examined\nmodels, we can see a small but consistent improvement of\nresults when using LM ﬁne-tuning. The relative improvement\nin the metrics is in the range of 1%–3%. In conclusion, LM\nﬁne-tuning to the legal text corpus is a crucial step for reaching\na high classiﬁcation performance.\nIn Table XII, we examine the effect of two factors, the\ntraining epochs (Iter.) hyperparameter, and of the use of the\ngradual unfreezing technique. Regarding number of epochs,\nboth models beneﬁt from longer training, for BERT the\ndifference is large (about 4% relative improvement in F1-\nscore), while for the simpler DistilBERT model less training\nappears to be required, after 36 epochs it even provides better\naccuracy than BERT at this point, and ﬁnally only gains a 1.2%\nimprovement from more training epochs. Secondly, we study\nthe effect of Gradual Unfreezing (GU), which for BERT has a\nlarge impact, with a relative improvement in F1 of about 6%.\nIn summary, longer training times beneﬁt esp. more complex\nmodels like BERT, and gradual unfreezing is a very helpful\nstrategy for optimizing performance.\nVI. D ISCUSSION\nMuch of the detailed discussion is already included in\nthe Evaluation Results section (Section V-C), so here we will\nsummarize and extend on some of the key ﬁndings.\nIn comparing model performance, starting with LSTM\nversus transformer architectures, the results show that the at-\ntention mechanism used in transformers is superior to LSTMs\nin ﬁnding aspects relevant for the classiﬁcation task in long\ndocuments. Within the transformer models, ﬁrstly we did not\nTABLE VIII. R ESULTS FOR OUR TRANSFORMER -BASED MODELS ON EURLEX57K, AND STRONG BASELINES FROM CHALKIDIS ET AL .\nOurs Chalkidis et al. [6]\nBERT RoBERTa DistilBERT BERT-BASE BIGRU-LW AN-ELMO BIGRU-LW AN-L2V\nMicro-F1 0.751 0.758 0.754 0.732 0.719 0.709\nRP@1 0.912 0.919 0.925 0.922 0.921 0.915\nRP@3 0.843 0.85 0.848 - - -\nRP@5 0.805 0.812 0.807 0.796 0.781 0.770\nRP@10 0.852 0.860 0.862 0.856 0.845 0.836\nnDCG@1 0.912 0.919 0.925 0.922 0.921 0.915\nnDCG@3 0.859 0.866 0.866 - - -\nnDCG@5 0.828 0.835 0.833 0.823 0.811 0.801\nnDCG@10 0.849 0.857 0.858 0.851 0.841 0.832\nTABLE IX. BERT RESULTS ON EURLEX57K WITH class reduction\nMETHODS APPLIED , PLUS THE BASELINE RESULTS OF BERT-BASE (DE)\nFROM CHALKIDIS ET AL . [6].\nDE TT MT DO\nDE\nbaseline\nMicro-F1 0.751 0.825 0.84 0.883 0.732\nRP@1 0.912 0.948 0.959 0.978 0.922\nRP@3 0.843 0.896 0.915 0.939 -\nRP@5 0.805 0.876 0.902 0.956 0.796\nRP@10 0.852 0.909 0.943 0.986 0.856\nnDCG@1 0.912 0.948 0.959 0.978 0.922\nnDCG@3 0.859 0.907 0.924 0.947 -\nnDCG@5 0.828 0.891 0.912 0.955 0.823\nnDCG@10 0.849 0.904 0.931 0.97 0.851\nTABLE X. BERT RESULTS ON EURLEX57K WITH THE NEW ITERATIVE\nSTRATIFICATION DATASET SPLIT .\nMicro-F1 RP@1 RP@5 nDCG@1 nDCG@5\n0.760 0.914 0.809 0.914 0.833\nTABLE XI. C LASSIFICATION METRICS FOR THE JRC-A CQUIS DATASET ,\nWHEN not USING LM FINE -TUNING – IN PARENTHESES THE RESULTS with\nFINE -TUNING (FOR COMPARISON ).\nBERT RoBERTa DistilBERT\nMicro-F1 0.64 (0.66) 0.65 (0.66) 0.61 (0.62)\nRP@1 0.86 (0.87) 0.87 (0.87) 0.86 (0.87)\nRP@3 0.77 (0.78) 0.77 (0.79) 0.75 (0.76)\nRP@5 0.70 (0.72) 0.70 (0.72) 0.67 (0.68)\nRP@10 0.76 (0.78) 0.77 (0.78) 0.74 (0.75)\nnDCG@1 0.86 (0.87) 0.87 (0.87) 0.86 (0.87)\nnDCG@3 0.79 (0.80) 0.79 (0.81) 0.77 (0.78)\nnDCG@5 0.74 (0.75) 0.74 (0.75) 0.71 (0.72)\nnDCG@10 0.77 (0.72) 0.77 (0.78) 0.75 (0.76)\nTABLE XII. A BLATION STUDY : BERT AND DISTIL BERT PERFORMANCE\nON JRC-A CQUIS REGARDING THE NUMBER OF TRAINING EPOCHS (ITER .)\nAND THE USE OF GRADUAL UNFREEZING (GU).\n# Iter. Use GU Prec. Rec. Mic.-F1\n36 True 0.678 0.601 0.637\n108 False 0.674 0.575 0.621\nBERT 108 True 0.695 0.630 0.661\n36 True 0.696 0.601 0.645\n108 False 0.663 0.583 0.620\nDistil-\nBERT108 True 0.701 0.611 0.653\nnotice much difference between BERT and RoBERTa, which\nis not unexpected, as they are technically very similar. Overall,\nresults were a bit better for RoBERTa. DistilBERT delivered\nsurprisingly good results for the EURLEX57K dataset, and\nhas the beneﬁts of lower computational cost. Both for the\nJRC-Aquis and the EURLEX57K datasets, the results indicate\nthat DistilBERT is better in retrieving the most probable label\ncompared with RoBERTa and BERT. XLNet on the other hand,\nrequires a lot of computational resources, and we were not able\nto properly train the model for that reason. Finally, the ﬁrst set\nof experiments on multilingual training with M-BERT gave\npromising results, hence it will be further studied in future\nwork.\nThe ablation studies showed the positive effects of the\ntraining (ﬁne-tuning) strategies that we applied, both LM-\nﬁnetuning on the target domain, as well as gradual unfreezing\nof the network layers (in groups) proved to be crucial in\nreaching state-of-the-art classiﬁcation performance.\nTo compare the computational costs, we calculated infer-\nence times for each model on an Intel i7-8700K CPU @\n3.70GHz. DistilBERT provides the lowest run time at 12\nms/example. RoBERTa and BERT (which have an identical\narchitecture) have very similar run times with 17.1 ms, and\n17.3 ms/example, respectively. XLNet, the heaviest model,\nrequires 77 ms/example.\nFor a fair comparison, we trained all transformer models\nwith the same set of hyperparameters (such as learning rate\nand number of training epochs). With customized and hand-\npicked parameters for each training cycle we expect further\nimprovements of scores, which will be studied in future\nwork together with model ensemble approaches and text data\naugmentation.\nVII. C ONCLUSIONS\nNatural Language Processing ( In) this work we evaluate\ncurrent transformer models for natural language processing\nin combination with training strategies like language model\n(LM) ﬁne-tuning, slanted triangular learning rates and grad-\nual unfreezing in the ﬁeld of LMTC (large multi-label text\nclassiﬁcation) on legal text datasets with long-tail label dis-\ntributions. The datasets contain around 20K documents (JRC-\nAcquis) and 57K documents (EUROLEX57K) and are labeled\nwith EuroV oc descriptors from the 7K terms in the EuroV oc\ntaxonomy. The use of an iterative stratiﬁcation algorithm\nfor dataset splitting (into training/validation/testing) allows\nto create standardized splits on the two datasets to enable\ncomparison and reproducibility in future experiments. In the\nexperiments, we provide new state-of-the-art results on both\ndatasets, with a micro-F1 of 0.661 for JRC-Acquis and 0.754\nfor EUROLEX57K, and even higher scores for new datasets\nwith reduced label sets inferred from the EuroV oc hierarchy\n(top terms, microthesauri, and domains ).\nThe main contributions are: (i) new state-of-the-art LMTC\nclassiﬁcation results on both datasets for a problem type that is\nstill largely unexplored [3], (ii) a comparison and interpretation\nof the performance of the applied models: AWD-LSTM,\nBERT, RoBERTa, DistilBERT and XLNet, (iii) the creation\nand provision (on GitHub) of new standardized versions of the\ntwo legal text datasets created with an iterative stratiﬁcation\nalgorithm, (iv) deriving new datasets with reduced label sets\nvia the semantic structure within EuroV oc, and (v) ablation\nstudies that quantify the contributions of individual training\nstrategies and hyperparameters such as gradual unfreezing,\nnumber of training epochs and LM ﬁne-tuning in this complex\nLMTC setting.\nThere are multiple angles for future work , including po-\ntentially deriving higher performance by using hand-picked\nlearning rates and other hyperparameters for each model\nindividually, and further experiments on using models such\nas multilingual BERT to proﬁt from the availability of parallel\ncorpora. Moreover, experiments with new architectures such as\nGraph Neural Networks [45] and various data augmentation\ntechniques are candidates to improve classiﬁcation perfor-\nmance.\nACKNOWLEDGEMENTS\nThis work was supported by the Government of the Russian\nFederation (Grant 074-U01) through the ITMO Fellowship and\nProfessorship Program.\nREFERENCES\n[1] F. Sebastiani, “Machine learning in automated text categorization,”\nACM computing surveys (CSUR), vol. 34, no. 1, 2002, pp. 1–47.\n[2] J. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, and J. Eisenstein,\n“Explainable prediction of medical codes from clinical text,” arXiv\npreprint arXiv:1802.05695, 2018.\n[3] A. Rios and R. Kavuluru, “Few-shot and zero-shot multi-label learning\nfor structured label spaces,” in Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing. Conference on\nEmpirical Methods in Natural Language Processing, vol. 2018. NIH\nPublic Access, 2018, p. 3132.\n[4] P. Ioannis et al., “Lshtc: A benchmark for large-scale text classiﬁcation,”\narXiv preprint arXiv:1503.08581, 2015.\n[5] E. Loza Menc ´ıa and J. F ¨urnkranz, Efﬁcient Multilabel Classiﬁcation\nAlgorithms for Large-Scale Problems in the Legal Domain. Berlin,\nHeidelberg: Springer, 2010, pp. 192–215, retrieved: 09, 2020. [Online].\nAvailable: https://doi.org/10.1007/978-3-642-12837-0 11\n[6] I. Chalkidis, E. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos,\n“Large-scale multi-label text classiﬁcation on EU legislation,” in Proc\n57th Annual Meeting of the ACL. Florence, Italy: Association for\nComputational Linguistics, Jul. 2019, pp. 6314–6322, retrieved: 09,\n2020. [Online]. Available: https://www.aclweb.org/anthology/P19-1636\n[7] Eurepean Union Law Website. Retrieved: 09,2020. [Online]. Available:\nhttps://eur-lex.europa.eu\n[8] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf, “Transfer learning\nin natural language processing,” in 2019 NAACL: Tutorials, 2019, pp.\n15–18.\n[9] The European Union’s multilingual and multidisciplinary thesaurus.\nRetrieved: 09,2020. [Online]. Available: https://eur-lex.europa.eu/\nbrowse/eurovoc.html\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Proc. 2019 NAACL: Human Language Technologies, V olume 1\n(Long and Short Papers). Minneapolis, Minnesota: ACL, Jun.\n2019, pp. 4171–4186, retrieved: 09, 2020. [Online]. Available:\nhttps://www.aclweb.org/anthology/N19-1423\n[11] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[12] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter,” arXiv preprint\narXiv:1910.01108, 2019.\n[13] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\nunderstanding,” in Advances in neural information processing systems,\n2019, pp. 5754–5764.\n[14] K. Sechidis, G. Tsoumakas, and I. Vlahavas, “On the stratiﬁcation of\nmulti-label data,” in Joint European Conference on Machine Learning\nand Knowledge Discovery in Databases. Springer, 2011, pp. 145–158.\n[15] W.-C. Chang, H.-F. Yu, K. Zhong, Y . Yang, and I. Dhillon, “X-\nbert: extreme multi-label text classiﬁcation using bidirectional encoder\nrepresentations from transformers,” arXiv preprint arXiv:1905.02331,\n2019.\n[16] R. Steinberger, M. Ebrahim, and M. Turchi, “Jrc eurovoc indexer\njex-a freely available multi-label categorisation tool,” arXiv preprint\narXiv:1309.5223, 2013.\n[17] G. Boella et al., “Linking legal open data: breaking the accessibility and\nlanguage barrier in european legislation and case law,” in Proceedings\nof the 15th International Conference on Artiﬁcial Intelligence and Law,\n2015, pp. 171–175.\n[18] J. Liu, W.-C. Chang, Y . Wu, and Y . Yang, “Deep learning for extreme\nmulti-label text classiﬁcation,” in Proceedings of the 40th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval, 2017, pp. 115–124.\n[19] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain, “Sparse local\nembeddings for extreme multi-label classiﬁcation,” in Advances in\nneural information processing systems, 2015, pp. 730–738.\n[20] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Empirical Methods in Natural Language\nProcessing (EMNLP), 2014, pp. 1532–1543. [Online]. Available:\nhttp://www.aclweb.org/anthology/D14-1162\n[21] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, “Deep contextualized word representations,” in\nProc. 2018 NAACL: Human Language Technologies, V olume 1 (Long\nPapers). New Orleans, Louisiana: Association for Computational\nLinguistics, Jun. 2018, pp. 2227–2237, retrieved: 09, 2020. [Online].\nAvailable: https://www.aclweb.org/anthology/N18-1202\n[22] R. You, S. Dai, Z. Zhang, H. Mamitsuka, and S. Zhu, “Attentionxml:\nExtreme multi-label text classiﬁcation with multi-label attention based\nrecurrent neural networks,” arXiv preprint arXiv:1811.01727, 2018.\n[23] SKOS Simple Knowledge Organization System. Retrieved: 09,2020.\n[Online]. Available: https://www.w3.org/2004/02/skos/\n[24] Resource Description Framework. Retrieved: 09,2020. [Online].\nAvailable: https://eur-lex.europa.eu/browse/eurovoc.html\n[25] RDF 1.1 Turtle. Retrieved: 09,2020. [Online]. Available: https:\n//www.w3.org/TR/turtle\n[26] E. Filtz, S. Kirrane, A. Polleres, and G. Wohlgenannt, “Exploiting eu-\nrovoc’s hierarchical structure for classifying legal documents,” in OTM\nConfederated International Conferences” On the Move to Meaningful\nInternet Systems”. Springer, 2019, pp. 164–181.\n[27] JRC-Acquis. Retrieved: 09,2020. [Online]. Available: https://ec.europa.\neu/jrc/en/language-technologies/jrc-acquis\n[28] EURLEX57K dataset. Retrieved: 09,2020. [Online]. Available: http:\n//nlp.cs.aueb.gr/software and datasets/EURLEX57K/\n[29] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for text\nclassiﬁcation,” arXiv preprint arXiv:1801.06146, 2018.\n[30] Fastai documentation. Retrieved: 09,2020. [Online]. Available: https:\n//docs.fast.ai/\n[31] Huggingface transformers. Retrieved: 09,2020. [Online]. Available:\nhttps://huggingface.co/transformers\n[32] Legal Documents, Large Multi-Label Text Classiﬁcation.\nRetrieved: 09,2020. [Online]. Available: https://github.com/zeinsh/\nLegal-Docs-Large-MLTC\n[33] S. Merity, N. S. Keskar, and R. Socher, “Regularizing and optimizing\nlstm language models,” arXiv preprint arXiv:1708.02182, 2017.\n[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems, 2017, pp. 5998–6008.\n[35] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n“Transformer-xl: Attentive language models beyond a ﬁxed-length\ncontext,” arXiv preprint arXiv:1901.02860, 2019.\n[36] H. Azarbonyad and M. Marx, “How many labels? determining the\nnumber of labels in multi-label text classiﬁcation,” in International\nConference of the Cross-Language Evaluation Forum for European\nLanguages. Springer, 2019, pp. 156–163.\n[37] Multi-label data stratiﬁcation. Retrieved: 09,2020. [Online]. Available:\nhttp://scikit.ml/stratiﬁcation.html#Multi-label-data-stratiﬁcation\n[38] BERT, Multi-Lingual Model. Retrieved: 09,2020. [Online]. Available:\nhttps://github.com/google-research/bert/blob/master/multilingual.md\n[39] Huggingface BERT base uncased model. Retrieved: 09,2020. [Online].\nAvailable: https://huggingface.co/bert-base-uncased\n[40] Huggingface RoBERTa base model. Retrieved: 09,2020. [Online].\nAvailable: https://huggingface.co/roberta-base\n[41] Huggingface DistilBERT cased model. Retrieved: 09,2020. [Online].\nAvailable: https://huggingface.co/distilbert-base-uncased\n[42] Huggingface XLNET cased model. Retrieved: 09,2020. [Online].\nAvailable: https://huggingface.co/xlnet-base-cased\n[43] H. Jain, Y . Prabhu, and M. Varma, “Extreme multi-label loss func-\ntions for recommendation, tagging, ranking & other missing label\napplications,” in Proceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 2016, pp. 935–\n944.\n[44] A. Esuli, A. Moreo, and F. Sebastiani, “Funnelling: A new ensemble\nmethod for heterogeneous transfer learning and its application to cross-\nlingual text classiﬁcation,” ACM Transactions on Information Systems\n(TOIS), vol. 37, no. 3, 2019, pp. 1–30.\n[45] A. Pal, M. Selvakumar, and M. Sankarasubbu, “Magnet: Multi-label text\nclassiﬁcation using attention-based graph neural network,” in Proc. 12th\nInt. Conf. on Agents and Artiﬁcial Intelligence - V olume 2: ICAART,\nINSTICC. SciTePress, 2020, pp. 494–505.",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7899482846260071
    },
    {
      "name": "Computer science",
      "score": 0.7272241115570068
    },
    {
      "name": "Generative grammar",
      "score": 0.708236813545227
    },
    {
      "name": "Transformer",
      "score": 0.6148566007614136
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6086781024932861
    },
    {
      "name": "Vocabulary",
      "score": 0.5492050051689148
    },
    {
      "name": "Natural language processing",
      "score": 0.4953712224960327
    },
    {
      "name": "Language model",
      "score": 0.4512098431587219
    },
    {
      "name": "Machine learning",
      "score": 0.3464961647987366
    },
    {
      "name": "Linguistics",
      "score": 0.14029622077941895
    },
    {
      "name": "Engineering",
      "score": 0.10246452689170837
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173089394",
      "name": "ITMO University",
      "country": "RU"
    },
    {
      "id": "https://openalex.org/I102248843",
      "name": "Vienna University of Economics and Business",
      "country": "AT"
    }
  ]
}