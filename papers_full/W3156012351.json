{
    "title": "Constrained Language Models Yield Few-Shot Semantic Parsers",
    "url": "https://openalex.org/W3156012351",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2113353992",
            "name": "Richard Shin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2128093545",
            "name": "Christopher Lin",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2186471829",
            "name": "Sam Thomson",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2095979647",
            "name": "Charles Chen",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2194586962",
            "name": "Subhro Roy",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2011834717",
            "name": "Emmanouil Antonios Platanios",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2118364164",
            "name": "Adam Pauls",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2056960045",
            "name": "Dan Klein",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2023211357",
            "name": "Jason Eisner",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2113965177",
            "name": "Benjamin Van Durme",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2250225488",
        "https://openalex.org/W2963106169",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2945735543",
        "https://openalex.org/W3150595019",
        "https://openalex.org/W3091229675",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2963794306",
        "https://openalex.org/W1967887284",
        "https://openalex.org/W1489834179",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2963655793",
        "https://openalex.org/W2963617989",
        "https://openalex.org/W3003374142",
        "https://openalex.org/W3023309694",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W2963364848",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2964120615",
        "https://openalex.org/W2091671846",
        "https://openalex.org/W2964029788",
        "https://openalex.org/W2963352809",
        "https://openalex.org/W2963877622",
        "https://openalex.org/W2919420119",
        "https://openalex.org/W2890431379",
        "https://openalex.org/W2251957808",
        "https://openalex.org/W3165960851",
        "https://openalex.org/W4287659415",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3154669786",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2563997644",
        "https://openalex.org/W3173157360",
        "https://openalex.org/W3016828850",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W3102095584",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W3105388824",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2102258316",
        "https://openalex.org/W3104713013",
        "https://openalex.org/W2890397703",
        "https://openalex.org/W3032766766",
        "https://openalex.org/W2508866736",
        "https://openalex.org/W2892452601",
        "https://openalex.org/W3169611685",
        "https://openalex.org/W3155000481",
        "https://openalex.org/W2134495021",
        "https://openalex.org/W2963382396",
        "https://openalex.org/W2951352467",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W3102961474",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2124204950",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2757361303",
        "https://openalex.org/W2970393840",
        "https://openalex.org/W4287811262",
        "https://openalex.org/W2163274265",
        "https://openalex.org/W3034284249",
        "https://openalex.org/W2032374895",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W2973088264"
    ],
    "abstract": "Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7699–7715\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n7699\nConstrained Language Models Yield Few-Shot Semantic Parsers\nRichard Shin, Christopher H. Lin, Sam Thomson, Charles Chen,\nSubhro Roy, Emmanouil Antonios Platanios, Adam Pauls,\nDan Klein, Jason Eisner, Benjamin Van Durme\nMicrosoft Semantic Machines\nsminfo@microsoft.com\nAbstract\nWe explore the use of large pretrained lan-\nguage models as few-shot semantic parsers.\nThe goal in semantic parsing is to generate a\nstructured meaning representationgiven a nat-\nural language input. However, language mod-\nels are trained to generate natural language .\nTo bridge the gap, we use language models\nto paraphrase inputs into a controlled sublan-\nguage resembling English that can be automat-\nically mapped to a target meaning representa-\ntion. Our results demonstrate that with only\na small amount of data and very little code to\nconvert into English-like representations, our\nblueprint for rapidly bootstrapping semantic\nparsers leads to surprisingly effective perfor-\nmance on multiple community tasks, greatly\nexceeding baseline methods also trained on the\nsame limited data.\n1 Introduction\nLarge pretrained language models (LMs) like GPT-\n3 (Brown et al., 2020) have shown increasingly\nimpressive few-shot performance by formulating\ntasks as text-to-text generation problems (Raffel\net al., 2020; Brown et al., 2020). Given only a\ntrained LM and a short textual prompt that de-\nscribes and/or exempliﬁes a task, one can produce\nsurprisingly accurate models for a variety of natu-\nral language processing problems. However, task-\nspeciﬁc semantic parsing does not naturally ﬁt into\nthis paradigm because such parsers typically use\ncustom meaning representations that are unlikely\nto already exist on the web, let alone exist in large\nenough quantities to affect the parameters of these\nLMs. We leverage two key insights to overcome\nthis barrier: (1) since LMs excel at generating natu-\nral language, we should formulate semantic parsing\nas paraphrasing into a controlled sublanguage (Be-\nrant and Liang, 2014; Marzoev et al., 2020) and (2)\nautoregressive LMs can be efﬁciently constrained\nto search over only valid paraphrases, so the sub-\nlanguage does not need to be learned from scratch.\nIn particular, following Berant and Liang (2014),\nwe envision a developer for some new domain\nﬁrst writing a synchronous context-free grammar\n(SCFG) that deﬁnes the space of supported (and\nwell-formed) meaning representations along with\ncanonical natural language constructions that ex-\npress them. Such a grammar maps between canon-\nical natural language forms and domain-speciﬁc\nmeaning representations, so that a separate LM-\nbased system can focus entirely on mapping an\nunconstrained utterance uto a canonical (but still\nnatural) form c. Furthermore, the grammar can\nbe used to constrain this LM-based system so that\nthe LM is only allowed to generate canonical ut-\nterances (i.e., utterances that correspond to well-\nformed meaning representations).\nGiven such a grammar, an LM, and a handful\nof examples for priming the LM for the task of in-\nterest, our approach immediately yields a working\nsemantic parser. While we do not expect the accu-\nracies of our models to reach state-of-the-art perfor-\nmance when compared to models trained on large\namounts of task-speciﬁc examples, the ability to\nrapidly prototype semantic parsers in new domains\ncan be immensely helpful for developers, both by\nfacilitating quick construction of a minimum vi-\nable product and by enabling the bootstrapping\nof new data collection through human-in-the-loop\nprocesses (Duan et al., 2016).\nWe report results on the Overnight (Wang\net al., 2015), Break (Wolfson et al., 2020) and\nSMCalFlow (Semantic Machines et al., 2020)\ndatasets1 using GPT-2 (Radford et al., 2019),\nGPT-3 (Brown et al., 2020), and BART (Lewis\net al., 2020) as the underlying LMs. Our re-\nsults demonstrate that our solution: (1) deliv-\ners greater accuracy when LMs target natural\nlanguage-like representations, (2) is further im-\nproved through the use of explicit decoder con-\n1Each of these preexisting datasets uses English inputs, but\ntheir output representations vary.\n7700\nCanonical UtteranceWhat time am I brewing coffee with Megan and Megan and Megan?Language ModelSCFG\nLanguage ModelConstrained Decoding\nMeaning RepresentationNatural Utterance\nFigure 1: Our proposed workﬂow for semantic parsing with a pretrained language model. Given a few examples\n(not shown) and a natural user utterance (blue, italic), a pretrained language model generates paraphrased utterances\n(purple). A grammar constrains the search over paraphrases to only canonical utterances, and the highest-scoring\ncanonical paraphrase is mechanically converted to a task-speciﬁc meaning representation (pink).\nstraints; and (3) performs surprisingly well with\nvery few examples, suggesting a new frontier for\nrapidly prototyping semantic parsers. The code\nand grammars developed in this work are pub-\nlicly available at https://github.com/microsoft/\nsemantic_parsing_with_constrained_lm.\n2 Background\nAutoregressive Language Models. A language\nmodel deﬁnes an (estimated) probability distribu-\ntion over sequences of tokens w = w1,...,w n.\nAutoregressive LMs factorize this distribution as:\np(s) =\nn∏\ni=1\np(wi |w1,...,w i−1). (1)\nUnlike a cloze model such as BERT (Devlin et al.,\n2019), an LM enables text generation, and an au-\ntoregressive LM makes it efﬁcient to generate in-\ncrementally. LMs like GPT-2 (Radford et al., 2019)\nand GPT-3 (Brown et al., 2020) are trained by max-\nimizing their likelihood on large web corpora.\nIt has been shown that autoregressive LMs are\npowerful at performing tasks not obviously con-\nnected to pure language modeling. For example,\nRaffel et al. (2020) showed that an LM was able to\nextend the prompt “Translate English to German:\nThat is good. ”with the correct translation “Das ist\ngut. ”Brown et al. (2020) used “few-shot” prompts\nthat included several examples of inputs followed\nby target outputs, with the actual task input ap-\npended at the end. In both cases, the task deﬁned\nby the prompt is carried out by asking the language\nmodel to generate the subsequent text. Even with-\nout task-speciﬁc ﬁne-tuning, this approach has al-\nready yielded reasonable results (see e.g., Radford\net al., 2018; Brown et al., 2020; Gao et al., 2020).\nThis has wide implications, indicating we may\nbe able to carry out various tasks simply by design-\ning the prompts that we feed to pretrained LMs,\nremoving the expense of training task-speciﬁc mod-\nels. There already exist multiple approaches to\nprompt design, like choosing appropriate examples\nto include in the prompt (e.g., Liu et al., 2021a)\nor reformulating the prompts into more human-\nfriendly forms (i.e., closer to natural language;\nSchick and Sch ¨utze, 2020a). More related to our\nwork, prompt-guided semantic parsing relates to\nideas in example-based machine translation dating\nback to work by Nagao (1984), that have been re-\ncently revisited in the context of semantic parsing\nwith retrieve-and-edit by Hashimoto et al. (2018).\nFine-tuning can still be used with these mod-\nels to perform various tasks (Li and Liang, 2021;\nLiu et al., 2021b; Schick and Sch¨utze, 2020b). Al-\nthough ﬁne-tuning requires additional training, the\nﬁne-tuned system can be more efﬁcient at inference\ntime, as it is no longer necessary to select training\nexamples to precede the test input.\nSemantic Parsing as Paraphrasing. We adopt\nthe insight from Berant and Liang (2014) that se-\nmantic parsing can make use of triples (natural\nutterance u, canonical utterance c, meaning repre-\nsentation m), where the parser maps u↦→c↦→m.\nBy design, it is easy to map c↦→mand vice-versa.\nOur innovation is to prompt and constrain an LM\nso as to make it map u ↦→c. This approach can\nexploit newly available large pretrained LMs.\nPrevious work in parsing as paraphrase has not\nused generative LMs for the u↦→cstep. Rather, it\nhas mapped u↦→cby obtaining candidate cvalues\nin some way and then scoring them according\nto whether they paraphrase u, using a semantic\nequivalence model that scores (u,c) pairs. For\n7701\nexample, Berant and Liang (2014) mapped from u\ndirectly to many candidate meanings m, and then\nevaluated the corresponding canonical utterances\nc against u. Wang et al. (2015) and Marzoev\net al. (2020) generated candidate cvalues (along\nwith their meanings m) from a grammar of legal\ncanonical utterances, but incrementally ﬁltered\nthe bottom-up or top-down generation by scoring\nthe partial candidates against u. Our procedure\nswaps the roles of the grammar and u. We use u\nto generate the candidate cvalues by prompting a\nlarge LM with u, and then incrementally ﬁlter the\nleft-to-right generation by assessing whether the\npartial candidates ﬁt the canonical grammar. This\nplaces the LM in the driver’s seat. The large LM\nthat we use for paraphrase generation is trained\non much more data than the specialized paraphrase\nscoring models used in prior work.\nBootstrapping a Semantic Parser. One line\nof prior work on quickly bootstrapping a se-\nmantic parser has focused on creating synthetic\ntraining examples from a grammar developed by\nhand (Campagna et al., 2019; Weir et al., 2020;\nMarzoev et al., 2020; Campagna et al., 2020) or\nderived automatically from existing data (Jia and\nLiang, 2016; Yu et al., 2020). Wang et al. (2015)\ndescribed an approach to bootstrapping that uses\na grammar to generate canonical forms, which are\nparaphrased by crowdworkers to produce training\ndata “overnight.” Xu et al. (2020) extended this\nwork by generating paraphrases for training data\nby ﬁltering examples generated from a grammar.\nIn this paper we take the approach of using the\ngrammar as a constraint, with an eye towards en-\nabling bootstrapping through human-in-the-loop\nsemantic parsing, where humans quickly annotate\ndata by manually correcting parses from an initial\nprototype (Duan et al., 2016; He et al., 2016; Yao\net al., 2019; Elgohary et al., 2021). With this mo-\ntivation in mind we report accuracy at K, deﬁned\nas the rate in which an annotator would ﬁnd the\ncorrect parse when selecting among K options.\n3 Approach\nWe propose a method for semantic parsing using\nlarge pre-trained LMs that requires little to no\ntask-speciﬁc training. For the prompt-based\nfew-shot setting, we use the 175-billion-parameter\nGPT-3 model (Brown et al., 2020) as our LM\nbecause at the time of writing it was the largest\navailable LM that provided an accessible API. 2\nOur goals are to show the approach is good enough\nto be practical, and to conﬁrm our claim that large\nLMs are better used to generate text that looks\nmore like natural language rather than an artiﬁcial\nprogramming language.\nOur approach consists of two parts: (1) LM prim-\ning, either through dynamic prompt creation or\nﬁne-tuning, and (2) constrained decoding, ensuring\nwell-formed output under the target representation.\nDynamic Prompt Creation. The prompt we\nfeed to GPT-3is designed so that it contains a small\nrepresentative set of examples mapping utterances\nto their desired outputs. As mentioned in§1, we tar-\nget rapid prototyping and so, for each task that we\ntackle we assume access to 1,000 or fewer training\nexamples. Each example is a pair (ui,ti) where\nui is an utterance and ti is the target output for\nthat utterance, speciﬁed as either the original mean-\ning representation, mi, or our canonical linguistic\nrepresentation, ci, which can then be translated to\nmi. Given a test input utterance u= “how long is\nthe weekly standup”, for example, a dynamically\nconstructed prompt looks something like:\nLet’s translate what a human user says into\nwhat a computer might say.\nHuman: when is the weekly standup\nComputer: start time of weekly standup\nHuman: what date is the weekly standup\nComputer: date of weekly standup\n...\nHuman: how long is the weekly standup\nComputer:\nIntuitively, we want the examples used to be similar\nto the test utterance uso GPT-3 can learn how to\ngenerate the target output based on just the prompt.\nWe propose to also use GPT-3 for selecting the\nexamples to include in the prompt. Consider a train-\ning example, (ui,ti). We quantify its relevance to\nthe test input u as p(u |ui), computed directly\nusing GPT-3.3 For each test utterance u, we sort all\ntraining examples by this metric, and construct the\nprompt from the P most relevant examples. Note\nthat the GPT-3 API accepts at most 2,048 tokens\n(after sub-word tokenization) and thus, if using P\nexceeds this limit, we reduce P accordingly. For\n2https://openai.com/blog/openai-api.\n3During development we also considered using S-\nRoBERTa (Reimers and Gurevych, 2019) and LASER\n(Artetxe and Schwenk, 2019) for estimating relevance in-\nstead of GPT-3, but we did not observe differences signiﬁcant\nenough to motivate the additional complexity.\n7702\nexample, to generate a 40-token output we need to\nlimit the prompt size to 2,008 tokens.\nFine-tuning. An alternative to few-shot prompt-\ning is to ﬁne-tune the LM on each task using just\nthe utterance as input. Since the GPT-3 API avail-\nable to us does not support ﬁne-tuning, we use the\nnext largest model of the same type, GPT-2 XL.4\nWe also ﬁne-tune BART (Lewis et al., 2020), a\npretrained sequence-to-sequence model with a bidi-\nrectional encoder and autoregressive decoder. As\nBART is trained to generate sentences given cor-\nrupted versions of those sentences, it is perhaps\nparticularly suited for generating paraphrases.\nWe use the same set of examples to ﬁne-tune that\nwe would otherwise use as candidates for prompt\ncreation, ﬁne-tuning an LM to do well at mapping\nutterance ui to the target output ti; no other ex-\namples are included in the prompt. When the tar-\nget is a structured representation, this amounts to\nsequence-to-sequence semantic parsing. When the\ntarget output is natural language, this might be\ncalled text rewriting or sentential paraphrasing.\nConstrained Decoding. The input to the LM is a\nprompt p, which always contains the utterance uto\nbe parsed. In the non–ﬁne-tuned case it is preceded\nby dynamically constructed examples as described\nabove. Given p, we use an LM to generate a contin-\nuation tand take this as the output. As mentioned\nin §2, we assume that each target task speciﬁes a\nway to constrain the generated continuation to guar-\nantee a well-formed output for that task. Formally,\nwe assume that each task provides a nextTokens\nfunction which, for any token sequence s, returns\nthe set of all tokens that can immediately follow s\nin the target output language. We then use the LM\nto produce the output tby extending the prompt p\nusing a length-normalized variant of beam search\n(Murray and Chiang, 2018; Wu et al., 2016). At\neach step of the search, we ﬁlter the set of valid\ncontinuations using nextTokens.\n4 Case Studies\nIn the following sections we present multiple case\nstudies to evaluate our approach. Each studies a\ndifferent task and follows the same workﬂow: a\nDeﬁnition of the task and the meaning representa-\ntion it uses; aFraming of the representation into our\n4We tried using GPT-2 with few-shot prompts and no ﬁne-\ntuning but the results were sufﬁciently poor that we did not\nexplore further.\nproposal, including a description of nextTokens;\nan Experimental Setup with task-speciﬁc details;\nand Results, where our experiments evaluate our\nability to predict the original meaning representa-\ntion m, either as u↦→mor as u↦→c↦→m.\n4.1 Overnight\nDeﬁnition. Wang et al. (2015) constructed the\nOvernight semantic parsing dataset, which con-\ntains a total of 13,682 examples across eight dif-\nferent domains exhibiting a variety of linguistic\nphenomena and semantic structures. The underly-\ning task aims to map natural language utterances\nto database queries. The authors initially gen-\nerated pairs (ci,mi) of canonical utterances and\ncorresponding queries (in the form of Lisp-like\nS-expressions) using a hand-crafted SCFG. They\nthen used crowdsourcing to paraphrase each ci into\na more natural-sounding utterance ui. An exam-\nple ui is shown below, followed by the canonical\nrepresentation ci and meaning representation mi:\nwhich january 2nd meetings is alice attenting [sic]\nmeeting whose date is jan 2 and whose attendee is alice\n(call listValue (call ﬁlter\n(call ﬁlter (call getProperty\n(call singleton en.meeting) (string !type))\n(string date) (string =) (date 2015 1 2))\n(string attendee) (string =) en.person.alice))\nThe resulting (ui,ci,mi) triples were used to train\na semantic parser that mapped u↦→(c,m).\nFraming. The publicly available release of the\nOvernight dataset conveniently contains all of the\n(ci,mi) pairs generated by enumerating SCFG\nderivation trees up to a certain depth. For some\nof these, the natural language paraphrase ui is also\navailable. For these, we can directly use mi as\nthe meaning representation for our setup, and ci\nas the canonical representation. Furthermore, we\nimplement the nextTokens function from §3 by\nbuilding a large trie that contains all of the ci or mi\nstrings (depending on whether our experimental\nsystem is attempting to map u ↦→cor u ↦→m).\nThis trie allows us to quickly look up all the ways\nin which a valid preﬁx of a (depth-limited) cor m\nstring can be extended to produce a longer valid\npreﬁx. In the case of m, it enforces not only syn-\ntactic well-formedness but also type safety.\nExperimental Setup. For each domain, we\nsimulate the low-data prototyping regime by using\nonly 200 training examples, randomly selected\nfrom the 640–3,535 examples provided in the\n7703\nModel Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social\nGPT-3 Constrained Canonical 200 0.859 0.634 0.792 0.741 0.776 0.792 0.840 0.687\nBARTf Constrained Canonical 200 0.847 0.581 0.845 0.725 0.758 0.773 0.831 0.731\nGPT-2f Constrained Canonical 200 0.836 0.549 0.804 0.640 0.752 0.787 0.762 0.726\nCao et al. (2019) 200 0.772 0.429 0.613 0.550 0.696 0.671 0.639 0.566\nCao et al. (2019) 640–3535 0.880 0.652 0.807 0.767 0.807 0.824 0.840 0.838\nBERT-LSTM (Xu et al., 2020) 640–3535 0.875 0.624 0.798 0.704 0.764 0.759 0.828 0.819\nAutoQA (Xu et al., 2020) 0 † 0.739 0.549 0.726 0.709 0.745 0.681 0.786 0.615\nTable 1: Denotation accuracies on Overnight.f indicates models that have been ﬁne-tuned on the training examples.\nFor results above the line, we use n = 200 randomly-sampled training examples; the ﬁrst three lines are our\nsystems, while for Cao et al. (2019), we ran their training code on the same 200. The results below the line come\nfrom prior work using many more training examples. †AutoQA was trained on a large set of >400,000 synthetic\nutterances ucreated from Overnight’s canonical utterances by automated paraphrasing.\nModel Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social\nGPT-3 Constrained Canonical 200 0.80* 0.62* 0.82* 0.71* 0.79* 0.84* 0.89* 0.72*\nGPT-3 Constrained Meaning 200 0.68* 0.53* 0.68* 0.58* 0.63* 0.75* 0.78* 0.63*\nGPT-3 Unconstrained Canonical 200 0.76* 0.46* 0.68* 0.56* 0.58* 0.74* 0.74* 0.55*\nGPT-3 Unconstrained Meaning 200 0.56* 0.39* 0.50* 0.42* 0.46* 0.66* 0.58* 0.48*\nGPT-3 Constrained Canonical 20 0.80* 0.55* 0.67* 0.68* 0.81* 0.60* 0.76* 0.67*\nBARTf Constrained Canonical 200 0.85 0.58 0.85 0.73 0.76 0.77 0.83 0.73\nBARTf Constrained Meaning 200 0.83 0.56 0.77 0.75 0.79 0.76 0.81 0.69\nBARTf Unconstrained Canonical 200 0.83 0.56 0.80 0.67 0.72 0.75 0.81 0.65\nBARTf Unconstrained Meaning 200 0.82 0.55 0.76 0.71 0.77 0.73 0.80 0.63\nTable 2: Variations of our method using GPT-3 and BART. “*” denotes accuracies computed on a smaller test set\nrandomly sampled from the full set due to the computational cost of using GPT-3.\nOvernight training set; with GPT-3, we also try 20\ntraining examples as a more extreme case. For each\nevaluation example, we create the GPT-3 prompt\nby selecting up to P = 20 training examples.\nWhen using constrained decoding, we perform\nbeam search with a beam size of 10. For uncon-\nstrained decoding with GPT-3, we use the API\nto greedily sample (using a softmax temperature\nof 0.0) from the prompt until we reach a newline\ncharacter; we also try beam search with beam size\n10, but to save on computation costs, we do so\nonly for the calendar domain. For parity, we report\nresults using greedy search for unconstrained\ndecoding with models other than GPT-3.\nResults. Table 1 shows our main results on the\nfull test sets in Overnight. As in prior work we com-\npute the denotation accuracy, checking whether\nexecution of the predicted m against a database\nreturns the gold answer, rather than exact match\naccuracy. We compare against the current state-of-\nthe-art method from Cao et al. (2019) also trained\non only 200 examples (see Appendix D.1 for de-\ntails).\nTable 1 also includes results using all training ex-\namples, from Cao et al. (2019) and Xu et al. (2020);\nand AutoQA, which uses only synthetic utterances\ncreated by automatically paraphrasing the canon-\n1 2 3 4 5 6 7 8 9 10 K\n0.6\n0.7\n0.8\n0.9\n1.0Denotation accuracy\nCC (200)\nCM (200)\nUC (200)\nUM (200)\nCC (20)\nFigure 2: Denotation accuracy @ K on the Calendar\nsubdomain of Overnight with GPT-3. Unlike Table 1,\nall conditions use beam search; Constrained Canoni-\ncal (CC), Constrained Meaning (CM), Unconstrained\nCanonical (UC), and Unconstrained Meaning (UM),\nusing (200) or (20) training examples.\nical utterances. On some of the domains, such as\nCalendar, Housing, and Restaurants, we obtain sim-\nilar numbers as the state-of-the-art approach using\n7 to 13 times less training data.\nOur method with GPT-3 performs the best\namong models trained on only 200 examples, ap-\nproaching the performance of the models trained on\nall training examples. BART and GPT-2, when ﬁne-\ntuned on the 200 examples, also perform quite well.\nBART outperforms GPT-2 despite having fewer\nparameters, suggesting that its denoising training\nobjective is particularly effective for paraphrasing.\n7704\nGiven that ﬁne-tuning was necessary for decent\nperformance for GPT-2, we expect that ﬁne-tuning\nGPT-3 may improve its performance even further –\nwhen it becomes practical to do so.\nTable 2 shows that both constrained decoding\nand the use of English-like canonical utterances\nrather than Lisp-like logical forms substantially\nincreases the accuracy. This same pattern holds for\nBART and GPT-2 as well. Using only 20 training\nexamples generally decreases accuracy by a modest\namount, but surprisingly not on all domains.\nFigure 2 shows accuracy@Kon the calendar do-\nmain, where the GPT-3 parser is scored as correct\non an input if any output in its top Khypotheses is\ncorrect. The accuracy@5 of Constrained Canonical\nis 0.98, even though this is only a rapid prototype\ntrained on 200 examples.\n4.2 Break\nDeﬁnition. Break (Wolfson et al., 2020) pairs\nnatural language questions with programs in the\nquestion decomposition meaning representation\n(QDMR). Each program is a sequence of database\nqueries in a controlled natural language, where\neach query can use the return values of previ-\nous queries. The utterances uare questions sam-\npled from many existing language understanding\ndatasets.5 Crowdworkers decomposed each ques-\ntion ui into a sequence mi of queries speciﬁed as\nstrings. The string of each step was restricted to: (i)\nwords and their inﬂections appearing in the ques-\ntions, (ii) 66 pre-deﬁned function words (e.g., “if” ,\n“on”, or “for each”), and (iii) tokens that refer to\nresults from the previous step. This resulted in\n44,321 train, 7,760 development, and 8,069 test\nexamples. An example is shown below, including\nour canonical representation (deﬁned next) and the\nQD meaning representation:\nWhat color are a majority of the objects?\n(colors of (objects)) where (number of (objects for each\n(colors of (objects))) is highest)\n1. objects\n2. colors of #1\n3. number of #1 for each #2\n4. #2 where #3 is highest\nFraming. For our canonical representation ci,\nwe mechanically and invertibly map the QDMR\n5Break covers semantic parsing (Price, 1990; Zelle and\nMooney, 1996; Li and Jagadish, 2014; Yu et al., 2018), reading\ncomprehension (Talmor and Berant, 2018; Yang et al., 2018;\nDua et al., 2019; Abujabal et al., 2019), and visual question\nanswering (Johnson et al., 2017; Suhr et al., 2019).\nModel Train n nem\nWolfson et al. 44,321 0.42\nColeman & Reneau 44,321 0.42\nGPT-3 Constrained Canonical 1,000 0.32*\nGPT-3 Constrained Canonical 100 0.24*\nGPT-3 Constrained Canonical 25 0.20*\nGPT-3 Constrained Canonical 200 0.31*\nGPT-3 Constrained Meaning 200 0.24*\nGPT-3 Unconstrained Canonical 200 0.20*\nGPT-3 Unconstrained Meaning 200 0.17*\nGPT-3 Constrained Canonical 200 0.24\nBARTf Constrained Canonical 200 0.22\nBARTf Constrained Meaning 200 0.22\nBARTf Unconstrained Canonical 200 0.18\nBARTf Unconstrained Meaning 200 0.19\nTable 3: NEM accuracy on the Break dataset, where n\nis the number of training examples used in each case.\nEntries drawn from the task leaderboard are included\nas reference points. “*” denotes accuracies on a random\nsample on validation. f indicates ﬁne-tuned models.\nmi into a single-line format that more closely re-\nsembles a detailed English request, as illustrated\nabove. We implement nextTokens by restricting\nthe allowed tokens to: (i) words or their inﬂections\nthat appear in the questions, (ii) the pre-deﬁned set\nof function words, and (iii) opening and closing\nparentheses. A string is considered valid if its to-\nkens belong to one of these three categories, and\nany parentheses used are balanced.\nExperimental Setup. The Break leaderboard 6\nreports four metrics, with a focus on normalized ex-\nact match accuracy (NEM), deﬁned as exact match\naccuracy after QDMR canonicalization. All four\nmetrics followed consistent relative trends in our\nexperiments; we focus on NEM for brevity and clar-\nity. We sampled n ∈{25,100,200,1000}items\nuniformly at random from the training set to simu-\nlate varying amounts of data in the low-data, rapid\nprototyping regime. For each evaluation example,\nwe create the prompt by selecting up to P = 20 of\nthe navailable training examples.\nResults. Table 3 shows the results. Similar\nto the ﬁrst case study ( §4.1), we observe that\nour Constrained Canonical approach obtains\ncompetitive accuracy despite using relatively few\ntraining examples. We can see that the canonical\nrepresentation is easier to predict than the meaning\nrepresentation, even though QDMR was already\ndesigned to be more natural than the original rep-\nresentations of the various Break datasets. We also\nsee that constrained decoding results in further im-\n6https://leaderboard.allenai.org/break.\n7705\n1 2 3 4 5 6 7 8 9 10 K\n0.25\n0.30\n0.35\n0.40\n0.45NEM\nCC (1000)\nCM (1000)\nCC (200)\nCM (200)\nFigure 3: NEM @Kon a sample from Break for GPT-3\nConstrained Canonical (CC) and Constrained Meaning\n(CM) with 200 training examples.\nprovements, leading to gains of 7–11% in absolute\naccuracy. All our methods outperform a standard\nseq2seq baseline (BART Unconstrained Meaning)\ntrained to predict the meaning representation on\nthe same number of training examples. For con-\nstrained decoding of canonical utterances, we see\nsteady improvements as we increase the number\nof training examples from 25 up to 1,000. Figure 3\nillustrates accuracy over the top-Kpredictions of\nthe model, with results consistent with §4.1.\n4.3 SMCalFlow\nDeﬁnition. SMCalFlow (Semantic Machines\net al., 2020) is a large dataset for task-oriented\ndialogue, spanning the domains of events,\nweather, places, and people. User utterances u\nin SMCalFlow are paired with rich executable\ndataﬂow programs mfeaturing API calls, function\ncomposition, complex constraints, and references\nto the programs from previous dialogue turns.\nThe dataset contains 133,821 training examples\n(ui,mi). Examples have a history of previous dia-\nlogue turns, which we ignore here in order to align\nour approach with the previous sections.7 The fol-\nlowing example shows the canonical representation\n(deﬁned next) and the meaning representation:\nWhat did I set as my response status for the team meeting?\nmy response status of ﬁnd event called something like\n“team meeting”\n(Yield :output\n(:responseStatus (singleton (:results\n(FindEventWrapperWithDefaults\n:constraint (Constraint[Event]\n:subject (?∼= #(String “team meeting”))))))))\nFraming. Unlike the previous datasets, SM-\nCalFlow does not come with a grammar. For such\na complex dataset, writing a grammar post-hoc that\n7Ignoring dialogue history hurts performance relative to\nprior work: history could be incorporated into a prompt in\nfuture work that strives for state of the art.\ncan produce ﬂuent, natural English is challenging.\nAt the same time, SMCalFlow is representative\nof the rich semantic parsing tasks our proposal is\nmeant to help rapidly prototype hence its inclusion.\nIn order to map between mand a canonical ut-\nterance c, we built an SCFG over (c,m′) pairs,\nwhere m′is a transformed intermediate represen-\ntation that is more SCFG-friendly than m(see Ap-\npendix A for details). While our transformation\nand SCFG allow us to map m ↦→m′ ↦→cdeter-\nministically (to construct training examples (ui,ci)\nfor the prompt), some simple guessing models are\nrequired in the reverse direction c ↦→m′ ↦→m\n(to convert GPT-3’s linguistic output to the desired\nSMCalFlow representation), since our canonical\nutterances care occasionally ambiguous and since\nm′omits some information about coreferent nodes.\nFrom this SCFG, we extract two CFGs that de-\nﬁne the well-formed sequences cand m′, respec-\ntively. As we generate a preﬁx from left to right,\nwe incrementally parse it using Earley’s algorithm\n(Earley, 1970). nextTokens inspects the state of\nthe incremental parser to return precisely the set of\nnext tokens that are allowed by the CFG.8\nExperimental Setup. We gather 300 training ex-\namples from the SMCalFlow training set through\nstratiﬁed sampling (see Appendix C) to simulate\na scenario where examples of different kinds are\nwritten by a domain developer in the course of de-\nveloping annotation guidelines. We also uniformly\nsample a set of 100 examples and use stratiﬁed\nsampling for a set of 150 examples from the SM-\nCalFlow validation set to assist in grammar devel-\nopment and hyperparameter tuning. We use a beam\nsize of 10. For some GPT-3 experiments, we uni-\nformly sample an evaluation set of 200 examples\nfrom the SMCalFlow validation set.\nResults. Results are shown in Table 4 and Fig-\nure 4. Note that we always evaluate on the orig-\ninal meaning representation. We ﬁnd similar rel-\native differences as in previous tasks: targeting a\nmore natural representation and constraining the\ndecoding improves results. Our methods also sig-\nniﬁcantly outperforms a standard sequence to se-\nquence baseline (BART Unconstrained Meaning)\ntrained to predict meaning representations.\n8To robustly handle tokenization mismatches between the\npretrained LM and grammar, we effectively transform the\ngrammar such that terminals are single characters. Details in\nAppendix A.\n7706\nModel Train n Accuracy\nSemantic Machines et al. (2020) 133,821 0.73\nGPT-3 Constrained Canonical 300 0.33*\nGPT-3 Constrained Meaning 300 0.25*\nGPT-3 Unconstrained Canonical 300 0.26*\nGPT-3 Unconstrained Meaning 300 0.20*\nGPT-3 Constrained Canonical 300 0.32\nBARTf Constrained Canonical 300 0.42\nBARTf Constrained Meaning 300 0.37\nBARTf Unconstrained Canonical 300 0.40\nBARTf Unconstrained Meaning 300 0.30\nTable 4: Performance on SMCalFlow. “*” indicates\nevaluation on a random sample of validation.f are ﬁne-\ntuned models.\n1 2 3 4 5 6 7 8 9 10 K\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60Exact Match\n GPT-3 CC\nBART CC\nBART CM\nFigure 4: Accuracy@K of three of our models on SM-\nCalFlow.\nMore Data. To analyze the effect of training\ndata size, we also evaluate our BART Constrained\nCanonical model on stratiﬁed training set sizes of\n1k, 10k, 50k and on the full SMCalFlow training\nset (≈120k). For comparison, we also consider\nthe current state-of-the-art model on SMCalFlow\n(V ACSP; Platanios et al., 2021) in the same set-\ntings. Figure 5 shows the results. Recall that for\nall experiments we do not use any dialogue context\nand so the performance of V ACSP is lower than the\nperformance reported by Platanios et al. (2021).\nOur proposed method outperforms V ACSP in\n/uni00000016/uni00000013/uni00000013/uni00000014/uni0000004e/uni00000014/uni00000013/uni0000004e/uni00000018/uni00000013/uni0000004e/uni00000014/uni00000015/uni00000013/uni0000004e\n/uni00000006/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000028/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000025/uni00000024/uni00000035/uni00000037/uni00000003/uni00000026/uni00000026\n/uni00000039/uni00000024/uni00000026/uni00000036/uni00000033\nFigure 5: Accuracy of our best model on SMCalFlow\nat different training set sizes, compared to the recent\nstate of the art model by Platanios et al. (2021).\nlow data regimes, further supporting the intuitions\nbehind our approach. With more training data the\nbeneﬁts of constrained decoding and an initialized\ndecoder become less important. Future work could\nassess the relative impact of sampling examples\nfrom the grammar for use in pretraining a model\nsuch as V ACSP, contrasting with using the same\ngrammar as constraints on paraphrase decoding.\n5 Discussion\nEmpirically, we demonstrated that (i) constrained\ndecoding is better than unconstrained and (ii) con-\ntrolled natural languages are better than meaning\nrepresentations when used with a pre-trained LM.\nThe beneﬁt of (i) is readily observable because un-\nconstrained decoding can produce not-quite-correct\nanswers. For example, GPT-3 constrained decod-\ning maps the Overnight example show me any\nmeetings labeled as important which are also three\nhours long to the correct canonical utterance meet-\ning that is important and whose length is three\nhours, whereas unconstrained decoding yields the\nnon-canonical utterance meeting whose label is\nimportant and whose length is three hours.\nThe effect of (ii) is harder to isolate, though we\nfound some suggestive examples, e.g., for the in-\nput utterance meetings that are not attended by\nalice, our method led to the correct meeting whose\nattendee is not alice. In contrast, constrained pre-\ndiction of the meaning representation dropped the\nnegation (using = instead !=), producing the mean-\ning representation for meeting whose attendee is al-\nice and is important. We speculate that constrained\nGPT-3 was more willing to preserve the input word\nnot than to produce !=. More impressively, in\nBreak, our method correctly interpreted the novel\nbigram as many, mapping Are there as many matte\nobjects as metallic objects? to ((number of (matte\nobjects)) is same as (number of (metallic objects)).\nIn contrast, constrained prediction of the QDMR\nled to the wrong predicate, whose canonical ut-\nterance would be ((number of (matte objects)) is\nhigher than (number of (metallic objects)).\n6 Further Related Work\nMotivated by tasks where a user requires certain\nphrases to be present or absent in the output of a\ntext generation system, researchers have explored\nincreasingly more efﬁcient approaches to restrict-\ning valid paths in beam search such that they satisfy\nexternally provided constraints (e.g., Hokamp and\n7707\nLiu, 2017; Anderson et al., 2017; Post and Vilar,\n2018; Hu et al., 2019). Grammar-constrained de-\ncoding restricts some or all of a successful transduc-\ntion path to result in a sequence parseable under\na grammar. Such techniques were used in task-\noriented speech recognition systems (Moore et al.,\n1997),9 where it was assumed a user knew the pre-\ncise way to phrase commands. In contemporary\nsettings we retain the notion of a parser support-\ning task-speciﬁc features, where we would like to\nenjoy the beneﬁts of a grammar in terms of laying\nout prescribed functionality but without constrain-\ning the user’s linguistic forms. Constraining neural\nsemantic parsing decoders has been explored by\nYin and Neubig (2017) and Krishnamurthy et al.\n(2017), among others, for generating structured\nforms rather than paraphrases. Herzig et al. (2021)\npredict intermediate semantic representations with\nstronger structural correspondence to natural lan-\nguage than m, replacing the role of c in our ap-\nproach with a modiﬁed meaning representation m′.\nLike the closely related problem of machine\ntranslation (Wong and Mooney, 2006; Andreas\net al., 2013), semantic parsing has recently been\ndriven by encoder-decoder neural architectures\n(starting with Dong and Lapata, 2016; Jia and\nLiang, 2016; Koˇcisk´y et al., 2016). More recently,\nChen et al. (2020) used pre-trained LMs, includ-\ning BART, to initialize both the encoder and the\ndecoder of a semantic parser. In concurrent work,\nDesai et al. (2021) reports gains on Chen et al.\n(2020) by modifying a target representation to be\nmore natural language-like. We argue that LMs\nare better suited for generating natural language\ndirectly rather than task-speciﬁc meaning represen-\ntations, using experiments designed to contrast the\nproﬁciency of LMs on these two output modalities.\nFinally, Wu et al. (2021) concurrently proposed\na similar solution to our own. We independently\nconﬁrm positive results on Overnight, with new\nstudies on Break and SMCalFlow. In contrast to\ntheir primary focus on the unsupervised setting, our\nexperiments were largely concerned with the few-\nshot scenario. We consider it reasonable to expect\nsmall hundreds of examples from a domain expert\nwhen building a real world parser, and our results\nsuggest that this obviates the concerns of Wu et al.\n9Prior to recent advances it was believed that “practical\napplication of speech recognition technology requires a vo-\ncabulary and grammar tailored to the particular application,\nsince for high accuracy the recognizer must be restricted as\nto what sequences of words it will consider” – Moore et al.\non initially tuning a paraphrase model beyond what\ncurrent off-the-shelf pretraining methods provide.\n7 Conclusion\nWe wish to rapidly develop semantic parsers in\nnew domains. To this end, we have demonstrated\nthat constrained decoding of powerful language\nmodels can enable the paraphrasing of user utter-\nances into a controlled sublanguage, which may\nthen be mapped to a task-speciﬁc representation.\nWith small hundreds of examples we are able to\nquickly bootstrap models for a variety of datasets,\nenabling future work that explores human in the\nloop interactions for iterative model reﬁnement.\nReferences\nAbdalghani Abujabal, Rishiraj Saha Roy, Mohamed\nYahya, and Gerhard Weikum. 2019. ComQA:\nA community-sourced dataset for complex factoid\nquestion answering with paraphrase clusters. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 307–317,\nMinneapolis, Minnesota.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2017. Guided open vocabulary im-\nage captioning with constrained beam search. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n936–945, Copenhagen, Denmark.\nJacob Andreas, Andreas Vlachos, and Stephen Clark.\n2013. Semantic parsing as machine translation. In\nProceedings of the 51st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 47–52, Soﬁa, Bulgaria.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nJonathan Berant and Percy Liang. 2014. Semantic pars-\ning via paraphrasing. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1415–\n1425, Baltimore, Maryland.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam\n7708\nMcCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language models are few-\nshot learners. Computing Research Repository ,\narXiv:2005.14165.\nGiovanni Campagna, Agata Foryciarz, Mehrad Morad-\nshahi, and Monica Lam. 2020. Zero-shot transfer\nlearning with synthesized data for multi-domain dia-\nlogue state tracking. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 122–132, Online.\nGiovanni Campagna, Silei Xu, Mehrad Moradshahi,\nRichard Socher, and Monica S. Lam. 2019. Genie:\nA generator of natural language semantic parsers for\nvirtual assistant commands. In Proceedings of the\n40th ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation (PLDI).\nRuisheng Cao, Su Zhu, Chen Liu, Jieyu Li, and Kai\nYu. 2019. Semantic parsing with dual learning. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 51–64,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nXilun Chen, Asish Ghoshal, Yashar Mehdad, Luke\nZettlemoyer, and Sonal Gupta. 2020. Low-resource\ndomain adaptation for compositional task-oriented\nsemantic parsing. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5090–5100, Online. As-\nsociation for Computational Linguistics.\nShrey Desai, Akshat Shrivastava, Alexander Zotov, and\nAhmed Aly. 2021. Low-resource task-oriented se-\nmantic parsing via intrinsic modeling.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota.\nLi Dong and Mirella Lapata. 2016. Language to logi-\ncal form with neural attention. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n33–43, Berlin, Germany.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota.\nManjuan Duan, Ethan Hill, and Michael White. 2016.\nGenerating disambiguating paraphrases for struc-\nturally ambiguous sentences. In Proceedings of\nthe 10th Linguistic Annotation Workshop held in\nconjunction with ACL 2016 (LAW-X 2016) , pages\n160–170, Berlin, Germany. Association for Compu-\ntational Linguistics.\nJay Earley. 1970. An efﬁcient context-free parsing al-\ngorithm. Communications of the ACM , 13(2):94–\n102.\nAhmed Elgohary, Chris Meek, Matthew Richardson,\nAdam Fourney, Gonzalo Ramos, and Ahmed H.\nAwadallah. 2021. NL-EDIT: Correcting semantic\nparse errors through natural language interaction. In\nNAACL 2021.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-\nshot learners. Computing Research Repository ,\narXiv:2012.15723.\nTatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren,\nand Percy Liang. 2018. A retrieve-and-edit frame-\nwork for predicting structured outputs. In 32nd Con-\nference on Neural Information Processing Systems.\nLuheng He, Julian Michael, Mike Lewis, and Luke\nZettlemoyer. 2016. Human-in-the-loop parsing. In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pages\n2337–2342, Austin, Texas. Association for Compu-\ntational Linguistics.\nJonathan Herzig, Peter Shaw, Ming-Wei Chang, Kelvin\nGuu, Panupong Pasupat, and Yuan Zhang. 2021. Un-\nlocking compositional generalization in pre-trained\nmodels using intermediate representations.\nChris Hokamp and Qun Liu. 2017. Lexically con-\nstrained decoding for sequence generation using grid\nbeam search. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1535–1546,\nVancouver, Canada.\nJ. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\nXia, Tongfei Chen, Matt Post, and Benjamin\nVan Durme. 2019. Improved lexically constrained\ndecoding for translation and monolingual rewriting.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 839–850,\nMinneapolis, Minnesota.\nRobin Jia and Percy Liang. 2016. Data recombination\nfor neural semantic parsing. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n12–22, Berlin, Germany.\nJustin Johnson, Bharath Hariharan, Laurens van der\nMaaten, Fei-Fei Li, C. Lawrence Zitnick, and Ross\nGirshick. 2017. Clevr: A diagnostic dataset for\ncompositional language and elementary visual rea-\nsoning. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\n7709\nTom´aˇs Ko ˇcisk´y, G ´abor Melis, Edward Grefenstette,\nChris Dyer, Wang Ling, Phil Blunsom, and\nKarl Moritz Hermann. 2016. Semantic parsing with\nsemi-supervised sequential autoencoders. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1078–\n1087, Austin, Texas.\nJayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-\nner. 2017. Neural semantic parsing with type con-\nstraints for semi-structured tables. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1516–1526, Copen-\nhagen, Denmark.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nFei Li and H. V . Jagadish. 2014. NaLIR: An inter-\nactive natural language interface for querying rela-\ntional databases. In International Conference on\nManagement of Data, SIGMOD.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021a. What\nmakes good in-context examples for GPT-3? Com-\nputing Research Repository, arXiv:2101.06804.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too.\nAlana Marzoev, Samuel Madden, M. Frans Kaashoek,\nMichael Cafarella, and Jacob Andreas. 2020. Unnat-\nural language processing: Bridging the gap between\nsynthetic and natural language data. In Proceedings\nof the First Workshop on Natural Language Inter-\nfaces (NLI).\nR. C. Moore, J. Dowding, H. Bratt, J. M. Gawron,\nY . Gorfu, and A. Cheyer. 1997. CommandTalk:\nA spoken-language interface for battleﬁeld simu-\nlations. In Fifth Conference on Applied Natural\nLanguage Processing, pages 1–7, Washington, DC,\nUSA.\nKenton Murray and David Chiang. 2018. Correcting\nlength bias in neural machine translation. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Research Papers , pages 212–223, Brussels,\nBelgium.\nMakoto Nagao. 1984. A Framework of Mechanical\nTranslation between Japanese and English by Anal-\nogy Principle. In A. Elithorn and R. Banerji, editors,\nARTIFICIAL AND HUMAN INTELLIGENCE, chap-\nter 11. Elsevier Science Publishers.\nEmmanouil Antonios Platanios, Adam Pauls, Subhro\nRoy, Yuchen Zhang, Alexander Kyte, Alan Guo,\nSam Thomson, Jayant Krishnamurthy, Jason Wolfe,\nJacob Andreas, and Dan Klein. 2021. Value-\nagnostic conversational semantic parsing. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 3666–\n3681, Online. Association for Computational Lin-\nguistics.\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1314–1324, New Orleans, Louisiana.\nP. J. Price. 1990. Evaluation of spoken language sys-\ntems: the ATIS domain. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Hidden\nValley, Pennsylvania, June 24-27,1990.\nAlec Radford, Karthik Narasimhan, Tim Salimans, ,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding by generative pre-training. Technical re-\nport.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2020a. Exploiting\ncloze questions for few shot text classiﬁcation and\nnatural language inference. Computing Research\nRepository, arXiv:2001.07676.\nTimo Schick and Hinrich Sch ¨utze. 2020b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners.\nSemantic Machines, Jacob Andreas, John Bufe, David\nBurkett, Charles Chen, Josh Clausman, Jean Craw-\nford, Kate Crim, Jordan DeLoach, Leah Dorner, Ja-\nson Eisner, Hao Fang, Alan Guo, David Hall, Kristin\nHayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Sm-\nriti Jha, Dan Klein, Jayant Krishnamurthy, Theo\n7710\nLanman, Percy Liang, Christopher H. Lin, Ilya\nLintsbakh, Andy McGovern, Aleksandr Nisnevich,\nAdam Pauls, Dmitrij Petters, Brent Read, Dan Roth,\nSubhro Roy, Jesse Rusak, Beth Short, Div Slomin,\nBen Snyder, Stephon Striplin, Yu Su, Zachary\nTellman, Sam Thomson, Andrei V orobev, Izabela\nWitoszko, Jason Wolfe, Abby Wray, Yuchen Zhang,\nand Alexander Zotov. 2020. Task-oriented dialogue\nas dataﬂow synthesis. Transactions of the Associa-\ntion for Computational Linguistics, 8:556–571.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 641–651, New Or-\nleans, Louisiana.\nYushi Wang, Jonathan Berant, and Percy Liang. 2015.\nBuilding a semantic parser overnight. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1332–1342,\nBeijing, China.\nNathaniel Weir, Prasetya Utama, Alex Galakatos, An-\ndrew Crotty, Amir Ilkhechi, Shekar Ramaswamy,\nRohin Bhushan, Nadja Geisler, Benjamin Hattasch,\nSteffen Eger, Carsten Binnig, and Ugur Cetintemel.\n2020. DBPal: A fully pluggable NL2SQL training\npipeline. In Proceedings of SIGMOD.\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question under-\nstanding benchmark. Transactions of the Associa-\ntion for Computational Linguistics, 8:183–198.\nYuk Wah Wong and Raymond Mooney. 2006. Learn-\ning for semantic parsing with statistical machine\ntranslation. In Proceedings of the Human Language\nTechnology Conference of the NAACL, Main Confer-\nence, pages 439–446, New York City, USA.\nShan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun,\nWeipeng Zhang, Jiansong Chen, Fan Yang, and Xun-\nliang Cai. 2021. From paraphrasing to semantic\nparsing: Unsupervised semantic parsing via syn-\nchronous semantic decoding. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5110–5121, Online. As-\nsociation for Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nSilei Xu, Sina Semnani, Giovanni Campagna, and\nMonica Lam. 2020. AutoQA: From databases to QA\nsemantic parsers with only synthetic training data.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 422–434, Online.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium.\nZiyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019.\nModel-based Interactive Semantic Parsing: A Uni-\nﬁed Framework and A Text-to-SQL Case Study. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5447–\n5458, Hong Kong, China. Association for Computa-\ntional Linguistics.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 440–450, Vancouver, Canada.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin\nWang, Yi Chern Tan, Xinyi Yang, Dragomir Radev,\nRichard Socher, and Caiming Xiong. 2020. Grappa:\nGrammar-augmented pre-training for table seman-\ntic parsing. Computing Research Repository ,\narXiv:2009.13845.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li,\nQingning Yao, Shanelle Roman, Zilin Zhang,\nand Dragomir Radev. 2018. Spider: A large-\nscale human-labeled dataset for complex and cross-\ndomain semantic parsing and text-to-SQL task. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n3911–3921, Brussels, Belgium.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In AAAI’96: Proceedings of the\nThirteenth National Conference on Artiﬁcial Intelli-\ngence, volume 2, pages 1050–1055, Portland, OR.\n7711\nA SMCalFlow SCFG\nWe use a synchronous context-free gram-\nmar (SCFG) to convert between SMCalFlow\nmeaning representations mand canonical English\nrepresentations c. Mapping m ↦→cis necessary\nin order to convert the SMCalFlow dataset into\nprompt examples (ui,ci), while mapping c↦→m\nis necessary to convert the predicted canonical\nEnglish paraphrases back into a target meaning\nrepresentation. In this section, we review SCFGs\nand discuss in general how they can be used to\nmap between canonical utterances and meaning\nrepresentations. We describe speciﬁc issues that\narose in the case of SMCalFlow, and how we\nhandled them. These techniques may also be\nuseful in other domains.\nA.1 Context Free Grammars\nA context free grammar (CFG) is a 4-tuple\n(V,Σ,R,v 0) where V is a set of nonterminal sym-\nbols, Σ is a set of terminal symbols, R = {V ×\n(V ∪Σ)∗}is a set of rules, and v0 ∈V is the start-\ning nonterminal. A CFG is speciﬁed by writing a\nlist of rules that expand a nonterminal v∈V into\na string of nonterminals and terminals,\nv→σ0v1σ1 ···vnσn,\nwhere vi ∈V∗,σi ∈Σ. The language Ldeﬁned by\na CFG consists of all strings that can be generated\nby using the rules to recursively expand nonter-\nminals starting from the start nonterminal v0 until\nthere are no nonterminals left. A string s∈Lcan\nbe parsed into one or more parse trees, which de-\nscribe the expansions that could have been used to\ngenerate s. A string is ambiguous if there is more\nthan one possible parse for it, and a grammar is am-\nbiguous if any string in its language is ambiguous.\nGrammars that attempt to cover natural language\ntend to be highly ambiguous, but in our setting an\nunambiguous grammar is preferable.\nAn SCFG can be thought of as two CFGs that\nshare nonterminals, but have their own set of ter-\nminals. Instead of specifying a single expansion,\neach rule speciﬁes two expansions, a source and a\ntarget expansion, which are synchronized by using\nthe same nonterminals:\nv→σ0v1σ1 ···vnσn , τ0v1τ2 ···vnτn\nThe two expansions must use the same nnontermi-\nnals, although the form above may be generalized\nto allow these nonterminals to appear in different\norders in the source and target expansions. The set\nof rules and their source expansions deﬁnes a CFG\nand a language C, and the set of rules and their\ntarget expansions deﬁnes a CFG and a language\nM. Because each expansion’s nonterminals are the\nsame in any given rule, given an SCFG and a string\nc ∈C, we can parse cto obtain a parse tree, and\nthen use this parse tree to generate its correspond-\ning string m ∈M. While one set of expansions\nis termed the source and the other the target, we\ncan also reverse the process and translate a string\nm∈M to a string c∈C. It is this ability to pair\ntwo languages together that we use to map between\ncanonical and meaning representations.\nA.2 SCFG for Semantic Parsing\nNow suppose we have some semantic pars-\ning domain with a set F of functions. Each\nfunction f ∈ F has a type signature\nf(af\n1 : Tf\n1 ,...,a f\nn : Tf\nn ) →Tf , where Tf is the\nreturn type and af\ni and Tf\ni are the name and type\nof the ith argument. For simplicity, we treat con-\nstants of the domain as 0-ary functions, writing\nthem without the parentheses.\nIn the case of SMCalFlow, we had to reconstruct\nthe type signatures for the functions in the dataset,\nas they were not provided with the dataset release.\nFor each function, we specify a corresponding\nEnglish template E(f) = σf\n0 af\n1 σf\n1 ···af\nnσf\nn, where\neach σf\ni is a possibly empty 10 string of English\ntext. Again, we may generalize to allow the ai to\nbe ordered differently in E(f) than in f.\nWe can deﬁne an SCFG that maps between pro-\ngrams and English for this domain by writing down\nthe rules\nTf →σf\n0 Tf\n1 σf\n1 ···Tf\nn σf\nn , f(Tf\n1 ,...,T f\nn )\nfor all f ∈F. Let T denote the set of types.\nFor example, consider a toy domain where we\ncan buy colored shapes. We have types T =\n{Command,CShape,Shape}, and functions\nfor returning shapes, coloring those shapes, and\n10For example, in SMCalFlow, our template for the function\nExecute($intension) is simply $intension.\n7712\nbuying the colored shapes:\nF = {buy($o: CShape) →Command,\ntoRed($s: Shape) →CShape,\ntoGreen($s: Shape) →CShape,\nsquare →Shape,\ntriangle →Shape}\nWe could write English templates:\nE(buy) = Buy a $o\nE(toRed) = red $s\nE(toGreen) = green $s\nE(square) = box\nE(triangle) = triangle\nThe resulting SCFG for our toy domain would be:\nCommand →Buy a CShape , buy(CShape) (1)\nCShape →red Shape , toRed(Shape) (2)\nCShape →green Shape , toGreen(Shape)(3)\nShape →box , square (4)\nShape →triangle , triangle (5)\nwhere we have bolded the nonterminals. Now\ngiven a canonical English utterance like Buy a\ngreen box, we can parse it to produce the parse\ntree (1 (3 (4)), which we can then use to gener-\nate the program buy(toGreen(square)).\nA.3 Ambiguity\nIdeally, the mappings c ↦→mand m ↦→cwould\nbe 1-1 mappings, but an SCFG does not guarantee\nthis. In the case of our SMCalﬂow SCFG, each\nmeaning representation does have only a single\nparse—as one would expect for code in a formal\nlanguage—so m ↦→c is deterministic. Unfortu-\nnately, a canonical utterance cmay have multiple\nparses, leading to different meanings m.\nThe ﬁrst reason for ambiguity arises directly\nfrom the ambiguity of English. For example, does\nCreate a meeting after the meeting with Bob mean\n“After the meeting, create a meeting with Bob” or\n“After the meeting with Bob, create a meeting”?\nWhile one could attempt to wordsmith the tem-\nplates to eliminate this kind of ambiguity, doing so\ncan quickly become unscalable for large domains.\nThe other reason that ambiguity occurs is\nthat we allow templates to not contain any En-\nglish literals, allowing a parser to loop arbi-\ntrarily many times on a nonterminal. For ex-\nample, consider the templates for the type co-\nercion functions toRecipient($person) and\npersonFromRecipient($recipient). Since\nthese functions coerce types in a way that English\nleaves implicit, our English templates for these two\nfunctions do not contain any English literals. This\nleads to SCFG rules like\nRecipient →Person ,\ntoRecipient(Person)\nPerson →Recipient ,\npersonFromRecipient(Recipient)\nPerson →Bob , Bob\nIn this case, given the canonical utterance Bob, one\ncan repeatedly apply the ﬁrst two rules any number\nof times, producing inﬁnitely many parses.\nWe could solve both problems by weighting\nthe rules in the grammar, and picking the lowest-\nweight parse. Since data is available, we can also\ntrain a model to predict the correct parse. However,\nwe ﬁnd that in practice, (1) limiting the allowable\nrecursion in the grammar so that the grammar can\nonly produce a ﬁnite number of parses and then\n(2) using some heuristic rules to pick among those\nﬁnite set of parses, is both simple and works well.\nTo limit the recursion in the grammar, we ﬁrst\ndeﬁne a graph induced by the SCFG, where nodes\nrepresent nonterminals and a directed edge from\nnode ns to nd represents usage of the nonterminal\nnd in a rule for ns. We greedily ﬁnd the set Nof\nthe minimal set of nodes that covers all the cycles\nin this graph. Then we make D copies of every\nnonterminal v1,...,v d for all v∈V, and for every\nrule\nv→→ σ1\n0v1σ1\n1 ···vnσ1\nn , σ2\n0v1σ2\n1 ···vnσ2\nn\nwe replace it with D copies of every rule where\ncopy dof a rule uses copy d+ 1 of a nonterminal\nvi if vi ∈N:\nvd →σ1\n0d(v1)σ1\n1 ···d(vn)σ1\nn , σ2\n0d(v1)σ2\n1 ···d(vn)σ2\nn\nwhere d(vi) = vd+1\ni if vi ∈N and vd\ni otherwise.\nFor our experiments, we set D = 10, which we\nﬁnd covers all the examples that we use.\nThen, to select from a ﬁnite set of parses,\nwe generate the corresponding program for each\nparse, and use a set of heuristic rules to discard\nprograms that we know are incorrect. These\nrules include discarding programs that call Yield\nmultiple times, as in (Yield :output (Yield\n:output ...)), and discarding programs that\n7713\nSMCalflow Original RepresentationIntermediateRepresentation\nRemove reentranciesAdd new functionsLanguage ModelCanonical English\nNatural English\nSCFG for Intermediate Representation\nIntermediate Representation (IR)\nAdd reentranciesRemove new functionsIRsDisambiguateIR\nFigure 6: Our pipeline for SMCalﬂow. We ﬁrst convert SMCalﬂow’s original representation to an intermediate\nrepresentation, upon which we induce an SCFG. This SCFG is used to generate pairs of natural and canonical\nEnglish utterances, which is used to train a language model to predict a canonical English utterance given a natural\none. Predicted canonical English utterances are then mapped back into intermediate meaning representations,\nwhich can then be transformed back into the original representation.\ncall CreatePreflightEventWrapper without\ncalling CreateCommitEventWrapper. In prac-\ntice we ﬁnd that our heuristic rules can recover\nthe correct parse 90% of the time.\nA.4 Character-level Parsing\nWhen writing English templates, it would be incon-\nvenient to ensure that the terminals of the gram-\nmar line up exactly with the tokens used by the\nlanguage model. Different LMs sometimes use\nsubtly different tokenizers, and it would be espe-\ncially inconvenient to write a different grammar\nfor each LM. In order to handle differences be-\ntween the LM’s tokenizer and the terminals of the\ngrammar, we effectively treat the grammar as one\nwhose terminals are all single characters. Then\nto implement nextTokens, we advance each LM-\nproposed token character-by-character and return\nthe set of tokens who, after being fully consumed,\nstill have live Earley chart items. By catering to\nthe LM’s preferred tokenization, we ensure that the\nLM’s likelihood after incremental search matches\nthe likelihood the LM would have assigned had it\nbeen given the full string to begin with.\nB Intermediate Representation\nWhile we have described how to build an SCFG\nfor mapping between meaning representations\nand canonical representations, we still have two\nproblems. The ﬁrst problem is that unfortunately as\nconstructed, the SCFG cannot handle reentrancies,\nwhere expressions are cached in variables inside\nlet expressions and then used multiple times.\nThe second problem arises from the fact that it is\nimpossible to engineer the English templates in a\nway that they produce natural English utterances\nfor every possible composition of functions. For\nexample, our English template for get($object,\n$path) is $path of $object, which produces\nﬂuent English when getting the start time of an\nevent, like in “start time of event”. However,\nconsider the program needed to deleting an\nevent: (DeleteCommitEventWrapper :event\n(DeletePreflightEventWrapper :id (get\n(constraint[Event]) #(Path \"id\"))). The\nSCFG would translate this program into “delete id\nof event” when we would prefer something closer\nto “delete event.”\nTo solve both these problems, instead of induc-\ning an SCFG based on the original SMCalﬂow rep-\nresentation, we instead ﬁrst transform SMCalﬂow\ninto an intermediate representation that 1) does not\ncontain any reentrancies and 2) replaces common\nprogram fragments with calls to macros, and in-\nduce an SCFG on that the resulting intermediate\nrepresentation. See Figure 6 for a visualization of\nour entire process.\nB.1 Reentrancies\nTo remove reentrancies, given an expression of\nthe form (let (var binding) (body)) where\nthe body contains usages of var, we replace\nthe ﬁrst usage of var (in postorder traversal)\nwith binding and all other usages into calls\nto (referWithinTurn T) where T ∈ T is\nthe return type of the body expression and\nreferWithinTurn is a new function that retrieves\nthe most “salient” evaluation of typeT from else-\nwhere in the program for the current utterance.\nGiven a program p in the intermediate repre-\nsentation, to convert a call to (referWithinTurn\nT) back into a let expression (to map from the\nintermediate representation back to the original),\nwe ﬁnd the ﬁrst expression e of type T in p (in\npostorder traversal), and replace p with the let\nexpression (let (x e) sub(p, e, x)), where\nsub replaces all expressions ein pwith x. Note that\nthis transformation is lossy. By picking the ﬁrst\nexpression that matches, it is possible to make mis-\ntakes, but we ﬁnd in practice that such a heuristic\n7714\nis often good enough.\nB.2 Macros\nTo reduce the number of unnatural sound-\ning utterances produced by our grammar, we\ndeﬁne macros to capture common program\nfragments, and then replace those fragments\nwith calls to those macros. For example, we\ndeﬁne a macro DeleteWrapper($event),\nwhich we use to replace fragments that look\nlike (DeleteCommitEventWrapper :event\n(DeletePreflightEventWrapper :id (get\n$event #(Path \"id\"))). After deﬁning\nmacros, we add the macros to the set of functions\nFand corresponding English templates. In the\ncase of DeleteWrapper, we write the template\ndelete $event. In total, we deﬁne 15 new\nfunctions and ﬁnd that they signiﬁcantly help\nﬂuentize the resulting English. When translating\nfrom the intermediate representation back to the\noriginal SMCalﬂow representation, we can remove\nthese new functions by simply replacing them with\ntheir deﬁnitions.\nC Stratiﬁed Datasets\nThe motivation for our stratiﬁed datasets is to try\nand imitate what a small dataset over SMCalFlow\nor similar would look like had it been collected with\na small target size in mind (i.e., collect and anno-\ntate 100 representative dialogue turns). In this case,\nwe expect that domain developers would do their\nbest to guarantee that each supported SMCalFlow\nfunctionality appears in at least one example. Such\nfunctionalities can be described by the functions\nthat are used in the respective programs. Therefore,\nour goal is to produce small subsets of the origi-\nnal large dataset (∼120,000 dialogue turns), which\nguarantee that each supported function appears in\nat least k examples (k = 1 in our experiments).\nThe procedure we used to do this consists of three\nsteps that we describe in the following paragraphs.\nFunction Histogram Extraction. We ﬁrst ex-\ntract function histograms for each example in our\ndata. This step consists of collecting all the func-\ntion signatures that appear in each example and\nthen constructing a histogram over these signatures\nfor our train, validation, and test datasets.\nRare Functions Filtering. SMCalFlow contains\nsome examples that use very rare functions. These\nexamples seem to be the result of annotation errors\nor incomplete data migrations and thus we do not\nwant to include them in our stratiﬁed datasets. Note\nthat including them would also render complete\ncoverage of the data (in terms of functionality) im-\npossible with only 100 or 300 examples. Therefore,\nin this step we: (i) use the extracted histograms to\ncollect all functions that appear less than ntimes\nin the training data ( n = 10 in our experiments),\n(ii) remove all examples that contain any of these\nfunctions across all of the train, validation, and\ntest data, and (iii) update the dataset histograms to\nreﬂect the ﬁltered data distributions.\nStratiﬁed Sampling. Our goal in this step is to\nuse the function histograms and sample subsets\nof the ﬁltered datasets which guarantee that each\nfunction appears in at least k examples in each\nsample. Let mbe the total number of examples\nin the dataset we are sampling from, and let f\nbe the total number of functions after the previous\nﬁltering step is applied. We formulate our sampling\nproblem as a mixed-integer program (MIP):\nmax\nx\nx⊤c, OBJECTIVE (6)\ns.t. x⊤1 = s, TARGET SIZE (7)\nx⊤H ≥k, COVERAGE (8)\nwhere x ∈ {0,1}m denotes whether or not an\nexample is included in the subset we are sam-\npling, c ∈ Rm is a random vector sampled\nfrom the standard Gaussian distribution, sis the\ntarget dataset size, and H ∈ {0,1}m×f de-\nnotes the function membership for each exam-\nple (i.e., Hij = 1 speciﬁes that example i con-\ntains function j). In our experiments we used the\nopen-source JOpt solver, which can be found at\nhttps://github.com/blubin/jopt.\nD Overnight\nD.1 Our reproduction of Cao et al. (2019)\nIn order to investigate how a state-of-the-art\nmethod for Overnight performs when it is only\ngiven 200 training examples for each domain, we\ndownloaded the code from https://github.com/\nrhythmcao/semantic-parsing-dual and made the\nfollowing modiﬁcations:\n• We used 200 training examples for each do-\nmain, the same examples as used in experi-\nments with our methods.\n• Overnight does not have an ofﬁcial develop-\nment set. Rather than holding out 20% of the\n7715\nModel Train n Basketball Blocks Calendar Housing Publications Recipes Restaurants Social\nGPT-2f Constrained Canonical 200 0.836 0.549 0.804 0.640 0.752 0.787 0.762 0.726\nGPT-2f Constrained Meaning 200 0.831 0.516 0.732 0.677 0.727 0.778 0.768 0.671\nGPT-2f Unconstrained Canonical 200 0.798 0.509 0.762 0.603 0.720 0.745 0.705 0.632\nGPT-2f Unconstrained Meaning 200 0.821 0.506 0.708 0.646 0.671 0.755 0.753 0.626\nTable 5: Accuracy on Overnight dataset using GPT-2 XL.\n200 training examples (the default method-\nology) as a development set to use for early\nstopping, we randomly sampled a develop-\nment set with a size of 20% of the original\ntraining set, from the original training set with\nthe 200 chosen earlier excluded.\n• We increased the total number of max epochs\nbefore stopping to 200, from 100. The code\nevaluates the model after each epoch on the\ndevelopment set, and chooses the snapshot\nthat performed best on the development set.\nD.2 Miscellaneous details\nFor our GPT-2, GPT-3, and BART experiments\nusing meaning representations as the target\noutput, we removed all instances of the string\nedu.stanford.nlp.sempre.overnight.SimpleWorld.\nfrom the meaning representations, as it is\nredundant.\nE Finetuning Experiments\nFor our ﬁnetuning experiments, we use BART-large\nmodel which has 406 million parameters, and the\nGPT2-XL model which has 1.5 billion parame-\nters. We train each model using the causal LM\nloss for 20,000 steps, where we linearly warmup\nthe learning rate for the ﬁrst 1000 steps, and then\nreduce the learning rate by a factor of 0.999 every\ntsteps. For choosing hyperparameters, we perform\na grid search by choosing the maximum learning\nrate from the set {10−5,10−6}and tfrom the set\n{2,4,6,8}. The best hyperparameters were chosen\nbased on performance on a development set. We\nuse a batch size of 32, clip gradient norm at 10, and\nset a minimum learning rate threshold of 10−9.\nWe add some additional experimental results\nusing ﬁnetuned GPT-2 XL in Tables 5 and 6.\nModel n nem\nColeman & Reneau 44,321 0.42\nWolfson et al. (2020) 44,321 0.29\nArad & Sapir 44,321 0.16\nBARTf Unconstrained Meaning 200 0.10\nGPT-2f Constrained Canonical 200 0.18\nGPT-2f Constrained Meaning 200 0.17\nGPT-2f Unconstrained Canonical 200 0.13\nGPT-2f Unconstrained Meaning 200 0.13\nTable 6: NEM accuracy on the Break dataset using\nGPT-2 XL.\nF Computing Infrastructure\nFor the GPT-3 experiments, we used OpenAI’s\nGPT-3 API hosted on Microsoft Azure. For the\nﬁnetuning experiments, we used NVIDIA DGX-2\nmachines contaning NVIDIA Tesla V100 GPUs.\nG Further Discussion\nA common thread among all of our datasets, and\narguably semantic parsing in general, is that an-\nnotation subtleties cause problems for automated\nmethods and annotators alike. For example, on the\nCalendar subset of Overnight, we found that of our\nbest model’s 18 errors, 8 were legitimately wrong,\n7 were annotation errors that the model actually\ngot right, and 3 differed only by equality strictness\n– which is often left ambiguous in natural language.\nFor example, for the input: tell me the all meetings\nbegins after 10am or 3pm, the annotated canonical\nform in the data is: meeting whose start time is at\nleast 10am or 3pm, but our system predicted: meet-\ning whose start time is larger than 10am or 3pm.\nWe would expect low interannotator agreement on\nthis subtle distinction (≥vs. >), just as we would\nexpect GPT-3 to perform poorly. As another exam-\nple, on the Calendar subdomain of Overnight, our\nbest model’s denotation accuracy @Ksaturated at\n0.98 when K ≥5; but we found that the 2 remain-\ning errors were caused by annotation mistakes on\nutterances that the model correctly interpreted."
}