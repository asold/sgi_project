{
  "title": "MedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with Retrieval Augmented Generation and Knowledge Graphs",
  "url": "https://openalex.org/W4411905858",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5100030119",
      "name": "Irene Siragusa",
      "affiliations": [
        "University of Palermo"
      ]
    },
    {
      "id": "https://openalex.org/A2327880973",
      "name": "Salvatore Contino",
      "affiliations": [
        "University of Palermo"
      ]
    },
    {
      "id": "https://openalex.org/A5100030120",
      "name": "Massimo La Ciura",
      "affiliations": [
        "University of Palermo"
      ]
    },
    {
      "id": "https://openalex.org/A5100030121",
      "name": "Rosario Alicata",
      "affiliations": [
        "University of Palermo"
      ]
    },
    {
      "id": "https://openalex.org/A245443857",
      "name": "Roberto Pirrone",
      "affiliations": [
        "University of Palermo"
      ]
    },
    {
      "id": "https://openalex.org/A5100030119",
      "name": "Irene Siragusa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2327880973",
      "name": "Salvatore Contino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5100030120",
      "name": "Massimo La Ciura",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5100030121",
      "name": "Rosario Alicata",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A245443857",
      "name": "Roberto Pirrone",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4366826420",
    "https://openalex.org/W2105289139",
    "https://openalex.org/W2941857399",
    "https://openalex.org/W4405925544",
    "https://openalex.org/W3131730029",
    "https://openalex.org/W4385476046",
    "https://openalex.org/W4400976919",
    "https://openalex.org/W6600248585",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2897980926",
    "https://openalex.org/W3104050923",
    "https://openalex.org/W4387211014",
    "https://openalex.org/W2901466771",
    "https://openalex.org/W3093937711",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W3165058054",
    "https://openalex.org/W6600599538",
    "https://openalex.org/W4281751314",
    "https://openalex.org/W2078524519",
    "https://openalex.org/W3110056903",
    "https://openalex.org/W2995808388",
    "https://openalex.org/W6888592886",
    "https://openalex.org/W3113662377",
    "https://openalex.org/W4402510436",
    "https://openalex.org/W4403667615",
    "https://openalex.org/W4404280355",
    "https://openalex.org/W6607643177",
    "https://openalex.org/W4401241231",
    "https://openalex.org/W4320523283",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W6756688054",
    "https://openalex.org/W2101105183"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nData Science and Engineering \nhttps://doi.org/10.1007/s41019-025-00297-8\nRESEARCH PAPERS\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set \nfor Advanced AI Applications with Retrieval Augmented Generation \nand Knowledge Graphs\nIrene Siragusa1 · Salvatore Contino1  · Massimo La Ciura1 · Rosario Alicata1 · Roberto Pirrone1\nReceived: 12 December 2024 / Revised: 30 April 2025 / Accepted: 3 May 2025 \n© The Author(s) 2025\nAbstract\nThe increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-\nquality data set, mainly due to privacy-related issues. In addition, the recent increase in Vision Language Models (VLM) \nleads to the need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding \nmedical scans. This paper illustrates the entire workflow for building the MedPix 2.0 data set. Starting with the well-known \nmultimodal data set  MedPix®, mainly used by physicians, nurses, and healthcare students for Continuing Medical Education \npurposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure \nin which noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a Graphi-\ncal User Interface aimed at navigating efficiently the MongoDB instance and obtaining the raw data that can be easily used \nfor training and/or fine-tuning VLMs. To enforce this point, in this work, we first recall DR-Minerva, a Retrieve Augmented \nGeneration-based VLM model trained upon MedPix 2.0. DR-Minerva predicts the body part and the modality used to scan \nits input image. We also propose the extension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B, \nand leverages MedPix 2.0. The resulting architecture can be queried in an end-to-end manner, as a medical decision support \nsystem. MedPix 2.0 is available on GitHub.\nKeywords MedPix · Biomedical data set · VLM · Decision Support System\n1 Introduction\nThe rise of computer-based applications in recent years has \nstrongly favored the digitization of historically analogous \nprocesses in the management and analysis of biomedical \ndata. In turn, the emergence of technologies based on Arti-\nficial Intelligence (AI) prompted the development of increas-\ningly precise models to support diagnosis for the construc-\ntion of personalized treatments. In the biomedical domain, \nin fact, different subareas can benefit from AI-based systems, \nranging from the administrative field (queue management in \nemergency areas, centralized integration of EHRs, and so \non) to the clinical one, thanks to the use of AI that can effi-\nciently extract useful features to achieve diagnosis reliably.\nOne of the fundamental requirements of these appli-\ncations is their trustworthiness, as they must help physi-\ncians with a high degree of confidence, being able to pro-\nvide reliable predictions and classifications. However, AI \nmodels require a considerable amount of data to achieve \ngood performance. Data availability makes expansion into \nIrene Siragusa and Salvatore Contino have contributed equally to \nthis work.\n * Salvatore Contino \n salvatore.contino01@unipa.it\n Irene Siragusa \n irene.siragusa02@unipa.it\n Massimo La Ciura \n massimo.laciura@community.unipa.it\n Rosario Alicata \n rosario.alicata@community.unipa.it\n Roberto Pirrone \n roberto.pirrone@unipa.it\n1 Department of Engineering, University of Palermo, Viale \ndelle Scienze, Palermo 90128, Sicily, Italy\n I. Siragusa et al.\nthe biomedical domain much more complex than in other \nones. One of the main problems lies in the availability of \ndata sets to allow the scientific community to develop new \nAI approaches. This problem arises mainly from the sen-\nsitive nature of data that encompasses privacy issues, and \nthis makes it difficult to build public data sets containing \nimages and/or clinical reports to be available to the scientific \ncommunity.\nTo overcome these obstacles, the European Community \ncreated the European Health Data Space (EHDS). 1 This is \na health-specific ecosystem composed of common rules, \nstandards and practices, infrastructure, and a governance \nframework that aims to empower people through increased \ndigital access and control of their personal electronic health \ndata. The EHDS promotes a single market for electronic \nhealth record systems, relevant medical devices, and high-\nrisk artificial intelligence systems. Finally, the EHDS aims \nto provide the researchers who make use of health data with \na trusted framework to control the entire analytical process \n[1, 2]. The systemic approach proposed by the EHDS will \nprovide a controlled pool of both shared data and applica-\ntions. It will allow AI in the medical field to access certified \nand controlled data by overcoming both medical and engi-\nneering obstacles through new solid foundations on which \na new generation of AI-based health applications can rely.\nIn view of the implications provided by the EHDS on the \napplication side, researchers wishing to pursue their activity \nin the biomedical domain by developing Vision Language \nModels (VLMs) should be able to find and optimize the data \nsets that are available in a way that maximizes the available \npublic resources. To date, there are not so many data sets \nthat contain images, such as Computed Tomography (CT) \nand Magnetic Resonance (MR)2 scan, and medical reports. \nOne of the most well-known data sets is  MedPix®,3 a free \nopen-access online database of medical images, teaching \ncases, and clinical topics, integrating images and textual \nmetadata provided by the National Library of Medicine \n(NLM), the largest collection of medical documentation in \nthe world. This collection is part of the much more famous \nNational Institutes of Health, which is responsible for man-\naging health and biomedical research in the United States, \nactively participating in the identification of new drugs from \n2010 to 2016 in collaboration with the Federal Drug Admin-\nistration. Given its important contribution to science, this \ninstitution is very careful about managing the privacy issue \nand applies strict rules for anonymising samples stored in \ntheir repositories [3 , 4], which have also been applied to \nthe MedPix data set used as input in the proposed work. \n MedPix® includes more than 12,000 patient case scenar -\nios, 9,000 topics, and nearly 59,000 images. The contribu-\ntion proposed in this paper lies in both curation and a new \narrangement of  MedPix® data in a non-relational database \nbased on MongoDB. This new arrangement of the  MedPix® \nstructure can be regarded as a brand new data set that we \ncall MedPix 2.0. MedPix 2.0 makes original data accessible \nand well structured, as it is very easy to build views capable \nof creating ready-made subsets for training VLMs. Data-\nbase queries have been further simplified by developing a \nuser-friendly Graphical User Interface (GUI). To enforce \nthe usability of MedPix 2.0, the user is made able to pose \nher/his query, and browse the results in a very close way as \nin the original website. For improved usability of MedPix \n2.0, the user can formulate their own question and browse \nthe results in a way that is very similar to the original web-\nsite. In fact, unlike the original MedPix, which was cre-\nated as an educational platform, our data set allows you to \ndynamically build training and/or test data sets of various \ntypes (image-only, text-only or multimodal) for AI models. \nThe use of a NoSQL database that offers a schema-on-read \napproach to data allows diversifying access to our data set \nby any downstream application, in order to simplify the que-\nrying of the data set. In particular, it is extremely simple to \nconstruct documentary data sets and prompt instructions or \nfew shots for a downstream LLM. Lastly, since we started \nfrom a data set containing documents that were not struc-\ntured but simply organised into sections, it seemed natural \nto use a loose structuring that reflected the organisation of \nthe original data.\nAnother contribution of this work is a direct demon -\nstration of how MedPix 2.0 makes it easy to train and test \nVLMs. At first, we recall the architecture and the training \nphase of DR-Minerva [5], a VLM developed by the authors \nthat leverages the Minerva LLM [6 ] and uses Flamingo \n[7] for multi-modality and that exploits the Retrieval Aug -\nmented Generation (RAG) approach [8]. RAG-based models \nare spreading in diverse domains, including the bio-medical \none, reaching promising results with a retrieval module com-\nposed of both vector stores [9, 10] and graph structures [11]. \nWe used our MongoDB data source to generate the train/\nvalidation/test split used for DR-Minerva.\nThen, we propose a Knowledge Graph (KG) built from \nMedPix 2.0, using Llama 3.1 Instruct 8B [12]. The KG can \nbe queried to obtain information about medical diseases and \ntheir diagnoses. The entire system, that is, the KG coupled \nwith DR-Minerva, can be queried end-to-end, starting from a \nmedical image to obtain suggestions about the most probable \ndiagnosis with a free-text answer, thus working as a medical \ndecision support system.\nThe paper is arranged as follows: Sect.  2 illustrates the \nState Of The Art (SOTA) for medical multimodal data sets. \n1 https:// www. europ ean- health- data- space. com/.\n2 We well refer also to MR scans as MRI, Magneric Resonance \nImaging.\n3 https:// medpix. nlm. nih. gov/ home.\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nThe details on the building process of the curated data set \nare reported in Sect. 3 along with the implementation of the \nMongoDB database and the GUI. The experimental results, \nthe architecture of DR-Minerva, and the creation of the KG \nare reported and discussed in Sect. 4, while Sect. 5 illustrates \nthe experimental results. Concluding remarks and future \nworks are drawn in Sect. 7.\n2  Related Works\nMedical data sets for AI applications suffer from diverse \nproblem, related both to the data and the peculiarity of the \ndomain. First, there is a privacy issue, since clinical data \ncontain private information about the patient, thus the pro-\ncess of creation of such type of data set has to start with an \nanonymization phase. To overcome this problem, research-\ners rely on either open-access and textbook data, or they \ncollaborate with hospitals to create data sets. The first two \nmethods allow for large scalability, relying on anonymous \ndata. On the other hand, anonymization must be carried out \nfrom scratch when dealing with hospitals. Moreover, mul -\ntimodal medical data suffer from scarcity when compared \nwith other data sets related to different domains, such as \nMS-COCO [13].\nOne of the most used open-access multimodal data-\nbases for developing medical data sets is PubMed  Central® \n(PMC).4 This is a widely used free archive of biomedical \nscientific literature, from which it is possible to build one’s \nown data sets via semi-automatic procedures. In PMC, data \nare anonymized, and high-quality captions can be extracted \nfrom the medical research papers the images belong to. The \nfollowing multimodal data sets were extracted from PMC: \nROCO [14], MedICaT [15], PMC-OA [16]. ROCO contains \npairs of radiology images and the corresponding captions, \nand it incorporates an out-of-class set to improve predic-\ntion and classification performances. MedICaT is a disjoint \ndata set from ROCO that is mainly composed on radiology \nimages and provides manually annotations for sub-figures. \nPMC-OA is the larger than the previous ones, and it keeps \na variety of diagnostic procedures, diseases, and findings, \nwhile introducing sub-figure separation.\nVQA-RAD [17] is a data set derived from  MedPix®, and \nit collects a subset of radiological images, while providing \nQuestion-Answer (QA) pairs validated by domain experts.\nAnother source of available high-quality data is text-\nbooks: PathVQA [18] is a Visual Question Answering \n(VQA) data set that collects both closed- and open-ended \nQA pairs, which are extracted from both pathology text -\nbooks and online digital libraries via a semi-automated \npipeline.\nOn the other side, open access data sets like MIMIC-CXR \n[19], IU-Xray [20] and SLAKE [21] are manually anno-\ntated by domain experts. Both MINIC-CXR and IU-Xray \nare chest radiography data sets derived from hospital’s clini-\ncal cases. Both if them contain a semi-structured radiology \nreport, describing the radiological findings of the images \nit is related to. SLAKE, on the contrary, collects images \nfrom different radiology open-access data sets and provides \nmanual annotations and QA pairs given by experienced doc-\ntors in English and Chinese.\nINSPECT [22] is a worth mentioning multimodal data set \nthat collects computed tomography pulmonary angiography \n(CTPA), radiology reports and structured data from EHRS, \nfor pulmonary embolism diagnosis.\nThe aforementioned data sets are multimodal ones, and \nthey are deeply focused on VQA tasks. Also some special-\nized data sets exist that provide only visual data, as Uni-\nToChest [23], ADNI [24], OMI-DB [25] or OASIS-3 [26] \nsuitable for image segmentation and classification, or free-\nform textual data such as in Named Entities in Medical Case \nReports (NE-MCR) in which the main focus is towards \nNamed Entity Recognition (NER) [27]. Other works worth \nmentioning are UniToBrain [28] that integrates a technical \nreport with the scanning modality, and the E3C corpus [29] \nthat is an annotated multilingual data set of clinical reports \nsuitable for NER and Relation Extraction (RE) tasks.\nThe proposed MedPix 2.0 data set may be close to VQA-\nRAD, they differ in both the sampling strategy and the sam-\nples themselves. In Med Pix 2.0, we do not integrate QA \npairs, but different QA pairs can be created relying on the \nstructured textual information provided in the data set itself, \nfollowing the structure of the other one. Thus, the creation \nof more complex tasks is possible, such as document sum-\nmarization or understanding.\nWith respect to the existing multimodal data sets, MedPix \n2.0: \n1. is derived from a freely open-access source, and it has \nno privacy-related issues;\n2. offers a balanced variety of CT and MRI scans of differ-\nent body parts;\n3. for each image, a complete structured clinical case is \nprovided.\nThanks to the annotation scheme we selected for the JSON \ndocuments in MedPix 2.0, the last point makes it suited for \nvarious tasks not only limited to document-level retrieval. \nUnfortunately, images are provided in PNG format, which \nlimits visual processing with respect to raw images in \n4 https:// www. ncbi. nlm. nih. gov/ pmc/.\n I. Siragusa et al.\nDICOM format. A comprehensive overview of the reported \ndata set, is reported in Table 1.\n3  MedPix 2.0\nMedPix® is a free open-access multimodal online database \nof medical images, teaching cases, and clinical topics, man-\naged by the National Library of Medicine (NLM) of the \nNational Institutes of Health (NIH). It mainly serves as a \nsupport system for Continuing Medical Education (CME) \nof physicians, nurses, and healthcare students. The database \ncollects clinical cases related to more than 12,000 patients. \nEach case contains at least one medical image, and the cor-\nresponding findings, discussion notes, diagnosis, differential \ndiagnosis, treatment, and follow up. Textual information is \nreported in a semi-structured format. Attached to the clini-\ncal case, there is the topic section, where the disease under \ninvestigation is discussed in detail from an academic and \ngeneral perspective.\nIn Fig.  1 an example from the  MedPix ® website is \nreported.\nDespite the richness of the data set, its free availability, \nthe possibility to add new cases, and the access to clinical \ncases of interest using wither keywords, the body part or \nthe disease, it is not possible to access the raw data. This \nfeature limits the usage of  MedPix® for training multimodal \nTable 1  Data set taxonomy Data set References # samples Data source Task\nTextual-only\nNE-MCR [27] 53 PMC NER\nE3C [29] 10.034 PMC NER, RE\nVisual-only\nUniToBrain [28] 258 Hospital Image segmentation\nUniToChest [23] 306.440 Hospital Image segmentation\nOASIS-3 [26] 6.471 Hospital Image segmentation\nOMI-DB [25] 3.072.878 Hospital Image classification\nADNI [24] 1921 Hospital Image classification\nVisual and textual\nROCO [14] 87.952 PMC Multimodal classification\nMedICaT [15] 217.060 PMC Subfigure-subcaption alignment\nPMC-OA [16] 1.650.00 PMC Multimodal classification\nVQA-RAD [17] 3.515 MedPix® VQA\nMINIC-CXR [19] 377.110 Hospital Classification, text generation\nIU-Xray [20] 8.121 Hospital Multimodal retrieve\nSLAKE [21] 14.028 Open data VQA\nPathQA [18] 32.799 Pathology books VQA\nINSPECT [22] 23.248 Hospital Multimodal classification\nMedPix 2.0 2050 MedPix® Multimodal classification, text generation\nFig. 1  An example from the \n MedPix® web site where the \noriginal screenshots have been \nrearranged for visualizing \npurposes, and the colored labels \nhave been added for clarity\n\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nAI systems. Therefore we decided to create a brand new \nstructured version of this data set because it represents a \nhigh quality source for AI-based medical applications. Med-\nPix 2.0 has been built essentially as a MongoDB instance \nthat is released along with a suitable GUI aimed both at \ngeneral purpose querying and extracting training data for AI \nmodels. Referring to the Fig.  1, a MongoDB version of the \ndata set was built using a semi-automated pipeline to create \ntwo kinds of JSON documents: the one collecting the infor-\nmation falling into the screenshots labeled as DESCRIP -\nTION, and the one that gathers the information falling into \nthe screenshots labeled both as CASE and TOPIC. As men-\ntioned in the section 1, the choice of MongoDB is motivated \nby the aim of respecting the organisation of the main data, \nimproving its readability. In fact, the pairs of images and \ntexts are connected to each other by reference URIs, mak -\ning it unnecessary to store the images within the documents \nas is typically done with a relational approach. In this way \nthe creation of multimodal data sets is made more intuitive, \nfacilitating any downstream application.\n3.1  Data Set Extraction\nWe decided to focus on a part of  MedPix® that involves cases \nfrom two diagnostic modalities, namely Computed Tomog-\nraphy (CT) and Magnetic Resonance Imaging (MRI). First \nof all, the images in the considered split were downloaded \nvia Open-i®:5 among them, there were some noisy images \nsuch as teaching materials or annotated images (Fig. 2) that \nare inconsistent as input image for training a VLM for which \nwe expect a clear CT or MRI. In particular, this process of \ncuring the data set by removing noisy samples, was per -\nformed via manual direct inspection of all the automatically \ndownloaded samples. This was the only non-automated task \nperformed during the data set extraction process; therefore, \nwe refer to our approach as semi-automated. This selection \nwas crucial, because the use of images with artefacts, such \nas the ones shown in the figure (e.g. symbols, annotations), \nwould not allow a correct visual feature extraction, leading \nto relevant biases in the classification process of the multi-\nmodal model.\nA subsequent automatic scraping pipeline was imple-\nmented to extract the textual data related to the selected \nimages using Selenium 6 and Beautiful Soup. 7 Finally, two \nkinds of JSON documents were devised to store, respec-\ntively, the information strictly connected with the images \n(descriptions document) and the one related to a \nFig. 2  Among the manually \nremoved images there are pres-\nentation’s slide (a), manually \n(b) and automatic annotated \nimages from DICOM viewer (c)\nFig. 3  Example of a JSON \ndocument in MedPix 2.0 as for \ndescription document \n5 https:// openi. nlm. nih. gov/s.\n6 https:// www. selen ium. dev/.\n7 https:// www. crummy. com/ softw are/ Beaut ifulS oup/ bs4/ doc/.\n I. Siragusa et al.\nclinical case (case-topic document). A one-to-many \nrelation has been created between a clinical cases and images \nby embedding the U_id defined for a case-topic docu-\nment in each descriptions document attached to each \nimage related to the clinical case itself. An example of the \ntwo kinds of JSON documents is shown in Figs. 3 and 4.\n3.2  MongoDB Representation\nIn order to process properly the data in MedPix 2.0, we built \na MongoDB database to host all the JSON documents along \nwith the images. The architectural choice stems not only \nfrom the nature of the data in MedPix 2.0 but also from \nconsiderations related to its high flexibility and scalability \nto distributed environments where also private multimodal \nmedical data could be added to the original collections with \nthe constraint of not being moved away from their genera-\ntion site as it is the case of hospital generated information.\nWe built a MongoDB instance made by two collec-\ntions, namely Image_ Descriptions, containing the \ndescriptions documents, and Clinical_reports  \nwhich contain all the case-topic documents. In our \nimplementation, the images are stored in a separate folder, \nand they are accessed using a proper file:// URL built \nstarting from their U_id. Finally, a view called Image_  \nReports allows a direct access to both collections via their \nU_id.\nA user-friendly GUI was built using PyQt5 8 to allow an \neasy access to the database, querying it to obtain the desired \ndata for visualization and/or download. As shown in Fig. 5a, \nit is possible to select either the collection or the view to be \nqueried and, add the query input in the relative fields. In \nFig. 5b, an example of the answer to a query is reported: \nsamples matching the query are reported as a list of JSON \nobjects and the user can save it or view a specific clinical \ncase and/or image selected in list of the query results. An \nexample of this view is shown in Fig.  6, where a  MedPix® \nlike GUI is reproduced to enforce usability for the users of \nthe original website. In our GUI, the curated images scraped \nalong with the texts are showed by default, but the user can \nFig. 4  Example of a JSON \ndocument in MedPix 2.0 as for \ncase-topic document \n8 https:// www. river bankc omput ing. com/ softw are/ pyqt/.\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nchoose to download the original (non curated) data related \nto the clinical case under investigation.\nMedPix 2.0 and its querying interface can be a valid tool \nfor both physicians and AI researchers since the desired data \nFig. 5  An example of the interface for querying the database where the user asks for CT scans of female patients (a) and the provided output (b)\nFig. 6  An example of the view of a selected clinical case retrieved by the query posed in Fig.  5. The image, and the relative description are \nreported along with the information of the clinical case the image belongs to\n I. Siragusa et al.\ncan be easily downloaded for further application, like train-\ning Deep Learning (DL) models where a large amount of \nstructured data are required. The structured output down-\nloaded from the interface, can be used in turn to fill a fixed \ntemplate, and to provide a VLM with a rich textual prompt. \nAlso multimodal tasks can be addressed if the textual \ndescriptions are combined with the corresponding image, \nas we will demonstrate in Sect. 4. Our re-arranged version of \n MedPix®, contains the same information of the original data \nset, but allows a easier usage for DL application in text-only, \nimage-only and multimodal setup. This flexible structure is \nalso suitable for adding new samples that meet the charac-\nteristics of the data set, i.e. they should be a clear CT or MR \nscan with the associated textual data as in Figs.  3 and 4 . \nThe interface can be locally executed as reported in https://  \ngithub. com/ CHILa b1/ MedPix- 2.0/ tree/ main/ Mongo DB- UI.\n4  Training a VLM Using MedPix 2.0\nThis section reports an application scenario for using Med-\npix 2.0. Specifically, a training pipeline for a multimodal \nmodel was implemented and, starting from the images, \nthrough a RAG-based approach, textual and visual embed-\nding are generated to predict the scan modality and the body \npart shown in the input image. The output at this stage is \nfurthermore used to query a generative Large Language \nModel (LLM) which will generate a consistent answer for \nthe end user through the use of a Knowledge Graph (KG), \nthe answer contains a diagnostic suggestion, leveraging the \nacquired knowledge on both scan modality and body part \nand the knowledge in the KG.\n4.1  Data Preparation\nThe training phase of the model was conducted using Med-\nPix 2.0, carefully divided into three splits: Train, Validation, \nand Test. In order to reduce data leakage, when splitting \nthe data set into Train, Validation and Test splits, all the \nimages referring to the same clinical case are reported in \nthe same split. In addition, we ensured a correct stratifica -\ntion of the data as for balancing both the typology of the \nscanning modality, CT and MR scans, and the body part \nshown. Body parts were grouped into five macro categories, \nnamely Abdomen, Head, Reproductive and Urinary System \n(RUS), Thorax and Spine and Muscles (SaM). Splits were \ncreated using a 80:20 in which Training and Testing split \nwere derived; from a subsequent 50:50 split over the testing \nsplit, Validation and Test splits were expunged.\nThe data obtained from this division are shown in \nTables 2 and 3.\n4.2  RAG‑Based Flamingo\nFigure 7 illustrates the overall RAG architecture used to test \nMedPix 2.0, which is called DR-Minerva [5 ]. DR-Minerva \nrelies on Flamingo [7] and Minerva [6], a new LLM trained \nfrom scratch on English and Italian data as part of the activi-\nties in the PNRR FAIR Transversal Project 2: “Vision, Lan-\nguage, and Multimodal Challenges”. 9 FAIR TP2 is a col-\nlaborative research carried out by more than twenty Italian \nuniversities. The authors are engaged in developing VLM \nfor the biomedical domain. We chose Minerva because it is \ntotally open. Not only the weights, but also the architecture \nis free along with the training data. Furthermore, its training \nTable 2  Summary of images and clinical cases for each split\nData set #CT #MR Clinical case\nTrain 878 775 535\nValidation 84 113 67\nTest 100 100 69\nTotal 1062 988 671\nTable 3  Summary of images divided per macro area of the body for \neach split\nData set Abdomen Head RUS Thorax SaM\nTrain 264 742 127 263 257\nValidation 23 66 20 30 58\nTest 32 76 11 41 40\nTotal 319 884 158 334 355\nFig. 7  Overview of DR-Minerva architecture\n9 https:// fonda zione- fair. it/ en/ trans versal- proje cts/ tp2- vision- langu \nage- and- multi modal- chall enges/.\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nset is half English and half Italian, making it suitable for \nfuture experimentation using Italian data directly.\nFlamingo was chosen as a suitable VLM since it has strong \nin-context learning capabilities that make it suitable to various \ndomains [7]. Both in training and inference phase, visual tokens \nare incorporated with textual encodings, via a suitable trained \ncross-attention layer. More in detail, Flamingo allows the flex-\nibility to adopt a custom visual and textual encoder, thus SOTA \nmodels can be easily used in this hybrid pipeline. On this basis, \nthis approach was preferred to other VLMs, such as CLIP [30], \nwhich leverages an encoder-only models as for textual encod-\ning, thus resulting in poor generative capabilities with respect \nto decoder-only models. In addition, the proposed classification \nvia CLIP, is limited to pre-defined categories, which prevent \nthe generative capabilities of the model in providing verbose \nanswers. In our implementation, we used Open Flamingo [31] \nin the 3B parameter version,10 where CLIP ViT-L/14 [30] was \nemployed as visual encoder and Minerva-3B 11 as language \nencoder. The Table 4 shows the prompt used to query the model \nin order to obtain the desired response, which consists of both \nthe scanning modality and the body part shown in the input \nimage.\nWe developed a suitable RAG component for DR-Min-\nerva [8 ] to improve the overall performance of the model \nat inference time, due to a prompt enriched with relevant \ninformation for the query. The RAG technique allows you to \nconstruct a prompt that is based not only on the instructions \nand input provided, but also on relevant ad hoc information \nretrieved from a suitable Knowledge Base and is used as \nstate of the art to respond to tasks of this type [32– 34]. On \nthis basis, the selected LLM can produce a coherent answer, \nthus mitigating any hallucinations due to the intrinsic knowl-\nedge in the LLM [8]. In addition, updated knowledge can be \ninjected to the model via an in-context strategy [35], with-\nout requiring an additional fine-tuning of the model after a \nknowledge update [36]. Various strategies have been pro-\nposed to effectively enhance retriever capabilities, leverag-\ning vector databases [9, 37]. Obviously, increasing retrieve \nperformances leads to better generated outputs and in opti-\nmizing computational costs.\nBecause AI models must be as precise as possible, espe-\ncially in the medical sector, we queried DR-Minerva with \nboth the target medical image and a template made up of \nsome personal information about the patient (e.g. age and \ngender), followed by the patient’s history. The most relevant \nclinical cases are then retrieved and added to the prompt \nin the form of few-shot learning examples, based on the \npatient’s history. The RAG component selects the k-closest \nclinical cases to the query, more specifically, in our experi-\nments k is equal to four. The value of this threshold was \nempirically found since it resulted in the optimal trade-off  \nboth for providing a sufficient context to the model and as \nfor reaching satisfactory results. The vector database in the \nRAG component has been filled suitably with textual infor-\nmation from clinical cases, which belong to the train split, \nand involve the age and gender of the patient, the relative \nclinical history, and the doctor’s diagnosis.\nOur RAG is built using the LangChain framework, 12 \nwhich in turn employs a FAISS vector database [38], where \nthe data are saved using Linq-Embed-Mistral [39]. This \nmodel is regarded as the best in the Massive Text Embed-\nding Benchmark (MTEB) [40] for Information Retrieval.13\nBoth text-encoding and text generation are demanded to \nMinerva LLM, a transformer decoder-only LLM ([41]) built \nupon the Mistral architecture ([42]) which was trained from \nscratch on 660B tokens, equally balanced between English \nand Italian. Flamingo leverages a perceiver-resampler net -\nwork to extract visual tokens from the visual encoding. In \nturn, visual tokens are employed to condition the textual \noutput generation by inserting cross-attention layers between \nthe existing pretrained ones and frozen LLM layers [7].\n4.3  Knowledge Graph Definition\nMedPix 2.0 is actually a huge archive of historical clinical \ncases that can be exploited as external knowledge for sophis-\nticated textual predictions. In addition to the information \nTable 4  The structure of the prompt is reported as well as the template of the corresponding expected answers\nPrompt Response\nGiven the following medical images and the patient history, provide informa-\ntion about the scanning modality and the body part shown in the image. \n/uni27E8.s1image/uni27E9.s1 \n/uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient. /uni27E8.s1clinical history of the patient/uni27E9.s1 . [... context... ]The image is a \n/uni27E8.s1scan type /uni27E9.s1 scan showing a /uni27E8.s1body part /uni27E9.s1 . [... context...]/uni27E8.s1image/uni27E9.s1 /uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient. \n/uni27E8.s1clinical history of the patient/uni27E9.s1\nThe image is a /uni27E8.s1predicted scan type /uni27E9.s1 scan showing a \n/uni27E8.s1predicted body part /uni27E9.s1\n10 https:// huggi ngface. co/ openf lamin go/ OpenF lamin go- 3B- vitl- \nmpt1b.\n11 https:// huggi ngface. co/ sapie nzanlp/ Miner va- 3B- base- v1.0.\n12 https:// www. langc hain. com/.\n13 As in https:// huggi ngface. co/ spaces/ mteb/ leade rboard in June \n2024.\n I. Siragusa et al.\nrelated to the particular case reported in the Case section, \nsuch as Findings and Discussion, there is also more general \ninformation related to the disease, like Disease Discussion in \nthe Topic section (Figs. 3, 4)). Due to the inherent domain-\nspecific semantics of this kind of textual information, clas-\nsical RAG relying on vector databases is not efficient for \nprecise retrieval. As a consequence, we decided to build a \nKG, which relies on both case-specific and general medi-\ncal information that can be retrieved at inference time to \ngenerate a diagnostic suggestion for the provided input. The \nwhole process for creating the KG and inferring suggestions \nis depicted in Fig. 8\nTo allow efficient relation extraction for building the KG, \nclinical cases belonging to the training set were arranged as \ndocuments according to the following template: \n/uni27E8.s1U_id/uni27E9.s1 is a clinical report of a /uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient suffering from a /uni27E8.s1disease/uni27E9.s1 displayed in /uni27E8.s1scan modality /uni27E9.s1./uni27E8.s1clinical history of the patient/uni27E9.s1\nThe disease /uni27E8.s1disease name/uni27E9.s1 located in /uni27E8.s1speciﬁc body part /uni27E9.s1 \n( /uni27E8.s1body part /uni27E9.s1)./uni27E8.s1clinical history of the patient/uni27E9.s1\n/uni27E8.s1Treatment and followup recommended from the doctor /uni27E9.s1.\nAbout /uni27E8.s1disease/uni27E9.s1 we can say that: /uni27E8.s1disease discussion/uni27E9.s1.\nWe used LlamaIndex [43] to generate the KG. In particu-\nlar, the KG is built starting from document chunks whose \nfixed size is 4096 tokens. At most n  triplets were extracted \nfrom each chunk to form the nodes and edges of the KG. \nTriplets were extracted using the default LlamaIndex query \nposed to Llama 3.1 8B Instruct [12].\nLlama 3.1 8B Instruct was chosen since it is an open-\nsource and multi-lingual model trained with instruction-tun-\ning strategy. More in detail, instruction tuned models, have \nthe ability to provide a coherent output given a well-defined \ninstruction, optionally enriched via some examples [35, 44]. \nThis peculiar characteristics, made it suitable for both KG \ncreation and the subsequent answer generation: both tasks \nrely on the capabilities of an external LLM to output a coher-\nent answer leveraging the instruction. In addition, no further \ncostly fine-tuning is requested at this step. No other models \nwere considered at this step, since, at the time of developing \nthe model: this was the latest multilingual instruct model \navailable, trained with a Supervised Fine-Tuning Strategy \nwith Direct Preference Optimization [45] for human prefer-\nence alignment [12].\nWe created six different KGs according to the maximum \nnumber of triplets to be extracted from each chunk (3, 5, \n10) and the option to process the chunk in lowercase or not. \nThe hyperparameter setups are reported in Table  5 and an \nexample figure of the generated subgraph is shown in Fig. 9. \nThe entire process of creation and inference over the KGs \nwas run on a cluster with 2 NVIDIA A100 64 GB GPUs.\n5  Results\nIn this section are reported the results obtained with the \ndeveloped multimodal pipeline, where an end-to-end VLM \nis used for medical diagnosis support. The architecture of \nthis system is depicted in Fig.  10, and couples the origi-\nnal DR-Minerva model with the KG built as reported in \nSect. 4.3. The main contribution is to assess the effective \nusability of the presented data set for complex tasks such as \nthe proposed multimodal pipeline.\nPerformance evaluation was carried out by querying \nall the KGs with different hyperparameter choices, using \nprompts of increasing complexity. Specifically, three \nprompts were constructed, which were called simple, \nFig. 8  Overview of the overall \nprocess for KG creation and \ninference\nTable 5  Overview of the hyperparameter setups used to generate the \ndifferent versions of the KG\nSetup name Max rel. per \nchunk\nChunks in \nlower-case\n# nodes # edges\nKG-s1 10 no 10,043 15,470\nKG-s2 10 yes 10,706 16,179\nKG-s3 3 no 5306 7385\nKG-s4 3 yes 5172 6940\nKG-s5 5 no 6473 9300\nKG-s6 5 yes 6862 9521\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nmid-complex and complex respectively. All the prompts are \nreported as follows.\nPrompt simple:\nCan you tell me which diseases is most probable to be found in a \npatient having a \n/uni27E8.s1DR-Minerva output/uni27E9.s1?\nPrompt mid-complex:\nCan you tell me which diseases is most probable to be found in a /uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient according to a /uni27E8.s1DR-Minerva output/uni27E9.s1?\nPrompt complex:\nCan you tell me which diseases are the most probable to be found \nin a \n/uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient according to a /uni27E8.s1DR-Minerva output/uni27E9.s1 ? \nConsider also the following additional information about the \npatient. /uni27E8.s1history/uni27E9.s1.\nThe three prompts use the information provided by \nDR-Minerva, which consists in the scanning modality and \nthe body part shown in the input image. Prompts were \ndesigned to provide more and more information to the KG, \nas their complexity increases. The maximum complexity \nwill be reached in the last prompt, where we want to emu-\nlate the query that a doctor might pose to a diagnostic \nsupport system. In this case, information concerning the \npatient’s clinical \n/uni27E8.s1history/uni27E9.s1\n is used in addition to the infor -\nmation extrapolated from the images supplied to the mul-\ntimodal system.\nThe availability of a test set extrapolated from MedPix \n2.0 allowed us to generate a golden answer according the \nfollowing template.\nFig. 9  Overview of the subgraph generated from the node ‘Meningiomas’ in KG-s1\nFig. 10  Overview of the \ngenerated architecture with \nRAG-based Flamingo and KG \ncreation and inference\n\n I. Siragusa et al.\n /uni27E8.s1age/uni27E9.s1 /uni27E8.s1sex/uni27E9.s1 patient suffering from /uni27E8.s1disease/uni27E9.s1 . /uni27E8.s1disease discussion/uni27E9.s1.\nGolden answers were used for the subsequent perfor -\nmance evaluation phase. Several metrics exist in the litera-\nture for automatic evaluation of LLM-generated text with \nrespect to a set of reference or “golden” answers. These \nmetrics score the degree of overlap between the reference \nanswer r={r1 ,… ,rm } and the hypothesis h0{h1 ,… ,hn} \ngenerated by the model. There are two approaches in this \nrespect: character, word, or n-gram overlap and embed-\nding similarity. BLEU [46] and ROUGE [47] belong to \nthe first category. They are generally more tailored for \nevaluating Question-Answering and Machine Translation \nsystems, for which a well-defined answer is known. Such \nmeasures require the generated text h  to adhere strictly to \nthe golden answer r. Despite the validity of these metrics, \nare not meaningful for evaluation purposes. In this context, \na semantically comparison is required for the proposed \ntask, with other metrics.\nIn the process of generating clinical text to support a \ndoctor in the diagnostic process, the same meaning can \nbe expressed in a variety of forms that depend on many \nfactors. Some of these factors are the available knowledge \nabout the particular case when the diagnosis is formulated, \nthe different clinical practices in writing medical records, \nand so on. As a consequence, despite the generated golden \nanswers and the data set splits, we cannot expect the model \nto generate text exactly as the golden answer: multiple \ncorrect answers (as for the meaning) can be proposed with \ndifferent linguistic nuances.\nFor these reasons, metrics based on embedding similar -\nity scores like METEOR [ 48] and BERT score [ 49], are \npreferred. In this case the generic rj and h i are embeddings \nand not words or n-grams. In particular, we decided to \nreport in detail the BERT score in this paper and other \nmetrics are reported in the supplementary material. BERT \nscore is a neural framework to compare the embeddings \nof the tokens in r  and h  in terms of classical Precision, \nRecall, and F1 measure, which assess a compliant evalua-\ntion with the traditional metrics in AI field. In contrast to a \ntraditional classification performance evaluation approach, \nRecall, Precision and BERT’s F1 aim to evaluate the \ndegree of overlap of the generated response with respect to \nthe target response. In our specific case, this golden label \nis represented by the entire topic document containing \nthe clinical information. Consequently, the BERT metrics \nare well suited to evaluating a specific task of the model, \nwhich is to provide a verbose response to the doctor that \nis as close as possible to the correct clinical terminology. \nEquation  1 reports the BERTs score formulation.\nSim(rj, hi) is the cosine similarity between the j-th reference \ntoken embedding and the i-th hypothesis token embedding, \nrespectively. Table 6 shows the results obtained using all the \nthree prompts. We report the results with the other metrics \n(BLEU, ROUGE, METEOR) in the supplementary mate-\nrial. Here we only report BERT score since it is the most \nsignificant.\nThe results show that the prompt simple achieves best per-\nformance for the models using KG-s1 and KG-s2, which are \nthe KGs with the highest number of relations per chunk. The \nprompt complex performs best for the model using KG-s6, and \nit is close to the prompt simple for the models using KG-s3. No \nparticular prompt prevails over the others in the models using \nKG-s4 and KG-s5.\nGiven the structure of the KGs, one can argue that the \nsimple prompt does not constrain the model too much in the \ngeneration phase, and allows for the construction of verbose \nresponses whose context is controlled by the high number of \nconnected document chunks in the KG. On the other hand, \nwhen the KG connects less document chunks, the best perfor-\nmance of the prompt complex in terms of BERT-F1 depends \non the increased generation precision induced by the insertion \nof the patient \n/uni27E8.s1history/uni27E9.s1\n.\nAt inference time, Llama 3.1 8B Instruct is invoked for \nanswer generation by retrieving relevant information from the \nKGs, leveraging the given question. We used a low tempera-\nture of 0.00001, thus the generative capabilities of the model \nare limited, and then, it is forced to produce an answer as close \nas possible to the retrieved information. Moreover, to prevent \nthe model from generating loops, the no_repeat_ngram_\nsize parameter was set to 2. The query strategy over the \nKG was done using the tree_summarize response mode \nand a value equal to 5 as for similarity_top_k param-\neter, that is the default parameters for the retriever. We report \nfour end-to-end inferences using the better prompts and KGs, \nnamely simple prompt and KG-s1 (Fig.  11) and complex \nprompt and KG-s6 (Fig. 12). Both the image, the input prompt \nto the model, the generated answer and the BERT-F1 score \nare reported. \n(1)\nBERT-P = 1\nn\n∑n\ni=1 max\nrj\nSim(hi,rj)\nBERT-R = 1\nm\n∑m\nj=1 max\nhi\nSim(rj,hi)\nBERT-F1 = 2 ⋅ BERT-P ⋅BERT-R\nBERT-P +BERT-R .\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\nTable 6  Summary of BERT metrics calculated on all KG setups for each of the proposed prompts\nBold values indicate the best score for each KG and body part respectively\nSimple Mid Complex\nKG-setup Location BERT-Precision BERT-Recall BERT-F1 BERT-Precision BERT-Recall BERT-F1 BERT-Precision BERT-Recall BERT-F1\nKG-s1 Head 0.8027 0.7840 0.7932 0.7870 0.7848 0.7857 0.7971 0.7858 0.7913\nThorax 0.8003 0.7801 0.79 0.7853 0.7806 0.7827 0.7875 0.7822 0.7847\nSaM 0.7888 0.7932 0.791 0.7964 0.7851 0.7905 0.7846 0.7885 0.7864\nAbdomen 0.8008 0.7855 0.793 0.7859 0.7782 0.7818 0.7843 0.7817 0.7829\nRUS 0.8050 0.7886 0.7967 0.7774 0.7787 0.778 0.7884 0.7766 0.7823\nKG-s2 Head 0.8090 0.7900 0.7994 0.7985 0.7863 0.7921 0.7888 0.7802 0.7843\nThorax 0.8039 0.7851 0.7943 0.7981 0.7746 0.7856 0.7873 0.7821 0.7846\nSaM 0.7981 0.7950 0.7964 0.7888 0.7908 0.7898 0.7949 0.7860 0.7903\nAbdomen 0.7980 0.7845 0.7912 0.7882 0.7830 0.7853 0.7902 0.7863 0.7882\nRUS 0.7971 0.7850 0.791 0.8011 0.7817 0.7912 0.8063 0.7890 0.7975\nKG-s3 Head 0.7990 0.7855 0.7921 0.7991 0.7825 0.7905 0.8019 0.7872 0.7944\nThorax 0.7987 0.7840 0.7912 0.7885 0.7800 0.784 0.7858 0.7856 0.7855\nSaM 0.8058 0.7958 0.8007 0.7820 0.7906 0.7862 0.7993 0.7902 0.7947\nAbdomen 0.7960 0.7874 0.7916 0.7815 0.7742 0.7775 0.7844 0.7846 0.784\nRUS 0.8021 0.7799 0.7908 0.7972 0.7781 0.7874 0.7985 0.7878 0.7929\nKG-s4 Head 0.7989 0.7799 0.7891 0.7915 0.7829 0.787 0.7952 0.7843 0.7895\nThorax 0.8031 0.7784 0.7905 0.7869 0.7764 0.7815 0.7922 0.7822 0.7868\nSaM 0.8023 0.7934 0.7977 0.7870 0.7874 0.7871 0.8006 0.7878 0.7939\nAbdomen 0.7989 0.7769 0.7875 0.7975 0.7837 0.7905 0.7779 0.7765 0.7771\nRUS 0.7979 0.7819 0.7898 0.7753 0.7798 0.7774 0.7937 0.7819 0.7877\nKG-s5 Head 0.7999 0.7858 0.7927 0.7960 0.7876 0.7917 0.7945 0.7814 0.7877\nThorax 0.7960 0.7808 0.7882 0.7915 0.7769 0.784 0.7883 0.7827 0.7852\nSaM 0.8037 0.7953 0.7995 0.7888 0.7932 0.7908 0.8049 0.7925 0.7985\nAbdomen 0.8012 0.7772 0.7889 0.7840 0.7795 0.7816 0.8028 0.7820 0.7921\nRUS 0.7861 0.7860 0.786 0.7876 0.7871 0.7873 0.8025 0.7898 0.796\nKG-s6 Head 0.7910 0.7864 0.7885 0.7923 0.7801 0.786 0.7931 0.7799 0.7862\nThorax 0.7859 0.7812 0.7835 0.7720 0.7754 0.7735 0.7879 0.7808 0.7842\nSaM 0.7832 0.7893 0.7861 0.7957 0.7893 0.7924 0.8106 0.7918 0.8009\nAbdomen 0.7807 0.7816 0.7811 0.7958 0.7875 0.7916 0.7886 0.7801 0.7841\nRUS 0.7973 0.7826 0.7898 0.7849 0.7842 0.7845 0.7994 0.7886 0.7938\n I. Siragusa et al.\nFig. 11  One example for each body part of end-to-end queries to the entire system leveraging KG-s1 and simple prompt, along with inputs and \nthe final generated answer\nFig. 12  One example for each body part of end-to-end queries to the entire system leveraging KG-s6 and complex prompt, along with inputs and \nthe final generated answer\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\n6  Limitations\nMedPix 2.0 is proposed as a multimodal data set for train-\ning deep neural architectures. To demonstrate the robust-\nness and ease of implementing the data set, a model was \nproposed that combined a RAG-based Flamingo, Dr-\nMinerva, (created by the authors in a previous work) in \ncombination with Llama 3.1 for the generation of verbose \nresponses. As can be seen from the results obtained and \nevaluated in terms of BERT Precision, Recall and F1, the \nresult is robust enough to allow an initial experimental \nversion of a multimodal model to achieve good perfor -\nmance in generating a coherent verbose response. Despite \nthe promising results, the construction of a reliable model \nis not the main topic of the proposed work, which is why \nvalidation by doctors has not been carried out, a step \nthat is a fundamental in proposing a model to support \ndiagnosis. Despite the simplicity with which MedPix 2.0 \ncan be queried and used in its Image-only, Text-only and \nMultimodal versions, its integration with other data sets \ncan improve its structure, making it more complete and \nmaximizing its effectiveness in AI training tasks for diag-\nnostic support.\n7  Conclusions and Future Work\nIn this work we presented MedPix 2.0, a multimodal data \nset of clinical reports, CT and MR scans. We devised a \nsemi-automated pipeline to download and curate the \nimages in the original data sets, while structuring the tex-\ntual information as a set of JSON document collections \nthat had been used to build a proper MongoDB instance. \nThe NoSQL version of the data set can be accessed and \nqueried with a usable GUI that has been developed pur -\nposely. Using the GUI one can browse the data set in the \nsame manner as in the original data set, and can down-\nload the structured output of the query that is suitable for \ntraining AI models. In fact,  MedPix ® and its MongoDB \ninterface, represent in our view a relevant starting point for \nthe development of AI multimodal models in the medical \ndomain. such as Information Extraction systems tailored \nfor clinical reports, automated analysis of the medical \nimages, or Generative AI models for clinical report gen-\neration as part of a Medical Decision Support System. \nAll these systems can rely on MedPix 2.0 as a structured \nsource of data containing both clinical cases and medical \nexplanations about the disease under investigation.\nTo demonstrate this point, we recalled the architec-\nture of DR-Minerva, a RAG-based Flamingo architecture \nalready developed by the authors, and presented a brand-\nnew VLM architecture that couples DR-Minerva with \na KG created automatically relying on Llama 3.1 8B \nInstruct. Different KG structures were tested, starting from \nthe MedPix 2.0 test set along with prompts of increasing \ncomplexity as regards the context provided to the model. \nThe results we obtained are very satisfactory but we are \nactively working to improve the model's generation ability \nby means of a structural and semi-automatic organization \nof the KG, relying on external medical ontologies.\nThe scalability of the developed MongoDB database \nmakes it suitable for future extensions with the possibil-\nity to add brand new clinical cases or existing cases from \ndata set mentioned in Sect.  2, which have to be compliant \nwith privacy regulations and follow the required informa-\ntion structure. Moreover, the inherent distributed nature of \nMongoDB allows for creating huge databases across dif-\nferent wards where the data owned by a single institution \ndo not need to be explicitly moved out of the hospital thus \nviolating privacy regulations. The structure of MedPix 2.0 \ncould also serve as a guide to develop suitable connectors \nto share allowed data in the EHDS.\nNew cases can also be easily added to the KG, thus \nexpanding the clinical knowledge base used by the system \nin the inference phase. Further improvement is being done \non the GUI, providing the user with advanced data visu-\nalization tools like the possibility to interactively compare \nsimilar cases, thus helping physicians during the diagnos-\ntic phase.\nSupplementary information Tables with the results \nobtained from the experiments for the clinical cases for \nthe three prompts and for all KG setups are given in the \nsupplementary file.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s41019- 025- 00297-8.\nAcknowledgements This work is supported by the project \nB73C22000810001, project code ECS_00000022, “SAMOTHRACE” \n(Sicilian MicronanoTech Research And Innovation Center). Models are \nbuilt on the Leonardo supercomputer with the support of CINECA-\nItalian Super Computing Resource Allocation, class C project IscrC_\nDOC-VLM (HP10C64J82).\nAuthor Contributions All authors contributed to the study conception \nanddesign. Material preparation and data collection were performed \nby M.L.C., R.A.Experiments were performed by I.S. and S.C., they \nalso cured the first draft of themanuscript. Scientific supervision and \nreview was cured by R.P.\nFunding This research was funded by“SAMOTHRACE” (Sicil-\nian MicronanoTech Research And Innovation Center),project \nB73C22000810001, project code ECS 00000022.\nData and Code Availability  Source code and is freely available at \nhttps://github.com/CHILab1/MedPix-2.0.git. The data used for the test \nand train can be downloaded free of charge from the zenodo repository \nat the following link https://zenodo.org/records/12624810.\n I. Siragusa et al.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Penedo AC (2024) The regulation of data spaces under the EU \ndata strategy: towards the ‘actification’ of the fifth European free-\ndom for data? Eur J Law Technol 15(1)\n 2. Terzis P, Echeverria EOS (2023) Interoperability and govern-\nance in the European Health Data Space regulation. Med Law \nInt. https:// doi. org/ 10. 1177/ 09685 33223 11656 92\n 3. El Emam K, Rodgers S, Malin B (2015) Anonymising and sharing \nindividual patient data. The BMJ 350:1139. https:// doi. org/ 10.  \n1136/ bmj. h1139\n 4. Chevrier R, Foufi V, Gaudet-Blavignac C, Robert A, Lovis C \n(2019) Use and understanding of anonymization and de-identifi-\ncation in the biomedical literature: scoping review. J Med Intern \nRes 21(5):13484. https:// doi. org/ 10. 2196/ 13484\n 5. Siragusa I, Contino S, Pirrone R (2025) DR-Minerva: a multi -\nmodal language model based on Minerva for diagnostic informa-\ntion retrieval. In: AIxIA 2024—-advances in artificial intelligence. \nSpringer, Cham, pp 288–300\n 6. Orlando R, Moroni L, Cabot P-LH, Barba E, Conia S, Orlandini \nS, Fiameni G, Navigli R (2024) Minerva LLMs: the first family \nof large language models trained from scratch on Italian data. \nIn: Proceedings of the 10th Italian conference on computational \nlinguistics (CLiC-it 2024)\n 7. Alayrac J-B, Donahue J, Luc P, Miech A, Barr I, Hasson Y, Lenc \nK, Mensch A, Millican K, Reynolds M, Ring R, Rutherford E, \nCabi S, Han T, Gong Z, Samangooei S, Monteiro M, Menick J, \nBorgeaud S, Brock A, Nematzadeh A, Sharifzadeh S, Binkowski \nM, Barreira R, Vinyals O, Zisserman A, Simonyan K (2022) Fla-\nmingo: a visual language model for few-shot learning\n 8. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, Küt-\ntler H, Lewis M, Yih W-T, Rocktäschel T et al (2020) Retrieval-\naugmented generation for knowledge-intensive NLP tasks. Adv \nNeural Inf Process Systs\n 9. Wang C, Ong J, Wang C, Ong H, Cheng R, Ong D (2024) Poten-\ntial for GPT technology to optimize future clinical decision-mak-\ning using retrieval-augmented generation. Ann Biomed Eng\n 10. Ghanbari Haez S, Segala M, Bellan P, Magnolini S, Sanna L, \nConsolandi M, Dragoni M (2024) A retrieval-augmented genera-\ntion strategy to enhance medical chatbot reliability. In: Artificial \nintelligence in medicine\n 11. Wu J, Zhu J, Qi Y, Chen J, Xu M, Menolascina F, Grau V (2024) \nMedical graph RAG: towards safe medical large language model \nvia graph retrieval-augmented generation. arxiv: 2408. 04187\n 12. Llama Team A.M (2024) The Llama 3 herd of models. arXiv:  \n2407. 21783\n 13. Lin T-Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, \nDollár P, Zitnick CL (2014) Microsoft coco: common objects in \ncontext. In: Computer vision—ECCV 2014: 13th European con-\nference, Zurich, Switzerland, September 6–12, 2014, Proceedings, \nPart V 13. Springer, pp 740–755\n 14. Pelka O, Koitka S, Rückert J, Nensa F, Friedrich C.M (2018) \nRadiology objects in context (ROCO): a multimodal image data-\nset. In: Intravascular imaging and computer assisted stenting and \nlarge-scale annotation of biomedical data and expert label synthe-\nsis: 7th joint international workshop, CVII-STENT 2018 and third \ninternational workshop, LABELS 2018, held in conjunction with \nMICCAI 2018, Granada, Spain, September 16, 2018, Proceedings \n3. Springer, pp 180–189\n 15. Subramanian S, Wang LL, Mehta S, Bogin B, Zuylen M, Parasa \nS, Singh S, Gardner M, Hajishirzi H (2020) Medicat: a dataset of \nmedical images, captions, and textual references. arXiv preprint \narXiv: 2010. 06000\n 16. Lin W, Zhao Z, Zhang X, Wu C, Zhang Y, Wang Y, Xie W \n(2023) PMC-clip: contrastive language-image pre-training using \nbiomedical documents. In: International conference on medical \nimage computing and computer-assisted intervention. Springer, \npp 525–536\n 17. Lau JJ, Gayen S, Ben Abacha A, Demner-Fushman D (2018) \nA dataset of clinically generated visual questions and answers \nabout radiology images. Sci Data 5(1):1–10\n 18. He X, Zhang Y, Mou L, Xing E, Xie P (2020) Pathvqa: 30000+ \nquestions for medical visual question answering. arXiv preprint \narXiv: 2003. 10286\n 19. Johnson AE, Pollard TJ, Berkowitz SJ, Greenbaum NR, Lungren \nMP, Deng C-Y, Mark RG, Horng S (2019) MIMIC-CXR, a de-\nidentified publicly available database of chest radiographs with \nfree-text reports. Sci Data 6(1):317\n 20. Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, \nRodriguez L, Antani S, Thoma GR, McDonald CJ (2016) Pre-\nparing a collection of radiology examinations for distribution \nand retrieval. J Am Med Inf Assoc 23(2):304–310\n 21. Liu B, Zhan L-M, Xu L, Ma L, Yang Y, Wu X-M (2021) Slake: \na semantically-labeled knowledge-enhanced dataset for medical \nvisual question answering. In: 2021 IEEE 18th international \nsymposium on biomedical imaging (ISBI). IEEE, pp 1650–1654\n 22. Huang S-C, Huo Z, Steinberg E, Chiang C-C, Lungren MP, \nLanglotz CP, Yeung S, Shah NH, Fries JA (2023) INSPECT: \na multimodal dataset for pulmonary embolism diagnosis and \nprognosis. arXiv preprint arXiv: 2311. 10798\n 23. Chaudhry HAH, Renzulli R, Perlo D, Santinelli F, Tibaldi S, \nCristiano C, Grosso M, Limerutti G, Fiandrotti A, Grangetto M \net al (2022) Unitochest: a lung image dataset for segmentation \nof cancerous nodules on CT scans. In: International conference \non image analysis and processing. Springer, pp 185–196\n 24. ...Jack CR Jr, Bernstein MA, Fox NC, Thompson P, Alexander \nG, Harvey D, Borowski B, Britson PJ, Whitwell JL, Ward C, \nDale AM, Felmlee JP, Gunter JL, Hill DLG, Killiany R, Schuff  \nN, Fox-Bosetti S, Lin C, Studholme C, DeCarli CS, Krueger \nG, Ward HA, Metzger GJ, Scott KT, Mallozzi R, Blezek D, \nLevy J, Debbins JP, Fleisher AS, Albert M, Green R, Bartzokis \nG, Glover G, Mugler J, Weiner MW (2008) The Alzheimer’s \ndisease neuroimaging initiative (ADNI): MRI methods. J Magn \nReson Imaging 27(4):685–691. https:// doi. org/ 10. 1002/ jmri.  \n21049\n 25. Halling-Brown MD, Warren LM, Ward D, Lewis E, Mackenzie \nA, Wallis MG, Wilkinson LS, Given-Wilson RM, McAvinchey R, \nYoung KC (2021) OPTIMAM mammography image database: a \nlarge-scale resource of mammography images and clinical data. \nRadiol Artif Intell 3(1):200103. https:// doi. org/ 10. 1148/ ryai.  \n20202 00103\n 26. LaMontagne PJ, Benzinger TL, Morris JC, Keefe S, Hornbeck \nR, Xiong C, Grant E, Hassenstab J, Moulder K, Vlassenko AG, \nRaichle ME, Cruchaga C, Marcus D (2019) OASIS-3: longitudi-\nnal neuroimaging, clinical, and cognitive dataset for normal aging \nand Alzheimer disease. medRxiv. https:// doi. org/ 10. 1101/ 2019. 12. \n13. 19014 902\nMedPix 2.0: A Comprehensive Multimodal Biomedical Data Set for Advanced AI Applications with…\n 27. Schulz S, Ševa J, Rodriguez S, Ostendorff M, Rehm G (2020) \nNamed entities in medical case reports: corpus and experiments. \nIn: Proceedings of the twelfth language resources and evalua-\ntion conference. European Language Resources Association, \nMarseille, France, pp 4495–4500. https:// aclan tholo gy. org/ 2020. \nlrec-1. 553\n 28. Gava U, D’ Agata F, Bennink E, Tartaglione E, Perlo D, Vernone \nA, Bertolino F, Ficiarà E, Cicerale A, Pizzagalli F, Guiot C, \nGrangetto M, Bergui M. UniTOBrain. https:// doi. org/ 10. 21227/ \nx8ea- vh16\n 29. Magnini B, Altuna B, Lavelli A, Speranza M, Zanoli R (2020) The \nE3C project: collection and annotation of a multilingual corpus of \nclinical cases. In: Proceedings of the seventh Italian conference \non computational linguistics CLiC-it 2020\n 30. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, \nSastry G, Askell A, Mishkin P, Clark J, Krueger G, Sutskever I \n(2021) Learning transferable visual models from natural language \nsupervision\n 31. Awadalla A, Gao I, Gardner J, Hessel J, Hanafy Y, Zhu W, Mar-\nathe K, Bitton Y, Gadre S, Sagawa S, Jitsev J, Kornblith S, Koh \nPW, Ilharco G, Wortsman M, Schmidt L (2023) OpenFlamingo: \nan open-source framework for training large autoregressive \nvision-language models. arXiv preprint arXiv: 2308. 01390\n 32. Jiang X, Fang Y, Qiu R, Zhang H, Xu Y, Chen H, Zhang W, Zhang \nR, Fang Y, Chu X, Zhao J, Wang Y (2024) TC-RAG: turing-\ncomplete rag’s case study on medical LLM systems. arXiv: 2408. \n09199. https:// doi. org/ 10. 48550/ arXiv. 2408. 09199\n 33. Salemi A, Zamani H (2024) Comparing retrieval-augmentation \nand parameter-efficient fine-tuning for privacy-preserving person-\nalization of large language models. arXiv: 2409. 09510 [cs]. https:// \ndoi. org/ 10. 48550/ arXiv. 2409. 09510\n 34. Lee J, Ahn S, Kim D, Kim D (2024) Performance comparison \nof retrieval-augmented generation and fine-tuned large language \nmodels for construction safety management knowledge retrieval. \nAutom Constr 168:105846. https:// doi. org/ 10. 1016/j. autcon. 2024. \n105846\n 35. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, \nNeelakantan A, Shyam P, Sastry G, Askell A et al (2020) Lan -\nguage models are few-shot learners. Adv Neural Inf Process Syst\n 36. Peng B, Galley M, He P, Cheng H, Xie Y, Hu Y, Huang Q, Liden \nL, Yu Z, Chen W, Gao J (2023) Check your facts and try again: \nimproving large language models with external knowledge and \nautomated feedback. arXiv: 2302. 12813\n 37. Vidivelli S, Ramachandran M, Dharunbalaji A (2024) Efficiency-\ndriven custom chatbot development: unleashing LangChain, RAG, \nand performance-optimized LLM fusion. Mater Contin Comput\n 38. Douze M, Guzhva A, Deng C, Johnson J, Szilvasy G, Mazaré P-E, \nLomeli M, Hosseini L, Jégou H (2024) The Faiss library. arXiv:  \n2401. 08281\n 39. Junseong K, Seolhwa L, Jihoon K, Sangmo G, Yejin K, Minkyung \nC, Jy-yong S, Chanyeol C (2024) Linq-Embed-Mistral:Elevating \ntext retrieval with improved GPT data through task-specific con-\ntrol and quality refinement. Linq AI Research Blog. https:// getli  \nnq. com/ blog/ linq- embed- mistr al/\n 40. Muennighoff N, Tazi N, Magne L, Reimers N (2022) MTEB: \nmassive text embedding benchmark. arXiv preprint arXiv: 2210.  \n07316. https:// doi. org/ 10. 48550/ ARXIV. 2210. 07316\n 41. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: \nProceedings of the 31st international conference on neural infor -\nmation processing systems. NIPS’17. Curran Associates Inc., Red \nHook, NY, USA, pp 6000–6010\n 42. Jiang AQ, Sablayrolles A, Mensch A, Bamford C, Chaplot DS, \nCasas D, Bressand F, Lengyel G, Lample G, Saulnier L, Lavaud \nLR, Lachaux M-A, Stock P, Scao TL, Lavril T, Wang T, Lacroix \nT, Sayed WE (2023) Mistral 7B. arXiv: 2310. 06825\n 43. Liu J. LlamaIndex. https:// doi. org/ 10. 5281/ zenodo. 1234. https:// \ngithub. com/ jerry jliu/ llama_ index\n 44. Mialon G, Dessì R, Lomeli M, Nalmpantis C, Pasunuru R, Raile-\nanu R, Rozière B, Schick T, Dwivedi-Yu J, Celikyilmaz A, Grave \nE, LeCun Y, Scialom T (2023) Augmented language models: a \nsurvey. arXiv: 2302. 07842\n 45. Rafailov R, Sharma A, Mitchell E, Ermon S, Manning C.D, Finn \nC (2024) Direct preference optimization: your language model is \nsecretly a reward model. arXiv: 2305. 18290\n 46. Papineni K, Roukos S, Ward T, Zhu W-J (2002) BLEU: a method \nfor automatic evaluation of machine translation. In: Proceedings \nof the 40th annual meeting of the association for computational \nlinguistics, pp 311–318\n 47. Lin C-Y (2004) Rouge: a package for automatic evaluation of \nsummaries. In: Text summarization branches out, pp 74–81\n 48. Banerjee S, Lavie A (2005) METEOR: an automatic metric for \nMT evaluation with improved correlation with human judgments. \nIn: Proceedings of the ACL workshop on intrinsic and extrinsic \nevaluation measures for machine translation and/or summariza-\ntion, pp 65–72\n 49. Zhang T, Kishore V, Wu F, Weinberger KQ, Artzi Y (2020) \nBERTScore: evaluating text generation with BERT. In: Interna-\ntional conference on learning representations. https:// openr eview. \nnet/ forum? id= SkeHu CVFDr",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8413063287734985
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5324519872665405
    },
    {
      "name": "Knowledge graph",
      "score": 0.5189926028251648
    },
    {
      "name": "Data set",
      "score": 0.4467146396636963
    },
    {
      "name": "Data science",
      "score": 0.3993666172027588
    },
    {
      "name": "Information retrieval",
      "score": 0.3943558633327484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33261626958847046
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}