{
  "title": "How to write effective prompts for large language models",
  "url": "https://openalex.org/W4386688987",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097416678",
      "name": "Zhicheng Lin",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386101060",
    "https://openalex.org/W6604786921",
    "https://openalex.org/W6600466347",
    "https://openalex.org/W6612613465",
    "https://openalex.org/W4385291101",
    "https://openalex.org/W4387995139",
    "https://openalex.org/W4387262170",
    "https://openalex.org/W4389500040",
    "https://openalex.org/W4387355871",
    "https://openalex.org/W2768343520",
    "https://openalex.org/W4367049580"
  ],
  "abstract": "As large language models (LLMs) proliferate across research landscapes, effectively engaging with them becomes increasingly vital. This article presents a practical guide for understanding their capabilities and limitations, along with the art of crafting well-structured queries, to extract maximum utility from these AI tools.",
  "full_text": " 1 \n 1 \nLin, Z. (in press). How to write effective prompts for large language models. Nature Human 2 \nBehaviour 3 \n 4 \nHow to write effective prompts for large language models 5 \n 6 \n 7 \n 8 \nZhicheng Lin 9 \nDepartment of Psychology,  10 \nUniversity of Science and Technology of China 11 \n 12 \n 13 \n 14 \nAuthor Note 15 \nCorrespondence should be addressed to Zhicheng Lin (Email: zhichenglin@gmail.com; 16 \nX/Twitter: ZLinPsy; address: No. 96 Jinzhai Road Baohe District, Hefei, Anhui, 230026, China). 17 \n 18 \nAcknowledgments: The writing was supported by the National Key R&D Program of China 19 \n(STI2030-Major Projects+2021ZD0204200), National Natural Science Foundation of China 20 \n(32071045), and Shenzhen Fundamental Research Program (JCYJ20210324134603010). I used 21 \nGPT-4 and Claude 2.0 to proofread the manuscript. 22 \n 23 \n 24 \n 25 \n 26 \n 27 \n 28 \n 29 \n 30 \n 31 \n 32 \n 33 \n 34 \n 35 \n 36 \n 37 \n 38 \n 39 \n 40 \n 41 \n 42 \n 43 \n 44 \n 2 \nStandfirst 45 \nAs large language models (LLMs) proliferate across research landscapes, effectively engaging 46 \nwith them becomes increasingly vital. This article presents a practical guide for understanding 47 \ntheir capabilities and limitations, along with the art of crafting well-structured queries, to extract 48 \nmaximum utility from these AI tools. 49 \n 50 \nWhy large language models (LLMs) 51 \nLLMs employ deep learning—a subset of artificial intelligence (AI) that emulates the neural 52 \nnetworks of the human brain—to generate human-like text in response to user queries, or 53 \n“prompts” (refer to Box 1 for a glossary and Box 2 for an LLM primer). Unlike traditional 54 \nsoftware that relies on predetermined algorithms, LLMs are able to interpret natural language 55 \ncommands and excel in tasks ranging from rudimentary sentence completion to intricate 56 \nproblem-solving, making them user-friendly and versatile. Training of LLMs has recently 57 \nexpanded to include images, videos, and audio; these models are sometimes referred to as large 58 \nmultimodal models (LMMs) or multimodal language models. Within a short span of time, LLMs 59 \nhave become a ubiquitous presence in the technological landscape, permeating diverse 60 \ndisciplines1. In writing, coding, and visualization—they are rapidly becoming indispensable for 61 \nboth professionals and academic researchers (for a comparison of leading LLMs, see Table 1)2-4. 62 \n 63 \nWhy prompts 64 \nInteracting with LLMs may seem deceptively simple: just type a question and get an instant 65 \nanswer! However, effective engagement with these models proves to be more challenging and 66 \nnuanced than it initially seems5. This imposes a substantial limitation on the utility of LLMs, as 67 \nthe quality of their output is directly tied to the quality of the prompts given—a consideration 68 \noften overlooked in current discussions about the utility and capacities of LLMs. A well-crafted 69 \nprompt can lead to a precise, accurate, and relevant response, thereby maximizing the model’s 70 \nperformance. Conversely, a poorly structured one may result in a vague, incorrect, or irrelevant 71 \nanswer.  72 \n 73 \nThis limitation stems in large part from the inherent nature of LLMs. Despite their sophisticated 74 \nalgorithms and voluminous training data (encompassing materials such as web pages, Wikipedia 75 \narticles, social media posts, academic articles, books, and instruction data), they are 76 \nmathematical models and lack an understanding of the world. These tools are designed to predict 77 \nthe likelihood of text, not to produce the truth. They predict text using statistical likelihood by 78 \nharnessing pattern recognition within the training data (“next-most-likely token”; see Box 2). 79 \nThe type of text generated depends heavily on the patterns of text found within the training data. 80 \nFurther, since each token prediction influences subsequent ones, a misstep early in the response 81 \ncan lead to cascading inaccuracies. A well-structured prompt therefore not only increases the 82 \nlikelihood of each token being accurately predicted but also minimizes the compounding effect 83 \nof errors. Another key factor contributing to the importance of prompts is the ability of LLMs for 84 \nin-context learning. This ability allows models to adapt temporarily to the prompts they receive, 85 \nrendering these prompts crucial for conveying contextual information.  86 \n 87 \nThus, mastering the art and science of formulating effective prompts—sometimes termed 88 \n“prompt engineering”—is essential for leveraging the capabilities of LLMs. Achieving optimal 89 \nresults requires a blend of domain-specific knowledge, model understanding, and skills, which 90 \n 3 \nmust be honed through learning and experience. Thus, the first and foremost recommendation is 91 \nto play with the models. The more we interact with a model, the better we’ll understand its 92 \nnuances and how it could help with our needs. This article outlines actionable strategies and rules 93 \nas well as their rationales to establish the foundation for mastery (Table 2).  94 \n 95 \nGuide the model to solutions 96 \nLLMs lack semantic understanding, making generalization beyond their training difficult. 97 \nHowever, their vast parameterization grants them an expensive ‘memory’ (that is, a back-98 \ncatalogue of data to draw upon) derived from their training data (‘long term memory’) and 99 \ncurrent textual context (‘working memory’, as provided in prompts and interaction history; see 100 \n“context window” in Table 1). This duality—restricted generalization but prodigious 101 \nmemorization—empowers LLMs to effectively handle complex tasks when decomposed into 102 \nsmaller tasks and steps.  103 \n 104 \nFor instance, rather than a broad command like “Translate the text into Chinese,” consider 105 \nbreaking it down into two steps: “First translate literally to preserve meaning, then refine the 106 \ntranslation to align with Chinese linguistic conventions.” Similarly, instead of demanding a 107 \n1000-word essay outright, decompose the task into subtasks—crafting the introduction, 108 \nconclusion, and central arguments with specific instructions.  109 \n 110 \nClear, incremental instructions reduce ambiguity and uncertainty for more accurate responses. 111 \nBy simplifying broad tasks into smaller, sequential components, this strategy productively 112 \nleverages the robust memorization of LLMs while compensating for their limited abstractions 113 \nthrough structured guidance. 114 \n 115 \nAdd relevant context 116 \nLLMs have a much larger ‘working memory’ than humans do. Thus, to elicit nuanced, 117 \ncontextually accurate responses, it’s crucial to provide relevant contexts as input. A well-framed 118 \nquery should: 119 \n 120 \n• Embed specifics: Root your query in specific details to guide the LLM toward a more 121 \naccurate, relevant interpretation. Thus, instead of asking it to draft a generic cover letter, 122 \nprovide it with the specific job ad and your CV to add relevant context.  123 \n• Prioritize evidence: Ground your interactions in relevant factual information. Rather 124 \nthan asking the model about the best way to achieve eternal happiness, provide it with a 125 \npeer-reviewed study and ask it questions based on those findings. 126 \n 127 \nThe aim is not to inundate the LLM with general knowledge but to prime it with the 128 \nparticularities pertinent to your inquiry. When queries brim with relevant details, LLMs generate 129 \nmore insightful, nuanced responses. 130 \n 131 \nBe explicit in your instructions 132 \nTo get your favorite drink, you don’t walk into a random coffee shop with the order “A cup of 133 \ncoffee, please!” Don’t expect LLMs to read your mind either. Although there is delight in having 134 \nLLMs correctly guessing your intent or even surpassing your expectations, imprecise requests 135 \nrisk off-target responses as the LLM grasps for intent. Clarity is key.  136 \n 4 \n 137 \nTo reduce uncertainty in model prediction, specify exactly what you want. Instead of “Revise the 138 \ntext,” use a more explicit instruction, by considering: What stylistic approach are you aiming 139 \nfor? Who is your intended audience? Do you have a particular focus, like clarity or brevity? Thus 140 \na more concrete instruction is “Act as a top editor for top journals to improve the clarity and flow 141 \nof the text.” 142 \n 143 \nAnother example: rather than asking for suggestions for a name, be more explicit by adding 144 \nconstraints—“The name must start with a verb and the implicit subject/actor is the user.” Hence 145 \nthe rule: try to be explicit in stating the task, its objectives, the desired emphasis, and any 146 \nconstraints. Vague requests lead to vague responses. Explicit instructions help: 147 \n 148 \n• Minimize ambiguity about the instructions and the text to be treated (e.g., using 149 \ndelimiters such as specific labels, characters, or symbols). 150 \n• Enable the LLM to concentrate capabilities on your specific needs. 151 \n• Provide clear criteria to judge the model’s accuracy. 152 \n 153 \nWhile LLMs are designed for conversational refinement, explicit instructions can streamline the 154 \nprocess by clearly declaring your aims upfront. Steer the dialogue by articulating your purpose 155 \nand constraints. At the same time, avoid over-specification when objectives are not yet fully 156 \ndefined, as it might lead to incorrect paths or miss unexpected, better responses. 157 \n 158 \nAsk for lots of options 159 \nA particular strength of LLMs is its enormous ‘long term memory.’ To exploit the potential of 160 \nLLMs, ask for an array of options rather than a single suggestion. Therefore, request three 161 \nanalogies to explain a concept, five ideas to begin the introduction, 10 alternatives to replace the 162 \nfinal paragraph, 20 names to name a function—let the model provide you with food for thought 163 \nand you choose from the buffet. In addition to requesting multiple options, consider regenerating 164 \nresponses multiple times using the same prompt. Regenerating responses can achieve more 165 \ndiversity in responses and improve their quality. Requesting multiple options and regenerating 166 \nresponses offer several advantages: 167 \n 168 \n• Encourage the model to explore multiple avenues, enhancing the creativity and diversity 169 \nof the output. 170 \n• Provide you with a comprehensive set of options, minimizing the risk of settling for a 171 \nsuboptimal or biased suggestion. 172 \n• Facilitate iterative refinement. 173 \n 174 \nTreat LLMs as a versatile ideation partner. Asking for plentiful options, from myriad angles, 175 \nenriches your decision process. Abundant choice unlocks maximal utility. 176 \n 177 \nAssign characters 178 \nThe enormous training datasets mean that LLMs are capable of simulating various roles to offer 179 \nspecialized feedback or unique perspectives. Instead of asking for generic advice or information, 180 \nconsider instructing the model to role-play. Ask it to act as a typical reader of your audience to 181 \nprovide feedback on the writing, as a writing coach to help revise the manuscript, as a Tibetan 182 \n 5 \nscientist specialized in human physiology to explain the impact of high altitudes, as a sentient 183 \ncheesecake explaining in cheesecake analogies—the possibilities are endless. Assigning 184 \ncharacters provides several benefits: 185 \n 186 \n• Contextualize the model’s responses, making them more relevant to your specific needs.  187 \n• Allow for a more interactive and engaging dialogue with the model.  188 \n• Yield more nuanced and specialized information, enhancing the quality of the output.  189 \n• Provide a creative approach to problem-solving, encouraging out-of-the-box thinking. 190 \n 191 \nPersonas provide framing to yield responses from unique vantage points. By leveraging the role-192 \nplaying capabilities of LLMs, you can obtain more targeted and contextually appropriate 193 \nresponses—and have more fun in the process. 194 \n 195 \nShow examples, don’t just tell 196 \nLLMs are adept at few-shot learning—learning from examples6. A particularly effective 197 \napproach then is to embody your intent with concrete examples. Rather than a vague “Create a 198 \nchart for this data,” provide an example: “Create a bar chart for this data, similar to the one in 199 \nFigure 3 of the attached paper.” Just as showing a picture to a hairstylist is far superior to trying 200 \nto describe your desired haircut, providing explicit examples—whether it’s a code snippet for a 201 \nprogramming query or a sample sentence for a writing task—serves as an invaluable guide for 202 \nthe model. By providing a tangible reference, you accomplish several objectives: 203 \n 204 \n• Clarify the context, enabling the LLM to better grasp the nuances of your request.  205 \n• Reduce the number of iterations needed to achieve the desired output.  206 \n• Offer a benchmark against which to evaluate the model’s output. 207 \n 208 \nExamples act as a roadmap for the LLM, guiding it toward generating responses that are closely 209 \naligned with your expectations. Consider supplementing your instructions with illustrative 210 \nexamples to catalyze performance. 211 \n 212 \nDeclare preferred response format 213 \nLLMs tend to be verbose. Specifying your desired formatting—bullet points, reading level, tone, 214 \nand so on—helps constrain the possible outputs, improving relevance. For example, instead of 215 \n“Summarize the key findings,” declare the response format: “Summarize the key findings in 216 \nbullet points and use language a high school student would understand.” Declaring format 217 \nupfront also provides clear criteria for evaluating LLM performance. Some options: 218 \n 219 \n• Bullet points for summarizing concisely. 220 \n• Casual tone for accessibility. 221 \n• Code comments for documentation. 222 \n• Restrict response length for conciseness or reading level for comprehension. 223 \n 224 \nDeclaring your preferred format sets clear expectations to streamline prompting. Constraints 225 \nfoster relevance. 226 \n 227 \nExperiment, experiment, experiment 228 \n 6 \nEffective prompting is not formulaic; small tweaks can sometimes yield dramatic, surprising 229 \ndifferences. Consider the following examples.  230 \n 231 \n• Example one: Across a range of reasoning tasks, simply adding the instruction “let’s 232 \nthink step by step” to the prompt in GPT-3 leads to much-improved performance—a 233 \nform of Chain of Thought (CoT) prompting7.  234 \n• Example two: LLMs can respond to emotional information. Adding phrases like “Take a 235 \ndeep breath—this is very important for my career” or “I will tip $200 for great responses” 236 \ncan increase the quality of responses according to one non-peer-reviewed preprint8. 237 \n• Example three: Non-peer-reviewed research suggests that performance in complex 238 \ncoding tasks can be improved by adding “Identify core concepts in the problem and 239 \nprovide a tutorial” and “Recall three relevant and distinct problems” in the prompt—a 240 \nform of analogical prompting9. 241 \n• Example four: While LLMs may falter in direct queries involving complex calculations, 242 \nthey shine in generating functional computer code that solves the same problems (e.g., 243 \n“Write Python code to solve it”)10.  244 \n 245 \nThese cases demonstrate just how sensitive LLMs are to the prompts. You must therefore 246 \nexperiment, experiment, experiment! Productive use of LLMs requires ongoing, creative 247 \nexperimentation. Consider: 248 \n 249 \n• Vary phrasings, lengths, specificity, and constraints. 250 \n• Toggle between different examples, contexts, and instructions. 251 \n• Attempt both conversational and concise declarative prompts. 252 \n• Try the same prompts on different LLMs. 253 \n 254 \nTreat prompts as testable hypotheses. Use results to inform iterations. Not all attempts will 255 \nsucceed, but evidence accrues with each. With tenacity, optimal results will emerge. 256 \n 257 \nConclusion 258 \nLLMs stand as unparalleled partners in the realm of natural language tasks. This article presents 259 \nactionable strategies and their rationales for crafting effective prompts to unlock the full potential 260 \nof LLMs. Central themes encompass the importance of breaking complex tasks into subtasks and 261 \nstructured steps; framing with relevant details; explicitly declaring aims; and illustrating intent 262 \nthrough examples. Additional recommendations include: assigning personas and requesting 263 \ndiverse options to tap into the versatility of LLMs; specifying format to set expectations; and 264 \nengaging in continuous experimentation for optimal outcomes. Combined, these strategies help 265 \nusers create elaborate, structured prompts that can most effectively tackle specific tasks. While 266 \nthe skill of effective prompting may not lead to eternal happiness, it promises to pay increasing 267 \ndividends in productivity and joy.  268 \n 269 \n 270 \n 271 \n 272 \n 273 \n 274 \n 7 \nReferences  275 \n 276 \n1 Lin, Z. Towards an AI policy framework in scholarly publishing. Trends Cogn. Sci. (in 277 \npress). 278 \n2 Lin, Z. Why and how to embrace AI such as ChatGPT in your academic life. Royal 279 \nSociety Open Science 10, 230658, doi:10.1098/rsos.230658 (2023). 280 \n3 Merow, C., Serra-Diaz, J. M., Enquist, B. J. & Wilson, A. M. AI chatbots can boost 281 \nscientific coding. Nat Ecol Evol 7, 960-962, doi:10.1038/s41559-023-02063-3 (2023). 282 \n4 Lin, Z. Supercharging academic writing with generative AI: framework, techniques, and 283 \ncaveats. Nature Biomedical Engineering, 2310.17143 (in press). 284 \n5 Zamfirescu-Pereira, J. D., Wong, R. Y., Hartmann, B. & Yang, Q. in Proceedings of the 285 \n2023 CHI Conference on Human Factors in Computing Systems    1-21 (2023). 286 \n6 Brown, T. et al. in Advances in Neural Information Processing Systems Vol. 33   1877-287 \n1901 (2020). 288 \n7 Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. in Advances in Neural 289 \nInformation Processing Systems Vol. 35   22199-22213 (2022). 290 \n8 Li, C. et al. Large language models understand and can be enhanced by emotional 291 \nstimuli. arXiv, 2307.11760 (2023). 292 \n9 Yasunaga, M. et al. Large language models as analogical reasoners. arXiv, 2310.01714 293 \n(2023). 294 \n10 Li, C. et al. Chain of code: Reasoning with a language model-augmented code emulator. 295 \narXiv, 2310.01714 (2023). 296 \n 297 \n 298 \n 299 \nCompeting Interests 300 \nThe author declares no competing interests. 301 \n 302 \n 303 \n 304 \n 305 \n 306 \n 307 \n 308 \n 309 \n 310 \n 311 \n 312 \n 313 \n 314 \n 315 \n 316 \n 317 \n 318 \n 319 \n 320 \n 8 \nTable 1. Comparisons of major LLMs as of January 12, 2024 321 \nFeature GPT-\n3.5/GPT-4 \n(ChatGPT by \nOpenAI; Bing \nby Microsoft) \nClaude 2.1 \n(by \nAnthropic) \nPaLM 2, \nGemini \nPro \n(Bard by \nGoogle \nAI) \nLLaMA \n2 (by \nMeta AI) \nSupport for file \ninput \nText and \nimage files* \nText files Image \nfiles \nImage \nfiles \nContext \nwindow (in \ntokens) \n4,096/8,192 \n(up to 128K \nfor GPT-4 \nTurbo) \n200K 32K 4,096 (up \nto \n32K) \nInternet access ✔* No ✔ No \nEditable \nprompts \nafter \nexecution \n✔ No ✔ No \nSupport for \nplugin or \nextension \n✔ No ✔ No \nSupport for \ncustomized \nversion \n✔ (GPT)* No No No \nAvailability of \nglobal \nsettings \n✔ No No ✔ \nChat-history \nsharing \n✔ No ✔ No \nSubscription \nrequirement \nGPT-4 only No No No \nOpen-source \nstatus \nNo No No ✔#  \nNote: Access to these LLMs may be restricted in certain regions. Additionally, some features 322 \nmay not be available in all regions or for all users. 323 \n 324 \n*GPT-4 only 325 \n#With restrictive clauses (not permitted for training other language models; special license 326 \nrequired for large-scale applications) 327 \n  328 \nChatGPT: https://chat.openai.com 329 \nClaude: https://claude.ai 330 \nBard: https://www.google.com/bard 331 \nLLaMA: https://www.llama2.ai 332 \n 333 \n 334 \n 9 \nTable 2. Prompting strategies and examples 335 \n 336 \nStrategy Example \nGuide the model to solutions Instead of “Translate the text into \nChinese,” use “First translate literally \nto preserve meaning, then refine the \ntranslation to align with Chinese \nlinguistic conventions.” \nAdd relevant context Instead of a generic cover letter, provide \nthe job ad and your CV for context. \nBe explicit in your instructions Instead of  “Revise the text,” use “Act as \na top editor for top journals to improve \nthe clarity and flow of the text.” \nAsk for lots of options Ask for three analogies to explain a \nconcept or 20 names for a function. \nAssign characters Instruct the model to act as a typical \nreader of your audience for feedback \non writing. \nShow examples, don’t just tell Instead of “Create a chart for this data,” \nsay “Create a bar chart for this data, \nsimilar to Figure 3 in the attached \npaper.” \nDeclare preferred response format Instead of “Summarize the key findings,” \nspecify “Summarize the key findings in \nbullet points and use language a high \nschool student would understand.” \nExperiment, experiment, experiment Add specific phrases like “let’s think step \nby step” to prompts to improve LLM \nperformance. \n 337 \n 338 \n 339 \n 340 \n 341 \n 342 \n 343 \n 344 \n 345 \n 346 \n 347 \n 348 \n 349 \n 350 \n 351 \n 352 \n 353 \n 10 \nBox 1. Glossary 354 \n 355 \nApplication programming interface (API): A set of rules for software entities to communicate 356 \nwith each other.  357 \nArtificial intelligence (AI): The simulation of human intelligence in machines.  358 \nBiases: Prejudices in machine learning models that arise from the data they are trained on.  359 \nChain of Thought (CoT) prompting: A technique to improve the model’s reasoning 360 \ncapabilities by adding specific instructions like “let’s think step by step”.  361 \nContext window or length: The maximum number of tokens that a model can consider from the 362 \nconversation history.  363 \nDeep learning: A subfield of AI that mimics the neural networks of the human brain to analyze 364 \ndata.  365 \nDelimiters: Symbols or characters used to indicate the beginning or end of some data/text that 366 \nLLMs should operate on. 367 \nHallucination: Incorrect or misleading statements generated by LLMs.  368 \nLarge language models (LLMs): Machine learning models trained on vast datasets to perform 369 \nlanguage-based tasks.  370 \nNeural networks: Computational models inspired by the human brain, used in machine learning 371 \nalgorithms to solve complex problems.  372 \nParameter: An adjustable weight representing the strengths of connections between artificial 373 \nneurons in the neural network.  374 \nPrompt: A user query or instruction that triggers a response from an LLM.  375 \nPrompt engineering: The art and science of crafting effective prompts to interact with LLMs.  376 \nPrompt injection: Malicious instructions embedded within the prompts to make the model 377 \nperform unintended actions.  378 \nReinforcement learning with human feedback (RLHF): A type of machine learning where 379 \nmodels learn to make decisions based on real-time feedback provided by human evaluators.  380 \nSelf-attention: A mechanism in transformers that evaluates the relevance of different segments 381 \nof input text.  382 \nToken: The unit of text that is processed by an LLM.  383 \nTransformers: A type of deep learning architecture designed to handle sequential data. 384 \n 385 \n 386 \n 387 \n 388 \n 389 \n 390 \n 391 \n 392 \n 393 \n 394 \n 395 \n 396 \n 397 \n 398 \n 399 \n 11 \nBox 2. Understanding and using LLMs 400 \n 401 \nDevelopers first train LLMs using deep learning. They use a deep learning architecture called 402 \ntransformers, which is specifically designed to handle sequential data. A key feature of 403 \ntransformers is self-attention, a mechanism that allows the model to evaluate the relevance of 404 \ndifferent segments of the input text. Through this process, the models ‘learn’ from extensive 405 \ndatasets, including web pages, academic articles, and books.  406 \n 407 \nTransformers break down text into smaller units called tokens, which can be as small as a 408 \ncharacter or as large as a word. They convert these tokens into numerical values, serving as the 409 \nmodel’s input. Inside the model, there are a large number of adjustable weights, commonly 410 \nreferred to as parameters. These parameters represent the strengths of connections between 411 \nartificial neurons in the model’s neural network architecture. The initial training fine-tunes these 412 \nparameters to capture complex linguistic patterns. Since extensive language datasets are used for 413 \nthis training, these models generate text that is remarkably similar to human language.  414 \n 415 \nAfter this initial training, LLMs are further refined by human feedback. Human evaluators 416 \nprovide real-time feedback on the model’s responses based on the accuracy and relevance of its 417 \noutput. This is known as reinforcement learning, as the LLM adjusts its parameters in response 418 \nto positive or negative feedback. This means that the model becomes more aligned with human 419 \nvalues and expectations.  420 \n 421 \nSince the publication of the transformers architecture in 2017, several leading LLMs have been 422 \nmade accessible via web-based conversational interfaces (see Table 1 for a comparison of four 423 \nleading LLMs). For those seeking more flexible integration, API (application programming 424 \ninterface) access is often available, allowing users to send HTTP requests to the LLM providers’ 425 \nservers to incorporate the models into various applications and services, such as creative writing 426 \nplatforms or translation services. Using the API also allows for more configurations of the 427 \nmodel, such as adjusting the randomness of the responses by setting the temperature parameter. 428 \nSome providers require a subscription or use a pay-per-use model. 429 \n 430 \nWhen choosing an LLM, consider the specific tasks you require the model for, its availability in 431 \nyour region, and your personal preferences (see Table 1). Build intuition by experimenting with 432 \ndifferent LLMs on various tasks and comparing their outputs. As LLMs are continually being 433 \nimproved, it pays to stay updated with release notes and new features.  434 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5986687541007996
    },
    {
      "name": "Natural language processing",
      "score": 0.4700288474559784
    },
    {
      "name": "Linguistics",
      "score": 0.3523273468017578
    },
    {
      "name": "Psychology",
      "score": 0.3205024003982544
    },
    {
      "name": "Philosophy",
      "score": 0.05444559454917908
    }
  ]
}