{
  "title": "Self-Consistency of Large Language Models under Ambiguity",
  "url": "https://openalex.org/W4389518275",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2986814733",
      "name": "Henning Bartsch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1953970931",
      "name": "Ole Jørgensen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2584954454",
      "name": "Domenic Rosati",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5076400003",
      "name": "Jason Hoelscher-Obermaier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2950795968",
      "name": "Jacob Pfau",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4312107542",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4384648622",
    "https://openalex.org/W4389524313",
    "https://openalex.org/W4309043734",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W4281748205",
    "https://openalex.org/W4383175775",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4396815653",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency–e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model’s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
  "full_text": "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 89–105\nDecember 7, 2023. ©2023 Association for Computational Linguistics\n89\nSelf-Consistency of Large Language Models under Ambiguity\nHenning Bartsch*\nIndependent Researcher\nbartsch.henning@gmail.com\nOle Jorgensen*\nImperial College London\nokj22@ic.ac.uk\nDomenic Rosati*\nDalhousie University\ndomenic.rosati@dal.ca\nJason Hoelscher-Obermaier\nPIBBSS fellow\njason.hoelscherobermaier@gmail.com\nJacob Pfau\nNew York University\njp6263@nyu.edu\nAbstract\nLarge language models (LLMs) that do not give\nconsistent answers across contexts are prob-\nlematic when used for tasks with expectations\nof consistency–e.g. question-answering, ex-\nplanations, etc. Our work presents an evalua-\ntion benchmark for self-consistency in cases\nof under-specification where two or more an-\nswers can be correct. We conduct a series of\nbehavioral experiments on the OpenAI model\nsuite using an ambiguous integer sequence\ncompletion task. We find that average con-\nsistency ranges from 67% to 82%, far higher\nthan would be predicted if a model’s consis-\ntency was random, and increases as model\ncapability improves. Furthermore, we show\nthat models tend to maintain self-consistency\nacross a series of robustness checks, includ-\ning prompting speaker changes and sequence\nlength changes. These results suggest that self-\nconsistency arises as an emergent capability\nwithout specifically training for it. Despite this,\nwe find that models are uncalibrated when judg-\ning their own consistency, with models display-\ning both over- and under-confidence. We also\npropose a nonparametric test for determining\nfrom token output distribution whether a model\nassigns non-trivial probability to alternative an-\nswers. Using this test, we find that despite\nincreases in self-consistency, models usually\nplace significant weight on alternative, incon-\nsistent answers. This distribution of probability\nmass provides evidence that even highly self-\nconsistent models internally compute multiple\npossible responses.\n1 Introduction\nLanguage model pre-training approximates a dis-\ntribution generated by many speakers. As a re-\nsult, LLMs learn to express inconsistent beliefs\ndrawn from distinct groups of people (Santurkar\net al., 2023). Recent work has investigated the\nconsistency of LLMs variously as: a logical valid-\nity check on model claims (Fluri et al., 2023), an\nexplanatory validity check on the simulatability of\nmodels’ explanations (Chen et al., 2023), and a tool\nto identify LLMs representations of truth (Burns\net al., 2023). All of these works rest to some degree\non the contention that fine-tuned LLMs can be un-\nderstood as holding beliefs, an assumption which\nhas recently come under scrutiny (Levinstein and\nHerrmann, 2023).\nConsistency is particularly of interest in cases\nof ambiguity. Recent work has evaluated LLMs’\nability to identify linguistic and classification-task\nambiguity (Liu et al., 2023; Tamkin et al., 2023).\nOur work brings together these threads of research,\nexamining how model explanations can be exam-\nined via self-consistency checks.\nWe offer a case study on ambiguity in an arith-\nmetical setting. We ask language models from\nOpenAI for a continuation of an integer sequence\nhaving multiple possible continuations. We then\nseparately ask the models for the formula that gen-\nerated the initial sequence, which we refer to as the\nexplanation. Finally, we evaluate whether model-\ngenerated continuations are consistent with model-\ngenerated explanations (§3). We present the model\nwith the full set of sequence generating functions\nso that ambiguity is, in principle, recognizable by\nthe model.\nWe find the following across evaluations\nusing davinci (GPT-3), text-davinci-003,\ngpt-3.5-turbo, and gpt-4:\n1. Models (with greedy decoding) improve in\ncross-context consistency rapidly with increas-\ning scale and capabilities (§4). This holds\nacross prompting strategies and data perturba-\ntions (§4.1).\n2. Models are not well-calibrated and incapable\nof self-assessing the consistency of their own\nanswers (Figure 1).\n3. Even a model ( text-davinci-003) that\n90\nchooses relatively consistently among sev-\neral correct answers across contexts still as-\nsigns non-trivial probability to other correct\nanswers (§5).\n4. Models can generally verbalize alternative an-\nswers in cases of ambiguity, but there is no\nclear effect of capability increase on this ver-\nbalization task (§5.2).\n2 Dataset: Ambiguous Integer Sequences\nIn order to evaluate self-consistency, we created\nand open-sourced a dataset of ambiguous integer\nsequences.1. Integer sequences were chosen be-\ncause we can readily identify sequences that have\nmultiple valid completions. This allows us to in-\ntroduce tasks with ambiguity for measuring proper-\nties like model self-consistency. Previous work on\nself-consistency considered open-ended question\nanswering or knowledge probing (Raj et al., 2022;\nElazar et al., 2021) which makes measuring con-\nsistency difficult (rendering unclear the space of\npossible answers, and what constitutes distinct an-\nswers), whereas in our setting the space of possible\nanswers is rigorously defined via an enumeration\nof generation functions.\nOur dataset was created as follows: We gen-\nerate integer sequences, e.g., 7, 11, 15, drawn\nfrom a fixed set of generating functions, e.g.,\nlambda x: (4 * x) + 3. Table 1 illustrates\nsome examples drawn from our dataset. The un-\nderlying function is referred to as the rule or ex-\nplanation of the sequence, and the next integer\nas the completion. Our experimental settings are\nmostly based on two fundamental tasks: (1) se-\nquence completion and (2) sequence explanation.\nFor completions, we query models for the next\nitem in a given integer sequence. For explana-\ntions, models are prompted for the underlying func-\ntion that generated the given sequence. In our ex-\nperiments, models should return explanations in\nthe form of Python lambda functions whose form\nis demonstrated through few-shot examples (see\nAppendix B). Models are informed of the func-\ntion space ahead of time by being presented with\nthe possible generating functions in the instruction\nprompt.\nAmbiguous sequences are sequences for which\nthere are multiple generating rules which differ in\n1https://github.com/JacobPfau/introspective-self-\nconsistency\nSequence Completion Rule\n4, 6, 8 10 lambda x: x + 2\n7, 11, 15 15 lambda x: (3 * x) | 3\n7, 11, 15 19 lambda x: (4 * x) + 3\nTable 1: Example of integer sequences that are either\nunambiguous or ambiguous given a specific set of gen-\nerating rules (enumerated in Table 4).\ntheir continuation of the sequence. Unambiguous\nsequences are sequences which have only one valid\ncompletion within our function space. Appendix A\ndescribes our algorithm for mining for ambiguous\nsequences as well as the parameters of the func-\ntion space we searched over. The function space\nconsists of eight function templates, each with two\nconstant arguments. We generate functions from\nthose templates by setting the constant terms in the\nrange [0, 4], resulting in 197 possible functions on\nwhich Algorithm 1 is used. Our dataset consists\nof 140 unambiguous sequences and 57 ambiguous\nsequences.\n3 Methodology: Evaluating Consistency\nWe measure consistency by comparing responses\nfrom the completion task to responses from the\nexplanation task, which we call cross-context be-\ncause the model sees each task in a separate context\nwindow. Each prompt uses eight demonstrations\nshowing the model how to complete the sequence\nor explain the sequence using a Python function.\nThe demonstrations are drawn randomly2 from the\nsame function space as the ambiguous and unam-\nbiguous functions. Examples of these prompts are\npresented in Appendix B.\nThe models chosen for evaluation were\ntext-davinci-003, gpt-3.5-turbo, and\ngpt-4.3 While we are not entirely sure how\nthese models are trained, these models were\nchosen because they are commonly used by both\nresearchers and the public, and they represent\na sequence of capability increases through data\nquality improvement, annotations, and innovation\nin training and inference techniques (see OpenAI\n(2023)).\nIn the below experiments, greedy sampling\n2To control for the effect of these random sequences on bi-\nasing consistency, we report results aggregated from multiple\nruns\n3https://platform.openai.com/docs/models. For gpt-4, we\nuse the gpt-4-0314 version. For gpt-3.5-turbo, we use the\nmodel that was available from March to June 2023.\n91\n(temperature set to 0) is used throughout. This\nchoice lets us conduct a best-case analysis of self-\nconsistency: studying whether a model is capable\nof self-consistency when the sampling strategy is\nadvantageous. In §5, we move on from greedy\ndecoding and examine what the full output distribu-\ntion implies about the possible continuation space\nof models. 4\n3.1 Explanation and completion accuracy\nBefore considering cross-context consistency, we\nfirst benchmark these models’ accuracy on se-\nquence completion or sequence explanation in un-\nambiguous cases. For the completion case, we\npresent the models with a sequence of four inte-\ngers and evaluate its accuracy on generating the\nnext item in the sequence. For the explanation case,\nwe present the models with a sequence and eval-\nuate the model’s accuracy on generating an exact\nmatch of the Python function used to produce the\nsequence.\nAccuracy (%) %\nModel Explanation Completion Valid\ndavinci 6.00 20.20 95.5\ntext-davinci-003 31.18 65.95 99.3\ngpt-3.5-turbo 50.25 77.56 97.6\ngpt-4 59.05 78.64 94.8\nTable 2: Mean explanation and completion accuracy\nscores in unambiguous cases, as well as fraction of\nvalid, parseable answers, for each model across three\nruns. Accuracy increases with general model capability\nand is higher for completion than for explanation.\nTable 2 presents our capability results. We report\nthe average explanation and completion accuracy\nscores across three runs. We also report the fraction\nof valid answers (out of a total of 140 test cases, our\nunambiguous functions) where the model provided\na valid parseable answer, such as a valid integer or\nPython function. The results are largely intuitive:\nas general model capacity increases, performance\non the explanation and completion tasks increases.\nNote that the explanation task is generally harder\nthan the completion task. On both tasks, davinci\ndoes poorly despite having a high number of valid\nanswers, so davinci was not used in subsequent\nexperiments.\n3.2 Explanation and completion consistency\nOur second set of experiments evaluates the con-\nsistency of a given explanation for a sequence and\na completion for the same sequence when a model\nis prompted separately for explanation and comple-\ntion. We use a similar setup as the previous exper-\niment, including the explanation and completion\nprompts used earlier. We measure the following\n(see Appendix B for corresponding prompts):\n• Cross-context consistency: whether the ex-\nplanation provided by the model generates\nthe given sequence, including the completion\nprovided separately by the model.\n• Model-judged consistency: whether the\nmodel, itself, judges the explanation (rule) it\nprovided and the completion it provided to be\nconsistent, i.e., the rule generates the sequence\nwith claimed completion (see Listing 5 for the\nprompt used in these judgements).\nFigure 1 illustrates the performance of each\nmodel on the above scores when we vary the num-\nber of integers in the initial sequence from a length\nof two to a length of four. Sequences with two ini-\ntial integers have 196 ambiguous sequences, three\ninitial integers has 76 total ambiguous sequences,\nand four initial integers have 140 ambiguous se-\nquences. This variance allow us to understand the\nbehavior of models as the space of ambiguity varies.\nThe two main results are (1) model improve in con-\nsistency as they improve in arithmetical capabil-\nity from text-davinci-003 to gpt-4, (2) models\ntend to consider their answers consistent when they\nare not, except for gpt-4 which underestimates its\nown consistency. Result (2) is noteworthy because\ncalibration, or the ability of a model to express\naccurate estimates of its own behavior, is an impor-\ntant safety property of LLMs (Fluri et al., 2023; Lin\net al., 2022a). In domains where human evaluation\ncannot be done, Fluri et al. (2023) identify model\nself-evaluations of consistency as a primary method\nuseful for invalidating untrustworthy responses. A\nwell-calibrated model should have cross-context\nconsistency and model-judged consistencyscores\nas close as possible.\n92\ngpt-3.5-turbo-0301 gpt-4-0314 text-davinci-003\n0\n20\n40\n60\n80\n100Consistency\nsequence length = 2\ngpt-3.5-turbo-0301 gpt-4-0314 text-davinci-003\nsequence length = 3\ngpt-3.5-turbo-0301 gpt-4-0314 text-davinci-003\nsequence length = 4\nConsistency measure\nCross-context consistency\nModel-judged consistency\nFigure 1: Cross-context consistency (orange). Model-judged consistency (blue); this drops drastically for gpt-4,\nwhich underestimates the consistency across answers itself produced.\n30 40 50 60 70 80\nAccuracy\n68\n70\n72\n74\n76\n78\n80\n82Consistency\nCross-context consistency v accuracy\n30 40 50 60 70 80\nAccuracy\n60\n65\n70\n75\n80\n85\n90\n95Model-judged consistency\nModel-judged consistency v accuracy\ngpt-3.5-turbo-0301\ngpt-4-0314\ntext-davinci-003\nexplanation\ncompletion\nFigure 2: Explanation and sequence completion accuracies plotted against cross-context consistency and model-\njudged consistency (mean over sequence lengths). Further illustration of gpt-4’s inability to correctly assess its\nown consistency despite being much more consistent.\n3.3 Consistency and Capability\nFigure 2 presents the results from §3.1 and §3.2\nplotted together. This analysis investigates the de-\ngree to which model capability relate separately\nwith cross-context consistency, and model-judged\nconsistency. We see as capability increases so does\ncross-context consistency but, the most capable\nmodel gpt-4 is worse evaluating its own consis-\ntency.\nAdditionally, we compute expected consistency\nif correct completion-explanation pairs were cho-\nsen uniformly randomly at different capability\nthresholds. Table 3 illustrates cross-context con-\nsistency performance by our models and expected\nrandom consistency based on the average perfor-\nmance of each model on explanation and sequence\n4Given the nature of black-box API-based evaluation, it\nis possible greedy decoding doesn’t ensure determinism (e.g.\nbecause of sparse mixture of experts routing considerations).\ncompletion accuracy. This tells us how consis-\ntent we should expect models to perform at differ-\nent capability levels if they chose their completion\nresponses independently from their explanations.\nNote that a model could score perfectly on the ca-\npability evaluations and consistency evaluations\nwhile having no self-consistency whatsoever. What\nwe find is that models approach perfect consistency\nrapidly with capability increases.\nAverage consistency (%)\nModel Actual Random\ntext-davinci-003 66.86 8.50\ngpt-3.5-turbo 74.68 10.02\ngpt-4 82.22 15.22\nTable 3: Average cross-context (Actual) consistency\nacross settings in Figure 1 and consistency we’d expect\nto see (Random) if valid answers were selected uniform\nrandomly given the average accuracy performance for\neach model.\n93\n4 Robustness Checks for Consistency\nWe conducted further experiments to better under-\nstand how robust these results were to changes in\nexperimental protocol by using a range of differ-\nent prompts. We consider: (1) speaker changes\nin which we prompt the model as if the sequences\nwere generated by different speakers; (2) change of\nbase in which the sequence integers are presented\nin base 2 instead of base 10; and (2) sequence\nlength changes. Full results are given in §C.\n4.1 Consistency Across Speaker Changes\nThe first robustness experiment was designed to\ninvestigate the robustness of self-consistency of\nmodels when asked to simulate different speak-\ners. This was intended to investigate whether\nmodels could be prompted to simulate more or\nless self-consistent speakers, which would deter-\nmine whether models should be expected to be\nself-consistent by default or whether the previous\nresults were artifacts of arbitrary features of the\nprompt.\nTo do this, we again conducted the same exper-\niments as §3, now varying the initial instruction\ngiven to the model. These instructions were split\ninto two separate components which we varied in-\ndependently: what task we wanted the model to\ncomplete, and which speaker we wanted the model\nto simulate completing that task. We used three\ndifferent task prompts, which one might expect to\ncorrespond to three different levels of consistency:\nthe self-consistent prompts asked explicitly for a\npair of responses which matched each other; the\nmost likely prompts asked for the most likely con-\ntinuation / explanation (most likely); and the ran-\ndom prompt asked the model to choose responses\nrandomly when there was ambiguity about the cor-\nrect answer. The prompts in full can be found in\nAppendix B. For example, the random explana-\ntion prompt was \"Assume the sequence is gener-\nated by some deterministic function. If multiple\nfunctions could generate the sequence, choose the\ncorresponding continuation randomly\".\nThe first plot in Figure 3 shows representative\nresults when varying the task prompt on correct-\nness and consistency. If the models were capable\nof computing multiple continuations, and merely\nappeared self-consistent by dropping other possibil-\nities, then we might expect there to be variable self-\nconsistency, e.g., higher on the self-consistency\nprompt, and lowest on the random prompt. Empir-\nically, we found that prompting the models with\nthese different tasks had little influence on the pro-\nportion of answers that were self-consistent. This\nwas found both for sequences of length 4 and 2.\nEven in the case where we were able to elicit a\nhigh proportion of correct answers being incon-\nsistent using the most likely prompt, we do not\nsee large changes in the number of inconsistent re-\nsponses when varying the task prompt. This serves\nas strong evidence that the relationship between\ncapability and consistency is unaffected by task\nprompt.\n4.2 Consistency Across Base Changes\nIn this robustness experiment, we investigate what\nimpact the base representation of functions and\nsequences had on capabilities and consistency of\nthe models. This was intended to investigate the\nrelationship between model capability and self-\nconsistency while holding model type and training\nconstant. We hypothesised that bases besides base\n10 would be more difficult for the model. We again\nprompted the model to produce a continuation of\na sequence and an explanation for the sequence,\nalthough the sequences were now in base 2, and\nthe functions were expected to output base 2 repre-\nsentations of integers.\nThe second plot in Figure 3 presents a correla-\ntion analysis for this experiment, considering both\nbase 10 and base 2 responses. It demonstrates a\nvery strong correlation between the model generat-\ning correct explanations and being self-consistent,\nsuggesting that this trend is robust across bases and,\nthus, task difficulty.\n5 Distributional Analysis of Model\nConsistency\n5.1 Models Do Not Converge to Calculating a\nUnique Solution\nIn the analysis so far, greedy sampling was used\nthroughout. We now pose a follow-up ques-\ntion: Given models increasingly converge to self-\nconsistency, preferring a unique answer, to what\nextent do these models calculate representations\nof other alternative answers? And, when models\nplace high probability on alternative answers, can\nthey verbalize these alternative solutions serially?\nSpecifically for models that were fine-tuned with\nRLHF (Christiano et al., 2017; Ouyang et al., 2022),\nthe output probabilities may not be well-calibrated\nto the relative frequency of tokens if the objective\n94\n40 50 60 70 80 90\nExplanation Correctness\n30\n40\n50\n60\n70\n80\n90\nConsistency\nsequence length = 2, base 10\ngpt-4 Random Prompt\ngpt-4 Most Likely Prompt\ngpt-4 Self-Consistent Prompt\ngpt-3.5-turbo Random Prompt\ngpt-3.5-turbo Most Likely Prompt\ngpt-3.5-turbo Self-Consistent Prompt\n10 20 30 40 50 60 70 80 90\nExplanation Correctness\n10\n20\n30\n40\n50\n60\n70\n80\n90\nConsistency\nsequence length = 4\n20 30 40 50 60 70\nExplanation Correctness\n10\n20\n30\n40\n50\n60\n70\nConsistency\nsequence length = 2\ngpt-4 base 10\ngpt-4 base 2\ngpt-3.5-turbo base 10\ngpt-3.5-turbo base 2\nFigure 3: Cross-context consistency plotted against explanation correctness, varying either the role prompt (left-hand\nside) or the base-representation of the integer sequences being evaluated on (middle and right-hand side).\nof RLHF encourages models to allocate probabil-\nity mass narrowly (Kadavath et al., 2022). Hence,\nmodels’ token probability distribution may not be\nreflective of their credences. While the models\nmay be uncalibrated, we make a weaker assump-\ntion below that model output probabilities are non-\nparametrically calibrated: higher probability mass\nimplies higher credence.\nApplying this assumption to our setting, given\ninitial ambiguous sequence, Sn, generating rules\n{F}, we can determine whether a model has cal-\nculated an alternative correct sequence completion,\nc′, other than the modal greedy-decoded solution\nby verifying that:\nP(c′|Sn) > P(z|Sn) for all z ∈ N \\ C (1)\nwhere C is the set of correct continuations of Sn\nand N is the set of all continuations.\n4 6 8 10\nNumber of Shots\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900Rate\nRate for Correct Completions Assigned Consistently Non-trivial Mass\ninvalid func type\nrandom\nexclude_class\nsame_class\nFigure 4: Rate at which correct completion alternatives\nare assigned non-trivial probability mass by function\nclass sampled for few shot exemplars. Across sampling\nmethods, that rate is relatively high indicating a consis-\ntent consideration of correct alternatives across contexts.\nFor input data, we use the full set of 40 func-\ntions that generated ambiguous sequences (see Al-\ngorithm 1). We prompt the model using the same\nprompts for integer sequence completion as in §3.2\nand determine whether alternative correct answers\nrank higher than all incorrect answers. In the expla-\nnation case, we change the prompt to be a multiple-\nchoice task so that only a single token is needed to\nevaluate the above inequality. Despite this simpli-\nfication, the rate at which high probability mass is\nspread on alternatives is much lower, with the best\nrate of 0.3. This indicates that correct alternatives\nare not generally considered. This may be because\nthe computation of correct alternative explanations\nis much more computationally intensive and more\ndifficult than the computation of correct alternative\nsequence completions.\nWe use text-davinci-003 for our experiments\nsince it is the only model that has token log proba-\nbilities accessible from the public API.5 Since the\nAPI returns up to nlogprobs = 5probabilities for\ntop output tokens, we assess if any incorrect answer\nwas listed and whether the correct all rank higher.\nWhen a possible correct answer is not in the top\noutput tokens but an incorrect one is, we consider\nthe test failed. Finally, we control the sampling\nmethods for few-shot example: exclude_class\nindicates that we exclude the sequence generating\nfunctions that are from the same class (See classes\nused here Table 4), same_class draws functions\nfrom the same function class and random draws\nthose randomly across function classes. These con-\ntrols are designed to give us insight on whether the\nclass of functions used makes considering correct\nanswers over incorrect ones more challenging. The\nevaluations are averaged over three runs.\nFigure 4 illustrates that in the sequence com-\npletion case, text-davinci-003 almost always\nassigns high probability to correct alternative an-\n5https://platform.openai.com/docs/api-\nreference/completions/create\n95\nswers. We only see small differences with function\nclass used for few-shot examples where the cases of\nsame_class and random functions appear to help\nwith computing correct alternative explanations as\nthe number of few-shot demonstrations is increased.\nSampling examples with exclude_class seems to\nmake it more challenging likely because functions\nthat explain the model completion have not been\nseen before.6\n35\n 30\n 25\n 20\n 15\n 10\n 5\nLog Probability\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Normalized Density\nDistribution of Log Probabilities by Class Label for Completion (num_shots = 8)\nclass label\ncorrect_and_not_pred\nincorrect_and_not_pred\ncorrect_and_pred\nincorrect_and_pred\nFigure 5: Distribution over output probabilities\nfor correct and incorrect completions for the sam-\npling function type random_class. Each histogram\nis normalized by the data points of the corre-\nsponding class label. With KL-divergences of\nKL(correct_and_pred||correct_not_pred) = 1.71 and\nKL(correct_and_pred||incorrect_not_pred) = 3 .45\nbits, the distributions of correct answers have higher\noverlap.7\nIn Figure 5, the distribution over log probability\nmass is shown for the sequence completion task\nacross four combinations over two variables: cor-\nrectness and (greedy) prediction, i.e., whether the\nresponse in question was predicted as the top-1\nresponse. The distribution for predicted answers\nlook similar: correct and predicted answers (blue)\nnarrowly concentrate relatively large log probabil-\nities and a single peak for incorrect predictions\n(red). For non-predicted answers, the distributions\nare generally flatter and their mean shifted towards\ncomparatively smaller values.\nFor correct and non-predicted answers (green),\nthe distributions’ median at around -13.8 is much\nlarger than at -20.7 for incorrect answers. This\n6Since we do not have access to the underlying pre-training\ncorpora distribution of the model, we cannot definitively rule\nout higher probability mass being assigned to sequences due\nto their frequency in the pre-training corpora.\n7To calculate the KL-divergence, we first obtained the den-\nsity histograms for the same points nbins = 40between the\nminimum and maximum value of log probabilities. Addition-\nally, we applied Gaussian smoothing with σ = 1to include\ninformation where the quotient would otherwise have been\nundefined.\ndifference indicates that the model allocates non-\ntrivial probability mass to those correct options.\nCorrect alternatives are calculated and represented\nby the model internally. When normalizing the\ndistribution across all data points the probability\nmass place on correct answers is relatively large\nand narrow, even for non-predicted answers (see\nFigure 7).\n5.2 Verbalizing Alternatives\nWhile inspecting the probability distribution over\nanswers gives insights into the potential consider-\nation of alternatives, we are further interested in\nthe extent to which models would verbalize those\nalternatives if prompted. This is important because\noutside of our simple sequence modeling cases, nat-\nural language questions will generally have distinct\nanswers which require multiple tokens to express,\nmaking it impractical to directly read off answer\nprobability from logits.\nIn this experiment, we prompt the model to pro-\nvide all possible answers for an ambiguous se-\nquence task and compare those with the correct\noptions (prompt in Listing 12). We provide in-\ncontext examples and consider only up to 5 alter-\nnatives. Precision and recall scores are calculated,\ncomparing verbalized answers with the valid con-\ntinuations. For input data, we consider the default\nambiguous sequences (see Algorithm 1).\nThe high precision scores in Figure 6 show that\nmodels do not tend to produce random, incorrect\nanswers. Recall scores are much lower, for comple-\ntion reaching a maximum of 0.41 and for explana-\ntion 0.49. Compared to precision this aligns with\nour expectations that verbalizing all alternatives is\nvery difficult. However, the rapid increase in recall\nwith additional in-context examples implies that the\nmodels adapt to include more correct alternatives.8\nIn contrast to our previous results, the performance\nfor the explanation tasks is similar to completion.\ntext-davinci-003 achieves the highest recall for\nexplanation despite being the generally less capa-\nble model, but likely preserving a wider options\nspace and multiple possible continuations due to\nless RLHF fine-tuning. The low precision score in-\ndicates that it thereby also produces false negatives.\nThe relatively high recall of gpt-4 for explanation\nand completion tasks show its verbalization capa-\nbilities. However, in the easier completion task,\n8For nshots > 10, our prompt exceeds the token limit. De-\nspite increasing recall scores, we were not able to investigate\nthe impact of few-shot examples further.\n96\n4 6 8 10\nNumber of Shots\n0.4\n0.5\n0.6\n0.7\n0.8Score\nprecision\n4 6 8 10\nNumber of Shots\n0.30\n0.35\n0.40\n0.45\n0.50Score\nrecall\nmodel\ngpt-3.5-turbo\ngpt-4\ntext-davinci-003\nresponse_type\ncompletion\nexplanation\nPrecision & Recall of Verbalized Alternatives by Model\nFigure 6: Precision and recall scores of alternative an-\nswers verbalized by different models compared to cor-\nrect answers, up to5 alternatives and only distinct values\nwere counted.\nhigh recall scores would be expected if the model\nconsidered more alternatives.\n6 Related Work\nOur work is motivated by previous research on\ntruthfulness. Approaches like Lin et al. (2022b)\ndirectly tackle this problem by developing bench-\nmarks for truthfulness of LLMs across a range of\nquestions such as health, law, and politics. Detect-\ning inconsistencies is helpful, but not sufficient, for\nevaluating the truthfulness of language models.\nEvaluating model behavior under ambiguity\nwould shed some light on this question, as ex-\nplored in Liu et al. (2023). Here, however, the\nemphasis is on interpreting ambiguous natural lan-\nguage sentences correctly, as opposed to making\nthe same judgment in a range of different contexts.\nThis means that failings might not be indications\nof inconsistency but rather a poor understanding of\nnatural language.\nSimilarly, the approach towards consistency eval-\nuations taken by Fluri et al. (2023) focuses on\nwhether different answers are logically consistent.\nWhen a set of conditions over different inputs holds,\nthen conditions over corresponding outputs should\nlogically follow. For instance, forecasting world\nrecords in 100m sprint should monotonically de-\ncrease over time. In contrast to our own work, the\ninvestigations focus on scenarios without known\nground truth. Our focus on being consistent across\ncontexts tests for poor world models and extends\nconsistency checks to arithmetic reasoning tasks.\nTamkin et al. (2023) presents a novel benchmark\nfor studying how well models are able to detect\nsalient features of sentences where this salient fea-\nture is undetermined. This relates to our ambiguous\nsequences setting, although the focus on interpret-\ning natural language means the evaluations will not\nseparate poor language understanding from inher-\nent inconsistency.\nSelf-consistency also relates to chain of thought\nprompting (Wei et al., 2022), which may be used to\nelicit truthful explanations of how models arrive at\nclaims. However, Turpin et al. (2023) demonstrates\nthat the given explanations can be misleading since\nmodels can be biased to change their answers in a\nway that is not reflected in their explanations–this\nis a form of explanation inconsistency.\nThere has been recent progress on this from work\nin interpretability. Burns et al. (2023) demonstrate\nthat directions in the latent space of networks can\nbe found that correspond to truthfulness better than\nthe outputs of models directly. Our approach could\ncomplement techniques like this, providing new\nphenomena to better understand the trustworthiness\nof models.\nA related investigation is into how language mod-\nels respond to open-ended questions for which a\nsingle correct answer does not exist (Yin et al.,\n2023). Our work can be seen as considering the\nrelated case where instead of there being no cor-\nrect answer, there exist multiple possible correct\nanswers. Similarly, Raj et al. (2022); Elazar et al.\n(2021) have focused on cross-prompt consistency\nover knowledge-focused QA.\n7 Conclusion\nAll tested models behaved more self-consistently\nacross contexts for ambiguous tasks than expected\nif the models had randomly consistent behaviour.\nThis is surprising given models are not explicitly\ntrained for cross-context self-consistency. We also\nfound that model consistency grows with model\ncapability. We varied the task prompt, as well as\nthe difficulty of the task (using base-2 sequences\ninstead of base-10 sequences and varying the se-\n97\nquence length), and found that our findings are\nrobust with respect to these changes. Across all\nevaluated models found that they are not well cali-\nbrated when it comes to evaluating their own con-\nsistency. We also tested that even when a model\nthat chooses relatively consistent answers among\nseveral correct answers across contexts, models\nmay still assign non-trivial probability to other cor-\nrect answers. Asking the models to verbalize cor-\nrect alternatives revealed high precision scores for\nall models which discern between correct and in-\ncorrect answers. In comparison, recall was rela-\ntively low where text-davinci-003 surprisingly\nachieved the highest recall, closely followed by\ngpt-4, indicating they can retrieve alternative cor-\nrect answers. The significance of our results is that\nwe shouldn’t assume the apparent consistency of\nLLMs points to actual internal consistency due to\nhigh probability mass placed on alternative answers\nwhich may equally be picked using common sam-\npling techniques for natural language generation.\nAs a community we should also be wary of con-\nsistency given our results on calibration that show\nmodels across capability classes strongly over and\nunder estimate their own consistency.\n8 Limitations\nAmbiguous integer sequences is an idealized do-\nmain removing linguistic concerns and knowledge-\nrelated complexities of natural language tasks.\nHence, results on this domain may not general-\nize. This is important because studies understand-\ning LLMs safety typically focus on model behav-\niors that have a direct impact on understanding\nreal-world risk, such as impact on socio-cultural\nprejudice or factual accuracy, of their deployment.\nFuture work could investigate consistency in more\ngeneral linguistic domains using a similar frame-\nwork of ambiguity.\nOur analysis of self-consistency was limited by\nonly having access to models through a public API.\nIn particular, we were only able to access the log\nprobabilities of one model under analysis, and at\nthe time of writing, this API is deprecated. Addi-\ntionally, we did not include evaluation of available\nopen-source models, which could have provided\ninsightful comparisons with the OpenAI models\nand possibility to test output behaviours more ex-\ntensively. Future work may be unable to access\nthe log probabilities of these models to perform\nsimilar analyses. Although we did use greedy de-\ncoding with zero temperature, the GPT model tend\nto behave non-deterministically, which already in-\ntroduces an implicit inconsistency and dependence\non the few-shot examples. Reporting results av-\neraged over several runs aimed to mitigate this.\nBut controls for each experiments could have been\ndone in addition to that. Our experiments in 5\nwere limited by the availability of token probabili-\nties, so no scaling results are available in that sec-\ntion. We chose popular LLMs used through public\nAPIs since we wanted to understand the behavior\nof those particular models, but future work should\ninvestigate open-source models that we are able to\nfully inspect. In particular, we believe the observed\nincrease in cross-context consistency results from\nRLHF and pre-training. However, given the closed\nsource nature of these models, it is possible that\nGPT-series models were trained with cross-context\nconsistency objectives.\n9 Acknowledgements\nThanks to Julian Michael and Miles Turpin for\nfeedback on a draft of this paper. This project has\nbenefited from financial support to Sam Bowman\nby Eric and Wendy Schmidt (made by recommen-\ndation of the Schmidt Futures program) and Open\nPhilanthropy. This material is based upon work sup-\nported by the National Science Foundation under\nGrant Nos. 1922658 and 2046556. Any opinions,\nfindings, and conclusions or recommendations ex-\npressed in this material are those of the author(s)\nand do not necessarily reflect the views of the Na-\ntional Science Foundation.\nReferences\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\nhardt. 2023. Discovering latent knowledge in lan-\nguage models without supervision. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nYanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao,\nHe He, Jacob Steinhardt, Zhou Yu, and Kathleen\nMcKeown. 2023. Do models explain themselves?\ncounterfactual simulatability of natural language ex-\nplanations.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\n98\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nLukas Fluri, Daniel Paleka, and Florian Tramèr. 2023.\nEvaluating superhuman models with consistency\nchecks.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know.\nB. A. Levinstein and Daniel A. Herrmann. 2023. Still no\nlie detector for language models: Probing empirical\nand conceptual roadblocks.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022a.\nTeaching models to express their uncertainty in\nwords. Transactions on Machine Learning Research.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022b.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nAlisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr,\nPeter West, Alexander Koller, Swabha Swayamdipta,\nNoah A. Smith, and Yejin Choi. 2023. We’re afraid\nlanguage models aren’t modeling ambiguity.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nHarsh Raj, Domenic Rosati, and Subhabrata Ma-\njumdar. 2022. Measuring reliability of large lan-\nguage models through semantic consistency. CoRR,\nabs/2211.05853.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose opinions do language models reflect?\nAlex Tamkin, Kunal Handa, Avash Shrestha, and Noah\nGoodman. 2023. Task ambiguity in humans and\nlanguage models. In The Eleventh International Con-\nference on Learning Representations.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don’t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know?\n99\nA Mining Ambiguous Sequences\nAlgorithm 1 describes how we find a set of ambiguous functions A given a set of function templates\nFtemplates and the parameters c : N and s : N which control the sequence of constants to use for filling a\nset of templates and the number steps we must check that a pair of functions must match for.\nDefinition A.1 (Integer Sequence Ambiguity). A pair of integer sequence-generating functions can be said\nto be ambiguous iff both functions generate the same sequence up to |S| but generate different integers at\nstep |S| + 1. This property holds if functions begin generation at different offsets.\nFtemplates is a set of functions that have slots for constant terms used to construct the function space\nwe will search for ambiguity within. For the purposes of our experiments, we generated templates using\nthe function templates in Table 4 which consisted of templates with two constant term slots. We generated\nfunctions using integer constants in the range [0, 4].\nFor our experiments, we checked ambiguity for sequences of length 4 and an offset maximum of 4.\nUnambiguous sequences are the complement of A and can easily be found by modifying the algorithm\nbelow to return sequences which are generated by only one function selected from the function space. It\nis important to note that the sequence is only unambiguous with respect to the function space selected.\nType Template\narithmetic progression lambda x: ({} * x) + {}\ngeometric progression lambda x: ({} * x) * {}\nexponential progression lambda x: ({} * x) ** {}\npower progression lambda x: {} ** ({} * x)\nbit or progression lambda x: ({} * x) | {}\nmodular progression lambda x: (x * {}) % ({}+1)\nindexing criteria lambda x:\nprogression [i for i in range(100) if i % ({} + 1) or i % ({} + 1)][x]\nrecursive progression (lambda a: lambda v: a(a,v))\n(lambda fn,x: 1 if x==0 else {} * x * fn(fn,x-1) + {})\nTable 4: Function templates with two constant term slots that were used for mining ambiguous sequences. Note our\nfunctions are indexed starting at one.\n100\nAlgorithm 1 Mining Ambiguous Sequences\nRequire: Ftemplates ▷ Construct function space\nSet c ∈ C is a set of constants to parameterize the function templates\nfor f ∈ Ftemplates do\nfor c1 ∈ C do\nfor c2 ∈ C do\nFfilled ← Ffilled ∪ f[c1; c2]\nend for\nend for\nend for\nRequire: P : Ffilled × Ffilled ▷ Check ambiguity\nSet S is a set of steps to check for ambiguity\nSet O us a set of offsets to check starting and ending positions\nSet A is the set of ambiguous functions\nfor f1, f2 ∈ Pdo\nfor o1 ∈ O do\nfor o2 ∈ O do\nSet seqa is a temporary set for keeping track of the output from f1\nSet seqb is a temporary set for keeping track of the output from f2\nfor s ∈ S do\nseqa ← f1(s + o1)\nseqb ← f2(s + o2)\nif seqa ̸= seqb then\nbreak\nend if\nif s = |S| then\nseqa ← f1(s + o1 + 1)\nseqb ← f2(s + o2 + 1)\nif seqa ̸= seqb then\nA ← A ∪f1, seqa, f2, seqb\nend if\nend if\nend for\nend for\nend for\nend for\nEnsure: Return A is the set of ambiguous functions identified above\n101\nB Prompt Examples\nListing 1: System-level prompt. We prepend each context with minimal instruction and provide the valid space of\nlambda functions to the model. For base 2 sequences the outputs are wrapped with the bin function.\nYou are helping with integer sequences like arithmetic or geometric sequences .\n,→ Sequences are not always 0 indexed , they may be offset by an arbitrary i-\n,→ index value . Progressions are written as Python lambda functions and you may\n,→ only respond in the same lambda - function format .\nValid lambdas :\n- arithmetic -> lambda x: (a * x) + b\n- geometric -> lambda x: (a * x) * b\n- exponential -> lambda x: (a * x) ** b\n- power -> lambda x: a ** (b * x)\n- bitwise_or -> lambda x: (a * x) | b\n- modular -> lambda x: (x * a) % (b +1)\n- indexing_criteria -> lambda x: [i for i in range (100) if i % (a + 1) or i % (b +\n,→ 1) ][x]\n- recursive -> ( lambda a: lambda v:a(a,v))( lambda fn ,x:1 if x ==0 else a * x * fn(fn ,x\n,→ -1) + b)\nListing 2: Prompt for sequence explanation. We provide six few-shot examples before we prompt the model with\nthe test prompt (highlighted in beige).\nFor the sequence : 3 ,7\nGive the code that generates the above sequence .\nlambda x: (2 * x) | 3\nFor the sequence : 1 ,1 ,1\nGive the code that generates the above sequence .\nlambda x: (1 * x) ** 0\nFor the sequence : 18 ,162\nGive the code that generates the above sequence .\n( lambda a: lambda v:a(a,v))( lambda fn ,x:1 if x ==0 else 3 * x * fn(fn ,x -1) + 0)\nFor the sequence : 4 ,7\nGive the code that generates the above sequence .\nlambda x: (3 * x) | 4\nFor the sequence : 1 ,1 ,1 ,1\nGive the code that generates the above sequence .\nlambda x: 5 ** (0 * x)\nFor the sequence : 3 ,5\nGive the code that generates the above sequence .\nlambda x: [i for i in range (100) if i % (3 + 1) or i % (3 + 1) ][x]\nFor the sequence : 4 ,5\nGive the code that generates the above sequence .\n102\nListing 3: Prompt for sequence completion. We provide eight few-shot examples before we prompt the model with\nthe test prompt (highlighted in beige).\nFor the sequence : 2 ,3 ,4\nComplete the next number and only the next number .\n5\nFor the sequence : 0 ,1 ,2 ,3\nComplete the next number and only the next number .\n0\nFor the sequence : 0 ,0 ,0\nComplete the next number and only the next number .\n0\nFor the sequence : 9 ,12 ,15 ,18\nComplete the next number and only the next number .\n21\nFor the sequence : 4 ,6 ,7 ,8\nComplete the next number and only the next number .\n9\nFor the sequence : 3 ,8 ,26\nComplete the next number and only the next number .\n106\nFor the sequence : 4 ,5 ,7\nComplete the next number and only the next number .\n8\nFor the sequence : 2 ,6 ,24\nComplete the next number and only the next number .\n120\nFor the sequence : 12 ,15\nComplete the next number and only the next number .\nListing 4: Prompt for sequence explanation. We provide six few-shot examples before we prompt the model with\nthe test prompt (highlighted in beige).\nFor the sequence : 3 ,7\nGive the code that generates the above sequence .\nlambda x: (2 * x) | 3\nFor the sequence : 1 ,1 ,1\nGive the code that generates the above sequence .\nlambda x: (1 * x) ** 0\nFor the sequence : 18 ,162\nGive the code that generates the above sequence .\n( lambda a: lambda v:a(a,v))( lambda fn ,x:1 if x ==0 else 3 * x * fn(fn ,x -1) + 0)\nFor the sequence : 4 ,7\nGive the code that generates the above sequence .\nlambda x: (3 * x) | 4\nFor the sequence : 1 ,1 ,1 ,1\nGive the code that generates the above sequence .\nlambda x: 5 ** (0 * x)\nFor the sequence : 3 ,5\nGive the code that generates the above sequence .\nlambda x: [i for i in range (100) if i % (3 + 1) or i % (3 + 1) ][x]\nFor the sequence : 4 ,5\nGive the code that generates the above sequence .\n103\nListing 5: Prompt for self-consistency. We provide six few-shot examples before we prompt the model with the test\nprompt (highlighted in beige).\nIs the following sequence : 1, 2, 3 consistent with the function lambda x: x + 1?\nAnswer (Y/N): Y\nIs the following sequence : 3, 3, 4 consistent with the function lambda x: 3 * x + 2?\nAnswer (Y/N): N\nIs the following sequence : 8, 18 , 32 consistent with the function lambda x: 2 * x **\n,→ 2?\nAnswer (Y/N): Y\nIs the following sequence : 256 , 1024 consistent with the function lambda x: 4 ** x?\nAnswer (Y/N): Y\nIs the following sequence : 7, 5, 12 consistent with the function lambda x: (3 * x) |\n,→ 4?\nAnswer (Y/N): N\nIs the following sequence : 2, 3, 0, 1 consistent with the function lambda x: (x * 5)\n,→ % 4?\nAnswer (Y/N): Y\nIs the following sequence : 2 ,3 ,4 consistent with the function lambda x: (2 * x) | 0?\nAnswer (Y/N):\nListing 6: Random prompt for explanations (preceded with few-shot examples). Note that \"Sequences are not\nalways 0 indexed , they may be offset by an arbitrary i-index value\" from Listing 1 is omitted for Listings 6-11.\nPick a function which generates the following sequence .\nIf there are multiple possible functions , choose randomly .\nAssume the first number was generated by f (0) , the second by f (1) , and so on.\nThe function should output numbers in the same representation as the sequence .\nGive the response in the format :\nExplanation : <function >\nListing 7: Random prompt for continuations (preceded with few-shot examples).\nChoose a continuation of the following sequence .\nAssume the sequence is generated by some deterministic function .\nIf multiple functions could generate the sequence , choose the corresponding\n,→ continuation randomly .\nListing 8: Self-consistency prompt for explanations (preceded with few-shot examples).\nChoose a function which generates the following sequence .\nNote that you will be separately asked to choose a continuation of the sequence .\nPlease choose a function which is consistent with the continuation you will choose .\nAssume the first number was generated by f (0) , the second by f (1) , and so on.\nThe function should output numbers in the same representation as the sequence .\nGive the response in the format :\nExplanation : <function >\nListing 9: Self-consistency prompt for continuations (preceded with few-shot examples).\nChoose a continuation of the following sequence .\nNote that you will be separately asked to choose a function which generated the\n,→ sequence .\nPlease choose a continuation which is consistent with the function you will choose .\nGive the response as a number ( with a base prefix if appropriate ), with no\n,→ additional commentary .\n104\nListing 10: Most-likely prompt for explanations (preceded with few-shot examples).\nWhat is the most likely function which generated the following sequence ?\nAssume the first number was generated by f (0) , the second by f (1) , the third by f (2)\n,→ , and so on.\nThe function should output the number in the same representation as the sequence .\nGive the response in the format :\nExplanation : <function >\nListing 11: Most-likely prompt for continuations (preceded with few-shot examples).\nWhat is the most likely continuation of the following sequence ?\nGive the response as a number ( with a base prefix if appropriate ), with no\n,→ additional commentary .\nListing 12: Prompt for verbalizing alternative completions.\nFor the sequence : 3 ,7\nComplete the next possible number .\nConsider up to 5 possible and valid answers separated by escape character '\\n ', as\n,→ determined by you , { model_name }.\n4 \\n 15 \\n\nFor the sequence : 1 ,1 ,1\nComplete the next number and only the next number .\nConsider up to 5 possible and valid answers separated by escape character '\\n ', as\n,→ determined by you , { model_name }.\n105\nC Robustness Experiment Results\nModel Base Length Correct Incorrect\nConsistent Inconsistent Consistent Inconsistent\ngpt-4 10 4 70 2 2 26\ngpt-4 10 2 88 3 0 9\ngpt-4 2 4 23 4 2 72\ngpt-4 2 2 19 15 0 66\ngpt-3.5-turbo 10 4 65 4 2 26\ngpt-3.5-turbo 10 2 38 16 2 44\ngpt-3.5-turbo 2 4 11 2 0 84\ngpt-3.5-turbo 2 2 9 3 2 81\nTable 5: The proportion of self-consistent continuation and explanation pairs (Consistent), alongside whether the\nexplanations are correct (Correct), for a given model (Model) on generated ambiguous sequences of length (Length),\nrepresented in base (Base). Also tracks whether explanations or continuations are invalid (Invalid).\nD Histogram of Log Probabilities for Alternative Completions of Ambiguous Sequences.\n30\n 25\n 20\n 15\n 10\n 5\nLog Probability\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175Normalized Density\nNormalized Distribution of Log Probabilities by Correctness for Completion (num_shots = 8)\nincorrect\ncorrect\nFigure 7: Distribution over log probabilities by correctness with densities normalized across all data points. It shows\na narrow concentration of relatively large probabilities for correct answers and incorrect answers with relatively\nsmall probabilities. The plot shows results for few-shots examples of random samples; distributions with different\nfew-shot sampling methods and number of shots look very similar.",
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6711165904998779
    },
    {
      "name": "Ambiguity",
      "score": 0.6126202344894409
    },
    {
      "name": "Computer science",
      "score": 0.6017569899559021
    },
    {
      "name": "Causal consistency",
      "score": 0.5660656094551086
    },
    {
      "name": "Sequential consistency",
      "score": 0.5573505759239197
    },
    {
      "name": "Nonparametric statistics",
      "score": 0.5490081310272217
    },
    {
      "name": "Consistency model",
      "score": 0.5359343886375427
    },
    {
      "name": "Weak consistency",
      "score": 0.4419771134853363
    },
    {
      "name": "Probability distribution",
      "score": 0.43663308024406433
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.41138380765914917
    },
    {
      "name": "Econometrics",
      "score": 0.3895472288131714
    },
    {
      "name": "Strong consistency",
      "score": 0.3756561279296875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3745199143886566
    },
    {
      "name": "Mathematics",
      "score": 0.2827855944633484
    },
    {
      "name": "Statistics",
      "score": 0.2684444785118103
    },
    {
      "name": "Algorithm",
      "score": 0.22813701629638672
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Correctness",
      "score": 0.0
    },
    {
      "name": "Estimator",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129902397",
      "name": "Dalhousie University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 3
}