{
  "title": "DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction",
  "url": "https://openalex.org/W4382240142",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2119441488",
      "name": "Yangyang Xu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104772684",
      "name": "Yibo Yang",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104063197",
      "name": "Lefei Zhang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2119441488",
      "name": "Yangyang Xu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104772684",
      "name": "Yibo Yang",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104063197",
      "name": "Lefei Zhang",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4280547190",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2104408738",
    "https://openalex.org/W2767434619",
    "https://openalex.org/W6736170873",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2784371332",
    "https://openalex.org/W3194582014",
    "https://openalex.org/W2618011341",
    "https://openalex.org/W4226495185",
    "https://openalex.org/W3035276179",
    "https://openalex.org/W6790935598",
    "https://openalex.org/W6750189243",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6703533685",
    "https://openalex.org/W3125886068",
    "https://openalex.org/W6791479011",
    "https://openalex.org/W4221153062",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3085046840",
    "https://openalex.org/W3001710985",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6810167572",
    "https://openalex.org/W2798441115",
    "https://openalex.org/W4313166855",
    "https://openalex.org/W2988403155",
    "https://openalex.org/W2949033552",
    "https://openalex.org/W2902303185",
    "https://openalex.org/W3177645420",
    "https://openalex.org/W3129958428",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963877604",
    "https://openalex.org/W2997473137",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2950103041",
    "https://openalex.org/W3204786153",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2964247799",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W3204397973",
    "https://openalex.org/W4313153210",
    "https://openalex.org/W2963430933",
    "https://openalex.org/W4383503846",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W2959581809",
    "https://openalex.org/W3097571420"
  ],
  "abstract": "Convolution neural networks (CNNs) and Transformers have their own advantages and both have been widely used for dense prediction in multi-task learning (MTL). Most of the current studies on MTL solely rely on CNN or Transformer. In this work, we present a novel MTL model by combining both merits of deformable CNN and query-based Transformer for multi-task learning of dense prediction. Our method, named DeMT, is based on a simple and effective encoder-decoder architecture (i.e., deformable mixer encoder and task-aware transformer decoder). First, the deformable mixer encoder contains two types of operators: the channel-aware mixing operator leveraged to allow communication among different channels (i.e., efficient channel location mixing), and the spatial-aware deformable operator with deformable convolution applied to efficiently sample more informative spatial locations (i.e., deformed features). Second, the task-aware transformer decoder consists of the task interaction block and task query block. The former is applied to capture task interaction features via self-attention. The latter leverages the deformed features and task-interacted features to generate the corresponding task-specific feature through a query-based Transformer for corresponding task predictions. Extensive experiments on two dense image prediction datasets, NYUD-v2 and PASCAL-Context, demonstrate that our model uses fewer GFLOPs and significantly outperforms current Transformer- and CNN-based competitive models on a variety of metrics. The code is available at https://github.com/yangyangxu0/DeMT.",
  "full_text": "DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense\nPrediction\nYangyang Xu1, Yibo Yang3, Lefei Zhang1,2*\n1 National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, China\n2 Hubei Luojia Laboratory, China\n3 JD Explore Academy, China\n{yangyangxu, zhanglefei}@whu.edu.cn, ibo@pku.edu.cn\nAbstract\nConvolution neural networks (CNNs) and Transformers have\ntheir own advantages and both have been widely used for\ndense prediction in multi-task learning (MTL). Most of the\ncurrent studies on MTL solely rely on CNN or Transformer.\nIn this work, we present a novel MTL model by com-\nbining both merits of deformable CNN and query-based\nTransformer for multi-task learning of dense prediction. Our\nmethod, named DeMT, is based on a simple and effec-\ntive encoder-decoder architecture (i.e.,deformable mixer en-\ncoder and task-aware transformer decoder). First, the de-\nformable mixer encoder contains two types of operators: the\nchannel-aware mixing operator leveraged to allow communi-\ncation among different channels (i.e., efficient channel loca-\ntion mixing), and the spatial-aware deformable operator with\ndeformable convolution applied to efficiently sample more\ninformative spatial locations (i.e., deformed features). Sec-\nond, the task-aware transformer decoder consists of the task\ninteraction block and task query block. The former is ap-\nplied to capture task interaction features via self-attention.\nThe latter leverages the deformed features and task-interacted\nfeatures to generate the corresponding task-specific feature\nthrough a query-based Transformer for corresponding task\npredictions. Extensive experiments on two dense image pre-\ndiction datasets, NYUD-v2 and PASCAL-Context, demon-\nstrate that our model uses fewer GFLOPs and significantly\noutperforms current Transformer- and CNN-based compet-\nitive models on a variety of metrics. The code is available\nat https://github.com/yangyangxu0/DeMT.\nIntroduction\nHuman vision capability is powerful and can perform dif-\nferent tasks from one visual scene, such as classification,\nsegmentation, recognition, etc. Therefore, multi-task learn-\ning (MTL) research is topical in computer vision. We expect\nto develop a powerful vision model to do multiple tasks si-\nmultaneously in different visual scenarios, and this model\nis expected to work efficiently. As shown in Figure 1, in\nthis paper, we aim to develop a powerful vision model to\nlearn multiple tasks, including semantic segmentation, hu-\nman parts segmentation, depth estimation, boundary detec-\n*Corresponding Author.\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ntion, saliency estimation, and normal estimation simultane-\nously, and this model is expected to work efficiently.\nRecently, existing works (Liu, Johns, and Davison 2019;\nVandenhende et al. 2020; Phillips et al. 2021; Ghiasi et al.\n2021; Bruggemann et al. 2021; Xu et al. 2022b; Bhat-\ntacharjee et al. 2022) have adopted CNN and Transformer\ntechnologies to advance the MTL of dense prediction. Al-\nthough CNN-based MTL models are carefully proposed to\nachieve promising performance on the multi-task dense pre-\ndiction task, these models still suffer from the limitations of\nconvolutional operations, i.e., lacking global modeling and\ncross-task interaction capability. Some works (Bruggemann\net al. 2021; Vandenhende et al. 2020) develop a distillation\nscheme to increase the expressiveness of the cross-task and\nglobal information passing via enlarging the receptive field\nand stacking multiple convolutional layers but still cannot\nbuild global dependency directly. For modeling global and\ncross-task interaction information, Transformer-based MTL\nmodels (Bhattacharjee et al. 2022; Xu et al. 2022b) utilize\nthe efficient attention mechanism (Vaswani et al. 2017) for\nglobal modeling and task interactions. However, such a self-\nattention approach may fail to focus on task awareness fea-\ntures because the queries, keys and values are based on the\nsame feature. Regular self-attention may lead to high com-\nputational costs and limit the ability to disentangle task-\nspecific features.\nWe can see that the CNN-based models better capture the\nmultiple task context in a local field but suffer from a lack\nof global modeling and task interaction. The Transformer-\nbased models better focus on global information of different\ntasks. However, they ignore task awareness and introduce\nmany computation costs. Therefore, a technical challenge in\ndeveloping a better MTL model is how to combine the mer-\nits of CNN-based and Transformer-based MTL models.\nTo address the challenges, we introduce the Deformable\nMixer Transformers (DeMT): a simple and effective method\nfor multi-task dense prediction based on combining both\nmerits of deformable CNN and query-based Transformer.\nSpecifically, our DeMT consists of the deformable mixer\nencoder and task-aware transformer decoder. Motivated by\nthe success of deformable convolutional networks (Zhu et al.\n2019) in vision tasks, our deformable mixer encoder learns\ndifferent deformed features for each task based on more effi-\ncient sampling spatial locations and channel location mixing\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3072\n(i.e., deformed feature). It learns multiple deformed features\nhighlighting more informative regions with respect to the\ndifferent tasks. In the task-aware transformer decoder, the\nmultiple deformed features are fused and fed into our task\ninteraction block. We use the fused feature to generate task-\ninteracted features via a multi-head self-attention for model\ntask interactions. To focus on the task awareness of each in-\ndividual task, we use deformed features directly asquery to-\nkens. We expect the set of candidate key/value to be from\ntask-interacted features. Then, our task query block tasks\nthe deformed features and task-interacted features as input\nand generates the task awareness features. In this way, our\ndeformable mixer encoder selects more valuable regions as\ndeformed features to alleviate the lack of global modeling\nin CNN. The task-aware transformer decoder performs the\ntask interactions by self-attention and enhances task aware-\nness via a query-based Transformer. This design both re-\nduces computational costs and focuses on task awareness\nfeatures. Through extensive experiments on several publicly\nMTL dense prediction datasets, we demonstrate that the pro-\nposed DeMT method achieves state-of-the-art results on a\nvariety of metrics.\nThe contributions of this paper are as follows: 1) We\npropose a simple and effective DeMT method for MTL of\ndense prediction via combining both merits of CNN and\nTransformer. Most importantly, our approach not only al-\nleviates the lack of global modeling in MTL models using\nCNNs but also avoids the lack of task awareness in MTL\nmodels using Transformers. 2) We introduce a deformable\nmixer transformer (DeMT) model which consists of the de-\nformable mixer encoder (Section ) and task-aware trans-\nformer decoder (Section ). The deformable mixer encoder\nproduces the deformed features. The task-aware transformer\ndecoder uses the deformed features to model task interac-\ntion via a self-attention and focus on the task awareness fea-\ntures via a query-based transformer. 3) The extensive exper-\niments on NYUD-v2 (Silberman et al. 2012) and PASCAL-\nContext (Chen et al. 2014) and visualization results show the\nefficacy of our model. DeMT‚Äôs strong performance on MTL\ncan demonstrate the benefits of combining the deformable\nCNN and query-based Transformer.\nRelated Work\nMulti-Task Learning (MTL)\nMTL has dramatically evolved with the development of\nDeep Neural Networks and Vision Transformers. MTL\ntasks are mainly distributed in two aspects: model struc-\ntures (Bruggemann et al. 2021) and task loss weighting\noptimization (Liu et al. 2021a). In the vision domain, the\ncore idea of MTL is to use a single model to predict se-\nmantic segmentation, human parts segmentation, depth, sur-\nface normal, boundary, etc., which is an interesting topic.\nMuST (Ghiasi et al. 2021) model uses the knowledge in\nindependent specialized teacher models to train a general\nmodel for creating general visual representations. Several re-\ncent MTL frameworks follow different technologies: (Gao\net al. 2019; Vandenhende et al. 2020) is CNN-based MTL\nmodel, (Bruggemann et al. 2021) is Neural Architecture\nSearch (NAS)-based model and (Bhattacharjee et al. 2022;\nXu et al. 2022b,a) are Transformer-based models. (Vanden-\nhende et al. 2021) states that the MTL structures in vi-\nsion tasks can be summarized into two categories: encoder-\nand decoder-focused architectures. Some encoder-focused\nworks (Kendall, Gal, and Cipolla 2018; Chen et al. 2018)\nrely on a shared encoder to learn a general visual feature.\nThe features from the shared encoder are input into the task-\nspecific heads to perform the dense predictions for every\ntask. The decoder-focused works (Gao et al. 2019; Zhang\net al. 2019; Bruggemann et al. 2021) use a shared backbone\nnetwork to extract a shared feature for each task. Then, the\ndesigned task-specific module captures valuable information\nfrom the shared feature. However, these MTL methods pri-\nmarily focus on the shared feature, which is hard to disen-\ntangle the task-specific features.\nDeformable CNNs and Transformers.\nDeformable CNNs.Deformable ConvNets (Dai et al. 2017;\nZhu et al. 2019) harness the enriched modeling capability of\nCNNs (Yang et al. 2020) via deformable convolution and de-\nformable spatial locations. Deformable ConvNets is the first\nto achieve competitive results in vision tasks (e.g.,object de-\ntection and semantic segmentation, etc.) using deformable\nconvolution. Deformable transformers (Zhu et al. 2021; Xia\net al. 2022) attend to a small set of crucial sampling points\naround a reference and capture more informative features.\nTransformers. Transformer and attention (Vaswani et al.\n2017) mechanism models were first employed in natural lan-\nguage processing (NLP) tasks with good performance. Re-\ncently, the transformer structures have also produced im-\npressive results in computer vision and tend to replace CNN\nprogressively. ViT (Dosovitskiy et al. 2021) is the first work\nto derive from the attention mechanism for computer vision\ntasks. More Transformer-based approaches (Carion et al.\n2020; Liu et al. 2021b; Ranftl, Bochkovskiy, and Koltun\n2021; Wang et al. 2021; Lan et al. 2022; Ru et al. 2022) have\nbeen introduced by improving the attention mechanism for\ndense prediction tasks. Recently, these works are also ex-\ntended to the MTL domain to learn good representations for\nmultiple task predictions. In contrast, we find the deformed\nfeatures to focus on the valuable region for different tasks. In\naddition, we use the query-based transformer approach for\nmodeling and leverage deformed features as queries in trans-\nformer calculations to enhance task-relevant features. These\nqueries can naturally disentangle the task-specific feature\nfrom the fused feature. Our approach combines the respec-\ntive advantages of CNN and Transformer, achieving state-\nof-the-art on NYUD-v2 and PASCAL-Context datasets.\nThe DeMT Method\nOverall Architecture\nWe describe the overall framework of our architecture in\nFigure 1. DeMT is the result of a non-shared encoder-\ndecoder procedure: First, we design a deformable mixer en-\ncoder to encode task-specific spatial features for each task.\nSecond, the task interaction block and task query block are\n3073\nFeature\nExtractor\nImage data\nùìõseg\nùìõdepth\nùìõnormal\nùìõbound\nDeformable\nMixer\nFeature Map\nq\nk,v\nDeformable\nMixer\nDeformable\nMixer\nDeformable\nMixer\nTask Query \nBlock\nTask Query \nBlock\nTask Query \nBlock\nTask Query \nBlock\nq\nk,v\nk,v\nk,v\nq\nqùìõ\nd: depth\nq: query\n: loss\nk: key\nv: value\nTask \nInteraction \nBlock\nHead\nHead\nHead\nHead\nDeformable Mixer Encoder Task-aware Transformer Decoder\n√ód\n√ód\n√ód\n√ód\nx\nx: feature\nFigure 1: An overview of our model jointly handles multiple tasks with a unified encoder-decoder architecture. Our DeMT\nmodel consists of the deformable mixer encoder and task-aware transformer decoder. The depth d is the number of repetitions\nof the Deformable Mixer (ablation on the d in Table 3b).\nproposed to model the decode the task interaction informa-\ntion and decode task-specific features via self-attention. In\nthe following section, we describe our task losses.\nFeature Extractor\nThe feature extractor is utilized to aggregate multi-scale fea-\ntures and manufacture a shared feature map for each task.\nThe initial image data Xin ‚àà RH√óW√ó3 (3 means image\nchannel) is input to the backbone, which then generates four\nstages of image features. Then the four stage image fea-\ntures are up-sampled to the same resolution, and then they\nare concatenated along the channel dimension to obtain an\nimage feature X ‚àà R\nH\n4 √óW\n4 √óC, where H, W, and C are the\nheight, width, and channel of the image feature, respectively.\nDeformable Mixer Encoder\nThe motivation.Inspired by the success of the Deformable\nConvNets (Zhu et al. 2019) and Deformable DETR mod-\nels (Zhu et al. 2021), we propose the deformable mixer en-\ncoder that adaptively provides more efficient receptive fields\nand sampling spatial locations for each task. For this pur-\npose, the deformable mixer encoder is designed to separate\nthe mixing of spatial-aware deformable spatial features and\nchannel-aware location features. As shown in Figure 2 (left),\nthe spatial-aware deformable and channel-aware mixing op-\nerators are interleaved to enable interaction of both input\nfeature dimensions (HW √ó C).\nSpecifically, we propose a deformable mixer encoder to\ncapture the unique receptive regions corresponding to the in-\ndividual task. The deformable mixer only attends to a small\nset of crucial sampling points which are learnable offset.\nThe spatial-aware deformable is capable of modeling spa-\ntial context aggregation. Then the spatial-aware deformable,\nchannel-aware mixing, and layer normalization operators\nare stacked to form one deformable mixer. The effect of the\ndepth of the deformable mixer stack on the model is shown\nin the Table 3b ablation experiment.\nThe deformable mixer encoder structure is shown in Fig-\nure 2. First, a linear layer reduces the channel dimension of\nthe image feature X ‚àà R\nH\n4 √óW\n4 √óC from C to a smaller di-\nmension C‚Ä≤. The linear layer can be written as follows:\nX = W ¬∑ Norm(X), (1)\nwhere Norm means LayerNorm function. After the linear\nlayer, we obtain a smaller dimension image feature mapX ‚àà\nR\nH\n4 √óW\n4 √óC‚Ä≤\nas the input for the downstream.\nSpatial-aware deformable.Given the input image feature\nXi,j ‚àà R\nH\n4 √óW\n4 √óC‚Ä≤\nfrom Eq.(1) output, the point (i, j) is\nthe spatial location on the single channel.\nTo generate the relative offsets with respect to the refer-\nence point, the image feature Xi,j is fed to the convolution\noperator to learn the corresponding offsets ‚àÜ(i,j) for all ref-\nerence points. For each location point (i, j) on the image\nfeature X, the spatial deformable can be written as:\nDS(Xi,j) =\nC‚Ä≤‚àí1X\nC‚Ä≤=0\nW1 ¬∑ X((i, j) + ‚àÜ(i,j), C‚Ä≤), (2)\nwhere the W1 is a deformable weight. The ‚àÜ(i,j) is the\nlearnable offset. The spatial-aware deformable is followed\nby a GELU activation, BatchNorm, and residual connection:\nXC‚Ä≤ = Xi,j + BN(œÉ(DS(Xi,j))), (3)\nwhere œÉ(¬∑) is the non-linearity function (GELU); BN is the\nBatchNorm operation.\n3074\nTask-query \nMHSA\nDeformable\nMixerDeformable\nMixer\nTask Query\nBlock\nTask Interaction \nBlock\nMHSA\n(q=xq, k=xf, v=xf)\nLN\nLN\nsMLP\nMHSA\n(q=xf, k=xf, v=xf)\nxq xq1 2\n1\nx xq xq\n1 2 xf\nxf\nxq\n1\nFeature\nMap: x xq\n1\nxq\n2\nxf\nReshape\nConcat\nsMLP\nDeformable Mixer Encoder Task-aware Transformer Decoder\nLinear\nReshape\nx1\nx2\nÃÇÃÇ\nÃÇÃÇ\nTT\nT\nT\n(N√óC‚Äô)(N√óC‚Äô) (2N√óC‚Äô) (H/4√óW/4√óC‚Äô)\nGELU & BN\nSpatial-aware \nDeformable\nChannel-aware \nMixing\nGELU & BN\nÃÇÃÇ\nÃÇÃÇ\nÃÇÃÇ\nÃÇÃÇ ÃÇÃÇ\nFigure 2: Illustration of our DeMT components. For sim-\nplicity, we assume there are two tasks (T=2) in this figure.\nSmall MLP (sMLP) only consists of Linear and LayerNorm\nfunctions.\nChannel-aware mixing.The channel-aware mixing allows\ncommunication between different channels. The channel-\naware mixing applies the standard point-wise convolution\n(the convolving kernel is 1√ó1) to mix channel locations.\nGiven the input image feature XC‚Ä≤ is from Eq. 3. It can be\nformulated as:\nXC‚Ä≤ =\nC‚Ä≤‚àí1X\nC‚Ä≤=0\nW2 ¬∑ XC‚Ä≤ + b, (4)\nwhere the W2 is the point-wise convolution weight. b is a\nlearnable bias. Subsequently, we add GELU activation and\nBatchNorm as well. This operation is calculated as:\nXq = Reshape(BN(œÉ(XC‚Ä≤ ))), (5)\nwhere the Reshape is applied to flatten the feature Xq ‚àà\nR\nH\n4 √óW\n4 √óC‚Ä≤\nto a sequence RN√óC‚Ä≤\n(N = H\n4 √ó W\n4 ). When\nthere are T tasks, the deformable mixer encoder generate a\nfeature set (X 1\nq , X2\nq , ¬∑¬∑¬∑ XT\nq ) (T means task number) (See\nFigure 2). These output task-specific features are learned by\na deformable mixer that we refer to as deformed features,\nwhich we add to the input of the downstream blocks (task\ninteraction block and task query block).\nTask-aware Transformer Decoder\nIn the task-aware transformer decoder, we design the task\ninteraction block and task query block (See Figure 2). It is\nimportant for MTL to consider task interactions. Thus, we\npropose a task interaction block to capture the task inter-\nactions at every task via an attention mechanism. Each task\ninteraction block is composed of two parts, i.e., a multi-head\nself-attention module (MHSA) and a small Multi-Layer Per-\nceptron (sMLP). The downstream task query block also con-\nsists of the MHSA and the sMLP. The difference between\nthe task interaction block and the task query block is that\ntheir query features are fundamentally different. The feature\nis projected into the queries (Q), keys (K) and values (V) of\ndimension dk and self attention is being computed by the Q,\nK and V. The self-attention operator is calculated as:\nMHSA(Q, K, V) = softmax(QKT\n‚àödk\n)V, (6)\nwhere Q ‚àà RN√óC‚Ä≤\n, K ‚àà RN√óC‚Ä≤\nand V ‚àà RN√óC‚Ä≤\nare the\nquery, key and value matrices; MHSA(Q, K, V) ‚àà RN√óC‚Ä≤\n.\nTask interaction block. As illustrated in Figure 2 (cen-\nter), We first concatenate the deformed features from the de-\nformable mixer encoder output.\nXf = Concat(X1\nq , X2\nq , ¬∑¬∑¬∑ XT\nq ), (7)\nwhere Xf ‚àà RTN √óC‚Ä≤\nis the fused feature. The T means\ntask number in XT\nq ‚àà RN√óC‚Ä≤\n. Then, for efficient task in-\nteraction, we construct a self-attention strategy via the fused\nfeature Xf :\nX‚Ä≤\nf = MHSA(Q, K, V= LN(Xf ), LN(Xf ), LN(Xf )),\n(8)\nÀÜXf = sMLP(X‚Ä≤\nf ), (9)\nwhere ÀÜXf ‚àà RTN √óC‚Ä≤\nis the task-interacted feature. LN\nmeans LayerNorm function. sMLP consists of a linear layer\nand a LayerNorm.\nTask query block.As illustrated in Figure 2 (right), we\ntake the deformed feature Xq as task query and the task-\ninteracted feature ÀÜXf as key & value to MHSA. The de-\nformed feature is applied as a query in MHSA to decode the\ntask awareness feature from the task-interacted feature for\neach task prediction. We first apply the LayerNorm in paral-\nlel to generate queries Q, keys K and values V :\nÀÜQ = LN(Xq), ÀÜK = LN( ÀÜXf ), ÀÜV = LN( ÀÜXf ), (10)\nwhere LN is the layer normalization.Xq and ÀÜXf are the out-\nput of deformable mixer encoder and task interaction block,\nrespectively. Then, the task query block operation using a\nMHSA is calculated as:\nÀÜXq = MHSA( ÀÜQ, ÀÜK, ÀÜV ), (11)\nÀÜX = Reshape(Xq + sMLP( ÀÜXq)), (12)\nwhere the residual feature Xq comes from Eq. (3). The\ntask awareness feature ÀÜX ‚àà R\nH\n4 √óW\n4 √óC‚Ä≤\nis reshaped from\nRN√óC‚Ä≤\n(N = H\n4 √ó W\n4 ) via Reshape operation.\nLoss Function\nFor balancing the loss contribution for each task, we set the\nweight Œ±t to decide the loss contribution for the task t. A\nweighted sum Ltotal of task-specific losses:\nLtotal =\nTX\nt=1\nŒ±tLt, (13)\nwhere the Lt is a loss function for task t. For fair compar-\nisons, we useŒ±t and Lt consistent with ATRC (Bruggemann\net al. 2021) and MQTransformer (Xu et al. 2022b).\n3075\nModel Backbone Params GFLOPs SemSeg Depth Normal Bound ‚àÜm[%]‚Üë(M) (G) (mIoU)‚Üë (rmse)‚Üì (mErr)‚Üì (odsF)‚Üë\nsingle task baseline HRNet18 16.09 40.93 38.02 0.6104 20.94 76.22 0.00\nmulti-task baseline HRNet18 4.52 17.59 36.35 0.6284 21.02 76.36 -1.89\nCross-Stitch(Misra et al. 2016) HRNet18 4.52 17.59 36.34 0.6290 20.88 76.38 -1.75\nPad-Net(Xu et al. 2018) HRNet18 5.02 25.18 36.70 0.6264 20.85 76.50 -1.33\nPAP(Zhang et al. 2019) HRNet18 4.54 53.04 36.72 0.6178 20.82 76.42 -0.95\nPSD(Ling et al. 2020) HRNet18 4.71 21.10 36.69 0.6246 20.87 76.42 -1.30\nNDDR-CNN(Gao et al. 2019) HRNet18 4.59 18.68 36.72 0.6288 20.89 76.32 -1.51\nMTI-Net(Vandenhende et al. 2020) HRNet18 12.56 19.14 36.61 0.6270 20.85 76.38 -1.44\nATRC(Bruggemann et al. 2021) HRNet18 5.06 25.76 38.90 0.6010 20.48 76.34 1.56\nDeMT (Ours) HRNet18 4.76 22.07 39.18 0.5922 20.21 76.40 2.37\nsingle task baseline Swin-T 115.08 161.25 42.92 0.6104 20.94 76.22 0.00\nmulti-task baseline Swin-T 32.50 96.29 38.78 0.6312 21.05 75.60 -3.74\nMQTransformer(Xu et al. 2022b) Swin-T 35.35 106.02 43.61 0.5979 20.05 76.20 0.31\nDeMT (Ours) Swin-T 32.07 100.70 46.36 0.5871 20.65 76.90 3.36\nsingle task baseline Swin-S 200.33 242.63 48.92 0.5804 20.94 77.20 0.00\nmulti-task baseline Swin-S 53.82 116.63 47.90 0.6053 21.17 76.90 -1.96\nMQTransformer(Xu et al. 2022b) Swin-S 56.67 126.37 49.18 0.5785 20.81 77.00 1.59\nDeMT (Ours) Swin-S 53.03 121.05 51.50 0.5474 20.02 78.10 4.12\nTable 1: Comparison of the MTL models with state-of-the-art on NYUD-v2 dataset. The notation ‚Äô‚Üì‚Äô: lower is better. The\nnotation ‚Äô‚Üë‚Äô: higher is better. ‚àÜm denotes average per-task performance drop. ‚ÄùParams‚Äù denotes parameters.\nExperiment\nIn this section, we conduct extensive experiments on two\nwidely-used dense prediction datasets to evaluate the per-\nformance of our method on different metrics. We also show\nthe visualization results on different datasets.\nExperimental Setup\nImplementation. All the leveraged backbones generate four\nscales (1/4, 1/8, 1/16, 1/32) features to perform multi-scale\naggregation in our feature extractor (Section ). We train\nour model with SGD setting the learning rate to 10‚àí3 and\nweight decay to 5 √ó 10‚àí4. The whole experiments are per-\nformed with pre-trained models on ImageNet. All our ex-\nperiments are performed on the Pytorch platform with eight\nA100 SXM4 40GB GPUs.\nDatasets. We conduct experiments on two publicly ac-\ncessible datasets, NYUD-v2 (Silberman et al. 2012) and\nPASCAL-Context (Chen et al. 2014). NYUD-V2 is com-\nprised of pairs of RGB and Depth frames that 795 images\nare used for training and 654 images for testing. NYUD-V2\nusually is mainly adopted for semantic segmentation (‚ÄòSem-\nSeg‚Äô), depth estimation (‚ÄòDepth‚Äô), surface normal estimation\n(‚ÄòNormal‚Äô), and boundary detection (‚ÄòBound‚Äô) tasks by pro-\nviding dense labels for every image. PASCAL-Context train-\ning and validation contain 10103 images, while testing con-\ntains 9637 images. PASCAL-Context usually is adopted for\nsemantic segmentation (‚ÄôSemSeg‚Äô), human parts segmenta-\ntion (‚ÄôPartSeg‚Äô), saliency estimation (‚ÄôSal‚Äô), surface normal\nestimation (‚ÄôNormal‚Äô), and boundary detection (‚ÄôBound‚Äô)\ntasks by providing annotations for the whole scene.\nMetrics. We adopt five evaluation metrics to compare our\nmodel with other prior multi-task models: mean Intersec-\ntion over Union (mIoU), root mean square error (rmse),\nmean Error (mErr), optimal dataset scale F-measure (odsF),\nand maximum F-measure (maxF). The average per-task per-\nformance drop (‚àÜ m) is used to quantify multi-task perfor-\nmance. ‚àÜm = 1\nT\nPT\ni=1(Fm,i ‚àí Fs,i)/Fs,i √ó 100%, where\nm, s and T mean multi-task model, single task baseline and\ntask numbers. ‚àÜm: the higher is the better.\nBackbones. We test our method using several CNN and\nVision Transformer backbones: HRNetV2-W18-small (HR-\nNet18), HRNetV2-W48 (HRNet48) (Sun et al. 2019), Swin-\nTiny (Swin-T), Swin-Small (Swin-S) and Swin-Base (Swin-\nB) (Liu et al. 2021b).\nComparison with the State-of-the-art\nWe compare our model with CNN-based and Transformer-\nbased models to show the advantages of our method.\nNYUD-v2. The Comparisons with state-of-the-art models\non the NYUD-v2 dataset are shown in Table 1. We first\nreport results comparison with three different backbones:\nHRNet18, Swin-T, and Swin-S. We demonstrate simulta-\nneous performance improvements over prior work in hav-\ning smaller parameters, a smaller number of GFLOPs, and\nbetter semantic segmentation, depth estimation, surface nor-\nmal and boundary detection accuracies. For example, a per-\nformance comparison between MQTransformer and DeMT\nproves the effectiveness of our framework. Besides this,\nDeMT also consistently outperforms previous state-of-the-\nart Transformer-based models, such as ATRC (Bruggemann\net al. 2021) and MQTransformer (Xu et al. 2022b). In addi-\ntion, we also observe that using a transformer as a backbone\nmodel is more promising compared to CNN as the back-\nbone. Because Transformer-based and CNN-based models\nuse similar GFLOPs, the former shows higher accuracy in all\nmetrics. Our DeMT obtains 46.36 SemSeg accuracy, which\nis 6.3% higher than that of MQTransformer with the same\nSwin-T backbone and slightly lower FLOPs (100.7G vs.\n106.02G). MuIT (Bhattacharjee et al. 2022) reports a 13.3%\nand 8.54% increase in relative performance for semantic\n3076\nModel Backbone SemSeg PartSeg Sal Normal Bound ‚àÜm[%]‚Üë(mIoU)‚Üë (mIoU)‚Üë (maxF)‚Üë (mErr)‚Üì (odsF)‚Üë\nsingle task baseline HRNet18 62.23 61.66 85.08 13.69 73.06 0.00\nmulti-task baseline HRNet18 51.48 57.23 83.43 14.10 69.76 -6.77\nPAD-Net (Xu et al. 2018) HRNet18 53.60 59.60 65.80 15.3 72.50 -4.41\nATRC (Bruggemann et al. 2021) HRNet18 57.89 57.33 83.77 13.99 69.74 -4.45\nMQTransformer(Xu et al. 2022b) HRNet18 58.91 57.43 83.78 14.17 69.80 -4.20\nDeMT (Ours) HRNet18 59.23 57.93 83.93 14.02 69.80 -3.79\nsingle task baseline Swin-T 67.81 56.32 82.18 14.81 70.90 0.00\nmulti-task baseline Swin-T 64.74 53.25 76.88 15.86 69.00 -3.23\nMQTransformer(Xu et al. 2022b) Swin-T 68.24 57.05 83.40 14.56 71.10 1.07\nDeMT (Ours) Swin-T 69.71 57.18 82.63 14.56 71.20 1.75\nsingle task baseline Swin-S 70.83 59.71 82.64 15.13 71.20 0.00\nmulti-task baseline Swin-S 68.10 56.20 80.64 16.09 70.20 -3.97\nMQTransformer(Xu et al. 2022b) Swin-S 71.25 60.11 84.05 14.74 71.80 1.27\nDeMT (Ours) Swin-S 72.01 58.96 83.20 14.57 72.10 1.36\nsingle task baseline Swin-B 74.91 62.13 82.35 14.83 73.30 0.00\nmulti-task baseline Swin-B 73.83 60.59 80.75 16.35 71.10 -3.81\nDeMT (Ours) Swin-B 75.33 63.11 83.42 14.54 73.20 1.04\nTable 2: Comparison of the MTL models with state-of-the-art on PASCAL-Context dataset. The notation ‚Äò‚Üì‚Äô: lower is better.\nThe notation ‚Äò‚Üë‚Äô: higher is better. ‚àÜm denotes average per-task performance drop (the higher is the better).\nsegmentation and depth tasks. While we have a 14.74% and\n9.43% increase. The comparison results show our model\nalso achieves good performance, evaluating the flexibility of\nour model. By comparison, our DeMT achieves new records\non the NYUD-v2, which are remarkably superior to previ-\nous CNNs and Transformers models in terms of all metrics.\nPASCAL-Context. We also evaluate the proposed DeMT\non PASCAL-Context with three backbones: HRNet18,\nSwin-T, Swin-S, and Swin-B. Table 2 shows the compar-\nison results. Our model obtains significantly better results\nwhen compared with the baseline and other models. For ex-\nample, DeMT improves MQTransformer (Xu et al. 2022b)\nwith the same Swin-T backbone by 1.47 point in SemSeg.\nOur DeMT achieves the best performance among models on\nseveral metrics and can reach a high performance of 75.33\nin the SemSeg task.\nAblation Studies\nWe ablate DeMT to understand the contribution of each\ncomponent and setting using Swin-T on NYUD-v2 dataset.\nAblation on modules.The DeMT model consists of three\ncomponents: deformable mixer, task interaction, and task\nquery blocks. As shown in Table 3a, we demonstrate the\nadvantages of the deformable mixer, task interaction, and\ntask query blocks. We observe that task interaction block has\nmore effect on the performance, and it is essential to inter-\nact the whole task features for task interaction information.\nThis indicates that task interaction and task query blocks are\nessential to the task-aware transformer decoder. From the\nFigure 4 and Table 3a it can be observed that different com-\nponents are playing a beneficial role.\nAblation on the depthsd. As shown in Figure 2, the depth\nd is the number of repetitions of the deformable mixer. We\nadd the d to analyze the effect of the depth of the deformable\nmixer on the DeMT model. In Table 3b, We vary the number\nof used deformable mixer depth (e.g., 1, 2, 4, 8) and com-\npare their performances. Comparing the first to last row in\nTable 3b, we observe the best performance when the depth\nis set to 4. However, as increasing the depth, the parameters\nand GFLOPs also become more extensive. Practically, we\nchoose a depth d = 1 for all models in this paper.\nAblation on scales.We explore the influence of using differ-\nent scale features. The backbone outputs four-scale (1/4, 1/8,\n1/16, 1/32) features. Table 3c shows the influence of using a\ndifferent number of scales. Note that the model performance\nincreases obviously with the increasing number of scales.\nOur method can capture valuable semantic information for\nmultiple tasks. Practically, we choose four-scale features for\nall models in this paper.\nAblation on backbones.Table 3d shows the results using\nthe different backbones. To deeper explore the capacity of\nthe our DeMT, we employ extensive backbones to conduct\nthe ablation experiment. It is worth noting that our DeMT\nleads to the best performance on all metrics when using\nSwin-B on NYUD-v2. In addition, we also observe the in-\nspiring fact that using a larger transformer backbone can eas-\nily reach top-tier performance. The different backbones are\ncompared to demonstrate the generalization of our method.\nVisualization\nTo deeper understand our DeMT model, we visualize the\nmultiple task predictions. We show the qualitative results in\ndifferent dimensions. For visual analysis (see Figure 3 and\nFigure 4), we employ a trained model with Swin-T. Fig-\nure 3 shows the capability of DeMT with Swin-T backbone\nto perform dense predictions with strong expressive power\nand successfully capture the task-specific features. As il-\nlustrated in Figure 3 (last two rows), the second and third\ncolumns focus mainly on specific semantics such as human,\nanimal, and other objects. Figure 4 showcases the impact of\nour approach using different components: while only the de-\nformable mixer encoder fails to visualize some objects, the\n3077\nModel SemSeg Depth Normal\nBound\n(mIoU)‚Üë (rmse)‚Üì (mErr)‚Üì (odsF)‚Üë\nbaseline 38.78 0.6312 21.05\n75.6\nw/ DM 42.40 0.6069 20.83 76.2\nw/ DM+TI 44.44 0.5969 20.75 76.4\nw/ DM+TI+TQ 46.36 0.5871 20.65 76.9\n(a) Ablation on components\nd SemSeg Depth Normal\nBound\n(mIoU)‚Üë (rmse)‚Üì (mErr)‚Üì (odsF)‚Üë\n1 46.36 0.5871 20.65\n76.9\n2 46.90 0.5622 20.05 77.0\n4 47.71 0.5613 19.90 77.1\n8 47.16 0.5518 19.87 77.1\n(b) Ablation on the depths (d)\nScale SemSeg Depth Normal\nBound\n(mIoU)‚Üë (rmse)‚Üì (mErr)‚Üì (odsF)‚Üë\n1/4 7.51 1.1961 33.26\n66.1\n1/4, 1/8 12.85 1.0433 27.66 70.4\n1/4, 1/8, 1/16 40.32 0.6966 21.44 76.3\n1/4, 1/8, 1/16, 1/32 46.36 0.5871 20.65 76.9\n(c) Ablation on scales\nBackbone SemSeg Depth Normal\nBound\n(mIoU)‚Üë (rmse)‚Üì (mErr)‚Üì (odsF)‚Üë\nHR48 baseline 41.96 0.5543\n20.36 77.6\nHR48 w/ ours 43.84 0.5517 19.88 77.7\nSwin-B baseline 51.44 0.5813 20.44 78.0\nSwin-B w/ ours 54.34 0.5209 19.21 78.5\n(d) Ablation on backbones\nTable 3: Ablation studies and analysis on NYUD-v2 dataset using a Swin-T backbone. Deformable mixer (DM), task interaction\n(TI) block, and task query (TQ) block are the parts of our model. HR48 denotes HRNet48. The notation ‚Äò‚Üì‚Äô: lower is better.\nThe notation ‚Äò‚Üë‚Äô: higher is better. The w/ indicates ‚Äùwith‚Äù.\nSemSeg NormalDepthInput Image\nSemSeg Normal SaliencyHuman PartsInput Image\nPASCAL-Context NYUD-v2\nBoundary\nBoundary\nFigure 3: The first two rows of the visualization illustrate\ntwo examples from the NYUD-v2. The last two rows of the\nvisualization illustrate the PASCAL-Context.\nthird row shows DeMT‚Äôs improvements to multiple task pre-\ndictions. Note that we not only report these results for qual-\nitative understanding of the model but also evaluate it quan-\ntitatively in Table 3a. We compared the prediction results of\nthe DeMT model with the ATRC (Figure 4 last row), and\nour results are significantly better than ATRC, especially on\nsemantic segmentation and human parts segmentation tasks\non PASCAL-Context dataset. Our DeMT model produces\nhigher-quality predictions than both the Swin baseline and\nthe existing CNN-based MTL model.\nConclusion\nIn this work, we introduce DeMT, a simple and effective\nmethod that leverages the combination of both merits of de-\nSemSeg BoundaryNormal SaliencyHuman PartsInput Image\nBase+DM+DM+TIDeMTATRC\nFigure 4: Qualitative analysis of the components on\nPASCAL-Context. Visualizations show the components in\nTable 3a. The last row shows the ATRC model visualization\nresults as a comparison.\nformable CNN and query-based Transformer for multi-task\nlearning of dense prediction. Significantly, the deformed fea-\nture produced by the deformable mixer encoder is lever-\naged as a task query in the task-aware transformer decoder\nby query-base attention to disentangle task-specific features.\nExtensive experiments on dense prediction datasets (i, e.,\nNYUD-v2 and PASCAL-Context datasets) validate the ef-\nfectiveness of our DeMT model.\nLimitations and future work.This work only uses a naive\noperation to aggregate multi-scale features and could be fur-\nther improved in two aspects: considering using the FPN or\nFPN variant to aggregate multi-scale features and how to de-\nsign flexible attention to learn more valuable information.\n3078\nAcknowledgements\nThis work was done when Yangyang Xu was a research in-\ntern at JD Explore Academy. This work was supported by\nthe National Natural Science Foundation of China under\nGrants 62122060, 62076188, and the Special Fund of Hubei\nLuojia Laboratory under Grant 220100014.\nReferences\nBhattacharjee, D.; Zhang, T.; S ¬®usstrunk, S.; and Salzmann,\nM. 2022. MulT: An End-to-End Multitask Learning Trans-\nformer. In CVPR, 12031‚Äì12041.\nBruggemann, D.; Kanakis, M.; Obukhov, A.; Georgoulis,\nS.; and Gool., L. V . 2021. Exploring relational context for\nmulti-task dense prediction. In ICCV, 15869‚Äì15878.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213‚Äì229.\nChen, X.; Mottaghi, R.; Liu, X.; Fidler, S.; Urtasun, R.; and\nYuille, A. 2014. Detect what you can: Detecting and rep-\nresenting objects using holistic models and body parts. In\nCVPR, 1971‚Äì1978.\nChen, Z.; Badrinarayanan, V .; Lee, C.-Y .; and Rabinovich,\nA. 2018. Gradnorm: Gradient normalization for adaptive\nloss balancing in deep multitask networks. In ICML, 794‚Äì\n803.\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and\nWei, Y . 2017. Deformable convolutional networks. InICCV,\n764‚Äì773.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nGao, Y .; Ma, J.; Zhao, M.; Liu, W.; and Yuille, A. L. 2019.\nNddr-cnn: Layerwise feature fusing in multi-task cnns by\nneural discriminative dimensionality reduction. In CVPR,\n3205‚Äì3214.\nGhiasi, G.; Zoph, B.; Cubuk, E. D.; Le, Q. V .; and Lin, T.-Y .\n2021. Multi-Task Self-Training for Learning General Rep-\nresentations. In ICCV, 8856‚Äì8865.\nKendall, A.; Gal, Y .; and Cipolla, R. 2018. Multi-task learn-\ning using uncertainty to weigh losses for scene geometry and\nsemantics. In CVPR, 7482‚Äì7491.\nLan, M.; Zhang, J.; He, F.; and Zhang, L. 2022. Siamese\nNetwork with Interactive Transformer for Video Object Seg-\nmentation. In AAAI, 1228‚Äì1236.\nLing, Z.; Zhen, C.; Chunyan, X.; Zhenyu, Z.; Chaoqun, W.;\nTong, Z.; and Jian, Y . 2020. Pattern-Structure Diffusion for\nMulti-Task Learning. In CVPR, 4514‚Äì4523.\nLiu, L.; Li, Y .; Kuang, Z.; Xue, J.; Chen, Y .; Yang, W.; Liao,\nQ.; and Zhang, W. 2021a. Towards impartial multi-task\nlearning. In ICLR.\nLiu, S.; Johns, E.; and Davison, A. J. 2019. End-to-end\nmulti-task learning with attention. In CVPR, 1871‚Äì1880.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. In ICCV, 10012‚Äì\n10022.\nMisra, I.; Shrivastava, A.; Gupta, A.; and Hebert, M. 2016.\nCross-stitch networks for multi-task learning. In CVPR,\n3994‚Äì4003.\nPhillips, J.; Martinez, J.; B ÀÜarsan, I. A.; Casas, S.; Sadat, A.;\nand Urtasun, R. 2021. Deep multi-task learning for joint\nlocalization, perception, and prediction. In CVPR, 4679‚Äì\n4689.\nRanftl, R.; Bochkovskiy, A.; and Koltun, V . 2021. Vision\ntransformers for dense prediction. In CVPR, 12179‚Äì12188.\nRu, L.; Zhan, Y .; Yu, B.; and Du, B. 2022. Learning Affinity\nfrom Attention: End-to-End Weakly-Supervised Semantic\nSegmentation with Transformers. In CVPR, 16846‚Äì16855.\nSilberman, N.; Hoiem, D.; Kohli, P.; and Fergus, R. 2012.\nIndoor segmentation and support inference from rgbd im-\nages. In ECCV, 746‚Äì760.\nSun, K.; Xiao, B.; Liu, D.; and Wang, J. 2019. Deep High-\nResolution Representation Learning for Human Pose Esti-\nmation. In CVPR, 5693‚Äì5703.\nVandenhende, S.; Georgoulis, S.; Gansbeke, W. V .; Proes-\nmans, M.; Dai, D.; and Gool, L. V . 2021. Multi-Task Learn-\ning for Dense Prediction Tasks: A Survey. IEEE TPAMI.\nVandenhende, S.; Georgoulis, S.; Van Gool, L.; and\nVan Gool, L. 2020. Mti-net: Multi-scale task interaction net-\nworks for multi-task learning. In ECCV, 527‚Äì543.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. NeurIPS.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction Without\nConvolutions. In ICCV, 568‚Äì578.\nXia, Z.; Pan, X.; Song, S.; Li, L. E.; and Huang, G. 2022. Vi-\nsion transformer with deformable attention. InCVPR, 4794‚Äì\n4803.\nXu, D.; Ouyang, W.; Wang, X.; and Sebe, N. 2018. Pad-\nnet: Multi-tasks guided prediction-and-distillation network\nfor simultaneous depth estimation and scene parsing. In\nCVPR, 675‚Äì684.\nXu, X.; Zhao, H.; Vineet, V .; Lim, S.-N.; and Torralba, A.\n2022a. MTFormer: Multi-task Learning via Transformer\nand Cross-Task Reasoning. In ECCV, 304‚Äì321.\nXu, Y .; Li, X.; Yuan, H.; Yang, Y .; Zhang, J.; Tong,\nY .; Zhang, L.; and Tao, D. 2022b. Multi-Task Learning\nwith Multi-query Transformer for Dense Prediction. arXiv\npreprint arXiv:2205.14354.\nYang, Y .; Li, H.; Li, X.; Zhao, Q.; Wu, J.; and Lin, Z. 2020.\nSognet: Scene overlap graph network for panoptic segmen-\ntation. In AAAI.\nZhang, Z.; Cui, Z.; Xu, C.; Yan, Y .; Sebe, N.; and Yang, J.\n2019. Pattern-affinitive propagation across depth, surface\nnormal and semantic segmentation. In CVPR, 4106‚Äì4115.\n3079\nZhu, X.; Hu, H.; Lin, S.; and Dai, J. 2019. Deformable con-\nvnets v2: More deformable, better results. In CVPR, 9308‚Äì\n9316.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable DETR: Deformable Transformers for End-to-\nEnd Object Detection. In ICLR.\n3080",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8094992637634277
    },
    {
      "name": "Transformer",
      "score": 0.6839408874511719
    },
    {
      "name": "Encoder",
      "score": 0.6262025237083435
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5716953873634338
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5316479802131653
    },
    {
      "name": "Multi-task learning",
      "score": 0.4357505142688751
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4064696431159973
    },
    {
      "name": "Task (project management)",
      "score": 0.2138659656047821
    },
    {
      "name": "Voltage",
      "score": 0.12409713864326477
    },
    {
      "name": "Engineering",
      "score": 0.0679546594619751
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}