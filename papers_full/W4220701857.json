{
    "title": "Protein language models trained on multiple sequence alignments learn phylogenetic relationships",
    "url": "https://openalex.org/W4220701857",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5060676115",
            "name": "Umberto Lupo",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5086331988",
            "name": "Damiano Sgarbossa",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5030010662",
            "name": "Anne‐Florence Bitbol",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2065921821",
        "https://openalex.org/W2593641128",
        "https://openalex.org/W4297734170",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W3010387158",
        "https://openalex.org/W3186612807",
        "https://openalex.org/W3216580380",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3186179742",
        "https://openalex.org/W3191761521",
        "https://openalex.org/W1995924392",
        "https://openalex.org/W2053671774",
        "https://openalex.org/W2146891463",
        "https://openalex.org/W2151457629",
        "https://openalex.org/W1979762151",
        "https://openalex.org/W2061042699",
        "https://openalex.org/W2008545402",
        "https://openalex.org/W1965839094",
        "https://openalex.org/W2137566700",
        "https://openalex.org/W2048310584",
        "https://openalex.org/W2783644078",
        "https://openalex.org/W2769882797",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W4210494137",
        "https://openalex.org/W74946117",
        "https://openalex.org/W1966399595",
        "https://openalex.org/W3044778276",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3037888463",
        "https://openalex.org/W3146384714",
        "https://openalex.org/W2890223884",
        "https://openalex.org/W2995361046",
        "https://openalex.org/W3208082951",
        "https://openalex.org/W2772248591",
        "https://openalex.org/W2884696611",
        "https://openalex.org/W2988868846",
        "https://openalex.org/W3165376129",
        "https://openalex.org/W2137991504",
        "https://openalex.org/W2979846625",
        "https://openalex.org/W2983468544",
        "https://openalex.org/W4206510828",
        "https://openalex.org/W4210672654",
        "https://openalex.org/W4223950755",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3122896559",
        "https://openalex.org/W2181523240",
        "https://openalex.org/W3211728297",
        "https://openalex.org/W4206925492"
    ],
    "abstract": "Abstract Self-supervised neural language models with attention have recently been applied to biological sequence data, advancing structure, function and mutational effect prediction. Some protein language models, including MSA Transformer and AlphaFold’s EvoFormer, take multiple sequence alignments (MSAs) of evolutionarily related proteins as inputs. Simple combinations of MSA Transformer’s row attentions have led to state-of-the-art unsupervised structural contact prediction. We demonstrate that similarly simple, and universal, combinations of MSA Transformer’s column attentions strongly correlate with Hamming distances between sequences in MSAs. There-fore, MSA-based language models encode detailed phylogenetic relationships. We further show that these models can separate coevolutionary signals encoding functional and structural constraints from phylogenetic correlations reflecting historical contingency. To assess this, we generate synthetic MSAs, either without or with phylogeny, from Potts models trained on natural MSAs. We find that unsupervised contact prediction is substantially more resilient to phylogenetic noise when using MSA Transformer versus inferred Potts models.",
    "full_text": null
}