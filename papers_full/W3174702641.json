{
    "title": "BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks",
    "url": "https://openalex.org/W3174702641",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2606038871",
            "name": "Jong Hoon Oh",
            "affiliations": [
                "Nara Institute of Science and Technology",
                "Northwest African American Museum"
            ]
        },
        {
            "id": "https://openalex.org/A2107204645",
            "name": "Ryu Iida",
            "affiliations": [
                "Nara Institute of Science and Technology",
                "Northwest African American Museum"
            ]
        },
        {
            "id": "https://openalex.org/A1222420776",
            "name": "Julien Kloetzer",
            "affiliations": [
                "Northwest African American Museum",
                "Nara Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2731654628",
            "name": "KENTARO TORISAWA",
            "affiliations": [
                "Nara Institute of Science and Technology",
                "Northwest African American Museum"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3034797320",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W3102127365",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2886690398",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3085422212",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3023672669",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1928278792",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W3116641301",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W2950621961",
        "https://openalex.org/W2609826708",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W4294329082",
        "https://openalex.org/W2734823783",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2971105107",
        "https://openalex.org/W2952319120",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2799081691",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W2970029631",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2951561177",
        "https://openalex.org/W2123228027",
        "https://openalex.org/W2481265265",
        "https://openalex.org/W3098266846",
        "https://openalex.org/W2994915912",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2953356739"
    ],
    "abstract": "Jong-Hoon Oh, Ryu Iida, Julien Kloetzer, Kentaro Torisawa. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2103–2115\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2103\nBERTAC: Enhancing Transformer-based Language Models with\nAdversarially Pretrained Convolutional Neural Networks\nJong-Hoon Ohx Ryu Iidax{ Julien Kloetzerx Kentaro Torisawax{\nData-driven Intelligent System Research Center (DIRECT),\nNational Institute of Information and Communications Technology (NICT) x\nGraduate School of Science and Technology, NAIST {\nfrovellia, ryu.iida, julien, torisawag@nict.go.jp\nAbstract\nTransformer-based language models (TLMs),\nsuch as BERT, ALBERT and GPT-3, have\nshown strong performance in a wide range of\nNLP tasks and currently dominate the ﬁeld\nof NLP. However, many researchers wonder\nwhether these models can maintain their dom-\ninance forever. Of course, we do not have\nanswers now, but, as an attempt to ﬁnd bet-\nter neural architectures and training schemes,\nwe pretrain a simple CNN using a GAN-style\nlearning scheme and Wikipedia data, and then\nintegrate it with standard TLMs. We show\nthat on the GLUE tasks, the combination of\nour pretrained CNN with ALBERT outper-\nforms the original ALBERT and achieves a\nsimilar performance to that of SOTA. Fur-\nthermore, on open-domain QA (Quasar-T and\nSearchQA), the combination of the CNN with\nALBERT or RoBERTa achieved stronger per-\nformance than SOTA and the original TLMs.\nWe hope that this work provides a hint for\ndeveloping a novel strong network architec-\nture along with its training scheme. Our\nsource code and models are available at\nhttps://github.com/nict-wisdom/bertac.\n1 Introduction\nTransformer-based language models (TLMs) such\nas BERT ( Devlin et al. , 2019), ALBERT ( Lan\net al., 2020), and GPT-3 ( Brown et al. , 2020) have\nshown that large-scale self-supervised pretraining\nleads to strong performance on various NLP tasks.\nMany researchers have used TLMs for various\ndownstream tasks, possibly as subcomponents of\ntheir methods, and/or they have focused on scaling\nup TLMs or improving their pretraining schemes.\nAs a result, other architectures like Recurrent Neu-\nral Networks (RNN) ( Hochreiter and Schmidhu-\nber, 1997; Cho et al. , 2014) and Convolutional\nNeural Networks (CNN) ( LeCun et al. , 1999) are\nfading away. In this work, we propose a method\n!!\n!\"\n!#\n!!!\"\" !#\"\"\n!\"\" \n!\"\" \n!!! \n#$%&'( \n... \n!!! \n!\"#$\"# !\"#$%&'()'*+ \n!!! \n!\"#$$%&%'( \n)*+,-.&/(0-\" \n)*+,-.&/(0-1 \n)*+,-.&/(0-2 \n!\"#\n#$%&'( \n%&$\"# !\" !\"#$\"#%\" !#$ !\"#$\"#%\" \"\n)!*+,- !\"#$\"#%\" ! )+./,- !\"#$\"#%\" \" )+./, \n!#!###\"\n01'(#1%2'\"3-456#1'6- \n78(6-'9:'66%;<$- \n+34(%%5678 \n!\"\" \n!8;2'(1%;<-18-';1%13=\n9#$>'6-$';1';?'$ \n!!!$\" !#$\"\n!!!%\" !#%\"\nFigure 1: Overall architecture of BERTAC under\nthe setting of classiﬁcation for a two-sentence input\n(sentencex and sentencey).\nfor improving TLMs by integrating a simple con-\nventional CNN to them. We pretrained this CNN\non Wikipedia using a Generative Adversarial Net-\nwork (GAN) style training scheme ( Goodfellow\net al. , 2014), and then combined it with TLMs.\nOh et al. (2019) similarly used GAN-style train-\ning to improve a QA model using a CNN, but\ntheir training scheme was applicable only to QA-\nspeciﬁc datasets. On the other hand, similarly to\nTLM, our proposed method for training the CNN\nis independent of speciﬁc tasks. We show that the\ncombination of this CNN with TLMs can achieve\nhigher performance than that of the original TLMs\non publicly available datasets for several distinct\ntasks. We hope that this gives an insight into how\nto develop novel strong network architectures and\ntraining schemes.\nWe call our combination of a TLM and a CNN\nBERTAC (BERT-style TLM with an Adversari-\nally pretrained Convolutional neural network). Its\narchitecture is illustrated in Fig. 1. We do not\nimpose any particular restriction on the TLM in\nBERTAC, so any TLM, ALBERT ( Lan et al. ,\n2104\n!\"#$%\"&\"'()*% !\n!\" +')\"),-&(#.+/0#+')+'$+0 #10+')\"), \n2%+(340*%025(.+4 \n6+(3-+')\"),-%+7%+#+')()\"*'0 \n8+'+%()*%0 \"\n9(.+-+')\"),-%+7%+#+')()\"*'0 \n8+'+%()*%0 #\n\"!$\"#$ %+(30%+7%+#+')()\"*'0 \n*50):+0+')\"),0 !\n#!%\"#$ 5(.+0%+7%+#+')()\"*'0 \n*50):+0+')\"),0 !\n!$%&'()'*+%!,-.,(/0(1 \" ![EM] ,2-3+',4')562-!',)- \n,)1#()'1,0)'4-',(-+%*7 \"\nFigure 2: GAN-style pretraining of CNNs. The dis-\ncriminator D takes either a real representation gener-\nated by R or a fake representation generated by F as\nits input and then it predicts whether the input is a real\nor fake representation.\ns1 ::::::::::Suvarnabhumi:::::::Airporte1 is Thailand’s main\ninternational air hub.\nm1 [EM] is Thailand’s main international air hub.\ne1 Suvarnabhumi Airport\nTable 1: Example of an entity-masked sentence ( m1)\nand the original sentence ( s1)\n2020) or RoBERTa ( Liu et al. , 2019) for example,\ncan be used as a subcomponent of BERTAC.\nWe used the CNN to compute representations\nof a slightly modiﬁed version of the input given\nto a TLM. To integrate these representations with\nthose of the TLM, we stacked on top of the TLM\nseveral layers of Transformers for Integrating\nExternal Representation (TIERs) , which are our\nmodiﬁed version of normal transformers ( Vaswani\net al. , 2017). A TIER has the same architec-\nture as that of a normal transformer encoder ex-\ncept for its attention: we replace the transformer’s\nself-attention with an attention based on the rep-\nresentation provided by the CNN. We expect that,\nby keeping the basic architecture of transformer\nencoders, the CNN’s representations can be inte-\ngrated more effectively with the TLM’s original\nrepresentations.\nWe pretrained the CNN using a GAN-style\ntraining scheme in order to generate represen-\ntations of sentences rather freely without the\nconstraint of token embedding prediction in the\nmasked language modeling used for TLMs, as we\nexplain later. For the training, we used masked\nsentences autogenerated from Wikipedia. As in\nthe masked language modeling, neither human in-\ntervention nor downstream task-speciﬁc hacking\nis required. As illustrated in Fig. 2, the GAN-\nstyle training requires three networks, namely, a\ndiscriminator D and two CNN-based generators R\nand F . Once the training is done, we use the gen-\nerator F as CNN in BERTAC. The training data\nconsists of pairs of an entity mention and a sen-\ntence in which the entity mention is masked with\na special token [EM]. For example, the entity-\nmasked sentence m1 in Table 1 is obtained by\nmasking the entity mention e1, “Suvarnabhumi\nAirport,” in the original text s1. The network F\ngenerates a vector representation of the masked\nsentence ( m1), while R produces a representation\nof the masked entity ( e1). The discriminator D\ntakes representations generated by either R or F\nas the input, and it predicts which generator actu-\nally gave the representation.\nIn the original GAN, a generator learns to gen-\nerate an artiﬁcial image from random noise so that\nthe resulting artiﬁcial image is indistinguishable\nfrom given real images. By analogy, we used an\nentity-masked sentence as “random noise” and a\nmasked entity as a “real image.” In our GAN-style\ntraining, we regard the vector representation of a\nmasked entity given by generator R as a real rep-\nresentation of the entity (or the representation of\nthe “real image” in the above analogy). On the\nother hand, we regard the representation of the\nmasked sentence, generated by F , as a fake rep-\nresentation of the entity (or the representation of\nthe “artiﬁcial image” generated from the “random\nnoise” in the above analogy). This representation\nis deemed fake because the entity is masked in the\nmasked sentence, and F does not know what the\nentity is exactly. During the training, F should try\nto deceive the discriminator D by mimicking the\nreal representation and generating a fake represen-\ntation that is indistinguishable from the real rep-\nresentation of the entity generated by R. On the\nother hand, R and D, as a team, try to avoid being\nmimicked by F and also to make the mimic prob-\nlem harder for F . If everything goes well, once\nthe training is over, F should be able to generate\na fake representation of the entity that is similar to\nits real representation.\nAn interesting point is that F ’s output can be\ninterpreted in two ways: it is a representation of a\nmasked sentence because it is computed from the\nsentence, and at the same time it is a representation\nof the masked entity because it is indistinguishable\nfrom R’s representation of the entity. This duality\nsuggests that F ’s output can be seen as a represen-\ntation of the entire sentence.\n2105\nWe exploit F as a CNN in BERTAC as follows:\nﬁrst, we use F to compute a representation of a\nmasked version of the sentence originally given as\ninput to a TLM. The entity mention to be masked\nis chosen by simple rules and, if the input consists\nof multiple sentences, we generate a representa-\ntion of each (masked) input sentence and concate-\nnate these together into a single one. Then, this\nrepresentation is integrated to the output of the\nTLM through multiple TIER layers.\nOur GAN-style pretraining is conceptually sim-\nilar to TLM pretraining with masked language\nmodeling (predicting what a masked word in a\nsentence should be). However, it was designed to\npretrain a model that is able to rather freely gen-\nerate entity representations without strongly stick-\ning to the prediction of token embeddings. Our hy-\npothesis is that such freely generated representa-\ntions may be useful for improving the performance\nof downstream tasks. Moreover, we assumed that\nusing multiple text representations computed from\ndifferent perspectives (i.e., predicting token em-\nbeddings and freely generating entity representa-\ntions) would help to improve the performance of\ndownstream tasks.\nIn our experiments, we show that for the GLUE\ntasks ( Wang et al. , 2018), BERTAC’s average\nperformance on the development set was 0.7%\nhigher than that of ALBERT, which was used as\na subcomponent of BERTAC, leading to a perfor-\nmance on the test set comparable to that of SOTA\n(90.3% vs 90.8% (SOTA)). It also outperformed\nthe SOTA method of open-domain QA ( Chen\net al. , 2017) on Quasar-T ( Dhingra et al. , 2017)\nand SearchQA ( Dunn et al. , 2017) using either\nALBERT or RoBERTa. We also compared our\nmethod with alternative models using a CNN pre-\ntrained in a self-supervised (non GAN-style) man-\nner to directly predict embeddings of the entity\nmentions. Consequently, we conﬁrmed that our\nmethod worked better: only the CNN trained by\nour GAN-style pretraining gave signiﬁcant perfor-\nmance improvement over base TLMs.\nNote that the computational overhead of\nBERTAC is reasonably small. It took 20 hours\nwith 16 GPUs to pretrain a single CNN model and\n180 hours for the nine models tested with differ-\nent parameter settings in this work (cf., 480 hours\nwith 96 GPUs for pretraining DeBERTa ( He et al. ,\n2021), for example). Moreover, once pretrained,\nthe CNN models can be re-used for various down-\nstream tasks and combined with various TLMs,\nincluding potentially future ones. As for the pa-\nrameter number, BERTAC had just a 14% increase\nin parameters when ALBERT-xxlarge was used as\nits base TLM (268 M parameters for BERTAC\nvs. 235 M for ALBERT-xxlarge). We conﬁrmed\nfrom these results that BERTAC could improve\npretrained TLMs with reasonably small computa-\ntional overhead.\nThe code and models of BERTAC are available\nat https://github.com/nict-wisdom/bertac.\n2 Related Work\nPretraining TLMs with entity information :\nThere have been attempts to explicitly learn entity\nrepresentation from text corpora using TLMs ( He\net al. , 2020; Peters et al. , 2019; Sun et al. , 2020;\nWang et al. , 2020a; Xiong et al. , 2020; Zhang\net al. , 2019). Our proposed method is a comple-\nmentary alternative to these existing methods in\nthe sense that entity representations are integrated\ninto TLMs via CNNs and not directly produced by\nthe TLMs.\nFine-tuning TLMs with external resources or\nother NNs : Yang et al. (2019a) and Liu et al.\n(2020) have used knowledge graphs for augment-\ning TLMs with entity representations during ﬁne-\ntuning. Unlike these approaches, BERTAC uses\nunstructured texts rather than clean structured\nknowledge, such as knowledge graphs, to adver-\nsarially train a CNN. Other previous works have\nproposed combining CNNs or RNNs with BERT\nfor NLP tasks ( Lu et al. , 2020; Safaya et al. , 2020;\nShao et al. , 2019; Zhang et al. , 2020), but their use\nof CNNs/RNNs was task-speciﬁc, so their models\nwere not directly applicable to other tasks.\nAdversarial learning for improving TLMs : Oh\net al. (2019) proposed a CNN-based answer rep-\nresentation generator for QA that can guess the\nvector representation of answers from given why-\ntype questions and answer passages. The gen-\nerator was trained in a GAN-style manner using\nQA datasets. We took inspiration from their ad-\nversarial training scheme to train task-independent\nrepresentation generators from unsupervised texts\n(i.e., Wikipedia sentences in which an entity was\nmasked in a cloze-test style).\nELECTRA ( Clark et al. , 2020) also employed\nan adversarial technique (not a GAN) to pretrain\ntwo TLMs: A generator was trained to perform\nmasked language modeling and a discriminator\n2106\nwas trained to distinguish tokens in the training\ndata from tokens replaced by the generator. On\ndownstream tasks, only the discriminator was ﬁne-\ntuned. In BERTAC, the GAN-style pretraining\nwas applied only to the CNN, thus reducing the\ntraining cost. Furthermore, the CNN can be com-\nbined easily with any available TLM, even poten-\ntially future ones, without having to re-do the pre-\ntraining. In this work, we show that BERTAC out-\nperformed ELECTRA on the GLUE task.\nVernikos et al. (2020) proposed a method that\nused an adversarial objective and an adversarial\nclassiﬁer for regularizing the ﬁne-tuning process\nof TLMs, inspired by adversarial learning for do-\nmain adaptation ( Ganin et al. , 2016). Our work\nuses a GAN-style training scheme only for pre-\ntraining CNNs, not for ﬁne-tuning TLMs.\n3 Pretraining of CNNs\nThis section describes the training data and train-\ning algorithm for our CNN.\n3.1 Training data\nWe pretrained our CNN with an entity-masked\nversion of Wikipedia sentences. WikiExtractor 1\nwas used to extract, from the English Wikipedia 2,\nsentences that have at least one entity mention,\ni.e., an entity with an internal Wikipedia link.\nThen we randomly selected one entity mention ei\nin each sentence and generated an entity-masked\nsentence mi by replacing the entire selected men-\ntion with [EM]. For example, we generated the\nmasked sentence m1, “ [EM] is Thailand’s main\ninternational air hub,” (in Table 1) by replacing\nthe entity mention e1, Suvarnabhumi Airport , in\nthe sentence s1, “:::::::::::::Suvarnabhumi:::::::Airport is Thai-\nland’s main international air hub,” with [EM]. We\nobtained about 43.3 million pairs of an entity men-\ntion and a masked sentence ( f(ei, mi)g) in this\nway and used 10% of them (randomly sampled)\nas the pretraining data for our CNN.\n3.2 GAN-style pretraining\nAs illustrated in Fig. 2, the adversarial train-\ning is done using three subnetworks: R (real-\nentity-representation generator ), F (fake-entity-\nrepresentation generator), and D (discriminator).\nR and F are CNNs with average pooling and D\n1https://github.com/samuelbroscheit/wikiextractor-\nwikimentions\n2We used the September 2020 version.\nis a feedforward neural network. Once the train-\ning is done, we use the generator F as CNN in\nBERTAC. In the training, we regard the represen-\ntation of a masked entity output by generator R\nas a real representation of the entity that the fake-\nentity-representation generator F should mimic.\nF is trained so that, taking an entity-masked sen-\ntence as its input, it can generate a representation\nof the masked entity mention (called a fake repre-\nsentation of the entity in this work) that D cannot\ndistinguish from the real representation. The rep-\nresentation generated by F is fake in the sense that\nthe entity mention is masked in the input sentence\nand F cannot know what it is exactly.\nAs mentioned in the Introduction, our GAN-\nstyle pretraining was designed to train a model ca-\npable of freely generating entity representations .\nWe assumed that using multiple text representa-\ntions computed from different perspectives (i.e.,\nprediction of token embeddings in TLMs and\ngeneration of entity representations in our CNN)\nwould help to improve the performance of down-\nstream tasks.\nAlgorithm 1: Adversarial Training Scheme\nInput: Training examples f(e,m)g, training epochs t,\nmini-batch steps b, mini-batch size n\nOutput: Real representation generator R, fake\nrepresentation generator F, discriminator D\n1 j  1\n2 Initialize \u0012R, \u0012F , and \u0012D (parameters of R, F, and D)\nwith random weights\n3 while j \u0014t do\n4 k  1\n5 while k \u0014b do\n6 Sample mini-batch of n examples f(ei,mi)gn\ni=1\n7 Generate word embeddings f(ei,mi)gn\ni=1 of the\nexamples.\n8 Update D and R by ascending their stochastic\ngradient:\n∇\u0012D;\u0012R\n1\nn\nn∑\ni=1\n[log D(R(ei)) + log\n(\n1 \u0000D(F(mi))\n)\n]\n9 Update F by descending its stochastic gradient:\n∇\u0012F\n1\nn\nn∑\ni=1\nlog\n(\n1 \u0000D(F(mi))\n)\n10 k  k + 1\n11 end\n12 j  j + 1\n13 end\nFor each pair of an entity mention ( ei) and an\nentity-masked sentence ( mi) in the training data,\nwe ﬁrst generate two matrices of word embed-\ndings ei and mi using word embeddings pretrained\non Wikipedia with fastText ( Bojanowski et al. ,\n2017). Then, R and F generate, respectively, a\n2107\nreal entity representation from ei and a fake entity\nrepresentation from mi. Finally, they are given\nto D, which is a feed-forward network that judges\nwhether F or R generated the representations, i.e.,\nwhether the representations are real or fake, us-\ning sigmoid outputs by the ﬁnal logistic regression\nlayer.\nThe pseudo code of the training scheme is given\nin Algorithm 1 . The training proceeds as fol-\nlows: R and D as a team try to avoid the possi-\nbility that D misjudges F’s output (i.e., a fake en-\ntity representation) as a real entity representation.\nMore precisely, R and D are trained so that D\ncan correctly judge the representation R(ei) given\nby generator R as real (i.e., D(R(ei)) = 1 ) and\nthe representation F (mi) given by generator F as\nfake (i.e., D(F (mi)) = 0 ). Therefore, the train-\ning is carried out with the objective of maximizing\nlog D(R(ei)) + log\n(\n1 \u0000 D(F (mi))\n)\n(line 8 in Al-\ngorithm 1). On the other hand, F tries to generate\nrepresentation F (mi) so that D judges it as real\n(i.e., D(F (mi)) = 1 ). Thus, F is trained to mini-\nmize log\n(\n1 \u0000D(F (mi))\n)\n(line 9 in Algorithm 1).\nThis minmax game is iterated for the pre-speciﬁed\nt training epochs.\n3.3 Pretraining settings\nWe extracted 43.3 million pairs of an entity men-\ntion and a masked sentence from Wikipedia and\nrandomly sampled 10% of them to use as train-\ning data (4.33 million pairs, around 700 MB in\nﬁle size). We used word-embedding vectors in\n300 dimensions (for 2.5 million words) pretrained\non Wikipedia using fastText ( Bojanowski et al. ,\n2017). The embedding vectors were ﬁxed during\nthe training.\nWe set the training epochs to 200 ( t = 200 in\nAlgorithm 1 ) and did not use any early-stopping\ntechnique. We chose t = 200 from the results\nof our preliminary experiments in which we used\n10% of the training data and set training epochs t\nto either of 100, 200, or 300; the loss robustly con-\nverged for t = 200 and t = 300 , and thus the ear-\nliest point t = 200 was chosen. We used the Rm-\nsProp optimizer ( Tieleman and Hinton, 2012) with\na batch size of 4,000 ( n = 4 ; 000 and b = 1 ; 084\nin Algorithm 1 ) and a learning rate of 2e-4. We\ntrained nine CNN models with all combinations\nof the ﬁlter’s window sizes 2 f “1,2,3”, “2,3,4”,\n“1,2,3,4”g and number of ﬁlters 2 f100, 200, 300 g\nfor the generators F and R. All of the weights in\nthe CNNs were initialized using He’s method ( He\net al. , 2015). We used a logistic regression layer\nwith sigmoid outputs as discriminator D. The\ntraining of a single CNN model took around 20\nhours using 16 Nvidia V100 GPUs with 32 GB of\nmemory (180 hours in total for the nine models).\nWe tested all nine CNN models for BERTAC\nin our GLUE and open-domain QA experiments\n(Section 5). For each task, the parameters in-\nside the CNNs (as well as the word-embedding\nvectors) were ﬁxed during the ﬁne-tuning of\nBERTAC.\n4 BERTAC\nAs illustrated in Fig. 1, BERTAC (BERT-style\nTLM with an Adversarially pretrained Convolu-\ntional neural network) incorporates the representa-\ntion provided by the adversarially pretrained CNN\nto the representation generated by a TLM. For the\nintegration, we use several layers of TIERs (Trans-\nformers for Integrating External Representation)\nstacked on top of the TLM.\n4.1 CNN in BERTAC\nFor simplicity, we describe how the CNN is inte-\ngrated in BERTAC using the task of recognizing\ntextual entailment (RTE) as an example. BERTAC\nfor the RTE task takes two sentences sx and sy\nas input and predicts whether sx entails sy. First,\nwe explain how the adversarially pretrained CNN\n(generator F in Section 3.2) generates the repre-\nsentation of the two input sentences. We regard\nthe longest common noun phrase 3 of the two sen-\ntences as the entity mention to be masked and cre-\nate entity-masked sentences mx and my from sx\nand sy by masking the noun phrase with [EM]\n(we use mx = sx and my = sy if no common\nnoun phrase is found). Then each of the masked\nsentences mx and my is given to the CNN. Our\nexpectation here is that the CNN generates similar\nrepresentations from the masked sentences if they\nhave an entailment relation and that this helps to\nrecognize the entailment relation.\nNote that the CNN in BERTAC is connected to\nseveral TIER layers and that, as shown in Fig. 1,\nits input is iteratively updated so that it provides\nupdated representations to the TIER layers. Let\nmi\nx 2 Rjmxj\u0002dw and mi\ny 2 Rjmyj\u0002dw be the ma-\ntrices of word embeddings of mx and my given\n3For single-sentence tasks such as CoLA ( Wang et al. ,\n2018), we regard the longest noun phrase in a sentence as\nan entity.\n2108\nto the CNN connected to the i-th TIER layer,\nwhere dw is the dimension of a word embed-\nding. We denote the representation generated by\nthe CNN when the matrix of word embeddings\nm was used as the input by CNN (m). The i-\nth TIER layer is given the concatenation of the\ntwo CNN representations of mx and my, ri =\n[ri\nx; ri\ny] 2 R2\u0002de, where ri\nx = CNN (mi\nx) 2 Rde,\nri\ny = CNN (mi\ny) 2 Rde and de is the dimension\nof the CNN representation. Note that, for single-\nsentence tasks, ri = ri\nx, the CNN representation\nof mx, is given to the TIER layers.\nThe initial matrices of word embeddings m1\nx\nand m1\ny are obtained using the fastText word em-\nbeddings ( Bojanowski et al. , 2017), the same as\nthat used in our adversarial learning. Then, the up-\ndated input matrices mi+1\nx and mi+1\ny for the (i+1)-\nth CNN are obtained from the i-th input matrices\nmi\nx and mi\ny as described below. For the word em-\nbedding mi\nx;j of the j-th word in mx, we compute\nits bilinear score to ri\nx (Sutskever et al. , 2009):\n\u0016mi\nx;j = softmaxj(miT\nx Bi\nxri\nx)mi\nx;j;\nwhere Bi\nx 2 Rdw\u0002de is a trainable matrix and\nsoftmaxj(v) denotes the j-th element of the soft-\nmaxed vector of v. The bilinear score indicates\nhow much the corresponding token should be\nhighlighted as one associated with the CNN repre-\nsentation ri\nx during the update process. We expect\nthat this allows the CNN in the next TIER layer\nto generate further reﬁned representations with the\nupdated embeddings.\nWe then compute word embeddings mi+1\nx in a\nhighway network manner ( Srivastava et al. , 2015)\nas follows:\nmi+1\nx = Hx( \u0016mi\nx)⊙Tx(mi\nx)+ mi\nx ⊙(1\u0000Tx(mi\nx));\nwhere H x(mi\nx) = Wi\nhmi\nx + bi\nh, T x(mi\nx) =\n\u001b(Wi\ntmi\nx + bi\nt), \u001b is the sigmoid function, ⊙ rep-\nresents the element-wise product, and Wi\nh, Wi\nt,\nbi\nh, and bi\nt are layer-speciﬁc trainable parameters.\nmi+1\ny is also computed from mi\ny and ri\ny in the\nsame way. During the ﬁne-tuning of BERTAC\nfor downstream tasks, we ﬁx the parameters of the\npretrained CNN but train these parameters for up-\ndating CNN’s input alongside those of TLMs and\nTIERs.\n4.2 Transformers for integrating external\nrepresentation (TIERs)\nAs explained in the Introduction, the main differ-\nence between a TIER and a normal transformer\n!\"#$#%&'()*#+,-./01& \n2.3&$45 \n64*7 \n214\"# \nr K V\nMatMul \n64&60\" J\n2.3&$45 \n64*7 \n214\"# \nQ K V\n64&60\" \n64&60\" \n!\"#$%&&'(&)*($)($ \n(*+,\"-$&+\"(./*+,'+. \n!0#$%&&'(&)*($)($*1+$ \n2345. \nFigure 3: Attention in normal transformers and TIERs\nencoder (Vaswani et al., 2017) lies in the attention\nmechanism. In the TIER attention mechanism, the\nquery representation, which is one of the three in-\nputs of the transformer’s self-attention, is replaced\nwith the representation given by the CNN.\nFig. 3 shows the difference between the TIERs’\nattention computation and that of normal trans-\nformers. Attention in normal transformers is com-\nputed in the following way:\nAttention(Q; K; V) = softmax(QKT\npdk\n)V:\nQ, K, and V are query, key, and value matrices in\nRlk\u0002dk , where lk is the length of an input sequence\nand dk is a dimension of keys. Q, K, and V all\ncome from the same representation of the token\nsequence provided from the previous transformer\nlayer. The attention should specify how much the\ncorresponding tokens in V should be highlighted,\nso we designed ours in the same way.\nIn TIERs, we use the following attention. We\nbasically replace the matrix Q with the CNN’s rep-\nresentation r 2 Ru\u0002dk while keeping the original\nK and V, where u is the number of sentences in\nthe input of the model ( u 2 f1; 2g in this paper).\nAttention(r; K; V) = ( softmax( rKT\npdk\n))TJu;dk ⊙V:\nSince r is a matrix with a different size from\nQ, we needed to adapt the attention computa-\ntion. We ﬁrst multiply r to KT, and then its soft-\nmaxed results are converted into a lk \u0002 dk dimen-\nsional matrix using the all-one matrix Ju;dk 2\nRu\u0002dk . Let the resulting matrix be A =\n(softmax( rKT\npdk\n))TJu;dk 2 Rlk\u0002dk . We apply the at-\ntention score to V by using the element-wise prod-\nuct between matrices: A ⊙ V.\n2109\nIn addition, the actual CNN’s representation\nrCNN 2 Ru\u0002de given by our CNNs usually have\na size that does not match the size requirement\nfor r. Thus, we convert it to r 2 Ru\u0002dk , a dk-\ncolumn matrix as follows: r = rCNN W + b,\nwhere W 2 Rde\u0002dk and b are trainable.\n5 Experiments\nWe tested our model on GLUE and on open-\ndomain QA. In this section, we report the results.\n5.1 GLUE\nGLUE ( Wang et al. , 2018) is a multi-task bench-\nmark composed of nine tasks including two single-\nsentence tasks (CoLA and SST-2) and seven\ntwo-sentence tasks of similarity/paraphrase tasks\n(MRPC, QQP, and STS-B) and natural language\ninference tasks (MNLI, QNLI, RTE, and WNLI).\nFollowing the previous work of ALBERT ( Lan\net al., 2020), we performed single-task ﬁne-tuning\nfor each task under the following settings: single-\nmodel for the development set and ensemble for\ntest set submissions. As in Liu et al. (2019) and\nLan et al. (2020), we report the performance on\nthe development set for each task by averaging\nover ﬁve runs with different random initialization\nseeds. As in Lan et al. (2020), for test set sub-\nmissions, we ﬁne-tuned the models for the RTE,\nSTS-B, and MRPC tasks by initializing them with\nthe ﬁne-tuned MNLI single-task model, and we\nalso used task-speciﬁc modiﬁcation for CoLA and\nWNLI to improve scores (see Appendix A for de-\ntails). We explored ensemble settings between 6\nand 30 models per task for our test set submission.\n5.1.1 Fine-tuning details of BERTAC for\nGLUE\nWe used ALBERT-xxlarge-v2 ( Lan et al. , 2020)\nas the pretrained TLM. As hyperparameters for\nBERTAC, for each task we tested learning rates\n2 f8e-6, 9e-6, 1e-5, 2e-5, 3e-5 g, a linear warmup\nfor the ﬁrst 6% of steps followed by a linear de-\ncay to 0, a maximum sequence length of 128, and\nall nine CNNs pretrained with different ﬁlter set-\ntings. We set the batch size to 128 for MNLI\nand QQP and 16 for the other tasks. Further-\nmore, we trained our model with the following set\nof training epochs: f1,2,3,4,5g for MNLI, QQP,\nand QNLI, f6,7,8,9,10g for CoLA, MRPC, RTE,\nSST-2, and STS-B, and f90,95,100,105,110g for\nWNLI. We set the number of TIER layers to 3 af-\nter preliminary experiments. See Table 9 in Ap-\npendix B for a summary of the hyperparameters\ntested in the GLUE experiments.\nDuring the ﬁne-tuning of BERTAC, the parame-\nters inside the CNNs (as well as word embeddings\nof fastText) were ﬁxed as explained in Section 3.3,\nwhile those used to update the input to the CNNs\nwere optimized. For each task, we selected the\npretrained CNN (out of nine) and the BERTAC hy-\nperparameters that gave the best performance on\nthe development data.\n5.1.2 Results\nTable 2 shows the results of eight tasks on the\nGLUE development set: all of them are single-\nmodel results. Our BERTAC consistently out-\nperformed the previous TLM-based models over\nseven tasks, except for QQP, and, as a result,\nshowed the best average performance on the de-\nvelopment set. Crucially, our model improved\nthe average performance around 0.7% over AL-\nBERT, the base TLM in our model. This indicates\nthe effectiveness of adversarially trained CNNs\nand TIERs in BERTAC. The test set results ob-\ntained from the GLUE leaderboard are summa-\nrized in Table 3. Our model showed comparable\nperformance to SOTA, DeBERTa/TuringNLRv4,\nand achieved state-of-the-art results on 3 out of 9\ntask. It also showed better performance than AL-\nBERT, our base TLM, in most tasks.\nTo investigate whether our GAN-style pretrain-\ning of CNNs contributed to the performance im-\nprovement, we also tested the following alter-\nnative training schemes for the CNN used in\nBERTAC.\nSelf-supervised CNN: We pretrained the CNN\nto generate representations of a masked sentence\nin a self-supervised way as follows: For an entity\nmention e and an entity-masked sentence m in the\ntraining data (Section 3.1), the CNN generates a\nrepresentation r from the masked sentence trying\nto minimize MSE (mean squared error) between r\nand the entity mention’s representation e (average\nword embedding of all tokens in e).\nRandomly initialized CNN: We did not pre-\ntrained the CNNs, but trained them alongside the\nTLMs during the ﬁne-tuning of BERTAC (the\nCNNs were randomly initialized).\nWe trained both the self-supervised and ran-\ndomly initialized CNNs using the same hyperpa-\nrameter settings as GAN-style CNNs (see Sec-\ntion 3.3). We conﬁrm from the results in Table 4\n2110\nModels MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.\nRoBERTaLARGE 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 88.9\nXLNETLARGE 90.8/90.8 94.9 92.3 85.9 97.0 90.8 69.0 92.5 89.2\nELECTRALARGE 90.9/- 95.0 92.4 88.0 96.9 90.8 69.1 92.6 89.5\nALBERTXXLARGE 90.8/- 95.3 92.2 89.2 96.9 90.9 71.4 93.0 90.0\nDeBERTaLARGE 91.1/91.1 95.3 92.3 88.3 96.8 91.9 70.5 92.8 90.0\nBERTACXXLARGE 91.3/91.1 95.7 92.3 89.9 97.2 92.4 73.7 93.1 90.7\nTable 2: GLUE dev set results. The results of RoBERTa ( Liu et al. , 2019), XLNET ( Yang et al. , 2019b), ELEC-\nTRA (Clark et al. , 2020), ALBERT (Lan et al. , 2020), and DeBERTa (He et al. , 2021) were taken from their papers.\nWe omit the results of the WNLI task, since many previous works did not report the dev set results.\nModels MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI Score\nEnsembles on test (from leaderboard as of Feb. 1, 2021)\nALBERT 91.3/91.0 - 90.5 89.2 97.1 91.2 69.1 92.0 91.8 -\nELECTRA+Standard Tricks 91.3/90.8 95.8 90.8 89.8 97.1 90.7 71.7 92.5 91.8 89.4\nERNIE 91.4/91.0 96.6 90.9 90.9 97.5 91.4 74.4 92.6 94.5 90.4\nStructBERT+TAPT 90.9/90.7 97.4 91.0 91.2 97.3 91.9 75.3 92.7 94.5 90.6\nMacALBERT+DKM 91.3/91.1 97.8 90.6 92.0 97.0 92.6 74.8 92.6 94.5 90.7\nDeBERTa/TuringNLRv4 91.9/91.6 99.2 90.8 93.2 97.5 92.0 71.5 92.6 94.5 90.8\nBERTAC 91.1/91.6 97.9 90.6 90.4 97.5 91.7 72.3 92.8 94.5 90.3\nTable 3: GLUE test set results. Our model for test set results incorporates task-speciﬁc modiﬁcation for CoLA and\nWNLI to improve scores (see Appendix A for details). All results are from the GLUE leaderboard.\nthat only the proposed method with our GAN-\nstyle CNNs showed a higher average score than\nALBERT. This suggests the effectiveness of our\nGAN-style pretraining scheme of CNNs.\n5.2 Open-domain QA\nWe also tested BERTAC on open-domain\nQA (Chen et al. , 2017) with the publicly available\ndatasets Quasar-T ( Dhingra et al. , 2017) and\nSearchQA ( Dunn et al. , 2017). We used the\npre-processed version 4 of the datasets provided\nby Lin et al. (2018), which contains passages\nretrieved for all questions, and followed their data\nsplit as described in Table 5.\n5.2.1 BERTAC for open-domain QA\nWe implemented our QA model following the ap-\nproach of Lin et al. (2018), which combines a pas-\nsage selector to choose relevant passages from re-\ntrieved passages and an answer span selector to\nidentify the answer span in the selected passages.\nFor the given question q and the set of retrieved\npassages P = fpig, we computed the probability\nP r(ajq; P ) of extracting answer span a to ques-\ntion q from P in the following way, and then we\nextracted the answer span ^a with the highest prob-\nability:\nP r(ajq; P ) =\n∑\ni\nP r(ajq; pi)P r(pijq; P );\n4Available at https://github.com/thunlp/OpenQA\nwhere P r(pijq; P ) and P r(ajq; pi) are computed\nby the passage selector and answer span selector,\nrespectively.\nWe input “ [CLS] question [SEP] passage\n[SEP]” to both the passage selector and answer\nspan selector, where [CLS] and [SEP] are spe-\ncial tokens. In the passage selector, the represen-\ntation of [CLS] in the top TIER layer is fed into\na linear layer with a softmax, which computes the\nprobability that the passage contains a correct an-\nswer to the question. Our BERTAC answer span\nselector identiﬁes answer spans from passages by\ncomputing start and end probabilities of each to-\nken in passages, where we feed the representation\nof each token in the top layer of TIERs to two lin-\near layers, each with a softmax for the probabili-\nties (Devlin et al. , 2019).\n5.2.2 Training details for open-domain QA\nWe used all nine pretrained CNNs, as in the\nGLUE experiments. As pretrained TLMs, we\nused ALBERT-xxlarge-v2 ( Lan et al. , 2020) and\nRoBERTa-large ( Liu et al. , 2019). We set the\nlearning rate to 1e-5, the number of epochs to\n2, the maximum sequence length to 384, and the\nnumber of TIER layers to 3. We used a linear\nwarmup for the ﬁrst 6% of steps followed by a lin-\near decay to 0 with a batch size of 48 for Quasar-\nT and 96 for SearchQA. We tested all of the pre-\ntrained CNNs and chose for each dataset the one\nthat maximizes EM (the percentage of the predic-\ntions matching exactly one of the ground truth an-\n2111\nCNNs used in BERTAC MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg.\nProposed (GAN-style CNN) 91.3/91.1 95.7 92.3 89.9 97.2 92.4 73.7 93.1 90.7\nSelf-supervised CNN 91.0/90.8 95.3 91.5 88.4 96.6 90.9 71.1 93.0 89.8\nRandomly initialized CNN 91.0/90.7 95.4 91.4 87.4 96.3 91.2 71.5 93.1 89.8\nALBERTXXLARGE 90.8/- 95.3 92.2 89.2 96.9 90.9 71.4 93.0 90.0\nTable 4: Comparison of BERTAC results in different CNN settings on GLUE dev set.\nTrain Dev Test #p\nQuasar-T 37,012 3,000 3,000 100\nSearchQA 99,811 13,893 27,247 50\nTable 5: Number of questions in each dataset. #p is the\nnumber of retrieved passages for each question.\nNon-TLM-based methods\nOPEN QA (Lin et al. , 2018): An RNN-based method\nthat jointly learns passage-selection and answer extrac-\ntion.\nOPEN QA+ARG (Oh et al. , 2019): An extension of\nOPEN QA that additionally uses an answer representation\ngenerator (ARG) trained by adversarial learning.\nTLM-based methods\nWKLM (Xiong et al. , 2020): This uses a TLM pre-\ntrained with a weakly supervised objective for learning\nWikipedia entity information. BERT-base was used for\nthe training.\nMBERT (Wang et al. , 2019): A BERT-based method\nthat extracts answers using globally normalized answer\nscores across all the passages retrieved by the same ques-\ntion. BERT-large was used for the training.\nCFORMER (Wang et al. , 2020b): It uses a clustering-\nbased sparse transformer for long-range dependency en-\ncoding. The method was trained using RoBERTa-large.\nTable 6: Compared QA methods\nswers) on the development set. See Table 10 in\nAppendix B for a summary of the hyperparame-\nters tested for open-domain QA.\n5.2.3 Results\nWe compared BERTAC with the previous works\ndescribed in Table 6. Table 7 shows the per-\nformance of all of the methods. The sub-\nscripts of the TLM-based methods represent the\ntype of pretrained TLM used by each method.\nAll the methods were evaluated using EM and\nF1 score (average overlap between the predic-\ntion and gold answer). BERTACALBERT-xxlarge\noutperformed all of the baselines including the\nSOTA method ( CFORMER ) on both EM and F1.\nBERTACRoBERTa-large in the same TLM setting\nas the SOTA method showed a better performance\nthan SOTA except for F1 in Quasar-T. These re-\nsults suggest that our framework is effective for\nQA tasks as well.\nFor ablation studies, we evaluated some vari-\nants of BERTACALBERT-xxlarge: “w/o CNN and\nModel Quasar-T SearchQA\nEM F1 EM F1\nOPEN QA 42.2 49.3 58.8 64.5\nOPEN QA+ARG 43.2 49.7 59.6 65.3\nWKLM BERT-base 45.8 52.2 61.7 66.7\nMBERT BERT-large 51.1 59.1 65.1 70.7\nCFORMER RoBERTa-large 54.0 63.9 68.0 75.1\nBERTACRoBERTa-large 55.8 63.7 71.9 77.1\nBERTACALBERT-xxlarge 58.0 65.8 74.0 79.2\nTable 7: QA test set results. Figures of the previous\nworks were taken from their original papers.\nModel Quasar-T SearchQA\nEM F1 EM F1\nBERTACALBERT-xxlarge 58.0 65.8 74.0 79.2\nw/o CNN and TIER 55.6 63.5 72.7 78.0\nw/o GAN-style CNN 56.1 63.9 73.1 78.4\nw/o update 56.8 65.0 73.3 78.5\nTable 8: Ablation test results.\nTIER,” which uses ALBERT-xxlarge alone with-\nout using our CNN and TIER, “w/o GAN-style\nCNN,” which does not use our CNN pretrained\nby the GAN-style training scheme but uses self-\nsupervised CNNs (the same as used in the GLUE\nexperiments, see Table 4), “w/o update,” which\ndoes not perform layer-wise update of the CNN\ninputs. The results in Table 8 suggest that all of\nthe following contributed to the performance im-\nprovement: the combination of TLMs and GAN-\nstyle CNNs, our GAN-style training of CNNs, and\nthe layer-wise update of the CNN inputs.\n6 Conclusion\nWe proposed BERTAC (BERT-style TLM with an\nAdversarially pretrained Convolutional neural net-\nwork), a combination of a TLM and a CNN, where\nthe CNN was pretrained using a novel GAN-style\ntraining scheme and masked sentences obtained\nautomatically from Wikipedia. Using this CNN,\nwe improved the performance of standard TLMs.\nWe conﬁrmed that BERTAC could achieve com-\nparable performance with the SOTA and outper-\nformed the base TLM used as a subcomponent\nof BERTAC in the GLUE task. We also show\nthat BERTAC outperformed the SOTA method of\nopen-domain QA on Quasar-T and SearchQA.\n2112\nReferences\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information . Transactions of the Associa-\ntion for Computational Linguistics , 5:135–146.\nSiddhartha Brahma. 2018. Unsupervised learning\nof sentence representations using sequence consis-\ntency. CoRR, abs/1808.04217.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neu-\nral Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual .\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open–\ndomain questions . In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879. Association for Computational Linguistics.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation . In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre–\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186.\nBhuwan Dhingra, Kathryn Mazaitis, and William W\nCohen. 2017. Quasar: Datasets for question an-\nswering by search and reading. arXiv preprint\narXiv:1707.03904.\nMatthew Dunn, Levent Sagun, Mike Higgins, V . Ugur\nG¨uney, V olkan Cirik, and Kyunghyun Cho.\n2017. SearchQA: A new Q&A dataset aug-\nmented with context from a search engine. CoRR,\nabs/1704.05179.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc ¸ois Lavi-\nolette, Mario March, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks .\nJournal of Machine Learning Research , 17(59):1–\n35.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets . In Advances in Neural Information\nProcessing Systems , volume 27, pages 2672–2680.\nCurran Associates, Inc.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun\nLiu, Nicholas Jing Yuan, and Tong Xu. 2020.\nBERT-MK: Integrating graph contextualized knowl-\nedge into pre-trained language models . In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 2281–2290.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁca-\ntion. In Proceedings of the 2015 IEEE International\nConference on Computer Vision (ICCV) , ICCV ’15,\npages 1026–1034, Washington, DC, USA. IEEE\nComputer Society.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-\nenhanced BERT with Disentangled Attention. In\nInternational Conference on Learning Representa-\ntions.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations . In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020.\nYann LeCun, Patrick Haffner, L ´eon Bottou, and\nYoshua Bengio. 1999. Object recognition with\ngradient-based learning. In Shape, contour and\ngrouping in computer vision , pages 319–345.\nSpringer.\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.\n2018. Denoising distantly supervised open-domain\nquestion answering . In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018 , pages 1736–1745.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-BERT:\nEnabling language representation with knowledge\ngraph. In Proceedings of AAAI 2020 .\n2113\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nZhibin Lu, Pan Du, and Jian-Yun Nie. 2020. VGCN-\nBERT: augmenting BERT with graph embedding for\ntext classiﬁcation. In Advances in Information Re-\ntrieval - 42nd European Conference on IR Research,\nECIR 2020, Lisbon, Portugal, April 14-17, 2020,\nProceedings, Part I, volume 12035 of Lecture Notes\nin Computer Science , pages 369–382. Springer.\nJong-Hoon Oh, Kazuma Kadowaki, Julien Kloetzer,\nRyu Iida, and Kentaro Torisawa. 2019. Open–\ndomain why-question answering with adversarial\nlearning to encode answer texts . In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4227–4237.\nMatthew E. Peters, Mark Neumann, Robert Logan,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations . In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 43–54.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research ,\n21(140):1–67.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social\nmedia. In Proceedings of the Fourteenth Workshop\non Semantic Evaluation , pages 2054–2059.\nBo Shao, Yeyun Gong, Weizhen Qi, Nan Duan, and\nXiaola Lin. 2019. Aggregating bidirectional en-\ncoder representations using MatchLSTM for se-\nquence matching . In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 6059–6063.\nRupesh K Srivastava, Klaus Greff, and J ¨urgen Schmid-\nhuber. 2015. Training very deep networks . In Ad-\nvances in Neural Information Processing Systems ,\nvolume 28, pages 2377–2385.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. ERNIE\n2.0: A continual pre-training framework for lan-\nguage understanding . In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , pages 8968–\n8975.\nIlya Sutskever, Joshua B. Tenenbaum, and Ruslan R\nSalakhutdinov. 2009. Modelling relational data us-\ning bayesian clustered tensor factorization . In Ad-\nvances in Neural Information Processing Systems\n22, pages 1821–1828. Curran Associates, Inc.\nT. Tieleman and G. Hinton. 2012. Lecture 6.5—\nRmsProp: Divide the gradient by a running average\nof its recent magnitude. COURSERA: Neural Net-\nworks for Machine Learning.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need . In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008. Curran As-\nsociates, Inc.\nGiorgos Vernikos, Katerina Margatina, Alexandra\nChronopoulou, and Ion Androutsopoulos. 2020.\nDomain Adversarial Fine-Tuning as an Effective\nRegularizer. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 3103–\n3112. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding . In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2020a. K-adapter: Infusing\nknowledge into pre-trained models with adapters.\nCoRR, abs/2002.01808.\nShuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun\nChen, Yuwei Fang, Siqi Sun, Yu Cheng, and\nJingjing Liu. 2020b. Cluster-former: Clustering-\nbased sparse transformer for long-range dependency\nencoding. arXiv preprint arXiv:2009.06097 .\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-\npati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized BERT model for open-do-\nmain question answering . In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5878–5882. Association\nfor Computational Linguistics.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In 8th International Conference on Learn-\ning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020 .\n2114\nAn Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu,\nHua Wu, Qiaoqiao She, and Sujian Li. 2019a. En-\nhancing pre-trained language representations with\nrich knowledge for machine reading comprehension .\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2346–2357.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding . In Advances in\nNeural Information Processing Systems 32 , pages\n5754–5764.\nShaohua Zhang, Haoran Huang, Jicong Liu, and Hang\nLi. 2020. Spelling error correction with soft-masked\nBERT. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL 2020, Online, July 5-10, 2020 , pages 882–\n890. Association for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1441–1451.\nA Task-speciﬁc Modiﬁcation for GLUE\nTest-set Submission\nWe applied task-speciﬁc modiﬁcation to WNLI\nand CoLA in the GLUE tasks to achieve com-\npetitive GLUE leaderboard results, i.e., the test\nset submission results presented in Table 3. For\nWNLI, we followed Raffel et al. (2020), while, for\nCoLA, we propose our own modiﬁcation. Note\nthat we did not apply the tricks in obtaining the re-\nsults on the development set results shown in Ta-\nble 2. In the following, we describe the tricks.\nA.1 WNLI\nWNLI is a coreference resolution task with a two-\nsentence input. The ﬁrst sentence has an ambigu-\nous pronoun and the second sentence is generated\nfrom the ﬁrst sentence by replacing the pronoun\nwith one of the possible referents (noun phrases)\nin the ﬁrst sentence ( Wang et al. , 2018). In this\ntask, we must predict whether the candidate ref-\nerent in the second sentence is the correct refer-\nent of the pronoun. Since the format of WNLI\nis known for being difﬁcult to learn by a model,\nmany previous works, including those using AL-\nBERT, RoBERTa, or T5 ( Liu et al. , 2019; Lan\net al., 2020; Raffel et al. , 2020), converted the data\nto a simpler format before training their WNLI\nmodel for GLUE test-set submission.\nFollowing these approaches, we also converted\nthe data in the same way as Raffel et al. (2020).\nFirst, we extract candidate referents for an am-\nbiguous pronoun as follows. Suppose that the\nfollowing sentence pair of s1 and s2 is from the\nWNLI task’s data and has the label correct (mean-\ning that Susan in s2 is the correct referent of the\npronoun she in s1).\ns1: Jane knocked on Susan’s door but she\ndid not get an answer .\ns2: Susan did not get an answer .\nWe ﬁrst ﬁnd all of the pronouns in the ﬁrst sen-\ntence (“she” in s1). For each pronoun, we ﬁnd\nthe longest sequence of words that precedes or\nfollows the pronoun in the ﬁrst sentence and that\nalso appears in the second sentence (“did not get\nan answer” underlined in s1 and s2). We then\nchoose the pronoun that precedes or follows the\nlongest matching word sequence and obtain a can-\ndidate referent by deleting the matched sequence\nof words from the second sentence. In the exam-\nple sentence pair ( s1, s2), we choose the pronoun\nshe from the ﬁrst sentence (since there is a single\npronoun) and obtain the candidate referent Susan\nfrom the second sentence through this process. Fi-\nnally, we convert the original sentence pair into a\npair of a masked sentence and a candidate refer-\nent by replacing the pronoun in the ﬁrst sentence\nwith [MASK] and replacing the second sentence\nwith the extracted referent. The ( s1, s2) pair is\nthus changed to the following ( s′\n1, s′\n2):\ns′\n1: Jane knocked on Susan’s door but [MASK]\ndid not answer.\ns′\n2: Susan\nNote that [MASK] in the sentence is different\nfrom the entity mask [EM] used in our GAN-\nstyle training for CNNs. For the input to our\nCNNs, we further replaced [MASK] with [EM].\nSince the format of this converted data is simi-\nlar to that of the training data for the GAN-style\ntraining scheme of our CNN, we expect that by\nusing this data conversion, BERTAC can more ef-\nfectively predict whether the candidate referent for\nthe masked pronoun is correct.\nA.2 CoLA\nIn the CoLA task, we need to predict whether a\ngiven sentence is grammatically acceptable. For\n2115\nMNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B WNLI\nLearning rate f8e-6, 9e-6,1e-5, 2e-5, 3e-5 g\nBatch size 128 16\nTraining epoch f1,2,3,4,5g f6,7,8,9,10g f90,95,100, 105,110g\nTIER layer 3\nMax sequence length 128\nWarmup step linear warmup for the ﬁrst 6% of steps\nCNN 9 models pretrained with different ﬁlter settings\nTable 9: Hyperparameters of BERTAC tested for GLUE experiments.\nQuasar-T SearchQA\nLearning rate 1e-5\nBatch size 48 96\nTraining epoch 2\nTIER layer 3\nMax sequence length 384\nWarmup step linear warmup for the ﬁrst 6% of steps\nCNN 9 models pretrained with different ﬁlter settings\nTable 10: Hyperparameters of BERTAC tested for open-domain QA experiments.\nthis task, we conducted a two-step ﬁne-tuning. In\nthe ﬁrst step, we ﬁne-tuned BERTAC with au-\ntomatically generated pseudo-training data. This\ndata was prepared as described below, and does\nnot include the original CoLA training data. In\nthe second step, we further ﬁned-tuned the model\nobtained in the ﬁrst step using the original CoLA\ntraining data. The BERTAC model obtained at this\nsecond step was used for the test-set submission.\nTo automatically generate pseudo-training data,\nwe regarded all of the sentences in the training\ndata of MNLI, QQP, and QNLI as grammatically\nacceptable and used them as positive examples\nin the pseudo-training data. After removing du-\nplicate sentences, for each positive example, we\ngenerated one negative example by modifying the\npositive example under the assumption that the\nmodiﬁcation makes the generated example gram-\nmatically unacceptable. As a modiﬁcation, we\nrandomly applied one of the following three op-\nerations: permutation (of four words randomly se-\nlected), insertion (of two random words to random\npositions), and deletion (of two randomly selected\nwords) (Brahma, 2018).\nWe obtained about 2.14 million examples in this\nway, half of them positives and the other half neg-\natives. We used all of the training samples auto-\nmatically generated in this way for the ﬁrst-step\nﬁne-tuning of BERTAC, with a learning rate of\n8e-6, a single training epoch, and a batch size of\n128, while applying the same settings for the other\nhyperparameters as those used for the other tasks.\nThe model obtained by the ﬁrst-step ﬁne-tuning is\nthen used as a starting point for the second-step\nﬁne-tuning, using the original CoLA training data\nthis time, of our ﬁnal model for CoLA.\nB Hyperparameters\nHyperparameters of BERTAC tested for GLUE\nand open-domain QA experiments are summa-\nrized in Tables 9 and 10, where CNN represents\nCNN models pretrained with different ﬁlter set-\ntings (ﬁlter’s window sizes 2 f“1,2,3”, “1,2,3,4”,\n“2,3,4”g and number of ﬁlters 2 f100, 200, 300 g)\ndescribed in Section 3.3. We tested all combina-\ntions of these hyperparameters and chose the best\none using the development set of each task."
}