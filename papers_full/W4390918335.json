{
  "title": "Residual LSTM-based short duration forecasting of polarization current for effective assessment of transformers insulation",
  "url": "https://openalex.org/W4390918335",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5069335978",
      "name": "Aniket Vatsa",
      "affiliations": [
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A5078464069",
      "name": "Ananda Shankar Hati",
      "affiliations": [
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A5028488633",
      "name": "Prashant Kumar",
      "affiliations": [
        "Dongguk University"
      ]
    },
    {
      "id": "https://openalex.org/A5062664944",
      "name": "Martin Margala",
      "affiliations": [
        "University of Louisiana at Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A5089771051",
      "name": "Prasun Chakrabartı",
      "affiliations": [
        "Sir Padampat Singhania University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4372037541",
    "https://openalex.org/W4285134847",
    "https://openalex.org/W4360995815",
    "https://openalex.org/W4322771391",
    "https://openalex.org/W2995099348",
    "https://openalex.org/W4360930637",
    "https://openalex.org/W2980624504",
    "https://openalex.org/W4379525379",
    "https://openalex.org/W3014828052",
    "https://openalex.org/W3018727768",
    "https://openalex.org/W2733485104",
    "https://openalex.org/W3048158693",
    "https://openalex.org/W3087439492",
    "https://openalex.org/W2896175751",
    "https://openalex.org/W2782126058",
    "https://openalex.org/W2499966161",
    "https://openalex.org/W3198750909",
    "https://openalex.org/W2963917928",
    "https://openalex.org/W4379390702",
    "https://openalex.org/W4309563912"
  ],
  "abstract": "Abstract The empirical application of polarization and depolarization current (PDC) measurement of transformers facilitates the extraction of critical insulation-sensitive parameters. This technique, rooted in time-domain dielectric response analysis, forms the bedrock for parameterization and insulation modeling. However, the inherently time-consuming nature of polarization current measurements renders them susceptible to data corruption. This article explores deep-learning-based short-duration techniques for forecasting polarization current to address this limitation. By incorporating spatial shortcuts, the residual long short-term memory (LSTM) network facilitates the seamless propagation of spatial and temporal gradients. Furthermore, the relative forecasting assessment of the proposed residual LSTM model’s performance is made against traditional LSTM, attention LSTM, gated recurrent units (GRU), and convolutional neural network (CNN) models. Thus, optimal model selection strategies are evaluated based on their capability to capture extended dependencies and short-term information present in the data. In addition, the Monte Carlo dropout prediction is employed to estimate uncertainty in polarization current forecasts. The findings demonstrate that the proposed residual LSTM network model for polarization current forecasting yields the lowest error metrics and maintains prediction consistency over the testing duration. Thus, the proposed approach significantly reduces PDC measurement time, providing an effective means to develop proactive maintenance strategies for evaluating the insulation condition of transformers.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports\nResidual LSTM‑based \nshort duration forecasting \nof polarization current for effective \nassessment of transformers \ninsulation\nAniket Vatsa 1, Ananda Shankar Hati 1, Prashant Kumar 2, Martin Margala 3* & \nPrasun Chakrabarti 4\nThe empirical application of polarization and depolarization current (PDC) measurement of \ntransformers facilitates the extraction of critical insulation‑sensitive parameters. This technique, \nrooted in time‑domain dielectric response analysis, forms the bedrock for parameterization and \ninsulation modeling. However, the inherently time‑consuming nature of polarization current \nmeasurements renders them susceptible to data corruption. This article explores deep‑learning‑\nbased short‑duration techniques for forecasting polarization current to address this limitation. By \nincorporating spatial shortcuts, the residual long short ‑term memory (LSTM) network facilitates \nthe seamless propagation of spatial and temporal gradients. Furthermore, the relative forecasting \nassessment of the proposed residual LSTM model’s performance is made against traditional LSTM, \nattention LSTM, gated recurrent units (GRU), and convolutional neural network (CNN) models. \nThus, optimal model selection strategies are evaluated based on their capability to capture extended \ndependencies and short‑term information present in the data. In addition, the Monte Carlo dropout \nprediction is employed to estimate uncertainty in polarization current forecasts. The findings \ndemonstrate that the proposed residual LSTM network model for polarization current forecasting \nyields the lowest error metrics and maintains prediction consistency over the testing duration. Thus, \nthe proposed approach significantly reduces PDC measurement time, providing an effective means to \ndevelop proactive maintenance strategies for evaluating the insulation condition of transformers.\nEnsuring the integrity of transformer insulation stands as an imperative facet for the seamless functioning of \npower system  networks1. Transformer insulation is subjected to multiple stress factors during its operational \nlife, leading to the deterioration of oil-paper  insulation2. Therefore, effective monitoring and assessment of \ninsulation conditions are essential to mitigate the risk of catastrophic failure and subsequent power  outages3. In \nthat regard, chemical-based methodologies, such as dissolved gas analysis (DGA), furan analysis, and degree of \npolymerization (DP), are employed to discern the condition of transformer insulation, albeit their application \nis constrained in assessing insulation integrity  comprehensively4. Furthermore, DGA analysis is susceptible to \nthe influence of gas fluctuations, thereby potentially impeding the accuracy of results. Moreover, DP measure -\nments entail a destructive nature, imposing constraints on the frequency and extent of testing, and the diagnostic \nscope of furan analysis is inherently limited, addressing specific aspects of transformer insulation health. On the \nother hand, the utilization of dielectric response analysis methodologies, such as polarization and depolarization \ncurrent (PDC) and recovery voltage measurement (RVM), has experienced remarkable proliferation in recent \nyears due to their non-invasive nature and highly efficient dynamic insulation characterization  capabilities5. By \nOPEN\n1Department of Electrical Engineering, Indian Institute of Technology (Indian School of Mines), Dhanbad 826004, \nIndia. 2Department of Mechanical, Robotics, and Energy Engineering, Dongguk University-Seoul, 30 Pil-dong \n1 gil, Jung-gu, Seoul 04620, Republic of Korea. 3School of Computing and lnformatics, University of Louisiana \nat Lafayette, Lafayette, USA. 4Department of Computer Science and Engineering, Sir Padampat Singhania \nUniversity, Udaipur, Rajasthan 313601, India. *email: martin.margala@louisiana.edu\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\ncritically examining PDC data, valuable insights into age-sensitive parameters that influence the operational \nhealth of a transformer can be  derived6.\nThe literature emphasizes the importance of time domain analysis techniques, such as PDC analysis con-\ncerning transformer oil-paper insulation, serving as a pivotal aspect of effective insulation  assessment7. Diverse \ntraditional insulation modeling techniques encompassing the conventional Debye mode (CDM) and the modi-\nfied Debye model (MDM) provide a unique characterization of transformer insulation  systems8,9. Furthermore, \nadvanced techniques such as the model with time-varying parameters (MTVP) and the modified Maxwell \nmodel (MMM) provide a foundation for sophisticated methodologies to comprehend and evaluate insulation \n performance10. Furthermore, the insulation-sensitive parameters extracted from PDC analysis, such as DC \nconductivity, transfer function zero, and detrapped charge, play a decisive role in predictive analysis, facilitating \nprognostication of insulation conditions of  transformers11,12. The application of PDC analysis and its associ-\nated insulation modeling techniques augments transformer insulation systems’ comprehension and prognostic \n capabilities13. Nevertheless, an aspect warranting attention is the discernible possibility of the PDC technique \nintroducing a noteworthy extension to measurement times, occasionally encompassing durations of several \nhours. Consequently, this prolonged duration renders the acquired low-magnitude PDC data vulnerable to \npotential corruption stemming from the deleterious effects of noise and adverse environmental conditions. \nTherefore, an escalating demand exists for novel and time-efficient PDC measurement methodologies. Although \ntraditional models like autoregressive integrated moving average (ARIMA), error-trend-seasonality (ETS), and \nneural networks (NN) have been proposed for predicting polarization  current14, they grapple with challenges in \nterms of outlier sensitivity and effectively handling seasonality and trends. In recognition of these limitations, \nresearchers are actively exploring alternative avenues, such as leveraging deep learning techniques, to enhance \ntransformer performance parameter estimation. The key novelties and contributions of the investigation can \nbe given as follows: \n1. A deep learning-based forecasting method based on residual long short-term memory (LSTM) networks is \nproposed in this article for short-duration PDC measurement of transformers’ insulation systems.\n2. The optimal polarization current forecasting model is established by relative forecasting assessment of the \nproposed residual LSTM model with attention-LSTM, LSTM, gated recurrent units (GRU), and convolutional \nneural network (CNN) models.\n3. The Monte Carlo dropout prediction approach, is utilized to enhance uncertainty estimation, contributing \nto polarization forecasts by estimating the mean and standard deviation of predictions across Monte Carlo \nsamples to quantify forecast confidence.\n4. In order to ensure the robustness of the proposed residual LSTM model, its forecasting capabilities in two \ndistinct scenarios, each employing a different historical data window for prediction, are validated.\n5. A crucial insulation-sensitive parameter, the DC conductivity of the transformer oil-paper insulation system, \nis determined based on the predicted polarization current for expedient insulation condition assessment.\nThe rest of the article is structured as follows. The “ Brief theory of the PDC approach  ” section introduces the \nPDC approach for analyzing polarization current in transformer insulation, detailing its theoretical foundations \nbased on dipole alignment and dielectric response functions. The “ Proposed short-duration PDC forecasting \nmethodology” section details the PDC forecasting methodology, covering sample preparation and residual LSTM \narchitecture to enhance temporal pattern identification, emphasizing gradient flow improvement. The “Experi-\nmental results” section presents the results for a deep learning residual LSTM forecasting model trained under \ntwo distinct temporal contexts and also comparatively analyses the residual LSTM model’s performance. Addi-\ntionally, uncertainty analysis is conducted using a Monte Carlo dropout prediction method. The “ Discussion” \nsection draws crucial insights from the results. Eventually, the “Conclusion” section concludes the investigation.\nBrief theory of the PDC approach\nThe polarization current in transformer insulation, which is crucial for evaluating such complex insulation \nsystems, originates from alignment dipoles with applied electric fields to create displacement  current15. In an \noil-paper insulation (OPI) system, interfacial and electronic polarization are prominent, and it is often followed \nby distinct relaxation of dipole groups after the electric field is removed. This relaxation behavior can be repre -\nsented using RC branches with different time constants. The current density j(t ) generated by dielectric material \nunder the influence of an external electric field E (t) while considering the macroscopic polarization P (t) effect \ncan be represented  as16:\nIn this context, the symbols σo and εo represent the DC conductivity and vacuum permittivity, respectively. \nPolarization can be calculated from the convolution in the time domain of the dielectric response function f (t) \nand electric susceptibility χ.\nThe equation describes material polarization in response to an electric field where the first term captures immedi-\nate polarization, determined by electric susceptibility and applied field; the second term involves convolution of \n(1)j(t) = σoE (t) + εo\ndE (t)\ndt + dP (t)\ndt\n(2)P (t) = εoχE (t) + εo\n∫ t\no\nf(t − τ) E (τ )d τ\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nthe dielectric response function and field history, characterizing cumulative polarization due to delayed responses \nand relaxation processes. By utilizing Eqs. (1) and (2), the current density can be further modified as follows:\nThis equation can be subsequently simplified to deduce the polarization current. Furthermore, for a direct current \ntest voltage Uo , the polarization current ( ipol  ) is intricately influenced by essential factors such as the geometric \ncapacitance ( Co ), the electric field ( E ), the direct current conductivity ( σo ), the vacuum permittivity ( εo ), the \ntransient polarization component ( δ(t)) which is neglected, and the dielectric response function ( f (t ) ) as follows:\nThe foundation of transformer insulation models rests upon utilizing dielectric spectroscopy-based PDC meas-\nurements. This approach facilitates the extraction of a wide array of parameters sensitive to the insulation proper-\nties. Furthermore, the developed models can capture intricate details concerning the insulation’s behavior and \nperformance, enabling a comprehensive understanding of the transformer’s health and efficiency.\nProposed short‑duration PDC forecasting methodology\nThe extended measurement duration of low-magnitude PDC signals, which can last for several hours, presents a \nsubstantial measurement challenge. It leaves the acquired data susceptible to potential corruption from noise and \nenvironmental conditions. Therefore, there is a growing interest in efficient short-duration PDC measurement \nmethodologies. Deep learning techniques hold the potential to address this challenge by enhancing the measure-\nment process, leading to shorter measurement times and ultimately elevating the precision and dependability of \npolarization current forecasts. This approach addresses the underlying problem by introducing a more efficient \nand reliable means of obtaining PDC measurements. The comprehensive polarization current forecasting scheme \nusing the residual LSTM network is shown in Fig. 1.\nThe preparation of the insulation specimens involved arranging layers comprising a pressboard cylinder (A), \nkraft paper (B, E), and high and low-voltage transformer windings (C, F) utilizing copper foil. Furthermore, \npressboard strips are used to emulate oil ducts (D). Moreover, this approach enables a comprehensive study of \npolarization phenomena within a controlled laboratory environment. In this study, the samples underwent ini-\ntial treatment to remove pre-absorbed moisture by subjecting them to heating at 90°. Afterward, the insulation \nsample is impregnated in mineral oil to simulate operational conditions, ensuring an accurate representation of \n(3)j(t) = σo E (t) + εo (1 + χ) δE (t)\ndt + εo\nd\ndt\n∫ t\no\nf(t − τ) E (τ )d τ\n(4)ipol = C o U 0[σo\nεo\n+ f(t)]\nFigure 1.  The topological configuration for efficient short-duration polarization current forecasting framework \nusing residual LSTM network.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nthe composite insulation system of transformers. Post-treatment, a Karl Fischer (KF) titration method was used \nto determine moisture content subsequently after allowing sufficient time for attaining equilibrium. In order \nto investigate the insulation analysis of the oil-paper insulation system, the polarization current is measured \nusing an electrometer technique. Figure  2a illustrates the PDC measurement approach employed for the OPI \nsample, utilizing a direct current (DC) excitation voltage of 1000 V . Furthermore, Fig. 2b presents an overhead \nschematic view of the constructed OPI sample. A residual LSTM-based deep learning forecasting methodology \nis then utilized for the investigation of reducing the cumulative PDC measurement time by effectively forecast-\ning polarization current.\nThe Residual LSTM approach extends the conventional LSTM architecture (depicted in Fig.  2c) by intro-\nducing residual connections, enabling it to capture long-range dependencies and complex temporal patterns \nconcealed within the sequential  data17. These connections help mitigate the vanishing and exploding gradient \nproblem, leading to improved learning and forecasting accuracy of polarization current forecasting problem. The \nresidual LSTM model architecture proposed in this study involves multiple LSTM layers, as shown in Fig.  2d, \nwith each layer comprising 64 units configured to return sequences.\nAn input layer precedes a single, 64-unit LSTM layer in the attention-based LSTM model, which is then \nenhanced by a unique attention mechanism. A tanh-activated dense layer, permutation layers for dimension \nreshaping, and a softmax-activated dense layer constitute the attention mechanism. These components function \ntogether to highlight crucial phases in the input sequence. The fundamental LSTM model takes a more simplified \napproach, with a single 64-unit LSTM layer and an output layer. The GRU model, a highly effective LSTM variant, \nconsists of an input layer, a GRU layer with 64 units, and an output layer. In contrast, the proposed polarization \ncurrent forecasting model incorporates a multi-layered LSTM structure, with each layer comprising 64 neurons. \nA notable feature is establishing a sequential flow of information, where the first LSTM layer returns sequences \nto the second, facilitating a coherent exchange of information. The distinctive aspect of this architecture is the \nincorporation of residual connections, a pivotal mechanism that facilitates seamless information transfer between \nthe LSTM layers. By effectively capturing intricate temporal patterns and long-term dependencies in the data, \nFigure 2.  (a) Diagram depicting the polarization current measurement procedure, (b) Construction of the OPI \ntest sample, (c) Schematics of an LSTM cell displaying gating variables, and (d) Architecture of the proposed \nresidual LSTM model.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nthe model becomes adept at complex sequence modeling tasks such as polarization current forecasting. Follow-\ning the series of LSTM layers, a flattened layer is employed to reshape the output into a compact 1D vector. This \noutput is then fed into a dense layer, which is responsible for generating predictions for the forecasted time steps \nas defined by the specified forecast horizon. The residual LSTM introduces spatial shortcuts, thereby enabling the \nsmooth flow of both spatial and temporal  gradients18. The strategic integration of the residual LSTM approach \ncontributes significantly to the model’s ability to surmount challenges posed by gradient-related issues, ultimately \nenhancing its forecasting accuracy and making it a promising solution for capturing complex temporal dynamics \nin various applications. To comprehensively evaluate the forecasting capabilities of the residual LSTM model, two \ndistinct cases are investigated in this article. Specifically, the cases involve employing different lookback periods: \none instance with a lookback of 60 (referred to as case 1) and another instance with a lookback of 100 (referred \nto as case 2). This approach allows for a thorough examination of the model’s performance under varying look-\nback settings, thereby providing valuable insights into its effectiveness across different temporal contexts. The \nstudy employed various hyperparameters in the deep learning model, including the number of LSTM layers, \nlearning rate, and sequence length, to predict the insulation condition of a transformer through polarization \ncurrent forecasting. The combination of grid search and manual fine-tuning method was employed to ensure \nthe selection of the best hyperparameters.\nIn a residual LSTM model, an auxiliary spatial shortcut connection is introduced in a traditional LSTM \narchitecture to model the temporal dependencies effectively. Thereby, it facilitates proper gradient flow and \nmitigates the vanishing gradient problem. Furthermore, at each time step t  , the input xt is processed, and the \noutput hidden state ht is computed. Simultaneously, a series of gating mechanisms update the memory cell state \nct . These gating variables are computed using various characteristic equations, as represented below:\nHere, σ denotes the sigmoid activation function, W matrices, and b vectors represent the learnable weights \nand biases associated with input and hidden states. Furthermore, ⊙ indicates element-wise multiplication. The \nresidual connection is introduced by adding the output ht to the input xt19, creating a shortcut trail that facilitates \nlearning incremental insights by leveraging the gradient information expressed as:\nThis mechanism enables addressing the vanishing gradient problem and enhances the capacity of the network \nto capture both short-term and long-term patterns in the data.\nExperimental results\nThis research paper analyzes a deep learning residual LSTM forecasting model’s performance when trained under \ntwo distinct temporal contexts of different past timestamps under case 1 and case 2. The lookback parameter \ndetermines the number of past time steps considered input for the LSTM model, impacting its ability to capture \ntemporal dependencies and patterns within the data. Furthermore, to select the optimal forecasting model, a \nrelative comparison of the proposed residual LSTM model is also made with similar approaches such as atten-\ntion LSTM, LSTM, GRU and CNN models. To assess the model’s accuracy and generalization across various \ntemporal contexts, essential metrics such as mean squared error (MSE), mean absolute error (MAE), and root \nmean squared error (RMSE) are employed for performance evaluation.\nThe MSE quantifies the average squared difference between the actual polarization current value ( ipol ) and \nthe predicted polarization current value ( ifor\npol ). The objective of the training process is to minimize the loss func-\ntion /Psi1 by optimizing the model’s trainable parameters, such as weights and biases. This optimization is achieved \nthrough the Adam optimizer, a widely utilized stochastic gradient descent (SGD) variant. It employs adaptive \nlearning rates, momentum, and second-moment estimates of the gradients to navigate the loss landscape effi-\nciently. The iterative update of model parameters concerning the loss function aims to converge towards the \nglobal minima. This results in a model that can provide more precise and accurate forecasts as it learns to capture \npatterns and relationships within the data. The loss function between the forecasted and original polarization \ncurrent is computed as follows:\nWhere N represents the number of instances, ipol is the actual polarization current value, and ifor\npol is the predicted \npolarization current value. After training is commenced, MSE, MAE, and RMSE performance metrics are used \n(5)it =σ( W xixt+ Whi ht−1 + bi)\n(6)ft =σ( W xfxt+ Whf ht−1 + bf)\n(7)ot =σ( W xoxt+ Whoh t−1 + bo)\n(8)˜ct=tanh(Wxc xt+ Whc ht−1 + bc)\n(9)ct =ft ⊙ ct − 1 + it ⊙˜ct\n(10)ht =ot ⊙ tanh(ct)\n(11)residual = ht + xt\n(12)/Psi1Loss mse = 1\nN\nN∑\ni=1\n(\nipol − ifor\npol\n)2\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nfor the evaluation of the proposed forecasting model on the test data set. Similarly, the mean absolute error \nmeasures the average absolute difference between the actual ( ipol ) and predicted ( ifor\npol ) polarization current values \nare evaluated as follows :\nWhere N represents the number of instances, ipol is the actual polarization current value, and ifor\npol is the predicted \npolarization current value. Additionally, the root mean squared error provides a square root of the average \nsquared differences between the actual ( ipol ) and predicted ( ifor\npol ) polarization current values, offering a more \nbalanced representation of errors. It can be defined as:\nWhere N represents the number of instances, ipol is the actual polarization current value, and ifor\npol is the predicted \npolarization current value.\nFigure 3a–d illustrates the RMSE curve of the trained forecasting model across epochs. It becomes apparent \nthat the initial transition in validation RMSE swiftly converges after 20 epochs for the residual LSTM model. In \ncontrast, for the attention of LSTM, LSTM, and GRU models, a prolonged transition is distinctly observable, \nsuggesting that the proposed forecasting model exhibits more efficient convergence dynamics. Figure 3e–h illus-\ntrates the progression of the loss curve across epochs of the different forecasting models. Notably, it becomes \nevident that the initial transition in validation loss swiftly converges within the first ten epochs for the residual \nLSTM model. In contrast, a more prolonged transition and higher loss are distinctly observable in the attention \nLSTM, LSTM, and GRU models. This distinction underscores the superior convergence dynamics exhibited by \nthe proposed forecasting model. The forecasting performance of attention LSTM, LSTM, GRU, CNN and residual \nLSTM models in the test set are shown in Fig. 4. Additionally, the models’ forecasting performance is examined \nunder two distinct cases: Case 1 and Case 2, as depicted in Fig.  4a and b, respectively. It becomes evident that \nattention LSTM, LSTM, and GRU models deliver elevated initial predictions. Particularly, the attention LSTM \nmodel demonstrates increased oscillations at the forecast horizon, while the CNN models showed improved \ninitial forecasting. In contrast, the residual LSTM not only maintains a low initial forecasting error but also \nshowcases minimal oscillations. This observation underscores the advantage of the residual LSTM model in \nachieving both accurate initial predictions and stable forecasting outcomes.\nTable 1 presents the polarization current forecasting results of different predictive models using two different \nlookback values: 60 and 100. The plot in Fig. 4c,d compares the MSE, MAE, and RMSE values for each model and \nlookback period, highlighting their respective forecasting accuracies. It is important to note that the XGBoost \nmodel’s performance remains unaffected by the variation in lookback, aligning with its inherent characteristic \nof not directly considering temporal dependencies. Among the sequence-based models, the residual LSTM \nstands out as the superior performer with both lookback settings, exhibiting the lowest MSE, MAE, and RMSE \nvalues. This superiority can be attributed to the residual LSTM’s architecture, incorporating residual connec-\ntions to capture intricate long-term patterns within the data effectively. On the other hand, the attention LSTM, \n(13)MAE = 1\nN\nN∑\ni=1\n⏐⏐⏐ipol − ifor\npol\n⏐⏐⏐\n(14)RMSE =\n√ 1\nN\nN∑\ni=1\n(\nipol − ifor\npol\n)2\nFigure 3.  RMSE curves for (a) Attention LSTM, (b) LSTM, (c) GRU, and (d) Residual LSTM models; Loss \ncurves for (e) Attention LSTM, (f) LSTM, (g) GRU, and (h) Residual LSTM models.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nLSTM, GRU and CNN models exhibit comparable but slightly higher errors, which can be detrimental for such \nlow-magnitude signals. However, the XGBoost model shows the highest errors among all approaches, indicating \nthat for this specific polarization current forecasting task, the residual LSTM model emerges as the preferred \nchoice due to its robust predictive capability and ability to capture complex temporal dependencies effectively.\nA Monte Carlo dropout prediction method, is implemented in this research paper to estimate the uncertainty \nof the polarization current forecasting models. In this context, for each deep learning model, the Monte Carlo \ndropout prediction function conducts a specified number of forward passes. These forward passes of Monte Carlo \nsamples traverse the model and gather corresponding  predictions20,21. The mean and standard deviation of these \npredictions across the number of Monte Carlo samples serve as crucial indicators of uncertainty, reflecting the \nmodel’s inherent variability and quantifying prediction confidence in forecasting outcomes. In the context of \nMonte Carlo uncertainty estimation, multiple predictions are generated by applying dropout during the testing \nphase. Each prediction is a sample from the distribution of possible outcomes given the model’s uncertainty. Each \nmodel’s Monte Carlo dropout predictions result in a set of mean uncertainty values. The histograms visualize the \ndistribution of mean uncertainties for each model by representing the probability density, which is a normalized \nmeasure of the likelihood of a particular range of mean uncertainty values. The analysis of Fig.  5a–j reveals the \nuncertainty associated with different models. Specifically, the residual LSTM model exhibits the most favorable \noutcome, manifesting the lowest uncertainty values. This achievement is underscored by the model’s low peak \ndensity, which materializes around 0.4 × 10−8 , achieved through the utilization of the preceding 60 seconds of \nhistorical data for predictive purposes. In contrast, the simple LSTM model displays the highest peak density \nlevel, reaching up to 8 × 109 , while the attention LSTM model follows with a peak density of approximately \n1.75 × 109 . Notably, the CNN model exhibits a reduction in uncertainty; however, its uncertainty in later stages \nremains higher than that of the residual LSTM model. This discrepancy becomes evident through the observation \nof heightened transients subsequent to the peak uncertainty period. Similarly, in the scenario where the model \nincorporates a considerable count of previous time steps or data points set at 100 for predictions at subsequent \ntime steps, the associated uncertainty estimations are illustrated in Fig. 5f–j. Examination of Fig. 5 yields insights \ninto the impact of expanded past data on uncertainty profiles. With an elevation in the number of preceding data \npoints, a discernible reduction is discerned in the upper bounds of uncertainty estimates for the attention LSTM \nmodel, decreasing from 6.75 × 10−9 to 6.2× 10−9 . In contrast, the simple LSTM model exhibits a modest incre-\nment in uncertainty estimations. This outcome is ascribed to the model’s limited capacity to manage substantial \nFigure 4.  Comparison of polarization current forecasting performance of different models for (a) Case 1 and \n(b) Case 2; Polarization current forecasting metrics (c) Case 1 and (d) Case 2.\nTable 1.  Average polarization current forecasting metrics of different approaches in the test set.\nModels\nCase 1 Case 2\nMSE MAE RMSE MSE MAE RMSE\nXGBoost 0.0059 0.0377 0.0766 0.0056 0.0370 0.0751\nAttention 0.0012 0.0087 0.0181 0.0011 0.0089 0.0184\nLSTM 0.0012 0.0085 0.0164 0.0010 0.0087 0.0166\nGRU 0.0011 0.0082 0.0154 0.0009 0.0086 0.0160\nCNN 0.0005 0.0058 0.0098 0.0004 0.0045 0.0081\nResidual 0.0003 0.0035 0.0052 0.0002 0.0034 0.0048\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\ndata volumes effectively. Furthermore, a noteworthy shift in the peak of uncertainty is observed in a rightward \ndirection, signaling tempered confidence in the predictive aptitude of the model. Conversely, the GRU model \nremains predominantly unaffected by the augmentation in historical data points. Concurrently, the residual \nLSTM model upholds its narrower characteristic uncertainty attributes while exhibiting a marginal reduction \nin its upper boundary. This adjustment underscores the model’s robust approach to forecasting polarization \ncurrents, preserving its reliability under evolving conditions.\nTable 2a presents an analysis of the residuals obtained from various forecasting models. Residuals signify the \ndiscrepancies between the predicted values and actual outcomes in a forecasting context. The Table showcases \nfive distinct forecasting models, including attention LSTM, LSTM, GRU, CNN, and residual LSTM networks. \nEach model’s performance is evaluated based on mean residual and standard deviation of residual, which are \npivotal indicators of the accuracy and consistency of the forecasting models. Mean residual quantifies the average \ndifference between the forecasted values and the actual outcomes. Negative values indicate an overestimation \ntrend, where the forecasted values tend to be slightly higher than the actual outcomes. Conversely, positive values \nreflect an underestimation trend, suggesting the forecasted values are slightly lower than the actual outcomes. \nIn this investigation, the attention LSTM, LSTM, GRU, and CNN-based forecasting models exhibit a negative \nmean residual, demonstrating a tendency to overestimate the actual values. However, the proposed residual \nLSTM model demonstrates a positive mean residual, implying a propensity to underestimate the actual values. \nFigure 5.  Histograms depicting the mean uncertainty of various polarization current forecasting models. \nFigures (a–e) are presented for Case 1, and Figures (f–i) for Case 2.\nTable 2.  Mean and standard deviation of residuals for different forecasting models in Case 1 and Case 2.\nModel\n(a) Case 1 (b) Case 2\nMean residual Std dev residual Mean residual Std dev residual\nAttention LSTM −1.6700 × 10−11 5.1807 × 10−11 −1.4836 × 10−11 4.8007 × 10−11\nLSTM −2.9950 × 10−11 7.0636 × 10−11 −2.5608 × 10−11 6.8006 × 10−11\nGRU −1.3892 × 10−11 4.5987 × 10−11 −1.1619 × 10−11 4.2006 × 10−11\nCNN −1.1850 × 10−11 3.2181 × 10−11 −1.1475 × 10−11 3.7358 × 10−11\nResidual LSTM 1.2017 × 10−11 2.7876 × 10−11 1.1260 × 10−11 2.3006 × 10−11\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nFurthermore, the standard deviation of residual measures the dispersion or variability of the forecast errors of \nmodels around their mean. Smaller standard deviations indicate consistent and precise predictions, while larger \nvalues suggest more variability and potential inconsistencies. Across all models, the standard deviations of residu-\nals are notably small, signifying that the forecasted values remain consistently relative to the actual outcomes. This \nconsistency aligns with the overall accuracy and reliability of the forecasting models. The presence of a positive \nmean residual in the Residual LSTM model indicates a distinctive predisposition towards underestimation. \nThis characteristic serves to distinguish the Residual LSTM model from the other models, all of which manifest \nnegative mean residuals. Similar results were observed for Case 2 when employing a larger number of historical \ndata points for the forecasting process. Notably, a slightly pronounced enhancement in forecasting outcomes is \nevident from the data presented in Table 2b.\nThe conductivity parameter σr can be approximately estimated by obtaining the steady-state DC component \nvalue of the forecasted polarization current ( ipol for\ndc  ) given as  follows14:\nTable 3 presents a comprehensive overview of the results obtained from investigating the DC conductivity of \noil-paper insulation samples using various deep-learning forecasting models. It diverges into two distinct cases, \neach accompanied by its corresponding parameters and outcomes. The Table presents the forecasted signal, \nDC conductivity values, and the associated percentage errors associated with each model’s predictions, which \nhave been scaled by a factor of 102 for clarity. Notably, it is evident that the residual LSTM model exhibits the \nlowest percentage error, indicating its superior forecasting accuracy in comparison to the other models across \nboth cases. It compares these deep-learning models’ comparative performance in estimating conductivity for \noil-paper insulation samples.\nDiscussion\nIn the systematic assessment of transformer insulation conditions by predicting short-duration polarization \ncurrents, a thorough assessment of various predictive models was conducted. A comprehensive evaluation of \ndistinct predictive models was undertaken to develop a short-duration PDC measurement approach, encompass-\ning Attention, LSTM, GRU, XGBoost, CNN, and Residual LSTM network configurations. Notably, the residual \nLSTM network model showcased a substantial reduction in MAE when contrasted with the baseline XGBoost \nmodel for case 2. These findings highlight the efficacy of the proposed forecasting model in capturing intricate \ntemporal relationships within polarization current data, emphasizing its potential to advance predictive accuracy \nacross various predictive horizons. This study investigates the refinement of predictive modeling techniques in \nanalyzing polarization phenomena of transformer insulation, which can be applied to reduce the cumulative \nmeasurement time of the time domain dielectric response technique.\nFurthermore, the Monte Carlo dropout prediction for uncertainty estimation in polarization current fore-\ncasting is implemented for quantifying the uncertainty handling capability of the proposed forecasting model. \nMultiple passes through deep learning models collect predictions, with mean and standard deviation indicating \nuncertainty. Analysis reveals that for case 1, the residual LSTM model with the least uncertainty has the lowest \npeak density, around 0.4 × 10−8 . Additionally, the simple LSTM model shows the highest peak density of 8 × 109 , \nwhile for attention LSTM model, uncertainty spread lies between 6.2× 10−9 to 6.75 × 10−9 with high density. \nFurthermore, the CNN model depicted comparable performance with the proposed residual LSTM model with \na slightly broader spread, indicating higher variability of probability density estimates.\nWith improved past data, attention to LSTM’s uncertainty decreases, while the simple LSTM’s uncertainty \nestimates show an increase that limits its capacity due to increases in computational complexity. The perfor -\nmance of the GRU model is still relatively stable; residual LSTM slightly reduces the upper boundary. Within \nthe framework of this investigation, the assessed forecasting models, namely attention LSTM, LSTM, and GRU, \ncollectively display a distinct pattern of negative mean residuals. This pattern implies a consistent inclination to \noverestimate the actual values during the intended prediction process. In contrast, the proposed residual LSTM \nmodel presents an intriguing departure from this trend, exhibiting a positive mean residual. This distinctive \ncharacteristic suggests a predisposition of this model to exhibit underestimation tendencies when forecasting \nactual values. The nuanced variations in mean residuals across these models offer valuable insights for refining \nvarious applications of polarization current forecasting scenarios. Furthermore, the insulation-sensitive conduc-\ntivity parameter derived from the proposed forecasted residual LSTM model shows the least percentage error \n(15)σr ≈ ε0\nipol for\ndc\nC 0E 0\nTable 3.  Parameter estimation from forecasted polarization current.\nForecasting model\nCase 1 Case 2\nσr  (DC conductivity) % Error (scaled by 102) σr  (DC conductivity) % Error (scaled by 102)\nAttention LSTM 1.7426e−11 6.3 1.7425e−11 5.7\nLSTM 1.7424e−11 5.1 1.7422e−11 4.0\nGRU 1.7423e−11 4.5 1.7421e−11 3.4\nCNN 1.7421e−11 3.4 1.7419e−11 2.3\nResidual LSTM 1.7419e−11 2.2 1.7417e−11 1.1\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\nin both cases. Moreover, a subtle improvement in the conductivity parameter is observed under case 2, which \nuses more past observed data samples.\nConclusion\nA short-duration polarization current forecasting methodology is proposed in this investigation. The proposed \ndeep-learning approach for short-term polarization current forecasting methodology substantially reduces the \ncumulative PDC measurement time and its subsequent alteration caused by external disruptions. The experimen-\ntal results using laboratory-prepared insulation samples demonstrate the proposed residual LSTM model’s effi-\ncacy in accurately forecasting polarization current data for a forecast horizon of 1300 s under both cases. Further-\nmore, the finding suggests that the proposed residual LSTM polarization current forecasting method shows the \nleast MSE, MAE, and RMSE values for case 2. Additionally, the statistical examination of the forecasting model \nreveals minimal deviation compared to other proposed models, with a mean residual value of 1.1260 × 10−11 \nand a standard deviation of residuals at 2.3006 × 10−11 for case 2. Moreover, a Monte Carlo dropout prediction \nmethod is utilized to measure the uncertainty of the polarization current forecasting models. Monte Carlo sam-\nples signify uncertainty, revealing prediction variability and confidence. Expanded preceding data reduces upper \nuncertainty bounds in the residual LSTM model. This marginal reduction in its upper boundary exhibits the \nmodel’s robust approach to forecasting polarization currents, preserving its reliability under varied conditions. \nFurthermore, the evaluation of an essential insulation-sensitive parameter σr is conducted using the anticipated \npolarization current. The findings underscore that the suggested polarization current forecasting methodology \ncan effectively curtail measurement duration by accurately computing insulation-sensitive parameters. This, in \nturn, prevents external distortions in polarization depolarization current measurements stemming from pro-\nlonged measurement periods by reducing the cumulative measurement time.\nThe polarization current measurements of samples in laboratory conditions exhibited minimal noise. How-\never, it’s noteworthy that in situ measurements could be susceptible to low-frequency noise caused by slow-\nvarying environmental conditions. Future research endeavors could investigate forecasting noise-affected polari-\nzation current with a more detailed analysis of multiple insulation-sensitive parameters. This extension of the \nmethodology could offer insights into addressing challenges posed by real-world environmental variations and \nfurther enhance the robustness of insulation modeling.\nData availability\nPlease contact corresponding author for data requests.\nReceived: 21 October 2023; Accepted: 22 December 2023\nReferences\n 1. He, D. et al. Dynamic behavior and residence characteristics of space charge in oil–paper insulation under polarity–reversal electric \nfield. IEEE Trans. Dielectr. Electr. Insul. (2023).\n 2. Vatsa, A. & Hati, A. S. Depolarization current prediction of transformers OPI system affected from detrapped charge using LSTM. \nIEEE Trans. Instrum. Meas. 71, 1–11 (2022).\n 3. Vatsa, A. & Hati, A. S. Transformer faults detection using inrush transients based on multi-class svm. In 2022 IEEE 6th International \nConference on Condition Assessment Techniques in Electrical Systems (CATCON), 24–29 (IEEE, 2022).\n 4. Vatsa, A. et al. Deep learning-based transformer moisture diagnostics using long short-term memory networks. Energies 16, 2382 \n(2023).\n 5. Wang, D. et al. A new testing method for the dielectric response of oil-immersed transformer. IEEE Trans. Ind. Electron. 67, \n10833–10843 (2019).\n 6. Mishra, D., Baral, A. & Chakravorti, S. Reliable assessment of oil-paper insulation used in power transformer using concise \ndielectric response measurement. IEEE Trans. Dielectr. Electr. Insul. 30,  1255–1264 (2023).\n 7. Mishra, D., Baral, A., Haque, N. & Chakravorti, S. Condition assessment of power transformer insulation using short-duration \ntime-domain dielectric spectroscopy measurement data. IEEE Trans. Instrum. Meas. 69, 4404–4411 (2019).\n 8. Shi, Q., Gu, C., Y ao, Z., Shen, H. & Wang, X. Study on relaxation current characteristics of oil-paper insulation under dc electric \nfield. In 2023 IEEE 4th International Conference on Electrical Materials and Power Equipment (ICEMPE), 1–4 (IEEE, 2023).\n 9. Banerjee, C. M., Baral, A. & Chakravorti, S. Detrapped charge-affected depolarization-current estimation using short-duration \ndielectric response for diagnosis of transformer insulation. IEEE Trans. Instrum. Meas. 69, 7695–7702 (2020).\n 10. Dutta, S., Mishra, D., Baral, A. & Chakravorti, S. Estimation of de-trapped charge for diagnosis of transformer insulation using \nshort-duration polarisation current employing detrended fluctuation analysis. High Volt. 5, 636–641 (2020).\n 11. Mishra, D., Haque, N., Baral, A. & Chakravorti, S. Assessment of interfacial charge accumulation in oil-paper interface in trans -\nformer insulation from polarization-depolarization current measurements. IEEE Trans. Dielectr. Electr. Insul. 24, 1665–1673 (2017).\n 12. Mishra, D., Verma, R., Baral, A. & Chakravorti, S. Investigation related to performance parameter estimation of power transformer \ninsulation using interfacial charge. IEEE Trans. Dielectr. Electr. Insul. 27, 1247–1255 (2020).\n 13. Mousavi, S. A., Hekmati, A., Sedighizadeh, M., Bigdeli, M. & Bazargan, A. Ann based temperature compensation for variations \nin polarization and depolarization current measurements in transformer. Therm. Sci. Eng. Prog. 20, 100671 (2020).\n 14. Mishra, D., Pradhan, A., Baral, A. & Chakravorti, S. Reduction of time domain insulation response measurement duration for fast \nand effective diagnosis of power transformer. IEEE Trans. Dielectr. Electr. Insul. 25, 1932–1940 (2018).\n 15. Mishra, D., Haque, N., Baral, A. & Chakravorti, S. Effect of charge accumulated at oil-paper interface on parameters considered \nfor power transformer insulation diagnosis. IET Sci. Meas. Technol. 12, 411–417 (2018).\n 16. Chakravorti, S., Dey, D. & Chatterjee, B. Recent trends in the condition monitoring of transformers. In Power Systems (Springer, \n2013).\n 17. Fu, S., Zhang, Y ., Lin, L., Zhao, M. & Zhong, S.-S. Deep residual LSTM with domain-invariance for remaining useful life prediction \nacross domains. Reliab. Eng. Syst. Saf. 216, 108012 (2021).\n 18. Kim, J., El-Khamy, M. & Lee, J. Residual lstm: Design of a deep recurrent architecture for distant speech recognition. arXiv pre-\nprintarXiv: 1701. 03360 (2017).\n 19. Sheng, Z., An, Z., Wang, H., Chen, G. & Tian, K. Residual lstm based short-term load forecasting. Appl. Soft Comput. 144, 110461 \n(2023).\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:1369  | https://doi.org/10.1038/s41598-023-50641-z\nwww.nature.com/scientificreports/\n 20. Lemay, A. et al. Improving the repeatability of deep learning models with Monte Carlo dropout. npj Digit. Med. 5, 174 (2022).\n 21. Sadr, M. A. M., Zhu, Y . & Hu, P . An anomaly detection method for satellites using Monte Carlo dropout. IEEE Trans. Aerosp. \nElectron. Syst. 59, 2044–2052 (2022).\nAuthor contributions\nA.V . and A.S.H. formulated methodology, software, conceptualization, formal analysis, investigation, and origi-\nnal draft preparation. A.V ., P .K., P .C., M.M., and A.S.H. are involved in conceptualization, formal analysis, and \nvalidation. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Residual",
  "concepts": [
    {
      "name": "Residual",
      "score": 0.7812281250953674
    },
    {
      "name": "Computer science",
      "score": 0.730983555316925
    },
    {
      "name": "Data mining",
      "score": 0.4779817461967468
    },
    {
      "name": "Monte Carlo method",
      "score": 0.47119948267936707
    },
    {
      "name": "Transformer",
      "score": 0.4708532691001892
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46702173352241516
    },
    {
      "name": "Machine learning",
      "score": 0.45919501781463623
    },
    {
      "name": "Convolutional neural network",
      "score": 0.41945961117744446
    },
    {
      "name": "Artificial neural network",
      "score": 0.4150780439376831
    },
    {
      "name": "Algorithm",
      "score": 0.31369176506996155
    },
    {
      "name": "Voltage",
      "score": 0.1916615068912506
    },
    {
      "name": "Engineering",
      "score": 0.12881061434745789
    },
    {
      "name": "Statistics",
      "score": 0.12606734037399292
    },
    {
      "name": "Electrical engineering",
      "score": 0.0888318419456482
    },
    {
      "name": "Mathematics",
      "score": 0.08692699670791626
    }
  ]
}