{
  "title": "You believe your LLM is not delusional? Think again! a study of LLM hallucination on foundation models under perturbation",
  "url": "https://openalex.org/W4410859056",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2183421436",
      "name": "Anirban Saha",
      "affiliations": [
        "Walmart (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2890880640",
      "name": "Binay Gupta",
      "affiliations": [
        "Walmart (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112198486",
      "name": "Anirban Chatterjee",
      "affiliations": [
        "Walmart (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095606750",
      "name": "Kunal Banerjee",
      "affiliations": [
        "Walmart (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2183421436",
      "name": "Anirban Saha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2890880640",
      "name": "Binay Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112198486",
      "name": "Anirban Chatterjee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095606750",
      "name": "Kunal Banerjee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4388585881",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W3198690080",
    "https://openalex.org/W4389217936",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W6857621569",
    "https://openalex.org/W3208206463",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4386272986"
  ],
  "abstract": "Abstract Large Language Model (LLM) has recently become almost a household term because of its wide range of applications and immense popularity. However, hallucination in LLMs is a critical issue as it affects the quality of an LLM’s response, reduces user trust and leads to the spread of misinformation. Detecting hallucination in the presence of the context or a golden response is relatively easier but it becomes considerably more challenging when both of these are absent, which is typically the case post deployment of an LLM. In this study, we present a framework that relies on query perturbation and consistency score calculation between the responses generated against the original query and the perturbed query to identify the potential hallucination scenarios. This framework has no dependency on the availability of the context or the ground truth. In this study, we focus on the popular foundation models because majority of the LLM applications leverage these specific models since training an LLM from scratch or even finetuning LLMs may require a lot of capital investment. Moreover, we specifically investigate LLM hallucinations under different levels of perturbation: character-level, word-level and sentence-level — robustness towards these perturbations indicates that an LLM has a good understanding of a concept, and thus is less susceptible to hallucinations – this, in turn, should help in the LLM’s user adoption. Our study shows that GPT-4 hallucinates the least when faced with perturbations; in contrast, other LLMs start hallucinating even with minor typos.",
  "full_text": "Vol.:(0123456789)\n Discover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7\nDiscover Data\nResearch\nYou believe your LLM is not delusional? Think again! a study of LLM \nhallucination on foundation models under perturbation\nAnirban Saha1 · Binay Gupta1 · Anirban Chatterjee1 · Kunal Banerjee1\nReceived: 2 January 2025 / Accepted: 2 May 2025\n© The Author(s) 2025  OPEN\nAbstract\nLarge Language Model (LLM) has recently become almost a household term because of its wide range of applications \nand immense popularity. However, hallucination in LLMs is a critical issue as it affects the quality of an LLM’s response, \nreduces user trust and leads to the spread of misinformation. Detecting hallucination in the presence of the context or a \ngolden response is relatively easier but it becomes considerably more challenging when both of these are absent, which \nis typically the case post deployment of an LLM. In this study, we present a framework that relies on query perturba-\ntion and consistency score calculation between the responses generated against the original query and the perturbed \nquery to identify the potential hallucination scenarios. This framework has no dependency on the availability of the \ncontext or the ground truth. In this study, we focus on the popular foundation models because majority of the LLM \napplications leverage these specific models since training an LLM from scratch or even finetuning LLMs may require a \nlot of capital investment. Moreover, we specifically investigate LLM hallucinations under different levels of perturbation: \ncharacter-level, word-level and sentence-level — robustness towards these perturbations indicates that an LLM has a \ngood understanding of a concept, and thus is less susceptible to hallucinations – this, in turn, should help in the LLM’s \nuser adoption. Our study shows that GPT-4 hallucinates the least when faced with perturbations; in contrast, other LLMs \nstart hallucinating even with minor typos.\nKeywords Large language model (LLM) · LLM hallucination · Perturbation · OpenAI GPT · Google Gemini\n1 Introduction\nLarge Language Model (LLM) is currently one of the most discussed topics in technology. Although the benefits of LLMs \nare undeniable, there are few concerns regarding these models which hinder their wider adoption – hallucination [1 ] \nlikely being the primary one. It may be important to note that in a recent work [2], the authors claim that hallucinations in \nLLMs is inevitable. Specifically, the authors formalize hallucinations as inconsistencies between a computable LLM and a \ncomputable ground truth function, then based on the results from information theory, they argue that it is impossible for \nan LLM to learn all possible computable functions – thus, inescapably leading to hallucinations. Moreover, the real world \nbeing much more complex than the formal world, hallucinations are much more likely to manifest in LLMs in practice.\n * Kunal Banerjee, kunal.banerjee1@walmart.com; Anirban Saha, anirban.saha@walmart.com; Binay Gupta, binay.gupta@walmart.com; \nAnirban Chatterjee, anirban.chatterjee@walmart.com | 1Walmart Global Tech, Bangalore 560103, Karnataka, India.\nVol:.(1234567890)\nResearch  \nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7\nTherefore, it is important to monitor an LLM post deployment to ensure its correct usage in production. However, unlike \nthe training stage, where ground truths or full contexts may be available, a practical strategy to check for hallucinations \npost deployment is to invoke the LLM multiple times with the same (or semantically similar) questions and then check for \nits consistency across the responses – this method was proposed in SelfCheckGPT [3]. Another line of work [4, 5] prescribes \nto add different levels of perturbations (word, character or sentence-level) to the questions and then check for consistency \namong the generated responses; a high consistency score even with perturbations indicates that the model has truly under-\nstood the concept and therefore, it is less likely to generate a hallucinated response. Note that these papers [4, 5] applied \nperturbations in a non-LLM context.\nThus, the contributions of this work are as follows: \n1. We propose for the first time to use perturbation based methodology to better evaluate robustness to hallucinations \nfor LLMs.\n2. We carry out extensive experiments with eight popular foundation models to underline the efficacy of the proposed \nstrategy; specifically, we show that the perturbation based method subsumes the one proposed by SelfCheckGPT [3], \nand one’s choice for an LLM based on its robustness to hallucination may alter by following our strategy compared \nto SelfCheckGPT.\n3. We make our code publicly available at https:// github. com/ anirb ans403/ robus tness_ frame work. — all our codes are \nmodularized, thereby allowing plug-n-play with different models, perturbation strategies and evaluation metrics \neasily.\n2  Related work\nHallucination detection in LLMs In spite of the challenges involved in detecting hallucinations, significant progress has been \nmade in the case of Retrieval-Augmented Generation (RAG) [6] based systems for LLMs [7, 8]. In fact, the RAGAs library [9], \nthat is primarily developed to evaluate RAG pipelines, provides metrics such as, context precision, context recall and answer \ncorrectness which may be re-purposed to measure LLM hallucinations. None of these methods, however, work for monitoring \nan LLM post its deployment especially for a non-RAG use-case (i.e., when the context is not present) because ground truths \nare hard to come by. Therefore, for such cases, the approach of SelfCheckGPT [3] is applicable whereby, the same (or semanti-\ncally similar) query is asked multiple times and if the LLM has understood the concept that the query pertains to, then all its \nresponses will be consistent with the original response. There are various models that may be used for checking consistency \n(hallucination) between a pair of strings such as, DeBERTa-v3 [10] based hallucination detector [11] and DistilRoBERTa [12] \nbased consistency checker [13]; however, based on our experience, we found that the hallucination evaluation model [14] \nby Vectara [15] performed the best since it had maximum matches with the human evaluations.\nPerturbation of textual data The faith in the robustness of a model is bolstered if it continues to perform equally well (i.e., \nwith minor degradation) when its inputs are perturbed, for example, in [4], the authors apply character and word-level per-\nturbations to the inputs of various language models and test their performance on tasks such as, text classification, named \nentity recognition and question answering; in another work [5], the authors apply word, character and sentence-level per-\nturbations to check the robustness of sentence encoders. In a more recent work [16], the authors explore character, word and \nsentence-level perturbations to check a LLM’s robustness to adversarial attacks [17]. To motivate why perturbation-based \nhallucination detection may be preferred, we use the example shown in Table 1 that shows two original queries (having \nperturbation-level “N/A”) involving the monthly growth of a baby boy and the annual growth of Mount Everest. The responses \nrecorded in this table are obtained from Gemini−1.5-Flash [18] – note that we have made minor cosmetic changes to the \nresponses for better illustration and brevity. As can be seen from Table 1, Gemini−1.5-Flash gives consistent answers even \nwith higher-level perturbations for the baby boy related queries compared to Mount Everest, and thus it may be concluded \nthat the concept of growth for a baby boy is better understood by this LLM than that of a mountain.\nVol.:(0123456789)\nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7 \n \n Research\nTable 1  An example using Gemini-1.5-Flash to illustrate perturbation-based hallucination detection\nPerturbation-level Query Response\nN/A What is the monthly growth in height of a 2 weeks old baby boy? A 2-week-old baby boy typically grows about 1/2 inch to 1 inch per month.\nCharacter What is the monhtly growth in height of a 2 weeks old bayby boy? A two-week-old baby boy typically grows about 1 inch per month.\nWord What is the monthly expansion in height of a 2 weeks old baby boy? A 2-week-old baby boy typically grows about 1 inch per month.\nSentence What is the rate of height increase per month for a baby boy who is two weeks \nold?\nIt’s impossible to determine a specific rate of height increase per month for \na two-week-old baby boy without more information but typically it’s 1/2 \nto 1 inch.\nN/A How much does Mount Everest grow higher every year? Mount Everest grows about 4 to 6 millimeters higher each year.\nCharacter How much does Mount Everest grw highr every year? Mount Everest grows about 4 millimeters higher each year.\nWord How much does Mount Everest elevate higher every year? Mount Everest does not elevate higher every year.\nSentence What is the annual increase in height of Mount Everest? Mount Everest is not growing in height annually.\nVol:.(1234567890)\nResearch  \nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7\n3  Methodology\nAlgorithm 1  Measure average consistency score for LLMs\nOur method is described concisely in Algorithm 1. Specifically, we explore six proprietary LLMs – the former three are from \nOpenAI while the latter three are from Google, and two open-source models – one from Meta and the other from Google.\n• GPT-3.5-Turbo: This model has 175 billion parameters that is the same as GPT-3, and is obtained by finetuning the \nGPT-3 model.\n• GPT-4: Some sources say that this model has 1.7 trillion parameters although OpenAI has not officially revealed its \nsize [19]. In contrast to its predecessor, this model supports multi-modality and has enhanced safety and alignment \nfeatures.\n• GPT-4-Turbo: This model is trained on data till December 2023 while GPT-4 is trained on data till September 2021 [20]. \nOur primary motivation for including both GPT-4 and GPT-4-Turbo is to check whether training on more data has any \nsignificant effect on LLM hallucinations.1\n1 Note that in practicality, the choice between GPT-4 and GPT-4-Turbo may depend on their costs, the supported context window length, \netc. – these differences, albeit important, do not play any role in this study.\nVol.:(0123456789)\nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7 \n \n Research\n• Gemini-1.0-Pro: We could not find any documentation that explicitly mentions the parameter size of any of the \nGoogle’s LLMs mentioned in this paper. This model [21], at the time of its creation, was the most capable and most \ngeneric generative AI model that Google had to offer.\n• Gemini-1.5-Pro: This model [18] improves upon its predecessor by introducing a new mixture-of-experts architecture \nwith support for context window up to a million tokens.\n• Gemini-1.5-Flash: This model is a smaller version of Gemini-1.5-Pro that performs slightly worse but still better than \nGemini-1.0-Pro [18] on NLP tasks.\n• Llama3–8B: This an open-source model from Meta having 8 billion parameters. This improves upon its predecessor [22] \nby reducing false refusal rates and increasing diversity in model responses among others.\n• Gemma2–9B: This is an open-source model from Google that is developed using the same research and technology as \nthat of Germini. It has 9 billion parameters and has been built responsibly to ensure safe usage. A primary motivation \nfor choosing this model is to compare the performances of a proprietary and an open-source LLM which have been \ndeveloped using the same methodology.\nNext we need a set of queries (called original queries) which will be used to check whether any of these LLMs are prone \nto hallucination with respect to the concepts that these queries pertain to. In case of real-world LLM monitoring, these \nqueries can be sampled from the prompts which are actually fed to the LLMs by its users. We perturb these queries to get \nan extended list of queries. For perturbation, we use the approach mentioned in [16] as described below briefly – note \nthat each level of perturbation has three different strategies to inject perturbation.\n• Character-level perturbation: (i) introduce typos to at most two words, (ii) change at most two letters in the sentence, \n(iii) add at most two extraneous characters to the end of the sentence.\n• Word-level perturbation: (i) replace at most two words with their synonyms, (ii) delete at most two words that do not \nalter the sentence’s meaning, (iii) add at most two semantically neutral words to the sentence.\n• Sentence-level perturbation: (i) add a randomly generated irrelevant handle at the end of the sentence, e.g., @abak, \n(ii) paraphrase the sentence, (iii) change the syntactic structure of the sentence.\nIt is important to note that replacing words by their synonyms, paraphrasing the sentence or changing the syntactic \nstructure (e.g., active to passive voice) do not alter the semantic meaning of the original query, and thus the perturbation \napproach of [16] subsumes the traditional approach that checks hallucination by modifying the original queries while \npreserving their semantic meanings.\nAs the next step, we fire each LLM with both the original queries and their perturbed versions. Finally, we check the \npair-wise consistency between the responses generated for the original query and each of its perturbed variants. One \nmay be interested in knowing whether an LLM M k has understood the concept that query Q l is related to, in such a case \nshe may look into the average consistency score computed in step 25 of Algorithm 1. If the user is interested in knowing \nthe overall average consistency score for an LLM for all the original queries, then they may refer to the score computed \nin step 27 of Algorithm 1. Note that consistency and hallucination are inversely related, i.e., higher consistency implies lesser \nhallucination and vice versa. It may be further noteworthy that our chosen model for computing consistency [14] produces \na real-valued score between 0 and 1 whereas, other models [11] may produce a binary score of either 0 or 1 to indicate \nwhether there is hallucination or no hallucination – the method described in Algorithm 1 is generic enough to handle \nsuch cases as well.\n4  Experimental results\n4.1  Dataset description\nWe have used the LongBench multitask dataset [23] for our experiments. LongBench is the first and widely adopted \nbenchmark dataset for evaluating the long-context understanding capabilities of LLMs. This dataset includes various \ndatasets for English and Chinese language related tasks. For our experiments, we selected datasets exclusively from the \nEnglish language related tasks. Among the chosen datasets, four datasets (TriviaQA, WikiQA, HotpotQA, and Qasper) \ncorrespond to regular question-answer tasks, one (SAMSum) corresponds to a summarization task, and one (TREC) \ninvolves text classification.\nVol:.(1234567890)\nResearch  \nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7\n4.2  Experiment 1\nAverage consistency scores of LLMs for character, word and sentence-level perturbations\nWe have measured the average consistency score of the six LLMs explored in this paper against perturbations at the \ncharacter, word, and sentence-levels aggregated across different datasets. From Fig. 1, we can see that, as expected, the \nperformance of the models degrade as one moves from the character-level to the sentence-level perturbations except \nfor GPT-4-Turbo and Gemini−1.5-Flash which see a small bump in performance for word-level perturbation with respect \nto character-level. Moreover, we observe that GPT-4 has the highest average consistency score compared to other LLMs, \nand surprisingly, GPT-4-Turbo has the least average consistency score. In case of Google’s LLMs, both the 1.5 models \noutperform that of 1.0 as expected; however, astonishingly, Gemini−1.5-Flash does better than the bigger Gemini−1.5-\nPro model. The two open-source models achieve much lower average consistency scores compared to the proprietary \nones with Gemma2–9B doing slightly better than Llama3–8B.\n4.3  Experiment 2\nGPT-4’s consistency score at sentence-level perturbation vs other models’ consistency scores at character-level \nperturbation\nWe now zoom into the average consistency score of GPT-4 for sentence-level perturbation and the scores for the \nrest of the models for character-level perturbation as shown in Fig.  2. Our scrutiny reveals that GPT-4’s performance is \nsuperior in spite of this skewed comparison; in other words, GPT-4 faced with sentence-level perturbation fares better \nthan other models even when the latter faced only minor typos. This finding reaffirms that GPT-4 is the most consistent \nmodel and is least prone to generating hallucinations.\n4.4  Experiment 3\nSelfCheckGPT vs our perturbation-based framework: Comparing average consistency scores when different LLMs \nare asked only semantically similar questions vs when asked questions with all types of perturbations\nFig. 1  Average consistency scores (after aggregating across datasets) of LLMs for character, word and sentence-level perturbations\nVol.:(0123456789)\nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7 \n \n Research\nIn this experiment, we did a comparative analysis between two comparison frameworks. We compared the results \nof SelfCheckGPT framework, where the model was asked only semantically similar questions, against our proposed \nframework where the model was asked questions with all three types of perturbations. As demonstrated in Fig.  3, the \nrelative ordering of the LLMs are different for the two frameworks. GPT-4 holds the top-most rank in both whereas, \nGemini−1.0-Pro and GPT-4-Turbo, among the proprietary LLMs, hold the bottom-most two ranks in both. However, with \nFig. 2  GPT-4’s average consistency score at sentence-level perturbation vs other models’ average consistency score at character-level per -\nturbation\nFig. 3  LLMs ranked by average consistency scores (across all datasets) for (left) semantically similar questions, (right) questions with charac -\nter, word and sentence-level perturbations\nVol:.(1234567890)\nResearch  \nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7\nSelfCheckGPT, GPT−3.5-Turbo was ranked second but it slips to the fourth place in our proposed perturbation-based \nframework. As previously mentioned in Sect.  3, the perturbation-based framework subsumes the framework that only \nasks semantically similar questions, and therefore we conclude that Gemini−1.5 models should be preferred over GPT−\n3.5-Turbo if one is aiming for a model that is less susceptible to hallucination – note that our recommendation lies in \ncontrast to what SelfCheckGPT would have recommended. The two open-source models rank the lowest in both the lists \nwith Gemma2–9B outperforming Llama3–8B in both the scenarios. Further note that although Gemini and Gemma2 are \ndeveloped using the same methodology, there is a substantial gap in their performances.\n5  Conclusion\nHallucinations in LLMs may hinder their adoption for critical applications. Consequently, it may be important for a client \nto choose an LLM that is least likely to hallucinate in production. Typically, following the concept of SelfCheckGPT [3], one \nasks semantically similar queries (to the original query) to evaluate an LLM’s hallucination susceptability. However, there \nis a line of work [4, 5, 16] that advocates to introduce perturbations to the LLM’s inputs (at character, word and sentence-\nlevel), and then check for hallucination — if the LLM continues to generate consistent answers compared to the original \nquery, then this should prove the LLM’s robustness to hallucination even further. Additionally, a more robust LLM should \nideally be less affected by adversarial attacks [16]. In this work, we present a framework that applies perturbations to \nLLM’s queries, generates their corresponding responses and then checks for hallucination; our code is modularized to \naccommodate different perturbation strategies, LLM models and hallucination detection models/metrics. We evaluate \nthis framework using the perturbations mentioned in PromptAttack [16] and Vectara’s hallucination detection model [14] \non eight prominent foundation models. Our evaluation shows that GPT-4 is most robust to hallucinations.\nAcknowledgements This is an independent research done by the authors and not endorsed by Walmart Global Tech in any manner.\nAuthor contributions A.S. was the primary contributor for the code; he also prepared all the figures. B.G. also contributed to the code and \nhelped identify the illustrative example. A.C. first proposed the idea and pointed out relevant literature, which form the foundation of the \ncurrent work. K.B. supervised the entire work; he wrote the paper and helped devise the experiments.\nFunding Not applicable\nData availability  The datasets generated and/or analyzed during the current study are available in the GitHub repository – https:// github.  \ncom/ anirb ans403/ robus tness_ frame work\nDeclarations \nEthics approval and consent to participate Not applicable\nConsent for publication Not applicable\nCompeting interests Not applicable\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which \npermits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to \nthe original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You \ndo not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by- nc- nd/4. 0/.\nReferences\n 1. Huang L, et al. A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions. CoRR arXiv:  \nabs/ 2311. 05232 2023.\n 2. Xu Z, Jain S, Kankanhalli MS. Hallucination is inevitable: an innate limitation of large language models. CoRR arXiv: abs/ 2401. 11817 2024.\n 3. Manakul P , Liusie A, Gales MJF. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models 2023.\n 4. Moradi M, Samwald M. Evaluating the robustness of neural language models to input perturbations 2021.\nVol.:(0123456789)\nDiscover Data            (2025) 3:20  | https://doi.org/10.1007/s44248-025-00041-7 \n \n Research\n 5. Chavan T, et al. SenTest: evaluating robustness of sentence encoders. CoRR arXiv: abs/ 2311. 17722 2023.\n 6. Lewis PSH, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks 2020.\n 7. Li J, Yuan Y, Zhang Z. Enhancing LLM factual accuracy with RAG to counter hallucinations: A case study on domain-specific queries in \nprivate knowledge-bases. CoRR arXiv: abs/ 2403. 10446 2024.\n 8. Friel R, Sanyal A. Chainpoll: A high efficacy method for LLM hallucination detection. CoRR arXiv: abs/ 2310. 18344 2023.\n 9. Es S, James J, Espinosa Anke L. Schockaert, S. RAGAs: automated evaluation of retrieval augmented generation 2024.\n 10. He P , Gao J, Chen W. DeBERTaV3: improving DeBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing \n2023.\n 11. Chowdary V. DeBERTa-v3-base fine-tuned for hallucination detection. https://  huggi ngface. co/ Varun- Chowd ary/ hallu cinat ion_ detect \n2024. Online; accessed 25-Jul-2024.\n 12. Sanh V, Debut L, Chaumond J. Wolf, T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR arXiv: abs/ 1910.  \n01108 2019.\n 13. cross encoder. Cross-encoder for natural language inference. https:// huggi ngface. co/ cross- encod er/ nli- disti lrobe rta- base 2021. ; Accessed \n25 Jul 2024.\n 14. Vectara. Hallucination evaluation model. https:// huggi ngface. co/ vecta ra/ hallu cinat ion_ evalu ation_ model 2024. Accessed 25 Jul 2024.\n 15. Vectara. The vectara platform. https:// docs. vecta ra. com/ docs/ 2024. Accessed 25 Jul 2024.\n 16. Xu X, et al. An LLM can fool itself: a prompt-based adversarial attack. CoRR arXiv: abs/ 2310. 13345 2023.\n 17. Wang B, et al. Adversarial GLUE: a multi-task benchmark for robustness evaluation of language models 2021.\n 18. Reid M, et al. Gemini 1.5: unlocking multimodal understanding across millions of tokens of context. CoRR arXiv: abs/ 2403. 05530 2024.\n 19. Heaven WD. Gpt-4 is bigger and better than chatgpt-but openai won’t say why. https:// www. techn ology review. com/ 2023/ 03/ 14/ 10698 \n23/ gpt-4- is- bigger- and- better- chatg pt- openai/ 2023. Online; accessed 25-Jul-2024.\n 20. OpenAI Platform. Models. https:// platf orm. openai. com/ docs/ models/ gpt-4- turbo- and- gpt-4 2024. Accessed 30 Jul 2024.\n 21. Anil R, et al. Gemini: a family of highly capable multimodal models. CoRR arXiv: abs/ 2312. 11805 2023.\n 22. Touvron H, et al. Llama 2: open foundation and fine-tuned chat models. CoRR arXiv: abs/ 2307. 09288 2023.\n 23. Bai Y, et al. Longbench: a bilingual, multitask benchmark for long context understanding. CoRR arXiv: abs/ 2308. 14508 2023.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6883045434951782
    },
    {
      "name": "Psychology",
      "score": 0.44393494725227356
    },
    {
      "name": "Psychoanalysis",
      "score": 0.41882574558258057
    },
    {
      "name": "Cognitive science",
      "score": 0.3280642628669739
    },
    {
      "name": "History",
      "score": 0.24104562401771545
    },
    {
      "name": "Archaeology",
      "score": 0.09126618504524231
    }
  ],
  "institutions": [],
  "cited_by": 2
}