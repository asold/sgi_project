{
  "title": "Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer",
  "url": "https://openalex.org/W4386608753",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3005591040",
      "name": "Obinna Agbodike",
      "affiliations": [
        "Chang Gung University"
      ]
    },
    {
      "id": "https://openalex.org/A2167328403",
      "name": "Jenhui Chen",
      "affiliations": [
        "Chang Gung University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2748263833",
    "https://openalex.org/W2617199345",
    "https://openalex.org/W2121396509",
    "https://openalex.org/W3215632849",
    "https://openalex.org/W2916002206",
    "https://openalex.org/W3201952986",
    "https://openalex.org/W3103549414",
    "https://openalex.org/W3036209635",
    "https://openalex.org/W6839940320",
    "https://openalex.org/W86140595",
    "https://openalex.org/W2132819211",
    "https://openalex.org/W634211087",
    "https://openalex.org/W4312938066",
    "https://openalex.org/W4297095624",
    "https://openalex.org/W4312885563",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4319300170",
    "https://openalex.org/W3043944826",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W4312832682",
    "https://openalex.org/W3035713416",
    "https://openalex.org/W4362653529",
    "https://openalex.org/W6810529214",
    "https://openalex.org/W4327521824",
    "https://openalex.org/W4312880823",
    "https://openalex.org/W4320900798",
    "https://openalex.org/W3194523157",
    "https://openalex.org/W4313413129",
    "https://openalex.org/W6799024212",
    "https://openalex.org/W2051328446",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W6794906783",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2161907179",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W4372270072",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4224223704",
    "https://openalex.org/W3104533206"
  ],
  "abstract": "Images corrupted by snowy adverse weather can impose performance impediments to critical high-level vision-based applications. Restoring snow-degraded images is vital, but the task is ill-posed and very challenging due to the veiling effect, stochastic distribution, and multi-scale characteristics of snow in a scene. In this regard, many existing image denoising methods are often less successful with respect to snow removal, being that they mostly achieve success with one snow dataset and underperform in others, thus questioning their robustness in tackling real-world complex snowfall scenarios. In this paper, we propose the wavelet in transformer (WiT) network to address the image desnow inverse problem. Our model exploits the joint systemic capabilities of the vision transformer and the renowned discrete wavelet transform to achieve effective restoration of snow-degraded images. In our experiments, we evaluated the performance of our model on the popular SRRS, SNOW100K, and CSD datasets, respectively. The efficacy of our learning-based network is proven by our obtained numeric and qualitative result outcomes indicating significant performance gains compared to image desnow benchmark models and other state-of-the-art methods in the literature. The source code is available at <uri>https://github.com/WINS-lab/WiT</uri>.",
  "full_text": "Received 17 August 2023, accepted 4 September 2023, date of publication 11 September 2023,\ndate of current version 18 September 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3313946\nRestoring Snow-Degraded Single Images With\nWavelet in Vision Transformer\nOBINNA AGBODIKE\n 1 AND JENHUI CHEN\n 2,3,4, (Senior Member, IEEE)\n1Department of Electrical Engineering, Chang Gung University, Taoyuan City 33302, Taiwan\n2Department of Computer Science and Information Engineering, Chang Gung University, Taoyuan City 33302, Taiwan\n3Division of Breast Surgery and General Surgery, Department of Surgery, Chang Gung Memorial Hospital, Taoyuan City 33375, Taiwan\n4Department of Electronic Engineering, Ming Chi University of Technology, New Taipei City 24301, Taiwan\nCorresponding author: Jenhui Chen (jhchen@mail.cgu.edu.tw)\nThis work was supported in part by the National Science and Technology Council, Taiwan, under Grant 110-2221-E-182-041-MY3; and in\npart by the Chang Gung Memorial Hospital, Taoyuan City, Taiwan, under Grant CMRPD2N0051.\nABSTRACT Images corrupted by snowy adverse weather can impose performance impediments to critical\nhigh-level vision-based applications. Restoring snow-degraded images is vital, but the task is ill-posed\nand very challenging due to the veiling effect, stochastic distribution, and multi-scale characteristics of\nsnow in a scene. In this regard, many existing image denoising methods are often less successful with\nrespect to snow removal, being that they mostly achieve success with one snow dataset and underperform\nin others, thus questioning their robustness in tackling real-world complex snowfall scenarios. In this paper,\nwe propose the wavelet in transformer (WiT) network to address the image desnow inverse problem. Our\nmodel exploits the joint systemic capabilities of the vision transformer and the renowned discrete wavelet\ntransform to achieve effective restoration of snow-degraded images. In our experiments, we evaluated the\nperformance of our model on the popular SRRS, SNOW100K, and CSD datasets, respectively. The efficacy\nof our learning-based network is proven by our obtained numeric and qualitative result outcomes indicating\nsignificant performance gains compared to image desnow benchmark models and other state-of-the-art\nmethods in the literature. The source code is available at https://github.com/WINS-lab/WiT.\nINDEX TERMS Attention, computer-vision, desnowing, transformer, wavelets.\nI. INTRODUCTION\nSnowfall is an inclement atmospheric weather condition that\noccurs seasonally per annum in over 50 countries spanning\nacross 23 percent of the earth’s surface. 1 The particles of\nsnow in a scene occur in various shapes, sizes, densities,\nand stochastic distribution [1] and result in complex pixel\nvariations that obscure the latent information in an image [2].\nAs depicted in Figure 1, this problem can introduce dire con-\nsequences in critical computer vision algorithms purposed\nfor object detection, classification, and even segmentation\napplications [1], [2], [3], [4], [5], [6], [7], [8], [9], which often\nrequires clean input image samples for optimal performance.\nTherefore, the need to restore snow-degraded images is\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Mohammad Shorif Uddin\n .\n1Britannica: https://britannica.com/science/snow-weather\ncrucial. Being an inverse problem, desnowing process\ninvolves the recovery of n number of unknown pixels in a\ndegraded vectorized image X ∈ RH×W ×C given that X =\nY +η where η denotes the additive noise vector (i.e., snowfall)\nsuperimposing the latent information in a clean image Y .\nEarly handcrafted solutions to this problem mostly relied\non physics-based algorithms to which guided filtering meth-\nods were commonly adopted [2], [10], [11], [12]. Invariably,\nthese forward processes are often formulated on very sparse\nassumptions and limited data that lack some of the intrin-\nsic feature characteristics of realistic snow and, therefore,\nunsurprisingly yield very poor image restoration outcomes.\nMeanwhile, since the emergence of deep learning, image\nrestoration methods have significantly improved [13], [14].\nPrecisely, the convolutional neural network (CNN) has pio-\nneered the liberation of the computer vision domain from\nhandcrafting visual features [15]; and thus became the defacto\n99470\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME 11, 2023\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nFIGURE 1. The outcome of YOLOv4 object detection on snow-degraded\nimages (top row) sampled from snow datasets [1], [7] versus their\ndesnowed and restored versions (bottom row) generated by our\nproposed method (best zoomed-in and viewed in color).\ntechnique for vision processing as a result of its eminent\noutcome improvements over previous methods. Despite these\naccolades, the limited receptive field of the CNN’s convolv-\ning kernels impedes its ability to effectively model long-range\nfeature dependencies [9], [16]. We attribute this limitation\nto account for why many existing CNN-based single image\ndesnow methods seemingly struggle to effectively remove\nmulti-scale snow syntheses when evaluated on different vary-\ning snow datasets [1], [6], [7] despite being fine-tuned for the\nspecific task.\nOn the other hand, the recent vision transformer (ViT) [17]\nand its subsequent modified variants [4], [9], [16] have man-\naged to demonstrate superior performance advantage over the\nCNN counterpart in various vision processing tasks, includ-\ning image denoising. By virtue of the transformer’s innate\nmulti-head self-attention (MHSA) and patch-based feature\nembedding mechanisms, the ViT is capable of facilitating\nglobal long-range feature representation superior to its prede-\ncessors [18]. For these reasons, we adopt ViT as the backbone\nnetwork for our image desnow task.\nRegardless, ViT also has a non-negligible downside involv-\ning a quadratic computational complexity that scales linearly\nin accordance with the size of the input image patches.\nBy default, the global long-range feature representation\nadvantage of the MHSA in ViT framework is designed for\ndense vision tasks [18] such as object classification, etc.,\nhence it is therefore not originally equipped to attend to small\nnoise elements, especially when the input patches are large,\ne.g., 16×16 [17] and above. So, for the image desnowing task,\nthis will result in information losses that invariably impact the\nlearning of local feature representations, which as a conse-\nquence, may tend to retain either remnants of omitted noise\ndetails (i.e., snow particles) or noticeable residual artifacts in\nthe generated output image.\nIn this paper, we propose the wavelet in transformer (WiT)\nnetwork, a single encoder-decoder model for restoring sin-\ngle images degraded by heterogeneous snowfall formations\ncomprising multi-scale η feature characteristics. Concisely,\nour method exploits the capabilities of Haar wavelets imple-\nmented in the transformer encoder to decompose the input\nimage features into multi-resolution low and high frequency\nrepresentations in-order to model better spatial and spectral\nlocalization of the features [19]. Firstly, we adapted the tech-\nniques in [4] and [20], whereby the input image patches are\nsub-partitioned into mini patch sizes during the embedding\nprocess to optimize the extraction of local representations in\nthe encoding block. Then, by applying the discrete wavelet\ntransform (DWT) and inverse discrete wavelet transform\n(IDWT) across the transformer MHSA [21], we achieve\nlossless downsampling, enhance the ViT’s attentional com-\nputation, and also simultaneously perform preliminary sup-\npression/filtering of some η elements (i.e., snow particles) in\nthe encoding process.\nIn summary, the main contributions of our work are given\nas follows.\n• We propose the WiT network for single-image snow\nremoval, which proves to be more robust and effective\nthan previous desnow methods upon evaluating 3 major\nsnow image datasets.\n• Our method adopts wavelet transform implementation\nin MHSA for image restoration task to optimize the\nlocalization of spatial and spectral input representations\nfor effective feature extraction while partially suppress-\ning snow noise elements simultaneously in the encoding\nprocess.\n• We performed extensive experiments that yielded state-\nof-the-art (SOTA) quantitative and qualitative results\ndemonstrating our model’s marginal performance gains\nover several CNN and transformer-based image desnow\nmethods, respectively.\nII. RELATED WORK\nThe restoration of single images degraded by particles of\nsnow adverse weather is an established ill-posed problem\nthat has attracted a lot of research attention since the era\nof deep-learning [1], [4], [5], [6], [7], [8], [9], [22], [23],\n[24], [25], [26], [27], [28], [29]. In [1], Liu et al. proposed\na multistage context-aware network (dubbed DesnowNet) to\ndeal with the complicated size, density, opaque and translu-\ncent characteristics of snow particle removal. Similarly, Chen\net al. in [7] proposed a joint size and transparency-aware\ndesnow network with the consideration of the veiling effect\n(i.e., haze) caused by the interaction of snow particles with\natmospheric luminance. In [6], dual-tree wavelet transform\nis implemented in CNN to enhance image desnowing perfor-\nmance involving snow streaks. Overall, the above-mentioned\nworks formulated the popular Snow100K [1], CSD [6], and\nSRRS [7] snow image datasets, respectively. In [28], Zhang et\nal. exploited semantic features and geometric prior informa-\ntion for snow removal with a deep dense multi-scale network\nVOLUME 11, 2023 99471\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\n(DDMSNet). Further scholarly works [5], [8], [25] have also\nattempted to restore snow-degraded images with generative\nadversarial networks (GAN). Recently, the emerging unified\nmodels designed for multiple noise elements’ removal have\nbegun to include desnow tasks in their portfolio. To this end,\nYe et al. [24] investigated the simultaneous removal of snow\nand haze with a degradation adaptive network. Likewise, [22]\nand [26] demonstrate fair success in snow, rain, and haze\nremoval tasks with complex CNN networks.\nWhile the aforementioned methods are based on CNN,\nthe modification of vision transformer [17] for single-image\nsnow removal is starting to gain new traction in litera-\nture [4], [9], [23], [27]. In [4] and [27], the authors proposed\nencoder-decoder ViT networks to tackle snow, rain, and fog\nremoval. Whereas the subsequent ViT variants in [9] and [23]\nare based on CNN-Transformer hybrid network design for the\nsole goal of single-image snow removal.\nHowever, inasmuch as the above-competing methods have\nimproved image desnow outcomes, their respective perfor-\nmances significantly oscillate from good to poor results\nupon evaluation with different snow image datasets [1],\n[6], [7]. For instance, while most of these methods attain\nvarying good results with a given snow dataset, they also\noften under-perform drastically when validated on other snow\nimage datasets, thus implying a potential lack of robustness\nfor tackling complicated snowfall scenarios. Our work is\nmotivated by this research gap, which prompted us to develop\nthe WiT network as a viable solution.\nSimilar to [4], our network architecture comprises a\nsingle-encoder or single-decoder transformer approach, but\nin contrast, we optimized our model specifically for\nmulti-scale snow removal tasks with the implementation of\nwavelet transform in the encoder block.\nIII. METHOD\nIn previous works, the classic pipeline approach to modeling\nthe ill-posed desnowing problem is formulated as\nX = Sη(T (ρ)) + A(1 − T (ρ)), (1)\nwhere X is the degraded input image, Sη denotes the scene\nwith snow particles (i.e., noise η) overlaying the latent image\ninformation given that there is an absence of veiling effect;\nA and T represents the coefficients of atmospheric illu-\nmination scattering (that introduces veiling effect) and the\ntransmission matrix, respectively. Further decomposition of\nSη will yield\nSη = Y (1 − Z)B(ρ) + C(ρ)X(ρ)B(ρ), (2)\nwhere Y denotes the clean image scene to be recovered, C and\nZ are the chromatic aberrations and snow mask, respectively;\nB denotes the binary mask for the localization of η in S, while\nρ is the varying pixels of each element.\nFrom the deductions in (1) and (2), it is apparent that\nsnow-mask is vital for the desnow process in previous meth-\nods. However, in real-world snowy scenarios, snow-mask\nis practically unavailable [23]. Hence, we summarize our\nmethod holistically as\nY = τ(X − η), (3)\nwhere τ represents our transformer encoder-decoder process\nof separating η snow-noise degradations from the input image\nX to generate the restored output image Y . Based on (3),\nwe formulate the operations of the proposed WiT network as\nundermentioned.\nA. ENCODING INITIALIZATION\nConsidering a snow-degraded input 2D RGB image X, where\nthe vectorized feature map is given by X ∈ RH×W ×C (with\nH × W height and width resolution, and C = 3 chan-\nnels), we first split X sequences into i number of patches\nX = [X1, X2, . . . ,Xi] ∈ Rn×H×W ×3. Unlike the standard\nViT [17], we adopt similar approaches in [4] and [20] to\nfurther subdivide the main patches into j mini patches which\ncan be given by X = [x1, x2, . . . ,xj]. Then, across M = 4 and\nm = 3 number of transformer blocks for main (M ) and mini-\npatch (m) encoding blocks, the split factor of ( H\nD × W\nD ) × Ci,j\nis applied, given that D is a spatial minimization divisor with\nvalues ranging from {4, 8, 16, 32} initiated respectively from\nthe first block to the nth block.\nThis facilitates the reduction of the image resolution across\neach encoding block stage to enhance the learning of global\nand local feature details. Also, by this operation, the com-\nputational complexity of the standard transformer MHSA is\nreduced from O(N2) to O(N2/D) [4], where N = H × W .\nMeanwhile, note that the initial main patches and their\ncorresponding mini-patch splits are generated prior to the\ntransformer attentional encoding process, as illustrated in\nFigure 2. Ab-initio, the image patch vectors Xi and xj are\nrespectively flattened for positional tokenization from where\ntheir lower-dimensional features (f 1, f2, . . . ,fn) are obtained.\nAs typical in the ViT backbone [17], we parsed the tokenized\nfeatures through layer normalization, after which they are\nlinearly projected sequentially into query Q, key K, and value\nV through the n number of hierarchically aligned transformer\nencoder blocks respectively comprising of the MHSA and\nfeed-forward layers.\nB. WAVELET IN TRANSFORMER OPERATION\nAs depicted in Figure 2, the WiT architecture has 4 main\ntransformer blocks that also receive inputs from 3 mini-\npatch blocks in the encoding stage. However, different from\nthe conventional MHSA computation, we implemented the\nDWT in the transformer attention block to perform the\ndownsampling operation. As shown in Figure 4, the DWT\nuses an invertible matrix to perform dyadic decomposi-\ntion of the normalized features of X into 4 sub-bands\ncomprising of multi-resolution low and high-frequency com-\nponents, namely: low-low (LL ) representing low-frequency\nband containing coarse-level patch features, and low-\nhigh (LH ), high-low (HL ), high-high (HH ) respectively,\nare the high-frequency bands of different fine-granularity\n99472 VOLUME 11, 2023\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nFIGURE 2. Overview of our proposed WiT network architecture. The pink arrows represent the upsample operation in the convolutional projection\nblock.\nFIGURE 3. The encoder attention block with wavelet transform\nintegration.\nof patch features. Next, the sub-bands are concatenated\n(X[LL,LH,HL,HH]), and with respect to [21], we use a single-\nlayer 3 × 3 convolution to impose spatial locality over the\nfeature map of X; after-which, the downsampled features are\nlinearly transformed into K and V (see Figure 3).\nRepresenting the MHSA function here-in as γ , the\n(Q, K, V ) computation in the WiT encoder can be given by\nγ (Q, K, V ) = softmax\n(\nQKT\nψ√\nd\n)\nVψ (4)\nwhere KT\nψ represents the transpose matrix of Kψ features, ψ\nrefers to the DWT downsampling which applies to K and V ;\nand d denotes the feature dimension which is the same for\nthe unanimous heads of the Q, K, and V features. Moreover,\ndownsampling the K and V feature projection further reduces\nthe γ computation [9]; and to achieve that with DWT also\nensures zero feature losses [21] unlike with average pooling\noften used in the typical ViT and its many variants [9], [20],\n[23], etc.\nIn addition, IDWT is applied across γ as shown in Figure 2\nand Figure 3 with the aim of enhancing the receptive field to\nstrengthen local feature contextualization [21]. Being that the\nnoise data η superimposing latent information in an image\nis classified as a high-frequency component [30], [31], it is\nbelieved that an implicit partial suppression of η from X is\nachieved in the encoder by the decomposition and filtering\nprocesses of the DWT-IDWT as expressed below in (5)\nX = ψ(L,H)\n\n\n\n\n\n\n\n\n↓ 2(fLL ), N\n22 × d\n4 , ↑ 2\n↓ 2(fLH ), N\n22 × d\n4 , ↑ 2\n↓ 2(fHL ), N\n22 × d\n4 , ↑ 2\n↓ 2(fHH ), N\n22 × d\n4 , ↑ 2\n\n\n\n\n\n\n\n\n= X′\nK,V (5)\nwhere N = H × W with a dyadic factor of 2 for DWT\ndownsampling (↓) and IDWT upsampling (↑), while d =\n4 sub-bands of the dimensional decimation.\nLastly, the outbound features of the IDWT is concatenated\nwith the Softmax to yield the output (denoted as X′) after the\nγ matrix multiplication of Q, K, V . At this juncture, the ten-\nsors are reshaped before X′ is fed to the encoder feed-forward\nnetwork (FFN) module. We adopted the FFN block that com-\nprises of a depth-wise convolution [32], [33] and multi-layer\nperceptron (MLP) with a single skip connection (as depicted\nin Figure 2). The feed-forward process computation can be\ngiven by\nFFN(X′) = MLP(GELU(DWC(X′))) (6)\nwhere GELU denotes the Gaussian error linear unit [34],\nand DWC is the depth-wise convolution. Based on the above\ndeductions, we can mathematically generalize the overall\nencoder process as\nτ(X) =\n∑\nFFN[Mi(γ (Xi)) ⊕ mj(γ (xj))], (7)\nVOLUME 11, 2023 99473\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nFIGURE 4. The dyadic DWT downsampling (↓); and IDWT upsampling (↑) operation implemented in the WiT encoder architecture. TheL\ndenotes feature reconstruction.\nwhereby Mi and mj represent the ith and jth main and\nmini-patch blocks respectively attended by γ ; and ⊕ denotes\nthe feature concatenation of m output to M input (see\nFigure 2).\nC. THE DECODING PROCESS\nThe decoder module of our network can be divided into two\ncomponent stages. The first stage is made up of 4 transformer\ndecoder blocks adapted from [4], [35], while the second stage\ncomponent comprises a convolutional projection block com-\nposed of 4 CNN layers. With respect to stage 1, we liken our\napproach to [4] which is inspired by [36] in employing the use\nof learnable object query Q embedding to classify and decode\ndifferent weather-type queries from the encoder-extracted K\nand V features. However, our transformer decoder, in con-\ntrast, subjects Q to specifically learn only the DWT-extracted\nand filtered multi-scale snow feature representations in X′\nconveyed via the tensors from the last M encoder block.\nIn the second stage process, the transformer-decoded feature\nvectors are forwarded through the convolution projection\nblock comprising 4 sequential CNN layers. Each convolu-\ntional layer is implemented with a dense residual block (RB)\nand aligned to an upsampling operation, respectively. The\nconvolutional projection (i.e., ConvProj) process performs\nthe final cleaning and regenerates the original size of the\nrestored image output Y .\nD. PROCESS ALGORITHM\nFrom the above-described processes, our WiT network’s\nmodus operandi for image desnowing and restoration task is\nhereby generalized in Algorithm 1 as given below.\nE. LOSS FUNCTIONS\nThe training of our WiT network is based on supervised\nlearning. To effectively learn the multi-scale image feature\nrepresentations, we deployed the smooth L1 and the feature\nloss functions, respectively, to support the end-to-end train-\ning. Let LsL1 denote the smooth L1 loss, then we have that\nLsL1 =\n{\n0.5([Y − Xg])2, if [Y − Xg] < 1\n[Y − Xg] − 0.5, otherwise, (8)\nwhere Xg denotes the ground-truth input images, and 1\nimplies the slope of the gradient decent which continu-\nously tends to smoothen the quadratic segment of [Y − Xg]\ntowards 0. Hence, the LsL1 is less susceptible to outliers\nAlgorithm 1Image (X ) Desnowing and Restoration\nInput: image degraded by snow (X )\nOutput: desnowed and restored image (Y )\nInitialization:\n1: Split X into overlapping patches, & forward the vec-\ntorized embedding of X ∈ RH×W ×C through M & m\nencoder blocks, respectively\nEncoding:\n2: Apply DWT (ψ) dyadic downsample matrices (↓2) & 3 ×\n3 convolution to extract spatial & spectral locality of K\nand V features\n3: Compute γ (Q, K, V ) in M = 4 and m = 3 blocks, with\n(4)\n4: Perform IDWT and concat the outputs of (4) and (5)\n5: If tensor mismatch at concat is True; resize tensors\n6: Forward extracted features (X ′) to τ–decoder\nDecoding:\n7: Use Q query embedding in τ–decoder to learn the\nextracted multi-scale η snow-particles in X′\n8: Forward X′ through 4 CNN-based (ConvProj + RB)\nupsampling blocks for cleaning & projection\n9: return Y restored output image\nand exploding gradients, thereby justifying our reason for its\nadoption.\nThe feature loss (denoted as LF ) also known as perceptual\nloss uses the summation of mean squared error (MSE) to\ncompare the feature similarities or discrepancies between Y\nand Xg based on the calculation of their Euclidean distances.\nHere, we use randomly sampled features from the 5th, 10th\nand 15th layers of a VGG19 [37] to compute LF as\nLF = MSE(VGG5,10,15(Y ), VGG5,10,15(Xg)), (9)\nand the MSE function is defined by,\nMSE(Y , Xg) = 1\nrc\n∑\nr,c\n[Xg − Y ]2, (10)\nwhere r and c represent the total number of rows and columns\nof the pixels in Xg and Y , respectively.\nOverall, we summarize the total losses for the WiT net-\nwork’s training as,\nLt = λ(LF ) + LsL1, (11)\nwhere λ is the network parameter that adjusts the weights of\nthe loss functions, respectively.\n99474 VOLUME 11, 2023\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nTABLE 1. Configuration settings of the WiT architecture.MLPer denotes\nthe MLP expansion ratio (here-in set to default values = 4),γh denotes the\nnumber of attention heads; and dim is the patch embedding dimensions.\nIV. EXPERIMENT\nA. DATASETS\nThe Snow100K[1] is the first large-scale snow image dataset\nconsisting of 100,000 input images with corresponding snow-\nfree ground-truths. In this data, 1,392 image samples are\nrealistic snow images while the rest are synthesized. Fur-\nthermore, this dataset is divided into 3 categorical subsets\nof different snow particles, namely: (a) the Snow100K-S\ncontaining synthesized images with small snow particles;\n(b) the Snow100K-M comprising both small and medium\nsnow particles; and (c) the Snow100K-L consisting of images\nwith small, medium and large sizes of synthesized snow\nparticle. For fair comparison, we evaluated our network with\nthe challenging Snow100K-L composed of heterogeneous\nsnow-particle sizes and distribution densities. Precisely,\nwe used 16,801 image samples for training, and 2,000 image\nsamples for our experimental test set.\nThe SRRS[7] snow dataset consists of 15,000 synthetic\nsnow images plus 1000 realistic snow images. Unlike in [1],\nthe input images in the SRRS include added synthesis of haze\nto simulate the veiling effect caused by the interaction of snow\nparticles with atmospheric luminance. For our experiment\nwith this dataset, we randomly sampled 14,000 image pairs as\nour training set and used the remaining 2,000 image samples\nas the test set.\nThe CSD [6] is a more recent synthetic snow dataset\nconsisting of only 10,000 image samples synthesized with\nthe inclusion of hazy veiling effect in a similar fashion as\nin [7]. But uniquely, the CSD contains additional syntheses\nof snow-streaks and differently randomized distribution of\nsnowballs of varying sizes and transparencies. We trained our\nproposed network with 8,000 input image samples from this\ndataset and reserved the remaining 2,000 image samples for\nthe test set.\nB. IMPLEMENTATION SETTINGS\nThe proposed WiT network is written in Python3 and imple-\nmented on the PyTorch 2 framework; and is end-to-end\n2PyTorch: https://pytorch.org/docs/1.7.1/\ntrainable. Firstly, the input training data pairs (i.e., snowy\nand ground-truth images) are auto-resized (i.e., cropped) to\n256 × 256 pixels. We trained the network with a single\nGPU NVIDIA RTX 2080 coupled to an Ubuntu-based OS.\nThe training iterations are set to 250 epochs with a batch\nsize of 32. We also set the initial learning rate to 2e −4 (i.e.,\n0.0002), and we adopted a learning-rate scheduler for step-\nbased decay to reduce the model’s learning rate by half at\nevery 50 epochs. Meanwhile, Adam optimizer [38] is adopted\nfor the gradient descent optimization; and lastly, the loss func-\ntions’ weight parameter λ is set to 0.03. Further configuration\nsettings used in the model are listed in Table 1.\nV. RESULTS\nIn this section, we present the outcome of our WiT network’s\nperformance validated on 3 different snow image datasets.\nHere, we also compared the performance of our method\nagainst other SOTA learning-based single image desnow\nmodels based on their quantitative (i.e., visual) and qualitative\n(i.e., numeric) outputs. With respect to the latter, we adopted\nthe renowned objective and subjective evaluation metrics,\nnamely the peak signal-to-noise ratio (PSNR) [39] and the\nstructural similarity index measure (SSIM) [40], respectively.\nMeanwhile, unlike many literary works that unfairly use the\ntraining results of snow-irrelevant architectures (e.g., dehaze\nand derain models) to measure the achievements of their\ndesnow approach, we strictly compare the output of our\nmethod with other methods that are validated on one or more\nsnow datasets by their original authors.\nA. QUALITATIVE PERFORMANCE ANALYSIS\nTo qualitatively measure the performance of our model,\nwe tabulate the numeric results obtained by our method\nin comparison to other SOTA methods validated on\nSnow100K [1], CSD [6], and SRRS [7] dissimilar synthetic\nsnow image datasets. As shown in Table 2, the bolded values\nindicate the best performing model(s) for each desnow task\nper snow image dataset. Evidently, our method outperformed\nall the enlisted benchmark and SOTA models upon evaluation\nwith the CSD and SRRS datasets at PSNR and SSIM, respec-\ntively (see Figure 5 for clarity). Regarding the evaluation with\nthe challenging Snow100K-L image dataset (comprising of\nheterogeneous snow-particle sizes and densities), our WiT\nnetwork attained the best outcome on the PSNR, but slightly\nlags behind [9] and [24] by a difference of 0.02 (approxi-\nmate 2.1%) at the SSIM. Nevertheless, in further analysis,\nwe distinctively observe that although the TransWeather [4]\nis our baseline in terms of being a single encoder-decoder\ntransformer model, our WiT network, however, significantly\noutperformed it across-board owing to the advantageous\neffect of our wavelet implementation.\nB. QUANTITATIVE PERFORMANCE ANALYSIS\nTo analyze the image quality of our method, we compared our\nvisual output against the four selected SOTA learning-based\nmodels [4], [6], [27], [28] that publicly availed their source\nVOLUME 11, 2023 99475\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nTABLE 2. Quantitative comparison of WiT versus other SOTA methods validated on Snow100K-L[1], CSD[6] and SRRS[7] test datasets. The boldly\nhighlighted values indicate best-performing methods.\nFIGURE 5. Graph plot showing the PSNR and SSIM results comparison of WiT versus other SOTA methods listed in Table 2.\ncodes. For a fair comparison, we adopt the same test data\ndistribution used in validating our network to evaluate these\nmodels on Snow100K-L [1], SRRS [7] and CSD [6] datasets,\nrespectively.\nIn Figure 6, we evaluated all the compared methods with\nthe Snow100K-L dataset and show their respective image\ndesnowing results. In the 2nd and 5th columns, one can\nobserve that the TransWeather [4] and the DDMS-Net [28]\nachieved similar snow removal outcomes that favorably com-\npete with ours (see columns 2, 5, and 6). Whereas on the other\nhand, the UMW-Transformer [27] and HDCW-Net [6] (in the\n3rd and 4th columns, respectively) visibly retained significant\nsnow particles η and artifacts in their restored image output;\nthus indicating their failure case on Snow100K-L dataset.\nFrom a literary perspective, note that [4] and [28] were trained\nand fine-tuned for the Snow100K-based desnow task by their\noriginal authors, while [27] and [6] were not.\nIn Figure 7, we portray the desnowed images generated\nfrom SRRS degraded input data. Here, the HDCW-Net out-\ncome greatly improved in contrast to its poor performance\nin Figure 6; although under keen observation, minuscule\nresidual artifacts can still be noticed. With respect to other\nmethods, one can observe visible remnants of η snowballs\nand mild haze in the DDMS-Net, TransWeather, and also\nin the UMW-Transformer models, respectively. Nonetheless,\none can see that the resulting outcome of the WiT network\nsurpassed all compared methods across the board on the\nSRRS dataset evaluation.\nFigure 8 shows the results of experimental evaluation with\nCSD dataset. In this case, the HDCW-Net and our proposed\n99476 VOLUME 11, 2023\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nFIGURE 6. Desnowed visual outputs from our proposed method versus the selected SOTA methods evaluated on Snow100K dataset[1]\n(zoom-in for details).\nFIGURE 7. Desnowed visual outputs from our proposed method versus other SOTA methods evaluated on SRRS dataset[7] (zoom-in for\ndetails).\nWiT network achieved the best visual quality outcomes (see\ncolumns 4 and 6). We attribute the strong competitive per-\nformance of the HDCW-Net to the integration of dual-tree\ncomplex wavelet transform (DTCWT) in its architecture,\ncoupled with the fact that the CSD dataset is proposed by\nthe original authors [6]. Further observations in Figure 8\nshows that the DDMS-Net and TransWeather struggles with\nthe removal of haze and translucent snow-balls; whereas, the\nUMW-Transformer was unable to remove the synthesized\nsnow-streaks.\nFurthermore, Figure 9 shows the plotted approximated\naverage PSNR and SSIM obtained by our method in com-\nparison to those of major benchmark desnow methods,\nnamely TransWeather [4], HDCW-Net [6], JSTASR [7] and\nDesnowNet [1]. To calculate the average scores of each\nmodel, we summed the PSNR and SSIM result values gotten\nfrom each of the three datasets used in our experiments (see\nTable 2) and then divided their respective summations by 3.\nConspicuously, one can observe in Figure 9 that the overall\naverage performance of our method significantly surpassed\nall the existing benchmark models unequivocally.\nLastly, in Figure 10, we show a bar chart of the inference\nruntime of our method compared against selected key bench-\nmark models, namely: the ViT-based TransWeather, and the\nCNN-based HDCW-Net and JSTASR, respectively. In each\nmodel, we performed inference with 3 images randomly\nVOLUME 11, 2023 99477\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nFIGURE 8. Desnowed visual outputs from our proposed method versus the selected SOTA methods evaluated on CSD test dataset[6]\n(zoom-in for details).\nFIGURE 9. Plot of average PSNR and average SSIM results of WiT versus\nselected benchmark desnow models validated on[1], [6], and[7] snow\ndatasets.\nsampled from the Snow100K-L dataset [1]. At a glance, one\ncan observe that the ViT-based approaches, i.e., TransWeather\nand WiT achieved a much significantly less runtime than\nthose of the CNN-based counterparts.\nThus, on the basis of the results’ analysis above, our\nproposed method proves to be robust and effective for the\nrestoration of 2D RGB images degraded by multi-scale η\nsnow particles.\nVI. ABLATION STUDY\nWe used the CSD dataset to conduct ablation studies on\nWiT’s architecture to ascertain the contributions of the key\nFIGURE 10. Inference runtime of WiT compared to selected benchmark\nmethods on 3 snow image data samples [1].\nTABLE 3. Ablation results with CSD dataset. Here mPE denotes\nmini-patch encoding.\ncomponents with respect to the model’s performance. Ini-\ntially, we removed all components except the ViT backbone\ncomprising of 4 encoding blocks and the convolutional\nprojection (ConvProj). Then we added the DWT/iDWT\nimplementation on the MHSA. Afterward, we deployed the\nmini-patch embedding/encoding blocks to the network. The\nresultant effect of the respective components on the overall\nnetwork performance is shown in Table 3.\n99478 VOLUME 11, 2023\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\nVII. CONCLUSION\nIn this study, we show that previous image desnowing models\nin the literature that perform very well with a given snow\ndataset often underperform drastically when evaluated with\nother different snow datasets; thus indicating their proba-\nble lack of robustness under dynamic real-world complex\nsnowy scenarios. To address this problem, we propose a\nstraight-forward wavelet in the transformer network. Firstly,\nwe based our method on a single encoder-decoder ViT\narchitecture that is end-to-end trainable. Secondly, we imple-\nmented a DWT in the MHSA block to exploit the lossless\ndownsampling and feature decomposition capabilities of\nHaar wavelets, while simultaneously achieving preliminary\nnoise suppression prior to the decoding process. In addi-\ntion, we employ the use of the main/mini-patch encoding\ntechnique to equip our model for effective generalization of\nfeature representations. Our extensive experiments with three\npopular snow image datasets produced robust qualitative\nand quantitative results that significantly outperform several\nexisting SOTA image desnow methods.\nIn future work, our proposed model can be opti-\nmized to simultaneously tackle the multiple tasks of all\nweather-induced noise removal (e.g., snow, rain, and haze)\nand scene restoration with image and video multi-modal input\ndata, respectively.\nREFERENCES\n[1] Y .-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, ‘‘DesnowNet:\nContext-aware deep network for snow removal,’’ IEEE Trans. Image Pro-\ncess., vol. 27, no. 6, pp. 3064–3073, Jun. 2018.\n[2] Y . Wang, S. Liu, C. Chen, and B. Zeng, ‘‘A hierarchical approach for rain\nor snow removing in a single color image,’’ IEEE Trans. Image Process.,\nvol. 26, no. 8, pp. 3936–3950, Aug. 2017.\n[3] L.-W. Kang, C.-W. Lin, and Y .-H. Fu, ‘‘Automatic single-image-based rain\nstreaks removal via image decomposition,’’ IEEE Trans. Image Process.,\nvol. 21, no. 4, pp. 1742–1755, Apr. 2012.\n[4] J. M. Jose Valanarasu, R. Yasarla, and V . M. Patel, ‘‘TransWeather:\nTransformer-based restoration of images degraded by adverse weather\nconditions,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2022, pp. 2343–2353.\n[5] Z. Li, J. Zhang, Z. Fang, B. Huang, X. Jiang, Y . Gao, and J.-N. Hwang,\n‘‘Single image snow removal via composition generative adversarial net-\nworks,’’IEEE Access, vol. 7, pp. 25016–25025, 2019.\n[6] W.-T. Chen, H.-Y . Fang, C.-L. Hsieh, C.-C. Tsai, I.-H. Chen, J.-J. Ding, and\nS.-Y . Kuo, ‘‘ALL snow removed: Single image desnowing algorithm using\nhierarchical dual-tree complex wavelet representation and contradict chan-\nnel loss,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Montreal,\nQC, Canada, Oct. 2021, pp. 4176–4185.\n[7] W.-T. Chen, H.-Y . Fang, J.-J. Ding, C.-C. Tsai, and S.-Y . Kuo, ‘‘JSTASR:\nJoint size and transparency-aware snow removal algorithm based on modi-\nfied partial convolution and veiling effect removal,’’ in Proc. ECCV Conf.,\nNov. 2020, pp. 754–770.\n[8] D.-W. Jaw, S.-C. Huang, and S.-Y . Kuo, ‘‘DesnowGAN: An efficient single\nimage snow removal framework using cross-resolution lateral connection\nand GANs,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 4,\npp. 1342–1350, Apr. 2021.\n[9] S. Chen, T. Ye, Y . Liu, T. Liao, J. Jiang, E. Chen, and P. Chen,\n‘‘MSP-former: Multi-scale projection transformer for single image\ndesnowing,’’ 2022, arXiv:2207.05621.\n[10] X. Zheng, Y . Liao, W. Guo, X. Fu, and X. Ding, ‘‘Single-image-based rain\nand snow removal using multi-guided filter,’’ in Proc. Int. Conf. Neural\nInfo. Process., Berlin, Germany, 2013, pp. 258–265.\n[11] J. Xu, W. Zhao, P. Liu, and X. Tang, ‘‘An improved guidance image based\nmethod to remove rain and snow in a single image,’’ Comput. Inf. Sci.,\nvol. 5, no. 3, pp. 49–55, Apr. 2012.\n[12] X. Ding, L. Chen, X. Zheng, Y . Huang, and D. Zeng, ‘‘Single image\nrain and snow removal via guided L0 smoothing filter,’’ Multimedia Tools\nAppl., vol. 75, no. 5, pp. 2697–2712, Mar. 2016.\n[13] L. Chen, X. Chu, X. Zhang, and J. Sun, ‘‘Simple baselines for image\nrestoration,’’ 2022, arXiv:2204.04676.\n[14] H. Yang, B. Sun, B. Li, C. Yang, Z. Wang, J. Chen, L. Wang, and H. Li,\n‘‘Iterative class prototype calibration for transductive zero-shot learning,’’\nIEEE Trans. Circuits Syst. Video Technol., vol. 33, no. 3, pp. 1236–1246,\nMar. 2023.\n[15] H. Zhang, J. Duan, M. Xue, J. Song, L. Sun, and M. Song, ‘‘Bootstrap-\nping ViTs: Towards liberating vision transformers from pre-training,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 8944–8953.\n[16] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\nImage restoration using Swin transformer,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis. Workshops (ICCVW), Oct. 2021, pp. 1833–1844.\n[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ in Proc. ICLR, May 2021, pp. 1–22.\n[18] X. Chen, Q. Hu, K. Li, C. Zhong, and G. Wang, ‘‘Accumulated trivial atten-\ntion matters in vision transformers on small datasets,’’ in Proc. IEEE/CVF\nWinter Conf. Appl. Comput. Vis. (WACV), Jan. 2023, pp. 3973–3981.\n[19] K. Bnou, S. Raghay, and A. Hakim, ‘‘A wavelet denoising approach\nbased on unsupervised learning model,’’ EURASIP J. Adv. Signal Process.,\nvol. 2020, no. 1, pp. 1–26, Jul. 2020.\n[20] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, ‘‘Transformer in\ntransformer,’’ in Proc. NeurIPS, Dec. 2021, pp. 1–12.\n[21] T. Yao, Y . Pan, Y . Li, C.-W. Ngo, and T. Mei, ‘‘Wave-ViT: Unifying wavelet\nand transformers for visual representation learning,’’ in Proc. ECCV Conf.,\nOct. 2022, pp. 328–345.\n[22] R. Li, R. T. Tan, and L.-F. Cheong, ‘‘All in one bad weather removal\nusing architectural search,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2020, pp. 3172–3182.\n[23] J. Lin, N. Jiang, Z. Zhang, W. Chen, and T. Zhao, ‘‘LMQFormer: A laplace-\nprior-guided mask query transformer for lightweight snow removal,’’ IEEE\nTrans. Circuits Syst. Video Technol., early access, Apr. 5, 2023, doi:\n10.1109/TCSVT.2023.3264824.\n[24] T. Ye, S. Chen, Y . Liu, E. Chen, and Y . Li, ‘‘Towards efficient single image\ndehazing and desnowing,’’ 2022, arXiv:2204.08899.\n[25] G. Kim, S. Cho, D. Kwon, S. H. Lee, and J. Kwon, ‘‘Dual gradient based\nsnow attentive desnowing,’’IEEE Access, vol. 11, pp. 26086–26098, 2023.\n[26] W.-T. Chen, Z.-K. Huang, C.-C. Tsai, H.-H. Yang, J.-J. Ding, and\nS.-Y . Kuo, ‘‘Learning multiple adverse weather removal via two-stage\nknowledge learning and multi-contrastive regularization: Toward a unified\nmodel,’’ inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2022, pp. 17632–17641.\n[27] A. Kulkarni, S. S. Phutke, and S. Murala, ‘‘Unified transformer network\nfor multi-weather image restoration,’’ in Proc. ECCV Conf., Oct. 2022,\npp. 344–360.\n[28] K. Zhang, R. Li, Y . Yu, W. Luo, and C. Li, ‘‘Deep dense multi-scale\nnetwork for snow removal using semantic and depth priors,’’ IEEE Trans.\nImage Process., vol. 30, pp. 7419–7431, 2021.\n[29] Y . Quan, X. Tan, Y . Huang, Y . Xu, and H. Ji, ‘‘Image desnowing via deep\ninvertible separation,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 33,\nno. 7, pp. 3133–3144, Jul. 2023.\n[30] Q. Li, L. Shen, S. Guo, and Z. Lai, ‘‘WaveCNet: Wavelet integrated CNNs\nto suppress aliasing effect for noise-robust image classification,’’ in Proc.\nIEEE CVPR, Jun. 2020, pp. 7245–7254.\n[31] X. Han and X. Chang, ‘‘An intelligent noise reduction method for chaotic\nsignals based on genetic algorithms and lifting wavelet transforms,’’ Inf.\nSci., vol. 218, pp. 103–118, Jan. 2013.\n[32] F. Chollet, ‘‘Xception: Deep learning with depthwise separable convo-\nlutions,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJul. 2017, pp. 1800–1807.\n[33] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool, ‘‘LocalViT: Bringing\nlocality to vision transformers,’’ 2021, arXiv:2104.05707.\n[34] D. Hendrycks and K. Gimpel, ‘‘Gaussian error linear units (GELUs),’’\n2016, arXiv:1606.08415.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv. Neu-\nral Inf. Process. Syst., Long Beach, CA, USA, Dec. 2017, pp. 5998–6008.\nVOLUME 11, 2023 99479\nO. Agbodike, J. Chen: Restoring Snow-Degraded Single Images With Wavelet in Vision Transformer\n[36] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nECCV Conf., 2020, pp. 213–229.\n[37] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ in Proc. Int. Conf. Learn. Represent.\n(ICLR), San Diego, CA, USA, May 2015, pp. 1–14.\n[38] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\nin Proc. ICLR, San Diego, CA, USA, May 2015, p. 1.\n[39] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ‘‘A statistical evaluation of\nrecent full reference image quality assessment algorithms,’’ IEEE Trans.\nImage Process., vol. 15, no. 11, pp. 3440–3451, Nov. 2006.\n[40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‘‘Image quality\nassessment: From error visibility to structural similarity,’’ IEEE Trans.\nImage Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.\nOBINNA AGBODIKEreceived the B.Eng. degree\nin electronic and computer engineering (ECE)\nfrom Nnamdi Azikiwe University, Nigeria, in\n2010, the M.S. degree in telecommunications engi-\nneering from the University of Sunderland, Tyne\nand Wear, England, in 2016, and the Ph.D. degree\nin electrical engineering from Chang Gung Uni-\nversity, Taoyuan, Taiwan, in 2023. His research\ninterests include but are not limited to computer-\nnetworking protocols, deep learning-based design\nand optimization techniques for computer vision, cyber-physical systems,\nand multi-modal encoding for human to computer interactions (HCI).\nJENHUI CHEN (Senior Member, IEEE) received\nthe B.S. and Ph.D. degrees in computer sci-\nence and information engineering (CSIE) from\nTamkang University, Taipei City, Taiwan, in\n1998 and 2003, respectively. Since 2003, he has\nbeen with the Department of CSIE, College of\nEngineering, Chang Gung University, Taiwan,\nwhere he is currently a Full Professor and the\nChairperson. He also serves as the Section Head of\nthe Technology Foresight, Artificial Intelligence\n(AI) Research Center, Chang Gung University. He is a Professor with\nthe Center for AI in Medicine, Chang Gung Memorial Hospital, Taoyuan\nCity, Taiwan. His main research interests include the design, analysis, and\nimplementation of human-like intelligence (HI), computer vision, image pro-\ncessing, deep learning, data science, and multimedia processing. He served\nas the Technical Program Committee (TPC) Member for IEEE Globecom,\nIEEE VTC, IEEE ICC, IEEE ICCC, IEEE ICCCN, IEEE 5G World Forum,\nand ACM CCIOT. He also served as a reviewer for many famous academic\njournals which are organized by ACM, Elsevier, IEEE, and Springer. He is a\nSenior Editor of Cogent Engineering.\n99480 VOLUME 11, 2023",
  "topic": "Snow",
  "concepts": [
    {
      "name": "Snow",
      "score": 0.7963331937789917
    },
    {
      "name": "Computer science",
      "score": 0.6987389326095581
    },
    {
      "name": "Wavelet",
      "score": 0.6501774787902832
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6427047252655029
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5796504616737366
    },
    {
      "name": "Transformer",
      "score": 0.5655555725097656
    },
    {
      "name": "Wavelet transform",
      "score": 0.4795389175415039
    },
    {
      "name": "Sobol sequence",
      "score": 0.4705422520637512
    },
    {
      "name": "Complex wavelet transform",
      "score": 0.4488799273967743
    },
    {
      "name": "Machine learning",
      "score": 0.4313633441925049
    },
    {
      "name": "Computer vision",
      "score": 0.37248826026916504
    },
    {
      "name": "Discrete wavelet transform",
      "score": 0.2953922748565674
    },
    {
      "name": "Electronic engineering",
      "score": 0.12999001145362854
    },
    {
      "name": "Engineering",
      "score": 0.10899662971496582
    },
    {
      "name": "Meteorology",
      "score": 0.09646877646446228
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Sensitivity (control systems)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173093425",
      "name": "Chang Gung University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I3020100970",
      "name": "Chang Gung Memorial Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I12213908",
      "name": "Ming Chi University of Technology",
      "country": "TW"
    }
  ],
  "cited_by": 10
}