{
  "title": "D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation",
  "url": "https://openalex.org/W4386409816",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4304358526",
      "name": "Slama, Rim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4283810492",
    "https://openalex.org/W3094530900",
    "https://openalex.org/W4281568353",
    "https://openalex.org/W6747470335",
    "https://openalex.org/W4211058727",
    "https://openalex.org/W4296478337",
    "https://openalex.org/W2973099078",
    "https://openalex.org/W2021761995",
    "https://openalex.org/W6759872093",
    "https://openalex.org/W2919910909",
    "https://openalex.org/W6787622577",
    "https://openalex.org/W2994931466",
    "https://openalex.org/W6747795875",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788136367",
    "https://openalex.org/W4313551195",
    "https://openalex.org/W3160967194",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W3009800047",
    "https://openalex.org/W6644904821",
    "https://openalex.org/W6648108922",
    "https://openalex.org/W2163901584",
    "https://openalex.org/W6644406482",
    "https://openalex.org/W2032984112",
    "https://openalex.org/W2169686358",
    "https://openalex.org/W3164110739",
    "https://openalex.org/W6642955444",
    "https://openalex.org/W910083903",
    "https://openalex.org/W2319894388",
    "https://openalex.org/W2029492234",
    "https://openalex.org/W2922370718",
    "https://openalex.org/W2501250055",
    "https://openalex.org/W2968932239",
    "https://openalex.org/W2778538683",
    "https://openalex.org/W6779877729",
    "https://openalex.org/W2982039175",
    "https://openalex.org/W2274145879",
    "https://openalex.org/W6808777092",
    "https://openalex.org/W4282981352",
    "https://openalex.org/W7073957439",
    "https://openalex.org/W2916620825",
    "https://openalex.org/W3047772167",
    "https://openalex.org/W6761274517",
    "https://openalex.org/W6640754710",
    "https://openalex.org/W6806676208",
    "https://openalex.org/W6762060095",
    "https://openalex.org/W2036196300",
    "https://openalex.org/W6756515473",
    "https://openalex.org/W6785169146",
    "https://openalex.org/W3217235291",
    "https://openalex.org/W2951095152",
    "https://openalex.org/W2783192928",
    "https://openalex.org/W4250881727",
    "https://openalex.org/W2580770992",
    "https://openalex.org/W2963369114",
    "https://openalex.org/W2885930233",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W2810021685",
    "https://openalex.org/W4243363229",
    "https://openalex.org/W2623902889",
    "https://openalex.org/W4234778558",
    "https://openalex.org/W4248252780",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3034428823",
    "https://openalex.org/W4287755748",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W190293641",
    "https://openalex.org/W4323062123",
    "https://openalex.org/W4240278061",
    "https://openalex.org/W4214540501"
  ],
  "abstract": null,
  "full_text": "D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network\nbased on Transformerfor Assessment of Patient PhysicalRehabilitation\nYoussef MOURCHIDa,âˆ—,1, Rim SLAMAb,1\naCESI LINEACT, UR 7527, Dijon, 21800, , France\nbCESI LINEACT, UR 7527, Lyon, 69100, , France\nARTICLE INFO\nKeywords:\nAutomatic assessment\nRehabilitation\nSpatio-Temporal\nGraph convolution networks\nTransformer\nAttention mechanism\nABSTRACT\nThis paper tackles the challenge of automatically assessing physical rehabilitation exercises for\npatients who perform the exercises without clinician supervision. The objective is to provide a\nquality score to ensure correct performance and achieve desired results. To achieve this goal, a new\ngraph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is\nintroduced. This model combines a modified version of STGCN and transformer architectures for\nefficienthandlingofspatio-temporaldata.Thekeyideaistoconsiderskeletondatarespectingitsnon-\nlinear structure as a graph and detecting joints playing the main role in each rehabilitation exercise.\nDense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and\neffectively model temporal dynamics. The transformer encoderâ€™s attention mechanism focuses on\nrelevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The\nevaluationofourproposedapproachontheKIMOREandUI-PRMDdatasetshighlighteditspotential,\nsurpassing state-of-the-art methods in terms of accuracy and computational time. This resulted in\nfaster and more accurate learning and assessment of rehabilitation exercises. Additionally, our model\nprovides valuable feedback through qualitative illustrations, effectively highlighting the significance\nof joints in specific exercises.\n1. Introduction\nIn the healthcare field, physical rehabilitation exercises\nplay a crucial role in post-surgery recovery and managing\nvarious musculoskeletal issues (Thiry et al., 2022). These\nexercises are usually monitored by a clinician in a hospital\nor clinic setting, but patients receive limited supervised\nsessions due to high expenses or staff availability problems.\nToachieveoptimalrecovery,itisvitalthatpatientscontinue\nto perform the prescribed exercises correctly in their own\nhomes. Recently advanced motion sensors were developed\nfor capturing human motion (AlarcÃ³n-Aldana et al., 2020).\nInparticular,low-costvisiondepthcamerasthatarerecently\ncommercializedsuchastheKinectvisiondevice(Scottetal.,\n2022). This latter are marker-less motion capture system\nusingthetime-of-flight(ToF)principleandisabletocapture\npreciselyRGB,depthimages,andjointskeletalcoordinates.\nIn the context of patient rehabilitation, many works\nconsider these joints for human motion analysis (Devanne\nand Sao Mai, 2017; Deb et al., 2022). They are encouraged\nby their effectiveness shown in various action recognition\napplications (Yue et al., 2022). Motivated by this, our work\naims to build an automatic model for physical rehabilitation\nexercise assessment using joint skeletal data of exercises as\nan input. The proposed model will help patients to continue\ntheir exercises independently while getting a feedback help-\ning them improve the accuracy of their movements.\nIn the literature, previous studies of Hamaguchi et al.\n(2020);Pogorelcetal.(2012)consideredexerciseevaluation\nâˆ—Corresponding author\nymourchid@cesi.fr (Y. MOURCHID);rsalmi@cesi.fr (.R.\nSLAMA)\nORCID(s):\n1These authors contributed equally to this work.\nasabinaryclassification(correctorincorrect)withoutbeing\nable to give a feedback for each exercise performance (see\nFigure 1). Other approaches Lee et al. (2019) predict a\ncontinuous score by addressing a regression problem and\nrelyingonhandcraftedfeatures(projectedtrajectory,relative\ntrajectory, etc.), which mostly require time-consuming pre-\nprocessing and expert knowledge. Advancements in com-\nputer vision, driven by graphs, statistical techniques, and\ndeeplearning,havegreatlyenhancedvisualdataprocessing,\nwhich is particularly beneficial for improving rehabilitation\nexercises assessment Mourchid et al. (2016); Benallal et al.\n(2022);Mourchidetal.(2021);MourchidandSlama(2023).\nRecently, Liao et al. (2020a) leveraged the power of\ndeep learning techniques for feature extraction by using\ndeep spatio-temporal neural network model for outputting\nmovement quality scores. Before feeding the network by\ninputvideos,theyconvertthelattertoafixedlength.Never-\ntheless, these methods do not respect the topological struc-\nture of the skeleton and do not consider interaction among\nneighborhoodjoints.Recently,graphshavebeenextensively\nemployed for various computer vision applications (Lafhel\net al., 2021; mou, 2019) and more particularly for skeleton-\nbased action identification since the human skeleton and\na graph are comparable. Spatio-Temporal Graph Convolu-\ntional Networks (STGCN), a subcategory of Graph Convo-\nlutionalNetworks(GCN),wasappliedtoskeleton-basedac-\ntivity recognition in (Yan et al., 2018a) by creating a spatio-\ntemporal graph through the connection of detected joints of\nahumanbodyinconsecutivetimesteps.Besides,oneofthe\nmost significant deep learning developments over the past\nfew years has been the Transformer architecture (Vaswani\net al., 2017). Beyond NLP, a variety of tasks, including\nFirst Author et al.:Preprint submitted to Elsevier Page 1 of 15\narXiv:2401.06150v2  [eess.IV]  25 Feb 2025\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nimage classification, image super-resolution, speech recog-\nnition, and particularly human motion analysis (Plizzari\netal.,2021;Zhangetal.,2023),haveshownthatmulti-head\nself-attention is effective. It is increasingly used to improve\nmodel accuracy by combining attention mechanisms with\nother deep learning blocks.\nInspiredbytherecentdevelopmentandachievementsof\n(GCNs)(Ahmadetal.,2021)andtheobservedeffectiveness\noftransformers,wepropose,inthiswork,anextendedarchi-\ntecture based on GCNs coupled with the power of attention\nmechanism.Theobjectiveistoevaluatepatientactionsusing\nsequential skeleton data. First, we use dense connections\nbetween STGC-GRU blocks which allow a more direct and\nefficient flow of information. Dense connections have been\nshown to improve results for our assessment task making\nit easier for the network to learn complex features and\npatterns (Huang et al., 2017). They allow to alleviate the\nvanishing-gradient problem, strengthen feature propagation\nand encourage feature reuse at different scales, which leads\nto better performance.\nSecond,insteadofusingConvolutionalLongShortTerm\nMemory(ConvLSTM)layersasinDebetal.(2022),wepro-\npose to employ Convolutional Gated recurrent units (Con-\nvGRU).ThemainadvantageofConvGRUoverConvLSTM\nis their simpler structure, which makes them more compu-\ntationallyefficient.ConvGRUonlyhastwogates(anupdate\ngate and a reset gate) compared to ConvLSTM, which has\nthree (an input gate, an output gate, and a forget gate). This\nsimplerstructureallowsConvGRUtohavefewerparameters\nandrequireslesscomputationduringtrainingandinference.\nMoreover, ConvGRU also tends to converge faster than\nConvLSTM in our task. The reason is that the update gate\nintheConvGRUallowsthemodeltolearnhowmuchofthe\nprevioushiddenstateshouldbepassedforwardtothecurrent\nhiddenstate.Thisreducestheriskofvanishinggradientsand\nmakes it easier to propagate gradients through time.\nThird, we employ the power of transformers instead of\ntheGlobalpoolinglayerorLSTMasusedinexistingworks.\nThe reasoning behind using a transformer is its ability to\nprocess input sequences of varying lengths while attending\nto specific parts with varying levels of detail, enabling it\ntocapturecomplextemporalrelationshipsbetweenskeleton\njoints and make precise predictions.\nThe main contributions of this paper are summarized as\nfollows:\nâ€¢ A dense STGC-GRU model is proposed for end-to-\nend assessment of rehabilitation exercises;\nâ€¢ A ConvGRU layer is employed as an alternative to\nConvLSTMtolowercomputationduringtrainingand\ninference;\nâ€¢ A transformer encoder architecture is proposed to\novercome basic LSTM limitations;\nâ€¢ The proposed system offers clear guidance on which\nbody parts or movements to focus on and enhance\nassessment quality, based on a self-attention mecha-\nnism;\nâ€¢ Theefficiencyoftheproposedmodelisshownthrough\nextensive experimentation on two physical rehabilita-\ntion datasets, KIMORE and UI-PRMD.\nThe rest of the paper is structured as follows: In Section\n2,relatedworkonrehabilitationexerciseassessmentandthe\nmotivation for our proposal are discussed. Section 3 pro-\nvides a thorough explanation of the proposed system. Sec-\ntion 4 outlines the experimental setup and results. Finally,\nconcludingremarksandfutureperspectivesarepresentedin\nSection 5.\n2. Related Works\nCurrent methods of evaluating movement involve com-\nparing a patientâ€™s exercise performance to that of healthy\nindividuals. A recent study by Liao et al. (2020b) reviewed\nvarious computer-based techniques for evaluating patient\nrehabilitation exercises using motion tracking technology.\nApproaches evaluating patient rehabilitation exercises can\nbedividedintothreecategories:(1)discretemovementscore\napproaches (2) rule-based approaches (3) template-based\napproaches.Inthefollowing,wegiveabriefreviewofthese\ndifferent approaches.\n2.1. Discrete movement score approaches\nUsing machine learning, these studies employ a dis-\ncrete movement score in order to distinguish between two\nclasses: correct and incorrect movement sequence classes.\nGenerally, they output a binary class value for the given test\npatient sequences. Using such motion classification system\nto evaluate post-stroke rehabilitation, k-nearest neighbors\n(Zhangetal.,2011),Adaboostclassifier(Tayloretal.,2010),\nrandom forest (Patel et al., 2010) or multi-layer perceptron\nneural networks (Jung et al., 2008) were used.\nFor home-based physiotherapy exercises assessment,\nUpper-Limbmotorfunctionimpairment,Bayesianclassifier\nand support vector machines (SVM) were used (Ar and\nAkgul, 2014; Otten et al., 2015). Deep and Convolutional\nNeural Networks (Um et al., 2018) were also used to\ndiagnose Parkinsonâ€™s disease using data from a wrist-worn\nwearable sensor. Despite their high accuracy, these meth-\nods cannot monitor changing movement quality or track\nimprovement in patient performance during rehabilitation.\nTherefore, this category is not adequate for a robust and\naccurate rehabilitation system.\n2.2. Rule-based approaches\nRule-based approachesfor assessingrehabilitationexer-\ncises involve using predefined rules and criteria to evaluate\nandquantifyapatientâ€™sperformance.Theseapproachesrely\nonclinicalguidelinesandbestpractices,tailoringtherulesto\nthepatientâ€™sspecificneedsandcondition.Examplesinclude\nthe Functional Independence Measure (FIM) for neurolog-\nical impairments and the Knee Injury and Osteoarthritis\nOutcome Score (KOOS) for knee disorders (Nolan et al.,\nFirst Author et al.:Preprint submitted to Elsevier Page 2 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\n.\n.\n.\nGlobal exercise evaluation\n(ğ´,ğ‘º?)\nData Acquisition System\n.\n.\n.\nDB with N sequences rated with quality score ğ‘†i for each \naction ğ´ğ‘–\nPatient Action: A\nPatient quality Score to predict: S\nMotion Assessment Decision\nModel training\nData preprocessing \nCorrect or incorrectScore = ğ‘º\nTest the patient Action\nMotion feedback \nand indication for \nmovement \ncorrection \nOR\nOR\nClassificationRegression\nRepeat or not \nthe action\nğ´1,ğ‘†1\nğ´2,ğ‘†2\nğ´ğ‘,ğ‘†ğ‘\nFigure 1:Physical rehabilitation exercises process overview.\n2022). These approaches employ standard rules to assess\npatient movement, such as monitoring knee and ankle an-\ngles (Bo et al., 2011) or defining kinematic rules (Zhao\net al., 2014). By providing a standardized and objective\nassessment, rule-based approaches ensure consistency and\naccuracy across evaluators and settings. However, they can\nbe time-consuming and may not consider individual patient\ngoals. Additionally, they may not be applicable to atypi-\ncal cases or new conditions, limiting their generalizability.\nThese approaches are particularly useful for simple exer-\ncises, but their effectiveness diminishes with exercise com-\nplexity.Moreover,theydonotadaptwelltonovelexercises.\n2.3. Template-based approaches\nTo avoid the need for rule-making and better reflect the\npatientâ€™s motor ability, these methods rely on a direct com-\nparison between the patient and a template motion and em-\nploy distance function-based techniques. Distance metrics\nlike Euclidean, Mahalanobis, and Hausdorff distances are\nusedtomeasuresimilarity(Benetazzoetal.,2014;Houman-\nfar et al., 2014; Huang et al., 2014). Generally in such\nsolutions,DynamicTimeWarping(DTW)ensuressequence\nlength invariance (Saraee et al., 2017). Another group of\nresearchers suggested probabilistic methods. They involve\nGaussian mixture models to evaluate movement quality and\ndetect deviations from ideal motions. The log-likelihood\nof individual sequences generated from a trained Gaussian\nmixture model is used for movement evaluation (Elkholy\net al., 2019). Gaussian mixture models are also utilized\nto represent ideal movements in various contexts, such as\ndetecting body part motion deviations (GÃ¶rer et al., 2017)\nand addressing low back pain rehabilitation (Devanne and\nSao Mai, 2017). Discrete Hidden Markov Models (HMM)\nand Hidden Semi-Markov Models (HSMM) were proposed\nforsegmentingandanalyzinghumanmotiondatainphysical\ntherapy exercises (Wei et al., 2019; Capecci et al., 2018;\nOsgouei et al., 2020). Besides, Williams et al. (2019) used\nautoencoder neural networks to reduce high-dimensional\nmotion trajectories to a low-dimensional space, followed by\nGaussian mixture models for modeling movement density.\nMoreover, performance metrics based on the log-likelihood\nof Gaussian mixture models are introduced to encode low-\ndimensionaldatarepresentationsachievedwithdeepautoen-\ncoder networks (Liao et al., 2020a).\n2.4. Deep Learning based approaches\nFeature extraction from motion sequences in the con-\ntext of exercise assessment has been approached through\nmanualselection,traditionalfeatureengineeringalgorithms\nsuch as manifold learning or PCA (Devanne and Sao Mai,\n2017; Tao et al., 2016; Akremi et al., 2022). While neu-\nral network architectures have been extensively explored\nfor modeling human motion in other contexts like action\nrecognition, only a few studies have focused on sequence\nmotion assessment for patient rehabilitation exercises (Sun\net al., 2022). Some researchers have proposed neural net-\nwork architectures for encoding data features. For instance,\nVakanski et al. (2016) introduced an architecture consisting\nof an autoencoder subnet for dimensionality reduction and\na mixture density network (MDN) to obtain probabilistic\nmodels of human motion. Zhu et al. (2019) proposed a\ncombinedDynamicConvolutionalneuralnetwork(D-CNN)\nand State transition probability CNN (S-CNN) to address\ndata alignment and capture discriminative exercise features.\nLiao et al. (2020a) presented a temporal-pyramid model\nthat combines CNN and Recurrent Neural Networks ar-\nchitectures (RNN), incorporating spatial information from\ndifferent body parts. Various methods using GCNs have\nalsobeenproposed,leveragingthegraphstructureofhuman\nbodyskeletondataforactionqualityassessmentandexercise\nevaluation (Song et al., 2020; Zhang et al., 2020; Du et al.,\n2015; Li et al., 2018). In the field of rehabilitation exercise\nassessment, GCNs have shown promise. Deb et al. (2022)\ndevelopedaspatio-temporalGCNforpredictingcontinuous\nscores in exercise assessment. Chowdhury et al. (2021)\nused a GCN for spatial feature extraction and an LSTM\nnetworkfortemporalfeatureextractionfromskeletaldatato\npredictexercisequality.Inthedomainofactionrecognition,\nFirst Author et al.:Preprint submitted to Elsevier Page 3 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\n \nPreprocessed Skeleton \nSequence \nInput RGB Video  Frames \nPose \nestimation \nData \nPreprocessing \nSkeleton Sequence \nSTGC-GRU STGC-GRU STGC-GRU \nFully Connected \nLinear Regression \nAssessment  Score \nDense \nConnections \nTranformer \nEncoder \nRegressor \nOutput \nInput \nMulti-Head \nAttention \nFeed Forward \nNeural Network \nAdd & \nNormalize \nAdd & \nNormalize \nPositional Encoding \nAdd \nFigure 2:Flowchart of the proposed approach.\nGCNs have been successfully employed, with architectures\ninspired by Spatio Temporal-GCN (Yan et al., 2018b; Li\net al., 2019; Shi et al., 2018). Inspired by the success of\nGCN-based methods and the potential for precise feedback\nonthehumanbodyskeleton,theproposedapproachaimsto\nrespect the topological structure of skeleton data, and adopt\na robust graph-based approach for exercise assessment. By\nincorporatinggraph-basedtechniquesandtransformers,this\napproach has the potential to provide accurate and visual\nfeedback for evaluating rehabilitation exercises.\n3. Proposed Approach\n3.1. Overview of the approach\nThe flowchart of our proposed approach for exercise\nassessment is depicted in Figure 2.\nThe pipeline of the proposed framework is composed\nof four consecutive blocks. It starts with the acquired input\ndatacontainingRGBDssequenceswithpatientsperforming\ntheneededevaluationexercises.Theconsideredmodalityin\nour work is the skeleton data which can be given directly\nby the acquisition device (such as kinect) or deduced from\nRGB video using skeleton estimation approaches (Shot-\nton et al., 2012; Pavllo et al., 2019; Bazarevsky et al.,\n2020). After the data preprocessing step on skeleton se-\nquences,weproposetoextractspatio-temporalfeatureswith\na Dense Graph Convolutional connection layer. Inspired by\nSpatio-Temporal Graph Convolutional Networks structure\n(STGCN) by Huang et al. (2020), we construct a network\ncomposed of multiple STGC-GRU blocks with direct con-\nnectionsfromtheoutputofeachSTGC-GRUblocktoallthe\noutput of the other blocks. Consequently, the output of the\nğ‘–ğ‘¡â„ block receives the feature-maps of all preceding blocks.\nIf we considerğ¹0,ğ¹1,...,ğ¹ ğ‘€âˆ’1 as the concatenation of the\nfeature-mapsproducedinSTGC-GRUblocks 0,1,...,ğ‘€ âˆ’1,\nwe have :\nğºğ‘ =ğ‘†ğ‘‡ğºğ¶ âˆ’ğºğ‘…ğ‘ˆ([ğ¹0,ğ¹1,...,ğ¹ ğ‘€âˆ’1]) (1)\nTheglobalrepresentationextractedfromSTGC-GRUblocks,\nusing Equation 1, is then used to feed the proposed trans-\nformerencoder.Finally,afullyconnectedlayerisemployed\nto predict the needed continuous assessment score.\nThenetworkarchitectureisdesignedtotakeintoaccount\nthesequentialdependenciesamongthespatio-temporal fea-\nturesacrossframes/bodymovementsbyincorporatingdense\nconnectionsbetweenSTGC-GRUblocks.Thisenhancesthe\npropagation of spatio-temporal features and promotes fea-\nture reuse across various STGC-GRU blocks. Additionally,\nuserscanperformthesameworkoutatvaryingspeeds(slow\nor fast), causing differing spatiotemporal characteristics. To\naddressthischallenge,atransformerisemployedtoaccount\nforthevaryingspatiotemporalfeaturesofidenticalexercises.\nBesides, transformer models have been used for se-\nquential data because they can learn those long-distance\nrelationshipsbutdonotincorporatethetopologicalstructure\nof the human skeleton. Therefore, we propose to combine\nthese two kinds of networks for physical rehabilitation. Our\nmodel takes advantage of an STGC-GRU architecture with\naself-attentionmechanismfromthetransformerencoder,to\ncalculate the score of each pair of joints and updated the\nattributesofthecurrentvertex.Thecommonlyusedsymbols\nFirst Author et al.:Preprint submitted to Elsevier Page 4 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nNotation Definition\nâŠ• The concatenation operation\nâŠ— The convolution operation\nâŠ™ The element-wise product\nğœ™ The normalizing factor\nÃ— The Hadamard product\nğ¹ The feature map\nğ‘‰ An RGBD video\nğ‘‹ A frame in the video sequence\nğ‘‡ The number of frames\nğ‘€ The self attention map\nğº The graph structure\nğ‘ğº The set of nodes ofğº\nğ¸ğº The set of edges ofğº\nğ´ The adjacency graph matrix forğº\nÌƒğ´ The nomalized adjacency graph matrix\nğ· The degree matrix\nğ¼ The identity matrix\nğ‘ƒ The processed video representation\nğ¾ğ‘ A kernel function\nğ‘Š The learnable model parameters\nğ‘¡ğ‘ğ‘›â„ The hyperbolic tangent activation function\nğœ The sigmoid functions\nğ‘§ğ‘¡ The update gate in GRU bloc\nğ‘Ÿğ‘¡ The rest gate in GRU bloc\nğ‘œğ‘¡ The hidden state candidate in GRU bloc\nâ„ğ‘¡ The hidden state output in GRU bloc\nğ‘ The output tensor of STGC-GRU\nğ‘„ The query component in attention mechanism\nğ¾ The keys component in attention mechanism\nğ‘‰ The values component in attention mechanism\nğ¿ The loss function\nğ‘¦ The true values\nÌƒ ğ‘¦ The predicted values\nTable 1\nSummary of commonly used notations.\nand notations, in equations and figures of our paper, are\nsummarized in the Table 1.\n3.2. Problem Formulation\nAnarbitraryexerciseofrehabilitationisdenotedby ğ‘‰ğ‘– =\n{ğ‘‹ğ‘¡=1...ğ‘‡}, where ğ‘‰ğ‘– refers respectively to theith RGBD\nvideo,ğ‘‹ğ‘¡isthe ğ‘¡ğ‘–ğ‘¡â„frameand ğ‘‡ isthenumberofframes.For\neach video, we associate a ground-truth performance score\nğ‘¦ğ‘– âˆˆ[ğ‘šğ‘–ğ‘›ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’,ğ‘šğ‘ğ‘¥ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’]thatrepresentstheexercisequality.\nWechooseskeleton-baseddataencodingsinceitismore\nrobust than RGB image-based modality to changes in body\nsizes, motion rates, camera perspectives, and interference\nbackgrounds. Given a sequence of skeletons, we consider\nğ‘ the number of joints representing each skeleton, where\neach joint has C-dimensional coordinates estimated by a\npose estimation approach or encoded with the sensor that\nhelped to capture the data. Dimension becomes for each\nvideo:ğ‘‰ğ‘– âˆˆğ•ğ‘‡Ã—ğ‘Ã—ğ¶ and for each frame:ğ‘‹ğ‘¡ âˆˆâ„ğ‘Ã—ğ¶.\nFor a given exercise, each skeleton motion plays an\nessentialroledependingontheperformedexercise.Ourgoal\nistopredictthescore Ì‚ ğ‘¦ğ‘— togivethepatientanideaaboutthe\nquality of his performance. We also capture the role of all\njoints and give a feedback that assists the patient to improve\nthe fluency of his exercise. Therefore, we consider a self-\nattention mapğ‘€ğ‘— âˆˆ â„ğ‘‡Ã—ğ‘Ã—ğ‘. The latter helps the patient\nto improve his performance by highlighting articulations,\ndenoted by joints, where improvement is needed.\n3.3. Dense Spatio-Temporal Feature Extraction\nThe skeleton can be viewed as a directed acyclic graph\nwith a natural structure, using biomechanical dependencies\nbetween joints and body parts. Each joint is depicted as a\nnode in the graph and connected to other joints via edges.\nEach of these joints has different features, such as the 3-\ndimensional coordinates and/or Euler angles. These values\nare given for each image (frame) belonging to a video se-\nquence.Recentstudiesdemonstratethatrepresentinghuman\nskeletondataasagraphisanaturalchoicetoextractspatio-\ntemporal features which characterize the best topological\nstructure of the body joints connection. Particularly, Graph\nConvolutional Networks (GCNs) have been used success-\nfully in the field of human skeleton motion analysis as\nrelationalnetworks(Fengetal.,2022).Inspiredbytherecent\nSpatio-TemporalGraphConvolutionalNetworksSTGCNby\nHuangetal.(2020),weproposeanextensionofSTGCNfor\nevaluating the effectiveness of physical therapy exercises.\nOurextendedarchitectureutilizesgraphconvolutionswitha\ndynamicadjacencymatrix,buildinguponSTGCNâ€™soriginal\nuse for recognizing actions based on skeleton data. In this\npaper, we propose a Dense STGC-GRU block as illustrated\nin Figure 3.\nEachframeoftheinputskeletonsequenceisrepresented\nby its graph structure ğº = (ğ‘ğº,ğ¸ğº,ğ´), where ğ‘ğº =\n{ğ‘›ğ‘–}ğ‘–=1..25 is the set of nodes,ğ¸ğº is the set of edges and\nğ´ is the adjacency matrix of the graph. We formulate our\nadjacency matrix Ak as follows:\nğ´k =ğ·k\nâˆ’1âˆ•2.( Ìƒğ´k +ğ¼).ğ·k\nâˆ’1âˆ•2 (2)\nÌƒğ´k istheadjacencymatrixofourgraphrepresentingthe\nconnections between the skeleton nodes. An identity matrix\nğ¼ is added to represent the self-connections of the nodes.\n( Ìƒğ´k +ğ¼) is multiplied byğ·k\nâˆ’1âˆ•2 (the inverse of the degree\nmatrixofthegraph)onbothsidestonormalizeit.Wedefine\nthe k-adjacency matrixÌƒğ´(ğ‘˜) as:\n(Ìƒğ´(ğ‘˜))ğ‘–,ğ‘— =\nâ§\nâª\nâ¨\nâªâ©\n1 if ğ‘‘(ğ‘›ğ‘–,ğ‘›ğ‘—)= ğ‘˜,\n1 if ğ‘–=ğ‘—,\n0 ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.\n(3)\nWhere ğ‘‘(ğ‘›ğ‘–,ğ‘›ğ‘—) gives the shortest distance in number of\nhops between nodeğ‘›ğ‘– and nodeğ‘›ğ‘—. Ìƒğ´(ğ‘˜), in Equation 3, is\nthus a generalization of Ìƒğ´ to further neighborhoods, with\nÌƒğ´(1) = Ìƒğ´and Ìƒğ´(0) =ğ¼.Figure 4showstheadopteddistance\npartitioning strategy for different k-hops.\nFirst,weprocesstheinputsequence ğ‘‰ asğ‘ƒ =ğ‘‰ âŠ•(ğ¾ğ‘âŠ—\nğ‘‰), whereâŠ•,âŠ—, andğ¾ğ‘ denote respectively the concatena-\ntion operation, the temporal convolution operation, and the\nused kernel.\nFirst Author et al.:Preprint submitted to Elsevier Page 5 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nSkeleton\nSequence TC ConvGRUW Expand Dim\nTC TC TC\nAdjacency Matrix\n.\nÎ¦\nSpatio \nTemporal \nFeature\nConcatenation\n. Element-wise product\nÎ¦ Normalizing factor\nTC Temporal Convolution\nMultiplicationMult\nMult\nFigure 3:STGC-GRU block details.\nâ€¦1-hop2-hopk-hop43212117181920131415169101112245678232225\nFigure 4:First skeleton represents the 25 joints of a skeleton from KIMORE dataset. The other skeletons represent the nodes (in\ngreen) involved in the computation of the1ğ‘ ğ‘¡, 2ğ‘›ğ‘‘ and ğ‘˜ğ‘–ğ‘’ğ‘š hop order regarding a certain joint (in dark red).\nSecond, a graph convolution is conducted as follows:\nğº(ğ‘ƒ)=\nÎ“aâˆ‘\nğ‘˜\n(ğ‘ƒ.ğ´k).ğ‘Šk, (4)\nWhere Î“a is the kernel size on the spatial dimension, which\nalsomatchesthenumberofadjacencymatrices.Thenumber\nofadjacencymatricesdependsontheusedpartitioningstrat-\negy, which we will explain below.ğ‘Šk is a trainable weight\nmatrix and is shared between all graphs to capture common\nproperties. In Equation 4, each kernelğ‘ƒ.ğ´k computes the\nweighted average of a nodeâ€™s features with its neighboring\nnodes, which is multiplied by(ğ‘ƒ.ğ´k).ğ‘Šk, a weight matrix.\nThe features generated by all kernels are then summed to\nform a single feature vector per node. This operation helps\nextracting spatial features from the non-linear structure of\ntheskeletalsequence.Itisinspiredbythegraphconvolution\nfrom ST-GCN (Yan et al., 2018b), which uses a similar\nGraphConvolutionformulationtotheoneproposedbyKipf\nand Welling (2016).\nThird, an improvement of STGCN block is proposed\nin this paper and consists of adding a Convolutional Gate\nRecurrent Unit (ConvGRU) layer. This layer helps to calcu-\nlate a self-attention map which makes the adjacency matrix\ndynamic and is recomputed each time through the added\nlayer. GRU includes gates in one unit as LSTM does within\na simpler structure. Thus, GRU is computationally cheaper.\nThis is very important in our application study and the\nbenefitofusing ConvGRUinsteadofConvLSTMis studied\ninexperimentalresultscomparingthecomputationtimeand\naccuracy on used metrics. ConvGRu combines CNN and\nGRU and thus has the advantage of maintaining the spatial\nstructure of the skeletal input sequence and it is also more\nconductive for spatial-temporal features in time series.\nWith an action sequence{ğ‘¥ğ‘¡=1...ğ‘‡} with ğ‘‡ frames, it\nperforms the forward propagation as follows:\nğ‘§ğ‘¡ =ğœ(ğ‘¤ğ‘§ğ‘¥ âŠ—ğ‘¥ğ‘¡ +ğ‘¤ğ‘§â„ âŠ—â„ğ‘¡âˆ’1 +ğ‘ğ‘§) (5)\nğ‘Ÿğ‘¡ =ğœ(ğ‘¤ğ‘Ÿğ‘¥ âŠ—ğ‘¥ğ‘¡ +ğ‘¤ğ‘Ÿâ„ âŠ—â„ğ‘¡âˆ’1 +ğ‘ğ‘Ÿ) (6)\nğ‘œğ‘¡ =ğ‘¡ğ‘ğ‘›â„(ğ‘¤ğ‘œğ‘¥ âŠ—ğ‘¥ğ‘¡ +ğ‘¤ğ‘œâ„ âŠ—(ğ‘Ÿğ‘¡ Ã—ğ‘“ğ‘¡âˆ’1)+ ğ‘ğ‘œ) (7)\nâ„ğ‘¡ =ğ‘§ğ‘¡ Ã—ğ‘¥ğ‘¡ +(1âˆ’ ğ‘§ğ‘¡)Ã— ğ‘œğ‘¡ (8)\nWe denote byâŠ— and Ã—respectively convolution operation\nandHadamardproduct. ğ‘¡ğ‘ğ‘›â„andğœaretangentandSigmoid\nfunctions. ğ‘¤ğ‘¥, ğ‘¤â„ and ğ‘ are corresponding weights and\nbiases. Equation 8 represents the hidden state for each time\nindexğ‘¡=1..ğ‘‡ (â„0 is set to0) and it is considered as output\nandbackgroundinformationgoinginthenetwork.Different\ngatesinGRUarerepresentedby ğ‘§ğ‘¡,ğ‘Ÿğ‘¡,and ğ‘œğ‘¡ representedin\nEquations 5, 6 and 7.\nAfterward, we proceed to inject an adjacency matrix, as\nderivedfromEquation 2,intotheConvGRUoutputthrough\nelementwisemultiplication,followedbytheapplicationofa\nnormalization factor.\nFirst Author et al.:Preprint submitted to Elsevier Page 6 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nFinally, in order to extract different levels of temporal\nfeatures,threeTemporalConvolutionallayersareperformed\nconsecutively and their respective output is concatenated to\ngive the needed spatio-temporal features. We use several\nconsecutive STGC-GRU blocks with the same structure to\nenable a better capture of different representation levels of\nnodes in the network. Besides, we propose to consider a\ndense network that uses shortcut connections. These dense\nconnectionoperationshelpto:(i)learnthemappingbetween\ntheinformationofpreviousandnextfeaturelevels(ii)enable\nthe reuse of contextual information at different scales so\nthatthenetworkcancaptureabundantspatial-temporalcon-\ntextual information (iii) capture richer dependencies among\njointsandretainmorestructuralinformationofhumanpose,\nleading to satisfactory performance.\n3.4. Position Encoding\nThe tensor obtained by STGC-GRU blocks does not\ncontain the order of joint tokens, and the identity of joints\ncannot be distinguished, making the self-attention unable to\ncapture the sequential characteristic of movements, which\nwill reduce the performance of movement scoring. The\nexperimental results confirm our hypothesis in Table 3.\nTo solve this issue, Vaswani et al. (2017) suggest using\nposition encoding to label each joint and applying sine and\ncosine functions with varying frequencies as the encoding\nfunctions :\nğ‘ƒğ¸(ğ‘¥,2ğ‘–)= ğ‘ ğ‘–ğ‘›(ğ‘¥âˆ•10000(2ğ‘–âˆ•ğ·)) (9)\nğ‘ƒğ¸(ğ‘¥,2ğ‘–+1)= ğ‘ğ‘œğ‘ (ğ‘¥âˆ•10000(2ğ‘–âˆ•ğ·)) (10)\nWhere ğ‘–ranges from0to ğ‘‘\n2 and ğ‘‘ represents the input\ndimension. This sinusoidal position encoding enables the\ntransformer to model the position of a joint token and the\ndistance between each pair of joint tokens.\n3.5. Transformer encoder block for\nvariable-length and smoothness\nRehabilitation exercises data show notable variability\nwithinvariable-lengthdata,incontrasttorelatedissueswith\nsequential data. One significant factor is that the exercise\nparticipants are generally diverse individuals, ranging from\nexperienced therapists to patients with various illnesses and\ndisabilities.Additionally,thenumberofrepetitionsrequired\nfor rehabilitation exercises may vary depending on the ther-\napistâ€™s prescription. As a result, different users assign the\nsame workout with the same number of repetitions with\nvaried lengths of time to complete. A recent work of Yan\net al. (2018a) extracted spatio-temporal features by employ-\ning a global pooling layer that comes before the FC lay-\ners,ignoringthespatio-temporalcharacteristicsâ€™underlying\nsequential relationships between frames/body movements.\nAs a result, users who execute the identical activity quickly\nor slowly provide various spatio-temporal information. To\novercome these limitations, Deb et al. (2022) use LSTM\narchitecture to capture sequential dependencies that exist in\nspatio-temporal features, to extract discriminative features\nthat have accumulated over time. In this work, we employ a\ntransformer architecture instead of LSTM.\nTransformershandlevariable-lengthinputsequencesbe-\ncause they use self-attention mechanisms, which allow the\nmodel to weigh the importance of different parts of the\ninputsequencewithoutrequiringafixed-lengthcontext.This\nmeans that the model can adapt to the specific length and\nstructure of the input, rather than being limited by a fixed-\nlength context window or requiring the input to be padded\nto a fixed length. This allows for more flexible and efficient\nprocessingofinputsequencesofvaryinglengths.Moreover,\ntransformers can learn the relationships between each ele-\nment of a sequence, thanks to their self-attention ability. It\naddresses the issue that LSTM and RNN networks struggle\ntoaccuratelysimulatelong-termsequencesbyhandlingvery\nlongsequences.Furthermore,ourmodelusesamulti-headed\nself-attention mechanism instead of traditional LSTM or\nRNN networks. Unlike token-by-token processing in these\nnetworks, the self-attention mechanism allows parallel pro-\ncessing of sentences. This enables efficient calculation of\njoint correlations in multiple consecutive frames, making\nself-attention a suitable choice for modeling skeleton data.\nThe transformer blocks in our proposed network follow the\nscaled dot-product attention as proposed by Vaswani et al.\n(2017). Our transformer encoder takes the output tensor of\nSTGC-GRU blocksğ‘ âˆˆ â„ğµ,ğ¹,ğ‘‡ , where ğµ indicates the\nbatchsize, ğ¹ isthenumberofthesequences,and ğ‘‡ denotes\nthesequencesize.Tostart,weusepositionalencodingtoas-\nsignavectortoeachjointtoken.Then,weusethreelearnable\nmatricesğ‘Šğ‘,ğ‘Šğ‘˜,and ğ‘Šğ‘£totransformthejointdata, ğ‘,into\nseparate spaces. These matrices typically have dimensions\nâ„ğµ,ğ¹,ğ‘‘ğ‘–ğ‘š,where ğ‘‘ğ‘–ğ‘šisahyperparameter.Followingthis,we\ncomputetheattentionforthequery,key,andvaluematrices,\nğ‘„, ğ¾, andğ‘‰, respectively, using the following equations in\neach head:\nğ‘„,ğ¾,ğ‘‰ =ğ‘ğ‘Šğ‘,ğ‘ğ‘Šğ‘˜,ğ‘ğ‘Šğ‘£ (11)\nğ´ğ‘¡,ğ‘— =ğ‘„ğ‘¡ğ¾ğ‘‡\nğ‘— (12)\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰ )= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥( ğ´âˆš\nğ‘‘ğ‘–ğ‘š\n)ğ‘‰ (13)\nIn Equation 12,ğ‘„ğ‘¡ represents the query vector for the\nğ‘¡ğ‘¡â„ joint token andğ‘— represents the joint token that theğ‘¡ğ‘¡â„\njoint token attends to.ğ¾ğ‘— is the key vector representation\nfor the ğ‘—ğ‘¡â„ joint token. The softmax operation is applied\nalong the last dimension. The ability of self-attention is\nimprovedthroughmulti-headself-attention,whichusesmul-\ntiple groups ofğ‘Šğ‘,ğ‘Šğ‘˜,ğ‘Šğ‘£ instead of just one group. Its\nformulas are as follows:\nFirst Author et al.:Preprint submitted to Elsevier Page 7 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nğ‘„(â„),ğ¾(â„),ğ‘‰ (â„) =ğ‘ğ‘Š(â„)\nğ‘ ,ğ‘ğ‘Š (â„)\nğ‘˜ ,ğ‘ğ‘Š (â„)\nğ‘£ (14)\nâ„ğ‘’ğ‘ğ‘‘(â„) =ğ´ğ‘¡ğ‘¡ğ‘›(ğ‘„(â„),ğ¾(â„),ğ‘‰ (â„)) (15)\nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ»)=[ â„ğ‘’ğ‘ğ‘‘(1);...;â„ğ‘’ğ‘ğ‘‘(ğ‘›)]ğ‘Šğ‘‚ (16)\nWhere ğ‘›, in Equation 16, refers to the number of heads\nand â„, in Equations 14 and 15, represents the head index.\nThe concatenation in the last dimension is represented as\n[â„ğ‘’ğ‘ğ‘‘(1);...;â„ğ‘’ğ‘ğ‘‘(ğ‘›)]. The learnable parameterğ‘Šğ‘‚ has size\nâ„ğ‘‘Ã—ğ‘‘, whereğ‘‘ =ğ‘‘ğ‘–ğ‘šÃ—ğ‘›.\nThemulti-headedself-attentionmechanismmapsaquery\nto a series of key and value pairs, allowing it to model the\nrelationship between input tokens after positional encoding.\nIt considers the influence of nodeğ‘›ğ‘– on other nodes and\nthe impact of all other nodes on nodeğ‘›ğ‘– when computing\nself-attention. The multi-head attention output is further\nprocessed through a basic feed-forward neural network. For\nfaster training, layer normalization is employed instead of\nthe commonly used batch normalization in standard feed-\nforward neural networks. Moreover, residual connections\nare used in our transformer encoder block. For instance,\nsmoothness is a crucial factor in determining how well an\nexercise is scored. To determine how smooth a movement\nis,wemustlookatthetemporalcharacteristics(velocity,ac-\nceleration)ofthetotalconjugativetimeframes.Apoolingor\nanLSTMlayermayfailtocapturethecompletesmoothness\ninformation,whichiscrucialindeterminingthecorrectness\nscore. By using a transformer encoder block, smoothness\ninformation from past to future movements can be captured\ninparallel,yieldingimprovedresultsasthedependenciesare\nbetter understood.\n3.6. Proposed Losses\nTo solve the regression problem, our proposed network\nis trained with various regression losses, including Mean\nSquareError(MSE),HuberLoss,andLog-CoshLoss.Dur-\ninginference,atestsequenceofskeletondataisprocessedto\ncompute a continuous assessment score using these losses.\nThe following provides a description of each one.\nMean Square Error Loss:\nMeansquareerror(MSE)isthemostwidelyusedregression\nlossfunctioninmachinelearning.Itmeasuresthesumofthe\nsquared difference between predicted and target values. A\nlower MSE indicates a better-performing regression model.\nMSEiscalculatedastheaveragesumofsquareddifferences\nbetween the actual value and the value estimated by the\nregression model.\nğ‘€ğ‘†ğ¸ =\nâˆ‘ğ‘›\nğ‘–=0(ğ‘¦ğ‘– âˆ’ Ì‚ ğ‘¦ğ‘–)\nğ‘› (17)\nWhereğ‘¦ğ‘–and Ì‚ ğ‘¦ğ‘–denotetheactualvalueandthepredicted\nvalue respectively andğ‘›represents the number of samples.\nHuber Loss:\nHuber loss is a regression loss function that combines the\nbenefitsof ğ‘™2andğ‘™1penalties.Itislessaffectedbyoutliersin\ndatacomparedtootherlosses,andtransitionsfromabsolute\nerror to quadratic error based on a hyperparameter,ğ›¿. The\nsmallertheerror,themoreitbecomesquadratic.Huberloss\napproaches MSE, presented in Equation 17, asğ›¿ â†’ 0and\nmean absolute error (MAE) asğ›¿ â†’ âˆ. The formula for\nHuber loss is as follows:\nğ¿(ğ‘¦ğ‘–âˆ’Ì‚ ğ‘¦ğ‘–)=\n{1\n2(ğ‘¦ğ‘– âˆ’ Ì‚ ğ‘¦ğ‘–)2 ;â (ğ‘¦ğ‘– âˆ’ Ì‚ ğ‘¦ğ‘–)ââ‰¤ ğ›¿\nğ›¿â ğ‘¦ğ‘– âˆ’ Ì‚ ğ‘¦ğ‘– â âˆ’ğ›¿\n2 ;ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ (18)\nThe selection ofğ›¿is crucial as it determines what is consid-\neredanoutlier.Huberlossisadvantageousinsuchscenarios\nas it smoothly bends around the minimum, reducing the\ngradient. Additionally, it is more resilient to outliers than\nMSE.\nLog-Cosh Loss:\nLog-cosh is another function used in regression tasks that\nis smoother than MSE. It is the logarithm of the hyperbolic\ncosine of the prediction error which can be defined by the\nformula below :\nğ¿(ğ‘¦ğ‘– âˆ’ğ‘¦ğ‘\nğ‘–)=\nğ‘›âˆ‘\nğ‘–=0\nğ‘™ğ‘œğ‘”(ğ‘ğ‘œğ‘ â„(ğ‘¦ğ‘– âˆ’ğ‘¦ğ‘\nğ‘–)) (19)\nTheLog-CoshLossoperatessimilarlytoMSEbutisless\nimpactedbyoccasionallargeerrors.LikeEquation18,ithas\nallbenefitsofHuberlossbutalsohastheadvantageofbeing\ntwice differentiable everywhere, unlike Huber loss.\n3.7. Network architecture\nIn this section, we present the detailed layers descrip-\ntionofourproposedmodel(D-STGCNT).OurSTGC-GRU\nblock constitutes a temporal convolution with 64 kernels of\nsize (9,1), followed by the ReLU activation layer. Then, the\noutput of the temporal convolution is concatenated with the\nfirst input sequence to produce a tensorğ‘. Subsequently,\na graph Conv-GRU with 64 and 25 kernels of size (1,1)\nand a GRU layer is utilized onğ‘ and the ğ‘˜ğ‘¡â„ hop ad-\njacency matrix to capture spatial characteristics from the\ntopological layout of human skeletons. This is followed\nby three temporal convolutional layers with equal padding\nand kernels of size (9,1), (15,1), and (20,1), respectively,\nwith 16 filters for each layer. The output of our extended\nSTGC-GRUblockistheconcatenationofthethreetemporal\nconvolution outputs. The purpose of concatenating is to\nidentify movement patterns at varying levels of abstraction.\nOur work uses dense connections in multiple STGC-GRU\nblocks to extract more intricate features, facilitating spa-\ntiotemporalfeaturepropagationacrosslayers,andpromoting\nFirst Author et al.:Preprint submitted to Elsevier Page 8 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nfeaturereuseacrossvariousSTGC-GRUblocks.Theoutput\nof the latter is processed by a positional encoding module\nto incorporate the order of sequences. Then, the two terms\nareaddedtogetherasfollows: ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘†ğ‘‡ğºğ¶âˆ’ğºğ‘…ğ‘ˆ+\nğ‘ƒğ‘œğ‘ ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘–ğ‘›ğ‘” (ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ğ‘†ğ‘‡ğºğ¶âˆ’ğºğ‘…ğ‘ˆ).Finally,numeroustrans-\nformer encoder blocks are employed as presented in Figure\n2, where each block constitutes a layer normalization with\nğœ–=1ğ‘’âˆ’insteadofusingbatchnormalization,tostabilizethe\nnetwork which results in substantially reducing the training\ntime necessary. Then, we use a multi-head attention layer\nwhere â„ğ‘’ğ‘ğ‘‘âˆ’ğ‘ ğ‘–ğ‘§ğ‘’ = 128and ğ‘›ğ‘¢ğ‘šâˆ’â„ğ‘’ğ‘ğ‘‘ğ‘  = 6. We add\nnext a dropout layer (dropout = 0.1) in order to avoid over-\nfitting.TwoFeedforwardlayersofConv1Dwith80,and128\nkernels are employed instead of the Dense layers which are\nused on traditional transformers. The concept behind using\nConv1Distoenhancetherepresentationofattentionoutputs\nthrough projection. Additionally, residual connections are\nemployed between layers to facilitate network training by\nfacilitating gradient flow.\nBy stacking transformer encodersğ‘ times, we increase\ninformation encoding. Every layer has the chance to learn\ndistinct attention representations, increasing the power of\nthe attention network. The result of the stacked transformer\nencoders is then processed by a linear activation fully con-\nnected layer.\n4. Experimentation and Results\nIn this section, we conducted extensive comparative\nexperiments to evaluate the performance of our model (D-\nSTGCNT).First,wedescriberehabilitationexercisedatasets\nand metrics used for evaluation. Then implementation de-\ntails are introduced. In the following, we conducted ex-\ntensive ablation studies to verify the contribution of the\nindividualcomponentsofourD-STGCNT.Finally,wequan-\ntitativelycompareourproposedapproachwithseveralstate-\nof-the-art methods.\n4.1. Evaluation Process\nIn the following, we present datasets used to assess the\neffectiveness of our proposed model. Then, we introduce\nthe evaluation metrics employed to measure model perfor-\nmance. The implementation details are also presented in\nthe paper, including the programming language used, any\nrelevantlibraries,andthepre-processingstepsappliedtothe\ndata.\n4.1.1. Dataset\nExtensive experiments are conducted on rehabilitation\nexercises from two datasets (see Table 2).\nâ€¢ KIMORE (Capecci et al., 2019): This dataset in-\ncludes RGBD videos and score annotations for five\nexercises, divided into two groups: control (expert\nand non-expert) and pain/postural disorder (Parkin-\nson, back-pain, stroke). The control group has 44\nhealthy subjects, with 12 being physiotherapists and\nexpertsinrehabilitationand32beingnon-expert.The\npain/postural disorder group consists of 34 subjects\nwith chronic motor disabilities.\nâ€¢ UI-PRMD(Vakanskietal.,2018):Thisdatasetispub-\nlicly available and contains movements of common\nexercisesperformedbypatientsinphysicalrehabpro-\ngrams. Ten healthy individuals performed 10 repeti-\ntionsofvariousphysicaltherapymovements,captured\nusing a Vicon optical tracker and a Microsoft Kinect\nsensor.Thedataincludesfull-bodyjointpositionsand\nangles, and its purpose is to serve as a foundation\nformathematicalmodelingoftherapymovementsand\nestablishingperformancemetricstoevaluatepatientsâ€™\nconsistency in executing rehabilitation exercises.\n4.1.2. Evaluation metrics\nToevaluateandcompareourapproachwithstate-of-the-\nart methods, we use the metrics used by Liao et al. (2020a)\nandDebetal.(2022).If ğ‘¦isourtarget, Ì‚ ğ‘¦isourprediction, ğ‘›\nthe number of observations andğ‘’= Ì‚ ğ‘¦âˆ’ğ‘¦is the error , then\nused metrics can be defined as follows:\nâ€¢ Mean Absolute Deviation (MAD): average of the ab-\nsolutedeviationbetweengroundtruthvaluesandpre-\ndicted values:ğ‘€ğ´ğ· =ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›(ğ‘’âˆ’ğ‘šğ‘’ğ‘‘ğ‘–ğ‘ğ‘›(ğ‘’)).\nâ€¢ Mean Squared Error (MSE) and Root Mean Squared\nError (RMSE) which are defined as follows:\nğ‘€ğ‘†ğ¸ = 1\nğ‘›\nâˆ‘ğ‘›\nğ‘–=1ğ‘’2\nğ‘– and ğ‘…ğ‘€ğ‘†ğ¸ =\nâˆš\nğ‘€ğ‘†ğ¸.\nThese are the most common regression metrics. They\nare very sensitive to outliers and penalize large errors\nmore heavily than small ones.\nâ€¢ Mean Absolute Percentage Error (MAPE) measures\nthe percentage error of the forecast in relation to the\nactual values:ğ‘€ğ´ğ‘ƒğ¸ = 100%\nğ‘›\nâˆ‘ğ‘›\nğ‘–=1\n||||\nğ‘¦ğ‘–Î»Ì‚ ğ‘¦ğ‘–\nğ‘¦ğ‘–\n||||\nThe proposed approach is designed for patients seeking\na quick and accurate indication of the quality of the reha-\nbilitation exercises they are performing, therefore we have\nalso calculated the response time of our proposed approach\nbothinthetrainandtestphaseswhilebeingwatchfulforany\npossibility of optimization. Furthermore, we also evaluated\nvisually and by interpretation the quality of the feedback\ngiven by our model. In all our experiments, we follow the\nevaluation protocol defined by Deb et al. (2022) for the\ndivision of the datasets into train and test parts.\n4.1.3. Implementation details\nTheproposedD-STGCNTmodelhasbeenimplemented\nwith python 3.6 using Tensorflow 2.x framework. We used\na PC with IntelÂ® XeonÂ® Silver 4215R CPU, with 32GB of\nRAM and a GeForce GTX 3080 Ti 16GB RAM graphics\ncard. Our D-STGCNT is trained using Adam optimizer for\n1500 epochs with batch sizes 10, and 3 for KIMORE and\nUI-PRMD datasets respectively. The learning rate is set to\n1ğ‘’ âˆ’ 4. We select the best model to assess the modelâ€™s\nFirst Author et al.:Preprint submitted to Elsevier Page 9 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nFeatures Sensor Depth Imaging System # of Subjects # of Exercises Range of Quality Scores\nUI-PRMD Vakanski et al. (2018) Vicon and Kinect v2 skeleton 10 10 0-1\nKIMORE Capecci et al. (2019) Kinect v2 RGB-D and skeleton 78 5 0-50\nTable 2\nUI-PRMD and KIMORE datasets description.\nOur model MAD RMSE MSE MAPE\nWithout positional encoding 0.721 1.902 3.619 1.904\nWith positional encoding 0.399 0.735 0.540 1.217\nTable 3\nPerformance of our proposed model with and without posi-\ntional encoding on Ex5 of KIMORE dataset.\nOur model MAD RMSE MSE MAPE\nFirst hope only 0.647 1.338 1.792 1.868\nSecond hope only 0.789 1.548 2.397 2.267\nConcatenate(First,Second) hope0.399 0.735 0.540 1.217\nTable 4\nPerformance of our proposed model with different k-hops on\nEx5 of KIMORE dataset.\neffectivenessonthetestsetinaccordancewiththevalidation\nset. To objectively assess the performance of our model in\ncomparisontorecentworks,weadditionallygivethe10-run\nresult, as like Liao et al. (2020a), Kipf and Welling (2016),\nandDebetal.(2022).Tentimesourmodelwasperformedon\ntrainingandtesting.Toensuretheaccuracyofourresults,we\nsavetheperformancemeasures(MAD,RMSE,andMAPE)\nfrom each run before averaging them.\n4.2. Ablation study\n4.2.1. Effect of positional encoding\nWe investigate the effect of the positional encoding as\nshown in Tab. 3. Results show that the performance of\nourmodelD-STGCNTwithoutpositionalencodingislower\nwhit higher values of MAD, RMSE, MSE, and MAPE, and\nby Utilizing positional encoding, we enhance the perfor-\nmance significantly. This can be explained by the fact that\ndifferent spatiotemporal joints play unique roles in action,\nand effectively utilizing this sequential information leads to\nsignificant improvement.\n4.2.2. Effect of theğ‘˜ğ‘¡â„ hop adjacency matrix\nIn order to validate the effectiveness of the number of\nhops in our D-STGCNT model, we respectively set the\nnumber of hops to be 1,2.\nTable 4presentstheresultsofD-STGCNTwithdifferent\nhops. We observe that combining (concatenating) the mul-\ntiplehopadjacencymatrixfromdifferentperspectivesleads\nto better performances using the proposed metrics. Indeed,\nthe concatenation represents long-range structural relations\nwhich leads to improving the movement assessment perfor-\nmance significantly.\nOur model MAD RMSE MSE MAPE\nWith MSE loss 0.623 1.291 1.668 1.784\nWith Log-Cosh loss 0.786 1.803 3.251 2.410\nWith Huber loss (ğ›¿=1) 0.848 2.036 4.147 2.63\nWith Huber loss (ğ›¿=0.1) 0.399 0.735 0.540 1.217\nWith Huber loss (ğ›¿=0.05) 0.522 1.023 1.046 1.635\nTable 5\nPerformance of our proposed model with different regression\nlosses on Ex5 of KIMORE dataset.\n4.2.3. Effect of regression losses\nThechoiceoftheregressionlossfunctioninthetraining\nof our model can have a significant impact on its perfor-\nmance. Regression loss functions are used to measure the\ndifference between the predicted output and the true output\nand are used to update the modelâ€™s parameters during train-\ning. We evaluate our model using three different regression\nlosses MSE, Log-Cosh, and Huber loss. Results in Table\n5 show that Huber loss gives better results for predicting\nassessment scores. One of the main advantages of using\nHuber loss for training our model is that it can help to\nimprove the robustness of the model. Another advantage of\nHuber loss is that it can provide a balance between MSE\nandMAE.MSEissensitivetooutliersanditpenalizeslarge\nerrors more heavily than smaller errors, while MAE is less\nsensitive to outliers and it gives equal weight to all errors.\nHuber loss is a combination of both, and it can provide the\nbest solution, depending on the value of the Huber delta\nparameter.\nFinally, Huber loss can also improve the stability of\nthe optimization process during training. It is less sensitive\nto outliers, which can make the optimization process more\nstable and less likely to be affected by extreme values in the\ndata. Varying theğ›¿ parameter in Huber loss can improve\nregression results by affecting the balance between mean\nsquared error (MSE) and mean absolute error (MAE). A\nsmaller delta value would make the loss closer to MSE,\nwhichismoresensitivetolargeerrorsandmoreappropriate\nfor datasets with normally distributed errors. A larger delta\nvalue would make the loss closer to MAE, which is more\nrobust to outliers and more appropriate for datasets with\nheavy-tailed errors. By tuning the delta value, we observe\nin Table 5 that withğ›¿=0.1, we obtain the best results.\n4.2.4. Effect of transformer vs LSTM\nThe effect of utilizing a transformer architecture com-\npared to LSTM in the context of online processing for pa-\ntientâ€™s physical rehabilitation assessment is worth consider-\ning.Transformersoffertheadvantageofparallelprocessing,\nenabling more efficient online analysis of patientsâ€™ physical\nFirst Author et al.:Preprint submitted to Elsevier Page 10 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nOur model MAD RMSE MSE MAPE Execution Time\nOur approach with LSTM 0.601 1.122 1.519 1.574 10.98 seconds\nOur approach with Transformer0.399 0.735 0.540 1.217 2.11seconds\nTable 6\nEffect of using transformer and LSTM components on our\nmodel performance using test data from Ex5 of KIMORE\ndataset.\nrehabilitation.Transformersoperateonself-attentionmech-\nanisms, allowing each element in the sequence to attend to\nall other elements simultaneously. These advantages enable\nfaster and more efficiency in terms of performance and\ncomputations(5timesfaster)thanLSTMasshowninTable\n6. This can be particularly beneficial in real-time patient\nassessment scenarios.\n4.3. Comparison with state of the art\nTo evaluate the effectiveness of the proposed model, a\nquantitativeassessmentwasconductedbycomparingitwith\nseveral existing state-of-the-art approaches using identical\ndatasets. The comparison was performed using the optimal\nparameters selected based on the results obtained from the\nablation study. Our model incorporated positional encoding\nwithinthetransformerarchitecture,employedconcatenation\nof the first and second hop adjacency matrix, and utilized\nHuber loss (withğ›¿ = 0.1) as the regression loss function.\nFurthermore,thecomputationaltimeoftheproposedmodel\nwasmeasuredandcomparedagainstthatofalternativemeth-\nods, shedding light on the efficiency of the proposed ap-\nproach.\n4.3.1. Quantitative comparison\nInTable 7and 8,wepresentourresultsonMAD,RMSE\nandMAPEperformancesandthoseofthestateofthearton\ntheKIMOREandUI-PRMDdatasetsrespectively.First,we\nreport results for each of the five exercises included in the\nKIMORE dataset. Then, we report results using computed\non the ten exercises of UI-PRMDE dataset.\nWe would like to point out that, in the Table 8, compar-\nisonsareconductedonKinectV2jointpositiondatainstead\nof Vicon angles data as reported by Liao et al. (2020a).\nOur approach, which combines Dense STGCN, ConvGRU,\nand Transformer architectures, demonstrates superior per-\nformanceintermsofMAD,RMSE,andMAPE.Whencom-\npared to the approach proposed by Deb et al. (2022), which\nusesGCNsfollowedbyanLSTM,ourapproachoutperforms\nintermsofaccuracyandprecision.Thisimprovementcanbe\nattributed to the combined use of Dense STGCN and Con-\nvGRU components, which effectively capture spatial and\ntemporal features, and the Transformer architecture, which\nefficientlyhandleslong-rangedependenciesandenablespar-\nallel processing. In comparison to Song et al. (2020), who\nuse a multi-stream Graph Convolutional Network, our ap-\nproach exhibits superior performance. The integration of\nDense STGCN, ConvGRU, and transformer architectures in\nour approach allows for a more comprehensive analysis of\nspatialandtemporalfeatures,resultinginenhancedaccuracy\nand lower error metrics.\n4.3.2. Computational time\nOurproposedmodelistested,intermsofcomputational\ntime and accuracy, on the Ex5 of the KIMORE dataset in\ncomparisonwithDebetal.(2022).ResultsinTable 9show\nthat it can provide real-time performance with a good score\nin terms of MAD, RMSE and MAPE. The computational\ncostwasmeasuredusingaGeForceGTX3080Tiwith16GB\nof RAM. Indeed, our extended architecture uses ConvGRU\nlayers which are generally considered to be faster.\nConvGRU offers faster processing due to fewer parame-\nters, making it more memory-efficient and quicker to train.\nConvGRUâ€™s efficiency in terms of speed is crucial for real-\ntimeanalysisinpatientrehabilitation.Itsmemoryefficiency\nis advantageous in scenarios with limited resources, ensur-\ningsmootherexecutionandreducingtheriskofbottlenecks.\nWhile ConvLSTM may excel in precise long-term model-\ning, ConvGRU, used in our experiments, showed sufficient\nperformances outperforming state of the arts methods for\ntracking and analyzing patient movements during rehabil-\nitation. Additionally, ConvGRU integrates seamlessly with\nthe Jetson Nano platform and a camera, enabling real-time\nanalysis and immediate feedback for patient rehabilitation.\nMoreover, our model employs transformer encoders\nwhich are faster than LSTMs because they can process the\nentire 3D skeletons input sequence in parallel, and use an\nattention mechanism to selectively focus on relevant parts\nof the input.\n4.4. Feedback and impact of joints in\nrehabilitation exercises\nSince our graph-based approach respects the non-linear\nstructure of the skeleton data, we can investigate the natural\ntopological structure of the body. Besides, spatial informa-\ntionisextractedviaattention-guidedgraphconvolution.The\nbodyâ€™s joint roles can be quantified in order to evaluate\nrehabilitation exercises.\nOur ConGRU output does not provide any structural\ninformation. Using the adjacency matrix and element-wise\nmultiplication, we inject the graph structure. Thus we ob-\ntain the self-attention map,ğ‘€1, which shows the attention\nweights for each body joint with its neighbors in each row.\nThe joint role,ğœ’ğ‘¡ is computed by the column-wise summa-\ntion overğ‘€1.\nDifferentself-attentionmapswhichemphasizethefunc-\ntion of the bodyâ€™s joints are shown in Figure 6. The higher\nemphasisonthesejointsisevidentfromthehigherattention\nvalue taken from the ğœ’ğ‘¡. Even though some joints may\nreceive equal values, they can influence both high and low\ntrial ratings. However, some joints play a bigger role in\ndetermining low ratings. The patient needs to concentrate\non joints like these.\nComputingjointâ€™srolefornotexpertusers,asillustrated\nin Figure 6, shows that they differ from the expertâ€™s pattern\nwhen the patient receives a low assessment score (<20). In\nthisvisualization,wedisplaybothliftingarms(Ex1),where\nusers are mainly moving their arms, and Pelvis rotation\nFirst Author et al.:Preprint submitted to Elsevier Page 11 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nMetric Ex Ours Deb et al. (2022) Song et al. (2020) Zhang et al. (2020) Liao et al. (2020a) Yan et al. (2018a) Li et al. (2018) Du et al. (2015)\nEx1 0.641 0.799 0.977 1.75 7 1.141 0.889 1.378 1.271\nEx2 0.753 0.774 1.282 3.139 1.528 2.096 1.877 2.199\nMAD Ex3 0.210 0.369 1.105 1.737 0.845 0.604 1.452 1.123\nEx4 0.206 0.347 0.715 1.202 0.468 0.842 0.675 0.880\nEx5 0.399 0.621 1.536 1.853 0.847 1.218 1.662 1.864\nEx1 2.020 2.024 2.165 2.916 2.534 2.017 2.344 2.440\nEx2 1.468 2.120 3.345 4.140 3.738 3.262 2.823 4.297\nRMSE Ex3 0.487 0.556 1.929 2.615 1.561 0.799 2.004 1.925\nEx4 0.527 0.644 2.018 1.836 0.792 1.331 1.078 1.676\nEx5 0.735 1.181 3.198 2.916 1.914 1.951 2.575 3.158\nEx1 1.623 1.926 2.605 5.054 2.589 2.339 3.491 3.228\nEx2 0.974 1.272 3.296 10.436 3.976 6.136 5.298 6.001\nMAPE Ex3 0.613 0.728 2.968 5.774 2.023 1.727 4.188 3.421\nEx4 0.541 0.824 2.152 3.901 2.333 2.325 1.976 2.584\nEx5 1.217 1.591 4.959 6.531 2.312 3.802 5.752 5.620\nTable 7\nResults of our method in comparison with other state-of-the-art approaches on the KIMORE dataset.\nMetrics MAD RMSE MAPE\nEx Ours Deb et al. (2022) Ours Deb et al. (2022) Ours Deb et al. (2022)\nEx1 0.011 0.012 0.019 0.020 1.289 1.337\nEx2 0.009 0.011 0.014 0.016 1.105 1.244\nEx3 0.013 0.015 0.020 0.024 1.592 1.758\nEx4 0.009 0.010 0.011 0.015 0.984 1.090\nEx5 0.009 0.010 0.013 0.014 1.032 1.176\nEx6 0.013 0.017 0.020 0.025 1.476 1.994\nEx7 0.022 0.023 0.034 0.036 2.697 2.980\nEx8 0.020 0.024 0.032 0.034 2.362 2.815\nEx9 0.013 0.017 0.019 0.022 1.455 1.873\nEx10 0.014 0.025 0.023 0.033 1.619 2.900\nTable 8\nResults of our method in comparison with other state-of-the-art approaches on the UI-PRMD dataset.\nPhase Number of Videos Execution Time MAD RMSE MAPE\nDeb et al. (2022) Train 373 57 hours - - -\nOur model Train 373 25 min - - -\nDeb et al. (2022) Test 100 13.87 seconds 0.631 1.185 1.602\nOur model Test 100 2.11 seconds 0.404 0.739 1.220\nTable 9\nComputational time for Ex5 of KIMORE dataset for 1500\nepochs.\n(Ex4),whereusersaremakingsubtlerotationswiththemost\nstress on the spinal column.\nIn Figure 5, we display the impact of various joints on\nspecific KIMORE dataset actions. More specifically, using\nthe attention values given byğœ’, we visualize in this figure\nhowtheroleofjointsvariesaccordingtodifferentrehabilita-\ntionexercises.Wecannoticethatinthefirstexercise(lifting\narm), joints that play an important role are: the thumb,\nelbow,wrist,andspine(hotcolors).Thesameisobservedin\nexercise (pelvis rotation), major contributing joints are the\nwrist and spine.\n5. Conclusion\nThe paper introduces a proposed attention-based D-\nSTGCNT model for evaluating physical rehab exercises.\nThe model takes 3D skeleton movement data in graph form\nas input and outputs a score evaluating the quality of the\nexecuted exercise. An extended architecture of the popular\nEx1\nEx2\nEx3\nEx4\nEx5\n4\n3\n21\n2\n1\n17\n18\n19\n20\n13\n14\n15\n16\n9\n10\n11\n12 24\n5\n6\n7\n8\n23\n22\n25\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\nFigure 5: An illustration of the attention value calculated by our approach that shows the involvement of the joints depending\non the corresponding activities. On the left, we can see the calculated joints importance throw 5 exercises from the KIMORE\ndataset (lifting arms, arms extension, trunk rotation, pelvis rotation, squatting), and on the right, we illustrate the 25 joints of\nthe skeleton human body as represented in KIMORE dataset.\nFirst Author et al.:Preprint submitted to Elsevier Page 12 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nğœ’ğ‘’ğ‘¡ ğ‘€1\nğ‘¡\nNot expert with  score=48 No expert Score=14\nLifting arms\nğœ’ğ‘’ğ‘¡ ğ‘€1\nğ‘¡\nExpert Not expert with score=48 Not expert with score=14\nPelvis rotation\nğœ’ğ‘¡ ğ‘€1\nğ‘¡ ğ‘€1\nğ‘¡\nğ‘€1\nğ‘¡ ğ‘€1\nğ‘¡\nNot expert with score=14\nğœ’ğ‘¡\nğœ’ğ‘¡ ğœ’ğ‘¡\nExpert\nFigure 6: Feedback visualization for different user profiles: expert, not an expert with a good score, and not an expert with a low\nscore. ğœ’ğ‘¡ represents the joint role vector andğ‘€ğ‘¡\n1 the self-attention map (hot colors represent high values). Colored circles on the\nskeleton bodies allow the visualization of the attention maps and the role of body joints for different exercises. The larger circle\nrepresents the higher role of that joint.\nSTGCN was proposed with transformers. Our extended\nSTGCN architecture employs first, dense connections to\nlearn complex features and patterns. Furthermore, Con-\nvGRU layers utilize a self-attention mechanism on the\nadjacency matrix of body joints, acknowledging the fact\nthat each body joint holds a different level of significance\nin exercise evaluation. Analysis of attention values enables\nto determine the key body joints that greatly affect the\nfinal score, thereby giving users insight to enhance their\nperformance in future attempts. Additionally, by employing\ntransformers our model overcomes LSTMs limitations by\nefficiently processing sequential data with variable-length\ninputs.Thisisimportantin3Dskeletonexerciseassessment,\nas the number of joints and frames in a given action can\nvary. The proposed model outperforms quantitatively state-\nof-the-artresultsonbothKIMOREandUI-PRMDdatasets.\nQualitativeillustrationsandafeedbackregardingtheimpor-\ntance of joints are given and commented. In future works,\nwe intend to investigate continuous assessment and develop\na visual feedback through a graphical interface.\nReferences\n, 2019. Movienet: a movie multilayer network model using visual and\ntextual semantic cues. Applied Network Science 4, 1â€“37.\nAhmad,T.,Jin,L.,Zhang,X.,Lai,S.,Tang,G.,Lin,L.,2021. Graphconvo-\nlutional neural network for human action recognition: a comprehensive\nsurvey. IEEE Transactions on Artificial Intelligence 2, 128â€“145.\nAkremi, M., Slama, R., Tabia, H., 2022. Spd siamese neural network\nforskeleton-basedhandgesturerecognition,in:17thInternationalCon-\nference on Computer Vision Theory and Applications VISAPP 2022),\nSCITEPRESS-Science and Technology Publications. pp. 394â€“402.\nAlarcÃ³n-Aldana, A.C., Callejas-Cuervo, M., Bo, A.P.L., 2020. Upper limb\nphysical rehabilitation using serious videogames and motion capture\nsystems: A systematic review. Sensors 20, 5989.\nAr,I.,Akgul,Y.S.,2014. Acomputerizedrecognitionsystemforthehome-\nbasedphysiotherapyexercisesusinganrgbdcamera. IEEETransactions\non Neural Systems and Rehabilitation Engineering 22, 1160â€“1171.\nBazarevsky,V.,Grishchenko,I.,Raveendran,K.,Zhu,T.,Zhang,F.,Grund-\nmann, M., 2020. Blazepose: On-device real-time body pose tracking.\narXiv preprint arXiv:2006.10204 .\nBenallal, H., Mourchid, Y., Abouelaziz, I., Alfalou, A., Tairi, H., Riffi,\nJ., El Hassouni, M., 2022. A new approach for removing point cloud\noutliers using box plot, in: Pattern recognition and tracking XXXIII,\nSPIE. pp. 63â€“69.\nBenetazzo,F.,Iarlori,S.,Ferracuti,F.,Giantomassi,A.,Ortenzi,D.,Freddi,\nA., Monteriu, A., Capecci, M., Ceravolo, M.G., Innocenzi, S., et al.,\n2014. Low cost rgb-d vision based system for on-line performance\nevaluation of motor disabilities rehabilitation at home, in: Proceedings\nofthe5thForumItalianoonAmbientAssistedLivingForItAAL.IEEE.\nBo, A.P.L., Hayashibe, M., Poignet, P., 2011. Joint angle estimation\nin rehabilitation with inertial sensors and its integration with kinect,\nin: 2011 Annual International Conference of the IEEE Engineering in\nMedicine and Biology Society, IEEE. pp. 3479â€“3483.\nCapecci,M.,Ceravolo,M.G.,Ferracuti,F.,Iarlori,S.,Kyrki,V.,Monteriu,\nA., Romeo, L., Verdini, F., 2018. A hidden semi-markov model based\napproach for rehabilitation exercise assessment. Journal of biomedical\ninformatics 78, 1â€“11.\nCapecci,M.,Ceravolo,M.G.,Ferracuti,F.,Iarlori,S.,Monteriu,A.,Romeo,\nL., Verdini, F., 2019. The kimore dataset: Kinematic assessment\nof movement and clinical scores for remote monitoring of physical\nrehabilitation. IEEETransactionsonNeuralSystemsandRehabilitation\nEngineering 27, 1436â€“1448.\nChowdhury, S.H., Al Amin, M., Rahman, A.M., Amin, M.A., Ali, A.A.,\n2021. Assessment of rehabilitation exercises from depth sensor data,\nin: 2021 24th International Conference on Computer and Information\nTechnology (ICCIT), IEEE. pp. 1â€“7.\nDeb, S., Islam, M.F., Rahman, S., Rahman, S., 2022. Graph convolutional\nnetworks for assessment of physical rehabilitation exercises. IEEE\nTransactions on Neural Systems and Rehabilitation Engineering 30,\n410â€“419.\nDevanne, M., Sao Mai, N., 2017. Multi-level motion analysis for physical\nexercises assessment in kinaesthetic rehabilitation, in: 2017 IEEE-RAS\n17th International Conference on Humanoid Robotics (Humanoids),\nIEEE. pp. 529â€“534.\nDu, Y., Wang, W., Wang, L., 2015. Hierarchical recurrent neural network\nfor skeleton based action recognition, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 1110â€“1118.\nFirst Author et al.:Preprint submitted to Elsevier Page 13 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nElkholy, A., Hussein, M.E., Gomaa, W., Damen, D., Saba, E., 2019.\nEfficient and robust skeleton-based quality assessment and abnormality\ndetectioninhumanactionperformance. IEEEjournalofbiomedicaland\nhealth informatics 24, 280â€“291.\nFeng,L.,Zhao,Y.,Zhao,W.,Tang,J.,2022. Acomparativereviewofgraph\nconvolutional networks for human skeleton-based action recognition.\nArtificial Intelligence Review 55, 4275â€“4305.\nGÃ¶rer, B., Salah, A.A., AkÄ±n, H.L., 2017. An autonomous robotic exercise\ntutor for elderly people. Autonomous Robots 41, 657â€“678.\nHamaguchi,T.,Saito,T.,Suzuki,M.,Ishioka,T.,Tomisawa,Y.,Nakaya,N.,\nAbo, M., 2020. Support vector machine-based classifier for the assess-\nment of finger movement of stroke patients undergoing rehabilitation.\nJournal of Medical and Biological Engineering 40, 91â€“100.\nHoumanfar, R., Karg, M., KuliÄ‡, D., 2014. Movement analysis of rehabili-\ntation exercises: Distance metrics for measuring patient progress. IEEE\nSystems Journal 10, 1014â€“1025.\nHuang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely\nconnected convolutional networks, in: Proceedings of the IEEE confer-\nence on computer vision and pattern recognition, pp. 4700â€“4708.\nHuang, M.C., Liu, J.J., Xu, W., Alshurafa, N., Zhang, X., Sarrafzadeh, M.,\n2014. Usingpressuremapsequencesforrecognitionofonbedrehabili-\ntation exercises. IEEE journal of biomedical and health informatics 18,\n411â€“418.\nHuang, Z., Shen, X., Tian, X., Li, H., Huang, J., Hua, X.S., 2020. Spatio-\ntemporal inception graph convolutional networks for skeleton-based\naction recognition, in: Proceedings of the 28th ACM International\nConference on Multimedia, pp. 2122â€“2130.\nJung, J.Y., Glasgow, J.I., Scott, S.H., 2008. Feature selection and classifi-\ncation for assessment of chronic stroke impairment, in: 2008 8th IEEE\nInternationalConferenceonBioInformaticsandBioEngineering,IEEE.\npp. 1â€“5.\nKipf, T.N., Welling, M., 2016. Semi-supervised classification with graph\nconvolutional networks. arXiv preprint arXiv:1609.02907 .\nLafhel, M., Cherifi, H., Renoust, B., El Hassouni, M., Mourchid, Y., 2021.\nMoviescriptsimilarityusingmultilayernetworkportraitdivergence,in:\nComplex Networks & Their Applications IX: Volume 1, Proceedings\nof the Ninth International Conference on Complex Networks and Their\nApplications COMPLEX NETWORKS 2020, Springer. pp. 284â€“295.\nLee, M.H., Siewiorek, D.P., Smailagic, A., Bernardino, A., Badia, S.B.i.,\n2019. Learning to assess the quality of stroke rehabilitation exercises,\nin:Proceedingsofthe24thInternationalConferenceonIntelligentUser\nInterfaces, pp. 218â€“228.\nLi, C., Zhong, Q., Xie, D., Pu, S., 2018. Co-occurrence feature learning\nfromskeletondataforactionrecognitionanddetectionwithhierarchical\naggregation. arXiv preprint arXiv:1804.06055 .\nLi, M., Chen, S., Chen, X., Zhang, Y., Wang, Y., Tian, Q., 2019. Actional-\nstructuralgraphconvolutionalnetworksforskeleton-basedactionrecog-\nnition,in:ProceedingsoftheIEEE/CVFconferenceoncomputervision\nand pattern recognition, pp. 3595â€“3603.\nLiao, Y., Vakanski, A., Xian, M., 2020a. A deep learning framework for\nassessingphysicalrehabilitationexercises. IEEETransactionsonNeural\nSystems and Rehabilitation Engineering 28, 468â€“477.\nLiao, Y., Vakanski, A., Xian, M., Paul, D., Baker, R., 2020b. A review\nof computational approaches for evaluation of rehabilitation exercises.\nComputers in biology and medicine 119, 103687.\nMourchid, Y., Donias, M., Berthoumieu, Y., 2021. Automatic image col-\norizationbasedonmulti-discriminatorsgenerativeadversarialnetworks,\nin:202028thEuropeansignalprocessingconference(EUSIPCO),IEEE.\npp. 1532â€“1536.\nMourchid,Y.,ElHassouni,M.,Cherif,H.,2016.Imagesegmentationbased\non community detection approach. International Journal of Computer\nInformationSystemsandIndustrialManagementApplications8,10â€“10.\nMourchid, Y., Slama, R., 2023. Mr-stgn: Multi-residual spatio temporal\ngraph network using attention fusion for patient action assessment, in:\n2023IEEE25thInternationalWorkshoponMultimediaSignalProcess-\ning (MMSP), IEEE. pp. 1â€“6.\nNolan, J., Godecke, E., Spilsbury, K., Singer, B., 2022. Post-stroke\nlateropulsionandrehabilitationoutcomes:aretrospectiveanalysis. Dis-\nability and rehabilitation 44, 5162â€“5170.\nOsgouei, R.H., Soulsby, D., Bello, F., et al., 2020. Rehabilitation ex-\nergames: use of motion sensing and machine learning to quantify ex-\nercise performance in healthy volunteers. JMIR Rehabilitation and\nAssistive Technologies 7, e17289.\nOtten, P., Kim, J., Son, S.H., 2015. A framework to automate assessment\nof upper-limb motor function impairment: A feasibility study. Sensors\n15, 20097â€“20114.\nPatel, S., Hughes, R., Hester, T., Stein, J., Akay, M., Dy, J.G., Bonato, P.,\n2010. A novel approach to monitor rehabilitation outcomes in stroke\nsurvivorsusingwearabletechnology. ProceedingsoftheIEEE98,450â€“\n461.\nPavllo, D., Feichtenhofer, C., Grangier, D., Auli, M., 2019. 3d human pose\nestimation in video with temporal convolutions and semi-supervised\ntraining, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 7753â€“7762.\nPlizzari, C., Cannici, M., Matteucci, M., 2021. Skeleton-based action\nrecognition via spatial and temporal transformer networks. Computer\nVision and Image Understanding 208, 103219.\nPogorelc, B., BosniÄ‡, Z., Gams, M., 2012. Automatic recognition of\ngait-related health problems in the elderly using machine learning.\nMultimedia tools and applications 58, 333â€“354.\nSaraee, E., Singh, S., Hendron, K., Zheng, M., Joshi, A., Ellis, T., Betke,\nM.,2017. Exercisecheck:remotemonitoringandevaluationplatformfor\nhome based physical therapy, in: Proceedings of the 10th international\nconferenceonPErvasivetechnologiesrelatedtoassistiveenvironments,\npp. 87â€“90.\nScott,B.,Seyres,M.,Philp,F.,Chadwick,E.K.,Blana,D.,2022.Healthcare\napplications of single camera markerless motion capture: a scoping\nreview. PeerJ 10, e13517.\nShi, L., Zhang, Y., Cheng, J., Lu, H., 2018. Non-local graph convolu-\ntional networks for skeleton-based action recognition. arXiv preprint\narXiv:1805.07694 1, 3.\nShotton, J., Girshick, R., Fitzgibbon, A., Sharp, T., Cook, M., Finocchio,\nM.,Moore,R.,Kohli,P.,Criminisi,A.,Kipman,A.,etal.,2012.Efficient\nhuman pose estimation from single depth images. IEEE transactions on\npattern analysis and machine intelligence 35, 2821â€“2840.\nSong, Y.F., Zhang, Z., Shan, C., Wang, L., 2020. Richly activated graph\nconvolutional network for robust skeleton-based action recognition.\nIEEE Transactions on Circuits and Systems for Video Technology 31,\n1915â€“1925.\nSun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., Liu, J., 2022.\nHumanactionrecognitionfromvariousdatamodalities:Areview. IEEE\ntransactions on pattern analysis and machine intelligence .\nTao, L., Paiement, A., Damen, D., Mirmehdi, M., Hannuna, S., Camplani,\nM., Burghardt, T., Craddock, I., 2016. A comparative study of pose\nrepresentationanddynamicsmodellingforonlinemotionqualityassess-\nment. Computer vision and image understanding 148, 136â€“152.\nTaylor, P.E., Almeida, G.J., Kanade, T., Hodgins, J.K., 2010. Classifying\nhuman motion quality for knee osteoarthritis using accelerometers,\nin: 2010 Annual international conference of the IEEE engineering in\nmedicine and biology, IEEE. pp. 339â€“343.\nThiry, P., Houry, M., Philippe, L., Nocent, O., Buisseret, F., Dierick, F.,\nSlama,R.,Bertucci,W.,ThÃ©venon,A.,Simoneau-Buessinger,E.,2022.\nMachine learning identifies chronic low back pain patients from an\ninstrumented trunk bending and return test. sensors 22, 5027.\nUm, T.T., Pfister, F.M.J., Pichler, D.C., Endo, S., Lang, M., Hirche, S.,\nFietzek, U., KuliÄ‡, D., 2018. Parkinsonâ€™s disease assessment from a\nwrist-worn wearable sensor in free-living conditions: Deep ensemble\nlearning and visualization. arXiv preprint arXiv:1808.02870 .\nVakanski, A., Ferguson, J., Lee, S., 2016. Mathematical modeling and\nevaluation of human motions in physical therapy using mixture density\nneural networks. Journal of physiotherapy & physical rehabilitation 1.\nVakanski, A., Jun, H.p., Paul, D., Baker, R., 2018. A data set of human\nbody movements for physical rehabilitation exercises. Data 3, 2.\nFirst Author et al.:Preprint submitted to Elsevier Page 14 of 15\nD-STGCNT for Assessment of Patient Physical Rehabilitation\nVaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,\nKaiser, Å., Polosukhin, I., 2017. Attention is all you need. Advances in\nneural information processing systems 30.\nWei, W., McElroy, C., Dey, S., 2019. Towards on-demand virtual physical\ntherapist: Machine learning-based patient action understanding, assess-\nment and task recommendation. IEEE Transactions on Neural Systems\nand Rehabilitation Engineering 27, 1824â€“1835.\nWilliams, C., Vakanski, A., Lee, S., Paul, D., 2019. Assessment of\nphysicalrehabilitationmovementsthroughdimensionalityreductionand\nstatistical modeling. Medical engineering & physics 74, 13â€“22.\nYan, S., Xiong, Y., Lin, D., 2018a. Spatial temporal graph convolutional\nnetworksforskeleton-basedactionrecognition,in:Thirty-secondAAAI\nconference on artificial intelligence.\nYan, S., Xiong, Y., Lin, D., 2018b. Spatial temporal graph convolutional\nnetworksforskeleton-basedactionrecognition,in:Thirty-secondAAAI\nconference on artificial intelligence.\nYue, R., Tian, Z., Du, S., 2022. Action recognition based on rgb and\nskeleton data sets: A survey. Neurocomputing .\nZhang, H., Yang, K., Cao, G., Xia, C., 2023. Vit-llmr: Vision transformer-\nbased lower limb motion recognition from fusion signals of mmg and\nimu. Biomedical Signal Processing and Control 82, 104508.\nZhang,P.,Lan,C.,Zeng,W.,Xing,J.,Xue,J.,Zheng,N.,2020. Semantics-\nguidedneuralnetworksforefficientskeleton-basedhumanactionrecog-\nnition,in:proceedingsoftheIEEE/CVFconferenceoncomputervision\nand pattern recognition, pp. 1112â€“1121.\nZhang, Z., Fang, Q., Wang, L., Barrett, P., 2011. Template matching\nbased motion classification for unsupervised post-stroke rehabilitation,\nin: International Symposium on Bioelectronics and Bioinformations\n2011, IEEE. pp. 199â€“202.\nZhao, W., Lun, R., Espy, D.D., Reinthal, M.A., 2014. Realtime motion\nassessment for rehabilitation exercises: Integration of kinematic mod-\neling with fuzzy inference. Journal of Artificial Intelligence and Soft\nComputing Research 4, 267â€“285.\nZhu, Z.A., Lu, Y.C., You, C.H., Chiang, C.K., 2019. Deep learning for\nsensor-basedrehabilitationexerciserecognitionandevaluation. Sensors\n19, 887.\nFirst Author et al.:Preprint submitted to Elsevier Page 15 of 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7577965259552002
    },
    {
      "name": "Transformer",
      "score": 0.6903209686279297
    },
    {
      "name": "Encoder",
      "score": 0.6136146783828735
    },
    {
      "name": "Rehabilitation",
      "score": 0.6022701859474182
    },
    {
      "name": "Graph",
      "score": 0.5616769194602966
    },
    {
      "name": "Machine learning",
      "score": 0.48304539918899536
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4745398759841919
    },
    {
      "name": "Data mining",
      "score": 0.327148973941803
    },
    {
      "name": "Theoretical computer science",
      "score": 0.236688494682312
    },
    {
      "name": "Voltage",
      "score": 0.09912016987800598
    },
    {
      "name": "Engineering",
      "score": 0.0829484760761261
    },
    {
      "name": "Medicine",
      "score": 0.07430672645568848
    },
    {
      "name": "Physical therapy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}