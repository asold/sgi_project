{
  "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation",
  "url": "https://openalex.org/W3144827309",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5107028150",
      "name": "Bei Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101028906",
      "name": "Quan Du",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102719463",
      "name": "Tao Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076295893",
      "name": "Shuhan Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100654181",
      "name": "Xin Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100600701",
      "name": "Tong Xiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100370145",
      "name": "Jingbo Zhu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2974916071",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2948981900",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2600297185",
    "https://openalex.org/W2169062786",
    "https://openalex.org/W3102816807",
    "https://openalex.org/W2963302407",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W3046835050",
    "https://openalex.org/W2947156405",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W1597944220",
    "https://openalex.org/W2963755523",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W2561907692",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W3034465644",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W3098011980",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3035083896",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3034742481",
    "https://openalex.org/W2788421913",
    "https://openalex.org/W2963359731",
    "https://openalex.org/W2962737770",
    "https://openalex.org/W2963975324",
    "https://openalex.org/W1998583908",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3047743574",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W3122154272",
    "https://openalex.org/W3035747971",
    "https://openalex.org/W2964110616"
  ],
  "abstract": "It has been found that residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODEs). In this paper, we explore a deeper relationship between Transformer and numerical methods of ODEs. We show that a residual block of layers in Transformer can be described as a higher-order solution to ODEs. This leads us to design a new architecture (call it ODE Transformer) analogous to the Runge-Kutta method that is well motivated in ODEs. As a natural extension to Transformer, ODE Transformer is easy to implement and parameter efficient. Our experiments on three WMT tasks demonstrate the genericity of this model, and large improvements in performance over several strong baselines. It achieves 30.76 and 44.11 BLEU scores on the WMT'14 En-De and En-Fr test data. This sets a new state-of-the-art on the WMT'14 En-Fr task.",
  "full_text": "ODE Transformer: An Ordinary Differential Equation-Inspired Model\nfor Neural Machine Translation\nBei Li1, Quan Du1,2, Tao Zhou1, Shuhan Zhou1, Xin Zeng1, Tong Xiao1,2∗, and Jingbo Zhu1,2\n1NLP Lab, School of Computer Science and Engineering,\nNortheastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\n{libei neu,duquanneu,zhoutao neu,zsh neu,iszengxin}@outlook.com\n{xiaotong,zhujingbo}@mail.neu.edu.cn\nAbstract\nIt has been found that residual networks are\nan Euler discretization of solutions to Ordi-\nnary Differential Equations (ODEs). In this pa-\nper, we explore a deeper relationship between\nTransformer and numerical methods of ODEs.\nWe show that a residual block of layers in\nTransformer can be described as a higher-order\nsolution to ODEs. This leads us to design\na new architecture (call it ODE Transformer)\nanalogous to the Runge-Kutta method that is\nwell motivated in ODEs. As a natural exten-\nsion to Transformer, ODE Transformer is easy\nto implement and parameter efﬁcient. Our ex-\nperiments on three WMT tasks demonstrate\nthe genericity of this model, and large im-\nprovements in performance over several strong\nbaselines. It achieves 30.76 and 44.11 BLEU\nscores on the WMT’14 En-De and En-Fr test\ndata. This sets a new state-of-the-art on the\nWMT’14 En-Fr task.\n1 Introduction\nResidual networks have been used with a great\nsuccess as a standard method of easing information\nﬂow in multi-layer neural models (He et al., 2016;\nVaswani et al., 2017). Given an input yt, models of\nthis kind deﬁne the output of a layer at depth tto\nbe:\nyt+1 = yt + F(yt,θt) (1)\nwhere F(·,·) is the function of the layer andθtis its\nparameter. Interestingly, recent work in machine\nlearning (Weinan, 2017; Lu et al., 2018; Haber\net al., 2018; Chang et al., 2018; Ruthotto and Haber,\n2019) points out that Eq. (1) is an Euler discretiza-\ntion of the Ordinary Differential Equation (ODE),\nlike this:\n∗Corresponding author.\ndy(t)\ndt = F(y(t),θ(t)) (2)\nwhere y(t) and θ(t) are continuous with respect to\nt. In this way, we can call Eq. (1) an ODE block.\nThis ﬁnding offers a new way of explaining resid-\nual networks in the view of numerical algorithms.\nThen, one can think of a multi-layer network as\napplying the Euler method (i.e., Eq. (1)) to solve\nEq. (2) subject to the initial conditions y(0) = y0\nand θ(0) = θ0.\nThe solution of Eq. (2) has a sufﬁciently low\nerror bound (call it a stable solution) only if θ(t)\nchanges slow along t(Haber and Ruthotto, 2017;\nChen et al., 2018). But this assumption does not\nalways hold for state-of-the-art natural language\nprocessing (NLP) systems, in which models are\nnon-linear and over-parameterized. For example,\nlanguage modeling and machine translation sys-\ntems learn quite different parameters for different\nlayers, especially when the layers are close to the\nmodel input (Vaswani et al., 2017; Dai et al., 2019).\nAlso, truncation errors are nonnegligible for the\nEuler method because it is a ﬁrst-order approxima-\ntion to the true solution (He et al., 2019). These\nproblems make the situation worse, when more lay-\ners are stacked and errors are propagated through\nthe neural network. It might explain why recent\nMachine Translation (MT) systems cannot beneﬁt\nfrom extremely deep models (Wang et al., 2019;\nLiu et al., 2020; Wei et al., 2020; Li et al., 2020).\nIn this paper we continue the line of research on\nthe ODE-inspired method. The basic idea is to use\na high-order method for more accurate numerical\nsolutions to the ODE. This leads to a larger ODE\nblock that generates a sequence of intermediate ap-\nproximations to the solution. We ﬁnd that the larger\nODE block is sufﬁcient to take the role of several\nODE blocks with ﬁrst-order solutions. The beneﬁt\narXiv:2104.02308v1  [cs.CL]  6 Apr 2021\n6 ODE blocks with\n1st-order solutions\nθ1\nθ2\nθ3\nθ4\nθ5\nθ6\n2 ODE blocks with\n3rd-order solutions\nθ1\nθ2: F(·,θt)\n: ODE\nBlock\nFigure 1: Models with different ODE blocks.\nis obvious: the use of fewer ODE blocks lowers\nthe risk of introducing errors in block switching,\nand the high-order method reduces the approxima-\ntion error in each ODE block. See Figure 1 for a\ncomparison of different models.\nOur method is parameter-efﬁcient because θ(t)\nis re-used within the same ODE block. As another\n“bonus”, the model can be improved by learning co-\nefﬁcients of different intermediate approximations\nin a block. We evaluate our method in strong Trans-\nformer systems, covering both the wide (and big)\nmodel and the deep model. It achieves 30.76 and\n44.11 BLEU scores on the WMT14 En-De and En-\nFr test sets. This result sets a new state-of-the-art\non the WMT14 En-Fr task.\n2 Transformer and ODEs\nWe start with a description of Transformer, fol-\nlowed by its relationship with ODEs. We choose\nTransformer for our discussion and experiments\nbecause it is one of the state-of-the-art models in\nrecent MT evaluations.\n2.1 Transformer\nTransformer is an example of the encoder-decoder\nparadigm (Vaswani et al., 2017). The encoder is\na stack of identical layers. Each layer consists of\na self-attention block and a feedforward network\n(FFN) block. Both of them equip with a residual\nconnection and a layer normalization unit. Note\nthat the term “block” is used in many different\nways. In this paper, the term refers to any neural\nnetwork that is enhanced by the residual connection\n(occasionally call it a residual block).\nFollowing the Pre-norm architecture (Wang\net al., 2019), we deﬁne a block as\nyt+1 = yt + G(LN(yt),θt) (3)\nwhere LN(·) is the layer normalization function1,\nand G(·) is either the self-attention or feedforward\nnetwork. The decoder shares a similar architec-\nture, having an additional encoder-decoder atten-\ntion block sandwiched between the self-attention\nand FFN blocks.\n2.2 Ordinary Differential Equations\nAn ordinary differential equation is an equation\ninvolving a function y(t) of a variable t and its\nderivatives. A simple form of ODE is an equation\nthat deﬁnes the ﬁrst-order derivative of y(t), like\nthis\ndy(t)\ndt = f(y(t),t) (4)\nwhere f(y(t),t) deﬁnes a time-dependent vector\nﬁeld if we know its value at all points of yand all\ninstants of time t. Eq. (4) covers a broad range\nof problems, in that the change of a variable is\ndetermined by its current value and a time variable\nt.\nThis formulation also works with Pre-norm\nTransformer blocks. For notational simplicity,\nwe re-deﬁne G(LN(yt),θt) as a new function\nF(yt,θt):\nF(yt,θt) = G(LN(yt),θt)) (5)\nWe then relax yt and θt to continuous functions\ny(t) and θ(t), and rewrite Eq. (3) to be:\ny(t+ ∆t) = y(t) + ∆t·F(y(t),θ(t)) (6)\nwhere ∆tis the change of t, and is general called\nstep size. Obviously, we have ∆t = 1 in Trans-\nformer. But we can adjust step size ∆t using a\nlimit, and have\nlim\n∆t→0\ny(t+ ∆t) −y(t)\n∆t = F(y(t),θ(t)) (7)\nGiven the fact that lim∆t→0\ny(t+∆t)−y(t)\n∆t =\ndy(t)\ndt , Eq. (7) is an instance of Eq. (4). The only\n1We drop the parameter of LN(·) for simplicity.\ndifference lies in that we introduce θ(t) into the\nright-hand side of Eq. (4).\nThen, we say that a Pre-norm Transformer block\ndescribes an ODE. It has been found that Eq. (3)\nshares the same form as the Euler method of solv-\ning the ODE described in Eq. (7) (Haber and\nRuthotto, 2017). This establishes a relationship be-\ntween Transformer and ODEs, in that, given F(·,·)\nand learned parameters {θt}, the forward pass of\na multi-block Transformer is a process of running\nthe Euler method for several steps.\n3 The ODE Transformer\nIn numerical methods of ODEs, we want to en-\nsure the precise solutions to the ODEs in a mini-\nmum number of computation steps. But the Euler\nmethod is not “precise” because it is a ﬁrst-order\nmethod, and naturally with local truncation errors.\nThe global error might be larger if we run it for\na number of times 2. This is obviously the case\nfor Transformer, especially when the multi-layer\nneural network arises a higher risk of unstability in\nsolving the ODEs (Haber and Ruthotto, 2017).\n3.1 High-Order ODE Solvers\nHere we use the Runge-Kutta methods for a higher\norder solution to ODEs (Runge, 1895; Kutta, 1901;\nButcher, 1996; Ascher and Petzold, 1998). They\nare a classic family of iterative methods with dif-\nferent orders of precision3. More formally, the ex-\nplicit Runge-Kutta methods of an n-step solution\nis deﬁned to be:\nyt+1 = yt +\nn∑\ni=1\nγiFi (8)\nF1 = hf(yt,t) (9)\nFi = hf(yt +\ni−1∑\nj=1\nβijFj,t + αih) (10)\nwhere his the step size and could be simply 1 in\nmost cases. Fi is an intermediate approximation\nto the solution at step t+ αih. α, βand γare co-\nefﬁcients which can be determined by the Taylor\nseries of yt+1 (Butcher, 1963). Eq. (10) describes a\n2The global error is what we would ordinarily call the error:\nthe difference between y(t) and the true solution. The local\nerror is the error introduced in a single step: the difference\nbetween y(t) and the solution obtained by assuming thaty(t−\n1) is the true solution\n3A p-order numerical method means that the global trun-\ncation error is proportional to ppower of the step size.\nsequence of solution approximations {F1,...,F n}\nover n steps {t+ α1h,...,t + αnh}. These ap-\nproximations are then interpolated to form the ﬁnal\nsolution, as in Eq. (8).\nThe Runge-Kutta methods are straightforwardly\napplicable to the design of a Transformer block. All\nwe need is to replace the function f (see Eq. (10))\nwith the function F (see Eq. (5)). The advantage\nis that the function F is re-used in a block. Also,\nthe model parameter θt can be shared within the\nblock4. In this way, one can omit t+ αihin Eq.\n(10), and compute Fi by\nFi = F(yt +\ni−1∑\nj=1\nβijFj,θt) (11)\nThis makes the system more parameter-efﬁcient.\nAs would be shown in our experiments, the high-\norder Runge-Kutta methods can learn strong NMT\nsystems with signiﬁcantly smaller models.\nThe Runge-Kutta methods are general. For ex-\nample, the Euler method is a ﬁrst-order instance\nof them. For a second-order Runge-Kutta (RK2)\nblock, we have\nyt+1 = yt + 1\n2(F1 + F2) (12)\nF1 = F(yt,θt) (13)\nF2 = F(yt + F1,θt) (14)\nThis is also known as the improved Euler method.\nLikewise, we can deﬁne a fourth-order Runge-\nKutta (RK4) block to be:\nyt+1 = yt +\n1\n6(F1 + 2F2 + 2F3 + F4) (15)\nF1 = F(yt,θt) (16)\nF2 = F(yt + 1\n2F1,θt) (17)\nF3 = F(yt + 1\n2F2,θt) (18)\nF4 = F(yt + F3,θt) (19)\nSee Figure 2 for a comparison of different\nRunge-Kutta blocks. It should be noted that the\nmethod presented here can be interpreted from\n4Although we could distinguish the parameters at different\nsteps in a block, we found that it did not help and made the\nmodel difﬁcult to learn.\nyt+1\nF\nyt\n(a) Residual block\nyt+1\nF\nF\nyt\n1\n2\n1\n2\n(b) RK2-block\nyt+1\nF\nF\nF\nF\nyt\n1\n6\n1\n2\n2\n6\n2\n6\n1\n6\n1\n2\n(c) RK4-block\nyt+1\nF\nF\nyt\n(d) RK2-block (γi = 1)\nFigure 2: Architectures of ODE Transformer blocks.\nthe perspective of representation reﬁnement (Greff\net al., 2017). It provides a way for a function to\nupdate the function itself. For example, Universal\nTransformer reﬁnes the representation of the input\nsequence using the same function and the same pa-\nrameters in a block-wise manner (Dehghani et al.,\n2019). Here we show that inner block reﬁnements\ncan be modeled with a good theoretical support.\n3.2 Coefﬁcient Learning\nIn our preliminary experiments, the RK2 and RK4\nmethods yielded promising BLEU improvements\nwhen the model was shallow. But it was found that\nthe improvement did not persist for deeper models.\nTo ﬁgure out why this happened, let us review the\nRunge-Kutta methods from the angle of training.\nTake the RK2 method as an example. We rewrite\nEq. (12) by substituting F1 and F2, as follow\nyt+1 = yt + 1\n2F(yt,θt) +\n1\n2F(yt + F(yt,θt),θt) (20)\nLet Ebe the loss of training, Lbe the number\nblocks of the model, and yL be the model output.\nThe gradient of Eat yt is\n∂E\n∂yt\n= ∂E\n∂yL\n· 1\n2L−t ·\nL−1∏\nk=t\n(1 + gk) (21)\nwhere\ngk =\n(\n1 + ∂F(yk,θk)\n∂yk\n)\n·\n(\n1 + ∂F(yk + F(yk,θk),θk)\n∂yk + F(yk,θk)\n)\n(22)\nSeen from Eq. (29), ∂E\n∂yt is proportional to the\nfactor 1\n2L−t . This leads to a higher risk of gradient\nvanishing when Lis larger.\nThe problem somehow attributes to the small\ncoefﬁcients of Fi, that is, γ1 = γ2 = 1\n2 . A natural\nidea is to empirically set γi = 1 to eliminate the\nproduct factor of less than 1 in gradient compu-\ntation, although this is not theoretically grounded\nin standard Runge-Kutta methods. We rewrite Eq.\n(20) with the new coefﬁcients, as follows\nyt+1 = yt + F(yt,θt) +\nF(yt + F(yt,θt),θt) (23)\nThen, we have the gradient, like this\n∂E\n∂yt\n= ∂E\n∂yL\n·\nL−1∏\nk=t\ngk (24)\nThis model is easy to optimize because ∂E\n∂yL\ncan\nbe passed to lower-level blocks with no scales.\nNote that, the methods here are instances of pa-\nrameter sharing (Dehghani et al., 2019). For exam-\nple, in each ODE block, we use the same function\nF with the same parameter θt for all intermediate\nsteps. Setting γi = 1 is a further step towards this\nbecause Fi is passed to next steps with the same\nscale. Here we call it implicit parameter sharing.\nAnother method of scaling Fi is to learn the co-\nefﬁcients automatically on the training data (with\nthe initial value γi = 1). It helps the system learn\nthe way of ﬂowing Fi in a block. Our experiments\nshow that the automatic coefﬁcient learning is nec-\nessary for better results (see Section 4).\nModel Layers WMT En-De WMT En-Fr\n#Param Steps BLEU SBLEU #Param Steps BLEU SBLEU\nVaswani et al. (2017) - Transformer 6-6 213M 100K 28.40 - 222M 300K 41.00 -\nOtt et al. (2018) - Scaling NMT 6-6 210M 100K 29.30 28.6 222M 100K 43.20 41.4\nDehghani et al. (2019) - Universal Transformer - - - 28.90 - - - - -\nLu et al. (2019) - MacaronNet 6-6 - - 30.20 - - - - -\nFan et al. (2020) - LayerDrop 12-6 286M 100K 30.20 - - - - -\nWu et al. (2019) - Depth growing 8-8 270M 800K 29.92 - - - 43.27 -\nWang et al. (2019) - Transformer-DLCL 30-6 137M 50K 29.30 28.6 - - - -\nZhang et al. (2019) - Depth-wise Scale 20-20 560M 300K 29.62 29.0 108M 300K 40.58 -\nWei et al. (2020) - Multiscale Collaborative 18-6 512M 300K 30.56 - - - - -\nLiu et al. (2020) - ADMIN 60-12 262M 250K 30.01 29.5 - 250K 43.80 41.8\nLi et al. (2020) - SDT 48-6 192M 50K 30.21 29.0 198M 100K 43.28 41.5\nZhu et al. (2020) - BERT-fused model 6-6 - - 30.71 - - - 43.78 -\nBase and Deep Models\nResidual-block 6-6 61M 50K 27.89 26.8 69M 100K 41.05 39.1\nRK2-block (learnable γi) 6-6 61M 50K 28.86 27.7 69M 100K 42.31 40.3\nRK4-block 6-6 61M 50K 29.03 27.9 69M 100K 42.56 40.6\nResidual-block 24-6 118M 50K 29.43 28.3 123M 100K 42.67 40.6\nRK2-block (learnable γi) 24-6 118M 50K 30.29 29.2 123M 100K 43.48 41.5\nRK4-block 24-6 118M 50K 29.80 28.8 123M 100K 43.28 41.3\nWide Models\nResidual-block-Big 6-6 211M 100K 29.21 28.1 221M 100K 42.89 40.9\nRK2-block (learnable γi) 6-6 211M 100K 30.53 29.4 221M 100K 43.59 41.6\nResidual-block-Big 12-6 286M 100K 29.91 28.9 297M 100K 43.22 41.2\nRK2-block (learnable γi) 12-6 286M 100K 30.76 29.6 297M 100K 44.11 42.2\nTable 1: Comparison with the state-of-the-arts on WMT En-De and WMT En-Fr tasks. We both report the tok-\nenized BLEU and sacrebleu scores for comparison with previous work.\n4 Experiments\n4.1 Experimental Setups\nOur proposed methods were evaluated on three\nwidely-used benchmarks: the WMT’14 English-\nGerman (En-De), WMT’14 English-French (En-Fr)\nand WMT’16 English-Romanian (En-Ro) transla-\ntion tasks.\nDatasets and Evaluations: For the En-De task,\nthe training data consisted of approximately 4.5M\ntokenized sentence pairs, as in (Vaswani et al.,\n2017). All sentences were segmented into se-\nquences of sub-word units (Sennrich et al., 2016)\nwith 32K merge operations using a shared vocab-\nulary. We selected newstest2013 as the valida-\ntion data and newstest2014 as the test data. For\nthe En-Fr task, we used the dataset provided by\nFairseq, i.e., 36M training sentence pairs from\nWMT’14. newstest2012+newstest2013 was the val-\nidation data andnewstest2014 was the test data. For\nthe En-Ro task, we replicated the setup of (Mehta\net al., 2020), which used 600K/2K/2K sentence\npairs for training, evaluation and inference, respec-\ntively.\nWe measured performance in terms of BLEU\n(Papineni et al., 2002). Both tokenized BLEU\nscores 5 and sacrebleu6 were reported on the En-\nDe and the En-Fr tasks. Also, we report tokenized\nBLEU scores on the En-Ro task. The beam size\nand length penalty were set to 4 and 0.6 for the\nEn-De and the En-Fr, and 5 and 1.3 for the En-Ro.\nTraining Details: As suggested in Li et al.\n(2020)’s work, we used relative positional represen-\ntation (RPR) for a stronger baseline (Shaw et al.,\n2018). All experiments were trained on 8 GPUs,\nwith 4,096 tokens on each GPU. For the En-De\nand the En-Fr tasks, we employed the gradient\naccumulation strategy with a step of 2 and 8, re-\nspectively. We used the Adam optimizer (Kingma\nand Ba, 2015) whose hyperparameters were set to\n(0.9,0.997), and the max point of the learning rate\nwas set to 0.002 for fast convergence. We regard\nmerging SAN and FFN as the default ODE block.\nMore details could be found in our supplementary\nmaterials.\n5Computed by multi-bleu.perl\n6BLEU+case.mixed+numrefs.1+smooth.exp+\ntok.13a+version.1.2.12\nModel Params Epochs BLEU\nTransformer in Mehta et al. (2020) 62M 170 34.30\nDeLight (Mehta et al., 2020) 53M 170 34.70\nInt Transformer†(Lin et al., 2020) - - 32.60\nTransformer (Our impl.) 69M 20 33.49\nRK2-block (learnable γi) 69M 20 34.94\nRK2-block-Big (learnable γi) 226M 20 35.28\nTable 2: Results on the WMT En-Ro task. †indicates\nthe related information is not reported.\n4.2 Results\nResults of En-De and En-Fr: Table 1 compares\nODE Transformer with several state-of-the-art sys-\ntems. Both RK2-block and RK4-block outperform\nthe baselines by a large margin with different model\ncapacities. For example, RK2-block obtains a 0.97\nBLEU improvement with the base conﬁguration\nwhen the depth is 6. RK4-block yields a gain of\n+0.17 BLEU points on top of RK2-block. This\nobservation empirically validates the conjecture\nthat high-order ODE functions are more efﬁcient.\nWhen we switch to deep models, RK2-block is\ncomparable with a 48-layer strong system reported\nin (Li et al., 2020) with signiﬁcantly fewer parame-\nters, indicating our method is parameter efﬁcient.\nWide models can also beneﬁt from the enlarging\nlayer depth (Wei et al., 2020; Li et al., 2020). The\nRK-2 ODE Transformer achieves BLEU score of\n30.76 and 44.11 on the En-De and the En-Fr tasks,\nsigniﬁcantly surpassing the standard Big model by\n1.32 and 0.70 BLEU points. This sets a new state-\nof-the-art on these tasks with fewer parameters.\nNote that more results on RK4-block (learnable γi)\nwill be reported.\nResults of En-Ro: Table 2 exhibits model pa-\nrameters, total training steps and BLEU scores of\nseveral strong systems on the En-Ro task. Again,\nODE Transformer outperforms these baseline. As\nstated in (Mehta et al., 2020), they trained the\nmodel up to 170 epochs and obtained a BLEU\nscore of 34.70 through the DeLight model. How-\never, the observation here is quite different. The\nvalidation perplexity begins to increase after 20\nepochs. Thus, our baseline is slightly inferior to\ntheirs, but matches the result reported in Lin et al.\n(2020). ODE Transformer achieves even better per-\nformance with DeLight within much less train-\ning cost. For a bigger model (line 6 in Table 2), it\nobtains a BLEU score of 35.28.\nModel Params BLEU\nTransformer (Vaswani et al., 2017) 62M 27.30\nEvolved Transformer (So et al., 2019) 46M 27.70\nLite Transformer†(Wu et al., 2020) - 26.50\nDeLight (Mehta et al., 2020) 37M 27.60\nRK2-block (learnable γi) 37M 28.24\nRK2-block (learnable γi) 29M 27.84\nTable 3: The comparison of model efﬁciency on the\nWMT En-De task.\nParameter Efﬁciency: Table 3 summaries the\nresults of several efﬁcient Transformer variants,\nincluding Lite Transformer (Wu et al., 2020), De-\nLight (Mehta et al., 2020) and a light version of the\nEvolved Transformer (So et al., 2019). As we ex-\npected, the proposed ODE Transformer is promis-\ning for smaller models. It is comparable in BLEU\nwith DeLight but having 9M fewer parameters.\nUnder the same model capacity, it outperforms\nDeLight by 0.84 BLEU points. These results\ndemonstrate that the proposed method is orthogo-\nnal to the model capacity. It may offer a new choice\nfor deploying NMT systems on edge devices.\n4.3 Analysis\nHere we investigate some interesting issues. For\nsimplicity, in the following, we call RK2-block\nwith learnable coefﬁcients as RK2-block-v2.\nBLEU against Encoder Depth: Figure 3 (left)\ndepicts BLEU scores of several ODE Transformer\nvariants and the baseline under different encoder\ndepths. All ODE Transformer variants are signiﬁ-\ncantly superior to the baseline when the depth≤24\n. And the RK2-block-v2 almost achieves the best\nperformance over all depths, especially when the\nmodel becomes deeper. Intuitively, a 6-layer RK2-\nblock is able to deliver comparable performance\ncompared with the 18-layer baseline system. Again,\nit indicates the proposed method is parameter efﬁ-\ncient. Another ﬁnding here is RK4-block behaves\nstrong on shallow models, similar phenomena are\nobserved in Table 1. It is inferior to RK2-block for\ndeeper models, though high-order ODE solvers can\nobtain lower errors. This is due to original coefﬁ-\ncients may cause the optimization problem in the\nbackward propagation when the model is deep (see\nSection 3.2). Also, Figure 3 (right) plots BLEU as\na function of the model size when the hidden size\nis 256. Our RK2 method signiﬁcantly surpasses\nthe baseline using much fewer parameters.\n6 12 18 24 30 36\n27.0\n28.0\n29.0\n30.0\nEncoder Depth\nBLEU\nBase\nRK2\nRK2-v2\nRK4\n20 40 60 80 100 12024.0\n25.0\n26.0\n27.0\n28.0\n29.0\nNumber of Parameters (M)\nBLEU\nBaseline\nRK2-block-v2\nFigure 3: The comparison of BLEU against different\nencoder depth and the number of model parameters.\n27.5 28.0 28.5 29.0\nSAN/FNN\nSAN\nFFN\nSAN+FFN\nBaseline\n28.65\n28.17\n28.55\n28.89\n27.6\nBLEU (%)\nFigure 4: BLEU scores [%] of several F(·,·) on the\nWMT En-De task.\nAblation Study on Different F(·,·): As we\nstated, the F(·,·) function can either be the sub-\nlayer, e.g. SAN, FFN or both of them (SAN+FFN).\nAs shown in Figure 4, high-order ODE works bet-\nter with FFN than SAN. An exploration might be\nthat the FFN component has more parameters than\nthe SAN component 7. The model that merging\nFFN and SAN as an ODE block shows the best\nperformance.\nTraining and Validation Perplexity: Figure 5\nplots the training and validation perplexity (PPL)\ncurves of RK blocks and the standard residual-\nblock. We compare the behaviors based on two\nconﬁgurations (base and wide models). Intuitively,\nRK2-block presents lower training and validation\nPPLs in both conﬁgurations.\nVisualization of the Gradient Norm: To study\nthe superiority of the proposed ODE Transformer,\nwe collect the gradient norm of several well-trained\nsystems during training. Figure 6 plots the gradi-\nent norm of RK2-block, RK4-block and the stan-\ndard residual-block (baseline). As we can see that\nPre-Norm residual block is able to make the train-\ning stable (Wang et al., 2019). Both RK2-block\nand RK4-block provide richer signals due to the\nimplicit parameter sharing among intermediate ap-\nproximations. And the two learning curves likewise\nappear to be nearly the same, which is consistent\n7Mostly, there are 2 · dmodel · 4dmodel parameters in FFN\nand dmodel · 3dmodel + dmodel · dmodel in SAN.\n2 6 10 14 18 22\n4.0\n8.0\n12.0\n16.0\nEpoch\nTraining PPL\nRK2-Big\nRPR-Big\nRK2-Base\nRPR-Base\n2 6 10 14 18 22\n4.0\n5.0\n6.0\n7.0\n8.0\nEpoch\nValidation PPL\nRK2-Big\nRPR-Big\nRK2-Base\nRPR-Base\nFigure 5: The comparison of training and validation\nPPL on base and wide models.\n0 10 20 30 40 50\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nStep (K)\nValue\nRK2-block\nRK4-block\nResidual-block\nFigure 6: Visualization of the gradient norm of ODE\nTransformers compared with the baseline.\nwith the result in Table 1.\nComparison of Different ODE Design Schemas:\nThen, we take a comprehensive analysis of sev-\neral ODE design schemas. As stated in Lu et al.\n(2018), several models in computer vision, such\nas LeapfrogNet (He et al., 2019), PolyNet (Zhang\net al., 2017), Multi-step Net (Lu et al., 2018) can\nalso be interpreted from the ODE perspective. The\nrelated ODE functions are summarized in Table 4.\nHere, we re-implement these methods using the\nsame codebase for fair comparisons. We set the\nencoder depth as 6 following the base conﬁguration\nand conducted experiments on the En-De task.\nAt time t, Multistep Euler methods requires pre-\nvious states, e.g. yt−1, to generate the current ap-\nproximation, instead of iterative reﬁnements based\non the current-time state. Basically, these meth-\nods are not parameter efﬁcient, and obtain inferior\nperformance than ours. Note that DLCL can also\nbe regarded as a multistep Euler method, which is\nmore competitive in deep Transformer. But there\nis only a small improvement upon a shallow base-\nline. Theoretically, the Backward Euler method is\nslightly better than the Forward Euler method in nu-\nmerical analysis, but the improvement is marginal.\nNote that our ODE Transformer achieves consis-\ntent BLEU improvements over the aforementioned\nmethods. The reason here is that this kind of iter-\native reﬁnements enable the parameters learning\nmore efﬁcient and effective. All models could be\nModel Information Flow Related ODEs BLEU\nHe et al. (2019) - Leapfrog yt+1 = yt−1 + 2F(yt,θt) Multistep Euler 28.07\nLu et al. (2018) - Multistep yt+1 = kn · yt + (1− kn) · yt−1 + F(yt,θt) Multistep Euler 28.17\nWang et al. (2019) - DLCL yt+1 = y0 + ∑t\nl=0 WlF(yl,θl) Multistep Euler 27.78\nZhang et al. (2017) - PolyNet yt+1 = yt + F(yt,θt) +F(F(yt,θt),θt) Backward Euler 28.15\nRK2-block yt+1 = yt + 1\n2 F(yt,θt) + 1\n2 F(yt + F(yt,θt),θt) Improved Euler 28.57\nRK2-block (γi = 1) yt+1 = yt + F(yt,θt) +F(yt + F(yt,θt),θt) Runge-Kutta 2nd-order 28.77\nRK2-block (learnable γi) yt+1 = yt + γ1 · F(yt,θt) +γ2 · F(yt + F(yt,θt),θt) Runge-Kutta 2nd-order 28.86\nRK4-block yt+1 = yt + 1\n6 F1 + 2\n6 F2 + 2\n6 F3 + 1\n6 F4 Runge-Kutta 4th-order 29.03\nTable 4: Comparison of several ODE-inspired design schemas on the En-De task. We re-implement and apply\nthese methods into Transformer. Note that yn denotes the model input of layer n. Due to the limited space, we use\nFi to denote the intermediate representation, where i∈[1,4].\nModel 1-layer PPL 2-layer PPL\nResidual Block 142.33 136.07\nRK2-block 131.80 123.12\nRK2-block (γi = 1) 132.67 123.90\nRK2-block (learnable γi) 128.48 121.02\nRK4-block 126.89 119.46\nTable 5: The comparison of PPL on several systems.\nMore details refer to the supplementary materials.\nfound in our attachment.\nQuantization of the Truncation Error: Here,\nwe aim at quantifying the truncation error. How-\never, we cannot obtain the “true” solution of each\nblock output in NMT, because we mainly exper-\nimented on the encoder side. Instead, we experi-\nmented on the language modeling task, where the\nloss between a single layer model output and the\nground truth is equivalent to the truncation error\nwithout error propagations. Table 5 shows the PPL\non the PTB task. All ODE Transformer variants\nreduce the errors signiﬁcantly. RK4-order achieves\nthe lowest PPL on both settings. In addition, a\nRK2-block can even obtain lower PPL than a 2-\nlayer residual-block. The observation here again\nveriﬁes our conjecture.\n5 Related Work\nDeep Transformer models: Recently, deep\nTransformer has witnessed tremendous success in\nmachine translation. A straightforward way is to\nshorten the path from upper-level layers to lower-\nlevel layers thus to alleviate the gradient vanishing\nor exploding problems (Bapna et al., 2018; Wang\net al., 2019; Wu et al., 2019; Wei et al., 2020). For\ndeeper models, the training cost is nonnegligible.\nTo speed up the training, an alternative way is to\ntrain a shallow model ﬁrst and progressively in-\ncreasing the model depth (Li et al., 2020; Dong\net al., 2020).\nApart from the model architecture improve-\nments, another way of easing the optimization is\nto utilize carefully designed parameter initializa-\ntion strategies, such as depth-scale (Zhang et al.,\n2019), Lipschitz constraint (Xu et al., 2020), T-\nﬁxup (Huang et al., 2020) and ADMIN (Liu et al.,\n2020). Note that the ODE Transformer is orthog-\nonal to the aforementioned methods, and we will\ntest it on these methods in the future work.\nOrdinary Differential Equations: The relation-\nship between the ResNet and ODEs was ﬁrst pro-\nposed by Weinan (2017). This brings the com-\nmunity a brand-new perspective on the design of\neffective deep architectures. Some insightful archi-\ntectures (Zhang et al., 2017; Larsson et al., 2017;\nLu et al., 2018; He et al., 2019) can also be inter-\npreted from the ODE perspective. But, in nature\nlanguage processing, it is still rare to see studies\non designing models from the ODE perspective.\nPerhaps the most relevant work with us is Lu et al.\n(2019)’s work. They interpreted the Transformer\narchitecture from a multi-particle dynamic system\nview and relocated the self-attention sandwiched\ninto the FFN. Unlike their work, we argue that\nthe stacked ﬁrst-order ODE blocks may cause er-\nror accumulation, thus hindering the model perfor-\nmance. We address this issue by introducing high-\norder blocks, and demonstrate signiﬁcant BLEU\nimprovements.\n6 Conclusions\nIn this paper, we have explored the relationship be-\ntween Transformer and ODEs. We have proposed\na new architecture (ODE Transformer) to help the\nmodel beneﬁt from high-order ODE solutions. Ex-\nperimental results show that ODE Transformer can\nsigniﬁcantly outperform the baseline with the same\nmodel capacity. It achieves 30.76 and 44.11 BLEU\nscores on the WMT’14 En-De and En-Fr test data.\nThis sets a new state-of-the-art on the En-Fr task.\nReferences\nUri M Ascher and Linda R Petzold. 1998. Computer\nmethods for ordinary differential equations and\ndifferential-algebraic equations, volume 61. Siam.\nAnkur Bapna, Mia Chen, Orhan Firat, Yuan Cao,\nand Yonghui Wu. 2018. Training deeper neural\nmachine translation models with transparent atten-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3028–3033, Brussels, Belgium. Association\nfor Computational Linguistics.\nJohn C Butcher. 1963. Coefﬁcients for the study of\nrunge-kutta integration processes. Journal of the\nAustralian Mathematical Society, 3(2):185–201.\nJohn Charles Butcher. 1996. A history of runge-\nkutta methods. Applied numerical mathematics ,\n20(3):247–260.\nBo Chang, Lili Meng, Eldad Haber, Lars Ruthotto,\nDavid Begert, and Elliot Holtham. 2018. Reversible\narchitectures for arbitrarily deep residual neural\nnetworks. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 2811–2818. AAAI Press.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt,\nand David K Duvenaud. 2018. Neural ordinary dif-\nferential equations. In Advances in neural informa-\ntion processing systems, pages 6571–6583.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo\nShang. 2020. Towards adaptive residual network\ntraining: A neural-ode perspective. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 2616–2626. PMLR.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nKlaus Greff, Rupesh Kumar Srivastava, and J ¨urgen\nSchmidhuber. 2017. Highway and residual net-\nworks learn unrolled iterative estimation. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nEldad Haber and Lars Ruthotto. 2017. Stable architec-\ntures for deep neural networks. Inverse Problems,\n34(1):014004.\nEldad Haber, Lars Ruthotto, Elliot Holtham, and\nSeong-Hwan Jun. 2018. Learning across scales -\nmultiscale methods for convolution neural networks.\nIn Proceedings of the Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence (EAAI-18),\nNew Orleans, Louisiana, USA, February 2-7, 2018 ,\npages 3142–3148. AAAI Press.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nXiangyu He, Zitao Mo, Peisong Wang, Yang Liu,\nMingyuan Yang, and Jian Cheng. 2019. Ode-\ninspired network design for single image super-\nresolution. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 1732–1741. Com-\nputer Vision Foundation / IEEE.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Mak-\nsims V olkovs. 2020. Improving transformer opti-\nmization through better initialization. In Proceed-\nings of Machine Learning and Systems 2020 , pages\n9868–9876.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nWilhelm Kutta. 1901. Beitrag zur naherungsweisen in-\ntegration totaler differentialgleichungen. Z. Math.\nPhys., 46:435–453.\nGustav Larsson, Michael Maire, and Gregory\nShakhnarovich. 2017. Fractalnet: Ultra-deep\nneural networks without residuals. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du,\nTong Xiao, Huizhen Wang, and Jingbo Zhu. 2020.\nShallow-to-deep training for neural machine trans-\nlation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 995–1005, Online. Association for\nComputational Linguistics.\nYe Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran\nLiu, and Jingbo Zhu. 2020. Towards fully 8-bit inte-\nger inference for the transformer model. In Proceed-\nings of the Twenty-Ninth International Joint Confer-\nence on Artiﬁcial Intelligence, IJCAI 2020 , pages\n3759–3765. ijcai.org.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020. Understanding the dif-\nﬁculty of training transformers. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5747–\n5763, Online. Association for Computational Lin-\nguistics.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin\nDong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.\nUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view. arXiv\npreprint arXiv:1906.02762.\nYiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin\nDong. 2018. Beyond ﬁnite layer neural networks:\nBridging deep architectures and numerical differen-\ntial equations. In Proceedings of the 35th Inter-\nnational Conference on Machine Learning, ICML\n2018, Stockholmsm ¨assan, Stockholm, Sweden, July\n10-15, 2018, pages 3282–3291.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2020.\nDelight: Very deep and light-weight transformer.\narXiv preprint arXiv:2008.00623.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation, Volume 1: Research Papers ,\npages 1–9, Belgium, Brussels. Association for Com-\nputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nCarl Runge. 1895. ¨Uber die numerische auﬂ ¨osung von\ndifferentialgleichungen. Mathematische Annalen ,\n46(2):167–178.\nLars Ruthotto and Eldad Haber. 2019. Deep neural\nnetworks motivated by partial differential equations.\nJournal of Mathematical Imaging and Vision vol-\nume, 62:352–364.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nDavid R. So, Quoc V . Le, and Chen Liang. 2019. The\nevolved transformer. In Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5877–5886. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongx-\niang Weng, and Weihua Luo. 2020. Multiscale col-\nlaborative deep models for neural machine transla-\ntion. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 414–426, Online. Association for Computa-\ntional Linguistics.\nE Weinan. 2017. A proposal on machine learning via\ndynamical systems. Communications in Mathemat-\nics and Statistics, 5(1):1–11.\nLijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei\nGao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2019.\nDepth growing for neural machine translation. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5558–\n5563, Florence, Italy.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and\nSong Han. 2020. Lite transformer with long-short\nrange attention. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nHongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi\nXiong, and Jingyi Zhang. 2020. Lipschitz con-\nstrained parameter initialization for deep transform-\ners. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 397–402, Online. Association for Computa-\ntional Linguistics.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019.\nImproving deep transformer with depth-scaled ini-\ntialization and merged attention. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 898–909, Hong\nKong, China. Association for Computational Lin-\nguistics.\nXingcheng Zhang, Zhizhong Li, Chen Change Loy,\nand Dahua Lin. 2017. Polynet: A pursuit of struc-\ntural diversity in very deep networks. In 2017 IEEE\nConference on Computer Vision and Pattern Recog-\nnition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, pages 3900–3908. IEEE Computer Society.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu.\n2020. Incorporating BERT into neural machine\ntranslation. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nA Details for Experimental Steps and\nDatasets\nAll models were trained on 8 NVIDIA TITAN V\nGPUs with mix-precision accelerating. And main\nresults were the average of three times running with\ndifferent random seeds. Note that we averaged the\nlast 5/10 checkpoints for more robust results.\nSince the proposed method is orthogonal to the\nmodel capacity, we evaluated the ODE Transformer\non Base/Deep/Wide conﬁgurations, respectively.\nThe detail of each conﬁguration is as follows:\n• Base/Deep Model. The hidden size of self-\nattention was 512, and the dimension of the\ninner-layer in FFN was 2,048. We used 8\nheads for attention. For training, we set all\ndropout to 0.1, including residual dropout,\nattention dropout, ReLU dropout. Label\nsmoothing ϵls = 0.1 was applied to enhance\nthe generation ability of the model. For deep\nmodels, we only enlarged the encoder depth\nconsidering the inference speed.\n• Wide (or Big) Model. We used the same archi-\ntecture as Transformer-Base but with a larger\nhidden layer size 1,024, more attention heads\n(16), and a larger feed forward inner-layer\n(4,096 dimensions). The residual dropout\nwas set to 0.3 for the En-De task and 0.1 for\nthe En-Fr tasks.\nLang Train Valid Test\nWMT’14 En-De 4.5M 3000 3003\nWMT’14 En-Fr 35.7M 26822 3003\nWMT’16 En-Ro 602K 1999 1999\nTable 6: The comparison of PPL on several systems.\nIn the training phase, Deep/Big models were\nupdated for 50K and 100K steps on the En-De task,\n100K steps on the En-Fr task, 17K steps on the\nEn-Ro task.\nTable 6 summarizes the details of our datasets,\nincluding the WMT En-De, the WMT En-Fr and\nthe WMT En-Ro tasks. We both present the sen-\ntences and tokens of each task. For En-De and\nEn-Fr task, the datasets used in this work could\nbe found in Fairseq8. For En-Ro, one can use\nthe preprocessed dataset provided by DeLight9.\nNote that we only share the target embedding and\nthe softmax embedding instead of a shared vocabu-\nlary between the source side and the target side.\nB Details for the PTB dataset\nHere, we introduce the details about the PTB\ndataset and the corresponding conﬁguration. It\ncontains 88K, 3370 and 3761 sentences for train-\ning, validation and test. The vocabulary size was\n10K. In this work, the layer depth of the language\nmodel was set to 1 or 2. The main concern here\nis to evaluate the truncate error. Assume the layer\ndepth is 1, then the loss between the block output\nand the ground-truth can be regarded as the trun-\ncate error. It alleviates the inﬂuence of the error\naccumulation among different layers.\nThe hidden size was512, and the ﬁlter size of the\nFFN was 2,048. We set all the dropout rate as 0.1,\nincluding the residual dropout, attention dropout\nand the relu dropout. Each model was trained up\nto 20 epochs, and most models achieved the lowest\nPPL on the validation set when the epoch is 10.\nThen the validation PPL began to increase, though\nthe training PPL is still declining. The warmup-\nstep was 2000 and the batch size was 4,096. The\nmax learning rate was set to0.0007. After warmup,\nthe learning rate decayed proportionally to the in-\nverse square root of the current step.\n8https://github.com/pytorch/fairseq/\ntree/master/examples/scaling_nmt\n9https://github.com/sacmehta/delight/\nblob/master/readme_files/nmt/wmt16_en2ro.\nmd\nC Derivations of the Equation\nLet Ebe the loss of training, L be the number\nblocks of the model, and yL be the model output.\nHere, we deﬁne\nzk = yk + F(yk,θk) (25)\nThen the information ﬂow of the RK2 method\ncan be described as follows:\nyk+1 = yk + 1\n2F(yk,θk) +\n1\n2F(yk + F(yk,θk),θk)\n= yk + 1\n2F(yk,θk) + 1\n2F(zk,θk)(26)\nwhere ∂zk\n∂yk\n= 1 + ∂F(yk,θk)\n∂yk\n. In this way, the detail\nderivation of Eq. (26) is as follows:\n∂yk+1\n∂yk\n= 1 + 1\n2\n∂F(yk,θk)\n∂yk\n+ 1\n2\n∂F(zk,θk)\n∂zk\n·∂zk\n∂yk\n= 1\n2 ·\n(\n1 + 1 + ∂F(yk,θk)\n∂yk\n+ ∂F(zk,θk)\n∂zk\n·\n(\n1 + ∂F(yk,θk)\n∂yk\n))\n= 1\n2 ·\n(\n1 +\n(\n1 + ∂F(zk,θk)\n∂zk\n)\n·\n(\n1 + ∂F(yk,θk)\n∂yk\n))\n(27)\nWith the chain rule, the error Epropagates from\nthe top layer yL to layer yt by the following for-\nmula:\n∂E\n∂yt\n= ∂E\n∂yL\n· ∂yL\n∂yL−1\n·∂yL−1\n∂yL−2\n··· ∂yt+1\n∂yt\nHere we have\ngk =\n(\n1 + ∂F(yk,θk)\n∂yk\n)\n·\n(\n1 + ∂F(zk,θk)\n∂zk\n)\n(28)\nThen, put the Eq. (28) into Eq. (27), the gradient\nof Eat yt is\n∂E\n∂yt\n= ∂E\n∂yL\n· 1\n2L−t ·\nL−1∏\nk=t\n(1 + gk) (29)\nSimilarly, we can easily obtain the gradient of\nRK2 method where γi = 1:\n∂E\n∂yt\n= ∂E\n∂yL\n·gL−1 ·gL−2 ···gt\n= ∂E\n∂yL\n·\nL−1∏\nk=t\ngk (30)",
  "concepts": [
    {
      "name": "Ode",
      "score": 0.9100143909454346
    },
    {
      "name": "Ordinary differential equation",
      "score": 0.6650148630142212
    },
    {
      "name": "Transformer",
      "score": 0.6245487332344055
    },
    {
      "name": "Computer science",
      "score": 0.5082501769065857
    },
    {
      "name": "Machine translation",
      "score": 0.4612374007701874
    },
    {
      "name": "Differential equation",
      "score": 0.34005212783813477
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3298865556716919
    },
    {
      "name": "Applied mathematics",
      "score": 0.3158959150314331
    },
    {
      "name": "Mathematics",
      "score": 0.31329482793807983
    },
    {
      "name": "Mathematical analysis",
      "score": 0.1859796643257141
    },
    {
      "name": "Engineering",
      "score": 0.15672904253005981
    },
    {
      "name": "Electrical engineering",
      "score": 0.11178582906723022
    },
    {
      "name": "Voltage",
      "score": 0.0846920907497406
    }
  ],
  "topic": "Ode",
  "institutions": []
}