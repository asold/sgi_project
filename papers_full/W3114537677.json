{
  "title": "Universal Sentence Representation Learning with Conditional Masked Language Model",
  "url": "https://openalex.org/W3114537677",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2101945034",
      "name": "Ziyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2155460496",
      "name": "Yinfei Yang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2164167138",
      "name": "Daniel Cer",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2961008537",
      "name": "Jax Law",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2000649667",
      "name": "Eric Darve",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2160660844",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2171068337",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W3033406728",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2965538726",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4300822525",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W2891844856",
    "https://openalex.org/W2964583233",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W3015190879",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970719206",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W3102455836",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W2963090765",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10% improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6216‚Äì6228\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n6216\nUniversal Sentence Representation Learning with Conditional Masked\nLanguage Model\nZiyi Yang1‚àó, Yinfei Yang2, Daniel Cer2, Jax Law2, Eric Darve1\n1Stanford University\n{ziyi.yang,darve}@stanford.edu\n2Google Research\n{yinfeiy,cer,jaxlaw}@google.com\nAbstract\nThis paper presents a novel training method,\nConditional Masked Language Modeling\n(CMLM), to effectively learn sentence repre-\nsentations on large scale unlabeled corpora.\nCMLM integrates sentence representation\nlearning into MLM training by conditioning\non the encoded vectors of adjacent sentences.\nOur English CMLM model achieves state-of-\nthe-art performance on SentEval (Conneau\nand Kiela, 2018), even outperforming models\nlearned using supervised signals. As a fully\nunsupervised learning method, CMLM can\nbe conveniently extended to a broad range\nof languages and domains. We Ô¨Ånd that a\nmultilingual CMLM model co-trained with\nbitext retrieval (BR) and natural language\ninference (NLI) tasks outperforms the previ-\nous state-of-the-art multilingual models by a\nlarge margin, e.g. 10% improvement upon\nbaseline models on cross-lingual semantic\nsearch. We explore the same language bias\nof the learned representations, and propose\na simple, post-training and model agnostic\napproach to remove the language identifying\ninformation from the representation while still\nretaining sentence semantics.\n1 Introduction\nSentence embeddings map sentences into a vector\nspace. The vectors capture rich semantic informa-\ntion that can be used to measure semantic textual\nsimilarity (STS) between sentences or train classi-\nÔ¨Åers for a broad range of downstream tasks (Con-\nneau et al., 2017; Subramanian et al., 2018; Lo-\ngeswaran and Lee, 2018; Cer et al., 2018; Reimers\nand Gurevych, 2019; Yang et al., 2019a,e). State-\nof-the-art models are usually trained on supervised\ntasks such as natural language inference (Conneau\net al., 2017), or with semi-structured data like\nquestion-answer pairs (Cer et al., 2018) and trans-\nlation pairs (Subramanian et al., 2018; Yang et al.,\n‚àó Work done during internship at Google Research.\n2019a). However, labeled and semi-structured data\nare difÔ¨Åcult and expensive to obtain, making it\nhard to cover many domains and languages. Con-\nversely, recent efforts to improve language mod-\nels include the development of masked language\nmodel (MLM) pre-training from large scale unla-\nbeled corpora (Devlin et al., 2019; Lan et al., 2020;\nLiu et al., 2019). While internal MLM model rep-\nresentations are helpful when Ô¨Åne-tuning on down-\nstream tasks, they do not directly produce good sen-\ntence representations, without further supervised\n(Reimers and Gurevych, 2019) or semi-structured\n(Feng et al., 2020) Ô¨Åne-tuning.\nIn this paper, we explore an unsupervised ap-\nproach, called Conditional Masked Language Mod-\neling (CMLM), to effectively learn sentence rep-\nresentations from large scale unlabeled corpora.\nThe CMLM model architecture is illustrated in\nFig. 1, which integrates sentence representation\nlearning into MLM training by conditioning on sen-\ntence level representations produced by adjacent\nsentences. The model therefore needs to learn ef-\nfective sentence representations in order to perform\ngood MLM. Since CMLM is fully unsupervised,\nit can be easily extended to new languages. We\nexplore CMLM for both English and multilingual\nsentence embeddings for 100+ languages. Our En-\nglish CMLM model achieves state-of-the-art per-\nformance on SentEval (Conneau and Kiela, 2018),\neven outperforming models learned using (semi-\n)supervised signals. Moreover, models training on\nthe English Amazon review data using our multi-\nlingual vectors exhibit strong multilingual transfer\nperformance on translations of the Amazon review\nevaluation data to French, German and Japanese,\noutperforming existing multilingual sentence em-\nbedding models by > 5% for non-English lan-\nguages and by >2% on English.\nWe further extend the multilingual CMLM to co-\ntrain with parallel text (bitext) retrieval task, and\nÔ¨Ånetune with cross-lingual natural language infer-\n6217\n‚ÄúLife is a box of chocolates.‚Äù\nTransformerEncoder\npoolingSentence vector ùë£\nùë£! ùë£\" ùë£#. . . . TransformerEncoder\n‚ÄúYou never [MASK] what you‚Äôre [MASK] get .‚Äù\nprojectionùëÉ(ùë£)\nùë£!ùë£\" ùë£#. . .\nknow, gonna\nFigure 1: The architecture of Conditional Masked Language Modeling (CMLM).\nence (NLI) data, inspired by the success of prior\nwork on multitask sentence representation learn-\ning (Subramanian et al., 2018; Yang et al., 2019a;\nReimers and Gurevych, 2020) and NLI learn-\ning (Conneau et al., 2017; Reimers and Gurevych,\n2019). We achieve performance 3.6% better than\nthe previous state-of-the-art multilingual sentence\nrepresentation model (see details in Section 4.2).\nOn cross-lingual semantic search task, our model\noutperforms baseline models by 10% on average\nover 36 languages. Language agnostic represen-\ntations require semantically similar cross-lingual\npairs to be closer in representation space than unre-\nlated same-language pairs (Roy et al., 2020). While\nwe Ô¨Ånd our original sentence embeddings do have\na bias for same language sentences, we discover\nthat removing the Ô¨Årst few principal components of\nthe embeddings eliminates the self language bias.\nThe rest of the paper is organized as follows.\nSection 2 describes the architecture for CMLM\nunsupervised learning. In Section 3 we present\nCMLM trained on English data and evaluation re-\nsults on SentEval. In Section 4 we apply CMLM\nto learn sentence multilingual sentence representa-\ntions. Multitask training strategies on how to effec-\ntively combining CMLM, bitext retrieval and cross-\nlingual NLI Ô¨Ånetuning are explored. In Section 5,\nwe investigate self language bias in multilingual\nrepresentations and propose a simple but effective\napproach to eliminate it. The pre-trained models\nare released at https://tfhub.dev/s?q=\nuniversal-sentence-encoder-cmlm.\n2 Conditional Masked Language\nModeling\nWe introduce Conditional Masked Language Mod-\neling (CMLM) as a novel architecture for combin-\ning next sentence prediction with MLM training.\nBy ‚Äúconditional‚Äù, we mean the MLM task for one\nsentence depends on the encoded sentence level\nrepresentation of the adjacent sentence. This builds\non prior work on next sentence prediction that has\nbeen widely used for learning sentence level rep-\nresentations (Kiros et al., 2015; Logeswaran and\nLee, 2018; Cer et al., 2018; Yang et al., 2019a),\nbut has thus far produced poor quality sentence em-\nbeddings within BERT based models using MLM\nloss (Reimers and Gurevych, 2019).\nWhile existing MLMs like BERT include next\nsentence prediction tasks, they do so without any\ninductive bias to try to encode the meaning of a\nsentence within a single embedding vector. We\nintroduce a strong inductive bias for learning sen-\ntence embeddings by structuring the task as fol-\nlows. Given a pair of ordered sentences, the Ô¨Årst\nsentence is fed to an encoder that produces a sen-\ntence level embedding. The embedding is then\nprovided to an encoder that conditions on the sen-\ntence embedding in order to better perform MLM\nprediction over the second sentence. This is no-\ntably similar to Skip-Thought (Kiros et al., 2015),\nbut replaces the generation of the complete second\nsentence with the MLM denoising objective. It is\nalso similar to T5‚Äôs MLM inspired unsupervised\nencode-decoder objective (Raffel et al., 2019), with\nthe second encoder acting as a sort of decoder given\nthe representation produced for the Ô¨Årst sentence.\nOur method critically differs from T5‚Äôs in that a\nsentence embedding bottleneck is used to pass in-\nformation between two model components and in\nthat the task involves denoising a second sentence\nwhen conditioning on the Ô¨Årst rather than denoising\na single text stream.\nFig. 1 illustrates the architecture of our model.\n6218\nThe Ô¨Årst sentence s1 is tokenized and input to a\ntransformer encoder and a sentence vector v ‚ààRd\nis computed from the sequence outputs by average\npooling.1 The sentence vector v is then projected\ninto N spaces with one of the projections being the\nidentity mapping, i.e. vp = P(v) ‚ààRd√óN. Here\nwe use a three-layer MLP as the projection P(¬∑).\nDetails of P(¬∑) are available in the supplementary\nmaterial. One motivation for the projections ofs1 is\nthat MLM of s2 then can attend to various represen-\ntations of s1 instead of only 1. In Section 5.1, we\nexplore various different conÔ¨Ågurations of CMLM,\nincluding the number of projection spaces N.\nThe second sentence s2 is then masked follow-\ning the procedure described in the original BERT\npaper, including random replacement and the use\nof unchanged tokens. The second encoder shares\nthe same weights with the encoder used to embed\ns1 2. Tokens in the masked s2 are Ô¨Årst converted\ninto token vectors. The masked language modeling\nof s2 depends on s1 such that the process involves\ncross-attention between s2 token vectors and vp.\nIn practice, this is implemented by concatenating\ntoken embeddings of s2 with vp3. Other implemen-\ntations are also experimented (see Section 5.1) and\nwe empirically Ô¨Ånd concatenation works the best.\nThe concatenated representations are then provided\nto the transformer encoder to predict the masked\ntokens in s2.\nAt inference time, we keep the Ô¨Årst encoding\nmodule and discard the subsequent MLM predic-\ntion. Similar to skip-thought, CMLM trains the\nencoder to produce sentence embeddings useful\nfor predicting material in the adjunct sentences.\nCMLM adapts this existing idea to MLM training.\nAppending multiple projections performs well due\nto Ô¨Åne-grained attention between tokens and the\ndifferent views of the sentence embeddings. Note\nthat CMLM differs from SkipThought in the fol-\nlowing aspects: (a) SkipThought relies on an extra\ndecoder network while CMLM only has the en-\ncoder. (b) SkipThought predicts the entire sentence\nwhile CMLM predicts masked tokens only so the\n1One can equivalently choose other pooling methods, such\nas max pooling or use the vector output corresponding to a\nspecial token position such as the [CLS] token.\n2The dual-encoder sharing encoder weights for different\ninputs can be also referred as ‚Äúsiamese encoder‚Äù\n3Representation concatenation has been used in previous\nwork for enabling cross attention between global vectors and\nlocal token embeddings to help the representations learning\nof long/structured inputs (Ainslie et al., 2020; Manzil Zaheer,\n2020).\npredictions can be done in parallel. These two dif-\nferences make CMLM more efÔ¨Åcient to train than\nSkipThought.\n3 Learning English Sentence\nRepresentations with CMLM\nFor training English sentence encoders with\nCMLM, we use three Common Crawl dumps.\nThe data are Ô¨Åltered by a classiÔ¨Åer which detects\nwhether a sentence belongs to the main content of\nthe web page or not. We use WordPiece tokeniza-\ntion and the vocabulary is the same as public En-\nglish uncased BERT. In order to enable the model\nto learn bidirectional information, for two consec-\nutive sequences s1 and s2, we swap their order\nfor 50% of the time. This order-swapping process\nechos with the preceding and succeeding sentences\nprediction in Skip-Thought (Kiros et al., 2015).\nThe length of s1 and s2 are set to be 256 tokens\n(the maximum length). The number of masked to-\nkens in s2 are 80 (31.3%), moderately higher than\nclassical BERT. This change in the ratio of masked\ntokens is to make the task more challenging, due to\nthe fact that in CMLM, language modeling has ac-\ncess to extra information from adjacent sentences.\nWe train with batch size of 2048 for 1 million steps.\nThe optimizer is LAMB (You et al., 2020) with\nlearning rate of 10‚àí3, Œ≤1 = 0.9, Œ≤2 = 0.999,\nwarm-up in the Ô¨Årst 10,000 steps and linear decay\nafterwards. We explore two transformer conÔ¨Ågura-\ntions same as in the original BERT paper, i.e., base\nand large. The number of projections N is 15 by\nexperimenting with multiple choices.\n3.1 Evaluation\nWe evaluate the sentence representations on the\nfollowing tasks: (1) classiÔ¨Åcation: MR (movie re-\nviews Pang and Lee (2005)), binary SST (sentiment\nanalysis, Socher et al. (2013)), TREC (question-\ntype, V oorhees and Tice (2000)), CR (product\nreviews, Hu and Liu (2004)), SUBJ (subjectiv-\nity/objectivity, Pang and Lee (2004)). (2) En-\ntailment: SICK dataset for entailment (SICK-E,\nMarelli et al. (2014)). The evaluation is done using\nSentEval (Conneau and Kiela, 2018) which is a pre-\nvailing evaluation toolkit for sentence embeddings.\nThe classiÔ¨Åer for the downstream is logistic regres-\nsion. For each task, the encoder and embeddings\nare Ô¨Åxed and only downstream neural structures\nare trained.\nThe baseline sentence embedding models in-\n6219\nclude SkipThought (Kiros et al., 2015), InferSent\n(Conneau et al., 2017), USE (Cer et al., 2018),\nQuickThought (Logeswaran and Lee, 2018) and\nEnglish BERT using standard pre-trained mod-\nels from TensorFlow Hub website (Devlin et al.,\n2019), XLNet (Yang et al., 2019d), RoBERTa (Liu\net al., 2019), SBert (Reimers and Gurevych, 2019).\nTo evaluate the possible improvements coming\nfrom training data and processes, we train standard\nBERT models (English BERT base/large (CC)) on\nthe same Common Crawl Corpora that CMLM is\ntrained on. Similarly, we also train QuickThought,\na competitive unsupervised sentence representa-\ntions learning model, on the same Common Crawl\nCorpora (denoted as ‚ÄúQuickThought (CC)‚Äù). To\nfurther address the possible advantage from using\nTransformer encoder, we use a Transformer en-\ncoder as the sentence encoder in QuickThought\n(CC). The representations for BERT are computed\nby averaging the sequence outputs (we also explore\noptions including [CLS] vector and max pooling\nand the results are available in the appendix).\n3.2 Results\nEvaluation results are presented in Table 1. The\nnumbers are averaged over 5 runs and the per-\nformance variances are provided in the appendix.\nCMLM outperforms existing models overall, best-\ning MLM (both English BERT and English BERT\n(CC)) using both base and large conÔ¨Ågurations.\nThe closest competing model is SBERT, which\nuses supervised NLI data rather than a purely un-\nsupervised approach. Interestingly, CMLM outper-\nforms SBERT on the SICK-E NLI task even the\nlater model is trained with a NLI task. We also eval-\nuate on Semantic Textual Similarity (STS) datasets.\nAs shown in Table 2, CMLM exhibits competi-\ntive performance compared with BERT and GloVe.\nOne interesting observation is that CMLM base sig-\nniÔ¨Åcantly outperforms other baselines (including\nCMLM large) on the STS Benchmark dataset.\n4 Learning Multilingual Sentence\nRepresentations with CMLM\nAs a fully unsupervised method, CMLM can be\nconveniently extended to multilingual modeling\neven for less well resourced languages. Learn-\ning good multilingual sentence representations is\nmore challenging than monolingual ones, espe-\ncially when attempting to capture the semantic\nalignment between different languages. As CMLM\ndoes not explicitly address cross-lingual alignment,\nwe explore several modeling approaches besides\nCMLM: (1) Co-training CMLM with a bitext re-\ntrieval task; (2) Fine-tuning with cross-lingual NLI\ndata.\n4.1 Multilingual CMLM\nWe follow the same conÔ¨Åguration used to learn\nEnglish sentence representations with CMLM, but\nextend the training data to include more languages.\nResults below will show that CMLM again exhibits\ncompetitive performance as a general technique to\nlearn from large scale unlabeled corpora.\n4.2 Multitask Training with CMLM and\nBitext Retrieval\nBesides the monolingual pretraining data, we col-\nlect a dataset of bilingual translation pairs {(si, ti)}\nusing a bitext mining system (Feng et al., 2020).\nThe source sentences {si}are in English and the\ntarget sentences {ti}covers over 100 languages.\nWe build a retrieval task with the translation paral-\nlel data, identifying the corresponding translation\nof the input sentence from candidates in the same\nbatch. Concretely, incorporating Additive Margin\nSoftmax (Yang et al., 2019b), we compute the bi-\ntext retrieval loss Ls\nbr for the source sentences as:\nLs\nbr = ‚àí1\nB\nB‚àë\ni=1\neœÜ(si,ti)‚àím\neœÜ(si,ti)‚àím + ‚àëB\nj=1,jÃ∏=ieœÜ(si,tj)\nAbove œÜ(si,tj) denotes the the inner products\nof sentence vectors of si and tj (embedded by the\ntransformer encoder); mand B denotes the addi-\ntive margin and the batch size respectively. Note\nthe way to generate sentence embeddings is the\nsame as in CMLM. We can compute the bitext\nretrieval loss for the target sentences Lt\nbr by nor-\nmalizing over source sentences, rather than target\nsentences, in the denominator. 4 The Ô¨Ånal bitext\nretrieval loss Lbr is given as Lbr = Ls\nbr + Lt\nbr.\nThere are several ways to incorporate the mono-\nlingual CMLM task and bitext retrieval (BR). We\nexplore the following multistage and multitask pre-\ntraining strategies:\nS1. CMLM+BR: Train with CMLM and BR from\nthe start;\nS2. CMLM ‚ÜíBR: Train with CMLM in the Ô¨Årst\nstage and then train with on BR;\n4i.e., by swapping the i and j subscripts in the last term of\nthe denominator.\n6220\nModel MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R Avg.\nSkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0 82.3 85.8 83.8\nInferSent 81.6 86.5 92.5 90.4 84.2 88.2 75.8 84.3 86.4 85.5\nUSE 80.1 85.2 94.0 86.7 86.4 93.2 70.1 82.4 85.9 84.9\nQuickThought (CC) 75.7 81.9 94.3 84.7 79.7 83.0 70.4 75.0 78.5 80.4\nXLNet 83.6 82.1 90.8 89.0 89.0 90.4 70.1 82.1 78.4 83.9\nBERT-based models\nEnglish BERT base 81.6 87.4 95.2 87.8 85.8 90.6 71.1 79.3 80.5 84.3\nEnglish BERT base (CC) 82.5 88.5 95.6 87.3 88.0 91.4 72.0 79.3 79.0 84.6\nSBERT (NLI, base) 83.6 89.4 94.4 89.9 88.9 89.6 76.0 79.9 80.6 85.8\nCMLM base (ours) 83.6 89.9 96.2 89.3 88.5 91.0 69.7 82.3 83.4 86.0\nEnglish BERT large 84.3 88.9 95.7 86.8 88.9 91.4 71.8 75.7 77.0 84.5\nEnglish BERT large (CC) 85.4 89.0 95.7 86.9 90.5 91.2 75.5 74.3 77.0 85.0\nRoBERTa (large) 85.2 90.6 97.0 90.0 89.5 93.6 74.2 75.1 78.9 86.0\nSBERT (NLI, large) 84.8 90.0 94.5 90.3 90.7 87.4 76.0 74.9 84.2 85.9\nCMLM large (ours) 85.6 89.1 96.6 89.3 91.4 92.4 70.0 82.2 84.5 86.8\nTable 1: Transfer learning test set results on SentEval for English models. Baseline models include BERT-based\n(BERT, RoBERTA and SBERT) and non-BERT models (XLNet, SkipThought, InferSent and USE).\nModel STS12 STS13 STS14 STS15 STS16 STSB SICK-R Avg.\nAvg. GloVe embeddings 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nBERT Mean embeddings 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\nBERT CLS-vector 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\nCMLM base (ours) 58.20 61.07 61.67 73.32 74.88 76.60 64.80 67.22\nCMLM large (ours) 59.02 61.68 62.80 74.16 75.64 69.39 66.56 67.03\nTable 2: Spearman rank correlation on Semantic Textual Similarity (STS) datasets. SICK-R is zero-shot evalua-\ntion by directly computing the cosine similarity of sentence embeddings, without training the task-speciÔ¨Åc neural\nnetwork.\nS3. CMLM ‚Üí CMLM+BR: Train with only\nCMLM in the Ô¨Årst stage and then with both\ntasks.\nWhen training with both CMLM and BR, the\noptimization loss is a weighted sum of the lan-\nguage modeling and the retrieval loss Lbr, i.e.\nL= LCMLM +Œ±Lbr. We empirically Ô¨ÅndŒ±= 0.2\nworks well. As shown in Table 4, S3 is found to be\nthe most effective. Unless otherwise denoted, our\nmodels trained with CMLM and BR follow S3. We\nalso discover that given a pre-trained transformer\nencoder, e.g. mBERT, we can improve the quality\nof sentence representations by Ô¨Ånetuning the trans-\nformer encoder with CMLM and BR. As shown in\nTable 4, the improvements of f-mBERT (Ô¨Ånetuned\nmBERT) upon mBERT are signiÔ¨Åcant.\n4.3 Finetuning with Cross-lingual Natural\nLanguage Inference\nFinetuning with NLI data has proved to be an ef-\nfective method to improve the quality of embed-\ndings for English models. We propose to leverage\ncross-lingual NLI Ô¨Ånetuning in multilingual repre-\nsentations. Given a premise sentence u in language\nl1 and a hypothesis sentence v in language l2, we\ntrain a 3-way classiÔ¨Åer on the concatenation of\n[u,v,|u ‚àív|,u ‚àóv]. Weights of transformer en-\ncoders are also updated in the Ô¨Ånetuning process.\nDifferent from previous work also using multilin-\ngual NLI data (Yang et al., 2019a), the premiseu\nand hypothesis v are in different languages. The\ncross-lingual NLI data are generated by translating\nMulti-Genre NLI Corpus (Williams et al., 2018)\ninto 14 languages using Google Translate API.\n4.4 ConÔ¨Ågurations\nMonolingual training data for CMLM are gener-\nated from 3 versions of Common Crawl data in\n113 languages. The data cleaning and Ô¨Åltering is\nthe same as the English-only ones. A new cased\nvocabulary is built from the all data sources using\nthe WordPiece vocabulary generation library from\nTensorÔ¨Çow Text. The language smoothing expo-\nnent from the vocab generation tool is set to 0.3,\nas the distribution of data size for each language is\nimbalanced. The Ô¨Ånal vocabulary size is 501,153.\nThe number of projections N is set to be 15, the\nbatch size Bis 2048 and the positive margin is 0.3.\n6221\nModel ar bg de el en es fr hi ru sw th tr ur vi zh Avg.\nmBERT 76.3 76.1 77.7 76.1 80.1 78.5 78.7 75.6 77.3 70.5 73.6 75.7 74.2 78.8 78.7 76.5\nMLM (CC) 79.2 79.1 81.7 79.9 84.4 82.1 82.2 79.2 81.2 70.3 76.9 79.0 74.3 81.3 81.0 79.4\nXLM-R 78.1 78.0 76.2 78.2 82.8 81.2 80.4 77.2 80.2 71.0 77.5 79.7 76.7 80.3 80.8 78.5\nCMLM 80.6 81.2 82.6 81.4 85.0 82.3 83.4 80.0 82.3 76.2 78.8 81.0 78.5 81.6 81.7 81.2\nTable 3: Performance (accuracy) of multilingual models trained with monolingual data on XEV AL. Highest num-\nbers are highlighted in bold.\nModel ar bg de el en es fr hi ru sw th tr ur vi zh Avg.\nLASER 82.1 81.2 81.7 78.1 82.3 81.0 80.8 78.9 82.2 75.8 80.3 81.8 77.2 81.6 82.1 80.4\nmUSE 80.4 ‚Äì 82.2 ‚Äì 83.3 82.7 82.4 ‚Äì 82.3 ‚Äì 81.6 80.3 ‚Äì ‚Äì 82.0 81.9\nS1 78.3 78.9 79.3 78.1 81.0 78.7 79.5 78.0 79.0 76.6 77.8 78.6 77.7 79.0 78.6 78.6\nS2 81.3 81.0 83.0 81.4 85.6 83.0 83.6 80.4 82.3 77.6 80.1 81.0 79.8 82.4 82.3 81.6\nS3 82.6 83.0 84.0 81.8 85.8 84.2 84.6 81.7 84.0 79.3 81.2 82.7 81.2 83.0 83.0 82.8\nS3+NLI 84.2 83.7 85.0 83.4 87.0 85.9 85.8 83.0 85.6 79.6 83.0 84.2 81.2 84.2 84.4 84.0\nmBERT 76.3 76.1 77.7 76.1 80.1 78.5 78.7 75.6 77.3 70.5 73.6 75.7 74.2 78.8 78.7 76.5\nf-mBERT77.2 78.5 79.7 76.7 81.4 80.0 80.3 77.2 79.1 73.3 76.1 77.1 76.9 79.8 80.4 78.3\nTable 4: Performance (accuracy) of models trained with cross-lingual data on XEV AL. We test with multiple\nstrategies for multitask pretraining: [S1]: CMLM ‚ÜíBR; [S2]: CMLM+BR; [S3]: CMLM ‚ÜíCMLM+BR.\n[f-mBERT] denotes Ô¨Ånetuning mBERT with CMLM and BR.\nFor CMLM only pretraining, the number of steps\nis 2 million. In multitask learning, for S1 and S3,\nthe Ô¨Årst stage is of 1.5 million and the second stage\nis of 1 million steps; for S2, number of training\nsteps is 2 million. The transformer encoder uses\nthe BERT base conÔ¨Åguration. Initial learning rate\nand optimizer chosen are the same as the English\nmodels. Motivations for choosing such conÔ¨Ågura-\ntions, training details and potential limitations of\nCMLM are discussed in the appendix.\n4.5 Evaluations\n4.5.1 XEV AL: Multilingual Benchmarks for\nSentence Representations Evaluation\nEvaluations in previous multilingual literature fo-\ncused on the cross-lingual transfer learning ability\nfrom English to other languages. However, this\nevaluation protocol that treats English as the ‚Äúan-\nchor‚Äù does not equally assess the quality of non-\nEnglish sentence representations with English ones.\nTo address the issue, we prepare a new benchmark\nfor multilingual sentence vectors, XEV AL, by trans-\nlating SentEval (English) to other 14 languages\nwith Google Translate API. The reliability of XE-\nV AL is discussed in the appendix.\nResults of models trained with monolingual data\nare shown in Table 3. Baseline models include\nmBERT (Devlin et al., 2019), XLM-R (Ruder et al.,\n2019) and a transformer encoder trained with MLM\non the same Common Crawl data (MLM(CC),\nagain this is to control the effects of training data).\nThe method to produce sentence representations\nfor mBERT and XLM-R is chosen to be average\npooling after exploring options including [CLS]\nrepresentations and max pooling. The multilingual\nmodel CMLM trained on monolingual data outper-\nform all baselines in all 15 languages.\nResults of models trained with cross-lingual data\nare presented in Table 4. Baseline models for com-\nparison include LASER (Artetxe and Schwenk\n(2019), trained with parallel data) and multilin-\ngual USE ((Yang et al., 2019a), trained with cross-\nlingual NLI. Note it only supports 16 languages).\nOur model (S3) outperforms LASER in all 15 lan-\nguages. Notably, Ô¨Ånetuning with NLI in the cross-\nlingual way produces signiÔ¨Åcant improvement (S3\n+ NLI v.s. S3). Multitask learning with CMLM and\nBR can also be used to increase the performance of\npretrained encoders, e.g. mBERT. mBERT trained\nwith CMLM and BR (f-mBERT) has a signiÔ¨Åcant\nimprovement upon mBERT.\n4.5.2 Amazon Reviews\nWe conduct a zero-shot transfer learning evalua-\ntion on Amazon reviews dataset (Prettenhofer and\nStein, 2010). Following Chidambaram et al. (2019),\nthe original dataset is converted to a classiÔ¨Åcation\nbenchmark by treating reviews with strictly more\nthan 3 stars as positive and negative otherwise. We\nsplit 6000 English reviews in the original training\nset into 90% for training and 10% for develop-\nment. The two-way classiÔ¨Åer, upon the concatena-\n6222\nLang. af ar bg bn de el es et eu fa Ô¨Å fr he hi hu id it ja\nmBERT 42.7 25.8 49.3 17 77.2 29.8 68.7 29.3 25.5 46.1 39 66.3 41.9 34.8 38.7 54.6 58.4 42\nMLM (CC) 60.5 51.4 74.8 45 89.3 68.3 81.8 56.8 59.5 81.3 76.6 82.6 72.2 76.2 68.4 82.6 72.8 65.7\nXLM 43.2 18.2 40 13.5 66.2 25.6 58.4 24.8 17.1 32.2 32.2 54.5 32.1 26.5 30.1 45.9 56.5 40\nXLM-R 58.2 47.5 71.6 43 88.8 61.8 75.7 52.2 35.8 70.5 71.6 73.7 66.4 72.2 65.4 77 68.3 60.6\nLASER 89.491.995.0 89.699.094.9 98.096.7 94.671.696.395.6 92.1 94.7 96.0 94.595.495.3\nCMLM 62.0 53.2 75.0 45.1 89.9 69.9 82.7 59.2 61.6 83.7 77.1 83.5 73.1 76.7 70.3 83.0 73.5 67.2\nCMLM+BR 96.390.695.4 91.297.795.4 98.195.6 92.095.695.996.1 92.8 97.6 96.5 95.694.295.6\nCMLM+BR+NLI90.5 83.6 92.6 86.4 97.6 91.6 95.5 82.6 76.3 90.7 88.9 93.5 86.8 94.6 89.6 91.7 90.4 88.4\njv ka kk ko ml mr nl pt ru sw ta te th tl tr ur vi zh Mean\nmBERT 17.6 20.5 27.1 38.5 19.8 20.9 68 69.9 61.2 11.5 14.3 16.2 13.7 16 34.8 31.6 62 71.6 38.7\nMLM (CC) 49.5 65.8 61.3 66.4 65.3 56.8 83.4 83.1 74.8 65.9 61.3 68.5 70.0 62.7 70.3 80.1 77.0 71.3 69.4\nXLM 22.4 22.9 17.9 25.5 20.1 13.9 59.6 63.9 44.8 12.6 20.2 12.4 31.8 14.8 26.2 18.1 47.1 42.2 32.6\nXLM-R 14.1 52.1 48.5 61.4 65.4 56.8 80.8 82.2 74.1 20.3 26.4 35.9 29.4 36.7 65.7 24.3 74.7 68.3 57.3\nLASER 23.0 35.9 18.6 88.9 96.9 91.5 96.3 95.2 94.4 57.5 69.4 79.7 95.4 50.6 97.5 81.9 96.8 95.5 84.4\nCMLM 51.8 65.5 62.7 67.2 65.8 57.0 83.8 83.6 75.5 66.6 61.7 68.8 70.3 63.5 70.5 80.3 77.4 71.7 70.3\nCMLM+BR 83.4 94.9 88.6 92.4 98.9 94.5 97.3 95.3 94.9 87.0 91.2 97.9 96.6 95.3 98.6 94.4 97.5 95.6 94.7\nCMLM+BR+NLI66.9 88.1 80.3 85.6 94.9 90.7 93.2 92.3 91.7 76.7 88.6 92.8 94.7 82.0 94.3 84.7 94.3 93.1 88.8\nTable 5: Tatoeba results (retrieval accuracy) for each language. Our model CMLM+BR achieves the best results\non 30 out of 36 languages.\nModels English French German Japanese\nEncoder parameters are frozen during Ô¨Ånetuning\nEriguchi et al. (2018) 83.2 81.3 - -\nMTDE en-fr 87.4 82.3 - -\nMTDE en-de 87.1 - 81.0 -\nmBERT 80.0 73.1 70.4 71.7\nXLM-R - 85.3 81.5 82.5\nMLM (CC) 84.6 84.9 84.3 82.1\nCMLM 88.4 88.2 87.5 83.7\nCMLM+ BR 88.3 87.2 86.4 83.2\nCMLM+ BR + NLI 89.4 88.8 88.4 82.8\nEncoder parameters are trained during Ô¨Ånetuning\nmBERT 89.3 83.5 79.4 74.0\nMLM (CC) 92.9 88.7 88.4 86.3\nCMLM 93.4 92.4 92.1 88.6\nCMLM+ BR 93.6 93.1 92.3 88.1\nCMLM+ BR + NLI 93.7 92.4 93.5 86.8\nTable 6: ClassiÔ¨Åcation accuracy on the Amazon Re-\nviews dataset.\ntion of [u,v,|u ‚àív|,u ‚àóv] (following works e.g.\nReimers and Gurevych (2019)), is trained on the\nEnglish training set and then evaluated on English,\nFrench, German and Japanese test sets (each has\n6000 examples). The same multilingual encoder\nand classiÔ¨Åer are used for all the evaluations. We\nalso experiment with whether freezing the encoder\nweights or not during training. As presented in Ta-\nble 6, CMLM alone has already outperformed base-\nline models, including Multi-task Dual-Encoder\n(MTDE, Chidambaram et al. (2019)), mBERT and\nXLM-R. Training with BR and cross-lingual NLI\nÔ¨Ånetuning further boost the performance.\n4.6 Tatoeba: Semantic Search\nWe test on Tatoeba dataset proposed in Artetxe and\nSchwenk (2019) to asses the ability of our mod-\nels on capturing cross-lingual semantics. The task\nis to Ô¨Ånd the nearest neighbor for the query sen-\ntence in the other language. The experiments is\nconducted on the 36 languages as in XTREME (Hu\net al., 2020). The evaluation metric is retrieval accu-\nracy. Results are presented in Table 5. Our model\nCMLM+BR outperforms all baseline models in 30\nout of 36 languages and has the highest average\nperformance. One interesting observation is that\nÔ¨Ånetuning with NLI actually undermines the model\nperformance on semantic search, in contrary with\nthe signiÔ¨Åcant improvements from CMLM+BR to\nCMLM+BR+NLI on XEV AL (Table 4). We spec-\nulate this is because unlike semantic search, NLI\ninference is not a linear process. Finetuning with\nNLI is not expected to help the linear retrieval by\nnearest neighbor search.\n5 Analysis\n5.1 Ablation Study\nWe explore different conÔ¨Ågurations of CMLM, in-\ncluding the number of projection spaces N (Ta-\nble 7). Projecting the sentence vector into N = 15\nspaces produces highest overall performance. We\nalso try a different CMLM architecture. Besides\nthe concatenation with token embeddings of s2 be-\nfore input to the transformer encoder, the projected\nvectors are also concatenated with the sequence\noutputs of s2 for the masked token prediction. This\nversion of architecture is denoted as ‚Äúskip‚Äù and the\nmodel performance is actually worse.\nNote that the projected vector can also be used to\nproduce the sentence representation vs, e.g. using\nthe average of projected vectors vs = 1\nN\n‚àë\niv(i)\np\nas the sentence embeddings. Recall v(i)\np is the ith\n6223\nModel MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R Avg.\nN = 1 82.3 89.7 95.8 88.8 87.6 90.4 71.5 80.8 83.4 85.5\nN = 5 83.7 90.0 95.5 89.0 89.4 86.6 69.5 79.3 81.7 85.0\nN = 10 83.4 89.0 96.1 88.9 88.2 90.2 68.5 79.7 81.5 84.9\nN = 15 83.6 89.9 96.2 89.3 88.5 91.0 69.7 82.3 83.4 86.0\nN = 20 81.1 89.5 95.8 88.9 85.9 89.8 69.7 80.2 85.0 85.1\nskip 80.3 86.8 94.5 87.5 84.9 86.0 69.2 72.8 74.7 81.9\nproj 82.6 89.7 96.0 87.3 87.5 89.2 70.5 81.7 83.8 85.4\nTable 7: Ablation study of CMLM designs, including the number of projection spaces, architecture and sentence\nrepresentations. The experiments are conducted on SentEval.\nde\nfr\nzh\nmBERTmBERT+ PCRours ours + PCR\nFigure 2: Language distribution of retrieved sentences. The histogram values represent the percentage of sentences\nretrieved in a language. The Ô¨Årst and third columns are mBERT and our models. Our model already in general has\na more uniform distribution than mBERT. The second and fourth columns are mBERT and our model with PCR.\nprojection. This version is denoted as ‚Äúproj‚Äù in\nTable 7. Sentence representations produced in this\nway still yield competitive performance, which fur-\nther conÔ¨Årm the effectiveness of the projection.\nfra cmn spa deu rus ita\nmBERT 60.2 60.2 62.8 65.9 53.8 55.7\nmBERT + PCR59.9 64.3 61.7 67.5 57.4 56.2\nours 96.1 95.6 98.1 97.7 94.9 94.2\nours + PCR 95.5 96.0 98.2 97.9 95.1 94.1\ntur por hun jpn nld Avg.\nmBERT 32.4 62.4 31.9 39.0 56.2 52.8\nmBERT + PCR33.3 64.4 36.5 42.3 61.1 54.8\nours 98.6 95.3 96.5 95.6 97.3 96.3\nours + PCR 98.5 95.8 96.6 95.3 97.2 96.4\nTable 8: Average retrieval accuracy on 11 languages\nof multilingual representations model with and without\nPCR on Tatoeba dataset.\n5.2 Language Agnostic Properties\nLanguage Agnosticism has been a property of\ngreat interest for multilingual representations.\nHowever, there has not been aqualitative measure-\nment or rigid deÔ¨Ånition for this property. We pro-\npose that ‚Äúlanguage agnostic‚Äù refers to the property\nthat sentences representations are neutral w.r.t their\nlanguage information. E.g., two sentences with\nsimilar semantics should be close in embedding\nspace whether they are of the same languages or\nnot. To capture this intuition, we convert the PAWS-\nX dataset (Yang et al., 2019c) to a retrieval task to\nmeasure the language agnostic property. SpeciÔ¨Å-\ncally, PAWS-X consists of English sentences and\ntheir translations in other six languages. Given a\nquery, we inspect the language distribution of the\nretrieved sentences. The similarity between a query\nvl1 in language l1 and a candidate vl2 in language\nl2 is computed as the cosine similarity\nvT\nl1 vl2\n‚à•vl1 ‚à•2‚à•vl2 ‚à•2\n.\nIn Fig. 2, representations of mBERT have a strong\nself language bias, i.e. sentences in the language\nmatching the query are dominant. In contrast, the\nbias is much weaker in our model, probably due to\nthe cross-lingual retrieval pretraining.\n6224\nengfra engdeu engrus engspa\noursmBERT\nFigure 3: Visualizations of sentence embeddings of CMLM (Ô¨Årst row) and mBERT (second row) in Tatoeba dataset\nin 2D. The target languages are all English and the source languages are French, German, Russian and Spanish.\nWe also discover that removing the Ô¨Årst princi-\npal component of each monolingual space from\nsentence representations effectively eliminates the\nself language bias. Given a monolingual space\nMl1 ‚ààRN√ód, where each row of Ml1 is a em-\nbedding in language l1. For example, in the eval-\nuation on Tatoeba dataset, the monolingual space\nmatrix Ml1 is computed with texts in language\nl1 in Tatoeba. The principal component cl1 is the\nÔ¨Årst right singular vector of Ml1 . Given a repre-\nsentation vl1 in language l1, the projection of vl1\nonto cl1 is removed: ÀÜvl1 = vl1 ‚àí\nvT\nl1 cl1\n‚à•vl1 ‚à•2\n. The simi-\nlarity score between vl1 and vl2 for cross-lingual\nretrieval is computed as:\nÀÜvT\nl1 ÀÜvl2\n‚à•ÀÜvl1 ‚à•2‚à•ÀÜvl2 ‚à•2\n.\nAs shown in the second and the fourth column\nin Fig. 2, with principal component removal (PCR),\nthe language distribution of retrieved texts is much\nmore uniform. We also explore PCR on the Tatoeba\ndataset. Table 8 shows the retrieval accuracy of\nmultilingual model with and w/o PCR. PCR in-\ncreases the overall retrieval performance for both\nmodels. This suggests the Ô¨Årst principal compo-\nnents in each monolingual space primarily en-\ncodes language identiÔ¨Åcation information.\nWe also visualize sentence embeddings on\nTatoeba dataset in Fig. 3. Our model shows both\nweak and strong semantic alignment (Roy et al.,\n2020). Representations are close to others with\nsimilar semantics regardless of their languages\n(strong alignment), especially for French and Rus-\nsian, where representations form several distinct\nclusters. Also representations from the same lan-\nguage tend to cluster (weak alignment). While rep-\nresentations from mBERT generally exhibit weak\nalignment.\n6 Conclusion\nWe present a novel sentence representation learn-\ning method Conditional Masked Language Model-\ning (CMLM) for training on large scale unlabeled\ncorpus. CMLM outperforms the previous state-\nof-the-art English sentence embeddings models,\nincluding those trained with (semi-)supervised sig-\nnals. For multilingual representations, we discover\nthat co-training CMLM with bitext retrieval and\ncross-lingual NLI Ô¨Ånetuning achieves state-of-the-\nart performance. We also Ô¨Ånd that multilingual\nrepresentations have the same language bias and\nprincipal component removal can eliminate the bias\nby separating language identity information from\nsemantics.\nAcknowledgments\nWe would like to thank our teammates from\nDescartes, Google Brain and other Google groups\nfor their feedback and suggestions. We also thank\nanonymous reviewers for their comments. Special\nthanks goes to Chen Chen and Hongkun Yu for help\nwith TensorFlow model garden, and Arno Eigen-\nwillig for help on releasing models on TensorFlow\nHub.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268‚Äì284, Online. Asso-\nciation for Computational Linguistics.\n6225\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597‚Äì610.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nMuthu Chidambaram, Yinfei Yang, Daniel Cer, Steve\nYuan, Yunhsuan Sung, Brian Strope, and Ray\nKurzweil. 2019. Learning cross-lingual sentence\nrepresentations via a multi-task dual-encoder model.\nIn Proceedings of the 4th Workshop on Represen-\ntation Learning for NLP (RepL4NLP-2019) , pages\n250‚Äì259, Florence, Italy. Association for Computa-\ntional Linguistics.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo√Øc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 670‚Äì680.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186.\nAkiko Eriguchi, Melvin Johnson, Orhan Firat, Hideto\nKazawa, and Wolfgang Macherey. 2018. Zero-\nshot cross-lingual classiÔ¨Åcation using multilin-\ngual neural machine translation. arXiv preprint\narXiv:1809.04686.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020. Language-\nagnostic bert sentence embedding. arXiv preprint\narXiv:2007.01852.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. arXiv preprint arXiv:2003.11080.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168‚Äì177.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\nand R. Garnett, editors,Advances in Neural Informa-\ntion Processing Systems 28, pages 3294‚Äì3302. Cur-\nran Associates, Inc.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLajanugen Logeswaran and Honglak Lee. 2018. An ef-\nÔ¨Åcient framework for learning sentence representa-\ntions. In International Conference on Learning Rep-\nresentations.\nKumar Avinava Dubey Joshua Ainslie Chris Alberti\nSantiago Ontanon Philip Pham Anirudh Ravula Qi-\nfan Wang Li Yang Amr Ahmed Manzil Zaheer,\nGuru Guruganesh. 2020. Big bird: Transformers for\nlonger sequences. In Advances in Neural Informa-\ntion Processing Systems. Curran Associates, Inc.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, Roberto Zamparelli,\net al. 2014. A sick cure for the evaluation of com-\npositional distributional semantic models. In LREC,\npages 216‚Äì223.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe 42nd Annual Meeting of the Association for Com-\nputational Linguistics (ACL-04), pages 271‚Äì278.\nBo Pang and Lillian Lee. 2005. Seeing stars: exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting on Association for Computa-\ntional Linguistics, pages 115‚Äì124.\nPeter Prettenhofer and Benno Stein. 2010. Cross-\nlanguage text classiÔ¨Åcation using structural corre-\nspondence learning. In Proceedings of the 48th an-\nnual meeting of the association for computational\nlinguistics, pages 1118‚Äì1127.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. arXiv e-prints.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\n6226\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3973‚Äì3983.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512‚Äì4525,\nOnline. Association for Computational Linguistics.\nUma Roy, Noah Constant, Rami Al-Rfou, Aditya\nBarua, Aaron Phillips, and Yinfei Yang. 2020.\nLareqa: Language-agnostic answer retrieval\nfrom a multilingual pool. arXiv preprint\narXiv:2004.05484.\nSebastian Ruder, Anders S√∏gaard, and Ivan Vuli ¬¥c.\n2019. Unsupervised cross-lingual representation\nlearning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics:\nTutorial Abstracts, pages 31‚Äì38, Florence, Italy. As-\nsociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631‚Äì1642.\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J Pal. 2018. Learning gen-\neral purpose distributed sentence representations via\nlarge scale multi-task learning. In International\nConference on Learning Representations.\nEllen M V oorhees and Dawn M Tice. 2000. Building\na question answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 200‚Äì207.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112‚Äì1122.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernan-\ndez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan\nSung, et al. 2019a. Multilingual universal sen-\ntence encoder for semantic retrieval. arXiv preprint\narXiv:1907.04307.\nYinfei Yang, Gustavo Hernandez Abrego, Steve Yuan,\nMandy Guo, Qinlan Shen, Daniel Cer, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2019b. Im-\nproving multilingual sentence embedding using bi-\ndirectional dual encoder with additive margin soft-\nmax. In Proceedings of the Twenty-Eighth In-\nternational Joint Conference on ArtiÔ¨Åcial Intelli-\ngence, IJCAI-19 , pages 5370‚Äì5378. International\nJoint Conferences on ArtiÔ¨Åcial Intelligence Organi-\nzation.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019c. Paws-x: A cross-lingual adver-\nsarial dataset for paraphrase identiÔ¨Åcation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3678‚Äì3683.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019d. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. Advances in Neural\nInformation Processing Systems, 32:5753‚Äì5763.\nZiyi Yang, Chenguang Zhu, and Weizhu Chen. 2019e.\nParameter-free sentence embedding via orthogonal\nbasis. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n638‚Äì648.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Con-\nference on Learning Representations.\n6227\nA Methods for Representations\nWe evaluate different representations method in\nTransformer-base models, including CMLM and\nBERT base (using the model on ofÔ¨Åcial TensorÔ¨Çow\nHub). The experiments are conducted on SentEval.\nResults in Table 9 show that MEAN representation\nexhibit better performance than CLS and MAX\nrepresentations.\nB Experiments with different Masking\nratios\nWe test with different masking ratios in CMLM\ntraining data. SpeciÔ¨Åcally, We tried masking 40,\n60, 80 and 100 tokens of 256 tokens in the CMLM\ndata. Performance of obtained models on SentEval\nare presented in Appendix B.\nC Training ConÔ¨Ågurations and\nImplementation Details\nProjection P in the CMLM modeling. Let h de-\nnote the dimension of the input sentence vector\n(e.g. h = 768 in BERT base; h = 1024 in BERT\nlarge). Let FC(h1,h2,n) denote a fully connected\nlayer with input dimension h1, output dimension\nh2 and nonlinearity function n. The three layers\nare FC(h,2√óh,ReLU), FC(2√óh,2√óh,ReLU),\nFC(2 √óh,h, None). We tried projections without\nintermediate layers and observed a drop in training\nLM accuracy. Adding more layers doesn‚Äôt improve\nthe MLM accuracy or downstream tasks perfor-\nmance. Using 2 √óhis empirically chosen based\non preliminary experiments. Other hidden sizes are\nalso explored.\nConÔ¨Ågurations for multilingual representa-\ntions learning. In general, larger batch sizes im-\nprove performance until we reach 2048, since each\nexample will see more ‚Äúmismatched‚Äù examples.\nAfter 2048, we do not see obvious improvements\nin performance from increasing batch size. We‚Äôll\nadd detailed results on this in the Ô¨Ånal version. The\ntraining steps for different stages are decided on a\nvalidation set.\nTraining Data and infrastructure. English\npretraining takes 5 days on 64 TPUs using 1TB\nof data from Common Crawl dumps 2020-1, 2020-\n05, 2020-10. More data could be beneÔ¨Åcial, but\nwould increase training time.\nD Reliability of XEV AL\nIn this section, we want to discuss the reliability\nof XEV AL. XEV AL contains sentence-level data\nand we expect its translation not to be too challeng-\ning. Inspection by in-house bilingual speakers also\nconÔ¨Årms the high quality of translation. Human\ntranslation is always preferred but we are limited\nby budget and annotator resources (especially for\nlow-resource languages).\nE CMLM‚Äôs Comparison with Next\nSentence Prediction (NSP) and\nPotential Limitations.\nWe tried MLM (CC) with and w/o NSP and it does\nnot make much difference on SentEval. Training\nNSP accuracy quickly converge to 95%, indicating\nthat NSP is not a challenging task.\nSentence embedding methods like CMLM can\nbe less effective for sequence labeling (e.g., NER)\nand natural language generation (NLG) and ques-\ntion answering (Q&A).\nF Performance Variances\nWe provide the performance variances of CMLM\nbase and CMLM large on SentEval dataset in Ta-\nble 11.\n6228\nModel MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R Avg.\nCMLM MAX 82.8 88.9 96.2 89.2 87.81 89.8 72.1 82.1 83.7 85.8\nCMLM MEAN 83.6 89.9 96.2 89.3 88.5 91.0 69.7 82.3 83.4 86.0\nCMLM CLS 79.1 84.3 94.2 86.9 84.9 82.6 68.4 79.3 81.7 82.4\nBERT base MAX 79.6 85.5 94.6 87.3 83.0 90.0 65.6 75.5 78.1 82.1\nBERT base MEAN 81.6 87.4 95.2 87.8 85.8 90.6 71.1 79.3 80.5 84.3\nBERT base CLS 79.9 83.9 93.8 85.4 86.1 81.0 69.5 62.5 48.8 76.8\nTable 9: Performance of sentence representations model with different representations method (MAX, MEAN and\nCLS).\nMask Tokens MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R Avg.\n40 81.8 89.3 95.3 87.8 87.0 90.2 68.5 77.5 77.6 83.9\n60 83.7 89.5 95.8 88.9 88.0 90.3 68.7 79.5 82.8 85.4\n80 83.6 89.9 96.2 89.3 88.5 91.0 69.7 82.3 83.4 86.0\n100 83.2 89.5 95.5 88.7 88.0 90.8 70.0 81.5 82.7 85.6\nTable 10: Performance with different masking ratios in data (X-out-of-256) of CMLM base on SentEval.\nModel MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R\nCMLM base 83.6¬±0.2 89.9 ¬±0.4 96.2 ¬±0.1 89.3 ¬±0.3 88.5 ¬±0.2 91.0 ¬±0.8 69.7 ¬±0.6 82.3 ¬±0.3 83.4 ¬±0.4\nCMLM large 85.6¬±0.2 89.1 ¬±0.3 96.6 ¬±0.2 89.3 ¬±0.3 91.4 ¬±0.1 92.4 ¬±0.7 70.0 ¬±1.0 82.2 ¬±0.5 84.5 ¬±0.4\nTable 11: Performance variances of CMLM base and CMLM large on SentEval.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8402888178825378
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7553598880767822
    },
    {
      "name": "Sentence",
      "score": 0.7445339560508728
    },
    {
      "name": "Natural language processing",
      "score": 0.7367401123046875
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6694234609603882
    },
    {
      "name": "Language model",
      "score": 0.5566375255584717
    },
    {
      "name": "Representation (politics)",
      "score": 0.5562098622322083
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5380252599716187
    },
    {
      "name": "Inference",
      "score": 0.48586198687553406
    },
    {
      "name": "Natural language",
      "score": 0.4776538610458374
    },
    {
      "name": "Treebank",
      "score": 0.4611222445964813
    },
    {
      "name": "Language understanding",
      "score": 0.4212537407875061
    },
    {
      "name": "Machine learning",
      "score": 0.23608702421188354
    },
    {
      "name": "Parsing",
      "score": 0.16415536403656006
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}