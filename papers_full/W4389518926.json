{
  "title": "KAPALM: Knowledge grAPh enhAnced Language Models for Fake News Detection",
  "url": "https://openalex.org/W4389518926",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2010371457",
      "name": "Jing Ma",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2095964268",
      "name": "Chen Chen",
      "affiliations": [
        "Nankai University"
      ]
    },
    {
      "id": "https://openalex.org/A2099828825",
      "name": "Chunyan Hou",
      "affiliations": [
        "Tianjin University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106302639",
      "name": "Xiaojie Yuan",
      "affiliations": [
        "Nankai University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2890930935",
    "https://openalex.org/W3007083472",
    "https://openalex.org/W3176191710",
    "https://openalex.org/W2142869398",
    "https://openalex.org/W2577888896",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2950336186",
    "https://openalex.org/W3031781733",
    "https://openalex.org/W2053299703",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034020579",
    "https://openalex.org/W2911668186",
    "https://openalex.org/W3152907744",
    "https://openalex.org/W2742330194",
    "https://openalex.org/W3034808961",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W4300952844",
    "https://openalex.org/W3156800454",
    "https://openalex.org/W2790166049",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3199468579",
    "https://openalex.org/W4288079542",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W4287958162",
    "https://openalex.org/W2123142779",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2100341149",
    "https://openalex.org/W3201124874",
    "https://openalex.org/W3174306305",
    "https://openalex.org/W2032897813",
    "https://openalex.org/W2784476247",
    "https://openalex.org/W2084591134",
    "https://openalex.org/W2737907513"
  ],
  "abstract": "Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarse- and fine-grained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph and graph pooling layer. Finally, we integrate the coarse- and fine-grained entity knowledge representations with the textual representation for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines. In addition, it is competitive in the few-shot scenario.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3999–4009\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nKAPALM: Knowledge grAPh enhAnced Language Model for Fake News\nDetection\nJing Ma1,2 Chen Chen1,2∗ Chunyan Hou3 Xiaojie Yuan1,2\n1 College of Computer Science, Nankai University, Tianjin, China\n2 MoE Key Lab of DISSec, Nankai University, Tianjin, China\n3 School of CSE, Tianjin University of Technology, Tianjin, China\nmajing@mail.nankai.edu.cn, {nkchenchen,yuanxj}@nankai.edu.cn,\nhouchunyan@tjut.edu.cn\nAbstract\nSocial media has not only facilitated news con-\nsumption, but also led to the wide spread of\nfake news. Because news articles in social me-\ndia are usually condensed and full of knowl-\nedge entities, existing methods of fake news\ndetection use external entity knowledge to im-\nprove the effectiveness. However, the majority\nof these methods focus on news entity informa-\ntion and ignore the structured relation knowl-\nedge among news entities. To address this issue,\nin this work, we propose a Knowledge grAPh\nenhAnced Language Model (KAPALM) which\nis a novel model that fuses coarse- and fine-\ngrained representations of entity knowledge\nfrom Knowledge Graphs (KGs). Firstly, we\nidentify entities in news content and link them\nto entities in KGs. Then, a subgraph of KGs is\nextracted to provide structured relation knowl-\nedge of entities in KGs and fed into a graph\nneural network to obtain the coarse-grained\nknowledge representation. This subgraph is\npruned to provide fine-grained knowledge and\nfed into the attentive graph pooling layer. Fi-\nnally, we integrate the coarse- and fine-grained\nentity knowledge representations with the rep-\nresentation of news content for fake news de-\ntection. The experimental results on two bench-\nmark datasets show that our method is superior\nto state-of-the-art baselines in the full-scale set-\nting. In addition, our model is competitive in\nthe few-shot setting.\n1 Introduction\nIn recent years, with the development of the In-\nternet, social media such as Facebook and Twitter\nhave become the main platforms for people to con-\nsume news due to their real-time and easy access.\nHowever, social media is a double-edged sword and\nit enables people to be exposed to fake news. The\nwide spread of fake news can misguide public opin-\nion, threaten people’s health, and even cause dev-\nastating effects on society (V osoughi et al., 2018).\n∗Corresponding author.\nFigure 1: A news article on PolitiFact and its entity\ngraph.\nThus, the research on automatic fake news detec-\ntion is desirable.\nPrior researches are based on traditional machine\nlearning methods and hand-crafted features based\non news content to combat fake news on social\nmedia (Castillo et al., 2011;Kwon et al., 2013;Zu-\nbiaga et al., 2017;Przybyla, 2020;). To avoid man-\nual features, deep neural networks models such as\nConvolutional Neural Networks (CNN) and Recur-\nrent Neural Network (RNN), have been used to\nlearn the high-level feature representations auto-\nmatically and achieve great performance in detect-\ning fake news (Ma et al., 2016; Wang, 2017; Shu\net al., 2019). For the past few years, Pre-trained\nlanguage models (PLMs), such as BERT (Devlin\net al., 2018) and RoBERTa (Liu et al., 2019), have\nbeen the mainstream approaches which provide the\ncontextualized representation of textual content in\nnatural language processing field and also achieve\nthe competitive performance for fake news detec-\ntion (Pelrine et al., 2021; Sheng et al., 2021). How-\never, these works fail to consider the knowledge\nentities in news articles. Although news content is\ncondensed and full of knowledge entities by which\npeople usually verify the veracity of news content,\nPLMs are not effective in capturing the knowledge-\nlevel relationship between entities. As shown in the\nleft of Figure 1, it is hard to detect the veracity of\nnews article exactly without the entity knowledge\nabout Dennis Bray and Hans von Storch.\n3999\nLarge Knowledge Graphs (KGs), such as Free-\nbase (Bollacker et al., 2008) and Wikidata (Vran-\ndecic and Krötzsch, 2014), contain a large number\nof structured triplets, which can serve as the entity\nknowledge base for detecting fake news. Exist-\ning researches have demonstrated the significant\nrole of KGs (Pan et al., 2018; Dun et al., 2021;\nJiang et al., 2022). Specifically, Dun et al. (2021)\nproposed knowledge attention networks to mea-\nsure the importance of knowledge and incorporate\nboth semantic-level and knowledge-level represen-\ntations of news content. Jiang et al. (2022) pro-\nposed a knowledge prompt learning method which\nincorporated KGs into the prompt representation to\nmake it more expressive for verbal word prediction.\nHu et al. (Hu et al., 2021) explored the structural\nentity embedding and compared contextual entity\nrepresentations with the corresponding entity repre-\nsentations in the knowledge graph. However, they\nfocus on the textual representation of entities in\nKGs and have not explored the in-depth structured\nknowledge associated with the new article. As\nshown in Figure 1, we construct a subgraph of KGs\nfor the news article which is named as entity graph\nthroughout this paper, and this entity graph is help-\nful to detect fake news with the knowledge that\n“Dennis Bray is a biologist and Hans von Storch is\na climatologist and meteorologist”.\nIn this paper, we proposed a Knowledge grAPh\nenhAnced Language Model (KAPALM) which is\nenabled to explore both coarse- and fine-grained\nknowledge in KGs for fake news detection. Firstly,\nwe identify entities from news content and link\nthem to the large-scale KGs. An entity graph is con-\nstructed by extracting the first-order neighbors of\nidentified entities, and fed into a Graph Neural Net-\nwork (GNN) to obtain coarse-grained structured\nknowledge representations. Then, we prune the en-\ntity graph and feed it into the attentive graph pool-\ning layer for fine-grained knowledge representa-\ntions. Finally, we integrate coarse- and fine-grained\nknowledge representations with the representation\nof news content encoded by PLMs for fake news\ndetection. The primary contributions of this paper\ncan be concluded as follows:\n• We propose a method to construct an entity\ngraph for each news article. Our method is\nenabled to extract coarse-grained structured\nknowledge from the entity graph and provide\nfine-grained knowledge by the pruned entity\ngraph.\n• We propose a Knowledge grAPh en-\nhAnced Language Model (KAPALM) to\nintegrate the coarse- and fine-grained knowl-\nedge representations with the representation\nof news content for fake news detection.\n• We compare our model with the state-of-the-\nart methods on two benchmark datasets in\nfew-shot and full-scale settings. The exper-\nimental results demonstrate that our model\noutperforms the state-of-the-art methods in\nthe full-scale setting. Moreover, our model is\ncompetitive in the few-shot setting.\n2 Related Work\n2.1 Fake News Detection\nIn this section, we will briefly introduce the related\nwork on fake news detection. This work (Zhou\nand Zafarani, 2021) outlines early methods which\ndesign handcrafted features and utilize statistical\nmachine learning methods to detect the authentic-\nity of a given news. These work (Castillo et al.,\n2011;Przybyla, 2020;Kwon et al., 2013;Zubiaga\net al., 2017) proposed to utilize statistical infor-\nmation to leverage textual and social features to\ndetect fake news. Also, some works (Ajao et al.,\n2019;Zhang et al., 2021) focus on capturing senti-\nment features to better detect fake news. Next, we\nwill explore relevant research on the application of\ndeep learning techniques in the task of detecting\nfake news. With the development of deep learning\ntechniques, some models such as CNN, RNN, and\nTransformer(Kim, 2014;Liu et al., 2016;Vaswani\net al., 2017) are being used for fake news detec-\ntion(Dun et al., 2021;Ma et al., 2016;Samadi et al.,\n2021;Shu et al., 2019;Jiang et al., 2022). Dun\net al.(2021) proposed a approach which applied\na knowledge attention network to incorporate ex-\nternal knowledge based on Transformer. Jiang\net al.(2022) proposed KPL which incorporated\nprompt learning on the PLMs for the first time and\nenriched prompt template representations with en-\ntity knowledge and achieved state-of-the-art perfor-\nmance on two benchmark datasets, PolitiFact(Shu\net al., 2020) and Gossipcop(Shu et al., 2017).\n2.2 Knowledge Graphs\nExternal knowledge can provide necessary supple-\nmentary information for detecting fake news. It\nis stored in a knowledge graph format, which con-\ntains information about entities in the real world,\n4000\nFigure 2: The overview of our proposed model KAPALM\nsuch as people, places, etc. Nowadays, knowl-\nedge graphs are used in various natural language\nprocessing applications, such as news recommen-\ndation(Wang et al., 2018), fact verification(Zhong\net al., 2020;Zhou et al., 2019), and fake news de-\ntection(Dun et al., 2021;Jiang et al., 2022). Among\nthem, Dun et al.(2021) designed two attention\nmechanisms of news towards entities and news\ntowards entities and entity context to capture the\ndifferent importance of entities in detecting fake\nnews. Jiang et al.(2022) utilized a prompt template\nand integrated entities extracted from news articles\ninto the prompt representation for fake news detec-\ntion. Hu et al. (Hu et al., 2021) proposed a novel\ngraph neural model, which compared the news to\nthe knowledge graph through entities for fake news\ndetection. The limitations of the these methods are\nthat they all focus on news entity information and\nignore the structured information of KGs associ-\nated with the news article. Our proposed model can\nlearn knowledge from large-scale PLMs, acquire\nentity and topological structure information from\nexternal KGs, and improve the performance of fake\nnews detection.\n3 Model\nWe illustrate the framework of KAPALM in Figure\n2. The input of KAPALM consists of the news con-\ntent and the subgraph constructed from KGs. The\noutput of the model is the label for binary classifica-\ntion of fake news detection. First, for each piece of\nnews, we use a pre-trained language model with the\nadapter layer (Pfeiffer et al., 2020) to encode the\ntext of news content. Next, entity linking is used\nto identify entities in new content and we extract\nthese entities and their corresponding first-order\nneighbors in the knowledge graph to construct the\ncoarse-grained entity graph of the news. Entity ex-\ntraction and entity graph construction are described\nin the knowledge extraction section. Then, we feed\nthe constructed entity graph into a graph attention\nnetwork (Velickovic et al., 2017) to generate a new\nentity graph that incorporates both the knowledge\ngraph and entity information. We use the interac-\ntion node in the entity graph to represent coarse-\ngrained entity knowledge. Furthermore, to obtain\nfine-grained entity knowledge representation, we\nprune the constructed entity graph and then use an\nattention mechanism for graph pooling. Finally, we\nconcatenate the representations of the news content,\ncoarse- and fine-grained entity knowledge repre-\nsentation, and feed them into a fully connected\nneural network to obtain the prediction for fake\nnews detection.\n3.1 Text Encoder\nThis module is to generate the representation of the\nnews content. We utilize a pre-trained language\nmodel with Adapter (Pfeiffer et al., 2020) to en-\ncode the news content for capturing the semantic\ninformation contained in the news article. The pre-\ntrained language model is usually pre-trained on\na large amount of textual corpus, and is able to\ncapture the semantics of the news content. How-\never, when the pre-training task is not similar to the\ndownstream task, fine-tuning pre-trained language\nmodel is required to achieve the state-of-the-art\nperformance on the downstream task. In addition,\nthe large number of parameters in pre-trained lan-\nguage model can lead to the high time and space\n4001\ncosts during the full fine-tuning. Thus, adapter\nturning is used as a lightweight fine-tuning strat-\negy that achieves competitive performance to full\nfine-tuning on downstream tasks. Adapters add a\nsmall set of additional parameters at every layer of\nTransformer (Vaswani et al., 2017). The parame-\nters of pre-trained language model are kept frozen,\nand the parameters of adapters are trained during\nfine-tuning. The adapter layer can be represented\nas the following equation:\nh= h+ WupReLU(hWdown) (1)\nwhere Wup and Wdown are parameter matrixes.\n3.2 Knowledge Encoder\n3.2.1 Entity Graph Construction and Pruning\nThis module is used to extract relevant entities from\nthe knowledge graph and construct two subgraphs.\nFigure 3 illustrates the pipeline process which in-\ncludes entity linking, entity graph construction, and\nentity graph pruning. Firstly, we use entity linking\n(Milne and Witten, 2008;Sil and Yates, 2013) to\nextract the entities mentions from the news content,\nalign them with entities in the knowledge graph,\nand obtain their first-order neighbors in the knowl-\nedge graph. As a result, we obtain the entity set\nE = {e1,e2,...,e n}and its first-order neighbor\nset N = ⋃\nei∈E Nei , where Nei is the first-order\nneighbor set of ei. Secondly, we construct the en-\ntity graph G. In order to better aggregate entity\ngraph information, we create an interaction node\nthat is connected to everyei ∈E, and then connect\neach ei to its corresponding first-order neighbors\nin Nei .\nDue to the large number of entities in the con-\nstructed entity graph G, the information associ-\nated with relevant entities in news content cannot\nbe effectively captured. Therefore, we prune the\nentity graph to retain only the neighbors on one\npath of a pair of entities. In other words, we re-\nmove the first-order neighbors with a degree of\none in the entity graph. In this case, we obtain\nthe pruned entity graph G′, which consists of all\nentities in Eand their remained first-order neigh-\nbors N′= ⋃\nei∈E N′\nei , where N′\nei is remained first-\norder neighbors of ei.\n3.2.2 Coarse-grained Knowledge\nIn this module, we aim to obtain coarse-grained\nknowledge representations for entities in the entity\ngraph. After the construction of the entity graph,\nwe use PLMs to initialize the representations of\nnodes. While the representation of the interaction\nnode is initialized with the embedding of [CLS] to-\nken for the news content, the initialization of other\nentity nodes is provided by encoding the text of the\nentity name. As shown in the bottom of Figure 2,\nthe initialized entity graph is fed into a Graph Atten-\ntion Network (Velickovic et al., 2017) to aggregate\nthe information among all entities. The represen-\ntation of interaction node is enabled to integrate\nentity knowledge with the contextual representa-\ntion of news content. Because the entity graph\nincludes many noisy entities, we input only the\nrepresentation of interaction node which is named\nthe coarse-grained knowledge representation. The\nhidden representation of coarse-grained knowledge\nais calculated as follows:\na= Interaction(GAT(G)) (2)\nwhere G denotes the entity graph and function\nInteraction returns the representation of the inter-\naction node.\n3.2.3 Fine-grained Knowledge\nAfter pruning the entity graph, we feed the pruned\ngraph into the attentive graph pooling layer to\nextract the fine-grained entity knowledge repre-\nsentation. Entities in the entity graph, especially\nthose first-order neighbor entities, do not have the\nsame role in detecting the veracity of news articles.\nTherefore, we propose to utilize the attentive graph\npooling layer which is based on the multi-head at-\ntention mechanism to measure the importance of\ndifferent entities to fake news detection. The output\nof the attentive graph pooling layer is called the\nfine-grained knowledge representation.\nAttentive Graph Pooling Layer As shown in Fig-\nure 2, the query is the hidden state of the news\narticle encoded by PLMs, while both the key and\nvalue have the same representation which is de-\nrived from the hidden states of ei ∈E⋃N′where\nE and N′denote the entity set and the remained\nneighbour set respectively. Each entity is assigned\na corresponding weight by calculating the similar-\nity between the news and this entity. The attention\nformula is shown as follows:\nQ= WQh,K = WKc,V = WV c (3)\ns= Attention(Q,K,V ) = Softmax(QKT\n√dk\n)V\n(4)\n4002\nFigure 3: The process of knowledge extraction.\nwhere hand cdenote the hidden states of a news\narticle and an entity in E⋃N′respectively, and\nWQ, WK and WV are parameter matrices.\n3.3 Knowledge Fusion\n3.3.1 Deep Neural Network Classifier\nWe concatenate the representations of the news con-\ntent h, coarse-grained knowledge representation a,\nand fine-grained knowledge representation sto ob-\ntain the final representation z. Then, zis fed into a\nfully connected layer followed by a softmax func-\ntion to predict the probability distribution P of the\nnews article’s label.\nz= Concat(h,a,s ) (5)\nP = Softmax(Woz+ bo) (6)\nThe model is trained to minimize the following\ncross-entropy loss function.\nJ = −\n∑\ni∈T\nYilog(Pi) + λ\n2 ||Θ||2 (7)\nwhere T refers to the training dataset, Pi and Yi\ndenote the distributions of the prediction and true\nlabel of the sample i, λdenotes the coefficient of\nL2 regularization, and Θ denotes the model param-\neters.\n4 Experiments\n4.1 Datasets\nTo evaluate the proposed model, we conduct exper-\niments on two datasets PolitiFact and Gossipcop\nthat are included in a benchmark datasets called\nFakeNewsNet (Shu et al., 2020). The detail of\nthese news datasets are shown in Table 1. We study\nour proposed model in both few-shot and full-scale\nsettings.\nFew-shot settings For the purpose of replicating\nlow-resource situations in real-world scenarios, we\nrandomly select k∈(2, 4, 8, 16, 100) news articles\nas the training set and create the validation set of\nthe same size. The other news articles are used\nas the test set. We follow (Jiang et al., 2022), and\nsample the few-shot data by 10 random seeds and\nuse the average value calculated after deleting the\nmaximum and minimum scores as the final score.\nFull-scale settings For a dataset, we reserve 10%\nof the news articles as the validation set, and 5-\nfold cross validation is conducted on the other new\narticles. Finally, the average score is reported.\n4.2 Implementation Details\nTagMe (Ferragina and Scaiella, 2010) is adopted\nto extract knowledge mentions from news articles.\nWe utilize the BERT-base version with adapter\nas the pre-trained language model (Pfeiffer et al.,\n2020) to extract text features, which is based on\nthe HuggingFace Transformer Library (Wolf et al.,\n4003\nStatistics Politifact Gossipcop\n# True news 442 9714\n# Fake news 371 4415\n# Total news 813 14129\navg.# words/news 1449 576\navg.# entities/news 55 29\nTable 1: Statistics of the new datasets. \"#\" and \"avg.#\"\ndenote \"the number of\" and \"the average number of\".\n2020). For the coarse-grained entity knowledge,\nwe use GAT (Velickovic et al., 2017) as our graph\nneural network model. In the attentive graph pool-\ning layer, we set the attention head to 2. The size\nof hidden layer is set to 200 and the dropout rate to\n0.2 in the MLP layer. Adam is used to optimize the\nmodel’s parameters in the training, The learning\nrate is 1e-5 and the batch size is 10 for training\nmodel.\nBecause our aim focuses on detecting fake news,\nfake news articles are regarded as positive examples\nand F1-score (F1) is used as the evaluation metric\nto measure the classification performance.\n4.3 Baselines\nWe compare our proposed model with the follow-\ning baselines:\n(1) DTC (Castillo et al., 2011): DTC is the deci-\nsion tree model, which detects the authenticity\nof news by utilizing hand-crafted features.\n(2) RFC (Kwon et al., 2013): RFC is the random\nforest classifier based on hand-crafted features\nto detect whether news is true or fake.\n(3) SVM (Yang et al., 2012): SVM denotes a\nclassification model that uses a hyperplane to\nseparate news into true and fake news in high\ndimensional feature space.\n(4) TextCNN (Kim, 2014): TextCNN is a pop-\nular deep learning model for text classifica-\ntion, which applies convolutional filters with\nvarious window sizes to extract text features.\nThese features are fed into pooling layer to\nfurtherly capture the most salient features to\njudge whether news is true or fake.\n(5) BiLSTM (Bahad et al., 2019;Hochreiter and\nSchmidhuber, 1997): BiLSTM denotes the\nbidirectional long short-term memory which\nintroduces a two-directional recurrent netural\nnetwork architecture to better capture the tem-\nporal dependencies in text data. We utilize it\nto judege news’s authenticity.\n(6) KCNN (Wang et al., 2018): KCNN is a CNN-\nbased model which concatenates news em-\nbedding and knowledge entities embedding to\nlearn the representation of news.\n(7) KLSTM (Liu et al., 2016;Wang et al., 2018):\nSimilar to KCNN, KLSTM change the CNN\nmodule to BiLSTM, which has achieved\ncomptitive results in fake news detection task.\n(8) FB (Peters et al., 2018): FB denotes the\nfeature-based method of utilizing the pretrain\nlanguage model for feature extraction. BERT\nbase version is used in the experiments.\n(9) FFT (Devlin et al., 2018): FFT is the full\nfine-tune approach based on BERT base.\n(10) KAN (Dun et al., 2021): KAN is a knowledge-\naware attention network which incorporates\nexternal knowledge entities through attention\nmechanisms to predict the veracity of news\narticles.\n(11) KPL (Jiang et al., 2022): KPL is the state-of-\nart model which designs one prompt template\nand incorporates external knowledge entities\ninto the prompt representation.\n4.4 Experimental Results\nExperiments are conducted in both few-shot and\nfull-scale settings on two datasets. Baseline mod-\nels are divided into five categories: traditional sta-\ntistical methods (i.e., DTC, RFC, SVM), neural\nnetwork methods without external knowledge (i.e.,\nTextCNN, BiLSTM), neural network methods with\nexternal knowledge (i.e., KCNN, KLSTM, KAN),\npre-trained language model methods without ex-\nternal knowledge (i.e., FB, FFT), and pre-trained\nlanguage model methods with external knowledge\n(i.e., KPL).\nThe experimental results are presented in Table 2.\nWe draw some conclusions in the few-shot setting.\nFirst, although the performance is not stable when\nthe number of training data varies, we observe that\nthe increase of training data usually gives rise to\nthe improvement. Second, the deep learning meth-\nods without external knowledge are usually worse\nthan the statistical methods. The cause may be that\n4004\nData Method Few shot Full scale\n2 4 8 16 100\nPolitiFact\nDTC 43.16 50.89 57.31 55.42 74.58 73.44\nRFC 30.43 48.37 38.28 69.66 82.73 82.24\nSVM 24.63 47.20 51.13 60.91 84.90 86.29\nTextCNN 20.91 19.88 31.91 51.44 75.72 84.34\nBiLSTM 30.36 29.78 45.75 62.11 76.11 85.96\nKCNN 20.89 16.24 31.43 55.71 76.80 85.40\nKLSTM 29.43 20.44 34.96 65.48 76.60 86.41\nKAN 34.05 38.05 47.94 66.27 76.91 87.28\nFB 33.26 39.65 48.34 35.96 75.93 61.02\nFFT 38.88 20.77 47.04 39.83 84.40 87.35\nKPL 61.25 63.92 68.34 74.60 83.51 89.52\nKAPALM(Ours) 52.78 52.66 55.49 71.74 85.77 91.34\nGossipcop\nDTC 25.62 32.40 33.02 34.35 40.90 50.82\nRFC 33.88 30.49 34.04 37.93 49.28 43.73\nSVM 33.69 38.34 40.20 44.66 54.36 65.03\nTextCNN 23.68 21.22 19.69 16.84 36.26 66.82\nBiLSTM 26.06 30.76 37.58 33.36 41.43 66.78\nKCNN 23.31 19.75 19.12 15.84 38.26 66.51\nKLSTM 26.54 27.76 20.45 20.84 42.25 65.94\nKAN 29.30 31.98 33.56 35.94 43.57 67.35\nFB 32.96 37.16 34.40 33.23 52.98 58.81\nFFT 34.92 40.05 39.05 30.67 58.79 70.71\nKPL 37.80 38.78 40.20 41.63 51.72 69.20\nKAPALM(Ours) 42.26 44.58 44.69 44.51 60.03 71.68\nTable 2: Comparsion with existing models.These scores refer to the F1 scores (%) of fake news.\nData Method Full scale\nPolitiFact\nOurs 91.34\nw/o GP 90.05\nw/o IN 89.72\nGossipcop\nOurs 71.68\nw/o GP 70.99\nw/o IN 71.02\nTable 3: Ablation experimental results. “w/o GP” means\nremoving attentive graph pooling layer. “w/o -IN”\nmeans that we remove interaction node part.\ndeep learning methods have more parameters than\nstatistical methods and are easy to be over-fitted\nby the lack of training data. Third, neural net-\nwork methods with external knowledge are better\nthan those without external knowledge. The results\ndemonstrate that knowledge integration can allevi-\nate the over-fitting problem to some extent. Four,\nthe pre-trained language models can improve the\neffectiveness of fake news detection generally. Our\nmodel is better than KPL in most cases, but wrose\nData Method Full scale\nPolitiFact\nFB+Graph 87.27\nFFT+Graph 90.05\nOurs(AT+Graph) 91.34\nGossipcop\nFB+Graph 50.99\nFFT+Graph 71.53\nOurs(AT+Graph) 71.68\nTable 4: Comparison with different fine-tuning methods.\n“FB+Graph” means that training our model by keep-\ning parameters of pre-train language model frozen and\nupdating other parameters only. “FFT+Graph” means\ntraining our model by full fine-tuning all parameters of\nthe model. “AT+Graph” means training our model by\nadapter tuning just a small amount of additional param-\neters.\nthan KPL in the few-shot setting on the PolitiFact\ndataset.\nIn the full-scale setting, our method outperforms\nall baselines on two datasets and achieves the\nhighest F1 score. Specifically, KPL is the recent\nstate-of-art model that incorporates large-scale pre-\n4005\nData Method Full scale\nPolitiFact Fine-grained 90.88\nCoarse- and Fine-grained91.34\nGossipcop Fine-grained 71.48\nCoarse- and Fine-grained71.68\nTable 5: Ablation experimental results. : “Fine-grained”\ndenotes that the pruned entity graph are fed into both\nGAT layer and attentive graph pooling layer. “Coarse-\nand Fine-grained” means that the original entity graph\nis fed into GAT layer while the pruned entity graph is\nfed into attentive graph pooling layer.\ntrained models and external knowledge, and our\nmodel is better than KPL with +1.82 and +2.45\nimprovement on PolitiFact and Gossipcop datasets\nrespectively. Thus, our model can effectively adopt\nboth external knowledge and pre-training language\nmodel. In addition, neural network methods with\nexternal knowledge are superior to those without\nexternal knowledge. It suggest that knowledge can\nalso improve the effectiveness of fake news detec-\ntion when the training data is sufficient.\n4.5 Analysis\nWe conduct a detailed analysis from different per-\nspectives to demonstrate the effectiveness of the\nmodules in our model for fake news detection.\n4.5.1 Ablation study\nWe conduct the ablation study to validate the ef-\nfectiveness of the coarse- and fine-grained knowl-\nedge in our approach. Table 3 presents the ex-\nperimental results of the models on two datasets.\nThe coarse-grained knowledge is represented by\nthe interaction node in the entity graph while the\nfine-grained knowledge is provided by the attentive\ngraph pooling layer. If the attentive graph pooling\nlayer is eliminated from our model, the F1 drops\nby 1.34 on the PolitiFact dataset, and F1 decreases\nby 0.69 on the Gossipcop dataset. When the inter-\naction node is removed from our model, the per-\nformance of our model declines by 1.6 F1 on the\nPolitiFact dataset and by 0.66F1 on the Gossipcop\ndataset. Therefore, both the coarse-grained knowl-\nedge and fine-grained knowledge are important for\nour model and able to improve the effectiveness of\nfake news detection independently.\nTo validate the contribution of adapting tuning,\nwe conduct an ablation study experiment on two\ndatasets. The results listed in Table 4 demon-\nstrate the superiority of adapter tuning. When the\nadapter turning method is changed to Full Fine-\nTuning (FFT) or Feature-Based (FB), our model\nis worse than “FB+Graph” and “FFT+Graph” on\ntwo datasets. Our model can benefit from adapter\ntuning.\nWe conduct the ablation study to validate the ne-\ncessity of pruning the entity graph before feeding\ninto the attentive graph pooling layer. We feed the\npruned entity graph into both GAT layer and the\nattentive graph pooling layer. As Table 5 demon-\nstrated, the performance of using both the coarse-\nand fine-grained knowledge is better than just us-\ning fine-grained knowledge on two datasets. Thus,\ncoarse-grained knowledge representations are ben-\neficial to fake news detection.\n5 Conclusion\nIn this paper, we propose the Knowledge grAPh\nenhAnced Language Model (KAPALM) for fake\nnews detection. The proposed model integrates\ncoarse- and fine-grained representations of entity\nknowledge. Entity graph is used to enrich the\nknowledge representation of the news article. The\nentity graph is pruned and fed into the attentive\ngraph pooling layer to represent the fine-grained\nknowledge. The coarse- and fine-grained knowl-\nedge representations extracted from large-scale\nknowledge graph are combined for improving the\nfake news detection. Experimental result on two\nbenchmark datasets have shown that the proposed\nKAPALM outperforms the state-of-the-art baseline\nmodels. In addition, KAPALM is able to obtain the\ncompetitive performance in the few-shot setting.\nIn future work, we will investigate other effective\napproaches to mine the accurate knowledge from\nknowledge graph for fake news detection.\n6 Limitations\nOne limitation of our model is that the constructed\nentity knowledge graph fails to consider the mul-\ntiple types of relationships or extra attribute infor-\nmation of entities and relationships. If the veracity\nof a news article is associated with the attributes\nof the entities or the relationships among entities\nin this article, it may lead to the poor performance.\nIn addition, although graph neural networks and\nattentive graph pooling layers can improve the per-\nformance in fake news detection, there is still a lack\nof the Interpretability for our model.\n4006\nAcknowledgements\nThis work was partially supported by the NSFC-\nGeneral Technology Joint Fund for Basic Research\n(No. U1936206, U1936105), the National Natu-\nral Science Foundation of China (No. 62372252,\n62172237, 62077031, 62176028, 62302245), Min-\nistry of Education of the People’s Republic of\nChina Humanities and Social Sciences Youth Foun-\ndation (No. 63232114). We thank the AC, SPC,\nPC and reviewers for their insightful comments on\nthis paper.\nReferences\nOluwaseun Ajao, Deepayan Bhowmik, and Shahrzad\nZargari. 2019. Sentiment aware fake news detection\non online social networks. In IEEE International\nConference on Acoustics, Speech and Signal Process-\ning, ICASSP 2019, Brighton, United Kingdom, May\n12-17, 2019, pages 2507–2511. IEEE.\nPritika Bahad, Preeti Saxena, and Raj Kamal. 2019.\nFake news detection using bi-directional lstm-\nrecurrent neural network. Procedia Computer Sci-\nence, 165:74–82. 2nd International Conference on\nRecent Trends in Advanced Computing ICRTAC -\nDISRUP - TIV INNOV ATION , 2019 November 11-\n12, 2019.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIG-\nMOD international conference on Management of\ndata, pages 1247–1250.\nCarlos Castillo, Marcelo Mendoza, and Barbara Poblete.\n2011. Information credibility on twitter. In Proceed-\nings of the 20th International Conference on World\nWide Web, WWW 2011, Hyderabad, India, March 28\n- April 1, 2011, pages 675–684. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nYaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, and\nXiaojie Yuan. 2021. KAN: knowledge-aware atten-\ntion network for fake news detection. In Thirty-Fifth\nAAAI Conference on Artificial Intelligence, AAAI\n2021, Thirty-Third Conference on Innovative Ap-\nplications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Ar-\ntificial Intelligence, EAAI 2021, Virtual Event, Febru-\nary 2-9, 2021, pages 81–89. AAAI Press.\nPaolo Ferragina and Ugo Scaiella. 2010. TAGME:\non-the-fly annotation of short text fragments (by\nwikipedia entities). In Proceedings of the 19th ACM\nConference on Information and Knowledge Manage-\nment, CIKM 2010, Toronto, Ontario, Canada, Octo-\nber 26-30, 2010, pages 1625–1628. ACM.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9(8):1735–\n1780.\nLinmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong,\nDuyu Tang, Chuan Shi, Nan Duan, and Ming Zhou.\n2021. Compare to the knowledge: Graph neural fake\nnews detection with external knowledge. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 754–763.\nGongyao Jiang, Shuang Liu, Yu Zhao, Yueheng Sun,\nand Meishan Zhang. 2022. Fake news detection\nvia knowledgeable prompt learning. Inf. Process.\nManag., 59(5):103029.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Spe-\ncial Interest Group of the ACL , pages 1746–1751.\nACL.\nSejeong Kwon, Meeyoung Cha, Kyomin Jung, Wei\nChen, and Yajun Wang. 2013. Prominent features of\nrumor propagation in online social media. In 2013\nIEEE 13th International Conference on Data Min-\ning, Dallas, TX, USA, December 7-10, 2013, pages\n1103–1108. IEEE Computer Society.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.\nRecurrent neural network for text classification with\nmulti-task learning. In Proceedings of the Twenty-\nFifth International Joint Conference on Artificial In-\ntelligence, IJCAI 2016, New York, NY, USA, 9-15 July\n2016, pages 2873–2879. IJCAI/AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the 25th\nInternational Joint Conference on Artificial Intelli-\ngence, pages 3818–3824.\nDavid N. Milne and Ian H. Witten. 2008. Learning to\nlink with wikipedia. In Proceedings of the 17th ACM\nConference on Information and Knowledge Manage-\nment, CIKM 2008, Napa Valley, California, USA,\nOctober 26-30, 2008, pages 509–518. ACM.\nJeff Z Pan, Siyana Pavlova, Chenxi Li, Ningxi Li, Yang-\nmei Li, and Jinshuo Liu. 2018. Content based fake\n4007\nnews detection using knowledge graphs. In Proceed-\nings of The 17th International Semantic Web Confer-\nence, pages 669–683.\nKellin Pelrine, Jacob Danovitch, and Reihaneh Rabbany.\n2021. The surprising performance of simple base-\nlines for misinformation detection. In Proceedings\nof the Web Conference 2021, pages 3432–3441.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237. Association for\nComputational Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. Adapterhub: A\nframework for adapting transformers. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP 2020): Systems\nDemonstrations, pages 46–54, Online. Association\nfor Computational Linguistics.\nPiotr Przybyla. 2020. Capturing the style of fake news.\nIn The Thirty-Fourth Conference on Artificial Intelli-\ngence, pages 490–497. AAAI Press.\nMohammadreza Samadi, Maryam Mousavian, and\nSaeedeh Momtazi. 2021. Deep contextualized text\nrepresentation and learning for fake news detection.\nInf. Process. Manag., 58(6):102723.\nQiang Sheng, Xueyao Zhang, Juan Cao, and Lei Zhong.\n2021. Integrating pattern-and fact-based fake news\ndetection via model preference learning. In Proceed-\nings of the 30th ACM international conference on\ninformation & knowledge management, pages 1640–\n1650.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. defend: Explainable fake news\ndetection. In Proceedings of the 25th ACM SIGKDD\ninternational conference on knowledge discovery &\ndata mining, pages 395–405.\nKai Shu, Deepak Mahudeswaran, Suhang Wang, Dong-\nwon Lee, and Huan Liu. 2020. Fakenewsnet: A data\nrepository with news content, social context, and spa-\ntiotemporal information for studying fake news on\nsocial media. Big Data, 8(3):171–188.\nKai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and\nHuan Liu. 2017. Fake news detection on social me-\ndia: A data mining perspective. SIGKDD Explor.,\n19(1):22–36.\nAvirup Sil and Alexander Yates. 2013. Re-ranking for\njoint named-entity recognition and linking. In 22nd\nACM International Conference on Information and\nKnowledge Management, CIKM’13, San Francisco,\nCA, USA, October 27 - November 1, 2013 , pages\n2369–2374. ACM.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Ben-\ngio. 2017. Graph attention networks. CoRR,\nabs/1710.10903.\nSoroush V osoughi, Deb Roy, and Sinan Aral. 2018.\nThe spread of true and false news online. Science,\n359(6380):1146–1151.\nDenny Vrandecic and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nHongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi\nGuo. 2018. DKN: deep knowledge-aware network\nfor news recommendation. CoRR, abs/1801.08284.\nWilliam Yang Wang. 2017. “liar, liar pants on fire”: A\nnew benchmark dataset for fake news detection. In\nProceedings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 422–426.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nFan Yang, Yang Liu, Xiaohui Yu, and Min Yang. 2012.\nAutomatic detection of rumor on sina weibo. In Pro-\nceedings of the ACM SIGKDD Workshop on Mining\nData Semantics, MDS ’12, New York, NY , USA.\nAssociation for Computing Machinery.\nXueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei\nZhong, and Kai Shu. 2021. Mining dual emotion\nfor fake news detection. In WWW ’21: The Web\nConference 2021, Virtual Event / Ljubljana, Slovenia,\nApril 19-23, 2021, pages 3465–3476. ACM / IW3C2.\nWanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu,\nNan Duan, Ming Zhou, Jiahai Wang, and Jian Yin.\n2020. Reasoning over semantic-level graph for fact\nchecking. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 6170–6180.\nAssociation for Computational Linguistics.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: graph-based evidence aggregating and rea-\nsoning for fact verification. In Proceedings of the\n4008\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n892–901. Association for Computational Linguistics.\nXinyi Zhou and Reza Zafarani. 2021. A survey of\nfake news: Fundamental theories, detection methods,\nand opportunities. ACM Comput. Surv., 53(5):109:1–\n109:40.\nArkaitz Zubiaga, Maria Liakata, and Rob Procter. 2017.\nExploiting context for rumour detection in social me-\ndia. In Social Informatics - 9th International Confer-\nence, SocInfo 2017, Oxford, UK, September 13-15,\n2017, Proceedings, Part I, volume 10539 of Lecture\nNotes in Computer Science, pages 109–123. Springer.\n4009",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8091968297958374
    },
    {
      "name": "Knowledge graph",
      "score": 0.6200616359710693
    },
    {
      "name": "Graph",
      "score": 0.5417038798332214
    },
    {
      "name": "Social media",
      "score": 0.5403907299041748
    },
    {
      "name": "Information retrieval",
      "score": 0.45697295665740967
    },
    {
      "name": "Pooling",
      "score": 0.4434789717197418
    },
    {
      "name": "Focus (optics)",
      "score": 0.43969449400901794
    },
    {
      "name": "Language model",
      "score": 0.41576820611953735
    },
    {
      "name": "Natural language processing",
      "score": 0.35959669947624207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3259134888648987
    },
    {
      "name": "World Wide Web",
      "score": 0.19449210166931152
    },
    {
      "name": "Theoretical computer science",
      "score": 0.18637138605117798
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    }
  ]
}