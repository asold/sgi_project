{
  "title": "BAS: An Answer Selection Method Using BERT Language Model",
  "url": "https://openalex.org/W2985639757",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302722381",
      "name": "Mozafari, Jamshid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302722383",
      "name": "Fatemi, Afsaneh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2491778677",
      "name": "Nematbakhsh Mohammad Ali",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2972229821",
    "https://openalex.org/W2798835007",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2539338396",
    "https://openalex.org/W2538374209",
    "https://openalex.org/W2792643794",
    "https://openalex.org/W2736400315",
    "https://openalex.org/W2803969139",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2183366530",
    "https://openalex.org/W2119788759",
    "https://openalex.org/W2988869004",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2171590421",
    "https://openalex.org/W2463886519",
    "https://openalex.org/W2948917253",
    "https://openalex.org/W2250305120",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W1591825359",
    "https://openalex.org/W2964154091",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2618101654",
    "https://openalex.org/W2251427843",
    "https://openalex.org/W62188290",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W1869587854",
    "https://openalex.org/W2170872814",
    "https://openalex.org/W2131785201"
  ],
  "abstract": "In recent years, Question Answering systems have become more popular and widely used by users. Despite the increasing popularity of these systems, the their performance is not even sufficient for textual data and requires further research. These systems consist of several parts that one of them is the Answer Selection component. This component detects the most relevant answer from a list of candidate answers. The methods presented in previous researches have attempted to provide an independent model to undertake the answer-selection task. An independent model cannot comprehend the syntactic and semantic features of questions and answers with a small training dataset. To fill this gap, language models can be employed in implementing the answer selection part. This action enables the model to have a better understanding of the language in order to understand questions and answers better than previous works. In this research, we will present the \"BAS\" (BERT Answer Selection) that uses the BERT language model to comprehend language. The empirical results of applying the model on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that using a robust language model such as BERT can enhance the performance. Using a more robust classifier also enhances the effect of the language model on the answer selection component. The results demonstrate that language comprehension is an essential requirement in natural language processing tasks such as answer-selection.",
  "full_text": "An Answer Selection Method Using BERT Model  1 \n \nBAS: An Answer Selection Method Using BERT Language Model \nJamshid Mozafari1 , Afsaneh Fatemi*,1 , Mohammad Ali Nematbakhsh1 \n1 Faculty of Computer Engineering, University of Isfahan, Isfahan, Iran \nemails: mozafari.jamshid@eng.ui.ac.ir, a_fatemi@eng.ui.ac.ir, nematbakhsh@eng.ui.ac.ir \nAbstract \nIn recent years, Question Answering systems have become more popular and widely used by users. Despite the \nincreasing popularity of these systems,  their performance is not even sufficient for textual data and requires \nfurther research. These systems consist of several parts that one of them is the Answer Selection component.  \nThis component detects the most relevant answer from a list of candidate answers. The methods presented in \nprevious researches have attempted to provide an independent model to undertake the answer-selection task. An \nindependent model cannot comprehend the syntac tic and semantic features of  questions and answers with a \nsmall training dataset. To fill this gap, language models can be employed in  implementing the answer selection \npart. This action enables the model to have a better understanding of the language in order  to understand  \nquestions and answers better than previous works . In this research, we will present the â€˜BASâ€™ stands for BERT \nAnswer Selection that uses the BERT language model to comprehend language. The empirical results of \napplying the model on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that using a robust \nlanguage model such as BERT  can enhance the performance . Using a more robust classifier also enhances the \neffect of the language model on the answer selection component.  The resu lts demonstrate that language \ncomprehension is an essential requirement in natural language processing tasks such as answer-selection. \nKeywords: Question answering systems, Deep learning, Answer selection, Language modeling \n1. Introduction \nHumans have always sought to find answers to their questions.  Based on the type of question s they encounter , \nthey are looking for answer s (Kolomiyets & Moens, 2011) . For example, for the question â€˜Which image is the \nmost beautiful landscape?â€™, the answer is an image, or for the question â€˜Which is the sound of the sparrow? â€™, the \nanswer is audio.  However, it can be argued that the most common type of answer s is textual. In the past, the \nquestioner found many answers within the books.  This method had two significant problems. First, all books \nwere not readily available, and second, it took a long time to read the book and find the answer.  With the advent \nof the Internet, resource inaccessibility problem was primarily resolved (Brill, Dumais, & Banko, 2002) , but the \nsecond problem still remained. To overcome this problem, information retrieval systems and search engines \nhave been  developed. These systems receive a query from the user and return the documents containing the \nanswer (Manning, Raghavan, & SchÃ¼tze, 2008) . The user could find the  answer by going through these \ndocuments. The emergence of search engines was not a precise solution to the second problem because these \nsystems returned the documents and the questioner needed to go through each of the documents in order to find \nthe answer.  To overcome the second  problem, questio n answering systems were developed.  These systems , \ninstead of the whole document, return a word, phrase, or sentence as an exact answer. \nQuestion answering systems are of two general types , containing Knowledge -based systems and Information \nretrieval-based (IR-based) systems. Knowledge-based question answering systems can be considered as a huge \ngraph in which entities are linked through edges.  Edges also represent the meaning of the relationship between \nentities. This information is stored in a structured manner that is extracted from the graph using q uery languages \nsuch as SPARQL (Perez, Arenas, & Gutierrez, 2009) . The benefits of these syste ms include the exact answer \nwhich the system returns  because it does not require text analysis , and the answer is produced through the \ninformation stored in the graph.  One of the drawbacks of these systems is related to the production of \nknowledge graphs, as producing and implementing this huge graph is by no means an easy task and can be very \ntime-consuming. IR-based question answering systems do not require this huge knowledge graph and attempt to \nextract the answer from raw texts. These systems attempt to provide a textual answer to the asked question using \nreading comprehension. The advantages and disadvantages of these systems are different than knowledge-based \n                                                           \n* Corresponding Author \nAn Answer Selection Method Using BERT Model  2 \n \nsystems. Nowadays, researchers have been focusing on thes e systems because such systems do not need to \ncreate knowledge graphs ; instead, they use raw text.  IR-based question answering systems consist of four \ndifferent parts, including Question Analysis, Document Retrieval, Answer Selection, and Answer Extraction \n(Jurafsky & Martin, 2014) . Figure 1 shows the general pipeline architecture of the Information retrieval -based \nsystems. \n \nFig 1. The general pipeline architecture of Information retrieval-based question answering systems (Sequiera et al., 2017) \nThe question analysis part receives the user question and tries to detect the answer type and generates a query \nfor the document retrieval section.  This query is passed  to the document retrieval part, and documents that are \nmore relevant to the query are retrieved, and then some of the most relevant passages are selected as retrieved \npassages. The answer selection part  selects the most relevant from candidate sentences as the most relevant  \nanswer to the question.  The answer selection part is for two genera l types: the extracted Answer and the \ngenerated Answer. In the extracted answer type, the answer is extracted from the passage  and is passed  to the \nanswer extraction part without any changes. In the generated answer type, the answer sentence does not exist  in \nthe passage and is produced based on  linguistic rules.  Finally, the answer extraction part extracts the exact \nanswer from the relevant answer sentence and returns it as the final exact answer. However, in some researches, \nthe answer extraction part is combined with the answer selection part, and even some researches omit the \nanswer extraction part and return the final answer as a sentence (Jurafsky & Martin, 2014; Mishra & Jain, 2016). \nIn the question answering domain, questions are divided into five general categories containing list type \nquestions, hypothetical questions, causal questions, confirmation questions, and factoid questions (Mishra & \nJain, 2016) . Factoid questions are questions whose answer is a piece of a document, passage or sentence.  In \nother words, the answer to the question exists in the text. \nOne of the most fundamental issues in natural language processing is the similarity measurement.  This issue \ntries to identify two sentences, two paragraphs or two documents that have more similarities in terms of \nsemantics and syntax. An essential application of similarity measurement is paraphrase recognition, which tries \nto identify two sentences th at have the same meaning but are syntactically different (Magnolini, 2014). One of \nthe factoid questionsâ€™ features is the semantic similarity of an answer with its question.  This feature makes it \npossible to use similarity measurement for finding answers of factoid questions in question answering systems \nwhich use the extracted answer type i n answer selection part. The similarity measurement issue is a supervised \nclassification problem.  In other words, a trained classifier can predict the simi larity of two sentences . \nConsequently, such a method can be used to respond to  factoid questions.  The answer-selection task can be \nexpressed as: if q = {q 1, q2,â€¦, qn} is a set of questions, for each question q i, there is a candidate set of answers \n{(si1, yi1), (si2, yi2), ..., (sim, yim)} where sij refers to the jth candidate answer for qi. yij also refers to the correctness \nof the answer, as if y ij = 1, the answer is correct and if y ij = 0, the answer is incorrect.  If a training dataset exists \nthat includes such information, we can train a classifier that can find the most relevant answer to factoid \nquestions using semantic and syntactic similarities  (Echihabi & Marcu, 2003; Yih, Chang, Meek, & Pa stusiak, \n2013). A question with three candidate answers is shown in Table 1. \n \n\nAn Answer Selection Method Using BERT Model  3 \n \nTable 1: A factoid question with three candidate answers. \nWho is the telephone inventor? Q \n= 1 11y The first telephone was invented by Alexander Graham Bell. 1a \n= 1 12y In 1875, Alexander Graham Bell succeeded in presenting the first telephone to human society. 2a \n= 0 13y The first telephone was invented in 1875. 3a \n \nUntil now, various methods have been proposed to undertake both answer-selection and the similarity \nmeasurement tasks. These methods can be divided into two general categories.  The first category is rule-based \nthat attempted to measure the similarity between two sentences based on linguistic rules.  The second category is \nthe methods which use machine learning algorithms. In these methods, models try to learn linguistic rules \nautomatically. In the second category, feature engineering was initially used, but in recent years, deep learning \nmethods have become more popular, and most researchers have used it instead of feature engineering. \nThe deep learning -based models initially attempted to overcome the problem independently and did not use \nother NLP tasks such as natural language inference (Khot, Sabharwal, & Clark, 2018) , paraphrase identification \n(Liu, He, Chen, & Gao, 2019) , language modeling (Devlin, Chang, Lee, & Toutanova, 2019)  and so on. \nHowever, it was revealed that  answer selection methods which w ere combined with the other NLP tasks c ould \nprovide more accurate models. In 2019, one of t he topics investigated is the combination of language models \nwith answer selection methods. In  methods that have used language models, less attention has been paid to the \ninfluence of the language model and attempt s to analyze the answer selection componen t attached to the \nlanguage model (Yoon, Dernoncourt, Kim, Bui, & Jung, 2019) . In this paper, we show that if a more robust \nlanguage model is used, it is not required to use other tasks of NLP  in answer selection task . We propose a \nmodel using a preprocessing section  and various neural network s stacked on the BERT model  (Devlin et al., \n2019) that finds the most relevant answer to a factoid question from candidate answers. The empirical results \ndemonstrate the superiority of our proposed model, which achieve state -of-the-art performance for TrecQA raw \n(M. Wang, Smith, & Mitamura, 2007) , TrecQA clean (Z. Wang & Ittycheriah, 2015)  and WikiQA (Y. Yang, \nYih, & Meek, 2015) datasets. \nIn this research, we endeavour to solve the following three research questions. Our experiments also aim to \nsolve these questions: \nï‚· Can the BAS model outperform the baseline models? \nï‚· Does the preprocessing have a significant effect on the performance of the BAS model? \nï‚· How do different classifiers affect the performance of the BAS model? \nThe contribution of this research paper includes: \nï‚· We propose the BAS (BERT Answer Selection model) that ranks the candidate answers in terms of \nsemantic and syntactic similarity, using language models. \nï‚· The preprocessing  increases the importance of EAT -type (Expected Answer Type)  entities to find \ncorrect answers. \nï‚· The BERT language model is used to captur e the meaning of input sentences better than ordinary \nneural networks. \nï‚· The MAP and MRR measures of the BAS model show that it performs better than state of the art. \nIn section 2 , related works w ill be explained. In section 3 , the proposed model will be de scribed in detail. In \nsection 4 , the baseline models, the datasets, and implementation details will be presented. In section 5, the \nproposed model will be evaluated, and the results of the experiments will be discussed. Finally, the paper will be \nconcluded in section 6. \n2. Related Works \nThis section consists of two part s: in the first, we discuss the answer selection related works, and in second, the \nBERT language model will be briefly examined. \n \n \nAn Answer Selection Method Using BERT Model  4 \n \n2.1 Answer Selection \nResearch history in the answer selection field can be divided into three different parts: the first includes the \nworks that used lexical features , the second includes those that used feature engineering techniques , and the \nmost recent part includes researches that used deep learning and deep neural networks. \n2.1.1 Feature Engineering \nThe research es presented in the first period used the question and answer overlap ; that is, the most relevant  \nanswer was selected based on the common words between the two sentences.  During this period, researches \nused bag-of-words and bag -of-grams methods (Wan, Dras, Dale, & Paris, 2006) . Some methods also used the \nweighted bag -of-words. For example, the question  and candidate answers  presented in Table 1 , indicate that \nusing these methods is not sensible  (Surdeanu, Ciaramita, & Zaragoza, 2008) . The weakness of these methods \nwas to not use semanti c and linguistic features of  sentences (Mozafari, Nematbakhsh, & Fatemi, 2019) . That's \nwhy some studies used lexical resources such as WordNet (Miller, 1998) to overcome the semantic problem, but \nthese researches failed to remove language constraints because some words were not mentioned in these lexical \nresources (Tu, 2018). \nThe researches presented in the second period attempted to use feature engineering.  Some researches used \nsyntactic and semantic structures of  sentences. For example, Punyakanok (Punyakanok, Roth, & Yih, 2004)  \nused the dependency tree of the sentences. Other researches developed more robust models for answer -selection \ntask using dependency tree methods  and tree edit distance algorithms (Heilman & Smith, 2010; M. Wang & \nManning, 2010; Yao, Durme, Callison -Burch, & Clark, 2013) . Yih et al. (Yih et al., 2013)  show the use of \nexternal tools such as WordNet and N amed entity recognition (NER) (Jurafsky & Martin, 2014)  with \ndependency trees, caused that  semantic features were more employe d. Finally, Severyn et al. (Severyn & \nMoschitti, 2013)  presented a framework that performed feature engineering automatically and attempted to \neliminate feature engineering problems to some extent . This framework can be considered one of the first \nattempts to eliminate feature engineering. \nNevertheless, the third period can be ca lled the best period for answer -selection task and question answering \nsystems because the speed of enhancing the performance of the models presented in this period is far fast than \nthe preceding ones. This period, also called the artificial intelligence explosion, owes to the emergence of deep \nneural networks and deep learning.  The models presented in this period uti lize deep neural networks, which \neliminates the need for feature engineering.  These models need substantial training data.  This need is a  \nsignificant challenge and is resolved hardly. Due to the vast number of researches in this period, t he researches \nare divided into five different categories, including Siamese -based, Attention-based, Compare-Aggregate-based, \nLanguage model-based, and specific methods. Each of these categories will be explained below. \n2.1.2 Siamese-based models \nThe proposed Siamese -based models are models that follow the structure of the Siamese network (Bromley et \nal., 1993)  and process  questions and answer s independently and provide a vector representation for each \nsentence. In these models, the information of the other sentence is not employed  during the processing of each \nsentence (Lai, Bui, & Li, 2018). \nYu et al. (Yu, Hermann, Blunsom, & Pulman, 2014)  presented the first model which  used the deep neural \nnetwork to overcome the answer -selection task. This model selects the most relevant answer from  candidate \nanswers using a convolutional neural network and logistic regression.  Feng et al. (Feng, Xiang, Glass, Wang, & \nZhou, 2015)  used the model presented by Yu et al.  They attempted to prod uce various  models that were \nproduced by combining deep  neural networks and fully -connected networks. In these models, various types of \nhidden layers, convolution operations, pooling and activation functions were used.  However, these models were \nindependent and were evaluated separately.  In this regard, He  et al. (He, Gimpel, & Lin, 2015)  developed a \nmodel that combined various models and produced a single model. They tried to produce a vector representation \nfor each sentence. These vectors resulted from  the processing o f various models. The ranking method of \nprevious models was a pointwise ranking, but Rao et al. (Rao, He, & Lin, 2016) showed that in case of using the \npairwise rank ing, the performance of the model is enhanced instead. In this research, a model was presented \nwhich converted each pointwise model into a pairwise model.  In this regard, the model presented by He et al. \n(He et al., 2015) was given as a pointwise model to the Rao et al. model, which enhanced the performance of the \nmodel. Madabushi et al. (Tayyar Madabushi, Lee, & Barnden,  2018) provided a pre-processing operation rather \nAn Answer Selection Method Using BERT Model  5 \n \nthan enhancing previous models. In their research, the named entities in candidate answers that are equivalent to \nthe answer type announced by the question processing part, are replaced with a special to ken, which makes it \neasier for models to find the most relevant answer. This preprocessing was applied to the model presented by \nRao et al. (Rao et al., 2016)  and confirmed its effectiveness. The problem was to replace all the tokens with a \nunique one. In this regard, Kamath et al. (Kamath, Grau, & Ma, 2019) , instead of replacing all named entities \nwith a unique token, replaced each named entity with a special token. However, unlike Madabushi et al. (Tayyar \nMadabushi et al., 2018), they did not apply the idea to one of the previous models; instead, they presented a new \nmodel with recurrent neural networks. \n2.1.3 Attention-based models \nThe proposed attention -based models are those that, unlike Siamese -based models, use context -sensitive \ninteractions between sentences. In these models, the attention mechanism (Bahdanau, Cho, & Bengio, 2015)  is \nused. The attention mechanism was first used in machine translation researches but later in other fields of \nnatural language processing such a s question answer ing and answer -selection task (Lai et al., 2018)  was also \nused. \nYang et al. (L. Yang, Ai, Guo, & Croft, 2016)  presented one of the first models which  used the attention \nmechanism for the answer-selection task. In this research, the attention mechanism, proposed by Bahdanau et al. \n(Bahdanau et al., 2015)  and implemented with recu rrent neural networks, was used to overcome the answer -\nselection task. The first attention mechanism was presented only for recurrent neural networks, but He et al. (He, \nWieting, Gimpel, Rao, & Lin, 2016)  could provide a model for answer -selection task which used the attention \nmechanism in convolutional neural networks.  This research showed  that the combination of the attention \nmechanism with convolutional neural networks is more efficient than the combination of the attention \nmechanism with recurrent neural networks.  Mozafari et al. (Mozafari et al., 2019)  showed that using feature \nvectors, convolutional neural networks and pairwise ranking algorithms in the model presented by He et al. (He \net al., 2015) can provide a more robust model. \n2.1.4 Compare-Aggregate-based models \nThe proposed compare -aggregate models are models that focus on context -interaction between sentences more \nthan attention-based models.  These models first compare smaller units of sentences such as words to capture \nmore information.  Then, they aggregate the  information obtained from the comparison between words and \npresent a vector representation for each sentence (Lai et al., 2018). \nHe et al. (He & Lin, 2016)  presented one of the firs t models to overcome the answer -selection task using  \ncompare-aggregate method s. Instead of converting the input sentences into a vector representation and \nmeasuring the similarity of the two sentences using the vectors, they compared the word vectors to each other \nand produced the  vector representations of each input senten ce by aggregating these values.  Wang et al. (S. \nWang & Jiang, 2017)  used the idea of He et al. (He & Lin, 2016)  and presented a general â€˜compare-aggregateâ€™ \nframework which provided excellent performance for the answer -selection task. Wang et al. (Z. Wang, Hamza, \n& Florian, 2017) developed this framework and showed that if two sentences are matched in two directions, and \ninstead of word-by-word matching, each word is matched with all the components of the other sentence, a more \nrobust model is presented. Bian et al . (Bian, Li, Yang, Chen, & Lin, 2017)  used dynamic-clip technique rather \nthan a simple attention mechanism in the â€˜compare-aggregateâ€™ framework and showed that this modification \neliminates ineffective information and  provide a more robust vector representation. Shen et al. (G. Shen, Yang, \n& Deng, 2017)  introduced an inter-weight layer and tried to set a weight to each word.  Tran et al. (Tran et al., \n2018) inspired by the Additive Recurrent Neural Network (Lee, Levy, & Zettlemoyer, 2017) , introduced a new \nrecurrent neural network which understood input text content more than previous models.  \n2.1.5 Language Model-based models \nThe proposed language model-based models are models that instead of overcoming the answer -selection task  \nfrom scratch, use pre -trained language models that have a complete understanding of the language.  These \nmodels used the pre -trained language models to overcome the answer -selection task in a similar way  proposed \nby Howard et al. (Howard & Ruder, 2018). \nAn Answer Selection Method Using BERT Model  6 \n \nYoon et al. (Yoon et al., 2019)  developed a model which used language models for answer -selection task. This \nmodel used the ELMo language model (Peters et al., 2018) along with techniques such as Latent -Clustering and \ndemonstrated that the combination of these components produced a robust model.  \n2.1.6 Special models \nSome models were not in line with earlier models and tried to provide a n independent model. These researches \ntried to create a new path in the answer selection field. However, it did not get much attention. \nWang et al. (Z. Wang, Mi, & Ittycheriah, 2016)  utilized dissimilar components of the input sentences alongside \nsimilar components. They believed th at dissimilar components were  crucial as much as similar components  in \nidentifying the semantic similarity of sentences . Shen et al. (Y. Shen et al., 2018)  developed the KABLSTM \nmodel, which utilizes knowledge graphs. They developed a context-knowledge interactive learning architecture, \nwhich used interactive information from input sentences and knowledge  graph. Yang et al. (R. Yang, Zhang, \nGao, Ji, & Chen, 2019)  presented the RE2 model which attempted to provide a lightweight model with \nsatisfactory performance. The model's name stands for Residual vectors, Embedding vectors and Encoded \nvectors. In Table 2, the related works for various datasets are shown. \nTable 2: Related works according to their characteristics. \nReference Architecture MAP \nTrecQA \nRaw \nMRR \nTrecQA \nRaw \nMAP \nTrecQA \nClean \nMRR \nTrecQA \nClean \nMAP \nWikiQA \nMRR \nWikiQA \n(Punyakanok et al., 2004) Feature Engineering 0.419 0.494 - - - - \n(Heilman & Smith, 2010) Feature Engineering 0.609 0.692 - - - - \n(M. Wang & Manning, \n2010) \nFeature Engineering 0.595 0.695 - - - - \n(Yao et al., 2013) Feature Engineering 0.631 0.748 - - - - \n(Yih et al., 2013) Feature Engineering 0.709 0.770 - - - - \n(Severyn & Moschitti, \n2013) \nFeature Engineering 0.678 0.736 - - - - \n(Yu et al., 2014) Siamese 0.711 0.785 - - - - \n(Feng et al., 2015) Siamese 0.711 0.800 - - - - \n(He et al., 2015) Siamese 0.762 0.830 0.777 0.836 - - \n(Rao et al., 2016) Siamese 0.780 0.834 0.801 0.877 0.709 0.723 \n(Tayyar Madabushi et \nal., 2018) \nSiamese 0.836 0.862 0.864 0.903 - - \n(Kamath et al., 2019) Siamese 0.850 0.892 - - 0.689 0.709 \n(L. Yang et al., 2016) Attention 0.750 0.811 - - - - \n(Mozafari et al., 2019) Attention 0.806 0.852 - - - - \n(He & Lin, 2016) Compare Aggregate 0.758 0.821 - - 0.709 0.723 \n(S. Wang & Jiang, 2017) Compare Aggregate - - - - 0.743 0.754 \n(Z. Wang et al., 2017) Compare Aggregate - - 0.801 0.877 0.743 0.755 \n(Bian et al., 2017) Compare Aggregate - - 0.821 0.899 0.754 0.764 \n(G. Shen et al., 2017) Compare Aggregate - - 0.822 0.889 0.733 0.750 \n(Tran et al., 2018) Compare Aggregate - - 0.829 0.875 - - \n(Yoon et al., 2019) Language Model - - 0.868 0.928 0.764 0.784 \n(Z. Wang et al., 2016) Special - - 0.771 0.845 0.705 0.722 \n(Y. Shen et al., 2018) Special 0.792 0.844 0.803 0.884 0.732 0.749 \n(R. Yang et al., 2019) Special - - - - 0.745 0.761 \n \n2.2 BERT language model \nWith the advent of deep learning , one of the issues which has received much attention in recent years  is the \ndevelopment of models that attempt to comprehend languages (Peters et al., 2018) . These researches present a \nmodel that learns the syntactic and semantic rules of language in a variety of methods, such as next word \nprediction, next sentence predict ion, masked word prediction  (Devlin et al., 2019) . In other words, this model \nlearns a language and can produce new texts with correct syntax and semantic rules.  One of the novel language \nmodels which can overcome all other language models is the BERT model (Devlin et al., 2019). This model has \ntaken advantage of the idea presented in Transformers (Vaswani et al., 2017) , which is now widely used in the \nnatural language processing community. The BERT model will be described in more detail below. \n \n \nAn Answer Selection Method Using BERT Model  7 \n \n2.2.1 Transformer \nOne of the architectures which that for machine translation s is the encoder -decoder architecture (Sutskever, \nVinyals, & Le, 2014). Since then, this architecture has been used as one of the most widely used architectures in \nmachine translation s. Based on this architecture, the Transformer was introduced (Vaswani et al., 2017)  in \nwhich a self -attention technique was used instead of using a recurrent neural network in the encoder and \ndecoder. The method used by the Transformer went beyond machine translations and was employed in various \nnatural language processing tasks.  One of these tasks is language model s such as BERT language model, which \nuses the transformer encoder component to implement the language model.  Figure 2 shows the transform er \nencoder architecture. \n \nFig 2: Transformer encoder which consists of self-attention heads and fully connected neural networks. This encoder \nmodifies the representation of each token to suit the contents of the other tokens and presents a new representation. Each \nself-attention head discovers a new semantic relation between various tokens and converts it into a new vector similar to  \ninput vectors using a fully connected neural network. \nThe first step of the Tra nsformer encoder is Self Attention. In this step, three vectors are created for each input \nvector X named Query, Key, and Value. The learned matrices WQiïƒâ„|X|Ã—|Qi|, WKiïƒâ„|X|Ã—|Ki|, and WViïƒâ„|X|Ã—|Vi| are \nemployed to produce Query, Key, and Value vectors for ith self-attention respectively: \nğ‘„ğ‘– =ğ‘‹Ã—ğ‘Šğ‘„ğ‘–                                                                              (1 \nğ¾ğ‘– =ğ‘‹Ã—ğ‘Šğ¾ğ‘–                                                                              (2 \nğ‘‰ğ‘– =ğ‘‹Ã—ğ‘Šğ‘‰ğ‘–                                                                              (3 \nThe output vector of ith self-attention which is generated as follows: \nğ‘ğ‘– =ğœ(\nğ‘„ğ‘–Ã—ğ¾ğ‘–ğ‘‡\nâˆš|ğ¾ğ‘–| )Ã—ğ‘‰ğ‘–                                                                           (4 \nAccording to Figure 2, there are | S| self-attention in the Tra nsformer encoder. By concatenating  the outputs of \nself attentions, the Z1..|S| vector is generated. This vector is multiplied by the learned matrix W Oïƒâ„|Z1..|S||Ã—|H| and Z \nvector is produced: \nğ‘=ğ‘1..|ğ‘†|Ã—ğ‘Šğ‘‚                                                                            (5 \nFinally, the Z vector is transferred to a fully connected layer and a new vector X new is produced. WFïƒâ„|H|Ã—|X| is a \nmatrix that is equivalent to the hidden layer parameters, and bFïƒâ„|X| is a vector that is equivalent to the bias: \nğ‘‹ğ‘›ğ‘’ğ‘¤ =ğ‘Ã—ğ‘Šğ¹ +ğ‘ğ¹                                                                        (6 \n2.2.2 BERT Model \nThe BERT language model consists of several transformer encoders stacked together.  Two general types are \ndefined based on the number of stacked encoders (L), the hidden layer size (H), and the number of self -attention \n\nAn Answer Selection Method Using BERT Model  8 \n \nheads (A).  These two general types include the BERT-base and the BERT-large. The characteri zes of these \nmodels are shown below: \nï‚· BERT-base: L:12, H: 768, A: 12, Training parameters: 110 M \nï‚· BERT-large: L:24, H: 1024, A: 16, Training parameters: 340 M \n \n2.2.3 Fine-Tuning \nFine-tuning is to train models already trained for a particular  task in order to be used for another task.  The fine-\ntuning is used when the captured knowledge by another model need s to be used for another task.  In addition to \nthe language model presented in BERT (Devlin et al., 2019) , fine-tuning has also been performed for different \ntasks. These tasks include Sentence Pair Classification Tasks, Single Sentence Classification Tasks, Question \nAnswering Tasks, and Single Sentence Tagging Tasks.  This paper demonstrates that the BERT language model \nhas a high comprehension of language because , in most other  tasks, it performs better than other models \n(Devlin et al., 2019). \n \nFig 3: BERT language model fine-tuned for various other tasks (Devlin et al., 2019). \n3. Proposed Model Architecture \nIn this paper, we present the BERT Answer Selection (BAS) Model which consists of three various sections, \nincluding preprocessing, language model, and classifier. The first section processes the input sentences and \npasses the processed sentences to the next section.  The second section passes the processed sentences to the pre -\ntrained language model and passes the vectors which capture the meaning of the sentences to  the next section.  \nThe third section uses these vectors and performs classification. Figure 4 shows the BAS model architecture. \n\nAn Answer Selection Method Using BERT Model  9 \n \nFig 4: BERT Answer Selection Model Architecture. The green sections are trainable and the orange section is non-trainable. \nAs mentioned above, the BAS model consists of three sections: preprocessing, language model, and classifier.  \nThe preprocessing receives a question sentence and an answer sentence  as input. In this section, the question is \nfirst given to the Expected Answer Type (EAT) Detector. This component detects the answer type of question \nand passes it to the Highlighter and annotates the question.  The Highlighter component replaces all the named \nentities whose type is the EAT, with a special token. The processed question and processed answer sentences are \nthen passed to the language model section.  This section uses the BERT language model (Devlin et al., 2019) . \nThe question and answer are  tokenized and transformed into the appropriate template for the BERT model.  In \nthis section, instead of using the BERT language model, the Sentence Pair Classification model is used.  This \nmodel is known as BertForSequenceClassification.  The BERT model pro cesses the inputs an d, for each token, \noutputs a new vector representation that captures more information from other tokens. These vectors are passed  \nas input to the classifier section.  In this section, various types of classification are employed containi ng \nclassification with the fully connected neural network, classification with bag -of-words, classification with a \nconvolutional neural netw ork, and classification with  a recurrent neural network.  We will explain each section \nin detail below. \n3.1 Preprocessing \nIn factoid questions, the exact answer to a  question is a word which is appeared in the answer  sentence. For \nexample, the answer to the question â€˜Who is the telephone inventor?â€™ is a sentence referring to â€˜Alexander \nGraham Bellâ€™. For example, the sentences â€˜The first telephone was invented by Alexander Graham Bell â€™ and â€˜In \n1875, Alexander Graham Bell succeeded in presenting the first telephone to human society â€™ are both correct \nanswers to this question. However, the exact answer is a human name. In other words, the exact answer to the \nquestion is â€˜Alexander Graham Bell â€™. For better understand ing, for example, the answer â€˜The first telephone \nwas invented in 1875â€™ is not a correct answer to the question because Alexander Graham Bell is not mentioned. \nAs a result, a correct answer must contain a human  name. Mor e generally, the correctness probability of a \ncandidate answer  which contains named entities whose type is EAT is more than other candidates (Tayyar \nMadabushi et al., 2018) . Earlier question answering systems process questions and answers without any \npreprocessing. In these systems, there is no guarantee that the system can automatica lly detect the answer type  \n\nAn Answer Selection Method Using BERT Model  10 \n \nand selects sentences containing EAT. Madabushi et al. (Tayyar Madabushi et al., 2018)  proposed a solution to \nthis problem. They t old that  each candidate answer was processed separately, and if the candidate answer \nincluded EAT, replaced it with a special token. This action causes the system learns that assigns more likelihood \nto the sentences which contain  the special token . As a result, the system can rank the candidate answers  better. \nThis idea is also used in the BAS model. To perform this, two components are needed: â€˜Expected Answer Type \nDetectorâ€™ and â€˜Highlighterâ€™. Each of these components will be explained below. \n3.1.1 Expected Answer Type Detector \nThis component detects  the answer  type of  questions. To perform this, we use the application programming \ninterface (API) provided by Madabushi et al. (Tayyar Madabushi et al., 2018) . Only the coarse-level of the API \noutput is used. For example, for the question â€˜Who is the telephone inventor?â€™, the answer type of the question \nis (HUM, ind) that coarse-level answer (HUM) is kept and the fine-level answer (ind) is discarded. \n3.1.2 Highlighter \nThis section replaces EAT words of candi date answers with a special token . To perform this, named entities \ntype of candidate answers is detected using the Spacy NER tool.  Then, the detected name d entities are replaced \nwith a special token, if their type is equal to EAT.  The mapping between the named entity type detected by the \nSpacy NER tool and the output of the EAT detector is presented in Table 3. \nTable 3: Mapping between the named entity type and the output of the expected answer type component (Kamath et al., \n2019). \nSpacy annotated tag EAT \nPERSON, ORG, NORP HUM \nLOC, GPE LOC \nPRODUCT, EVENT, LANGUAGE, WORK OF ART, LAW, FAC ENTY \nDATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, CARDINAL NUM \n \nThe following steps describe preprocessing steps â€˜Who is the telephone inventor?â€™ and â€˜The first telephone was \ninvented by Alexander Graham Bell.â€™. \nğ¼) ğ‘Šâ„ğ‘œ ğ‘–ğ‘  ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘œğ‘Ÿ? \nğ¸ğ´ğ‘‡ ğ·ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ\nâ†’         (ğ‘Šâ„ğ‘œ ğ‘–ğ‘  ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘œğ‘Ÿ?,ğ»ğ‘ˆğ‘€) \nğ¼ğ¼) ğ‘‡â„ğ‘’ ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘¤ğ‘ğ‘  ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘¦ ğ´ğ‘™ğ‘’ğ‘¥ğ‘ğ‘›ğ‘‘ğ‘’ğ‘Ÿ ğºğ‘Ÿğ‘â„ğ‘ğ‘š ğµğ‘’ğ‘™ğ‘™.\n                                      ğ‘†ğ‘ğ‘ğ‘ğ‘¦ ğ‘ğ¸ğ‘… ğ‘¡ğ‘œğ‘œğ‘™                                  \nâ†’                                      \n     ğ‘‡â„ğ‘’ ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ [ğ‘ƒğ‘…ğ‘‚ğ·ğ‘ˆğ¶ğ‘‡] ğ‘¤ğ‘ğ‘  ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘¦ [ğ‘ƒğ¸ğ‘…ğ‘†ğ‘‚ğ‘]\n                         ğ‘…ğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘–ğ‘›ğ‘” ğ‘†ğ‘ğ‘ğ‘ğ‘¦ ğ‘ğ‘›ğ‘›ğ‘œğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘¡ğ‘ğ‘” ğ‘¤ğ‘–ğ‘¡â„ ğ¸ğ´ğ‘‡                         \nâ†’                                                   \n     ğ‘‡â„ğ‘’ ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ ğ¸ğ‘ğ‘‡ğ‘Œ ğ‘¤ğ‘ğ‘  ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘¦ ğ»ğ‘ˆğ‘€\n  ğ‘…ğ‘’ğ‘ğ‘™ğ‘ğ‘ğ‘–ğ‘›ğ‘” ğ¸ğ´ğ‘‡ ğ‘‘ğ‘’ğ‘¡ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ ğ‘¤ğ‘–ğ‘¡â„ ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘ğ‘™ ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘ğ‘›ğ‘‘ ğ‘‘ğ‘–ğ‘ ğ‘ğ‘ğ‘Ÿğ‘‘ğ‘–ğ‘›ğ‘” ğ‘œğ‘¡â„ğ‘’ğ‘Ÿ ğ¸ğ´ğ‘‡ \nâ†’                                                              \n     ğ‘‡â„ğ‘’ ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘¤ğ‘ğ‘  ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘¦ ğ‘†ğ‘ƒğ¸ğ¶ğ¼ğ´ğ¿_ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ \n3.2 Language model \nTexts processed by the preprocessing section are passed to the language model section. In this section, questions \nand answers should be transformed into an appropriate template for the BERT model (Devlin et al., 2019) . In \nthis research, we use the BERT -base language model, which has been fine -tuned for classification problems. I t \nhas a better understanding of the classification problem. The input of this model should be as follows: \nğµğ¸ğ‘…ğ‘‡_ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡(ğ‘†ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘›ğ‘ğ‘’1,ğ‘†ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘›ğ‘ğ‘’2)= [ğ¶ğ¿ğ‘†] ğ‘†ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘›ğ‘ğ‘’1 [ğ‘†ğ¸ğ‘ƒ] ğ‘†ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘›ğ‘ğ‘’2 [ğ‘†ğ¸ğ‘ƒ]                      (7 \nFor example, for the question â€˜Who is the telephone inventor?â€™ and the candidate answer â€˜The first telephone \nwas invented by Alexander Graham Bell.â€™, the input will be the following: \n[ğ¶ğ¿ğ‘†] ğ‘Šâ„ğ‘œ ğ‘–ğ‘  ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘œğ‘Ÿ? [ğ‘†ğ¸ğ‘ƒ] ğ‘‡â„ğ‘’ ğ‘“ğ‘–ğ‘Ÿğ‘ ğ‘¡ ğ‘¡ğ‘’ğ‘™ğ‘’ğ‘â„ğ‘œğ‘›ğ‘’ ğ‘¤ğ‘ğ‘  ğ‘–ğ‘›ğ‘£ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘‘ ğ‘ğ‘¦ ğ‘†ğ‘ƒğ¸ğ¶ğ¼ğ´ğ¿_ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ [ğ‘†ğ¸ğ‘ƒ]      (8 \nThe [CLS] token in the BERT model is used for  classification. The output of the [CLS] can be considered as a \nvector representation. The outputs of the [SEP] tokens do not apply to the answer -selection task and can be \nignored. A new vector is presented for each token by sending this input to the language model, which captures \nthe meaning of the token.  In other words, the BERT model replaces the s emantic vector of each word which  \nAn Answer Selection Method Using BERT Model  11 \n \nindependently captures the me aning of the word, with a vector that captures the meaning of the word according \nto its position in the sentence. The BERT model can be illustrated as follows: \n(ğ¸[ğ¶ğ¿ğ‘†],ğ¸1,â€¦,ğ¸ğ‘,ğ¸[ğ‘†ğ¸ğ‘ƒ],ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²)=ğµğ¸ğ‘…ğ‘‡([ğ¶ğ¿ğ‘†],ğ‘‡ğ‘œğ‘˜ 1,â€¦,ğ‘‡ğ‘œğ‘˜ ğ‘,[ğ‘†ğ¸ğ‘ƒ],ğ‘‡ğ‘œğ‘˜ 1â€²,â€¦,ğ‘‡ğ‘œğ‘˜ ğ‘â€²)           (9 \n3.3 Classifier \nIn BERT paper (Devlin et al., 2019) , it is suggested to use the output of the [CLS] token for classification . In \nthis section, in addition to the classification method presented in the BERT paper, other methods will be \nimplemented. Each of these methods will be explained below. \n3.3.1 BERT-base-Baseline (BB-Baseline) \nIn this method, the classification method proposed by Devlin et al. (Devlin et al., 2019) is employed. That is, the \noutput of the [CLS] token, a vector of length 768, is passed as input to a fully connected neural network with a \nhidden layer of length 1024.  The output layer of th e fully connected neural network consists of two elements \nthat the first indicates the correctness of the answer candidate , and the latter indicates the incorrectness of the \nanswer candidate. Figure 5 presents the pseudo-code of this method. Wh1ïƒâ„1024Ã—768 is a matrix that is equivalent \nto the hidden layer parameters, and b h1ïƒâ„1024 is a vector that is equivalent to the bias for the hidden layer.  \nWh2ïƒâ„2Ã—1024 is a matrix that is equivalent to the output layer parameters, and b h1ïƒâ„2 is a vector  that is \nequivalent to the bias for the output layer.  Relu (Nair & Hinton, 2010)  and Softmax (Jurafsky & Martin, 2014)  \nactivation functions are also used. Figure 6 illustrates the architecture of this method. \nğµğµ_ğµğ´ğ‘†ğ¸ğ¿ğ¼ğ‘ğ¸(ğ‘,ğ‘) \n       ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘¬ğ‘¨ğ‘»âˆ’ğ‘«ğ’†ğ’•ğ’†ğ’„ğ’•ğ’ğ’“(ğ‘) \n       ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ=ğ‘¯ğ’Šğ’ˆğ’‰ğ’ğ’Šğ’ˆğ’‰ğ’•ğ’†ğ’“(ğ‘) \n       ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡=ğ‘©ğ‘¬ğ‘¹ğ‘»_ğ‘°ğ’ğ’‘ğ’–ğ’•(ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›,ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ) \n       (ğ¸[ğ¶ğ¿ğ‘†],ğ¸1,â€¦,ğ¸ğ‘,ğ¸[ğ‘†ğ¸ğ‘ƒ],ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²,ğ¸[ğ‘†ğ¸ğ‘ƒ])=ğ‘©ğ‘¬ğ‘¹ğ‘»(ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡.ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ğ‘†) \n       ğ»ğ¿ =ğ’“ğ’†ğ’ğ’–(ğ‘Šâ„1ğ¸[ğ¶ğ¿ğ‘†]+ğ‘â„1) \n       ğ‘“(ğ‘,ğ‘)= ğ’”ğ’ğ’‡ğ’•ğ’ğ’‚ğ’™(ğ‘Šâ„2ğ»ğ¿ +ğ‘â„2) \nFig 5: BB-Baseline model pseudo-code \nAn Answer Selection Method Using BERT Model  12 \n \n \nFig 6: BB-Baseline model architecture \n \nIn this method, only the output of the [CLS] token is used, and the other output vectors are ignored. \n\nAn Answer Selection Method Using BERT Model  13 \n \n3.3.2 BERT-base-BOW (BB-BOW) \nIn this method, in addition to the output vector of the [CLS] token, the output vectors of  questions and answers \ntokens are also used for classification . That is, the token vectors of each sentence are summed , and a new vector \nof length 768 is presented f or each sentence. As a result, there will be three vectors of length 768 w hich are the \noutput vectors of question tokens, answer tokens, and the [CLS] token, respectively. A vector of length 2304 is \nproduced by concatenating these three vectors together and is passed as input to a fully neural network \nconnected with a hidden layer of length 1024.  Figure 7 presents the pseudo-code of this method.  Wh1ïƒâ„1024Ã—2304 \nis a matrix that is equivalent to the hidden layer parameters, and b h1ïƒâ„1024 is a vector that is equivalent to the \nbias for the hidden layer. Wh2ïƒâ„2Ã—1024 is a matrix that is equivalent to the output layer parameters, and b h2ïƒâ„2 is \na vector that is equivalent to the bias for the output layer.  The Concat function concatenates the input vectors \nand produces a matrix. Figure 8 illustrates the architecture of this method. \nğµğµ_ğµğ‘‚ğ‘Š(ğ‘,ğ‘) \n       ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘¬ğ‘¨ğ‘»âˆ’ğ‘«ğ’†ğ’•ğ’†ğ’„ğ’•ğ’ğ’“(ğ‘) \n       ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ=ğ‘¯ğ’Šğ’ˆğ’‰ğ’ğ’Šğ’ˆğ’‰ğ’•ğ’†ğ’“(ğ‘) \n       ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡=ğ‘©ğ‘¬ğ‘¹ğ‘»_ğ‘°ğ’ğ’‘ğ’–ğ’•(ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›,ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ) \n       (ğ¸[ğ¶ğ¿ğ‘†],ğ¸1,â€¦,ğ¸ğ‘,ğ¸[ğ‘†ğ¸ğ‘ƒ],ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²,ğ¸[ğ‘†ğ¸ğ‘ƒ])=ğ‘©ğ‘¬ğ‘¹ğ‘»(ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡.ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ğ‘†) \n       ğ¸1_ğ‘=ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1,â€¦,ğ¸ğ‘) \n       ğ¸1â€²_ğ‘â€² =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²) \n       ğµğ‘‚ğ‘Š1 = âˆ‘ğ¸1_ğ‘[ğ‘–]\nğ‘\nğ‘–=1\n \n       ğµğ‘‚ğ‘Š2 = âˆ‘ğ¸1â€²_ğ‘â€²[ğ‘–]\nğ‘\nğ‘–=1\n \n       ğ¼ğ¿ =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸[ğ¶ğ¿ğ‘†],ğµğ‘‚ğ‘Š1,ğµğ‘‚ğ‘Š2) \n       ğ»ğ¿ =ğ’“ğ’†ğ’ğ’–(ğ‘Šâ„1ğ¼ğ¿+ğ‘â„1) \n       ğ‘“(ğ‘,ğ‘)= ğ’”ğ’ğ’‡ğ’•ğ’ğ’‚ğ’™(ğ‘Šâ„2ğ»ğ¿ +ğ‘â„2) \nFig 7: BB-BOW model pseudo-code \nAn Answer Selection Method Using BERT Model  14 \n \n \nFig 8: BB-BOW model architecture \n\nAn Answer Selection Method Using BERT Model  15 \n \n3.3.3 BERT-base-CNN (BB-CNN) \nThis method also uses the output vector of the other tokens.  However, the bag-of-words method is not \nemployed; instead, the convolutional neural network is used.  In this network, the window size is 3 and \naccordingly, the padding value is  2. The number of filters is 200, and  MaxPooling is  used for the pooling \noperation. A vector of length 200 is produced for each sentence  by applying a convolutional neur al network \nwith these features. These vectors are concatenated to the output ve ctor of the [CLS] token and a vector of 1168 \nlengths is produced.  This vector is passed to a fully connected neural network whose hidden layer size is 1024. \nThen, classification operation is performed. Figure 9 is a pseudo-code of this method. Wh1ïƒâ„1024Ã—1168 is a matrix \nthat is equivalent to the hidden layer parameters, and b h1ïƒâ„1024 is a vector  that is equivalent to the bias for the \nhidden layer. Wh2ïƒâ„2Ã—1024 is a matrix that is equivalent to the output layer parameters, and b h2ïƒâ„2 is a vector  \nthat is equivalent to the bias for the output layer.  The CNN function refers to the convolutional neural network.  \nThe MaxPool function also performs maximum pooling. Figure 10 illustrates the architecture of this method. \nğµğµ_ğ¶ğ‘ğ‘(ğ‘,ğ‘) \n       ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘¬ğ‘¨ğ‘»âˆ’ğ‘«ğ’†ğ’•ğ’†ğ’„ğ’•ğ’ğ’“(ğ‘) \n       ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ=ğ‘¯ğ’Šğ’ˆğ’‰ğ’ğ’Šğ’ˆğ’‰ğ’•ğ’†ğ’“(ğ‘) \n       ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡=ğ‘©ğ‘¬ğ‘¹ğ‘»_ğ‘°ğ’ğ’‘ğ’–ğ’•(ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›,ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ) \n       (ğ¸[ğ¶ğ¿ğ‘†],ğ¸1,â€¦,ğ¸ğ‘,ğ¸[ğ‘†ğ¸ğ‘ƒ],ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²,ğ¸[ğ‘†ğ¸ğ‘ƒ])=ğ‘©ğ‘¬ğ‘¹ğ‘»(ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡.ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ğ‘†) \n       ğ¸1_ğ‘=ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1,â€¦,ğ¸ğ‘) \n       ğ¸1â€²_ğ‘â€² =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²) \n       ğ¶ğ‘ğ‘1 = ğ‘ªğ‘µğ‘µğ’˜ğ’Šğ’ğ’…ğ’ğ’˜ğ’”ğ’Šğ’›ğ’†=ğŸ‘,ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’”=ğŸğŸğŸ,ğ’‘ğ’‚ğ’…ğ’…ğ’Šğ’ğ’ˆ= ğŸ(ğ¸1_ğ‘) \n       ğ¶ğ‘ğ‘2 = ğ‘ªğ‘µğ‘µğ’˜ğ’Šğ’ğ’…ğ’ğ’˜ğ’”ğ’Šğ’›ğ’†=ğŸ‘,ğ’‡ğ’Šğ’ğ’•ğ’†ğ’“ğ’”=ğŸğŸğŸ,ğ’‘ğ’‚ğ’…ğ’…ğ’Šğ’ğ’ˆ= ğŸ(ğ¸1â€²_ğ‘â€²) \n       ğ¹ğ‘’ğ‘ğ‘¡ğ‘‰ğ‘’ğ‘1 =ğ‘´ğ’‚ğ’™ğ‘·ğ’ğ’ğ’(ğ¶ğ‘ğ‘1) \n       ğ¹ğ‘’ğ‘ğ‘¡ğ‘‰ğ‘’ğ‘2 =ğ‘´ğ’‚ğ’™ğ‘·ğ’ğ’ğ’(ğ¶ğ‘ğ‘2) \n       ğ¼ğ¿ =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸[ğ¶ğ¿ğ‘†],ğ¹ğ‘’ğ‘ğ‘¡ğ‘‰ğ‘’ğ‘1,ğ¹ğ‘’ğ‘ğ‘¡ğ‘‰ğ‘’ğ‘2) \n       ğ»ğ¿ =ğ’“ğ’†ğ’ğ’–(ğ‘Šâ„1ğ¼ğ¿+ğ‘â„1) \n       ğ‘“(ğ‘,ğ‘)= ğ’”ğ’ğ’‡ğ’•ğ’ğ’‚ğ’™(ğ‘Šâ„2ğ»ğ¿ +ğ‘â„2) \nFig 9: BB-CNN model pseudo-code \nAn Answer Selection Method Using BERT Model  16 \n \n \nFig 10: BB-CNN model architecture \n\nAn Answer Selection Method Using BERT Model  17 \n \n3.3.4 BERT-base-RNN (BB-RNN) \nIn this method, other token s vectors are also used. However, instead of using a convolutional neural network, it \nuses a recurrent neural network. The network is a two stacked RNN whose hidden layer size is 768 . For each of \nthe input sentences, a vector of length 768 is produced, and a vector of length 2304 is produced by \nconcatenating these vectors together . This vector is passed to a fully connected neural network whose hidden \nlayer size is 1024. Then, classification operation is performed. Figure 11 presents a pseudo-code of this method. \nWh1ïƒâ„1024Ã—2304 is a matrix  that is equivalent to the hidden layer parameters, and b h1ïƒâ„1024 is a vector  that is \nequivalent to the bias for the hidden layer.  Wh2ïƒâ„2Ã—1024 is a matrix  that is equivalent to the output layer \nparameters, and bh2ïƒâ„2 is a vector that is equivalent to the bias for the output layer.  The RNN function refers to \nthe recurrent neural network. Figure 12 illustrates the architecture of this method. \nğµğµ_ğ‘…ğ‘ğ‘(ğ‘,ğ‘) \n       ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘¬ğ‘¨ğ‘»âˆ’ğ‘«ğ’†ğ’•ğ’†ğ’„ğ’•ğ’ğ’“(ğ‘) \n       ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ=ğ‘¯ğ’Šğ’ˆğ’‰ğ’ğ’Šğ’ˆğ’‰ğ’•ğ’†ğ’“(ğ‘) \n       ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡=ğ‘©ğ‘¬ğ‘¹ğ‘»_ğ‘°ğ’ğ’‘ğ’–ğ’•(ğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›,ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ) \n       (ğ¸[ğ¶ğ¿ğ‘†],ğ¸1,â€¦,ğ¸ğ‘,ğ¸[ğ‘†ğ¸ğ‘ƒ],ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²,ğ¸[ğ‘†ğ¸ğ‘ƒ])=ğ‘©ğ‘¬ğ‘¹ğ‘»(ğ¼ğ‘›ğ‘ğ‘¢ğ‘¡.ğ‘‡ğ‘‚ğ¾ğ¸ğ‘ğ‘†) \n       ğ¸1_ğ‘=ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1,â€¦,ğ¸ğ‘) \n       ğ¸1â€²_ğ‘â€² =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸1\nâ€²,â€¦,ğ¸ğ‘\nâ€²) \n       ğ‘…ğ‘ğ‘1 = ğ‘¹ğ‘µğ‘µğ’‰ğ’Šğ’…ğ’…ğ’†ğ’ğ‘³ğ’‚ğ’šğ’†ğ’“=ğŸ•ğŸ”ğŸ–,ğ‘µ=ğŸ(ğ¸1_ğ‘) \n       ğ‘…ğ‘ğ‘2 = ğ‘¹ğ‘µğ‘µğ’‰ğ’Šğ’…ğ’…ğ’†ğ’ğ‘³ğ’‚ğ’šğ’†ğ’“=ğŸ•ğŸ”ğŸ–,ğ‘µ=ğŸ(ğ¸1â€²_ğ‘â€²) \n       ğ¼ğ¿ =ğ’„ğ’ğ’ğ’„ğ’‚ğ’•(ğ¸[ğ¶ğ¿ğ‘†],ğ‘…ğ‘ğ‘1,ğ‘…ğ‘ğ‘2) \n       ğ»ğ¿ =ğ’“ğ’†ğ’ğ’–(ğ‘Šâ„1ğ¼ğ¿+ğ‘â„1) \n       ğ‘“(ğ‘,ğ‘)= ğ’”ğ’ğ’‡ğ’•ğ’ğ’‚ğ’™(ğ‘Šâ„2ğ»ğ¿ +ğ‘â„2) \nFig 11: BB-RNN model pseudo-code \nAn Answer Selection Method Using BERT Model  18 \n \n \nFig 12: BB-RNN model architecture \n\nAn Answer Selection Method Using BERT Model  19 \n \n3.4 Training Algorithm \nTo train the proposed model, the training parameters must be tuned so that the model enable s us to find the best \nanswer to the user's question. The t raining algorithm is as follows. The Q refers to the collection of question s, A \nrefers to the  collection of answers,  and L refers to the la bels of the answers. BAS_Mod el refers to one of the  \nmodels, such as BB-Baseline, BB-BOW, BB-CNN or BB-RNN. Cross_entropy (Buduma & Locascio, 2017)  is \na loss function. Optimize function also endeavours to tune the training parameters in order to minimize loss \nvalue. \nğ‘‡ğ‘…ğ´ğ¼ğ‘_ğµğ´ğ‘†(ğ‘„,ğ´,ğ¿) \n       For each ğ‘¬ğ’‘ğ’ğ’„ğ’‰ in {1,2,3,4}: \n              Foreach ğ‘©ğ’‚ğ’•ğ’„ğ’‰ in (ğ‘„,ğ´,ğ¿): \n                     Foreach (ğ’’,ğ’‚,ğ’) in ğµğ‘ğ‘¡ğ‘â„: \n                            ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›=ğ‘©ğ‘¨ğ‘º_ğ‘´ğ’ğ’…ğ’†ğ’(ğ‘,ğ‘) \n                            ğ‘™ğ‘œğ‘ ğ‘ ğ‘£ğ‘ğ‘™ =ğ‘ªğ’“ğ’ğ’”ğ’”_ğ’†ğ’ğ’•ğ’“ğ’ğ’‘ğ’š(ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›,ğ‘™) \n                            ğ‘¶ğ’‘ğ’•ğ’Šğ’ğ’Šğ’›ğ’†(ğ‘Šğµğ´ğ‘†_ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™,ğ‘™ğ‘œğ‘ ğ‘ ğ‘£ğ‘ğ‘™) \nFig 13: Training algorithm \n4. Experiments \nIn this section, we briefly describe the baseline models and compare their results with the proposed model . We \nthen explain the datasets we evaluate the proposed model with. Finally, we provide evaluation metrics and \nimplementation details. \n4.1 Baseline Models \nTo prove the superiority of the proposed model, it should be compared with some competitive baseline models. \nThat's why , we compare the proposed model  with the two competitive baseline models which have the best \nresults. The baseline models and our proposed model are summarized in Table 4. \nTable 4: Summarization of baseline models and the proposed model \nArchitecture Description \n(Kamath et al., 2019) \nA Bi-LSTM model which performs a preprocessing algorithm on the input sentences. In this preprocessing, the \nnamed entities which are equivalent to the answer type announced by the question processing part, are replaced \nwith a special token. \n(Yoon et al., 2019) \nA model which used language models for answer -selection task. This model used the ELMo langu age model \nalong with techniques such as Latent -Clustering and demonstrated that the combination of these components \nproduced a robust model. \nBB-BOW \nA model which uses the output vector of the [CLS] token, the output vectors of questions and answers tokens of \nthe BERT language model to find the best answer. That is, the token vectors of each sentence are summed, and a \nnew vector is presented for each sentence. \nBB-CNN \nA model which uses the output vector of the [CLS] token, the output vectors of questions and answers tokens of \nthe BERT language model to find the best answer. That is, the token vectors of each sentence are transferred to a \nCNN, and a new vector is presented for each sentence. \nBB-RNN \nA model which uses the output vector of the [CLS] token, the output vectors of questions and answers tokens of \nthe BERT language model to find the best answer. That is, the token vectors of each sentence are transferred to a \nRNN, and a new vector is presented for each sentence. \n \n4.2 Dataset \nThree datasets are used to evaluate the BAS model, including TrecQA Raw (M. Wang et al., 2007) , TrecQA \nClean (Z. Wang & Ittycheriah, 2015) , and WikiQA (Y. Yang et al., 2015) . Each of these datasets will be \nexplained in more detail below. \n \n \nAn Answer Selection Method Using BERT Model  20 \n \n4.2.1 TrecQA Raw \nThe TrecQA Raw dataset is one of the most commonly used datasets in the answer-selection task built by Yao et \nal. (Yao et al., 2013)  from Trec Question Answering Tracks.  Trec Question Answering Track 8 -12 data is used \nto produce training data, and Trec Question Answering Track 13 data is used for validation data and test data. In \nthis dataset, training data consist of 1229 questions and 53417 pairs, evaluation data consist of 82 questions and \n1148 pairs and test data consist of 100 questions and 1517 pairs. \n4.2.2 TrecQA Clean \nThe TrecQA Clean dataset is made from the TrecQA Raw dataset.  In this dataset, questions that have no correct \nanswers or only one correct/incorrect answer are removed from the validation and test data.  Training data such \nas Trec QA Raw consists of 1229 questions and 53417 pairs.  However, the validation data and test data are \ndifferent from the TrecQA Raw dataset.  Validation data consist of 65 questions and 1117 pairs and test data \nconsist of 68 questions and 1142 pairs. \n4.2.3 WikiQA \nThe WikiQA dataset consists of Bing search engine logs.  Candidate answers to each question are extracted from \nWikipedia pages. This dataset also eliminates questions that do not have the correct candidate answers.  Training \ndata consists of 873 questions an d 8672 pairs, validation data consist of 126 questions and 1130 pairs , and test \ndata consist of 243 questions and 2351 pairs. \nThe characteristics of these datasets are presented in Table 5. \nTable 5: Detail of the TrecQA Raw, TrecQA Clean, and WikiQA datasets \nDataset Set Number of Questions Number of Pairs \nTrecQA RAW \nTrain 1229 53417 \nValidation 82 1148 \nTest 100 1517 \nTrecQA CLEAN \nTrain 1229 53417 \nValidation 65 1117 \nTest 68 1142 \nWikiQA \nTrain 873 8672 \nValidation 126 1130 \nTest 243 2351 \n \n4.3 Evaluation Metrics \nMAP and MRR metrics are used in the answer -selection task to evaluate models and methods.  These measures \nshow the rating quality of candidate answers.  The MRR measure only considers the rank of the first relevant \nanswer, but the MAP measure considers the order of all relevant answers (Manning et al., 2008) . These two \nmeasures are shown below.  \nğ‘€ğ´ğ‘ƒ(ğ‘„)=\n1\n|ğ‘„|âˆ‘ 1\nğ‘šğ‘—\n|ğ‘„|\nğ‘—=1 âˆ‘ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›(ğ‘…ğ‘—ğ‘˜)\nğ‘šğ‘—\nğ‘˜=1                                                          (10 \nğ‘€ğ‘…ğ‘…(ğ‘„)=\n1\n|ğ‘„|âˆ‘ ğ‘Ÿğ‘—\n|ğ‘„|\nğ‘—=1                                                                               (11 \nIn these equations, Q is the set of questions, m j is the number of relevant answers to q j, Rjk is a list of candidate \nanswers that co ntains top k relevant answers, Precision function is a function that measure s the ratio of the \nnumber of relevant answers to the total candidate answers,  rj is the inverse of the f irst relevant answer rank for \nqj. \n4.4 Implementation Details \nWe implement the BAS model with PyTorch library (Subramanian, 2018) in Python 3.6 programming language \non the Colab platform â€ . T he model is trained on NVIDIA Tesla K80 . We use BERT wordpiece tokenizer  to \n                                                           \nâ€  https://colab.research.google.com \nAn Answer Selection Method Using BERT Model  21 \n \ntokenize input sentences. The batch size is equal to 32. We consider a vector initialized with zero vectors. The \ndropout is set to 0. 2. Gelu (Hendrycks & Gimpel, 2016 ) function is used for activation function  in BERT \nlanguage model and Relu (Agarap, 2018) is used for fully connected layer activation function. \nIn BB-Baseline, BB-BOW, and BB-RNN models, the number of hidden units of the fully connected layer is \nequal to 1024.  In BB-CNN model, t he number of filters of the convolutio nal neural network is equal  to 200. \nThee window size is set to 2. The Max Pooling is applied for pooling operation. \nTo train the proposed model, we set the learning rate to 0.0001 . The model is trained for 4  epochs. AdamW \noptimizer (Loshchilov & Hutter, 2019)  and WarmupLinearSchedule scheduler (Devlin et al., 2019)  are used for \ntraining. The BERT model used in this research is fine-tuned based on the training dataset during training.  \nAs shown in Figure 4 , the Language model  and the Classifier sections are trainable and the Preprocessing \nsection is non -trainable. The training parameters number of the Language model section is equal to 110M. In \nBB-Baseline, t he total number of training parameters of the fully connected layer is about \n768Ã—1024+1024Ã—2=789k. Hence the total number of training parameters in B B-Baseline is \n110000k+789kâ‰ˆ110789k. In BB-BOW, the total number of training parameters of the fully connect ed layer is \nabout 3Ã—768Ã—1024+1024Ã—2=2361k. Hence the total number of training parameters in BB-BOW is \n110000k+2361kâ‰ˆ112361k. In B B-CNN, t he number of parameters in the convolution layer is about \n2Ã—768Ã—2=3k, and t he total number of training parameters of the fully connected layer is about \n(2Ã—200+768)Ã—1024+1024Ã—2=1200k. Hence the total number of training parameters in B B-CNN is \n110000k+1200k+3kâ‰ˆ111203k. In BB-RNN, the number of parameters in the recurrent layer is about 2Ã—768=1k, \nand the total number of training parameters of the fully connected layer is about 3Ã—768Ã—1024+1024Ã—2=2361k. \nHence the total number of training parameters in BB-RNN is 110000k+2361k+1kâ‰ˆ112362k. \n5. Results and Discussion \nIn this section, we explain the experiments  results of the BAS model in detail . In other words, we respond to the \nresearch questions.  In this regard, section 5.1 answers whether t he BAS model can outperform  other baseline \nmodels. Section 5.2 an swers whether the preprocessing has a significant effect on the performance.  Section 5.3 \nanswers how different classifiers affect the BAS model performance. \n \n5.1 Model Performance \nThe model presented by Kamath et al . (Kamath et al., 2019) , is a model that uses the idea of appending pre -\nprocessing to answer -selection models. This model uses a preprocessing algorithm similar to the preprocessing \nsection presented in our research. To prove their claim, they apply a simple LSTM to model the input sentences.  \nKamath et al. prove their claim by evaluating their model on the TrecQA Raw dataset.  Currently, this model is \nthe state-of-the-art model on the TrecQA Raw dataset. \nThe model presented by Yoon et al. (Yoon et al., 2019)  is a model that uses the idea of using language models. \nIt employs the ELMo language model  (Peters et al., 2018) . In one of their experiments, Yoon et al. i nvestigated \nthe effect of the lack of the language model on their answer -selection model performance and demonstrated that \nusing the language model can improve model performance.  By evaluating this model on the TrecQA Clean and \nthe WikiQA datasets, they pr oved their claim.  Currently, this model is the state-of-the-art model  for TrecQA \nClean and WikiQA datasets.  The WikiQA dataset is the only dataset shared between the two baseline models.  \nBy examining the results of the baseline models on this dataset, it i s proven that the idea of using language \nmodels has a more significant impact than preprocessing on the answer-selection model performance. \nTable 6: Evaluation of the proposed model \nArchitecture TrecQA Raw TrecQA Clean WikiQA \nMAP MRR MAP MRR MAP MRR \n(Kamath et al., 2019) 0.850 0.892 - - 0.689 0.709 \n(Yoon et al., 2019) - - 0.868 0.928 0.764 0.784 \nBB-BOW 0.871 0.898 0.909 0.946 0.817 0.835 \nBB-CNN 0.863 0.893 0.909 0.938 0.790 0.805 \nBB-RNN 0.872 0.899 0.915 0.959 0.784 0.801 \n \nAn Answer Selection Method Using BERT Model  22 \n \nThe BAS model is compared with baselines in Table 6. The results show that the idea of using language models \nimproves the performance of answer -selection models. The MAP and MRR metrics are increased for TrecQA \nRaw, TrecQA Clean, and WikiQA datasets.  This proves  idea of using language modeling  are significantly \npractical. For TrecQA Raw data, the best results be long to the BB-RNN model.  In this model, the MAP and \nMRR metrics are improved by 2.2% and 0.7%, respectively. For TrecQA Clean data, the best results also belong \nto BB-RNN. As shown in Table 6, the MAP and MRR metrics are improved by 4.3% and 3.1%, respect ively. \nFor WikiQA data, the best results belong to the BB-BOW model.  The results show t hat the MAP and MRR \nmetrics are improved by 5.3% and 5.1%, respectively. These results are shown in Figure 14 and Figure 15. \n \nFig 14: Model performance on different datasets in comparison with baseline models in terms of MAP \n \nFig 15: Model performance on different datasets in comparison with baseline models in terms of MRR \nThese results demonstrate that using a robust language model has a significant impact on the performance of the \nanswer-selection model. This shows that just using a language model is not enough, and each language model \ncan lead to different results.  The effect of preprocessing on language model -based answer -selection model is \nstill unclear. In the next section, we will examine the lack of the preprocessing. \n \n5.2 Lack of Preprocessing \nThe preprocessing section  attempts to replace the possible answer token contained in the candidate's answer \nwith a special token. This allows the model to learn that the candidate answers that contain the special token are \nprobably correct.  The model presented by Kamath et al. proved this claim.  But the impact of this idea on \nlanguage model-based answer-selection models has not been examined so far.  In this section, the preprocessing \nis removed from the BAS model and the results of the modified model are computed. \n85\n68.9\n86.6\n76.4\n87.1\n90.9\n81.7\n86.3\n90.9\n79\n87.2\n91.5\n78.4\n60\n65\n70\n75\n80\n85\n90\n95\nTrecQA Raw TrecQA Clean WikiQA\nMAP\nDataset\nKamath et al. 2019 Yoou et al. 2019 BB-BOW BB-CNN BB-RNN\n89.2\n70.9\n92.8\n78.4\n89.8\n94.6\n83.5\n89.3\n93.8\n80.5\n89.9\n95.9\n80.1\n60\n65\n70\n75\n80\n85\n90\n95\n100\nTrecQA Raw TrecQA Clean WikiQA\nMRR\nDataset\nKamath et al. 2019 Yoou et al. 2019 BB-BOW BB-CNN BB-RNN\nAn Answer Selection Method Using BERT Model  23 \n \nTable 7: Evaluation of the effect of lack of the preprocessing (pp) on the BAS model \n \nTable 7 shows the results of th e alteration. As the results demonstrate, this alteration ha s a negative effect on \nmodel performance . The performance of the BB-BOW model has reduced by  removing the preprocessing \nsection. However, the performance of the BB-CNN model and the BB-RNN model have fewer changes than the \nBB-BOW model.  The reason  that lack of  the preproc essing has a low  effect is t he complexity of classifiers \nemployed in these models. It has also reduced the BAS model performance for TrecQA Raw and TrecQA Clean \ndatasets. In contrast, the performance of the BB-CNN and BB-RNN models for WikiQA dataset has improved \nslightly. The reason is t he sparsity of training data in WikiQA dat aset because the model has not been able to \nproperly learn the effect of the special token on finding the correct answer.  These results are shown in Figure 16 \nand Figure 17. \n \nFig 16: Effect of lack of the preprocessing on different datasets in terms of MAP \n \nFig 17: Effect of lack of the preprocessing on different datasets in terms of MRR \nThese results show that the preprocessing has a positive impact on the performance of the language model -based \nanswer-selection models.  Given that its impact in some cases is low and even harmful, it is still useful. As \nmentioned, t his may be due to the complexity of the classifiers.  But this question needs to be considered \nseparately and the impact of complex classifiers is examined. \n \n86.2\n88.9\n79.3\n86.1\n90.3\n79.2\n86.8\n91.2\n78.6\n87.1\n90.9\n81.7\n86.3\n90.9\n79\n87.2\n91.5\n78.4\n75\n80\n85\n90\n95\nTrecQA Raw TrecQA Clean WikiQA\nMAP\nDataset\nBB-BOW(without pp) BB-CNN(without pp) BB-RNN(without pp) BB-BOW BB-CNN BB-RNN\n88\n92.4\n81.9\n89\n93.8\n81\n89.4\n94.8\n80.3\n89.8\n94.6\n83.5\n89.3\n93.8\n80.5\n89.9\n95.9\n80.1\n75\n80\n85\n90\n95\n100\nTrecQA Raw TrecQA Clean WikiQA\nMRR\nDataset\nBB-BOW(without pp) BB-CNN(without pp) BB-RNN(without pp) BB-BOW BB-CNN BB-RNN\nArchitecture TrecQA Raw TrecQA Clean WikiQA \nMAP MRR MAP MRR MAP MRR \nBB-BOW(without pp) 0.862 0.880 0.889 0.924 0.793 0.819 \nBB-CNN(without pp) 0.861 0.890 0.903 0.938 0.792 0.810 \nBB-RNN (without pp) 0.868 0.894 0.912 0.948 0.786 0.803 \nBB-BOW 0.871 0.898 0.909 0.946 0.817 0.835 \nBB-CNN 0.863 0.893 0.909 0.938 0.790 0.805 \nBB-RNN 0.872 0.899 0.915 0.959 0.784 0.801 \n       \nAn Answer Selection Method Using BERT Model  24 \n \n5.3 The Classifiers Impact \nThe BERT model has been fine -tuned for various tasks.  One of these tasks is sentence classification.  BERT \nmodel uses the [CLS] token output for the classification task. The model transmits the [CLS] token output \nvector as the input  vector to a fully connected neural network whose output layer presents the classes.  This \nmodel does not use the output vectors of the other tokens.  In this section, the results of BB-BOW, BB-CNN and \nBB-RNN models are compared with the BB-Baseline model. \nTable 8: Evaluation of the impact of the classifiers on the BAS model \nThe results are shown in Table 8. These results show that all the classifiers employed instead of the typical Bert \nclassifier improve the performance of the answer -selection model. Using output vectors of all tokens instead of \n[CLS] token causes  the BB-BOW, BB-CNN and BB-RNN models have better performance than the BB-\nBaseline model. BB-BOW uses the output vector of the other tokens as well as the output vector of the [CLS] \ntoken in o rder to capture more information about the input sentences.  In addition to using the output of all \ntokens, B B-CNN uses the convolutional neural network to overcome the words order problem.  In addition to \npreserving the order of words, B B-RNN also uses recu rrent neural network memory to store sentencesâ€™ \ninformation. These results are shown in Figure 18 and Figure 19. \n \nFig 18: Model performance with different classifiers on different datasets in terms of MAP \n \nFig 19: Model performance with different classifiers on different datasets in terms of MRR \nThese results show that using different classifiers instead of the typical classifier of language models can \nimprove the performance of answer-selection models. \n \n86.9\n90.8\n78.9\n87.1\n90.9\n81.7\n86.3\n90.9\n79\n87.2\n91.5\n78.4\n70\n75\n80\n85\n90\n95\nTrecQA Raw TrecQA Clean WikiQA\nMAP\nDataset\nBB-Baseline BB-BOW BB-CNN BB-RNN\n88.6\n94.2\n81\n89.8\n94.6\n83.5\n89.3\n93.8\n80.5\n89.9\n95.9\n80.1\n70\n80\n90\n100\nTrecQA Raw TrecQA Clean WikiQA\nMRR\nDataset\nBB-Baseline BB-BOW BB-CNN BB-RNN\nArchitecture TrecQA Raw TrecQA Clean WikiQA \nMAP MRR MAP MRR MAP MRR \nBB-Baseline 0.869 0.886 0.908 0.942 0.789 0.810 \nBB-BOW 0.871 0.898 0.909 0.946 0.817 0.835 \nBB-CNN 0.863 0.893 0.909 0.938 0.790 0.805 \nBB-RNN 0.872 0.899 0.915 0.959 0.784 0.801 \n       \nAn Answer Selection Method Using BERT Model  25 \n \n6. Conclusion \nIn this research, we present the BAS model, which stands for BERT Answer Selection.  This model aim s to \nextract the answer of the user's question from the candidate answers pool and provide it as a final answer to the \nuser. The model consists of three different sections.  The first sect ion replaces the EAT tokens with a special \ntoken. The second section receives the modified question and answer and generates a representation for each \none using the BERT language model.  Finally, in the third section, using different classifiers, the releva nce \nmatching is calculated.  To evaluate our model, we performed several experiments.  The experiments were \nperformed on TrecQA Raw, TrecQA Clean and WikiQA datasets. \nIn the first experiment, the model performance was evaluated.  In this experiment, the BAS model was compared \nwith the two baseline models, and it was shown that for the TrecQA Raw and TrecQA Clean datasets, the BB -\nROW model was state-of-the-art. BB-BOW model was also state -of-the-art for the WikiQA dataset.  The results \nof the experiment showed that the language model comprehends the sentences better than ordinary neural \nnetworks and produces robust representations. \nIn the second experiment, the effect of the preprocessing section  was evaluated.  In this exper iment, the \npreprocessing was removed and shown to have a more significant effect on the BB -BOW model than the BB -\nCNN and BB -RNN models. The results of this experiment showed that the lack of the preproc essing leads to a \nreduction in the model performance. \nIn the third experiment,  the impact of different classifiers such as BOW, CNN and RNN was evaluated.  In this \nexperiment, the performance of the model with the typical BERT classifier was compared with mentioned \nclassifiers in the paper, and it was shown that the BOW, CNN and RNN classifiers performed better than the \ntypical BERT classifiers.  The results of this experiment showed that using the output  of all tokens instead of \n[CLS] token lead to a better comprehension of the input sentences. \nAs a conclusion, we have shown that usin g strong language models eliminates the need to use knowledge base s \nand external resources.  In other words, if a robust  language model such as BERT is  employed, the need for \nadditional parts will be eliminated.  The reason is the  excellent comprehension of language models from \nlanguages which makes it easier for the model to identify the relevant answer s. The results prove the idea of \nusing the language models. This idea can also be applied to other natural language processing tasks.  \nAs future work, we w ould like to employ the language models derived from BERT language model.  The \nRoBERTa (Y. Liu et al., 2019) and ALBERT (Lan et al., 2019) are such models. The RoBERTa model has been \ntrained on more data and has provided a more efficient model than the BERT  model, which has a better \ncomprehension of the language.  Using this mod el, we can produce more robust representations of input \nsentences. Instead, the ALBERT model offers a lite BERT model. The reduction of the training parameters in \nthis model has resulted in a reduction in the performance of the BERT model.  Instead, it allo ws us to employ \nmore complex models alongside the language model.  Also, we can use more powerful classifiers rather than the \nclassifiers presented in this research. \nCompeting interests statement: \nThis research did not receive any grant from funding agencies in the public, commercial, or not-for-profit \nsectors. The authors declare that they have no conflict of interest. \nReferences \nAgarap, A. F. (2018). Deep Learning using Rectified Linear Units (ReLU). CoRR, abs/1803.08375.  \nBahdanau, D., Cho, K., & Bengio, Y. (2015, 2015). Neural Machine Translation by Jointly Learning to Align \nand Translate. Paper presented at the 3rd International Conference on Learning Representations, ICLR \n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. \nBian, W., Li, S., Yang, Z., Chen, G., & Lin, Z. (2017). A Compare -Aggregate Model with Dynamic -Clip \nAttention for Answer Selection. Paper presented at the CIKM. \nBrill, E., Dumais, S., & Banko, M. (2002). An analysis of the AskMSR question -answering system.  Paper \npresented at the Proceedings of the ACL -02 conference on Empirical methods in natural language \nprocessing-Volume 10. \nBromley, J., Bentz, J. W., Bottou, L., Guyon, I., Lecun, Y., Moore, C., . . . Shah, R. (1993). Signature \nVerification Using a â€œSiameseâ€ Time Delay Neural Network. International Journal of Pattern \nAn Answer Selection Method Using BERT Model  26 \n \nRecognition and Artificial Intelligence, 07 (04), 669 -688. \ndoi:https://doi.org/10.1142/S0218001493000339 \nBuduma, N., &  Locascio, N. (2017). Fundamentals of deep learning: Designing next -generation machine \nintelligence algorithms: O'Reilly Media, Inc. \nDevlin, J., Chang, M. -W., Lee, K., & Toutanova, K. (2019). BERT: Pre -training of Deep Bidirectional \nTransformers for Language Understanding. Paper presented at the NAACL-HLT (1). \nEchihabi, A., & Marcu, D. (2003). A noisy -channel approach to question answering.  Paper presented at the \nProceedings of the 41st Annual Meeting on Association for Computational Linguistics -Volume 1. \nFeng, M., Xiang, B., Glass, M. R., Wang, L., & Zhou, B. (2015). Applying deep learning to answer selection: A \nstudy and an open task . Paper presented at the 2015 IEEE Workshop on Automatic Speech \nRecognition and Understanding (ASRU).  \nHe, H., Gimpel, K., & Lin, J. (2015). Multi-perspective sentence similarity modeling with convolutional neural \nnetworks. Paper presented at the Proceedings of the 2015 Conference on Empirical Methods in Natural \nLanguage Processing. \nHe, H., & Lin, J. (2016). Pairwise word interaction modeling with deep neural networks for semantic similarity \nmeasurement. Paper presented at the Proceedings of the 2016 Conference of the North American \nChapter of the Association for Computational Linguistics: Human Language Technologies.  \nHe, H., Wieting, J., Gimpel, K., Rao, J., & Lin, J. (2016). UMD-TTIC-UW at SemEval -2016 Task 1: Attention -\nbased multi -perspective convolutional neural networks for textual similarity measurement.  Paper \npresented at the Proceedings of the 10th International Workshop o n Semantic Evaluation (SemEval -\n2016). \nHeilman, M., & Smith, N. A. (2010). Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and \nAnswers to Questions. Paper presented at the HLT-NAACL. \nHendrycks, D., & Gimpel, K. (2016). Bridging Nonlineari ties and Stochastic Regularizers with Gaussian Error \nLinear Units. CoRR, abs/1606.08415.  \nHoward, J., & Ruder, S. (2018). Universal Language Model Fine-tuning for Text Classification. Paper presented \nat the ACL (1). \nJurafsky, D., & Martin, J. H. (2014). Speech and language processing (Vol. 3): Pearson London. \nKamath, S., Grau, B., & Ma, Y. (2019). Predicting and Integrating Expected Answer Types into a Simple \nRecurrent Neural Network Model for Answer Sentence Selection . Paper presented at the 20th \nInternational Conference on Computational Linguistics and Intelligent Text Processing, La Rochelle, \nFrance. https://hal.archives-ouvertes.fr/hal-02104488 \nKhot, T., Sabharwal, A., & Clark, P. (2018). SciTaiL: A Textual Entailment Dataset from Science Question \nAnswering. Paper presented at the AAAI. \nKolomiyets, O., & Moens, M. F. (2011). A survey on question answering technology from an information \nretrieval perspective. Information Sciences, 181(24), 5412-5434. doi:10.1016/j.ins.2011.07.047 \nLai, T. M., Bui, T., & Li, S. (2018). A Review on Deep Learning Techniques Applied to Answer Selection.  Paper \npresented at the COLING. \nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sh arma, P., & Soricut, R. (2019). ALBERT: A Lite BERT for \nSelf-supervised Learning of Language Representations. CoRR, abs/1909.11942.  \nLee, K., Levy, O., & Zettlemoyer, L. (2017). Recurrent Additive Networks. CoRR, abs/1705.07393.  \nLiu, X., He, P., Chen, W.,  & Gao, J. (2019). Multi-Task Deep Neural Networks for Natural Language \nUnderstanding. Paper presented at the ACL (1). \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., . . . Stoyanov, V. (2019). RoBERTa: A Robustly \nOptimized BERT Pretraining Approach. CoRR, abs/1907.11692.  \nLoshchilov, I., & Hutter, F. (2019). Decoupled Weight Decay Regularization.  Paper presented at the ICLR \n(Poster). \nMagnolini, S. (2014). A Survey on Paraphrase Recognition. Paper presented at the DWAI@AI*IA. \nManning, C. D., Ragha van, P., & SchÃ¼tze, H. (2008). Introduction to Information Retrieval . Cambridge, UK: \nCambridge University Press. \nMiller, G. (1998). WordNet: An electronic lexical database: MIT press. \nMishra, A., & Jain, S. K. (2016). A survey on question answering systems  with classification. Journal of King \nSaud University - Computer and Information Sciences, 28 (3), 345 -361. \ndoi:10.1016/j.jksuci.2014.10.007 \nMozafari, J., Nematbakhsh, M. A., & Fatemi, A. (2019). Attention -based Pairwise Multi -Perspective \nConvolutional Neural Network for Answer Selection in Question Answering. CoRR, abs/1909.01059.  \nNair, V., & Hinton, G. E. (2010). Rectified Linear Units Improve Restricted Boltzmann Machines.  Paper \npresented at the Proceedings of the 27th International Conference on Machine Learning (ICML-10). \nPerez, J., Arenas, M., & Gutierrez, C. (2009). Semantics and complexity of SPARQL. ACM Transactions on \nDatabase Systems, 34(3), 1--45.  \nAn Answer Selection Method Using BERT Model  27 \n \nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep \nContextualized Word Representations. Paper presented at the NAACL-HLT. \nPunyakanok, V., Roth, D., & Yih, W. T. (2004). Mapping dependencies trees: An application to question \nanswering. Paper presented at the Proceedings of AI&Math 2004. \nRao, J., He, H.,  & Lin, J. (2016). Noise-contrastive estimation for answer selection with deep neural networks.  \nPaper presented at the Proceedings of the 25th ACM International on Conference on Information and \nKnowledge Management. \nSequiera, R., Baruah, G., Tu, Z., Mohamm ed, S., Rao, J., Zhang, H., & Lin, J. J. (2017). Exploring the \nEffectiveness of Convolutional Neural Networks for Answer Selection in End -to-End Question \nAnswering. CoRR, abs/1707.07804.  \nSeveryn, A., & Moschitti, A. (2013). Automatic Feature Engineering f or Answer Selection and Extraction.  \nPaper presented at the EMNLP. \nShen, G., Yang, Y., & Deng, Z. -H. (2017). Inter-Weighted Alignment Network for Sentence Pair Modeling.  \nPaper presented at the EMNLP. \nShen, Y., Deng, Y., Yang, M., Li, Y., Du, N., Fan, W., & Lei, K. (2018). Knowledge-aware Attentive Neural \nNetwork for Ranking Question Answer Pairs. Paper presented at the SIGIR. \nSubramanian, V. (2018). Deep Learning with PyTorch: A practical approach to building neural network models \nusing PyTorch: Packt Publishing Ltd. \nSurdeanu, M., Ciaramita, M., & Zaragoza, H. (2008, 2008). Learning to Rank Answers on Large Online QA \nCollections. Paper presented at the Proceedings of ACL-08: HLT, Columbus, Ohio. \nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks.  Paper \npresented at the NIPS. \nTayyar Madabushi, H., Lee, M., & Barnden, J. (2018, aug). Integrating Question Classification and Deep \nLearning for improved Answer Selection, Santa Fe, New Mexico, USA. \nTran, Q. H., Lai,  T. M., Haffari, G., Zukerman, I., Bui, T., & Bui, H. (2018). The Context-Dependent Additive \nRecurrent Neural Net. Paper presented at the NAACL-HLT. \nTu, Z. (2018). An Experimental Analysis of Multi-Perspective Convolutional Neural Networks. In: UWSpace. \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). \nAttention is all you need. Paper presented at the Advances in neural information processing systems.  \nWan, S., Dras, M., Dale, R., & Paris, C. (2006).  Using dependency-based features to take theâ€™para -farceâ€™out of \nparaphrase. Paper presented at the Proceedings of the Australasian Language Technology Workshop \n2006. \nWang, M., & Manning, C. D. (2010). Probabilistic Tree -Edit Models with Structured Latent Va riables for \nTextual Entailment and Question Answering. Paper presented at the COLING. \nWang, M., Smith, N. A., & Mitamura, T. (2007). What is the Jeopardy model? A quasi -synchronous grammar \nfor QA.  Paper presented at the Proceedings of the 2007 Joint Confer ence on Empirical Methods in \nNatural Language Processing and Computational Natural Language Learning (EMNLP -CoNLL). \nWang, S., & Jiang, J. (2017). A Compare-Aggregate Model for Matching Text Sequences.  Paper presented at the \nICLR (Poster). \nWang, Z., Hamza, W., & Florian, R. (2017). Bilateral Multi -Perspective Matching for Natural Language \nSentences. Paper presented at the IJCAI. \nWang, Z., & Ittycheriah, A. (2015). FAQ -based Question Answering via Word Alignment. CoRR, \nabs/1507.02628.  \nWang, Z., Mi, H., & Itt ycheriah, A. (2016). Sentence Similarity Learning by Lexical Decomposition and \nComposition. Paper presented at the COLING. \nYang, L., Ai, Q., Guo, J., & Croft, W. B. (2016). aNMM: Ranking short answer texts with attention -based \nneural matching model.  Paper presented at the Proceedings of the 25th ACM international on \nconference on information and knowledge management. \nYang, R., Zhang, J., Gao, X., Ji, F., & Chen, H. (2019). Simple and Effective Text Matching with Richer \nAlignment Features. Paper presented at the ACL (1). \nYang, Y., Yih, W. T., & Meek, C. (2015). Wikiqa: A challenge dataset for open -domain question answering.  \nPaper presented at the Proceedings of the 2015 Conference on Empirical Methods in Natural Language \nProcessing. \nYao, X., Durme, B. V., Cal lison-Burch, C., & Clark, P. (2013). Answer Extraction as Sequence Tagging with \nTree Edit Distance. Paper presented at the HLT-NAACL. \nYih, W. T., Chang, M. W., Meek, C., & Pastusiak, A. (2013). Question answering using enhanced lexical \nsemantic models. Paper presented at the Proceedings of the 51st Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers). \nYoon, S., Dernoncourt, F., Kim, D. S., Bui, T., & Jung, K. (2019). A Compare -Aggregate Model with Latent \nClustering for Answer Selection. CoRR, abs/1905.12897.  \nAn Answer Selection Method Using BERT Model  28 \n \nYu, L., Hermann, K. M., Blunsom, P., & Pulman, S. (2014). Deep Learning for Answer Sentence Selection. \nCoRR, abs/1412.1632.  \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8484493494033813
    },
    {
      "name": "Question answering",
      "score": 0.7319585680961609
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6835312843322754
    },
    {
      "name": "Language model",
      "score": 0.6523115038871765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.625670850276947
    },
    {
      "name": "Natural language processing",
      "score": 0.5495945811271667
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.5224010944366455
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5013329982757568
    },
    {
      "name": "Popularity",
      "score": 0.4999806880950928
    },
    {
      "name": "Natural language",
      "score": 0.48883289098739624
    },
    {
      "name": "Model selection",
      "score": 0.4323433041572571
    },
    {
      "name": "Comprehension",
      "score": 0.4308074116706848
    },
    {
      "name": "Task (project management)",
      "score": 0.41537004709243774
    },
    {
      "name": "Machine learning",
      "score": 0.4031587839126587
    },
    {
      "name": "Programming language",
      "score": 0.08780813217163086
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39268498",
      "name": "University of Isfahan",
      "country": "IR"
    }
  ]
}