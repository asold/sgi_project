{
  "title": "SwinIR: Image Restoration Using Swin Transformer",
  "url": "https://openalex.org/W3196057788",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2627551397",
      "name": "Liang Jing-yun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225581206",
      "name": "Cao, Jiezhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3195060995",
      "name": "Sun, Guolei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1913798565",
      "name": "Zhang Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742174613",
      "name": "Van Gool Luc",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744068600",
      "name": "Timofte, Radu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2128254161",
    "https://openalex.org/W2963645458",
    "https://openalex.org/W2056370875",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3167568784",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W2922435819",
    "https://openalex.org/W2963494934",
    "https://openalex.org/W2741137940",
    "https://openalex.org/W2805163084",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W54257720",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W2142683286",
    "https://openalex.org/W3135921327",
    "https://openalex.org/W1791560514",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W2952773607",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W3095671795",
    "https://openalex.org/W3164972083",
    "https://openalex.org/W2964277374",
    "https://openalex.org/W3109737331",
    "https://openalex.org/W2047920195",
    "https://openalex.org/W3129817135",
    "https://openalex.org/W2914992179",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2987869089",
    "https://openalex.org/W3202903235",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W1578089973",
    "https://openalex.org/W2950217418",
    "https://openalex.org/W3168684807",
    "https://openalex.org/W3096739052",
    "https://openalex.org/W3170666462",
    "https://openalex.org/W1906770428",
    "https://openalex.org/W2607041014",
    "https://openalex.org/W3164815015",
    "https://openalex.org/W3038857985",
    "https://openalex.org/W3009428327",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2947156405",
    "https://openalex.org/W2890584119",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3174060721",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3202167470",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W3034785019",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W3164705069",
    "https://openalex.org/W2971719842",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W3105328221",
    "https://openalex.org/W2135065661",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2613155248",
    "https://openalex.org/W1912194039",
    "https://openalex.org/W2962935103",
    "https://openalex.org/W3105540235",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W2919720707",
    "https://openalex.org/W3184190059",
    "https://openalex.org/W3011456574",
    "https://openalex.org/W2963729050",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3035413931",
    "https://openalex.org/W2150081556",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3184726201",
    "https://openalex.org/W2048695508",
    "https://openalex.org/W3203631022",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3104028135",
    "https://openalex.org/W3104725225",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2167191464",
    "https://openalex.org/W2192954843",
    "https://openalex.org/W2964125708",
    "https://openalex.org/W3154723007",
    "https://openalex.org/W2126926806",
    "https://openalex.org/W2988916019"
  ],
  "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\\textbf{up to 0.14$\\sim$0.45dB}$, while the total number of parameters can be reduced by $\\textbf{up to 67%}$.",
  "full_text": "SwinIR: Image Restoration Using Swin Transformer\nJingyun Liang1 Jiezhang Cao1 Guolei Sun1 Kai Zhang1,* Luc Van Gool1,2 Radu Timofte1\n1Computer Vision Lab, ETH Zurich, Switzerland 2KU Leuven, Belgium\n{jinliang, jiezcao, guosun, kai.zhang, vangool, timofter}@vision.ee.ethz.ch\nhttps://github.com/JingyunLiang/SwinIR\nAbstract\nImage restoration is a long-standing low-level vision\nproblem that aims to restore high-quality images from low-\nquality images (e.g., downscaled, noisy and compressed im-\nages). While state-of-the-art image restoration methods are\nbased on convolutional neural networks, few attempts have\nbeen made with Transformers which show impressive per-\nformance on high-level vision tasks. In this paper, we pro-\npose a strong baseline model SwinIR for image restora-\ntion based on the Swin Transformer. SwinIR consists of\nthree parts: shallow feature extraction, deep feature extrac-\ntion and high-quality image reconstruction. In particular,\nthe deep feature extraction module is composed of several\nresidual Swin Transformer blocks (RSTB), each of which\nhas several Swin Transformer layers together with a resid-\nual connection. We conduct experiments on three represen-\ntative tasks: image super-resolution (including classical,\nlightweight and real-world image super-resolution), image\ndenoising (including grayscale and color image denoising)\nand JPEG compression artifact reduction. Experimental re-\nsults demonstrate that SwinIR outperforms state-of-the-art\nmethods on different tasks byup to 0.14∼0.45dB, while the\ntotal number of parameters can be reduced byup to 67%.\n1. Introduction\nImage restoration, such as image super-resolution (SR),\nimage denoising and JPEG compression artifact reduction,\naims to reconstruct the high-quality clean image from its\nlow-quality degraded counterpart. Since several revolu-\ntionary work [18, 40, 90, 91], convolutional neural net-\nworks (CNN) have become the primary workhorse for im-\nage restoration [43, 51, 43, 81, 92, 95, 24, 93, 46, 89, 88].\nMost CNN-based methods focus on elaborate architec-\nture designs such as residual learning [43, 51] and dense\nconnections [97, 81]. Although the performance is sig-\nniﬁcantly improved compared with traditional model-based\n*Corresponding author.\n0.2 0.4 0.6 0.8 1.0 1.2\nNumber of Parameters\n1e8\n32.45\n32.50\n32.55\n32.60\n32.65\n32.70PSNR (dB)\nEDSR (CVPR2017)\nRNAN (ICLR2019)\nOISR (CVPR2019)\nRDN (CVPR2018)\nRCAN (ECCV2018)\nIGNN (NeurIPS2020)\nHAN (ECCV2020)\nNLSA (CVPR2021)\nIPT (CVPR2021)\nSwinIR (ours)\nFigure 1: PSNR results v.s the total number of parameters of dif-\nferent methods for image SR (×4) on Set5 [3].\nmethods [73, 14, 28], they generally suffer from two basic\nproblems that stem from the basic convolution layer. First,\nthe interactions between images and convolution kernels are\ncontent-independent. Using the same convolution kernel to\nrestore different image regions may not be the best choice.\nSecond, under the principle of local processing, convolution\nis not effective for long-range dependency modelling.\nAs an alternative to CNN, Transformer [76] designs a\nself-attention mechanism to capture global interactions be-\ntween contexts and has shown promising performance in\nseveral vision problems [6, 74, 19, 56]. However, vision\nTransformers for image restoration [9, 5] usually divide\nthe input image into patches with ﬁxed size ( e.g., 48×48)\nand process each patch independently. Such a strategy in-\nevitably gives rise to two drawbacks. First, border pixels\ncannot utilize neighbouring pixels that are out of the patch\nfor image restoration. Second, the restored image may in-\ntroduce border artifacts around each patch. While this prob-\nlem can be alleviated by patch overlapping, it would intro-\nduce extra computational burden.\nRecently, Swin Transformer [56] has shown great\npromise as it integrates the advantages of both CNN and\nTransformer. On the one hand, it has the advantage of\nCNN to process image with large size due to the local at-\ntention mechanism. On the other hand, it has the advantage\nof Transformer to model long-range dependency with the\nshifted window scheme.\n1\narXiv:2108.10257v1  [eess.IV]  23 Aug 2021\nIn this paper, we propose an image restoration model,\nnamely SwinIR, based on Swin Transformer. More specif-\nically, SwinIR consists of three modules: shallow feature\nextraction, deep feature extraction and high-quality image\nreconstruction modules. Shallow feature extraction module\nuses a convolution layer to extract shallow feature, which\nis directly transmitted to the reconstruction module so as to\npreserve low-frequency information. Deep feature extrac-\ntion module is mainly composed of residual Swin Trans-\nformer blocks (RSTB), each of which utilizes several Swin\nTransformer layers for local attention and cross-window in-\nteraction. In addition, we add a convolution layer at the\nend of the block for feature enhancement and use a resid-\nual connection to provide a shortcut for feature aggregation.\nFinally, both shallow and deep features are fused in the re-\nconstruction module for high-quality image reconstruction.\nCompared with prevalent CNN-based image restoration\nmodels, Transformer-based SwinIR has several beneﬁts: (1)\ncontent-based interactions between image content and at-\ntention weights, which can be interpreted as spatially vary-\ning convolution [13, 21, 75]. (2) long-range dependency\nmodelling are enabled by the shifted window mechanism.\n(3) better performance with less parameters. For example,\nas shown in Fig. 1, SwinIR achieves better PSNR with less\nparameters compared with existing image SR methods.\n2. Related Work\n2.1. Image Restoration\nCompared to traditional image restoration methods [28,\n72, 73, 62, 32] which are generally model-based, learning-\nbased methods, especially CNN-based methods, have be-\ncome more popular due to their impressive performance.\nThey often learn mappings between low-quality and high-\nquality images from large-scale paired datasets. Since pi-\noneering work SRCNN [18] (for image SR), DnCNN [90]\n(for image denoising) and ARCNN [17] (for JPEG com-\npression artifact reduction), a ﬂurry of CNN-based mod-\nels have been proposed to improve model representation\nability by using more elaborate neural network architec-\nture designs, such as residual block [40, 7, 88], dense\nblock [81, 97, 98] and others [10, 42, 93, 78, 77, 79, 50, 48,\n49, 92, 70, 36, 83, 30, 11, 16, 96, 64, 38, 26, 41, 25]. Some\nof them have exploited the attention mechanism inside the\nCNN framework, such as channel attention [95, 15, 63],\nnon-local attention [52, 61] and adaptive patch aggrega-\ntion [100].\n2.2. Vision Transformer\nRecently, natural language processing model Trans-\nformer [76] has gained much popularity in the computer\nvision community. When used in vision problems such\nas image classiﬁcation [66, 19, 84, 56, 45, 55, 75], ob-\nject detection [6, 53, 74, 56], segmentation [84, 99, 56, 4]\nand crowd counting [47, 69], it learns to attend to impor-\ntant image regions by exploring the global interactions be-\ntween different regions. Due to its impressive performance,\nTransformer has also been introduced for image restora-\ntion [9, 5, 82]. Chen et al. [9] proposed a backbone model\nIPT for various restoration problems based on the stan-\ndard Transformer. However, IPT relies on large number of\nparameters (over 115.5M parameters), large-scale datasets\n(over 1.1M images) and multi-task learning for good perfor-\nmance. Cao et al. [5] proposed VSR-Transformer that uses\nthe self-attention mechanism for better feature fusion in\nvideo SR, but image features are still extracted from CNN.\nBesides, both IPT and VSR-Transformer are patch-wise at-\ntention, which may be improper for image restoration. In\naddition, a concurrent work [82] proposed a U-shaped ar-\nchitecture based on the Swin Transformer [56].\n3. Method\n3.1. Network Architecture\nAs shown in Fig. 2, SwinIR consists of three modules:\nshallow feature extraction, deep feature extraction and high-\nquality (HQ) image reconstruction modules. We employ the\nsame feature extraction modules for all restoration tasks, but\nuse different reconstruction modules for different tasks.\nShallow and deep feature extraction. Given a low-\nquality (LQ) input ILQ ∈RH×W×Cin (H, W and Cin are\nthe image height, width and input channel number, respec-\ntively), we use a 3 ×3 convolutional layer HSF(·) to extract\nshallow feature F0 ∈RH×W×C as\nF0 = HSF(ILQ), (1)\nwhere C is the feature channel number. The convolution\nlayer is good at early visual processing, leading to more\nstable optimization and better results [86]. It also provides\na simple way to map the input image space to a higher\ndimensional feature space. Then, we extract deep feature\nFDF ∈RH×W×C from F0 as\nFDF = HDF(F0), (2)\nwhere HDF(·) is the deep feature extraction module and it\ncontains K residual Swin Transformer blocks (RSTB) and\na 3 ×3 convolutional layer. More speciﬁcally, intermediate\nfeatures F1,F2,...,F K and the output deep featureFDF are\nextracted block by block as\nFi = HRSTBi(Fi−1), i = 1,2,...,K,\nFDF = HCONV(FK), (3)\nwhere HRSTBi(·) denotes the i-th RSTB and HCONV is the\nlast convolutional layer. Using a convolutional layer at the\n2\nShallow FeatureExtraction\nDeep FeatureExtractionHQ ImageReconstruction\nRSTBRSTBRSTBRSTBRSTBRSTBConv+\nConvSTL\nSTLSTLSTLSTL +\nLayerNormMSA+LayerNormMLP+(a) ResidualSwinTransformer Block(RSTB) (b) SwinTransformer Layer(STL)\nSTL\nFigure 2: The architecture of the proposed SwinIR for image restoration.\nend of feature extraction can bring the inductive bias of the\nconvolution operation into the Transformer-based network,\nand lay a better foundation for the later aggregation of shal-\nlow and deep features.\nImage reconstruction. Taking image SR as an example,\nwe reconstruct the high-quality image IRHQ by aggregating\nshallow and deep features as\nIRHQ = HREC(F0 + FDF), (4)\nwhere HREC(·) is the function of the reconstruction mod-\nule. Shallow feature mainly contain low-frequencies, while\ndeep feature focus on recovering lost high-frequencies.\nWith a long skip connection, SwinIR can transmit the low-\nfrequency information directly to the reconstruction mod-\nule, which can help deep feature extraction module focus\non high-frequency information and stabilize training. For\nthe implementation of reconstruction module, we use the\nsub-pixel convolution layer [68] to upsample the feature.\nFor tasks that do not need upsampling, such as image\ndenoising and JPEG compression artifact reduction, a single\nconvolution layer is used for reconstruction. Besides, we\nuse residual learning to reconstruct the residual between the\nLQ and the HQ image instead of the HQ image. This is\nformulated as\nIRHQ = HSwinIR(ILQ) +ILQ, (5)\nwhere HSwinIR(·) denotes the function of SwinIR.\nLoss function. For image SR, we optimize the parameters\nof SwinIR by minimizing the L1 pixel loss\nL= ∥IRHQ −IHQ∥1, (6)\nwhere IRHQ is obtained by takingILQ as the input of SwinIR,\nand IHQ is the corresponding ground-truth HQ image. For\nclassical and lightweight image SR, we only use the naive\nL1 pixel loss as same as previous work to show the effec-\ntiveness of the proposed network. For real-world image SR,\nwe use a combination of pixel loss, GAN loss and percep-\ntual loss [81, 89, 80, 27, 39, 81] to improve visual quality.\nFor image denoising and JPEG compression artifact re-\nduction, we use the Charbonnier loss [8]\nL=\n√\n∥IRHQ −IHQ∥2 + ϵ2, (7)\nwhere ϵis a constant that is empirically set to 10−3.\n3.2. Residual Swin Transformer Block\nAs shown in Fig. 2(a), the residual Swin Transformer\nblock (RSTB) is a residual block with Swin Transformer\nlayers (STL) and convolutional layers. Given the input fea-\nture Fi,0 of the i-th RSTB, we ﬁrst extract intermediate fea-\ntures Fi,1,Fi,2,...,F i,L by LSwin Transformer layers as\nFi,j = HSTLi,j (Fi,j−1), j = 1,2,...,L, (8)\nwhere HSTLi,j (·) is the j-th Swin Transformer layer in the\ni-th RSTB. Then, we add a convolutional layer before the\nresidual connection. The output of RSTB is formulated as\nFi,out = HCONVi(Fi,L) +Fi,0, (9)\nwhere HCONVi(·) is the convolutional layer in the i-th\nRSTB. This design has two beneﬁts. First, although Trans-\nformer can be viewed as a speciﬁc instantiation of spatially\nvarying convolution [21, 75], covolutional layers with spa-\ntially invariant ﬁlters can enhance the translational equivari-\nance of SwinIR. Second, the residual connection provides a\nidentity-based connection from different blocks to the re-\nconstruction module, allowing the aggregation of different\nlevels of features.\n3\nSwin Transformer layer. Swin Transformer layer\n(STL) [56] is based on the standard multi-head self-\nattention of the original Transformer layer [76]. The main\ndifferences lie in local attention and the shifted window\nmechanism. As shown in Fig. 2(b), given an input of size\nH×W ×C, Swin Transformer ﬁrst reshapes the input to\na HW\nM2 ×M2 ×C feature by partitioning the input into\nnon-overlapping M ×M local windows, where HW\nM2 is the\ntotal number of windows. Then, it computes the standard\nself-attention separately for each window ( i.e., local atten-\ntion). For a local window feature X ∈RM2×C, the query,\nkey and value matrices Q, Kand V are computed as\nQ= XPQ, K = XPK, V = XPV, (10)\nwhere PQ, PK and PV are projection matrices that are\nshared across different windows. Generally, we have\nQ,K,V ∈RM2×d. The attention matrix is thus computed\nby the self-attention mechanism in a local window as\nAttention(Q,K,V ) =SoftMax(QKT/\n√\nd+ B)V, (11)\nwhere B is the learnable relative positional encoding. In\npractice, following [76], we perform the attention function\nfor htimes in parallel and concatenate the results for multi-\nhead self-attention (MSA).\nNext, a multi-layer perceptron (MLP) that has two fully-\nconnected layers with GELU non-linearity between them is\nused for further feature transformations. The LayerNorm\n(LN) layer is added before both MSA and MLP, and the\nresidual connection is employed for both modules. The\nwhole process is formulated as\nX = MSA(LN(X)) +X,\nX = MLP(LN(X)) +X. (12)\nHowever, when the partition is ﬁxed for different lay-\ners, there is no connection across local windows. There-\nfore, regular and shifted window partitioning are used al-\nternately to enable cross-window connections [56], where\nshifted window partitioning means shifting the feature by\n(⌊M\n2 ⌋,⌊M\n2 ⌋) pixels before partitioning.\n4. Experiments\n4.1. Experimental Setup\nFor classical image SR, real-world image SR, image\ndenoising and JPEG compression artifact reduction, the\nRSTB number, STL number, window size, channel num-\nber and attention head number are generally set to 6, 6,\n8, 180 and 6, respectively. One exception is that the win-\ndow size is set to 7 for JPEG compression artifact reduc-\ntion, as we observe signiﬁcant performance drop when us-\ning 8, possibly because JPEG encoding uses 8 ×8 image\npartions. For lightweight image SR, we decrease RSTB\nnumber and channel number to 4 and 60, respectively. Fol-\nlowing [95, 63], when self-ensemble strategy [51] is used\nin testing, we mark the model with a symbol “+”, e.g.,\nSwinIR+. Due to page limit, training and evaluation details\nare provided in the supplementary.\n4.2. Ablation Study and Discussion\nFor ablation study, we train SwinIR on DIV2K [1] for\nclassical image SR (×2) and test it on Manga109 [60].\nImpact of channel number, RSTB number and STL\nnumber. We show the effects of channel number, RSTB\nnumber and STL number in a RSTB on model performance\nin Figs. 3(a), 3(b) and 3(c), respectively. It is observed that\nthe PSNR is positively correlated with these three hyper-\nparameters. For channel number, although the performance\nkeeps increasing, the total number of parameters grows\nquadratically. To balance the performance and model size,\nwe choose 180 as the channel number in rest experiments.\nAs for RSTB number and layer number, the performance\ngain becomes saturated gradually. We choose 6 for both of\nthem to obtain a relatively small model.\nImpact of patch size and training image number; model\nconvergence comparison. We compare the proposed\nSwinIR with a representative CNN-based model RCAN to\ncompare the difference of Transformer-based and CNN-\nbased models. From Fig. 3(d), one can see that SwinIR\nperforms better than RCAN on different patch sizes, and\nthe PSNR gain becomes larger when the patch size is larger.\nFig. 3(e) shows the impact of the number of training images.\nExtra images from Flickr2K are used in training when the\npercentage is larger than 100% (800 images). There are\ntwo observations. First, as expected, the performance of\nSwinIR rises with the training image number. Second, dif-\nferent from the observation in IPT that Transformer-based\nmodels are heavily relied on large amount of training data,\nSwinIR achieves better results than CNN-based models us-\ning the same training data, even when the dataset is small\n(i.e., 25%, 200 images). We also plot the PSNR during\ntraining for both SwinIR and RCAN in Fig. 3(f). It is clear\nthat SwinIR converges faster and better than RCAN, which\nis contradictory to previous observations that Transformer-\nbased models often suffer from slow model convergence.\nImpact of residual connection and convolution layer in\nRSTB. Table 1 shows four residual connection variants\nin RSTB: no residual connection, using 1 ×1 convolu-\ntion layer, using 3 ×3 convolution layer and using three\n3 ×3 convolution layers (channel number of the interme-\ndiate layer is set to one fourth of network channel num-\nber). From the table, we can have following observations.\nFirst, the residual connection in RSTB is important as it\nimproves the PSNR by 0.16dB. Second, using 1 ×1 con-\nvolution brings little improvement maybe because it cannot\n4\n60 90 120 150 180 210 240\nChannel Number\n39.00\n39.14\n39.28\n39.42\n39.56\n39.70PSNR (dB)\n(a)\n0 2 4 6 8 10 12\nRSTB Number\n38.7\n38.9\n39.1\n39.3\n39.5\n39.7PSNR (dB)\n (b)\n0 2 4 6 8 10 12\nLayer Number in a RSTB\n38.90\n39.06\n39.22\n39.38\n39.54\n39.70PSNR (dB)\n (c)\n32 40 48 56 64 72 80\nTraining Patch Size\n39.2\n39.3\n39.4\n39.5\n39.6\n39.7PSNR (dB)\nSwinIR\nRCAN (CNN-based)\n(d)\n0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5\nPercentage of Used Images\n39.00\n39.18\n39.36\n39.54\n39.72\n39.90PSNR (dB)\nSwinIR\nRCAN (CNN-based) (e)\n0 1 2 3 4 5\nTraining Iterations\n1e5\n38.2\n38.4\n38.6\n38.8\n39.0\n39.2\n39.4\n39.6PSNR (dB)\nSwinIR\nRCAN (CNN-based) (f)\nFigure 3: Ablation study on different settings of SwinIR. Results are tested on Manga109 [60] for image SR (×2).\nTable 1: Ablation study on RSTB design.\nDesign No residual 1 × 1 conv 3 × 3 conv Three 3 × 3 conv\nPSNR 39.42 39.45 39.58 39.56\nextract local neighbouring information as 3 ×3 convolution\ndoes. Third, although using three 3 ×3 convolution lay-\ners can reduce the number of parameters, the performance\ndrops slightly.\n4.3. Results on Image SR\nClassical image SR. Table 2 shows the quantitative com-\nparisons between SwinIR (middle size) and state-of-the-art\nmethods: DBPN [31], RCAN [95], RRDB [81], SAN [15],\nIGNN [100], HAN [63], NLSA [61] and IPT [9]. As one\ncan see, when trained on DIV2K, SwinIR achieves best\nperformance on almost all ﬁve benchmark datasets for all\nscale factors. The maximum PSNR gain reaches 0.26dB\non Manga109 for scale factor 4. Note that RCAN and\nHAN introduce channel and spatial attention, IGNN pro-\nposes adaptive patch feature aggregation, and NLSA is\nbased on the non-local attention mechanism. However, all\nthese CNN-based attention mechanisms perform worse than\nthe proposed Transformer-based SwinIR, which indicates\nthe effectiveness of the proposed model. When we train\nSwinIR on a larger dataset (DIV2K+Flickr2K), the perfor-\nmance further increases by a large margin (up to 0.47dB),\nachieving better accuracy than the same Transformer-based\nmodel IPT, even though IPT utilizes ImageNet (more than\n1.3M images) in training and has huge number of param-\neters (115.5M). In contrast, SwinIR has a small number\nof parameters (11.8M) even compared with state-of-the-art\nCNN-based models (15.4 ∼44.3M). As for runtime, repre-\nsentative CNN-based model RCAN, IPT and SwinIR take\nabout 0.2, 4.5s and 1.1s to test on a 1,024 ×1,024 im-\nage, respectively. Visual comparisons are show in Fig. 4.\nSwinIR can restore high-frequency details and alleviate the\nblurring artifacts, resulting in sharp and natural edges. In\ncontrast, most CNN-based methods produces blurry images\nor even incorrect textures. IPT generates better images com-\npared with CNN-based methods, but it suffers from image\ndistortions and border artifact.\nLightweight image SR. We also provide comparison of\nSwinIR (small size) with state-of-the-art lightweight im-\nage SR methods: CARN [2], FALSR-A [12], IMDN [35],\nLAPAR-A [44] and LatticeNet [57]. In addition to PSNR\nand SSIM, we also report the total numbers of parame-\nters and multiply-accumulate operations (evaluated on a\n1280 ×720 HQ image) to compare the model size and com-\nputational complexity of different models. As shown in Ta-\nble 3, SwinIR outperforms competitive methods by a PSNR\nmargin of up to 0.53dB on different benchmark datasets,\nwith similar total numbers of parameters and multiply-\naccumulate operations. This indicates that the SwinIR ar-\nchitecture is highly efﬁcient for image restoration.\nReal-world image SR. The ultimate goal of image SR\nis for real-world applications. Recently, Zhang et al. [89]\nproposed a practical degradation model BSRGAN for real-\nworld image SR and achieved surprising results in real\nscenarios1. To test the performance of SwinIR for real-\nworld SR, we re-train SwinIR by using the same degra-\ndation model as BSRGAN for low-quality image syn-\nthesis. Since there is no ground-truth high-quality im-\nages, we only provide visual comparison with representa-\ntive bicubic model ESRGAN [81] and state-of-the-art real-\nworld image SR models RealSR [37], BSRGAN [89] and\nReal-ESRGAN [80]. As shown in Fig. 5, SwinIR pro-\nduces visually pleasing images with clear and sharp edges,\nwhereas other compared methods may suffer from unsat-\nisfactory artifacts. In addition, to exploit the full poten-\ntial of SwinIR for real applications, we further propose a\n1https://github.com/cszn/BSRGAN\n5\nTable 2: Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for classical image SRon bench-\nmark datasets. Best and second best performance are in red and blue colors, respectively. Results on ×8 are provided in\nsupplementary.\nMethod Scale Training\nDataset\nSet5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nRCAN [95] ×2 DIV2K 38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786\nSAN [15] ×2 DIV2K 38.31 0.9620 34.07 0.9213 32.42 0.9028 33.10 0.9370 39.32 0.9792\nIGNN [100] ×2 DIV2K 38.24 0.9613 34.07 0.9217 32.41 0.9025 33.23 0.9383 39.35 0.9786\nHAN [63] ×2 DIV2K 38.27 0.9614 34.16 0.9217 32.41 0.9027 33.35 0.9385 39.46 0.9785\nNLSA [61] ×2 DIV2K 38.34 0.9618 34.08 0.9231 32.43 0.9027 33.42 0.9394 39.59 0.9789\nSwinIR (Ours) ×2 DIV2K 38.35 0.9620 34.14 0.9227 32.44 0.9030 33.40 0.9393 39.60 0.9792\nSwinIR+ (Ours) ×2 DIV2K 38.38 0.9621 34.24 0.9233 32.47 0.9032 33.51 0.9401 39.70 0.9794\nDBPN [31] ×2 DIV2K+Flickr2K 38.09 0.9600 33.85 0.9190 32.27 0.9000 32.55 0.9324 38.89 0.9775\nIPT [9] ×2 ImageNet 38.37 - 34.43 - 32.48 - 33.76 - - -\nSwinIR (Ours) ×2 DIV2K+Flickr2K 38.42 0.9623 34.46 0.9250 32.53 0.9041 33.81 0.9427 39.92 0.9797\nSwinIR+ (Ours) ×2 DIV2K+Flickr2K 38.46 0.9624 34.61 0.9260 32.55 0.9043 33.95 0.9433 40.02 0.9800\nRCAN [95] ×3 DIV2K 34.74 0.9299 30.65 0.8482 29.32 0.8111 29.09 0.8702 34.44 0.9499\nSAN [15] ×3 DIV2K 34.75 0.9300 30.59 0.8476 29.33 0.8112 28.93 0.8671 34.30 0.9494\nIGNN [100] ×3 DIV2K 34.72 0.9298 30.66 0.8484 29.31 0.8105 29.03 0.8696 34.39 0.9496\nHAN [63] ×3 DIV2K 34.75 0.9299 30.67 0.8483 29.32 0.8110 29.10 0.8705 34.48 0.9500\nNLSA [61] ×3 DIV2K 34.85 0.9306 30.70 0.8485 29.34 0.8117 29.25 0.8726 34.57 0.9508\nSwinIR (Ours) ×3 DIV2K 34.89 0.9312 30.77 0.8503 29.37 0.8124 29.29 0.8744 34.74 0.9518\nSwinIR+ (Ours) ×3 DIV2K 34.95 0.9316 30.83 0.8511 29.41 0.8130 29.42 0.8761 34.92 0.9526\nIPT [9] ×3 ImageNet 34.81 - 30.85 - 29.38 - 29.49 - - -\nSwinIR (Ours) ×3 DIV2K+Flickr2K 34.97 0.9318 30.93 0.8534 29.46 0.8145 29.75 0.8826 35.12 0.9537\nSwinIR+ (Ours) ×3 DIV2K+Flickr2K 35.04 0.9322 31.00 0.8542 29.49 0.8150 29.90 0.8841 35.28 0.9543\nRCAN [95] ×4 DIV2K 32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173\nSAN [15] ×4 DIV2K 32.64 0.9003 28.92 0.7888 27.78 0.7436 26.79 0.8068 31.18 0.9169\nIGNN [100] ×4 DIV2K 32.57 0.8998 28.85 0.7891 27.77 0.7434 26.84 0.8090 31.28 0.9182\nHAN [63] ×4 DIV2K 32.64 0.9002 28.90 0.7890 27.80 0.7442 26.85 0.8094 31.42 0.9177\nNLSA [61] ×4 DIV2K 32.59 0.9000 28.87 0.7891 27.78 0.7444 26.96 0.8109 31.27 0.9184\nSwinIR (Ours) ×4 DIV2K 32.72 0.9021 28.94 0.7914 27.83 0.7459 27.07 0.8164 31.67 0.9226\nSwinIR+ (Ours) ×4 DIV2K 32.81 0.9029 29.02 0.7928 27.87 0.7466 27.21 0.8187 31.88 0.9423\nDBPN [31] ×4 DIV2K+Flickr2K 32.47 0.8980 28.82 0.7860 27.72 0.7400 26.38 0.7946 30.91 0.9137\nIPT [9] ×4 ImageNet 32.64 - 29.01 - 27.82 - 27.26 - - -\nRRDB [81] ×4 DIV2K+Flickr2K 32.73 0.9011 28.99 0.7917 27.85 0.7455 27.03 0.8153 31.66 0.9196\nSwinIR (Ours) ×4 DIV2K+Flickr2K 32.92 0.9044 29.09 0.7950 27.92 0.7489 27.45 0.8254 32.03 0.9260\nSwinIR+ (Ours) ×4 DIV2K+Flickr2K 32.93 0.9043 29.15 0.7958 27.95 0.7494 27.56 0.8273 32.22 0.9273\nUrban100 (4×):img012\nHR VDSR [40] EDSR [51] RDN [97] OISR [33]\nSAN [15] RNAN [96] IGNN [100] IPT [9] SwinIR(ours)\nFigure 4: Visual comparison of bicubic image SR(×4) methods. Compared images are derived from [9]. Best viewed by zooming.\nTable 3: Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for lightweight image SRon bench-\nmark datasets. Best and second best performance are in red and blue colors, respectively.\nMethod Scale #Params #Mult-Adds Set5 [3] Set14 [87] BSD100 [58] Urban100 [34] Manga109 [60]\nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\nCARN [2] ×2 1,592K 222.8G 37.76 0.9590 33.52 0.9166 32.09 0.8978 31.92 0.9256 38.36 0.9765\nFALSR-A [12] ×2 1,021K 234.7G 37.82 0.959 33.55 0.9168 32.1 0.8987 31.93 0.9256 - -\nIMDN [35] ×2 694K 158.8G 38.00 0.9605 33.63 0.9177 32.19 0.8996 32.17 0.9283 38.88 0.9774\nLAPAR-A [44] ×2 548K 171.0G 38.01 0.9605 33.62 0.9183 32.19 0.8999 32.10 0.9283 38.67 0.9772\nLatticeNet [57] ×2 756K 169.5G 38.15 0.9610 33.78 0.9193 32.25 0.9005 32.43 0.9302 - -\nSwinIR (Ours) ×2 878K 195.6G 38.14 0.9611 33.86 0.9206 32.31 0.9012 32.76 0.9340 39.12 0.9783\nCARN [2] ×3 1,592K 118.8G 34.29 0.9255 30.29 0.8407 29.06 0.8034 28.06 0.8493 33.50 0.9440\nIMDN [35] ×3 703K 71.5G 34.36 0.9270 30.32 0.8417 29.09 0.8046 28.17 0.8519 33.61 0.9445\nLAPAR-A [44] ×3 544K 114.0G 34.36 0.9267 30.34 0.8421 29.11 0.8054 28.15 0.8523 33.51 0.9441\nLatticeNet [57] ×3 765K 76.3G 34.53 0.9281 30.39 0.8424 29.15 0.8059 28.33 0.8538 - -\nSwinIR (Ours) ×3 886K 87.2G 34.62 0.9289 30.54 0.8463 29.20 0.8082 28.66 0.8624 33.98 0.9478\nCARN [2] ×4 1,592K 90.9G 32.13 0.8937 28.60 0.7806 27.58 0.7349 26.07 0.7837 30.47 0.9084\nIMDN [35] ×4 715K 40.9G 32.21 0.8948 28.58 0.7811 27.56 0.7353 26.04 0.7838 30.45 0.9075\nLAPAR-A [44] ×4 659K 94.0G 32.15 0.8944 28.61 0.7818 27.61 0.7366 26.14 0.7871 30.42 0.9074\nLatticeNet [57] ×4 777K 43.6G 32.30 0.8962 28.68 0.7830 27.62 0.7367 26.25 0.7873 - -\nSwinIR (Ours) ×4 897K 49.6G 32.44 0.8976 28.77 0.7858 27.69 0.7406 26.47 0.7980 30.92 0.9151\n6\nLR (×4) ESRGAN [81] RealSR [37] BSRGAN [89] Real-ESRGAN [80] SwinIR (ours)\nFigure 5: Visual comparison of real-world image SR(×4) methods on real-world images.\nTable 4: Quantitative comparison (average PSNR/SSIM/PSNR-B) with state-of-the-art methods for\nJPEG compression artifact reductionon benchmark datasets. Best and second best performance are in red and\nblue colors, respectively.\nDataset q ARCNN [17] DnCNN-3 [90] QGAC [20] RNAN [96] RDN [98] DRUNet [88] SwinIR (ours)\nClassic5\n[22]\n10 29.03/0.7929/28.76 29.40/0.8026/29.13 29.84/0.8370/29.43 29.96/0.8178/29.62 30.00/0.8188/- 30.16/0.8234/29.81 30.27/0.8249/29.95\n20 31.15/0.8517/30.59 31.63/0.8610/31.19 31.98/0.8850/31.37 32.11/0.8693/31.57 32.15/0.8699/- 32.39/0.8734/31.80 32.52/0.8748/31.99\n30 32.51/0.8806/31.98 32.91/0.8861/32.38 33.22/0.9070/32.42 33.38/0.8924/32.68 33.43/0.8930/- 33.59/0.8949/32.82 33.73/0.8961/33.03\n40 33.32/0.8953/32.79 33.77/0.9003/33.20 - 34.27/0.9061/33.4 34.27/0.9061/- 34.41/0.9075/33.51 34.52/0.9082/33.66\nLIVE1\n[67]\n10 28.96/0.8076/28.77 29.19/0.8123/28.90 29.53/0.8400/29.15 29.63/0.8239/29.25 29.67/0.8247/- 29.79/0.8278/29.48 29.86/0.8287/29.50\n20 31.29/0.8733/30.79 31.59/0.8802/31.07 31.86/0.9010/31.27 32.03/0.8877/31.44 32.07/0.8882/- 32.17/0.8899/31.69 32.25/0.8909/31.70\n30 32.67/0.9043/32.22 32.98/0.9090/32.34 33.23/0.9250/32.50 33.45/0.9149/32.71 33.51/0.9153/- 33.59/0.9166/32.99 33.69/0.9174/33.01\n40 33.63/0.9198/33.14 33.96/0.9247/33.28 - 34.47/0.9299/33.66 34.51/0.9302/- 34.58/0.9312/33.93 34.67/0.9317/33.88\nlarge model and train it on much larger datasets. Exper-\niments show that it can deal with more complex corrup-\ntions and achieves even better performance on real-world\nimages than the current model. Due to page limit, the details\nare given in our project page https://github.com/\nJingyunLiang/SwinIR.\n4.4. Results on JPEG Compression Artifact Reduc-\ntion\nTable 4 shows the comparison of SwinIR with state-\nof-the-art JPEG compression artifact reduction methods:\nARCNN [17], DnCNN-3 [90], QGAC [20], RNAN [96],\nRDN [98] and DRUNet [88]. All of compared methods\nare CNN-based models. Following [98, 88], we test dif-\nferent methods on two benchmark datasets (Classic5 [22]\nand LIVE1 [67]) for JPEG quality factors 10, 20, 30 and\n40. As we can see, the proposed SwinIR has average PSNR\ngains of at least 0.11dB and 0.07dB on two testing datasets\nfor different quality factors. Besides, compared with the\nprevious best model DRUNet, SwinIR only has 11.5M pa-\nrameters, while DRUNet is a large model that has 32.7M\nparameters.\n4.5. Results on Image Denoising\nWe show grayscale and color image denoising re-\nsults in Table 5 and Table 6, respectively. Com-\npared methods include traditional models BM3D [14]\nand WNNM [29], CNN-based models DnCNN [90], IR-\nCNN [91], FFDNet [92], N3Net [65], NLRN [52], FOC-\nNet [38], RNAN [96], MWCNN [54] and DRUNet [88].\nFollowing [90, 88], the compared noise levels include 15,\n25 and 50. As one can see, our model achieves better per-\nformance than all compared methods. In particular, it sur-\npasses the state-of-the-art model DRUNet by up to 0.3dB\non the large Urban100 dataset that has 100 high-resolution\ntesting images. It is worth pointing out that SwinIR only\nhas 12.0M parameters, whereas DRUNet has 32.7M param-\neters. This indicates that the SwinIR architecture is highly\nefﬁcient in learning feature representations for restoration.\nThe visual comparison for grayscale and color image de-\nnoising of different methods are shown in Figs. 6 and 7.\nAs we can see, our method can remove heavy noise cor-\nruption and preserve high-frequency image details, result-\ning in sharper edges and more natural textures. By contrast,\nother methods suffer from either over-smoothness or over-\nsharpness, and cannot recover rich textures.\n5. Conclusion\nIn this paper, we propose a Swin Transformer-based im-\nage restoration model SwinIR. The model is composed of\nthree parts: shallow feature extraction, deep feature extrac-\n7\nTable 5: Quantitative comparison (average PSNR) with state-of-the-art methods for grayscale image denoisingon bench-\nmark datasets. Best and second best performance are in red and blue colors, respectively.\nDataset σ BM3D\n[14]\nWNNM\n[29]\nDnCNN\n[90]\nIRCNN\n[91]\nFFDNet\n[92]\nN3Net\n[65]\nNLRN\n[52]\nFOCNet\n[38]\nRNAN\n[96]\nMWCNN\n[54]\nDRUNet\n[88] SwinIR (ours)\nSet12\n[90]\n15 32.37 32.70 32.86 32.76 32.75 - 33.16 33.07 - 33.15 33.25 33.36\n25 29.97 30.28 30.44 30.37 30.43 30.55 30.80 30.73 - 30.79 30.94 31.01\n50 26.72 27.05 27.18 27.12 27.32 27.43 27.64 27.68 27.70 27.74 27.90 27.91\nBSD68\n[59]\n15 31.08 31.37 31.73 31.63 31.63 - 31.88 31.83 - 31.86 31.91 31.97\n25 28.57 28.83 29.23 29.15 29.19 29.30 29.41 29.38 - 29.41 29.48 29.50\n50 25.60 25.87 26.23 26.19 26.29 26.39 26.47 26.50 26.48 26.53 26.59 26.58\nUrban100\n[34]\n15 32.35 32.97 32.64 32.46 32.40 - 33.45 33.15 - 33.17 33.44 33.70\n25 29.70 30.39 29.95 29.80 29.90 30.19 30.94 30.64 - 30.66 31.11 31.30\n50 25.95 26.83 26.26 26.22 26.50 26.82 27.49 27.40 27.65 27.42 27.96 27.98\nTable 6: Quantitative comparison (average PSNR) with state-of-the-art methods for color image denoisingon benchmark\ndatasets. Best and second best performance are in red and blue colors, respectively.\nDataset σ BM3D\n[14]\nDnCNN\n[90]\nIRCNN\n[91]\nFFDNet\n[92]\nDSNet\n[64]\nRPCNN\n[85]\nBRDNet\n[71]\nRNAN\n[96]\nRDN\n[98]\nIPT\n[9]\nDRUNet\n[88] SwinIR (ours)\nCBSD68\n[59]\n15 33.52 33.90 33.86 33.87 33.91 - 34.10 - - - 34.30 34.42\n25 30.71 31.24 31.16 31.21 31.28 31.24 31.43 - - - 31.69 31.78\n50 27.38 27.95 27.86 27.96 28.05 28.06 28.16 28.27 28.31 28.39 28.51 28.56\nKodak24\n[23]\n15 34.28 34.60 34.69 34.63 34.63 - 34.88 - - - 35.31 35.34\n25 32.15 32.14 32.18 32.13 32.16 32.34 32.41 - - - 32.89 32.89\n50 28.46 28.95 28.93 28.98 29.05 29.25 29.22 29.58 29.66 29.64 29.86 29.79\nMcMaster\n[94]\n15 34.06 33.45 34.58 34.66 34.67 - 35.08 - - - 35.40 35.61\n25 31.66 31.52 32.18 32.35 32.40 32.33 32.75 - - - 33.14 33.20\n50 28.51 28.62 28.91 29.18 29.28 29.33 29.52 29.72 - 29.98 30.08 30.22\nUrban100\n[34]\n15 33.93 32.98 33.78 33.83 - - 34.42 - - - 34.81 35.13\n25 31.36 30.81 31.20 31.40 - 31.81 31.99 - - - 32.60 32.90\n50 27.93 27.59 27.70 28.05 - 28.62 28.56 29.08 29.38 29.71 29.61 29.82\nNoisy BM3D [14] DnCNN [90] FFDNet [92] DRUNet [88] SwinIR (ours)\nFigure 6: Visual comparison of grayscale image denoising(noise level 50) methods on image “ Monarch” from Set12 [90]. Compared\nimages are derived from [88].\nNoisy DnCNN [90] FFDNet [92] IPT [9] DRUNet [88] SwinIR (ours)\nFigure 7: Visual comparison ofcolor image denoising(noise level 50) methods on image “163085” from CBSD68 [59]. Compared images\nare derived from [88].\ntion and HR reconstruction modules. In particular, we use a\nstack of residual Swin Transformer blocks (RSTB) for deep\nfeature extraction, and each RSTB is composed of Swin\nTransformer layers, convolution layer and a residual con-\nnection. Extensive experiments show that SwinIR achieves\nstate-of-the-art performance on three representative image\nrestoration tasks and six different settings: classic image\nSR, lightweight image SR, real-world image SR, grayscale\nimage denoising, color image denoising and JPEG com-\npression artifact reduction, which demonstrates the effec-\ntiveness and generalizability of the proposed SwinIR. In the\nfuture, we will extend the model to other restoration tasks\nsuch as image deblurring and deraining.\nAcknowledgements This paper was partially supported\nby the ETH Zurich Fund (OK), a Huawei Technologies Oy\n(Finland) project, the China Scholarship Council and an\nAmazon AWS grant. Special thanks goes to Yijue Chen.\n8\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition Workshops, pages 126–135, 2017. 4\n[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn.\nFast, accurate, and lightweight super-resolution with cas-\ncading residual network. In European Conference on Com-\nputer Vision, pages 252–268, 2018. 5, 6\n[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and\nMarie line Alberi Morel. Low-complexity single-image\nsuper-resolution based on nonnegative neighbor embed-\nding. In British Machine Vision Conference, pages 135.1–\n135.10, 2012. 1, 6\n[4] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\naopeng Zhang, Qi Tian, and Manning Wang. Swin-unet:\nUnet-like pure transformer for medical image segmenta-\ntion. arXiv preprint arXiv:2105.05537, 2021. 2\n[5] Jiezhang Cao, Yawei Li, Kai Zhang, and Luc Van Gool.\nVideo super-resolution transformer. arXiv preprint\narXiv:2106.06847, 2021. 1, 2\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. InEuropean\nConference on Computer Vision, pages 213–229. Springer,\n2020. 1, 2\n[7] Lukas Cavigelli, Pascal Hager, and Luca Benini. Cas-cnn:\nA deep convolutional neural network for image compres-\nsion artifact suppression. In 2017 International Joint Con-\nference on Neural Networks, pages 752–759, 2017. 2\n[8] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert,\nand Michel Barlaud. Two deterministic half-quadratic reg-\nularization algorithms for computed imaging. In Interna-\ntional Conference on Image Processing, volume 2, pages\n168–172. IEEE, 1994. 3\n[9] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 12299–12310, 2021. 1, 2, 5, 6, 8\n[10] Yunjin Chen and Thomas Pock. Trainable nonlinear reac-\ntion diffusion: A ﬂexible framework for fast and effective\nimage restoration. IEEE transactions on pattern analysis\nand machine intelligence, 39(6):1256–1272, 2016. 2\n[11] Wenlong Cheng, Mingbo Zhao, Zhiling Ye, and Shuhang\nGu. Mfagan: A compression framework for memory-\nefﬁcient on-device super-resolution gan. arXiv preprint\narXiv:2107.12679, 2021. 2\n[12] Xiangxiang Chu, Bo Zhang, Hailong Ma, Ruijun Xu,\nand Qingyuan Li. Fast, accurate and lightweight super-\nresolution with neural architecture search. In International\nConference on Pattern Recognition, pages 59–64. IEEE,\n2020. 5, 6\n[13] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin\nJaggi. On the relationship between self-attention and con-\nvolutional layers. arXiv preprint arXiv:1911.03584, 2019.\n2\n[14] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik,\nand Karen Egiazarian. Image denoising by sparse 3-d\ntransform-domain collaborative ﬁltering. IEEE Transac-\ntions on image processing, 16(8):2080–2095, 2007. 1, 7,\n8\n[15] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single im-\nage super-resolution. In IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 11065–11074, 2019.\n2, 5, 6\n[16] Xin Deng, Yutong Zhang, Mai Xu, Shuhang Gu, and Yiping\nDuan. Deep coupled feedback network for joint exposure\nfusion and image super-resolution. IEEE Transactions on\nImage Processing, 30:3098–3112, 2021. 2\n[17] Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou\nTang. Compression artifacts reduction by a deep convolu-\ntional network. In IEEE International Conference on Com-\nputer Vision, pages 576–584, 2015. 2, 7\n[18] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Learning a deep convolutional network for image\nsuper-resolution. In European Conference on Computer Vi-\nsion, pages 184–199, 2014. 1, 2\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2\n[20] Max Ehrlich, Larry Davis, Ser-Nam Lim, and Abhinav\nShrivastava. Quantization guided jpeg artifact correction.\nIn European Conference on Computer Vision, pages 293–\n309, 2020. 7\n[21] Gamaleldin Elsayed, Prajit Ramachandran, Jonathon\nShlens, and Simon Kornblith. Revisiting spatial invariance\nwith low-rank local connectivity. In International Confer-\nence on Machine Learning, pages 2868–2879, 2020. 2, 3\n[22] Alessandro Foi, Vladimir Katkovnik, and Karen Egiazar-\nian. Pointwise shape-adaptive dct for high-quality de-\nnoising and deblocking of grayscale and color images.\nIEEE Transactions on Image Processing, 16(5):1395–1411,\n2007. 7\n[23] Rich Franzen. Kodak lossless true color image suite.\nsource: http://r0k. us/graphics/kodak, 4(2), 1999. 8\n[24] Manuel Fritsche, Shuhang Gu, and Radu Timofte. Fre-\nquency separation for real-world super-resolution. In IEEE\nConference on International Conference on Computer Vi-\nsion Workshops, pages 3599–3608, 2019. 1\n[25] Xueyang Fu, Menglu Wang, Xiangyong Cao, Xinghao\nDing, and Zheng-Jun Zha. A model-driven deep unfold-\ning method for jpeg artifacts removal. IEEE Transactions\non Neural Networks and Learning Systems, 2021. 2\n[26] Xueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, and\nJohn Paisley. Jpeg artifacts reduction via deep convolu-\ntional sparse coding. In IEEE International Conference on\nComputer Vision, pages 2501–2510, 2019. 2\n[27] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\n9\nand Yoshua Bengio. Generative adversarial nets. In Ad-\nvances in Neural Information Processing Systems, pages\n2672–2680, 2014. 3\n[28] Shuhang Gu, Nong Sang, and Fan Ma. Fast image su-\nper resolution via local regression. In IEEE Conference\non International Conference on Pattern Recognition, pages\n3128–3131, 2012. 1, 2\n[29] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu\nFeng. Weighted nuclear norm minimization with applica-\ntion to image denoising. In IEEE conference on computer\nvision and pattern recognition, pages 2862–2869, 2014. 7,\n8\n[30] Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang\nCao, Zeshuai Deng, Yanwu Xu, and Mingkui Tan. Closed-\nloop matters: Dual regression networks for single image\nsuper-resolution. In IEEE Conference on Computer Vision\nand Pattern Recognition, pages 5407–5416, 2020. 2\n[31] Muhammad Haris, Gregory Shakhnarovich, and Norim-\nichi Ukita. Deep back-projection networks for super-\nresolution. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 1664–1673, 2018. 5, 6\n[32] Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze\nremoval using dark channel prior. IEEE transactions on\nPattern Analysis and Machine Intelligence, 33(12):2341–\n2353, 2010. 2\n[33] Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu,\nMingyuan Yang, and Jian Cheng. Ode-inspired network\ndesign for single image super-resolution. In IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages\n1732–1741, 2019. 6\n[34] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja.\nSingle image super-resolution from transformed self-\nexemplars. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 5197–5206, 2015. 6, 8\n[35] Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei\nWang. Lightweight image super-resolution with informa-\ntion multi-distillation network. In ACM International Con-\nference on Multimedia, pages 2024–2032, 2019. 5, 6\n[36] Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin\nWang, and Qi Tian. Video super-resolution with recurrent\nstructure-detail network. In European Conference on Com-\nputer Vision, pages 645–660. Springer, 2020. 2\n[37] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li,\nand Feiyue Huang. Real-world super-resolution via ker-\nnel estimation and noise injection. In IEEE Conference\non Computer Vision and Pattern Recognition Workshops,\npages 466–467, 2020. 5, 7\n[38] Xixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc-\nnet: A fractional optimal control network for image denois-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6054–6063, 2019. 2, 7, 8\n[39] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision, pages 694–711.\nSpringer, 2016. 3\n[40] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accu-\nrate image super-resolution using very deep convolutional\nnetworks. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 1646–1654, 2016. 1, 2, 6\n[41] Yoonsik Kim, Jae Woong Soh, Jaewoo Park, Byeongyong\nAhn, Hyun-Seung Lee, Young-Su Moon, and Nam Ik Cho.\nA pseudo-blind convolutional neural network for the reduc-\ntion of compression artifacts. IEEE Transactions on Cir-\ncuits and Systems for Video Technology, 30(4):1121–1135,\n2019. 2\n[42] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\nHsuan Yang. Deep laplacian pyramid networks for fast\nand accurate super-resolution. In IEEE Conference on\nComputer Vision and Pattern Recognition, pages 624–632,\n2017. 2\n[43] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Ca-\nballero, Andrew Cunningham, Alejandro Acosta, Andrew\nAitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al.\nPhoto-realistic single image super-resolution using a gener-\native adversarial network. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 4681–4690,\n2017. 1\n[44] Wenbo Li, Kun Zhou, Lu Qi, Nianjuan Jiang, Jiangbo Lu,\nand Jiaya Jia. Lapar: Linearly-assembled pixel-adaptive\nregression network for single image super-resolution and\nbeyond. arXiv preprint arXiv:2105.10422, 2021. 5, 6\n[45] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transform-\ners. arXiv preprint arXiv:2104.05707, 2021. 2\n[46] Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwang-\ngil Jeon, and Wei Wu. Feedback network for image super-\nresolution. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 3867–3876, 2019. 1\n[47] Dingkang Liang, Xiwu Chen, Wei Xu, Yu Zhou, and Xiang\nBai. Transcrowd: Weakly-supervised crowd counting with\ntransformer. arXiv preprint arXiv:2104.09116, 2021. 2\n[48] Jingyun Liang, Andreas Lugmayr, Kai Zhang, Martin\nDanelljan, Luc Van Gool, and Radu Timofte. Hierarchi-\ncal conditional ﬂow: A uniﬁed framework for image super-\nresolution and image rescaling. In IEEE Conference on In-\nternational Conference on Computer Vision, 2021. 2\n[49] Jingyun Liang, Guolei Sun, Kai Zhang, Luc Van Gool, and\nRadu Timofte. Mutual afﬁne network for spatially variant\nkernel estimation in blind image super-resolution. In IEEE\nConference on International Conference on Computer Vi-\nsion, 2021. 2\n[50] Jingyun Liang, Kai Zhang, Shuhang Gu, Luc Van Gool, and\nRadu Timofte. Flow-based kernel prior with application to\nblind super-resolution. In IEEE Conference on Computer\nVision and Pattern Recognition, pages 10601–10610, 2021.\n2\n[51] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for sin-\ngle image super-resolution. In IEEE Conference on Com-\nputer Vision and Pattern Recognition Workshops, pages\n136–144, 2017. 1, 4, 6\n[52] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and\nThomas S Huang. Non-local recurrent network for image\nrestoration. arXiv preprint arXiv:1806.02919, 2018. 2, 7, 8\n10\n[53] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie\nChen, Xinwang Liu, and Matti Pietik ¨ainen. Deep learning\nfor generic object detection: A survey. International Jour-\nnal of Computer Vision, 128(2):261–318, 2020. 2\n[54] Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and\nWangmeng Zuo. Multi-level wavelet-cnn for image restora-\ntion. In IEEE conference on computer vision and pattern\nrecognition workshops, pages 773–782, 2018. 7, 8\n[55] Yun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli,\nand Luc Van Gool. Transformer in convolutional neural\nnetworks. arXiv preprint arXiv:2106.03180, 2021. 2\n[56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 1, 2, 4\n[57] Xiaotong Luo, Yuan Xie, Yulun Zhang, Yanyun Qu, Cui-\nhua Li, and Yun Fu. Latticenet: Towards lightweight image\nsuper-resolution with lattice block. In European Confer-\nence on Computer Vision, pages 272–289, 2020. 5, 6\n[58] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images\nand its application to evaluating segmentation algorithms\nand measuring ecological statistics. In IEEE Conference on\nInternational Conference on Computer Vision, pages 416–\n423, 2001. 6\n[59] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images and\nits application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In IEEE International Con-\nference on Computer Vision, pages 416–423, 2001. 8\n[60] Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fuji-\nmoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu\nAizawa. Sketch-based manga retrieval using manga109\ndataset. Multimedia Tools and Applications, 76(20):21811–\n21838, 2017. 4, 5, 6\n[61] Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image super-\nresolution with non-local sparse attention. In IEEE Confer-\nence on Computer Vision and Pattern Recognition, pages\n3517–3526, 2021. 2, 5, 6\n[62] Tomer Michaeli and Michal Irani. Nonparametric blind\nsuper-resolution. In IEEE Conference on International\nConference on Computer Vision, pages 945–952, 2013. 2\n[63] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lian-\nping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao,\nand Haifeng Shen. Single image super-resolution via a\nholistic attention network. In European Conference on\nComputer Vision, pages 191–207, 2020. 2, 4, 5, 6\n[64] Yali Peng, Lu Zhang, Shigang Liu, Xiaojun Wu, Yu Zhang,\nand Xili Wang. Dilated residual networks with symmet-\nric skip connection for image denoising. Neurocomputing,\n345:67–76, 2019. 2, 8\n[65] Tobias Pl ¨otz and Stefan Roth. Neural nearest neighbors net-\nworks. arXiv preprint arXiv:1810.12575, 2018. 7, 8\n[66] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 2\n[67] HR Sheikh. Live image quality assessment database release\n2. http://live. ece. utexas. edu/research/quality, 2005. 7\n[68] Wenzhe Shi, Jose Caballero, Ferenc Husz ´ar, Johannes\nTotz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and\nZehan Wang. Real-time single image and video super-\nresolution using an efﬁcient sub-pixel convolutional neural\nnetwork. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1874–1883, 2016. 3\n[69] Guolei Sun, Yun Liu, Thomas Probst, Danda Pani Paudel,\nNikola Popovic, and Luc Van Gool. Boosting crowd count-\ning with transformers. arXiv preprint arXiv:2105.10926,\n2021. 2\n[70] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu.\nMemnet: A persistent memory network for image restora-\ntion. In IEEE International Conference on Computer Vi-\nsion, pages 4539–4547, 2017. 2\n[71] Chunwei Tian, Yong Xu, and Wangmeng Zuo. Image de-\nnoising using deep cnn with batch renormalization. Neural\nNetworks, 121:461–473, 2020. 8\n[72] Radu Timofte, Vincent De Smet, and Luc Van Gool. An-\nchored neighborhood regression for fast example-based\nsuper-resolution. In IEEE Conference on International\nConference on Computer Vision, pages 1920–1927, 2013.\n2\n[73] Radu Timofte, Vincent De Smet, and Luc Van Gool. A+:\nAdjusted anchored neighborhood regression for fast super-\nresolution. In Asian Conference on Computer Vision, pages\n111–126, 2014. 1, 2\n[74] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J ´egou. Train-\ning data-efﬁcient image transformers & distillation through\nattention. arXiv preprint arXiv:2012.12877, 2020. 1, 2\n[75] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,\nNiki Parmar, Blake Hechtman, and Jonathon Shlens. Scal-\ning local self-attention for parameter efﬁcient visual back-\nbones. arXiv preprint arXiv:2103.12731, 2021. 2, 3\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. arXiv\npreprint arXiv:1706.03762, 2017. 1, 2, 4\n[77] Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu\nXu, Jungang Yang, Wei An, and Yulan Guo. Unsuper-\nvised degradation representation learning for blind super-\nresolution. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 10581–10590, 2021. 2\n[78] Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping\nLin, Jungang Yang, Wei An, and Yulan Guo. Learning par-\nallax attention for stereo image super-resolution. In IEEE\nConference on Computer Vision and Pattern Recognition,\npages 12250–12259, 2019. 2\n[79] Longguang Wang, Yingqian Wang, Zaiping Lin, Jungang\nYang, Wei An, and Yulan Guo. Learning a single net-\nwork for scale-arbitrary super-resolution. In IEEE Con-\nference on International Conference on Computer Vision,\npages 10581–10590, 2021. 2\n[80] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.\nReal-esrgan: Training real-world blind super-resolution\n11\nwith pure synthetic data. arXiv preprint arXiv:2107.10833,\n2021. 3, 5, 7\n[81] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan:\nEnhanced super-resolution generative adversarial networks.\nIn European Conference on Computer Vision Workshops,\npages 701–710, 2018. 1, 2, 3, 5, 6, 7\n[82] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and\nJianzhuang Liu. Uformer: A general u-shaped transformer\nfor image restoration. arXiv preprint arXiv:2106.03106,\n2021. 2\n[83] Yunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Long-\ncun Jin, and Hengjie Song. Unsupervised real-world im-\nage super resolution via domain-distance aware training. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 13385–13394, 2021. 2\n[84] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka,\nJoseph Gonzalez, Kurt Keutzer, and Peter Vajda. Vi-\nsual transformers: Token-based image representation\nand processing for computer vision. arXiv preprint\narXiv:2006.03677, 2020. 2\n[85] Zhihao Xia and Ayan Chakrabarti. Identifying recurring\npatterns with deep neural networks for natural image de-\nnoising. In IEEE Winter Conference on Applications of\nComputer Vision, pages 2426–2434, 2020. 8\n[86] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Pi-\notr Doll ´ar, and Ross Girshick. Early convolutions help\ntransformers see better. arXiv preprint arXiv:2106.14881,\n2021. 2\n[87] Roman Zeyde, Michael Elad, and Matan Protter. On single\nimage scale-up using sparse-representations. In Interna-\ntional Conference on Curves and Surfaces, pages 711–730,\n2010. 6\n[88] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc\nVan Gool, and Radu Timofte. Plug-and-play image restora-\ntion with deep denoiser prior.IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2021. 1, 2, 7, 8\n[89] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-\nfte. Designing a practical degradation model for deep blind\nimage super-resolution. In IEEE Conference on Interna-\ntional Conference on Computer Vision, 2021. 1, 3, 5, 7\n[90] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising. IEEE transactions on\nimage processing, 26(7):3142–3155, 2017. 1, 2, 7, 8\n[91] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 3929–3938, 2017. 1, 7, 8\n[92] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet:\nToward a fast and ﬂexible solution for cnn-based im-\nage denoising. IEEE Transactions on Image Processing,\n27(9):4608–4622, 2018. 1, 2, 7, 8\n[93] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a\nsingle convolutional super-resolution network for multiple\ndegradations. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 3262–3271, 2018. 1, 2\n[94] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color\ndemosaicking by local directional interpolation and nonlo-\ncal adaptive thresholding. Journal of Electronic imaging,\n20(2):023016, 2011. 8\n[95] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In European Confer-\nence on Computer Vision, pages 286–301, 2018. 1, 2, 4, 5,\n6\n[96] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and\nYun Fu. Residual non-local attention networks for image\nrestoration. arXiv preprint arXiv:1903.10082, 2019. 2, 6,\n7, 8\n[97] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong,\nand Yun Fu. Residual dense network for image super-\nresolution. In IEEE Conference on Computer Vision and\nPattern Recognition, pages 2472–2481, 2018. 1, 2, 6\n[98] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image restoration.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 43(7):2480–2495, 2020. 2, 7, 8\n[99] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 6881–6890, 2021. 2\n[100] Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, and\nChen Change Loy. Cross-scale internal graph neu-\nral network for image super-resolution. arXiv preprint\narXiv:2006.16673, 2020. 2, 5, 6\n12",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7681643962860107
    },
    {
      "name": "Computer science",
      "score": 0.6831682920455933
    },
    {
      "name": "Image restoration",
      "score": 0.6010210514068604
    },
    {
      "name": "Computer vision",
      "score": 0.57956862449646
    },
    {
      "name": "Transformer",
      "score": 0.5793987512588501
    },
    {
      "name": "Compression artifact",
      "score": 0.5709460973739624
    },
    {
      "name": "Residual",
      "score": 0.5685501098632812
    },
    {
      "name": "JPEG",
      "score": 0.562606692314148
    },
    {
      "name": "Feature extraction",
      "score": 0.4679948687553406
    },
    {
      "name": "Image quality",
      "score": 0.43171170353889465
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4191240668296814
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41742199659347534
    },
    {
      "name": "Image compression",
      "score": 0.307946115732193
    },
    {
      "name": "Image processing",
      "score": 0.2651016414165497
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2617822289466858
    },
    {
      "name": "Engineering",
      "score": 0.15755590796470642
    },
    {
      "name": "Algorithm",
      "score": 0.13389334082603455
    },
    {
      "name": "Voltage",
      "score": 0.07416850328445435
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}