{
  "title": "Vision Transformer with Progressive Sampling",
  "url": "https://openalex.org/W3190378193",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2474131909",
      "name": "Xiao-yu Yue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106137541",
      "name": "Shuyang Sun",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2095970592",
      "name": "Zhanghui Kuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005149222",
      "name": "Meng Wei",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2941318783",
      "name": "Philip Torr",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2492235083",
      "name": "Wayne Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154770419",
      "name": "Lin Dahua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2122710056",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W6786423403",
    "https://openalex.org/W6757040593",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3003642782",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W6628927728",
    "https://openalex.org/W6767279747",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W6682137061",
    "https://openalex.org/W6751288959",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W6785665406",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6775170262",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6767111047",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6772853553",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W6776721752",
    "https://openalex.org/W6780775906",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W2928555987",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W6763239785",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2800748770",
    "https://openalex.org/W2903226808",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3204057394",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2969627337",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3110267192",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2994760783",
    "https://openalex.org/W2972969579",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2964036520",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2951527505",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W3030163527"
  ],
  "abstract": "Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Transformer (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8% higher than the vanilla ViT in terms of top-1 accuracy with about $4\\times$ fewer parameters and $10\\times$ fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.",
  "full_text": "Vision Transformer with Progressive Sampling\nXiaoyu Yue‚àó1 Shuyang Sun‚àó2 Zhanghui Kuang3 Meng Wei4\nPhilip Torr2 Wayne Zhang3,6 Dahua Lin1,5\n1Centre for Perceptual and Interactive Intelligence 2University of Oxford\n3SenseTime Research 4Tsinghua University 5The Chinese University of Hong Kong\n6Qing Yuan Research Institute, Shanghai Jiao Tong University\nAbstract\nTransformers with powerful global relation modeling\nabilities have been introduced to fundamental computer vi-\nsion tasks recently. As a typical example, the Vision Trans-\nformer (ViT) directly applies a pure transformer architec-\nture on image classiÔ¨Åcation, by simply splitting images into\ntokens with a Ô¨Åxed length, and employing transformers to\nlearn relations between these tokens. However, such naive\ntokenization could destruct object structures, assign grids\nto uninterested regions such as background, and introduce\ninterference signals. To mitigate the above issues, in this\npaper, we propose an iterative and progressive sampling\nstrategy to locate discriminative regions. At each itera-\ntion, embeddings of the current sampling step are fed into\na transformer encoder layer, and a group of sampling off-\nsets is predicted to update the sampling locations for the\nnext step. The progressive sampling is differentiable. When\ncombined with the Vision Transformer, the obtained PS-ViT\nnetwork can adaptively learn where to look. The proposed\nPS-ViT is both effective and efÔ¨Åcient. When trained from\nscratch on ImageNet, PS-ViT performs 3.8% higher than\nthe vanilla ViT in terms of top-1 accuracy with about 4√ó\nfewer parameters and 10√ófewer FLOPs. Code is available\nat https://github.com/yuexy/PS-ViT.\n1. Introduction\nTransformers [39, 11] have become the de-facto standard\narchitecture for natural language processing tasks. Thanks\nto their powerful global relation modeling abilities, re-\nsearchers attempt to introduce them to fundamental com-\nputer vision tasks such as image classiÔ¨Åcation [6, 38, 12,\n44, 32], object detection [56, 4, 53, 10, 37] and image seg-\nmentation [40] recently. However, transformers are initially\ntailored for processing mid-size sequences, and of quadratic\ncomputational complexity w.r.t. the sequence length. Thus,\n‚àóEqual Contribution.\nPatches\nTransformers\n‚Ä¶\nTransformer\nTokens\nOffsets\nTransformers\nx N\n(a)\nPatches\nTransformers\n‚Ä¶\nTransformer\nTokens\nOffsets\nTransformers\nx N\n(b)\nFigure 1. Comparison between the naive tokenization scheme in\nViT [12] and the progressive sampling in our proposed PS-ViT.\n(a) The naive tokenization scheme generates a sequence of image\npatches which are embedded and then fed into a stack of trans-\nformers. (b) Our PS-ViT iteratively samples discriminative loca-\ntions. √óN indicates N sampling iterations.\nthey cannot directly be used to process images with massive\npixels.\nTo overcome the computational complexity issue, the pi-\noneer Vision Transformer (ViT) [12] adopts a naive tok-\nenization scheme that partitions one image into a sequence\nof regularly spaced patches, which are linearly projected\ninto tokens. In this way, the image is converted into hun-\ndreds of visual tokens, which are fed into a stack of trans-\nformer encoder layers for classiÔ¨Åcation. ViT attains ex-\ncellent results, especially when pre-trained on large-scale\ndatasets, which proves that full-transformer architecture is\na promising alternative for vision tasks. However, the lim-\nitations of such a naive tokenization scheme are obvious.\nFirst, the hard splitting might separate some highly seman-\ntically correlated regions that should be modeled with the\nsame group of parameters, which destructs inherent object\narXiv:2108.01684v1  [cs.CV]  3 Aug 2021\n69\n71\n73\n75\n77\n79\n81\n83\n0 5 10 15 20\nAccuracy (%)\nFLOPS (B)\nPS-ViT Deit ViT ResNet\n69\n71\n73\n75\n77\n79\n81\n83\n0 20 40 60 80 100\nAccuracy (%)\nParams (M)\nPS-ViT Deit ViT ResNet\n75\n76\n77\n78\n79\n80\n81\n82\n83\n0 500 1000 1500 2000 2500\nAccuracy (%)\nSpeed (img/s)\nPS-ViT Deit ViT\n4√ó 2√ó\nFigure 2. Comparisons between PS-ViT with state-of-the-art networks in terms of top-1 accuracy on ImageNet, parameter number, FLOPs,\nand speed. The chart on the left, middle and right show top-1 accuracy vs. parameter numbers, FLOPs and speed respectively. The speed\nis tested on the same V100 with a batch size of 128 for fair comparison.\nstructures and makes the input patches to be less informa-\ntive. Figure 1 (a) shows that the cat head is divided into\nseveral parts, resulting in recognition challenges based on\none part only. Second, tokens are placed on regular grids\nirrespective of the underlying image content. Figure 1 (a)\nshows that most grids focus on the uninterested background,\nwhich might lead to the interesting foreground object is sub-\nmerged in interference signals.\nThe human vision system organizes visual information\nin a completely different way than indiscriminately process-\ning a whole scene at once. Instead, it progressively and se-\nlectively focuses attention on interesting parts of the visual\nspace when and where it is needed while ignoring uninter-\nested parts, combining information from different Ô¨Åxations\nover time to understand the scene [33].\nInspired by the procedure above, we propose a novel\ntransformer-based progressive sampling module, which is\nable to learn where to look in images, to mitigate the issues\ncaused by the naive tokenization scheme in ViT [12]. In-\nstead of sampling from Ô¨Åxed locations, our proposed mod-\nule updates the sampling locations in an iterative manner.\nAs shown in Figure 1 (b), at each iteration, tokens of the\ncurrent sampling step are fed into a transformer encoder\nlayer, and a group of sampling offsets is predicted to update\nthe sampling locations for the next step. This mechanism\nutilizes the capabilities of the transformer to capture global\ninformation to estimate offsets towards regions of interest,\nby combining with the local contexts and the positions of\ncurrent tokens. In this way, attention progressively con-\nverges to discriminative regions of images step by step as\nwhat human vision does. Our proposed progressive sam-\npling is differentiable, and readily plugged into ViT instead\nof the hard splitting, to construct end-to-end Vision Trans-\nforms with Progressive Sampling Networks dubbed as PS-\nViT. Thanks to task-driven training, PS-ViT tends to sample\nobject regions correlated with semantic structures. More-\nover, it pays more attention to foreground objects while less\nto ambiguous background compared with simple tokeniza-\ntion.\nThe proposed PS-ViT outperforms the current state-of-\nthe-art transformer-based approaches when trained from\nscratch on ImageNet. Concretely, it achieves 82.3% top-\n1 accuracy on ImageNet which is higher than that of the\nrecent ViT‚Äôs variant DeiT [38] with only about 4√ófewer\nparameters and 2√ó fewer FLOPs. As shown in Fig-\nure 2, we observe that PS-ViT is remarkably better, faster,\nand more parameter-efÔ¨Åcient compared to state-of-the-art\ntransformer-based networks ViT and DeiT.\n2. Related Work\nTransformers are Ô¨Årst proposed for sequence models\nsuch as machine translation [39]. BeneÔ¨Åting from their\npowerful global relation modeling abilities, and highly ef-\nÔ¨Åcient training, transformers achieve signiÔ¨Åcant improve-\nments and become the de-facto standard in many Natural\nLanguage Processing (NLP) tasks [11, 3, 30, 29, 48].\nTransformers in Computer Vision. Inspired by the suc-\ncess of transformers in NLP tasks, many researchers attempt\nto apply transformers, or attention mechanism in computer\nvision tasks, such as image classiÔ¨Åcation [6, 38, 12, 44, 32,\n2, 18, 36], object detection [56, 4, 53, 10, 50, 37], image\nsegmentation [40], low-level image processing [5, 47, 27],\nvideo understanding [42] generation [43], multi-modality\nunderstanding [7, 35, 22], and Optical Character Recogni-\ntion (OCR) [41, 49, 34]. Transformers‚Äô powerful modelling\ncapacity comes at the cost of computational complexity.\nTheir consumed memory and computation grow quadrati-\ncally w.r.t. the token length, which prevents them to being\ndirectly applied to images with massive pixels as tokens.\nAxial attention [17] applied attention along a single axis of\nthe tensor without Ô¨Çattening to reduce the computational re-\nsource requirement. iGPT [6] simply down-sampled images\nto one low resolution, trained a sequence of transformers\nto auto-regressively predict pixels and achieved promising\nperformance with a linear probe. ViT [12] regularly par-\ntitioned one high-resolution image into 16 √ó16 patches,\nwhich were fed into one pure transformer architecture for\nclassiÔ¨Åcation, and attained excellent results even compared\nto state-of-the-art convolutional networks for the Ô¨Årst time.\nHowever, ViT needs pretraining on large-scale datasets,\nthereby limiting their adoption. DeiT [38] proposed a data-\nefÔ¨Åcient training strategy and a teacher-student distillation\nmechanism [16], and improved ViT‚Äôs performance greatly.\nMoreover, it is trained on ImageNet only, and thus consid-\nerably simpliÔ¨Åes the overall pipeline of ViT. Our proposed\nPS-ViT also starts from ViT. Instead of splitting pixels into a\nsmall number of visual tokens, we propose a novel progres-\nsive sampling strategy to avoid structure destruction and fo-\ncus more attention on interesting regions.\nHard Visual Attention. PS-ViT as a series of glimpses\nakin to hard visual attention [25, 1, 46, 13], makes deci-\nsions based on a subset of locations only in the input im-\nage. However, PS-ViT is differentiable and can be easily\ntrained in an end-to-end fashion while previous hard visual\nattention approaches are non-differentiable and trained with\nReinforcement Learning (RL) methods. These RL-based\nmethods have proven to be less effective when scaled onto\nmore complex datasets [13]. Moreover, our PS-ViT targets\nat progressively sampling discriminative tokens for Vision\nTransformers while previous approaches locate interested\nregions for convolutional neural networks [25, 1, 13] or se-\nquence decoders [46]. Our work is also related to the de-\nformable convolution [9, 54] and deformable attention [55]\nmechanism, however, the motivation and the way of pixel\nsampling in this work are different from what proposed in\nthe deformable convolution and attention mechanism.\n3. Methodology\nIn this section, we Ô¨Årst introduce our progressive sam-\npling strategy and then describe the overall architecture of\nour proposed PS-ViT network. Finally, we will elaborate on\nthe details of PS-ViT. Symbols and notations of our method\nare presented in Table 1.\n3.1. Progressive Sampling\nViT [12] regularly partitions one image into 16 √ó16\npatches, which are linearly projected into a set of tokens, re-\ngardless of the content importance of image regions and the\nintegral structure of objects. To pay more attention to inter-\nesting regions of images and mitigate the problem of struc-\nture destruction, we propose one novel progressive sam-\npling module. As it is differentiable, it is adaptively driven\nvia the following vision transformer based image classiÔ¨Åca-\ntion task.\nOur progressive sampling module is an iterative frame-\nwork. Given the input feature map F ‚ààRC√óH√óW with C,\nH, and W being the feature channel dimension, height and\nName Description\nF ‚ààRC√óH√óW the feature map extracted by the\nfeature extractor module\npt ‚ààR2√ó(n√ón) the sampling points at the iteration t\nPt ‚ààRC√ó(n√ón) the position embeddings at the\niteration t\not ‚ààR2√ó(n√ón) the sampling offsets at the\niteration t\nT\n‚Ä≤\nt ‚ààRC√ó(n√ón) tokens sampled from F at\nthe iteration t\nTt ‚ààRC√ó(n√ón) tokens predicted by the progressive\nsampling module at the iteration t\nC the dimension of tokens\nN the iteration number in the\nprogressive sampling module\nNv\nthe transformer layer number in the\nvision transformer module\nM the head number\nTable 1. The list of symbols and notations used in this paper.\nùëù0\nùêπ\nùëù2ùëù1\nFeature extractor\nSampling Layer\nFC\n‚Ä¶\nTransformer \nEncoder Layer\nùêìùë°\n‚Ä≤\nùê®ùë°\nùêìùë°\n‚Ä¶\n+\nùê©ùë°\nùêÖ\nùêìùë°‚àí1\n+\nSampling Layer\nùëá1 ùëá2\n‚Ä¶\nPosition \nEncoding ùêèùë°\n√óùëÅ\nùê©ùë°+1\nFigure 3. The architecture of the progressive sampling module.\nAt each iteration, given the sampling location pt and the feature\nmap F, we sample the initial tokens T\n‚Ä≤\nt at pt over F, which are\nelement-wisely added with the positional encodings Pt generated\nbased on pt, and the output tokens Tt‚àí1 of the last iteration, and\nthen fed into one transformation encoder layer to predict the tokens\nTt of the current iteration. The offset matrix ot is predicted via\none fully-connected layer based on Tt, which is added with pt\nto obtain the sampling positions pt+1 for the next iteration. The\nabove procedure is iterated N times.\nwidth respectively, it Ô¨Ånally outputs a sequence of tokens\nTN ‚ààRC√ó(n√ón), where (n√ón) indicates the number of\nsamples over one image and N is the total iterative number\nin the progressive sampling module.\nAs shown in Figure 3, at each iteration, the sampling lo-\ncations are updated via adding them with the offset vectors\nof the last iteration. Formally,\npt+1 = pt + ot, t‚àà{1,...,N ‚àí1}, (1)\nwhere pt ‚ààR2√ó(n√ón), and ot ‚ààR2√ó(n√ón) indicate the\nsampling location matrix and the offset matrix predicted at\nthe iteration t. For the Ô¨Årst iteration, we initialize the p1\nto be the regularly-spaced locations as done in ViT [12].\nConcretely, the i-th location pi\n1 is given by\npi\n1 = [œÄy\ni sh + sh/2,œÄx\ni sw + sw/2]\nœÄy\ni = ‚åäi/n‚åã\nœÄx\ni = i‚àíœÄy\ni ‚àón\nsh = H/n\nsw = W/n,\n(2)\nwhere œÄy\ni and œÄx\ni map the location index i to the row in-\ndex and the column one respectively. ‚åä¬∑‚åãindicates the Ô¨Çoor\noperation. sh and sw are the step size in the y and xaxial\ndirection respectively. Initial tokens are then sampled over\nthe input feature map at the sampled locations as follows:\nT\n‚Ä≤\nt = F(pt), t‚àà{1,...,N }, (3)\nwhere T\n‚Ä≤\nt ‚ààRC√ó(n√ón) is the initial sampled tokens at the\niteration t. As elements of pt are fractional, the sampling is\nimplemented via the bilinear interpolation operation, which\nis differentiable w.r.t. both the input feature map F and the\nsampling locations pt. The initial sampled tokens, the out-\nput tokens of the last iteration, and the positional encodings\nof the current sampling locations are further element-wisely\nadded before being fed into one transformer encoder layer\nto get the output tokens of the current iteration. Formally,\nwe have\nPt = Wtpt\nXt = T\n‚Ä≤\nt ‚äïPt ‚äïTt‚àí1\nTt = Transformer(Xt), t‚àà{1,...,N },\n(4)\nwhere Wt ‚ààRC√ó2 is the linear transformation that projects\nthe sampled locations pt to the positional encoding matrix\nPt of size C √ó(n√ón), all iterations share the same Wt.\n‚äïindicates the element-wise addition while Transformer(¬∑)\nis the mulit-head self-attention based transformer encoder\nlayer, which will be elaborated in Section 3.3. Note that T0\nis a zero matrix in Equation (4). ViT [12] uses the 2-D sinu-\nsoidal positional embeddings of patch indices. Since their\npatches are regularly spaced, patch indices can exactly en-\ncode the relative coordinates of patch centers in one image.\nHowever, it does not hold true in our case as our sampled lo-\ncations are non-isometric as shown in Figure 1. We project\nthe normalized absolute coordinates of sampled locations to\none embedding space as the positional embeddings instead.\nFinally, the sampling location offsets are predicted for the\nnext iteration except at the last iteration as follows:\not = MtTt, t‚àà{1,...,N ‚àí1}, (5)\nwhere Mt ‚ààR2√óC is the learnable linear transformation\nfor predicting the sampling offset matrix.\nWith the progressive sampling strategy, the sampled lo-\ncations progressively converge to interesting regions of im-\nages. Therefore, we name it by progressive sampling.\n3.2. Overall Architecture\nAs shown in Figure 4, the architecture of the PS-ViT\nconsists of four main components: 1) a feature extrac-\ntor module to predict dense tokens; 2) a progressive sam-\npling module to sample discriminative locations; 3) a vision\ntransformer module that follows the similar conÔ¨Åguration of\nViT [12] and DeiT [38]; 4) a classiÔ¨Åcation module.\nThe feature extractor module aims at extracting the dense\nfeature map F, where the progressive sampling module can\nsimple tokens Tt. Each pixel of the dense feature map F\ncan be treated as a token associated with a patch of the im-\nage. We employ the convolutional stem and the Ô¨Årst two\nresidual blocks in the Ô¨Årst stage of the ResNet50 [14] as our\nfeature extractor module because the convolution operator\nis especially effective at modeling spatially local contexts.\nThe vision transformer module follows the architecture\nadopted in ViT [12] and DeiT [38]. We pad an extra to-\nken named by the classiÔ¨Åcation token Tcls ‚ààRC√ó1 to the\noutput tokens TN of the last iteration in the progressive\nsampling module, and fed them into the vision transformer\nmodule. Formally,\nT = VTM([Tcls,TN ]), (6)\nwhere VTM indicates the vision transformer module func-\ntion which is a stack of transformer encoder layers, and\nT ‚ààRC√ó(n√ón+1) is the output. Note that, the positional\ninformation has been fused intoTN in the progressive sam-\npling module, so we do not need to add positional embed-\nding here. The classiÔ¨Åcation token reÔ¨Åned through the vi-\nsion transformer module is Ô¨Ånally used to predict the image\nclasses. We use the cross entropy loss to end-to-end train\nthe proposed PS-ViT network.\n3.3. Implementation\nTransformer Encoder Layer. The transformer encoder\nlayer serves as the basic building block for the progressive\nsampling module and the vision transformer module. Each\ntransformer encoder layer has a multi-head self-attention\nand a feed-forward unit.\nGiven queries Q ‚ààRD√óL, keys K ‚ààRD√óL and values\nV ‚ààRD√óL with D being the dimension and Lbeing the\nsequence length, the scaled dot-product self attention can\nbe calculated as:\nAttn(Q,K,V) =softmax(QT K/\n‚àö\nD)VT , (7)\nwhere QT indicates the transpose of Q, and softmax (¬∑) is\nthe softmax operation applied over each row of the input\nùê©1\nùêÖ\nùê©3ùê©2\nFeature Extractor \nModule\nSampling \nIteration 1\nùêì1\n‚Ä¶\nSampling \nIteration 2\nùêì2\nProgressive Sampling Module\n‚Ä¶\nùêìùëêùëôùë†\nùêìùëÅ\nTransformer Encoder Layer\n‚Ä¶\nTransformer Encoder Layer\nVision Transformer Module\nClassification Module\nTransformer Encoder Layer\nFigure 4. Overall architecture of the proposed Progressive Sampling Vision Transformer (PS-ViT). Given an input image, its feature map\nF is Ô¨Årst extracted by the feature extractor module. Tokens Ti are then sampled progressively and iteratively at adaptive locationspi over\nF in the progressive sampling module. The Ô¨Ånal output tokens TN of the progressive sampling module are padded with the classiÔ¨Åcation\ntoken Tcls and further fed into the vision transformer module to reÔ¨Åne Tcls, which is Ô¨Ånally classiÔ¨Åed in the classiÔ¨Åcation module.\nNetworks N Nv C M #params #FLOPs\nPS-ViT-Ti 4 8 192 3 4.7 M 1.6 B\nPS-ViT-Ti‚Ä† 4 8 192 3 3.6 M 1.6 B\nPS-ViT-B 4 10 384 6 21.3 M 5.4 B\nPS-ViT-B‚Ä† 4 10 384 6 16.9 M 5.4 B\nTable 2. PS-ViT conÔ¨Ågurations. ‚Ä†indicates weight sharing be-\ntween different iterations in the Progressive Sampling Module\n(PSM). N, Nv, C and M are the iteration number in PSM, the\ntransformer encoder layer number in the vision transformer mod-\nule, the dimension of tokens, and the head number in each trans-\nformer respectively.\nmatrix. For Multi-Head self-Attention (MHA), the queries,\nkeys and values are generated via linear transformations on\nthe inputs for M times with one individual learned weight\nfor each head. Then attention function is applied in parallel\non queries, keys and values of each head. Formally,\nMHA(Z) =Wo[H1,..., HM ]T ,\nHi = Attn(WQ\ni Z,WK\ni Z,WV\ni Z),\n(8)\nwhere Wo ‚àà R\nC\nM √óC is a learnable linear projection.\nWQ\ni ‚ààR\nC\nM √óC, WK\ni ‚ààR\nC\nM √óC and WV\ni ‚ààR\nC\nM √óC are\nthe linear projections for the queries, keys and values of the\ni-th head respectively.\nThe feed-forward unit of the transformer encoder layer\nconsists of two fully connected layers with one GELU non-\nlinear activation [15] between them and the latent variable\ndimension being 3C. For simplicity, the transformer en-\ncoder layers in both the progressive sampling module and\nthe vision transformer module keep the same settings.\nProgressive Sampling Back-propagation. The back-\npropagation of the progressive sampling is straightforward.\nAccording to Equation (1) and Equation (3), for each sam-\npling location i, the gradient w.r.t. the sampling offsets oi\nt\nat the iteration tis computed as:\n‚àÇT\n‚Ä≤i\nt\n‚àÇoi\nt‚àí1\n= ‚àÇF(pi\nt‚àí1 + oi\nt‚àí1)\n‚àÇoi\nt‚àí1\n=\n‚àë\nq\n‚àÇK(q,pi\nt‚àí1 + oi\nt‚àí1)\n‚àÇoi\nt‚àí1\nF(q),\n(9)\nwhere K(¬∑,¬∑) is the kernel for bilinear interpolation to cal-\nculate weights for each integral spatial location q.\nNetwork ConÔ¨Åguration. The feature dimension C, the it-\neration number N in the progressive sampling module, the\nvision transformer layer number Nv in the vision trans-\nformer module, and the head number M in each trans-\nformer layer affect the model size, FLOPs, and perfor-\nmances. In this paper, we conÔ¨Ågure them with different\nspeed-performance tradeoffs in Table 2 so that the proposed\nPS-ViT can be used in different application scenarios. The\nnumber of sampling points along each spatial dimension n\nis set as 14 by default.\nConsidering the sampling in each iteration is conducted\nover the same feature map F in the progressive sampling\nmodule, we try to share weights between those iterations\nto further reduce the number of trainable parameters. As\nshown in Table 2, about 25 % parameters can be saved in\nthis setting.\nModel Image\nsize\nParams\n(M)\nFLOPs\n(B)\nTop-1\n(%)\nTop-5\n(%)\nCNN-based\nR-18 [14] 2242 11.7 1.8 69.8 89.1\nR-50 [14] 2242 25.6 4.1 76.1 92.9\nR-101 [14] 2242 44.5 7.9 77.4 93.5\nX-50-32√ó4d [45] 2242 25.0 4.3 79.3 94.5\nX-101-32√ó4d [45] 2242 44.2 8.0 80.3 95.1\nRegNetY-4GF [31] 2242 20.6 4.0 79.4 -\nRegNetY-6.4GF [31] 2242 30.6 6.4 79.9 -\nRegNetY-16GF [31] 2242 83.6 15.9 80.4 -\nTransformer-based\nViT-B/16 [12] 3842 86.4 55.5 77.9 -\nDeiT-Ti [38] 2242 5.7 1.3 72.2 -\nDeiT-S [38] 2242 22.1 4.6 79.8 -\nDeiT-B [38] 2242 86.4 17.6 81.8 -\nPS-ViT-Ti/14 2242 4.8 1.6 75.6 92.9\nPS-ViT-B/10 2242 21.3 3.1 80.6 95.2\nPS-ViT-B/14 2242 21.3 5.4 81.7 95.8\nPS-ViT-B/18 2242 21.3 8.8 82.3 96.1\nTable 3. Comparison with state-of-the-art networks on ImageNet\nwith single center crop testing. The number after ‚Äú/‚Äù is the sam-\npling number in each axial direction. e.g., PS-ViT-Ti/14 indicates\nPS-ViT-Ti with14 √ó14 sampling locations.\n4. Experiments\nEpochs 300\nOptimizer AdamW\nBatch size 512\nLearning rate 0.0005\nLearning rate decay cosine\nWeight decay 0.05\nWarmup epochs 5\nLabel smooth 0.1\nDropout 0.1\nRand Augment (9, 0.5)\nMixup probability 0.8\nCutMix probability 1.0\nTable 4. Training strategy and hyper-parameter settings.\n4.1. Experimental Details on ImageNet\nAll the experiments for image classiÔ¨Åcation are con-\nducted on the ImageNet 2012 dataset [21] that includes 1k\nclasses, 1.2 million images for training, and 50 thousand\nimages for validation. We train our proposed PS-ViT on Im-\nageNet without pretraining on large-scale datasets. We train\nall the models of PS-ViT using PyTorch [28] with 8 GPUs.\nInspired by the data-efÔ¨Åcient training as done in [38], we\nuse the AdamW [24] as the optimizer. The total training\nepoch number and the batch size are set to 300 and 512 re-\nspectively. The learning rate is initialized with 0.0005, and\ndecays with the cosine annealing schedule [23]. We regu-\nlarize the loss via the smoothing label with œµ = 0.1. We\nuse random crop, Rand-Augment [8], Mixup[52], and Cut-\nMix [51] to augment images during training. Images are re-\nsized to 256√ó256, and cropped at the center with224√ó224\nsize when testing. Training strategy and its hyper-parameter\nsettings are summarized in Table 4.\n4.2. Results on ImageNet\nWe compare our proposed PS-ViT with state-of-the-art\nnetworks on the standard image classiÔ¨Åcation benchmark\nImageNet in terms of parameter numbers, FLOPS, and top-\n1 and top-5 accuracies in Table 3.\nComparison with CNN based networks. Our PS-ViTs\nconsiderably outperform ResNets [14] while with much\nfewer parameters and FLOPs. SpeciÔ¨Åcally, Compared with\nResNet-18, PS-ViT-Ti/14 absolutely improves the top-1 ac-\ncuracy by 5.8% while reducing 6.9 M parameters and 0.2\nB FLOPs. We can observe a similar trend when comparing\nPS-ViT-B/10 (PS-ViT-B/14) and ResNet-50 (ResNet-101).\nOur proposed PS-ViT achieves superior performance and\ncomputational efÔ¨Åciency when compared with the state-of-\nthe-art CNN based network RegNet [31]. Particularly, when\ncompared with RegNetY-16GF, PS-ViT-B/18 improves the\ntop-1 accuracy by 1.8% with about a quarter of parameters\nand a half of FLOPS.\nComparison with transformer based networks. Table 3\nshows that our proposed PS-ViT outperforms ViT [12] and\nits recent variant DeiT [38]. In particular, PS-ViT-B/18\nachieves 82.3% top-1 accuracy which is 0.5% higher than\nthe baseline model DeiT-B while with 21 M parameters and\n8.8 B FLOPs only. Our performance gain attributes to two\nparts. First, PS-ViT samples CNN-based tokens which is\nmore efÔ¨Åcient than raw image patches used in ViT [12] and\nDeiT [38]. Second, our progressive sampling module can\nadaptively focus on regions of interest and produce more\nsemantically correlated tokens than the naive tokenization\nused in [12, 38].\n4.3. Ablation Studies\nThe PS-ViT models predict on the class token in all the\nablation studies.\nA larger sampling number n leads to better perfor-\nmance. We Ô¨Årst evaluate how the sampling number pa-\nrameter n affects the PS-ViT performance. The sequence\nlength of sampled tokens which is fed into the vision trans-\nformer module is n2. The more the sampled tokens, the\nmore information PS-ViT can extract. However, sampling\nmore tokens would increase the computation and memory\nusage. Table 5 reports the FLOPs, and top-1 and top-5 ac-\nn Params (M) FLOPs (B) Top-1 (%) Top-5 (%)\n10 21.3 3.1 80.6 95.2\n12 4.2 81.3 95.5\n14 5.4 81.7 95.8\n16 7.0 82.1 95.8\n18 8.8 82.3 96.1\nTable 5. Effect of the sampling number n in each axial direction.\nModel N Top-1 (%) Top-5 (%)\nPS-ViT-B/14 1 80.6 95.3\n2 81.5 95.6\n4 81.7 95.8\n6 81.8 95.7\n8 81.9 95.7\n9 81.7 95.8\n10 81.6 95.8\nTable 6. Effect of the iteration number N in the progressive sam-\npling module.\ncuracies with different n. It has been shown that the FlOPs\nincreases as n becomes larger, and the accuracy increases\nwhen n ‚â§16 and plateaus when n >16. Considering the\nspeed-accuracy trade-off, we set n = 14by default except\nas otherwise noted.\nThe performance can be further improved with more it-\nerations of progressive sampling. We then evaluate the ef-\nfect of the iteration number N of the progressive sampling\nmodule in Table 6. To keep the computational complex-\nity unchanged, all models in Table 6 have 14 ‚àíN trans-\nformer layers in the vision transformer module, and totally\n14 transformer layers in the entire network. N = 1 indi-\ncates the sampling points will not be updated. It has been\nshown that PS-ViT performs the best when N = 8and the\naccuracy begins to decline when N >8. As we keep the\ntotal number of transformer layers unchanged, increasing\nN will result in the decrease of transformer layers in lateral\nmodeling, which might damage the performance. Consid-\nering the accuracy improvement is negligible from N = 4\nto N = 8, we set N = 4 by default except as otherwise\nnoted.\nFair comparison with ViT.The network hyper-parameters\nin the transformer encoder of PS-ViT are different from\nthe original setting of ViT. For a fair comparison, we fur-\nther study how ViT performs when the network hyper-\nparameters are set to be the same as ours. We set the num-\nber of layers, channels, heads, and the number of tokens\nto be the same as what was proposed in PS-ViT-B/14, and\ntrain the network under the same training regime. As shown\nin Table 7, ViT achieves 78.4% top-1 accuracy, which is\ngreatly inferior to its PS-ViT counterpart. We thereby con-\nclude that the progressive sampling module can fairly boost\nTop-1 (%) Top-5 (%)\nViT‚àó 78.4 94.1\nPS-ViT-B/14 81.7 95.8\nTable 7. Comparison between our PS-ViT with ViT. ‚àómeans the\nmodel with the same model conÔ¨Åguration and training strategy.\nModel Params (M) Top-1 (%) Top-5 (%)\nPS-ViT-Ti/14 4.8 75.6 92.9\nPS-ViT-Ti‚Ä†/14 3.7 74.1 92.3\nPS-ViT-B/10 21.3 80.6 95.2\nPS-ViT-B‚Ä†/10 16.9 80.0 94.8\nPS-ViT-B/12 21.3 81.3 95.5\nPS-ViT-B‚Ä†/12 16.9 80.9 95.3\nPS-ViT-B/14 21.3 81.7 95.8\nPS-ViT-B‚Ä†/14 16.9 81.5 95.6\nTable 8. Comparison PS-ViT with and without weight sharing in\nthe progressive sampling module. ‚Ä†indicates weight sharing.\nthe performance of ViT.\nSharing weights between sampling iterations.Model size\n(parameter number) is one of the key factors when deploy-\ning deep models on terminal devices. Our proposed PS-ViT\nis very terminal device friendly as it can share weights in the\nprogressive sampling module with a negligible performance\ndrop. Table 8 compares PS-ViT with and without weight\nsharing in the progressive sampling module. It has been\nshown that weight sharing can reduce the parameter num-\nber by about 21% ‚àº23% while with a slight performance\ndrop, especially for PS-ViT-B/12 and PS-ViT-B/14.\n4.4. Speed Comparison\nOur proposed PS-ViT is efÔ¨Åcient not only in theory but\nalso in practice. Table 9 compare the efÔ¨Åciency of state-of-\nthe-arts networks in terms of FLOPs and speed (images per\nsecond). For fair comparison, we measure the speed of all\nof the models on a server with one 32GB V100 GPU. The\nbatch size is Ô¨Åxed to 128 and the number of images that can\nbe inferred per second is reported averaged over 50 runs.\nIt has been shown that PS-ViT is much more efÔ¨Åcient than\nViT and DeiT when their top-1 accuracies are comparable.\nSpeciÔ¨Åcally, PS-ViT-B/14 and DeiT-B have similar accu-\nracy around 81.7%. However, PS-ViT-B/14 achieves about\n2.0 times and 3.3 times as fast as DeiT-B in terms of speed\nand FLOPs respectively. PS-ViT-B/10 speeds up ViT-B/16\nby about 13.7 times and 16.9 times in terms of speed and\nFLOPs while improving 2.7% top-1 accuracy.\n4.5. Visualization\nIn order to explore the mechanism of the learnable sam-\npling locations in our method, we visualize the predicted\nFigure 5. Visualization of sampled locations in the proposed progressive sampling module. The start points of arrows are initial sampled\nlocations (p1) while the end points of arrows are the Ô¨Ånal sampled locations (p4).\nModel FLOPs (B) Speed (img/s) Top-1\nRegNetY-4.0GF 4.0 1097.6 79.4\nRegNetY-6.4GF 6.4 487.0 79.9\nRegNetY-16GF 15.9 351.0 80.4\nViT-B/16 55.5 92.4 77.9\nDeiT-S 4.6 1018.2 79.8\nDeiT-B 17.6 316.1 81.8\nPS-ViT-Ti/14 1.6 1955.3 75.6\nPS-ViT-B/10 3.1 1348.0 80.6\nPS-ViT-B/14 5.4 765.6 81.7\nPS-ViT-B/18 8.8 463.8 82.3\nTable 9. Comparison the efÔ¨Åciency of PS-ViT, and that of state-of-\nthe-art networks in terms of FLOPs and speed.\noffsets of our proposed progressive sampling module in Fig-\nure 5. We can observe that the sampling locations are adap-\ntively adjusted according to the content of the images. Sam-\npling points around objects tend to move to the foreground\narea and converge to the key parts of objects. With this\nmechanism, discriminative regions such as the chicken head\nare sampled densely, retaining the intrinsic structure infor-\nmation of highly semantically correlated regions.\n4.6. Transfer Learning\nIn addition to ImageNet, we also transfer PS-ViT to\ndownstream tasks to demonstrate its generalization ability.\nWe follow the practice done in DeiT [38] for fair compari-\nson. Table 10 shows results for models that have been pre-\ntrained on ImageNet and Ô¨Ånetuned for other datasets includ-\ning CIFAR-10 [20], CIFAR-100 [20], Flowers-102 [26] and\nStanford Cars [19]. PS-ViT-B/14 can perform on-par-with\nModel IM C10 C100 Flowers Cars\nViT-B/16 77.9 98.1 87.1 89.5 -\nViT-L/16 76.5 97.9 86.4 89.7 -\nDeiT-B 81.8 99.1 90.8 98.4 92.1\nPS-ViT-B/14 81.7 99.0 90.8 98.8 92.9\nTable 10. Top-1 accuracy on other datasets. ImageNet and CIFAR\nare abbreviated to ‚ÄúIM‚Äù and ‚ÄúC‚Äù.\nor even better than DeiT-B with about4√ófewer FLOPS and\nparameters on all these datasets, which demonstrates the su-\nperiority of our PS-ViT.\n5. Conclusions\nIn this paper, we propose an efÔ¨Åcient Vision Transform-\ners with Progressive Sampling (PS-ViT). PS-ViT Ô¨Årst ex-\ntracts feature maps via a feature extractor, and then progres-\nsively selects discriminative tokens with one progressive\nsampling module. The sampled tokens are fed into a vision\ntransformer module and the classiÔ¨Åcation module for image\nclassiÔ¨Åcation. PS-ViT mitigates the structure destruction is-\nsue in the ViT and adaptively focuses on interesting regions\nof objects. It achieves considerable improvement on Ima-\ngeNet compared with ViT and its recent variant DeiT. We\nalso provide a deeper analysis of the experimental results to\ninvestigate the effectiveness of each component. Moreover,\nPS-ViT is more efÔ¨Åcient than its transformer based competi-\ntors both in theory and in practice.\nAcknowledgement. This work was partially supported\nby Innovation and Technology Commission of the Hong\nKong Special Administrative Region, China (Enterprise\nSupport Scheme under the Innovation and Technology Fund\nB/E030/18).\nReferences\n[1] Jimmy Ba, V olodymyr Mnih, and Koray Kavukcuoglu. Mul-\ntiple object recognition with visual attention. ICLR, 2015. 3\n[2] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, 2019. 2\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv:2005.14165,\n2020. 2\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1,\n2\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer. In\nCVPR, 2021. 2\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, 2020. 1, 2\n[7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nUniversal image-text representation learning. In ECCV,\n2020. 2\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPRW, 2020. 6\n[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764‚Äì773, 2017. 3\n[10] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.\nUp-detr: Unsupervised pre-training for object detection with\ntransformers. In CVPR, 2021. 1, 2\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. NAACL, 2019. 1, 2\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 1, 2,\n3, 4, 6\n[13] Gamaleldin F Elsayed, Simon Kornblith, and Quoc V Le.\nSaccader: improving accuracy of hard attention models for\nvision. arXiv:1908.07644, 2019. 3\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 4, 6\n[15] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv:1606.08415, 2016. 5\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the\nknowledge in a neural network. arXiv:1503.02531, 2015. 3\n[17] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv:1912.12180, 2019. 2\n[18] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 2\n[19] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for Ô¨Åne-grained categorization. In\nProceedings of the IEEE international conference on com-\nputer vision workshops, pages 554‚Äì561, 2013. 8\n[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 8\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiÔ¨Åcation with deep convolutional neural net-\nworks. In NeurIPS, 2012. 6\n[22] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and performant\nbaseline for vision and language. arXiv:1908.03557, 2019.\n2\n[23] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. arXiv:1608.03983, 2016. 6\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv:1711.05101, 2017. 6\n[25] V olodymyr Mnih, Nicolas Heess, Alex Graves, and Ko-\nray Kavukcuoglu. Recurrent models of visual attention.\narXiv:1406.6247, 2014. 3\n[26] Maria-Elena Nilsback and Andrew Zisserman. Automated\nÔ¨Çower classiÔ¨Åcation over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, pages 722‚Äì729. IEEE, 2008. 8\n[27] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 2\n[28] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. 2017. 6\n[29] Matthew E Peters, Mark Neumann, Robert L Logan IV , Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A Smith.\nKnowledge enhanced contextual word representations.arXiv\npreprint arXiv:1909.04164, 2019. 2\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 2\n[31] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ¬¥ar. Designing network design\nspaces. In CVPR, 2020. 6\n[32] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019. 1, 2\n[33] Ronald A Rensink. The dynamic representation of scenes.\nVisual cognition, 2000. 2\n[34] Fenfen Sheng, Zhineng Chen, and Bo Xu. Nrtr: A no-\nrecurrence sequence-to-sequence model for scene text recog-\nnition. In 2019 International Conference on Document Anal-\nysis and Recognition (ICDAR), 2019. 2\n[35] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In ICCV, 2019. 2\n[36] Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, and\nWanli Ouyang. Fishnet: A versatile backbone for im-\nage, region, and pixel level prediction. arXiv preprint\narXiv:1901.03495, 2019. 2\n[37] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.\nRethinking transformer-based set prediction for object detec-\ntion. arXiv:2011.10881, 2020. 1, 2\n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through at-\ntention. arXiv:2012.12877, 2020. 1, 2, 3, 4, 6, 8\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2\n[40] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 1, 2\n[41] Peng Wang, Lu Yang, Hui Li, Yuyan Deng, Chunhua Shen,\nand Yanning Zhang. A simple and robust convolutional-\nattention network for irregular text recognition. arXiv\npreprint arXiv:1904.01375, 2019. 2\n[42] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua\nShen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-\nto-end video instance segmentation with transformers.\narXiv:2011.14503, 2020. 2\n[43] Dirk Weissenborn, Oscar T ¬®ackstr¬®om, and Jakob Uszkoreit.\nScaling Autoregressive Video Models. In ICLR, 2020. 2\n[44] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and\nPeter Vajda. Visual transformers: Token-based im-\nage representation and processing for computer vision.\narXiv:2006.03677, 2020. 1, 2\n[45] Saining Xie, Ross Girshick, Piotr Doll ¬¥ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 6\n[46] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption gen-\neration with visual attention. In ICML, 2015. 3\n[47] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In CVPR, 2020. 2\n[48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V Le. Xlnet: General-\nized autoregressive pretraining for language understanding.\narXiv:1906.08237, 2019. 2\n[49] Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun,\nand Wayne Zhang. Robustscanner: Dynamically enhanc-\ning positional clues for robust text recognition. In European\nConference on Computer Vision, 2020. 2\n[50] Xiaoyu Yue, Zhanghui Kuang, Zhaoyang Zhang, Zhen-\nfang Chen, Pan He, Yu Qiao, and Wei Zhang. Boosting\nup scene text detectors with guided cnn. arXiv preprint\narXiv:1805.04132, 2018. 2\n[51] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiÔ¨Åers with localizable\nfeatures. In ICCV, 2019. 6\n[52] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In ICLR, 2018. 6\n[53] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng\nLi, and Hao Dong. End-to-end object detection with adaptive\nclustering transformer. arXiv:2011.09315, 2020. 1, 2\n[54] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-\nformable convnets v2: More deformable, better results. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9308‚Äì9316, 2019. 3\n[55] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 3\n[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021. 1, 2",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6971043348312378
    },
    {
      "name": "Transformer",
      "score": 0.6838895082473755
    },
    {
      "name": "Encoder",
      "score": 0.6114866733551025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4906677007675171
    },
    {
      "name": "FLOPS",
      "score": 0.48932120203971863
    },
    {
      "name": "Computer vision",
      "score": 0.36853355169296265
    },
    {
      "name": "Parallel computing",
      "score": 0.15946736931800842
    },
    {
      "name": "Engineering",
      "score": 0.1259981393814087
    },
    {
      "name": "Voltage",
      "score": 0.11441969871520996
    },
    {
      "name": "Electrical engineering",
      "score": 0.09438660740852356
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4362561690",
      "name": "International Commission on Missing Persons",
      "country": null
    },
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 5
}