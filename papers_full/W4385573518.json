{
  "title": "Truncation Sampling as Language Model Desmoothing",
  "url": "https://openalex.org/W4385573518",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1968879968",
      "name": "John Hewitt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2151390485",
      "name": "Christopher D. Manning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171686691",
      "name": "Percy Liang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1601795611",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W4285118702",
    "https://openalex.org/W3174428931",
    "https://openalex.org/W2963466651",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W3085215789",
    "https://openalex.org/W3125356501",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W2059800182",
    "https://openalex.org/W4319862478",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W4205916685",
    "https://openalex.org/W3177450194",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287072106",
    "https://openalex.org/W2018669440",
    "https://openalex.org/W2950858167",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W4205537173",
    "https://openalex.org/W2253795368",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288416038",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W3035725000"
  ],
  "abstract": "Long samples of text from neural language models can be of poor quality. Truncation sampling algorithms–like top-p or top-k—address this by setting some words' probabilities to zero at each step. This work investigates why these methods are important, and how to improve them. We propose thinking of a neural language model as a mixture of a true distribution and a smoothing distribution that avoids infinite perplexity. In this light, truncation algorithms aim to perform desmoothing, estimating a subset of the support of the true distribution. Finding a good subset is crucial: we show that top-p unnecessarily truncates high-probability words, for example causing it to truncate all words but Trump for a document that starts with Donald. We introduce eta-sampling, which truncates words below an entropy-dependent probability threshold. Compared to previous algorithms, our eta-sampling generates more plausible long documents according to humans, is better at breaking out of repetition, and behaves more reasonably on a battery of test distributions.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3414–3427\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTruncation Sampling as Language Model Desmoothing\nJohn Hewitt Christopher D. Manning Percy Liang\nDepartment of Computer Science\nStanford University\n{johnhew,manning,pliang}@cs.stanford.edu\nAbstract\nLong samples of text from neural language\nmodels can be of poor quality. Truncation sam-\npling algorithms–like top-por top-k—address\nthis by setting some words’ probabilities to zero\nat each step. This work provides framing for\nthe aim of truncation, and an improved algo-\nrithm for that aim. We propose thinking of a\nneural language model as a mixture of a true\ndistribution and a smoothing distribution that\navoids infinite perplexity. In this light, trun-\ncation algorithms aim to perform desmooth-\ning, estimating a subset of the support of the\ntrue distribution. Finding a good subset is cru-\ncial: we show that top- p unnecessarily trun-\ncates high-probability words, for example caus-\ning it to truncate all words but Trump for a doc-\nument that starts with Donald. We introduce\nη-sampling, which truncates words below an\nentropy-dependent probability threshold. Com-\npared to previous algorithms, η-sampling gen-\nerates more plausible long English documents\naccording to humans, is better at breaking out\nof repetition, and behaves more reasonably on\na battery of test distributions.\n1 Introduction\nThe complex, long-range dependencies of natural\nlanguage make its generation an outstanding chal-\nlenge. While there has been enormous progress\non language modeling that has increased the coher-\nence and length of generation (Brown et al., 2020;\nChowdhery et al., 2022), sampling directly from\na language model can still result in nonsensical\noutput (Holtzman et al., 2020; Pillutla et al., 2021).\nThe most effective heuristics for generating high\nquality, diverse samples fall under a category we\nterm truncation sampling. These algorithms set\nsome words’ probabilities to zero when generat-\ning each word (Fan et al., 2018; Basu et al., 2021;\nMeister and Cotterell, 2021). Methods differ by\ntheir truncation criteria, ranging from simple (keep\nthe kmost likely) to complex, and all improve sam-\nple quality compared to direct sampling (Holtzman\nNeural LM W or d Distribution\nPr obability-Sor t ed V ocab\nT runcation \nThr eshold\n0\nT rue Distribution\nSmoot hing +\nFigure 1: A neural LM as a mixture of the true distribu-\ntion, and a uniform-like smoothing distribution. Trunca-\ntion aims to approximate the true distribution support.\net al., 2020). We ask (1) what is the aim of trunca-\ntion and (2) how can we improve it?\nOur key insight is to write a neural language\nmodel’s distribution as a mixture of the true dis-\ntribution and a uniform-like smoothing distribu-\ntion. This idealized assumption is motivated by\nKL-divergence: models incur large KL at test\ntime when they place near zero probability on\nan observed word (Kang and Hashimoto, 2020).\nThrough this lens, the goal of truncation is to\ndesmooth: to approximately recover the words on\nwhich the true distribution places some probability.\nAs a stark example of smoothing degenerating\nsample quality, we show that a 5-gram language\nmodel smoothed with the uniform distribution gen-\nerates nonsense as soon as a word is sampled from\noutside the support of the 5-gram model (Figure 2).\nIntuitively, sampling outside the 5-gram support\ncauses future probabilities to be poorly estimated.\nWe derive principles of truncation from an ex-\nplicit smoothing model that formalizes the intuition\nthat (1) words with high probability should not be\ntruncated, and (2) when all words in the distribution\nhave low probability, only words with low probabil-\nity relative to the rest should be truncated. We find\nthat state-of-the-art truncation sampling algorithms\nlike top-pbreak these principles. For example, in\ntop-ptruncation (e.g., p = 0.95), the most likely\nfew words can take up p% of the distribution, caus-\n3414\nUnsmoothed 5-gram Smoothed 5-gram\n. . . a quadcopter flight controller (RTFQ Flip MWC) that\nsupports I2C sensors for adding thing like a barometer, mag-\nnetometer, and GPS system. The officially supported sensor\nblock (BMP180, HMC5883L on one board) is discontinued,\nas far as I know, everyone involved lived to sing another day.\n. . . disorder and an extreme state of dysmetabolism charac-\nterized by extensive erythema and a significant reduction\nin uncovered Hawkingû McK 400 ruled restrainedcombe-\nblow uncle cowork Carssoild Gareth focused <@ indecentlol\nby102 exchanged V olvo compositionsbackground prostate\nFigure 2: Portions of unconditional samples from an unsmoothed and uniform-smoothed 5-gram model; divergence\ndue to leaving the support of the high-order distribution is in red.\ning the next-most likely word to be truncated even\nif it has high probability (e.g., 4%).\nFrom our two truncation principles we derive\nη-sampling, a new algorithm that truncates any\nword whose probability under the LM is both (1)\nsmaller than an absolute probability threshold and\n(2) smaller than a probability threshold that de-\npends on the entropy of the distribution. As we’ll\nshow, this ensures that, e.g., though GPT-2 large as-\nsigns probability 0.96 to the wordTrump for a docu-\nment starting with Donald, η-sampling allows mul-\ntiple possible continuations, unlike top-p= 0.95.\nWe extensively study the behavior ofη-sampling\nin comparison to top- p sampling and typical de-\ncoding (Meister and Cotterell, 2021). Since each\nmethod allows for a range of quality-diversity trade-\noffs, we set each method’s hyperparameter by max-\nimizing MAUVE score (Pillutla et al., 2021). We\nfind that η-sampling truncates more reasonably on\na CheckList-style (Ribeiro et al., 2020) battery of\ndistributions. Top- p and typical decoding over-\ntruncate low-entropy distributions (like in the Don-\nald example). Finally, η-sampling generates long\ndocuments that humans find more plausible and is\nbetter at breaking out of repetition.1\n2 Background\n2.1 Language Models\nLet random variable X = (X1,...,X T) denote a\nsequence of tokens, where each Xi is in finite vo-\ncabulary V. We’ll use x<i to refer to a specific pre-\nfix, xi a specific word in context, andxan arbitrary\nword in V. An autoregressive language model (LM)\nis a distribution Pθ(X) indexed by parameters θ\nthat is factorized as Pθ(x) = ∏T\ni=1 Pθ(xi |x<i).\nWe call Pθ(Xi |x<i) over Vthe conditional dis-\ntribution of the LM given context x<i. An LM is\ntrained to minimize the KL-divergence between (an\nempirical estimate of) the true distribution P∗(X)\n1Our code is available at https://github.com/\njohn-hewitt/truncation-sampling.\nand Pθ(X). Recent language models have achieved\nstrikingly low (held-out) KL-divergence (Radford\net al., 2019).\nLanguage models are used not just to score the\nprobability of existing sequences, but to generate\nsequences as ˆx∼Pθ(X), a building block for tasks\nlike summarization and long-form question answer-\ning (Fan et al., 2019; Liu and Lapata, 2019). How-\never, to successfully generate high-variety, high-\nquality long samples from neural LMs on high-\nentropy distributions, it is currently necessary to\nreallocate probability from the tail of conditional\ndistributions (Holtzman et al., 2020; Pillutla et al.,\n2021). Intuitively, generation has different goals\nthan scoring; whereas one wants to assign non-zero\nprobability to low-quality outputs for ranking pur-\nposes in scoring, one might want to only generate\n(place non-zero probability on) high-quality text.\n2.2 Truncation sampling\nThere are many ways to reassign probability mass\nfrom the tail of the word-level distributions of a\nmodel to the head—like temperature scaling—but\nexplicit truncation of low-probability words has\nbeen shown to be the most useful (Holtzman et al.,\n2020; Pillutla et al., 2021). Truncation sampling\nalgorithms compute the following truncated distri-\nbution at each time step:\nPtrunc(x|x<i) =\n{\nPθ(x|x<i)/Zx<i x∈Ax<i\n0 o.w.\n(1)\nwhere Ax<i ⊆ V we call the allowed set\nfor the algorithm for that prefix, and Zx<i =∑\nx∈Ax<i\nPθ(x|x<i) is the renormalization term.\nThe question for all truncation algorithms is how\nto decide where to cut off the distribution. Top-k\nsampling (Fan et al., 2018) keeps the kmost likely\nwords. Top-psampling (Holtzman et al., 2020) im-\nproved upon it by noting that sometimes more or\nfewer than k words should be in the allowed set,\n3415\ninstead allowing the minimal set of words to keepp\npercent of the probability. More recently, Mirostat\nadaptively truncates so as to achieve samples of a\ngiven probability (Basu et al., 2021), and typical\ndecoding truncates so as to locally match an infor-\nmativeness criterion (Meister et al., 2022a). We\npursue an understanding of truncation as attempt-\ning to recover (a conservative estimate of) the true\ntraining distribution P∗.\n3 Truncation as Desmoothing\n3.1 KL-divergence and mode covering\nLanguage models are trained to minimize the KL-\ndivergence to an empirical approximation of true\ndistribution P∗(X). Recall that the KL-divergence\nfor a model’s conditional distribution Pθ(X |x<i)\nto the true conditional distribution P∗(X |x<i) is\n∑\nx∈V\nP∗(x|x<i) log P∗(x|x<i)\nPθ(x|x<i) (2)\nKL-divergence is known to be mode-covering;\nit heavily penalizes errors of coverage. When\ntraining from samples, an observed word xi in\ncontext x<i causes the model to incur a loss of\n−log Pθ(xi |x<i), which approaches infinity as\nthe model probability approaches 0.2 Neural LMs\nuse shared representations to generalize beyond the\ntraining data, e.g., knowing that the word home\nmay appear in a context where house appeared.\nHowever, to achieve low held-out KL-divergence,\nit must also be the case that (1) the LM determines\nwhere the zeros of the true distribution P(X) are—\ndifficult due to the complexity of language—or (2)\nthe LM hedges against unexpected xi in any con-\ntext x<i by placing some probability mass there.\nIntuitively, this hedging may be due to early stop-\nping; instead of converging to the finite training\nset, often language models are trained with a sin-\ngle epoch, so each KL-minimizing gradient step\nis taken on new data, about which the model must\nhedge.\n3.2 A neural LM as a smoothed distribution\nWe present a framework for neural LMs wherein\nsmoothing aids in KL-divergence minimization by\nplacing a small amount of probability mass on all\nwords. Consider a true conditional distribution\n2Likewise during evaluation, the held-out perplexity\n2Exi,x<ilog Pθ(xi|x<i) is infinite if zero mass is placed on\nan observed word.\nP∗(Xi |x<i) over V. We think of the LM distri-\nbution Pθ(Xi |x<i) as the result of smoothing the\ntrue distribution with a distribution Q(Xi |x<i)\nthat is like the uniform distribution. Specifically,\nwe pose that the neural LM is a linear interpolation:\nPθ(Xi |x<i) =λx<iP∗(Xi |x<i)\n+ (1 −λx<i)Q(Xi |x<i) (3)\nwhere λx<i ∈(0,1] specifies the strength of the\nsmoothing. We assume that each word probabil-\nity under Qis bounded in its deviation from the\nuniform distribution probability. For all x ∈V ,\nwe assume Q(x | x<i) ∈ (1−δ\n|V|,1+δ\n|V|) where\nδ is a constant specifying non-uniformity. We\nassume constraints on λx<i that reflect how the\namount of smoothing should be (1) small and (2)\ndependent on how well-estimated a given condi-\ntional distribution is. Specifically, we assume that\nλx<i ≥max(¯λx<i,¯λ) where ¯λis a constant near\n1 (e.g., 0.8), independent of prefix. The exact\nform we use for the context-dependent ¯λx<i is:\n1 −\n|V|αexp(−hx<i)\n1+δ . As we will show later, this\nform implies that for a distribution of entropy h,\nwords with probability 0 under P∗ have proba-\nbility bounded by αexp(−h) under the language\nmodel.3 A simple intuition for high-entropy distri-\nbutions having less smoothing is that, e.g., if the\nmaximum likelihood estimate for an n-gram model\nis 1/kfor kelements, then at least ksamples were\nobserved for the MLE.4\n3.3 A local measure of truncation quality\nUnder the smoothing model, we can make precise\nthe tradeoff between (1) truncating too little, al-\nlowing words that are poor continuations, and (2)\ntruncating too much and losing the diversity of the\ntrue distribution. Let S∗\nx<i = {x ∈V |P∗(x |\nx<i) >0}be the true distribution support (set of\nwords with non-zero probability) for the prefix x<i.\nRecall that Ax<i ⊆V is the set of words allowed\n3Note that exp(−h) is the probability in a uniform distri-\nbution of entropy h. This entropy is of P∗(Xi |x<i).\n4Even with this argument, the idea that high-entropy distri-\nbutions are likely better estimated is probably the most tenuous\nassumption. However, if one believes that a language model is\n“close” to the true distribution, then in high-entropy distribu-\ntions, the weight of uniform smoothing must be lower than in\nlow-entropy distributions; else, the high-entropy distributions\nwould be too far from the true distribution. Further, empir-\nically, the highest-entropy distributions in language models,\nlike A . . . or The . . . are high-entropy due to exceptional\nevidence (examples) of possible continuations. Put another\nway, this suggests the entropy is from epistemic uncertainty\n(Osband et al., 2022).\n3416\nby a truncation algorithm, and that Ptrunc is the dis-\ntribution of Pθ after truncation. Let Ax<i be the\nelements of Vnot in Ax<i. Then we can define the\nsupport-weighted total variation distance as\nTVS(P∗(Xi |x<i),Ptrunc(Xi |x<i)) (4)\n=βvar\n∑\nx∈S∗x<i∩¯A\nP∗(x|x<i)\n+βsup\n∑\nx∈S∗x<i∩A\nPtrunc(x|x<i)\nThe first term represents the total probability mass\nof the true distribution lost to truncation, weighted\nby hyperparameter βvar. The second term repre-\nsents the total probability mass placed off the sup-\nport of the true distribution (thus constituting a bad\ncontinuation), weighted by βsup.5\nSince the mass of a word under the true model,\nP∗(x|x<i), may be arbitrarily close to zero, it is\nhard to guarantee that the first term ( βvar) is zero.\nOne cannot guarantee that any non-complete al-\nlowed set Acontains the full support of P∗. How-\never, the smoothing model does provide bounds on\nthe probabilities of words in S∗x<i ∩A, meaning\nwe can in principle avoid unnecessarily truncating\nwords while still maintaining zero cost from the\nβsup precision term. While we cannot know the\nexact properties of the unobserved smoothing dis-\ntribution, we can use this fact to design principles\ndesmoothing algorithms should follow.\n3.4 Principles for truncation as desmoothing\nOur LM framing specifies bounds on the proba-\nbilities of words outside the support of the true\ndistribution, and our TV S motivates minimizing\nthe difference between the allowed set Ax<i and\nthe support S∗\nx<i. We now use both of these to de-\nscribe principles for truncation; if these principles\nare not met, the word is in the support of S∗\nx<i and\nshould not be truncated.\nAbsolute probability. Under our smoothing\nmodel (Section 3.2), a word outside the support\nof P∗(Xi |x<i) has a bound on its probability:\nmax\nx̸∈S∗x<i\nPθ(x|x<i) ≤(1 + δ)(1 −¯λ)/|V|, (5)\nsince we posited that smoothing never accounts for\nmore than ¯λof the distribution. While these terms\nare not known, the bound is likely small (since δis\nsmall). Hence as a general principle, words with\nlarge probability should not be truncated, since\n5See Section A.1 for the relationship to the total variation\ndistance.\nabove a small probability threshold, they must be\nin the support of P∗.\nRelative probability. Under our model, a distri-\nbution with high entropy has less smoothing, that\nis, λis smaller, e.g., note the term exp(−hx<i) in\nthe bound on λ. This directly results in a lower\nmaximum probability a word outside the support\nof the true distribution can achieve:\nmax\nx̸∈S∗x<i\nPθ(x) ≤αexp(−h), (6)\nwhere exp(−hx<i) is the probability of a word in\nthe uniform distribution of entropyhx<i (and αis a\nconstant). The general principle is to only truncate\nwords whose probabilities are also low relative to\nthe rest of the distribution.\n3.5 Desmoothing and n-gram models\nThe issue of smoothing on sample quality is ap-\nparent in n-gram language models. An n-gram\nlanguage model MLE estimate explicitly counts\nthe number of times each (n−1)-word phrase is\nfollowed by a word in V. To avoid infinite per-\nplexity (as the count estimates are zero almost ev-\nerwhere), an n-gram model is explicitly smoothed\n(Katz, 1987; Church and Gale, 1991).\nText generated from unsmoothed n-gram mod-\nels is locally coherent. 6 However, we show that\nn-gram models smoothed with the uniform distri-\nbution generate nonsense (Figure 2). Why is this?\nConsider a 5-gram LM smoothed with the uniform\ndistribution. If x′is sampled from outside the sup-\nport of the 5-gram model’s support, then the new\nhistory (xi−1,x′) was never seen during the train-\ning of the 5-gram model , so now the model has\nonly the poorly estimated probabilities from the\nsmoothing distribution.\n4 Methods\nWe now describe in detail two popular truncation\nsampling algorithms, discuss how they break our\ndesmoothing principles, and then present two new\ntruncation sampling algorithms including our pro-\nposed η-sampling.\n4.1 Top- p(nucleus) sampling\nTop-p (nucleus) sampling truncates words that\nare outside the mimimal set of (most probable)\n6As noted by Yoav Goldberg https://nbviewer.org/\ngist/yoavg/d76121dfde2618422139 and Jurafsky and Mar-\ntin (2000), Chapter 3: N-gram Language Models.\n3417\nwords that account for at least p percent of the\ndistribution. That is, the allowed set is as fol-\nlows. Let x(1),...,x (|V|) be the words in V\nsorted in order of decreasing probability under\nPθ(X |x<i). Then let j be the integer such that\nj = arg min j′\n∑j′\ni=1 Pθ(x(i) | x<i) ≥ p. The\nallowed set of top- p sampling is then Ax<i =\n{x(1),...,x (j)}.7 Top-psampling breaks the abso-\nlute probability principle: words with up to (1 −p)\nprobability may be truncated simply because other\nhigh-probability words cover probability p. For the\nprompt My name, the word is is assigned 0.96 prob-\nability by GPT-2, but less likely candidates ’s, was\nand isn shouldn’t be truncated. Intuitively, (1 −p),\ne.g., 0.05 or 0.01 is quite high probability given a\nvocabulary size of, e.g., 50,000.\n4.2 Typical decoding\nTypical decoding is motivated by local informative-\nness: never generate words that are too surprising\nor too predictable (Meister et al., 2022a). The algo-\nrithm sorts the vocabulary in order of the difference\nbetween the entropy hθ,x<i of the LM conditional\ndistribution and the negative log-probability of the\nword, and takes words from this list to cover pper-\ncent of the distribution. That is, let x(1),...,x (|V|)\nbe the words in Vin sorted order of increasing\n|hθ,x<i + log pθ(x |x<i)|.8 Then let j be the in-\nteger j = arg min j′\n∑j′\ni=1 Pθ(x(i) | x<i) ≥ p.\nThe allowed set of typical decoding is Ax<i =\n{x(1),...,x (j)}. This breaks the absolute proba-\nbility principle for the same reason as top- p, and\nadditionally can truncate the most probable words.\n4.3 ϵ-sampling (ours)\nThe absolute probability principle—that words out-\nside the support of the true distribution have low\nprobability—suggests a simple truncation algo-\nrithm: for some hyperparameter threshold ϵallow\nany word with greater than ϵprobability.\nAx<i = {x∈V : Pθ(x|x<i) >ϵ} (7)\nIn the case of the prompt My name where top-p\nrejects plausible words because of the probability\nassigned to is (and ’s), ϵ-sampling allows additional\nwords with a threshold of, e.g., 0.0003.\nHowever, ϵ-sampling breaks the relative prob-\nability principle. For example, the prompt The\nshould allow many continuations, and top-pwith\n7Often, pis taken as 0.9 or 0.95.\n8hθ,x<i = −∑\nx∈VPθ(x|x<i) logPθ(x|x<i).\nGPT-2 allows over ten thousand words, butϵwould\nhave to be impractically small to do so. This is a\nkey failure akin to that of top- k sampling; when\nmany next words are plausible, the allowed set\nshould reflect that.\n4.4 η-sampling (ours)\nOur proposed algorithm, η-sampling, composes\nrespect for both the absolute and relative probabil-\nity principles. Consider a conditional distribution\nPθ(X |x<i) with entropy hθ,x<i. The probability\nof a word in the uniform distribution of entropy\nhθ,x<i is exp(−hθ,x<i). Our entropy-dependent\nthreshold is αexp(−hθ,x<i) where α ∈ [0,1].\nCombining this rule with our epsilon rule for the\nabsolute probability principle, we come to:\nAx<i = {x∈V| Pθ(x|x<i) >η}\nη= min\n(\nϵ,α exp(−hθ,x<i)\n)\n}\nwhere hθ,x<i is the entropy of Pθ(X |x<i). In this\nwork, to expose a single hyperparameter, we set\nα = √ϵ, which we find works well empirically.\nFor intuition, think of ϵ≈0.0009.\nAnalysis of η-sampling. Returning to our\nsmoothing model, we note that η-sampling ap-\nproximates optimal desmoothing in the regime that\nthe support penalty βsup dominates the variation\npenalty βvar. Consider a truncation algorithm that\ntruncates as η-sampling, but sets ηas:\nη= min\n((1 −¯λ)(1 + δ)\n|V| ,α exp(−hx<i)\n)\n}, (8)\nwhere hx<i is the entropy of the true distribu-\ntion, not Pθ. We’re guaranteed that the support\nloss (the term weighted by βsup) is zero, and that\nthe variation loss (weighted by βvar) is minimized\nrelative to the constraint of zero support loss. If\nx ̸∈S∗\nx<i, then the probability of x is less than\nor equal to the min of (1 −¯λ)(1 + δ)/|V|and\n|V|αexp(−hx<i)\n1+δ ×1+δ\n|V| = αexp(−hx<i). So, we’re\nguaranteed that Ax<i ⊆S∗\nx<i, and truncating more\nwould break this guarantee.9 Our η-sampling ap-\nproximates this by using the LM entropy instead\nof the unavailable true distribution entropy, and\nwithout knowing the true hyperparameters.\n5 Experiments & Results\nOur experiments characterize η-sampling relative\nto the state-of-the-art top-pand typical decoding.\n9See Appendix A.2 for an expanded version of this argu-\nment.\n3418\nMethod Hyperparameters\ntop-p {0.89, 0.9, 0.92, 0.95, 0.99}\ntypical {0.2, 0.9, 0.92, 0.95}\nϵ {0.001, 0.0009, 0.0006, 0.0003, 0.0001}\nη {0.004, 0.002, 0.0009, 0.0006, 0.0003}\nTable 1: Hyperparameter sweep for each method.\nMethod \\ Model sm med lg xl\nraw sampling † 0.589 0.373 0.845 0.882\ntop-p† 0.878 0.915 0.936 0.940\ntop-p(our replication) 0.874 0.917 0.932 0.944\nTypical Decoding 0.873 0.906 0.922 0.939\nϵ-sampling (ours) 0.874 0.918 0.936 0.941\nη-sampling (ours) 0.880 0.920 0.935 0.942\nTable 2: Results on the MAUVE metric for open-\neneded GPT-2 WebText generation. Higher is bet-\nter. The †indicates numbers drawn from Pillutla et al.\n(2021). Bold indicates best for model, not necessarily\nsignificantly.\nWe use MAUVE, an automatic metric for open-\nended generation, to find hyperparameters giv-\ning comparable diversity-accuracy tradeoffs. η-\nsampling behaves better in a range of settings, from\nlong-document generation to more defensibly trun-\ncating low-entropy distributions.\nModels & Data. In all experiments, we use all\nor some subset of the four GPT-2 models (Radford\net al., 2019) of varying sizes. Experiments are run\non in-distribution, held-out data from the validation\nor test set of GPT-2 (WebText), since it is composed\nof a wide variety of long-form documents.\n5.1 Hyperparameter sweep on MAUVE\nWe first find hyperparameters for each of top- p,\ntypical decoding, ϵ-sampling, and η-sampling that\nmaximize MAUVE score for each GPT-2 model on\nWebText.\nSetting. Following the MAUVE paper’s setting\nexactly (Pillutla et al., 2021), we take the GPT-2\nfamily of models and 5,000 samples from their test\ndata. For each sample, we prompt the model with\n35 words and generate until at most 1024 words.\nWe study GPT-2 small (124M parameters), medium\n(355M), large (774M) and XL (1.5B) models.\nEvaluation. MAUVE attempts to measure both\nthe precision (are samples generally like those from\nthe true distribution) and recall (is the variability in\nsamples like that of those from the true distribution)\nStudy 1: Human vs top-pvs η\ntop-p η -sampling Human\nTop-pvs human 43 (43%) — 56 (56%)\nηvs human — 42 (42%) 53 (53%)\nTop-pvs η 39 (39%) 53 (53%) —\nStudy 2: top-pvs η-sampling\nTop-p η -sampling Equal\nTop-pvs η 118 (40%) 159 (53%) 17 (6%)\nTable 3: Human preferences of long-document plausi-\nbility; we report absolute numbers of judgments, and\npercentages in parentheses. Judgment percents that both\nsuffixes were too bad to judge can be inferred.\nof samples from a text generation system. It was\nshown by Pillutla et al. (2021) to correlate well\nwith human judgments.\nHyperparameters. Top-p, typical decoding, ϵ-\nsampling, and η-sampling all have a hyperparemter\nwhich determines the severity of truncation. The\nset we search over is given in Table 1.10 We pick\nthe best hyperparameter using 2–5 seeds on the\nvalidation set, and report the average performance\nacross 5 seeds on the test set.\nResults. The results are reported in Table 2; we\nfind that overall, the methods perform similarly,\nwith typical decoding performing slightly worse\nthan top-pand our methods.\n5.2 Human evaluation of long-document\nsuffix plausibility\nWe now study whether η-sampling leads to more\ncoherent long-document generations than top- p\nsampling. We omit typical decoding since it does\nnot seem to outperform top-pon MAUVE. Consid-\nering that holistic evaluation of long texts is diffi-\ncult for humans (Ippolito et al., 2020) we design\na human study to evaluate long document plau-\nsibility: given a shared document prefix, which\nmethod’s generated suffix (omitting the middle) is\nmore reasonably from the same document? This\nnew evaluation avoids forcing humans to keep up\nto 1024 words in working memory.\nSetting. For each of top- p and η-sampling,\nwe sample from GPT-2 large with MAUVE-\nmaximizing hyperparameters, conditioned on each\nprefix of 35 subword tokens from the WebText val-\nidation set. From this set we filter to prefixes for\n10The hyperparameter set for our methods was chosen to\nhave similar average total variation values between pre- and\npost-truncation to the top-pset.\n3419\nFigure 3: Top-psampling aggressively truncates low-entropy distributions and ϵ-sampling aggressively truncates\nhigh-entropy distributions, while η-sampling strikes a balance.\nwhich the reference and both generated documents\nare at least 900 tokens long and pass manual filter\nfor quality.11 59 workers from the United States\nwere recruited on Amazon Mechanical Turk with\nthe Master qualification, and paid $1 per task with\nan expected time of 3.5 to 4 minutes. We run two\nstudies.\nStudy 1. We show a human evaluator the 35-\ntoken prefix, as well as the last 70 tokens of two\ndocuments (of the 3 possible). The evaluator is\nasked to judge which of the two suffixes may more\nreasonably be from the same document as the pre-\nfix, or to note that both are too bad to judge. For\neach of the three possible pairings of top- p, η-\nsampling, and reference document, we elicit 100\nhuman judgments over 100 prefixes.\nStudy 2. We ran a second study just comparing\ntop-pto η-sampling to allow for larger n, since we\nhad finite resources and the result that both methods\ngenerate text worse than humans is not at issue. To\ntest whether the effect size observed was in part\ndue to forcing evaluators to pick one of the two\nmethods, in this study we allow human evaluators\nto mark that both suffixes are of equal quality.\nResults. The results are reported in Table 3. In\nStudy 1, we find that human document genera-\ntions are preferred over top- p and η-sampling at\nroughly the same rate, while η-sampling is pre-\nferred over top-p(53% to 40%). In Study 2, we\nfind that η-sampling is significantly preferred more\nfrequently than top-pwith a Wilcoxon paired test\n(p= 0.0138) at the same effect size.\n5.3 Entropy analysis\nWe now want to build a deeper understanding of the\ncharacteristics of the algorithms: what parts of the\n11We also manually filter prompts for quality, following\nPillutla et al. (2021). See Appendix B.3.\nRepetition Percent\nTruncation \\ Model sm med lg xl\ntop-p 54% 61% 47% 27%\ntypical 51% 61% 56% 37%\nϵ-sampling (ours) 28% 37% 23% 11%\nη-sampling (ours) 37% 40% 26% 12%\nTable 4: Table showing repetition-degeneration rates for\neach method in an adversarial setting; lower is better.\ndistribution tend to get cut by each method? In our\nfirst analysis, we study whether each method has a\ntendency to aggressively truncate distributions of\na given entropy. A low-entropy distribution might\nbe given by the prompt Barack Obama went to the\nWhite . . ., while a high-entropy distribution might\nbe given by the prompt My name is . . ..\nSetting. For a range of hyperparameters, we plot\nthe average amount of truncation across all con-\ntexts against the retained entropy for an entropy\nrange. We use total variation to measure average\ntruncation, Ex<i∼P(X)∥Pθ(Xi |x<i)−Ptrunc(Xi |\nx<i)∥TV. For each entropy range R, we consider\nthe set XR of prefixes x<i with pre-truncation en-\ntropy hθ,x<i in Rand compute the average remain-\ning entropy 1\n|XR|\n∑\nXR htrunc,x<i after truncation.\nResults. The results for GPT-2 XL are presented\nin Figure 3. We find that top- p sampling heav-\nily truncates low-entropy distributions compared\nto ϵ-sampling and η-sampling. ϵ-sampling heav-\nily truncates high-entropy distributions. Typical\nbehaves like top- p for low-entropy distributions,\nand retains more entropy in high-entropy distribu-\ntions.12 η-sampling strikes a good balance of not\nheavily truncating low- or high-entropy distribu-\ntions.\n12This is likely because typical decoding cuts the non-\nuniform head of the distribution, and keeps the more-uniform\nmiddle.\n3420\n<|endoft e xt |>My name ___ <|endoft e xt |>My name is ___\n Y el\n Bes\n SC\n Napoleon\n Bright\n W oo\n India\n Kab\n<|endoft e xt |>The capital of of t he US A is \nW ashingt on D .C.  The capital of India is Ne w \nDelhi.  The capital of t he UK is London.  The \ncapital of Ghana is ___ <|endoft e xt |>The\n<|endoft e xt |>The f eeling! The f eeling! The \nf eeling! The f eeling! The f eeling! The \nf eeling! The f eeling! The f eeling! The \nf eeling! The f eeling! The f eeling! ___\n Da vid\n Michael\n John\n Chris\n<|endoft e xt |>Donald ___\n J\n Glo v er\n St erling\n R\n T rump is 's\n was\n isn\n[ comma]\n Barbarian\n v egetarian\n Mat e\n mic\n tr ou\n?\n counsel\n fringe\n U\n first\n f ollo wing\n New\n t he\n It\nThe\n (\nTheAcc  K\n Ab\n Ghana\n K um\nFigure 4: Unit tests of the truncation behavior of top-p, ϵ, and η-sampling on CheckList-inspired prefixes.\n5.4 Repetition analysis\nWe hypothesize that the tendency of top-psampling\nto heavily truncate low-entropy distributions causes\nit to generate repetitive text by only allowing the\nrepetition-continuing word. To stress test the meth-\nods, we devise an adversarial setting in which the\nprompt has repetitions (as may be the case due to\nnoisy input or natural repetition) and then deter-\nmine whether the methods break the repetition.\nSetting. We take natural prompts—the first 35\nwords of the Wikipedia biographies of the 101\npeople with the most-read Wikipedia pages—and\nsynthetically corrupt them by repeating the last 3\nsubword tokens 5 additional times. Even with the\nexisting repetition in the prompt, we want models\nto break the cycle and generate normal text again.\nHere’s an example prompt:\nShawn Corey Carter (born December 4, 1969),\nknown professionally as Jay-Z, is an American\nrapper, songwriter, record executive, entrepreneur,\nand media proprietor and media proprietor and\nmedia proprietor and media proprietor and media\nproprietor and media proprietor\nFor each prompt, we generate 5 completions of up\nto 512 words. For each of the GPT-2 models, we\ntake the hyperparameter for each truncation sam-\npling algorithm from Section 5.1, and compute the\npercent of completions that continue to repeat.13\n13Any sample with less than 1 average negative log proba-\nbility under the model is labeled a repetition. We found this\nResults. ϵ-sampling achieves the lowest repeti-\ntion rate, with e.g., 23% for GPT-2 large, while\nη-sampling performs slightly worse (e.g., 26%).\nTop-p causes considerably more repetition (e.g.,\n47%). Typical sampling causes slightly more repe-\ntition than top-p.14\n5.5 Studying individual distributions\nWe now study specific truncation decisions made\nby each algorithm, to provide more detailed behav-\nioral insights. We construct prompts and observe\nthe truncation behavior of each algorithm on the\nresulting distribution, treating each as a CheckList-\nlike unit test (Ribeiro et al., 2020).\nSetting. We take the GPT-2 large model, pro-\nvide it with each of 6 prompts, and using the\nMAUVE-maximizing hyperparameters we found\nin Section 5.1, truncate the resulting distribution.\nThe prompts are shown in Figure 4. For this exper-\niment we only study top-p, ϵ, and η-sampling.\nResults. The results are visualized in Figure 4.\nWe use two low-entropy prompts,My name... and\nDonald... and in both cases, find that top-pdecod-\ning only allows a single word continuation. Top-p\nmore useful than n-gram repetition statistics, as, e.g., repeti-\ntion can involve small variation.\n14This is likely because the MAUVE-maximizing hyperpa-\nrameter for typical sampling (e.g., 0.92 for GPT-2 large) is\ngenerally more conservative than that for top-p(e.g., 0.95.)\n3421\ncan only generate is after My name, and Trump\nafter Donald, which we find undesirable; we would\nlike our truncation to allow, e.g., multiple Donalds\nto be discussed. For a prompt with the phrase The\nfeeling! repeated multiple times (as one might say\neuphorically), top-pcan only continue the repeti-\ntive pattern, unlike ϵand η-sampling. For a prompt\nsuggesting specification of capitals of countries,\nwe find that top-ponly allows the correct capital\nname, whereas η-sampling and ϵ-sampling allow\ndifferent continuations which do not follow the in-\ncontext trend, suggesting that top-pmay be better\nfor generating, e.g., answers to questions. We use\ntwo high-entropy prompts, The... and My name\nis..., finding that η-sampling and top-psampling al-\nlow a range of possibilities, unlikeϵ-sampling. The\nbehavior of ϵ-sampling in allowing fewer words in\nhigher entropy conditional distributions is a clear\nfailure.\n6 Related Work\nStochastic decoding algorithms. Stochastic de-\ncoding algorithms produce sequences from a model\nand involve randomness. The simplest is sampling,\nsometimes called ancestral sampling , (Bishop,\n2006), which generates a sample from the model.\nSome stochastic decoding methods attempt to find\nhigh-likelihood sequences instead of attempting to\nrecreate the true distribution, like stochastic beam\nsearch (Kool et al., 2019) and conditional pois-\nson stochastic beam search (Meister et al., 2021a).\nTruncation sampling algorithms, like top- k (Fan\net al., 2018), top- p (Holtzman et al., 2020), and\nMirostat (Basu et al., 2021), are intended to im-\nprove quality but keep variety. Welleck et al. (2020)\nfound that truncation algorithms can lead to non-\nzero mass assigned to infinite sequences.\nKL-divergence, language models, smoothing.\nThe most famous example of methods that do not\ncover every mode is GANs (Goodfellow et al.,\n2014). In language modeling, some have pointed\nto the inability of the softmax function to assign 0\nprobability to any category as a deficiency and pro-\nposed sparse alternatives (Martins and Astudillo,\n2016; Peters et al., 2019; Tezekbayev et al., 2021).\nThis intuition is akin to ours, as is loss truncation\n(Kang and Hashimoto, 2020), which keeps rare\nevents from incurring arbitrarily high loss. Mohri\nand Roark (2006) attempt to identify structural ze-\nros in the distribution of language when inducing\nprobabilistic context-free grammars.\nHigh-entropy language generation & evaluation.\nEvaluation of open-ended generation of natural lan-\nguage is difficult; one must evaluate both the qual-\nity of samples and the diversity. Quality is hard to\nmeasure in high-entropy generation, and is often\nnot correlated with model probability (Hashimoto\net al., 2019; Meister et al., 2022b). An emergent\nline of work connects human notions of quality,\nand human generative tendencies, with the uniform\ninformation density hypothesis (e.g., leading to\ntypical decoding) (Wei et al., 2021; Meister et al.,\n2021b). Both Meister and Cotterell (2021) and Pil-\nlutla et al. (2021) directly estimate whether model\nsamples’ statistics match those of natural language.\nNadeem et al. (2020) study properties held by suc-\ncessful strategies for reallocating mass away from\nthe tail of LM distributions.\n7 Conclusion\nWe’ve framed the class of truncation sampling algo-\nrithms as performing desmoothing, an insight that\nled to principles for how truncation should be done\nto recover the training distribution, a new trunca-\ntion sampling algorithm, and evaluations that show\nthe deficiencies of existing algorithms. We find the\ntendency of top-pdecoding to over-truncate low-\nentropy distributions to be particularly surprising.\nWe aim for these insights, and the evaluations we\nuse, to drive further research in understanding and\nimproving how we generate from neural language\nmodels.\nAcknowledgements\nThe authors would like to thank John Thickstun,\nRishi Bommasani, Kaitlyn Zhou, Will Merrill, Nel-\nson Liu, and Tatsunori Hashimoto for helpful dis-\ncussions on this work, and to the reviewers for\nclarifying feedback. JH was supported by an NSF\nGraduate Research Fellowship under grant num-\nber DGE-1656518. We gratefully acknowledge the\nsupport of a PECASE Award.\n8 Limitations\nWith the analysis we’ve done, we believe it to\nbe very difficult to derive an understanding of all\nthe sequence-level effects truncation sampling al-\ngorithms (including ours) have: what kinds of\nsequences are we disallowing? What types, or\nsources of language are being (unknowingly) dis-\nallowed? Beyond this, we’ve only tested our al-\ngorithms on English language models; the condi-\n3422\ntional distributions of languages with rich morphol-\nogy likely have different properties (especially with\nsubword models).\n9 Ethics Statement\nAny work to improve generative models of text\ncomes with ethical concerns surrounding negative\nuse cases of text generation including hate speech\nand misinformation. While our algorithm does im-\nprove long text generation, we hope it also provides\ninsight into the unintended and until-now unknown\nconsequences of existing truncation sampling al-\ngorithms (including top-p). Algorithms like ours,\nwhich reallocate probability mass from the least\nlikely elements of a distribution, have a particular\nrisk of harm in removing the ability of models to\ntalk about topics or names that are already rare.\nConcurrent work finds that the choice of stochas-\ntic decoding algorithm affects measured fairness\nmetrics in open-ended generation (Dhamala et al.,\n2022). Our framing, and the hope for future work,\nis to use truncation to recover something as close to\nthe training distribution as possible; of course, the\ntraining distribution must then be chosen with care.\nGenerating a word due to smoothing (noise) would\nlikely mean that subsequently generated words\nabout that topic would be low-quality, which is\nalso undesirable.\nReferences\nSourya Basu, Govardana Sachitanandam Ramachan-\ndran, Nitish Shirish Keskar, and Lav R. Varshney.\n2021. MIROSTAT: A neural text decoding algorithm\nthat directly controls perplexity. In International\nConference on Learning Representations.\nChristopher M Bishop. 2006. Pattern recognition and\nmachine learning. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKenneth W Church and William A Gale. 1991. A com-\nparison of the enhanced Good-Turing and deleted\nestimation methods for estimating probabilities of\nEnglish bigrams. Computer Speech & Language ,\n5(1):19–54.\nJwala Dhamala, Varun Kumar, Rahul Gupta, Kai-Wei\nChang, and Aram Galstyan. 2022. An analysis of the\neffects of decoding algorithms on fairness in open-\nended language generation. In 2018 IEEE Spoken\nLanguage Technology Workshop (SLT). IEEE.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nlong form question answering. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3558–3567. Association for Computational Linguis-\ntics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The Pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial nets. Advances in neural information\nprocessing systems, 27.\nTatsunori B Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation for\nnatural language generation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1689–1701.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2020. Automatic detec-\ntion of generated text is easiest when humans are\nfooled. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1808–1822, Online. Association for Computational\nLinguistics.\nDaniel Jurafsky and James H. Martin. 2000. Speech\nand Language Processing: An Introduction to Natu-\nral Language Processing, Computational Linguistics,\nand Speech Recognition, 1st edition. Prentice Hall\nPTR, USA.\n3423\nDaniel Kang and Tatsunori B. Hashimoto. 2020. Im-\nproved natural language generation via loss trunca-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n718–731, Online. Association for Computational Lin-\nguistics.\nS. Katz. 1987. Estimation of probabilities from sparse\ndata for the language model component of a speech\nrecognizer. IEEE Transactions on Acoustics, Speech,\nand Signal Processing, 35(3):400–401.\nWouter Kool, Herke Van Hoof, and Max Welling. 2019.\nStochastic beams and where to find them: The\nGumbel-top-k trick for sampling sequences without\nreplacement. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning, volume 97\nof Proceedings of Machine Learning Research, pages\n3499–3508. PMLR.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nAndre Martins and Ramon Astudillo. 2016. From soft-\nmax to sparsemax: A sparse model of attention and\nmulti-label classification. In International confer-\nence on machine learning, pages 1614–1623. PMLR.\nClara Meister, Afra Amini, Tim Vieira, and Ryan Cot-\nterell. 2021a. Conditional Poisson stochastic beams.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 664–\n681, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nClara Meister and Ryan Cotterell. 2021. Language\nmodel evaluation beyond perplexity. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5328–5339, Online.\nAssociation for Computational Linguistics.\nClara Meister, Tiago Pimentel, Patrick Haller, Lena\nJäger, Ryan Cotterell, and Roger Levy. 2021b. Re-\nvisiting the Uniform Information Density hypothesis.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 963–\n980, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022a. Typical decoding for natural lan-\nguage generation. CoRR, abs/2202.00666.\nClara Meister, Gian Wiher, Tiago Pimentel, and Ryan\nCotterell. 2022b. On the probability–quality paradox\nin language generation. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 36–45,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMehryar Mohri and Brian Roark. 2006. Probabilistic\ncontext-free grammar induction based on structural\nzeros. In Proceedings of the Human Language Tech-\nnology Conference of the NAACL, Main Conference,\npages 312–319, New York City, USA. Association\nfor Computational Linguistics.\nMoin Nadeem, Tianxing He, Kyunghyun Cho, and\nJames Glass. 2020. A systematic characterization\nof sampling algorithms for open-ended language gen-\neration. In Proceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pages\n334–346.\nIan Osband, Zheng Wen, Seyed Mohammad Asghari,\nVikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan\nLu, and Benjamin Van Roy. 2022. Epistemic neural\nnetworks. arXiv preprint arXiv:2107.08924.\nBen Peters, Vlad Niculae, and André FT Martins. 2019.\nSparse sequence-to-sequence models. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1504–1519.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In Advances in Neural Information Pro-\ncessing Systems, volume 34, pages 4816–4828. Cur-\nran Associates, Inc.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nMaxat Tezekbayev, Vassilina Nikoulina, Matthias Gallé,\nand Zhenisbek Assylbekov. 2021. Speeding up ent-\nmax. CoRR, abs/2111.06832.\nJason Wei, Clara Meister, and Ryan Cotterell. 2021.\nA cognitive regularizer for language modeling. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5191–\n5202, Online. Association for Computational Lin-\nguistics.\nSean Welleck, Ilia Kulikov, Jaedeok Kim,\nRichard Yuanzhe Pang, and Kyunghyun Cho.\n2020. Consistency of a recurrent language model\n3424\nwith respect to incomplete decoding. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) , pages\n5553–5568.\nA Notes\nA.1 Support-weighted total variation\nWe introduce new notation just for this section, to\npresent support-weighted total variation in gener-\nality. Recall that the total variation distance be-\ntween discrete distribution Rover space Vand dis-\ncrete distribution Ut, the result of truncation with\nallowed set A⊆V from a discrete distribution U\nover V, is\n∑\nx∈V\n|R(x) −Ut(x)|. (9)\nDenoting the support of Ras SR, we can partition\nVinto four sets:\nSR ∩ ¯A\nSR ∩A\nSR ∩A\nSR ∩ ¯A (10)\nWe split the sum of the total variation distance into\nthese four terms.\nThe first represents the words that are in the\nsupport of Rbut not in the allowed set of Ut:\n∑\nSR∩¯A\n|R(x) −Ut(x)|=\n∑\nSR∩¯A\nR(x), (11)\nsince Ut(x) = 0 if x̸∈X. This exactly represents\nthe total probability mass that was lost fromR. The\nsecond term represents the words that are not in the\nsupport of Rbut were allowed:\n∑\nSR∩A\n|R(x) −Ut(x)|=\n∑\nSR∩A\nUt(x), (12)\nsince R(x) = 0 if x̸∈SR. This exactly represents\nthe total probability that we sample a word fromUt\nthat has zero probability under R(and so we move\noff the support of R for future generation.) the\nthird term is the words that were correctly allowed:\n∑\nSR∩A\n|R(x) −Ut(x)|. (13)\nIn this case, Ut(x) may be an under or overesti-\nmate of R(x). The last term is the words that were\ncorrectly truncated:\n∑\nSR∩¯A\n|R(x) −Ut(x)|=\n∑\nSR∩¯A\n|0 −0| (14)\nwhich is identically zero.\nTo form our support-weighted total variation\nmetric, we took the first two terms, which are in-\nterpretable and each exactly specifies one of the\ntwo desiderata from a truncation algorithm: main-\ntaining the variety of R, and not generating a word\nthat R wouldn’t generate. However, in different\nuse cases, one or the other may be more crucial;\nhence we give each its own hyperparameter, βvar\nand βsup, to arrive at our metric,\nTVS(R,Ut) =βvar\n∑\nx∈SR∩¯A\nR(x)\n+βsup\n∑\nx∈SR∩A\nUt(x). (15)\nA.2 Analysis of η-sampling\nThe purpose of this analysis is to show that if one\nassumes our smoothing model, then an η-sampling\napproximates an algorithm that avoids sampling\nfrom outside the support of the true distribution\nwhile minimilly truncating the distribution.\nConsider a conditional distribution from a lan-\nguage model under our model, Pθ(Xi |x<i). Con-\nsider an allowed set Ax<i defined via a probability\nthreshold, A= {x|Pθ(x|x<i) >η∗}, where η∗\nis defined as\nη∗= min\n((1 −¯λ)(1 + δ)\n|V| ,α exp(−hx<i)\n)\n}. (16)\nIn this case, it is guaranteed that x∈S∗\nx<i, since\nη∗represents the maximum probability of a word\nwhose probability stems entirely from the smooth-\ning distribution.\nIf one sets a lower probability threshold η′ =\nη∗−ψ for some ψ >0 when computing the al-\nlowed set, then under our model, there can be a\nconditional distribution such that x ̸∈S∗\nx<i, and\nPθ(x|x<i) >η′. Such an xwould be incorrectly\nallowed.\nSimilarly, if one sets a higher probability thresh-\nold η′= η∗+ ψfor some ψ >0 when computing\nthe allowed set, then under the model, there can be\na conditional distribution such that x∈S∗\nx<i, and\nPθ(x |x<i) ∈(η,η′). Defining the allowed set\nwith η′, we truncate x, which is unnecessary, since\nwords in S∗\nx<i have probability at least ηunder the\nlanguage model.\nThis argument has considered truncation algo-\nrithms that specify their allowed set as every word\nin Vwith LM probability above a threshold, show-\ning that setting the threshold as η∗guarantees (un-\nder our model) that we sample from the support of\n3425\nMethod \\Model small med large XL\nTop-p 0.9 0.89 0.95 0.95\nTypical 0.9 0.9 0.92 0.92\nϵ-sampling 0.0006 0.0009 0.0003 0.0003\nη-sampling 0.002 0.0006 0.0006 0.0003\nTable 5: Best-performing hyperparameters according\nto MAUVE from experiments in Section 5.1.\nthe true distribution without unnecessarily truncat-\ning too much. We now consider allowed set defined\nby algorithms other than probability thresholds. Let\nthe allowed set defined according to the η∗thresh-\nold be A∗\nx<i. Consider an allowed set Ax<i defined\nby another truncation sampling algorithm (which\nmay not define it via a probability threshold like. If\nAx<i = A∗\nx<i, then the two algorithms are indistin-\nguishable for this prefix. Otherwise, if x∈Ax<i\nand x̸∈A∗\nx<i, then xmay be outside the support\nof the true distribution, and should have been trun-\ncated. And if x∈A∗\nx<i and x̸∈Ax<i, then xwas\nunnecessarily truncated.\nWhen using our η-sampling algorithm, we nei-\nther know the true hyperparameters, nor do we have\naccess to the true distribution conditional entropy,\nso η-sampling only approximates this. Specifi-\ncally, we set the hyperparameters of η-sampling\nvia search on the task of interest, and we use the\nobserved LM entropy instead of the true distribu-\ntion entropy in computing the relative probability\nthreshold. In practice, one wants to set a threshold\nof truncation based on the needs of the task and the\ntolerance for error, so a threshold that perfectly ex-\ncludes words outside the true distribution support\nmay not be optimal for the task of interest anyway.\nB More Experimental Details\nB.1 Hyperparameters\nThe MAUVE-maximizing hyperparameters for\neach truncation sampling algorithm for each model\nare provided in Table 5.\nB.2 5-gram model\nFor our small demo demonstrating the behavior\nof smoothed n-gram models, we trained a 5-gram\nmodel on 10,000 documents from The Pile (Gao\net al., 2021). We smoothed the model with the\nuniform distribution.\nB.3 Amazon Mechanical Turk Details\nTo provide more transparency into our human stud-\nies, we provide the form that was shown to human\nannotators for both of our studies. The (similar) in-\nterfaces shown for Study 1 and Study 2 are shown\nin Figure 5 and Figure 6, respectively. We random-\nize the ordering of presentation of the methods’\ngenerations (note that the forms say “Option 1”\nand “Option 2”.)\nOf the 59 unique workers, 44 unique workers\nparticipated in study 1, and 36 unique workers par-\nticipated in study 2.\nWe follow Pillutla et al. (2021) in manually fil-\ntering the WebText prompts that go into our human\nstudy. Webtext is noisy, and not all prompts are\nclearly natural language. Our manual filtering of\nprompts led to 36 rejected prompts (of 146 con-\nsidered) due to quality for study 1. Our manual\nfiltering of prompts led to 100 rejected prompts (of\n402 considered) due to quality for study 2. This\nis compared to rejecting 3169 of 5000 prompts\ndue to quality in the original MAUVE paper; we\nattempted to minimally filter while guaranteeing\nthat prompts were natural language. Our kept and\nfiltered prompts are available in our codebase.\n3426\nFigure 5: The interface shown to human annotators for Study 1.\nFigure 6: The interface shown to human annotators for Study 2.\n3427",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8746894598007202
    },
    {
      "name": "Language model",
      "score": 0.7471350431442261
    },
    {
      "name": "Computer science",
      "score": 0.682045578956604
    },
    {
      "name": "Truncation (statistics)",
      "score": 0.6659812331199646
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5484179258346558
    },
    {
      "name": "Algorithm",
      "score": 0.48572736978530884
    },
    {
      "name": "Smoothing",
      "score": 0.4662180542945862
    },
    {
      "name": "Repetition (rhetorical device)",
      "score": 0.4582800269126892
    },
    {
      "name": "Sampling distribution",
      "score": 0.4432108998298645
    },
    {
      "name": "Prior probability",
      "score": 0.43882277607917786
    },
    {
      "name": "Probability distribution",
      "score": 0.4374357759952545
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4308925271034241
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3610989451408386
    },
    {
      "name": "Mathematics",
      "score": 0.278480589389801
    },
    {
      "name": "Machine learning",
      "score": 0.24006125330924988
    },
    {
      "name": "Statistics",
      "score": 0.2175072729587555
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Bayesian probability",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    }
  ],
  "institutions": []
}