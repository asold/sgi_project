{
    "title": "Human-like problem-solving abilities in large language models using ChatGPT",
    "url": "https://openalex.org/W4377942506",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2320168061",
            "name": "Graziella Orrù",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A2034060904",
            "name": "Andrea Piarulli",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A2052190509",
            "name": "Ciro Conversano",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A2090742406",
            "name": "Angelo Gemignani",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A2320168061",
            "name": "Graziella Orrù",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2034060904",
            "name": "Andrea Piarulli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052190509",
            "name": "Ciro Conversano",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2090742406",
            "name": "Angelo Gemignani",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W118936270",
        "https://openalex.org/W1969030216",
        "https://openalex.org/W6679434410",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W4221057198",
        "https://openalex.org/W6638114406",
        "https://openalex.org/W6748947987",
        "https://openalex.org/W4323338788",
        "https://openalex.org/W6815122036",
        "https://openalex.org/W2973997433",
        "https://openalex.org/W3200293897",
        "https://openalex.org/W2998802298",
        "https://openalex.org/W2999930990",
        "https://openalex.org/W1969959732",
        "https://openalex.org/W2962961621",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W6680385414",
        "https://openalex.org/W2794715776",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3149277062",
        "https://openalex.org/W4297734170",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2970460809",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W1971944486",
        "https://openalex.org/W1800356822",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4236965008"
    ],
    "abstract": "Backgrounds The field of Artificial Intelligence (AI) has seen a major shift in recent years due to the development of new Machine Learning (ML) models such as Generative Pre-trained Transformer (GPT). GPT has achieved previously unheard-of levels of accuracy in most computerized language processing tasks and their chat-based variations. Aim The aim of this study was to investigate the problem-solving abilities of ChatGPT using two sets of verbal insight problems, with a known performance level established by a sample of human participants. Materials and methods A total of 30 problems labeled as “ practice problems ” and “ transfer problems ” were administered to ChatGPT. ChatGPT's answers received a score of “0” for each incorrectly answered problem and a score of “1” for each correct response. The highest possible score for both the practice and transfer problems was 15 out of 15. The solution rate for each problem (based on a sample of 20 subjects) was used to assess and compare the performance of ChatGPT with that of human subjects. Results The study highlighted that ChatGPT can be trained in out-of-the-box thinking and demonstrated potential in solving verbal insight problems. The global performance of ChatGPT equalled the most probable outcome for the human sample in both practice problems and transfer problems as well as upon their combination. Additionally, ChatGPT answer combinations were among the 5% of most probable outcomes for the human sample both when considering practice problems and pooled problem sets. These findings demonstrate that ChatGPT performance on both set of problems was in line with the mean rate of success of human subjects, indicating that it performed reasonably well. Conclusions The use of transformer architecture and self-attention in ChatGPT may have helped to prioritize inputs while predicting, contributing to its potential in verbal insight problem-solving. ChatGPT has shown potential in solving insight problems, thus highlighting the importance of incorporating AI into psychological research. However, it is acknowledged that there are still open challenges. Indeed, further research is required to fully understand AI's capabilities and limitations in verbal problem-solving.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/four.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nOPEN ACCESS\nEDITED BY\nChristos Troussas,\nUniversity of West Attica, Greece\nREVIEWED BY\nFrancisco Antonio Castillo,\nPolytechnic University of Querétaro, Mexico\nTakashi Kuremoto,\nNippon Institute of Technology, Japan\n*CORRESPONDENCE\nGraziella Orrù\ngraziella.orru@unipi.it\nRECEIVED /zero.tnum/six.tnum April /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /zero.tnum/nine.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/four.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nOrrù G, Piarulli A, Conversano C and\nGemignani A (/two.tnum/zero.tnum/two.tnum/three.tnum) Human-like\nproblem-solving abilities in large language\nmodels using ChatGPT.\nFront. Artif. Intell./six.tnum:/one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Orrù, Piarulli, Conversano and\nGemignani. This is an open-access article\ndistributed under the terms of the\nCreative\nCommons Attribution License (CC BY) . The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that\nthe original publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nHuman-like problem-solving\nabilities in large language models\nusing ChatGPT\nGraziella Orrù *, Andrea Piarulli, Ciro Conversano and\nAngelo Gemignani\nDepartment of Surgical, Medical, Molecular and Critical Area Pathology, University of Pisa, Pisa, Italy\nBackgrounds: The ﬁeld of Artiﬁcial Intelligence (AI) has seen a major shift in rece nt\nyears due to the development of new Machine Learning (ML) model s such as\nGenerative Pre-trained Transformer (GPT). GPT has achieved pr eviously unheard-\nof levels of accuracy in most computerized language processing tas ks and their\nchat-based variations.\nAim: The aim of this study was to investigate the problem-solving ab ilities of\nChatGPT using two sets of verbal insight problems, with a known p erformance\nlevel established by a sample of human participants.\nMaterials and methods: A total of /three.tnum/zero.tnum problems labeled as “practice problems” and\n“transfer problems” were administered to ChatGPT. ChatGPT’s answers received\na score of “/zero.tnum” for each incorrectly answered problem and a score of “/one.tnum”for each\ncorrect response. The highest possible score for both the practice and transfer\nproblems was /one.tnum/five.tnum out of /one.tnum/five.tnum. The solution rate for each problem (based on a sample\nof /two.tnum/zero.tnum subjects) was used to assess and compare the performance of ChatGPT with\nthat of human subjects.\nResults: The study highlighted that ChatGPT can be trained in out-of-the -box\nthinking and demonstrated potential in solving verbal insig ht problems. The global\nperformance of ChatGPT equalled the most probable outcome for t he human\nsample in both practice problems and transfer problems as well as upon their\ncombination. Additionally, ChatGPT answer combinations were am ong the /five.tnum% of\nmost probable outcomes for the human sample both when consideri ng practice\nproblems and pooled problem sets. These ﬁndings demonstrate that Cha tGPT\nperformance on both set of problems was in line with the mean rate o f success of\nhuman subjects, indicating that it performed reasonably well.\nConclusions: The use of transformer architecture and self-attention in Chat GPT\nmay have helped to prioritize inputs while predicting, contrib uting to its potential\nin verbal insight problem-solving. ChatGPT has shown potent ial in solving insight\nproblems, thus highlighting the importance of incorporating AI into psychological\nresearch. However, it is acknowledged that there are still open chal lenges. Indeed,\nfurther research is required to fully understand AI’s capabili ties and limitations in\nverbal problem-solving.\nKEYWORDS\nChatGPT, machine learning, NLP, problem-solving, AI, Artiﬁcial Intelligence\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\n/one.tnum. Introduction\nThe use of machine learning (ML) in psychology is becoming\nincreasingly widespread. Large amounts of data generated in\npsychological studies can be eﬀectively analyzed and interpreted\nusing ML algorithms. Such methods can assist researchers in\nidentifying patterns and relationships in data that are not\nimmediately obvious. ML algorithms can, for example, be used\nto analyse brain imaging data and identify features associated\nwith various neurological or psychiatric disorders (\nOrrù et al.,\n2012, 2021; Ferrucci et al., 2022 ), or in overlapping futures of\nchronic conditions (i.e., ﬁbromyalgia: Orrù et al., 2020a ) which are\nparticularly frequent ( Dell’Osso et al., 2015). ML can be eﬀectively\napplied in a range of ﬁelds such as forensic sciences (malingering:\ni.e.,\nSartori et al., 2017 ; Pace et al., 2019 ; personality faking-good:\ni.e., Mazza et al., 2019 ) and psychological research ( Orrù et al.,\n2020b), amongst others.\nNew ML models such as large language models (LLM) are\npurportedly the promoters of a recent “paradigm shift” in artiﬁcial\nintelligence-based (AI-based) language analysis. Indeed, LLMs are\nsophisticated AI systems that are trained on vast amounts of\ntextual data. These models have the ability to generate human-\nlike language and perform a wide range of language tasks\nsuch as translation, question-answering and sentiment analysis,\namongst others. These models, like the Bidirectional Encoder\nRepresentations from Transformers (BERT) and the Generative\nPre-trained Transformer (GPT) and their chat-based variations\n(i.e., ChatGPT), have produced previously unheard-of levels of\naccuracy (\nDevlin et al., 2018 ) in most computerized language\nprocessing tasks and have recently gained widespread attention.\nGPT is a language model created by OpenAI. The latest and most\nadvanced language model developed is known as GPT and has\nbeen trained on a massive amount of text data from the internet.\nIt can generate human-like text and perform various language\ntasks such as translation, summarization, question answering\nand coding. GPT-3 enables a personalized conversation with\nan AI bot capable of providing detailed responses to questions\n(prompts) at signiﬁcant speed. Speciﬁcally, GPT-3 is a deep learning\nautoregressive language model (a simple feed-forward model), that\nproduces human-like text from a set of words given in a speciﬁc\ncontext. In general terms, the ability of the LLM to mathematically\nrepresent words in context is presumably largely responsible for\nits success.\nThe Transformer-based model is a speciﬁc neural network\narchitecture introduced by\nVaswani et al. (2017) in “ Attention is all\nyou need” (2017) and has become the foundation of many state-of-\nthe-art models in Natural Language Processing (NPL), including\nGPT-3. The main innovation of transformer architecture is the use\nof attention mechanisms, which allow the model to selectively focus\non diﬀerent parts of the input during its processing, and thus to\nunderstand more eﬀectively the relationships between words and\nphrases in the input. Speciﬁcally, attention allows the association of\ndistant portions of text within a sentence; for example, it enables the\nunderstanding that in the sentence “ the boy chasing the horse is fat , ”\n“fat” refers to the boy and not the horse. LLMs are massive neural\nnetworks, consisting of billions of parameters, that are trained on\nvast quantities of text and rely on an attention mechanism. One of\nthe most eﬃcient training systems is the one used in GPT-3, which\ninvolves predicting the next word in a sentence; for example, by\ndisplaying the sentence “ the dog barks and the cat. . . , ” it predicts the\nword “meows.”\nFine-tuning is a method used for training a pre-trained\ntransformer-based model on a new dataset, with the aim of\nadapting it to a speciﬁc task. This is achieved by training the\nmodel on a smaller dataset that is speciﬁc to a certain task, while\nmaintaining the weights from the pre-trained model ﬁxed. This\nprocedure enables the model to employ the knowledge that it has\nlearned from the larger dataset to quickly learn how to perform a\nnew task.\nThe purpose of the present work was to evaluate the ability\nof ChatGPT in solving verbal insight problems from two sets\nof problems, both of which were originally solved by a group\nof 20 human participants in a study conducted by\nAnsburg and\nDominowski (2000). The aim of this evaluation was twofold:\nﬁrstly, to determine whether ChatGPT could solve these types of\nverbal insight problems, which are typically associated with human\nintelligence and have been previously considered challenging for\ncomputers to solve; secondly, to compare ChatGPT’s problem-\nsolving abilities to those of humans, as established in the\nmentioned study.\nOverall, the present study sought to assess the potential of\nChatGPT as an intelligent tool for problem-solving and to explore\nthe extent to which machine intelligence can match or surpass\nhuman intelligence in this domain.\nIn the sections that follow, the study begins by providing (i) a\nframework to better understand problem-solving; (ii) a summary\nof the state-of-the-art classiﬁcation techniques used in speech\ncontexts and of the transformer architecture, emphasizing the\ncomponents involved in encoding and decoding; (iii) the major\nﬁndings are then presented; (vi) ﬁnally, the diﬃculties involved, and\npotential futures directions are discussed.\n/one.tnum./one.tnum. Verbal insight problem-solving\nTo comprehend insight problem-solving, key terms must ﬁrst\nbe deﬁned. Problem-solving is deﬁned as a set of cognitive\nprocesses aimed at transforming a given circumstance into a\ndesirable scenario when there is no clear solution (\nMayer and\nWittrock, 2006). In other words, it represents the process of ﬁnding\na solution to a problem or a set of problems that involves the\nuse of diﬀerent strategies or techniques to overcome obstacles\nand reach a speciﬁc goal, “ when no solution method is obvious to\nthe problem solver ” (\nMayer, 1992 ). There are diﬀerent approaches\nto problem-solving, including analytical, creative, and intuitive\nmethods: analytical methods involve breaking down the problem\ninto smaller parts and systematically analyzing each part to ﬁnd a\nsolution (i.e.,\nPolya, 2004); creative methods involve generating new\nand unique ideas to solve the problem; intuitive methods involve\nthe use of past experiences and knowledge to inform the problem-\nsolving process. Problem-solving has been thoroughly investigated\nby cognitive science in general with a number of theories and\nmodels being put forward. One of the most inﬂuential theories\nis the “ General Problem Solver ” (GPS) hypothesis of\nNewell and\nSimon (1972) which states that problem-solving is a logical and\nsystematic process that adheres to a set of norms and processes.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nAnother important theory is the “ Dual-Process Hypothesis” (DPT),\nproposed by Kahneman (2011), which suggests that problem-\nsolving can be achieved through both quick, intuitive processes\n(System 1) and slow, deliberate processes (System 2). A further\nproblem-solving method is known as “ Insight” can be deﬁned as the\nunexpected development of a new concept or a fresh perspective on\nthe issue and it is frequently characterized by an “ Aha!” moment.\nInsight problem-solving has been extensively studied by cognitive\npsychologists and was ﬁrst described by\nWallas (1926) as a four-\nstage process consisting of preparation, incubation, illumination,\nand veriﬁcation.\n/one.tnum./two.tnum. A brief overview of cutting-edge\nsequence classiﬁcation methods in text or\nspeech contests\nIn end-to-end sequence classiﬁcation, ML, and speciﬁcally\ndeep learning, are becoming increasingly popular. This type of\nclassiﬁcation requires only a single model to learn all those stages\nin between initial inputs and the ﬁnal outputs. There are two basic\nmethods for analyzing sequential data in text or speech; these\nmethods are referred to as Transformers and Recurrent Neural\nNetworks (RNNs).\nA transformer-based model, as mentioned above, is a type\nof neural network architecture introduced by\nVaswani et al.\n(2017). The key innovation of the transformer is the self-\nattention mechanism, also known as intra-attention, which\nconnects “ diﬀerent positions in a single sequence to compute\na representation of the sequence ” according to the authors.\nSelf-attention allows the model to evaluate the importance of\ndiﬀerent parts of the input when making a prediction. Reading\ncomprehension, meaningful summarization, and learning task-\nindependent sentence representations have all been eﬀectively\nimplemented with self-attention using transformer architecture,\nwhich has been widely employed in a wide range of NPL processing\napplications (i.e., BERT and GPT-2), serving as the foundation for\na number of pioneering models (i.e.,\nParikh et al., 2016 ; Petrov\net al., 2016 ). In contrast, RNNs, the predecessors to transformers,\nprocess inputs sequentially word after word (i.e., text or speech)\nand struggle with long-range dependencies (\nLi et al., 2018 ). In\nan RNN, the hidden state of the network at each timestep is a\nfunction of the hidden state at the previous timestep and the\ncurrent input. This means that the prediction at each timestep\ndepends on predictions at all previous timesteps, making it diﬃcult\nto parallelize computation (\nLe et al., 2015 ). RNNs generate a\nvector from a sequence in order to capture the meaning of an\nentire sentence, a strategy that performs poorly when dealing\nwith long and complex sentences. In this context, the transformer\narchitecture was speciﬁcally designed to overcome this RNN\nlimitation by introducing a self-attention mechanism.\n/one.tnum./three.tnum. Encoding and decoding components\nMost competitive neural sequence transduction models, such\nas those used in NLP tasks, have an encoder-decoder structure.\nFIGURE /one.tnum\nEncoding and decoding components in a machine translation\napplication.\nThe encoder-decoder structure was ﬁrst introduced in the paper\n“Sequence to Sequence Learning with Neural Networks ” by\nSutskever\net al. (2014). They suggested employing RNNs for both the\nencoder and the decoder. This architecture was later improved\nby the incorporation of the attention mechanism, which allows\nthe decoder to evaluate the importance of diﬀerent parts of\nthe input when generating the output, as proposed in “ Neural\nMachine Translation by Jointly Learning to Align and Translate ”\nby\nBahdanau et al. (2014). This structure consists of two main\ncomponents: an encoder, which processes the input sequence\nand produces a ﬁxed-length representation, and a decoder,\nwhich generates the output sequence based on the ﬁxed-length\nrepresentation, for example in a machine translation application\nfrom Italian to English (\nFigure 1).\nEach encoder has the same structure and is made up of two sub-\nlayers: Self-Attention and Feed Forward Network . Encoder inputs\nﬁrst pass through a self-attention layer, which assists the encoder\nin looking at other words in the input sentence while encoding\na speciﬁc word. Self-attention layer outputs are fed into a feed-\nforward neural network. The same feed-forward network is applied\nto each position independently. Both layers are also present in the\ndecoder yet between them is an attention layer, the encoder-decoder\nattention, that assists the decoder in focusing on relevant parts of\nthe input sentence such as constituents, dependencies, semantic\nroles and coreferences, among others (\nFigure 2).\n/two.tnum. Materials and methods\nThirty verbal insight problems were administered to ChatGPT,\nas listed in the study by Ansburg and Dominowski (2000): the\nﬁrst set of 15 problems was referred to as “ practice problems , ”\nwhile the second set of 15 problems was referred to as “ transfer\nproblems” (see Ansburg and Dominowski, 2000 ; Appendix A, p.\n54–59). The two sets of problems were only used to verify the ability\nof ChatGPT to solve verbal problem-solving tasks, not to replicate\ntheir experimental procedure.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nFIGURE /two.tnum\nEncoder and decoder structure and sublayers.\n/two.tnum./one.tnum. Language-based instructions\nBefore beginning the administration of verbal insight problems\nto ChatGPT, a contest was created (known as a prompt in LLM\njargon) for ChatGPT with the following short instruction: “ Try to\nsolve this practice problem. The ﬁrst one is the following ” and the\nﬁrst practice problem was provided: “ A farmer in California owns a\nbeautiful pear tree. He supplies the fruit to a nearby grocery store.\nThe store owner has called the farmer to see how much fruit is\navailable for him to purchase. The farmer knows that the main trunk\nhas 24 branches. Each branch has exactly 6 twigs. Since each twig\nbears one piece of fruit, how many plums will the farmer be able\nto deliver? ”. In this case ChatGPT failed to solve the problem and\nthe feedback “ The answer provided is not correct ” was provided.\nIt was then fed with “ control instructions ” (CI) (see\nAnsburg and\nDominowski, 2000 , Appendix B, p. 59) before solving the ﬁrst\nproblem again. After the second failure, the ChatGPT was provided\nwith “ strategic instructions ” (SI) (see\nAnsburg and Dominowski,\n2000, Appendix C, p. 60). The CI and SI were listed by Ansburg\nand Dominowski (2000; p. 59–60). The remaining verbal tasks were\nthen subsequently presented, and relevant feedback was provided\nto ChatGPT based on the correct/incorrect answer given, such as\n“The answer is correct. The correct answer is + solution” or “ The\nanswer is not correct. The correct answer is + solution.”\n/two.tnum./two.tnum. Methods\nThe ChatGPT (Jan. 9, 2023) was employed. As stated above,\na total of 30 problems, labeled “ practice problems ” and “ transfer\nproblems” (see the study of Ansburg and Dominowski for more\ninformation) were administered to ChatGPT. According to the\nsolutions for the practice problems and transfer problems described\nby\nAnsburg and Dominowski (2000), the score “0” was assigned for\neach problem which ChatGPT answered incorrectly and “1” was\nattributed for each correct answer. For the two sets of 15 problems,\nthe maximum score attributable was 15. For an exhaustive overview\nof the administered problems and of human sample and ChatGPT’s\nperformance refer to\nTables 1, 2. The performance of ChatGPT\nwas compared to that of the human sample by using the solution\nrate to each problem (\nTables 1, 2) wich was derived from Ansburg\nand Dominowski (2000)’s study based on a sample of 20 subjects.\nOur intention was to utilize only the stimuli from Ansburg and\nDominowski, rather than replicate the study itself.\nPutative between-set diﬀerences were estimated both for\nChatGPT and for the human sample ( n = 20). In the\nformer case signiﬁcance was assessed using Fischer Exact Test,\nwhile in the latter by performing a Wilcoxon rank sum\ntest on the solution rates. Unless otherwise stated, descriptive\nstatistics are presented as median and interquartile ranges\n(25 percentile-75 percentile).\n/two.tnum./two.tnum./one.tnum. Probability distributions of the possible\noutcomes for the human samples on each set of\nproblems\nFor both problem sets, the probability distribution related to\neach possible total score (i.e., number of correct answers) on the\nhuman sample (see\nSupplementary material 1 ) was estimated. Note\nthat the most likely outcome is the score showing the higher\noccurrence probability.\nThe kth score probability was obtained as follows:\n- The set of all possible answer combinations was ﬁrst identiﬁed,\nproducing the desired score. As an example, let us consider\na score of three (three correct answers out of ﬁfteen): such\na score can be obtained by various combinations of answers,\ntheir number equalling the binomial coeﬃcient\n( 15\n3\n)\n. Given a\nset of n items (problems/answers, n = 15), for each kth score\n(kth ranging from 0 to 15), all answer combinations composed\nby k correct answers were identiﬁed (the total number of\ncombinations Ckth\n(\nn, k\n)\nis equal to: n!\nk!(n−k)! ).\n- As a second step, the probability associated with the\nki combination was computed, ( ki ∈ kth as: pki =∏\nj∈ki p∗\nj\n∏\nj∈(n−ki)\n(\n1 − pj\n)\n- Finally, the total probability of obtaining a total score of kth was\nobtained as the sum of the related probabilities over the entire\nset of combinations ( Ckth ): pkth = ∑\nki∈ Ckth pki .\nFor each set of problems, the most likely outcome in the human\nsample was compared (i.e., the total score showing the highest\noccurrence probability) to the total score obtained by ChatGPT.\nIt should be noted that this test accounts for the similarities\nbetween humans and ChatGPT global performance on a set of\nproblems irrespective of paired diﬀerences/similarities related to the\nperformance on the single problems within the set.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /one.tnum Problem-solving tasks: the /one.tnum/five.tnum practice problems and solutions (correct answers), ChatGPT answers, the score attributed (answer accuracy) and\nsolution rate of the human sample.\nProblem-solving tasks\nPractice problems Correct\nanswer\nChatGPT response Answer\naccuracy\n(1) A farmer in California owns a beautiful pear tree. He\nsupplies the fruit to a nearby grocery store. The store owner\nhas called the farmer to see how much fruit is available for\nhim to purchase. The farmer knows that the main trunk has\n24 branches. Each branch has exactly 6 twigs. Since each twig\nbears one piece of fruit, how many plums will the farmer be\nable to deliver?\nNone, plums\ndo not grow\non pear trees\nTo solve this problem, we need to ﬁnd the total number of twigs on the\npear tree. According to the information given in the problem, the main\ntrunk has 24 branches and each branch has 6 twigs. This means tha t the\ntotal number of twigs is 24 branches ∗ 6 twigs/branch = ≪ 24∗6 = 144\n≫ 144 twigs.\n0\nSince each twig bears one piece of fruit, the farmer will be able to deliv er\n144 twigs ∗ 1 fruit/twig = ≪144∗1 = 144 ≫ 144 plums. Answer:\n\\boxed{144}.\nSolution rate: /five.tnum/zero.tnum%\n(2) Hardy Pyle was bragging about his church’s baseball\nteam. He said, “ Three of our players hit home runs and two of\nthose home runs were hit with the bases loaded. Our guys won\n9 to 0 and not a single man crossed home plate .” How was this\npossible?\nThe men were\nall married, or\nthe players\nwere all\nfemale.\nIt is possible that Hardy Pyle’s church’s baseball team won the game 9 to 0\nwithout any men crossing home plate because they scored all of their\nruns through home runs. If three players hit home runs and two of those\nhome runs were hit with the bases loaded, then the team would have\nscored at least 5 runs (3 home runs + 2 home runs with the bases loaded\n= 5 runs). If the team scored a total of 9 runs, then the remaining 4 runs\ncould have been scored through additional home runs or other means\nsuch as errors or walks. Therefore, it is possible for a team to win a\nbaseball game 9 to 0 without any men crossing home plate if they score\nall of their runs through home runs or other means.\n0\nSolution rate: /two.tnum/one.tnum./one.tnum%\n(3) In what year did Christmas and New Year’s fall in the\nsame year?\nThey always\ndo.\nIt is not possible to determine the year in which Christmas and New\nYear’s fell in the same year based on the information provided in the\nproblem. Christmas and New Year’s always fall in the same year , as\nthey are consecutive holidays that are celebrated on December 25th and\nJanuary 1st, respectively. Therefore, every year, Christmas and New\nYear’s fall in the same year.\n1\nSolution rate: /seven.tnum/five.tnum%\n(4) How many cubic meters of dirt are in a hole 6 meters\nlong, 2 meters wide and one meter deep?\nThere is no\ndirt in a hole.\nTo ﬁnd the volume of a rectangular prism (such as a hole), you can use\nthe formula: volume = length ∗ width ∗ height. In this case, the hole is 6\nmeters long, 2 meters wide and one meter deep, so the volume of the hole\nis 6 meters ∗ 2 meters ∗ 1 meter = ≪621 = 12≫12 cubic meters.\nTherefore, there are 12 cubic meters of dirt in the hole. Answer:\n\\boxed{12}.\n0\nSolution rate: /two.tnum/zero.tnum%\n(5) There is an ancient invention still used in some parts of\nthe world today that allows people to see through walls.\nWhat is it?\nWindows It is not possible to determine the ancient invention that allows peo ple to\nsee through walls based on the information provided in the problem.\nThere is no such invention that allows people to see through walls. Some\nforms of technology, such as certain types of radar or thermal imaging\ncameras, can be used to detect objects or people behind walls or other\nbarriers, but they do not allow people to actually see through the walls.\nTherefore, it is not possible to accurately answer this problem.\n0\nSolution rate: /six.tnum/five.tnum%\n(6) Captain Frank was out for a walk when it started to rain.\nHe did not have an umbrella and he wasn’t wearing a hat.\nHis clothes were soaked yet not a hair on his head got wet.\nHow could this happen?\nHe was bald It is possible that Captain Frank’s hair did not get wet because he was\nbald. If a person is bald, they do not have any hair on their head, so it is\nnot possible for their hair to get wet. Therefore, it is possible for C aptain\nFrank’s hair to not get wet even though his clothes were soaked in the\nrain, if he is bald. This is the only explanation that can be inferred from\nthe information provided in the problem.\n1\nSolution rate: /six.tnum/five.tnum%\n(7) Mr. and Mrs. Plum have six daughters and each daughter\nhas one brother. How many people are in the Plum family?\nThere are nine\npeople in the\nPlum family.\nThe Plum family consists of Mr. Plum, Mrs. Plum, and their six\ndaughters. Each daughter has one brother, so there are an additional 6\nbrothers in the family. In total, there are 6 daughters + 6 brothers + Mr.\nPlum + Mrs. Plum = ≪6 + 6 + 1 + 1 = 14≫14 people in the Plum\nfamily. Answer: \\boxed{14}.\n0\nSolution rate: /one.tnum/nine.tnum%\n(Continued)\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /one.tnum (Continued)\nProblem-solving tasks\nPractice problems Correct\nanswer\nChatGPT response Answer\naccuracy\n(8) If you drop a rock, would it fall more rapidly through\nwater at 40 degrees Fahrenheit or 20 degrees Fahrenheit?\nExplain your answer.\n40 degrees\nbecause at 20\nwater is frozen\nA rock would fall more rapidly through water at 40 degrees\nFahrenheit than at 20 degrees Fahrenheit . The density of a substance is\na measure of the mass of the substance per unit volume. The density of\nwater decreases as the temperature increases, which means that warm\nwater is less dense than cold water. This means that a rock will fall m ore\nslowly through warm water than through cold water because the warm\nwater is less dense and oﬀers more resistance to the movement of the\nrock. At 40 degrees Fahrenheit, the water is warmer and less dense\nthan at 20 degrees Fahrenheit, so the rock will fall more rapid ly\nthrough the water at 20 degrees Fahrenheit than at 40 degrees\nFahrenheit. This is because the denser, colder water at 20 degrees\nFahrenheit will oﬀer more resistance to the movement of the rock,\ncausing it to fall more slowly through the water.\n1\nSolution rate: /five.tnum/five.tnum%\n(9) Shadow opened the door to Dr. Apple’s oﬃce and\nsurveyed the scene. Dr. Apple’s head lay on his desk in a pool\nof blood. On the ﬂoor to his right lay a gun. There were\npowder burns on his right temple indicating that he was shot\nat close range. On his desk was a suicide note, and in his\nright hand the pen that had written it. Shadow noted that\ndeath had occurred in the past hour. All of a sudden Dr.\nApple’s wife burst into the oﬃce and screamed “ My\nhusband’s been shot!” She ran toward the body and saw the\nnote and cried, “ Why would he want to kill himself? ” Shadow\nreplied “This was no suicide, it is a clear case of murder. ”\nHow does Shadow know?\nThe gun would\nbe the last\nthing in his\nright hand not\nthe pen if it\nwere suicide.\nShadow knows that the death of Dr. Apple was a case of murder because\nthere are powder burns on his right temple, indicating that he was sho t at\nclose range. A suicide victim typically shoots themselves at close rang e, so\nthe presence of powder burns suggests that the shooter was in close\nproximity to the victim when the shot was ﬁred. In a suicide, the victim\nis the one who ﬁres the shot, so there would not be any powder burns on\ntheir temple. Therefore, the presence of powder burns on Dr. Apple’s\ntemple suggests that someone else was present when the shot was ﬁred\nand that Dr. Apple was murdered.\n0\nSolution rate: /six.tnum/five.tnum%\n(10) A man who lived in a small town in the United States\nmarried 20 diﬀerent women of the same town. All are still\nliving and he never divorced any of them. In this town,\npolygamy is unlawful; yet, he has broken no law. Can you\nexplain this?\nThe man is a\nminister or\njustice of the\npeace.\nIt is not possible to explain how the man in the small town in the\nUnited States has married 20 diﬀerent women without breaking the law\non polygamy, as polygamy is the practice of having more than one spouse\nat the same time, and it is unlawful in the United States. It is not lega l for\na person to be married to multiple spouses at the same time in the\nUnited States. Therefore, it is not possible for the man in the small to wn\nto have married 20 diﬀerent women without breaking the law. The\ninformation provided in the problem is contradictory and cannot b e\nreconciled.\n0\nSolution rate: /one.tnum/zero.tnum%\n(11) Can a man marry his widow’s sister? Explain your\nanswer.\nNo, the man\nwould be dead.\nIt is not possible for a man to marry his widow’s sister, as the ma n\nwould have to be deceased in order for his spouse to be a widow . A\nwidow is a woman whose spouse has died, and a person cannot marry\ntheir own spouse if they are deceased. Therefore, it is not possible for a\nman to marry his own widow’s sister.\n1\nSolution rate: /five.tnum/five.tnum./six.tnum%\n(12) Sid Shady was working for a large construction\ncompany that was very concerned about employee theft.\nSomeone tipped the company security that Shady was the\nman to watch. Each night, he passed through security with a\nwheelbarrow full of scrap lumber, discarded electrical wires\nand chunks of concrete. The security guards checked the\ncontents daily, but could ﬁnd nothing of value. What was\nShady stealing?\nHe was\nstealing\nwheelbarrows.\nIt is not possible to determine what Sid Shady was stealing based o n the\ninformation provided in the problem. The security guards checked the\ncontents of Shady’s wheelbarrow each night and found nothing of value,\nso it is possible that Shady was not stealing anything at all. The company\nmay have been mistaken in its suspicion of Shady or the person who\ntipped oﬀ the company may have provided incorrect or misleading\ninformation. Without additional information, it is not possibl e to\naccurately answer this problem.\n0\nSolution rate: /five.tnum/seven.tnum./nine.tnum%\n(13) At 7 a.m., a train moving 90 kph leaves Montreal\nheading for Toronto. At 8 a.m., a train running 110 kph\nleaves Toronto heading for Montreal. Which train will be\ncloser to Montreal when they meet?\nWhen they\nmeet, the\ntrains will be at\nthe same spot.\nWhen the two trains meet, they will be the same distance from\nMontreal. This is because the trains are traveling in opposite directions\nand will cover equal distances from their respective starting points. A t\n7 a.m., the train leaving Montreal is 90 kph and at 8 a.m., the tra in\nleaving Toronto is 110 kph. If the trains are traveling in opposi te\ndirections, they will cover equal distances from their respective\nstarting points, so when they meet, they will be the same distan ce from\nMontreal.\n1\nSolution rate: /two.tnum/eight.tnum./six.tnum%\n(Continued)\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /one.tnum (Continued)\nProblem-solving tasks\nPractice problems Correct\nanswer\nChatGPT response Answer\naccuracy\n(14) A woman said to her husband: “ This morning, one of my\nearrings fell into my coﬀee. Even though my cup was full, the\nearring did not get wet.” How could this be true?\nIt fell into\ncoﬀee\ngrounds.\nYes, it is possible that the woman’s earring fell into the coﬀe e grounds\nrather than the coﬀee itself, which would explain why the earr ing did\nnot get wet even though the cup was full . When coﬀee is brewed, the\nwater passes through the coﬀee grounds and the liquid coﬀee is produced.\nThe coﬀee grounds remain in the ﬁlter or coﬀee pot and do not get wet.\nIf the earring fell into the coﬀee grounds rather than the liquid coﬀee, it\nwould not get wet. This is a possible explanation for the woman’s\nstatement that her earring fell into her coﬀee but did not get wet.\n1\nSolution rate: /five.tnum/zero.tnum%\n(15) Barney Dribble is carrying a pillow case full of feathers.\nHardy Pyle is carrying three pillow cases the same size as\nBarney’s, yet Hardy’s load is lighter. How can this be?\nHardy’s pillow\ncases are\nempty\nYes, if Hardy’s pillow cases are empty, they would be lighter than\nBarney’s pillow case ﬁlled with feathers , which would explain how\nHardy’s load is lighter even though he is carrying three pillow cases the\nsame size as Barney’s. I’m glad that I could help clarify this for you. If\nyou have any other questions, please don’t hesitate to ask.\n1\nSolution rate: /six.tnum/zero.tnum%\nTotal score 7/15\n/two.tnum./two.tnum./two.tnum. Problem-wise associations between human\npopulation solution rates and ChatGPT\nperformance\nAmong possible answer combinations leading to a total score\nequal to that obtained by ChatGPT, those corresponding to\nChatGPT answer combinations were identiﬁed and subsequently\ncompared to the occurrence probability of all other answer patterns\nleading to the same total score. This procedure was applied\nboth problems.\n/two.tnum./two.tnum./three.tnum. Probability distributions of the possible\noutcomes and associations between human\npopulation solution rates and ChatGPT\nperformance on the pooled dataset\nThe same analyses were then conducted by pooling together\nthe two problem sets. This choice was motivated by the utility of\nhaving a general view of ChatGPT performance as compared to that\nof the human population, independently of the problem type and\nthe related solution strategy. The appropriateness of the pooling in\nbased on the fact that the number of problems is balanced across\nthe two sets (15 problems each).\n/three.tnum. Results\nThe performance of ChatGPT on practice and transfer\nproblems are shown in Tables 1, 2, respectively. Each table displays\nthe problems assigned to ChatGPT as well as the proper solutions\n(correct answers), ChatGPT answers, the score obtained (answer\naccuracy) and the solution rate of the human sample.\nThe performance of ChatGPT was subsequently compared with\nthose of a sample of individuals ( n = 20) in the two sets of problems.\nChatGPT answered correctly to seven problems out of\nﬁfteen in the case of the practice problems , and to ﬁve out\nof ﬁfteen in the case of transfer problems : the ChatGPT\nperformance between-set diﬀerence was not signiﬁcant (Fischer\nExact Test, p < 0.72, see\nSupplementary material 2 ). Human\nsample solution rates were respectively 0.55 (0.23–0.64) on\nthe Practice set, and 0.35 (0.30–0.40) on the transfer set.\nWhile the solution rate was higher for the former set as\ncompared to the latter, the between-set diﬀerence was not\nsigniﬁcant (Wilcoxon rank sum test, z = 1.29, p < 0.20, see\nSupplementary material 3 ).\nNotably, for both sets of problems, ChatGPT performance (i.e.,\nthe number of correct answers) was equal to the total score of\nthe human sample showing the highest occurrence probability, as\nclearly apparen from\nFigures 3A, B.\nFor each problem set, all answer combinations leading to\nthe total ChatGPT score was identiﬁed (see Figures 4A, B) and\ntheir occurrence probability on the human sample estimated (see\nFigures 3A, B). For each set, the ensemble of combinations included\nthat of ChatGPT:\nFirst set of problems ( practice problems): [0, 0, 1, 0, 0, 1, 0, 1, 0,\n0, 1, 0, 1, 1, 1].\nSecond set of problems ( transfer problems): [1, 0, 0, 0, 0, 1, 0, 1,\n0, 0, 0, 1, 1, 0, 0].\nWhen considering the Practice Problems , the ChatGPT\ncombination occurrence probability ( p ∼\n= 1.52e-04), was\nobserved as being above the threshold identifying the\n5% percentile of those combinations showing the highest\noccurrence probability in the human sample ( p ∼\n= 1.50e-04,\nsee\nFigure 5A, and Supplementary material), thus indicating an\nassociation between the human sample problem by problem\nperformance and that of ChatGPT. However, the ChatGPT\ncombination occurrence probability on the Transfer set ( p\n∼\n= 5.01e-05) neared the 27th percentile of the occurrence\nprobability distribution, thus resulting in a high unlikely\nsolution for the human population (i.e., no association\nbetween the problems correctly solved by ChatGPT and\nthose with a higher solution rate for the human sample, see\nFigure 5B).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /two.tnum Problem-solving tasks: the /one.tnum/five.tnum transfer problems and solutions (correct answers), ChatGPT answers, the score attributed ( answer accuracy) and\nsolution rate of the human sample.\nProblem-solving tasks\nTransfer problems Correct\nanswer\nGPT’s response Answer\naccuracy\n(1) Professor Bumble was driving along in his old car when\nsuddenly it shifted gears by itself. He paid no attention and\nkept on driving. Why wasn’t he concerned?\nHe was driving\nan automatic\nThe Professor Bumble was not concerned when the car shifted gears b y\nitself because the car was most likely an Automatic gear shifti ng car.\nIn an automatic car, gears are changed automatically by the car its elf\nwithout the need for manual shifting from the driver. This means that it’ s\nnormal for the car to shift gears by itself and the driver doesn’t need to\ntake any action, Professor Bumble would expect this behavior from the\ncar. Therefore, in this case, he didn’t pay attention and kept on dr iving\nbecause he was not concerned that anything was wrong.\n1\nSolution rate: /four.tnum/zero.tnum%\n(2) Which would be worth more, a pound of $10 pure gold\ncoins or half a pound of $20 pure gold coins; or would they\nbe worth the same amount? Explain your answer.\nA pound of\ngold is always\nworth more\nthan half a\npound of gold.\nBoth a pound of $10 pure gold coins and half a pound of $20 pure gold\ncoins would be worth the same amount. Even though the $20 pure gold\ncoins are worth twice as much as the $10 pure gold coins, half a pound of\n$20 pure gold coins would have half the weight as a pound of $10 pure\ngold coins. Since the value of the gold coins is based on their weight and\ngold’s value is per ounce, the weight of the coins will determine the value\nof it. And since the weight of both options is the same, the value would b e\nthe same as well. Therefore, the answer is that both options would be\nworth the same amount, regardless of the face value of the coins.\n0\nSolution rate: /four.tnum/five.tnum%\n(3) Dee Septor, the famous magician, claimed to be able to\nthrow a ping-pong ball so that it would go a short distance,\ncome to a dead stop, and then reverse itself. He also added\nthat he would not bounce the ball against any object or tie\nanything to it. How could he perform this feat?\nHe could\nthrow it\nstraight up\nDee Septor, the magician, could perform this feat by using a hidden\nmagnet. He could throw a ping-pong ball that has a small magnet\nembedded inside it, and as it comes to a stop, the magnet would be\nattracted to a hidden magnet that he has positioned in the opposite\ndirection. This would cause the ball to reverse its direction without\nbouncing against any object or having anything tied to it.\nAnother way, he could use air pressure to change the direction of the ball,\nby blowing air to the ball as it comes to a stop, it will change its direction.\nThis is a common trick used by magicians to make it look like the ball i s\nchanging direction on its own.\nBoth of these explanations involve the use of hidden and unseen elements\nto create the illusion of the ball reversing itself, and are not actually using\nthe ball’s properties to reverse it, they are just hidden tricks to make the\naudience think that the ball is reversing its direction.\n0\nSolution rate: /three.tnum/six.tnum./four.tnum%\n(4) A prisoner was attempting to escape from a tower. He\nfound in his cell a rope that was half long enough to permit\nhim to reach the ground safely. He divided the rope in half,\ntied the two parts together, and escaped. How could he have\ndone this?\nHe unraveled\nthe rope and\ntied the two\npieces together\nThe prisoner could have done this by tying the two halves of the rope\ntogether, forming a knot and making it whole again. By doing this he\ncould have doubled the strength of the rope, and this would have made it\nlong enough to safely reach the ground.\nAnother way, he could have used the two halves of the rope as two\nseparate ropes to make a ladder by tying one end of each rope on a\ndiﬀerent level of the tower, and then climb down each step of the ladder.\nThis way he can combine the two halves to reach the ground.\nIn this problem, the prisoner used the limited resources he had at his\ndisposal, his intelligence and creativity to overcome the obstacle an d ﬁnd\na solution to escape.\n0\nSolution rate: /four.tnum/zero.tnum%\n(5) Uriah Fuller, the famous super-psychic, can tell you the\nscore of any baseball game before it starts. What is his secret ?\nThe score is\nalways zero to\nzero before a\ngame starts\nAs a language model AI, I do not have the ability to access current or\npast information about any baseball games, because my training d ata\ngoes until 2021 and my browsing ability is disabled.\nBut to answer your question, Uriah Fuller the super-psychic would have\nsome kind of ability to predict the future, such as precognition,\nclairvoyance or divination. He could use this ability to know the outcom e\nof a baseball game before it starts.\nIt is important to note that while some people may claim to have psy chic\nabilities, there is no scientiﬁc evidence to support the existence of the se\nabilities, and Uriah Fuller’s “super-psychic” abilities are also ﬁctional and\nnot real.\n0\nSolution rate: /three.tnum/zero.tnum%\n(Continued)\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /two.tnum (Continued)\nProblem-solving tasks\nTransfer problems Correct\nanswer\nGPT’s response Answer\naccuracy\n(6) Two mothers and two daughters were ﬁshing. They\nmanaged to catch one big ﬁsh, one small ﬁsh and one fat ﬁsh.\nSince only three ﬁsh were caught, how is it possible that they\neach took home a ﬁsh?\nOne of the\nmothers was\nalso a daughter\nThis question is a classic riddle, the solution to this problem is not a\nstraightforward one. The key to the answer is to realize that the\nmothers are the two daughters’ mother, and the daughters are the two\nmothers’ daughters. So, they are a family of three: a grandmother, a\nmother, and a daughter . With this information, it becomes clear that\neach of them is able to take a ﬁsh home, because they are three diﬀerent\npeople, not four.\nThe riddle is describing a scenario that is not intended to be taken\nliterally, instead, it is meant to present a problem in a way that requi res\na diﬀerent way of thinking, to ﬁnd the solution.\n1\nSolution rate: /three.tnum/zero.tnum%\n(7) Calendars made in England do not show Lincoln’s\nBirthday, of course. Do these calendars show the fourth of\nJuly? Explain your answer.\nYes, July\nalways has a\nfourth day\nThe answer is no, calendars made in England do not show the fourth of\nJuly. The reason is that the fourth of July is Independence Day in the\nUnited States, and it is not a national holiday in England. Linco ln’s\nBirthday is also not a national holiday in England, as it is in the\nUnited States. This means that neither Lincoln’s Birthday nor the 4 th of\nJuly are typically included on calendars made in England. As such, both\nholidays are speciﬁc to the United States and not recognized in Engla nd.\n0\nSolution rate: /four.tnum/two.tnum./one.tnum%\n(8) A dealer in antique coins got an oﬀer to buy a beautiful\nbronze coin. The coin had an emperor’s head on one side\nand the date 544 BC stamped on the other. The dealer\nexamined the coin and realized that it was a fake. How did\nhe know the coin was phony?\nChrist was had\nnot been born,\nso a coin from\nthat time\nwould not be\nmarked BC\nThe dealer knew the coin was a fake because the date 544 BC was\nstamped on the other side. This is not possible because the concept of\n“BC” (Before Christ) was not created until several centuries later , and\nthe concept of using “BC” as a notation to indicate a date before th e\nbirth of Jesus Christ was not widely used until the Middle Age s.\nAdditionally, the concept of a calendar based on the birth of Jesus was\nnot established until several centuries after the death of Jesus. This m eans\nthat the concept of “BC” would not have existed at the time period\nrepresented by the date 544 BC. Therefore, the dealer realized that the\ncoin was a fake because the date stamped on it was not accurate for the\ntime period it purported to be from.\n1\nSolution rate: /three.tnum/five.tnum%\n(9) There is a town in Northern Ontario where 5% of all the\npeople living in the town have unlisted phone numbers. If\nyou selected 100 names at random from the town’s phone\ndirectory, on average, how many of these people selected\nwould have unlisted phone numbers?\nZero, they\nwould not be\nlisted in the\ndirectory\nIf 5% of all the people in the town have unlisted phone numbers, that\nmeans for every 100 people, 5 of them will have unlisted phone numbers.\nSo, if you select 100 names at random from the phone directory, on\naverage, 5 of those people selected will have unlisted phone numbers.\n0\nSolution rate: /three.tnum/zero.tnum%\n(10) It is estimated that the earth weighs 6 sextillion tons.\nHow much more would the earth weigh if one sextillion tons\nof concrete and stone were used to build a wall?\nIt would weigh\nthe same since\nall materials\nare taken from\nthe original\nweight of the\nearth\nA sextillion is a very large number, it is equivalent to 1 followed by 21\nzeroes (10∧21). Since the wall is made of concrete and stone, the weight\nof the wall will be added to the earth’s weight. If one sextillion to ns of\nconcrete and stone were used to build the wall, the earth would weigh 7\nsextillion tons, which is one sextillion tons more than its curren t weight.\n0\nSolution rate: /three.tnum/one.tnum./eight.tnum%\n(11) A young boy turned oﬀ the light in his bedroom and\nmanaged to get into bed before the room was dark. If the bed\nis ten feet from the light switch and he used no wires, strings\nor other contraptions to turn oﬀ the light, how did he do it?\nIt was still\ndaylight\nThe answer is that the light was turned oﬀ remotely. The boy is able to\nturn the light oﬀ before the room is dark because he is using a remote\ncontrol or some other form of wireless technology to turn oﬀ the light\ninstead of physically moving to the switch and turning it oﬀ.\n0\nSolution rate: /three.tnum/five.tnum%\n(12) While on safari in the wild jungles of Africa, Professor\nQuantum woke one morning and felt something in the\npocket of his shorts. It had a head and tail, but no legs. When\nQuantum got up he could feel it move inside his pocket.\nQuantum, however, showed little concern and went about\nhis morning rituals. Why such a casual attitude toward the\nthing in his pocket?\nIt was a coin Answer: The object in question was a coin . 1\nSolution rate: /three.tnum/six.tnum./eight.tnum%\n(Continued)\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nTABLE /two.tnum (Continued)\nProblem-solving tasks\nTransfer problems Correct\nanswer\nGPT’s response Answer\naccuracy\n(13) Professor Bumble, who is getting on in years, is growing\nabsent minded. On the way to a lecture 1 day, he went\nthrough a red light and turned down a one-way street in the\nwrong direction. A policeman observed the entire scene but\ndid nothing about it. How could Professor Bumble get away\nwith such behavior? Montreal when they meet?\nHe was on foot Answer: He was walking not driving 1\nSolution rate: /three.tnum/zero.tnum%\n(14) A giant inverted steel pyramid is perfectly balanced on\nits point. Any movement of the pyramid will cause it to\ntopple over. Underneath the pyramid is a $100 bill. How\nwould you remove the bill without disturbing the pyramid?\nBurn or tear\nthe dollar bill\nYou could use a long, thin tool, such as a chopstick or a pencil, to slide\nthe bill out from underneath the pyramid without physically touching the\npyramid itself. This way, the pyramid remains balanced and\nundisturbed.\n0\nSolution rate: /four.tnum/zero.tnum%\n(15) Hardy Pyle was washing windows on a high-rise oﬃce\nbuilding when he slipped and fell oﬀ a sixty-foot ladder onto\nthe concrete sidewalk below. Incredibly, he did not injure\nhimself in any way. How was this possible?\nHe was on the\nbottom rung\nof the ladder\nThe ladder was leaning against a wall on the ﬁrst ﬂoor, Hardy was n ot at\na high altitude when he fell.\n0\nSolution rate: /three.tnum/one.tnum./eight.tnum%\nTotal score 5/15\nFIGURE /three.tnum\nHuman sample outcome probabilities: outcome probabilities for eac h possible total score (i.e., number of correct answers, range /zero.tnum–/one.tnum/five.tnum) are\npresented for the Practice set (A), the Transfer set (B) and the pooled set [ (C), Practice + Transfer Problems]. In each plot, the outcome with the\nhighest probability is highlighted in black. Notably, the total scor e with the highest probability is equal to the performance of Cha tGPT for both for\neach set of problems and for the pooled set.\n/three.tnum./one.tnum. Pooled problems set\nAs a further step the two sets of problems were pooled together.\nThis choice was supported by the following three main points:\n1. The number of problems was balanced across the two sets (15\nproblems each).\n2. ChatGPT performance on the Practice set was not\nsigniﬁcantly diﬀerent from that obtained on the Transfer set\n(Fischer Exact Test, p < 0.72, see\nSupplementary material 1 ).\n3. The median solution rate of the human population on the\nPractice set was not signiﬁcantly diﬀerent from that obtained\non the transfer set (Wilcoxon rank sum test, z = 1.53, p <\n0.13, Supplementary material 2 ).\nWhen considering the pooled datasets, ChatGPT performance\n(i.e., number of correct answers = 12), was again equal to the total\nscore of the human population showing the highest occurrence\nprobability (\nFigure 3A). The ChatGPT combination ( p ∼\n=7.61e-09)\nwas above the threshold identifying the 5% percentile of those\npatterns showing the highest occurrence probability in the human\nsample (p ∼\n= 7.35e-09, see\nFigure 5C), thus indicating an association\nbetween the human sample problem by problem performance and\nthat of ChatGPT, including when considering the entire dataset.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nFIGURE /four.tnum\nAnswer patterns equalling the ChatGPT total score: for each set of problems, all possible answer combinations leading to a score e qual to that\nobtained by ChatGPT are presented [Practice set, (A) and Transfer set, (B)]. In each matrix, rows correspond to all possible combinations and columns\nto the answers. Each matrix element identiﬁes a possible answ er within a combination (black = correct, white = wrong). The matrix related to the\npooled set is not presented as the number of possible combinations e xceeded /eight.tnum/six.tnum,/zero.tnum/zero.tnum/zero.tnum,/zero.tnum/zero.tnum/zero.tnum and as such the image would have been unintelligible.\nFIGURE /five.tnum\nDistributions of answer combination probabilities equalling C hatGPT score: for each set of problems the distribution of probabi lities associated with\nall possible answer combinations leading to a total score equal to that obtained by ChatGPT is presented using a scatterplot. The /five.tnumth, /five.tnum/zero.tnumth, and /nine.tnum/five.tnumth\npercentiles of the distribution are highlighted by black horizon tal lines, whereas the probability associated with answer comb inations equalling that\nof ChatGPT is identiﬁed by a black dot. Note that for ease of visua lization, in each plot a down sampled number of combinations and pr obabilities are\npresented using a logarithmic scale (y-axis). (A–C) refer, respectively to Practice Problems, Transfer Problems, and Pooled Problems sets.\n/four.tnum. Discussion and conclusions\nIn the current study, ChatGPT was provided with two sets of\nverbal insight problems, namely one set of practice and one of\ntransfer problems (each set consisting of 15 problems, for a total of\n30 problems). The score was assigned based on the accuracy of the\nanswers provided by ChatGPT. The study’s ﬁndings revealed that\nthe global performance of ChatGPT was equal, as apparent from\nFigure 3, to the one showing the highest occurrence probability\nin the human sample: this ﬁnding is consistent for the practice,\ntransfer and pooled problems sets. These results indicate that\nChatGPT performance on both tasks (and on the pooled tasks),\nwere completely in line with those of the average human subject,\nindicating that it performed similarly to humans. Moreover, both\nwhen considering the practice and the pooled problem sets, the\nChatGPT answers’ combination occurrence probability was above\nthe threshold identifying the 5% percentile of those combinations\n(producing the same total score), showing the highest occurrence\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nprobability in the human sample. This was not the case for the\ntransfer problems set.\nIn general terms, LLMs are unquestionably highly competent\nat making connections and the research presented demonstrates\nhow such connections may be used to complete tasks which were\nonce thought to be impossible. Indeed, LLMs, such as ChatGPT-\n3, are neural networks that have undergone training to forecast\nthe most probable verbal output (i.e., word/sentence) based on a\ncertain order of words. They make predictions by identifying the\nword with the strongest association (or probability). It is worth\nnoting that associationism, ﬁrst introduced as one of the earliest\ntheories in psychology (\nJames, 1890 , 2007) waned in popularity\nwhen the rise of other theoretical frameworks such as behaviorism\nand cognitive psychology demonstrated that associationism could\nnot fully explained the intricacy of human language production.\nAn issue that has sparked widespread debate in the cognitive\nscience community is whether LLMs truly have problem-solving\nabilities, or such abilities are the result of a deep understanding of\nthe problem. In this context, our ﬁndings suggest the possibility\nthat LLMs could employ associationism, thus drastically reducing\nthe number of tasks that complex associational models are unable\nto perform. Some LLMs, like ChatGPT-3, are able to complete\ntasks in line with the average human ability and certainly are\nadvanced associators. From this perspective, this calls into question\ntheories which may have led cognitive psychology to rule out\nassociationism., of note this concept was also highlighted by a\npre-print paper by\nLoconte et al. (2023).\nIt is therefore evident that the number of tasks that an associator\nis unable to solve is gradually reducing and future research will\nhave to identify limits that cannot be pushed any further as LLMs\nbecome ever more competent.\nWhile this study sheds light on the “behavior” of ChatGPT\nwhen dealing with verbal problems, it presents some limitations:\n(i) the size of the sample representative of humans ( n = 20)\nto which ChatGPT was compared, was relatively small, and\nas such, additional testing is required to validate the results\nherein presented; (ii) the study examined the performance of\nChatGPT using only a single version of the model. From\nthis perspective, it would be beneﬁcial to replicate the current\nstudy with more recent and/or advanced versions of the model\nin order to verify whether there has been any improvement;\n(iii) ﬁnally, the study only examined ChatGPT performance\non verbal insight problems; it would be of utter interest to\ninvestigate how the model performs on other types of problems\nor tasks.\nIn conclusion, while this study provides some evidence that\nChatGPT performance on verbal insight problems, is similar to\nthose of an average human subject, it is important to recognize\nits limitations and to continue exploring both the potential and\nlimitations of the model in future studies. Additional research\nmay be carried out in order to expand on the methods and\nﬁndings presented in this study, allowing for a more comprehensive\nunderstanding of the capabilities and limitations of ChatGPT and\nother LLMs.\nData availability statement\nThe data analyzed in this study is subject to the following\nlicenses/restrictions: The dataset used and analyzed during the\ncurrent study is available from the corresponding author upon\nreasonable request. Requests to access these datasets should be\ndirected to\ngraziella.orru@unipi.it.\nAuthor contributions\nGO: conceived the experiment, designed the experimental task,\nand drafted the manuscript. GO and AP: contributed to data\nacquisition, data analysis, and writing the ﬁnal version of the\nmanuscript. All authors: data interpretation. All authors revised\nthe manuscript critically and gave ﬁnal approval of the version to\nbe published.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/frai.2023.\n1199350/full#supplementary-material\nReferences\nAnsburg, P. I., and Dominowski, R. I. (2000). Promoting insi ghtful problem solving.\nJ. Creat. Behav. 34, 30–60. doi: 10.1002/j.2162-6057.2000.tb01201.x\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473 .\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nOrrù et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/nine.tnum/nine.tnum/three.tnum/five.tnum/zero.tnum\nDell’Osso, L., Bazzichi, L., Baroni, S., Falaschi, V., Convers ano, C., Carmassi,\nC., et al. (2015). The inﬂammatory hypothesis of mood spectrum br oadened to\nﬁbromyalgia and chronic fatigue syndrome. Clin. Exp. Rheumatol. 33(1 Suppl.\n88), S109–S116.\nDevlin, J., Chang, M. W., Lee, K., and Toutanova, K. (2018). Be rt: pre-\ntraining of deep bidirectional transformers for language un derstanding. arXiv\npreprint arXiv:1810.04805.\nFerrucci, R., Mameli, F., Ruggiero, F., Reitano, M., Miccoli, M., Gemignani, A., et al.\n(2022). Alternate ﬂuency in Parkinson’s disease: a machine learning analysis. PLoS ONE\n17, e0265803. doi: 10.1371/journal.pone.0265803\nJames, W. (1890). The Principles of Psychology Volume II By William James . Bristol:\nThoemmes Press. doi: 10.1037/10538-000\nJames, W. (2007). The Principles of Psychology . Cosimo, Inc.\nKahneman, D. (2011). Thinking, Fast and Slow . Macmillan.\nLe, Q. V., Jaitly, N., and Hinton, G. E. (2015). A simple way to init ialize recurrent\nnetworks of rectiﬁed linear units. arXiv preprint arXiv:1504.00941 .\nLi, S., Li, W., Cook, C., Zhu, C., and Gao, Y. (2018). “Independ ently recurrent\nneural network (indrnn): building a longer and deeper rnn, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (Salt Lake City, UT: IEEE),\n5457–5466. doi: 10.1109/CVPR.2018.00572\nLoconte, R., Orrù, G., Tribastone, M., Pietrini, P., and Sart ori, G. (2023). Challenging\nChatGPT ’ Intelligence’ with Human Tools: A Neuropsychological Inve stigation on\nPrefrontal Functioning of a Large Language Model . Available online at: https://ssrn.com/\nabstract=4377371 (accessed March 20, 2023).\nMayer, R. E. (1992). Thinking, Problem Solving, Cognition . WH Freeman/Times\nBooks/Henry Holt and Co.\nMayer, R. E., and Wittrock, M. C. (2006). “Problem solving, ” in Handbook\nof Educational Psychology , eds P. A. Alexander and P. H. Winne (Mahwah, NJ:\nErlbaum), 287–303.\nMazza, C., Monaro, M., Orrù, G., Burla, F., Colasanti, M., Ferra cuti, S., et al. (2019).\nIntroducing machine learning to detect personality faking-go od in a male sample: a\nnew model based on Minnesota multiphasic personality inventory -2 restructured form\nscales and reaction times. Front. Psychiatry 10, 389. doi: 10.3389/fpsyt.2019.00389\nNewell, A., and Simon, H. A. (1972). Human Problem Solving (Vol. 104, No. 9) .\nEnglewood Cliﬀs, NJ: Prentice-Hall.\nOrrù, G., Conversano, C., Ciacchini, R., and Gemignani, A. (20 21).\nA brief overview on the contribution of machine learning in sy stems\nneuroscience. Curr. Psychiatry Res. Rev. Formerly Curr. Psychiatry Rev. 17, 66–71.\ndoi: 10.2174/2666082217666210913101627\nOrrù, G., Gemignani, A., Ciacchini, R., Bazzichi, L., and Conv ersano, C. (2020a).\nMachine learning increases diagnosticity in psychometric ev aluation of alexithymia in\nﬁbromyalgia. Front. Med. 6, 319. doi: 10.3389/fmed.2019.00319\nOrrù, G., Monaro, M., Conversano, C., Gemignani, A., and Sarto ri, G. (2020b).\nMachine learning in psychometrics and psychological research. Front. Psychol. 10,\n2970. doi: 10.3389/fpsyg.2019.02970\nOrrù, G., Pettersson-Yeo, W., Marquand, A. F., Sartori, G., and Mechelli, A.\n(2012). Using support vector machine to identify imaging biom arkers of neurological\nand psychiatric disease: a critical review. Neurosci. Biobehav. Rev. 36, 1140–1152.\ndoi: 10.1016/j.neubiorev.2012.01.004\nPace, G., Orrù, G., Monaro, M., Gnoato, F., Vitaliani, R., Boone, K. B., et al. (2019).\nMalingering detection of cognitive impairment with the B test is boosted using machine\nlearning. Front. Psychol. 10, 1650. doi: 10.3389/fpsyg.2019.01650\nParikh, A., Täckström, O., Das, D., and Uszkoreit, J. (2016) . “A decomposable\nattention model, ” in Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing (Austin, TX: Association of Computational Linguistics),\n2249–2255.\nPetrov, S., Barrett, L., Thibaux, R., and Klein, D. (2016). “L earning accurate,\ncompact, and interpretable tree annotation, ” in Proceedings of the 21st International\nConference on Computational Linguistics and 44th Annual Meeting of the A CL\n(ACL), 433–440.\nPolya, G. (2004). How to Solve It: A New Aspect of Mathematical Method (No. 246) .\nPrinceton university press.\nSartori, G., Zangrossi, A., Orrù, G., and Monaro, M. (2017). “D etection of\nmalingering in psychic damage ascertainment, ” in P5 Medicine and Justice: Innovation,\nUnitariness and Evidence (Springer), 330–341. doi: 10.1007/978-3-319-67092-8_21\nSutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to se quence learning with\nneural networks. Adv. Neural Inform. Process. Syst. 27, 1–9.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inform. Process. Syst. 30, 1–11.\nWallas, G. (1926). The Art of Thought (Vol. 10) . Harcourt, Brace.\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org"
}