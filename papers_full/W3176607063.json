{
    "title": "StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling",
    "url": "https://openalex.org/W3176607063",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2105902150",
            "name": "Yikang Shen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2738935859",
            "name": "Yi Tay",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2635066196",
            "name": "Che Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2890704379",
            "name": "Dara Bahri",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2151486164",
            "name": "Donald Metzler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2328522601",
            "name": "Aaron Courville",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2991516293",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2600702321",
        "https://openalex.org/W28418033",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2129882630",
        "https://openalex.org/W2971351900",
        "https://openalex.org/W2962788148",
        "https://openalex.org/W2566475580",
        "https://openalex.org/W3004117589",
        "https://openalex.org/W3168987555",
        "https://openalex.org/W2896556401",
        "https://openalex.org/W2164119735",
        "https://openalex.org/W3099862735",
        "https://openalex.org/W2932637973",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1571710858",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2088672056",
        "https://openalex.org/W3117738520",
        "https://openalex.org/W2104806460",
        "https://openalex.org/W2164910554",
        "https://openalex.org/W2163536689",
        "https://openalex.org/W2991265431",
        "https://openalex.org/W2250520759",
        "https://openalex.org/W2962858186",
        "https://openalex.org/W2744831323",
        "https://openalex.org/W2169126000",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2995856824",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2183666181",
        "https://openalex.org/W2986267869",
        "https://openalex.org/W2081228205",
        "https://openalex.org/W2888844359",
        "https://openalex.org/W2962782699",
        "https://openalex.org/W2949399644",
        "https://openalex.org/W4289373464",
        "https://openalex.org/W3086917037",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2932376173",
        "https://openalex.org/W2963754491",
        "https://openalex.org/W2798569372",
        "https://openalex.org/W2252238553"
    ],
    "abstract": "Yikang Shen, Yi Tay, Che Zheng, Dara Bahri, Donald Metzler, Aaron Courville. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 7196–7209\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n7196\nStructFormer: Joint Unsupervised Induction of Dependency and\nConstituency Structure from Masked Language Modeling\nYikang Shen∗\nMila/Universit´e de Montr´eal\nYi Tay\nGoogle Research\nChe Zheng\nGoogle Research\nDara Bahri\nGoogle Research\nDonald Metzler\nGoogle Research\nAaron Courville\nMila/Universit´e de Montr´eal\nAbstract\nThere are two major classes of natural lan-\nguage grammars — the dependency grammar\nthat models one-to-one correspondences be-\ntween words and the constituency grammar\nthat models the assembly of one or several\ncorresponded words. While previous unsuper-\nvised parsing methods mostly focus on only in-\nducing one class of grammars, we introduce a\nnovel model, StructFormer, that can simulta-\nneously induce dependency and constituency\nstructure. To achieve this, we propose a new\nparsing framework that can jointly generate a\nconstituency tree and dependency graph. Then\nwe integrate the induced dependency relations\ninto the transformer, in a differentiable man-\nner, through a novel dependency-constrained\nself-attention mechanism. Experimental re-\nsults show that our model can achieve strong\nresults on unsupervised constituency parsing,\nunsupervised dependency parsing, and masked\nlanguage modeling at the same time.\n1 Introduction\nHuman languages have a rich latent structure. This\nstructure is multifaceted, with the two major classes\nof grammar being dependency and constituency\nstructures. There has been an exciting breath of\nrecent work targeted at learning this structure in a\ndata-driven unsupervised fashion (Klein and Man-\nning, 2002; Klein, 2005; Le and Zuidema, 2015;\nShen et al., 2018c; Kim et al., 2019a). The core\nprinciple behind recent methods that induce struc-\nture from data is simple - provide an inductive\nbias that is conducive for structure to emerge as\na byproduct of some self-supervised training, e.g.,\nlanguage modeling. To this end, a wide range of\nmodels have been proposed that are able to success-\nfully learn grammar structures (Shen et al., 2018a,c;\n∗ Corresponding author: yikang.shn@gmail.ca.\nWork done while interning at Google Reseach.\nWang et al., 2019; Kim et al., 2019b,a). However,\nmost of these works focus on inducing either con-\nstituency or dependency structures alone.\nIn this paper, we make two important techni-\ncal contributions. First, we introduce a new neu-\nral model, StructFormer, that is able to simultane-\nously induce both dependency structure and con-\nstituency structure. Speciﬁcally, our approach aims\nto unify latent structure induction of different types\nof grammar within the same framework. Second,\nStructFormer is able to induce dependency struc-\ntures from raw data in an end-to-end unsupervised\nfashion. Most existing approaches induce depen-\ndency structures from other syntactic information\nlike gold POS tags (Klein and Manning, 2004; Co-\nhen and Smith, 2009; Jiang et al., 2016). Previous\nworks, having trained from words alone, often re-\nquires additional information, like pre-trained word\nclustering (Spitkovsky et al., 2011), pre-trained\nword embedding (He et al., 2018), acoustic cues\n(Pate and Goldwater, 2013), or annotated data from\nrelated languages (Cohen et al., 2011).\nWe introduce a new inductive bias that enables\nthe Transformer models to induce a directed depen-\ndency graph in a fully unsupervised manner. To\navoid the necessity of using grammar labels during\ntraining, we use a distance-based parsing mecha-\nnism. The parsing mechanism predicts a sequence\nof Syntactic Distances T (Shen et al., 2018b) and a\nsequence of Syntactic Heights ∆ (Luo et al., 2019)\nto represent dependency graphs and constituency\ntrees at the same time. Examples of ∆ and T are\nillustrated in Figure 1a. Based on the syntactic\ndistances (T) and syntactic heights ( ∆), we pro-\nvide a new dependency-constrained self-attention\nlayer to replace the multi-head self-attention layer\nin standard transformer model. More speciﬁcally,\nthe new attention head can only attend its parent (to\navoid confusion with self-attention head, we use\n“parent” to denote “head” in dependency graph) or\n7197\n(a) An example of Syntactic Distances T (grey bars) and\nSyntactic Heights ∆ (white bars). In this example, like\nis the parent (head) of constituent (like cats) and\n(I like cats).\n(b) Two types of dependency relations. The parent distribution\nallows each token to attend on its parent. The dependent distribu-\ntion allows each token to attend on its dependents. For example\nthe parent of cats is like. Cats and I are dependents of\nlike Each attention head will receive a different weighted sum\nof these relations.\nFigure 1: An example of our parsing mechanism and dependency-constrained self-attention mechanism. The\nparsing network ﬁrst predicts the syntactic distanceT and syntactic height ∆ to represent the latent structure of the\ninput sentence I like cats. Then the parent and dependent relations are computed in a differentiable manner\nfrom T and ∆.\nits dependents in the predicted dependency struc-\nture, through a weighted sum of relations shown\nin Figure 1b. In this way, we replace the complete\ngraph in the standard transformer model with a\ndifferentiable directed dependency graph. During\nthe process of training on a downstream task (e.g.\nmasked language model), the model will gradu-\nally converge to a reasonable dependency graph\nvia gradient descent.\nIncorporating the new parsing mechanism, the\ndependency-constrained self-attention, and the\nTransformer architecture, we introduce a new\nmodel named StructFormer. The proposed model\ncan perform unsupervised dependency and con-\nstituency parsing at the same time, and can leverage\nthe parsing results to achieve strong performance\non masked language model tasks.\n2 Related Work\nPrevious works on unsupervised dependency pars-\ning are primarily based on the dependency model\nwith valence (DMV) (Klein and Manning, 2004)\nand its extension (Daum ´e III, 2009; Gillenwater\net al., 2010). To effectively learn the DMV model\nfor better parsing accuracy, a variety of inductive bi-\nases and handcrafted features, such as correlations\nbetween parameters of grammar rules involving\ndifferent part-of-speech (POS) tags, have been pro-\nposed to incorporate prior information into learning.\nThe most recent progress is the neural DMV model\n(Jiang et al., 2016), which uses a neural network\nmodel to predict the grammar rule probabilities\nbased on the distributed representation of POS tags.\nHowever, most existing unsupervised dependency\nparsing algorithms require the gold POS tags to\nge provided as inputs. These gold POS tags are la-\nbeled by humans and can be potentially difﬁcult (or\nprohibitively expensive) to obtain for large corpora.\nSpitkovsky et al. (2011) proposed to overcome this\nproblem with unsupervised word clustering that\ncan dynamically assign tags to each word consid-\nering its context. He et al. (2018) overcame the\nproblem by combining DMV model with invertible\nneural network to jointly model discrete syntactic\nstructure and continuous word representations.\nUnsupervised constituency parsing has recently\nreceived more attention. PRPN (Shen et al., 2018a)\nand ON-LSTM (Shen et al., 2018c) induce tree\nstructure by introducing an inductive bias to re-\ncurrent neural networks. PRPN proposes a pars-\ning network to compute the syntactic distance of\nall word pairs, while a reading network uses the\nsyntactic structure to attend to relevant memories.\nON-LSTM allows hidden neurons to learn long-\nterm or short-term information by a novel gating\nmechanism and activation function. In URNNG\n(Kim et al., 2019b), amortized variational infer-\nence was applied between a recurrent neural net-\nwork grammar (RNNG) (Dyer et al., 2016) decoder\nand a tree structure inference network, which en-\ncourages the decoder to generate reasonable tree\nstructures. DIORA (Drozdov et al., 2019) proposed\nusing inside-outside dynamic programming to com-\npose latent representations from all possible binary\n7198\ntrees. The representations of inside and outside\npasses from the same sentences are optimized to\nbe close to each other. The compound PCFG (Kim\net al., 2019a) achieves grammar induction by max-\nimizing the marginal likelihood of the sentences\nwhich are generated by a probabilistic context-free\ngrammar (PCFG). Tree Transformer (Wang et al.,\n2019) adds extra locality constraints to the Trans-\nformer encoder’s self-attention to encourage the\nattention heads to follow a tree structure such that\neach token can only attend on nearby neighbors in\nlower layers and gradually extend the attention ﬁeld\nto further tokens when climbing to higher layers.\nNeural L-PCFG (Zhu et al., 2020) demonstrated\nthat PCFG can beneﬁt from modeling lexical de-\npendencies. Similar to StructFormer, the Neural\nL-PCFG induces both constituents and dependen-\ncies within a single model.\nThough large scale pre-trained models have\ndominated most natural language processing tasks,\nsome recent work indicates that neural network\nmodels can see accuracy gains by leveraging syn-\ntactic information rather than ignoring it (Marcheg-\ngiani and Titov, 2017; Strubell et al., 2018).\nStrubell et al. (2018) introduces syntactically-\ninformed self-attention that force one attention\nhead to attend on the syntactic governor of the\ninput token. Omote et al. (2019) and Deguchi\net al. (2019) argue that dependency-informed self-\nattention can improve Transformer’s performance\non machine translation. Kuncoro et al. (2020)\nshows that syntactic biases help large scale pre-\ntrained models, like BERT, to achieve better lan-\nguage understanding.\n3 Syntactic Distance and Height\nIn this section, we ﬁrst reintroduce the concepts\nof syntactic distance and height, then discuss their\nrelations in the context of StructFormer.\n3.1 Syntactic Distance\nSyntactic distance is proposed in Shen et al.\n(2018b) to quantify the process of splitting sen-\ntences into smaller constituents.\nDeﬁnition 3.1. Let T be a constituency tree for\nsentence (w1,...,w n). The height of the lowest\ncommon ancestor for consecutive words xi and\nxi+1 is ˜τi. Syntactic distances T = (τ1,...,τ n−1)\nare deﬁned as a sequence of n−1 real scalars that\nshare the same rank as (˜τ1,..., ˜τn−1).\nIn other words, each syntactic distance di is as-\nsociated with a split point (i,i + 1) and specify the\nrelative order in which the sentence will be split\ninto smaller components. Thus, any sequence of\nn−1 real values can unambiguously map to an\nunlabeled binary constituency tree with nleaves\nthrough the Algorithm 1 (Shen et al., 2018b). As\nShen et al. (2018c,a); Wang et al. (2019) pointed\nout, the syntactic distance reﬂects the information\ncommunication between constituents. More con-\ncretely, a large syntactic distance τi represents that\nshort-term or local information should not be com-\nmunicated between (x≤i) and (x>i). While cooper-\nating with appropriate neural network architectures,\nwe can leverage this feature to build unsupervised\ndependency parsing models.\nAlgorithm 1 Distance to binary constituency\ntree\n1: function CONSTITUENT (w, d)\n2: if d = [] then\n3: T ⇐Leaf(w)\n4: else\n5: i ⇐arg maxi(d)\n6: childl ⇐Constituent(w≤i, d<i)\n7: childr ⇐Constituent(w>i, d>i)\n8: T ⇐Node(childl, childr)\n9: return T\nAlgorithm 2 Converting binary constituency\ntree to dependency graph\n1: function DEPENDENT (T, ∆)\n2: if T = w then\n3: D ⇐[], parent ⇐w\n4: else\n5: childl, childr ⇐T\n6: Dl, parentl ⇐Dependent(childl, ∆)\n7: Dr, parentr ⇐Dependent(childr, ∆)\n8: D ⇐Union(Dl, Dr)\n9: if ∆(parentl) > ∆(parentr) then\n10: D.add(parentl ←parentr)\n11: parent ⇐parentl\n12: else\n13: D.add(parentr ←parentl)\n14: parent ⇐parentr\n15: return D, parent\n3.2 Syntactic Height\nSyntactic height is proposed in Luo et al. (2019),\nwhere it is used to capture the distance to the root\nnode in a dependency graph. A word with high\nsyntactic height means it is close to the root node.\nIn this paper, to match the deﬁnition of syntactic\ndistance, we redeﬁne syntactic height as:\nDeﬁnition 3.2. Let D be a dependency graph for\nsentence (w1,...,w n). The height of a token wi\nin D is ˜δi. The syntactic heights of D can be any\n7199\nsequence of n real scalars ∆ = ( δ1,...,δ n) that\nshare the same rank as (˜δ1,..., ˜δn).\nAlthough the syntactic height is deﬁned based\non the dependency structure, we cannot rebuild the\noriginal dependency structure by syntactic heights\nalone, since there is no information about whether a\ntoken should be attached to the left side or the right\nside. However, given an unlabelled constituent tree,\nwe can convert it into a dependency graph with\nthe help of syntactic distance. The converting pro-\ncess is similar to the standard process of converting\nconstituency treebank to dependency treebank (Gel-\nbukh et al., 2005). Instead of using the constituent\nlabels and POS tags to identify the parent of each\nconstituent, we simply assign the token with the\nlargest syntactic height as the parent of each con-\nstituent. The conversion algorithm is described in\nAlgorithm 2. In Appendix A.1, we also propose a\njoint algorithm, that takes T and ∆ as inputs and\njointly outputs a constituency tree and dependency\ngraph.\nFigure 2: An example of T, ∆ and respective depen-\ndency graph D. Solid lines represent dependency rela-\ntions between tokens. StructFormer only allow tokens\nwith dependency relation to attend on each other.\n3.3 The relation between Syntactic Distance\nand Height\nAs discussed previously, the syntactic distance con-\ntrols information communication between the two\nsides of the split point. The syntactic height quanti-\nﬁes the centrality of each token in the dependency\ngraph. A token with large syntactic height tends\nto have more long-term dependency relations to\nconnect different parts of the sentence together. In\nStructFormer, we quantify the syntactic distance\nand height on the same scale. Given a split point\n(i,i + 1) and it’s syntactic distanceδi, only tokens\n(a) Model Architecture\n (b) Parsing Network\nFigure 3: The Architecture of StructFormer. The parser\ntakes shared word embeddings as input, outputs syntac-\ntic distances T, syntactic heights ∆, and dependency\ndistributions between tokens. The transformer layers\ntake word embeddings and dependency distributions\nas input, output contextualized embeddings for input\nwords.\nxj with τj > δi can attend across the split point\n(i,i + 1). Thus tokens with small syntactic height\nare limited to attend to nearby tokens. Figure 2\nprovides an example of T, ∆ and respective depen-\ndency graph D.\nHowever, if the left and right boundary syntac-\ntic distance of a constituent [l,r] are too large, all\nwords in [l,r] will be forced to only attend to other\nwords in [l,r]. Their contextual embedding will\nnot be able to encode the full context. To avoid this\nphenomena, we propose calibrating T according to\n∆ in Appendix A.2\n4 StructFormer\nIn this section, we present the StructFormer model.\nFigure 3a shows the architecture of StructFormer,\nwhich includes a parser network and a Transformer\nmodule. The parser network predicts T and ∆,\nthen passes them to a set of differentiable func-\ntions to generate dependency distributions. The\nTransformer module takes these distributions and\nthe sentence as input to computes a contextual em-\nbedding for each position. The StructFormer can\nbe trained in an end-to-end fashion on a Masked\nLanguage Model task. In this setting, the gradient\nback propagates through the relation distributions\ninto the parser.\n4.1 Parsing Network\nAs shown in Figure 3b, the parsing network takes\nword embeddings as input and feeds them into sev-\neral convolution layers:\nsl,i = tanh (Conv (sl−1,i−W,...,s l−1,i+W)) (1)\nwhere sl,i is the output of l-th layer at i-th position,\ns0,iis the input embedding of tokenwi, and 2W+1\nis the convolution kernel size.\n7200\nGiven the output of the convolution stack sN,i,\nwe parameterize the syntactic distance T as:\nτi =\n\n\n\nWτ\n1 tanh\n(\nWτ\n2\n[ sN,i\nsN,i+1\n])\n,\n1 ≤i≤n−1\n∞, i = 0 or i= n\n(2)\nwhere τi is the contextualized distance for the i-\nth split point between token wi and wi+1. The\nsyntactic height ∆ is parameterized in a similar\nway:\nδi = Wδ\n1 tanh\n(\nWδ\n2sN,i + bδ\n2\n)\n+ bδ\n1 (3)\n4.2 Estimate the Dependency Distribution\nGiven T and ∆, we now explain how to estimate\nthe probability p(xj|xi) such that the j-th token is\nthe parent of the i-th token. The ﬁrst step is iden-\ntifying the smallest legal constituent C(xi), that\ncontains xi and xi is not C(xi)’s parent. The sec-\nond step is identifying the parent of the constituent\nxj = Pr(C(xi)). Given the discussion in section\n3.2, the parent of C(xi) must be the parent of xi.\nThus, the two-stages of identifying the parent of xi\ncan be formulated as:\nD(xi) = Pr(C(xi)) (4)\nIn StructFormer, C(xi) is represented as con-\nstituent [l,r], where lis the starting index (l≤i) of\nC(xi) and ris the ending index (r≥i) of C(xi).\nIn a dependency graph, xi is only connected to\nits parent and dependents. This means that xi does\nnot have direct connection to the outside of C(xi).\nIn other words, C(xi) = [l,r] is the smallest con-\nstituent that satisﬁes:\nδi <τl−1, δi <τr (5)\nwhere τl−1 is the ﬁrst τ<i that is larger then δi\nwhile looking backward, and τr is the ﬁrst τ≥i that\nis larger then δi while looking forward. For ex-\nample, in Figure 2, δ4 = 3 .5, τ3 = 4 > δ4 and\nτ8 = ∞ > δ4, thus C(x4) = [4 ,8]. To make\nthis process differentiable, we deﬁne τk as a real\nvalue and δi as a probability distribution p(˜δi). For\nthe simplicity and efﬁciency of computation, we\ndirectly parameterize the cumulative distribution\nfunction p(˜δi >τk) with sigmoid function:\np(˜δi >τk) = σ((δi −τk)/µ1) (6)\nwhere σis the sigmoid function, δi is the mean of\ndistribution p(˜δi) and µ1 is a learnable temperature\nterm. Thus the probability that the l-th (l < i)\ntoken is insideC(xi) is equal to the probability that\n˜δi is larger then the maximum distance τ between\nland i:\np(l∈C(xi)) = p(˜δi >max(τi−1,...,τ l)) (7)\n= σ((δi −max(τl,...,τ i−1))/µ)\nThen we can compute the probability distribution\nfor l:\np(l|i) = p(l∈C(xi)) −p(l−1 ∈C(xi))\n= σ((δi −max(τl,...,τ i−1))/µ) −\nσ((δi −max(τl−1,...,τ i−1))/µ) (8)\nSimilarly, we can compute the probability distribu-\ntion for r:\np(r|i) = σ((δi −max(τi,...,τ r−1))/µ) −\nσ((δi −max(τi,...,τ r))/µ) (9)\nThe probability distribution for [l,r] = C(xi) can\nbe computed as:\npC([l,r]|i) =\n{p(l|i)p(r|i), l ≤i≤r\n0, otherwise(10)\nThe second step is to identify the parent of [l,r].\nFor any constituent [l,r], we choose the j =\nargmaxk∈[l,r](δk) as the parent of [l,r]. In the\nprevious example, given constituent [4,8], the\nmaximum syntactic height is δ6 = 4 .5, thus\nPr([4,8]) = x6. We use softmax function to pa-\nrameterize the probability pPr(j|[l,r]):\npPr(j|[l,r]) =\n{ exp(hj/µ2)∑\nl≤k≤r exp(hk/µ2) , l ≤t≤r\n0, otherwise\n(11)\nGiven probability p(j|[l,r]) and p([l,r]|i), we can\ncompute the probability that xj is the parent of xi:\npD(j|i) =\n{∑\n[l,r] pPr(j|[l,r])pC([l,r]|i), i ̸= j\n0, i = j\n(12)\n4.3 Dependency-Constrained Multi-head\nSelf-Attention\nThe multi-head self-attention in the transformer\ncan be seen as a information propagation mecha-\nnism on the complete graph G = (X,E), where\nthe set of vertices X contains all ntokens in the\nsentence, and the set of edges Econtains all possi-\nble word pairs (xi,xj). StructFormer replace the\n7201\ncomplete graph G with a soft dependency graph\nD = (X,A), where Ais the matrix of n×nprob-\nabilities. Aij = pD(j|i) is the probability of the\nj-th token depending on the i-th token. The reason\nthat we called it a directed edge is that each speciﬁc\nhead is only allow to propagate information either\nfrom parent to dependent or from from dependent\nto parent. To do so, structformer associate each\nattention head with a probability distribution over\nparent or dependent relation.\npparent = exp(wparent)\nexp(wparent) + exp(wdep) (13)\npdep = exp(wdep)\nexp(wparent) + exp(wdep) (14)\nwhere wparent and wdep are learnable parameters\nthat associated with each attention head, pparent is\nthe probability that this head will propagate infor-\nmation from parent to dependent, vice versa. The\nmodel will learn to assign this association from the\ndownstream task via gradient descent. Then we\ncan compute the probability that information can\nbe propagated from node jto node ivia this head:\npi,j = pparentpD(j|i) + pdeppD(i|j) (15)\nHowever, Htut et al. (2019) pointed out that differ-\nent heads tend to associate with different type of\nuniversal dependency relations (including nsubj,\nobj, advmod, etc), but there is no generalist head\ncan that work with all different relations. To ac-\ncommodate this observation, we compute a indi-\nvidual probability for each head and pair of tokens\n(xi,xj):\nqi,j = sigmoid\n(QKT\n√dk\n)\n(16)\nwhere Q and K are query and key matrix in a\nstandard transformer model and dk is the dimen-\nsion of attention head. The equation is inspired\nby the scaled dot-product attention in transformer.\nWe replace the original softmax function with a\nsigmoid function, so qi,j became an independent\nprobability that indicates whether xi should attend\non xj through the current attention head. In the\nend, we propose to replace transformer’s scaled dot-\nproduct attention with our dependency-constrained\nself-attention:\nAttention(Qi,Kj,Vj,D) = pi,jqi,jVj (17)\n5 Experiments\nWe evaluate the proposed model on three tasks:\nMasked Language Modeling, Unsupervised Con-\nstituency Parsing and Unsupervised Dependency\nParsing.\nOur implementation of StructFormer is close to\nthe original Transformer encoder (Vaswani et al.,\n2017). Except that we put the layer normalization\nin front of each layer, similar to the T5 model (Raf-\nfel et al., 2019). We found that this modiﬁcation\nallows the model to converges faster. For all exper-\niments, we set the number of layers L= 8, the em-\nbedding size and hidden size to be dmodel = 512,\nthe number of self-attention heads h= 8, the feed-\nforward size dff = 2048, dropout rate as 0.1, and\nthe number of convolution layers in the parsing\nnetwork as Lp = 3.\n5.1 Masked Language Model\nMasked Language Modeling (MLM) has been\nwidely used as a pretraining object for larger-scale\npretraining models. In BERT (Devlin et al., 2018)\nand RoBERTa (Liu et al., 2019), authors found\nthat MLM perplexities on held-out evaluation set\nhave a positive correlation with the end-task per-\nformance. We trained and evaluated our model\non 2 different datasets: the Penn TreeBank (PTB)\nand BLLIP. In our MLM experiments, each token\nhas an independent chance to be replaced by a\nmask token <mask>, except that we never replace\n< unk >token. The training and evaluation ob-\nject for Masked Language Model is to predict the\nreplaced tokens. The performance of MLM is eval-\nuated by measuring perplexity on masked words.\nPTB is a standard dataset for language model-\ning (Mikolov et al., 2012) and unsupervised con-\nstituency parsing (Shen et al., 2018c; Kim et al.,\n2019a). Following the setting proposed in Shen\net al. (2018c), we use Mikolov et al. (2012)’s\nprepossessing process, which removes all punc-\ntuations, and replaces low frequency tokens with\n<unk>. The preprocessing results in a vocabu-\nlary size of 10001 (including <unk>, <pad> and\n<mask>). For PTB, we use a 30% mask rate.\nBLLIP is a large Penn Treebank-style parsed\ncorpus of approximately 24 million sentences. We\ntrain and evaluate StructFormer on three splits of\nBLLIP: BLLIP-XS (40k sentences, 1M tokens),\nBLLIP-SM (200K sentences, 5M tokens), and\nBLLIP-MD (600K sentences, 14M tokens). They\nare obtained by randomly sampling sections from\n7202\nModel PTB BLLIP BLLIP BLLIP\n-XS -SM -MD\nTransformer 64.05 93.90 19.92 14.31\nStructFormer 60.94 57.28 18.70 13.70\nTable 1: Masked Language Model perplexities on dif-\nferent datasets.\nBLLIP 1987-89 Corpus Release 1. All models are\ntested on a shared held-out test set (20k sentences,\n500k tokens). Following the settings provided in\n(Hu et al., 2020), we use subword-level vocabulary\nextracted from the GPT-2 pre-trained model rather\nthan the BLLIP training corpora. For BLLIP, we\nuse a 15% mask rate.\nThe masked language model results are shown in\nTable 1. StructFormer consistently outperforms our\nTransformer baseline. This result aligns with previ-\nous observations that linguistically informed self-\nattention can help Transformers achieve stronger\nperformance. We also observe that StructFormer\nconverges much faster than the standard Trans-\nformer model.\n5.2 Unsupervised Constituency Parsing\nThe unsupervised constituency parsing task com-\npares the latent tree structure induced by the model\nwith those annotated by human experts. We use\nthe Algorithm 1 to predict the constituency trees\nfrom T predicted by StructFormer. Following the\nexperiment settings proposed in Shen et al. (2018c),\nwe take the model trained on PTB dataset and eval-\nuate it on WSJ test set. The WSJ test set is section\n23 of WSJ corpus, it contains 2416 human expert\nlabeled sentences. Punctuation is ignored during\nthe evaluation.\nMethods UF1\nRANDOM 21.6\nLBRANCH 9.0\nRBRANCH 39.8\nPRPN (Shen et al., 2018a) 37.4 (0.3)\nON-LSTM (Shen et al., 2018c) 47.7 (1.5)\nTree-T (Wang et al., 2019) 49.5\nURNNG (Kim et al., 2019b) 52.4\nC-PCFG (Kim et al., 2019a) 55.2\nNeural L-PCFGs (Zhu et al., 2020) 55.31\nStructFormer 54.0 (0.3)\nTable 2: Unsupervised constituency parsing tesults. *\nresults are from Kim et al. (2020). UF1 stands for Un-\nlabeled F1.\nTable 2 shows that our model achieves strong re-\nsults on unsupervised constituency parsing. While\nPRPN ON C-PCFG Tree-T Ours\nSBAR 50.0% 52.5% 56.1% 36.4% 48.7%\nNP 59.2% 64.5% 74.7% 67.6% 72.1%\nVP 46.7% 41.0% 41.7% 38.5% 43.0%\nPP 57.2% 54.4% 68.8% 52.3% 74.1%\nADJP 44.3% 38.1% 40.4% 24.7% 51.9%\nADVP 32.8% 31.6% 52.5% 55.1% 69.5%\nTable 3: Fraction of ground truth constituents that were\npredicted as a constituent by the models broken down\nby label (i.e. label recall)\nthe C-PCFG (Kim et al., 2019a) achieve a stronger\nparsing performance with its strong linguistic con-\nstraints (e.g. a ﬁnite set of production rules), Struct-\nFormer may have a border domain of application.\nFor example, it can replace the standard trans-\nformer encoder in most of the popular large-scale\npre-trained language models (e.g. BERT and Re-\nBERTa) and transformer based machine translation\nmodels. Different from the transformer-based Tree-\nT (Wang et al., 2019), we did not directly use con-\nstituents to restrict the self-attention receptive ﬁeld.\nBut StructFormer achieves a stronger constituency\nparsing performance. This result may suggest that\ndependency relations are more suitable for gram-\nmar induction in transformer-based models. Table\n3 shows that our model achieves strong accuracy\nwhile predicting Noun Phrase (NP), Preposition\nPhrase (PP), Adjective Phrase (ADJP), and Adverb\nPhrase (ADVP).\n5.3 Unsupervised Dependency Parsing\nThe unsupervised dependency parsing evaluation\ncompares the induced dependency relations with\nthose in the reference dependency graph. The most\ncommon metric is the Unlabeled Attachment Score\n(UAS), which measures the percentage that a token\nis correctly attached to its parent in the reference\ntree. Another widely used metric for unsupervised\ndependency parsing is Undirected Unlabeled At-\ntachment Score (UUAS) measures the percentage\nthat the reference undirected and unlabeled connec-\ntions are recovered by the induced tree. Similar\nto the unsupervised constituency parsing, we take\nthe model trained on PTB dataset and evaluate it\non WSJ test set (section 23). For the WSJ test set,\nreference dependency graphs are converted from\nits human-annotated constituency trees. However,\nthere are two different sets of rules for the conver-\nsion: the Stanford dependencies and the CoNLL de-\npendencies. While Stanford dependencies are used\nas reference dependencies in previous unsupervised\n7203\nRelations MLM Constituency Stanford Conll\nPPL UF1 UAS UUAS UAS UUAS\nparent+dep 60.9 (1.0) 54.0 (0.3) 46.2 (0.4) 61.6 (0.4) 36.2 (0.1) 56.3 (0.2)\nparent 63.0 (1.2) 40.2 (3.5) 32.4 (5.6) 49.1 (5.7) 30.0 (3.7) 50.0 (5.3)\ndep 63.2 (0.6) 51.8 (2.4) 15.2 (18.2) 41.6 (16.8) 20.2 (12.2) 44.7 (13.9)\nTable 4: The performance of StructFormer with different combinations of attention masks. UAS stands for Unla-\nbeled Attachment Score. UUAS stands for Undirected Unlabeled Attachment Score.\nMethods UAS\nw/o gold POS tags\nDMV (Klein and Manning, 2004) 35.8\nE-DMV (Headden III et al., 2009) 38.2\nUR-A E-DMV (Tu and Honavar, 2012) 46.1\nCS* (Spitkovsky et al., 2013) 64.4*\nNeural E-DMV (Jiang et al., 2016) 42.7\nGaussian DMV (He et al., 2018) 43.1 (1.2)\nINP (He et al., 2018) 47.9 (1.2)\nNeural L-PCFGs (Zhu et al., 2020) 40.5 (2.9)\nStructFormer 46.2 (0.4)\nw/ gold POS tags (for reference only)\nDMV (Klein and Manning, 2004) 39.7\nUR-A E-DMV (Tu and Honavar, 2012) 57.0\nMaxEnc (Le and Zuidema, 2015) 65.8\nNeural E-DMV (Jiang et al., 2016) 57.6\nCRFAE (Cai et al., 2017) 55.7\nL-NDMV†(Han et al., 2017) 63.2\nTable 5: Dependency Parsing Results on WSJ testset.\nStarred entries (*) beneﬁt from additional punctuation-\nbased constraints. Daggered entries ( †) beneﬁt from\nlarger additional training data. Baseline results are\nfrom He et al. (2018).\nparsing papers, we noticed that our model some-\ntimes output dependency structures that are closer\nto the CoNLL dependencies. Therefore, we report\nUAS and UUAS for both Stanford and CoNLL\ndependencies. Following the setting of previous\npapers (Jiang et al., 2016), we ignored the punctua-\ntion during evaluation. To obtain the dependency\nrelation from our model, we compute the argmax\nfor dependency distribution:\nk= argmaxj̸=ipD(j|i) (18)\nand assign the k-th token as the parent ofi-th token.\nTable 5 shows that our model achieves competi-\ntive dependency parsing performance while com-\nparing to other models that do not require gold\nPOS tags. While most of the baseline models still\nrely on some kind of latent POS tags or pre-trained\nword embeddings, StructFormer can be seen as an\neasy-to-use alternative that works in an end-to-end\nfashion. Table 6 shows that our model recovers\n61.6% of undirected dependency relations. Given\nthe strong performances on both dependency pars-\ning and masked language modeling, we believe\nthat the dependency graph schema could be a vi-\nable substitute for the complete graph schema used\nin the standard transformer. Appendix A.4 provides\nexamples of parent distribution.\nSince our model uses a mixture of the relation\nprobability distribution for each self-attention head,\nwe also studied how different combinations of re-\nlations affect the performance of our model. Table\n6 shows that the model can achieve the best per-\nformance while using both parent and dependent\nrelations. The model suffers more on dependency\nparsing if the parent relation is removed. And if the\ndependent relationship is removed, the model will\nsuffer more on the constituency parsing. Appendix\nA.3 shows the weight for parent and dependent\nrelations learnt from MLM tasks. It’s interesting\nto observe that Structformer tends to focus on the\nparent relations in the ﬁrst layer, and start to use\nboth relations from the second layer.\n6 Conclusion\nIn this paper, we introduce a novel dependency and\nconstituency joint parsing framework. Based on\nthe framework, we propose StructFormer, a new\nunsupervised parsing algorithm that does unsuper-\nvised dependency and constituency parsing at the\nsame time. We also introduced a novel dependency-\nconstrained self-attention mechanism that allows\neach attention head to focus on a speciﬁc mixture\nof dependency relations. This brings Transformers\ncloser to modeling a directed dependency graph.\nThe experiments show promising results that Struct-\nFormer can induce meaningful dependency and\nconstituency structures and achieve better perfor-\nmance on masked language model tasks. This re-\nsearch provides a new path to build more linguistic\nbias into a pre-trained language model.\n7204\nReferences\nJiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf\nautoencoder for unsupervised dependency parsing.\narXiv preprint arXiv:1708.01018.\nShay B Cohen, Dipanjan Das, and Noah A Smith. 2011.\nUnsupervised structure prediction with non-parallel\nmultilingual guidance. In Proceedings of the 2011\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 50–61.\nShay B Cohen and Noah A Smith. 2009. Shared logis-\ntic normal distributions for soft parameter tying in\nunsupervised grammar induction. In Proceedings of\nHuman Language Technologies: The 2009 Annual\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics, pages 74–\n82.\nHal Daum ´e III. 2009. Unsupervised search-based\nstructured prediction. In Proceedings of the 26th\nAnnual International Conference on Machine Learn-\ning, pages 209–216.\nHiroyuki Deguchi, Akihiro Tamura, and Takashi Ni-\nnomiya. 2019. Dependency-based self-attention for\ntransformer nmt. In Proceedings of the Interna-\ntional Conference on Recent Advances in Natural\nLanguage Processing (RANLP 2019), pages 239–\n246.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive auto-encoders. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1129–1141.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of NAACL-HLT, pages\n199–209.\nAlexander Gelbukh, Sulema Torres, and Hiram Calvo.\n2005. Transforming a constituency treebank into a\ndependency treebank. Procesamiento del lenguaje\nnatural, (35):145–152.\nJennifer Gillenwater, Kuzman Ganchev, Jo ˜ao Grac ¸a,\nFernando Pereira, and Ben Taskar. 2010. Sparsity\nin dependency grammar induction. ACL 2010, page\n194.\nWenjuan Han, Yong Jiang, and Kewei Tu. 2017.\nDependency grammar induction with neural lexi-\ncalization and big training data. arXiv preprint\narXiv:1708.00801.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2018. Unsupervised learning of syntac-\ntic structure with invertible neural projections. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n1292–1302.\nWilliam P Headden III, Mark Johnson, and David Mc-\nClosky. 2009. Improving unsupervised dependency\nparsing with richer contexts and smoothing. In Pro-\nceedings of human language technologies: the 2009\nannual conference of the North American chapter of\nthe association for computational linguistics, pages\n101–109.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R Bowman. 2019. Do attention heads in\nbert track syntactic dependencies? arXiv preprint\narXiv:1911.12246.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger P Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. arXiv preprint arXiv:2005.03692.\nYong Jiang, Wenjuan Han, Kewei Tu, et al. 2016. Un-\nsupervised neural dependency parsing. Association\nfor Computational Linguistics (ACL).\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-\ngoo Lee. 2020. Are pre-trained language mod-\nels aware of phrases? simple but strong base-\nlines for grammar induction. arXiv preprint\narXiv:2002.00737.\nYoon Kim, Chris Dyer, and Alexander M Rush. 2019a.\nCompound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2369–2385.\nYoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G´abor Melis. 2019b. Unsuper-\nvised recurrent neural network grammars. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1105–1117.\nDan Klein. 2005. The unsupervised learning of natural\nlanguage structure. Stanford University Stanford.\nDan Klein and Christopher D Manning. 2002. A gener-\native constituent-context model for improved gram-\nmar induction. In Proceedings of the 40th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 128–135.\nDan Klein and Christopher D Manning. 2004. Corpus-\nbased induction of syntactic structure: Models of de-\npendency and constituency. In Proceedings of the\n42nd annual meeting of the association for computa-\ntional linguistics (ACL-04), pages 478–485.\n7205\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried,\nDani Yogatama, Laura Rimell, Chris Dyer, and Phil\nBlunsom. 2020. Syntactic structure distillation pre-\ntraining for bidirectional encoders. arXiv preprint\narXiv:2005.13482.\nPhong Le and Willem Zuidema. 2015. Unsupervised\ndependency parsing: Let’s use supervised parsers.\narXiv preprint arXiv:1504.04666.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHongyin Luo, Lan Jiang, Yonatan Belinkov, and James\nGlass. 2019. Improving neural language models by\nsegmenting, attending, and predicting the future. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1483–\n1493.\nDiego Marcheggiani and Ivan Titov. 2017. En-\ncoding sentences with graph convolutional net-\nworks for semantic role labeling. arXiv preprint\narXiv:1703.04826.\nTom´aˇs Mikolov et al. 2012. Statistical language mod-\nels based on neural networks. Presentation at\nGoogle, Mountain View, 2nd April, 80:26.\nYutaro Omote, Akihiro Tamura, and Takashi Ninomiya.\n2019. Dependency-based relative positional encod-\ning for transformer nmt. In Proceedings of the In-\nternational Conference on Recent Advances in Natu-\nral Language Processing (RANLP 2019), pages 854–\n861.\nJohn K Pate and Sharon Goldwater. 2013. Unsuper-\nvised dependency parsing with acoustic cues. Trans-\nactions of the Association for Computational Lin-\nguistics, 1:63–74.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nYikang Shen, Zhouhan Lin, Chin-wei Huang, and\nAaron Courville. 2018a. Neural language modeling\nby jointly learning syntax and lexicon. In Interna-\ntional Conference on Learning Representations.\nYikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessan-\ndro Sordoni, Aaron Courville, and Yoshua Bengio.\n2018b. Straight to the tree: Constituency parsing\nwith neural syntactic distance. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1171–1180.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018c. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nInternational Conference on Learning Representa-\ntions.\nValentin I Spitkovsky, Hiyan Alshawi, Angel Chang,\nand Dan Jurafsky. 2011. Unsupervised dependency\nparsing without gold part-of-speech tags. In Pro-\nceedings of the 2011 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1281–\n1290.\nValentin I Spitkovsky, Daniel Jurafsky, and Hiyan Al-\nshawi. 2013. Breaking out of local optima with\ncount transforms and model recombination: A study\nin grammar induction.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. arXiv preprint arXiv:1804.08199.\nKewei Tu and Vasant Honavar. 2012. Unambiguity reg-\nularization for unsupervised learning of probabilis-\ntic grammars. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 1324–1334.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYaushian Wang, Hung-Yi Lee, and Yun-Nung Chen.\n2019. Tree transformer: Integrating tree structures\ninto self-attention. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1060–1070.\nHao Zhu, Yonatan Bisk, and Graham Neubig. 2020.\nThe return of lexical dependencies: Neural lexical-\nized pcfgs. Transactions of the Association for Com-\nputational Linguistics, 8:647–661.\n7206\nA Appendix\nA.1 Joint Dependency and Constituency\nParsing\nAlgorithm 3 The joint dependency and con-\nstituency parsing algorithm. Inputs are a sequence\nof words w, syntactic distances d, syntactic heights\nh. Outputs are a binary constituency tree T, a de-\npendency graph D that is represented as a set of\ndependency relations, the parent of dependency\ngraph D, and the syntactic height of parent.\n1: function BUILD TREE (w, d, h)\n2: if d = [] and w = [w] and h = [h] then\n3: T ⇐Leaf(w), D ⇐[], parent ⇐w, height\n⇐h\n4: else\n5: i ⇐arg max(d)\n6: Tl, Dl, parentl, heightl ⇐\nBuildTree(d<i, w≤i, h≤i)\n7: Tr, Dr, parentr, heightr ⇐\nBuildTree(d>i, w>i, h>i)\n8: T ⇐Node(childl ⇐Tl, childr ⇐Tr)\n9: D ⇐Union(Dl, Dr)\n10: if heightl > heightr then\n11: D.add(parentl ←parentr)\n12: parent ⇐parentl, height ⇐heightl\n13: else\n14: D.add(parentr ←parentl)\n15: parent ⇐parentr, height ⇐heightr\n16: return T, D, parent, height\nA.2 Calibrating the Syntactic Distance and\nHeight\nIn Section 3.3, we explained the relation between\n∆ and T, that if δi < τj, the i-th word won’t be\nable to attend beyond the j-th split point. How-\never, in a speciﬁc case, the constraint will isolate\na constituent [l,r] from the rest of the sentence.\nIf τl−1 and τr are larger then all height δl,...,r in\nthe constituent, then all words in [l,r] won’t be\nable to attend on the outside of the constituent.\nThis phenomenon will prevent their output contex-\ntual embedding from encoding the full context. To\navoid this phenomenon, we propose to calibrate\nthe syntactic distance T according to the syntactic\nheight ∆. First, we compute the maximum syntac-\ntic height for each constituent:\nδ[l,r] = max (δl,...,δ r) , l<r (19)\nThen we compute the minimum difference be-\ntween δ[l,r] and [l,r]’s left and right boundary dis-\ntance. Since we only care about constituents that\nthe boundary distance is larger than its maximum\nheight, we use a ReLU activation function to keep\nonly the positive values:\nϵ[l,r] = ReLU\n(\nmin\n(\nτl−1 −δ[l,r],τr −δ[l,r]\n))\n(20)\nTo make sure all constituent are not isolated and\nmaintain the rank of T, we subtract all T by the\nmaximum of ϵ:\nˆδi = δi − max\n{[l,r]}/[1,n]\n(\nϵ[l,r]\n)\n(21)\n7207\nA.3 Dependency Relation Weights for Self-attention Heads\n(a) Dependency relation weights learnt on PTB\n(b) Dependency relation weights learnt on BLLIP-SM\nFigure 4: Dependency relation weights learnt on different datasets. Row i constains relation weights for all at-\ntention heads in the i-th transformer layer. p represents the parent relation. d represents the dependent relation.\nWe observe a clearer preference for each attention head in the model trained on BLLIP-SM. This probably due to\nBLLIP-SM has signﬁcantly more training data. It’s also interesting to notice that the ﬁrst layer tend to focus on\nparent relations.\n7208\nA.4 Dependency Distribution Examples\n(a)\n(b)\nFigure 5: Dependency distribution examples from WSJ test set. Each row is the parent distribution for the respec-\ntive word. The sum of each distribution may not equal to 1. Against our intuition, the distribution is not very\nsharp. This is partially due to the ambiguous nature of the dependency graph. As we previously discussed, at least\ntwo styles of dependency rules (Conll and Stanford) exist. And without extra constraint or supervision, the model\nseems trying to model both of them at the same time. One interesting future work will be ﬁnding an inductive bias\nthat can encourage the model to converge to a speciﬁc style of dependency graph.\n7209\nA.5 The Performance of StructFormer with different mask rates\nMask rate MLM Constituency Stanford Conll\nPPL UF1 UAS UUAS UAS UUAS\n0.1 45.3 (1.2) 51.45 (2.7) 31.4 (11.9) 51.2 (8.1) 32.3 (5.2) 52.4 (4.5)\n0.2 50.4 (1.3) 54.0 (0.6) 37.4 (12.6) 55.6 (8.8) 33.0 (5.7) 53.5 (4.7)\n0.3 60.9 (1.0) 54.0 (0.3) 46.2 (0.4) 61.6 (0.4) 36.2 (0.1) 56.3 (0.2)\n0.4 76.9 (1.2) 53.5 (1.5) 34.0 (10.3) 52.0 (7.4) 29.5 (5.4) 50.6 (4.1)\n0.5 100.3 (1.4) 53.2 (0.9) 36.3 (9.8) 53.6 (6.8) 30.6 (4.2) 51.3 (3.2)\nTable 6: The performance of StructFormer on PTB dataset with different mask rates. Dependency parsing is\nespecially affected by the masks. Mask rate 0.3 provides the best and the most stable performance."
}