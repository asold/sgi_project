{
  "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference",
  "url": "https://openalex.org/W4407425079",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2421265022",
      "name": "Gu Yufeng",
      "affiliations": [
        "University of Michiganâ€“Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2300526254",
      "name": "Khadem Alireza",
      "affiliations": [
        "University of Michiganâ€“Ann Arbor"
      ]
    },
    {
      "id": null,
      "name": "Umesh, Sumanth",
      "affiliations": [
        "University of Michiganâ€“Ann Arbor"
      ]
    },
    {
      "id": "https://openalex.org/A2067774666",
      "name": "Liang Ning",
      "affiliations": [
        "University of Michiganâ€“Ann Arbor"
      ]
    },
    {
      "id": null,
      "name": "Servot, Xavier",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2748121668",
      "name": "Mutlu, Onur",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2977428485",
      "name": "Iyer, Ravi",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2749365187",
      "name": "Das Reetuparna",
      "affiliations": [
        "University of Michiganâ€“Ann Arbor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4282555269",
    "https://openalex.org/W4377865306",
    "https://openalex.org/W4320067983"
  ],
  "abstract": "Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks. We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\\times$ higher throughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
  "full_text": "PIM Is All You Need: A CXL-Enabled GPU-Free System\nfor Large Language Model Inference\nYufeng Guâˆ—\nUniversity of Michigan\nAnn Arbor, USA\nyufenggu@umich.edu\nAlireza Khademâˆ—\nUniversity of Michigan\nAnn Arbor, USA\narkhadem@umich.edu\nSumanth Umesh\nUniversity of Michigan\nAnn Arbor, USA\nsumanthu@umich.edu\nNing Liang\nUniversity of Michigan\nAnn Arbor, USA\nnliang@umich.edu\nXavier Servot\nETH ZÃ¼rich\nZÃ¼rich, Switzerland\nxservot@student.ethz.ch\nOnur Mutlu\nETH ZÃ¼rich\nZÃ¼rich, Switzerland\nomutlu@gmail.com\nRavi Iyerâ€ \nGoogle\nMountain View, USA\nraviiyer20@gmail.com\nReetuparna Das\nUniversity of Michigan\nAnn Arbor, USA\nreetudas@umich.edu\nAbstract\nLarge Language Model (LLM) inference uses an autoregres-\nsive manner to generate one token at a time, which exhibits\nnotably lower operational intensity compared to earlier Ma-\nchine Learning (ML) models such as encoder-only trans-\nformers and Convolutional Neural Networks. At the same\ntime, LLMs possess large parameter sizes and use key-value\ncaches to store context information. Modern LLMs support\ncontext windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value\ncache unique to each prompt requires a large memory capac-\nity, limiting the inference batch size. Both low operational\nintensity and limited batch size necessitate a high memory\nbandwidth. However, contemporary hardware systems for\nML model deployment, such as GPUs and TPUs, are pri-\nmarily optimized for compute throughput. This mismatch\nchallenges the efficient deployment of advanced LLMs and\nmakes users pay for expensive compute resources that are\npoorly utilized for the memory-bound LLM inference tasks.\nWe propose CENT, a CXL-ENabled GPU-Free sysTem for\nLLM inference, which harnesses CXL memory expansion\nâˆ—Yufeng Gu and Alireza Khadem contributed equally to this research\nâ€ This research was done while the author was at Intel Corporation\nThis work is licensed under a Creative Commons Attribution-\nNonCommercial-ShareAlike 4.0 International License.\nASPLOS â€™25, Rotterdam, Netherlands\nÂ© 2025 This is the authorâ€™s version of the work. It is posted here by permis-\nsion of ACM for your personal use. Not for redistribution. The definitive\nversion was published in Proceedings of the 30th ACM International Confer-\nence on Architectural Support for Programming Languages and Operating\nSystems, Volume 2 (ASPLOS â€™25)\nACM ISBN 979-8-4007-1079-7/25/03\nhttps://doi.org/10.1145/3676641.3716267\ncapabilities to accommodate substantial LLM sizes, and uti-\nlizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT\nexploits a scalable CXL network to support peer-to-peer\nand collective communication primitives across CXL devices.\nWe implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with\nmaximum supported batch sizes and similar average power,\nCENT achieves 2.3Ã—higher throughput and consumes 2.9Ã—\nless energy. CENT enhances the Total Cost of Ownership\n(TCO), generating 5.2Ã—more tokens per dollar than GPUs.\nCCS Concepts:â€¢ Computer systems organization â†’Par-\nallel architectures; Neural networks.\nKeywords: Computer Architecture, Processing-In-Memory,\nCompute Express Link, Generative Artificial Intelligence,\nLarge Language Models.\n1 Introduction\nGenerative Artificial Intelligence (GenAI) has become piv-\notal in transforming a myriad of sectors. In the realm of con-\ntent creation, Large Language Models (LLMs) [4, 29, 84, 106]\nprovide assistance in writing, summarizing, and translating\nacross diverse languages, revolutionizing the way textual\ncontent is produced. LLMs are reshaping various fields in\ndaily life, such as generating creative arts [7, 83], customer\nservices through chatbots, generating code and debugging\nassistance in software development [47]. However, harness-\ning the power of LLMs presents substantial economic chal-\nlenges, underlined by their significant resource requirements.\nA business cost model indicates that running ChatGPT infer-\nence tasks requires âˆ¼3617 HGX A100 [75] servers and costs\nâˆ¼$694,444 per day [19]. Therefore, efficient and cost-effective\narXiv:2502.07578v3  [cs.AR]  3 May 2025\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nserver farms play a critical role in the broader adoption and\npractical application of LLMs.\nDecoder-only LLMs have witnessed exponentially larger\nparameter sizes. At the same time, LLMs use key-value (KV)\ncaches to store context information, and modern LLMs sup-\nport context windows from 128K to 1M to generate versatile\ntexts, audios, and videos [29, 81]. Both the model parameters\nand KV caches require a large memory capacity. To meet\nthis demand, advanced GPU stations feature multiple GPUs.\nHowever, the computational resources of multi-GPU systems\nare often underutilized in LLM inference tasks. Unlike earlier\nML models, LLMs exhibit lower operational intensity char-\nacteristics, necessitating high memory bandwidth, primarily\ndue to the sequential token generation and the lack of in-\nherent parameter reuse. Although batching strategies could\nmitigate this issue, KV caches specific to each user require\nlarge memory capacity, limiting the feasibility of high batch\nsizes. Hence, the expensive compute throughput of GPUs\nand custom ML accelerators is significantly under-utilized\nfor LLM inference because of the limited external memory\nbandwidth. As a result, users pay for expensive computing\nresources for memory-bound LLM inference tasks.\nThe high cost and low compute utilization of GPU systems\nmotivate an alternative solution for LLM inference tasks.\nProcessing-In-Memory (PIM) architectures [ 13, 14, 26, 40,\n41, 50, 52, 55â€“58, 60, 68, 96, 118] place processing units (PU)\nadjacent to DRAM banks within memory chips, facilitating a\nsignificantly higher internal bandwidth. However, near-bank\nPUs, fabricated in the DRAM process, impose a high area\noverhead that reduces the memory density. A lower memory\ndensity is especially detrimental to LLMs with large memory\nrequirements. On the other hand, Processing-Near-Memory\n(PNM) architectures [24, 27, 28, 46, 48, 51, 65, 72, 80, 87, 88]\nemploy compute units near memory chips, e.g., in memory\ncontrollers. PNM units are manufactured using CMOS pro-\ncess, offering more area-efficient compute capability at the\ncost of lower memory bandwidth compared to PIM.\nTo address these challenges, CENT exploits Compute eX-\npress Link (CXL) [63] based memory expansion to provide\nthe requisite memory capacity for LLMs. CENT establishes\na practical CXL network to interconnect CXL devices. Each\nCXL device consists of 16 memory chips, with each chip\ncontaining two GDDR6-PIM channels, and compute units\nnear these memory chips (PNM). This hierarchical PIM-PNM\ndesign supports the entire transformer block computation,\neliminating the need for expensive GPUs.\nCENT uses a CXL switch to connect multiple CXL de-\nvices, that are driven by a host CPU. The inter-device com-\nmunication is enabled by CXL transactions [63]. The intra-\ndevice communication between PIM chips and PNM units\nis supported through a Shared Buffer. Using these proto-\ncols, CENT provides peer-to-peer and collective communica-\ntion primitives such as send/receive, broadcast, multicast and\ngather. These primitives enable various parallelism strate-\ngies, efficiently distributing LLMs across CXL devices. In\nPipeline Parallel (PP) [38] mapping, we assign each trans-\nformer block to multiple memory channels within a single\nCXL device, facilitating the concurrent processing of mul-\ntiple prompts on different pipeline stages. PP prioritizes in-\nference throughput to accommodate a large user base. In\nTensor Parallel (TP) [100, 115] mapping, we distribute a trans-\nformer block across all CXL devices. TP focuses on reducing\nlatency for real-time applications, providing smooth user\nexperiences [25]. We also explore hybrid TP-PP mappings\nto strike a balance between the latency and throughput.\nWithin a CXL device, we introduce the detailed mapping\nof a transformer block onto the hierachical PIM-PNM archi-\ntecture. In PIM chips, near-bank PUs incorporate Multiply-\nAccumulate (MAC) units, which support more than 99% of\nthe arithmetic operations within a transformer block. The\nPNM units are composed of accelerators and RISC-V cores\nto perform other special and complex operations, such as\nSoftmax, square root, and division. The integration of RISC-V\ncores allows for the flexible support of a wide range of LLMs.\nIn summary, this paper makes the following contributions:\nâ€¢We propose CENT, a GPU-free system that uses CXL\nmemory expansion to accommodate the considerable\nmemory capacity requirements of LLMs. We design\na hierarchical PIM-PNM architecture to support the\nentire transformer block computation, eliminating the\nneed for expensive GPUs.\nâ€¢We introduce a scalable CXL network to support collec-\ntive and peer-to-peer communication primitives. We\ndescribe the mapping of LLM parallelization strategies\nacross CXL devices based on the CXL communication\nprimitives.\nâ€¢We evaluate CENT on Llama2 [106] models. Compared\nto state-of-the-art GPUs with maximum supported\nbatch sizes and similar average power, CENT achieves\n2.3Ã—higher throughput and consumes 2.9Ã—less energy.\nCENT exhibits a lower Total Cost of Ownership (TCO),\ngenerating 5.2Ã—more tokens per dollar than GPUs1.\nCENT is evaluated on Llama2 70B with a context length\nof up to 32K, but it can show higher benefits for larger model\nsizes and extended context lengths. As model sizes scale up,\nsuch as Grok 314B [111], Llama3 405B [18], and DeepSeek-\nV3 671B [64], inference serving demands significantly more\nhardware resources. In such cases, CENT offers greater cost-\nefficiency compared to GPUs.\nIn reasoning tasks [33, 82] and video generation [29, 83,\n91], where context length can range from tens of thousands\nto 1 million tokens, CENT achieves higher throughput speedup\ndue to its high memory bandwidth, which enhances memory-\nbound attention computations. Notably, GPUs can still ben-\nefit from long-text and video understanding tasks, as the\n1Open-source CENT simulator https://github.com/Yufeng98/CENT/\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nprefill stage exhibits high operational intensity. In these sce-\nnarios, prefill and decoding processes can be disaggregated\nbetween GPUs and CENT, respectively [90, 116].\n2 Motivation\nThe exponential growth of LLM parameters requires multi-\nGPU systems to accommodate the requisite memory capacity.\nHowever, LLMs exhibit limited operational intensity, mak-\ning them memory-bound and resulting in suboptimal GPU\nutilization. Consequently, LLM service providers are paying\nsignificant costs for substantial computational throughput\nof multiple GPUs, which remains largely under-utilized.\nHigh Memory Capacity Requirement. LLM parameter\nsize has witnessed an exponential increase from Billion to\nTrillion magnitudes, far surpassing previous Machine Learn-\ning (ML) models. In addition, the context windows that mod-\nern LLMs support range from 128K to 1M [29, 81], enabling\nthem to understand and generate longer contents. The long\ncontext window results in large KV caches, requiring sub-\nstantial memory capacity. These KV caches are unique to\neach user, further limiting the ability to scale up the inference\nbatch size due to the memory capacity requirement.\nLow Operational Intensity. LLM inference has two\nstages: (a) The prefill stage concurrently encodes input to-\nkens within a prompt using matrix-matrix multiply (GEMM)\noperations. (b) The decoding stage decodes output tokens\nsequentially with matrix-vector multiply (GEMV) operations.\nThe operational intensity of GEMV is substantially lower\nthan GEMM. To mitigate this, several techniques are ap-\nplied. Batching strategies combine GEMV operations across\nmultiple queries of a batch into GEMM operations. This\ntechnique improves the operational intensity non-linearly\nbecause attention calculations rely on unique KV caches of\neach prompt. Grouped-query attention [3] merges multiple\nGEMV operations into narrow GEMM, but its operational\nintensity still remains less than the GPU capabilities.\nGPU Performance Characterization. We use vLLM [54],\nthe state-of-the-art inference serving framework, to study\nthe effect of batch size and context length on 4 Nvidia A100\n80GB GPUs running the Llama2-70B model [12, 106]. Figure 1\nshows that inference throughput improves with larger batch\nsizes but reaches a plateau once the memory requirement\nexceeds the GPU memory size. As context length increases,\ninference throughput saturates with even smaller batch sizes,\nfrom batch=128 at 4K context length to batch=8 at 32K con-\ntext length. Moreover, Figure 2(a) shows that LLM inference\nquery latency increases with larger batch sizes and longer\ncontexts, violating a realistic query latency Service Level\nAgreements (SLA) constraint [6].\n0\n320\n640\n0\n200\n400\n600\n800\n1000\n32\n64\n128\n256\n16\n32\n64\n128\n8\n16\n32\n64\n4\n8\n16\n32\n4K Contexts 8K Contexts 16K Contexts 32K Contexts\nMemory Requirement (GB)\nThroughput (Tokens/s)\nBatch Size\nMemory Requirement (GB) Throughput (Token/s)Throughput (Tokens/s)\n320GB \nMemory \n4x A100 \n80GB\nFigure 1. Llama2-70B [12, 106] inference throughput and\nmemory requirement on 4 A100 80GB GPUs.\nFigure 2(b) compares the GPU compute utilization of an\nLLM (Llama2-70B [ 12, 106]) with an encoder-only trans-\nformer model (BERT [15]) and a Convolutional Neural Net-\nwork (ResNet-152 [35]). BERT and ResNet-152 models pre-\ndominantly consist of GEMM operations with high opera-\ntional intensity, effectively utilizing GPU compute through-\nput. Conversely, LLama2-70B exhibits limited operational\nintensity, resulting in a mere 21% utilization of the available\nGPU compute throughput. Finally, decoding an output to-\nken in the decoding stage takes 3.4Ã—longer than encoding a\nprompt token in the prefill stage due to the significant lower\noperational intensity of GEMV operations.\n214380\n0255075100\nLlama270BBertResNet152\nGPU Utilization (%)Batch Size\nQuery Latency (min)\n(b) GPU Utilization(a) Llama2-70B Query Latency\nbatch128batch317\nbatch 8191\nSLA Violation\nFigure 2. (a) Llama2-70B inference query latency increases\nwith larger batches on 4 A100 80GB GPUs, Prompt size=512,\nDecoding size=3584. (b) GPU compute utilization, measured\nby Nvidia Nsight Compute profiler on 4 GPUs for Llama2-\n70B and 1 GPU for the other two models.\nPIM Provides Higher Memory Bandwidth . Table 1\ncompares various manufactured industrial PIM prototypes\nand GPU. PIM enables the compute units to utilize the in-\nternal memory bandwidth, which significantly exceeds the\nexternal memory bandwidth of high-end GPUs with high\nbandwidth memories (HBM). For example, GDDR6-based\nAiM [56, 60] achieves 16 ğ‘‡ğµ/ğ‘  internal memory bandwidth\ncompared to 2 ğ‘‡ğµ/ğ‘  external bandwidth of an A100 GPU\nwith five HBM2E memory stacks. This large internal band-\nwidth coupled with a lower operational intensity makes PIM\narchitectures a suitable alternative for expensive GPUs to\nperform LLM inference tasks.\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nTable 1. Hardware System Comparison\nType PIM GPU\nName UPMEM AiM FIMDRAM A100\nMem. Units 8 DIMMs 32 channels 5 stacks 5 stacks\nEx. BW (TB/s) 0.15 1 1.5 2\nIn. BW (TB/s) 1 16 12.3 -\nCapacity (GB) 64 16 30 80\nTFLOPS 0.5 TOPS2 16 6.2 312\nOps/Byte 0.5 1 0.5 156\nMem. Density 25%-50% 75% 75% -\nLow Memory Density of PIM. PIM suffers from a lower\nmemory density due to the near-bank processing units that\nare fabricated in the DRAM process. For instance, DDR4-\nbased UPMEM R-DIMM and GDDR6-based AiM reduce the\nmemory capacity to 25%âˆ’50% and 75% compared to con-\nventional DDR4 R-DIMMs and GDDR6 channels, respec-\ntively [13, 56]. An HBM2-based FIMDRAM cube consists of\n4 PIM-enabled DRAM dies with 50% memory density and\n4 conventional dies, lowering the memory capacity by 25%\non average [57]. Given the lower memory density of PIM\ntechnologies and the substantial memory demands of LLMs,\nleveraging PIM as a scalable solution for LLMs presents sig-\nnificant challenges.\nScalable Network of PIM. Scaling the memory capac-\nity of PIM-enabled memories requires a scalable intercon-\nnect, efficient collective communication primitives, and par-\nallelization strategies to optimally map LLMs to PIM devices.\nWe utilize CXL 3.0 [63] as a low-latency interconnect pro-\ntocol, built on top of the PCIe physical layer. CXL 3.0 sup-\nports inter-device communication through a CXL switch.\nCompared to network-based RDMA, CXL.mem offers âˆ¼8Ã—\nlower latency [31]. The CXL 3.0 protocol can support up to\n4,096 nodes, exhibiting better scalability than NVLink [76].\nNVLink provides higher bandwidth (at a higher cost), which\nis critical for LLM training. However, we show that the lower\nbandwidth of CXL is not a bottleneck for LLM inference due\nto the limited volume of data transfers in various paralleliza-\ntion strategies.\nTo distribute the LLMs, we detail the mapping of the trans-\nformer blocks to the CXL devices based on the Pipeline Par-\nallel (PP) [100, 115] and Tensor Parallel (TP) [38] strategies.\nFor PP, we provide peer-to-peer send and receive primitives\nfor the transmission of the embedding vector between the\npipeline stages across CXL devices. For TP, we implement\ngather and broadcast collective communication primitives\nto transfer partial results. To balance the throughput and\nlatency of the network, we study the hybrid TP-PP paral-\nlelization strategy using the multicast primitive.\nHierarchical PIM-PNM Architecture. In addition to\nGEMV, a transformer block contains different layers, such\nRMSNorm [113], Rotary Embedding [ 102], and SiLU [ 22].\nFor the end-to-end execution of transformer blocks as an\n2UPMEM supports only integer precision, so unit is TOPs.\nâ€¦Query Key Value\nÃ—\nÃ—\nFFN Input\nW1W3\nÃ—\nSiLU\nOutput\nScore\nInput\nâ¨\nâ¨\nInput Emb.\nNext Token\nPrevious Token\nÃ— N\nOutput \nEmb.\nSoftMax\nTop-k \nSampling\nSelf \nAttention\nFeed \nForward\nAdd & \nNorm\nAdd & \nNorm\nPrompt: \nLLM is a\n(a) LLM Example (c) Llama2 Block Architecture(b) Decoder\nRotary Embedding\nOutWo Ã—\n...\nuseful\nDecoder\nâ€¦\ntool\nDecoder\nÃ— N\nPrefill StageDecoding Stage\nÃ—\nWq\nÃ—\nRMSNorm\nWv\nÃ—\nWk\nâ€¦â€¦\nKey\nCache\nValue\nCache\nâ€¦=\n=\nRMSNorm\nÃ—\nSoftmax\n=\n===\nW2 Ã—\n=\nELAddâ¨\nELMul\nDecoder\nâ€¦\nDecoder\nÃ— N\nDecoder\nâ€¦\nDecoder\nÃ— N\nFigure 3. (a) Prefill stage encodes prompt tokens in parallel.\nDecoding stage generates output tokens sequentially. (b)\nLLM contains NÃ—decoder transformer blocks. (c) Llama2\nmodel architecture.\nalternative to costly GPUs, there are two options: (a) Per-\nform all operations near-bank using a general-purpose PU\nsimilar to UPMEM [13] architecture. (b) Perform MAC oper-\nations of GEMVs in domain-specific near-bank PUs similar\nto AiM [56], and assign other operations to the PNM units,\nshared by multiple PIM chips. We use the second approach\nand propose a hierarchical PIM-PNM solution because of\ntwo primary reasons: First, a general-purpose near-bank\nPU incurs more overhead on memory density and yields\nlower compute throughput compared to domain-specific al-\nternatives. Second, MAC operations constitute over 99% of\narithmetic operations within a transformer block, rendering\ngeneral-purpose near-bank PUs over-provisioned for other\ninfrequent arithmetic operations.\n3 Background\nFigure 3(a) shows that a decoder-only LLM initially processes\na user prompt in the â€œprefillâ€ stage and subsequently gener-\nates tokens sequentially during the â€œdecodingâ€ stage. Both\nstages contain an input embedding layer, multiple decoder\ntransformer blocks, an output embedding layer, and a sam-\npling layer. Figure 3(b) demonstrates that the decoder trans-\nformer blocks consist of a self attention and a feed-forward\nnetwork (FFN) layer, each paired with residual connection\nand normalization layers.\nFigure 3(c) demonstrates the Llama2 [ 106] model archi-\ntecture as a representative LLM. In the self-attention layer,\nquery, key and value vectors are generated by multiplying\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\ninput vector to corresponding weight matrices. These matri-\nces are segmented into multiple heads, representing different\nsemantic dimensions. The query and key vectors go though\nRotary Positional Embedding (RoPE) to encode the relative\npositional information [102]. Within each head, the gener-\nated key and value vectors are appended to their caches. The\nquery vector is multiplied by the key cache to produce a\nscore vector. After the Softmax operation, the score vector\nis multiplied by the value cache to yield the output vector.\nThe output vectors from all heads are concatenated and mul-\ntiplied by output weight matrix, resulting in a vector that\nundergoes residual connection and Root Mean Square layer\nNormalization (RMSNorm) [113]. The residual connection\nadds up the input and output vectors of a layer to avoid van-\nishing gradient [35]. The FFN layer begins with two parallel\nfully connections, followed by a Sigmoid Linear Unit (SiLU),\nand ends with another fully connection.\n4 CENT Architecture\nFigure 4 presents the CENT architecture, where a CXL switch\ninterconnects 32 CXL devices, driven by a host CPU. Each\nCXL device integrates a CXL controller, PNM units, and 16\nmemory chips, each equipped with two GDDR6-PIM chan-\nnels (hereafter referred to as PIM channels). We introduce a\nCXL-based network architecture, a hierarchical PIM-PNM\ndesign and the CENT ISA in this section.\nCENTHost CPUCXL Switch\nCXL Device â€¦CXL Device CXL Device \nCXL Device CXL ControllerPNM UnitsGDDR6-PIM32x\nFigure 4. CENT Architecture.\n4.1 CXL-based Network Architecture\nCENT integrates the CXL 3.0 protocol, using the PCIe 6.0\nphysical interface. The CXL switch is connected to the host\nmachine with x16 lanes, whereas each CXL device is con-\nnected to the switch through x4 lanes. The switch supports\nthe communication between the host and CXL devices, and\npeer-to-peer communication between CXL devices.\nInter-Device Communication. Figure 5 shows the ar-\nchitecture of a CXL device. Communication between CXL\ndevices involves the Shared Buffer and is orchestrated by\nthe inter-device communication controller in conjunction\nwith the CXL port. We introduce a broadcast primitive, al-\nlowing one CXL device to write data to multiple devices\nthrough a single request. The standard CXL.mem protocol\nlacks this support. We implement it by using one of the re-\nserved header codes within the Header slot (H-slot) of the\nPort Based Routing (PBR) flit. The H-slot is decoded by the\nswitch for routing. Upon identifying a flit encoded with this\nreserved H-slot code, the switch interprets it as a broadcast\nrequest and forwards the flit to designated CXL devices. We\nalso modified the CXL port to (1) incorporate a device ID\nmask within the header slot of the broadcast message, and (2)\nexpect write acknowledgements from all destination devices.\nSharedBuffer(64KB)DecoderInstructionBuffer(2MB)PCCXL Port\nInter-device Communication ControllerPNM Units\nPIM CtrlPIM CtrlLD/STLD/ST\nControl FlowInter-Device Data Flowx16Intra-Device Data Flowâ€¦GDDR6-PIMGDDR6-PIM\nLegends\nFigure 5. CXL Device Architecture.\nInter-device communication is supported by SEND_CXL,\nRECV_CXL and BCAST_CXL instructions. The non-blocking\nSEND_CXL specifies the device ID ( DVid) and the Shared\nBuffer address in source and destination devices. Conversely,\nRECV_CXL operates in a blocking manner and does not spec-\nify a device ID. A pair of send and receive instructions con-\nstitutes a CXL write transaction. BCAST_CXL is also non-\nblocking and uses an 8-bit DVcount parameter to specify\nthe number of subsequent CXL devices to which the data is\nbroadcast. The multicast primitive is supported in a similar\nmanner. To accomplishgather, the receiving device executes\nmultiple CXL_RECV instructions, while each sender executes\none SEND_CXL instruction. Note that the receive instruction\nomits any device ID specification, thereby rendering the\norder of incoming CXL flits inconsequential.\nCXL Port is depicted in Figure 6. CXL nodes are classi-\nfied into three categories: Host (H), representing the host\nmachine; Local (L), the CXL device we are considering; and\nRemote (R), referring to other CXL devices interconnected\nvia the switch. CXL port is equipped with virtual channels.\nRequests from the host and remote nodes are unpacked onto\nthe Rx H2L and R2L queues, and responses to the host and\nremote nodes are allocated to the Tx L2H and L2R queues.\nTransactions comprise a request and a response. On the\ntransmit (Tx) datapath, the CXL port packs requests into\nflits, which are unpacked on the receive (Rx) datapath by\nthe destination device. The CXL port supports 2 types of\ntransactions: read transactions, initiated with aRequest (Req)\nand concluded with Data with Response (DRS); and write\ntransactions that begin with a Request with Data (RWD) and\nfinish with No Data Response (NDR) acknowledgment.\n4.2 Hierarchical PIM-PNM Architecture\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nTx RxFlit UnpackFlit PackR2LNDRL2HNDRL2HDRS H2LRWDPrepare RequestIntegrity CheckData RequestLocal PIM Device\nHost / Remote PIM Device\nL2RNDR H2LReqL2RRWD R2LRWD\nFigure 6. CXL Port Architecture.\nIn Figure 5, CENT instructions are transmitted from the\nhost to a 2MB instruction buffer in each device. These in-\nstructions are further distributed to PIM channels and PNM\nunits. Standard read/write transactions are dispatched to\nPIM controllers similar to non-PIM memory modules. CENT\narithmetic instructions are decoded into micro-ops and sub-\nsequently directed to PIM controllers and PNM units.\nGDDR6-PIM Channel. The CXL device integrates 16\nPIM controllers, each managing two PIM channels. These\ncontrollers receive micro-ops from the decoder and convert\nthem into DRAM commands. Figure 7(a) shows that the PIM\nchannel consists of a 2KB Global Buffer shared by four bank\ngroups. The bank group contains four banks. Each bank has\na 32MB memory capacity coupled with a near-bank PU.\nWithin the PU is a 16 MAC reduction tree, operating on\nBfloat16 (BF16) data elements. Each multiplier receives 16-\nbit data directly from its associated local bank, in addition\nto another 16-bit data from either the Global Buffer or its\nneighboring bank (such as Bank 0 and Bank 1). The Global\nBuffer is capable of broadcasting 256-bit data to all PUs con-\ncurrently. 32 accumulation registers are incorporated to hold\nthe MAC results in the PU and are designated by the CENT\nISA. The activation function (AF) leverages lookup tables\nstored within the DRAM bank and linear interpolation.\nThe PU operates at 1GHz, equivalent to ğ‘¡ğ¶ğ¶ğ·ğ‘† (2ğ‘¡ğ¶ğ¾) of\nthe PIM bank, yielding a compute throughput of 32 GFLOPS.\nPIM channels are optimized to allow 16 near-bank PUs to\noperate in parallel. To facilitate this, the PIM controller is-\nsues an activate-all-banks ACTab command, followed by PIM\ncommands such as MACab and concludes with a precharge-\nall-banks PREab command. The ACTab command is enabled\nby the reservoir capacitors introduced in AiM [56, 60], and\nPREab is already supported by the GDDR6 DRAM [97].\nPNM Units. While near-bank PUs could efficiently sup-\nport MAC operations, LLMs necessitate a broader set of oper-\nations beyond MACs. To address this, the CXL device incor-\nporates the following PNM units, as shown in Figure 7(b): (1)\n32 Accumulators : each retrieves two values from the Shared\nBuffer as inputs and segments the 256-bit inputs into 16\ngroups for BF16 accumulations. (2) 32 Reduction Trees : each\nfetches a single 256-bit value from the Shared Buffer, reduc-\ning 16 BF16 input elements to a singleBF16 value. The result\nis stored into the first 16-bit element in a 256-bit Shared\nBuffer slot. (3) 32 Exponent Accelerators , each accesses a 256-\nbit value from the Shared Buffer, dividing it into 16 lanes. In\neach lane, the exponent of aBF16 input element is calculated\nby a 10-order Taylor Series approximation. (4)8 BOOM-2wide\nRISC-V cores [10], facilitating the execution of less common\noperations (such as square root and inversion), and accom-\nmodating future improvements in LLMs. Each RISC-V core is\nequipped with a 64KB instruction buffer, which is initialized\nby the host through CXL write transactions.\nIntra-Device Communication between PIM channels\nand PNM units is enabled through CENT data movement\ninstructions and a 64KB Shared Buffer. The Shared Buffer is\nviewed by PIM channels as 256-bit registers. CENT facili-\ntates data transfers between DRAM banks and the Shared\nBuffer by WR_SBK and RD_SBK instructions. These transfers\nare conducted by the load/store unit associated with each\nmemory controller. Additionally, WR_ABK instruction seg-\nments a 256-bit register into 16 discreteBF16 values and con-\ncurrently stores them in the same row and column address\nof all 16 banks within a channel. Communication among\nbanks in a PIM channel is mediated by the Global Buffer\nthrough COPY_BKGB and COPY_GBBK instructions. Similar to\nPIM channels, PNM units interface with the Shared Buffer\nat a 256-bit granularity and abstract it as a register file. The\nRISC-V core views the Shared Buffer as a byte-addressable\nmemory and interacts with it through 16-bit loads and stores\nin a designated 64KB region of the memory space.\n4.3 ISA Summary\nTable 2 shows CENT arithmetic instructions. The CHmask\nparameter directs the PIM decoder to broadcast micro-ops\nto specified PIM channels. PIM decoder generates OPsize\nmicro-ops from a single instruction, targeting subsequent\nShared Buffer slots and DRAM column addresses. TheRegid\nparameter identifies the specific accumulation register within\nthe PU, while AFid determines the type of non-linear activa-\ntion function. The RISCV instruction is designed to initiate\nthe execution of RISC-V cores at the specific start program\ncounter (PC) address.\nTable 2. CENT Arithmetic Instructions\nInstruction Assembly\nNear-Bank PUs\nMAC All Bank MAC_ABK CHmask OPsize RO CO Regid\nElement-wise Mult. EW_MUL CHmask OPsize RO CO\nActivation Function AF CHmask AFid Regid\nPNM Units\nExponent EXP OPsize Rd Rs\nReduction RED OPsize Rd Rs\nAccumulation ACC OPsize Rd Rs\nRISCV operation RISCV OPsize PC Rd Rs\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nBank Group 0 ~ 3\nBank 0\nBank 1\nBank 3\nBank 2\nPU\nPU\nPU\nPU\n(a) GDDR6-PIM Channel\nLocal bus from BankGlobal bus from Global Buffer\nBank I/O\nÃ— â€¦\n+\nMAC \nx16\nÃ— Ã— Ã—\n+\n+\n+\nPU\n16b\n16b\n256b\nâ€¦\nGlobal Buffer (2KB)\n32x Reduction Trees\nğ‘¹ğ’… = à·\nğ‘–=0\n15\nğ‘¹ğ’” ğ’Š\n(b) PNM Units\nx16â€¦+\n+\n+\n+ +\n+\n+\n+ +\n32x Accumulators\nâ€¦\nx16\n+ + + + + + + +\nğ‘¹ğ’… ğ’Š = ğ‘¹ğ’… ğ’Š +ğ‘¹ğ’” ğ’Š\nâœ•\n+\nâœ•\n16x Exponent Lanes\nâ€¦\nTaylor Coefficients\n32x Exponent Processors\nReg RegReg\nğ‘¹ğ’… ğ’Š = ğ’†ğ‘¹ğ’”[ğ’Š]\n8x BOOM-2wide \nRISC-V Cores\nğ‘…ğ‘‘ ğ‘– = ğ‘“(ğ‘…ğ‘  ğ‘– )\nDecoder\nShared Buffer (64KB)\nFigure 7. Hierarchical PIM-PNM Architecture\nTable 3 summarizes CENT data movement instructions,\nspecifying DRAM bank locations using channel (CHid), bank\n(BK), row (RO), and column (CO). The source and destination\nShared Buffer addresses are specified by Rd and Rs.\nTable 3. CENT Data Movement Instructions\nInstruction Assembly\nCXL Device â†”CXL Device\nSend SEND_CXL DVid Rs Rd\nReceive RECV_CXL\nBroadcast BCAST_CXL DVcount Rs Rd\nShared Buffer â†”DRAM Banks\nWrite Single Bank WR_SBK CHid OPsize BK RO CO Rs\nRead Single Bank RD_SBK CHid OPsize BK RO CO Rd\nWrite All Banks WR_ABK CHid RO CO Rs Regid\nGlobal Buffer â†”DRAM Banks\nCopy Bank â†’Global Buffer COPY_BKGB CHmask OPsize RO CO\nCopy Global Buffer â†’Bank COPY_GBBK CHmask OPsize RO CO\nShared Buffer â†”PUs\nWrite bias WR_BIAS CHmask Rs\nRead MAC register RD_MAC CHmask Rd Regid\nShared Buffer â†’Global Buffer\nWrite Global Buffer WR_GB CHmask OPsize CO Rs\n5 Model Mapping\nThe ever-increasing parameter size of the LLMs, coupled\nwith the lower memory density of PIM, necessitates the\ndistribution of the LLM inference on a scalable network of\nPIM modules. In this section, we introduce the mapping\nof various LLM parallelization strategies on CENTâ€™s CXL-\nbased network architecture using the proposed collective\nand peer-to-peer communication primitives.\n5.1 Pipeline-Parallel Mapping (PP)\nCloud providers serve a large user base, where inference\nthroughput is crucial. To improve throughput, PP [ 38] as-\nsigns each transformer block to a pipeline stage. The indi-\nvidual queries in a batch are simultaneously processed in\ndifferent stages of the pipeline. Figure 8 shows that we map\nmultiple pipeline stages (e.g., T0-3) to a CXL device (e.g., D0).\nEach stage requires multiple PIM channels, depending on\nthe memory requirements of the decoder block. To prevent\nexcessive communication and keep the latency of pipeline\nstages identical, we avoid splitting a pipeline stage between\nthe PIM channels of two CXL devices.\nIn each iteration, the output of each transformer block is\ntransferred to the next pipeline stage. CENT performs this\ndata transfer using intra-device communication for pipeline\nstages within the same CXL device, and using peer-to-peer\nsend and receive primitives for those in different CXL devices.\nThis CXL data transfer contains only an 8K embedding vector\n(16KB data) in Llama2-70B. The CXL transfer latency of PP\nis negligible compared to PIM and PNM latencies.\nNote that CENT does not support batch processing within\na single pipeline stage because of two primary reasons: First,\nbatching requires a significantly larger Global Buffer and\nShared Buffer (Section 4.2) to concurrently store the embed-\nding vectors of multiple queries. Second, batching enhances\nthe operational intensity and compute utilization (Section 2),\nwhile PP fully utilizes PIM compute resources. Therefore,\napplying batching on top of PP only increases the latency.\nHost\nCXL Switch\nT0\nT1\nT2\nT3\nTransformer \nDecoder Blocks /\nPipeline Stages\nCXL Devices\nMultiple Prompts\nT4\nT5\nT6\nT7\nT8\nT9\nT10\nT11\nâ€¦\nInter-Device \nData Transfers\n(16KB)\nT0\nT1\nT2\nT3\nâ€¦\nT0\nT1\nT2\nT3\nâ€¦\nT0\nT1\nT2\nT3\ntimeP1\nP2\nP3\nD0 D1 D2\n(a) (b)\nFigure 8. Pipeline parallelism: (a) Transformer decoder\nblocks are distributed across CXL devices and form the\npipeline stages. Each block is mapped to multiple GDDR6-\nPIM channels. (b) Multiple prompts are executed in different\nstages of the pipeline.\n5.2 Tensor-Parallel Mapping (TP)\nInference latency is critical in real-time applications to pro-\nvide a smooth user experience [25]. To enhance the latency,\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nTP [100, 115] uses all compute resources to process decoder\nblocks one at a time. To implement TP, Figure 9(a) shows\nthat CENT assigns each transformer decoder block across all\nCXL devices. Figure 9(b) illustrates the detailed mapping of a\ntransformer block using TP. The infrequent residual connec-\ntion and normalization layers are confined within a single\nmaster CXL device. Distributing the attention layer requires\nthe frequent use of expensive AllReduce collective commu-\nnication primitive, which significantly increases the CXL\ncommunication overhead [100]. Consequently, the attention\nlayer is mapped to the master CXL device.\nHostCXL Switch\nDecoder 0Decoder 1Decoder 2Single Prompt\n(a) Decoder block mapping\ntime\nâ€¦\nCXL DeviceCXL Device\nCXL Device\nHost\nDâ€¦D\nAdd & NormFC Layer\nLayers mapped to a single CXL deviceLayers mapped to multiple CXL devices\nD\nBroadcast\nHost\nDâ€¦DD\nGather\nCXL Switch\nCXL SwitchFC Layer\nFC Layer\nFC Layer\nAdd & Norm\nSiLU\nAttention score\n(b) Layer Mapping in Each Decoder Block\n16KB\n16KB\nFigure 9. (a) Tensor parallelism: each transformer block is\nassigned to multiple CXL devices. Prompts are processed se-\nquentially. (b) In a transformer block, fully connected layers\nare spread across CXL devices, while other operations are\nconfined to a single device.\nPrior to the execution of an FC layer, the embedding vec-\ntor (16KB for Llama2-70B) is broadcast from the master CXL\ndevice to all devices via the CXL switch. This enables each\ndevice to locally perform GEMV on multiple rows of the\nweight matrix. Following the execution of an FC layer, par-\ntial result vectors aregathered to the master CXL device. This\napproach optimizes the execution of FC layers across multi-\nple devices, while reducing the communication overhead of\nTP through the CXL switch to only 135KB data transfer for\neach transformer block of the Llama2-70B model.\n5.3 Hybrid Tensor-Pipeline Parallel Mapping\nThe TP and PP mappings focus either on inference latency\nor throughput. However, balancing both can be crucial in\nreal-world deployment scenarios when considering Quality\nof Service (QoS) requirements [6]. We explore a hybrid TP-\nPP strategy to achieve this balance, where each transformer\ndecoder is allocated to multiple consecutive CXL devices.\nFor example, among 32 devices, mapping each decoder to\n32/4 = 8 devices enables TP=8 and PP=4. The embedding\nvectors are multicast and gathered by the master CXL device\nof each pipeline stage. This configuration effectively reduces\ntoken decoding latency by utilizing compute resources from\nmultiple CXL devices (TP), while also improving the through-\nput by processing multiple prompts in parallel (PP).\n5.4 Transformer Block Mapping\nCENT involves a fine-grained mapping of the transformer\nblock onto CXL devices, PNM accelerators, and PIM chan-\nnels. This technique permits the complete execution of a\ntransformer block within the CXL device, thereby eliminat-\ning the necessity for any interaction with the host system.\nFigure 10(a) illustrates the operations within a Llama2 trans-\nformer block. Operations within the blue blocks are assigned\nto PIM channels, including GEMV in fully connected layers,\nvector dot product in RMSNorm, and element-wise multipli-\ncation in RMSNorm, SiLU, Softmax and Rotary Embedding,\nas detailed in Figure 10(b), (c), (d), and (e), respectively. On\nthe other hand, model-specific operations marked in orange,\nsuch as square root, division, Softmax, and vector addition\nin residual connections, are handled by the PNMâ€™s RISC-V\ncores and accelerators. CENT supports Grouped-Query At-\ntention [3] in Llama2-70B by unrolling GEMM to GEMV.\nRMSNorm (b)\nFully Connection\nQuery Key\nValue\nRotary Emb. (e)\nGEMV *\nScale\nSoftMax (d)\nGEMV\nScore\nFully Connection\nFully Connection\nSiLU (c)\nRMSNorm (b)\nFully Connection\nğ‘’ğ‘ ğ‘–\n1\nÏƒğ‘’ğ‘ ğ‘–\nğ‘ ğ‘ğ‘œğ‘Ÿğ‘’[4ğ¾]\nSoftMax(score)(d)\nğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘¥)\nğ‘¥\nSiLU(x)(c)\nResidual Residual\n(a) Llama2 Transformer Block\nOperations on PIM\nOperations on PNM\nFeed Forward\nSelf Attention\nRMSNorm\n1\nÏƒ(ğ‘¥2)\nğ‘›\nğ‘¥[8ğ¾] ğ‘¥\nğ‘¥\nğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘¥)\n(b)\nWeights\nComplex \nTransform\nRotary Embedding(e)\nQuery[128]\nWeights\nğ‘¥2\nğ‘¥[8ğ¾]\n1\nâ„ğ‘’ğ‘ğ‘‘ğ·ğ‘–ğ‘š\nReal \nTransform\nELMul\n* Narrow GEMM if model applies Grouped-Query Attention. \nFigure 10. (a) Llama2-70B Transformer Block. Blue and or-\nange operations are mapped to PIM and PNM PUs, respec-\ntively. (b)âˆ¼(e) Operation mapping for RMSNorm, SiLU, Soft-\nMax and Rotary embedding.\nIn Figure 10(d), the score dimension varies between 1 and\n4ğ‘˜, accommodating the 4K sequence length in this example.\nThe embedding dimensions, as shown in Figure 10(b) and\n(c), are set to 8ğ¾. The rotary embedding process, depicted\nin Figure 10(e), begins with the RISC-V PNM cores trans-\nforming an attention head of dimension 128 into 64 groups\nof the complex number representations ( e.g., [ğ‘,ğ‘,ğ‘,ğ‘‘ ]to\n[(ğ‘+ğ‘—ğ‘),(ğ‘+ğ‘—ğ‘‘)]). The PIM PUs within memory chips then\nmultiply complex values and pre-loaded weights. Finally,\nRISC-V PNM cores convert the computed results back to\ntheir real value representations.\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nCENTâ€™s PIM computations include three key operations.\nThis paragraph explains the execution of each operation\nwithin a GDDR6-PIM channel. (a) GEMV : The matrix is par-\ntitioned along its rows and distributed across all 16 banks.\nThe vector is transferred to the Global Buffer. MAC_ABK in-\nstructions then broadcast 256-bit vector segments from the\nGlobal Buffer to all near-bank PUs, retrieve 256-bit segments\nof the matrix rows from the banks, and perform MAC opera-\ntions. (b) Vector dot product : In this operation, input vectors\nare stored in neighboring banks. MAC_ABK instructions re-\ntrieve 256-bit segments from these banks and perform MAC\noperations. Throughout this process, only one of the two\nneighboring near-bank PUs is utilized. (c) Element-wise mul-\ntiplication: Before this operation, input vectors are stored in\ntwo banks within each bank group, which consists of four\nbanks. EW_MUL instructions then retrieve 256-bit segments\nfrom these two banks, perform the multiplication, and store\nthe results in another bank within the same bank group.\n5.5 End-to-End Model Mapping\nCENT supports the end-to-end query execution in LLM\ninference tasks. In the prefill stage, CENT processes tokens\nin the prompt one after another to fill out KV caches, using\na similar approach to that in the decoding stage. Within\neach token, both input embeddings and transformer blocks\nare mapped to CXL devices using the mapping techniques\nintroduced in Section 5.4. In the decoding stage, after a series\nof transformer blocks, the top-k sampling operations are\nexecuted on the host CPU.\n5.6 Programming Model\nUsers can specify the CENT hardware configuration, includ-\ning the number of PIM channels to utilize, and the number of\npipeline stages. The tensor mapping strategy is determined\nby this configuration. CENT library provides Python APIs to\nallocate memory space and load model parameters accord-\ning to the model mapping strategy. These APIs also support\ncommonly used LLM operations, such as GEMV, LayerNorm,\nRMSNorm, RoPE, SoftMax, GeLU, SiLU, etc. CENT uses an in-\nhouse compiler to generate arithmetic and data movement\ninstructions illustrated in Section 4.3.\n1.Vector = {shared_buffer_addr, vector_dim} 2.   Matrix = {num_row, row_addr, matrix_dim} 3.   def  GEMV(Vector, Matrix, Hardware_config) { 4.       for each channel_index in Hardware_config->num_channels: 5.           WR_GB (Vector->shared_buffer_addr) 6.           for each row_index in Matrix->num_rows: 7.                WR_BIAS (channel_index) 8.                MAC_ABK (channel_index, row_addr + row_index) 9.                RD_MAC (channel_index)     10.   }\nFigure 11. Vector-matrix multiplication compilation\nFigure 11 shows an example of compiling GEMV to CENT\ninstructions. Initially, the operands are designated to partic-\nular memory spaces, i.e., the vector operands in the Shared\nBuffer and the matrix operands in PIM channels (lines 1 and\n2). CENT instructions are then generated based on input\noperandsâ€™ dimensions and memory addresses. Subsequently,\nthe vector is copied to the Global Buffers in the PIM chan-\nnels with WR_GB instructions (line 5). This is followed by\na sequence of operations for each matrix row within the\nnear-bank PIM PUs. The WR_BIAS instruction sets up the ac-\ncumulation registers (line 7).MAC_ABK performs the multiply-\naccumulate operations across all near-bank PUs in the PIM\nchannel (line 8). Finally, RD_MAC retrieves the results from\nthe accumulation registers (line 9).\n6 Methodology\nTable 4 lists the system configurations of CENT and our GPU\nbaseline. The GPU system contains 4 NVIDIA A100 80GB\nGPUs equipped with the NVLink 3.0 interconnect. CENT\nhas 32 CXL devices, resulting in a similar average power to\nthe GPU system, as further explained in Section 7.2.\nTable 4. Evaluated system configurations\nSystem CENT GPU\nHardware 32 CXL devices 4 NVIDIA A100\nProcess 1Y nm (14-16nm) 7nm\nMemory 512GB, GDDR6 320GB, HBM2E\nCompute\nThroughput\n512 TFLOPS (PIM) 1248 TFLOPS96 TFLOPS (PNM)\nPeak Bandwidth 512 TB/s (Internal) 8 TB/s (External)\n3-Year Owned TCO 0.73$/hour 1.76$/hour\n3-Year Rental TCO 1.05$/hour 5.45$/hour\nGDDR6-PIM ğ‘¡ğ‘…ğ¶ğ·ğ‘…ğ·=18ns, ğ‘¡ğ‘…ğ´ğ‘†=27ns, ğ‘¡ğ¶ğ¿=25ns\nTiming Constraints ğ‘¡ğ‘…ğ¶ğ·ğ‘Šğ‘…=14ns, ğ‘¡ğ¶ğ¶ğ·ğ‘†=1ns, ğ‘¡ğ‘…ğ‘ƒ=16ns\nWe benchmark Llama2 7B, 13B, and 70B models [ 106].\nEach evaluated query contains 512 tokens in the prefill stage\nand 3584 tokens in the decoding stage, adding up to a context\nlength of 4K, i.e., the maximum supported by the Llama2\nmodels. For a fair comparison between CENT and the GPU\nbaseline, we deploy these models using different configura-\ntions for different parameter sizes: 1, 2, and 4 GPUs, and 8, 20\nand 32 CXL devices. We use vLLM [54], the state-of-the-art\ninference serving framework on GPUs with a batch size of\n128, where the inference throughput saturates (Figure 1).\nWe generate CENT instruction traces for a single block\nand verify the correctness using a functional simulator. We\nmodify Ramulator2 [67] to model a CXL device containing\n32 GDDR6-PIM memory channels with timing constraints in\nTable 4. The inter-device communication through the CXL\n3.0 protocol is modeled by an analytical model based on the\nCXL latency [61] and PCIe 6.0 bandwidth. To model a CXL\nswitch supporting multicast, we use half of the bandwidth\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nand double the latency of the baseline switch. We use Intel\nXeon Gold 6430L CPU [42] as the host machine in CENT.\nWe use Micron DRAM Power Calculator [69] to evaluate\nDRAM core power using current and voltage specifications\nof Samsungâ€™s 8Gb GDDR6 SGRAM C-die [97]. The MAC op-\neration power is modeled assuming 3Ã—more current than a\ntypical gapless read [56]. We assume that each GDDR6 mem-\nory controller for two channels consumes 314.6 mW [108]\nand each BOOM RISC-V core consumes 250 mW [ 9]. We\nimplement the RTL of the remaining components in the CXL\ncontroller and synthesize it using a TSMC 28nm technology\nlibrary and the Synopsys Design Compiler [ 104]. We find\nthe critical path delay as 1ns at 28nm and project the CXL\ncontroller clock frequency to be 2.0 GHz at 7nm [101].\nWe estimate the die area of CXL controller in two parts.\nFirst, we synthesize the custom logic in 28nm (See Table 5)\nand scale it down to 7nm [101]. Then, we add measurements\nof the memory controller, PCIe controller, and PHY from the\nNVIDIA GPU die shots [89, 110], which are also scaled down\nto 7nm. This results in an estimated area of 19.0ğ‘šğ‘š2 in 7nm.\nTable 5. CXL Controller Custom Logic Area&Power in 28nm\nComponents Area (mm2) Power (W)\nSRAM Instruction Buffer 3.33 0.61\nShared Buffer 0.11 0.03\nLogics\nAccelerators 1.34 0.18\nRISC-V Cores 2.94 0.19\nOthers 0.12 0.05\nTotal 7.85 1.06\nTable 4 presents the 3-year Total Cost of Ownership (TCO)\nfor both owned and rental hardware. (a) Own TCO: We\nmodel a local server by accounting for hardware and op-\nerational costs. (b) Rental TCO: The cost for host CPU in\nCENT and GPU are estimated based on the Microsoft Azure\nprices [1]. The CXL devices in CENT are evaluated using\nthe owned TCO methodology, as there are no available refer-\nences for rental costs. To calculate operational cost, we use\n$0.139/ğ¾ğ‘Šâ„ [79] and average power consumption. Hard-\nware costs are listed in Table 6. While the lowest available\nprice for A100 80GB is close to $20,000, we instead use only\n$10,000 by conservatively deducting 50% margin [20]. The\nPIM module cost is estimated as 10 Ã—the cost of standard\nDRAM modules [17, 107].\nTable 6. Hardware Costs\nSystem Hardware Cost ($)\nGPU\nXeon Gold 6430 CPU [43] 2,128\n4 NVIDIA A100 80GB GPU [20] 40,000\nTotal Cost 42,128\nCENT\n32 devices\nXeon Gold 6430 CPU [43] 2,128\n512GB GDDR6-PIM [17, 107] 11,873\n32 CXL Controllers 381.3\n96-lane 48-port switch [21] 490\nTotal Cost 14,873\n0510152025\nNRE CostMillion Dollar ($) \nSystem NREPackage DesignIP LiscensingFrontend LaborBackend CADBackend LaborMask0102030\n12345\nCost ($)\nProduction V olume (Millions)\nDie costPackaging costNRE cost\nVo l u m e :  3 MCost: $11.9\nFigure 12. CXL Controller Cost Breakdown\nFigure 12 illustrates the breakdown of CXL controller cost\nper CENT CXL device (Figure 5). The CXL controller costs\nare broken down into die, packaging and Non Recurring\nEngineering (NRE) cost components [49, 71]. Die cost is de-\nrived from the wafer cost, considering the CXL controller\ndie area (19.0ğ‘šğ‘š2 in 7nm) and yield rate. A 300mm diameter\n7nm wafer costs $9,346 with a defect density of 0.0015 per\nğ‘šğ‘š2 [71]. Cost of 2D packaging is assumed to be 29% of chip\ncost [59], while the 2.5D packaging cost is calculated based\non interposer, die placement and substrate assembly [ 85].\nNRE cost is influenced by chip production volumes, which\nwe estimate at 3 million units based on the following assump-\ntions. NVIDIA shipped 3.76ğ‘€ datacenter GPUs in 2023 [70].\nWe assume that 10% of datacenter GPUs (around 370ğ¾) are\nused for LLM inference. Since each GPU consumesâˆ¼8Ã—more\npower compared to a CENT device (explained in Section 7.2),\nwe project âˆ¼3ğ‘€ volume for CENT devices.\n7 Results\n7.1 CENT versus GPU Baseline\nFigure 13 compares the performance of CENT and our\nGPU baseline under two scenarios: (a) Latency Critical: We\nuse a batch of 1 query (CENTâ€™s tensor parallel mapping).\n2.3\n012347B13B70B7B13B70B7B13B70BGeomeanPrefillDecodingEnd-to-End(b) Throughput\n4.6\n012345677B13B70BGeomeanEnd-to-EndSpeedup\n(a) Latency\n5.2\n0123456787B13B70BGeomeanEnd-to-End(c) Tokens/$Latency Critical(Batch=1)Throughput Critical(GPU Batch=128, CENT Batch=32/40/80)\nFigure 13. CENT speedup over GPU baselines.\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\n0\n2\n4\n6\n8\n10\n0 4 8 12 16 20\nQuery Latency (minute)\nThroughput (query/minute)\nCENT GPU\n0 1 2 3 4 5 6\nPP=80\nPP=16 TP=2\nPP=8 TP=4\nPP=4 TP=8\nPP=2 TP=16\nPP=1 TP=32\nQuery Latency (minute)\nPIM\nCXL\nPNM\nHost CPU\n0\n2\n4\n6\n8\nGPU\nCENT\nGPU\nCENT\nGPU\nCENT\nGPU\nCENT\nIn 512\nOut 128\nIn 512\nOut 512\nIn 512\nOut 1k\nIn 512\nOut 3.5k\nQuery Latency (minute)\nGPU Batch=128, CENT Batch=80\nPrefill\nDecoding\n(b) QoS Comparison (c) CENT Latency Breakdown (d) Query Latency Comparison\n0\n1\n2\n3\n4\n4K 8K 16K 32K\nThroughput Speedup\nContext Length\nPP=16 TP=2\nPP=80\n(a) Decoding Throughput\nunder Long Contexts\nFigure 14. Analysis on Llama2-70B. (a) CENT achieves higher decoding throughputs with long context windows and 3584\ndecoding sizes (Section 6). 16K and 32K context scenarios with PP=80 configurations require the 16Gb GDDR6-PIM module,\nincreasing CENT capacity to 1TB (2X of that used in main results). (b) QoS analysis: CENT provides lower query latency while\nachieving similar throughput as GPU. (c) CENT latency breakdown with different parallelism strategies. (d) Prefill (In) and\ndecoding (Out) latency comparison with different In/Out sizes, at maximum supported batch size for both GPU and CENT.\nIn this case, CENT reduces the end-to-end latency by 4.6Ã—\ncompared to GPUs. This speedup is due to the higher internal\nmemory bandwidth of PIM. (b) Throughput Critical: We use\nthe maximum batch size of 128 for GPU experiments, as\nexplained in Section 6. On the other hand, CENT utilizes\npipeline parallelism to enable batches of 32/40/80 queries for\nthe three models (batch size = pipeline stages). Using this\nconfiguration, CENT achieves a geomean of 2.3Ã—higher end-\nto-end throughput across three models. CENT demonstrates\n1.2Ã—speedup on Llama2-70B because this model applies\nthe grouped-query attention technique [3], improving the\noperational intensity of the attention layers. Figure 13(c)\nshows that CENT processes 5.2Ã—higher tokens per dollar\nthan GPU, which attributes to CENTâ€™s higher throughput\nand 2.5Ã—cheaper TCO (Table 4).\nFigure 13(b) compares throughput in the prefill and de-\ncoding stages. GPU achieves 2.5Ã—higher throughput in the\ncompute-intensive prefill stage than CENT due to GPUâ€™s\n2.0Ã—higher peak compute throughput. Conversely, CENT\noutperforms GPU in the memory-intensive decoding stage\nby 2.5Ã—due to PIMâ€™s higher internal memory bandwidth. No-\ntably, the prefill stage accounts for only 2% of the total GPU\nend-to-end processing time, so the overall LLM inference\nthroughput closely aligns with that of the decoding stage.\nCENT performs better than GPU in long context\nscenarios. The results in Figure 13 use a 4K context win-\ndow. However, state-of-the-art LLMs support longer con-\ntexts, ranging from 128K to 1M tokens [29, 81]. As discussed\nin Section 2, with longer contexts, the GPU system satu-\nrates at smaller batch sizes, from batch=128 at 4K context\nto batch=16 at 32K context. On Llama2-70B, CENT achieves\nhigher speedup than GPUs as context length increases, at-\ntaining up to 3.3 Ã—speedup in decoding throughput for a\ncontext length of 32K, as shown in Figure 14(a).\nCENT has lower query latency than GPU at similar\nthroughput. Figure 14(b) illustrates our QoS comparison on\nLlama2-70B. These results are collected with different batch\nsizes on GPUs and different TP/PP mapping strategies on\nCENT. CENT provides 3.4-7.6Ã—lower query latency while\nachieving similar throughput to the baseline GPU.\nLatency Breakdown. Figure 14(c) shows CENTâ€™s latency\nbreakdown with different TP/PP mapping strategies. PIM\nlatency always dominates because most of the operations\nare mapped to PIM channels. As TP increases (from top\nto bottom), PIM latency reduces. This is because more PIM\nchannels are allocated to a single transformer block. Yet, CXL\ncommunication latency increases with higher TP, because\ndistributing a transformer block across more CXL devices\nnecessitates more broadcast and gather transactions. Fig-\nure 14(d) depicts the latency comparison between CENT and\nGPU at maximum supported batch sizes. Compared to GPU,\nCENT shows 1.4Ã—higher latency in the prefill stage and 1.7-\n2.0Ã—lower latency in the decoding stage. Decoding latency\ndominates the end-to-end latency.\n7.2 Power and Energy Consumption Analysis\nWe developed an activity-based power model for CENT.\nWhen deploying the Llama2-70B model on 32 CXL devices\nwith the pipeline parallel model mapping, 27 devices are used.\nAmong 80 transformer blocks (80 pipeline stages), 3 of them\nare mapped to each device, resulting in an average power of\n32.4W per device. PIM operations and activation/precharge\ncommands consume 54.5% and 30.2% of power, respectively.\nSimilarly, we used nvidia-smi to measure GPU power\nduring the prefill and decoding stages in 100ms intervals.\nFigure 15(a) illustrates the average power consumption of\nCENT versus Nvidia A100 80GB GPUs. One A100 GPU con-\nsumes â‰ˆ8Ã—higher power than one CENT device. Modern\nGPUs consume significantly higher power as they support\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\n2.9\n0.0\n1.0\n2.0\n3.0\n4.0\n7B\n13B\n70B\n7B\n13B\n70B\n7B\n13B\n70B\nGeomean\nPrefill Decoding End-to-End\nCENT/GPU Normalized Tokens/J0\n300\n600\n900\n1200\nPrefill\nDecoding\nEnd-to-End\nPrefill\nDecoding\nEnd-to-End\nPrefill\nDecoding\nEnd-to-End\nPrefill\nDecoding\nEnd-to-End\nPrefill\nDecoding\nEnd-to-End\nPrefill\nDecoding\nEnd-to-End\n8\nCENT\nDevices\n1\nA100\nGPU\n20\nCENT\nDevices\n2\nA100\nGPUs\n32\nCENT\nDevices\n4\nA100\nGPUs\n7B 13B 70B\nPower Consumption (W)\n0\n100\n200\n300\n400\n1100\n1200\n1300\n1400\n1500\nGPU Board Power (W)\nGPU SM Frequency  (MHz)\n(c) Energy Efficiency (a) Power Consumption\nInit Decoding\n(b) GPU Clock/Power Throttling\nPrefill\n300W \nTDP\n1410MHz\nMax Freq\nSM Clock Frequency            Board Power\nFigure 15. (a) Power consumption of CENT and GPU (b) GPU SM frequency and board power, and (c) energy efficiency\n(Tokens per Joule) of CENT and GPU using the maximum batch size, 512 prefill tokens and 3584 decoding tokens.\nMatrix \nUnits\n(c) AttAcc (d) NeuPIM(b) CXL-PNM(a) CENT\nCXL \nCtrl.\nGDDR6-PIMVector \nUnits PIM\nPIM\nPIM\nâ€¦\nPIM\nPIM\nPIM\nPIM\nPIM\nPIM\nCXL Switch\nPIM PIM PIM\nPIM PIM PIM\nHost â€¦\nPNM PNM\nPNM PNM\nâ€¦Host\nPNM\nPNM\nPIM PIM PIM\nGPU GPU GPU\nHost Interconnect e.g., \nNVLink, PCIe, CXL â€¦\nPIM PIM PIM\nGPU GPU GPU\nHost Interconnect e.g., \nNVLink, PCIe, CXL â€¦\nHBM2-PIM PIM Module\nDRAM Module\nVector Units\nCXL Controller\nMatrix Units DRAM\nDRAM\nDRAM\nâ€¦\nLPDDR5X\nVector Units\nVector Units Matrix Units\nController\nPIM Controller\nPIM Devices\nPNM Devices\nGPU\nInterconnect\nHost\nHBM3-PIM\nFigure 16. (a) CENT employs vector units near PIM modules and utilizes a CXL switch to interconnect PIM devices with novel\nCXL communication primitives. (b) CXL-PNM [88] applies a processing-near-memory solution without integrating compute\nlogic into DRAM chips. (c-d) AttAcc [86] and NeuPIM [37] are heterogeneous systems comprising GPUs and PIM devices.\ngeneral-purpose PTX ISA [77], a large number of Streaming\nMultiprocessors (108 SMs in A100), multithreading with fast\ncontext switching, and a multi-level cache hierarchy (â‰ˆ60\nMB in A100 [74]). In contrast, CENT is a custom architecture\nwith minimal silicon used for near-bank compute units.\nGPUs operate near their thermal design power (TDP) of\n300W [74] during both the prefill and decoding stages when\nprocessing a large batch size of 128 queries. Figure 15(b) il-\nlustrates this by showing the GPUâ€™s SM clock frequency and\nboard power consumption for the Llama2-7B model. During\nvLLM [54] initialization, the clock frequency is maximized\nat 1410 MHz due to low compute throughput and memory\nbandwidth utilization. In the prefill stage, high SM utiliza-\ntion signals the GPUâ€™s power manager to throttle the clock\nfrequency, maintaining power consumption within the TDP.\nDuring the decoding stage, reduced SM utilization allows\nfor an increase in clock frequency. A higher clock rate and\nmemory bandwidth usage keep power near the TDP.\nFigure 15(c) shows that CENT processes 2.9Ã—more tokens\nper Joule than GPU, on average. In the compute-bound prefill\nstage, GPU is 2.4Ã—more energy efficient, as it achieves effi-\ncient data reuse in the on-chip SRAM. In the memory-bound\ndecoding stage, CENT achieves 3.2Ã—higher energy efficiency,\nwhile GPU cannot efficiently reuse data in the SRAM because\nof the low operational intensity. Our evaluation shows that\nCENT consumes 0.6 pJ/bit on MAC_ABK operations, making it\n6.6Ã—more energy efficient than evenonly the HBM2 memory\nread accesses of GPU, which consumes 3.97 pJ/bit [78].\n7.3 CENT versus PIM/PNM Baselines\nWe compare CENT with the state-of-the-art CXL-PNM [88]\nand heterogeneous GPU-PIM baselines [ 37, 86]. Figure 16\nprovides an architectural overview of these systems.\nCENT versus CXL-PNM. Figure 16(b) shows that CXL-\nPNM [51, 88] is a processing-near-memory (PNM) platform\nthat leverages a CXL controller to manage eight LPDDR5X\npackages within a single device. The CXL controller deploys\nmatrix and vector units to perform computations near com-\nmodity LPDDR5X chips. In contrast, Figure 16(a) depicts\nCENT, which utilizes processing-in-memory (PIM) technol-\nogy to place compute logic adjacent to DRAM banks within\nDRAM chips. Figure 17(b) shows that compared to CXL-\nPNM, CENT provides significantly higher compute through-\nput (TFLOPs) and memory bandwidth (TB/s), at the cost\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nof less memory capacity (GB). Figure 17(a) illustrates that\nCENTâ€™s higher compute and memory bandwidth results in\n4.5Ã—higher throughput than CXL-PNM, at the maximum\nsupported batch sizes for each system.\n0\n0.4\n0.8\n1.2\n1 8 32 24\nK Tokens / s\nCXL-PNM CENT\n(a) CENT and CXL -PNM (b) System Configurations\nSystem CXL-PNM CENT\nDevice \nCount 1 8 32 24\nTFLOPS 8.2 65.6 262 456\nMem BW \n(TB/s) 1.1 8.8 35.2 384\nMem \nCapacity\n512 \nGB\n4 \nTB\n16 \nTB\n384 \nGBDevice Count\nFigure 17. CENT and CXL-PNM baseline comparison on\nOPT-66B [114] with prefill=64 and decoding=1024.\nCENT versus GPU-PIM. AttAcc [86] and NeuPIM [37]\nare heterogeneous systems consisting of GPUs and PIM de-\nvices as shown in Figure 16(c) and (d). The AttAcc system\nconsists of 8 A100 GPUs with HBM3 memory [ 73] and 8\nHBM-PIM devices. Each HBM-PIM device consumes 116W\nand has a memory capacity of 80GB. The NeuPIM device inte-\ngrates a TPUv4-like NPU [45] architecture near PIM modules\nand extends PIM with dual row buffers, enabling concurrent\nPIM-NPU memory access. The evaluated NeuPIM platform\ncomprises 8 A100 GPUs and 8 NeuPIM devices.\nDistinct from these systems, CENT introduces a GPU-\nfree inference server, providing an alternative cost-effective\nsolution and eliminating the need for expensive GPUs. In\nGPU-PIM systems, the prefill stage is mapped to GPUs while\nthe remaining computation is mapped to the PIM subsystem.\nCENT does not employ GPUs for the prefill stage for vari-\nous reasons. First, end-to-end LLM inference performance\nis primarily constrained by the decoding phase rather than\nthe prefill phase; only 2% of the total GPUâ€™s inference time\nis taken by the prefill stage across Llama2 models, on av-\nerage (Section 7.1). Second, CENTâ€™s compute throughput is\nnot much worse than GPU (â‰ˆ49%, Table 4). Third, using ex-\npensive GPUs solely to support the prefill stage is a costly\noption. Using the methodology from Section 6, we find that\nthe TCO of AttAcc and NeuPIM is 3.5Ã—and 2.6Ã—higher than\nCENT, respectively. The cost of HBM-PIM is estimated at\n10Ã—the price of HBM [98], while the NPU cost is modeled\nbased on die, 2.5D packaging, and NRE costs [45, 49, 85].\nFigure 18 shows the performance of CENT versus AttAcc\nand NeuPIM. For a power-neutral evaluation, we assume 12\nCENT devices per GPU-PIM node. Across different sequence\nlengths and batch sizes, the blue bars show that CENT pro-\ncesses 1.8-3.7Ã—and 1.8-5.3Ã—more tokens per dollar than\nAttAcc and NeuPIM systems, respectively. The orange dots\nshow that CENTâ€™s raw throughput (Tokens/s) is 0.5-1.1Ã—and\n0.7-2.1Ã—the throughput of AttAcc and NeuPIM, respectively.\nIn scenarios with short sequence lengths, query batching\n0\n10\n20\n0\n1\n2\n3\n4\nBatch 512\nBatch 256\nBatch 128\nBatch 64\nBatch 96\nK Tokens / s\nM Tokens / $\nM Tokens/$ K Tokens/s\n0\n2\n4\n6\n8\n10\n0\n1\n2\n3\n4\nAttAcc\nCENT\nAttAcc\nCENT\nAttAcc\nCENT\nAttAcc\nCENT\nIn 128\nOut 128\nIn 128\nOut 2K\nIn 2K\nOut 128\nIn 2K\nOut 2K\nK Tokens / s\nM Tokens / $\nM Tokens/$ K Tokens/s\n(b) CENT and NeuPIM(a) CENT and AttAcc\nNeuPIM CENT\nFigure 18. CENT versus GPU-PIM (a) CENT and AttAcc sys-\ntems are evaluated on the GPT3-175B model across various\ninput and output sizes, tested at the maximum supported\nbatch sizes. (b) The CENT and NeuPIM systems are evalu-\nated on GPT3-175B with data-parallel mapping (DP=4) and\npipeline-parallel mapping (PP=4), respectively, using the\nShareGPT dataset [105]. NeuPIM uses different batch sizes\nwhile CENT uses the maximum supported batch size 96.\nenhances operational intensity in FC layers, improving per-\nformance on GPUs (or NPUs) with more TFLOPs. However,\nin cases with long sequence lengths that limit batch sizes,\nCENT maintains higher raw throughput than the GPU-PIM\nbaselines. Latest LLM models typically support 128K con-\ntext windows [81]. With these extended context lengths, we\nexpect CENT to provide even higher performance.\n7.4 Design Space Exploration\nCENT can interconnect a flexible number of CXL devices,\nallowing for scalable system configurations. Figure 19 shows\nthe scalability of CENT on Llama2-70B from 16 to 128 devices,\nwith throughput increasing from 0.68 K tokens/s to 5.7 K\ntokens/s. We start with pipeline-parallel (PP) mapping and\nthen apply various levels of data-parallel (DP) mapping to\nfurther boost the throughput as the CENT system scales up.\nAs the number of devices increases, the throughput reaches\nintermittent plateaus at certain points. This is due to the\ninefficiency of distributing transformer blocks across CXL\ndevices. For example, 80 transformer blocks in the Llama2-\n70B model can be allocated to 40 devices, with two blocks\nper device. Expanding from 40 to 44 devices results in a\ndistribution of 1.8 blocks per device. Yet, dividing a single\nblock across multiple CXL devices introduces substantial\ninter-device communication overhead, ultimately reducing\nperformance. To mitigate this, we maintain the same block\ndistribution with 44 devices as with 40, leaving the remaining\n4 devices idle.\nThe scalability of CXL devices is constrained by two pri-\nmary factors: (1) The number of lanes and ports provided by\na CXL switch. For example, a commercial PCIe 5.0 switch\ncan accommodate up to 144 lanes and 72 ports [8]. (2) The\nmaximum power supply available for the server, such as\nthe DGX A100â€™s peak input power of 6.5 kW [ 73]. Due to\nthese constraints, the CENT system with a single switch can\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nDevice Count\nK Tokens / Seconds\nDevice Count\nDevice UtilizationCENT\n32 Devices\nCENT\n32 Devices\nFigure 19. CENT scalability study on Llama2-70B.\nsupport up to 64 devices per server. A larger number of de-\nvices can be driven by multi-socket CPUs or a memory pool\nimplementation facilitated by two levels of CXL switches.\n7.5 Generality\nLLMs exhibit similar architectures but differ in their specific\nimplementations of activation functions and positional en-\ncodings. CENT is designed to support a variety of activation\nfunctions, including GeLU [36], Swish [95], and their GLU\nvariants [99]. This versatility is achieved by decomposing\nthese functions into fundamental non-linear operations, such\nas sigmoid and tanh, which are supported through lookup\ntables, as well as through basic PIM and RISC-V operations.\nMoreover, CENT is capable of accommodating different types\nof positional embeddings, including both absolute [92] and\nrelative [102] implementations. The integration of general-\npurpose RISC-V cores within the CENT system opens up\npossibilities for further enhancements and optimizations of\nLLMs in the future.\n8 Related Work\nVarious ML accelerators and HW/SW co-designs have re-\ncently been proposed [11, 53, 62]. CXL memory expansion\ntechniques are also widely explored [2, 5, 30, 31, 44, 103]. Sec-\ntions 1 and 2 already discuss PIM and PNM related works.\nTransformer Accelerators. A variety of transformer accel-\nerators [34, 66, 93, 94] have been developed to enhance this\nprevalent ML architecture. TransPIM [117] accelerates infer-\nence of transformer encoders like BERT [16] by reducing data\nloading time with an efficient token-based dataflow. How-\never, decoder-only LLMâ€™s inference tasks present a unique\nchallenge due to their lower operational intensities, which\nhave been less investigated. Approaches like Sprint [ 112],\nOliVe [32], FABNet [23], and SpAtten [109] employ quantiza-\ntion, approximation, and pruning strategies, respectively,\naimed at reducing computations within the transformer\nblocks, which are orthogonal to CENT.\nCXL-Based NDP Accelerators. Samsungâ€™s CXL-PNM plat-\nform [51, 88] integrates an LLM inference accelerator in the\nCXL controller. CENT also integrates PIM memory chips\nwith PUs adjacent to DRAM banks, providing both higher\ninternal memory bandwidth and compute throughput than\nCXL-PNM. Beacon [ 39] explores near-data processing in\nboth DIMMs and CXL switches, with customized processing\nunits for accelerating genome sequencing analysis.\n9 Conclusion\nGiven the challenges posed by the low operational intensity\nand substantial memory capacity requirements of decoder-\nonly LLMs, we introduce CENT, utilizing PIM technology\nto facilitate the high internal memory bandwidth and CXL\nmemory expansion to ensure ample memory capacity. When\ncompared to GPU baselines with the maximum supported\nbatch sizes, CENT achieves 2.3Ã—higher throughput and con-\nsumes 2.3Ã—less energy. CENT also enables lower TCO and\ngenerates 5.2Ã—more tokens per dollar than GPUs.\nAcknowledgments\nWe thank the anonymous reviewers for their valuable feed-\nback. This work was generously supported by NSF CAREER-\n1652294, NSF-1908601 and Intel gift awards. SAFARI authors\nacknowledge support from the Semiconductor Research Cor-\nporation, ETH Future Computing Laboratory (EFCL), AI\nChip Center for Emerging Smart Systems Limited (ACCESS),\nand the European Unionâ€™s Horizon Programme for research\nand innovation under Grant Agreement No. 101047160.\nA Artifact Appendix\nA.1 Abstract\nThis document provides a concise guide for reproducing the\nmain performance, power, cost efficiency, and energy effi-\nciency results of this paper in Figures 12, 13, 14, and 15. The\ninstructions cover the steps required to clone the GitHub\nrepository, build the simulator, set up the necessary Python\npackages, execute the end-to-end simulation, process results,\nand generate figures. The trace generator, performance simu-\nlator, power model, automation scripts, expected results, and\ndetailed instructions are available in our GitHub repository.\nA.2 Artifact check-list (meta-information)\nâ€¢Program: C++ and Python.\nâ€¢Compilation: g++-11/12/13 or clang++-15.\nâ€¢Software: pandas, matplotlib, torch, and scipy Python\npackages.\nâ€¢Model: Llama2 7B, 13B, and 70B [106].\nâ€¢Metrics: latency, throughput (tokens/S), cost efficiency (to-\nkens/$), energy efficiency (tokens/J), and power.\nâ€¢Output: CSV and PDF files corresponding to Figures 12-15.\nâ€¢Experiments: PIM trace generation and simulation, and\nCENT power modeling.\nâ€¢How much disk space is required?: Approximately 100GB.\nâ€¢How much time is needed?: Approximately 24 hours on a\ndesktop and 8 12 hours on a server.\nâ€¢Publicly available?: Available on GitHub and Zenodo.\nâ€¢Code licenses: MIT License.\nâ€¢Work automation?: Automated by a few scripts.\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nA.3 Description\nThis artifact provides the necessary components to repro-\nduce the main results presented in Figures 12, 13, 14, and 15.\nIt includes a trace generator, AiM simulator, power model,\nfigure generator, and automation script. While these figures\nincorporate simulation results from CENT, they also rely\non a baseline GPU system featuring four Nvidia A100 80GB\nGPUs, as detailed in Table 4. Due to the high cost associated\nwith these servers, only the expected results for the GPU\nbaseline system are provided in the data directory.\nA.3.1 How to access. Clone the artifact from our GitHub\nrepository using the following command. Please do not for-\nget the --recursive flag to ensure that the AiM simulator\nis also cloned:\ngit clone -- recursive https :// github . com / Yufeng98 /\nCENT . git\nA.3.2 Software dependencies. AiM simulator requires\ng++-11/12/13 or clang++-15 for compilation. The Python\ninfrastructure requirespandas, matplotlib, torch, andscipy\npackages.\nA.3.3 Models. Section 6 shows that we evaluate three\nLlama2 models [106]. The model architecture and its PIM\nmapping are implemented in thecent_simulation/Llama.py\nscript. The model weights are required only for the functional\nsimulation of the PIM infrastructure. While the functional\nsimulator is available in our GitHub repository, the perfor-\nmance simulator and power model described in this appendix\ndo not model real values, as this does not impact the main\nresults. Consequently, the model weights and parameters\nare not required for this appendix.\nA.4 Installation\nBuilding AiM Simulator. To build the simulator, use the\nfollowing script:\ncd CENT / aim_simulator /\nmkdir build && cd build && cmake ..\nmake -j4\nSetting up Python Packages. Install the aforementioned\nPython packages. You can use the following script to create\na conda environment:\ncd CENT /\nconda create -n cent python =3.10 -y\nconda activate cent\npip install -r requirements . txt\nA.5 Experiment workflow\nWe provide scripts to facilitate the end-to-end reproduction\nof the results. The following steps outline the process.\nGenerate and Simulate the Traces. This step generates\nand simulates all required PIM traces. It also processes the\nsimulation logs, calculates individual latencies, and utilizes\nthe CENT power model to determine energy consumption\nand average power. Upon completion, the generated trace\nand simulation log files will be stored in thetrace directory,\nwhile the processed latency and power results can be found\nin cent_simulation/simulation_results.csv.\ncd CENT /\nbash remove_old_results .sh\ncd cent_simulation /\nbash simulation .sh [ NUM_THREADS ] [ SEQ_GAP ]\nNote: The argument [NUM_THREADS] should be set accord-\ning to the number of available parallel threads on your pro-\ncessor. For instance, 8 threads are recommended for desktop\nprocessors, while server processors can utilize 96 threads.\nThe argument [SEQ_GAP] determines the gap between\neach simulated token. Setting this value to one simulates\nevery token sequentially, requiring approximately 100GB of\ndisk space and taking around 24 hours on a processor with\n8 threads or 12 hours on a processor with 96 threads. To im-\nprove disk usage and reduce simulation time, the[SEQ_GAP]\nargument can be set to a larger value, such as 128. This con-\nfiguration simulates one out of every 128 tokens, processing\ntoken IDs of 128, 256, 384, and so on up to 4096.\nProcess the Results. This step processes the simulation\nresults and computes the latency, throughput, power, and\nenergy for the prefill, decoding, and end-to-end phases. After\nprocessing the results, this script stores them in this file:\ncent_simulation/processed_results.csv.\ncd CENT / cent_simulation /\nbash process_results .sh\nGenerate Figures. The following script generates Fig-\nures 12-15. This process utilizes the baseline GPU results,\navailable in the data directory, along with the processed re-\nsults. It computes the normalized results and generates both\na PDF file containing the figures and a CSV file with the\ncorresponding numerical data.\ncd CENT /\nbash generate_figures .sh\nA.6 Evaluation and expected results\nThe normalized results and the figures will be located in\nthe figure_source_data and figures directories. The ex-\npected results can be found in Figures 12- 15 or in the gener-\nated CSV and PDF files on our GitHub repository. Figures\nin the paper are generated using Microsoft Excel. To visu-\nalize the figures in the paperâ€™s format, copy the normalized\ndata from the CSV files to the Data sheet of the provided\nFigures.xlsx. Figures will be generated in the Figures sheet.\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\nReferences\n[1] Azure pricing calculator. URL: https://azure.microsoft.com/en-us/\npricing/calculator/.\n[2] Minseon Ahn, Andrew Chang, Donghun Lee, Jongmin Gim, Jungmin\nKim, Jaemin Jung, Oliver Rebholz, Vincent Pham, Krishna Malladi,\nand Yang Seok Ki. Enabling cxl memory expansion for in-memory\ndatabase management systems. InProceedings of the 18th International\nWorkshop on Data Management on New Hardware , pages 1â€“5, 2022.\n[3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy,\nFederico LebrÃ³n, and Sumit Sanghai. Gqa: Training generalized multi-\nquery transformer models from multi-head checkpoints, 2023. URL:\nhttps://arxiv.org/abs/2305.13245, arXiv:2305.13245.\n[4] Anthropic. Introducing the next generation of claude. URL: https:\n//www.anthropic.com/news/claude-3-family.\n[5] Moiz Arif, Kevin Assogba, M Mustafa Rafique, and Sudharshan\nVazhkudai. Exploiting CXL-based memory for distributed deep learn-\ning. In Proceedings of the 51st International Conference on Parallel\nProcessing, pages 1â€“11, 2022.\n[6] Thomas Atta-fosu. Llama 2 70b: An mlperf inference benchmark for\nlarge language models. URL: https://mlcommons.org/2024/03/mlperf-\nllama2-70b/.\n[7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie\nLi, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam\nManassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya\nRamesh. Improving image generation with better captions. Computer\nScience., 2(3):8, 2023. URL: https://cdn.openai.com/papers/dall-e-\n3.pdf.\n[8] Broadcom. 144-lane, 72-port, pci express gen 5.0 pex89144 express-\nfabric platform. URL: https://www.broadcom.com/products/pcie-\nswitches-bridges/expressfabric/gen5/pex89144.\n[9] Christopher Celio, Krste Asanovic, and David Patterson. The\nberkeley out-of-order machine (boom): An open-source industry-\ncompetitive, synthesizable, parameterized risc-v processor. URL:\nhttps://riscv.org/wp-content/uploads/2016/01/Wed1345-RISCV-\nWorkshop-3-BOOM.pdf.\n[10] Christopher Celio, Pi-Feng Chiu, Borivoje Nikolic, David A. Patterson,\nand Krste AsanoviÄ‡. BOOM v2: an open-source out-of-order RISC-\nV core. Technical Report UCB/EECS-2017-157, EECS Department,\nUniversity of California, Berkeley, Sep 2017. URL: http://www2.eecs.\nberkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html.\n[11] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss:\nAn energy-efficient reconfigurable accelerator for deep convolutional\nneural networks. IEEE journal of solid-state circuits , 52(1):127â€“138,\n2016.\n[12] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song\nHan, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context\nlarge language models. arXiv preprint arXiv:2309.12307 , 2023.\n[13] Fabrice Devaux. The true processing in memory accelerator. In 2019\nIEEE Hot Chips 31 Symposium (HCS) , pages 1â€“24. IEEE Computer\nSociety, 2019.\n[14] Alexandar Devic, Siddhartha Balakrishna Rai, Anand Sivasubrama-\nniam, Ameen Akel, Sean Eilert, and Justin Eno. To pim or not for\nemerging general purpose processing in ddr memory systems. In\nProceedings of the 49th Annual International Symposium on Computer\nArchitecture, pages 231â€“244, 2022.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 , 2018.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: pre-training of deep bidirectional transformers for language\nunderstanding. CoRR, abs/1810.04805, 2018. URL: http://arxiv.org/\nabs/1810.04805, arXiv:1810.04805.\n[17] dramexchange. Dram spot price. URL: https://www.dramexchange.\ncom/.\n[18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Ka-\ndian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAmy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783, 2024.\n[19] Afzal Ahmad Dylan Patel. The inference cost of search disruption â€“\nlarge language model cost analysis. URL: https://www.semianalysis.\ncom/p/the-inference-cost-of-search-disruption .\n[20] ebay. Nvidia tesla a100 80gb gpu sxm4 deep learning com-\nputing graphics card oem. URL: https://www.ebay.com/itm/\n126596600113?chn=ps&mkevt=1&mkcid=28&srsltid=AfmBOop8-\nDCL9WiHC15MU05ZikXFveIxl95uEuqd55d5LHBrMjRXNMiwSTg.\n[21] Mouser Electronics. Pci interface ic. URL: https://www.mouser.com/\nc/semiconductors/interface-ics/pci-interface-ic/ .\n[22] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear\nunits for neural network function approximation in reinforcement\nlearning, 2017. URL: https://arxiv.org/abs/1702.03118, arXiv:1702.\n03118.\n[23] Hongxiang Fan, Thomas Chau, Stylianos I Venieris, Royson Lee,\nAlexandros Kouris, Wayne Luk, Nicholas D Lane, and Mohamed S\nAbdelfattah. Adaptable Butterfly Accelerator for Attention-based\nNNs via Hardware and Algorithm Co-design. In 2022 55th IEEE/ACM\nInternational Symposium on Microarchitecture (MICRO) , pages 599â€“\n615. IEEE, 2022.\n[24] Amin Farmahini-Farahani, Jung Ho Ahn, Katherine Morrow, and\nNam Sung Kim. NDA: Near-DRAM acceleration architecture lever-\naging commodity DRAM devices and standard memory modules. In\n2015 IEEE 21st International Symposium on High Performance Com-\nputer Architecture (HPCA) , pages 283â€“295. IEEE, 2015.\n[25] Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Todd Mas-\nsengill, Ming Liu, Daniel Lo, Shlomi Alkalay, Michael Haselman,\nLogan Adams, Mahdi Ghandi, Stephen Heil, Prerak Patel, Adam\nSapek, Gabriel Weisz, Lisa Woods, Sitaram Lanka, Steven K. Reinhardt,\nAdrian M. Caulfield, Eric S. Chung, and Doug Burger. A configurable\ncloud-scale DNN processor for real-time AI. In 2018 ACM/IEEE 45th\nAnnual International Symposium on Computer Architecture (ISCA).\n2018. URL: https://doi.org/10.1109/isca.\n[26] Christina Giannoula, Ivan Fernandez, Juan GÃ³mez Luna, Nectarios\nKoziris, Georgios Goumas, and Onur Mutlu. Sparsep: Towards ef-\nficient sparse matrix vector multiplication on real processing-in-\nmemory architectures. Proceedings of the ACM on Measurement and\nAnalysis of Computing Systems , 6(1):1â€“49, 2022.\n[27] Juan GÃ³mez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula,\nGeraldo F Oliveira, and Onur Mutlu. Benchmarking a new paradigm:\nExperimental analysis and characterization of a real processing-in-\nmemory system. IEEE Access, 10:52565â€“52608, 2022.\n[28] Juan GÃ³mez-Luna, Yuxin Guo, Sylvan Brocard, Julien Legriel, Remy\nCimadomo, Geraldo F Oliveira, Gagandeep Singh, and Onur Mutlu.\nEvaluating machine learningworkloads on memory-centric comput-\ning systems. In 2023 IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS) , pages 35â€“49. IEEE, 2023.\n[29] Google. Our next-generation model: Gemini 1.5. URL:\nhttps://blog.google/technology/ai/google-gemini-next-generation-\nmodel-february-2024/ .\n[30] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee,\nand Myoungsoo Jung. Memory pooling with cxl.IEEE Micro, 43(2):48â€“\n57, 2023.\n[31] Donghyun Gouk, Sangwon Lee, Miryeong Kwon, and Myoungsoo\nJung. Direct access, High-Performance memory disaggregation with\nDirectCXL. In 2022 USENIX Annual Technical Conference (USENIX\nATC 22), pages 287â€“294, 2022.\n[32] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang,\nFan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. OliVe: Acceler-\nating Large Language Models via Hardware-friendly Outlier-Victim\nPair Quantization. In Proceedings of the 50th Annual International\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\nSymposium on Computer Architecture , pages 1â€“15, 2023.\n[33] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang,\nRunxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948 , 2025.\n[34] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soosung Kim, Hyunji Choi,\nSung Jun Jung, and Jae W Lee. ELSA: Hardware-software co-design\nfor efficient, lightweight self-attention mechanism in neural networks.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer\nArchitecture (ISCA), pages 692â€“705. IEEE, 2021.\n[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition , pages 770â€“778,\n2016.\n[36] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).\narXiv preprint arXiv:1606.08415 , 2016.\n[37] Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon\nLee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse\nPark. Neupims: Npu-pim heterogeneous acceleration for batched llm\ninferencing. In Proceedings of the 29th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, Volume 3 , ASPLOS â€™24, page 722â€“737, New York, NY, USA,\n2024. Association for Computing Machinery.doi:10.1145/3620666.\n3651380.\n[38] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu\nChen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le,\nYonghui Wu, and Zhifeng Chen. GPipe: efficient training of giant\nneural networks using pipeline parallelism . Curran Associates Inc.,\nRed Hook, NY, USA, 2019.\n[39] Wenqin Huangfu, Krishna T Malladi, Andrew Chang, and Yuan Xie.\nBEACON: Scalable Near-Data-Processing Accelerators for Genome\nAnalysis near Memory Pool with the CXL Support. In 2022 55th\nIEEE/ACM International Symposium on Microarchitecture (MICRO) ,\npages 727â€“743. IEEE, 2022.\n[40] Mohsen Imani, Saransh Gupta, Yeseong Kim, and Tajana Rosing.\nFloatpim: In-memory acceleration of deep neural network training\nwith high precision. In Proceedings of the 46th International Sympo-\nsium on Computer Architecture , pages 802â€“815, 2019.\n[41] Mohsen Imani, Saransh Gupta, and Tajana Rosing. Ultra-efficient\nprocessing in-memory for data intensive applications. In Proceedings\nof the 54th Annual Design Automation Conference 2017 , pages 1â€“6,\n2017.\n[42] Intel. Intel xeon gold 6430 processor, 60m cache, 2.10 ghz.\nURL: https://www.intel.com/content/www/us/en/products/\nsku/231737/intel-xeon-gold-6430-processor-60m-cache-2-10-\nghz/specifications.html.\n[43] Intel. Intel Â® xeonÂ® gold 6430 processor. URL: https://www.intel.\ncom/content/www/us/en/products/sku/231737/intel-xeon-gold-\n6430-processor-60m-cache-2-10-ghz/specifications.html .\n[44] Junhyeok Jang, Hanjin Choi, Hanyeoreum Bae, Seungjun Lee,\nMiryeong Kwon, and Myoungsoo Jung. CXL-ANNS: Software-\nHardware Collaborative Memory Disaggregation and Computation\nfor Billion-Scale Approximate Nearest Neighbor Search. In 2023\nUSENIX Annual Technical Conference (USENIX ATC 23) , pages 585â€“\n600, 2023.\n[45] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan,\nLifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian\nTowles, Clifford Young, Xiang Zhou, Zongwei Zhou, and David A\nPatterson. Tpu v4: An optically reconfigurable supercomputer for\nmachine learning with hardware support for embeddings. In Pro-\nceedings of the 50th Annual International Symposium on Computer\nArchitecture, ISCA â€™23, New York, NY, USA, 2023. Association for\nComputing Machinery. doi:10.1145/3579371.3589350.\n[46] Sanjay Kariyappa, Hsinyu Tsai, Katie Spoon, Stefano Ambrogio, Pri-\ntish Narayanan, Charles Mackin, An Chen, Moinuddin Qureshi, and\nGeoffrey W Burr. Noise-resilient DNN: Tolerating noise in PCM-\nbased AI accelerators via noise-aware training. IEEE Transactions on\nElectron Devices, 68(9):4356â€“4362, 2021.\n[47] Enkelejda Kasneci, Kathrin SeÃŸler, Stefan KÃ¼chemann, Maria Bannert,\nDaryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan\nGÃ¼nnemann, Eyke HÃ¼llermeier, Stephan Krusche, Gitta Kutyniok,\nTilman Michaeli, Claudia Nerdel, JÃ¼rgen Pfeffer, Oleksandra Poquet,\nMichael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen\nWeller, Jochen Kuhn, and Gjergji Kasneci. ChatGPT for good? On\nopportunities and challenges of large language models for education.\nLearning and individual differences , 103:102274, 2023.\n[48] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas\nChandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia,\nHsien-Hsin S Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim\nNaumov, Martin Schatz, Mikhail Smelyanskiy, Xiaodong Wang, Bran-\ndon Reagen, Carole-Jean Wu, Mark Hempstead, and Xuan Zhang. Rec-\nnmp: Accelerating personalized recommendation with near-memory\nprocessing. In 2020 ACM/IEEE 47th Annual International Symposium\non Computer Architecture (ISCA) , pages 790â€“803. IEEE, 2020.\n[49] Moein Khazraee, Lu Zhang, Luis Vega, and Michael Bedford Taylor.\nMoonwalk: Nre optimization in asic clouds.ACM SIGARCH Computer\nArchitecture News, 45(1):511â€“526, 2017.\n[50] Jin Hyun Kim, Shin-Haeng Kang, Sukhan Lee, Hyeonsu Kim, Yuh-\nwan Ro, Seungwon Lee, David Wang, Jihyun Choi, Jinin So, YeonGon\nCho, Kyomin Sohn, and Nam Sung Kim. Aquabolt-XL HBM2-PIM,\nLPDDR5-PIM with in-memory processing, and AXDIMM with accel-\neration buffer. IEEE Micro, 42(3):20â€“30, 2022.\n[51] Jin Hyun Kim, Yuhwan Ro, Jinin So, Sukhan Lee, Shin-haeng Kang,\nYeonGon Cho, Hyeonsu Kim, Byeongho Kim, Kyungsoo Kim, Sangsoo\nPark, Jin-Seong Kim, Sanghoon Cha, Won-Jo Lee, Jin Jung, Jong-\nGeon Lee, Jieun Lee, JoonHo Song, Seungwon Lee, Jeonghyeon Cho,\nJaehoon Yu, and Kyomin Sohn. Samsung PIM/PNM for Transfmer\nBased AI: Energy Efficiency on PIM/PNM Cluster. In 2023 IEEE Hot\nChips 35 Symposium (HCS) , pages 1â€“31. IEEE Computer Society, 2023.\n[52] Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong\nPark, Gi-Moon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park,\nKyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Nahsung Kim, Yongkee\nKwon, Vladimir Kornijcuk, Woojae Shin, Jongsoon Won, Minkyu Lee,\nHyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jaewook\nLee, Donguc Ko, Younggun Jun, Ilwoong Kim, Choungki Song, Ilkon\nKim, Chanwook Park, Seho Kim, Chunseok Jeong, Euicheol Lim,\nDongkyun Kim, Jieun Jang, Il Park, Junhyun Chun, and Joohwan Cho.\nA 1ynm 1.25v 8gb 16gb/s/pin gddr6-based accelerator-in-memory\nsupporting 1tflops mac operation and various activation functions\nfor deep learning application. IEEE Journal of Solid-State Circuits ,\n58(1):291â€“302, 2023. doi:10.1109/JSSC.2022.3200718.\n[53] Hyoukjun Kwon, Ananda Samajdar, and Tushar Krishna. Maeri:\nEnabling flexible dataflow mapping over dnn accelerators via recon-\nfigurable interconnects. ACM SIGPLAN Notices , 53(2):461â€“475, 2018.\n[54] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin\nZheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.\nEfficient memory management for large language model serving with\npagedattention. In Proceedings of the 29th Symposium on Operating\nSystems Principles , pages 611â€“626, 2023.\n[55] Yongkee Kwon, Guhyun Kim, Nahsung Kim, Woojae Shin, Jongsoon\nWon, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin,\nDayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan\nPark, Chanwook Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee,\nSeungyeong Park, Wonjun Lee, Seongju Lee, Kyuyoung Kim, Daehan\nKwon, Chunseok Jeong, John Kim, Euicheol Lim, and Junhyun Chun.\nMemory-centric computing with sk hynixâ€™s domain-specific memory.\nIn 2023 IEEE Hot Chips 35 Symposium (HCS) , pages 1â€“26, 2023. doi:\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\n10.1109/HCS59251.2023.10254717.\n[56] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jong-\nsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim,\nByeongju An, Jeongbin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park,\nChanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho\nKim, Daehan Kwon, Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joon-\nhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje\nPark, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee,\nMinyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Ki-\njoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, and Junhyun\nChun. System architecture and software stack for GDDR6-AiM. In\n2022 IEEE Hot Chips 34 Symposium (HCS) , pages 1â€“25. IEEE, 2022.\n[57] Young-Cheon Kwon, Suk Han Lee, Jaehoon Lee, Sang-Hyuk Kwon,\nJe Min Ryu, Jong-Pil Son, O Seongil, Hak-Soo Yu, Haesuk Lee,\nSoo Young Kim, Youngmin Cho, Jin Guk Kim, Jongyoon Choi, Hyun-\nSung Shin, Jin Kim, BengSeng Phuah, HyoungMin Kim, Myeong Jun\nSong, Ahn Choi, Daeho Kim, SooYoung Kim, Eun-Bong Kim, David\nWang, Shinhaeng Kang, Yuhwan Ro, Seungwoo Seo, JoonHo Song,\nJaeyoun Youn, Kyomin Sohn, and Nam Sung Kim. 25.4 a 20nm 6gb\nfunction-in-memory DRAM, based on HBM2 with a 1.2 tflops pro-\ngrammable computing unit using bank-level parallelism, for machine\nlearning applications. In 2021 IEEE International Solid-State Circuits\nConference (ISSCC), volume 64, pages 350â€“352. IEEE, 2021.\n[58] Donghun Lee, Jinin So, Minseon Ahn, Jong-Geon Lee, Jungmin\nKim, Jeonghyeon Cho, Rebholz Oliver, Vishnu Charan Thummala,\nRavi shankar JV, Sachin Suresh Upadhya, Donghun Lee, Jinin So, Min-\nseon Ahn, Jong-Geon Lee, Jungmin Kim, Jeonghyeon Cho, Rebholz\nOliver, Vishnu Charan Thummala, Ravi shankar JV, Sachin Suresh\nUpadhya, Mohammed Ibrahim Khan, and Jin Hyun Kim. Improving\nin-memory database operations with acceleration DIMM (AxDIMM).\nIn Proceedings of the 18th International Workshop on Data Management\non New Hardware , pages 1â€“9, 2022.\n[59] Melvin Lee. Using machine learning to increase yield and lower\npackaging costs. URL: https://semiengineering.com/using-machine-\nlearning-to-increase-yield-and-lower-packaging-costs/ .\n[60] Seongju Lee, Kyuyoung Kim, Sanghoon Oh, Joonhong Park, Gimoon\nHong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang,\nJungyeon Kim, Junyeol Jeon, Nahsung Kim, Yongkee Kwon, Kornijcuk\nVladimir, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo,\nHaerang Choi, Jaewook Lee, Donguc Ko, Younggun Jun, Keewon Cho,\nIlwoong Kim, Choungki Song, Chunseok Jeong, Daehan Kwon, Jieun\nJang, Il Park, Junhyun Chun, and Joohwan Cho. A 1ynm 1.25 v 8gb,\n16gb/s/pin gddr6-based accelerator-in-memory supporting 1tflops\nmac operation and various activation functions for deep-learning\napplications. In 2022 IEEE International Solid-State Circuits Conference\n(ISSCC), volume 65, pages 1â€“3. IEEE, 2022.\n[61] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zar-\ndoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee,\nIshwar Agarwal, Huaicheng Li, Daniel S. Berger, Lisa Hsu, Daniel\nErnst, Pantea Zardoshti, Stanko Novakovic, Monish Shah, Samir Ra-\njadnya, Scott Lee, Ishwar Agarwal, Mark D. Hill, Marcus Fontoura,\nand Ricardo Bianchini. Pond: CXL-based memory pooling systems\nfor cloud platforms. In Proceedings of the 28th ACM International\nConference on Architectural Support for Programming Languages and\nOperating Systems, Volume 2 , pages 574â€“587, 2023.\n[62] Youjie Li, Iou-Jen Liu, Yifan Yuan, Deming Chen, Alexander Schwing,\nand Jian Huang. Accelerating distributed reinforcement learning\nwith in-switch computing. In Proceedings of the 46th International\nSymposium on Computer Architecture , pages 279â€“291, 2019.\n[63] Compute Express Link â„¢. Specification. URL: https://www.\ncomputeexpresslink.org/.\n[64] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda\nLu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\net al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 ,\n2024.\n[65] Liu Liu, Jilan Lin, Zheng Qu, Yufei Ding, and Yuan Xie. Enmc: Extreme\nnear-memory classification via approximate screening. In MICRO-54:\n54th Annual IEEE/ACM International Symposium on Microarchitecture ,\npages 1309â€“1322, 2021.\n[66] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang,\nand Yun Liang. Sanger: A co-design framework for enabling sparse\nattention using reconfigurable architecture. InMICRO-54: 54th Annual\nIEEE/ACM International Symposium on Microarchitecture , pages 977â€“\n991, 2021.\n[67] Haocong Luo, Yahya Can TuÄŸrul, F BostancÄ±, Ataberk Olgun, A Giray\nYaÄŸlÄ±kÃ§Ä±, and Onur Mutlu. Ramulator 2.0: A Modern, Modular, and\nExtensible DRAM Simulator. arXiv preprint arXiv:2308.11030 , 2023.\n[68] Siming Ma, David Brooks, and Gu-Yeon Wei. A Binary-activation,\nMulti-level Weight RNN and Training Algorithm for ADC-/DAC-\nfree and Noise-resilient Processing-in-memory Inference with eNVM.\nIEEE Transactions on Emerging Topics in Computing , 2023.\n[69] Micron. Dram power calculator. URL: https://www.micron.com/\nsupport/tools-and-utilities/power-calc .\n[70] Jowi Morales. Nvidia shipped 3.76m data center gpus in 2023. URL:\nhttps://www.tomshardware.com/tech-industry/nvidia-shipped-\n376m-data-center-gpus-in-2023-dominates-business-with-98-\nrevenue-share.\n[71] August Ning, Georgios Tziantzioulis, and David Wentzlaff. Supply\nchain aware computer architecture. In Proceedings of the 50th Annual\nInternational Symposium on Computer Architecture , pages 1â€“15, 2023.\n[72] Dimin Niu, Shuangchen Li, Yuhao Wang, Wei Han, Zhe Zhang, Yijin\nGuan, Tianchan Guan, Fei Sun, Fei Xue, Lide Duan, Yuanwei Fang,\nHongzhong Zheng, Xiping Jiang, Song Wang, Fengguo Zuo, Yubing\nWang, Bing Yu, Qiwei Ren, and Yuan Xie. 184QPS/W 64Mb/mm 2 3D\nlogic-to-DRAM hybrid bonding with process-near-memory engine\nfor recommendation system. In 2022 IEEE International Solid-State\nCircuits Conference (ISSCC) , volume 65, pages 1â€“3. IEEE, 2022.\n[73] NVIDIA. Introduction to the nvidia dgx a100 system. URL:\nhttps://docs.nvidia.com/dgx/dgxa100-user-guide/introduction-to-\ndgxa100.html#power-specifications.\n[74] NVIDIA. Nvidia a100 tensor core gpu. URL: https://www.nvidia.\ncom/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-\na100-datasheet-us-nvidia-1758950-r4-web.pdf .\n[75] Nvidia. Nvidia hgx a100, the most powerful end-to-end ai supercom-\nputing platform. URL: https://www.nvidia.com/content/dam/en-\nzz/Solutions/Data-Center/HGX/a100-80gb-hgx-a100-datasheet-\nus-nvidia-1485640-r6-web.pdf .\n[76] Nvidia. Nvlink and nvlink switch. URL: https://www.nvidia.com/en-\nus/data-center/nvlink/.\n[77] NVIDIA. Ptx isa. URL: https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.\npdf.\n[78] Mike Oâ€™Connor, Niladrish Chatterjee, Donghyuk Lee, John Wilson,\nAditya Agrawal, Stephen W Keckler, and William J Dally. Fine-\ngrained dram: Energy-efficient dram for extreme bandwidth systems.\nIn Proceedings of the 50th Annual IEEE/ACM International Symposium\non Microarchitecture, pages 41â€“54, 2017.\n[79] U.S. Bureau of Labor Statistics. Average energy prices for\nthe united states, regions, census divisions, and selected metro-\npolitan areas. URL: https://www.bls.gov/regions/midwest/data/\naverageenergyprices_selectedareas_table.htm.\n[80] Geraldo F Oliveira, Juan GÃ³mez-Luna, Saugata Ghose, Amirali\nBoroumand, and Onur Mutlu. Accelerating neural network inference\nwith processing-in-dram: From the edge to the cloud. IEEE Micro ,\n42(6):25â€“38, 2022.\n[81] OpenAI. Gpt-4 turbo and gpt-4. URL: https://platform.openai.com/\ndocs/models/gpt-4-turbo-and-gpt-4 .\n[82] OpenAI. Learning to reason with llms. URL: https://openai.com/\nindex/learning-to-reason-with-llms/ .\nPIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference ASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands\n[83] OpenAI. Video generation models as world simulators. URL:\nhttps://openai.com/research/video-generation-models-as-world-\nsimulators.\n[84] OpenAI. GPT-4 Technical Report, 2023. arXiv:2303.08774.\n[85] Chet Palesko, Amy Palesko, and E Jan Vardaman. Cost and yield\nanalysis of multi-die packaging using 2.5 d technology compared to\nfan-out wafer level packaging. In Proceedings of the 5th Electronics\nSystem-integration Technology Conference (ESTC) , pages 1â€“5. IEEE,\n2014.\n[86] Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael Jaemin Kim,\nYongsuk Kwon, Nam Sung Kim, and Jung Ho Ahn. Attacc! unleashing\nthe power of pim for batched transformer-based generative model\ninference. In Proceedings of the 29th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, Volume 2 , ASPLOS â€™24, page 103â€“119, New York, NY, USA,\n2024. Association for Computing Machinery.doi:10.1145/3620665.\n3640422.\n[87] Jaehyun Park, Byeongho Kim, Sungmin Yun, Eojin Lee, Minsoo Rhu,\nand Jung Ho Ahn. Trim: Enhancing processor-memory interfaces\nwith scalable tensor reduction in memory. In MICRO-54: 54th Annual\nIEEE/ACM International Symposium on Microarchitecture , pages 268â€“\n281, 2021.\n[88] Sang-Soo Park, KyungSoo Kim, Jinin So, Jin Jung, Jonggeon Lee, Ky-\noungwan Woo, Nayeon Kim, Younghyun Lee, Hyungyo Kim, Yong-\nsuk Kwon, Jinhyun Kim, Jieun Lee, YeonGon Cho, Yongmin Tai,\nJeonghyeon Cho, Hoyoung Song, Jung Ho Ahn, and Nam Sung Kim.\nAn LPDDR-based CXL-PNM Platform for TCO-efficient Inference of\nTransformer-based Large Language Models. In 2024 IEEE Interna-\ntional Symposium on High-Performance Computer Architecture (HPCA) ,\npages 970â€“982. IEEE, 2024.\n[89] Dylan Patel. Nvidia ada lovelace leaked specifications, die sizes,\narchitecture, cost, and performance analysis. URL: https://www.\nsemianalysis.com/p/nvidia-ada-lovelace-leaked-specifications .\n[90] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, ÃÃ±igo\nGoiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient gen-\nerative llm inference using phase splitting. In 2024 ACM/IEEE 51st\nAnnual International Symposium on Computer Architecture (ISCA) ,\npages 118â€“132. IEEE, 2024.\n[91] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh\nSinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao\nChuang, et al. Movie gen: A cast of media foundation models. arXiv\npreprint arXiv:2410.13720, 2024.\n[92] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long:\nAttention with linear biases enables input length extrapolation.arXiv\npreprint arXiv:2108.12409, 2021.\n[93] Yubin Qin, Yang Wang, Dazheng Deng, Zhiren Zhao, Xiaolong Yang,\nLeibo Liu, Shaojun Wei, Yang Hu, and Shouyi Yin. FACT: FFN-\nAttention Co-optimized Transformer Architecture with Eager Cor-\nrelation Prediction. In Proceedings of the 50th Annual International\nSymposium on Computer Architecture , pages 1â€“14, 2023.\n[94] Zheng Qu, Liu Liu, Fengbin Tu, Zhaodong Chen, Yufei Ding, and Yuan\nXie. Dota: detect and omit weak attentions for scalable transformer\nacceleration. In Proceedings of the 27th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, pages 14â€“26, 2022.\n[95] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for\nactivation functions. arXiv preprint arXiv:1710.05941 , 2017.\n[96] Elaheh Sadredini, Reza Rahimi, Marzieh Lenjani, Mircea Stan, and\nKevin Skadron. Impala: Algorithm/architecture co-design for in-\nmemory multi-stride pattern matching. In 2020 IEEE international\nsymposium on high performance computer architecture (HPCA) , pages\n86â€“98. IEEE, 2020.\n[97] Samsung. 8gb gddr6 sgram c-die. URL: https://datasheet.lcsc.com/\nlcsc/2204251615_Samsung-K4Z80325BC-HC14_C2920181.pdf .\n[98] Kiwoom Securities. Generative ai winds in memory semiconduc-\ntors, total demand for server drams is declining. URL: https://www.\nbusinesspost.co.kr/BP?command=article_view&num=316574.\n[99] Noam Shazeer. Glu variants improve transformer. arXiv preprint\narXiv:2002.05202, 2020.\n[100] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-\nbillion parameter language models using model parallelism. arXiv\npreprint arXiv:1909.08053, 2019.\n[101] Aaron Stillmaker and Bevan Baas. Scaling equations for the accu-\nrate prediction of cmos device performance from 180nm to 7nm.\nIntegration, 58:74â€“81, 2017. URL: https://www.sciencedirect.com/\nscience/article/pii/S0167926017300755, doi:10.1016/j.vlsi.2017.\n02.002.\n[102] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Ro-\nFormer: Enhanced Transformer with Rotary Position Embedding.\narXiv preprint arXiv:2104.09864 , 2021.\n[103] Yan Sun, Yifan Yuan, Zeduo Yu, Reese Kuper, Ipoom Jeong, Ren Wang,\nand Nam Sung Kim. Demystifying CXL Memory with Genuine CXL-\nReady Systems and Devices. arXiv preprint arXiv:2303.15375 , 2023.\n[104] Synopsys. Design compiler. concurrent timing, area, power, and test\noptimization. URL: https://www.synopsys.com/implementation-and-\nsignoff/rtl-synthesis-test/dc-ultra.html .\n[105] ShareGPT Team. Sharegpt. URL: https://sharegpt.com/.\n[106] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,\nAndrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xi-\nang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela\nFan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert\nStojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 ,\n2023.\n[107] UPMEM. Accelerating compute by cramming it into dram mem-\nory. URL: https://www.upmem.com/nextplatform-com-2019-10-03-\naccelerating-compute-by-cramming-it-into-dram/ .\n[108] Stavros Volos. Memory systems and interconnects for scale-out\nservers. Technical report, EPFL, 2015.\n[109] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse\nattention architecture with cascade token and head pruning. In\n2021 IEEE International Symposium on High-Performance Computer\nArchitecture (HPCA), pages 97â€“110. IEEE, 2021.\n[110] Wikipedia. Die shot of the tu104 gpu used in rtx 2080 cards.\nURL: https://en.wikipedia.org/wiki/Turing_(microarchitecture)\n#/media/File:Nvidia@12nm@Turing@TU104@GeForce_\nRTX_2080@S_TAIWAN_1841A1_PKYN44.000_TU104-400-\nA1_DSCx7_poly@5xExt.jpg.\n[111] XAI. Open release of grok-1. URL: https://x.ai/blog/grok-os.\n[112] Amir Yazdanbakhsh, Ashkan Moradifirouzabadi, Zheng Li, and Mingu\nKang. Sparse attention acceleration with synergistic in-memory prun-\ning and on-chip recomputation. In 2022 55th IEEE/ACM International\nSymposium on Microarchitecture (MICRO) , pages 744â€“762. IEEE, 2022.\n[113] Biao Zhang and Rico Sennrich. Root mean square layer normalization.\nAdvances in Neural Information Processing Systems , 32, 2019.\nASPLOS â€™25, March 30-April 3, 2025, Rotterdam, Netherlands Yufeng Gu and Alireza Khadem et al.\n[114] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya\nChen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Vic-\ntoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke\nZettlemoyer. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068 , 2022.\n[115] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng\nChen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo,\nEric P. Xing, Joseph E. Gonzalez, and Ion Stoica. Alpa: Automat-\ning inter- and Intra-Operator parallelism for distributed deep learn-\ning. In 16th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 22) , pages 559â€“578, Carlsbad, CA, July\n2022. USENIX Association. URL: https://www.usenix.org/conference/\nosdi22/presentation/zheng-lianmin.\n[116] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xu-\nanzhe Liu, Xin Jin, and Hao Zhang. Distserve: Disaggregating prefill\nand decoding for goodput-optimized large language model serving.\narXiv preprint arXiv:2401.09670 , 2024.\n[117] Minxuan Zhou, Weihong Xu, Jaeyoung Kang, and Tajana Rosing.\nTranspim: A memory-based acceleration via software-hardware co-\ndesign for transformer. In 2022 IEEE International Symposium on\nHigh-Performance Computer Architecture (HPCA) , pages 1071â€“1085.\nIEEE, 2022.\n[118] Farzaneh Zokaee, Fan Chen, Guangyu Sun, and Lei Jiang. Sky-Sorter:\nA Processing-in-Memory Architecture for Large-Scale Sorting. IEEE\nTransactions on Computers , 72(2):480â€“493, 2022.\nReceived 24 June 2024; revised 2 October 2024; accepted 27 January\n2025",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.860276460647583
    },
    {
      "name": "Inference",
      "score": 0.5841017961502075
    },
    {
      "name": "Language model",
      "score": 0.5013415813446045
    },
    {
      "name": "Encoder",
      "score": 0.4930705726146698
    },
    {
      "name": "Software deployment",
      "score": 0.48407331109046936
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.45931676030158997
    },
    {
      "name": "Context (archaeology)",
      "score": 0.44197148084640503
    },
    {
      "name": "Throughput",
      "score": 0.4367714822292328
    },
    {
      "name": "Parallel computing",
      "score": 0.3782998323440552
    },
    {
      "name": "Computer network",
      "score": 0.2332758903503418
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19777411222457886
    },
    {
      "name": "Wireless",
      "score": 0.15328481793403625
    },
    {
      "name": "Telecommunications",
      "score": 0.10748636722564697
    },
    {
      "name": "Operating system",
      "score": 0.1029873788356781
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}