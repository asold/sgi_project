{
  "title": "TransGOP: Transformer-Based Gaze Object Prediction",
  "url": "https://openalex.org/W4393147659",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5043220498",
      "name": "Binglu Wang",
      "affiliations": [
        "Beijing Institute of Technology",
        "Xi'an University of Architecture and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5073915205",
      "name": "Chenxi Guo",
      "affiliations": [
        "Xi'an University of Architecture and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5113151477",
      "name": "Yang Jin",
      "affiliations": [
        "Xi'an University of Architecture and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5026129330",
      "name": "Haisheng Xia",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100377352",
      "name": "Nian Liu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6851961291",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2897890508",
    "https://openalex.org/W6774653587",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W6761108903",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W6631782140",
    "https://openalex.org/W6843539342",
    "https://openalex.org/W3001625344",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W6630406260",
    "https://openalex.org/W6647038853",
    "https://openalex.org/W6786014021",
    "https://openalex.org/W6719625255",
    "https://openalex.org/W109919546",
    "https://openalex.org/W6753494528",
    "https://openalex.org/W2298427086",
    "https://openalex.org/W4221146420",
    "https://openalex.org/W4221148504",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6800709815",
    "https://openalex.org/W6753668856",
    "https://openalex.org/W6746871258",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W3164843090",
    "https://openalex.org/W4225464511",
    "https://openalex.org/W3199093552",
    "https://openalex.org/W3090112028",
    "https://openalex.org/W4313287871",
    "https://openalex.org/W6795901243",
    "https://openalex.org/W4221146106",
    "https://openalex.org/W6786507698",
    "https://openalex.org/W2765629358",
    "https://openalex.org/W6769864171",
    "https://openalex.org/W2914868659",
    "https://openalex.org/W6679336668",
    "https://openalex.org/W4385261800",
    "https://openalex.org/W4385805120",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W2130313210",
    "https://openalex.org/W4313061846",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W3102701618",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W2129987527",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W4313447521",
    "https://openalex.org/W3106262690",
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2953126257",
    "https://openalex.org/W4322747093",
    "https://openalex.org/W4391743392",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W4312312588",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W2947492009",
    "https://openalex.org/W4312446817",
    "https://openalex.org/W4312996499",
    "https://openalex.org/W4384918781",
    "https://openalex.org/W3035735638",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2884915206",
    "https://openalex.org/W4245050582",
    "https://openalex.org/W4281944033",
    "https://openalex.org/W3167597877",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W2963985934",
    "https://openalex.org/W1510835000",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963927307",
    "https://openalex.org/W4312510569",
    "https://openalex.org/W2984401364"
  ],
  "abstract": "Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.",
  "full_text": "TransGOP: Transformer-Based Gaze Object Prediction\nBinglu Wang1, 2, Chenxi Guo1, Yang Jin1, Haisheng Xia3, Nian Liu4*\n1Xi’an University of Architecture and Technology\n2Beijing Institute of Technology\n3University of Science and Technology of China\n4Mohamed bin Zayed University of Artificial Intelligence\n{wbl921129, guochenxix, jin91999}@gmail.com, hsxia@ustc.edu.cn, liunian228@gmail.com\nAbstract\nGaze object prediction aims to predict the location and cat-\negory of the object that is watched by a human. Previ-\nous gaze object prediction works use CNN-based object de-\ntectors to predict the object’s location. However, we find\nthat Transformer-based object detectors can predict more\naccurate object location for dense objects in retail scenar-\nios. Moreover, the long-distance modeling capability of the\nTransformer can help to build relationships between the hu-\nman head and the gaze object, which is important for the\nGOP task. To this end, this paper introduces Transformer\ninto the fields of gaze object prediction and proposes an\nend-to-end Transformer-based gaze object prediction method\nnamed TransGOP. Specifically, TransGOP uses an off-the-\nshelf Transformer-based object detector to detect the loca-\ntion of objects and designs a Transformer-based gaze autoen-\ncoder in the gaze regressor to establish long-distance gaze re-\nlationships. Moreover, to improve gaze heatmap regression,\nwe propose an object-to-gaze cross-attention mechanism to\nlet the queries of the gaze autoencoder learn the global-\nmemory position knowledge from the object detector. Finally,\nto make the whole framework end-to-end trained, we propose\na Gaze Box loss to jointly optimize the object detector and\ngaze regressor by enhancing the gaze heatmap energy in the\nbox of the gaze object. Extensive experiments on the GOO-\nSynth and GOO-Real datasets demonstrate that our Trans-\nGOP achieves state-of-the-art performance on all tracks, i.e.,\nobject detection, gaze estimation, and gaze object predic-\ntion. Our code will be available at https://github.com/chenxi-\nGuo/TransGOP.git.\nIntroduction\nPredicting where a person is looking at a screen or an object\nhas important applications in the real world, such as detect-\ning goods of interest to people in retail scenarios, and fatigue\ndetection is possible in autonomous driving, and in the med-\nical field, it can help some patients with mobility or speech\nimpairment to express their intentions (Kleinke 1986; Land\nand Tatler 2009; Yu et al. 2022; de Belen et al. 2023).\nPrevious research on gaze-related topics has primar-\nily focused on gaze estimation (GE) task, which predicts\n*Nian Liu is the corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n（a）Detection results \nof  GaTector\n（b）Detection \nresults of  TransGOP\nFigure 1: Object Detection results of GaTector (Wang et al.\n2022a) (a) and our TransGOP (b) when IoU threshold is\n0.75. TransGOP predicts the object location more accurately\nthan the GaTector, especially for the objects that are close to\nhuman or goods shelves.\nheatmaps or points showing the location of human gaze ob-\njects. However, GE models fall short of identifying the ex-\nact location and category of the gaze object. Since the gaze\nobject is closely linked to human behavior, it is important\nto identify the object due to its significant practical impli-\ncations. Tomas et al. (Tomas et al. 2021) proposed the\ngaze object prediction (GOP) task, which aims to predict\nboth the location and category of the human gaze object.\nTo facilitate research in the GOP task, they introduced the\nfirst dataset, the GOO dataset, which consists of images of\npeople looking at different objects in retail scenarios. Com-\npared to GE, the GOP task is more challenging as it re-\nquires richer information for model predictions. Wang et al.\n(Wang et al. 2022a) proposed GaTector, the first model for\nthe GOP task, which combines a CNN-based object detec-\ntor (YOLOv4 (Bochkovskiy, Wang, and Liao 2020)) with a\nCNN-based gaze prediction branch (Chong et al. 2020). Ga-\nTector was trained and evaluated on the GOO dataset and\nachieved state-of-the-art performance on the GOP task.\nThe GOP task is highly related to the object detection\ntask, as both tasks aim to accurately localize objects in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10180\nimages. The accuracy of GOP is highly dependent on the\naccuracy of the object detector. CNN-based object detec-\ntion has been extensively studied and achieved good perfor-\nmance scores (Bochkovskiy, Wang, and Liao 2020; Wang,\nBochkovskiy, and Liao 2023). However, in recent years,\nTransformer-based object detection methods have received\nincreasing research attention (Carion et al. 2020; Li et al.\n2022b). In this paper, we found that Transformer-based ob-\nject detectors perform better than CNN-based object detec-\ntors in object-dense retail scenes (see Fig. 1). Transformer-\nbased object detectors are more effective at handling dense\nobject scenes due to the attention mechanism, which pro-\nvides them with long-range modeling capability. There is no\nresearch on introducing Transformer-based methods into the\nGOP task. Compared to traditional methods, we believe that\nthe capacity to capture long-range feature dependencies of\nTransformer methods could establish a better attention re-\nlationship between the human and the gaze object, further\nimproving prediction accuracy.\nIn this paper, we introduce TransGOP, an end-to-end\nmethod for GOP tasks based on the Transformer architec-\nture. TransGOP comprises two branches, an object detector\nand a gaze regressor, as shown in Fig. 2 (a). The object de-\ntector takes the entire image as input and detects the location\nand category of objects using an off-the-shelf Transformer-\nbased object detector. The gaze regressor takes the head im-\nage and scene image as input and predicts the gaze heatmap.\nSpecifically, in the gaze regressor, we design a Transformer-\nbased gaze autoencoder to establish long-range dependen-\ncies of gaze-related features. To improve gaze heatmap re-\ngression, in the gaze autoencoder, we propose an object-to-\ngaze attention mechanism that enables the gaze autoencoder\nqueries to learn global-memory position knowledge from the\nobject detector (see in Fig. 2 (b)).\nTo facilitate end-to-end training of the model, we pro-\npose a Gaze Box loss that jointly optimizes the object de-\ntector and gaze regressor. As illustrated in Fig. 2 (c), our\ngaze box loss can further optimize the generation of gaze\nheatmaps through GT gaze boxes so that they can reflect\nobject information. Moreover, the gradient backpropagation\nof gaze box loss is propagated to the object detector back-\nbone through scene features in the gaze fusion module to\nachieve joint optimization. Since the Transformer architec-\nture can make the object detector end-to-end trained without\nany post-processing operation, our TransGOP would be the\nfirst end-to-end approach for the GOP task. Results on the\nGOO-Synth and GOO-Real datasets demonstrate that Trans-\nGOP outperforms existing state-of-the-art GOP methods by\nsignificant margins. To summarize, the contribution of this\npaper is fourfold:\n• We introduce the Transformer mechanism into the gaze\nobject prediction task and propose an end-to-end model\nTransGOP.\n• We propose an object-to-gaze cross-attention mechanism\nto establish the relationship between the object detector\nand gaze regressor.\n• We propose a gaze box loss to jointly optimize the object\ndetector and gaze regressor.\n• Extensive experiments on the GOO-Synth and GOO-\nReal datasets show that TransGOP outperforms the state-\nof-the-art GOP methods.\nRelated Works\nGaze Estimation\nGaze estimation has important applications in many\nfields (Kleinke 1986; Land and Tatler 2009), which aims to\nestimate where are people looking by taking eye or face im-\nages as input (Yin et al. 2022; Balim et al. 2023). Gaze point\nestimation (Krafka et al. 2016; He et al. 2019), gaze follow-\ning (Judd et al. 2009; Leifman et al. 2017; Zhao et al. 2020;\nZhu and Ji 2005), and 3D gaze estimation (Zhang et al. 2015,\n2017; Cheng, Lu, and Zhang 2018; Park, Spurr, and Hilliges\n2018) are sub-tasks of gaze estimation. The gaze-following\ntask was first proposed by Recasens et al. (Recasens et al.\n2015) who also released the dataset publicly. Chong et al.\n(Chong et al. 2020) proposed a cross-frame gaze-following\nmodel for video, which achieved a significant score metric.\nRecently, some GE works (Cheng and Lu 2021; Guo, Hu,\nand Liu 2022; Tu et al. 2022; Yu et al. 2021) introduced the\ntransformer model. Tuet al.(Tu et al. 2022) and Toniniet al.\n(Tonini et al. 2023) proposed transformer-based end-to-end\nGE model, which aims to estimate the human gaze heatmap\nbut can not detect the bounding boxes and the categories of\nthe gaze objects.\nGaze Object Prediction\nDifferent from the GE task, the GOP task predicts not only\nthe human gaze heatmap but also the location and category\nof the gaze object. The GOP task is first proposed by Tomas\net al. (Tomas et al. 2021), who also contribute a novel\ndataset, i.e., the GOO dataset which consists of a large num-\nber of synthetic images (GOO-Synth dataset) and a smaller\nnumber of real images (GOO-Real dataset) of people gaz-\ning object in a retail environment. However, Tomas et al.\ndo not propose a model to resolve the GOP problem. After-\nward, Wang et al. (Wang et al. 2022a) propose the first uni-\nfied framework GaTector for the GOP task which utilizes a\nCNN-based object detector (Bochkovskiy, Wang, and Liao\n2020) to detect the objects and design another CNN-based\ngaze prediction branch (Chong et al. 2020) to predict the\ngaze heatmap. To further improve the performance of gaze\nestimation, GaTector proposed an energy aggregation loss to\nsupervise the range of gaze heatmaps.\nIn this paper, we want to propose a Transformer-based\nGOP model due to the long-distance modeling capability\nof the Transformer can help to build human-object relation-\nships, which can improve the performance of the GOP task.\nObject Detection\nObject detection (OD) is a fundamental task in computer\nvision. CNN-based object detectors can be classified into\nanchor-based (He et al. 2015; Girshick 2015; Law and Deng\n2018; Duan et al. 2019; Bochkovskiy, Wang, and Liao 2020)\nand anchor-free (Huang et al. 2015; Yang et al. 2020; Zhou,\nZhuo, and Krahenbuhl 2019; Tian et al. 2022; Kong et al.\n2020) methods.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10181\nHead Location\nGaze Fusion \nScene\nImage\nEncoder\nLayers \n× N\nDecoder\nLayers \n× M\nHead\nImage\nObject \nPositional \nEmbeddings\nGaze\nPositional\nEmbeddings\nEncoder\nLayer\nDecoder\nLayer\nBackbone\nBackbone\nFlatten\nObject Box\nGaze Heatmap\nObject-to-Gaze\nCross-Attention\n(a) Object Detector\n(b) Gaze Regressor\n(c) Optimization\nDetection\nHead\nRegression\nHead\nℒ￿￿￿\nℒ￿￿￿￿\nℒ￿￿\nGaze Autoencoder\nFigure 2: Overview framework of our TransGOP method. (a) The object detector in TransGOP is the of-the-shelf Transformer-\nbased object detection method that detects object location and category. (b) The gaze regressor feeds the fused feature into the\nTransformer-based gaze autoencoder to predict the gaze heatmap. (c) The optimization of TransGOP consists of three parts: the\nobject detection loss Ldet for optimizing the object detector, the gaze regression loss Lgaze for optimizing the gaze regressor,\nand the gaze box loss Lgb to jointly optimize the object detector and gaze regressor.\nRecently, Transformer has also been widely applied to re-\nsolve the object detection task (Vaswani et al. 2017; Dai\net al. 2021; Fang et al. 2021; Liu et al. 2022b; Li et al.\n2022b). DETR (Carion et al. 2020) is the first Transformer-\nbased method that treats object detection as a set sequence\nprediction problem. Many subsequent DETR series meth-\nods (Zhu et al. 2020; Meng et al. 2021; Wang et al. 2022b;\nLiu et al. 2022a; Li et al. 2022a; Zheng et al. 2023) are try-\ning to resolve the problem of DETR about slow convergence\nand low precision. DINO (Zhang et al. 2022) achieves better\nperformance and faster speed of convergence than previous\nDETR-like models by using contrastive denoising training\nmethods and excellent query design strategy.\nIn this paper, we find that Transformer-based object de-\ntectors can predict more accurately than CNN-based object\ndetectors, especially in dense object retail scenarios, so we\nwant to propose a Transformer-based method for the GOP\ntask to better utilize this advantage.\nMethod\nGiven a scene image and a head image, our goal is to pre-\ndict the category, bounding box, and gaze heatmap for the\nhuman gaze object. In this section, we first present the over-\nall framework of TransGOP, then we introduce the object\ndetector and the gaze regressor in detail. Finally, we give a\ndetailed introduction to the proposed gaze box loss function.\nOverview\nAs illustrated in Fig. 2, the proposed TransGOP consists of\nan object detector and a gaze regressor. The object detec-\ntor is a Transformer-based object detection method, which\ntakes the whole scene image as input and predicts cate-\ngories and locations for all objects. The gaze regressor has a\nTransformer-based gaze autoencoder, which takes the head\nimage, scene image, and head location map as input and\ngenerates queries with gaze information, which will be re-\ngressed to the gaze heatmap by the regression head. In in-\nference, the gaze object is determined by the value of gaze\nheatmap energy in the predicted object boxes. The overall\nloss function during the training process is defined as:\nL = Ldet + αLgaze + βLgb, (1)\nwhere α and β are the weights of the gaze heatmap loss and\nthe gaze box loss, respectively.\nFor the Transformer-based object detector, we directly use\nan existing method DINO (Zhang et al. 2022). The object\ndetector takes the whole image as input and aims to predict\nthe object category and bounding box. The loss of the object\ndetector is denoted by Ldet.\nThis paper proposes a novel Transformer-based gaze re-\ngressor to predict the gaze heatmap. The gaze regressor first\nextracts the head feature of the person in the image, then\nthe fused feature the head feature, scene feature, and head\nlocation map as the input of the Transformer-based gaze au-\ntoencoder. The gaze heatmap is optimized by Lgaze.\nFinally, we propose a new gaze box loss function Lgb to\njointly optimize the object detector and the gaze regressor,\nand make the whole framework be trained end-to-end.\nTransformer-based Object Detector\nAs we illustrate in Fig. 2 (a), a DETR-like object detector\nusually consists of a backbone to extract semantic features,\nmultiple layers of Transformer encoders, multiple layers of\nTransformer decoders, and a prediction head. In this paper,\nwe use DINO (Zhang et al. 2022) as our object detector,\nwhich achieves remarkable results with a convergence speed\nsimilar to previous CNN-based methods, surpassing the ac-\ncuracy of other DETR series models.\nThe loss of our object detector Ldet consists of a classifi-\ncation loss and a box regression loss. Following the setting\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10182\n Self-Attention\nAdd & Norm\nFFN\nAdd & Norm\nEncoder of Gaze Autoencoder\nFusion Feature\nGaze Positional Embedings\nSelf-Attention\nAdd & Norm\nDecoder of Gaze Autoencoder \nObject to Gaze\nCross-Attention\n+ +\n++\nAdd & Norm\nFFN\nKey&Value from Object Detector\nAdd & Norm\nCross-Adapter\n+\n퐊￿￿￿￿ 퐐￿￿￿￿\n퐕￿￿￿￿\n￿￿\n퐕￿￿￿￿\n1 ×1 Conv\n퐊￿￿￿￿\n￿￿ 퐐￿￿￿￿\n￿￿\n퐕￿￿￿\n￿￿ 퐊￿￿￿\n￿￿\n퐐￿￿￿￿\n￿￿\n{퐊￿￿￿\n￿￿ , 퐕￿￿￿\n￿￿ }\n퐺ퟎ\nFigure 3: Details of the Transformer-based gaze autoencoder\nand the object-to-gaze cross-attention in the gaze regressor.\nof DINO, we employ focal loss (Lin et al. 2017) as the classi-\nfication loss to address the imbalanced positive and negative\nsamples and use L1 and GIoU loss (Rezatofighi et al. 2019)\nto supervise the regression of the predicted box.\nIt is notable that the key-value pairs {Ken\ndet, Ven\ndet ∈\nRD×M } from the object encoder are fed to the decoder layer\nof the gaze autoencoder to make the query in the gaze re-\ngressor decoder can perceive objects in the scene. Where\nD = 256 is the hidden size of tokens, and M = 1045 is\nthe sum of spatial scale for multi-scale features in DINO.\nMoreover, our proposed TransGOP can use any DETR-like\nmethod as the object detector, as long as it can provide keys\nand values from the encoder.\nTransformer-based Gaze Regressor\nThe gaze regressor aims to predict a gaze heatmap for the\nhuman gaze object. As illustrated in Fig. 2 (b), we use the\nhead image, scene image, and head location map as the input\nof the gaze regressor. The gaze regressor consists of a gaze\nbackbone, a gaze fusion module, a single-layer Transformer-\nbased gaze autoencoder, and a gaze predictor.\nInput. As illustrated in the left part of Fig. 2 (b), the\nscene image is resized into the size of 3 × H0 × W0, where\nH0, W0 = 224. The head location map is a binary map with\nthe same size as the scene image, where the value in the\ngaze box is 1 and 0 otherwise. The head image is obtained\nby cropping the scene image according to the head location.\nGaze feature fusion.In this paper, we use the gaze fusion\nmodule proposed in the GaTector (Wang et al. 2022a) to\nfuse features from the head image and scene image. The\nscene image and head image are fed into two independent\nResnet50 backbones to extract the salient feature of the\nscene and head direction feature, respectively. The head lo-\ncation map is fed into five convolutional layers to generate\nhead location features that have the same spatial resolution\nas image features (Wang et al. 2022a). Then, the head fea-\ntures and scene features are fused with the help of the head\nlocation features. Specifically, the gaze feature module first\nstacks the head feature with the head location map and feeds\nit into a linear layer to generate an attention map that en-\ncodes directional cues. By computing the inner product be-\ntween the attention map and the scene feature map, a fused\ngaze feature Ffuse ∈ RC×H×W is generated, which can en-\nhance the region that is highly related to the gaze behavior.\nTypical values we set are C = 256, H, W= 15.\nEncoder. Different from GaTector which directly predicts\nthe gaze heatmap from the fused feature, we propose a\nTransformer-based gaze autoencoder to build the long-range\nrelationship for better gaze heatmap prediction results. As\nillustrated in Fig. 3, a 1 × 1 convolution operation is used\nto reduce the channel dimension of the fused gaze feature\nFfuse from C to the hidden size D, and get a new fea-\nture map G0 ∈ RD×H×W . The spatial dimensions of G0\nare collapsed into one dimension and result in a D × HW\nfeature map. Then, we generate queries, keys, and values\n{Qgaze, Kgaze, Vgaze ∈ RD×HW } for the encoder, which\nconsists of a multi-head self-attention module and a feed-\nforward network (FFN). Gaze positional embeddings are\nalso added to the input of each attention layer.\nDecoder and Object-to-gaze cross-attention.The decoder\nlayer of the gaze autoencoder mainly consists of a self-\nattention block and an object-to-gaze cross-attention block.\nAs shown in Fig. 3, we use the encoded queries, keys, and\nvalues {Qen\ngaze, Ken\ngaze, Ven\ngaze ∈ RD×HW } as the input of\nthe self-attention block, gaze positional embeddings are also\nadded to queries and keys of each self-attention layer.\nTo improve gaze heatmap regression, this paper pro-\nposes an object-to-gaze cross-attention mechanism to make\ngaze autoencoder learn more accurate position information\nabout the gaze object. As illustrated in Fig. 3, we first fed\nkey-value pairs from the encoder of object detector, i.e.,\n{Ken\ndet, Ven\ndet}, into a cross-adapter module to optimizes the\nkeys and values {Kad\ndet, Vad\ndet} to provide more specific in-\nformation about the gaze object. {Kad\ndet, Vad\ndet} are used as\nthe input of the object-to-gaze cross-attention module in the\ngaze decoder layer. Moreover, the output queries from the\nself-attention module in the decoder—Q sa\ngaze—are used as\nthe input queries of the cross-attention module, and gaze\npositional embeddings are also added to Qsa\ngaze. We use\nthe object-to-gaze cross-attention mechanism to makeQsa\ngaze\nlearn global-memory position knowledge from the object\ndetector, so can help to predict a more accurate gaze heatmap\nfor the gaze object.\nGaze prediction. After the gaze autoencoder, we fed the\ndecoded features into a regression head to predict the gaze\nheatmap. In this paper, we use the same regression head as\nthe GaTector (Wang et al. 2022a) and generate ground truth\ngaze heatmaps T ∈ RHT×WT by Gaussian blurred gaze\npoints (px, py):\neT = 1\n2πσxσy\nexp\n\"\n−1\n2\n \n(x − qx)2\nσ2x\n+ (y − qy)2\nσ2y\n!#\n,\n(2)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10183\nGaze Box\nGaze  Heatmap ℒ￿￿ = 1 − 1\n푤 × ℎ ￿\n푖=1\n푤\n￿\n푗=1\nℎ\n퐌￿,￿\n푤\n퐌\nℎ\nFigure 4: Illustration of the gaze box loss.\nTi,j =\neTi,j\nmax(eT)\n, (3)\nwhere eT is the Gaussian-blurred heatmap and max(eT) is its\nmaximum value. σx and σy indicate the standard deviation,\nwhich we follow Chong et al. (Chong et al. 2020) to set\nσx = 3,σy = 3.HT and WT represent the height and width\nof the heatmap.\nSuppose the predicted gaze heatmaps is M ∈ RHT×WT,\nwe use the mean square betweenMand Tcalculate the gaze\nheatmap loss:\nLgaze = 1\nHT × WT\nHTX\ni=1\nWTX\nj=1\n(Mi,j − Ti,j)2. (4)\nGaze Box loss\nSince DETR-like object detectors can achieve end-to-end\ntraining without any extra post-processing operations, we\nwant to make our proposed Transformer-based gaze regres-\nsor can be trained together with the DETR-like object de-\ntector to obtain an end-to-end framework for the GOP task.\nIn this paper, we propose a gaze box loss to jointly optimize\nthe object detector and gaze regressor.\nIn previous work (Wang et al. 2022a), an energy aggre-\ngation loss was proposed to optimize the gaze heatmap re-\ngression process by supervising the overall distribution of\nthe heatmap to concentrate in the gaze box. However, we\nfind that the actual predicted gaze heatmap area is usually\nmuch larger than the area of the gaze box, indicating that\nthe energy aggregation loss has a very limited role in opti-\nmizing the gaze regressor. Moreover, because the gradient\nbackpropagation of the gaze heatmap will affect the object\ndetector backbone through the scene features in the gaze\nfusion, Table 6 illustrates that the energy aggregation loss\ncould cause a decrease in object detection performance.\nTo this end, we want to propose a novel loss to provide a\npositive effect for both the object detector and gaze regres-\nsor. As shown in Fig. 4, the proposed gaze box loss aims to\nfocus on improving the heatmap energy in the gaze box:\nLgb = 1− 1\nh × w\nx2X\ni=x1\ny2X\nj=y1\nMi,j. (5)\nwhere h, ware the height and width of the ground truth gaze\nbox. The gaze box loss aims to maximize the high-energy\nMethods mSoC\nmSoC50 mSoC75 mSoC95\nGaTector\n67.9 98.1 86.2 0.1\nTransGOP 92.8 99.0 98.5 51.9\nTable 1: Gaze object prediction results on GOO-Synth.\nMethods mSoC\nmSoC50 mSoC75 mSOC95\nNo Pre-train\nGaTector\n62.4 95.1 73.5 0.2\nTransGOP 82.6 98.3 93.5 15.3\nPre-trained on\nGOO-Synth\nGaTector\n71.2 97.5 88.7 0.4\nTransGOP 89.0 98.9 97.5 33.2\nTable 2: Gaze object prediction results on GOO-Real.\nvalue part of the predicted gaze heatmap distributed within\nthe gaze box, to accurately reflect object position informa-\ntion. Experimental results in Table 6 demonstrate that our\ngaze box loss has a positive effect on both object detection\nand gaze heatmap regression process.\nExperiments\nDatasets & Settings\nAll experiments were conducted on GOO-Synth and GOO-\nReal datasets (Tomas et al. 2021). TransGOP is trained for\n50 epochs, with an initial learning rate of 1e − 4, and the\nlearning rate is reduced by 0.94 times every 5 epochs. We\nuse AdamW as our optimizer. For the gaze autoencoder, we\nset the hidden size to 256 and employ 200 decoder queries.\nIn Eq.1, the loss weightα is 1000 and β is 10. The input size\nof the image is set to 224 × 224 and the predicted heatmap\nsize is 64 × 64. All experiments are implemented based on\nthe PyTorch and one GeForce RTX 3090Ti GPU.\nMetrics We use the Average Precision (AP) to evaluate the\nobject detection performance. For gaze estimation, we adopt\ncommonly used metrics including AUC, L2 distance error\n(Dist.), and angle error (Ang.).\nFor the gaze object prediction, we utilize mSoC 1 metric\nthat can measure differentiation even when the predicted box\nand ground truth box do not overlap with each other.\nComparison to State-of-the-Art\nGaze object prediction.Table 1 and Table 2 show the per-\nformance comparison with the GaTector (Wang et al. 2022a)\non the GOO-Synth and GOO-Real datasets respectively.\nAs shown in Table 1, TransGOP achieves better re-\nsults, which is 24.9% mSoC higher than Gatector (92.8%\nvs.67.9%). The performance of GaTector drops sharply with\nincreasingly strict mSoC constraints, while TransGOP can\nachieve comparable results. On the GOO-Real dataset in\n1The mSoC metric is proposed by Wanget al.in the latest arXiv\nversion.URL: https://arxiv.org/abs/2112.03549.\nThe code can be available at https://github.com/CodeMonsterPHD/\nGaTector-A-Unified-Framework-for-Gaze-Object-Prediction.git.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10184\nMethods AP\nAP50 AP75 AP95\nGOO-Synth\nYOLOv4 46.6\n88.1 46.8 0.1\nYOLOv7 54.4 98.3 54.4 0.2\nGaTector 56.8 95.3 62.5 0.1\nDeformable DETR 81.0 98.3 92.8 14.9\nDINO 83.7 98.5 93.9 21.7\nTransGOP 87.6 99.0 97.3 25.5\nGOO-Real\nYOLOv4 43.7\n84.0 43.6 0.1\nYOLOv7 57.3 96.4 63.2 0.3\nGaTector 52.2 91.9 55.3 0.1\nDeformable DETR 81.3 96.0 93.4 11.3\nDINO 82.8 98.7 94.6 13.5\nTransGOP 84.1 98.8 94.7 20.1\nTable 3: Object detection results on GOO-Synth and GOO-\nReal datasets.\nTable 2, TransGOP can achieve a comparable performance\nof 82.6% mSoC without pre-train. GaTector requires pre-\ntraining to achieve better performance (71.2% mSoC) on the\nGOO-Real dataset. This demonstrates the powerful model-\ning and feature extraction capabilities rooted in the Trans-\nformer, which enable our model to exhibit better GOP per-\nformance and generalization capability.\nObject detection.Comparison with CNN-based object de-\ntectors YOLOv4 (Bochkovskiy, Wang, and Liao 2020),\nYOLOv7 (Wang, Bochkovskiy, and Liao 2023), GaTec-\ntor (Wang et al. 2022a) and Transformer-based object de-\ntectors Deformable DETR (Zhu et al. 2020), DINO (Zhang\net al. 2022) on the GOO-Synth dataset and GOO-Real\ndatasets in Table 3. The global modeling capability of the\nTransformer makes Transformer-based methods have better\nperformance in dense object scenes, such as the GOO-Synth\ndataset and GOO-Real datasets, than CNN-based methods.\nTransGOP also achieves good object detection performance,\nand although our object detector is based on DINO, through\nthe joint optimization capability of gaze box loss, TransGOP\nobject detection performance exceeds DINO.\nGaze estimation.Comparison with SOTA GE methods (Re-\ncasens et al. 2017; Lian, Yu, and Gao 2018; Chong et al.\n2020; Wang et al. 2022a) on the GOO-Synth dataset is sum-\nmarized in Table 4. Our TransGOP achieves current SOTA\nperformance with AUC (0.963) and angular error (13.30 ◦).\nThis is attributed to the capability of our proposed gaze\nautoencoder to establish long-distance gaze relationships,\nwhile the object-to-gaze cross-attention mechanism enables\nit to learn global-memory position knowledge, thereby en-\nhancing gaze estimation performance. The L2 distance error\nof TransGOP slightly lags behind GaTector by 0.006 (0.079\nvs. 0.073). Because our gaze box loss makes the predicted\ngaze heatmap prefer to reflect the object position informa-\ntion, this results in a slightly worse L2 distance error cal-\nculated using gaze points. Meanwhile, Our TransGOP also\noutperforms GaTector on the GOO-Real dataset in Table 5.\nAblation Studies and Model Analysis\nAblation study about each component.We conducted ab-\nlation studies in Table 6 to analyze the effectiveness of our\nMethods A\nUC↑ Dist.↓ Ang.↓\nRandom 0.497\n0.454 77.0\nRecasens 0.929 0.162 33.0\nLian 0.954 0.107 19.7\nChong 0.952 0.075 15.1\nGaTector 0.957 0.073 14.9\nTransGOP 0.963 0.079 13.3\nTable 4: Gaze estimation results on GOO-Synth.\nMethods A\nUC↑ Dist.↓ Ang.↓\nNo Pre-train\nGaTector\n0.927 0.196 39.50\nTransGOP 0.947 0.097 16.73\nPre-trained on\nGOO-Synth\nGaTector\n0.940 0.087 14.79\nTransGOP 0.957 0.081 14.71\nTable 5: Gaze estimation results on GOO-Real.\nproposed gaze autoencoder(GA) and gaze box loss(Lgb)\non the GOO-Synth dataset. In the first row of Table 6, we\nbuild a strong baseline by combining DINO with a CNN-\nbased gaze regressor (Chong et al. 2020) for comparison.\nWe first compare our Lgb to the energy aggregation loss\n(Leng) proposed in Gatector (Wang et al. 2022a). Based on\nthe results, Leng can enhance GE and GOP performance of\nthe baseline but leads to a 2.5% decrease in object detection\nAP (81.2% vs.83.7%). However, our Lgb can jointly opti-\nmize performance gains for both GE and OD. The baseline\nwith the gaze autoencoder (in Table 6 fourth line) achieves\na 4.9% mSoC improvement in GOP performance (89.8%\nvs.84.9%), which demonstrates that the gaze autoencoder\ncan establish long-distance gaze relationships, thus predict\nthe more accurate gaze object. Our complete model can\nachieve 92.8% mSoC. From the experimental results, our\nproposed methods in TransGOP further improve the perfor-\nmance significantly.\nAblation study about cross-adapter.Table 7 reports the ef-\nfectiveness of the cross-adapter in the gaze autoencoder. The\nresults show that the cross-adapter can improve the perfor-\nmance of GOP (92.9% vs 90.8%), which is mainly because\nthe cross-adapter can optimize the task conversion from the\nkey-value pair of the object detector to the gaze autoencoder.\nComparative between different cross-attention mecha-\nnisms. In Table 8, we compare the effectiveness of dif-\nferent cross-attention mechanisms, i.e., the gaze-to-object\nand object-to-gaze mechanism refers to obtaining key-value\npairs from the different components. The results show that\nthe key-value pairs from the gaze autoencoder have a lim-\nited effect on the object detector, while the global-memory\nposition knowledge of the object detector can optimize the\nperformance of the gaze autoencoder.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10185\nBaseline Loss GA Gaze Object\nPrediction Object Detection Gaze Estimation\nLeng Lgb mSoC mSoC 50 mSoC75 mSOC95 AP AP 50 AP75 AP95 AUC\n↑ Dist.↓ Ang.↓\n√ 84.9 98.5\n93.2 27.3 83.7 98.5\n93.9 21.7 0.949 0.095\n17.90√ √ 85.4 98.5\n95.1 28.7 81.2 98.3\n92.7 20.2 0.952 0.091\n16.60√ √ 90.4 99.0\n97.5 41.2 84.1 98.7\n95.1 22.7 0.958 0.089\n15.93√ √ 89.8 98.9\n97.2 38.6 85.0 98.9\n96.2 23.2 0.955 0.092\n14.31√ √ √ 92.8 99.0\n98.5 51.9 87.6 99.0\n97.3 25.5 0.963 0.079\n13.30\nTable 6: Ablation comparison about each component on the GOO-Synth dataset.\nSetups Gaze Object\nPrediction Object Detection Gaze Estimation\nmSoC mSoC50 mSoC75 mSoC95 AP AP50 AP75 AP95 AUC\n↑ Dist.↓ Ang.↓\nw/o cross-adapter 90.8 99.0\n97.8 41.8 85.8 98.0\n96.5 23.8 0.960 0.084\n14.20\nw/ cross-adapter 92.8 99.0\n98.5 51.9 87.6 99.0\n97.3 25.5 0.963 0.079\n13.30\nTable 7: Ablation study about cross-adapter on the GOO-Synth dataset.\nSetups Gaze Object\nPrediction Object Detection Gaze Estimation\nmSoC mSoC50 mSoC75 mSoC95 AP AP50 AP75 AP95 AUC\n↑ Dist.↓ Ang.↓\ngaze-to-object 79.4 93.1\n89.3 13.2 70.0 94.4\n79.4 4.2 0.860 0.173\n23.43\nobject-to-gaze 92.8 99.0\n98.5 51.9 87.6 99.0\n97.3 25.5 0.963 0.079\n13.30\nTable 8: Comparison of the different cross-attention mechanisms on the GOO-Synth dataset.\nTransGOP\n GaTector\nFigure 5: Object detection visualization of GaTector and\nTransGOP when IoU is 0.75.\nGTGaTectorTransGOP\nFigure 6: Gaze object prediction visualization results of Ga-\nTector and TransGOP.\nVisualization\nVisualization about object detection. Fig. 5 shows the\nqualitative results of object detection for GaTector and our\nproposed TransGOP when IoU is 0.75. Our TransGOP out-\nperforms GaTector in detecting objects located at the inter-\nsection of people and goods or the edge of the shelf.\nVisualization about gaze object prediction.As shown in\nFig. 6, the GaTector predicts a relatively accurate heatmap,\nbut due to its poor object detection performance, it cannot\naccurately locate the gaze box. In contrast, scene our Trans-\nGOP can learn precise location information from the object\ndetector, improving the accuracy of gaze object prediction.\nThe last column is failure cases.\nConclusion\nThis paper proposes an end-to-end Transformer-based gaze\nobject prediction method named TransGOP, which consists\nof an object detector and a gaze regressor. The object de-\ntector is a DETR-like method to predict object location\nand category. Transformer-based gaze autoencoder is de-\nsigned to build the long-range human gaze relationship in\nthe gaze regressor. Meanwhile, to improve the regression of\ngaze heatmap, we propose an object-to-gaze cross-attention\nmechanism that utilizes the key-value pairs from the object\ndetector as the input of the cross-attention block in the de-\ncoder layer of gaze autoencoder, so can help the model to\npredict more accurate gaze heatmap. Moreover, we also pro-\npose a novel gaze box loss that only focuses on the gaze en-\nergy in the gaze box to jointly optimize the performance of\nthe object detector and gaze regressor. Finally, comprehen-\nsive experiments on the GOO-Synth and GOO-Real dataset\ndemonstrates the effectiveness of our TransGOP.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10186\nReferences\nBalim, H.; Park, S.; Wang, X.; Zhang, X.; and Hilliges, O.\n2023. EFE: End-to-end Frame-to-Gaze Estimation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2687–2696.\nBochkovskiy, A.; Wang, C.-Y .; and Liao, H.-Y . M. 2020.\nYolov4: Optimal speed and accuracy of object detection.\narXiv preprint arXiv:2004.10934.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Eur. Conf. Comput. Vis., 213–229.\nSpringer.\nCheng, Y .; and Lu, F. 2021. Gaze Estimation Using Trans-\nformer. arxiv:2105.14424.\nCheng, Y .; Lu, F.; and Zhang, X. 2018. Appearance-based\ngaze estimation via evaluation-guided asymmetric regres-\nsion. In Eur. Conf. Comput. Vis., 100–115.\nChong, E.; Wang, Y .; Ruiz, N.; and Rehg, J. M. 2020. De-\ntecting attended visual targets in video. In IEEE Conf. Com-\nput. Vis. Pattern Recog., 5396–5406.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 1601–1610.\nde Belen, R. A.; Eapen, V .; Bednarz, T.; and Sowmya, A.\n2023. Using visual attention estimation on videos for auto-\nmated prediction of Autism Spectrum Disorder and symp-\ntom severity in preschool children. medRxiv, 2023–06.\nDuan, K.; Bai, S.; Xie, L.; Qi, H.; Huang, Q.; and Tian, Q.\n2019. Centernet: Keypoint triplets for object detection. In\nIEEE Conf. Comput. Vis. Pattern Recog., 6569–6578.\nFang, Y .; Liao, B.; Wang, X.; Fang, J.; Qi, J.; Wu, R.; Niu,\nJ.; and Liu, W. 2021. You only look at one sequence: Re-\nthinking transformer in vision through object detection.Adv.\nNeural Inform. Process. Syst., 34: 26183–26197.\nGirshick, R. 2015. Fast r-cnn. In Int. Conf. Comput. Vis.,\n1440–1448.\nGuo, H.; Hu, Z.; and Liu, J. 2022. MGTR: End-to-End Mu-\ntual Gaze Detection with Transformer. ACCV.\nHe, J.; Pham, K.; Valliappan, N.; Xu, P.; Roberts, C.; Lagun,\nD.; and Navalpakkam, V . 2019. On-device few-shot person-\nalization for real-time gaze estimation. InInt. Conf. Comput.\nVis. Worksh., 0–0.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Spatial\npyramid pooling in deep convolutional networks for visual\nrecognition. IEEE Trans. Pattern Anal. Mach. Intell., 37(9):\n1904–1916.\nHuang, L.; Yang, Y .; Deng, Y .; and Yu, Y . 2015. Dense-\nbox: Unifying landmark localization with end to end object\ndetection. arXiv preprint arXiv:1509.04874.\nJudd, T.; Ehinger, K.; Durand, F.; and Torralba, A. 2009.\nLearning to predict where humans look. In Int. Conf. Com-\nput. Vis., 2106–2113. IEEE.\nKleinke, C. L. 1986. Gaze and eye contact: a research re-\nview. Psychological bulletin, 100(1): 78.\nKong, T.; Sun, F.; Liu, H.; Jiang, Y .; Li, L.; and Shi, J. 2020.\nFoveabox: Beyound anchor-based object detection. IEEE\nTrans. Image Process., 29: 7389–7398.\nKrafka, K.; Khosla, A.; Kellnhofer, P.; Kannan, H.; Bhan-\ndarkar, S.; Matusik, W.; and Torralba, A. 2016. Eye tracking\nfor everyone. In IEEE Conf. Comput. Vis. Pattern Recog.,\n2176–2184.\nLand, M.; and Tatler, B. 2009. Looking and acting: vision\nand eye movements in natural behaviour. Oxford University\nPress.\nLaw, H.; and Deng, J. 2018. Cornernet: Detecting objects as\npaired keypoints. In Eur. Conf. Comput. Vis., 734–750.\nLeifman, G.; Rudoy, D.; Swedish, T.; Bayro-Corrochano, E.;\nand Raskar, R. 2017. Learning gaze transitions from depth\nto improve video saliency estimation. In Int. Conf. Comput.\nVis., 1698–1707.\nLi, F.; Zhang, H.; Liu, S.; Guo, J.; Ni, L. M.; and Zhang,\nL. 2022a. Dn-detr: Accelerate detr training by introducing\nquery denoising. InIEEE Conf. Comput. Vis. Pattern Recog.,\n13619–13627.\nLi, Y .; Mao, H.; Girshick, R.; and He, K. 2022b. Explor-\ning plain vision transformer backbones for object detec-\ntion. In European Conference on Computer Vision, 280–\n296. Springer.\nLian, D.; Yu, Z.; and Gao, S. 2018. Believe it or not, we\nknow what you are looking at! In ACCV, 35–50. Springer.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017. Focal loss for dense object detection. In IEEE Conf.\nComput. Vis. Pattern Recog., 2980–2988.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2022a. Dab-detr: Dynamic anchor boxes are\nbetter queries for detr. arXiv preprint arXiv:2201.12329.\nLiu, Z.; Hu, H.; Lin, Y .; Yao, Z.; Xie, Z.; Wei, Y .; Ning, J.;\nCao, Y .; Zhang, Z.; Dong, L.; et al. 2022b. Swin transformer\nv2: Scaling up capacity and resolution. In IEEE Conf. Com-\nput. Vis. Pattern Recog., 12009–12019.\nMeng, D.; Chen, X.; Fan, Z.; Zeng, G.; Li, H.; Yuan, Y .;\nSun, L.; and Wang, J. 2021. Conditional detr for fast training\nconvergence. In Int. Conf. Comput. Vis., 3651–3660.\nPark, S.; Spurr, A.; and Hilliges, O. 2018. Deep pictorial\ngaze estimation. In Eur. Conf. Comput. Vis., 721–738.\nRecasens, A.; Khosla, A.; V ondrick, C.; and Torralba, A.\n2015. Where are they looking? Adv. Neural Inform. Pro-\ncess. Syst., 28.\nRecasens, A.; V ondrick, C.; Khosla, A.; and Torralba, A.\n2017. Following gaze in video. In Int. Conf. Comput. Vis.,\n1435–1443.\nRezatofighi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union:\nA metric and a loss for bounding box regression. In IEEE\nConf. Comput. Vis. Pattern Recog., 658–666.\nTian, Z.; Chu, X.; Wang, X.; Wei, X.; and Shen, C. 2022.\nFully Convolutional One-Stage 3D Object Detection on Li-\nDAR Range Images. arXiv preprint arXiv:2205.13764.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10187\nTomas, H.; Reyes, M.; Dionido, R.; Ty, M.; Mirando, J.;\nCasimiro, J.; Atienza, R.; and Guinto, R. 2021. Goo: A\ndataset for gaze object prediction in retail environments. In\nIEEE Conf. Comput. Vis. Pattern Recog., 3125–3133.\nTonini, F.; Dall’Asen, N.; Beyan, C.; and Ricci, E. 2023.\nObject-aware Gaze Target Detection. arXiv preprint\narXiv:2307.09662.\nTu, D.; Min, X.; Duan, H.; Guo, G.; Zhai, G.; and Shen,\nW. 2022. End-to-End Human-Gaze-Target Detection with\nTransformers. arxiv:2203.10433.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Adv. Neural Inform. Process. Syst.,\n30.\nWang, B.; Hu, T.; Li, B.; Chen, X.; and Zhang, Z. 2022a.\nGaTector: A Unified Framework for Gaze Object Prediction.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 19588–19597.\nWang, C.-Y .; Bochkovskiy, A.; and Liao, H.-Y . M. 2023.\nYOLOv7: Trainable bag-of-freebies sets new state-of-the-\nart for real-time object detectors. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7464–7475.\nWang, Y .; Zhang, X.; Yang, T.; and Sun, J. 2022b. Anchor\ndetr: Query design for transformer-based detector. In AAAI,\n2567–2575.\nYang, H.; Deng, R.; Lu, Y .; Zhu, Z.; Chen, Y .; Roland, J. T.;\nLu, L.; Landman, B. A.; Fogo, A. B.; and Huo, Y . 2020. Cir-\ncleNet: Anchor-free glomerulus detection with circle rep-\nresentation. In International Conference on Medical Im-\nage Computing and Computer-Assisted Intervention, 35–44.\nSpringer.\nYin, P.; Dai, J.; Wang, J.; Xie, D.; and Pu, S. 2022. NeRF-\nGaze: A Head-Eye Redirection Parametric Model for Gaze\nEstimation. arXiv preprint arXiv:2212.14710.\nYu, L.; Zhou, X.; Wang, L.; and Zhang, J. 2022. Boundary-\nAware Salient Object Detection in Optical Remote-Sensing\nImages. Electronics, 11(24): 4200.\nYu, Q.; Xia, Y .; Bai, Y .; Lu, Y .; Yuille, A. L.; and Shen, W.\n2021. Glance-and-Gaze Vision Transformer. InAdvances in\nNeural Information Processing Systems, volume 34, 12992–\n13003. Curran Associates, Inc.\nZhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni, L.;\nand Shum, H.-Y . 2022. Dino: Detr with improved denoising\nanchor boxes for end-to-end object detection. In Int. Conf.\nLearn. Represent.\nZhang, X.; Sugano, Y .; Fritz, M.; and Bulling, A. 2015.\nAppearance-based gaze estimation in the wild. In IEEE\nConf. Comput. Vis. Pattern Recog., 4511–4520.\nZhang, X.; Sugano, Y .; Fritz, M.; and Bulling, A. 2017. Mpi-\nigaze: Real-world dataset and deep appearance-based gaze\nestimation. IEEE Trans. Pattern Anal. Mach. Intell., 41(1):\n162–175.\nZhao, H.; Lu, M.; Yao, A.; Chen, Y .; and Zhang, L. 2020.\nLearning to draw sight lines. Int. J. Comput. Vis., 128(5):\n1076–1100.\nZheng, D.; Dong, W.; Hu, H.; Chen, X.; and Wang, Y . 2023.\nLess is More: Focus Attention for Efficient DETR. arXiv\npreprint arXiv:2307.12612.\nZhou, X.; Zhuo, J.; and Krahenbuhl, P. 2019. Bottom-up\nobject detection by grouping extreme and center points. In\nIEEE Conf. Comput. Vis. Pattern Recog., 850–859.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\nZhu, Z.; and Ji, Q. 2005. Eye gaze tracking under natu-\nral head movements. In IEEE Conf. Comput. Vis. Pattern\nRecog., volume 1, 918–923. IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10188",
  "topic": "Gaze",
  "concepts": [
    {
      "name": "Gaze",
      "score": 0.7489568591117859
    },
    {
      "name": "Transformer",
      "score": 0.622580349445343
    },
    {
      "name": "Computer science",
      "score": 0.5283623337745667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48407498002052307
    },
    {
      "name": "Computer vision",
      "score": 0.4523159861564636
    },
    {
      "name": "Engineering",
      "score": 0.1447073221206665
    },
    {
      "name": "Electrical engineering",
      "score": 0.10919728875160217
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I148099405",
      "name": "Xi'an University of Architecture and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    }
  ],
  "cited_by": 7
}