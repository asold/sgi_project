{
    "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models",
    "url": "https://openalex.org/W3192285294",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1689984307",
            "name": "Liu, Zheyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227962154",
            "name": "Rodriguez-Opazo, Cristian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226513948",
            "name": "Teney, Damien",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2751766829",
            "name": "Gould, Stephen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3026458074",
        "https://openalex.org/W2964196083",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W2138079527",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963917086",
        "https://openalex.org/W2910603373",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W2971157635",
        "https://openalex.org/W2963907629",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2963921132",
        "https://openalex.org/W2012292677",
        "https://openalex.org/W2963131783",
        "https://openalex.org/W2561715562",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2118253129",
        "https://openalex.org/W2894280539",
        "https://openalex.org/W2964345214",
        "https://openalex.org/W2947448682",
        "https://openalex.org/W2963686907",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3108144224",
        "https://openalex.org/W3099206234",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2905544595",
        "https://openalex.org/W3038703236",
        "https://openalex.org/W3034585290",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2471768434",
        "https://openalex.org/W2883672875",
        "https://openalex.org/W2964211610",
        "https://openalex.org/W1528802670",
        "https://openalex.org/W1948251820"
    ],
    "abstract": "We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&amp;L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.",
    "full_text": "Image Retrieval on Real-life Images with Pre-trained\nVision-and-Language Models\nZheyuan Liu1 Cristian Rodriguez-Opazo2 Damien Teney2,3 Stephen Gould1\n1Australian National University\n2Australian Institute for Machine Learning, University of Adelaide 3Idiap Research Institute\n{zheyuan.liu, stephen.gould}@anu.edu.au\ncristian.rodriguezopazo@adelaide.edu.au, damien.teney@idiap.ch\nAbstract\nWe extend the task of composed image retrieval, where\nan input query consists of an image and short textual de-\nscription of how to modify the image. Existing methods have\nonly been applied to non-complex images within narrow do-\nmains, such as fashion products, thereby limiting the scope\nof study on in-depth visual reasoning in rich image and lan-\nguage contexts. To address this issue, we collect the Com-\npose Image Retrieval on Real-life images (CIRR) dataset,\nwhich consists of over 36,000 pairs of crowd-sourced,\nopen-domain images with human-generated modifying text.\nTo extend current methods to the open-domain, we pro-\npose CIRPLANT, a transformer based model that leverages\nrich pre-trained vision-and-language (V&L) knowledge for\nmodifying visual features conditioned on natural language.\nRetrieval is then done by nearest neighbor lookup on the\nmodiÔ¨Åed features. We demonstrate that with a relatively\nsimple architecture, CIRPLANT outperforms existing meth-\nods on open-domain images, while matching state-of-the-\nart accuracy on the existing narrow datasets, such as fash-\nion. Together with the release of CIRR, we believe this work\nwill inspire further research on composed image retrieval.\nOur dataset, code and pre-trained models are available at\nhttps://cuberick-orion.github.io/CIRR/.\n1. Introduction\nWe study the task of composed image retrieval, that is,\nÔ¨Ånding an image from a large corpus that best matches a\nuser query provided as an image-language pair. Unlike tra-\nditional content-based [38] or text-based [24, 42] image re-\ntrieval where a single modality is used to describe the target\nimage, composed image retrieval involves both visual and\ntextual modalities to specify the user‚Äôs intent. For humans\nthe advantage of a bi-modal query is clear: some concepts\nand attributes are more succinctly described visually, others\nReference ImageTarget Image #1Target Image #2Modification text for #1:‚ÄúBe a same breed dog with his puppy running‚ÄùModification text for #2:‚ÄúTwo dogs of the same breed on the floor‚Äù\nFigure 1. Example of composed image retrieval from the proposed\nCIRR dataset. The input is composed of a reference image and a\nmodifying text, to which the model must Ô¨Ånd a close match. A\nmajor challenge is the inherent ambiguity and underspeciÔ¨Åcation\nof visual aspects to be preserved or modiÔ¨Åed. Our dataset includes\nopen-domain images with rich contexts to facilitate the study of\nsuch challenge.\nthrough language. By cross-referencing the two modalities,\na reference image can capture the general gist of a scene,\nwhile the text can specify Ô¨Åner details. The challenge is the\ninherent ambiguity in knowing what information is impor-\ntant (typically one object of interest in the scene) and what\ncan be ignored ( e.g., the background and other irrelevant\nobjects). However, existing datasets for this task fall short\nof allowing us to adequately study this problem.\nConsider the example in Fig. 1. Real-life images usu-\nally contain rich object interactions on various scales. In\neach case, to readily identify the relevant aspects to keep or\nchange and pay less attention elsewhere ( e.g., the color of\nthe dog‚Äôs fur and background objects), a model must de-\nvelop in-depth visual reasoning ability and infer implicit\nhuman agreements within both the visual and language con-\ntexts. However, existing datasets are constrained to domains\nsuch as fashion products [4, 12, 13] or synthetic objects [40]\nwith relatively simple image contents. We argue that the\ncurrent datasets are insufÔ¨Åcient for exploring the unique re-\nsearch opportunity mentioned above.\nMotivated by this problem, we collect the Compose Im-\narXiv:2108.04024v1  [cs.CV]  9 Aug 2021\nage Retrieval on Real-life images (CIRR) dataset. It is\nbased on the open-domain collection of real images from\nNLVR2 [35], for which we collected rich, high-quality an-\nnotations that aim to tease out the important aspects of the\nreference image and textual description for a given query.\nCompared with existing datasets, CIRR places more em-\nphasis on distinguishing between visually similar images,\nwhich provides a greater challenge, as well as a chance for\nstudying Ô¨Åne-grained vision-and-language (V&L) reason-\ning in composed image retrieval. Our dataset also allows\nfor evaluation on fully labeled subsets, which addresses a\nshortcoming of existing datasets that are not fully labeled\nand therefore contain multiple false-negatives (as unlabeled\nimages are considered negative).\nMeanwhile, we propose Composed Image Retrieval\nusing Pretrained LANguage Transformers (CIRPLANT),\nwhich extends current methods into open-domain images by\nleveraging the knowledge of large-scale V&L pre-trained\n(VLP) model [25]. Although the advantages of such pre-\ntrained models have been validated in many visiolinguis-\ntic tasks [6, 25, 28], to the best of our knowledge, none\nhave been applied to composed image retrieval. We conjec-\nture one of the reasons being the existing domain-speciÔ¨Åc\ndatasets cannot greatly beneÔ¨Åt from the pre-training, which\nuses more complex, open-world images. Moreover, to\nadopt the VLP models for Ô¨Åne-tuning, most of the down-\nstream tasks are formulated as classiÔ¨Åcation tasks [6, 25].\nFor composed image retrieval, it requires taking as input\nboth the reference and target images. However, this greatly\nraises the computational overhead for retrieval, as the model\nneeds to exhaustively assess each input query paired with\neach candidate target before yielding the one with the\nhighest prediction score. Instead, we propose to preserve\nthe conventional metric learning pipeline, where the input\nqueries are jointly embedded using the VLP model and later\ncompared with features of candidate images through ‚Ñì2-\nnorm distance. SpeciÔ¨Åcally, our design maintains the same\nobjective of ‚Äúlanguage-conditioned image feature modiÔ¨Åca-\ntion‚Äù as previous work [5, 8, 40], while manages to utilize\nthe pre-trained V&L knowledge in large-scale models. We\ndemonstrate that our proposed model reaches state-of-the-\nart on the existing fashion dataset while outperforming cur-\nrent methods on CIRR.\n2. Related Work\nImage retrieval. Existing work on image retrieval using\ndeep learning can be categorized by the type of queries con-\nsidered. Content-based Image Retrieval (CBIR) refers to\nthe use of image-only queries for product search [26], face\nrecognition [29, 34], etc. This setup leaves little room for\niterative user feedback or reÔ¨Ånement. Other possible modal-\nities to form queries include attributes [13], natural lan-\nguage [24, 42], and sketches [31]. These are motivated by\na more natural user experience, but require more advanced\nretrieval mechanisms. V o et al. [40] proposecomposed im-\nage retrieval that combines visual and text modalities. Here\nthe query consists of a reference image and short text de-\nscribing desired differences with this image. Guo et al. [12]\ndemonstrate the potential of this setup for the narrow do-\nmain of fashion recommendation.\nOur work focuses on composed image retrieval in an\nopen-domain setting, i.e., not restricted to fashion products\nfor example. We speciÔ¨Åcally address the case of distin-\nguishing visually similar images, which requires more in-\ndepth, Ô¨Åne-grained reasoning ablility over both the visual\nand language modalities.\nCompositional learning. The topic of compositional\nlearning has been extensively studied in V&L tasks includ-\ning visual question answering (VQA) [3], image caption-\ning [1, 2] and video retrieval [41]. The aim is to produce\nlearned joint-embedding features that capture the salient in-\nformation in both visual and text modalities along with their\ninteractions. For composed image retrieval, V o et al. [40]\nÔ¨Årst propose a residual-gating mechanism that aims to con-\ntrol variation of the input image features through text. Hos-\nseinzadeh and Wang [17] use region-based visual features\nfrom R-CNN models [10, 32] originally proposed for im-\nage captioning [1] and VQA [37]. Recently, Chen et al.\n[5] use a transformer-based model [39] and inject the text\nmodality at varying depths of the image model. Dodds\net al. [8] introduce the concept of modality-agnostic tokens,\nwhich they obtain from ‚Äúdivided‚Äù spatial convolutional fea-\ntures and LSTM hidden states. In this work, we propose a\nmethod that leverages the rich knowledge in VLP models.\nOur method can modify the input image features based on\nnatural language without the need of developing monolithic\narchitecture on the speciÔ¨Åc task.\nVision-and-language pre-training. The success of pre-\ntrained BERT [7] inspired numerous attempts on VLP mod-\nels, including [6, 23, 25, 28, 36]. The aim is to de-\nvelop Transformer-based [39] models trained on large-scale\nimage-text triplets to produces V&L representations appli-\ncable to various tasks. The advantage is clear, instead of\ntraining monolithic models on task-speciÔ¨Åc datasets from\nzero, different V&L tasks can start with the representations\nlearned from (usually) a considerably larger image-text cor-\npus, and Ô¨Åne-tune on speciÔ¨Åc tasks. Motivated by success\nin other V&L tasks, we propose to adopt the VLP model on\ncomposed image retrieval. The key obstacle is to design the\narchitecture to encourage a controlled modiÔ¨Åcation of im-\nage features, which, differs greatly from the conventional\nuse cases of such models.\nDatasets for composed image retrieval. Most existing\ndatasets suitable for composed image retrieval are repur-\nposed from other tasks [13, 18, 40]. Images are paired\n2\nwithin classes and textual descriptions of their differences\nare generated automatically from existing labels. These\ndatasets are relatively simple visually and only contain short\ndescriptions with simple language. CSS [40] uses the syn-\nthetic images of geometric 3D shapes from CLEVR [20],\npaired with descriptions generated according to differences\nin appearance of the objects. Fashion200k [13] contains\napprox. 200k images tagged with attributes that can be\nused to compose text descriptions of differences between\nimages. MIT-States [18] contains images of entities in dif-\nferent states each labelled with one noun and one adjective.\nThe adjectives can describe limited differences between im-\nages. More recent works introduced human-generated de-\nscriptions. Guo et al. [11] present annotations for Shoes [4],\na dataset of 10k footwear images. Fashion-IQ [12] contains\ncrowd-sourced descriptions of differences between images\nof fashion products. Dodds et al. [8] introduce benchmarks\nfor the Birds-to-Words [9] and Spot-the-Diff [19] datasets.\nIn this paper, we introduce a new dataset that addresses\ncurrent deÔ¨Åciencies. Our dataset is open-domain and not\nrestricted, e.g., to fashion products [4, 12, 13]. We design\na careful collection process to produce high-quality pairs\nfrom our diverse collection of images by only associating\nvisually- and semantically-related images. We also address\nthe issue of false-negative targets, that is, candidate target\nimages that are valid for a certain input query, but not la-\nbeled as such. Previous datasets failed to resolve this issue\ndue to the cost of exhaustively labeling images against ev-\nery possible query, which is mitigated by our data collection\nstrategy. Although not used in our current work, the dataset\nalso contains a rich set of auxiliary annotations that clarify\nambiguities not addressed in the textual query.\n3. The Proposed Model\nIn this section, we Ô¨Årst brieÔ¨Çy introduce the vision-and-\nlanguage pre-trained (VLP) models, then we discuss our\nadaptation of it for the task of composed image retrieval.\n3.1. Vision-and-Language Pre-trained Models\nContemporary VLP models are inspired by BERT [7],\nwhich is constructed with multi-layer transformers [39].\nThe model accepts variable-length sequential inputs iVLP,\nwhich consist of a concatenation among words in the text\nsequence(s) w = {w1,...,w T }, regional features from the\nimage v = {v1,...,v K}, and other optional tokens. For in-\nstance, in OSCAR [25], an object label associated with each\nregional feature is appended to the end as l = {l1,...,l K}.\nWithin each transformer layer, a multi-head self-\nattention mechanism is designed to capture the dependen-\ncies among the sequential tokens. Layers are stacked hier-\narchically to attend to the output of the previous layer. Once\npre-trained on a large corpus, the Ô¨Ånal output representa-\ntions can be used for Ô¨Åne-tuning on arbitrary downstream\ntasks, where the usage varies depending on the task.\nThat said, downstream tasks share some common as-\npects. Mostly, a classiÔ¨Åcation token [CLS] is inserted at\nthe start of the input text sequence, which aggregates infor-\nmation from the modalities. The Ô¨Ånal [CLS] output is then\nused to make predictions, such as for image classiÔ¨Åcation.\n3.2. Adaptation to Composed Image Retrieval\nThe task of composed image retrieval can be formally\ndescribed as Ô¨Ånding the target image in a large corpus of\nimages IT ‚àà Dthat best matches a query provided by a\nreference image-text pair q = ‚ü®IR,t‚ü©. Our goal is to learn a\ntext-image composition module, which maps a given‚ü®IR,t‚ü©\ninto the same embedding space as, and close to, the corre-\nsponding IT. Intuitively speaking, this requires the compo-\nsition module to modify IR conditioned on t.\nIn this work, we employ OSCAR [25], a recently pro-\nposed VLP model with state-of-the-art performance as the\ncomposition module to perform the mapping as follows.\nInput sequence. We denote the input sequence of OS-\nCAR as iVLP = {w,v}, where we initialize OSCAR with-\nout the optional object label inputs l. We then follow Li\net al. [25] for processing text sequences, but introduce the\nfollowing adaptations on image representations.\nRather than including a set of regional features,\nwe pre-process images through an ImageNet pre-trained\nResNet [14] model and extract features from before the Ô¨Ånal\nFC-layer. We then process these features through a (newly)\nlearned FC-layer and ‚Ñì2-normalization to give a single im-\nage feature v = {v1}as the input to OSCAR. This same\nfeature representation is used for the corpus of candidate\ntarget images I‚Ä≤\nT ‚ààD as shown in Fig. 2.\nWe choose this relatively simple design for two reasons.\nFirst, recent work ( e.g., [16]) has shown the compatibility\nbetween VLP models and non-regional features of images.\nSecond, we hypothesize that using global image features is\neasier to achieve our goal of modifying IR conditioned on t\nso as to closely match IT.\nOutput token. As shown in Fig. 2, contrary to typical\ndownstream tasks, we do not use the Ô¨Ånal representation of\nthe [CLS] token as the text-image joint embedding. In-\nstead, we extract the representation corresponding to the\nimage feature token and treat it as the composed image-\ntext feature. This resembles the Ô¨Åne-tuning of REF [23], as\nwell as VLN-BERT [16]. In both cases, tokens other than\n[CLS] are used for prediction. For composed image re-\ntrieval, our design makes sense since the transformer model\nincludes residual connections between input and output to-\nkens. Intuitively, the reference image features are modiÔ¨Åed\nby aggregating the information from other word tokens to\nproduce the target image features.\n3\nText ùíï‚Äì‚ÄúA bigger pub with more people in it.‚ÄùFC + NormalizeResNetShared weightsFC + NormalizeResNetTokenize ùë£ùüèVLP Multi-layer Transformers ùë§ùüê ùë§ùüë ùë§ùüí‚Ä¶ ùë§ùëª‡¨øùüè ùë§ùëª ùë§ùüè ùúëùíä‚Ä¶ ùúôùíä‡¨æùìõ\nReference image ùë∞ùêë  ùúë‡Øú‚Ä¶ ùë£ùüè ùë§ùüê ùë§ùüë ùë§ùüí‚Ä¶ ùë§ùëª‡¨øùüè ùë§ùëª ùë§ùüèùíò‚Ä¶Target image     ùë∞ùêì\nTarget image      ùë∞ùêì\nFigure 2. (Left) Schematic of our model. Given a pair of reference image and text as input, we aim at learning a modiÔ¨Åed image feature of\nthe reference image conditioned on the text, such that it matches the feature of the target image. To compare image features of reference\nand candidate target images, we extract ResNet features and use a shared FC-layer (with normalization) to project them into the same\ndomain. (Right) Overview of the image-text composition module using vision-and-language pre-trained (VLP) multi-layer transformers.\nDashed lines (not fully drawn) represent feature aggregation by attention, which learns a language-conditioned image feature modiÔ¨Åcation.\nùë∞ùüèùë∞ùüêùíëùüèùüêSimilarity Ranking‚Ä¶   ùë∞ùüè  ùë∞ùüê‚Ä¶ Subsetùìì6 Images  ùë∞ùüîxx‚Ä¶  ùë∞ùíèx(a) Form image subset(b) Form image pair9 Pairs/Subset(c) Collect (main) annotation‡Æ±\n‚ÄúSimilar angle photograph, with a larger field of view depicting decor of bedroom‚ÄùAnnotation is unique for the given pair in subset ‚àó\nFigure 3. Overview of the data collection process. (a) We demonstrate the construction of an image subset. (b) We illustrate how we choose\nand form 9 image pairs within one subset, where each arrow suggests the direction from a reference to a target image. (c) ‚Ä†represents\nHuman Tasks with AMT workers. ‚àóindicates the instruction that mitigates the issue of false-negative.\nMetric learning. We use soft triplet-based loss with ‚Ñì2-\nnorm distance as in V o et al. [40] to bring the composed\nimage-text feature closer to the feature of the target image\n(positive pair), while pulling apart the features of negative\npairs. In essence, given the i-th positive pair ‚ü®œïi,œÜ+\ni ‚ü©and\nan arbitrary negative œÜ‚àí\ni,j among all negatives œÜ‚àí\ni , the loss\nis computed as:\nL= log[1 + exp(Œ∫(œïi,œÜ‚àí\ni,j) ‚àíŒ∫(œïi,œÜ+\ni ))], (1)\nwhere Œ∫is ‚Ñì2-norm distance. In training, we randomly sam-\nple the negative for each pair and average the loss over all\nsampled triplets ‚ü®œïi,œÜ+\ni ,œÜ‚àí\ni,j‚ü©.\n4. The CIRR Dataset\nExisting datasets for composed image retrieval [12, 40]\ncontain training and testing examples as triplets ‚ü®IR,q,I T‚ü©\nwhere q= ‚ü®IR,t‚ü©forms the query and IT is (an example of)\nthe desired target from a large image corpus D. However,\nthese existing datasets have two major shortcomings. First,\nthey lack the sufÔ¨Åcient visual complexity to facilitate the\nstudy of one of the major challenges in composed image re-\ntrieval, which is the subtle reasoning over what aspects are\nimportant and what shall be ignored. Second, since the can-\ndidate images cannot be extensively labeled for each‚ü®IR,t‚ü©\npair, existing datasets contain many false-negatives. That is,\nimages I ‚ààD that are valid matches for the query but not\nlabeled as the ground-truth target IT. Indeed, all images in\nD\\{IR,IT}are considered as negatives. To circumvent this\nshortcoming, existing works choose to evaluate models with\nRecall@K and set K to larger values ( e.g., 10, 50 [12]),\nthus accounting for the presence of false-negatives. How-\never, the issue persists during training. Moreover, by setting\nlarger K values, these methods are essentially trading in\ntheir ability for learning detailed text-image modiÔ¨Åcations.\nTo mitigate these issues, we introduce the Compose Im-\nage Retrieval on Real-life images (CIRR) dataset, which\nincludes over 36,000 annotated query-target pairs, ‚ü®q =\n‚ü®IR,t‚ü©,IT‚ü©. Unlike existing datasets, we collect the mod-\nifying text to distinguish the target from a set of similar im-\nages (addressing the problem of false-negatives) and creat-\ning challenging examples that require careful consideration\nof visual and textual cues. Details are as follows.\n4.1. Data Collection\nWe Ô¨Årst form image pairs then collect related annotations\nby crowd-sourcing. The pairs are drawn from subsets of\nimages, as described below. This strategy plays a major\nrole in mitigating the issue of false negatives (see Sec. 5).\nFig. 3 outlines our data collection procedure.\nImage source. We use the popular NLVR2 dataset for nat-\nural language visual reasoning [35] as our source of images.\nWe choose NLVR2 for several reasons. First, it contains\nimages of real-world entities with reasonable complexity in\nImageNet-type [22]. Second, the setup of our task requires\nimage in pairs that are similar enough, and NLVR 2 is de-\nsigned to have collections of similar images regarding 1,000\n4\nsynsets (e.g., acorn, seawall). Also, Suhr et al. [35] employs\nan additional step to manually remove non-interesting im-\nages, thus ensuring the content quality.\nImage subset construction. The nature of our task re-\nquires collections of negative images with high visual sim-\nilarity, as otherwise, it would be trivial to discriminate be-\ntween the reference and target image. Thus, prior to form-\ning reference-target image pairs, we construct multiple sub-\nsets of six images that are semantically and visually similar,\ndenoted as S= {I1,...,I 6}, shown in Fig. 3(a).\nHere, to construct a subset, we randomly pick one im-\nage from the large corpus I1 ‚àà D. We then sort the re-\nmaining images in Dby their cosine similarity to I1 using\nResNet152 [14] image feature vectors pre-trained on Ima-\ngeNet [22]. Denote by Œ∫i the cosine similarity for image Ii.\nWe then pick Ô¨Åve additional images to produce a similar yet\ndiverse subset, as follows: First, we Ô¨Ålter out images with\nŒ∫i ‚â•0.94 to avoid near-identical images to I1. Then for\nthe next top-20 ranked images, we greedily add each image\nin turn, skipping an image if its cosine similarity is within\n0.002 of the last image added. If a subset of size six cannot\nbe created, then the entire set is discarded.\nOnce constructed we further Ô¨Ålter the collection subsets\nto avoid heavy overlap. We obtain in total 52,732 subsets\nfrom NLVR2, from which we randomly choose 4,351 for\nthe construction of CIRR.\nImage pairing. Within each constructed image subset S,\nwe draw nine pairs of images, as shown in Fig. 3(b). We\nchoose these pairs to have (1) consecutive modiÔ¨Åcations\nthat will allow future training of a dialogue systems; and\n(2) multiple outcomes from the same reference image.\nAnnotations. We collect a modiÔ¨Åcation sentence for each\npair of reference-target images using Amazon Mechanical\nTurk (AMT). To ensure that no false-negatives exist within\nthe same image subset from which we draw the pair, as il-\nlustrated in Fig. 3(c), we show AMT workers the remaining\nimages from the subset and speciÔ¨Åcally ask them to write\nsentences that can only lead to the true target image.\nAMT workers were instructed to avoid subjective de-\nscriptions, text mentions, plain side-by-side comparisons,\nor simple descriptions that only address the target images.\nFollowing the collection of the modiÔ¨Åcation sentences\nfor each pair, we additionally collect some auxiliary annota-\ntions that more explicitly address the ambiguities associated\nwith implicit human-agreements. While we believe that\nthese auxiliary annotations will be useful for future work,\nwe do not make use them in our current work1.\nData splits. Following convention, we randomly assign\n80% of the data for training, 10% for validation and 10%\nfor test. Detailed statistics are shown in Table 2.\n1See supp. mat. and our project website for details on auxiliary anno-\ntations.\n4.2. Analysis on CIRR\nWe follow Suhr et al . [35] and analyze coverage of\nvarious semantic concepts by keywords and sentence pat-\nterns (see Table 1). Here, we show comparisons with\nFashion-IQ [12], the most popular, comparable human-\nlabeled dataset. We observe a greater diversity and aver-\nage length in the sentences in CIRR, indicating broad cov-\nerage and linguistic diversity. Over 40% of the annotations\nare compositional, which indicates an appreciable level of\ncomplexity of the sentences. Interestingly, our annotations\nshould also encourage models to attend to both the refer-\nence and target images by implicitly (rows 1‚Äì4) or explicitly\n(rows 5‚Äì6) referring to the visual contents of both images.\n5. Experiments\nDatasets. To demonstrate the model‚Äôs ability in untiliz-\ning pre-trained V&L knowledge, as well as its generaliz-\nability to images of different domains, we evaluate our pro-\nposed model against baselines and state-of-the-art (SoTA)\nmethods on two datasets, including (1) CIRR, our proposed\ndataset on open-domain composed image retrieval, and (2)\nFashion-IQ [12], which contains images of fashion products\namong three subtypes ( Dress, Shirt, Toptee) with\nhuman-generated annotations. We do not evaluate on other\ndatasets discussed in Sec. 2, as they either contain synthetic\nimage/annotation or are domain-wise similar to Fashion-IQ\n(e.g., Fashion200k [13]).\nCompared methods. For CIRR, we evaluate the follow-\ning methods using publicly available implementations2:\n‚Ä¢ TIRG [40] is an image-text composition model for\ncomposed image retrieval, which has proven to be ef-\nfective on multiple datasets [12, 13, 18, 40]. The\nmethod uses a gating and residual design to encour-\nage the learning of cross-modal features. Two setups\nfor TIRG are available based on whether to inject text\nfeatures at the last FC-layer ( default), or the last con-\nvolution layer (LastConv). We test both setups.\n‚Ä¢ MAAF [8] is speciÔ¨Åcally designed for composed im-\nage retrieval with state-of-the-art performance. By\ndefault, it treats the convolutional spatial image fea-\ntures and the learned text embeddings (randomly ini-\ntialized with LSTM [15]) as modality-agnostic tokens,\nwhich are passed to a Transformer [39]. We evalu-\nate three design choices that were originally reported\nwith comparable results: (+BERT) pretrained context-\naware word representations using BERT [7], (-IT) re-\nmoving the output of text tokens in the last pooling\nlayer, (-RP) substituting the Ô¨Ånal resolution-wise pool-\ning with average pooling.\n2https://github.com/google/tirg, https://github.\ncom/yahoo/maaf\n5\nSemantic aspect Coverage (%)Example (boldface added here for emphasis)CIRR Fashion-IQ\n1 Cardinality 29.3 ‚Äì Only oneof the boars and the ground is browner.\n2 Addition 15.2 15.7 Addhuman feet and a collar.\n3 Negation 11.9 4.0 ‚Ä† Removethe chair, make the dog sit in an open box.\n4 Direct Addressing 57.4 49.0 ‚Ä† Show some lemons with a glass of lemonade.\n5 Compare & Change 31.7 3.0 Same computer butdifferent Ô¨Ånish and black background.\n6 Comparative Statement 51.7 32.0 ‚Ä† Abiggerpub withmorepeople on it.\n7 Statement with Conjunction 43.7 19.0‚Ä† Remove all but one birdandhave it facing rightandputting food in its mouth.\n8 Spatial Relations & Background 61.4 ‚Äì Change the sky to blue color.\n9 Viewpoint 12.7 ‚Äì Focus widely on all available cookies package.\nAvg. Sentence length (words)11.3 5.3\n1 2 3 4 5 6 7 8 9\nTable 1. Analysis of semantic aspects covered by the annotations in CIRR and in Fashion-IQ [12]. We also show average sentence length\n(nb. words). ‚Ä†Numbers from [12]. Image pair for each example is shown below with row number (left-right: reference-target).\nNb. image subsets Nb. pairs Nb. pairs per subset Nb. images\nTrain 3,345 28,225 7.54 16,939\nVal. 503 4,184 8.32 2,297\nTest 503 4,148 8.25 2,316\nTotal 4,351 36,554 8.40 21,552\nTable 2. Statistics of CIRR. Each reference-target image pair is\nassociated with one annotation.\nFor comparison, we also evaluate the following base-\nlines, implemented by V o et al. [40]:\n‚Ä¢ Random (theoretical): theoretical random guess.\n‚Ä¢ Random (init. ResNet): pretrained ImageNet [22] fea-\ntures, but random weights for others parameters.\n‚Ä¢ Image and text-only: substituting the combined image-\ntext feature with the reference image or text feature.\n‚Ä¢ Random image with text: randomly sampling images\nto pair with text during training and validation.\n‚Ä¢ Concatenation: replacing the image-text composition\nlayer with a simple concatenation of features followed\nby a 2-layer perceptron with ReLU.\nFor Fashion-IQ, we additionally include published re-\nsults from the following methods:\n‚Ä¢ MRN [21] uses stacked blocks of element-wise prod-\nucts with residual learning to embed V&L jointly.\n‚Ä¢ FiLM [30] modulates the image feature map condi-\ntioned on text features after the layers of CNN.\n‚Ä¢ Relationship [33] learns the joint embeddings through\nrelationship features constructed by concatenating the\nimage and text features followed by FC-layers.\n‚Ä¢ V AL [5] is specially designed for composed image\nretrieval, which adopts the Transformer to compose\nmulti-level V&L joint representations. For images\nwith text descriptions as side information, an addi-\ntional visual-semantic loss is applied to align visual\nfeatures and the corresponding text features.\nMetric. We follow previous work to report retrieval per-\nformance in Recall within top- K (Recall@K). For CIRR,\nwe additionally report Recallsubset, which is an extension to\nthe standard (global) Recall, made possible by the unique\ndesign of our dataset.\nAs discussed, our input queries q = ‚ü®IR,t‚ü©and target\nimages IT in our dataset are constructed such that both IR\nand IT are sampled from the same image set S(Sec. 4.1).\nWe formulate Recallsubset task by ranking images inS\\{IR}\naccording to model score. We deÔ¨Åne Recall subset@K as the\nproportion of (test) examples where the ground-truth target\nimage IT is ranked within the top-Kimage in its subset.\nConceptually, Recallsubset can be viewed as Recall while\nonly considering images within the same subset as the pair.\nThe beneÔ¨Åts are twofold: First, Recallsubset is not affected by\nfalse-negative samples, thanks to our careful design in data\ncollection procedures. Second, with a selected batch of neg-\native samples with high visual similarities, Recall subset can\nfacilitate analysis on the reasoning ability of the methods\nfor capturing Ô¨Åne-grained image-text modiÔ¨Åcations.\nImplementation details. All experiments are conducted\non a single NVIDIA RTX3090 with PyTorch. SoTA mod-\nels use the default conÔ¨Ågurations proposed by their authors.\nSee supp. mat. and our project website for more de-\ntails on baseline training. For our proposed model, we use\nResNet152 for image feature extraction. The model is op-\ntimized with AdamW [27] with an initial learning rate of\n10‚àí5. We set a linearly decreasing schedule without warm-\nup. The batch size is set to 32 and the network is trained for\n300 epochs. Other settings are kept as default by OSCAR.\n5.1. Results\nBaseline comparison on CIRR. Table 3 (rows 1-13)\ncompares the retrieval performance of baseline and SoTA\nmethods for both Recall and RecallSubset@Kon CIRR.\nFor global Recall, we notice that TIRG performs similar\n6\nRecall@K RecallSubset@K (R@5 +RSubset@1)/2Methods K= 1K= 5K= 10K= 50K= 1K= 2K= 3\nBASELINES\n1 Random (theoretical) 0.02 0.12 0.24 1.20 20.00 40.00 60.00 10.06\n2 Random (init. ResNet) 7.18 25.74 36.91 66.68 20.84 41.02 61.65 23.29\n3 Image-only 13.73 48.46 65.81 89.94 20.93 42.15 63.26 34.70\n4 Text-only 3.90 13.17 20.43 49.16 39.69 62.23 78.52 26.43\n5 Random Image+Text 2.99 11.91 19.85 46.97 39.41 62.33 78.71 25.66\n6 Image+Text Concatenation 12.44 40.24 57.52 87.29 23.74 45.12 65.50 31.99\n7 Human Performance‚Ä† ‚Äì ‚Äì ‚Äì ‚Äì 86.09 ‚Äì ‚Äì ‚Äì\nSOTA\n8 TIRG [40] 14.61 48.37 64.08 90.03 22.67 44.97 65.14 35.52\n9 TIRG+LastConv [40] 11.04 35.68 51.27 83.29 23.82 45.65 64.55 29.75\n10 MAAF [8] 10.31 33.03 48.30 80.06 21.05 41.81 61.60 27.04\n11 MAAF+BERT [8] 10.12 33.10 48.01 80.57 22.04 42.41 62.14 27.57\n12 MAAF‚àíIT [8] 9.90 32.86 48.83 80.27 21.17 42.04 60.91 27.02\n13 MAAF‚àíRP [8] 10.22 33.32 48.68 81.84 21.41 42.17 61.60 27.37\n14 Ours (no init.) 15.18 43.36 60.48 87.64 33.81 56.99 75.40 38.59\n15 Ours (init. OSCAR) 19.55 52.55 68.39 92.38 39.20 63.03 79.49 45.88\nTable 3. Retrieval performance on CIRR. Best (resp. second-best) numbers are in bold-black (resp. blue). ‚Ä†See supplementary material\non our collection details of human performance. We additionally report the average score over R @5 and RSubset@1, which better reveals\nthe overall performance of models (discussed in Sec. 5.1). Note that R@5 accounts for possible false-negatives in the entire image corpus.\nSince RSubset is not affected by such issues (Sec. 5), we consider RSubset@1 to better illustrate the Ô¨Åne-grained reasoning ability of methods.\nDress Shirt Toptee Avg (R@10+ R@50)/2Methods R @10 R@50 R@10 R@50 R@10 R@50 R@10 R@50\n1 Image-only 4.20 13.29 4.51 14.47 4.13 14.30 4.28 14.20 9.15\n2 Image+Text Concatenation 10.52 28.98 13.44 34.60 11.36 30.42 11.77 31.33 21.55\n3 TIRG [40] 8.10 23.27 11.06 28.08 7.71 23.44 8.96 24.93 16.95\n4 TIRG+Side Information [12] 11.24 32.39 13.73 37.03 13.52 34.73 12.82 34.72 23.77\n5 MRN [21] 12.32 32.18 15.88 34.33 18.11 36.33 15.44 34.28 24.86\n6 FiLM [30] 14.23 33.34 15.04 34.09 17.30 37.68 15.52 35.04 25.28\n7 TIRG [40] 14.87 34.66 18.26 37.89 19.08 39.62 17.40 37.39 27.40\n8 Relationship [33] 15.44 38.08 18.33 38.63 21.10 44.77 18.29 40.49 29.39\n9 V AL (init. GloVe) [5]22.53 44.00 22.38 44.15 27.53 51.68 24.15 46.61 35.40\n10 MAAF [8] 23.8 48.6 21.3 44.2 27.9 53.6 24.3 48.8 36.6\n13 Ours (no init.) 14.38 34.66 13.64 33.56 16.44 38.34 14.82 35.52 25.17\n14 Ours (init. OSCAR) 17.45 40.41 17.53 38.81 21.64 45.38 18.87 41.53 30.20\nTable 4. Retrieval performance on Fashion-IQ, we follow [12] to report average scores of R @10 and 50. Best numbers for SoTA models\nare in bold-black. Rows 1-4 reported by [12], rows 5-9 (shaded) reported by [5]. Rows 9-10 are SoTA methods developed for composed\nimage retrieval, where we report the originally published numbers of their best conÔ¨Ågurations. Note that we see multiple scores reported\nfor TIRG on Fashion-IQ, here we only show the published results from the above two sources. Additional non peer-reviewed methods that\ninvolve ensembles of models or data augmentation are not included.\n‚ÄúBrown dog sits upright on the green grass‚Äù ‚ÄúHorse drawn carriage takes people around the city on the pavement‚Äù\nTIRGùë∞ùêëùë∞ùêì\nCIRPLANTùë∞ùêë\nùë∞ùêì\nùë∞ùêì\nùë∞ùêëùë∞ùêìCIRPLANT\nFigure 4. Qualitative results of image retrieval on CIRR, red/green boxes: reference/target images. Predictions are ranked from left to\nright. We show the ranked images within subsets, see Sec. 5 for details on metric. (Left) We compare the retrieval on the same query for\nTIRG and CIRPLANT. (Right) We demonstrate the implicit ambiguities within the dataset (in this case, the difÔ¨Åculty in selecting themost\nsuitable candidate by preserving the breed of the dog across the images, which requires identifying subtle characteristics‚Äìe.g. pointy ears).\nto the Image-only baseline, suggesting that its multi-modal\ncomposition layers often fail to extract information from the\ntext. Instead, it relies primarily on visual content. We con-\njecture that CIRR focuses more on the Ô¨Åne-grained changes\nthat are harder to capture and associate across modalities,\ntherefore, requires stronger image-text composition layers.\nIn addition, we note that MAAF (rows 10-13) does not\ngeneralize well to our dataset, even though it outperforms\nTIRG and other methods on existing ones [8]. We be-\nlieve the choice of forming image tokens by spatial feature\n7\nmaps does not generalize to our dataset where the modi-\nÔ¨Åcation concepts are more diverse and at multiple levels.\nMeanwhile, adding the contextual-aware BERT pretrained\nweights yields little effects, suggesting a plain initialization\nof word embeddings, though contains validated pre-trained\nlanguage information, may not help the composition layers.\nThe Recall Subset results tell a similar story. Here the\nperformance of all SoTA models is close to the theoret-\nical random guess, indicating that current models fail to\ncapture Ô¨Åne-grained modiÔ¨Åcations between similar images.\nInterestingly, we discover that the Text-only and Random-\nImage+Text baselines (rows 4,5) outperform SoTA models\nsigniÔ¨Åcantly. We believe this is because the modiÔ¨Åcation\nsentences usually contain descriptions of visual content that\nis unique to the target image once limited to the smaller re-\ntrieval set (e.g., ‚Äúadd a leash to the dog‚Äù where only the tar-\nget image contains the leash). However, as demonstrated by\nthe low Recall performance, such descriptions are not de-\ntailed enough to single out the target image in the entire im-\nage corpus. This scenario further demonstrates Recall Subset\nreveals behaviors of models on different aspects, and can be\nused for more detailed analysis.\nIn short, the relatively low retrieval performance sug-\ngests that our dataset poses a challenge to existing methods\ndeveloped and tested on narrow-domain datasets.\nPerformance of CIRPLANT on CIRR. Results in Ta-\nble 3 (rows 14,15) compares our proposed model with SoTA\nmethods on CIRR. We notice that on CIRR, CIRPLANT\nwith no initialization (row 14) performs similarly as TIRG\non Recall, while surpassing all other SoTA methods. This\nvalidates our design choice of using non-regional image fea-\ntures for composing image and text through the transformer\narchitecture. Meanwhile, on Recall Subset our model, even\nwithout initialization, yields much higher scores than oth-\ners, suggesting transformers are better in capturing more\nÔ¨Åne-grained visiolinguistic cues when composing image\nand text features. Comparing with SoTA methods that\nuse LSTMs for generating a single language embedding of\nthe entire sentence, we believe that the key difference lies\nwithin the fact that transformers accept word tokens as in-\nput, which can later be attended individually. Our model\noutperforms all other methods with OSCAR initialization\n(row 15) by a signiÔ¨Åcant margin, demonstrating the beneÔ¨Åt\nof VLP knowledge on open-domain images.\nPerformance of CIRPLANT on Fashion-IQ. Table 4\ncompares the performance of our model with SoTA meth-\nods. We notice that our model with OSCAR initializa-\ntion (row 14) outperforms most methods, including generic\nmultimodal learning methods and TIRG. This strengthens\nthe beneÔ¨Åts of using transformer architecture that lever-\nages VLP models. Additionally, we note that even on\nFashion-IQ, our model still beneÔ¨Åts greatly from OSCAR\npre-trained initialization (rows 13,14). Given that the im-\nages in Fashion-IQ differ greatly from the data used for\nOSCAR pre-training [25], we believe this further demon-\nstrates that the pre-trained model can transfer the learned\nV&L knowledge and adapt to various contexts.\nWe note that two recent SoTA methods for composed\nimage retrieval (V AL and MAAF, rows 9,10) perform better\nthan our model. Despite the visible improvements brought\nby OSCAR initialization, we hypothesize that our model\nis still underperformed by the apparent domain shift in im-\nages, as the VLP model is pre-trained on generic ImageNet-\ntype data. Meanwhile, the low generalizability of MAAF on\nCIRR (Table 3 rows 10-13) hints the possibility that current\nSoTA methods developed and tested on existing datasets\nmay have been overly adapted to domain-speciÔ¨Åc images of\nlow complexity. Hence, additional open-domain datasets,\nsuch as CIRR, can be beneÔ¨Åcial in future research.\n5.2. Qualitative Results\nFig. 4 (left) demonstrates the retrieval rankings within\nthe image subset (see Sec. 5) on the same query for TIRG\nand CIRPLANT. SpeciÔ¨Åcally, we show the effectiveness\nof pre-training in CIRPLANT when encountering visiolin-\nguistic concepts ( i.e., pavement) that occur less frequently\nin the training data. Additionally, CIRPLANT better cap-\ntures Ô¨Åne-grained cues within language ( e.g., takes people\naround, which implies must have people in the back of the\ncarriage), thanks to the transformer architecture that ac-\ncepts, and attends to individual word tokens.\nWe show one failure case of CIRPLANT on CIRR in\nFig. 4 (right). Note the implicit requirement of preserving\nsame breed of dog across the reference and target image.\nThis requires models to identify the Ô¨Åne-grained visiolin-\nguistic cues (i.e., pointy ears in this sample) and retrieve the\nmost suitable image, bringing more challenge to the task.\n6. Conclusion\nThis work expands the task of composed image retrieval\ninto more complex, open-domain images. We collect the\nCIRR dataset, which addresses shortcomings of existing\ndatasets by placing more emphasis on distinguishing open-\ndomain visually similar images. Our publicly available\ndataset is designed to facilitate future studies on subtle rea-\nsoning over visiolinguistic concepts, as well as iterative re-\ntrieval with dialogue. We also introduce CIRPLANT, a\ntransformer-based model that leverages V&L pre-training\nto compose image and text features. We validate CIR-\nPLANT on both CIRR and the existing fashion dataset,\ndemonstrating the generalizability of our design and the\neffectiveness of V&L pre-training. Collectively, we hope\nto inspire future work on composed image retrieval on a\nbroader scope, yet Ô¨Åne-grained level.\n8\nReferences\n[1] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson,\nS. Gould, and L. Zhang. Bottom-up and top-down atten-\ntion for image captioning and visual question answering. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2018. 2\n[2] J. Aneja, A. Deshpande, and A. G. Schwing. Convolutional\nimage captioning. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2018. 2\n[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\nC. Lawrence Zitnick, and D. Parikh. VQA: Visual Question\nAnswering. In IEEE International Conference on Computer\nVision, 2015. 2\n[4] T. L. Berg, A. C. Berg, and J. Shih. Automatic attribute dis-\ncovery and characterization from noisy web data. In Euro-\npean Conference on Computer Vision, 2010. 1, 3\n[5] Y . Chen, S. Gong, and L. Bazzani. Image search with text\nfeedback by visiolinguistic attention learning. In IEEE Con-\nference on Computer Vision and Pattern Recognition, 2020.\n2, 6, 7\n[6] Y .-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan,\nY . Cheng, and J. Liu. Uniter: Universal image-text repre-\nsentation learning. In European Conference on Computer\nVision, 2020. 2\n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT:\nPre-training of deep bidirectional transformers for language\nunderstanding. In Conference of the North American Chap-\nter of the Association for Computational Linguistics , 2019.\n2, 3, 5\n[8] E. Dodds, J. Culpepper, S. Herdade, Y . Zhang, and\nK. Boakye. Modality-agnostic attention fusion for visual\nsearch with text feedback. ArXiv, abs/2007.00145, 2020. 2,\n3, 5, 7, 11, 15\n[9] M. Forbes, C. Kaeser-Chen, P. Sharma, and S. J. Belongie.\nNeural Naturalist: Generating Ô¨Åne-grained image compar-\nisons. In Conference on Empirical Methods in Natural Lan-\nguage Processing, 2019. 3, 15\n[10] R. B. Girshick. Fast R-CNN. In IEEE International Confer-\nence on Computer Vision, 2015. 2\n[11] X. Guo, H. Wu, Y . Cheng, S. Rennie, G. Tesauro, and\nR. Feris. Dialog-based interactive image retrieval. In Ad-\nvances in Neural Information Processing Systems, 2018. 3\n[12] X. Guo, H. Wu, Y . Gao, S. J. Rennie, and R. Feris. The\nFashion IQ Dataset: Retrieving images by combining side\ninformation and relative natural language feedback. ArXiv,\nabs/1905.12794, 2019. 1, 2, 3, 4, 5, 6, 7, 14, 15, 17\n[13] X. Han, Z. Wu, P. X. Huang, X. Zhang, M. Zhu, Y . Li,\nY . Zhao, and L. S. Davis. Automatic spatially-aware fash-\nion concept discovery. In IEEE International Conference on\nComputer Vision, 2017. 1, 2, 3, 5, 15\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In IEEE Conference on Computer\nVision and Pattern Recognition, 2016. 3, 5\n[15] S. Hochreiter and J. Schmidhuber. Long Short-Term Mem-\nory. Neural Computation, 9:1735‚Äì1780, 1997. 5\n[16] Y . Hong, Q. Wu, Y . Qi, C. Rodriguez-Opazo, and S. Gould.\nA recurrent vision-and-language bert for navigation. IEEE\nConference on Computer Vision and Pattern Recognition ,\n2020. 3\n[17] M. Hosseinzadeh and Y . Wang. Composed query image re-\ntrieval using locally bounded features. In IEEE Conference\non Computer Vision and Pattern Recognition, 2020. 2\n[18] P. Isola, J. J. Lim, and E. H. Adelson. Discovering states and\ntransformations in image collections. In IEEE Conference\non Computer Vision and Pattern Recognition, 2015. 2, 3, 5,\n15\n[19] H. Jhamtani and T. Berg-Kirkpatrick. Learning to describe\ndifferences between pairs of similar images. In Confer-\nence on Empirical Methods in Natural Language Processing,\n2018. 3, 15\n[20] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L.\nZitnick, and R. Girshick. CLEVR: A diagnostic dataset for\ncompositional language and elementary visual reasoning. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2017. 3\n[21] J.-H. Kim, S.-W. Lee, D. Kwak, M.-O. Heo, J. Kim, J.-W.\nHa, and B.-T. Zhang. Multimodal residual learning for visual\nqa. In Advances in neural information processing systems ,\n2016. 6, 7\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassiÔ¨Åcation with deep convolutional neural networks. In\nAssociation for Computing Machinery, 2017. 4, 5, 6, 11\n[23] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang.\nVisualbert: A simple and performant baseline for vision and\nlanguage, 2019. 2, 3\n[24] W. Li, L. Duan, D. Xu, and I. W. Tsang. Text-based image\nretrieval using progressive multi-instance learning. In IEEE\nInternational Conference on Computer Vision, 2011. 1, 2\n[25] X. Li, X. Yin, C. Li, X. Hu, P. Zhang, L. Zhang, L. Wang,\nH. Hu, L. Dong, F. Wei, Y . Choi, and J. Gao. Oscar: Object-\nsemantics aligned pre-training for vision-language tasks. In\nEuropean Conference on Computer Vision, 2020. 2, 3, 8\n[26] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. DeepFashion:\nPowering robust clothes recognition and retrieval with rich\nannotations. In IEEE Conference on Computer Vision and\nPattern Recognition, 2016. 2\n[27] I. Loshchilov and F. Hutter. Decoupled weight decay regu-\nlarization. In International Conference on Learning Repre-\nsentations, 2019. 6\n[28] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-and-\nlanguage tasks. In Advances in Neural Information Process-\ning Systems, 2019. 2\n[29] I. Masi, Y . Wu, T. Hassner, and P. Natarajan. Deep face\nrecognition: A survey. In SIBGRAPI Conference on Graph-\nics, Patterns and Images, 2018. 2\n[30] E. Perez, F. Strub, H. de Vries, V . Dumoulin, and\nA. Courville. Film: Visual reasoning with a general con-\nditioning layer, 2017. 6, 7\n[31] F. Radenovi ¬¥c, G. Tolias, and O. Chum. Deep shape matching.\nIn European Conference on Computer Vision, 2018. 2\n[32] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:\nTowards real-time object detection with region proposal net-\nworks. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 39:1137‚Äì1149, 2015. 2\n9\n[33] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple neu-\nral network module for relational reasoning. In Advances\nin neural information processing systems, pages 4967‚Äì4976,\n2017. 6, 7\n[34] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A\nuniÔ¨Åed embedding for face recognition and clustering. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2015. 2\n[35] A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y . Artzi.\nA corpus for reasoning about natural language grounded in\nphotographs. Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2019. 2, 4, 5, 14,\n20\n[36] H. Tan and M. Bansal. Lxmert: Learning cross-modality en-\ncoder representations from transformers. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing, 2019. 2\n[37] D. Teney, P. Anderson, X. He, and A. V . D. Hengel. Tips\nand tricks for visual question answering: Learnings from the\n2017 challenge. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2018. 2\n[38] S. Tong and E. Chang. Support Vector Machine active learn-\ning for image retrieval. In Proceedings of the Ninth ACM\nInternational Conference on Multimedia, 2001. 1\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, u. Kaiser, and I. Polosukhin. Attention is all\nyou need. In International Conference on Neural Informa-\ntion Processing Systems, 2017. 2, 3, 5\n[40] N. V o, L. Jiang, C. Sun, K. Murphy, L.-J. Li, L. Fei-Fei, and\nJ. Hays. Composing text and image for image retrieval - an\nempirical odyssey. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2019. 1, 2, 3, 4, 5, 6, 7, 11, 15\n[41] H. Xu, K. He, B. A. Plummer, L. Sigal, S. Sclaroff, and\nK. Saenko. Multilevel language and vision integration for\ntext-to-clip retrieval. In AAAI Conference on ArtiÔ¨Åcial Intel-\nligence, 2019. 2\n[42] C. Zhang, J. Y . Chai, and R. Jin. User term feedback in in-\nteractive text-based image retrieval. Proceedings of the 28th\nAnnual International ACM SIGIR Conference on Research\nand Development in Information Retrieval, 2005. 1, 2\n10\nSupplementary Material\nA. Implementation Details\nExisting methods on CIRR. As discussed in Sec. 5, we\nadopt the default conÔ¨Ågurations for state-of-the-art (SoTA)\nmethods when testing on our proposed dataset CIRR.\nSpeciÔ¨Åcally, for TIRG [40] and its corresponding base-\nlines (incl. Random, Image/text-only, Random Image+Text\nand Concatenation), we use ResNet18 pretrained on Ima-\ngeNet [22] as the image encoder, and a randomly initialized\nLSTM as the text encoder. We note that the above methods\ndo not beneÔ¨Åt from more complex ResNet features ( e.g.,\nResNet152) or word embedding initializations on CIRR.\nWe train the models using soft-triplet based loss [40], as we\ndiscover that the batch-based classiÔ¨Åcation loss introduces\nserious overÔ¨Åtting for TIRG on CIRR. For MAAF, follow-\ning Dodds et al. [8], we use a pretrained ResNet50 along\nwith an LSTM, while training the model with batch-based\nclassiÔ¨Åcation loss.\nNote that the implementations of MAAF and TIRG share\nthe same codebase. Hence, all methods above use a hidden\nsize of 512, and models are optimized with vanilla stochas-\ntic gradient descent (SGD) as in V oet al. [40].\nCIRPLANT on Fashion-IQ. We do not perform hyper-\nparameter tuning for our model on Fashion-IQ ( i.e., the\nsetup is kept the same as on CIRR, see Sec. 5 for details).\nAdditionally, since the three subtypes in Fashion-IQ distinct\ngreatly from each other, we sample each minibatch from a\nsingle subtype during training, as in Dodds et al. [8].\nB. Additional Metrics\nSee Table 5 on performance in mAP@K, where the com-\nparisons are similar to Recall (Table 3). Note that since our\ntask has only one true-positive for each query, Precision@K\nand Recall@Kare the same (hence P@Knot shown).\nmAP@K mAPSubset@K\nMethods K= 1K= 5K= 10K= 50K= 1K= 2K= 3\n8 TIRG [40] 14.61 25.05 27.25 28.63 22.67 33.25 39.9210 MAAF [8] 10.31 15.77 17.72 19.43 21.05 29.84 36.80\n14 Ours (no init.) 15.18 25.06 27.19 28.65 33.81 45.50 51.6015 Ours (init.) 19.55 30.40 32.55 33.85 39.20 50.68 56.28\nTable 5. mAP scores for SoTA methods (and ours) on CIRR. See\nTable 3 (corresp. row numbers) for comparison with Recall.\nC. Auxiliary Annotations in CIRR\nAs discussed in Sec. 4.1, following the collection of the\nmodiÔ¨Åcation sentences (main annotation), we additionally\ncollect auxiliary annotations for each image pair. The aux-\niliary annotations are meant to provide explicit training sig-\nnals that address the ambiguities caused by implicit human-\nagreements. Although we do not use such annotations in\nthis work, we believe that they can beneÔ¨Åt future work for\nclarifying and interpreting such ambiguities.\nCollection. For each pair of reference-target image, we\ncollect the answers to the following four questions from\nAmazon Mechanical Turk (AMT) workers, which tangibly\naddress the implicit ambiguities mentioned above:\nQ1 What characteristics of the objects are preserved across\nimages?\nQ2 What objects were changed, but not relevant to the\nmodifying sentence?\nQ3 Is there any change in camera angle/focus/viewpoint?\nQ4 Is there any change in the background/lighting across\nthe images?\nWe provide the AMT workers with the reference-target\nimage pair along with the collected modiÔ¨Åcation sentence\n(main annotation). For each question, workers can choose\nto answer with a sentence or mark as not applicable ( e.g.,\nnothing worth mentioning or already covered by the main\nannotation). Statistics are shown in Table 6. Collection\ninterface is shown in Fig. 12 (bottom), see examples in\nFig. 11.\nNb. imagesubsets Nb.pairs Nb. pairsper subset Nb.images Pairs with auxiliary (%)\nQ1 Q2 Q3 Q4\nTrain 3,345 28,225 7.54 16,939 66.72 68.09 48.06 58.45Val. 503 4,184 8.32 2,297 71.87 67.67 49.43 64.66Test 503 4,148 8.25 2,316 69.62 69.01 46.44 63.00\nTotal 4,351 36,554 8.40 21,552 67.65 68.15 48.02 59.69\nTable 6. Statistics of CIRR with auxiliary annotations. The visual\ncontents and the (main) annotation determine whether a pair also\nhas auxiliary annotations for Q1‚Äì4.\nD. Collection Details on CIRR\nWe provide additional details about our data collection\nprocedure (Sec. 4.1) including examples of each step (excl.\nthe auxiliary annotations, which is discussed in Sec. C).\nImage subsets. Fig. 5 shows the procedure for con-\nstructing an image subset of six elements, noted as S =\n{I1,...,I 6}in Sec. 4.1. We speciÔ¨Åcally demonstrate cases\nwhere images are removed. The process was designed to\nensure that images in a given subset are visually similar to\none another while exhibiting some appreciable differences.\nImage pairs. As explained in Sec. 4.1, we draw nine pairs\nfrom each subset. Fig. 6 demonstrates how we form consec-\nutive modiÔ¨Åcations among pairs, which could facilitate the\ntraining and evaluation of dialogue systems in the future.\nFig. 7 shows that one reference image leads to multiple tar-\ngets in each subset. This should allow the study of the im-\npact of language modality in the future.\n11\n(a) Randomly pick an image as I1 (leftmost), sort the remaining images in the large image corpus Dby their cosine similarity to I1 using ResNet\nfeatures pre-trained on ImageNet, noted as Œ∫i for Ii. Images are ranked from left to right.\nI1\nŒ∫i = 1.0 0.9981 0.8691 0.8663 0.8603 0.8490 0.8488 0.8456 0.8435 ...\n(b) Remove near-identical images with Œ∫i ‚â•0.94.\nI1 Removed\n\u0017\u0017\nŒ∫i = 1.0 0.9981 0.8691 0.8663 0.8603 0.8490 0.8488 0.8456 0.8435 ...\n(c) Select the next top-20 ranked images (not fully shown below).\nI1 ‚áêShifted ‚áítop-20\nŒ∫i = 1.0 0.8691 0.8663 0.8603 0.8490 0.8488 0.8456 0.8435 0.8421 ...\n(d) Greedily add each image as ranked. Meanwhile, to ensure sufÔ¨Åcient variations between images, skip an image if its Œ∫i is within 0.002 of the last\nadded image. We demonstrate the greedy process as below. In each step, curved arrow suggests a comparison of Œ∫i and Œ∫i+1, added image is marked\nwith a tick while skipped is crossed out.\nI1 I2\n\u0013\u0013\n \u0013\u0013\nŒ∫i = 1.0 0.8691 ...\n1.0‚àí0.8691>0.002\nI1 I2 I3\n\u0013\u0013\n \u0013\u0013\n \u0013\u0013\nŒ∫i = 1.0 0.8691 0.8663 ...\n0.8691‚àí0.8663>0.002\n¬∑¬∑¬∑\nI1 I2 I3 I4 I5 Skipped\n\u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0017\u0017\nŒ∫i = 1.0 0.8490 0.8488 ...\n0.8490‚àí0.8488‚â§0.002\nI1 I2 I3 I4 I5 Skipped I6\n\u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0013\u0013\n \u0017\u0017\n \u0013\u0013\nŒ∫i = 1.0 0.8490 0.8456 ...\n0.8490‚àí0.8456>0.002\n(e) Form an image subset S= {I1,...,I 6}if 6 images can be greedily added (true for this example), otherwise discard the entire set and restart at(a).\nI1 I2 I3 I4 I5 I6\nFigure 5. The procedure of forming an image subset as described in Sec. 4.1. We speciÔ¨Åcally show cases where images are removed or\nskipped. Note that after forming the subsets, we further Ô¨Ålter them to avoid heavy overlaps.\n12\nùë∞ùüèùë∞ùüêùë∞ùüëùë∞ùüíùë∞ùüìùë∞ùüîùë∞ùüèùë∞ùüêùë∞ùüëùë∞ùüíùë∞ùüìùë∞ùüî\nI1 I2 I3 I4 I5 I6\nI1 ‚ÜíI2: Turn on the Ô¨Çat screen tv in the living room.\nI2 ‚ÜíI3: Pull up the blinds to let in sunlight.\nI3 ‚ÜíI4: Put a window by the Ô¨Åreplace.\nI4 ‚ÜíI5: Have a window on the right-hand wall.\nI5 ‚ÜíI6: Have a bookshelf to the right of the window.\nI6 ‚ÜíI1: Have multiple television screens.\nFigure 6. Left: The six pairs we draw from a subset (in total we draw nine) that form a closed-loop dialogue. Each arrow represents a\nreference-to-target image pair with modiÔ¨Åcation sentences. Right: An example of consecutive modiÔ¨Åcation sentences that forms a dialogue.\nùë∞ùüèùë∞ùüêùë∞ùüëùë∞ùüíùë∞ùüìùë∞ùüîùë∞ùüèùë∞ùüêùë∞ùüëùë∞ùüíùë∞ùüìùë∞ùüî\nI1 I2 I3 I4 I5 I6\nI1 ‚ÜíI2: Turn on the Ô¨Çat screen tv in the living room.\nI1 ‚ÜíI3: A hall with two bright sofas and a brown table between them.\nI1 ‚ÜíI4: Room with a large window, a bright armchair and a Ô¨Åreplace.\nI1 ‚ÜíI5: A room with a large window and windowsill, a high sofa and shelves above it, instead of\na hall with one large sofa and four screens in front of it.\nFigure 7. Left: The four pairs we draw from a subset to have multiple outcomes from the same reference image. Each arrow represents a\nreference-to-target image pair with modiÔ¨Åcation sentences. Right: An example of the four pairs with the same reference image.\nùë∞ùüèùë∞ùüêùë∞ùüëùë∞ùüíùë∞ùüìùë∞ùüîùë∞‚Ä≤ùüèùë∞‚Ä≤ùüêùë∞‚Ä≤ùüëùë∞‚Ä≤ùüíùë∞‚Ä≤ùüìùë∞‚Ä≤ùüî\nI1 I2 I3 I4 I5 I6\nI‚Ä≤\n1 I‚Ä≤\n2 I‚Ä≤\n3 I‚Ä≤\n4 I‚Ä≤\n5 I‚Ä≤\n6\nFigure 8. An example of connecting pairs from two subsets to form longer dialogue paths. Note that in this example, I6 ‚â°I‚Ä≤\n1.\nWe point out that the length of dialogue paths can vary\nfor two reasons. First, we allow a slight overlap between the\nimages of two subsets. Therefore, it is possible to form di-\nalogue paths across subsets with variable lengths, as shown\nin Fig. 8. Second, AMT workers can mark pairs of poor\nquality and choose not to annotate them (see below). Such\npairs will be removed from the dataset, thus rendering the\ndialogue incomplete. In total, 71.1% of the subsets have\nclosed-loop dialogue paths (see Table 6 for detailed statis-\ntics).\nAnnotation collection on AMT. Table 8 demonstrates\nour guideline to AMT workers, specifying types of anno-\ntations to avoid.\nFig. 12 shows our collection interface. (top) For main\nannotations, we require AMT workers to write sentences\nthat only lead to the true target image, thus removing false-\nnegatives in each subset. We also allow them to mark image\npairs of poor quality for removal. (middle) For auxiliary\nannotations, we ask four detailed questions to clarify ambi-\nguities within the given pair. (bottom) We evaluate human\nretrieval performance in RecallSubset using the test-split.\nQuality control. We conduct a pre-selection process to\nmanually whitelist workers with good annotation quality.\nOur pre-selection procedure plays a critical role in quality\n13\nassurance, where we Ô¨Ålter out over 60% of the submitted\nworkers. Workers who have passed the selection process\nproduce annotations with over 90% acceptance rate.\nFor annotations submitted by workers in the whitelist,\nwe manually review ‚àº30% of the annotations from each\nworker. The remaining are examined with an automated\nscript to check for potential abuse of the use of checkboxes,\nirresponsible contents ( e.g., very short sentences), and an-\nnotations that violate our guidelines.\nHuman performance. Table 3 (row 7) lists the human re-\ntrieval performance of RecallSubset@1 (see Sec. 5 for details\nof the RecallSubset metric) on test-split. Here, we present the\ncollection procedures of this score.\nFig. 12 (bottom) shows the collection interface. Specif-\nically, we ask AMT workers to choose the most probable\ntarget image for a given text-image query. We employ three\ndifferent AMT workers for each pair in the test-split, our\nÔ¨Ånal score is calculated by averaging over all submitted re-\nsults.\nE. Additional Analysis on CIRR\nImage synsets. We analyze the image contents using the\nsynset information in NLVR2 [35]. CIRR includes 124 out\nof the 1000 synsets in NLVR 2. Each synset is associated\nwith 136.6 ¬±73.1 ( ¬µ ¬±œÉ) images. The Ô¨Åve most com-\nmon synsets are bookcase, bookshop, dobreman,\ntimber wolf and pug. The Ô¨Åve least common synsets\nare acorn, skunk, orange, ox, broccoli and\npadlock. Distributions of samples are shown in Fig. 9.\nNote that we do not distinguish synsets of similar con-\ncepts ( e.g., dobreman and French bulldog) when\nforming image pairs, instead, we choose by visual similar-\nity. Additionally, we point out that for composed image\nretrieval, synset may not fully characterize an image, as the\nannotations focus on Ô¨Åne-grained visual comparisons.\nComparison to existing datasets. Table 7 compares\nCIRR with existing datasets used for composed image re-\ntrieval. We demonstrate that CIRR is comparable in size\nwith existing datasets. Additionally, it provides rich auxil-\niary annotations for open-domain images.\nFalse-negative analysis. Fig. 10 demonstrates the pres-\nence of false-negatives in Fashion-IQ [12], as explained in\nSec. 4. For comparison, our data collection procedures en-\nsure that no false-negatives are present within each image\nsubset, as discussed in Sec. D. Examples of CIRR are shown\nin Fig. 6, Fig. 7, and Fig. 11.\nF. Additional Examples of CIRR\nWe provide additional examples from the dataset in\nFig. 11, where we demonstrate negative retrieval results\nfrom both TIRG and CIRPLANT. We point out that CIRR\nfocuses more on the challenging task of distinguishing\namong visually similar images. Let us note that the aux-\niliary annotations provide explicit interpretations of errors,\nparticularly regarding the implicit human-agreements in vi-\nsual and language modalities. This suggests that the anno-\ntations can be used for Ô¨Åne-grained analysis or as training\nsignals in future research on composed image retrieval.\nG. Dataset File Description\nTable 9 summarizes information we provide for each im-\nage pair.\n14\nDatasets Statistics Images Annotations Non-\nrepurposed\nAdditional\nannotationNb.\npairs\nNb. images Domain Natural\nimages\nPairing\nstrategy\nNatural\nlanguage\nDialogue\npaths\nExamples\n1 CSS [40] ‚Äì 38,069 ‚àó ‚Äì ‚Äì Make top-center blue\nobject yellow.\n\u0013\n2 MIT-States [18, 40] ‚Äì 63,440‚àó Entity states\u0013 by entity\nclass\n(Change state) to\nmelted.\n3 Fashion200k [13, 40] ‚Äì 338,372‚àó‚Ä† Fashion \u0013 by sim-\nilar\nattributes\n(Replace with) beige.\n4 Fashion-IQ [12] 30,122¬ß46,609‚Ä° Fashion \u0013 by prod-\nuct cate-\ngory\n\u0013 Is short sleeved and\nhas a collar.\n\u0013 Product\nattribute\n(partial)\n5 Birds-to-Words [8, 9] 3,347 ‚Äì Birds \u0013 by visual\nsimilarity\n\u0013 Animal1 is white with\ndark brown and white\nwings and a golden\nhead . Animal2 is\nbrown-gold with dark\nsolid-colored brown\nwings and a dark\nhead.\n6 Spot-the-Diff [8, 19] 23,089 ‚Äì Surveillance\nfootage\n\u0013 by video\nframe\n\u0013 A white truck has ap-\npeared in the after im-\nage. A person is now\nwalking on the foot-\npath.\n7 CIRR 36,554 21,552 Open \u0013 by visual\nsimilar-\nity\n\u0013 \u0013 Room with a large\nwindow, a bright\narmchair and a\nÔ¨Åreplace.\n\u0013 Auxiliary\nanno-\ntation\nclarifying\nambigui-\nties\n‚àóNb. pairs not pre-deÔ¨Åned, pairs are generated on-the-Ô¨Çy.\n‚Ä†Approx. 100,000 images have low detection score, thus could be removed [13]. Here, we show the available nb. images in total.\n¬ßEach pair has two sentences.\n‚Ä°Combining all three subtypes. Note that pairs and images overlap between subtypes.\nTable 7. Comparison between CIRR (bolded) and existing datasets for composed image retrieval. CIRR is comparable in size (nb. pairs)\nwhile containing richer annotations of open-domain images.\nTo avoid Examples\n1 Mentioning text/numbers Text on the pillow says ‚ÄúLOVE‚Äù.\n2 Discussing photo editing properties Crop the staircases out of the photo.\n3 Subjective opinions The dogs look very cute.\n4 Simply describing the target image, not comparing the pairHaving a large table in the center of the room.\n5 Simple side-by-side comparison The left image shows a laptop on the wooden table, the right image has a Ô¨Çatscreen.\n6 Writing sentences that are not unique for the given image pair in the subset ‚Äì\nTable 8. Types of annotations we discourage workers from writing. Rows 4 and 5 might be admissible if the annotation contains implicit\ncomparisons. (see Fig. 12 (top)).\n15\nacornskunkorangeoxbroccolipadlockIndian elephantmacawking penguindrumbeakerdining tablered wineAfghan hounddoughdung beetleLeonbergfluteBlenheim spanielstingrayyurtgolf balldumbbellcheetahsnowplowpaper towelchina cabinetsafety pinbananazebravulturejellyfishbindersea anemonelipstickbath towelthatchhartebeestItalian greyhoundblack-footed ferretcanoebeer bottlesoap dispenserlemonbassetcollieBorder terriervending machinellamaballoonwarthoggooselorikeetchimpanzeeminiature schnauzerhorse cartwater bottletriflesyringesea lionperfumeborzoilotionguinea pigcupyawlwild boarChihuahuaacademic gownpop bottleibexGreat PyreneesDungeness crabbikinielectric locomotivemalinoisbannistercellular telephonemarmotknee padschool buswindow shadepajamaconvertiblemittenchowmashed potatorestaurantmonasteryFrench bulldogmosquito netpelicangiant pandatelevisioncocker spanielLabrador retrieverhamsterbeaglepencil boxdingopizzavasebarbershoppillowhyenawater buffalolaptopbakerybaboonPembrokesliding doorrunning shoesaxmalamutegorillavizslaSamoyedwashbasindogsledpugtimber wolfDobermanbookshopbookcase\n0\n50\n100\n150\n200\n250\n300\n350\n400\nFigure 9. Number of examples per synset (sorted in ascending).\n16\n(a) Is shiny and silver with shorter sleeves + Ô¨Åt and Ô¨Çare.\n(b) Is less formal with different colored stripes + Does not have a collar.\n(c) Is a solid black color, also shorter and tighter Ô¨Åtting + Is black and more skimpy.\n(d) Has more grey and longer sleeves + Is lighter.\nFigure 10. Examples of false-negatives in Fashion-IQ [12]. First column shows the reference image. Each sample contains two modiÔ¨Åcation\nsentences. For each query set (reference image + modiÔ¨Åcation sentences), only one candidate image is labeled as the target. Thus, rendering\nthe remaining valid predictions as false-negatives.\n17\n(a)\nMain ‚Äì Goes from a black and white dog running to two dogs running.\nQ1 ‚Äì [N/A] Nothing worth mentioning\nQ2 ‚Äì Change to a brown-and-white dog and a black-and-white dog.\nQ3 ‚Äì [N/A] Nothing worth mentioning\nQ4 ‚Äì Make the grass a darer green.\n(b)\nMain ‚Äì Remove the concret to the right.\nQ1 ‚Äì Has marine animal in similar blue backdrop.\nQ2 ‚Äì Remove the blue thing on right.\nQ3 ‚Äì [N/A] Nothing worth mentioning\nQ4 ‚Äì [N/A] Nothing worth mentioning\n(c)\nMain ‚Äì Remove the seashells and make the water green.\nQ1 ‚Äì Shows manta rays.\nQ2 ‚Äì Make the rays older, spread the rays further apart.\nQ3 ‚Äì View straight on.\nQ4 ‚Äì [N/A] Covered in main annotation\n(d)\nMain ‚Äì More monkeys\nQ1 ‚Äì A group of monkeys side by side in same color.\nQ2 ‚Äì [N/A] Nothing worth mentioning\nQ3 ‚Äì More focused on the animals.\nQ4 ‚Äì [N/A] Nothing worth mentioning\nFigure 11. Negative results of TIRG and CIRPLANT on CIRR. Here, we show the Recall Subset rankings where we consider candidates\nfrom corresponding image subsets (see Sec. 5). First column shows the reference images. True targets are in green boxes. Each pair\ncontains a main annotation (Main) and four auxiliary annotations (Q1‚Äì4) as explained in Sec. 4.1 and Sec. C. We demonstrate errors of\nthe models where: (a) fails to associate text with both reference and target image; and (b‚Äìd) fails to identify and preserve implicit global\nvisual similarity. We show that CIRR focuses on the challenging task of distinguishing harder negatives that require Ô¨Åne-grained visual\nreasoning. Let us note that the errors can be explicitly interpreted with our auxiliary annotations (bolded), which previous datasets cannot.\nThis suggests that future work can leverage the auxiliary annotations for analysis of methods, and possibly training of models that account\nfor implicit human ambiguities.\n18\n/0 /1 /2 /3 /4 /2 /5/4 /6 /7 /i255 /9 /6 /10 /5/2 /1 /10 /i255 /11 /12 /13 /14/4 /15 /15 /2 /16 /i255 /13 /17 /i255 /18/19 /1 /20 /2 /1 /10\n/21 /22 /23 /24 /i255 /26/27 /24 /24 /28 /29 /27 /i255 /23 /24 /i255 /30 /31 /32 /33 /i255 /34 /23 /24 /23 /35 /32 /27 /i255 /36 /30 /i255 /33 /30 /37 /i255 /28 /31 /38 /i255 /39 /23 /32 /32 /i255 /31 /30 /36 /i255 /35 /27 /i255 /24 /22 /30 /39 /31 /i255 /36 /30 /i255 /40/30 /41 /42 /27 /41 /24 /43\n/44 /30 /37 /i255 /45 /28 /31 /i255 /36 /27 /24 /36 /i255 /45 /30 /26/46 /32 /27 /36 /23 /31 /29 /i255 /36 /22 /27 /i255 /36 /28 /24 /42 /i255 /35 /27 /32 /30 /39 /i255 /28 /31 /38 /i255 /45 /32 /23 /45 /42 /i255 /47 /48 /37 /35 /26/23 /36 /47 /i255 /23 /31 /i255 /30 /41 /38 /27 /41 /i255 /36 /30 /i255 /46 /41 /27 /34 /23 /27 /39 /i255 /36 /22 /27 /i255 /38 /28 /36 /28 /i255 /28 /31 /38 /i255 /49 /30 /41 /26/28 /36 /i255 /30 /49 /i255 /36 /22 /27 /i255 /24 /37 /35 /26/23 /36 /36 /27 /38 /i255 /41 /27 /24 /37 /32 /36 /24 /43\n/50 /6 /10 /15 /1 /12 /51 /15 /4 /19 /6 /10 /11 /52 /19 /1 /15 /51 /12 /15 /10\n/i255\n/50 /14/53 /7 /2 /i255 /54 /53 /4 /1 /i255 /55 /56 /57 /i255 /i255 /i255 /i255/i255/i255 /i255 /i255 /i255\n/58/2 /6 /15 /4 /19 /6 /4 /6 /7 /i255 /15 /2 /59 /15 /56 /6 /12 /14/13 /2 /1 /10 /i255 /23 /31 /i255 /36 /22 /27 /i255 /23 /26/28 /29 /27 /60 /i255 /27 /43 /29 /43 /61 /i255 /47 /21 /27 /62 /36 /i255 /30 /31 /i255 /36 /22 /27 /i255 /46 /23 /32 /32 /30 /39 /i255 /24 /28 /33 /24 /i255 /63/47\n/64 /23 /24 /45 /37 /24 /24 /23 /31 /29 /i255 /54 /52 /19 /15 /19 /i255 /2 /16 /4 /15 /4 /6 /7 /60 /i255 /27 /43 /29 /43 /61 /i255 /47 /65 /41 /30 /46 /i255 /36 /22 /27 /i255 /24 /36 /28 /23 /41 /24 /i255 /30 /37 /36 /43 /47 /i255 /30 /41 /i255 /47 /66 /38 /38 /i255 /28 /i255 /24 /36 /23 /45 /42 /27 /41 /i255 /32 /30 /29 /30 /i255 /28 /36 /i255 /36 /22 /27 /i255 /45 /30 /41 /31 /27 /41 /i255 /30 /49 /i255 /36 /22 /27 /i255 /46 /22 /30 /36 /30 /43 /47\n/11 /12 /13 /67 /2 /51 /15 /4 /3 /2 /i255 /19 /54 /4 /6 /4 /19 /6 /10 /60 /i255 /27 /43 /29 /43 /61 /i255 /47 /68/28 /42 /27 /i255 /36 /22 /27 /i255 /38 /30 /29 /i255 /32 /30 /30 /42 /i255 /45 /37 /36 /27 /41 /43 /47\n/69 /24 /23 /31 /29 /i255 /36 /22 /27 /i255 /36 /27 /41 /26/i255 /70 /71 /53 /1 /7 /2 /15 /i255 /50 /14/53 /7 /2 /56 /0 /52 /19 /15 /19 /70 /i255 /30 /41 /i255 /70 /15 /52 /2 /i255 /11 /2 /51 /19 /6 /16 /i255 /50 /14/53 /7 /2 /56 /0 /52 /19 /15 /19 /70 /60\n/11 /4 /14 /54 /72 /17 /i255 /16 /2 /10 /51 /1 /4 /13 /4 /6 /7 /i255 /36 /22 /27 /i255 /36 /28 /41 /29 /27 /36 /i255 /23 /26/28 /29 /27 /61 /i255 /31 /30 /36 /i255 /45 /30 /26/46 /28 /41 /23 /31 /29 /i255 /36 /22 /27 /i255 /46 /28 /23 /41 /43\n/18/1 /4 /15 /2 /i255 /53 /i255 /10 /2 /6 /15 /2 /6 /51 /2 /i255 /10 /19 /i255 /15 /52 /53 /15 /i255 /73\n/i255\n /i255\n/74 /27 /49 /27 /41 /27 /31 /45 /27 /i255 /75 /26/28 /29 /27 /i255 /76 /i255 /21 /28 /41 /29 /27 /36 /i255 /75 /26/28 /29 /27/i255\n/58/2 /53 /6 /5/52 /4 /72 /2 /77 /i255 /78 /79 /19 /15 /2 /80 /81 /i255 /17 /19 /12 /1 /i255 /10 /2 /6 /15 /2 /6 /51 /2 /i255 /16 /19 /i255 /6 /19 /15 /i255 /72 /2 /53 /16 /i255 /15 /19 /i255 /73\n/i255\n /i255\n /i255\n /i255\n/82 /i255 /83/36 /22 /27 /41 /i255 /24 /23 /26/23 /32 /28 /41 /i255 /23 /26/28 /29 /27 /24\n/84 /10 /2 /i255 /15 /52 /2 /i255 /54 /1 /19 /14/54 /15 /i255 /13 /2 /72 /19 /5/i255 /53 /10 /i255 /53 /i255 /7 /12 /4 /16 /2 /77 /i255 /51 /19 /14/54 /72 /2 /15 /2 /i255 /15 /52 /2 /i255 /1 /2 /10 /15 /i255 /19 /85 /i255 /15 /52 /2 /i255 /10 /2 /6 /15 /2 /6 /51 /2 /86 /86 /86\n/9 /72 /15 /2 /1 /6 /53 /15 /4 /3 /2 /72 /17 /77 /i255 /15 /52 /2 /i255 /71 /53 /1 /7 /2 /15 /i255 /50 /14/53 /7 /2 /i255 /4 /10 /86 /86 /86 /i255\n/i255\n/i255\n/69 /31 /32 /23 /42 /27 /i255 /36 /22 /27 /i255 /74 /27 /49 /27 /41 /27 /31 /45 /27 /i255 /75 /26/28 /29 /27 /61 /i255 /75 /i255 /39 /28 /31 /36 /i255 /36 /22 /27 /i255 /21 /28 /41 /29 /27 /36 /i255 /75 /26/28 /29 /27 /i255 /36 /30 /i255 /63/i255 /87 /i255 /36 /22 /27 /i255 /21 /28 /41 /29 /27 /36 /i255 /75 /26/28 /29 /27 /i255 /88 /23 /24 /87 /22 /28 /24 /87 /24 /22 /30 /39 /24 /89 /i255 /63\n/36 /22 /27 /i255 /24 /28 /26/27 /i255 /23 /26/28 /29 /27 /i255 /88 /30 /41 /i255 /28 /i255 /45 /41 /30 /46 /46 /27 /38 /i255 /34 /27 /41 /24 /23 /30 /31 /89 /i255 /28 /24 /i255 /36 /22 /27 /i255 /74 /27 /49 /27 /41 /27 /31 /45 /27 /i255 /75 /26/28 /29 /27 /43\n/31 /30 /36 /i255 /36 /22 /27 /i255 /24 /28 /26/27 /61 /i255 /35 /37 /36 /i255 /22 /28 /24 /i255 /31 /30 /i255 /38 /23 /49 /49 /27 /41 /27 /31 /45 /27 /24 /i255 /33 /30 /37 /i255 /45 /28 /31 /i255 /46 /23 /45 /42 /i255 /37 /46 /i255 /36 /30 /i255 /38 /27 /24 /45 /41 /23 /35 /27 /43\n/65 /83/68 /90 /91 /92 /21 /92 /91 /44 /i255 /23 /41 /41 /27 /32 /27 /34 /28 /31 /36 /i255 /45 /30 /31 /36 /27 /31 /36 /24 /61 /i255 /24 /37 /45 /22 /i255 /28 /24 /i255 /34 /27 /41 /33 /i255 /38 /23 /49 /49 /27 /41 /27 /31 /36 /i255 /36 /33 /46 /27 /i255 /30 /49 /i255 /30 /35 /93 /27 /45 /36 /24 /i255 /30 /41 /i255 /24 /45 /27 /31 /27 /24 /43\n/11 /12 /13 /14/4 /15 /i255 /53 /72 /72\n/0 /1/2 /3 /4 /i255 /6 /2 /7 /8 /i255 /9 /10 /11 /i255 /i255 /i255 /i255 /i255/i255 /i255 /i255 /i255\n/i255 /i255\n/14 /15 /16 /17 /18 /19 /20 /i255 /21 /16 /20 /i255 /17 /18 /19 /22 /20 /i255 /21 /23 /i255 /24 /25 /26 /20 /27 /i255 /17 /26 /26 /i255 /24 /21 /20 /20 /22 /i255 /16 /17 /18 /26 /28 /17 /25 /22 /29 /14\n/30 /31 /2 /1/7 /32 /4 /i255 /33 /34 /4 /i255 /2 /35 /36 /37 /4 /i255 /7 /1/2 /3 /4 /i255 /6 /2 /7 /8 /i255 /2 /32 /38 /i255 /33 /34 /4 /i255 /1/36 /38 /7 /39 /40 /7 /32 /3 /i255 /41 /4 /32 /33 /4 /32 /42 /4 /43 /i255 /2 /32 /41 /44/4 /8 /i255 /4 /2 /42 /34 /i255 /45 /46 /4 /41 /33 /7 /36 /32 /i255 /35 /40 /i255 /4 /7 /33 /34 /4 /8 /i255 /44/8 /7 /33 /7 /32 /3 /i255 /2 /i255 /41 /4 /32 /33 /4 /32 /42 /4 /43 /i255 /36 /8 /i255 /42 /47 /7 /42 /48 /7 /32 /3 /i255 /49 /36 /32 /4 /i255 /36 /39 /50 /i255 /33 /34 /4 /i255 /35 /46 /33 /33 /36 /32 /49 /41 /50 /i255 /35 /4 /47 /36 /44/51\n/52/9 /i255 /53/16 /17 /21 /i255 /54 /16 /17 /28 /17 /54 /21 /20 /28 /25 /24 /21 /25 /54 /24 /i255 /23 /55 /i255 /21 /16 /20 /i255 /23 /56 /57 /20 /54 /21 /24 /i255 /17 /28 /20 /i255 /58 /28 /20 /24 /20 /28 /59 /20 /26 /i255 /17 /54 /28 /23 /24 /24 /i255 /25 /60/17 /19 /20 /24 /27 /i255 /56 /61 /21 /i255 /16 /17 /24 /18 /62 /21 /i255 /56 /20 /20 /18 /i255 /60/20 /18 /21 /25 /23 /18 /20 /26 /i255 /25 /18 /i255 /21 /16 /20 /i255 /60/23 /26 /25 /55 /63 /25 /18 /19 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /64\n/i255\n/65 /i255 /66 /20 /i255 /60 /23 /28 /20 /i255 /24 /58 /20 /54 /25 /55 /25 /54 /27 /i255 /67 /28 /25 /21 /20 /i255 /20 /29 /19 /29 /27 /i255 /14 /68 /16 /20 /i255 /67 /23 /23 /26 /20 /18 /i255 /21 /17 /56 /22 /20 /14 /27 /i255 /25 /18 /24 /21 /20 /17 /26 /i255 /23 /55 /i255 /14 /68 /16 /20 /i255 /21 /17 /56 /22 /20 /14 /29 /i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /21 /17 /22 /72 /i255 /17 /56 /23 /61 /21 /i255 /56 /17 /54 /72 /19 /28 /23 /61 /18 /26 /i255 /54 /23 /18 /21 /20 /18 /21 /i255 /25 /18 /i255 /21 /16 /25 /24 /i255 /73 /61 /20 /24 /21 /25 /23 /18 /29 /i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /21 /17 /22 /72 /i255 /17 /56 /23 /61 /21 /i255 /21 /16 /25 /18 /19 /24 /i255 /21 /16 /17 /21 /i255 /17 /28 /20 /i255 /54 /16 /17 /18 /19 /20 /26 /27 /i255 /55 /23 /54 /61 /24 /i255 /23 /18 /i255 /21 /16 /20 /i255 /74 /75 /76 /77 /76 /75 /78 /76 /69 /i255 /17 /24 /58 /20 /54 /21 /24 /29\n/i255\n/52/79 /i255 /53/16 /17 /21 /i255 /23 /56 /57 /20 /54 /21 /24 /i255 /67 /20 /28 /20 /i255 /54 /16 /17 /18 /19 /20 /26 /27 /i255 /56 /61 /21 /i255 /18 /23 /21 /i255 /28 /20 /22 /20 /59 /17 /18 /21 /i255 /21 /23 /i255 /21 /16 /20 /i255 /60/23 /26 /25 /55 /63 /25 /18 /19 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /i255 /80 /67 /20 /i255 /17 /28 /20 /i255 /22 /23 /23 /72 /25 /18 /19 /i255 /55 /23 /28 /i255 /14 /18 /20 /19 /22 /25 /19 /56 /22 /20 /i255 /54 /16 /17 /18 /19 /20 /24 /14 /i255 /16 /20 /28 /20 /81 /64\n/i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /24 /25 /60/58 /22 /63 /i255 /22 /25 /24 /21 /25 /18 /19 /i255 /21 /16 /20 /i255 /23 /56 /57 /20 /54 /21 /24 /29 /i255 /82 /18 /24 /21 /20 /17 /26 /27 /i255 /67 /28 /25 /21 /20 /i255 /26 /23 /67 /18 /i255 /21 /16 /23 /24 /20 /i255 /54 /16 /17 /18 /19 /20 /24 /i255 /21 /16 /17 /21 /i255 /21 /23 /23 /72 /i255 /58 /22 /17 /54 /20 /i255 /25 /18 /i255 /26 /20 /21 /17 /25 /22 /24 /29 /i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /21 /17 /22 /72 /i255 /17 /56 /23 /61 /21 /i255 /56 /17 /54 /72 /19 /28 /23 /61 /18 /26 /i255 /54 /23 /18 /21 /20 /18 /21 /i255 /25 /18 /i255 /21 /16 /25 /24 /i255 /73 /61 /20 /24 /21 /25 /23 /18 /29 /i255\n/65 /i255 /83 /i255 /62 /15 /16 /17 /18 /19 /20 /62 /i255 /60/20 /17 /18 /24 /i255 /54 /16 /17 /18 /19 /20 /i255 /55 /28 /23 /60/i255 /21 /16 /20 /i255 /22 /20 /55 /21 /80 /21 /23 /58 /81 /i255 /25 /60/17 /19 /20 /i255 /21 /23 /i255 /21 /16 /20 /i255 /28 /25 /19 /16 /21 /80 /56 /23 /21 /21 /23 /60/81 /i255 /25 /60/17 /19 /20 /29\n/i255\n/52/11 /i255 /82 /24 /i255 /21 /16 /20 /28 /20 /i255 /17 /18 /63 /i255 /54 /16 /17 /18 /19 /20 /i255 /25 /18 /i255 /54 /17 /60/20 /28 /17 /i255 /17 /18 /19 /22 /20 /i255 /84 /i255 /55 /23 /54 /61 /24 /i255 /84 /i255 /59 /25 /20 /67 /58 /23 /25 /18 /21 /64\n/i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /54 /23 /60/58 /17 /28 /20 /i255 /21 /16 /20 /i255 /25 /60/17 /19 /20 /24 /i255 /24 /25 /26 /20 /85 /56 /63 /85 /24 /25 /26 /20 /29\n/i255/65 /i255 /82 /18 /24 /21 /20 /17 /26 /27 /i255 /67 /28 /25 /21 /20 /i255 /21 /16 /20 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /i255 /21 /16 /17 /21 /i255 /26 /20 /24 /54 /28 /25 /56 /20 /24 /i255 /21 /16 /20 /i255 /54 /16 /17 /18 /19 /20 /24 /i255 /55 /28 /23 /60/i255 /21 /16 /20 /i255 /22 /20 /55 /21 /80 /21 /23 /58 /81 /i255 /21 /23 /i255 /21 /16 /20 /i255 /28 /25 /19 /16 /21 /80 /56 /23 /21 /21 /23 /60/81 /i255 /25 /60/17 /19 /20 /29\n/i255\n/i255\n/i255\n/52/86 /i255 /82 /24 /i255 /21 /16 /20 /28 /20 /i255 /17 /18 /63 /i255 /54 /16 /17 /18 /19 /20 /i255 /25 /18 /i255 /21 /16 /20 /i255 /56 /17 /54 /72 /19 /28 /23 /61 /18 /26 /i255 /17 /54 /28 /23 /24 /24 /i255 /21 /16 /20 /i255 /25 /60/17 /19 /20 /24 /64\n/i255\n/65 /i255 /69 /23 /i255 /70 /71/68 /i255 /54 /23 /60/58 /17 /28 /20 /i255 /21 /16 /20 /i255 /25 /60/17 /19 /20 /24 /i255 /24 /25 /26 /20 /85 /56 /63 /85 /24 /25 /26 /20 /29\n/i255/65 /i255 /82 /18 /24 /21 /20 /17 /26 /27 /i255 /67 /28 /25 /21 /20 /i255 /21 /16 /20 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /i255 /21 /16 /17 /21 /i255 /26 /20 /24 /54 /28 /25 /56 /20 /24 /i255 /21 /16 /20 /i255 /54 /16 /17 /18 /19 /20 /24 /i255 /55 /28 /23 /60/i255 /21 /16 /20 /i255 /22 /20 /55 /21 /80 /21 /23 /58 /81 /i255 /21 /23 /i255 /21 /16 /20 /i255 /28 /25 /19 /16 /21 /80 /56 /23 /21 /21 /23 /60/81 /i255 /25 /60/17 /19 /20 /29\n/i255\n/i255\n/i255\n/20 /29 /19 /29 /27 /i255 /87 /17 /59 /25 /18 /19 /i255 /21 /67 /23 /i255 /26 /61 /54 /72 /24 /i255 /23 /55 /i255 /21 /16 /20 /i255 /24 /17 /60/20 /i255 /56 /28 /20 /20 /26 /i255 /84 /i255 /66 /20 /25 /18 /19 /i255 /17 /i255 /58 /16 /23 /21 /23 /i255 /23 /55 /i255 /17 /i255 /56 /17 /21 /16 /28 /23 /23 /60/i255 /88\n/70 /23 /21 /16 /25 /18 /19 /i255 /25 /24 /i255 /58 /28 /20 /24 /20 /28 /59 /20 /26 /27 /i255 /23 /28 /i255 /20 /59 /20 /28 /63 /21 /16 /25 /18 /19 /i255 /25 /24 /i255 /17 /22 /28 /20 /17 /26 /63 /i255 /54 /23 /59 /20 /28 /20 /26 /i255 /25 /18 /i255 /21 /16 /20 /i255 /19 /25 /59 /20 /18 /i255 /21 /20 /89 /21 /29 /i255 /80 /71/18 /22 /63 /i255 /61 /24 /20 /i255 /25 /18 /i255 /28 /17 /28 /20 /i255 /23 /54 /54 /17 /21 /25 /23 /18 /24 /83 /81\n/80 /83 /i255 /90 /23 /22 /22 /23 /67 /i255 /21 /16 /20 /24 /20 /i255 /20 /89 /17 /60/58 /22 /20 /24 /i255 /83 /81 /i255 /20 /29 /19 /29 /27 /i255 /91 /26 /26 /i255 /17 /i255 /54 /23 /22 /22 /17 /28 /i255 /21 /23 /i255 /21 /16 /20 /i255 /26 /23 /19 /27 /i255 /17 /18 /26 /i255 /54 /16 /17 /18 /19 /20 /i255 /21 /16 /20 /i255 /54 /23 /22 /23 /28 /i255 /23 /55 /i255 /25 /21 /24 /i255 /20 /63 /20 /i255 /21 /23 /i255 /56 /22 /17 /54 /72 /i255 /88\n/68 /16 /20 /28 /20 /i255 /25 /24 /i255 /18 /23 /21 /16 /25 /18 /19 /i255 /21 /23 /i255 /56 /20 /i255 /26 /20 /24 /54 /28 /25 /56 /20 /26 /29\n/70 /23 /27 /i255 /21 /16 /20 /28 /20 /i255 /25 /24 /18 /62 /21 /i255 /17 /18 /63 /i255 /54 /16 /17 /18 /19 /20 /i255 /67 /23 /28 /21 /16 /i255 /60/20 /18 /21 /25 /23 /18 /25 /18 /19 /29\n/92 /20 /24 /27 /i255 /17 /18 /26 /i255 /25 /21 /i255 /16 /17 /24 /i255 /17 /22 /28 /20 /17 /26 /63 /i255 /56 /20 /20 /18 /i255 /54 /23 /59 /20 /28 /20 /26 /i255 /25 /18 /i255 /21 /16 /20 /i255 /60/23 /26 /25 /55 /63 /25 /18 /19 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /29\n/92 /20 /24 /27 /i255 /66 /93 /68 /i255 /25 /21 /i255 /16 /17 /24 /18 /62 /21 /i255 /56 /20 /20 /18 /i255 /60/20 /18 /21 /25 /23 /18 /20 /26 /i255 /63 /20 /21 /i255 /80 /58 /22 /20 /17 /24 /20 /i255 /67 /28 /25 /21 /20 /i255 /21 /16 /20 /60/i255 /26 /23 /67 /18 /81 /94\n/20 /29 /19 /29 /27 /i255 /95/23 /28 /20 /i255 /55 /23 /54 /61 /24 /20 /26 /i255 /23 /18 /i255 /21 /16 /20 /i255 /58 /25 /22 /22 /23 /67 /24 /i255 /84 /i255 /54 /16 /17 /18 /19 /20 /i255 /21 /23 /i255 /17 /i255 /22 /23 /67 /20 /28 /i255 /17 /18 /19 /22 /20 /i255 /23 /55 /i255 /24 /16 /23 /21 /i255 /88\n/70 /23 /27 /i255 /21 /16 /20 /28 /20 /i255 /25 /24 /18 /62 /21 /i255 /17 /18 /63 /i255 /54 /16 /17 /18 /19 /20 /i255 /67 /23 /28 /21 /16 /i255 /60/20 /18 /21 /25 /23 /18 /25 /18 /19 /29\n/92 /20 /24 /27 /i255 /17 /18 /26 /i255 /25 /21 /i255 /16 /17 /24 /i255 /17 /22 /28 /20 /17 /26 /63 /i255 /56 /20 /20 /18 /i255 /54 /23 /59 /20 /28 /20 /26 /i255 /25 /18 /i255 /21 /16 /20 /i255 /60/23 /26 /25 /55 /63 /25 /18 /19 /i255 /24 /20 /18 /21 /20 /18 /54 /20 /29\n/92 /20 /24 /27 /i255 /66 /93 /68 /i255 /25 /21 /i255 /16 /17 /24 /18 /62 /21 /i255 /56 /20 /20 /18 /i255 /60/20 /18 /21 /25 /23 /18 /20 /26 /i255 /63 /20 /21 /i255 /80 /58 /22 /20 /17 /24 /20 /i255 /67 /28 /25 /21 /20 /i255 /21 /16 /20 /60/i255 /26 /23 /67 /18 /81 /94\n/20 /29 /19 /29 /27 /i255 /66 /17 /54 /72 /19 /28 /23 /61 /18 /26 /i255 /24 /16 /23 /61 /22 /26 /i255 /54 /23 /18 /21 /17 /25 /18 /i255 /19 /28 /17 /24 /24 /i255 /84 /i255 /17 /26 /26 /i255 /17 /i255 /67 /25 /18 /26 /23 /67 /i255 /25 /18 /i255 /21 /16 /20 /i255 /56 /17 /54 /72 /19 /28 /23 /61 /18 /26 /i255 /88\n/96 /46 /35 /1/7 /33 /i255 /2 /47 /47\n/0 /0 /1 /i255 /0 /0 /1\n/3 /4/5 /6 /7 /i255 /9 /5 /10 /11 /i255 /12 /13 /14 /i255 /i255 /i255 /i255 /i255/i255 /i255 /i255 /i255\n/15 /16 /17 /6 /11 /5 /18 /19 /20 /5 /18 /10 /16 /17 /21 /i255 /16 /17 /i255 /9 /5 /21 /21 /10 /17 /6 /i255 /16 /19 /11 /i255 /21 /7 /20 /7 /22 /18 /10 /16 /17 /i255 /9 /11 /16 /22 /7 /21 /21 /23 /i255 /24 /25 /5 /17 /26 /i255 /27 /16 /19 /i255 /28 /16 /11 /i255 /27 /16 /19 /11 /i255 /10 /17 /18 /7 /11 /7 /21 /18 /i255 /10 /17 /i255 /16 /19 /11 /i255 /18 /5 /21 /26 /23\n/29 /30 /31 /i255 /32 /33 /34 /33 /31 /35 /i255 /36 /33 /35 /37 /34 /38 /30 /39 /37 /i255 /40 /i255 /37 /33 /41 /41 /35 /37 /34 /38 /30 /39 /37 /i255 /42 /43 /35 /44 /37 /35 /i255 /45 /30 /39 /34 /44 /45 /34 /i255 /33 /37 /i255 /44 /34 /i255 /44 /45 /31 /46 /47 /44 /34 /33 /31 /48 /35 /31 /49/41 /50/44 /38 /43 /47 /45 /30 /50\n/3 /17 /21 /18 /11 /19 /22 /18 /10 /16 /17 /21 /51\n/52/38 /46 /35 /39 /i255 /44 /i255 /53 /7 /28 /7 /11 /7 /17 /22 /7 /i255 /3 /4/5 /6 /7 /i255 /44 /39 /54 /i255 /44 /i255 /4/16 /55 /10 /28 /27 /10 /17 /6 /i255 /21 /7 /17 /18 /7 /17 /22 /7 /i255 /56 /57 /7 /20 /16 /58/59 /60 /i255 /61 /62 /38 /45 /62 /i255 /54 /35 /37 /45 /31 /38 /63 /35 /37 /i255 /37 /30 /50/35 /i255 /54 /35 /37 /38 /31 /35 /54 /i255 /45 /62 /44 /39 /41 /35 /37 /64\n/65/35 /i255 /61 /44 /39 /34 /i255 /34 /30 /i255 /32 /38 /39 /54 /66 /i255 /34 /62 /35 /i255 /63 /35 /37 /34 /i255 /50/44 /34 /45 /62 /35 /54 /i255 /24 /5 /11 /6 /7 /18 /i255 /3 /4/5 /6 /7 /i255 /32 /31 /30 /50/i255 /34 /62 /35 /i255 /32 /38 /46 /35 /i255 /45 /44 /39 /54 /38 /54 /44 /34 /35 /37 /47\n/3 /17 /i255 /16 /18 /25 /7 /11 /i255 /58/16 /11 /55 /21 /51\n/67 /62 /30 /30 /37 /35 /i255 /34 /62 /35 /i255 /38 /50/44 /41 /35 /i255 /34 /62 /44 /34 /66\n/45 /30 /39 /34 /44 /38 /39 /37 /i255 /34 /62 /35 /i255 /54 /35 /37 /38 /31 /35 /54 /i255 /45 /62 /44 /39 /41 /35 /37 /i255 /68 /54 /35 /37 /45 /31 /38 /63 /35 /54 /i255 /38 /39 /i255 /34 /35 /69 /34 /70 /60\n/50/35 /44 /39 /61 /62 /38 /43 /35 /i255 /31 /35 /50/44 /38 /39 /37 /i255 /44 /37 /i255 /45 /43 /30 /37 /35 /43 /71 /i255 /44 /37 /i255 /42 /30 /37 /37 /38 /63 /43 /35 /i255 /34 /30 /i255 /34 /62 /35 /i255 /72 /35 /32 /35 /31 /35 /39 /45 /35 /i255 /73 /50/44 /41 /35 /47\n/74 /20 /7 /5 /21 /7 /i255 /21 /7 /7 /i255 /7 /75 /5 /4/9 /20 /7 /21 /i255 /16 /17 /i255 /58/25 /5 /18 /i255 /58/7 /i255 /4/7 /5 /17 /23\n/3 /17 /i255 /18 /25 /7 /i255 /22 /5 /21 /7 /i255 /58/25 /7 /11 /7 /i255 /27 /16 /19 /i255 /5 /11 /7 /i255 /11 /7 /5 /20 /20 /27 /i255 /19 /17 /21 /19 /11 /7 /i255 /58/25 /10 /22 /25 /i255 /18 /16 /i255 /22 /25 /16 /16 /21 /7 /51\n/73 /32 /i255 /71 /30 /33 /i255 /34 /62 /38 /39 /48 /i255 /34 /61 /30 /i255 /30 /31 /i255 /50/30 /31 /35 /i255 /38 /50/44 /41 /35 /37 /i255 /44 /31 /35 /i255 /76 /77 /78 /79 /80 /80 /81 /i255 /44 /42 /42 /43 /38 /45 /44 /63 /43 /35 /60 /i255 /45 /62 /30 /30 /37 /35 /i255 /35 /38 /34 /62 /35 /31 /47\n/73 /32 /i255 /45 /44 /39 /54 /38 /54 /44 /34 /35 /37 /i255 /45 /30 /39 /34 /44 /38 /39 /i255 /34 /61 /30 /i255 /30 /31 /i255 /50/30 /31 /35 /i255 /38 /54 /35 /39 /34 /38 /45 /44 /43 /i255 /38 /50/44 /41 /35 /37 /60 /i255 /45 /62 /30 /30 /37 /35 /i255 /35 /38 /34 /62 /35 /31 /47\n/3 /17 /18 /7 /11 /28 /5 /22 /7 /51\n/82 /37 /35 /i255 /63 /30 /34 /34 /30 /50/i255 /63 /33 /34 /34 /30 /39 /37 /i255 /30 /31 /i255 /37 /62 /30 /31 /34 /45 /33 /34 /i255 /48 /35 /71 /37 /i255 /68\n /83 /i255 /30 /31 /i255\n /69 /70 /i255 /34 /30 /i255 /39 /44 /46 /38 /41 /44 /34 /35 /i255 /44 /50/30 /39 /41 /i255 /42 /44 /38 /31 /37 /47\n/84 /30 /i255 /83 /30 /30 /50/i255 /38 /39 /i255 /30 /39 /i255 /34 /62 /35 /i255 /38 /50/44 /41 /35 /66 /i255 /31 /38 /41 /62 /34 /i255 /45 /43 /38 /45 /48 /i255 /30 /39 /i255 /38 /34 /60 /i255 /34 /62 /35 /39 /i255\n /85/42 /35 /39 /i255 /38 /50/44 /41 /35 /i255 /38 /39 /i255 /39 /35 /61 /i255 /34 /44 /63 /47\n/86 /33 /63 /50/38 /34 /i255 /44 /32 /34 /35 /31 /i255 /32 /38 /39 /38 /37 /62 /i255 /44 /43 /43 /i255 /42 /44 /38 /31 /37 /47\n/i255 /i255\n/89 /31 /35 /50/30 /46 /35 /i255 /44 /43 /43 /i255 /63 /33 /34 /i255 /30 /39 /35 /i255 /54 /30 /41 /i255 /44 /39 /54 /i255 /44 /54 /54 /i255 /44 /i255 /61 /30 /50/44 /39 /i255 /62 /33 /41 /41 /38 /39 /41 /i255 /38 /34 /89\n/15 /25 /16 /16 /21 /7 /i255 /18 /25 /7 /i255 /4/16 /21 /18 /i255 /5 /9 /9 /11 /16 /9 /11 /10 /5 /18 /7 /i255 /18 /5 /11 /6 /7 /18 /i255 /10 /4/5 /6 /7 /51 /i255 /68 /73 /34 /i255 /37 /62 /30 /33 /43 /54 /i255 /45 /30 /39 /34 /44 /38 /39 /37 /i255 /34 /62 /35 /i255 /54 /35 /37 /45 /31 /38 /63 /35 /54 /i255 /45 /62 /44 /39 /41 /35 /37 /60 /i255 /44 /39 /54 /i255 /37 /34 /38 /43 /43 /i255 /31 /35 /37 /35 /50/63 /43 /35 /37 /i255 /34 /62 /35 /i255 /72 /35 /32 /35 /31 /35 /39 /45 /35 /i255 /73 /50/44 /41 /35 /i255 /38 /39 /i255 /30 /34 /62 /35 /31 /i255 /61 /44 /71 /37 /70\n/i255/i255 /i255 /i255\n/i255\n/74 /11 /7 /90 /10 /7 /58/10 /17 /6 /i255 /91 /17 /21 /58/7 /11 /21 /i255 /92 /19 /57 /4/10 /18 /18 /7 /55 /i255 /57 /27 /i255 /93/16 /11 /26 /7 /11 /21\n/84 /62 /38 /37 /i255 /50/35 /37 /37 /44 /41 /35 /i255 /38 /37 /i255 /30 /39 /43 /71 /i255 /46 /38 /37 /38 /63 /43 /35 /i255 /34 /30 /i255 /71 /30 /33 /i255 /44 /39 /54 /i255 /61 /38 /43 /43 /i255 /39 /30 /34 /i255 /63 /35 /i255 /37 /62 /30 /61 /39 /i255 /34 /30 /i255 /65/30 /31 /48 /35 /31 /37 /47\n/94 /30 /33 /i255 /45 /44 /39 /i255 /34 /35 /37 /34 /i255 /45 /30 /50/42 /43 /35 /34 /38 /39 /41 /i255 /34 /62 /35 /i255 /34 /44 /37 /48 /i255 /63 /35 /43 /30 /61 /i255 /44 /39 /54 /i255 /45 /43 /38 /45 /48 /i255 /89 /86 /33 /63 /50/38 /34 /89 /i255 /38 /39 /i255 /30 /31 /54 /35 /31 /i255 /34 /30 /i255 /42 /31 /35 /46 /38 /35 /61 /i255 /34 /62 /35 /i255 /54 /44 /34 /44 /i255 /44 /39 /54 /i255 /32 /30 /31 /50/44 /34 /i255 /30 /32 /i255 /34 /62 /35 /i255 /37 /33 /63 /50/38 /34 /34 /35 /54 /i255 /31 /35 /37 /33 /43 /34 /37 /47\n/92 /19 /57 /4/10 /18 /i255 /5 /20 /20\nFigure 12. Snapshots of our collection interface (recommend viewing digitally by zooming in). (top) (Main) annotation, where we speciÔ¨Å-\ncally require unique sentences within each subset. (middle) Auxiliary annotation, where we ask Amazon Mechanical Turk (AMT) workers\nof four detailed questions per pair. (bottom) Human performance (Recall Subset@1) evaluation on test-split, where we ask AMT workers to\nchoose the most probable target image within the subset.\n19\nIdentiÔ¨Åers (keys) Explanations Content Details (values) Examples\n1 pairid Unique pair id‚àó 12554\n2 reference Reference image\nFollow NLVR2[35] image naming conventions.\n\"dev-147-2-img0\"\n3 targethard Target image¬ß \"dev-846-2-img0\"\n4 targetsoft Target image with additional\nlabeling (if exists)¬ß‚Ä†\n{dev-846-2-img0\": 1.0,\n\"dev-743-3-img0\": -1.0}\n5 caption (Main) annotation \"Catch the crab in the\ncircular ring and place them\non the metal table.\"\n6 captionextendAuxiliary annotation‚Ä°\n7 0 Q1 Begin with[c]if N/A. \"[c] None existed\"\n8 1 Q2 \"We don‚Äôt see the gloved hands\nof the fisherman\"\n9 2 Q3 Begin with[cr0]if Nothing worth mentioning,\nbegin with[cr1]if Covered in brief annotation.\n\"Focus on the net full of\ncrabs\"\n10 3 Q4 \"[cr0] Nothing worth\nmentioning\"\n11 imgset Subset information\n12 id Unique subset id 106\n13 members Images within subset Follow NLVR2[35] image naming conventions.[\"dev-147-2-img0\",\n\"dev-224-1-img1\",\n\"dev-410-2-img0\",\n\"dev-743-3-img0\",\n\"dev-846-2-img0\",\n\"dev-998-1-img0\"]\n14 referencerankSequence identiÔ¨Åer\nas in Fig. 6¬ß Range from 0 to 5, correspond toI1-I6. 0\n15 targetrank 1\n‚àóUsed for cross-referencing image pairs between cap.sample.json and cap.ext.sample.json.\n‚Ä†See Fig. 12 (a) for the three possible labels. When constructing target soft, images labelled as [The same image] is added as 1.0, [No differences worth\nmentioning] is added as 0.5, [Images that are too different] is added as -1.0.\n‚Ä°See Fig. 12 (b) for the options we provide for AMT workers.\n¬ßNot public for test-split. Instead, see our project website for the test-split evaluation server.\nTable 9. Data structure as in the data Ô¨Åles. For details please refer to our project website.\n20"
}