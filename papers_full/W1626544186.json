{
    "title": "Improving Statistical Language Model Performance with Automatically Generated Word Hierarchies",
    "url": "https://openalex.org/W1626544186",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2766384824",
            "name": "John G. McMahon",
            "affiliations": [
                "Queen's University Belfast"
            ]
        },
        {
            "id": "https://openalex.org/A2113938778",
            "name": "Francis J. Smith",
            "affiliations": [
                "Queen's University Belfast"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2084531783",
        "https://openalex.org/W2099345940",
        "https://openalex.org/W1966812932",
        "https://openalex.org/W2163514362",
        "https://openalex.org/W2127314673",
        "https://openalex.org/W1518059413",
        "https://openalex.org/W144990771",
        "https://openalex.org/W136130055",
        "https://openalex.org/W1984480735",
        "https://openalex.org/W2059800182",
        "https://openalex.org/W1767837355",
        "https://openalex.org/W1572705468",
        "https://openalex.org/W2061271742",
        "https://openalex.org/W1924403233",
        "https://openalex.org/W125820043",
        "https://openalex.org/W3022556212",
        "https://openalex.org/W1507680813",
        "https://openalex.org/W2167434254",
        "https://openalex.org/W2170206653",
        "https://openalex.org/W80619934",
        "https://openalex.org/W2082092506",
        "https://openalex.org/W2096071381",
        "https://openalex.org/W1495952636",
        "https://openalex.org/W2095958485",
        "https://openalex.org/W2040004971",
        "https://openalex.org/W2099111195",
        "https://openalex.org/W2107029693",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W2079656678",
        "https://openalex.org/W2110993209",
        "https://openalex.org/W1990005915",
        "https://openalex.org/W1542847127",
        "https://openalex.org/W2118172714",
        "https://openalex.org/W2100796029",
        "https://openalex.org/W2086699924",
        "https://openalex.org/W2012837062",
        "https://openalex.org/W2075201173",
        "https://openalex.org/W1516391399",
        "https://openalex.org/W2008422177",
        "https://openalex.org/W2150583827",
        "https://openalex.org/W2114930830",
        "https://openalex.org/W2024060531",
        "https://openalex.org/W2149741699",
        "https://openalex.org/W2127836646",
        "https://openalex.org/W2105594594",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2143685721",
        "https://openalex.org/W2055528812"
    ],
    "abstract": "An automatic word classification system has been designed which processes word unigram and bigram frequency statistics extracted from a corpus of natural language utterances. The system implements a binary top-down form of word clustering which employs an average class mutual information metric. Resulting classifications are hierarchical, allowing variable class granularity. Words are represented as structural tags --- unique $n$-bit numbers the most significant bit-patterns of which incorporate class information. Access to a structural tag immediately provides access to all classification levels for the corresponding word. The classification system has successfully revealed some of the structure of English, from the phonemic to the semantic level. The system has been compared --- directly and indirectly --- with other recent word classification systems. Class based interpolated language models have been constructed to exploit the extra information supplied by the classifications and some experiments have shown that the new models improve model performance.",
    "full_text": "cmp-lg/9503011   09 Mar 1995\nImpro ving Statistical Language Mo del P erformance with\nAutomatically Generated W ord Hierarc hies\nJ/. McMahon\n/\u0003\nand F/.J/. Smith\nDepartmen t of Computer Science/, The Queen/'s Univ ersit y of Belfast\nMarc h /9/, /1/9/9/5\nAbstract\nAn automatic w ord classi/\fcation system has b een designed whic h pro cesses w ord unigram and bigram\nfrequency statistics extracted from a corpus of natural language utterances/. The system implemen ts a bi/-\nnary top/-do wn form of w ord clustering whic h emplo ys an a v erage class m utual information metric/. Resulting\nclassi/\fcations are hierarc hical/, allo wing v ariable class gran ularit y /. W ords are represen ted as structur al tags\n/| unique n /-bit n um b ers the most signi/\fcan t bit/-patterns of whic h incorp orate class information/. Access\nto a structural tag immediately pro vides access to all classi/\fcation lev els for the corresp onding w ord/. The\nclassi/\fcation system has successfully rev ealed some of the structure of English/, from the phonemic to the\nseman tic lev el/. The system has b een compared /| directly and indirectly /| with other recen t w ord clas/-\nsi/\fcation systems/. Class based in terp olated language mo dels ha v e b een constructed to exploit the extra\ninformation supplied b y the classi/\fcations and some exp erimen ts ha v e sho wn that the new mo dels impro v e\nmo del p erformance/.\n/1 In tro duction\nMan y applications whic h pro cess natural language can b e enhanced b y incorp orating information ab out the\nprobabilities of w ord strings/; that is/, b y using statistical language mo del information /[/8/, /9 /, /1/8/, /2/7 /]/. The\nqualit y of these comp onen ts is often measured b y test set p erplexity /[/2/, /1/]/, in spite of some limitations /[/4/4 /] /:\nP P /=\n/^\nP /( w\nN\n/1\n/)\n/BnZr\n/1\nN\n/, where there are N w ords in the w ord stream/. With an arbitrarily c hosen standard test set/,\nstatistical language mo dels can b e compared /[/5/]/. This allo ws researc hers to mak e incremen tal impro v emen ts to\nthe mo dels /[/2/5/]/. Cognitiv e scien tists are in terested in those features of automatic w ord classi/\fcation whic h ha v e\nimplications for language acquisition /[/1/4/, /3/9/]/.\nOne common mo del of language calculates the probabilit y of the i th w ord w\ni\nin a test set b y considering the\nn most recen t w ords h w\ni /BnZr n\n/; w\ni /BnZr n /+/1\n/; /. /. /. /; w\ni /BnZr /1\ni /, or h w\ni /BnZr /1\ni /BnZr n\ni in a more compact notation/. The mo del is /\fnitary\n/(according to the Chomsky hierarc h y/) and linguistically na / /\u0010v e/, but it has the adv an tage of b eing easy to construct\nand its structure allo ws the application of Mark o v mo del theory /[/3/6/]/.\nMuc h w ork has b een carried out on w ord/-based n /-gram mo dels/, although there are recognised w eaknesses\nin the paradigm/. One suc h problem concerns the w a y n /-grams partition the space of p ossible w ord con texts/.\nIn estimating the probabilit y of the i th w ord in a w ord/-stream/, the mo del considers all previous w ord con texts\nto b e iden tical if and only if they share the same /\fnal n w ords/. This sim ultaneously fails to di/\u000beren tiate some\nlinguistically imp ortan t con texts and unnecessarily fractures others/. F or example/, if w e restrict our consideration\nto the t w o previous w ords in a stream /| i/.e/. to the trigram conditional probabilit y estimate\n/^\nP /( w\ni\nj w\ni /BnZr /1\ni /BnZr /2\n/) /|\nthen the sen tences\n/(/1a/) The boys eat the sandwiches quickly/.\n/\u0003\nSupp orted b y the Departmen t of Education for Northern Ireland and British T elecom Researc h Labs/. Corp ora supplied b y the\nOxford T ext Arc hiv e/, except v odis whic h w as supplied b y British T elecom/. Author/'s con tact address/, Departmen t of Computer\nScience/, Q/.U/.B/./, Belfast BT/7 /1NN/, N/. Ireland/. Email/: J/.McMahon/@qub/.ac/.uk\n/1\nand\n/(/2a/) The cheese in the sandwiches is delicious/.\ncon tain p oin ts where the con text is inaccurately considered iden tical/. These p oin ts o ccur after the common\nbigram h the sandwiches i /; w e can illustrate the danger of con/\rating the t w o sen tence con texts b y considering\nthe non/-sen tences\n/*/(/1b/) The boys eat the sandwiches is delicious/.\nand\n/*/(/2b/) The cheese in the sandwiches quickly/.\nIn /(/1a/)/, w e should not b e surprised to /\fnd an adv erb suc h as h quickly i /, but w e w ould b e surprised to /\fnd\nthe v erb h is i as sho wn in /(/1b/)/; con v ersely for /(/2a/) and /(/2b/)/. In b oth cases/, ho w ev er/, w e can construct other\nsen tences whic h mak e these w ords more lik ely/; for example /:\n/(/1c/) The way the boys eat the sandwiches is delicious/.\nand\n/(/2c/) The cheese in the sandwiches quickly melts/.\nW e can use results of information theory /[/4/3/] to quan tify op erationally our degree of surprise/. W e ha v e seen\nho w w ord based trigram mo dels fail to di/\u000beren tiate some ob viously di/\u000beren t con texts/; there are some tec hniques\nto alleviate this problem /[/3/1/, /3/2 /] but researc hers are constan tly confron ted with the sparseness of the n /-gram\nw ord segmen ts found in ev en the largest corp ora and with p o w erful criticisms of the /\fnite/-state grammars they\nuse/, based on theoretical linguistic grounds /[/7/, /3/7/]/.\nA second w eakness of w ord based language mo dels is their unnecessary fragmen tation of con texts/. The set\nof w ords whic h are in terc hangeable with a giv en w ord in a sen tence suc h that the newly generated sen tence is\nalso w ell formed is called a paradigmatic set/. Consider the w ord h boys i in /(/1a/) ab o v e/. W e can structure our\nen tire v o cabulary around this w ord as a series of la y ers/, eac h la y er of whic h con tains w ords whic h share man y\nprop erties with h boys i /. F or example/, w ords lik e h lads i migh t b e p ositioned in an inner paradigmatic la y er/.\nOther w ords migh t b e placed further a w a y /| for example h children i /. A linguistically signi/\fcan t la y er around\nthe w ord h boys i is one whic h con tains all plural nouns/; at this stage ho w ev er/, in tersubstitution can pro duce\nsen tences whic h ma y b e di/\u000ecult to in terpret/.\nIf sen tences /(/1a/) and /(/2a/) are con v erted to w ord class streams\n/(/3/) determiner noun verb determiner noun adverb\nand\n/(/4/) determiner noun preposition determiner noun verb adjective\nresp ectiv ely /, then bigram/, trigram and p ossibly ev en higher n /-gram statistics ma y b ecome a v ailable with greater\nreliabilit y for use as con text di/\u000beren tiators /(although Sampson /[/4/1/] suggets that no amoun t of w ord/-class n /-\ngrams ma y b e su/\u000ecien t to c haracterise natural language fully/)/. In particular/, w e should b e able to use trigram\ncon texts h verb determiner noun i and h preposition determiner noun i to di/\u000beren tiate reliably b et w een the\ncon texts in /(/1a/) and /(/2a/) at the p oin t where the w ord h sandwiches i is b eing pro cessed/. Of course/, this still\nfails to di/\u000beren tiate the con texts in sen tences /(/1c/) and /(/2c/)/; whilst n /-gram mo dels of language ma y nev er fully\nmo del long distance linguistic phenomena/, w e argue that it is still useful to extend their scop e/.\nIn order to mak e these impro v emen ts/, w e need access to w ord class information/, whic h is usually obtained\nin three main w a ys /: /\frstly /, w e can use corp ora whic h ha v e b een man ually tagged b y linguistically informed\nexp erts /[/1/3/]/; secondly /, w e can construct automatic part/-of/-sp eec h taggers and pro cess un tagged corp ora /[/2/6/, /1/5 /]/.\nThis metho d b oasts a high degree of accuracy /, although often the construction of the automatic tagger in v olv es\na b o otstrapping pro cess based on a core corpus whic h has b een man ually tagged /[/1/0/]/; the third option is to\n/2\nderiv e a fully automatic w ord classi/\fcation system from un tagged corp ora/. Some adv an tages of this approac h\ninclude its applicabilit y to an y natural language for whic h some corpus exists/, indep enden t of the degree of\ndev elopmen t of its grammar/, and its parsimonious committmen t to the mac hinery of mo dern linguistics/. One\ndisadv an tage is that the classes whic h are deriv ed usually allo w no linguistically sensible summarising lab el\nto b e attac hed/. Researc h has b een carried out recen tly in this area/, often indep enden t of an y application to\nstatistical language mo delling /[/2/0 /, /1/6 /, /3/8/, /3/, /2/4 /, /3/5 /, /4/0 /, /3/0/]/. The next section con tains a presen tation of a new\nautomatic w ord classi/\fcation algorithm/.\n/2 W ord Classi/\fcation and Structural T ags\nMost statistical language mo dels whic h mak e use of class information do so with a single la y er of w ord classes /[/4/]\n/| often at the lev el of common linguistic classes /(nouns/, v erbs/, etc/. /) /[/1/3 /]/. Also/, underlying these mo dels is\nan assumed relationship b et w een the set of w ord/-ob jects in the v o cabulary and some set of class/-ob jects/,\noften de/\fned b y professional linguists/; deriving a propitious classi/\fcation system corresp onds to searc hing the\nspace of p ossible w ord/-to/-class mappings/. In con trast/, w e presen t the structur al tag represen tation/, where\nthe sym b ol whic h represen ts the w ord sim ultaneously represen ts the classi/\fcation of that w ord /(/[/2/9 /] mak es\nconnections b et w een this and other represen tations /| for example Sc h / utze/'s category space /[/4/2 /] and/, ultimately\nSaussure/'s /[/1/2/] structuralist conception of the sign /| and /[/2/8 /] describ es its implemen tation more fully/)/. In our\nmo del/, eac h w ord is represen ted b y an s /-bit n um b er the most signi/\fcan t bits of whic h corresp ond to v arious\nlev els of classi/\fcation/; giv en some w ord whic h is represen ted as structural tag w /, w e can gain immediate access\nto all s lev els of classi/\fcation of that w ord/. Generally /, the broader the classi/\fcation gran ularit y w e c hose/, the\nmore con/\fden t w e can b e ab out the distribution of classes at that lev el/, but the less information this distribution\no/\u000bers us ab out next/-w ord prediction/.\nIn man y w ord classi/\fcation systems/, the hierarc h y is not explicitly represen ted/, and further pro cessing/,\noften b y standard statistical clustering tec hniques/, is required /( e/.g/. Elman/, Sc h / utze/, Brill et al/. /, Finc h et al/. /,\nHughes et al/. and P ereira et al/. /)/. With the structural tag represen tation/, eac h tag con tains classi/\fcation\ninformation whic h is explicitly represen ted/; the p osition of that w ord in class/-space can b e obtained without\nreference to the p ositions of other w ords/. Also/, man y lev els of classi/\fcation gran ularit y can b e made a v ailable\nsim ultaneously /, and the w eigh t whic h eac h of these lev els can b e giv en in/, for example a statistical language\nmo del can alter dynamically /. Using the structural tag represen tation/, the computational o v erheads for using\nclass information can b e k ept to a minim um/. Also/, it is p ossible to organise an n /-gram frequency database\nso that close structural tags are stored near to eac h other/; this could b e exploited to reduce the searc h space\nexplored in sp eec h recognition systems/. F or example/, if the system is searc hing for the frequency of a particular\nnoun in an attempt to /\fnd the most lik ely next w ord/, then alternativ e w ords should already b e nearb y in the\nn /-gram database/. Finally /, w e note that in the curren t implemen tation of the structural tag represen tation w e\nallo w only one tag p er orthographic w ord/-form/; although man y of the curren t w ord classi/\fcation systems do\nthe same/, w e w ould prefer a structural tag implemen tation whic h mo dels the m ulti/-mo dal nature of some w ords\nmore successfully /.\nConsider sen tences /(/1a/) and /(/2a/) again/; w e w ould lik e to construct a clustering algorithm whic h assigns\nsome unique s /-bit n um b er to eac h w ord in our v o cabulary so that the w ords are distributed according to some\nappro ximation of the paradigmatic la y ering describ ed ab o v e /| that is/, h boys i should b e close to h people i and\nh is i should b e close to h eat i /. W e w ould also lik e seman tically related w ords to cluster/, so that/, although h boys i\nma y b e near h sandwiches i b ecause b oth are nouns/, h girls i should b e ev en closer to h boys i b ecause b oth are\nh uman t yp es/. In theory /, structural tag represen tations can b e dynamically up dated /| for example/, h bank i\nmigh t b e close to h river i in some con texts and closer to h money i in others/.\nAlthough w e could construct a useful set of structural tags man ually /[/2/8/]/, w e prefer to design an algorithm\nwhic h builds suc h a classi/\fcation/. The algorithm is lo cally optimal and implemen ts a binary form of sim ulated\nannealing/, whic h/, informally /, follo ws Bro wn/'s la w of cum ulativ e complexit y /[/6/, /4/5/]/; it states that/, during h uman\ndev elopmen t/, simple cognitiv e structures are more lik ely to b e acquired b efore more complex ones/. In our\nv ersion applied to structural tags/, this is equiv alen t to an algorithm whic h /\frst treats all w ords in a corpus\nas if they b elonged to only t w o classes and then attempts to /\fnd the optimal t w o/-class w ord division/; it then\n/3\nassumes that w ords can b elong to an y of /4 classes and con tin ues as b efore/; the algorithm /\fnishes when it reac hes\na classi/\fcation with /2\ns\nclasses/.\nF or a giv en v o cabulary /, V /, the mapping t initially translates w ords in to their corresp onding unique structural\ntags/. This mapping is constructed b y making random w ord/-to/-tag assignmen ts/. Bro wn et al/. ha v e sho wn that\nan y classi/\fcation system whose a v erage class m utual information is maximised will lead to class/-based language\nmo dels of lo w er p erplexities/.\nThe m utual information /[/1/1/] b et w een t w o ev en ts x and y is /:\nI /( x/; y /) /= log\nP /( x/; y /)\nP /( x /) P /( x /)\nIt follo ws that if the t w o ev en ts/, x and y stand for the o ccurrence of certain w ord classes in a sample/, then w e\ncan estimate the m utual information b et w een the t w o classes/. In these exp erimen ts/, w e use maxim um lik eliho o d\nprobabilit y estimates based on a training corpus/. In order to estimate the a v erage class m utual information/, for\na classi/\fcation depth of s bits w e compute /:\nM\ns\n/( t /) /=\nX\nc\ni\n/;c\nj\nP /( c\ni\n/; c\nj\n/) /\u0002 log\nP /( c\ni\n/; c\nj\n/)\nP /( c\ni\n/) P /( c\nj\n/)\n/(/1/)\nwhere c\ni\nand c\nj\nare w ord classes and M\ns\n/( t /) is the a v erage class m utual information for structural tag classi/\fcation\nt at bit depth s /. It follo ws that the optimal classi/\fcation/, t\no\ncan b e found b y computing\nM\ns\n/( t\no\n/) /= max\nt\nM\ns\n/( t /) /(/2/)\nNo metho d exists at presen t whic h can /\fnd the optimal classi/\fcation/, but sub/-optimal strategies exist whic h\nlead to useful classi/\fcations/. The sub/-optimal strategy used in the curren t automatic w ord classi/\fcation system\nin v olv es selecting the lo c al ly optimal structure b et w een t and t\n/0\n/, whic h di/\u000ber only in their classi/\fcation of a\nsingle w ord/. An initial structure is built b y using the computer/'s pseudo/-random n um b er generator to pro duce\na random w ord hierarc h y /. Its M /( t /) v alue is calculated/. Next/, another structure/, t\n/0\nis created as a cop y of the\nmain one/, with a single w ord mo v ed to a di/\u000beren t place in the classi/\fcation space/. Its M /( t\n/0\n/) v alue is calculated/.\nThis second calculation is rep eated for eac h w ord in the v o cabulary and w e k eep a record of the transformation\nwhic h leads to the highest M /( t\n/0\n/)/. After an iteration through ev ery w ord/, w e select that t\n/0\nwhic h has the highest\nM /( t\n/0\n/) v alue and re/-iterate/. With this metho d/, w ords whic h at one time are mo v ed to a new region in the\nclassi/\fcation hierarc h y can mo v e bac k at a later time/, if licensed b y the m utual information metric/. In practice/,\nthis do es happ en/. Therefore/, eac h transformation p erformed b y the algorithm is not irrev ersible within a lev el/,\nwhic h should allo w the algorithm to explore a larger space of p ossible w ord classi/\fcations/.\nThe algorithm is em b edded in a system whic h calculates the b est classi/\fcations for all lev els /: /\frst/, the\nhighest classi/\fcation lev el is pro cessed/. Since the structural tag represen tation is binary /, this /\frst lev el seeks to\n/\fnd the b est distribution of w ords in to t w o classes/.\nWhen the lo cally optimal t w o/-class hierarc h y has b een disco v ered b y maximising M\n/1\n/( t /)/, whatev er later re/-\nclassi/\fcations o ccur at /\fner lev els of gran ularit y /, w ords will alw a ys remain in the lev el /1 class to whic h they no w\nb elong/. F or example/, if man y nouns no w b elong to class /0 and man y v erbs to class /1/, later sub/-classi/\fcations\nwill not in/\ruence the M\n/1\n/( t /) v alue/. This reasoning also applies to all classes s /= /2 /; /3 /. /. /. /1/6/.\nThis algorithm/, whic h is O /( sV\n/3\n/) for v o cabulary size V /, w orks w ell with the most frequen t w ords from a\ncorpus/; ho w ev er w e ha v e dev elop ed a second algorithm/, to b e used with the /\frst/, to allo w v o cabulary co v erage\nin the range of tens of thousands of w ord t yp es/. This second algorithm exploits Zipf /'s la w /[/4/6/] /| the most\nfrequen t w ords accoun t for the ma jorit y of w ord tok ens /| b y adding in lo w frequency w ords only after the /\frst\nalgorithm has /\fnished pro cessing high frequency ones/. W e mak e the assumption that an y in/\ruence that these\ninfrequen t w ords ha v e on the /\frst batc h of frequen t w ords can b e discoun ted/. The algorithm is an order of\nmagnitude less computationally in tensiv e and so can pro cess man y more w ords in a giv en time/. By this metho d/,\nw e can also a v oid mo delling only a simpli/\fed subset of the phenomena in whic h w e are in terested and hence\na v oid the danger of designing systems whic h do not scale/-up adequately /[/1/4 /]/.\n/4\n/3 W ord Classi/\fcation Results\nW e applied our algorithm to sev eral corp ora in order to explore its scop e of application/. It successfully disco v ers\nma jor noun/-v erb distinctions in a w ell/-kno wn t yp e /4 grammar in tro duced b y Elman /[/1/4 /]/, mak es near p erfect\nv o w el/-consonan t distinctions when applied to a phonemic corpus and mak es imp ortan t syn tactic and seman tic\ndistinctions in a Latin corpus/; /[/2/8/] con tains full descriptions of these and sev eral other results/, including a\ndetailed comparison with a merge/-based clustering tec hnique whic h Bro wn et al/. incorp orate as part of their\nw ord clustering system/. W e ha v e found that the merge/-based tec hniques implemen ted b y Bro wn et al/. /, Hughes/,\nFinc h et al/. /, Sc h / utze and Brill et al/. result in taxonomies whic h ha v e complemen tary strengths and w eaknesses\ncompared to our top/-do wn classi/\fcation/, whic h leads us to suggest that h ybrid top/-do wn and b ottom/-up systems\nmigh t inherit the strengths of b oth approac hes/. In the presen t pap er ho w ev er/, w e rep ort on the p erformance\nof our top/-do wn algorithm when applied to the most frequen t w ords from an un tagged v ersion of the lob\ncorpus /[/2/3/] and also when applied to a h ybrid w ord and class v ersion of the lob /. W e use structural tags whic h\nare /1/6 bits long and w e considered the /5/6/9 most frequen t w ords/. With these exp erimen ts w e aim to illustrate\nthe qualit y of the clustering tec hnique/; w e include a summarising p erformance measure/, dev elop ed b y Hughes et\nal/. whic h/, whilst not ideal /[/2/8 /]/, represen ts an ob jectiv e measure of w ord cluster qualit y /.\nIn /\fgure /1/, w e observ e the /\fnal state of the classi/\fcation/, to a depth of /\fv e bits/. Man y syn tactic and some\nseman tic divisions are apparen t /| prep ositions/, pronouns/, v erbs/, nouns and determiners cluster /| but man y\nmore distinctions are rev ealed when w e examine lo w er lev els of the classi/\fcation/. F or example/, /\fgure /2 sho ws\nthe sub/-cluster of determiners whose initial structural tag is iden ti/\fed b y the four/-bit sc hema /0/0/0/0/. In /\fgure /3\nw e examine the /\fner detail of a cluster of nouns/. Here/, some seman tic di/\u000berences b ecome clear/. Man y of the /2\n/5\ngroups listed in /\fgure /1 sho w this t yp e of in teresting /\fne detail/. In another exp erimen t/, a h ybrid v ersion of the\nlob corpus w as created b y replacing eac h w ord and part/-of/-sp eec h pair b y the w ord only if the part/-of/-sp eec h\nw as a singular noun/, the base form of a v erb/, or the third p erson singular presen t tense of a v erb and b y the\npart/-of/-sp eec h itself otherwise/. When w e examine the most frequen t /`w ords/' of this h ybrid corpus/, w e /\fnd\nthat there are man y more con ten t w ords presen t/, but that the remaining con ten t w ords still ha v e an indirect\ne/\u000bect on w ord classi/\fcation/, since they are represen ted b y the part/-of/-sp eec h of whic h they are an example/.\nFigures /4 and /5 sho w man y of the largest groupings of w ords found after pro cessing/, at a classi/\fcation lev el\nof /9 bits/. By insp ection/, w e observ e a v ariet y of seman tic asso ciations/, although there are anomalies/. In eac h\ngroup w e include here/, the en tire mem b ership is listed/. The remaining groups not presen ted here also displa y\nstrong seman tic clustering/. Finally /, /\fgure /6 sho ws the complete phoneme classi/\fcation of a phonemic v ersion\nof the v odis corpus/. The most ob vious feature of this /\fgure is the successful distinction b et w een v o w els and\nconsonan ts/. Bey ond the v o w el/-consonan t distinction/, other similarities emerge /: v o w el sounds with similar v o cal\ntract p ositions are clustered closely /| the h a i sounds/, for example/, and the phoneme pair h o i and h oo i /; some\nconsonan ts whic h are similarly articulated also map on to lo cal regions of the classi/\fcation space /| h r i and\nh rx i /, h ch i and h z i and h n i and h ng i /, for example/.\nAssessing the quan titativ e signi/\fcance of these results is di/\u000ecult/; Hughes /[/1/9/]/, for example/, suggests b enc h/-\nmark ev aluation /| a standard tagged corpus is used as a reference against whic h automatic comparisons can\nb e made/; whilst this ma y not b e appropriate for the designers of ev ery automatic classi/\fcation system /(for\nexample/, researc hers whose main in terest in automatic classi/\fcation is as a means to w ards the end of impro ving\nstatistical language mo dels/)/, it has man y adv an tages o v er qualitativ e insp ection as an ev aluation metho d/, whic h\nto date has b een the dominan t metho d/. The ev aluator dev elop ed b y Hughes allo ws accuracy scores to b e made\nat eac h lev el in a hierarc h y /. Figure /7 sho ws p erformance curv es for three systems /| that of Finc h /[/1/7/]/, whic h\nuses Sp earman/'s rank correlation co e/\u000ecien t/, that of Hughes/, whic h used W ard/'s metho d/, and the top/-do wn\nm utual information algorithm/. The top/-do wn algorithm fares w ell/, ev en though it is curren tly based on con/-\ntiguous bigram information only /, whereas b oth of the other classi/\fcation systems are based on con tiguous and\nnon/-con tiguous bigrams /| that is/, bigrams of the form h w\ni /BnZr /1\n/; w\ni\ni and h w\ni /BnZr /2\n/; w\ni\ni /. On Hughes et al/. /'s ev aluation\nmeasure/, our system p erforms b etter that Finc h et al/. /'s but w orse than Hughes et al/. /'s/.\nNot only can w e estimate and compare the inheren t linguistic qualit y of the classi/\fcation/, but w e can also\nassess its utilit y b y measuring ho w m uc h it impro v es the most successful statistical language mo dels to date\n/| those based on Mark o v mo del theory /. This indirect ev aluation allo ws us to examine the signi/\fcance of sub/-\nsyn tactic clusters/; for example/, ev en though a high lev el cluster ma y con tain man y nouns/, the direct ev aluation\n/5\nof Hughes et al/. is curren tly unable to rew ard successful seman tic sub/-clusterings/.\n/4 Structural T ags and Statistical Language Mo dels\nThere are sev eral w a ys of incorp orating w ord classi/\fcation information in to statistical language mo dels using\nthe structural tag represen tation /[/2/8/]/. Here/, w e shall describ e a metho d/, deriv ed from Mark o v mo del theory /[/2/1 /]/,\nwhic h is based on in terp olating sev eral language comp onen ts/. The in terp olation parameters are estimated b y\nusing a held/-out corpus/.\nF or the follo wing exp erimen ts/, a formatted v ersion of the one million w ord Bro wn corpus w as used as a\nsource of language data/; /6/0/% of the corpus w as used to generate maxim um lik eliho o d probabilit y estimates/,\n/3/0/% to estimate frequency/-dep enden t in terp olation parameters/, and the remaining /1/0/% as a test set/.\nF or completeness/, w e also calculated some test/-set p erplexities of simple language mo dels/. The simplest\nunigram language mo del is an equiprobable one/; its p erplexit y is /3/3/,/3/6/0/. A maxim um lik eliho o d unigram\nmo del/, whilst not using held/-out data/, results in a test set p erplexit y of /1/,/2/2/6/./7/. The simplest language mo del\nsystem to use held/-out data is a frequency/-indep enden t/, w ord/-based trigram language mo del/, summarised b y\nthe equation /:\nP /( w\nk\n/) /= /\u0015\nu\n/\u0002 P /( w\nk\n/) /+ /\u0015\nb\n/\u0002 P /( w\nk\nj w\nj\n/) /+ /\u0015\nt\n/\u0002 P /( w\nk\nj w\ni\n/; w\nj\n/)\nThe three parameters /\u0015\nu\n/, /\u0015\nb\nand /\u0015\nt\n/, corresp onding to unigram/, bigram and trigram w eigh ts/, initailly are giv en\nequal w eigh ts of\n/1\n/3\n/; a simpli/\fed v ersion of the F orw ard/-Bac kw ard algorithm /[/3/4/] is iterated un til the halting\ncondition j /\u0015\nt /+/1\n/BnZr /\u0015\nt\nj /< /0 /: /0/0/1/. This condition is c hosen arbitrarily /, but sta ys constan t throughout the remaining\nexp erimen ts/. The resulting language mo del system reduces test set p erplexit y to /7/0/1/./7/.\nImpro v ed p erformanc e can b e obtained b y making in terp olation parameters dep end up on some distinguishing\nfeature of the prediction con text/. One easily calculated feature is the frequency of the previously pro cessed w ord/.\nIn our exp erimen t/, this resulted in /4/2/8 sets of /\u0015 v alues/. The optimised parameters are /\ftted in to an in terp olated\nlanguage mo del the core of whic h is describ ed b y the equation /:\nP /( w\nk\n/) /= /\u0015\nu\n/( f /) /\u0002 P /( w\nk\n/) /+ /\u0015\nb\n/( f /) /\u0002 P /( w\nk\nj w\nj\n/) /+ /\u0015\nt\n/( f /) /\u0002 P /( w\nk\nj w\ni\n/; w\nj\n/)\nwhere f /= f /( w\nj\n/)/, the frequency of w ord h w\nj\ni if there a v alid w\nj\nexists and /0 otherwise /| namely at the b eginning\nof the test set/, and when the previous w ord is not in the training v o cabulary /. The resulting p erplexit y v alue for\nthis system is /6/2/1/./6/. This represen ts a pragmatically sensible baseline v alue against whic h an y v arian t language\nmo del should b e compared/. Another similar w ord/-based language mo del has b een dev elop ed b y O/'Bo yle et\nal/. /[/3/3/]/, the w eigh ted a v erage language mo del/. This mo del is describ ed as follo ws /:\nP /( w\nk\nj w\nk /BnZr /1\n/1\n/) /=\nP\nm\ni /=/1\n/\u0015\ni\n/\u0002 P\nM L\n/( w\nk\nj w\nk /BnZr /1\nk /BnZr i\n/) /+ /\u0015\n/0\n/\u0002 P\nM L\n/( w\nk\n/)\nP\nm\ni /=/0\n/\u0015\ni\nwhere there are statistically signi/\fcan t segmen ts up to m /+ /1 w ords long and P\nM L\n/( w\nk\n/) is the maxim um lik eliho o d\nprobabilit y estimate of a w ord/. The n umerator acts as a normaliser/. It has b een found that\n/\u0015\ni\n/= /2\n/( j w\nk /BnZr /1\nk /BnZr i\nj /)\n/\u0002 log f /( w\nk /BnZr /1\nk /BnZr i\n/)\nwhere j w\nk /BnZr /1\nk /BnZr i\nj is the size of the segmen t/, results in a near/-optim um language mo del of this form/. When applied\nto the Bro wn corpus/, excluding the /3/0/% whic h has b een allo cated for in terp olation/, the mo del still p erforms\nw ell/, ac hieving a p erplexit y score of /6/3/0/./9/, /1/./5/% ab o v e the in terp olated optim um v alue/.\nUsing structural tags whic h are /1/6 bits long allo ws us/, in principle/, to build an in terp olated mo del whic h\ncon tains /1/6 lev els of w ord class information/. The mo del can b e describ ed as follo ws /:\nP /( w\nk\n/) /=\n/1/6\nX\ns /=/1\n/3\nX\ni /=/1\n/\u0015\ns\ni\n/( f /) /\u0002 P\ns\ni\n/( w\nk\n/)\nwhere i /= /1 /; /2 /; /3 corresp onds to unigram/, bigram and trigram comp onen ts resp ectiv ely /, and P\ns\ni\n/( w\nk\n/) is the\nprobabilit y estimate of the i th language mo del of depth s for w ord w\nk\n/. F or example/, if w e are pro cessing\n/6\nthe w ord h sandwiches i in the sequence h eat the sandwiches i /, then P\n/7\n/2\n/( sandwiches /) is the estimate of the\nbigram conditional probabilit y P /( c\n/7\n/( the sandwiches /) j c\n/7\n/( the /)/) /\u0002 P /( sandwiches j c\n/7\n/( sandwiches /)/)/, where c\n/7\n/( the\nsandwiches /) is the structural tag equiv alen t of h determiner noun/-plural i /| that is/, the represen tation of\nthat class bigram/, at depth /7/, the /\frst elemen t of whic h con tains as a class mem b er the w ord h the i and the\nsecond elemen t of whic h con tains as a class mem b er the w ord h sandwiches i /. If our w ord classi/\fcation system\np erforms w ell/, w e should exp ect other fo o dstu/\u000b w ords to b e included in the c\n/7\nclassi/\fcation/. Figure /3 lends\nsupp ort to our b elief that our algorithm disco v ers in teresting w ord classes /| it illustrates a c\n/9\ngroup whic h\ncon tains the w ords f boy girl child man woman person g /. Figures /4 and /5 con tain man y seman tic groupings\nobserv ed at the c\n/9\nlev el/.\nDue to the mo dest size of the Bro wn corpus/, w e did not implemen t a full/-lev el in terp olated class/-based\nlanguage mo del /| there w ould ha v e b een to o man y training parameters and not enough training data/; instead/,\nw e implemen ted sev eral t w o/-lev el and three/-lev el v arian ts of the system/, the most successful of whic h is a three/-\nlev el system whic h tak es information from lev el /1/6 /(corresp onding to a standard w ord/-based trigram system/)/,\nlev el /8 and lev el /5/. This language mo del registered a test set p erplexit y of /5/8/6/./5/, compared to the trigram\nw ord/-based language mo del score of /6/2/1/./6/. Figure /8 summarises all of the test set p erplexit y results/.\n/5 Discussion\nAs an illustration of the kind of adv an tage whic h structural tag language mo dels can o/\u000ber/, w e in tro duce nine\noron yms based up on the uttered sen tence\nthe boys eat the sandwiches\nIf w e assume that w e already p ossess a p erfect sp eec h recognition acoustic mo del /[/2/2 /]/, it ma y b e able to reco v er\nthe optimal phoneme string /:\n//DH a b OI z EE t DH A s AA n d w i j i z//\nHo w ev er/, the original sen tence is not the only sp eec h utterance whic h could giv e rise to the observ ed phoneme\nstring/. F or example/, the meaningless and ungrammatical sen tence\nthe buoy seat this and which is\ncan also giv e rise to the observ ed phonemic stream/. Humans usually re/-construct the most lik ely sen tence\nsuccessfully /, but arti/\fcial sp eec h recognisers with no language mo del comp onen t cannot/. A useful statistical\nlanguage mo del will assign a lo w probabilit y to the second sen tence and a high probabilit y to the /\frst/. A more\ntraditional/, non/-probabilistic language mo del/, in the form of a grammar/, could also di/\u000beren tiate b et w een the\nt w o sen tences/, w eeding out the second/, ungrammatica l sen tence/. Ho w ev er/, suc h mo dels/, whilst theoretically\nw ell grounded/, so far tend to ha v e p o or co v erage/. Another problem with non/-probabilistic mo dels can b e seen\nif w e consider a third h yp othesised sen tence /:\nthe buoys eat the sand which is\nThis sim ultaneously surreal and metaph ysical sen tence ma y b e accepted b y grammar systems whic h detect w ell/-\nformedness/, but it is subsequen tly considered just as plausible as the original sen tence/. A probabilistic language\nmo del should assign a relativ ely lo w probabilit y to the third sen tence/. W e constructed nine h yp othesised\nsen tences/, eac h of whic h could ha v e pro duced the phoneme string/; w e presen ted these sen tences as input to a\nhigh/-qualit y w ord/-based language mo del /(the w eigh ted a v erage language mo del/) and to the /1/6/-/8/-/5 three/-lev el\nstructural tag language mo del/. Figure /9 sho ws the normalised probabilit y results of these exp erimen ts/. The\nnew language mo del successfully iden ti/\fes the most lik ely utterance/. In all but t w o cases/, sen tences whic h\nare grammatically w ell/-formed are assigned a higher ra w probabilit y b y the new mo del/, and vice/-v ersa for\nungrammatical sen tences/. Using the top t w o sen tences h the boy seat the sandwiches i and h the boys eat\nthe sandwiches i /, w e can examine the practical b ene/\fts of class information for statistical language mo delling/.\nThe only di/\u000berence b et w een the t w o is in the bigrams h boy seat i and h boys eat i /, neither of whic h o ccured in\n/7\nthe training corpus/. F or the mo del whic h uses w ord frequencies exclusiv ely /, it di/\u000beren tiates b et w een the t w o\nh yp othesised sen tences b y examining the unigrams h boy i /, h seat i /, h boys i and h eat i /. In our training corpus/,\nh boy i and h seat i are individually more lik ely than h boys i and h eat i /. Ho w ev er/, with the /1/6/-/8/-/5 structural tag\nmo del/, extra w ord class information allo ws the system to recognise the familiar noun/-verb pattern as b eing\nmore lik ely than the noun/-noun pattern/.\nThe automatic w ord classi/\fcation system based on a binary top/-do wn m utual information algorithm leads to\nqualitativ ely impressiv e syn tactic and seman tic clustering results /(see /\fgures /1 to /5/)/; quan titativ ely /, it fares w ell\nwith other successful systems/, demonstrating complemen tary strengths and w eaknesses compared to the more\nusual merge/-based classi/\fcation systems/, ev en though it curren tly uses con tiguous bigrams /(see /\fgure /7 and\nalso /[/2/8/] for a direct comparison with another merge/-based classi/\fcation system/)/; thirdly /, results from a simple\nimplemen tation of the system /(/3 classi/\fcation lev els out of /1/6/) sho w a signi/\fcan t impro v emen t in statistical\nlanguage mo del p erformance/. W e ha v e incorp orated structural tag information in to an in terp olated trigram\nlanguage mo del system b ecause it pro vided a w ell/-attested and successful base system against whic h w e could\nmeasure impro v emen t/; ho w ev er/, w e b eliev e that the w eigh ted a v erage system describ ed earlier/, with its scop e\nfor including n /-gram information b ey ond the trigram and its a v oidance of data/-in tensiv e and computationally\nin tensiv e parameter optimisation/, could o/\u000ber a more con v enien t platform within whic h to place structural tag\ninformation/. W e b eliev e that although v ariable gran ularit y class/-based language mo dels ma y nev er fully capture\nlinguistic dep endencies/, they can o/\u000ber mo dest adv ances in co v erage compared to exclusiv ely w ord/-based systems/.\nReferences\n/[/1/] Lalit R/. Bahl/, P eter F/. Bro wn/, P eter V/. DeSouza/, and Rob ert L/. Mercer/. A tree/-based statistical lan/-\nguage mo del for natural language sp eec h recognition/. IEEE T r ansactions on A c oustics/, Sp e e ch and Signal\nPr o c essing /, /3/7/(/7/)/:/1/0/0/1 /{ /1/0/0/8/, July /1/9/8/9/.\n/[/2/] Lalit R/. Bahl/, F rederic k Jelinek/, and Rob ert L/. Mercer/. A maxim um lik eliho o d approac h to con tin uous\nsp eec h recognition/. I/.E/.E/.E/. T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, P AMI/-/5/(/2/)/:/1/7/9\n/{ /1/9/0/, Marc h /1/9/8/3/.\n/[/3/] Eric Brill/, Da vid Magerman/, Mitc hell Marcus/, and Beatrice San torini/. Deducing linguistic structure from\nthe statistics of large corp ora/. In Pr o c e e dings of the D ARP A Sp e e ch and Natur al L anguage Workshop /, /1/9/9/0/.\n/[/4/] P eter F/. Bro wn/, Vincen t Della Pietra/, P eter De Souza/, Jennifer C/. Lai/, and Rob ert Mercer/. Class/-based\nn /-gram mo dels of natural language/. Computational Linguistics /, /1/8/(/4/)/:/4/6/7 /{ /4/7/9/, /1/9/9/2/.\n/[/5/] P eter F/. Bro wn/, Vincen t J/. Della Pietra/, Rob ert L/. Mercer/, Stephen A/. Della Pietra/, and Jennifer C/. Lai/.\nAn estimate of an upp er b ound for the en trop y of English/. Computational Linguistics /, /1/8/(/1/)/:/3/1 /{ /4/0/, /1/9/9/2/.\n/[/6/] R/. Bro wn/. A First L anguage /: The Early Stages /. P enguin/, Harmondsw orth/, England/, /1/9/7/3/.\n/[/7/] Noam Chomsky /. Syntactic Structur es /. The Hague/: Mouton/, /1/9/5/7/.\n/[/8/] K/. Ch urc h/, W/. Gale/, P /. Hanks/, and D/. Hindle/. Using statistics in lexical analysis/. In Uri Zernik/, editor/,\nL exic al A c quisition /: Exploiting On/-Line R esour c es to Build a L exic on /, c hapter /6/, pages /1/1/5 /{ /1/6/4/. La wrence\nErlbaum Asso ciates/, /1/9/9/1/.\n/[/9/] Kenneth W/. Ch urc h and Rob ert L/. Mercer/. In tro duction to the sp ecial issue on computational linguistics\nusing large corp ora/. Computational Linguistics /, /1/9/(/1/)/:/1 /{ /2/3/, /1/9/9/3/.\n/[/1/0/] Kenneth W ard Ch urc h/. A sto c hastic parts program and noun phrase parser for unrestricted text/. In Se c ond\nConfer enc e on applie d Natur al L anguage pr o c essing /, /1/9/8/8/.\n/[/1/1/] Thomas M/. Co v er and Jo y A/. Thomas/. Elements of Information the ory /. John Wiley and Sons/, /1/9/9/1/.\n/[/1/2/] F erdinand de Saussure/. Course in Gener al Linguistics /. Duc kw orth/, /1/9/8/3/.\n/8\n/[/1/3/] Anne/-Marie Derouault and Bernard Merialdo/. Natural language mo delling for phoneme/-to/-text transcrip/-\ntion/. I/.E/.E/.E/. T r ansactions on Pattern A nalysis and Machine Intel ligenc e /, P AMI/-/8/(/6/)/, No v em b er /1/9/8/6/.\n/[/1/4/] Je/\u000brey L/. Elman/. Finding structure in time/. Co gnitive Scienc e /, /1/4/:/1/7/9 /{ /2/1/1/, /1/9/9/0/.\n/[/1/5/] Roger Garside Ezra Blac k and Geo/\u000brey Leec h/. Statistic al ly/-Driven Computer Gr ammars of English /: The\nIBM//Lanc aster Appr o ach /. Ro dopi/, /1/9/9/3/.\n/[/1/6/] Stev en Finc h and Nic k Chater/. Learning syn tactic categories /: A statistical approac h/. In M/. Oaksford and\nG/.D/.A/. Bro wn/, editors/, Neur o dynamics and Psycholo gy /, c hapter /1/2/. Academic Press/, /1/9/9/4/.\n/[/1/7/] Stev en P aul Finc h/. Finding Structur e in L anguage /. PhD thesis/, Cen tre for Cognitiv e Science/, Univ ersit y\nof Edin burgh/, /1/9/9/3/.\n/[/1/8/] William A/. Gale/, Kenneth W/. Ch urc h/, and Da vid Y aro wsky /. W ork on statistical metho ds for w ord sense\ndisam biguation/. In Pr ob abilistic Appr o aches to Natur al L anguage /. American Asso ciation for Arti/\fcial\nIn telligence/, AAAI Press/, /1/9/9/2/. T ec hnical rep ort FS/-/9/2/-/0/5/.\n/[/1/9/] John Hughes/. A utomatic al ly A c quiring a Classi/\fc ation of Wor ds /. PhD thesis/, Sc ho ol of Computer Studies/,\nUniv ersit y of Leeds/, /1/9/9/4/.\n/[/2/0/] John Hughes and Eric A t w ell/. The automated ev aluation of inferred w ord classi/\fcations/. In Eleventh\nEur op e an Confer enc e on A rti/\fcial Intel ligenc e /, /1/9/9/4/.\n/[/2/1/] F rederic k Jelinek/. Self/-organized language mo delling for sp eec h recognition/. In W aib el and Lee/, editors/,\nR e adings in Sp e e ch r e c o gnition /. Morgan Kaufmann/. San Mateo/, California/, /1/9/9/0/.\n/[/2/2/] F rederic k Jelinek/, Rob ert L/. Mercer/, and Salim Rouk os/. Principles of lexical language mo delling for sp eec h\nrecognition/. In S/. F urui and M/.M/. Sondhi/, editors/, A dvanc es in Sp e e ch Signal Pr o c essing /. Maral Dekku/,\nInc/./, /1/9/9/2/.\n/[/2/3/] S/.J/. Johansson/, E/.S/. A t w ell/, R/. Garside/, and G/. Leec h/. The tagge d LOB Corpus /: Users/' Manual /. The\nNorw egian Cen tre for the Humanities/, Bergen/, /1/9/8/6/.\n/[/2/4/] George R/. Kiss/. Grammatical w ord classes /: A learning pro cess and its sim ulation/. Psycholo gy of L e arning\nand Motivation /, /7/:/1 /{ /4/1/, /1/9/7/3/.\n/[/2/5/] Ronald Kuhn and Renato De Mori/. A cac he/-based natural language mo del for sp eec h recognition/. I/.E/.E/.E/.\nT r ansactions on Pattern A nalysis and Machine Intel ligenc e /, /1/2/(/6/)/:/5/7/0 /{ /5/8/3/, June /1/9/9/0/.\n/[/2/6/] Julian Kupiec/. Robust part/-of/-sp eec h tagging using a hidden mark o v mo del/. Computer Sp e e ch and L an/-\nguage /, /6/:/2/2/5 /{ /2/4/2/, /1/9/9/2/.\n/[/2/7/] Elizab eth D/. Liddy and W o o jin P aik/. Statistically/-guided w ord sense disam biguation/. In Pr ob abilistic Ap/-\npr o aches to Natur al L anguage /. American Asso ciation for Arti/\fcial In telligence/, AAAI Press/, /1/9/9/2/. T ec hnical\nrep ort FS/-/9/2/-/0/5/.\n/[/2/8/] John McMahon/. Statistic al L anguage pr o c essing Base d on Self/-Or ganising Wor d Classi/\fc ation /. PhD thesis/,\nDepartmen t of Computer Science/, Queen/'s Univ ersit y of Belfast/, /1/9/9/4/.\n/[/2/9/] John McMahon and F/. J/. Smith/. Structural tags/, annealing and automatic w ord classi/\fcation/. A rti/\fcial\nIntel ligenc e and the Simulation of Behaviour Quarterly /, /(/9/0/)/, /1/9/9/4/.\n/[/3/0/] Hermann Ney /, Ute Essen/, and Reinhard Kneser/. On structuring probabilistic dep endencies in sto c hastic\nlanguage mo delling/. Computer Sp e e ch and L anguage /, /8/:/1 /{ /3/8/, /1/9/9/4/.\n/[/3/1/] P eter O/'Bo yle/. A Study of an N/-gr am L anguage Mo del for Sp e e ch R e c o gnition /. PhD thesis/, Departmen t of\nComputer Science/, Queen/'s Univ ersit y /, Belfast/, /1/9/9/3/.\n/9\n/[/3/2/] P eter O/'Bo yle/, Marie Ow ens/, and F/.J/. Smith/. A study of a statistical mo del of natural language/. The Irish\nJournal of Psycholo gy /, /1/4/(/3/)/:/3/8/2 /{ /3/9/6/, /1/9/9/3/.\n/[/3/3/] P eter O/'Bo yle/, Marie Ow ens/, and F/.J/. Smith/. A w eigh ted a v erage N/-gram mo del of natural language/.\nComputer Sp e e ch and L anguage /, /8/:/3/3/7 /{ /3/4/9/, /1/9/9/4/.\n/[/3/4/] P eter O/'Bo yle and F/.J/. Smith/. Mark o v mo dels of natural language/. Unpublished Rep ort /: Queen/'s\nUniv ersit y of Belfast/, /1/9/9/4/.\n/[/3/5/] F ernando P ereira and Naftali Tish b y /. Distributed similarit y /, phase transitions and hierarc hical clustering/.\nIn Pr ob abilistic Appr o aches to Natur al L anguage /. American Asso ciation for Arti/\fcial In telligence/, AAAI\nPress/, /1/9/9/2/. T ec hnical rep ort FS/-/9/2/-/0/5/.\n/[/3/6/] L/. R/. Rabiner and B/. J/. Juang/. An in tro duction to hidden mark o v mo dels/. I/.E/.E/.E/. A/.S/.S/.P/. Magazine /,\npages /4 /{ /1/6/, Jan uary /1/9/8/6/.\n/[/3/7/] Allan Ramsa y /. Linguistics /: The cognitiv e science of natural language/. In Thir d Confer enc e on the Co gnitive\nScienc e of Natur al L anguage Pr o c essing /. Dublin Cit y Univ ersit y /, July /1/9/9/4/.\n/[/3/8/] Martin Redington/, Nic k Chater/, and Stev en Finc h/. Distributional information and the acquisition of\nlinguistic categories /: A statistical approac h/. In Pr o c e e dings of the Fifte enth A nnual Confer enc e of the\nCo gnitive Scienc e So ciety /, /1/9/9/3/.\n/[/3/9/] Martin Redington/, Nic k Chater/, and Stev en Finc h/. The p oten tial con tribution of distributional information\nto early syn tactic category acquisition/. Unpublished Rep ort/, /1/9/9/4/.\n/[/4/0/] Philip S/. Resnik/. Sele ction and Information /: A Class/-Base d Appr o ach to L exic al R elationships /. PhD thesis/,\nComputer and Information Science/, Univ ersit y of P ennsylv ania/, Decem b er /1/9/9/3/. Institute for Researc h in\nCognitiv e Science Rep ort I/.R/.C/.S/./-/9/3/-/4/2/.\n/[/4/1/] Geo/\u000brey Sampson/. Evidence against the Grammatical//Ung ram m at ical distinction/. In Wilem Meijs/, editor/,\nCorpus Linguistics and Beyond /| Pr o c e e dings of the Seventh International Confer enc e on English L anguage\nR ese ar ch on Computerize d Corp or a /, pages /2/1/9 /{ /2/2/6/. Ro dopi/,Amsterdam /, /1/9/8/7/.\n/[/4/2/] Hinric h Sc h / utze/. P art/-of/-sp eec h induction from scratc h/. In Pr o c e e dings of the Asso ciation for Computational\nLinguistics /3/1 /, pages /2/5/1 /{ /2/5/8/, /1/9/9/3/.\n/[/4/3/] C/.E/. Shannon/. Prediction and en trop y of prin ted English/. Bel l System T e chnic al Journal /, /1/9/5/1/.\n/[/4/4/] Jo erg Ueb erla/. Analysing a simple language mo del /| some general conclusions for language mo dels for\nsp eec h recognition/. Computer Sp e e ch and L anguage /, /8/:/1/5/3 /{ /1/7/6/, /1/9/9/4/.\n/[/4/5/] J/. Gerard W ol/\u000b/. T owar ds a The ory of Co gnition and Computing /. Ellis Horw o o d/, /1/9/9/1/.\n/[/4/6/] G/. K/. Zipf/. Human Behaviour and the Principle of L e ast E/\u000bort /. Addison/-W esley /, /1/9/4/9/.\n/1/0\n3 4 6 Britian John Sir all another both each her keep let make making many once several some such\ntaking ten these this those too whom\nDr Miss Mr Mrs a an any every his its my no our their whose your\nI he it she there they we who you\n*' **' although but cent certainly even everything having how however indeed nor particularly\nperhaps so sometimes then therefore though thus what whether which while why yet ~*'\nEnglish French Minister President act age air amount answer area art bed board boody book boy\nbuilding business car case cases century change child children church committee company\nconditions control cost council countries country course day days deal death development door\ndoubt early education effect end evening evidence experience eyes face fact family father feeling\nfeet field figure figures food form friends full future general girl government group hand hands\nheart history house idea increase individual industry influence interest job kind knowledge land level\nlife light line man market matter means meeting members men mind moment money morning most\nmother movement music name nature night number office paper part particular party people period\nperson place point police policy political position power private problem problems public question\nrate reason result results room school section sense service short side simple social society stage\nstate story subject system table terms thing things time top town trade type use value various view\nvoice war water way week west wife woman women word words work world year\n) **[formula**] ... 1 10 2 5 A England God London Lord again ago alone away back certain close d\ndifferent either enough example five forward four free further half hard here herself high him\nhimself home hours important itself later least less living love me months more need one open order\nothers out play right six them themselves three times today together true turn two us working years\nable added almost anything asked began being believe better brought came clear come coming\nconcerned considered cut decided difficult doing done due ever except expected far feel followed\nfound getting given go going gone got heard held help hope just kept know known left likely look\nlooked looking made mean much necessary nothing now often only possible put rather read required\nsaid seem seemed seems seen set show shown something soon start still stood sure taken talk\nthought turned used usually want wanted well went yes\nalready also always be become been bring find get give leave meet n't never not pay probably say\nsee take\nthe\nhave really tell\nabout after along around as because before if like near outside over quite reached since that when\nwhere without\nand or\n(\nto\nde of\nacross against among at behind between by during for from in into on through towards under until\nupon with within\n'd 'll can could did do does may might must shall should think will would\n'm 're 's 've am are became felt gave had has is knew saw says told took was were\nbig carried complete few good great large little long longer own personal real small special very\nAmerican British above best common first following human labour last local main modern national\nnew next old other past present same second total white whole yound\n*- , ;\ncalled down met off round s up\n. than\n! : ? per\nFigure /1/: Final distribution of the most frequen t w ords from the lob corpus/. Only the /\frst /\fv e lev els of\nclassi/\fcation are giv en here/, but imp ortan t syn tactic relations are already clear/.\n/1/1\nher\nanother\neach\nsome\nmany several those\nthis\nBritain\nSir\nJohn\nthese\nlet\nkeep\nmake\nall both once whom\nmaking talking such\n3 4 6 ten\ntoo\nany\nevery\nan\na\nno\nour\nits\ntheir\nhis\nmy your whose\nMr\nMiss\nDr\nMiss\nFigure /2/: Detail of relationship b et w een w ords whose /\fnal tag v alue starts with the four bits /0/0/0/0/. Man y of\nthese w ords exhibit determiner/-lik e b eha viour/.\n/1/2\ndoubt\ncases days\nday night morning evening week year moment\nreason thing place way time job\nbook line word field system position story subject\nboy girl child man woman person\nnumber amount value figure level rate case matter sense end period stage\ngroup meeting idea point question problem result kind type part side\nact means effect feeling form answer cost deal change increase influence\nuse evidence\ntable section terms\nfact\nindividual particular general public private political social English French\nshort simple early west air\nfull most top various President\nboard body committee party Minister company council government police\nsociety industry war trade market policy office building church door house\nroom school country town world car paper family century future movement\nlife death food light water land development age education experience\nknowledge history conditions figures work view name area nature interest\nbusiness service course members problems results art music money bed\ncontrol power state\neyes face feet hand hands head heart mind voice father mother wife\nchildren friends people men women countries words things\nFigure /3/: Detail/, from lev el /5 to lev el /9/, of man y noun/-lik e w ords/. Clear seman tic di/\u000berences are registered/.\n/1/3\narm breath breathing  cheek chin coat eye fist forehead hair handkerchief hat head knee lad memory mouth\nneck nose purse shoulder skirt throat whistle wrist ability curiosity destination discretion employer hairdresser\nheritage inheritance intention tone irritation lifetime loneliness midst mistress profession reluctance typewriter\ncareer castle\naunt brother father father-in-law husband mother sister uncle wife gaze jaw lip voice diary wallet\ncompanion lordship partner wholesaler\napproval charm conscience contempt disposal embarrassment engagement fate fault good ideal imagination\nmind name opinion resignation sake soul speech temper thesis vision will arrival audience correspondent tutor\ndesigner friend neighbour lord lover environment childhood cousin daughter son niece elbow hand tongue lap\napartment cave surgery uniform\ndecade year month fortnight week hour inch lot spot step string host row pool pot ending matter instant\nminute moment while pause second bit minimum way clue chance fool illusion joke gasp mistake nuisance\noffence pity reason proposition enquiry job ship compartment room chair leg ridge blanket altar envelope\npowder adult debt determination disposition\nactor actress artist boy bride captain catholic chap child citizen composer couple critic doctor engineer fellow\ngentleman girl god hostess individual journalist king lady lawyer legend man novelist observer painter patient\npeople person priest prisoner producer psychologist queen ruler scholar scientist singer soldier sovereign\nstranger teacher widow woman writer bird cow creature dog mantis rat tree assembly republic accident\nincident situation experiment target corruption scandal struggle armistice invitation obligation opportunity\ntemptation tendency willingness desire passion mood fortune multitude maximum alternative amendment\nproposal decision document lease catechism inscription phrase word poem attempt effort gesture sigh\nwhisper message bell knife pen meal poultice clock taxi cloud lawn staircase device explosive explosion shot\nray shower dress cathedral hut\nliving-room lounge attic roof bathroom bedroom dining-room drawing reception drawing-room kitchen\nsitting-room carpet floor window cooker oven cabinet desk lamp mirror cupboard shelf telephone sofa hearth\nfire fireplace flame turf corner corridor hall door doorway entrance tunnel lock key gate background rear\nboundary hedge wall yard ground surface square garage garden terrace farm farmhouse outside field\ncourtyard barn stable cabin cage pit cell cottage flat studio hotel house laboratory palace tower bay beach\nlake pond river pier mud net city town village neighbourhood country nation landscape park forest hill slope\nmine lane road street island moon moonlight sun sky storm wind weather horse tractor continent world\nlandlord owner maid manufacturer peasant cafe clinic concert funeral inquest hearing honeymoon wedding\nairport platform helicopter boat bus car coach plane flight journey mission model bishop vicar parish church\npulpit throne title temple devil gospel truth oath trio tribe budget loan pension penalty prize exchequer\ntaxpayer campaign presentation election term conservative ambulance wound brain baby body finger chest\nwaist stomach womb heart flesh skin heel collar pocket bomb receiver tape gun pistol rifle bullet sergeant\ndrillbriefcase dictionary magazine camera cigarette jacket bow shaft stem sword wing ontrary craft crying\ndark week-end weekend evening future past defendant registrar spectator student subject editor\ngrandfather downwash flood trawl incidence left reverse outset peak waltz wolf nucleus\nfeels regards sees thinks knows likes loves wants wishes seeks calls asks changes describes says uses plays\nowns loses\nadmit assume believe think understand realise realize reckon conclude remember decide dare deserve expect\nsuppose seem tend appear prefer want own bother afford refuse fail feel forget hate hesitate intend know\nlearn like  begin propose try continue cease\nacts arises begins consists depends lies occurs refers rests varies\nassumption certainty doubt hope wonder\nendeavour excuse hurry need request risk urge right sign\nFigure /4/: Seman tic clustering results/. Mem b erships of /1/1 of the /3/8 classes whose size is greater than ten/, at\na classi/\fcation lev el of /9 bits/. F rom the lob corpus/. Bo dy parts/, relativ es/, men tal states/, h uman roles/, house\nparts/, t w o classes of men tal v erb/, relation v erbs/, hop e/-nouns/, e/\u000bort v erbs/.\n/1/4\nadmiration affection agony anger ambition amusement anticipation anxiety apathy authority awareness\nbeauty belief bitterness chaos character coincidence communication composition continuity\nconcentration concern confidence conflict conformity controversy confusion consciousness courage\ncourtesy criticism cruelty depression despair destiny devotion difficulty dignity disappointment disorder\ndisturbance drama efficiency enjoyment entertainment enthusiasm equality evil excitement experience\nfailure fairness fantasy freedom friendship frustration fun genius goodwill gossip grace gratitude gravity\ngrief growth guilt happiness harmony haste hatred hospitality humanity humour ideology ignorance\nimpatience impulse inclination independence indignation injustice inspiration interest jealousy joy\njudgement judgment justice kindness laughter learning liberty licence luck maturity morality nonsense\noptimism pain patience perception personality perspective philosophy pleasure popularity pregnancy\nprejudice prestige pride principle quality questioning rage realism relief resentment respect responsibility\nrivalry romance salvation satisfaction scholarship schooling shame shock silence sin size skill stability\nstyle success symmetry sympathy talent temperament tension terror tiredness tragedy triumph trouble\nuncertainty urgency victory vigour violence virtue warmth weakness wealth weight spirit communism\ndemocracy socialism federation parliament technology tradition revolution evolution progress nature\nsociety literature poetry knowledge thinking singing writing thought language logic mathematics\ndisarmament politics economics economy investment inflation employment productivity policy\npornography poverty publicity reality religion hell paradise theology action effect activity routine output\nproduction occupation motion movement operation occurrence behaviour capacity essence shape being\nbaptism life death suicide punishment execution disease illness injury medicine destruction disaster\nimprisonment residence settlement adultery sex marriage crime warning error myth after-care agriculture\nadmission access exposure convenience addition accumulation expansion consumption possession\nexcess completion duration consequence contamination contact greeting assistance evidence\ninformation example selection explanation observation interpretation instruction behalf detail attention\nconsideration conversation consultation permission investigation inspection supervision view visibility\nsight discussion advantage celebration ceremony tribute accordance preparation recognition protection\nredemption remuneration provision pursuit adjustment approximation limitation precision compromise\nnegotiation analogy correspondence comparison parallel practice contrast response reference relation\nrhythm conjunction connection connexion integration implication suspicion rumour outlook direction\nexpulsion acid antibody radioactivity oxidation chemistry physics dose part thread bottle sweat fluid liquid\ngin sherry whisky wine bream cap cream toast gear material money payment hay vegetation wheat\nwood wool rose acre pitch place hole layer length age date midnight instance annum white black mist\nchairman president decoration painting firing noise sport\nappendix diagram fig p Op page paragraph momentum heaven suet\naluminium cotton gold hydrogen oxygen polythene sodium grill salvage football tennis prison wartime grading\nboiling baking cooking engineering manufacturing shopping housing boarding trading export finance finishing\nsintering electricity ignition motor rocket railway science-fiction bismuth brass metal nerve oak resin grammar\nnoun infant kinship dairy census county amateur golf\nbending torsion-flexure drift equilibrium equity rotor coolant ion exhaust humus leather plastic textile consumer\nretail labour wage borstal constituency qualifying slenderness\nbend boil brush clasp cover cut feed fly grasp help hurt kick kiss knock melt move offset open pass pop push\nregister repair repeat ride ruin run sail scream sleep stand start stay stick swim talk tie turn wake walk wash\nwatch wear worry blame regret call concentrate look count fancy offer guess mention\nFigure /5/: More seman tic clustering results/. Mem b erships of /6 of the /3/8 classes whose size is greater than ten/,\nat a classi/\fcation lev el of /9 bits/. F rom the lob corpus/. Men tal states/, writing reference/, materials/, materials\nand pro cessing/, more materials and pro cessing/, common manipulation v erbs/.\n/1/5\nw\ng zh\nb th\ny\na\nuu\ne aa\ni\no oo\ner u\nar aw\nai oi\nie ou\ney\nee\noa\nei ia\nn ng\nr rx\ndh z\nm v\nl s\ndj\nsh t\nk p\nch z\nf\nFigure /6/: Automatic Phoneme Clustering/; di/\u000beren tiation b et w een v o w els and consonan ts/. F rom a phonemic\nv ersion of the v odis corpus/.\n/1/6\n0 20 40 60 80 100 120 140 160 180 200\nNumber of  Clusters\n20\n30\n40\n50\n60\n70\n80\n90\n100\nHughes Evaluation Percentage\nAnnealing Based Clustering\nWard's Method and Manhattan Metric\nSpearman Rank and Group Average\nFigure /7/: Graph sho wing the p erformance of our annealing classi/\fcation system compared to t w o recen t systems\n/| those of Hughes and A t w ell and Finc h and Chater/. P erformance is measured b y the Hughes/-A t w ell cluster\nev aluation system using the tagged lob corpus/.\nLanguage Mo del T est Set P erplexit y\nEquiprobable /3/3/,/3/6/0\nUnigram /1/,/2/2/6/./7\nIn terp olated T rigram /( f /-indep enden t/) /7/0/1/./7\nW eigh ted Av erage /6/3/0/./9\nIn terp olated T rigram /( f /-dep enden t/) /6/2/1/./6\n/1/6/-/8/-/5 Structural T ag /5/8/6/./5\nFigure /8/: A list of language mo del systems/, whic h sho ws a steady reduction in test set p erplexit y /. The w eigh ted\na v erage mo del result is based on t w o thirds of the training material a v ailable to the other systems/. A three/-lev el\nclass/-based mo del has results in an impro v emen t o v er the frequency/-dep enden t in terp olated trigram mo del/.\nsen tence W/.A/. /1/6/-/8/-/5\nthe b o y seat the sandwic hes /0/./5/5/9/7/0 /0/./4/4/9/6/6\nthe b o ys eat the sandwic hes /0/./2/9/2/5/5 /0/./5/0/5/3/8\nthe b o y seat this and whic h is /0/./0/7/1/2/8 /0/./0/0/7/8/3\nthe b o ys eat this and whic h is /0/./0/3/7/9/4 /0/./0/0/8/5/5\nthe buo ys eat the sandwic hes /0/./0/3/1/8/9 /0/./0/2/6/8/8\nthe buo ys eat this and whic h is /0/./0/0/4/1/4 /0/./0/0/0/4/5\nthe b o ys eat the sand whic h is /0/./0/0/2/2/5 /0/./0/0/1/1/9\nthe buo ys eat the sand whic h is /0/./0/0/0/2/4 /0/./0/0/0/0/6\nthe buo y seat this and whic h is /0 /0\nFigure /9/: Nine v ersions of a phonemically iden tical oron ym/, ordered b y w eigh ted a v erage /(W/.A/./) probabilit y /,\nnormalised o v er the test sequences/. The W/.A/. language mo del ranks the preferred sen tence second/. The /1/6/-/8/-/5\nstructural tag mo del successfully predicts the original utterance as the most lik ely /.\n/1/7"
}