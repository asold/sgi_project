{
    "title": "A Generalization of Transformer Networks to Graphs",
    "url": "https://openalex.org/W3113177135",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4282733030",
            "name": "Dwivedi, Vijay Prakash",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3083767140",
            "name": "Bresson, Xavier",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2997736261",
        "https://openalex.org/W3012871709",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2558460151",
        "https://openalex.org/W2996491671",
        "https://openalex.org/W3007332492",
        "https://openalex.org/W2559839022",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2244807774",
        "https://openalex.org/W2972510165",
        "https://openalex.org/W2768242641",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2962711740",
        "https://openalex.org/W2962900880",
        "https://openalex.org/W2911710347",
        "https://openalex.org/W3008037438",
        "https://openalex.org/W2027482274",
        "https://openalex.org/W2946721323",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W3023009950",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2971933740",
        "https://openalex.org/W3094003325",
        "https://openalex.org/W2964145825",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3007913393",
        "https://openalex.org/W3093499132",
        "https://openalex.org/W3000577518",
        "https://openalex.org/W2097308346",
        "https://openalex.org/W2606780347",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W3081636763",
        "https://openalex.org/W2804057010",
        "https://openalex.org/W2970066309"
    ],
    "abstract": "We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.",
    "full_text": "A Generalization of Transformer Networks to Graphs\nVijay Prakash Dwivedi,¶ Xavier Bresson¶\n¶ School of Computer Science and Engineering, Nanyang Technological University, Singapore\nvijaypra001@e.ntu.edu.sg, xbresson@ntu.edu.sg\nAbstract\nWe propose a generalization of transformer neural network\narchitecture for arbitrary graphs. The original transformer\nwas designed for Natural Language Processing (NLP), which\noperates on fully connected graphs representing all connec-\ntions between the words in a sequence. Such architecture does\nnot leverage the graph connectivity inductive bias, and can\nperform poorly when the graph topology is important and\nhas not been encoded into the node features. We introduce a\ngraph transformer with four new properties compared to the\nstandard model. First, the attention mechanism is a function\nof the neighborhood connectivity for each node in the graph.\nSecond, the positional encoding is represented by the Lapla-\ncian eigenvectors, which naturally generalize the sinusoidal\npositional encodings often used in NLP. Third, the layer\nnormalization is replaced by a batch normalization layer,\nwhich provides faster training and better generalization per-\nformance. Finally, the architecture is extended to edge feature\nrepresentation, which can be critical to tasks s.a. chemistry\n(bond type) or link prediction (entity relationship in knowl-\nedge graphs). Numerical experiments on a graph benchmark\ndemonstrate the performance of the proposed graph trans-\nformer architecture. This work closes the gap between the\noriginal transformer, which was designed for the limited case\nof line graphs, and graph neural networks, that can work with\narbitrary graphs. As our architecture is simple and generic,\nwe believe it can be used as a black box for future applica-\ntions that wish to consider transformer and graphs.1\n1 Introduction\nThere has been a tremendous success in the ﬁeld of nat-\nural language processing (NLP) since the development of\nTransformers (Vaswani et al. 2017) which are currently the\nbest performing neural network architectures for handling\nlong-term sequential datasets such as sentences in NLP.\nThis is achieved by the use of attention mechanism (Bah-\ndanau, Cho, and Bengio 2014) where a word in a sentence\nattends to each other word and combines the received in-\nformation to generate its abstract feature representations.\nFrom a perspective of message-passing paradigm (Gilmer\nAAAI’21 Workshop on Deep Learning on Graphs: Methods and\nApplications (DLG-AAAI’21). Copyright © 2021, Association for\nthe Advancement of Artiﬁcial Intelligence (www.aaai.org). All\nrights reserved.\n1https://github.com/graphdeeplearning/graphtransformer.\net al. 2017) in graph neural networks (GNNs), this process\nof learning word feature representations by combining fea-\nture information from other words in a sentence can alter-\nnatively be viewed as a case of a GNN applied on a fully\nconnected graph of words (Joshi 2020). Transformers based\nmodels have led to state-of-the-art performance on several\nNLP applications (Devlin et al. 2018; Radford et al. 2018;\nBrown et al. 2020). On the other hand, graph neural net-\nworks (GNNs) are shown to be the most effective neural\nnetwork architectures on graph datasets and have achieved\nsigniﬁcant success on a wide range of applications, such\nas in knowledge graphs (Schlichtkrull et al. 2018; Chami\net al. 2020), in social sciences (Monti et al. 2019), in physics\n(Cranmer et al. 2019; Sanchez-Gonzalez et al. 2020), etc.\nIn particular, GNNs exploit the given arbitrary graph struc-\nture while learning the feature representations for nodes and\nedges and eventually the learned representations are used for\ndownstream tasks. In this work, we explore inductive biases\nat the convergence of these two active research areas in deep\nlearning towards presenting an improved version of Graph\nTransformer (see Figure 1) which extends the key design\ncomponents of the NLP transformers to arbitrary graphs.\n1.1 Related Work\nAs a preliminary, we highlight the most recent research\nworks which attempt to develop graph transformers (Li et al.\n2019; Nguyen, Nguyen, and Phung 2019; Zhang et al. 2020)\nwith few focused on specialized cases such as on heteroge-\nneous graphs, temporal networks, generative modeling, etc.\n(Yun et al. 2019; Xu, Joshi, and Bresson 2019; Hu et al.\n2020; Zhou et al. 2020).\nThe model proposed in Li et al. (2019) employs attention\nto all graph nodes instead of a node’s local neighbors for\nthe purpose of capturing global information. This limits the\nefﬁcient exploitation ofsparsity which we show is a good in-\nductive bias for learning on graph datasets. For the purpose\nof global information, we argue that there are other ways to\nincorporate the same instead of letting go sparsity and local\ncontexts. For example, the use of graph-speciﬁc positional\nfeatures (Zhang et al. 2020), or node Laplacian position\neigenvectors (Belkin and Niyogi 2003; Dwivedi et al. 2020),\nor relative learnable positional information (You, Ying, and\nLeskovec 2019), virtual nodes (Li et al. 2015), etc. Zhang\net al. (2020) propose Graph-BERT with an emphasis on pre-\narXiv:2012.09699v2  [cs.LG]  24 Jan 2021\nAdd&Norm \nAdd&Norm \nAdd&Norm \nAdd&Norm \nAdd&Norm \nAdd&Norm \nGraph Transformer Layer Graph Transformer Layer\nwith edge features\n+ + \nLaplacian\tEigV ecs\tas \nPositional\tEncoding \n+ + \nFigure 1: Block Diagram of Graph Transformer with Laplacian Eigvectors ( λ) used as positional encoding (LapPE). LapPE\nis added to input node embeddings before passing the features to the ﬁrst layer. Left: Graph Transformer operating on node\nembeddings only to compute attention scores; Right: Graph Transformer with edge features with designated feature pipeline to\nmaintain layer wise edge representations. In this extension, the available edge attributes in a graph is used to explicitly modify\nthe corresponding pairwise attention scores.\ntraining and parallelized learning using a subgraph batch-\ning scheme that creates ﬁxed-size linkless subgraphs to be\npassed to the model instead of the original graph. Graph-\nBERT employs a combination of several positional encod-\ning schemes to capture absolute node structural and rela-\ntive node positional information. Since the original graph is\nnot used directly in Graph-BERT and the subgraphs do not\nhave edges between the nodes ( i.e., linkless), the proposed\ncombination of positional encodings attempts at retaining\nthe original graph structure information in the nodes. We\nperform detailed analysis of Graph-BERT positional encod-\ning schemes, along with experimental comparison with the\nmodel we present in this paper in Section 4.1.\nYun et al. (2019) developed Graph Transformer Networks\n(GTN) to learn on heterogeneous graphs with a target to\ntransform a given heterogeneous graph into a meta-path\nbased graph and then perform convolution. Notably, their\nfocus behind the use of attention framework is for inter-\npreting the generated meta-paths. There is another trans-\nformer based approach developed for heterogeneous in-\nformation networks, namely Heterogeneous Graph Trans-\nformer (HGT) by Hu et al. (2020). Apart from its ability\nof handling arbitrary number of node and edge types, HGT\nalso captures the dynamics of information ﬂow in the hetero-\ngeneous graphs in the form of relative temporal positional\nencoding which is based on the timestamp differences of the\ncentral node and the message-passing nodes. Furthermore,\nZhou et al. (2020) proposed a transformer based generative\nmodel which generates temporal graphs by directly learn-\ning from dynamic information in networks. The architecture\npresented in Nguyen, Nguyen, and Phung (2019) somewhat\nproceeds along our goal to develop graph transformer for ar-\nbitrary homogeneous graphs with a coordinate embedding\nbased positional encoding scheme. However, their experi-\nments show that the coordinate embeddings are not universal\nin performance and only helps in a couple of unsupervised\nlearning experiments among all evaluations.\n1.2 Contributions\nOverall, we ﬁnd that the most fruitful ideas from the trans-\nformers literature in NLP can be applied in a more efﬁcient\nway and posit that sparsity and positional encodings are two\nkey aspects in the development of a Graph Transformer. As\nopposed to designing a best performing model for speciﬁc\ngraph tasks, our work attempts for a generic, competitive\ntransformer model which draws ideas together from the do-\nmains of NLP and GNNs. For an overview, this paper brings\nthe following contributions:\n• We put forward a generalization of transformer networks\nto homogeneous graphs of arbitrary structure, namely\nGraph Transformer, and an extended version of Graph\nTransformer with edge features that allows the usage of\nexplicit domain information as edge features.\n• Our method includes an elegant way to fuse node po-\nsitional features using Laplacian eigenvectors for graph\ndatasets, inspired from the heavy usage of positional en-\ncodings in NLP transformer models and recent research\non node positional features in GNNs. The comparison\nwith literature shows Laplacian eigenvectors to be well-\nplaced than any existing approaches to encode node posi-\ntional information for arbitrary homogeneous graphs.\n• Our experiments demonstrate that the proposed model\nsurpasses baseline isotropic and anisotropic GNNs. The\narchitecture simultaneously emerges as a better attention\nbased GNN baseline as well as a simple and effective\nTransformer network baseline for graph datasets for fu-\nture research at the intersection of attention and graphs.\n2 Proposed Architecture\nAs stated earlier, we take into account two key aspects to\ndevelop Graph Transformers – sparsity and positional en-\ncodings which should ideally be used in the best possible\nway for learning on graph datasets. We ﬁrst discuss the mo-\ntivations behind these using a transition from NLP to graphs,\nand then introduce the architecture proposed.\n2.1 On Graph Sparsity\nIn NLP transformers, a sentence is treated as a fully con-\nnected graph and this choice can be justiﬁed for two reasons\n– a) First, it isdifﬁcult to ﬁnd meaningful sparse interactions\nor connections among the words in a sentence. For instance,\nthe dependency of a word in a sentence on another word can\nvary with context, perspective of a user and speciﬁc applica-\ntion. There can be numerous plausible ground truth connec-\ntions among words in a sentence and therefore, text datasets\nof sentences do not have explicit word interactions available.\nIt thereby makes sense to have each word attending to each\nother word in a sentence, as followed by the Transformer\narchitecture (Vaswani et al. 2017). – b) Next, the so-called\ngraph considered in an NLP transformer often has less than\ntens or hundreds of nodes ( i.e. sentences are often less than\ntens or hundreds of words). This makes for computationally\nfeasibility and large transformer models can be trained on\nsuch fully connected graphs of words.\nIn case of actual graph datasets, graphs have arbitrary con-\nnectivity structure available depending on the domain and\ntarget of application, and have node sizes in ranges of up\nto millions, or billions. The available structure presents us\nwith a rich source of information to exploit as an inductive\nbias in a neural network, whereas the node sizes practically\nmakes it impossible to have a fully connected graph for such\ndatasets. On these accounts, it is ideal and practical to have\na Graph Transformer where a node attends to local node\nneighbors, same as in GNNs (Defferrard, Bresson, and Van-\ndergheynst 2016; Kipf and Welling 2017; Monti et al. 2017;\nGilmer et al. 2017; Veliˇckovi´c et al. 2018; Bresson and Lau-\nrent 2017; Xu et al. 2019).\n2.2 On Positional Encodings\nIn NLP, transformer based models are, in most cases, sup-\nplied with a positional encoding for each word. This is criti-\ncal to ensure unique representation for each word, and even-\ntually preserve distance information. For graphs, the design\nof unique node positions is challenging as there are sym-\nmetries which prevent canonical node positional informa-\ntion (Murphy et al. 2019). In fact, most of the GNNs which\nare trained on graph datasets learn structural node informa-\ntion that are invariant to the node position (Srinivasan and\nRibeiro 2020). This is a critical reason why simple attention\nbased models, such as GAT (Veliˇckovi´c et al. 2018), where\nthe attention is a function of local neighborhood connectiv-\nity, instead full-graph connectivity, do not seem to achieve\ncompetitive performance on graph datasets. The issue of po-\nsitional embeddings has been explored in recent GNN works\n(Murphy et al. 2019; You, Ying, and Leskovec 2019; Srini-\nvasan and Ribeiro 2020; Dwivedi et al. 2020; Li et al. 2020)\nwith a goal to learn both structural and positional features.\nIn particular, Dwivedi et al. (2020) make the use of avail-\nable graph structure to pre-compute Laplacian eigenvectors\n(Belkin and Niyogi 2003) and use them as node positional\ninformation. Since Laplacian PEs are generalization of the\nPE used in the original transformers (Vaswani et al. 2017)\nto graphs and these better help encode distance-aware in-\nformation ( i.e., nearby nodes have similar positional fea-\ntures and farther nodes have dissimilar positional features),\nwe use Laplacian eigenvectors as PE in Graph Transformer.\nAlthough these eigenvectors have multiplicity occuring due\nto the arbitrary sign of eigenvectors, we randomly ﬂip the\nsign of the eigenvectors during training, following Dwivedi\net al. (2020).We pre-compute the Laplacian eigenvectors of\nall graphs in the dataset. Eigenvectors are deﬁned via the\nfactorization of the graph Laplacian matrix;\n∆ = I −D−1/2AD−1/2 = UT ΛU, (1)\nwhere A is the n ×n adjacency matrix, D is the degree\nmatrix, and Λ, U correspond to the eigenvalues and eigen-\nvectors respectively. We use theksmallest non-trivial eigen-\nvectors of a node as its positional encoding and denote byλi\nfor node i. Finally, we refer to Section 4.1 for a comparison\nof Laplacian PE with existing Graph-BERT PEs.\n2.3 Graph Transformer Architecture\nWe now introduce the Graph Transformer Layer and Graph\nTransformer Layer with edge features. The layer archi-\ntecture is illustrated in Figure 1. The ﬁrst model is de-\nsigned for graphs which do not have explicit edge attributes,\nwhereas the second model maintains a designated edge fea-\nture pipeline to incorporate the available edge information\nand maintain their abstract representations at every layer.\nInput First of all, we prepare the input node and edge em-\nbeddings to be passed to the Graph Transformer Layer. For\na graph Gwith node features αi ∈Rdn×1 for each node i\nand edge features βij ∈Rde×1 for each edge between node\ni and node j, the input node features αi and edge features\nβij are passed via a linear projection to embed these to d-\ndimensional hidden features h0\ni and e0\nij.\nˆh0\ni = A0αi + a0 ; e0\nij = B0βij + b0, (2)\nwhere A0 ∈Rd×dn , B0 ∈Rd×de and a0,b0 ∈Rd are the\nparameters of the linear projection layers. We now embed\nthe pre-computed node positional encodings of dim kvia a\nlinear projection and add to the node features ˆh0\ni .\nλ0\ni = C0λi + c0 ; h0\ni = ˆh0\ni + λ0\ni , (3)\nwhere C0 ∈Rd×k and c0 ∈Rd. Note that the Laplacian po-\nsitional encodings are only added to the node features at the\ninput layer and not during intermediate Graph Transformer\nlayers.\nGraph Transformer Layer The Graph Transformer is\nclosely the same transformer architecture initially proposed\nin (Vaswani et al. 2017), see Figure 1 (Left). We now pro-\nceed to deﬁne the node update equations for a layer ℓ.\nˆhℓ+1\ni = Oℓ\nh\nHn\nk=1\n(∑\nj∈Ni\nwk,ℓ\nij Vk,ℓhℓ\nj\n)\n, (4)\nwhere, wk,ℓ\nij = softmaxj\n(Qk,ℓhℓ\ni ·Kk,ℓhℓ\nj√dk\n)\n, (5)\nand Qk,ℓ,Kk,ℓ,V k,ℓ ∈Rdk×d, Oℓ\nh ∈Rd×d, k= 1 to Hde-\nnotes the number of attention heads, and ∥denotes concate-\nnation. For numerical stability, the outputs after taking expo-\nnents of the terms inside softmax is clamped to a value be-\ntween −5 to +5. The attention outputs ˆhℓ+1\ni are then passed\nto a Feed Forward Network (FFN) preceded and succeeded\nby residual connections and normalization layers, as:\nˆˆh\nℓ+1\ni = Norm\n(\nhℓ\ni + ˆhℓ+1\ni\n)\n, (6)\nˆˆˆh\nℓ+1\ni = Wℓ\n2 ReLU(Wℓ\n1\nˆˆh\nℓ+1\ni ), (7)\nhℓ+1\ni = Norm\n(ˆˆh\nℓ+1\ni + ˆˆˆh\nℓ+1\ni\n)\n, (8)\nwhere Wℓ\n1 ,∈R2d×d, Wℓ\n2 ,∈Rd×2d, ˆˆh\nℓ+1\ni ,ˆˆˆh\nℓ+1\ni denote in-\ntermediate representations, and Norm can either be Layer-\nNorm(Ba, Kiros, and Hinton 2016) or BatchNorm (Ioffe and\nSzegedy 2015). The bias terms are omitted for clarity of pre-\nsentation.\nGraph Transformer Layer with edge features The\nGraph Transformer with edge features is designed for bet-\nter utilization of rich feature information available in several\ngraph datasets in the form of edge attributes. See Figure 1\n(Right) for a reference to the building block of a layer. Since\nour objective remains to better use the edge features which\nare pairwise scores corresponding to a node pair, we tie these\navailable edge features to implicit edge scores computed by\npairwise attention. In other words, say an intermediate at-\ntention score before softmax, ˆwij, is computed when a node\niattends to node j after the multiplication of query and key\nfeature projections, see the expression inside the brackets in\nEquation 5. Let us treat this scoreˆwij as implicit information\nabout the edge < i,j >. We now try to inject the available\nedge information for the edge < i,j >and improve the al-\nready computed implicit attention score ˆwij. It is done by\nsimply multiplying the two values ˆwij and eij, see Equa-\ntion 12. This kind of information injection is not seen to be\nexplored much, or applied in NLP Transformers as there is\nusually no available feature information between two words.\nHowever, in graph datasets such as molecular graphs, or\nsocial media graphs, there is often some feature informa-\ntion available on the edge interactions and it becomes nat-\nural to design an architecture to use this information while\nlearning. For the edges, we also maintain a designated node-\nsymmetric edge feature representation pipeline for propagat-\ning edge attributes from one layer to another, see Figure 1.\nWe now proceed to deﬁne the layer update equations for a\nlayer ℓ.\nˆhℓ+1\ni = Oℓ\nh\nHn\nk=1\n(∑\nj∈Ni\nwk,ℓ\nij Vk,ℓhℓ\nj\n)\n, (9)\nˆeℓ+1\nij = Oℓ\ne\nHn\nk=1\n(\nˆwk,ℓ\nij\n)\n, where, (10)\nwk,ℓ\nij = softmaxj( ˆwk,ℓ\nij ), (11)\nˆwk,ℓ\nij =\n(Qk,ℓhℓ\ni ·Kk,ℓhℓ\nj√dk\n)\n·Ek,ℓeℓ\nij, (12)\nand Qk,ℓ,Kk,ℓ,V k,ℓ,Ek,ℓ ∈ Rdk×d, Oℓ\nh,Oℓ\ne ∈ Rd×d,\nk= 1 to H denotes the number of attention head, and ∥de-\nnotes concatenation. For numerical stability, the outputs af-\nter taking exponents of the terms inside softmax is clamped\nto a value between−5 to +5. The outputs ˆhℓ+1\ni and ˆeℓ+1\nij are\nthen passed to separate Feed Forward Networks preceded\nand succeeded by residual connections and normalization\nlayers, as:\nˆˆh\nℓ+1\ni = Norm\n(\nhℓ\ni + ˆhℓ+1\ni\n)\n, (13)\nˆˆˆh\nℓ+1\ni = Wℓ\nh,2ReLU(Wℓ\nh,1\nˆˆh\nℓ+1\ni ), (14)\nhℓ+1\ni = Norm\n(ˆˆh\nℓ+1\ni + ˆˆˆh\nℓ+1\ni\n)\n, (15)\nwhere Wℓ\nh,1,∈R2d×d, Wℓ\nh,2,∈Rd×2d, ˆˆh\nℓ+1\ni ,ˆˆˆh\nℓ+1\ni denote\nintermediate representations,\nˆˆe\nℓ+1\nij = Norm\n(\neℓ\nij + ˆeℓ+1\nij\n)\n, (16)\nˆˆˆe\nℓ+1\nij = Wℓ\ne,2ReLU(Wℓ\ne,1ˆˆe\nℓ+1\nij ), (17)\neℓ+1\nij = Norm\n(\nˆˆe\nℓ+1\nij + ˆˆˆe\nℓ+1\nij\n)\n, (18)\nwhere Wℓ\ne,1,∈ R2d×d, Wℓ\ne,2,∈ Rd×2d, ˆˆe\nℓ+1\nij ,ˆˆˆe\nℓ+1\nij denote\nintermediate representations.\nTask based MLP Layers The node representations ob-\ntained at the ﬁnal layer of Graph Transformer are passed\nto a task based MLP network for computing task-dependent\noutputs, which are then fed to a loss function to train the\nparameters of the model. The formal deﬁnitions of the task\nbased layers that we use can be found in Appendix A.1.\n3 Numerical Experiments\nWe evaluate the performance of proposed Graph Trans-\nformer on three benchmark graph datasets– ZINC (Irwin\net al. 2012), PATTERN and CLUSTER (Abbe 2017) from\na recent GNN benchmark (Dwivedi et al. 2020).\nZINC, Graph Regression ZINC (Irwin et al. 2012) is a\nmolecular dataset with the task of graph property regres-\nsion for constrained solubility. Each ZINC molecule is rep-\nresented as a graph of atoms as nodes and bonds as edges.\nSince this dataset have rich feature information in terms of\nbonds as edge attributes, we use the ‘Graph Transformer\nwith edge features’ for this task. We use the 12K subset of\nthe data as in Dwivedi et al. (2020).\nPATTERN, Node Classiﬁcation PATTERN is a node\nclassiﬁcation dataset generated using the Stochastic Block\nModels (SBM) (Abbe 2017). The task is classify the nodes\ninto 2 communities. PATTERN graphs do not have explicit\nedge features and hence we use the simple ‘Graph Trans-\nformer’ for this task. The size of this dataset is 14K graphs.\nCLUSTER, Node Classiﬁcation CLUSTER is also a\nsynthetically generated dataset using SBM model. The task\nis to assign a cluster label to each node. There are total 6\ncluster labels. Similar to PATTERN, CLUSTER graphs do\nnot have explicit edge features and hence we use the simple\n‘Graph Transformer’ for this task. The size of this dataset is\n12K graphs. We refer the readers to (Dwivedi et al. 2020)\nfor additional information, inlcuding preparation, of these\ndatasets.\nModel Conﬁgurations For experiments, we follow the\nbenchmarking protocol introduced in Dwivedi et al. (2020)\nbased on PyTorch (Paszke et al. 2019) and DGL (Wang et al.\n2019). We use 10 layers of Graph Transformer layers with\neach layer having 8 attention heads and arbitrary hidden di-\nmensions such that the total number of trainable parameters\nis in the range of 500k. We use learning rate decay strategy\nto train the models where the training stops at a point when\nthe learning rate reaches to a value of1×10−6. We run each\nexperiment with 4 different seeds and report the mean and\naverage performance measure of the 4 runs. The results are\nreported in Table 1 and comparison in Table 2.\n4 Analysis and Discussion\nWe now present the analysis of our experiments on the pro-\nposed Graph Transformer Architecture, see Tables 1 and 2.\n• The generalization of transformer network on graphs is\nbest when Laplacian PE are used for node positions and\nBatch Normalization is selected instead of Layer Normal-\nization. For all three benchmark datasets, the experiments\nscore the highest performance in this setting, see Table 1.\n• The proposed architecture performs signiﬁcantly better\nthan baseline isotropic and anisotropic GNNs (GCN and\nGAT respectively), and helps close the gap between the\noriginal transformer and transformer for graphs. Notably,\nour architecture emerges as a fresh and improved attention\nbased GNN baseline surpassing GAT (see Table 2), which\nemploys multi-headed attention inspired by the original\ntransformer (Vaswani et al. 2017) and have been often\nused in the literature as a baseline for attention-based\nGNN models.\n• As expected, sparse graph connectivity is a critical in-\nductive bias for datasets with arbitrary graph structure, as\ndemonstrated by comparing sparse vs. full graph experi-\nments.\n• Our proposed extension of Graph Transformer with edge\nfeatures reaches close to the best performing GNN,\ni.e., GatedGCN, on ZINC. This architecture speciﬁcally\nbrings exciting promise to datasets where domain infor-\nmation along pairwise interactions can be leveraged for\nmaximum learning performance.\n4.1 Comparison to PEs used in Graph-BERT\nIn addition to the reasons underscored in Sections 1.1 and\n2.2, we demonstrate the usefulness of Laplacian eigenvec-\ntors as a suitable candidate PE for Graph Transformer in\nthis section, by its comparison with different PE schemes ap-\nplied in Graph-BERT (Zhang et al. 2020).2 In Graph-BERT,\nwhich operates on ﬁxed size sampled subgraphs, a node at-\ntends to every other node in a subgraph. For a given graph\nG= (V,E) with Vnodes and Eedges, a subgraph gi of size\nk+ 1 is created for every node iin the graph, which means\nthe original single graph Gis converted to Vsubgraphs. For\na subgraph gi corresponding to node ui, the k other nodes\nare the ones which have the topkintimacy scores with node\nui based on a pre-computed intimacy matrix that maps every\nedge in the graphGto an intimacy score. While the sampling\nis great for parallelization and efﬁciency, the original graph\nstructure is not directly used in the layers. Graph-BERT uses\n2Note that we do not perform empirical comparison with other\nPEs in Graph Transformer literature except Graph-BERT, because\nof two reasons: i) Some existing Graph Transformer methods do\nnot use PEs, ii) If PEs are used, they are usually specialised; for\ninstance, Relative Temporal Encoding (RTE) for encoding dynamic\ninformation in heterogeneous graphs in (Hu et al. 2020).\nSparse Graph Full GraphDataset LapPEL #ParamTest Perf.±s.d. Train Perf.±s.d. #Epoch Epoch/TotalTest Perf.±s.d. Train Perf.±s.d. #Epoch Epoch/Total\nBatch Norm:False; Layer Norm:True\nZINC x 10 588353 0.278±0.018 0.027 ±0.004 274.75 26.87s/2.06hr 0.741±0.008 0.431 ±0.013 196.75 37.64s/2.09hr✓ 10 588929 0.284±0.012 0.031 ±0.006 263.00 26.64s/1.98hr 0.735±0.006 0.442 ±0.031 196.75 31.50s/1.77hr\nCLUSTER x 10 523146 70.879±0.295 86.174±0.365 128.50 202.68s/7.32hr19.596±2.071 19.570±2.053 103.00 512.34s/15.15hr✓ 10 524026 70.649±0.250 86.395±0.528 130.75 200.55s/7.43hr27.091±3.920 26.916±3.764 139.50 565.13s/22.37hr\nPATTERN x 10 522742 73.140±13.633 73.070±13.589 184.25 276.66s/13.75hr50.854±0.111 50.906±0.005 108.00 540.85s/16.77hr✓ 10 522982 71.005±11.831 71.125±11.977 192.50 294.91s/14.79hr56.482±3.549 56.565±3.546 124.50 637.55s/22.69hr\nBatch Norm:True; Layer Norm:False\nZINC x 10 588353 0.264±0.008 0.048 ±0.006 321.50 28.01s/2.52hr 0.724±0.013 0.518 ±0.013 192.25 50.27s/2.72hr✓ 10 588929 0.226±0.014 0.059±0.011 287.50 27.78s/2.25hr 0.598±0.049 0.339 ±0.123 273.50 45.26s/3.50hr\nCLUSTER x 10 523146 72.139±0.405 85.857±0.555 121.75 200.85s/6.88hr21.092±0.134 21.071±0.037 100.25 595.24s/17.10hr✓ 10 524026 73.169±0.622 86.585±0.905 126.50 201.06s/7.20hr27.121±8.471 27.192±8.485 133.75 552.06s/20.72hr\nPATTERN x 10 522742 83.949±0.303 83.864±0.489 236.50 299.54s/19.71hr50.889±0.069 50.873±0.039 104.50 621.33s/17.53hr✓ 10 522982 84.808±0.068 86.559±0.116 145.25 309.95s/12.67hr54.941±3.739 54.915±3.769 117.75 683.53s/22.77hr\nTable 1: Results of GraphTransformer (GT) on all datasets. Performance Measure for ZINC is MAE, for PATTERN and CLUS-\nTER is Acc. Results (higher is better for all except ZINC) are averaged over 4 runs with 4 different seeds. Bold: the best\nperforming model for each dataset. We perform each experiment with given graphs (Sparse Graph) and (Full Graph) in\nwhich we create full connections among all nodes; For ZINC full graphs, edge features are discarded given our motive of the\nfull graph experiments without any sparse structure information.\nModel ZINC CLUSTER PATTERN\nGNN BASELINE SCORESfrom (Dwivedi et al. 2020)\nGCN 0.367±0.011 68.498±0.976 71.892±0.334\nGAT 0.384±0.007 70.587±0.447 78.271±0.186\nGatedGCN0.214±0.013 76.082±0.196 86.508±0.085\nOUR RESULTS\nGT (Ours)0.226±0.014 73.169±0.622 84.808±0.068\nTable 2: Comparison of our best performing scores (from\nTable 1) on each dataset against the GNN baselines (GCN\n(Kipf and Welling 2017), GAT (Veliˇckovi´c et al. 2018), Gat-\nedGCN(Bresson and Laurent 2017)) of 500k model param-\neters. Note: Only GatedGCN and GT models use the avail-\nable edge attributes in ZINC.\na combination of node PE schemes to inform the model on\nnode structural, positional, and distance information from\noriginal graph– i) Intimacy based relative PE, ii) Hop based\nrelative distance encoding, and iii) Weisfeiler Lehman based\nabsolute PE (WL-PE). The intimacy based PE and the hop\nbased PE are variant to the sampled subgraphs, i.e., these\nPEs for a node in a subgraph gi depends on the node ui w.r.t\nwhich it is sampled, and cannot be directly used in other\ncases unless we use similar sampling strategy. The WL-PE\nwhich are absolute structural roles of nodes in the original\ngraph computed using WL algorithm (Zhang et al. 2020;\nNiepert, Ahmed, and Kutzkov 2016), are not variant to the\nsubgraphs and can be easily used as a generic PE mecha-\nnism. On that account, we swap Laplacian PE in our experi-\nments for an ablation analysis and use WL-PE from Graph-\nBERT, see Table 3. As Laplacian PE capture better struc-\ntural and positional information about the nodes, which es-\nsentially is the objective behind using the three Graph-BERT\nPEs, they outperform the WL-PE. Besides, WL-PEs tend to\noverﬁt SBM datasets and lead to poor generalization.\nSparse GraphDatasetPE #ParamTest Perf.±s.d. Train Perf.±s.d. #Epoch Epoch/Total\nBatch Norm:True; Layer Norm:False;L= 10\nZINCx 5883530.264±0.008 0.048±0.006 321.50 28.01s/2.52hrL 5889290.226±0.014 0.059±0.011 287.50 27.78s/2.25hrW 5907210.267±0.012 0.059±0.010 263.25 27.04s/2.00hr\nCLUSTERx 52314672.139±0.405 85.857±0.555 121.75 200.85s/6.88hrL 52402673.169±0.622 86.585±0.905 126.50 201.06s/7.20hrW 53114670.790±0.537 86.829±0.745 119.00 196.41s/6.69hr\nPATTERNx 52274283.949±0.303 83.864±0.489 236.50 299.54s/19.71hrL 52298284.808±0.068 86.559±0.116 145.25 309.95s/12.67hrW 53074275.489±0.216 97.028±0.104 109.25 310.11s/9.73hr\nTable 3: Analysis of GraphTransformer (GT) using different\nPE schemes. Notations x: No PE; L: LapPE (ours); W: WL-\nPE (Zhang et al. 2020). Bold: the best performing model for\neach dataset.\n5 Conclusion\nThis work presented a simple yet effective approach to gen-\neralize transformer networks on arbitrary graphs and intro-\nduced the corresponding architecture. Our experiments con-\nsistently showed that the presence of – i) Laplacian eigen-\nvectors as node positional encodings and – ii) batch normal-\nization, in place of layer normalization, around the trans-\nformer feed forward layers enhanced the transformer univer-\nsally on all experiments. Given the simple and generic na-\nture of our architecture and competitive performance against\nstandard GNNs, we believe the proposed model can be used\nas baseline for further improvement across graph applica-\ntions employing node attention. In future works, we are in-\nterested in building upon the graph transformer along as-\npects such as efﬁcient training on single large graphs, ap-\nplicability on heterogeneous domains, etc., and perform ef-\nﬁcient graph representation learning keeping in account the\nrecent innovations in graph inductive biases.\nAcknowledgments\nXB is supported by NRF Fellowship NRFF2017-10.\nReferences\nAbbe, E. 2017. Community detection and stochastic block\nmodels: recent developments. The Journal of Machine\nLearning Research 18(1): 6446–6531.\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. NeurIPS workshop on Deep Learning .\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473 .\nBelkin, M.; and Niyogi, P. 2003. Laplacian eigenmaps for\ndimensionality reduction and data representation. Neural\ncomputation 15(6): 1373–1396.\nBresson, X.; and Laurent, T. 2017. Residual gated graph\nconvnets. arXiv preprint arXiv:1711.07553 .\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165 .\nChami, I.; Wolf, A.; Juan, D.-C.; Sala, F.; Ravi, S.; and R ´e,\nC. 2020. Low-Dimensional Hyperbolic Knowledge Graph\nEmbeddings. arXiv preprint arXiv:2005.00545 .\nCranmer, M. D.; Xu, R.; Battaglia, P.; and Ho, S. 2019.\nLearning Symbolic Physics with Graph Networks. arXiv\npreprint arXiv:1909.05862 .\nDefferrard, M.; Bresson, X.; and Vandergheynst, P. 2016.\nConvolutional Neural Networks on Graphs with Fast Local-\nized Spectral Filtering. In Advances in Neural Information\nProcessing Systems 29, 3844–3852.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nDwivedi, V . P.; Joshi, C. K.; Laurent, T.; Bengio, Y .; and\nBresson, X. 2020. Benchmarking graph neural networks.\narXiv preprint arXiv:2003.00982 .\nGilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and\nDahl, G. E. 2017. Neural message passing for quantum\nchemistry. In Proceedings of the 34th International Confer-\nence on Machine Learning-Volume 70, 1263–1272. JMLR.\norg.\nHu, Z.; Dong, Y .; Wang, K.; and Sun, Y . 2020. Heteroge-\nneous graph transformer. In Proceedings of The Web Con-\nference 2020, 2704–2710.\nIoffe, S.; and Szegedy, C. 2015. Batch normalization: Accel-\nerating deep network training by reducing internal covariate\nshift. arXiv preprint arXiv:1502.03167 .\nIrwin, J. J.; Sterling, T.; Mysinger, M. M.; Bolstad, E. S.;\nand Coleman, R. G. 2012. ZINC: a free tool to discover\nchemistry for biology. Journal of chemical information and\nmodeling 52(7): 1757–1768.\nJoshi, C. 2020. Transformers are Graph Neural Networks.\nThe Gradient .\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Clas-\nsiﬁcation with Graph Convolutional Networks. In Interna-\ntional Conference on Learning Representations (ICLR).\nLi, P.; Wang, Y .; Wang, H.; and Leskovec, J. 2020. Dis-\ntance Encoding–Design Provably More Powerful GNNs\nfor Structural Representation Learning. arXiv preprint\narXiv:2009.00142 .\nLi, Y .; Liang, X.; Hu, Z.; Chen, Y .; and Xing, E. P. 2019.\nGraph Transformer. URL https://openreview.net/forum?id=\nHJei-2RcK7.\nLi, Y .; Tarlow, D.; Brockschmidt, M.; and Zemel, R. 2015.\nGated graph sequence neural networks. arXiv preprint\narXiv:1511.05493 .\nMonti, F.; Boscaini, D.; Masci, J.; Rodola, E.; Svoboda, J.;\nand Bronstein, M. M. 2017. Geometric Deep Learning on\nGraphs and Manifolds Using Mixture Model CNNs. 2017\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) doi:10.1109/cvpr.2017.576.\nMonti, F.; Frasca, F.; Eynard, D.; Mannion, D.; and Bron-\nstein, M. M. 2019. Fake news detection on social\nmedia using geometric deep learning. arXiv preprint\narXiv:1902.06673 .\nMurphy, R.; Srinivasan, B.; Rao, V .; and Ribeiro, B. 2019.\nRelational Pooling for Graph Representations. In Interna-\ntional Conference on Machine Learning, 4663–4673.\nNguyen, D. Q.; Nguyen, T. D.; and Phung, D. 2019. Univer-\nsal Self-Attention Network for Graph Classiﬁcation. arXiv\npreprint arXiv:1909.11855 .\nNiepert, M.; Ahmed, M.; and Kutzkov, K. 2016. Learning\nconvolutional neural networks for graphs. In International\nconference on machine learning, 2014–2023.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; K¨opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training.\nSanchez-Gonzalez, A.; Godwin, J.; Pfaff, T.; Ying, R.;\nLeskovec, J.; and Battaglia, P. W. 2020. Learning to sim-\nulate complex physics with graph networks. arXiv preprint\narXiv:2002.09405 .\nSchlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg, R.;\nTitov, I.; and Welling, M. 2018. Modeling relational data\nwith graph convolutional networks. In European Semantic\nWeb Conference, 593–607. Springer.\nSrinivasan, B.; and Ribeiro, B. 2020. On the Equivalence\nbetween Node Embeddings and Structural Graph Represen-\ntations. International Conference on Learning Representa-\ntions .\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVeliˇckovi´c, P.; Cucurull, G.; Casanova, A.; Romero, A.; Li`o,\nP.; and Bengio, Y . 2018. Graph Attention Networks. Inter-\nnational Conference on Learning Representations .\nWang, M.; Yu, L.; Zheng, D.; Gan, Q.; Gai, Y .; Ye, Z.; Li,\nM.; Zhou, J.; Huang, Q.; Ma, C.; Huang, Z.; Guo, Q.; Zhang,\nH.; Lin, H.; Zhao, J.; Li, J.; Smola, A. J.; and Zhang, Z.\n2019. Deep Graph Library: Towards Efﬁcient and Scalable\nDeep Learning on Graphs. ICLR Workshop on Representa-\ntion Learning on Graphs and Manifolds .\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2019. How\nPowerful are Graph Neural Networks? InInternational Con-\nference on Learning Representations.\nXu, K.; Li, C.; Tian, Y .; Sonobe, T.; Kawarabayashi, K.-\ni.; and Jegelka, S. 2018. Representation learning on\ngraphs with jumping knowledge networks. arXiv preprint\narXiv:1806.03536 .\nXu, P.; Joshi, C. K.; and Bresson, X. 2019. Multi-graph\ntransformer for free-hand sketch recognition. arXiv preprint\narXiv:1912.11258 .\nYou, J.; Ying, R.; and Leskovec, J. 2019. Position-aware\ngraph neural networks. International Conference on Ma-\nchine Learning .\nYun, S.; Jeong, M.; Kim, R.; Kang, J.; and Kim, H. J. 2019.\nGraph transformer networks. In Advances in Neural Infor-\nmation Processing Systems, 11983–11993.\nZhang, J.; Zhang, H.; Sun, L.; and Xia, C. 2020. Graph-Bert:\nOnly Attention is Needed for Learning Graph Representa-\ntions. arXiv preprint arXiv:2001.05140 .\nZhou, D.; Zheng, L.; Han, J.; and He, J. 2020. A Data-\nDriven Graph Generative Model for Temporal Interaction\nNetworks. In Proceedings of the 26th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery & Data Min-\ning, 401–411.\nA Appendix\nA.1 Task based MLP layer equations\nGraph prediction layer For graph prediction task, the ﬁ-\nnal layer node features of a graph is averaged to get a d-\ndimensional graph-level feature vector yG.\nyG= 1\nV\nV∑\ni=0\nhL\ni , (19)\nThe graph feature vector is then passed to a MLP to obtain\nthe un-normalized prediction score for each class, ypred ∈\nRC for each class:\nypred = P ReLU (QyG) , (20)\nwhere P ∈Rd×C,Q ∈Rd×d,C is the number of task la-\nbels (classes) to be predicted. Since we perform single-target\ngraph regression in ZINC, C = 1, and the L1-loss between\nthe predicted and groundtruth values is minimized during\ntraining.\nNode prediction layer For node prediction task, each\nnode’s feature vector is passed to a MLP for computing\nthe un-normalized prediction scores yi,pred ∈RC for each\nclass:\nyi,pred = P ReLU\n(\nQhL\ni\n)\n, (21)\nwhere P ∈Rd×C,Q ∈Rd×d. During training, the cross-\nentropy loss weighted inversely by the class size is used.\nAs a note, these task based layers can be modiﬁed as\nper the requirements of the dataset, and or the prediction to\nbe done. For example, the Graph Transformer edge outputs\n(Figure 1 (Right)) can be used for edge prediction tasks and\nthe task based MLP layers can be deﬁned in similar fashion\nas we do for node prediction. Besides, different styles of us-\ning ﬁnal and/or intermediate Graph Transformer layers can\nbe used as inputs to the task based MLP layers, such as JK\nReadout (Jumping Knowledge) (Xu et al. 2018), etc. used\noften in GNNs.\nA.2 Hardware Information\nAll experiments are run on Intel Xeon CPU E5-2690 v4\nserver with 4 Nvidia 1080Ti GPUs. At a given time, 4 exper-\niments were run on the server with each single GPU running\n1 experiment. The maximum training time for an experiment\nis limited to 24 hours."
}