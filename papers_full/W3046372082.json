{
  "title": "Discriminatively trained continuous Hindi speech recognition using integrated acoustic features and recurrent neural network language modeling",
  "url": "https://openalex.org/W3046372082",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1972612459",
      "name": "A. Kumar",
      "affiliations": [
        "National Institute of Technology Kurukshetra",
        "Galgotias University"
      ]
    },
    {
      "id": "https://openalex.org/A2102849541",
      "name": "R. K. Aggarwal",
      "affiliations": [
        "National Institute of Technology Kurukshetra"
      ]
    },
    {
      "id": "https://openalex.org/A1972612459",
      "name": "A. Kumar",
      "affiliations": [
        "National Institute of Technology Kurukshetra",
        "Galgotias University"
      ]
    },
    {
      "id": "https://openalex.org/A2102849541",
      "name": "R. K. Aggarwal",
      "affiliations": [
        "National Institute of Technology Kurukshetra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2128454066",
    "https://openalex.org/W2089461207",
    "https://openalex.org/W2080025464",
    "https://openalex.org/W2027060129",
    "https://openalex.org/W1494108583",
    "https://openalex.org/W2496955520",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W2088571672",
    "https://openalex.org/W2586419604",
    "https://openalex.org/W4245923654",
    "https://openalex.org/W2130839206",
    "https://openalex.org/W2800057634",
    "https://openalex.org/W2789497025",
    "https://openalex.org/W2030488530",
    "https://openalex.org/W2143657265",
    "https://openalex.org/W2161329381",
    "https://openalex.org/W2001679125",
    "https://openalex.org/W2090861223",
    "https://openalex.org/W1992716054",
    "https://openalex.org/W2745025069",
    "https://openalex.org/W2085715982",
    "https://openalex.org/W2080400971",
    "https://openalex.org/W1992475611",
    "https://openalex.org/W2889152503",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2054145113",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W1974059926",
    "https://openalex.org/W2319748993",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2009150118",
    "https://openalex.org/W2072184010",
    "https://openalex.org/W2092958002",
    "https://openalex.org/W170098597",
    "https://openalex.org/W1553271421",
    "https://openalex.org/W4235154690",
    "https://openalex.org/W2007645738",
    "https://openalex.org/W2156983866",
    "https://openalex.org/W88081813",
    "https://openalex.org/W44638342",
    "https://openalex.org/W2786167576",
    "https://openalex.org/W1758466134",
    "https://openalex.org/W2102346872",
    "https://openalex.org/W2135984923",
    "https://openalex.org/W2109063468",
    "https://openalex.org/W2148154194",
    "https://openalex.org/W2605320104"
  ],
  "abstract": "Abstract This paper implements the continuous Hindi Automatic Speech Recognition (ASR) system using the proposed integrated features vector with Recurrent Neural Network (RNN) based Language Modeling (LM). The proposed system also implements the speaker adaptation using Maximum-Likelihood Linear Regression (MLLR) and Constrained Maximum likelihood Linear Regression (C-MLLR). This system is discriminatively trained by Maximum Mutual Information (MMI) and Minimum Phone Error (MPE) techniques with 256 Gaussian mixture per Hidden Markov Model(HMM) state. The training of the baseline system has been done using a phonetically rich Hindi dataset. The results show that discriminative training enhances the baseline system performance by up to 3%. Further improvement of ~7% has been recorded by applying RNN LM. The proposed Hindi ASR system shows significant performance improvement over other current state-of-the-art techniques.",
  "full_text": "Open Access.© 2020 A. Kumar and R.K. Aggarwal, published by De Gruyter.\n This work is licensed under the Creative Commons\nAttribution 4.0 License\nJ. Intell. Syst. 2021; 30:165–179\nResearch Article\nA. Kumar* and R.K. Aggarwal\nDiscriminatively trained continuous Hindi\nspeech recognition using integrated acoustic\nfeatures and recurrent neural network\nlanguage modeling\nhttps://doi.org/10.1515/jisys-2018-0417\nReceived Oct 17, 2018; accepted Dec 01, 2019\nAbstract:ThispaperimplementsthecontinuousHindiAutomaticSpeechRecognition(ASR)systemusingthe\nproposed integrated features vector with Recurrent Neural Network (RNN) based Language Modeling (LM).\nTheproposedsystemalsoimplementsthespeakeradaptationusingMaximum-LikelihoodLinearRegression\n(MLLR) and Constrained Maximum likelihood Linear Regression (C-MLLR). This system is discriminatively\ntrainedbyMaximumMutualInformation(MMI)andMinimumPhoneError(MPE)techniqueswith256Gaus-\nsianmixtureperHiddenMarkovModel(HMM)state.Thetrainingofthebaselinesystemhasbeendoneusing\na phonetically rich Hindi dataset. The results show that discriminative training enhances the baseline sys-\ntem performance by up to 3%. Further improvement of ~7% has been recorded by applying RNN LM. The\nproposed Hindi ASR system shows significant performance improvement over other current state-of-the-art\ntechniques.\nKeywords: Automatic speech recognition, MFCC, GFCC, WERBC, PLP, discriminative training, MMI, MPE,\nRNN LM\n1 Introduction\nASRistheprocessoftakingspeechutteranceandconvertingitintotextsequenceascloseaspossible.There\nare many functional areas in ASR. Some are as follows: dictation, a program control application, dialog\nsystems, audio indexing, speech-to-speech translation, and query-based information retrieval system, i.e.,\nweather information system, or some travel information system. With the increase in need of end-user fo-\ncused applications such as look for voice and voice communication with the cellular device and domicile\namusement systems, the robust speech recognition that works in all the real-world noises and other acous-\ntic distorting conditions is in demand [29]. To implement the ASR system, some obstacles may occur due to\nabnormalityinspeakingstyleandnoisesintheenvironment.TheacousticenvironmentforASRismuchdif-\nficult or different than in the past [13]. Despite several technological advancements of the ASR system, there\nis a huge gap in terms of accuracy and speed in comparison to the human perspective [2]. The main objec-\ntive behind developing the ASR system is to convert a speech utterance into text sequence, independent of a\nspeaker and the surrounding environment.\n*Corresponding Author: A. Kumar:Research Scholar, Computer Engineering Department, National Institute of Technology,\nKurukshetra, Haryana, India; Assistant Professor, Computer Science and Engineering Department, Galgotias University, Greater\nNoida, Uttar Pradesh, India; Email: anketvit@gmail.com\nR.K. Aggarwal:Associate Professor, Computer Engineering Department, National Institute of Technology, Kurukshetra,\nHaryana, India; Email: rka15969@gmail.com\n166 /bar.twoA. Kumar and R.K. Aggarwal\nThefeatureextractionplaysavitalroleintheASRasanylossofusefulinformationcannotberetrievedin\nlaterprocessingstages.Therearevariousnumberoftechniquesavailabletoextractthespeechfeaturessuch\nas Mel Frequency Cepstral Coefficient (MFCC) [12], Perceptual Linear Predictive Analysis (PLP) [20], Gam-\nmatone Frequency Cepstral Coefficients (GFCC) [43, 44], Linear Prediction Cepstral Coefficients (LPCC) [49],\nand wavelet-based feature extraction techniques [45]. Among all these techniques, MFCC is more popular\nas it shows promising results in clean environment conditions, but the performance of MFCC deteriorates in\nnoisyenvironmentalconditions.Tillnow,thereisnooptimalfeatureextractiontechniqueinthefieldofASR.\nEach feature extraction technique has some advantages and disadvantages. The back-end processing of the\nASR system includes the pattern matching of speech features that are stored in memory with the feature set\nextracted from the test speech signal. The acoustic and language modeling are the two major fields at the\nback-end processing. For more than four decades, HMM was the first choice for acoustic modeling [18]. The\nExpectation-Maximization (EM) algorithm is used to train the parameters in HMM Acoustic Modeling. HMM\nhas some serious issues like an inability to train a large amount of training data with no intra-speaker vari-\nability [35]. As a result, various other acoustic modeling technique comes into existence such as Gaussian\nMixture Model (GMM) [1], Subspace Gaussian Mixture Model (S-GMM) [34], and discriminative techniques.\nThe discriminative techniques like MMI and MPE show significant improvement over Maximum-Likelihood\nEstimation(MLE).ToovercometheproblemsofBaumWelch(BW)algorithm,Extended-BW(E-BW)algorithm\nis used in MMI and MPE techniques [36]. Speech recognition and machine translation are the areas in which\nlanguage modeling plays a vital role [11]. Often, a better language model increases the performance of the\nunderlyingsystem,whichmakesLMvaluableinASR[22].Inthepastfewyears,RNNanddeeplearninghave\nfueled language modeling research [6–8]. The majority of work has been done on RNN models, which are\ncapable of retaining long-term dependencies [22]. Speaker adaptation has also become more popular in the\nlast few years [50]. While speaker-dependent (SD) speech recognition systems can show impressive perfor-\nmance, speaker-independent (SI) systems can provide an average Word Error Rate (WER), a factor of two to\nthree lower than an SD system if both systems use the same amount of training data. In speaker adaptation,\nthe small amount of training data from a new speaker is sufficient to adopt the characteristics of new speak-\ners. Adaptation can significantly improve the WER for outlier speakers such as non-native or others who are\nnot well represented in the SI training set.\nIn this research paper, we used a limited resource Hindi speech dataset (2.5 hours) [41]. For feature ex-\ntraction, various heterogeneous feature combinations were done in this work, and relative information gain\nor losses were recorded. We have shown that heterogeneous features performed well and lead to the ASR\nsystem with better generalization capability. MPE and MMI discriminative techniques were used to train the\nacousticmodel,whichgavesignificantperformancegain.Forlanguagemodeling,theRNNmodelwasusedto\ntrainthetextdata.TheimplementationofLMwasdonebyRNNLMtrainingtoolkitCUED-RNNLMdeveloped\nat CUED [52]. In this work, MLLR and C-MLLR supervised speaker adaptation were also applied. This paper\nhas three outcomes. First, Integrated acoustic features significantly improve the accuracy over traditional\nfeatures. Second, it discriminatively trains the integrated feature vector using MMI and MPE discriminative\ntechniques. Third, it applies RNN LM to improve the accuracy of the proposed system further.\nThe remaining part of the paper is organized as follows: Section 2 explains the concept of different fea-\nture extraction techniques, speaker adaptation, discriminative techniques, and RNN LM. Section 3 gives the\ndetailsoftheproposedarchitecture.Section4describestheHindispeechcorpus.Section5coverstheexper-\niment part of the paper, and section 6 is the conclusion of the proposed system.\nDiscriminatively trained continuous Hindi speech recognition/bar.two167\n2 Preliminaries\n2.1 Feature Extraction\n2.1.1 Frequency Cepstrum Coeflcient (MFCC)\nLet X(n) be the input speech signal and frames are blocked and smoothed by applying hamming window\nW(n). Feature extraction through MFCC involves mainly the following five steps.\ni) Afterperformingpre-emphasisstepoverspeechsignal,shorttimeFourieranalysisisdoneusingham-\nming window [28]. To amplify the energy at a higher frequency, pre-emphasis is generally performed\n[10]. The power spectral estimation is done as:\n˜x(k) =\nN−1∑︁\nn=0\nx(n)W(n).e−J2πnk/ N0 ≤ n< N (1)\nwhere Ncorresponds to hamming window.\nii) AfterthatpowerspectrumispassesthroghtheMel-scaletriangularfilterbank,gettheenergiesofeach\nfilter bank as:\nEm =\nk−1∑︁\nk=0\nϕm(k)Xk; m= 1, 2, ...,M (2)\nwhere Mdenotes the number of triangular filters.\niii) Discrete Cosine Transform (DCT) is applied to the filterbank energies to get the MFCCs(ci):\ncj =\nM∑︁\nm=1\nlog10(Em).cos(j(m+ 0.5) π\nM); j= 1, 2, ...,L (3)\niv) Append normalized frame energy, producing a 13-dimensional standard feature vector.\nv) More features can get by applying first and second derivatives as follows:\n∂ci\n∂τ =\n∑︀\nt τ(︀ct\ni\n)︀ − (︀c−t\ni\n)︀\n2. ∑︀\nt t2 (4)\n∂2ci\n∂τ2 =\n∑︀\nt τ\n(︁∂ct\ni\n∂τ − ∂c−t\ni\n∂τ\n)︁\n2. ∑︀\nt t2 (5)\nwheretis time, andc(t)\ni and c(−t)\ni represents thetth following and previous cepstral coefficients in time\nframe, respectively.\n2.1.2 Gammatone-Frequency Cepstral Coeflcient (GFCC)\nThe MFCC [12] features give promising results in a clean environment, but in a noisy environment, the per-\nformance of MFCC decreases. The GFCC [43, 44] features work well in a noisy environment as their model is\nbased on the human auditory system. The GFCC features are determined by Equivalent Rectangular Band-\nwidth (ERB) scale and set of gammatone filterbanks. We found GFCC features more robust in comparison to\nMFCC and PLP features [15]. The initial operations of GFCC and MFCC are similar. The output of the fourier\ntransform is passed through gammatone filterbank where center frequencyf can be defined as:\ng(f, t) =a.tn−1e−2πbtcos(2π/f_t+ ϕ) (6)\nWhere adenotes a constant,ϕ represent the phase of the filter, andndenotes the order of the filter. The\nfilterbank bandwidthbfactor is denoted as:\nb= 25.17\n(︂4.37f\n1000 + 1\n)︂\n(7)\n168 /bar.twoA. Kumar and R.K. Aggarwal\nAfter that DCT is performed to get the uncorrelated cepstral features.\n2.1.3 Wavelet packet based ERB Cepstral features (WERBC)\nThe wavelet transforms effectively do the time-frequency analysis in the case of the non-stationary or quasi-\nstationary signal [4]. Wavelet packets (WP) [3, 26] shows their importance in signal representation schemes\nsuch as speech analysis [9]. WP’s with the broad coverage of time-frequency characteristics outperform in\ncomparison to standard MFCC features for the speech recognition task.\nFigure 1:Distribution of center frequency (Hz) to 24 Wavelet Packet subbands\nSomeamountofresearchhasbeendoneby[9,16,21,38,39,45]inHindispeechrecognitionusingwavelet\ntransformation. The WERBC feature extraction technique is proposed in 2014 [4]. The process of converting\nthe speech signal into WERBC features via admissible wavelet packet transform is shown in Figure 1. The\nframe size of 25 ms with 70% overlapping is used to extract WERBC features.\nAfter applying the hamming window, the entire frequency band is decomposed with the help of 3-level\nWPdecomposition.Itwillgive8subbandsof1kHzeach.AgainapplyWPdecompositionasshowninFigure1\nto get 24 frequency subband.\nThefirst0-500Hzfrequencybandisdividedinto8subbandsof62.5Hzeach.Thisdivisionfinelyemphasis\na frequency band of 0-500 Hz, which contains a large amount of signal energy. Next, 500-1000 Hz is further\ndecomposedbyapplying2levelWPdecompositiontoget4subbandsof125Hzeach.Next,4sub-bandof250\nHz is found by applying 2 level WP decomposition on 1-2 Hz frequency band. Similarly, 4 subbands of 500\nHz and 4 subbands of 1 kHz are found subsequently. The equal loudness and log are performed to those 24\nsubbandstoget24coefficients.Finally,DCThasbeenappliedtoget12cepstralfeatures.Togetmorecepstral\nfeatures, delta and acceleration coefficients are applied.\nDiscriminatively trained continuous Hindi speech recognition/bar.two169\nTable 1:24 uniformly spaced wavelet packet sub-band Comparison with center frequencies (Hz) [25]\nFilter Wavelet Subband Filter Wavelet Subband Filter Wavelet Subband Filter Wavelet Subband\n1 62.5 7 437.5 13 1250 19 3500\n2 125 8 500 14 1500 20 4000\n3 187.5 9 625 15 1750 21 5000\n4 250 10 750 16 2000 22 6000\n5 312.5 11 875 17 2500 23 7000\n6 375 12 1000 18 3000 24 8000\n2.2 Discriminative techniques\nHMMs are the statistical model of speech production [36], whose parameters are optimized by the BW al-\ngorithm. Most popular ASR systems are based on statistical-based acoustic modeling. In the past few years,\nthediscriminativetechniquegetsmoreattentionasitfurtheroptimizestheHMMparameterstoachievehigh\naccuracy [14, 37]. In conventional GMM-HMM based acoustic modeling, HMM parameters are estimated via\nMLE technique [18]. The MLE technique has the ability to produce an accurate system that is quickly trained\nusing the BW algorithm [24]. The MLE is unbeatable if observations are from the known Gaussian family dis-\ntribution, training data is unlimited, and the priorly known true language model is available. Unfortunately,\nthese assumptions are not true in case of speech. The discriminative techniques try to optimize the model\ncorrectness in such a way to penalize parameters that are responsible to creating confusion between right\nand wrong predictions [48].\nInthiswork,thebaselineacousticmodelingisbasedonHMM,wherethestatesarerepresentedbyGaus-\nsian mixtures, and the discriminative technique is applied over this to optimize the HMM parameters using\ntheE-BWalgorithm[19].Inthispaper,lattice-baseddiscriminativetrainingisappliedbyusingtheHMMIRest\ntool of HTK3.5 toolkit, and these lattices are generated by a weak language model (e.g., bi-gram) to improve\nthe generalization capability of the discriminative technique. To make the discriminative technique more ef-\nficient,phone-markedlatticesareusedbyHMMIRest.Forthediscriminativetechnique,HTKusesmorethan\none expected hypothesis for each speech utterance. In this paper, MMI and MPE discriminative techniques\nare applied to the integrated feature set supported by the HMMIRest tool. Speaker adaptation is also applied\ninacousticmodeling.Speakeradaptationistheprocessofmodifyingtheacousticmodelparametersbyusing\na small amount of speech data of the specific user in such a way so that the resultant model able to recog-\nnize the speech of that speaker. Generally, these techniques are applied to the well trained SI model set to\nmodel the characteristics of a new speaker [47]. In many situations, if a large well-defined SI model is used,\nthe baseline SI performance can be quite high. Hence, the error rate gain from speaker adaptation may be\nsmaller than the simpler model. Speaker adaptation techniques can be grouped into two families: i) Linear\ntransformation-based adaptation and ii) Maximum a posteriori (MAP) adaptation [52]. In this work, we only\nexplored the linear transform based adaptation techniques. These techniques estimate the linear transfor-\nmation from the adaptation data to modify HMM parameters.\n2.2.1 Mutual Information (MMI)\nThe main motive behind the discriminative technique is to assess HMM parameters so as to boost the ac-\ncuracy of the ASR system. The objective function of MMI to maximize the mutual information on the set of\nobservation is defined as:\nfMMI (λ) = 1\nR\nR∑︁\nr=1\nlog\n⎛\n⎝\nP\n(︁\nOr\nHr\nref\n)︁\nP\n(︁\nHr\nref\n)︁\n∑︀\nH P\n(︁\nOr\nH\n)︁\nP(H)\n⎞\n⎠ (8)\n170 /bar.twoA. Kumar and R.K. Aggarwal\nP(Hr\nref) denotes the word sequence probability given by LM andHr\nref is the HMM corresponding to the word\ntranscription.Thedenominatortermsumovereachpossiblewordsequences.Toboosttheobjectivefunction,\nthe numerator term should be high, and the denominator term should be low. The MMI tries to make the\ncorrect hypotheses more probable and incorrect hypotheses less probable at the same time [48].\n2.2.2 Minimum Phone Error (MPE)\nIn MPE, we try to maximize the phone level accuracy rather than maximizing the word accuracy. The MPE\ntraining relies on minimum Baye’s risk training. The objective function is defined as:\nfMPE (λ) =\nR∑︁\nr=1\n∑︁\nH\nP\n(︂ H\nOrλ\n)︂\nL(︀H, Hr\nref\n)︀ (9)\nIn HMMIRest MPE criterion is expressed as:\nfMPE (λ) =\nR∑︁\nr=1\n∑︁\nH\n⎛\n⎝\nP\n(︁\nOr\nMH\n)︁\nP\n(︁\nOr\nMdenr\n)︁\n⎞\n⎠L(︀H, Hr\nref\n)︀ (10)\nMH is a numerator acoustic model, andMdenr is the denominator acoustic model for utterancer. The nota-\ntiondenotes thelossbetween hypothesesandreference. Themaindifferencebetween MMIandMPE islying\nin how the denominator and numerator lattices are computed. The parameter estimation is based on E-BW\nalgorithm. The loss function is measured by Levenshtein-edit distance. This distance is measured between\nthe phone sequences of the reference and the hypotheses. The MPE technique has been found to improve\naccuracy as it supports word transcription with corresponding phone accuracy [18].\n2.3 Recurrent Neural Network (RNN) Language Modeling (LM)\nMostly, state-of-the-art LM used in LVCSR systems are based on RNN [30, 51]. In various work [31–33], the\nusefulness of RNN LM has been reported in LVCSR task. The traditional RNN is the three-layer architecture,\nas shown in figure 2. The first layer, known as the input layer, contains a full history vector by concatenating\nhi−2 and Xi−1 as input to the hidden layer. For empty history, it is initialized to a vector of all ones. RNN LM\nencode full, non-truncated historyh1\ni−1 = [Xi−1, ...,X1] for current wordXi. The current wordXi is predicted\nbyusing1-of-kencodingofthemostrecentprecedingword Xi−1 andhistorycontext hi−2.Informationreceives\nat the hidden layer is further compressed using the sigmoid activation function. It also gives feedback to the\ninput layer. An Out-of-Vocabulary (OOV) node is also added at the input layer to cover those words which\nare not present in the recognition dictionary. In the third layer, the softmax activation function is applied to\nproduce normalized RNN LM probabilities [52]. The output of this layer is also feedback into the input layer\nas remaining history to compute LM probability for the next future word.\nPRNN = (︀X(i+1)/ Xi, hi−1\n)︀ (11)\nThe training and decoding are computationally expensive in RNN LM, and major computation is done at the\noutput layer. To reduce the computational cost at the output layer, one more node Out-of-Shortlisted (OOS)\nis used, which contains the most frequent words. The extension of the back-propagation algorithm, Back-\npropagation through Time (BPTT), is used to train the RNN LM [40]. In BPTT training, the output error is\nbackpropagated in time for a specific number of time steps. This paper uses a 3-8 word length history. In our\nwork, we use one million Hindi text corpus to train the RNN LM. The texts from the various sources were\ncollected to train the language model. These sources were Emili corpus, magazines, newspapers, web text,\nand newsletters. One million text corpus is assumed as the medium-sized dataset for conventional n-gram\nLM, but it is reasonably large for RNN LM. In order to further reduce the computational cost at the output\nDiscriminatively trained continuous Hindi speech recognition/bar.two171\nSoftmax Softmax Softmax…....\nOOS\nSigmoid Sigmoid\nSigmoid\n........\n…....\nOOV\nHistory Vector\nHidden \nlayer\nInput layer\nOutput layer\nXi-1\nXnXi+1\nXi\nXi\nXn-1\nFigure 2:Architecture of Recurrent Neural Network Language Modeling\nlayer,thistoolkitalsosupportscertainothertrainingcriteriasuchasVarianceRegularization(VR)andNoise\nContrastive Estimation (NCE). Both methods use history independent normalization to increase the RNN LM\nevaluation speed.\n3 Proposed Architecture\nThe proposed architecture is divided into two parts. The first part describes the process of feature extraction\nand the integration of different feature sets. In this part of the proposed architecture, feature vectors are\ngeneratedusingGFCC,MFCC,PLP,WERBCfeatureextractiontechniques.Thesecondpartofthearchitecture\ncovers the discriminative training of the feature vector proposed in the first part.\n3.1 Proposed integrated feature set\nTheideaofanacousticfeaturecombinationwasinitiallyproposedbyHarmanskyin1994[25].Inhiswork,he\ncombined PLP features with RASTA features to improve the performance of the targeted ASR system. In the\nprocessofspeechrecognition,featureextractionplaysavitalroleinachievinghighaccuracy.Recently,Many\npapers come into existence to prove their superiority over the previous one by achieving high-performance\ngain[1,15,23,42,53,54].Inthelastfewyears,GFCC[5]andwavelet-basedtechniques[45]havebecomemore\npopularastheyworkwellinanoisyenvironment.Thispropertyattractsmanyresearcherstocombinedthese\nfeatures with other features to improve the accuracy of the ASR system [1, 15, 23, 42, 54].\nInthisproposedwork,thesequentialcombinationofMFCC,GFCC,andWERBCfeaturesisdone,asmen-\ntion in Figure 3. The dimension of the feature vector is further reduced by applying Heteroscedastic Linear\nDiscriminant Analysis (HLDA) [27] as HLDA reduces the feature dimension 25%, which helps to reduce the\ncomputational load of the ASR system. HLDA is the method of projecting high-dimensional acoustic repre-\nsentation into lower-dimensional spaces [46]. The finding of lower-dimensional representation reduces the\n172 /bar.twoA. Kumar and R.K. Aggarwal\nPre Emphasis, Framing, Windowing,FFT Pre Emphasis, Windowing\nMel Scale \nFilter Bank\nGramatone \nFilters\nEqual Loudness\nWavelet \nSubband 1\nEnergy \nCalculation \nIn Baad 1 \nAverage \nenergy  \nF1=E1/N1\nLogarithmic \nNon-linearity LogLog\nDCT\nHLDA\nMFCC+GFCC+WERBC\nSpeech Signal\nWavelet \nSubband 1\nEnergy \nCalculation \nIn Baad 1 \nAverage \nenergy  \nF1=E1/N1\nWavelet \nSubband 1\nEnergy \nCalculation \nIn Baad 1 \nAverage \nenergy  \nF1=E1/N1\nFigure 3:Procedure to obtain proposed integrated feature set (MFCC-GFCC-WERBC)\nnumber of parameters that are used to train the acoustic model and thereby, a significant reduction in com-\nputational load.\n3.2 Discriminative training\nInthesecondpartoftheproposedarchitecture,anintegratedfeaturesetistrainedusingdiscriminativetrain-\ning approaches. The MMI and MPE discriminative techniques are used in the proposed work. The acoustic\nmodeling is done by the HTK3.5 beta toolkit developed by Cambridge University, USA. To apply the discrimi-\nnativetechnique,across-wordtriphonesetofHMM’sareinitiallytrainedusingMLE.Aweaklanguagemodel\n(e.g.,bi-gramLM)isthenextrequirementtoapplythediscriminativetechnique.Thelanguagemodelcreates\nthe lattice used by MMI and MPE in training.\nTwo sets of \"phone-marked\" lattices are required for the discriminative technique known as denomina-\ntor lattice and numerator lattice.HDEcode is used to create the denominator lattice, and numerator lattice\nis created by theHLRescore tool. The numerator lattice includes language model log probabilities, and de-\nnominator lattice implements confusable hypotheses [36, 48]. The initial word lattices are further processed\nto create the phone marked lattice, and these phone marked lattices are used byHMMIRest tool to discrim-\ninatively trained the HMM. The E-BW algorithm does the parameter estimation. In this work, 4 iterations of\neach MMI and MPE is done to train the acoustic model.\nDiscriminatively trained continuous Hindi speech recognition/bar.two173\nSpeech \nsignal\nPre-\nprocessing\nMFCC\nPLP\nGFCC\nMFCC+GFCC\n+WERBC\nWERBC\nGFCC\nGFCC+WER\nBC\nAcoustic Features Vector\nHMM Prototype\nHEREST HMM(i)\nHindi \nMonophone\nPhone level \nTraining \nData\nCo\nnve\nrge\nCI-Hindi ASR using HTK\nPronunciation Model\nLanguage \nModel\nMonophones \nTranscriptions\nTriphone \nTranscriptions\nHLED\nHEREST\nTriphone HMM\nConv\n-erge\nHDECODEHLRESCORE Word \nLattice\nPhone Marked \nLattice\nHMMIREST\nMMIMPE\nDecoder\nCorresponding Text\nDiscriminative training of \nProposed Hindi ASR System\nCD-Hindi ASR using HTK\nFigure 4:Training and testing procedure of an ASR system\n4 Hindi Speech Corpus\nHindi is the fourth most natively spoken language in the world. According to Ethnologue, India have almost\n260 million people who use this language. After Chinese and Spanish, English is in third place, with 335\nmillion speakers. Except for Hindi, all these languages have well developed ASR system with their standard\ndataset. One major challenge for Hindi speech recognition is the deficiency in the Hindi speech dataset and\ntext corpora. In this work, a well-annotated and phonetically rich Hindi dataset is used developed by TIFR,\nMumbai [41]. This dataset contains 100 speakers, and each speaker utters 10 sentences out of which two\nsentencesarecommontoeveryone.ThesetwocommonsentencescoverallphonemesoftheHindilanguage.\nThe next eight sentences also cover the maximum phones of the Hindi language. The recording was done by\ntwomicrophonesinaquietroomon16kHzsamplingfrequency.Fortraining,80speakersoutof100speakers\nare randomly selected, out of which 55 are male speakers, and 25 are female speakers. The remaining 20\nspeakers are used for testing purposes.\nTable 2:Hindi Language Dataset\nDataset No. of Speakers Utterances Total Words Unique Words Hours\nTrain 80 800 5420 2015 2.1\nTest 20 200 1240 856 .20\n174 /bar.twoA. Kumar and R.K. Aggarwal\n5 Simulation details and experiment results\nTheacousticmodelingisdonebythenewversionoftheHTKtoolkit3.5.Forfront-endfeatureextractionand\ncombination, MATLAB R2015a has been used. The training and evaluation of RNN LM have been carried out\nusing the CUED-RNN LM toolkit [8]. The speech database is divided into two parts: training and evaluation.\nFor training, 80 speakers out of 100 speakers randomly selected, and the remaining 20 speakers left for the\ntesting purpose. The training data is further divided into three parts of 30 speakers each. Set 1 contains only\n30 speakers out of 80 who speak Hindi frequently and belong to the northern part of India. Same as Set 2\ncontains 30 Hindi speakers out of the remaining 50 speakers who belong to the south region of India. Set 3\ncontains the remaining 20 speakers and the mixture of speakers from set 1 and set 2. Same as the training\nset, the testing set is also further divided into three parts. Set 1 includes only male speakers of count 12. Set 2\ncontains 8 female speakers and set 3 contain all 20 speakers for evaluation purposes.\n5.1 Performance analysis of multiple feature combination\nIn this experiment, we continue with the work started in [15]. The baseline GMM-HMM system contains 256\nGaussianmixtureperHMMstatewithtri-phonebasedacousticmodeling.Thecomparativeanalysisofvarious\nmultiplefeaturecombinationtechniquesusingthebaselinesystemhasbeenshowninTable3.Thestandard\nfeature set size without integration is 39 in this experiment. In the case of integrated acoustic features, the\ndimensionality of the feature vector is reduced by HLDA. To get the integrated feature set of MF-GFCC, the\nfirst four MFCC features were chosen to integrate with 13 GFCC features. In this way, MF-GFCC makes the\nset of 17 features. After taking the first and second derivatives, the feature vector size will become 51. These\n51 MF-GFCC features are reduced to 39 features after applying the HLDA technique. The same procedure is\napplied to get the MFCC+GFCC+WERBC features, in which the first four features were taken from MFCC and\ncombined with 13 GFCC and WERBC features (i.e., 30) and take the first and second derivative. In the same\nway, all other feature combinations have been taken place. The results clearly show that the combination of\nMFCC+GFCC+WERBCwithHLDAtransformationoutperformsoverallotherfeaturecombinations.Thetrain-\ning set 3 with testing set 3 gives the best results where both male and female speakers come from the north\nand south region of India in comparison to other combinations. The proposed feature combination shows\n9% relative improvement over MFCC based ASR system. The bi-gram LM was used in this experiment.\n5.2 System combination\nFor thedetailed study ofperformance measurement ofthe proposed integratedfeature set withother config-\nurationsettingsattheback-end,severalmodelshaveproposedinthisexperiment.Thebestthreeintegrated\nfeature sets are choosen from the previous experiment to apply discriminative training with speaker adapta-\ntion.SpeakeradaptationcansignificantlyimprovetheWERintheSItrainingset[52].Ithasbeenobservedin\nthe previous work [1] that MLLR will help to achieve low WER as the size of vocabulary increases. In this ex-\nperiment, we proposed a series of system configurations. The best possible system combination of front-end\nprocessing with back-end processing is chosen in this experiment. The proposed baseline system is tested\nwith and without speaker adaptive training (SAT) to measure the performance gain.\nThenamingconventionfortheproposedseriesofthesystemisdonebycapitallettersindicatethetypeof\nfeature combination, speaker adaptation (MLLR,C-MLLR), and discriminative training criteria. For example,\nPG-MMMIindicatesPLP+GFCCfeaturesetwithMLLRadaptationmodeledbyMMIdiscriminativetechnique.\nIn this experiment, we choose 15 systems based on their feature extraction techniques, model type, discrim-\ninative training, no of iteration, and no of the transform for acoustic model adaptation. The acronyms used\nfor the different number of systems helps for further discussion.\nDiscriminatively trained continuous Hindi speech recognition/bar.two175\nTable 3:Comparative analysis of multiple feature combinations\nTraining Set\nTest Set\nFeatures\nHLDA\nAccuracy%\nTraining Set\nTest Set\nFeatures\nHLDA\nAccuracy%\nTraining Set\nTest Set\nFeatures\nHLDA\nAccuracy%\nSet-1\nSet-1\nMFCC No 63.40\nSet-2\nSet-1\nMFCC No 67.20\nSet-3\nSet-1\nMFCC No 62.04\nPLP No 62.20 PLP No 64.02 PLP No 61.86\nGFCC No 71.05 GFCC No 69.36 GFCC No 70.40\nWERBC No 72.56 WERBC No 70.56 WERBC No 71.36\nMFCC+PLP Yes 65.02 MFCC+PLP Yes 68.20 MFCC+PLP Yes 63.40\nMFCC+GFCC Yes 72.36 MFCC+GFCC Yes 71.02 MFCC+GFCC Yes 70.86\nMFCC+WERBC Yes 73.20 MFCC+WERBC Yes 71.36 MFCC+WERBC Yes 7202\nPLP+GFCC Yes 70.10 PLP+GFCC Yes 69.02 PLP+GFCC Yes 6820\nPLP+WERBC Yes 72.86 PLP+WERBC Yes 71.02 PLP+WERBC Yes 71.40\nGFCC+WERBC Yes 74.02 GFCC+WERBC Yes 72.02 GFCC+WERBC Yes 73.20\nMFCC+PLP\n+GFCC\nYes 74.56 MFCC+PLP\n+GFCC\nYes 72.86 MFCC+PLP\n+GFCC\nYes 74.10\nMFCC+GFCC\n+WERBC\nYes 75.20 MFCC+GFCC\n+WERBC\nYes 73.20 MFCC+GFCC\n+WERBC\nYes 75.36\nSet-2\nMFCC No 67.02\nSet-2\nMFCC No 67.9\nSet-2\nMFCC No 62.90\nPLP No 65.36 PLP No 64.86 PLP No 62.40\nGFCC No 70.02 GFCC No 71.40 GFCC No 71.86\nWERBC No 71.86 WERBC No 72.10 WERBC No 72.20\nMFCC+PLP Yes 68.02 MFCC+PLP Yes 70.20 MFCC+PLP Yes 65.02\nMFCC+GFCC Yes 72.40 MFCC+GFCC Yes 72.02 MFCC+GFCC Yes 72.36\nMFCC+WERBC Yes 72.56 MFCC+WERBC Yes 72.96 MFCC+WERBC Yes 72.86\nPLP+GFCC Yes 73.36 PLP+GFCC Yes 72.02 PLP+GFCC Yes 72.02\nPLP+WERBC Yes 72.02 PLP+WERBC Yes 73.40 PLP+WERBC Yes 72.56\nGFCC+WERBC Yes 73.02 GFCC+WERBC Yes 73.56 GFCC+WERBC Yes 73.20\nMFCC+PLP\n+GFCC\nYes 73.86 MFCC+PLP\n+GFCC\nYes 73.96 MFCC+PLP\n+GFCC\nYes 75.02\nMFCC+GFCC\n+WERBC\nYes 74.56 MFCC+GFCC\n+WERBC\nYes 74.20 MFCC+GFCC\n+WERBC\nYes 76.10\nSet-3\nMFCC No 65.02\nSet-3\nMFCC No 70.40\nSet-3\nMFCC No 71.20\nPLP No 64.56 PLP No 68.20 PLP No 69.40\nGFCC No 71.56 GFCC No 72.56 GFCC No 73.56\nWERBC No 72.02 WERBC No 73.15 WERBC No 74.04\nMFCC+PLP Yes 67.56 MFCC+PLP Yes 69.02 MFCC+PLP Yes 70.56\nMFCC+GFCC Yes 72.86 MFCC+GFCC Yes 73.40 MFCC+GFCC Yes 74.02\nMFCC+WERBC Yes 73.20 MFCC+WERBC Yes 73.86 MFCC+WERBC Yes 74.86\nPLP+GFCC Yes 72.02 PLP+GFCC Yes 73.56 PLP+GFCC Yes 73.40\nPLP+WERBC Yes 73.56 PLP+WERBC Yes 73.96 PLP+WERBC Yes 74.36\nGFCC+WERBC Yes 73.86 GFCC+WERBC Yes 74.25 GFCC+WERBC Yes 75.36\nMFCC+PLP\n+GFCC\nYes 74.36 MFCC+PLP\n+GFCC\nYes 75.10 MFCC+PLP\n+GFCC\nYes 76.56\nMFCC+GFCC\n+WERBC\nYes 75.02 MFCC+GFCC\n+WERBC\nYes 76.02 MFCC+GFCC\n+WERBC\nYes 77.86\n176 /bar.twoA. Kumar and R.K. Aggarwal\nTable 4:System Combination with various parameters\nSN System Name Features Model SAT Type Transformation Discriminative\nTraining\nIterations\n1 MPG-M MFCC+PLP+GFCC GMM No MLLR 1 No 1\n2 MPG-M MMI MFCC+PLP+GFCC GMM Yes MLLR 1 MMI 4\n3 MPG-M MPE MFCC+PLP+GFCC GMM Yes MLLR 1 MPE 4\n4 MPG-C MMI MFCC+PLP+GFCC GMM Yes C-MLLR 2 MMI 4\n5 MPG-C MMI MFCC+PLP+GFCC GMM Yes C-MLLR 1 MPE 4\n6 GW-M GFCC+WERBC GMM No MLLR 1 No 1\n7 GW-M MMI GFCC+WERBC GMM Yes MLLR 1 MMI 4\n8 GW-M MPE GFCC+WERBC GMM Yes MLLR 1 MPE 4\n9 GW-C MMI GFCC+WERBC GMM Yes C-MLLR 1 MMI 4\n10 GW-C MPE GFCC+WERBC GMM Yes C-MLLR 1 MPE 4\n11 MGW-M MFCC+GFCC+WERBC GMM No MLLR 1 No 1\n12 MGW-M MMI MFCC+GFCC+WERBC GMM Yes MLLR 1 MMI 4\n13 MGW-M MPE MFCC+GFCC+WERBC GMM Yes MLLR 1 MPE 4\n14 MGW-C MMI MFCC+GFCC+WERBC GMM Yes C-MLLR 1 MMI 4\n15 MGW-C MPE MFCC+GFCC+WERBC GMM Yes C-MLLR 1 MPE 4\n5.3 Performance evaluation of different systems\nThechoiceofthefront-endfeaturecombinationhasatremendousimpactonASRperformance.Fromthepre-\nvious experiment, we choose three best feature combinations and evaluate them with a number of different\nparameters.Inthissection,theperformanceisevaluatedofallthe15proposedsystemdescribedintheprevi-\nousexperiment.Here,again,thetrainingset3,whichcontainsamixtureofnorthandsouthIndiandialects,\ngives maximum accuracy with the test set 3. Discriminative techniques help to optimize the HMM parame-\nters,whichleadstotheperformancegain.Inallexperiments,itwasobservedthatdiscriminativetechniques\nimprovethegeneralizationcapabilityoftheASRsystem.TheMGW-CMPEsystemgivesthebestperformance\nresults of 80.36%, which is ~3% more than the baseline configuration system. The MPE discriminative tech-\nniqueperformsslightlybetterfromMMIdiscriminativetechniqueinallexperiments.Speakeradaptationalso\nhelps to maintain low WER.\nTable 5:Comparative analysis of various system performance\nTraining Set\nTesting Set\nSystem Accuracy %\nMPG-M\nMPG-M MMI\nMPG-M MPE\nMPG-C MMI\nMPG-C MPE\nGW-M\nGW-M MMI\nGW-M MPE\nGW-C MMI\nGW-C MPE\nMGW-M\nMGW-M MMI\nMGW-M MPE\nMGW-C MMI\nMGW-C MPE\nSet-1\nset-1 74.86 75.56 76.02 75.86 76.56 74.20 75.02 76.20 75.56 76.86 75.4 76.86 77.56 77.20 78.02\nset-2 74.02 75.20 76.20 75.96 76.86 73.36 74.20 75.02 74.40 75.56 74.86 75.96 76.86 76.36 77.20\nset-3 74.56 75.86 76.56 76.40 77.02 74.20 75.10 75.96 75.56 76.40 75.20 76.86 77.20 77.02 77.56\nSet-2\nset-1 72.20 73.36 73.96 74.02 74.20 72.15 72.96 73.56 73.40 74.02 73.40 75.02 75.40 75.36 76.40\nset-2 74.36 75.56 76.02 76.20 76.40 73.86 74.56 75.20 74.96 75.56 74.56 76.02 76.56 77.02 77.20\nset-3 75.20 76.40 77.20 77.02 77.56 74.56 75.20 75.96 75.86 76.40 76.36 77.56 78.20 78.10 78.56\nSet-3\nset-1 74.36 75.20 75.86 75.86 76.36 73.40 73.86 74.65 74.40 75.02 75.86 77.20 77.86 78.02 78.86\nset-2 75.56 76.86 77.20 77.36 77.56 73.56 73.96 74.86 74.56 75.96 76.36 78.02 78.56 78.40 79.20\nset-3 76.86 77.56 77.96 77.96 77.02 75.86 76.40 77.36 76.86 77.86 78.02 79.20 79.96 79.56 80.36\n5.4 Experiment with language modeling\nBased on the performance evaluation in the previous section, the best four systems have been selected for\nthisexperiment.TheperformanceoftheproposedHindiASRsystemisfurtherimprovedusingRNNLM.RNN\narchitecture is suitable to deal with variable-length inputs. RNN LM is well suited to model the sequential\nDiscriminatively trained continuous Hindi speech recognition/bar.two177\ndata. By applying RNN LM, the computational load of the system increased but gave the significant perfor-\nmancegain.RNNLMcanlearnthelong-termcontextualinformationwithinthetext.ToimplementRNNLM,\nCUED-RNN LM [8] toolkit has been used. The RNN LM experiment uses 500 hidden units and N-best lattice\nrescoring. The performance is further improved by up to 87.96% using RNN LM. One million text transcrip-\ntion is used from various sources to train the language model. One more observation has been recorded in\nthis experiment that the performance of the ASR system is increased up tri-gram LM only.\nTable 6:Performance comparison of n-gram LM with RNN LM\nSystem\nName\nBi-gram Tri-gram 4-gram RNN LM\nMPG-C MPE 78.02% 80.56% 80.2% 84.96%\nMGW-M MMI 79.2% 81.02% 80.96% 85.02%\nMGW-C MMI 79.56% 81.4% 81.02% 85.96%\nMGW-C MPE 80.36% 82.16% 81.56% 87.96%\n6 Conclusion\nA novel integrated features combination of MF-GFCC with WERBC features are discriminatively trained with\nRNNlanguagemodelingtoimprovetheperformanceoftheHindiASRsystem.Forspeakeradaptation,MLLR\nand C-MLLR techniques have been applied, and their corresponding improvements have been recorded. The\nperformanceoftheproposedHindiASRhasbeenevaluatedonadifferentnumberofparameters.Theresults\nconcludethatMFCC+GFCC+WERBCfeaturesaremorerobustandgivemaximumaccuracywithMPEdiscrim-\ninative training. The MPE technique show 1% relative improvement over the MMI technique. This work can\nfurther be extended by applying these feature combinations to DNN based acoustic modeling, and various\noptimization techniques with the proposed feature set can also be tested.\nReferences\n[1] R. K. Aggarwal and M. Dave, Performance evaluation of sequentially combined heterogeneous feature streams for Hindi\nspeech recognition system,Telecommun. Syst., 52(2013), 1-10.\n[2] M.A.AnusuyaandS.K.Katti,Frontendanalysisofspeechrecognition:areview, International Journal of Speech Technology,\n14.2 (2011): 99-145.\n[3] A. Biswas et al., Feature extraction technique using ERB like wavelet sub-band periodic and aperiodic decomposition for\nTIMIT phoneme recognition,International Journal of Speech Technology, 17.4(2014): 389-399.\n[4] A. Biswas et al., Hindi phoneme classification using Wiener filtered wavelet packet decomposed periodic and aperiodic\nacoustic feature,Computers & Electrical Engineering, 42 (2015): 12-22.\n[5] W. Burgos,Gammatone and MFCC Features in Speaker Recognition, Dissertation, 2014.\n[6] X. Chen et al., Improving the training and evaluation eflciency of recurrent neural network language models, in:2015 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2015.\n[7] X.Chenetal.,Eflcienttrainingandevaluationofrecurrentneuralnetworklanguagemodelsforautomaticspeechrecognition,\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 24.11(2016): 2146-2157.\n[8] X.Chenetal.,CUED–RNNLM–Anopen-sourcetoolkitforeflcienttrainingandevaluationofrecurrentneuralnetworklanguage\nmodels, in:2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2016\n[9] A.D.Cheveignéetal.,Concurrentvowelidentification.II.Effectsofphase,harmonicity,andtask, The Journal of the Acoustical\nSociety of America, 101.5 (1997): 2848-2856.\n[10] H. P. Combrinck and E. C. Botha, On the Mel-scaled cepstrum, in:Department of Electrical and Electronic Engineering, Uni-\nversity of Pretoria, Pretoria, South Africa, 1996.\n178 /bar.twoA. Kumar and R.K. Aggarwal\n[11] A.Curreyetal.,Dynamicadjustmentoflanguagemodelsforautomaticspeechrecognitionusingwordsimilarity,in: Spoken\nLanguage Technology Workshop (SL T), IEEE, 2016.\n[12] S. B. Davis, and P. Mermelstein, Comparison of parametric representations for monosyllabic word recognition in continu-\nously spoken sentences,Readings in speech recognition, (1990): 65-74.\n[13] L. Deng et al., Distributed speech processing in MiPadś multimodal user interface,IEEE Transactions on Speech and Audio\nProcessing, 10.8(2002): 605-619.\n[14] M.Dua,R.K.Aggarwal,andM.Biswas,DiscriminativelytrainedcontinuousHindispeechrecognitionsystemusinginterpo-\nlated recurrent neural network language modeling, In:Neural Computing and Applications, (2018): 1-9.\n[15] M. Dua, R. K. Aggarwal, and M. Biswas, Discriminative training using noise robust integrated features and refined HMM\nmodeling, Journal of Intelligent Systems, (2018).\n[16] O. Farooq O, S. Datta, M.C. Shrotriya, Wavelet sub-band based temporal features for robust Hindi phoneme recognition,\nInternational Journal of Wavelets, Multiresolution and Information Processing, 8.6 (2010):847-59.\n[17] M. Ferras et al., Comparison of speaker adaptation methods as feature extraction for SVM-based speaker recognition,IEEE\nTransactions on Audio, Speech, and Language Processing, 18.6 (2010): 1366-1378.\n[18] D. Gillick, S. Wegmann, and L. Gillick, Discriminative training for speech recognition is compensating for statistical depen-\ndence in the HMM framework, in:2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\nIEEE, 2012.\n[19] G. Heigold, N. Hermann, S. Ralph, and W. Simon, Discriminative training for automatic speech recognition: Modeling, crite-\nria, optimization, implementation, and performance,In:IEEE Signal Processing Magazine, 29.6(2012): 58-69.\n[20] H. Hermansky, Perceptual linear predictive (PLP) analysis of speech,the Journal of the Acoustical Society of America, 87.4\n(1990): 1738-1752.\n[21] K. Ishizuka, and T. Nakatani, A feature extraction method using subband based periodicity and aperiodicity decomposition\nwith noise robust frontend processing for automatic speech recognition,Speech communication,48.11(2006): 1447-1457.\n[22] R. Jozefowicz et al.,Exploring the limits of language modeling,preprint(2016), http://arxiv.org/abs/1602.02410.\n[23] V. Kadyan, A. Mantri and R. K. Aggarwal, A heterogeneous speech feature vectors generation approach with hybrid hmm\nclassifiers,Int. J. Speech Technology, 20.4 (2017): 761-769.\n[24] S. Kapadia,Discriminative training of hidden Markov models, Doctoral dissertation, University of Cambridge, 1998.\n[25] J.Koehler,N.Morgan,H.Hermansky,H.G.HirschandG.Tong,IntegratingRASTA-PLPintoSpeechRecognition,in: 1994 IEEE\nInternational Conference on Acoustics, Speech, and Signal Processing,1 Adelaide, SA, Australia, 1994.\n[26] R. Kumar, A. Kumar, and R. K. Pandey, Beta wavelet based ECG signal compression using lossless encoding with modified\nthresholding,Computers & Electrical Engineering, 39.1(2013): 130-140.\n[27] N. Kumar and A. G. Andreou, Heteroscedastic discriminant analysis and reduced rank HMMs for improved speech recogni-\ntion, Speech Commun, 26.4 (1998), 283-297.\n[28] A.G.Kunkle, Sequence scoring experiments using the TIMIT corpus and the HTK recognition framework,Dissertation,Florida\nInstitute of Technology, Florida, USA, 2010.\n[29] J. Li et al., An overview of noise-robust automatic speech recognition,IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 22.4 (2014): 745-777.\n[30] K. Li et al., Recurrent neural network language model adaptation for conversational speech recognition, In:INTERSPEECH,\nHyderabad (2018): 1-5.\n[31] T.Mikolovetal.,Recurrentneuralnetworkbasedlanguagemodel,In: Eleventh annual conference of the international speech\ncommunication association,(2010).\n[32] T.Mikolovetal.,Extensionsofrecurrentneuralnetworklanguagemodel,In: 2011 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP),IEEE, (2011):5528-5531.\n[33] T. Mikolov et al., Context dependent recurrent neural network language model, In:2012 IEEE Spoken Language Technology\nWorkshop (SL T),IEEE, (2012):234-239.\n[34] A.Mohanetal.,AcousticmodellingforspeechrecognitioninIndianlanguagesinanagriculturalcommoditiestaskdomain,\nSpeech Communication, 56 (2014): 167-180.\n[35] J. M. Naik, L. P. Netsch, and G. R. Doddington, Speaker verification over long distance telephone lines, in:International\nConference on Acoustics, Speech, and Signal Processing ICASSP-89, IEEE, 1989.\n[36] D. Povey,Discriminative training for large vocabulary speech recognition, PhD Diss. University of Cambridge, 2005.\n[37] D. Povey et al.,Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI, In:Interspeech, (2016): 2751-\n2755.\n[38] S. Ranjan. A discrete wavelet transform based approach to Hindi speech recognition, In:2010 International Conference on\nSignal Acquisition and Processing, Bangalore(2010):345-348.\n[39] S.Ranjan,ExploringthediscretewavelettransformasatoolforHindispeechrecognition, International Journal of Computer\nTheory and Engineering, 2.4 (2010): 642.\n[40] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors,nature, 323.6088\n(1986): 533.\n[41] K. Samudravijaya, P. V. S. Rao and S. S. Agrawal, Hindi speech database, in:International Conference on spoken Language\nProcessing, Beijing, China, 2002, pp. 456–464.\nDiscriminatively trained continuous Hindi speech recognition/bar.two179\n[42] R. Schluter et al., Gammatone features and feature combination for large vocabulary speech recognition, in:IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP 2007), 4 IEEE, 2007.\n[43] Y.Shao,andW.DeLiang,Robustspeakeridentificationusingauditoryfeaturesandcomputationalauditorysceneanalysis,\nIn: 2008 IEEE International Conference on Acoustics, Speech and Signal Processing,IEEE, (2008):1589-1592.\n[44] Y.Shaoetal.,Acomputationalauditorysceneanalysissystemforspeechsegregationandrobustspeechrecognition, Com-\nputer Speech & Language, 24.1(2010): 77-93.\n[45] A. Sharma et al., Hybrid wavelet based LPC features for Hindi speech recognition,International Journal of Information and\nCommunication Technology, 1.3-4 (2008): 373-381.\n[46] N. Singh-Miller, Natasha, M. Collins, and T. J. Hazen, Dimensionality reduction for speech recognition using neighborhood\ncomponents analysis, in:Eighth Annual Conference of the International Speech Communication Association, 2007.\n[47] A. Stolcke et al., MLLR transforms as features in speaker recognition, in:Ninth European Conference on Speech Communi-\ncation and Technology, 2005.\n[48] K.Vertanen,Anoverviewofdiscriminativetrainingforspeechrecognition, University of Cambridge,Cambridge,UK(2004).\n[49] E. Wong, and S. Sridharan, Comparison of linear prediction cepstrum coeflcients and mel-frequency cepstrum coeflcients\nfor language identification, in:Proceedings of 2001 International Symposium on Intelligent Multimedia, Video and Speech\nProcessing, IEEE, 2001.\n[50] Z. Wu et al., A study of speaker adaptation for DNN-based speech synthesis, in:Sixteenth Annual Conference of the Interna-\ntional Speech Communication Association, 2015.\n[51] D.Yogatamaetal.,Memoryarchitecturesinrecurrentneuralnetworklanguagemodels,in: Seventh International Conference\non Learning Representations, (2018).\n[52] S. Young, G. Evermann, M. Gales,T. Hain, D. Kershaw,X. Liu,V. Valtchev,The HTK book, Cambridge University Engineering\nDepartment, vol 3, pp 1–285,2002.\n[53] X. Zhao and D. L. Wang, Analyzing noise robustness of MFCC and GFCC features in speaker identification, in:2013 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2013\n[54] A. Zolnay, R. Schluter, and H. Ney., Acoustic feature combination for robust speech recognition, in:IEEE International Con-\nference on Acoustics, Speech, and Signal Processing (ICASSP’05),1 IEEE, 2005.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8382692337036133
    },
    {
      "name": "Discriminative model",
      "score": 0.8018233776092529
    },
    {
      "name": "Hidden Markov model",
      "score": 0.7992618083953857
    },
    {
      "name": "Speech recognition",
      "score": 0.6933502554893494
    },
    {
      "name": "Hindi",
      "score": 0.6234722137451172
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5950904488563538
    },
    {
      "name": "Artificial intelligence",
      "score": 0.584934651851654
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48493102192878723
    },
    {
      "name": "Language model",
      "score": 0.46649882197380066
    },
    {
      "name": "Artificial neural network",
      "score": 0.4444687068462372
    },
    {
      "name": "Gaussian",
      "score": 0.4305487871170044
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155125381",
      "name": "Galgotias University",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I105094715",
      "name": "National Institute of Technology Kurukshetra",
      "country": "IN"
    }
  ]
}