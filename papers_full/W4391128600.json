{
  "title": "State-of-Health Prediction of Lithium-Ion Batteries Using Exponential Smoothing Transformer With Seasonal and Growth Embedding",
  "url": "https://openalex.org/W4391128600",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5112430347",
      "name": "Muhammad Rifqi Fauzi",
      "affiliations": [
        "University of Brawijaya"
      ]
    },
    {
      "id": "https://openalex.org/A5001428236",
      "name": "Novanto Yudistira",
      "affiliations": [
        "University of Brawijaya"
      ]
    },
    {
      "id": "https://openalex.org/A5057755959",
      "name": "Wayan Firdaus Mahmudy",
      "affiliations": [
        "University of Brawijaya"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972664419",
    "https://openalex.org/W2559148811",
    "https://openalex.org/W4327954986",
    "https://openalex.org/W4388562864",
    "https://openalex.org/W3120041482",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2078279667",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3197714756",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W6764679822",
    "https://openalex.org/W4200232999",
    "https://openalex.org/W3173497357",
    "https://openalex.org/W6640442106",
    "https://openalex.org/W6763309814",
    "https://openalex.org/W2127342270",
    "https://openalex.org/W4303614072",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W27994497",
    "https://openalex.org/W2973897293",
    "https://openalex.org/W6810853030",
    "https://openalex.org/W6797155008",
    "https://openalex.org/W6784835917",
    "https://openalex.org/W2144352195",
    "https://openalex.org/W4313420094",
    "https://openalex.org/W6777009791",
    "https://openalex.org/W3162371057",
    "https://openalex.org/W4205303226",
    "https://openalex.org/W4298003829",
    "https://openalex.org/W4281674107",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2996552856",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3138250876",
    "https://openalex.org/W4221148002",
    "https://openalex.org/W4404040077",
    "https://openalex.org/W2963340555"
  ],
  "abstract": "In the world of modern energy, Lithium-Ion batteries reign supreme, offering rechargeability, sustainability, and long-term energy storage. However, their lifespan is not infinite, calling for accurate prediction of remaining life under various conditions. Deep learning shines in this domain, with the Transformer architecture blossoming as a powerful tool for time series forecasting. This research dives into data collection, processing, model design, training, and evaluation, making key methodological contributions to battery life prediction. Notably, the SGEformer model, a Transformer enhanced with growth and seasonal embedding, emerges as a groundbreaking innovation. Comparing SGEformer to ETSformer, Informer, Reformer, Transformer, and LSTM reveals its unique strengths. With an impressive MSE score of 0.000117, SGEformer establishes itself as a highly effective tool for battery life prediction, highlighting the value of growth and seasonal embedding in boosting accuracy. This research propels the state-of-the-art Lithium-Ion battery state-of-health prediction, offering a robust methodological foundation for precise and reliable forecasts. Code can be accessed at <uri>https://github.com/MRifqiFz/SGEformer</uri>.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nState-of-Health Prediction of Lithium-Ion\nBatteries using Exponential Smoothing\nTransformer with Seasonal and Growth\nEmbedding\nMUHAMMAD RIFQI FAUZI1, NOVANTO YUDISTIRA1, and WAYAN FIRDAUS MAHMUDY1\n1Departemen Teknik Informatika, Fakultas Ilmu Komputer, Universitas Brawijaya, Malang, Indonesia\nCorresponding author: Novanto Yudistira (e-mail: yudistira@ub.ac.id).\nABSTRACT In the world of modern energy, Lithium-Ion batteries reign supreme, offering rechargeability,\nsustainability, and long-term energy storage. However, their lifespan is not infinite, calling for accurate\nprediction of remaining life under various conditions. Deep learning shines in this domain, with the\nTransformer architecture blossoming as a powerful tool for time series forecasting. This research dives into\ndata collection, processing, model design, training, and evaluation, making key methodological contributions\nto battery life prediction. Notably, the SGEformer model, a Transformer enhanced with growth and seasonal\nembedding, emerges as a groundbreaking innovation. Comparing SGEformer to ETSformer, Informer,\nReformer, Transformer, and LSTM reveals its unique strengths. With an impressive MSE score of 0.000117,\nSGEformer establishes itself as a highly effective tool for battery life prediction, highlighting the value of\ngrowth and seasonal embedding in boosting accuracy. This research propels the state-of-the-art Lithium-\nIon battery state-of-health prediction, offering a robust methodological foundation for precise and reliable\nforecasts.\nINDEX TERMS Time Series Forecasting; Lithium-Ion Battery; Seasonal embedding; Growth embedding;\nTransformer; State-of-Health (SoH)\nI. INTRODUCTION\nIn the modern world, we have a multitude of energy sources\nthat cater to our daily needs. Among these, one of the most\nwidely utilized sources is the battery, a device designed to\nstore and convert chemical energy into electrical power. The\nterm \"battery\" was originally coined by Benjamin Franklin in\n1749, and the first official battery was crafted by Alessandro\nV olta in 1800. However, in 1938, a remarkable discovery in\nBaghdad hinted at the existence of ancient batteries, poten-\ntially originating in Mesopotamia around 250 BC [1].\nAmong the array of battery types, Lithium-Ion batteries\nare a prevalent choice. Their inception dates back to M.S.\nWhittingham’s research in 1970, followed by development\nat Bell Laboratories in 1981 [3]. Lithium-ion batteries, with\ntheir lightweight design, high energy density, and long cycle\nlife, are pivotal in modern technology. They serve as the back-\nbone for efficient portable energy storage in devices ranging\nfrom smartphones to electric vehicles. Additionally, these\nbatteries support renewable energy development by reliably\nstoring power from sources like the sun and wind, while\nalso driving the evolution of eco-friendly electric vehicles\nand reducing reliance on fossil fuels. Their widespread use\nrepresents a significant milestone in advancing a more sus-\ntainable lifestyle in today’s society. Therefore, maintaining\nthe quality of Lithium-Ion batteries is essential, despite their\neventual capacity decline. Maintenance practices include pre-\nserving battery charging quality, regular health assessments,\nand monitoring of batteries as they approach their projected\nlifespan [19].\nTo forecast battery lifespan, predictive models based on\nhistorical data are imperative. This prediction process can\nbe accomplished through statistical methods and advanced\ndeep-learning techniques. Statistics provides a simple and\naccessible approach to forecasting, making it user-friendly\nfor individuals without a technical background. However,\nstatistical methods have limitations, including challenges in\nhandling large or complex datasets, vulnerability to change,\nand difficulties with non-stationary data. Common statisti-\ncal methods for forecasting include Exponential Smoothing,\nMoving Average, ARIMA, and Seasonal Decomposition of\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTime Series (STL).\nDeep learning, on the other hand, has witnessed substantial\ngrowth in recent years and can uncover hidden insights within\ndata, particularly in time series data, a critical aspect of bat-\ntery life estimation. Battery life estimation is a prime example\nof a time series forecasting challenge. Various approaches\nto solving these challenges exist, including Recurrent Neu-\nral Networks (RNN), Fuzzy Neural Systems, and Extreme\nLearning Machines (ELM) [6, 13, 14]. Among the recent ad-\nvancements, the Exponential Smoothing Transformer (ETS-\nformer) model has emerged [25].\nThe Exponential Smoothing Transformer, or ETSformer,\nis a model rooted in the Transformer time-series architecture.\nIt leverages the Exponential Smoothing principle to enhance\ntime-series data forecasting with Transformers. ETSformer\nemploys two key mechanisms: Exponential Smoothing At-\ntention (ESA) and Frequency Attention (FA), resulting in\na more effective time-series forecasting model compared to\nstandard transformers. This method has demonstrated strong\nperformance across six real-world time-series datasets [25].\nIn summary, this paper offers a comprehensive overview of\nbattery development, focusing on the evolution of Lithium-\nIon batteries. It also delves into methods for estimating bat-\ntery life, encompassing both statistical and deep learning\ntechniques. Furthermore, it introduces the growth and sea-\nsonal embedding within the Transformer (SGEformer) as a\npromising approach for improving time-series forecasting in\nthe context of battery life estimation.\nDespite these advancements, the study identifies a re-\nsearch gap in the existing literature. While several studies\nexplore battery health monitoring and predictive modeling,\nthe integration of growth and seasonal embedding within the\nTransformer architecture, as exemplified by the SGEformer\nmodel, represents a novel contribution. The study aims to fill\nthis gap by conducting extensive experiments on NASA Li-\nIon Battery Aging and CALCE Battery Research datasets,\nproviding robust findings on the effectiveness of the proposed\nSGEformer model. This innovative approach, coupled with\nthe comprehensive overview of battery development and es-\ntimation methods, enhances the understanding and advance-\nment of battery life prediction models.\nSGEformer is an enhanced version derived from the Ex-\nponential Smoothing Transformer (ETSformer) model. The\ndifference between ETSformer and SGEformer lies in the\naddition of embedding layers. In the ETSformer model, the\nembedding layer is only at the beginning of the model be-\nfore entering the encoder. In contrast, the SGEformer model\nincludes an additional embedding layer between the encoder\nand decoder. The input to this embedding layer consists of\ngrowth and seasonality from the encoder, which will later\nbe fed into the decoder. From the explanation above, several\nimportant contributions are presented below:\n• Our research provides a thorough exploration of meth-\nods for estimating battery life, encompassing both sta-\ntistical and deep learning approaches.\n• Innovative Growth and Seasonal Embedding in SGE-\nformer: We introduce a novel enhancement to the Trans-\nformer architecture with the SGEformer model. Notably,\nthe addition of a unique embedding layer between the\nencoder and decoder, capturing growth and seasonality,\nestablishes the SGEformer as a promising tool for time-\nseries forecasting in battery life estimation.\n• We conducted extensive experiments on NASA Li-Ion\nBattery Aging and CALCE Battery Research datasets,\nemploying Transformer-based models, including our\nproposed SGEformer. This comprehensive experimen-\ntation adds robustness to our findings and demonstrates\nthe effectiveness of our proposed approach.\nAging and CALCE Battery Research dataset using\nTransformer-based models.\nII. RELATED WORKS\nIn a study exploring prognostic methods for battery health\nmonitoring within a Bayesian framework [18], an approach is\noutlined for utilizing Bayesian methods in monitoring battery\nhealth. The researchers emphasize that employing Bayesian\ntechniques enables the integration of existing information\nwith their understanding of battery behavior, enhancing the\nprecision of battery life predictions.\nOver time, extensive research has been conducted on the\nprediction of the state of health (SoH). A recent paper in-\ntroduced a combined model utilizing CNN, Bidirectional\nLSTM, and Attention Mechanism techniques for predicting\nthe life of Lithium-Ion batteries [20]. The study demonstrated\nthat this integrated CNN-BiLSTM-AM model outperforms\nother models, yielding superior results with a lower RMSE\nvalue of 0.0120 and a decreased MAPE value of 0.9%, indi-\ncating enhanced accuracy.\nA novel approach utilizing a GNN architecture has shown\npromising results in predicting Lithium-Ion battery lifespan\n[29]. This method, dubbed CL-GraphSAGE, outperformed\nestablished models like RNN, LSTM, and CNN-LSTM by\nachieving significantly lower error rates and higher accuracy,\noffering a potentially valuable tool for battery management\nand optimization.\nA novel method for rapidly and accurately estimating the\nhealth of in-service battery packs is proposed [5]. The method\nuses short-term charging data and limited labels to train a\ndeep convolutional neural network (CNN). The method is\nevaluated on field data from 10 electric vehicles with different\nSoHs and achieves an accuracy of 3.2%.\nTwo data-driven approaches tackle the challenge of accu-\nrate battery health assessment in electric vehicles. The first,\nfrom [4] leverages on-road charging data and a statistical\nanalysis of capacity changes during charging to predict re-\nmaining capacity. It extracts informative features and trains\na machine-learning model for robust prognostics even with\nlimited battery management system resources. Meanwhile,\n[33] proposes a novel technique using Independent Compo-\nnent Analysis and an improved Back Propagation Learning\nnetwork. This method utilizes constant current charging volt-\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nage data to extract health indicators for both in-service and re-\ntired batteries, leading to accurate state-of-health estimation.\nBoth methods contribute valuable tools for ensuring optimal\nbattery performance and lifespan in electric vehicles.\nBuilding upon earlier research, a groundbreaking hybrid\nmodel has emerged for predicting lithium-ion battery capac-\nity and remaining useful life. This innovation combines the\nBroad Learning System (BLS) for efficient feature extraction\nfrom past capacity data with the sequential learning prowess\nof a LSTM (Long Short-Term Memory) neural network [34].\nLSTM and its explain-ability have been proved to perform\nwell for various multivariate cases [30][31]. This synergy\nunlocks incredible predictive accuracy while requiring only\na quarter of the typical training data, making it remarkably\nresource-friendly. This opens exciting possibilities for reli-\nable battery health monitoring and optimal power manage-\nment, revolutionizing a wide range of applications.\nIntroduce a new hybrid method for estimating future ca-\npacity and Remaining Useful Life (RUL) of battery packs.\nIt skillfully combines Variational Modal Decomposition\n(VMD) to separate capacity data into aging and noise trends,\nwith Particle Filters (PF) and Gaussian Process Regression\n(GPR) to predict each component [32]. This integrated ap-\nproach addresses complex aging mechanisms and dynamic\noperating conditions, resulting in accurate prognoses even\nwith limited data. This paves the way for optimal battery\nmanagement and early warning systems, ensuring efficient\nand safe operation.\nExponential Smoothing Transformer (ETSformer) model\ncombines the traditional Exponential Smoothing method with\nthe Transformer method [25]. It discusses the development\nof the Transformer method for forecasting time-series data.\nETSformer utilizes the Exponential Smoothing Attention and\nFrequency Attention methods, which are more effective in\ndealing with time-series problems. The evaluation results of\nthe ETSformer model show that this model achieves state-\nof-the-art performance by beating competing baselines in 35\nout of 40 and 17 out of 23 for multivariate and univariate\nforecasting.\nIII. METHODOLOGY\nA. TRANSFORMER FOR TIME SERIES PREDICTIONS\nThe application of the Transformer architecture to time series\nforecasting has garnered attention as an effective approach.\nThis adaptation of the Transformer, originally designed for\nnatural language processing tasks, has proven its versatility\nin handling sequential data [22]. In this context, time series\ndata is treated as a sequence of data points, each including\nattributes such as timestamps and observed values. To facil-\nitate the model’s understanding of temporal dynamics, re-\nsearchers employ techniques such as positional encodings and\nembeddings. The pivotal innovation lies in the Transformer’s\nself-attention mechanism, which allows the model to assign\nvarying degrees of relevance to different time steps within\nthe sequence [22]. This mechanism proves instrumental in\ncapturing intricate temporal dependencies, regardless of their\ntemporal proximity, enabling the identification of complex\npatterns, trends, and recurring seasonal variations within the\ntime series. Furthermore, the multi-head attention mechanism\nextends this capability, permitting the model to simultane-\nously focus on multiple aspects or patterns within the tempo-\nral data. This approach, leveraging the Transformer architec-\nture for time series forecasting, represents a promising avenue\nfor capturing the nuanced and dynamic characteristics of time\nseries data.\nB. SEASONAL AND GROWTH EMBEDDING\nTRANSFORMER\nSGEformer is based on a model of a time-series Transformer\narchitecture that utilizes the Exponential Smoothing principle\n[25] to improve forecasting of time-series data. The architec-\nture of the proposed SGEformer model can be seen in Figure\n1 below. Figure 1 illustrates the entire encoder-decoder in the\nSGEformer model. At each layer, the encoder is designed to\nextract the growth and the random pattern iteratively from\nthe lookback window. The levels are then extracted by the\nclassical method of level smoothing. The extracted compo-\nnents are then fed into the decoder to produce the last H-step\nforward estimates through the composition of level, growth,\nand seasonal forecasts using Equation 1.\nˆXt:t+H = Et:t+H + Linear\n NX\nn=1\n\u0010\nB(n)\nt:t+H + S(n)\nt:t+H\n\u0011!\n(1)\nWhere Et:t+H ∈ RH×m, and B(n)\nt:t+H , S(n)\nt:t+H ∈ RH×d\nrepresents the estimated level, growth, and seasonal latency\nof each step of the forecast horizon. The superscript denotes\nthe stack index. For the encoder, the stacks are N in number.\n1) Encoder\nThe encoder’s main objective is to capture incremental growth\nand seasonal patterns from the historical data window by ex-\ntracting latent representations. Unlike traditional approaches\nthat assume additive or multiplicative seasonality, this model\nutilizes the concept of learning residue to handle complex\npatterns beyond those assumptions, inspired by [16] and [7].\nEach layer in the encoder sequentially analyzes the input\nsignal, separating the extracted growth and seasonality signals\nfrom the residue. These signals undergo nonlinear transfor-\nmations before passing to the next layer. At each encoder\nlayer (n), the input is the residual from the previous layer,\ndenoted as Z(n−1)\nt−L:t , and the outputs are Z(n)\nt−L:t (residue), B(n)\nt−L:t\n(latent growth), and S(n)\nt−L:t (latent seasonal) for the lookback\nwindows. This is accomplished using the Multi-Head Ex-\nponential Smoothing Attention (MH-ESA) and Frequency\nAttention (FA) modules.\n2) Seasonal and Growth Embedding Layer\nThe raw signal in the Lookback Window is mapped to the\nlatent space using the embedding module, as defined in\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 1: SGEformer Model Architecture\nZ(0)\nt−L:t = E(0)\nt−L:t = Conv(Xt−L:t ). Here, Conv represents a\ntemporary convolution filter with a kernel size of 3, input\nchannels (m), and output channels (d). In the design of ETS-\nformer, inspiration is taken from previous works such as [12],\n[26], [27], and [35].\nApart from embedding when inputting data, this embed-\nding layer is also used between the Encoder and Decoder\nlayers. After getting Growth and Seasonality on the Encoder\nlayer, the results will be entered into the embedding layer\nbefore entering the decoder layer. The function of using this\nembedding layer is to reduce the dimensionality of the data\nso that the model becomes more efficient.\n3) Decoder\nThe decoder has the task of generating an estimate of the H-\nstep ahead. The final estimate is a composition of the Et:t+H\nlevel forecast, the B(n)\nt:t+H growth representation and the sea-\nsonal S(n)\nt:t+H in the forecast horizon. This decoder consists of\nN Growth + Seasonal (G+S) Stacks and level stacks. The G+S\nstack consists of Growth Damping(GD) and FA blocks, which\nutilize B(n)\nt , S(n)\nt−L:t to predict B(n)\nt:t+H , S(n)\nt:t+H using Equation 2.\nGrowth: B(n)\nt:t+H = TD(B(n)\nt )\nSeasonal: S(n)\nt:t+H = FAt:H (S(n)\nt−L:t )\n(2)\nTo get the level on the forecast horizon, we will repeat the\nlevel at the last time step t on the Level Stack along the fore-\ncast horizon using the equation Et:t+H = RepeatH\n\u0010\nE(N)\nt\n\u0011\n=h\nE(N)\nt , . . . ,E(N)\nt\ni\n, with RepeatH (·) : R1×m → RH×m\nTo get a representation of growth in the forecast hori-\nzon, trend damping is used in the equation ˆxt+h|t = et +\n\u0000\n∅ + ∅2 + . . .+ ∅h\u0001\nbt +st+h−p, to generate robust multi-step\nforecasts.\n4) Exponential Smoothing Attention\nIn self-attention, weights are based on similarity between in-\nput tokens [21]. In Exponential Smoothing Attention (ESA),\nweighting depends on time lag rather than content. ESA is\ndefined as AES : RL×d → RL×d , where AES (V )t ∈ Rd\nrepresents the token at time step t in the output matrix. ESA\nis calculated using Equation 3:\nAES(V)t =αVt + (1−α)AES(V)t−1 =Pt−1\nj=0α(1−α)jVt−j + (1−α)tv0\n(3)\nHere, 0 < α < 1 and v0 are parameters for smoothing\nand initial state. Direct implementation of ESA constructs\nthe attention matrix AES and performs matrix multiplication,\nresulting in O(L2) complexity (Equation 4):\nAES (V ) =\n\n\nAES (V )1\n···\nAES (V )L\n\n = AES ·\n\u0014vT\n0\nV\n\u0015\n(4)\nHowever, by exploiting the structure of the ESA matrix, an\nefficient algorithm can be developed. Each row of the atten-\ntion matrix can be shifted iteratively using padding, enabling\nmatrix-vector multiplication through cross-correlation, lead-\ning to a fast implementation using Fourier transform [15].\nESA is the basic block and is extended to Multi-Head Ex-\nponential Smoothing Attention (MH-ESA) for latent growth\nextraction. Growth is obtained by differencing the residuals.\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nThis can be done using Equation 5:\neZ(n)\nt−L:t = Linear\n\u0010\nZ(n−1)\nt−L:t\n\u0011\n,\nB(n)\nt−L:t = MH-AES (Zt − L : t(n) − [Zt − L : t − 1n, v0(n)]),\nB(n)\nt−L:t = Linear\n\u0010\nB(n)\nt−L:t\n\u0011\n,\n(5)\nHere, MH- AES is the multi-head version of AES , and v(n)\n0\nrepresents the initial state of ESA.\n5) Frequency Attention\nExtracting seasonal patterns from the lookback window has\ntwo purposes, to deseasonalize the input signal, enabling\ndownstream components to focus on modeling level and\ngrowth information, and to extrapolate seasonal patterns for\nconstructing a forecast horizon representation. Automatically\nidentifying these seasonal patterns is the main challenge, but\nprevious research, exemplified by [23], has explored the use\nof power spectral density estimation for detecting periodicity.\nBuilding on this, the Frequency Attention (FA) mechanism is\ndeveloped, utilizing the discrete Fourier transform (DFT) to\nextract dominant seasonal patterns.\nIV. EVALUATION METRICS (ERRORS)\nIn this investigation, the assessment of each model’s efficacy\nis conducted through the utilization of two prominent metrics:\nMean Squared Error (MSE) and Root Mean Squared Error\n(RMSE). These metrics are chosen due to their differentiable\nnature, rendering them conducive to mathematical computa-\ntions, in contrast to non-differentiable functions such as Mean\nAbsolute Error (MAE). The Mean Squared Error (MSE) is\ncomputed as the average square of the discrepancies between\nthe predicted ( y\n′\ni ) and actual ( yi) values, as expressed in\nEquation (6). Subsequently, the Root Mean Squared Error\n(RMSE), denoted by Equation (7), is calculated as the square\nroot of the MSE..\nMSE = 1/N ∗ ΣN\ni=1(yi − y\n′\ni ) (6)\nRMSE =\nq\n1/N ∗ ΣN\ni=1(yi − y\n′\ni ) (7)\nThe Mean Absolute Error (MAE) will be utilized to com-\npute the error value of the prediction outcomes, MAE is a\nperformance metric commonly employed to assess the effec-\ntiveness of a model. In Eq. 8, MAE is the average absolute\nerror, expressed as a percentage. The variable et denotes the\nerror value of the prediction, yt represents the actual value,\nand n signifies the quantity of series. The error value is de-\ntermined by computing the difference between the expected\nvalues and the observed values.\nMAPE = 1\nn\nnX\nt=1\n\f\f\f\f\net\nyt\n\f\f\f\f (8)\nV. EXPERIMENTAL SETUP\nA. EXPERIMENTAL DATA\n1) Data Description\nThe data used in this study is the NASA Li-Ion Battery Aging\nDataset [17]. This dataset contains several batteries that were\ntested under various conditions. In this study, the dataset used\nwas the B0005, B0006, B0007, and B0018 battery datasets.\nThe four batteries run through charging, discharging, and\nimpedance profiles at room temperature. Charging is carried\nout at a constant current at 1.5A until the battery voltage\nreaches 4.2V . Then it is continued in constant voltage until the\ncharge current drops to 20mA. Discharging was carried out\nat a constant current of 2A until the battery voltage dropped\nto 2.7V , 2.5V , 2.2V , and 2.5V for batteries 5, 6, 7, and 18,\nrespectively. Impedance measurements were carried out via\nelectrochemical frequency sweep impedance spectroscopy\n(EIS) of 0.1Hz to 5kHz.\nThe dataset used next is the CALCE Battery Research Data\ndataset [28] [24] [8]. In this research, the CALCE dataset used\nis the CS2 battery dataset which contains CS2_35, CS2_36,\nCS2_37, and CS2_38 batteries. All CS2 cells underwent the\nsame charging profile which was a standard constant cur-\nrent/constant voltage protocol with a constant current rate\nof 0.5 °C until the voltage reached 4.2V and then 4.2V was\nsustained until the charging current dropped to below 0.05A.\nUnless specified, the discharge cut-off voltage for these bat-\nteries was 2.7V .\n2) Data Preprocessing\nAt this stage, the data that has been obtained will be pre-\nprocessed before entering the modeling process. This stage\nincludes data cleaning and data normalization. The data\ncleaning process involves removing outliers and handling\nempty or missing values. Then the data normalization stage is\ncarried out. Data normalization is carried out using the Min-\nMax method, where the data will be converted into a range\nbetween 0 and 1. The normalization process is carried out to\nreduce data redundancy and increase the model’s accuracy.\nIn addition to the data cleaning and normalization process,\nadditional columns are also carried out, namely State-of-\nhealth (SoH) or health status of the battery using Equation\n9.\nSoH = Cactual\nCnom\n× 100%, (9)\nWhere Cactual and Cnom are the actual and nominal values of\nthe battery capacity, respectively.\nIn the NASA dataset, the features that will be used\nas model input are cycle, ambient_temperature, datetime,\ncapacity, voltage_measured, current_measured, tempera-\nture_measured, current_load, voltage_load, and time. Mean-\nwhile, the output variable from this model is SoH.\nIn the CALCE dataset, the features that will be used as\nmodel input are datetime, cycle, capacity, resistance, CCCT,\nCVCT. Meanwhile, the output variable from this model is\nSoH. The prediction input of this model is 72 cycles long and\nthe prediction output is 16 cycles long.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nB. EXPERIMENTAL PROCEDURES\nTo assess the performance of the SGEformer model in pre-\ndicting the age of lithium-ion batteries, a procedure for con-\nducting experiments is required. The procedure of the exper-\niment we carried out can be seen in the Figure 2. We will\nexplain the experimental procedure in more detail as follows:\n1) Obtain the state-of-health values from the battery\ndataset using equation 9.\n2) Normalize the data used as input.\n3) Divide the data into training and testing sets by dividing\n60%, 70%, and 80% of the dataset, then the first 81 and\n101 cycles of the dataset for training data.\n4) Preprocessed data enters the model, passing through the\nembedding layer first.\n5) Extract Growth, Seasonality, and Level from the En-\ncoder layer.\n6) For Growth and Seasonality, perform an embedding\nprocess before entering the Decoder.\n7) In the Decoder layer, obtain prediction results for eval-\nuation.\n8) Conduct evaluation by calculating the Mean Squared\nError (MSE) from the model’s prediction results.\nVI. RESULTS AND DISCUSSION\nIn this test, the State of Health (SoH) of the battery is pre-\ndicted several cycles into the future. The use of this cycle is\nbecause the battery charge and discharge cycle can reflect the\nactual use of the battery. Apart from that, the cycle also makes\nit easier to monitor the condition of the battery.\nThe tests will be carried out based on several conditions.\nIn each condition using the same initial hyperparameter with\na lookback window length of 96, prediction length of 8, K\nvalue of 0, learning rate value of 0.00001, number of epochs\nof 10, and training data length from cycle 1 to 100. The\nhyperparameter value changes according to the conditions\ntested.\nA. HYPERPARAMETERS TUNING\nIn this section, we present the results of the hyperparameter\nsetting tests that have been conducted. The first hyperparam-\neter we tested is the lookback window size. The lookback\nwindow represents the number of previous time periods used\nto predict future values. In our model, the size of the look-\nback window corresponds to the length of the training data\nsequence used for model training.\nThe second hyperparameter under examination is the\namount of training data. Training data refers to the data used\nto train the model. This training data is sourced from the\nresearch dataset and is typically divided into training data,\nvalidation data (if applicable), and test data.\nThe third hyperparameter considered is the learning rate\nvalue. The learning rate is a crucial value used to adjust the\nmodel parameters during each training iteration. The learning\nrate influences the speed at which the model converges to\nan optimal solution during the training process. Throughout\ntraining, the machine learning algorithm minimizes the error\nby modifying the model’s parameter values, with the learning\nrate governing the extent of these changes.\nThe fourth hyperparameter being explored is the K value.\nK is a parameter that controls the number of top frequen-\ncies retained during the feature extraction process using the\nFourier layer. The value of K determines how many of the\nfrequencies with the highest amplitudes are retained, while\nthose with lower amplitudes are excluded.\n1) Lookback Window Size\nIn testing the hyperparameter on the lookback window size,\nthe lookback window sizes used are 32, 48, 96, and 128. The\nresults of this test can be seen in Table 1.\nTABLE 1: Table of Lookback Window Size Test Results\nHyperparameter Results\nLookback window ETSformer Our Model (SGEformer)\n48 0.000339 0.000115\n96 0.000418 0.000097\n128 0.000339 0.000100\nThe graph of the results of the hyperparameter test on the\nsize of the lookback window and the graph of the prediction\nresults can be seen in Figure 3\nIn the tests carried out on the size of the lookback window\nin the model training process, the lookback window size that\nhas the best results is a lookback window value of 96 from\nSGEformer with an MSE value of 0.000097. In Figure 3 it can\nbe seen that almost all the lookback window size values used\nhave a trend similar to the original data. To reach the threshold\nvalue or limit of cycles where the battery reaches the end\npoint of its remaining life, almost all lookback window size\nvalues also reach that limit point in the same cycle. Only when\nthe size of the lookback window is 16 does it not match the\noriginal data. The up-and-down trend is almost the same but\nwith higher SoH values. From the results obtained, the size\nof the lookback window affects the MSE value of the model.\nTo get a good MSE value, you must choose the appropriate\nlookback window size. If it is too small, the model will study\nthe data in too much detail so that overfitting can occur. If\nit is too large, it will be difficult for the model to study the\ndata because there is too much noise that interferes with the\naccuracy of the model [2].\n2) The Amount of Training Data Used\nIn testing the amount of training data, two settings are made,\nnamely based on the percentage of training data and based on\ncertain cycles. In the hyperparameter test for the Amount of\nTraining Data based on the percentage of training data, the\npercentage values for the total training data used are 60%,\n70%, and 80%. In this data, the percentage of the amount\nof training data used is based on the total battery charge\ncycles in the dataset. The use of comparison ratios between\ntraining data and test data, namely 60%, 70%, and 80%, is\nbecause it is the most commonly used practice. These ratios\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 2: Experimental Procedures\nFIGURE 3: Graph of Prediction Results for Each Lookback\nWindow Size\nindicate an attempt to capture most of the patterns present in\nthe data without introducing excessive noise. Another reason\nis to provide the model with more data to learn, resulting in\nbetter accuracy. Choosing a smaller percentage like 60% is\nthe decision to introduce some level of generalization and\nprevent overfitting. In testing the amount of further training\ndata, namely testing the amount of training data based on\ncertain cycles in the dataset. For example, if the cycle value\nused is 101, then the amount of training data contains data\nfrom cycle 1 to 100, and the rest is testing data. The amount\nof training data used in this experiment is 81 and 101. The\nresults of this test can be seen in Table 2.\nTABLE 2: Table of Test Results for the Amount of Training\nData\nHyperparameter Results\nTraining Data ETSformer Our Model (SGEformer)\n60% 0.000299 0.000095\n70% 0.000283 0.000109\n80% 0.000177 0.000058\n81 0.000418 0.000232\n101 0.000646 0.000097\nThe graph of the results of the hyperparameter test on the\namount of training data and the graph of the prediction results\ncan be seen in Figure 4\nIn the tests carried out on the amount of training data in the\nmodel training process, the amount of training data that has\nthe best results is to use a total of 80% training data from our\nmodel with an MSE value of 0.000058. For testing the amount\nof training data based on a certain cycle, the best result is the\n101st cycle from our model with an MSE value of 0.000097.\nFrom the graph of the prediction results in Figure 4 it can be\nseen that almost the entire amount of training data used has a\ntrend similar to the original data. To reach the threshold value\nor the limit of the cycle where the battery reaches the end point\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 4: Graph of Prediction Results for Each Amount of\nTraining Data\nof its remaining life, only when the amount of training data\nis 80% cannot be used because the tests carried out exceed\nthe endpoint of the battery can be used. From the test results,\nit can be seen that the greater the percentage of the amount\nof training data, the smaller the resulting MSE value. This\nis because the SGEformer model studies more data in the\ntraining process, so when testing on less data, the MSE value\nis better.\n3) Learning Rate Value\nIn testing the hyperparameter for the learning rate value,\nthe learning rate values used were 0.001, 0.0003, 0.0001,\n0.00003, and 0.00001. The results of this test can be seen in\nTable 3.\nTABLE 3: Table of Learning Rate Test Results\nHyperparameter Results\nLearning rate ETSformer Our Model (SGEformer)\n1e-3 0.099591 0.106214\n3e-4 0.081708 0.084233\n1e-4 0.028502 0.035029\n3e-5 0.000163 0.000076\n1e-5 0.000418 0.000097\nThe graph of the results of the hyperparameter test on the\nlearning rate value and the graph of the prediction results can\nbe seen in Figures 5\nIn the tests carried out on the learning rate value in the\nmodel training process, the learning rate value that had the\nbest results was using a learning rate value of 0.00003 from\nour model with an MSE value of 0.000076. In Figure 5 it can\nbe seen that each prediction result of each learning rate used\nin the test. Similar to the original data are the learning rates of\n0.00001 and 0.00003. In addition to these two learning rates,\nthe SoH values are higher even though they have an almost\nsimilar up-and-down trend. The results of the tests carried out\nshow that the smaller the learning rate value, the smaller the\nMSE value. A learning rate that is too large can cause the\nFIGURE 5: Graph of Prediction Results for Each Learning\nRate Value\nmodel to become unstable when it reaches a convergent point\nor even fails to reach a convergent point. But if it is too small,\nit will take too long for the model to reach a convergent point\nor the model will be stuck at the saddle point (local optimum)\nso that it does not reach a convergent point [10].\n4) Top-K Value\nIn testing the hyperparameter on the Amount of Training\nData, the K values used are 0, 1, 2, 3, and 4. The results of\nthis test can be seen in Table 4.\nTABLE 4: Table of Top-K Value Test Results\nHyperparameter Results\nK ETSformer Our Model (SGEformer)\n0 0.000653 0.000097\n1 0.000841 0.006165\n2 0.001664 0.010638\n3 0.002302 0.026935\n4 0.002267 0.030782\nGraphs of the results of the hyperparameter test on the K\nvalue and the graphs of the prediction results can be seen in\nFigures 6\nIn the tests carried out on the K value in the model training\nprocess, the K value that had the best results was using a K\nvalue of 0 from our model with an MSE value of 0.000097. In\nFigure 6 it can be seen the predicted results of each K value\nused in the test. All K values tested have results that are almost\nsimilar to the original data. There is only a small difference\nin the SoH value between each of the K values carried out\nin the experiment. From the results of the tests that have been\ncarried out, it can be seen that the higher the K value used, the\nhigher the MSE value. This happens because when the value\nof K is smaller, the frequency that is maintained is less, so the\nmodel will be simpler. Reducing the number of frequencies\ncan also help reduce overfitting resulting in better results.\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 6: Graph of Prediction Results for Each K Value\nB. 10-FOLD CROSS VALIDATION\nCross-validation is a method used to measure how well a\nmodel will perform on data it has never seen before. In 10-\nfold cross-validation, your time series data is divided into\n10 subsets (folds) of equal size. Our model will be tested\n10 times, where each time one fold is used as test data, and\nthe other nine folds are used as training data. This is done\nalternately until every fold has been used as test data. The\nresults of 10-fold cross-validation can be seen in Table 5.\nTABLE 5: Table of 10-Fold Cross Validation Test Results\nFold ETSformer Our Model\n(SGEformer)\n1 0.000323 0.000371\n2 0.000419 0.000506\n3 0.003741 0.004245\n4 0.009954 0.010912\n5 0.009761 0.007234\n6 0.003749 0.003407\n7 0.003328 0.002676\n8 0.002784 0.002501\n9 0.002487 0.002521\n10 0.002272 0.001969\nAverage 0.0038818 0.003634\nStandard\nDeviation\n0.003197 0.003050\nFrom the 10-fold cross-validation test, it can be seen from\ntable 5 that our model has a smaller mean and standard\ndeviation compared to the ETSformer model. This low MSE\nresult shows that this model is very good at predicting battery\nlife.\nC. EXPERIMENT TO PREDICT ON DIFFERENT NUMBER OF\nCYCLES\nN-cycle forward prediction is carried out to understand the\neffect of cycle length prediction on errors. The results of the\nfuture n-cycle predictions can be seen in Table 6. From the\nresults in Table 6, it can be seen that our model is better\nat predicting future n-cycles compared to the ETSformer\nmodel. The MSE value results from our model predictions\nare 0.000153 at 10 cycles, 0.000144 at 20 cycles, 0.000108 at\n40 cycles, and 0.000137 at 80 cycles.\nTABLE 6: Table of Prediction Results on Different Number\nof Cycles\nNumber of Cycles ETSformer Our Model\n(SGEformer)\n10 0.000148 0.000153\n20 0.000196 0.000144\n40 0.000298 0.000108\n80 0.000302 0.000137\nD. MULTIVARIATE AND UNIVARIATE EXPERIMENTS\nThis test will compare the results of our model testing on\nmultivariate and univariate problems. From this test, the re-\nsults obtained were that multivariate problems had an MSE\nresult of 0.000097 and univariate problems had an MSE result\nof 0.000255. The graph of the prediction results from the\nSGEformer model on multivariate and univariate problems\ncan be seen in Figure 7.\nFIGURE 7: Graph of Prediction Results between Multivariate\nand Univariate SGEFormer on SoH prediction\nFigure 7 shows that our model is equally good at\nhandling multivariate and univariate problems on this\ndataset. Moreover, the results shows that in the multivariate\ncase (cycle, ambient_temperature, datetime, capacity, volt-\nage_measured, current_measured, temperature_measured,\ncurrent_load, voltage_load, time, and SoH) produces a better\nMSE value with a value of 0.000097 compared to Univariate\n(SoH) case which has an MSE value of 0.000255.\nE. EXPERIMENT USING ANOTHER BATTERY DATASET\nIn this test, we will compare the test results from the ETS-\nformer and our model (SGEformer) using another battery\ndataset. The battery dataset used is the NASA Battery Dataset\n(B0005, B0006, B0007, and B0018) and CALCE Battery\nDataset. For the CALCE battery dataset, we combine the\nCS2_35, CS2_37, and CS2_38 datasets as training data and\nthe CS2_36 battery as test data. The results of testing the\nETSformer and Our Model (SGEformer) against other battery\ndatasets can be seen in Table 7.\nFrom the tests that have been carried out, it shows that\nour model is better at handling each dataset compared to the\nETSformer model. The presented results from the comparison\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 7: Table of ETSformer and Our Model Test Results\nUsing Other Datasets\nDataset ETSformer Our Model\n(SGEformer)\nB0005 0.000418 0.000097\nB0006 0.000300 0.000072\nB0007 0.000395 0.000089\nB0018 0.000400 0.000253\nCALCE 0.007773 0.007362\nof the ETSformer model with the proposed SGEformer model\non different datasets provide valuable insights into the predic-\ntive performance of the two models in estimating the State of\nHealth (SoH) of batteries. The numerical values represent the\nMean Squared Error (MSE) or a similar metric, indicating the\naverage squared difference between the predicted SoH values\nand the actual values. A lower MSE generally indicates better\npredictive accuracy.\nAcross all datasets (B0005, B0006, B0007, B0018,\nCALCE), it is evident that the SGEformer model consistently\noutperforms the ETSformer model. The substantially lower\nMSE values achieved by the SGEformer model, such as\n0.000097 compared to 0.000418 for dataset B0005, under-\nscore its superior predictive capabilities. These results suggest\nthat the graph-based architecture of the SGEformer is effec-\ntive in capturing and leveraging the complex relationships\nwithin the battery datasets, leading to more accurate SoH\npredictions.\nNotably, the relative improvements in MSE are consistent\nacross various datasets, indicating the generalizability of the\nSGEformer model across different battery types or conditions\nrepresented by these datasets (B0005, B0006, B0007, B0018,\nCALCE). This robust performance enhances the model’s\ncredibility and potential applicability in diverse real-world\nscenarios.\nWhile both models exhibit relatively low MSE values, the\nmarginal differences observed in favor of the SGEformer\nmodel are crucial, especially in applications where precise\nSoH estimation is paramount, such as battery management\nsystems in electric vehicles or renewable energy storage.\nF. EXPERIMENT WITH OTHER MODELS\nIn this test, the accuracy value of the ETSformer and our\nmodel (SGEformer) will be compared with other models. The\nmodels used as a comparison are Reformer [11], Informer\n[35], Transformer [22], and LSTM [9]. For this test, we used\nB0005 battery data as the training set and B0006 battery\ndata as the test set. The prediction results are illustrated\nin Figure 9. The graphical representations present a clear\nvisual depiction of the predicted SoH values, providing a\nbasis for understanding the model’s efficacy. One notable\nobservation is the consistent and competitive performance of\nthe SGEformer model across diverse battery chemistries. The\ngraphs illustrate the model’s capability to accurately predict\nSoH values over multiple cycles, showcasing its robustness in\ncapturing the intricate dynamics of battery degradation. The\ncomparison with other models highlights instances where the\nSGEformer outperforms or matches the predictive accuracy\nof benchmark models, showcasing its potential as a reliable\ntool for SoH estimation.\nFIGURE 8: Bar Chart for Testing SGEformer Model Against\nOther Models using Different Batteries\nThe results of testing the ETSformer and Our Model (SGE-\nformer) against the Reformer and Informer models can be\nseen in Table 8 & Figure 8.\nFrom the comprehensive evaluation of our model, the\nSGEformer, and several other models, it is evident that our\nmodel (SGEformer) outperforms the competition in terms\nof Mean Squared Error ( MSE). Our model achieved a re-\nmarkable 0.000117 MSE, showcasing superior predictive ac-\ncuracy compared to the ETSformer (0.000190), Reformer\n(0.003599), Informer (0.002991), Transformer (0.002263),\nand LSTM (0.003241) models.\nIn addition to MSE, our model also excelled in Mean\nAbsolute Error ( MAE) with a value of 0.000117 and Root\nMean Squared Error ( RMSE) with a value of 0.009626. These\nresults further emphasize the effectiveness of our model in\ncapturing and predicting complex patterns in the data.\nFurthermore, considering the computational efficiency, our\nmodel’s training time of 670.74 seconds and testing time of\n199.76 seconds position it competitively among the compared\nmodels. While the ETSformer exhibits slightly lower testing\ntime, our model’s superior performance in terms of accuracy\nmakes it a compelling choice.\nIn summary, our SGEformer model demonstrates state-of-\nthe-art performance in the task at hand, showcasing its ca-\npability to provide highly accurate predictions with efficient\ntraining and testing times in seconds.\nVII. CONCLUSION\nBased on research conducted to predict the life of Lithium-\nIon batteries using the Smoothing Transformer with the Sea-\nsonal and Growth Embedding method, the best results were\nachieved through a specific combination of hyperparameters.\nThis combination includes a lookback window size of 96, a\nprediction length of 8, a K value of 0, a starting point of 101\nfor the amount of test data, a learning rate value of 0.00003,\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 9: Graph for Predicted SoH of SGEformer Model Against Other Models using Different Batteries over Cycles\nTABLE 8: Table of Testing Results of the ETSformer and Our Model against Other Models\nModel Name MSE MAE RMSE Training Time Testing Time\nOur Model\n(SGEformer)\n0.000117 0.009626 0.010814 670.74 199.76\nETSformer [25] 0.000190 0.012411 0.013794 581.6 171.77\nReformer [11] 0.003599 0.058746 0.059993 830.43 141.42\nInformer [35] 0.002991 0.053506 0.054692 755.63 232.5\nTransformer\n[22]\n0.002263 0.046998 0.047566 776.28 153.19\nLSTM [9] 0.003241 0.054376 0.056929 348.55 110.34\nand ten epochs, resulting in an MSE value of 0.000076. When\ncompared with the ETSformer model and other models such\nas Informer, Reformer, Transformer, and LSTM, our model\ndemonstrates superior performance with an MSE value of\n0.000117. These results establish that our model achieves\nstate-of-the-art performance.\nTo facilitate further development and improvement in fu-\nture research, several suggestions can be considered. One\noption is to conduct a grid search to explore hyperparame-\nter combinations, potentially leading to even better results.\nAnother avenue involves comparing our model not only with\nprevious methods but also with the latest machine learning\ntechniques to assess its competitiveness. Lastly, incorporating\nadditional battery data or datasets, such as those available\nfrom MIT, can broaden the scope of data and validate the\nmodel’s effectiveness on a more extensive sample.\nIncorporating these suggestions into future investigations\nmay contribute to enhancing the accuracy and applicability\nof the ETSformer with Seasonal and Growth Embedding\nmethods for predicting Lithium-Ion battery life, ultimately\nadvancing the field of battery research.\nAcknowledgement: In particular, thanks to the NASA Ames\nCenter of Excellence Diagnostic Center and the CALCE bat-\ntery team for providing experimental data for this research.\nREFERENCES\n[1] Jose Alarco and Peter Talbot. The history and develop-\nment of batteries, 2015.\n[2] Ammar Azlan, Yuhanis Yusof, and Mohamad\nFarhan Mohamad Mohsin. Determining the impact of\nwindow length on time series forecasting using deep\nlearning. International Journal of Advanced Computer\nResearch, 9:260–267, 9 2019.\n[3] George E. Blomgren. The development and future of\nlithium ion batteries. Journal of The Electrochemical\nSociety, 164:A5019–A5025, 2017.\n[4] Zhongwei Deng, Le Xu, Hongao Liu, Xiaosong Hu,\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhixuan Duan, and Yu Xu. Prognostics of battery ca-\npacity based on charging data and data-driven methods\nfor on-road vehicles. Applied Energy, 339, 6 2023.\n[5] Zhongwei Deng, Le Xu, Hongao Liu, Xiaosong Hu,\nBing Wang, and Jingjing Zhou. Rapid health estimation\nof in-service battery packs based on limited labels and\ndomain adaptation. Journal of Energy Chemistry , 2\n2023.\n[6] Andi Hamdianah, Wayan Firdaus Mahmudy, and Eko\nWidaryanto. Comparison of neural network and recur-\nrent neural network to predict rice production in east\njava, 2020.\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition. 6\n2016.\n[8] Wei He, Nicholas Williard, Michael Osterman, and\nMichael Pecht. Prognostics of lithium-ion batter-\nies based on dempster–shafer theory and the bayesian\nmonte carlo method. Journal of Power Sources ,\n196:10314–10321, 2011.\n[9] Sepp Hochreiter and Jürgen Schmidhuber. Long short-\nterm memory. Neural Computation , 9:1735–1780, 11\n1997.\n[10] Jennifer Jepkoech, David Muchangi Mugo, Benson K\nKenduiywo, and Edna Chebet Too. The effect of adap-\ntive learning rate on the accuracy of neural networks,\n2021.\n[11] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efficient transformer. 1 2020.\n[12] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou,\nWenhu Chen, Yu-Xiang Wang, and Xifeng Yan. En-\nhancing the locality and breaking the memory bottle-\nneck of transformer on time series forecasting. vol-\nume 32. Curran Associates, Inc., 2019.\n[13] Wayan Firdaus Mahmudy, Adyan Nur Alfiyatin, Can-\ndra Fajri Ananda, and Agus Wahyu Widodo. Inflation\nrate forecasting using extreme learning machine and\nimproved particle swarm optimization. International\nJournal of Intelligent Engineering and Systems , 14:95–\n104, 12 2021.\n[14] Wayan Firdaus Mahmudy, Aji Prasetya Wibawa, Na-\ndia Roosmalita Sari, H. Haviluddin, and P. Purnawan-\nsyah. Genetic algorithmised neuro fuzzy system for\nforecasting the online journal visitors. International\nJournal of Computing , 20:181–189, 2021.\n[15] Michael Mathieu, Mikael Henaff, and Yann LeCun. Fast\ntraining of convolutional networks through ffts. 12\n2013.\n[16] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados,\nand Yoshua Bengio. N-beats: Neural basis expansion\nanalysis for interpretable time series forecasting. 5\n2019.\n[17] Bhaskar Saha and Kai Goebel. Battery data set. NASA\nAMES prognostics data repository , 2007.\n[18] Bhaskar Saha, Kai Goebel, Scott Poll, and Jon Christo-\nphersen. Prognostics methods for battery health mon-\nitoring using a bayesian framework. IEEE Transac-\ntions on Instrumentation and Measurement , 58:291–\n296, 2009.\n[19] Tektronik. Lithium-ion battery maintenance guidelines,\n2023.\n[20] Yukai Tian, Jie Wen, Yanru Yang, Yuanhao Shi, and\nJianchao Zeng. State-of-health prediction of lithium-\nion batteries based on cnn-bilstm-am. Batteries, 8, 10\n2022.\n[21] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\nTransformer dissection: An unified understanding for\ntransformer’s attention via the lens of kernel. pages\n4343–4352. Association for Computational Linguistics,\n2019.\n[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\nvolume 30. Curran Associates, Inc., 2017.\n[23] Michail Vlachos, Philip Yu, and Vittorio Castelli. On\nperiodicity detection and structural periodic similar-\nity. pages 449–460. Society for Industrial and Applied\nMathematics, 4 2005.\n[24] Nick Williard, Wei He, Michael Osterman, and Michael\nPecht. Comparative analysis of features for determining\nstate of health in lithium-ion batteries. International\nJournal of Prognostics and Health Management , 4,\n2013.\n[25] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Ku-\nmar, and Steven Hoi. Etsformer: Exponential smoothing\ntransformers for time-series forecasting. 2 2022.\n[26] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng\nLong. Autoformer: Decomposition transformers with\nauto-correlation for long-term series forecasting. vol-\nume 34, pages 22419–22430. Curran Associates, Inc.,\n2021.\n[27] Sifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying\nWei, and Junzhou Huang. Adversarial sparse trans-\nformer for time series forecasting. volume 33, pages\n17105–17115. Curran Associates, Inc., 2020.\n[28] Yinjiao Xing, Eden W M Ma, Kwok-Leung Tsui, and\nMichael Pecht. An ensemble model for predicting the\nremaining useful performance of lithium-ion batteries.\nMicroelectronics Reliability, 53:811–820, 2013.\n[29] Xing Yan Yao, Guolin Chen, Michael Pecht, and Bin\nChen. A novel graph-based framework for state of\nhealth prediction of lithium-ion battery. Journal of\nEnergy Storage, 58, 2 2023.\n[30] Novanto Yudistira. Covid-19 growth prediction using\nmultivariate long short term memory. arXiv preprint\narXiv:2005.04809, 2020.\n[31] Novanto Yudistira, Sutiman Bambang Sumitro, Al-\nberth Nahas, and Nelly Florida Riama. Learn-\ning where to look for covid-19 growth: Multivari-\nate analysis of covid-19 cases over time using ex-\nplainable convolution–lstm. Applied Soft Computing ,\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n109:107469, 2021.\n[32] Chaolong Zhang, Shaishai Zhao, and Yigang He. An\nintegrated method of the future capacity and rul predic-\ntion for lithium-ion battery pack. IEEE Transactions on\nVehicular Technology, 71:2601–2613, 3 2022.\n[33] Chaolong Zhang, Shaishai Zhao, Zhong Yang, and Yuan\nChen. A reliable data-driven state-of-health estima-\ntion model for lithium-ion batteries in electric vehicles.\nFrontiers in Energy Research , 10, 9 2022.\n[34] Shaishai Zhao, Chaolong Zhang, and Yuanzhi Wang.\nLithium-ion battery capacity and remaining useful life\nprediction using board learning system and long short-\nterm memory neural network. Journal of Energy Stor-\nage, 52, 8 2022.\n[35] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai\nZhang, Jianxin Li, Hui Xiong, and Wancai Zhang. In-\nformer: Beyond efficient transformer for long sequence\ntime-series forecasting. Proceedings of the AAAI Con-\nference on Artificial Intelligence , 35:11106–11115, 5\n2021.\nMUHAMMAD RIFQI FAUZIreceived a Bache-\nlor’s degree in Informatics Engineering from the\nDepartment of Informatics Engineering, Brawi-\njaya University in 2023. He is interested in found-\ning and starting an AI start-up. His research focus\narea was Deep Learning and its application to Time\nSeries Forecasting. He can be contacted at email:\nmrifqi389@gmail.com.\nNOVANTO YUDISTIRA is a lecturer and re-\nsearcher at Brawijaya University, Indonesia. He\nobtained a Bachelor of Science in Informatics En-\ngineering from Institut Teknologi Sepuluh Nopem-\nber in 2007, a Master of Science in Computer\nScience from the Universiti Teknologi Malaysia in\n2011, and a Doctor of Engineering in Information\nTechnology from Hiroshima University, Japan in\n2018. In 2016, he was involved in research collab-\noration with the Mathematical Neuroinformatics\nGroup at the National Institute of Advanced Industrial Science and Technol-\nogy (AIST) in Japan. In 2018, he continued as a Postdoctoral fellow in the\nfields of Informatics and Data Science for 2 years, working with prominent\nJapanese research institutions, RIKEN and Osaka University. His current\nresearch interests include Deep Learning, Multimodal Computer Vision,\nMedical Informatics, and Big Data Analysis.\nWAYAN FIRDAUS MAHMUDYobtained a Bach-\nelor of Science degree from the Mathematics De-\npartment, Brawijaya University in 1995. His Mas-\nter in Informatics Engineering degree was obtained\nfrom the Sepuluh Nopember Institute of Technol-\nogy, Surabaya in 1999 while a PhD in Manufactur-\ning Engineering was obtained from the University\nof South Australia in 2014. He is a Professor at De-\npartment of Informatics Engineering, Brawijaya\nUniversity (UB), Indonesia. His research interests\ninclude optimization of combinatorial problems and machine learning. He\ncan be contacted at email: wayanfm@ub.ac.id.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3357736\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7061694860458374
    },
    {
      "name": "Computer science",
      "score": 0.592013955116272
    },
    {
      "name": "Embedding",
      "score": 0.5684565305709839
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.4574027359485626
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4370739161968231
    },
    {
      "name": "State of health",
      "score": 0.4230094254016876
    },
    {
      "name": "Machine learning",
      "score": 0.41097304224967957
    },
    {
      "name": "Reliability engineering",
      "score": 0.37391936779022217
    },
    {
      "name": "Battery (electricity)",
      "score": 0.30910801887512207
    },
    {
      "name": "Electrical engineering",
      "score": 0.27564185857772827
    },
    {
      "name": "Engineering",
      "score": 0.19161662459373474
    },
    {
      "name": "Voltage",
      "score": 0.1728481650352478
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177055848",
      "name": "University of Brawijaya",
      "country": "ID"
    }
  ],
  "cited_by": 17
}