{
  "title": "Normalized Log-Linear Interpolation of Backoff Language Models is Efficient",
  "url": "https://openalex.org/W2511530731",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5070589644",
      "name": "Kenneth Heafield",
      "affiliations": [
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5090089591",
      "name": "Chase Geigle",
      "affiliations": [
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5077883199",
      "name": "Sean Massung",
      "affiliations": [
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5006314068",
      "name": "Lane Schwartz",
      "affiliations": [
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2250695194",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W16967297",
    "https://openalex.org/W2106540279",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2010416582",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W101576916",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W98731357",
    "https://openalex.org/W635530177",
    "https://openalex.org/W2171457693",
    "https://openalex.org/W22168010",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W2251232800",
    "https://openalex.org/W2115081467",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2295800168",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2180952760",
    "https://openalex.org/W2096375461",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2270190199",
    "https://openalex.org/W2045633041",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W111602769"
  ],
  "abstract": "We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model, contradicting Hsu (2007).While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation, normalizing at query time was impractical.We normalize the model offline in advance, which is efficient due to a recurrence relationship between the normalizing factors.To tune interpolation weights, we apply Newton's method to this convex problem and show that the derivatives can be computed efficiently in a batch process.These findings are combined in new open-source interpolation tool, which is distributed with KenLM.With 21 out-of-domain corpora, log-linear interpolation yields 72.58 perplexity on TED talks, compared to 75.91 for linear interpolation.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 876–886,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nNormalized Log-Linear Interpolation of Backoff Language Models is\nEfﬁcient\nKenneth Heaﬁeld\nUniversity of Edinburgh\n10 Crichton Street\nEdinburgh EH8 9AB\nUnited Kingdom\nkheafiel@inf.ed.ac.uk\nChase Geigle Sean Massung\nUniversity of Illinois at Urbana-Champaign\n707 S. Mathews Ave.\nUrbana, IL 61801\nUnited States\n{geigle1,massung1,lanes}@illinois.edu\nLane Schwartz\nAbstract\nWe prove that log-linearly interpolated\nbackoff language models can be efﬁciently\nand exactly collapsed into a single nor-\nmalized backoff model, contradicting Hsu\n(2007). While prior work reported that\nlog-linear interpolation yields lower per-\nplexity than linear interpolation, normaliz-\ning at query time was impractical. We nor-\nmalize the model ofﬂine in advance, which\nis efﬁcient due to a recurrence relationship\nbetween the normalizing factors. To tune\ninterpolation weights, we apply Newton’s\nmethod to this convex problem and show\nthat the derivatives can be computed ef-\nﬁciently in a batch process. These ﬁnd-\nings are combined in new open-source in-\nterpolation tool, which is distributed with\nKenLM. With 21 out-of-domain corpora,\nlog-linear interpolation yields 72.58 per-\nplexity on TED talks, compared to 75.91\nfor linear interpolation.\n1 Introduction\nLog-linearly interpolated backoff language mod-\nels yielded better perplexity than linearly interpo-\nlated models (Klakow, 1998; Gutkin, 2000), but\nexperiments and adoption were limited due the im-\npractically high cost of querying. This cost is due\nto normalizing to form a probability distribution\nby brute-force summing over the entire vocabu-\nlary for each query. Instead, we prove that the\nlog-linearly interpolated model can be normalized\nofﬂine in advance and exactly expressed as an or-\ndinary backoff language model. This contradicts\nHsu (2007), who claimed that log-linearly inter-\npolated models “cannot be efﬁciently represented\nas a backoff n–gram model.”\nWe show that ofﬂine normalization is efﬁcient\ndue to a recurrence relationship between the\nnormalizing factors (Whittaker and Klakow,\n2002). This forms the basis for our open-\nsource implementation, which is part of KenLM:\nhttps://kheafield.com/code/kenlm/.\nLinear interpolation (Jelinek and Mercer, 1980),\ncombines several language models pi into a single\nmodel pL\npL(wn |wn−1\n1 ) =\n∑\ni\nλipi(wn |wn−1\n1 )\nwhere λi are weights and wn\n1 are words. Because\neach component model pi is a probability distri-\nbution and the non-negative weights λi sum to 1,\nthe interpolated model pL is also a probability dis-\ntribution. This presumes that the models have the\nsame vocabulary, an issue we discuss in §3.1.\nA log-linearly interpolated model pLL uses the\nweights λi as powers (Klakow, 1998).\npLL(wn |wn−1\n1 ) ∝\n∏\ni\npi(wn |wn−1\n1 )λi\nThe weights λi are unconstrained real numbers,\nallowing parameters to soften or sharpen distribu-\ntions. Negative weights can be used to divide a\nmixed-domain model by an out-of-domain model.\nTo form a probability distribution, the product is\nnormalized\npLL(wn |wn−1\n1 ) =\n∏\nipi(wn |wn−1\n1 )λi\nZ(wn−1\n1 )\nwhere normalizing factor Zis given by\nZ(wn−1\n1 ) =\n∑\nx\n∏\ni\npi(x|wn−1\n1 )λi\nThe sum is taken over all wordsxin the combined\nvocabulary of the underlying models, which can\nnumber in the millions or even billions. Comput-\ning Zefﬁciently is a key contribution in this work.\nOur proofs assume the component modelspiare\nbackoff language models (Katz, 1987) that mem-\norize probability for seen n–grams and charge a\n876\nbackoff penalty bi for unseen n–grams.\npi(wn |wn−1\n1 ) =\n{\npi(wn |wn−1\n1 ) if wn\n1 is seen\npi(wn |wn−1\n2 )bi(wn−1\n1 ) o.w.\nWhile linearly or log-linearly interpolated mod-\nels can be queried online by querying the compo-\nnent models (Stolcke, 2002; Federico et al., 2008),\ndoing so costs RAM to store duplicated n–grams\nand CPU time to perform lookups. Log-linear in-\nterpolation is particularly slow due to normalizing\nover the entire vocabulary. Instead, it is preferable\nto combine the models ofﬂine into a single back-\noff model containing the union of n–grams. Do-\ning so is impossible for linear interpolation (§3.2);\nSRILM (Stolcke, 2002) and MITLM (Hsu and\nGlass, 2008) implement an approximation. In con-\ntrast, we prove that ofﬂine log-linear interpolation\nrequires no such approximation.\n2 Related Work\nInstead of building separate models then weight-\ning, Zhang and Chiang (2014) show how to train\nKneser-Ney models (Kneser and Ney, 1995) on\nweighted data. Their work relied on prescriptive\nweights from domain adaptation techniques rather\nthan tuning weights, as we do here.\nOur exact normalization approach relies on the\nbackoff structure of component models. Sev-\neral approximations support general models: ig-\nnoring normalization (Chen et al., 1998), noise-\ncontrastive estimation (Vaswani et al., 2013), and\nself-normalization (Andreas and Klein, 2015). In\nfuture work, we plan to exploit the structure of\nother features in high-quality unnormalized log-\nlinear language models (Sethy et al., 2014).\nIgnoring normalization is particularly common\nin speech recognition and machine translation.\nThis is one of our baselines. Unnormalized mod-\nels can also be compiled into a single model by\nmultiplying the weighted probabilities and back-\noffs.1 Many use unnormalized models because\nweights can be jointly tuned along with other fea-\nture weights. However, Haddow (2013) showed\nthat linear interpolation weights can be jointly\ntuned by pairwise ranked optimization (Hopkins\nand May, 2011). In theory, normalized log-linear\ninterpolation weights can be jointly tuned in the\nsame way.\n1Missing probabilities are found from the backoff algo-\nrithm and missing backoffs are implicitly one.\nDynamic interpolation weights (Weintraub et\nal., 1996) give more weight to models famil-\niar with a given query. Typically the weights\nare a function of the contexts that appear in the\ncombined language model, which is compatible\nwith our approach. However, normalizing factors\nwould need to be calculated in each context.\n3 Linear Interpolation\nTo motivate log-linear interpolation, we examine\ntwo issues with linear interpolation: normalization\nwhen component models have different vocabular-\nies and ofﬂine interpolation.\n3.1 Vocabulary Differences\nLanguage models are normalized with respect to\ntheir vocabulary, including the unknown word.\n∑\nx∈vocab(p1)\np1(x) = 1\nIf two models have different vocabularies, then\nthe combined vocabulary is larger and the sum is\ntaken over more words. Component models as-\nsign their unknown word probability to these new\nwords, leading to an interpolated model that sums\nto more than one. An example is shown in Table 1.\np1 p2 pL Zero\n<unk> 0.4 0.2 0.3 0.3\nA 0.6 0.4 0.3\nB 0.8 0.6 0.4\nSum 1 1 1.3 1\nTable 1: Linearly interpolating two models p1\nand p2 with equal weight yields an unnormalized\nmodel pL. If gaps are ﬁlled with zeros instead, the\nmodel is normalized.\nTo work around this problem, SRILM (Stol-\ncke, 2002) uses zero probability instead of the un-\nknown word probability for new words. This pro-\nduces a model that sums to one, but differs from\nwhat users might expect.\nIRSTLM (Federico et al., 2008) asks the user to\nspecify a common large vocabulary size. The un-\nknown word probability is downweighted so that\nall models sum to one over the large vocabulary.\nA component model can also be renormalized\nwith respect to a larger vocabulary. For unigrams,\nthe extra mass is the number of new words times\nthe unknown word probability. For longer con-\ntexts, if we assume the typical case where the\n877\nunknown word appears only as a unigram, then\nqueries for new words will back off to unigrams.\nThe total mass in context wn−1\n1 is\n1 +|new|p(<unk>)\nn−1∏\ni=1\nb(wn−1\ni )\nwhere new is the set of new words. This is efﬁ-\ncient to compute online or ofﬂine. While there are\ntools to renormalize models, we are not aware of a\ntool that does this for linear interpolation.\nLog-linear interpolation is normalized by con-\nstruction. Nonetheless, in our experiments we ex-\ntend IRSTLM’s approach by training models with\na common vocabulary size, rather than retroﬁtting\nit at query time.\n3.2 Ofﬂine Linear Interpolation\nGiven an interpolated model, ofﬂine interpolation\nseeks a combined model meeting three criteria: (i)\nencoding the same probability distribution, (ii) be-\ning a backoff model, and (iii) containing the union\nof n–grams from component models.\nTheorem 1. The three ofﬂine criteria cannot be\nsatisﬁed for general linearly interpolated backoff\nmodels.\nProof. By counterexample. Consider the models\ngiven in Table 2 interpolated with equal weight.\np1 p2 pL\np(A) 0.4 0.2 0.3\np(B) 0.3 0.3 0.3\np(C) 0.3 0.5 0.4\np(C |A) 0.4 0.8 0.6\nb(A) 6\n7 ≈0.857 0.4 2\n3 ≈0.667\nTable 2: Counterexample models.\nThe probabilities shown for pL result from encod-\ning the same distribution. Taking the union of n–\ngrams implies that pL only has entries for A, B, C,\nand A C. Since the models have the same vocabu-\nlary, they are all normalized to one.\np(A |A) +p(B |A) +p(C |A) = 1\nSince all models have backoff structure,\np(A)b(A) +p(B)b(A) +p(C |A) = 1\nwhich when solved for backoffb(A) gives the val-\nues shown in Table 2. We then query pL(B |A)\nonline and ofﬂine. Online interpolation yields\npL(B |A) =1\n2p1(B |A) +1\n2p2(B |A)\n= 1\n2p1(B)b1(A) +1\n2p2(B)b2(A) = 33\n175\nOfﬂine interpolation yields\npL(B |A) =pL(B)bL(A) = 0.2 ̸= 33\n175 ≈0.189\nThe same problem happens with real language\nmodels. To understand why, we attempt to solve\nfor the backoff bL(wn−1\n1 ). Supposing wn\n1 is not in\neither model, we query pL(wn |wn−1\n1 ) ofﬂine\npL(wn|wn−1\n1 )\n=pL(wn|wn−1\n2 )bL(wn−1\n1 )\n=(λ1p1(wn|wn−1\n2 ) +λ2p2(wn|wn−1\n2 ))bL(wn−1\n1 )\nwhile online interpolation yields\npL(wn|wn−1\n1 )\n=λ1p1(wn|wn−1\n1 ) +λ2p2(wn|wn−1\n1 )\n=λ1p1(wn|wn−1\n2 )b1(wn−1\n1 ) +λ1p2(wn|wn−1\n2 )b2(wn−1\n1 )\nSolving for bL(wn−1\n1 ) we obtain\nλ1p1(wn|wn−1\n2 )b1(wn−1\n1 ) +λ2p2(wn|wn−1\n2 )b2(wn−1\n1 )\nλ1p1(wn|wn−1\n2 ) +λ2p2(wn|wn−1\n2 )\nwhich is a weighted average of the backoff weights\nb1(wn−1\n1 ) and b2(wn−1\n1 ). The weights depend on\nwn, so bL is no longer a function of wn−1\n1 .\nIn the SRILM approximation (Stolcke, 2002),\nprobabilities for n–grams that exist in the model\nare computed exactly. The backoff weights are\nchosen to produce a model that sums to one.\nHowever, newer versions of SRILM (Stolcke et\nal., 2011) interpolate by ingesting one component\nmodel at a time. For example, the ﬁrst two mod-\nels are approximately interpolated before adding\na third model. An n–gram appearing only in the\nthird model will have an approximate probabil-\nity. Therefore, the output depends on the order\nin which users specify models. Moreover, weights\nwere optimized for correct linear interpolation, not\nthe approximation.\nStolcke (2002) ﬁnd that the approximation actu-\nally decreases perplexity, which we also see in the\nexperiments (§6). However, approximation only\nhappens when the model backs off, which is less\nlikely to happen in ﬂuent sentences used for per-\nplexity scoring.\n878\n4 Ofﬂine Log-Linear Interpolation\nLog-linearly interpolated backoff models pi can\nbe collapsed into a single ofﬂine model pLL. The\ncombined model takes the union of n–grams in\ncomponent models.2 For those n–grams, it mem-\norizes correct probability pLL.\npLL(wn |wn−1\n1 ) =\n∏\nipi(wn |wn−1\n1 )λi\nZ(wn−1\n1 ) (1)\nWhen wn\n1 does not appear, the backoff bLL(wn−1\n1 )\nmodiﬁes pLL(wn | wn−1\n2 ) to make an appropri-\nately normalized probability. To do so, it can-\ncels out the shorter query’s normalization term\nZ(wn−1\n2 ) then applies the correct term Z(wn−1\n1 ).\nIt also applies the component backoff terms.\nbLL(wn−1\n1 ) =Z(wn−1\n2 )\nZ(wn−1\n1 )\n∏\ni\nbi(wn−1\n1 )λi (2)\nAlmost by construction, the model satisﬁes two\nof our criteria ( §3.2): being a backoff model and\ncontaining the union of n–grams. However, back-\noff models require that the backoff weight of an\nunseen n–gram be implicitly 1.\nLemma 1. If wn−1\n1 is unseen in the combined\nmodel, then the backoff weight bLL(wn−1\n1 ) = 1.\nProof. Because we have taken the union of en-\ntries, wn−1\n1 is unseen in component models. These\ncomponents are backoff models, so implicitly\nbi(wn−1\n1 ) = 1∀i. Focusing on the normalization\nterm Z(wn−1\n1 ),\nZ(wn−1\n1 ) =\n∑\nx\n∏\ni\npi(x|wn−1\n1 )λi\n=\n∑\nx\n∏\ni\npi(x|wn−1\n2 )λi bi(wn−1\n1 )λi\n=\n∑\nx\n∏\ni\npi(x|wn−1\n2 )λi\n= Z(wn−1\n2 )\nAll of the models back off because wn−1\n1 xis un-\nseen, being a superstring of wn−1\n1 . Relevant back-\noff weights bi(wn−1\n1 ) = 1as noted earlier. Recall-\ning the deﬁnition of bLL(wn−1\n1 ),\nZ(wn−1\n2 )\nZ(wn−1\n1 )\n∏\ni\nbi(wn−1\n1 )λi = Z(wn−1\n2 )\nZ(wn−1\n1 ) = 1\n2We further assume that every substring of a seenn–gram\nis also seen. This follows from estimating on text, except in\nthe case of adjusted count pruning by SRILM. In such cases,\nwe add the missing entries to component models, with no\nadditional memory cost in trie data structures.\nWe now have a backoff model containing the\nunion of n–grams. It remains to show that the of-\nﬂine model produces correct probabilities.\nTheorem 2. The proposed ofﬂine model agrees\nwith online log-linear interpolation.\nProof. By induction on the number of words\nbacked off in ofﬂine interpolation. To disam-\nbiguate, we will use pon to refer to online inter-\npolation and poff to refer to ofﬂine interpolation.\nBase case: the queried n–gram is in the ofﬂine\nmodel and we have memorized the online prob-\nability by construction.\nInductive case: Let poff(wn |wn−1\n1 ) be a query\nthat backs off. In online interpolation,\npon(wn |wn−1\n1 ) =\n∏\nipi(wn |wn−1\n1 )λi\nZ(wn−1\n1 )\nBecause wn\n1 is unseen in the ofﬂine model and we\ntook the union, it is unseen in every model pi.\n=\n∏\nipi(wn |wn−1\n2 )λi bi(wn−1\n1 )λi\nZ(wn−1\n1 )\n=\n(∏\nipi(wn |wn−1\n2 )λi\n)∏\nibi(wn−1\n1 )λi\nZ(wn−1\n1 )\nRecognizing the unnormalized probability\nZ(wn−1\n2 )pon(wn |wn−1\n2 ),\n= Z(wn−1\n2 )pon(wn |wn−1\n2 ) ∏\nibi(wn−1\n1 )λi\nZ(wn−1\n1 )\n= pon(wn |wn−1\n2 )Z(wn−1\n2 )\nZ(wn−1\n1 )\n∏\ni\nbi(wn−1\n1 )λi\n= pon(wn |wn−1\n2 )boff(wn−1\n1 )\nThe last equality follows from the deﬁnition of\nboff and Lemma 1, which extended the domain of\nboff to any wn−1\n1 . By the inductive hypothesis,\npon(wn | wn−1\n2 ) = poff(wn | wn−1\n2 ) because it\nbacks off one less time.\n= poff(wn |wn−1\n2 )boff(wn−1\n1 ) =poff(wn |wn−1\n1 )\nThe ofﬂine model poff(wn |wn−1\n1 ) backs off be-\ncause that is the case we are considering. Combin-\ning our chain of equalities,\npon(wn |wn−1\n1 ) =poff(wn |wn−1\n1 )\nBy induction, the claim holds for all wn\n1 .\n879\n4.1 Normalizing Efﬁciently\nIn order to build the ofﬂine model, the normaliza-\ntion factor Z needs to be computed in every seen\ncontext. To do so, we extend the tree-structure\nmethod of Whittaker and Klakow (2002), which\nthey used to compute and cache normalization fac-\ntors on the ﬂy. It exploits the sparsity of language\nmodels: when summing over the vocabulary, most\nqueries will back off. Formally, we deﬁne s(wn\n1 )\nto be the set of wordsxwhere pi(x|wn\n1 ) does not\nback off in some model.\ns(wn\n1 ) ={x: wn\n1 xis seen in any model}\nTo exploit this, we use the normalizing factor\nZ(wn\n2 ) from a lower order and patch it up by sum-\nming over s(wn\n1 ).\nTheorem 3. The normalization factors Z obey a\nrecurrence relationship:\nZ(wn\n1 ) =\n∑\nx∈s(wn\n1 )\n∏\ni\npi(x|wn\n1 )λi +\n\nZ(wn\n2 ) −\n∑\nx∈s(wn\n1 )\n∏\ni\npi(x|wn\n2 )λi\n\n∏\ni\nbi(wn\n1 )λi\nProof. The ﬁrst term handles seen n–grams while\nthe second term handles unseen n–grams. The\ndeﬁnition of Z\nZ(wn\n1 ) =\n∑\nx∈vocab\n∏\ni\npi(x|wn\n1 )λi\ncan be partitioned by cases.\n∑\nx∈s(wn\n1 )\n∏\ni\npi(x|wn\n1 )λi +\n∑\nx̸∈s(wn\n1 )\n∏\ni\npi(x|wn\n1 )λi\nThe ﬁrst term agrees with the claim, so we focus\non the case where x ̸∈s(wn\n1 ). By deﬁnition of s,\nall models back off.\n∑\nx̸∈s(wn\n1 )\n∏\ni\npi(x|wn\n1 )λi\n=\n∑\nx̸∈s(wn\n1 )\n∏\ni\npi(x|wn\n2 )λi bi(wn\n1 )λi\n=\n\n ∑\nx̸∈s(wn\n1 )\n∏\ni\npi(x|wn\n2 )λi\n\n∏\ni\nbi(wn\n1 )λi\n=\n\nZ(wn\n2 ) −\n∑\nx∈s(wn\n1 )\n∏\ni\npi(x|wn\n2 )λi\n\n∏\ni\nbi(wn\n1 )λi\nThis is the second term of the claim.\nLM1\nLM2\n. . .\nLMℓ\nMerge probabilities (§4.2.1)\nApply Backoffs (§4.2.2)\nNormalize (§4.2.3)\nOutput (§4.2.4)\nContext sort\n⟨wn\n1 , m(wn\n1 ), ⟩∏\nipi(wn|wn−1\nmi(wn\n1 ))λi ),∏\nipi(wn|wn−1\nmi(wn\n2 ))λi )\nIn context order\n⟨wn\n1 ,∏\nibi(wn−1\n1 )λi , ⟩∏\nipi(wn |wn−1\n1 )λi ,∏\nipi(wn |wn−1\n2 )λi\nIn sufﬁx order\nbLL(wn\n1 )\nSufﬁx sort⟨\nwn\n1 ,pLL(wn|wn−1\n1 )\n⟩\nFigure 1: Multi-stage streaming pipeline for of-\nﬂine log-linear interpolation. Bold arrows indicate\nsorting is performed.\nThe recurrence structure of the normalization\nfactors suggests a computational strategy: com-\npute Z(ϵ) by summing over the unigrams, Z(wn)\nby summing over bigramswnx, Z(wn\nn−1) by sum-\nming over trigrams wn\nn−1x, and so on.\n4.2 Streaming Computation\nPart of the point of ofﬂine interpolation is that\nthere may not be enough RAM to ﬁt all the com-\nponent models. Moreover, with compression tech-\nniques that rely on immutable models (Whittaker\nand Raj, 2001; Talbot and Osborne, 2007), a mu-\ntable version of the combined model may not ﬁt in\nRAM. Instead, we construct the ofﬂine model with\ndisk-based streaming algorithms, using the frame-\nwork we designed for language model estimation\n(Heaﬁeld et al., 2013). Our pipeline (Figure 1) has\nfour conceptual steps: merge probabilities, apply\nbackoffs, normalize, and output. Applying back-\noffs and normalization are performed in the same\npass, so there are three total passes.\n4.2.1 Merge Probabilities\nThis step takes the union of n–grams and multi-\nplies probabilities from component models. We\n880\nassume that the component models are sorted in\nsufﬁx order (Figure 4), which is true of models\nproduced by lmplz (Heaﬁeld et al., 2013) or\nstored in a reverse trie. Moreover, despite having\ndifferent word indices, the models are consistently\nsorted using the string word, or a hash thereof.\n3 2 1\nA\nA A\nA A A\nB A A\nB\nTable 3: Merging probabilities processes n–grams\nin lexicographic order by sufﬁx. Column headings\nindicate precedence.\nThe algorithm processes n–grams in lexico-\ngraphic (depth-ﬁrst) order by sufﬁx (Table 3). In\nthis way, the algorithm processes pi(A) before it\nmight be used as a backoff point for pi(A |B)\nin one of the models. It jointly streams through all\nmodels, so thatp1(A|B) and p2(A|B) are avail-\nable at the same time. Ideally, we would compute\nunnormalized probabilities.\n∏\ni\npi(wn |wn−1\n1 )λi\nHowever, these queries back off when models con-\ntain different n–grams. The appropriate backoff\nweights bi(wn−1\n1 ) are not available in a stream-\ning fashion. Instead, we proceed without charging\nbackoffs\n∏\ni\npi(wn |wn−1\nmi(wn\n1 ))λi\nwhere mi(wn\n1 ) records what backoffs should be\ncharged later.\nThe normalization step (§4.2.3) also uses lower-\norder probabilities\n∏\ni\npi(wn |wn−1\n2 )λi\nand needs to access them in a streaming fashion,\nso we also output\n∏\ni\npi(wn |wn−1\nmi(wn\n2 ))λi\nSufﬁx\n3 2 1\nZ B A\nZ A B\nB B B\nContext\n2 1 3\nZ A B\nB B B\nZ B A\nTable 4: Sorting orders (Heaﬁeld et al., 2013). In\nsufﬁx order, the last word is primary. In context\norder, the penultimate word is primary. Column\nheadings indicate precedence.\nEach output tuple has the form\n⟨\nwn\n1 ,m(wn\n1 ),\n∏\ni\npi(wn|wn−1\nmi(wn\n1 ))λi ,\n∏\ni\npi(wn|wn−1\nmi(wn\n2 ))λi\n⟩\nwhere m(wn\n1 ) is a vector of backoff requests, from\nwhich m(wn\n2 ) can be computed.\n4.2.2 Apply Backoffs\nThis step fulﬁlls the backoff requests from merg-\ning probabilities. The merged probabilities are\nsorted in context order (Table 4) so that n–\ngrams wn\n1 sharing the same context wn−1\n1 are\nconsecutive. Moreover, contexts wn−1\n1 appear\nin sufﬁx order. We use this property to stream\nthrough the component models again in their\nnative sufﬁx order, this time reading backoff\nweights bi(wn−1\n1 ),bi(wn−1\n2 ),...,b i(wn−1). Mul-\ntiplying the appropriate backoff weights by∏\nipi(wn|wn−1\nmi(wn\n1 ))λi yields unnormalized proba-\nbility ∏\ni\npi(wn|wn−1\n1 )λi\nThe same applies to the lower order.\n∏\ni\npi(wn|wn−1\n2 )λi\nThis step also merges backoffs from component\nmodels, with output still in context order.\n⟨\nwn\n1 ,\n∏\ni\nbi(wn−1\n1 )λi ,\n∏\ni\npi(wn|wn−1\n1 )λi\n∏\ni\npi(wn|wn−1\n2 )λi\n⟩\nThe implementation is combined with normaliza-\ntion, so the tuple is only conceptual.\n881\n4.2.3 Normalize\nThis step computes normalization factor Z for\nall contexts, which it applies to produce pLL and\nbLL. Recalling §4.1, Z(wn−1\n1 ) is efﬁcient to com-\npute in a batch process by processing sufﬁxes\nZ(ϵ),Z(wn),...Z (wn−1\n2 ) ﬁrst. In order to min-\nimize memory consumption, we chose to evaluate\nthe contexts in depth-ﬁrst order by sufﬁx, so that\nZ(A) is computed immediately before it is needed\nto compute Z(A A) and forgotten at Z(B).\nComputing Z(wn−1\n1 ) by applying Theorem 3\nrequires the sum\n∑\nx∈s(wn−1\n1 )\n∏\ni\npi(x|wn−1\n1 )λi\nwhere s(wn−1\n1 ) restricts to seenn–grams. For this,\nwe stream through the output of the apply backoffs\nstep in context order, which makes the various val-\nues of x consecutive. Theorem 3 also requires a\nsum over the lower-order unnormalized probabili-\nties ∑\nx∈s(wn−1\n1 )\n∏\ni\npi(x|wn−1\n2 )λi\nWe placed these terms in the input tuple for\nwn−1\n1 x. Otherwise, it would be hard to access\nthese values while streaming in context order.\nWhile we have shown how to compute\nZ(wn−1\n1 ), we still need to normalize the probabil-\nities. Unfortunately, Z(wn−1\n1 ) is only known after\nstreaming through all records of the form wn−1\n1 x,\nwhich are the very same records to normalize. We\ntherefore buffer up to the vocabulary size for each\norder in memory to allow rewinding. Processing\ncontext wn−1\n1 thus yields normalized probabilities\npLL(x|wn−1\n1 ) for all seen wn−1\n1 x.\n⣨\nwn\n1 ,pLL(x|wn−1\n1 )\n⟩\nThese records are generated in context order, the\nsame order as the input.\nThe normalization step also computes backoffs.\nbLL(wn−1\n1 ) =Z(wn−1\n2 )\nZ(wn−1\n1 )\n∏\ni\nbi(wn−1\n1 )λi\nNormalization Z(wn−1\n1 ) is computed by this step,\nnumerator Z(wn−1\n2 ) is available due to depth-ﬁrst\nsearch, and the backoff terms ∏\nibi(wn−1\n1 )λi are\npresent in the input. The backoffs bLL are gener-\nated in sufﬁx order, since each context produces a\nbackoff value. These are written to a sidechannel\nstream as bare values without keys.\n4.2.4 Output\nLanguage model toolkits store probability\npLL(wn |wn−1\n1 ) and backoff bLL(wn\n1 ) together as\nvalues for the key wn\n1 . To reunify them, we sort\n⟨wn\n1 ,pLL(wn |wn−1\n1 )⟩in sufﬁx order and merge\nwith the backoff sidechannel from normalization,\nwhich is already in sufﬁx order. Sufﬁx order is\nalso preferable because toolkits can easily build a\nreverse trie data structure.\n5 Tuning\nWeights are tuned to maximize the log probability\nof held-out data. This is a convex optimization\nproblem (Klakow, 1998). Iterations are expensive\ndue to the need to normalize over the vocabulary\nat least once. However, the number of weights is\nsmall, which makes the Hessian matrix cheap to\nstore and invert. We therefore selected Newton’s\nmethod.3\nThe log probability of tuning data wis\nlog\n∏\nn\npLL(wn |wn−1\n1 )\nwhich expands according to the deﬁnition of pLL\n∑\nn\n(∑\ni\nλilog pi(wn |wn−1\n1 )\n)\n−log Z(wn−1\n1 )\nThe gradient with respect toλihas a compact form\n∑\nn\nlog pi(wn |wn−1\n1 ) +CH(pLL,pi |wn−1\n1 )\nwhere CH is cross entropy. However, comput-\ning the cross entropy directly would entail a sum\nover the vocabulary for every word in the tun-\ning data. Instead, we apply Theorem 3 to ex-\npress Z(wn−1\n1 ) in terms of Z(wn−1\n2 ) before tak-\ning the derivative. This allows us to perform the\nsame depth-ﬁrst computation as before ( §4.2.3),\nonly this time ∂\n∂λi\nZ(wn−1\n1 ) is computed in terms\nof ∂\n∂λi\nZ(wn−1\n2 ).\nThe same argument applies when taking the\nHessian with respect to λi and λj. Rather than\ncompute it directly in the form\n∑\nn\n−\n∑\nx\npLL(x|wn−1\n1 ) logpi(x|wn−1\n1 ) logpj(x|wn−1\n1 )\n+ CH(pLL,pi |wn−1\n1 )CH(pLL,pj |wn−1\n1 )\nwe apply Theorem 3 to compute the Hessian for\nwn\n1 in terms of the Hessian for wn\n2 .\n3We also considered minibatches, though grouping tuning\ndata to reduce normalization cost would introduce bias.\n882\n6 Experiments\nWe perform experiments for perplexity, query\nspeed, memory consumption, and effectiveness in\na machine translation system.\nIndividual language models were trained on En-\nglish corpora from the WMT 2016 news transla-\ntion shared task (Bojar et al., 2016). This includes\nthe seven newswires (afp, apw, cna, ltw, nyt,\nwpb, xin) from English Gigaword Fifth Edition\n(Parker et al., 2011); the 2007–2015 news crawls;4\nNews discussion; News commmentary v11; En-\nglish from Europarl v8 (Koehn, 2005); the English\nside of the French-English parallel corpus (Bojar\net al., 2013); and the English side of SETIMES2\n(Tiedemann, 2009). We additionally built one lan-\nguage model trained on the concatenation of all\nof the above corpora. All corpora were prepro-\ncessed using the standard Moses (Koehn et al.,\n2007) scripts to perform normalization, tokeniza-\ntion, and truecasing. To prevent SRILM from run-\nning out of RAM, we excluded the large mono-\nlingual CommonCrawl data, but included English\nfrom the parallel CommonCrawl data.\nAll language models are 5-gram backoff lan-\nguage models trained with modiﬁed Kneser-Ney\nsmoothing (Chen and Goodman, 1998) using\nlmplz (Heaﬁeld et al., 2013). Also to prevent\nSRILM from running out of RAM, we pruned sin-\ngleton trigrams and above.\nFor linear interpolation, we tuned weights us-\ning IRSTLM. To work around SRILM’s limitation\nof ten models, we interpolated the ﬁrst ten then\ncarried the combined model and added nine more\ncomponent models, repeating this last step as nec-\nessary. Weights were normalized within batches\nto achieve the correct ﬁnal weighting. This simply\nextends the way SRILM internally carries a com-\nbined model and adds one model at a time.\n6.1 Perplexity experiments\nWe experiment with two domains: TED talks,\nwhich is out of domain, and news, which is in-\ndomain for some corpora. For TED, we tuned\non the IWSLT 2010 English dev set and test on\nthe 2010 test set. For news, we tuned on the\nEnglish side of the WMT 2015 Russian–English\nevaluation set and test on the WMT 2014 Russian–\nEnglish evaluation set. To measure generalization,\nwe also evaluated news on models tuned for TED\nand vice-versa. Results are shown in Table 5.\n4For News Crawl 2014, we used version 2.\nComponent Models\nComponent TED test News test\nGigaword afp 163.48 221.57\nGigaword apw 140.65 206.85\nGigaword cna 299.93 448.56\nGigaword ltw 106.28 243.35\nGigaword nyt 97.21 211.75\nGigaword wpb 151.81 341.48\nGigaword xin 204.60 246.32\nNews 07 127.66 243.53\nNews 08 112.48 202.86\nNews 09 111.43 197.32\nNews 10 114.40 209.31\nNews 11 107.69 187.65\nNews 12 105.74 180.28\nNews 13 104.09 155.89\nNews 14 v2 101.85 139.94\nNews 15 101.13 141.13\nNews discussion 99.88 249.63\nNews commentary v11 236.23 495.27\nEuroparl v8 268.41 574.74\nCommonCrawl fr-en.en 149.10 343.20\nSETIMES2 ro-en.en 331.37 521.19\nAll concatenated 80.69 96.15\nTED weights\nInterpolation TED test News test\nOfﬂine linear 75.91 100.43\nOnline linear 76.93 152.37\nLog-linear 72.58 112.31\nNews weights\nInterpolation TED test News test\nOfﬂine linear 83.34 107.69\nOnline linear 83.94 110.95\nLog-linear 89.62 124.63\nTable 5: Test set perplexities. In the middle ta-\nble, weights are optimized for TED and include\na model trained on all concatenated text. In the\nbottom table, weights are optimized for news and\nexclude the model trained on all concatenated text.\n883\nLM Tuning Compiling Querying\nAll concatenated N/A N/A N/A N/A 0.186µs 46.7G\nOfﬂine linear 0.876m 60.2G 641m 123G 0.186µs 46.8G\nOnline linear 0.876m 60.2G N/A N/A 5.67µs 89.1G\nLog-linear 600m 63.9G 89.8m 63.9G 0.186µs 46.8G\nTable 6: Speed and memory consumption of LM combination methods. Interpolated models include the\nconcatenated model. Tuning and compiling times are in minutes, memory consumption in gigabytes,\nand query time in microseconds per query (on 1G of held-out Common Crawl monolingual data).\nLog-linear interpolation performs better on\nTED (72.58 perplexity versus 75.91 for ofﬂine lin-\near interpolation). However, it performs worse\non news. In future work, we plan to investigate\nwhether log-linear wins when all corpora are out-\nof-domain since it favors agreement by all models.\nTable 6 compares the speed and memory per-\nformance of the competing methods. While the\nlog-linear tuning is much slower, its compilation is\nfaster compared to the ofﬂine linear model’s long\nrun time. Since the model formats are the same\nfor the concatenation and log-linear, they share\nthe fastest query speeds. Query speed was mea-\nsured using KenLM’s (Heaﬁeld, 2011) faster prob-\ning data structure.5\n6.2 MT experiments\nWe trained a statistical phrase-based machine\ntranslation system for Romanian-English on the\nRomanian-English parallel corpora released as\npart of the 2016 WMT news translation shared\ntask. We trained three variants of this MT system.\nThe ﬁrst used a single language model trained on\nthe concatenation of the 21 individual LM train-\ning corpora. The second used 22 language mod-\nels, with each LM presented to Moses as a sep-\narate feature. The third used a single language\nmodel which is an interpolation of all 22 mod-\nels. This variant was run with ofﬂine linear, online\nlinear, and log-linear interpolation. All MT sys-\ntem variants were optimized using IWSLT 2011\nRomanian-English TED test as the development\nset, and were evaluated using the IWSLT 2012\nRomanian-English TED test set.\nAs shown in Table 7, no signiﬁcant difference in\nMT quality as measured by BLEU was observed;\nthe best BLEU score came from separate features\nat 18.40 while log-linear scored 18.15. We leave\n5KenLM does not natively implement online linear inter-\npolation, so we wrote a custom wrapper, which is faster than\nquerying IRSTLM.\nLM BLEU BLEU-c\n22 separate LMs 18.40 17.91\nAll concatenated 18.02 17.55\nOfﬂine linear 18.00 17.53\nOnline linear 18.27 17.82\nLog-linear 18.15 17.70\nTable 7: Machine translation performance com-\nparison in an end-to-end system.\njointly tuned normalized log-linear interpolation\nto future work.\n7 Conclusion\nNormalized log-linear interpolation is now a\ntractable alternative to linear interpolation for\nbackoff language models. Contrary to Hsu (2007),\nwe proved that these models can be exactly col-\nlapsed into a single backoff language model.\nThis solves the query speed problem. Empiri-\ncally, compiling the log-linear model is faster than\nSRILM can collapse its approximate ofﬂine linear\nmodel. In future work, we plan to improve per-\nformace of feature weight tuning and investigate\nmore general features.\nAcknowledgments\nThanks to Jo ˜ao Sedoc, Grant Erdmann, Jeremy\nGwinnup, Marcin Junczys-Dowmunt, Chris Dyer,\nJon Clark, and MT Marathon attendees for discus-\nsions. Partial funding was provided by the Ama-\nzon Academic Research Awards program. This\nmaterial is based upon work supported by the NSF\nGRFP under Grant Number DGE-1144245.\nReferences\nJacob Andreas and Dan Klein. 2015. When and why\nare log-linear models self-normalizing? In NAACL\n2015.\n884\nOndˇrej Bojar, Christian Buck, Chris Callison-Burch,\nChristian Federmann, Barry Haddow, Philipp\nKoehn, Christof Monz, Matt Post, Radu Soricut, and\nLucia Specia. 2013. Findings of the 2013 workshop\non statistical machine translation. In Proceedings of\nthe Eighth Workshop on Statistical Machine Trans-\nlation, pages 1–44, Soﬁa, Bulgaria, August. Associ-\nation for Computational Linguistics.\nOndˇrej Bojar, Christian Buck, Rajen Chatterjee, Chris-\ntian Federmann, Liane Guillou, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Aur ´elie\nN´ev´eol, Mariana Neves, Pavel Pecina, Martin Popel,\nPhilipp Koehn, Christof Monz, Matteo Negri, Matt\nPost, Lucia Specia, Karin Verspoor, J ¨org Tiede-\nmann, and Marco Turchi. 2016. Findings of the\n2016 Conference on Machine Translation. In Pro-\nceedings of the First Conference on Machine Trans-\nlation (WMT’16), Berlin, Germany, August.\nStanley Chen and Joshua Goodman. 1998. An em-\npirical study of smoothing techniques for language\nmodeling. Technical Report TR-10-98, Harvard\nUniversity, August.\nStanley F. Chen, Kristie Seymore, and Ronald Rosen-\nfeld. 1998. Topic adaptation for language modeling\nusing unnormalized exponential models. In Acous-\ntics, Speech and Signal Processing, 1998. Proceed-\nings of the 1998 IEEE International Conference on,\nvolume 2, pages 681–684. IEEE.\nMarcello Federico, Nicola Bertoldi, and Mauro Cet-\ntolo. 2008. IRSTLM: an open source toolkit for\nhandling large scale language models. In Proceed-\nings of Interspeech, Brisbane, Australia.\nAlexander Gutkin. 2000. Log-linear interpolation of\nlanguage models. Master’s thesis, University of\nCambridge, November.\nBarry Haddow. 2013. Applying pairwise ranked opti-\nmisation to improve the interpolation of translation\nmodels. In Proceedings of NAACL.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modi-\nﬁed Kneser-Ney language model estimation. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics, Soﬁa, Bulgaria,\nAugust.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, Edin-\nburgh, UK, July. Association for Computational Lin-\nguistics.\nMark Hopkins and Jonathan May. 2011. Tuning as\nranking. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1352—-1362, Edinburgh, Scotland, July.\nBo-June Hsu and James Glass. 2008. Iterative lan-\nguage model estimation: Efﬁcient data structure &\nalgorithms. In Proceedings of Interspeech , Bris-\nbane, Australia.\nBo-June Hsu. 2007. Generalized linear interpolation\nof language models. In Automatic Speech Recogni-\ntion & Understanding, 2007. ASRU. IEEE Workshop\non, pages 136–140. IEEE.\nFrederick Jelinek and Robert L. Mercer. 1980. In-\nterpolated estimation of Markov source parameters\nfrom sparse data. In Proceedings of the Workshop\non Pattern Recognition in Practice, pages 381–397,\nMay.\nSlava Katz. 1987. Estimation of probabilities from\nsparse data for the language model component of a\nspeech recognizer. IEEE Transactions on Acoustics,\nSpeech, and Signal Processing , ASSP-35(3):400–\n401, March.\nDietrich Klakow. 1998. Log-linear interpolation of\nlanguage models. In Proceedings of 5th Interna-\ntional Conference on Spoken Language Processing,\npages 1695–1699, Sydney, November.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing , pages\n181–184.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ond ˇrej Bojar, Alexan-\ndra Constantin, and Evan Herbst. 2007. Moses:\nOpen source toolkit for statistical machine transla-\ntion. In Annual Meeting of the Association for Com-\nputational Linguistics (ACL), Prague, Czech Repub-\nlic, June.\nPhilipp Koehn. 2005. Europarl: A parallel corpus\nfor statistical machine translation. In Proceedings\nof MT Summit.\nRobert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword ﬁfth edi-\ntion, June. LDC2011T07.\nAbhinav Sethy, Stanley Chen, Bhuvana Ramabhadran,\nand Paul V ozila. 2014. Static interpolation of ex-\nponential n–gram models using features of features.\nIn 2014 IEEE International Conference on Acoustic,\nSpeech and Signal Processing (ICASSP).\nAndreas Stolcke, Jing Zheng, Wen Wang, and Vic-\ntor Abrash. 2011. SRILM at sixteen: Update\nand outlook. In Proc. 2011 IEEE Workshop on\nAutomatic Speech Recognition & Understanding\n(ASRU), Waikoloa, Hawaii, USA.\nAndreas Stolcke. 2002. SRILM - an extensible lan-\nguage modeling toolkit. In Proceedings of the Sev-\nenth International Conference on Spoken Language\nProcessing, pages 901–904.\nDavid Talbot and Miles Osborne. 2007. Randomised\nlanguage modelling for statistical machine trans-\nlation. In Proceedings of ACL , pages 512–519,\nPrague, Czech Republic.\n885\nJ¨org Tiedemann. 2009. News from OPUS - A col-\nlection of multilingual parallel corpora with tools\nand interfaces. In N. Nicolov, K. Bontcheva,\nG. Angelova, and R. Mitkov, editors, Recent\nAdvances in Natural Language Processing , vol-\nume V , pages 237–248. John Benjamins, Amster-\ndam/Philadelphia, Borovets, Bulgaria.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale\nneural language models improves translation. In\nProceedings of EMNLP.\nMitch Weintraub, Yaman Aksu, Satya Dharanipragada,\nSanjeev Khudanpur, Hermann Ney, John Prange,\nAndreas Stolcke, Fred Jelinek, and Liz Shriberg.\n1996. LM95 project report: Fast training and\nportability. Research Note 1, Center for Language\nand Speech Processing, Johns Hopkins University,\nFebruary.\nEdward D. W. Whittaker and Dietrich Klakow. 2002.\nEfﬁcient construction of long-range language mod-\nels using log-linear interpolation. In 7th Interna-\ntional Conference on Spoken Language Processing,\npages 905–908.\nEdward Whittaker and Bhiksha Raj. 2001.\nQuantization-based language model compres-\nsion. In Proceedings of Eurospeech, pages 33–36,\nAalborg, Denmark, December.\nHui Zhang and David Chiang. 2014. Kneser-Ney\nsmoothing on expected counts. In Proceedings of\nthe 52nd Annual Meeting of the Association for\nComputational Linguistics, pages 765–774. ACL.\n886",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9391953945159912
    },
    {
      "name": "Interpolation (computer graphics)",
      "score": 0.7663552761077881
    },
    {
      "name": "Linear interpolation",
      "score": 0.6393614411354065
    },
    {
      "name": "Computer science",
      "score": 0.5583667159080505
    },
    {
      "name": "Trilinear interpolation",
      "score": 0.5043319463729858
    },
    {
      "name": "Algorithm",
      "score": 0.43751460313796997
    },
    {
      "name": "Linear model",
      "score": 0.41654258966445923
    },
    {
      "name": "Applied mathematics",
      "score": 0.3972132205963135
    },
    {
      "name": "Mathematics",
      "score": 0.37908071279525757
    },
    {
      "name": "Mathematical optimization",
      "score": 0.32063597440719604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19667893648147583
    },
    {
      "name": "Language model",
      "score": 0.17766204476356506
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.11678430438041687
    },
    {
      "name": "Machine learning",
      "score": 0.09379991888999939
    },
    {
      "name": "Image (mathematics)",
      "score": 0.05715659260749817
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 4
}