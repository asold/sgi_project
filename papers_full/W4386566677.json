{
  "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
  "url": "https://openalex.org/W4386566677",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2286099440",
      "name": "Abhijeet Awasthi",
      "affiliations": [
        "Indian Institute of Technology Bombay"
      ]
    },
    {
      "id": "https://openalex.org/A2123358686",
      "name": "Nitish Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2740391093",
      "name": "Bidisha Samanta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131025052",
      "name": "Shachi Dave",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A156875573",
      "name": "Sunita Sarawagi",
      "affiliations": [
        "Indian Institute of Technology Bombay"
      ]
    },
    {
      "id": "https://openalex.org/A2248318254",
      "name": "Partha Talukdar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2892248135",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2734443755",
    "https://openalex.org/W3099744315",
    "https://openalex.org/W3098101716",
    "https://openalex.org/W3185414019",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3154046074",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4285219589",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W3085479580",
    "https://openalex.org/W3183587160",
    "https://openalex.org/W3213752920",
    "https://openalex.org/W3101642036",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4285180332",
    "https://openalex.org/W3099192675",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385572438",
    "https://openalex.org/W4226397843",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3175603587",
    "https://openalex.org/W4386576744",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W1574273296",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W4320500110",
    "https://openalex.org/W2949555952",
    "https://openalex.org/W4300506197"
  ],
  "abstract": "Abhijeet Awasthi, Nitish Gupta, Bidisha Samanta, Shachi Dave, Sunita Sarawagi, Partha Talukdar. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2455–2467\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nBootstrapping Multilingual Semantic Parsers\nusing Large Language Models\nAbhijeet Awasthi1∗ Nitish Gupta2 Bidisha Samanta2\nShachi Dave2 Sunita Sarawagi1 Partha Talukdar2\n1Indian Institute of Technology Bombay,2Google Research India\n{awasthi,sunita}@cse.iitb.ac.in\n{guptanitish,bidishasamanta,shachi,partha}@google.com\nAbstract\nDespite cross-lingual generalization demon-\nstrated by pre-trained multilingual models, the\ntranslate-train paradigm of transferring En-\nglish datasets across multiple languages re-\nmains to be a key mechanism for training task-\nspeciﬁc multilingual models. However, for\nmany low-resource languages, the availability\nof a reliable translation service entails signiﬁ-\ncant amounts of costly human-annotated trans-\nlation pairs. Further, translation services may\ncontinue to be brittle due to domain mismatch\nbetween task-speciﬁc input text and general-\npurpose text used for training translation mod-\nels. For multilingual semantic parsing, we\ndemonstrate the effectiveness and ﬂexibility\noffered by large language models (LLMs) for\ntranslating English datasets into several lan-\nguages via few-shot prompting. Through ex-\ntensive comparisons on two public datasets,\nMTOP and MASSIVE, spanning 50 languages\nand several domains, we show that our method\nof translating data using LLMs outperforms a\nstrong translate-train baseline on 41 out of 50\nlanguages. We study the key design choices\nthat enable more effective multilingual data\ntranslation via prompted LLMs.\n1 Introduction\nEnabling language technologies across several lan-\nguages is an important goal for serving a diverse\nrange of users in an inclusive manner. Recent ad-\nvances in large-scale self-supervised multilingual\nlanguage models hold immense promise in bridg-\ning the quality gap that currently exists between En-\nglish and many other low resource languages (Con-\nneau et al., 2020; Brown et al., 2020; Xue et al.,\n2021). Even though multilingual models exhibit\ncross-lingual generalization, getting meaningful\nperformance across several languages still requires\nsigniﬁcant amounts of task-speciﬁc labeled data.\nWe consider the problem of automatically syn-\nthesizing semantic parsing datasets across several\n∗Work done during an internship at Google Research\nlanguages. Semantic parsing (Zelle and Mooney,\n1996; Zettlemoyer and Collins, 2005; Berant et al.,\n2013) is the task of mapping natural language\ntext into an executable logical-form. For exam-\nple, given a user instruction ( x) : “Wake me\nup by 5 am” , mapping it to the logical-form\n(y): [IN:CREATE_ALARM [SL:DATE_TIME 5 am ]].\nManual annotation of queries with their logical\nforms requires human expertise which makes data\ncollection across multiple languages challenging.\nA common approach to automatic multilingual\ndataset creation is translating existing English\ndatasets into target languages. Prior methods uti-\nlize an off-the-shelf machine translation model for\ntranslating the English utterance into the target lan-\nguage xeng →xtgt, followed by projecting lan-\nguage speciﬁc components in the English logical-\nform yeng to obtain the logical-form ytgt in the tar-\nget language (Moradshahi et al., 2020, 2021; Xia\nand Monti, 2021; Nicosia et al., 2021; Gritta et al.,\n2022; Wang et al., 2022). The projection step is\noften learned independent of the translation service,\nresulting in poor generalization across languages.\nIn this work we aim to utilize the few-shot gen-\neralization abilities exhibited by large language\nmodels (LLMs) (Brown et al., 2020; Chowdh-\nery et al., 2022; Scao et al., 2022) for bootstrap-\nping semantic parsing datasets across ﬁfty lan-\nguages. We propose a recipe of using LLMs\nto translate an English semantic parsing dataset\ncontaining (utterance, logical-form) pairs:\nDeng = {(xi\neng, yi\neng)}into a corresponding dataset\nin a target language: Dtgt = {(xi\ntgt, yi\ntgt)}. The gen-\nerated dataset Dtgt is then used to train a semantic\nparser in the target language. Our method uses\na small amount of manually translated semantic\nparsing examples to teach the LLM how to trans-\nlate English examples in the target language via\nin-context learning (Min et al., 2022).\nFigure 1 describes our data-translation pipeline\nwhich we refer to as LLM-T (§ 3). In contrast\n2455\nFigure 1: Proposed semantic parsing data translation pipeline using LLMs (§ 3): With the help of human\ntranslators, we ﬁrst collect translations of a small seed set of English examples in the Target Language (e.g. Hindi;\n§ 3.1). Given a new English example, a small subset from this initial seed set of examples with their respective\ntranslations is chosen to prompt the LLM (§ 3.2). The prompted LLM translates the given English example in\nthe Target Language. We repeat this process for each example in the English training data to generate a training\ndataset in the Target Language. To ensure high-quality of the resulting dataset, we generate diverse translations via\ntop-p (nucleus) sampling (§ 3.3) and apply consistency ﬁltering (§ 3.4).\nto prior translation based methods that involved\na two-staged process requiring different modules,\nour method uses the LLM to jointly translate\nan English (xeng, yeng) pair directly into the tar-\nget language (xtgt, ytgt). We identify two impor-\ntant choices that make the LLM translated data\nmore effective for training a downstream parser:\n(i) Sampling diverse translations (§ 3.3): De-\ncoding translations using top-p (Fan et al., 2018)\nand top-k (Holtzman et al., 2019) sampling leads\nto improved downstream performance compared\nto using greedy decoding. Sampling multiple di-\nverse translations per example further improves the\ndownstream performance; (ii) Filtering inconsis-\ntent examples (§ 3.4): Decoding via sampling can\nresult in noisy joint translations of the (utterance,\nlogical-form) pairs. To ﬁlter out the inconsistent\npairs, we propose a slot-value match based ﬁltering\ntechnique that improves the training data quality.\nWe perform experiments on two multilingual se-\nmantic parsing datasets: MTOP (Li et al., 2021)\nand MASSIVE (FitzGerald et al., 2022). On 4\nout of 5 languages in MTOP and 41 out of 50 lan-\nguages in MASSIVE, our method LLM-T outper-\nforms TAF (Nicosia et al., 2021), a strong baseline\nthat utilizes a supervised translation service (§ 5.1).\nFurther, we see that LLM-T achieves 93% of the\nperformance obtained by “fully-supervised” mod-\nels that use 30×more manually translated exam-\nples (§ 5.2). We justify the importance of generat-\ning multiple translations using sampling, ﬁltering\nout inconsistent examples, and using larger-sized\nLLMs in improving translated data quality (§ 5.3).\nFinally, we perform an error analysis of our parser\nand show the key sources of disagreements between\nthe model predictions and the ground truth (§ 5.4).\n2 Background\nIn this section, we provide an overview of semantic\nparsing and prior translation-based methods for\ncreating multilingual semantic parsing datasets.\n2.1 Semantic Parsing\nSemantic parsing is the task of mapping text\nqueries to their meaning representations or logical\nforms (Zelle and Mooney, 1996; Zettlemoyer\nand Collins, 2005; Berant et al., 2013). We focus\non task-oriented semantic parsing (Gupta et al.,\n2018) where the user utterance needs to be parsed\ninto a high-level intent specifying the overall goal,\nand ﬁne-grained slots containing details about\nthe utterance. The intents and slots come from a\ntask-speciﬁc vocabulary. For example, given an\nutterance x: “How is the rainfall today?” ,\nthe parser should generate the logical-form\ny: [IN:GET_WEATHER [SL:ATTRIBUTE rainfall]\n[SL:DATE today ] ]\nHere, IN:GET_WEATHER is the high-level intent,\nSL:ATTRIBUTEand SL:DATEare the slots that spec-\nify details about the intent. We refer to the logical-\n2456\nform with its slot values removed as its \"signature\".\nFor example, the signature of y is\n[IN:GET_WEATHER [SL:ATTRIBUTE][SL:DATE]]\n2.2 Translating Semantic Parsing Datasets\nGiven an English semantic parsing dataset con-\ntaining (utterance, logical-form) pairs Deng =\n{(xi\neng, yi\neng)}, many methods aim to translate Deng\nto a dataset Dtgt = {(xi\ntgt, yi\ntgt)}in the target lan-\nguage (tgt). Here xi\ntgt is the translation of xi\neng,\nand yi\ntgt is the logical form grounded in the trans-\nlated utterance xi\ntgt. Target logical form yi\ntgt has the\nsame signature as yi\neng and only differs in terms\nof the translated slot values. Most translation\nbased approaches (Moradshahi et al., 2020, 2021;\nXia and Monti, 2021; Nicosia et al., 2021) trans-\nlate an English example (xi\neng, yi\neng) to the corre-\nsponding target language example (xi\ntgt, yi\ntgt) via a\ntwo step process: (i) Translate: Use a supervised\ntranslation service to convert the English utterance\nxi\neng into the target language utterance xi\ntgt; and\n(ii) Project: Replace the English slot values in yi\neng\nwith spans copied from the translated utterance\nxi\ntgt via a learned alignment model. The translated\nexamples are then used to train a downstream mul-\ntilingual semantic parser. For example, Nicosia\net al. (2021) implement the project step by training\na ﬁller module on English data to ﬁll slot-values\nin a logical-form signature by copying spans from\nthe utterance. During inference, the trained ﬁller\nmodule is then used in a zero-shot manner to ﬁll\nlogical-form signatures with spans copied from the\ntranslated utterances.\n3 Our Method: Prompting LLMs for\nDataset Translation\nOur goal is to learn a multilingual semantic parser\ncapable of parsing user queries in many languages.\nTowards this goal, we propose a method for gen-\nerating multilingual training datasets via few-shot\nprompting of an LLM to translate existing English\ndatasets into several languages.\nIn contrast to prior approaches, we jointly\nperform example translation by prompting an\nLLM with a few exemplars of translating English\n(xeng, yeng) pairs to target language (xtgt, ytgt) pairs.\nFigure 1 describes our data-translation method\nwhich we refer to as LLM-T. With the help of\nhuman translators we ﬁrst collect a small seed set\nof exemplar translations used for prompting the\nLLM (§ 3.1). Given an input English example, we\ndynamically construct the LLM prompt by identify-\ning a relevant subset of seed exemplars (§ 3.2). The\nLLM translates the English example into the target\nlanguage by in-context learning from the exem-\nplars provided in the prompt. Instead of decoding\nthe most likely translation, we generate multiple\ndiverse translations (§ 3.3) using top-p (nucleus)\nsampling (Holtzman et al., 2019). While sampling\nimproves the text diversity, it can lead to more noisy\ngenerations. We ﬁlter out the noisy generations us-\ning a simple string-match based technique before\ntraining a parser on the translated data (§ 3.4).\n3.1 Selecting Seed Exemplars for Translation\nGiven an English semantic parsing dataset Deng =\n{(xi\neng, yi\neng)}, we ﬁrst want to identify a small seed\nset Seng ⊂Deng that will be translated into the tar-\nget language (Stgt) with the help of human transla-\ntors. The examples in Seng and their corresponding\ntranslations in Stgt will be used for prompting the\nLLM. Therefore, the choice of the seed examples\nin Seng that are manually translated into Stgt be-\ncomes important—we would like that the multiple\ndomains (e.g. Alarms, Music, News, Weather,\netc.) and the intents and slot types in each do-\nmain are covered. This ensures that for a given\nEnglish example to be translated, we will be able\nto prompt the LLM in a manner such that at least\none of the few-shot exemplars will share the intent\nand slots with the test English example. In practice,\nwe select seed examples in a manner to cover all\nthe intents and slots in a domain at least once. If\nthe selected examples are less than 20 for a domain,\nwe select the remaining examples randomly.\n3.2 Constructing the Prompt using\nTranslation Pairs in the Seed Sets\nLLM inference is constrained by the maximum\nnumber of tokens in the input. Hence, we can\nonly ﬁt a limited number of examples to construct\nthe LLM prompt. The choice of prompt examples\nand their ordering is known to signiﬁcantly impact\nthe quality of the generations (Kumar and Taluk-\ndar, 2021; Rubin et al., 2021; Lu et al., 2022). To\nimprove the likelihood of correctly translating an\nEnglish example (xeng, yeng), we retrieve seed ex-\namples {(xs\neng, ys\neng, xs\ntgt, ys\ntgt)}that share the same\ndomain with yeng. To bias the LLM further, we\norder the more relevant prompt examples closer\nto the input English example. Here, relevance be-\ntween two examples is considered higher if they\nshare the same intent. The remaining examples are\n2457\nFigure 2: Constructing the LLM Prompt(§ 3.2): The\ninput to the LLM contains a brief task description in\nthe beginning followed by a series of English examples\n(xs\neng, ys\neng) and their translations in the target language\n(xs\ntgt, ys\ntgt) chosen from the seed setsSeng and Stgt respec-\ntively. Following the prompt examples, we append the\nnew English example (xeng, yeng) to the input prompt\nwhich is fed to LLM. In the output, the LLM generates\nthe translation for the new English example (xtgt, ytgt).\narbitrarily arranged to appear earlier in the prompt.\nFigure 2 shows an example translation—the LLM\ninput contains two exemplars and then the English\nexample that needs to be translated. The LLM out-\nput shows the translated output from the LLM.\n3.3 Decoding Diverse Outputs from LLM\nThe text decoded from language models using the\nstandard greedy decoding or beam search is often\nrepetitive (Vijayakumar et al., 2016; Shao et al.,\n2017). To mimic how users express the same inten-\ntions in diverse ways, we experiment with thetop-k\nand top-p sampling techniques (Fan et al., 2018;\nHoltzman et al., 2019) to decode multiple diverse\ntranslations per example. We expect sampling mul-\ntiple translations to yield a better quality training\ndataset which in turn should result in better down-\nstream semantic parsing performance compared to\ntraining on greedily decoded examples.\n3.4 Data Filtering using Slot-Consistency\nWhile the sampling techniques produce more di-\nverse text, the sampled translations can be rela-\ntively noisy if they have lower likelihoods as per\nthe model (Zhang et al., 2021). Thus, the trans-\nlated pairs (xtgt, ytgt) in the LLM output can be\nFigure 3: Slot Consistency Based Filtering (§ 3.4):\nWe present the input English example (xeng, yeng) and\nits four translated samples {(xi\ntgt, yi\ntgt)}) the target lan-\nguage. The ﬁrst two samples are slot-consistent as the\nslot-values (in green) in the logical forms appear ex-\nactly in the text utterances, while the last two samples\nare slot-inconsistent as the slot-values (in red) do not\nappear as an exact sub-string of the text utterance.\ninconsistent w.r.t. each other. For example, con-\nsider the LLM translated pair (x3\ntgt, y3\ntgt) shown in\nFigure 3. Here, y3\ntgt contains a slot value (in red)\nthat does not appear in the corresponding utterance\nx3\ntgt making the pair (x3\ntgt, y3\ntgt) inconsistent. As per\nthe task deﬁnition, for a given example (x, y), the\nslot-values in the logical form y should come from\nthe spans of the utterance x. Thus, we ﬁlter out\nthe translated examples (xtgt, ytgt) like these where\nthe slot-values in ytgt do not appear exactly as an\nexact sub-span in xtgt. Figure 3 shows examples of\nslot-consistent and slot-inconsistent generations by\nan LLM through top-k sampling.\n4 Experimental Set-up\nWe describe our experimental setup in this section.\nDatasets We experiment on two public datasets\n— MTOP (Li et al., 2021) andMASSIVE (FitzGer-\nald et al., 2022). MTOP contains examples from\nsix languages: English, French, German, Hindi,\nSpanish, and Thai, spanning 11 domains covering\n117 intents and 78 slot types. On average, MTOP\ncontains 12.3K examples in the train split, 1.5K in\nthe dev split, and 2.7K in the test split per language.\nMASSIVE contains examples from 51 typologi-\ncally diverse languages including English spanning\n18 domains covering 60 intents and 50 slot types.\nFor each language, MASSIVE contains roughly\n11.5K examples in the train split, 2K examples in\nthe dev split and 3K examples in the test split.\n2458\nEvaluation Metric Prior work (Li et al., 2021;\nNicosia et al., 2021) uses Exact Match (EM) accu-\nracy as a primary metric which compares predicted\nand gold logical-forms strings. However, the exact\nstring-match penalizes correct predictions where\nthe order of slots within an intent is different. For\nexample, consider the following logical-forms:\nLF-1: [IN:GET_WEATHER [SL:ATTRIBUTE rainfall]\n[SL:DATE today ] ]\nLF-2: [IN:GET_WEATHER [SL:DATE today][SL:ATTRIBUTE\nrainfall ] ]\nLF-1 and LF-2 are equivalent but the difference in\nthe ordering of slots results in a negative match.\nThus, we correct the EM metric by making the\nmatch function agnostic to the ordering of slots\nwithin an intent in the logical-form. We compare\ndifferent models as per this corrected EM metric.\nSemantic Parsing Model We use a pre-trained\nmT5-Large checkpoint (1.2B parameters) to initial-\nize the downstream semantic parsing models that\nmap utterances in the input to logical-forms in the\noutput. We ﬁnetune the mT5 model on the original\nEnglish dataset mixed with the translated datasets\nin target languages. We train using the Adafactor\noptimizer (Shazeer and Stern, 2018) with a ﬁxed\nlearning rate of 1e−3 and a batch size of 256, for\n30K steps using the T5X library (Roberts et al.,\n2022) on 64 TPU-v3 chips. Examples from each\nlanguage are sampled uniformly for batch creation.\nFor model selection, we choose the best perform-\ning checkpoint as per the dev splits and report our\nresults on the test splits.\nLLM-T (Our Method) We experiment with 8B,\n62B, and 540B sized variants of PaLM (Chowdh-\nery et al., 2022) as our LLM, and primarily utilize\nLLM-540B for translating English examples in dif-\nferent languages. For the seed set Stgt used for\nprompting the LLM, we borrow roughly 250 ex-\namples covering 11 domains from MTOP ’s train\nset and 350 examples covering 18 domains from\nMASSIVE’s train set (§ 3.1). During decoding,\nwe sample 8 translations per example using top-p\nsampling (§ 3.3), with p = 0.95 and temperature\nscaling T = 0.7, followed by ﬁltering out slot-\ninconsistent examples (§ 3.4). We present an anal-\nysis of our design choices in § 5.3.\nBaselines (i) Zero-Shot: Train the model only\non the English data and evaluate on other languages\nin a zero-shot manner. (ii) Few-Shot: In addi-\ntion to the English training data, use the seed set\nof examples Stgt for each language during train-\ning. For MTOP , |Stgt|≈ 250 and for MASSIVE,\n|Stgt|≈ 350. (iii) TAF: We implement the method\nfrom Nicosia et al. (2021) that uses an off-the-shelf\ntranslation service (§ 2.2) to construct Dtgt in all\nthe target languages. We borrow Dtgt from Nicosia\net al. (2021) for MTOP and from Nicosia and Pic-\ncinno (2022) for MASSIVE.\n5 Results and Analysis\nWe ﬁrst present downstream performance of seman-\ntic parsing models trained on data generated by our\nmethod (§ 5.1) and compare with zero-shot setting,\nfew-shot setting, and the TAF method (Nicosia\net al., 2021). We then compare our method against\nthe “full-shot” skyline where we utilize the origi-\nnal training datasets that were manually translated\nwith the help of human annotators in the target\nlanguages (§ 5.2). We then present an analysis\nof different design choices that result in effective\ndata translation using LLM-T (§ 5.3). Finally, we\npresent an error analysis to show the key sources of\ndisagreements between the parser predictions and\nthe ground truth (§ 5.4). All the experiments use\nour corrected EM metric (§ 4; Evaluation Metric).\n5.1 Evaluation on MTOP and MASSIVE\nIn Table 1, we compare performance of different\nmethods for the 5 non-English languages in the\nMTOP dataset. The Zero-Shot baseline trains an\nmT5 model only on the English part of the train-\nsplit. The Few-Shot baseline additionally includes\nthe human translated seed sets Stgt for each lan-\nguage. Both TAF and LLM-T train on the original\nEnglish train set mixed with their respective trans-\nlated datasets in each language. As all the baselines\nutilize the original English train set, we see com-\nparable performance on English (around 85.0 EM).\nWe observe LLM-T outperforms TAF in 4 out of 5\nlanguages by 3.6 EM. Since LLM-T uses Stgt for\nprompting, we also mix Stgt with TAF data and still\nobserve that LLM-T improves overTAF+Few-Shot\nby 2.9 EM. On relatively low-resource languages,\nHindi (hi) and Thai ( th), LLM-T leads to much\nlarger improvements over TAF.\nFigure 4 shows the performance difference\nbetween our LLM-T method and TAF for the\nMASSIVE dataset (FitzGerald et al., 2022). On\n41 out of 50 languages, we ﬁnd LLM-T to be bet-\nter than TAF. For nine languages LLM-T outper-\nforms TAF by more than 5.0 EM—Simple Man-\n2459\nFigure 4: EM accuracy difference between LLM-T and T AF across the 50 languages in MASSIVE dataset\n(§ 5.1). LLM-T outperforms T AF on 41 out of 50 languages, with gains of more than 5 EM for nine of these\nlanguages. Only for Hebrew (he), LLM-T performs worse than TAF by more than 3 EM.\nMethod de es fr hi th Avg\nZero-Shot 54.4 57.8 62.8 42.3 42.1 51.9\nFew-Shot 62.8 69.5 65.9 55.3 53.9 61.5\nTAF 75.0 74.9 78.0 63.0 60.8 70.3\nTAF + Few-Shot75.1 74.5 78.5 63.9 62.9 71.0\nLLM-T(ours) 74.0 75.4 79.6 72.3 68.0 73.9\nTable 1: EM accuracy comparison on MTOP (§ 5.1):\nData generated using LLM-T yields better performance\non 4 out of 5 languages in MTOP. We observe large\nimprovements for low-resource languages hi and th.\ndarin ( zhc, +11.9), Traditional Mandarin ( zht,\n+10.1), Japanese ( ja, +9.3), Telugu ( te, +6.9),\nMalayalam (ml, +6.6), Kannada (ka, +6.1), Lat-\nvian (lv, +5.7), Tamil (ta, +5.5), and Khmer (km,\n+5.2). Only for Hebrew ( he, −4.0), LLM-T is\nworse by more than 3.0 EM. Averaged across all\nlanguages, LLM-T outperforms TAF by 2.2 EM. In\nAppendix A.1, we provide detailed baseline com-\nparisons for all the 50 languages.\n5.2 Comparison with gold translations\nAn ideal translate-train method should be com-\npetitive w.r.t. training on fully human translated\ndatasets. Table 2 provides a comparison between\ntraining on TAF, LLM-T, and the datasets fully\ntranslated with the help of human annotators in the\ntarget languages (Gold). Between TAF and Gold,\nwe observe a signiﬁcant gap of 9.2 EM in MTOP\nand 6.7 EM in MASSIVE. Our method LLM-T, re-\nduces this gap by 3.6 EM in MTOP and 2.2 EM in\nMASSIVE. Overall, LLM-T achieves roughly 93%\nof the performance obtained by the Gold skyline\nthat use more than 30×human translated examples.\nAppendix A.1, provides per-language comparisons\nDataset Few-Shot T AF LLM-T Gold\nMTOP 61.5 70.3 73.9 79.5\nMASSIVE 55.9 61.0 63.2 67.7\nTable 2: Comparison with Gold skyline (§ 5.2):\nWhile training on the human translated datasets (Gold)\nyields the best performance, LLM-T results in a smaller\nperformance gap compared to TAF. All numbers are av-\neraged over the 5 non-English languages in MTOP.\nwith the Gold skyline for both the datasets.\nDecoding de es fr hi th AvgStrategy\nGreedy 71.1 71.7 72.6 68.1 66.0 69.9\n+ Filtering 72.2 73.5 74.8 71.5 67.4 71.9\nTop-p Sampling(p = 0.95)\n(#samples)\n1 70.1 71.5 74.3 66.9 67.2 70.0\n2 71.4 72.1 74.5 68.8 67.2 70.8\n4 71.1 72.8 76.4 69.0 66.0 71.1\n8 71.9 72.7 74.2 70.0 68.4 71.4\nTop-p Sampling+ Filtering(p = 0.95)\n(#samples)\n1 72.0 75.2 78.9 71.6 68.1 73.2\n2 73.7 75.2 79.5 72.0 67.6 73.6\n4 73.4 75.3 79.0 72.1 67.7 73.5\n8 74.0 75.4 79.6 72.3 68.0 73.9\nTable 3: Impact of decoding strategy and ﬁltering :\nGenerating multiple translations per English example\nusing top- p sampling followed by ﬁltering inconsis-\ntent examples offers superior downstream performance\ncompared to using greedy decoding or sampling just\none translation per example. In Appendix A.2 we\npresent results for top-k sampling as well.\n2460\nMax Len de es fr hi th Avg\n768 73.4 75.4 76.9 73.1 69.7 73.7\n1024 74.0 75.4 79.6 72.3 68.0 73.9\n1792 74.3 75.7 80.5 74.0 71.1 75.1\nTable 4: Impact of prompt length : Longer prompts\ncontaining more exemplars result in more effective\ntranslated datasets yielding higher EM accuracy.\n5.3 Analysis of Design Choices\nWe now present an analysis of the design choices\nthat enabled more effective data translation via\nLLM-T. All the experiments in this section are\ncarried out on the MTOP dataset.\nRole of decoding strategy and ﬁltering In Ta-\nble 3, we present the EM accuracy of parsers\ntrained on datasets translated using various com-\nbinations of decoding (§ 3.3) and ﬁltering (§ 3.4)\nmethods. For generating the translated outputs we\nexperiment with greedy decoding, top-k (Fan et al.,\n2018) and top-p (Holtzman et al., 2019) sampling.\nLike prior translate-train methods, we begin with\nonly one translation per example and observe sam-\npling to be comparable with greedy decoding in\ndownstream EM accuracy. In contrast, decoding\ntwo translations per example via sampling boosts\nthe EM accuracy across all the languages. However,\nfurther increasing the translated samples to 4 and\n8 results in only marginal performance differences.\nManual inspection of the translated data revealed\ninconsistent utterance and logical-form pairs which\nmotivated our design of slot-consistency based ﬁl-\ntering (§ 3.4). Training the parser on ﬁltered data\nprovides further gains over training on unﬁltered\ndata. In Appendix A.2, we also present the results\nfor top-k sampling. Overall, utilizing upto 8 top-p\ntranslated samples per English example followed\nby slot-consistency ﬁltering provides the best per-\nformance averaged over all the languages.\nImpact of Prompt Length We expect prompts\ncontaining more exemplars to yield higher qual-\nity translated examples owing to more information\nfor in-context learning. In Table 4, we compare\nEM performance when using maximum prompt-\nlengths of 768, 1024, and 1792 tokens. Training on\ndatasets translated using prompt-length of 1792\ntokens provides the best downstream EM perfor-\nmance across all the languages. However, longer\nprompts lead to considerably longer inference\ntimes. Hence, we conduct our main experiments\nde es fr hi th Avg\nLLM-T-8B 65.3 69.4 70.7 56.6 55.1 62.0\nLLM-T-62B 72.0 73.3 76.7 68.2 65.6 71.2\nLLM-T-540B74.0 75.4 79.6 72.3 68.0 73.9\nTable 5: Impact of LLM size : EM performance of\nsemantic parsers trained on translated datasets improve\nwith increasing the size of LLMs used for translation.\nwith prompt the length of 1024 tokens.\nRole of LLM size In Table 5, we compare parser\nperformance when trained on data generated by\nLLMs of different sizes. Training on larger LLM\ngenerated data leads to better performance— LLM-\nT-540B yields the best performance on all the lan-\nguages, followed by LLM-T-62B which outper-\nforms LLM-T-8B on all the languages.\n5.4 Error Analysis\nFigure 5: Distribution of error categories: estimated\nacross all ﬁve languages on MTOP’s dev set.\nWe analyze the examples where the predic-\ntions from our semantic parser do not match with\nthe ground truth. In Table 6, we categorize all\nthe erroneous examples into ﬁve broad categories\n(with English examples): (i) Slot Value Mismatch\n(ii) Wrong Intent (iii) Missing Slot (iv) Extra Slot\nand (v) Slot Confusion. Figure 5 presents the distri-\nbution of the error categories aggregated across all\nthe languages on the MTOP dev-split. The \"Slot\nValue Mismatch\" is the most frequent error cate-\ngory (41.1%)—here the predicted parse structure\nis correct but the slot-values do not match perfectly\nwith the gold parse. After manually inspecting 300\nsuch errors we found that in roughly 50% of the\ncases the predicted and gold slot-values often have\nminor mismatches which may not be recognized as\nerror by another human annotator and should not\nlead to incorrect output upon logical form execu-\n2461\nSlot Value Mismatch (41.1%)\nUtterance: Set an alarm for 5 pm tomorrow\nPrediction:[IN:CREATE_ALARM [SL:DATE_TIME for 5 pm ] [SL:DATE_TIME tomorrow ]\nTarget:[IN:CREATE_ALARM [SL:DATE_TIME 5 pm ] [SL:DATE_TIME tomorrow ]\nWrong Intent (19.5%)\nUtterance: What can I do today\nPrediction:[IN:QUESTION_NEWS [SL:DATE_TIME today ]]\nTarget:[IN:GET_EVENT [SL:DATE_TIME today ] ]\nMissing Slot (15.1%)\nUtterance: Play Justin Timberlake ’s newest single\nPrediction:[IN:PLAY_MUSIC [SL:MUSIC_TYPE single ] ]\nTarget:[IN:PLAY_MUSIC [SL:MUSIC_ARTIST_NAME Justin Timberlake ] [SL:MUSIC_TYPE single ] ]\nExtra Slot (14.4%)\nUtterance: play music on the speaker\nPrediction:[IN:PLAY_MUSIC [SL:MUSIC_TYPE music ] [SL:MUSIC_TYPE speaker ] ]\nTarget:[IN:PLAY_MUSIC [SL:MUSIC_TYPE music ] ]\nSlot Confusion (9.9%)\nUtterance: audio call wedding planner please\nPrediction:[IN:CREATE_CALL [SL:CONTACT wedding planner ] ]\nTarget:[IN:CREATE_CALL [SL:GROUP wedding planner ] ]\nTable 6: Examples of Error Categories (§ 5.4) The errors in the predicted parse can be broadly classiﬁed into\nﬁve categories: (i) Slot Value Mismatch: Predicted parse has the correct signature but the slot-values are incorrect,\n(ii) Wrong Intent: High-level intent of the predicted parse is incorrect, (iii) Missing Slot: One or more slots in the\ngold parse do not appear in the output, (iv) Extra Slot: Output contains extra slot(s) compared to the gold, (v) Slot\nConfusion: Prediced parse contains the correct correct intent and number of slots but the wrong slot-types.\ntion. For example, in the ﬁrst row of Table 6, the\npredicted value for the DATE_TIME slot is ‘for 5\npm’, while the target value is just ‘5 pm’.\n6 Related Work\nMultilingual Semantic Parsing Multilingual se-\nmantic parsers are typically initialized with a foun-\ndation model (Bommasani et al., 2021) pre-trained\non vast amounts of multilingual data (Conneau\net al., 2020; Xue et al., 2021; Li et al., 2021;\nFitzGerald et al., 2022) followed by supervised\ntraining on synthetic or real multilingual datasets.\nA standard approach for constructing multilin-\ngual datasets is to translate and localize English\ndatasets with the help of multilingual speakers\nor machine translation. For example, MTOP (Li\net al., 2021), MASSIVE (FitzGerald et al., 2022),\nand MultiAtis++ (Xu et al., 2020) were con-\nstructed by translating TOP (Gupta et al., 2018),\nSLURP (Roberts et al., 2022), and ATIS (Price,\n1990) respectively through human translators.\nMachine Translation based methods Machine\ntranslation based approaches continue to be\nimportant for multilingual task-speciﬁc mod-\nels (Hartrumpf et al., 2008; Liang et al., 2020; Hu\net al., 2020; Fang et al., 2021; Ladhak et al., 2020)\nincluding semantic parsing. Machine translation\ncan either be used during the inference time to\ntranslate a user query into English for feeding it\nto an English-only model. This approach is re-\nferred to as translate-test (Artetxe et al., 2020;\nUhrig et al., 2021). A more common way of using\nmachine translation is in the form of data augmen-\ntation, referred as translate-train where English\ntext in training data is translated into several lan-\nguages (Sherborne et al., 2020; Moradshahi et al.,\n2020, 2021; Xia and Monti, 2021; Nicosia et al.,\n2021; Gritta et al., 2022; Wang et al., 2022). In\npractice, translate-train methods tend to outper-\nform translate-test methods while also reducing\nthe latency associated with translating text during\nthe inference time (Yang et al., 2022).\nLLMs and Few-Shot learning Trans-\nformer (Vaswani et al., 2017) based generative\nLLMs (Radford et al., 2019; Brown et al., 2020;\nThoppilan et al., 2022; Soltan et al., 2022; Smith\net al., 2022; Zhang et al., 2022; Chowdhery et al.,\n2022) trained on massive amounts of web-scale\ntext corpora using next token prediction objective\nexhibit strong few-shot generalization abilities.\nWhen prompted with a task description and a\nhandful of task-speciﬁc examples, LLMs can often\nmatch the performance of ﬁnetuned models via\nin-context learning (Xie et al., 2021; Min et al.,\n2022; Wei et al., 2022; Zhou et al., 2022). We\nutilize LLMs for translating English datasets in\nseveral languages using few-shot prompting.\n7 Conclusion\nWe present a method of utilizing large language\nmodels (LLMs) for bootstrapping multilingual se-\nmantic parsers across several languages. In compar-\nison to using off-the-shelf translation services that\nrely on signiﬁcant amounts of human supervision,\nwe demonstrate that prompting self-supervised\nLLMs can be a more effective and scalable alter-\n2462\nnative for dataset translation. We ﬁnd that gen-\nerating multiple diverse translations using sam-\npling techniques followed by consistency-based\nﬁltering make the translated datasets more effec-\ntive for training multilingual semantic parsers. On\n41 out of 50 typologically diverse languages within\ntwo large datasets spanning several domains, our\nmethod outperforms a strong translate-train method\nthat utilizes a supervised translation service.\n8 Limitations\nWhile translating English queries in different lan-\nguages is a useful form of data augmentation, we\nthink that further performance improvements can\nbe obtained by careful localization of entities in the\ntext queries. This will result in examples where\nthe training dataset contains entities that are often\ntalked about in the target language and might lead\nto less train-test domain shift. LLMs contain lan-\nguage speciﬁc priors which can be harnessed to per-\nform such localization of the translated queries thus\nenabling more realistic data augmentations. In this\nwork we presented a simple string-match based ﬁl-\ntering technique to remove noisy translations. Data\nﬁltering can be further improved with the help of\nlearned models. We observed that larger LLMs are\nimportant to generate more effective translated data.\nHowever running these experiments is constrained\nby the availability of large amounts of compute\nresources. We hope future work will address these\nlimitations of our approach.\n9 Ethical Considerations\nWe utilize large language models to translate\ndatasets initially available in English into several\nlanguages. The real-world deployment of models\ntrained on LLM-translated data should undergo a\ncareful review of any harmful biases. However,\nthe LLM-translated data and the logical-forms gen-\nerated by a semantic parser are not user-facing,\nthus a smaller risk of any direct harms. The in-\ntended users of any semantic parsing model must\nbe made aware that the answers returned by the\nmodel could be incorrect, more so for user-queries\nin low-resource languages. We do not immediately\nforesee any serious negative implications of the\nspeciﬁc contributions that we make in this work.\n10 Acknowledgment\nWe thank Rahul Goel, Slav Petrov, and Kartikeya\nBadola for discussions and their feedback on an ear-\nlier draft of this paper. We thank Massimo Nicosia\nfor sharing the TAF translated datasets and helpful\ndiscussions during this project.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020.\nTranslation artifacts in cross-lingual transfer learn-\ning. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7674–7684.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural lan-\nguage processing, pages 1533–1544.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities\nand risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898.\n2463\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun,\nand Jingjing Liu. 2021. Filter: An enhanced fu-\nsion method for cross-lingual language understand-\ning. In Proceedings of the AAAI Conference on Arti-\nﬁcial Intelligence, volume 35, pages 12776–12784.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\net al. 2022. Massive: A 1m-example multilin-\ngual natural language understanding dataset with\n51 typologically-diverse languages. arXiv preprint\narXiv:2204.08582.\nMilan Gritta, Ruoyu Hu, and Ignacio Iacobacci. 2022.\nCrossAligner & co: Zero-shot transfer methods for\ntask-oriented cross-lingual natural language under-\nstanding. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022 , pages 4048–4061,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nSonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-\nmar, and Mike Lewis. 2018. Semantic parsing for\ntask oriented dialog using hierarchical representa-\ntions. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2787–2792.\nSven Hartrumpf, Ingo Glöckner, and Johannes Level-\ning. 2008. Efﬁcient question answering with ques-\ntion decomposition and multiple answer streams. In\nWorkshop of the Cross-Language Evaluation Forum\nfor European Languages, pages 421–428. Springer.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n4411–4421. PMLR.\nSawan Kumar and Partha Talukdar. 2021. Reorder-\ning examples helps during priming-based few-shot\nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4507–4518.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summa-\nrization. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4034–\n4048, Online. Association for Computational Lin-\nguistics.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 2950–2962, Online. Association for\nComputational Linguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Daniel Campos, Ran-\ngan Majumder, and Ming Zhou. 2020. XGLUE: A\nnew benchmark datasetfor cross-lingual pre-training,\nunderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2022. Fantastically\nordered prompts and where to ﬁnd them: Overcom-\ning few-shot prompt order sensitivity. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8086–8098.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nMehrad Moradshahi, Giovanni Campagna, Sina Sem-\nnani, Silei Xu, and Monica Lam. 2020. Localizing\nopen-ontology QA semantic parsers in a day using\nmachine translation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5970–5983, On-\nline. Association for Computational Linguistics.\nMehrad Moradshahi, Victoria Tsai, Giovanni Cam-\npagna, and Monica S Lam. 2021. Contextual seman-\ntic parsing for multilingual task-oriented dialogues.\narXiv preprint arXiv:2111.02574.\nMassimo Nicosia and Francesco Piccinno. 2022. Eval-\nuating byte and wordpiece level models for mas-\nsively multilingual semantic parsing. arXiv preprint\narXiv:2212.07223.\nMassimo Nicosia, Zhongdi Qu, and Yasemin Altun.\n2021. Translate & ﬁll: Improving zero-shot mul-\ntilingual semantic parsing with synthetic data. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 3272–3284.\nP. J. Price. 1990. Evaluation of spoken language sys-\ntems: the ATIS domain. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Hidden\nValley, Pennsylvania, June 24-27,1990.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n2464\nAdam Roberts, Hyung Won Chung, Anselm Lev-\nskaya, Gaurav Mishra, James Bradbury, Daniel An-\ndor, Sharan Narang, Brian Lester, Colin Gaffney,\nAfroz Mohiuddin, et al. 2022. Scaling up mod-\nels and data with t5x and seqio . arXiv preprint\narXiv:2203.17189.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nYuanlong Shao, Stephan Gouws, Denny Britz, Anna\nGoldie, Brian Strope, and Ray Kurzweil. 2017. Gen-\nerating high-quality and informative conversation re-\nsponses with sequence-to-sequence models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2210–\n2219.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning ,\npages 4596–4604. PMLR.\nTom Sherborne, Yumo Xu, and Mirella Lapata. 2020.\nBootstrapping a crosslingual semantic parser. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 499–517.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using\ndeepspeed and megatron to train megatron-turing\nnlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990.\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGer-\nald, Rahul Gupta, Wael Hamza, Haidar Khan,\nCharith Peris, Stephen Rawls, Andy Rosenbaum,\nAnna Rumshisky, et al. 2022. Alexatm 20b:\nFew-shot learning using a large-scale multilingual\nseq2seq model. arXiv preprint arXiv:2208.01448.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nSarah Uhrig, Yoalli Garcia, Juri Opitz, and Anette\nFrank. 2021. Translate, then parse! a strong baseline\nfor cross-lingual AMR parsing. In Proceedings of\nthe 17th International Conference on Parsing Tech-\nnologies and the IWPT 2021 Shared Task on Pars-\ning into Enhanced Universal Dependencies (IWPT\n2021), pages 58–64, Online. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2016. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. arXiv preprint arXiv:1610.02424.\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F\nXu, and Graham Neubig. 2022. Mconala: A bench-\nmark for code generation from multiple natural lan-\nguages. arXiv preprint arXiv:2203.08388.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nMenglin Xia and Emilio Monti. 2021. Multilin-\ngual neural semantic parsing for low-resourced lan-\nguages. In The Tenth Joint Conference on Lexical\nand Computational Semantics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2021. An explanation of in-context\nlearning as implicit bayesian inference. In Interna-\ntional Conference on Learning Representations.\nWeijia Xu, Batool Haider, and Saab Mansour. 2020.\nEnd-to-end slot alignment and recognition for cross-\nlingual nlu. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 5052–5063.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nDiyi Yang, Ankur Parikh, and Colin Raffel. 2022.\nLearning with limited text data. In Proceedings of\nthe 60th Annual Meeting of the Association for Com-\nputational Linguistics: Tutorial Abstracts, pages 28–\n31, Dublin, Ireland. Association for Computational\nLinguistics.\nJohn M. Zelle and Raymond J. Mooney. 1996. Learn-\ning to parse database queries using inductive logic\nprogramming. In AAAI/IAAI, pages 1050–1055,\nPortland, OR. AAAI Press/MIT Press.\nLuke S. Zettlemoyer and Michael Collins. 2005. Learn-\ning to map sentences to logical form: Structured\nclassiﬁcation with probabilistic categorial grammars.\nIn Proceedings of the Twenty-First Conference on\nUncertainty in Artiﬁcial Intelligence , UAI’05, page\n658–666, Arlington, Virginia, USA. AUAI Press.\n2465\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and\nArvind Neelakantan. 2021. Trading off diversity and\nquality in natural language generation. In Proceed-\nings of the Workshop on Human Evaluation of NLP\nSystems (HumEval), pages 25–33.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n2466\nA Appendix\nA.1 Additional results\nLang Zero-Shot Few-Shot TAF TAF+Few-ShotLLM-T(top-k) LLM-T(top-p) Gold(skyline)\naf 48.5 59.0 64.5 64.5 66.7 66.3 68.5am 31.0 47.6 58.3 57.4 56.1 55.5 64.6ar 35.9 50.5 57.2 58.0 56.6 57.3 65.5az 39.3 57.1 60.5 60.5 62.6 62.8 68.8bn 40.8 55.4 62.1 61.7 61.1 62.0 68.3cy 26.7 44.8 59.1 58.3 61.1 61.8 65.1da 57.5 62.4 66.2 66.3 69.1 68.5 71.0de 54.3 62.8 67.5 67.7 68.3 68.4 70.4el 47.3 57.8 64.2 65.5 65.2 65.1 68.7en 72.7 71.4 73.5 72.9 73.3 73.4 73.0es 53.4 58.1 64.6 64.6 64.7 64.7 66.6fa 48.8 58.0 63.1 62.9 62.8 63.2 68.1ﬁ 47.5 58.4 65.0 65.3 66.7 67.2 70.9fr 54.6 58.0 65.3 64.9 63.9 63.7 67.1he 35.3 56.1 60.6 61.2 55.3 56.6 68.3hi 40.1 54.4 61.6 62.5 63.1 63.5 66.2hu 44.1 57.1 63.8 63.6 64.5 65.4 69.7hy 39.3 53.8 58.7 59.2 62.3 62.5 67.1id 55.3 60.2 65.5 65.9 66.6 66.0 69.1is 41.3 54.4 62.2 61.5 63.6 63.5 69.5it 52.3 58.6 64.0 63.6 65.2 65.8 67.2ja 45.6 55.1 56.3 56.5 65.6 65.6 67.3jv 34.3 51.7 58.6 60.2 62.0 61.6 66.7ka 36.5 53.4 53.5 54.6 59.2 59.6 65.7km 37.8 51.1 49.1 53.7 55.3 54.3 62.8kn 37.1 49.3 55.0 55.9 57.7 57.2 62.1ko 42.1 56.3 62.2 63.6 62.4 63.5 69.3lv 45.4 56.0 60.4 61.3 66.0 66.1 68.8ml 38.6 53.9 55.5 56.9 62.5 62.1 67.5mn 30.9 51.4 57.6 59.4 59.5 59.2 68.0ms 48.6 58.9 66.2 66.2 65.8 65.7 69.2my 38.1 54.9 60.5 62.3 61.5 60.6 69.6nb 55.2 63.0 67.5 67.7 67.7 67.4 71.0nl 53.1 61.2 67.3 68.5 68.7 68.5 70.5pl 50.5 57.4 61.1 61.4 62.9 62.5 65.6pt 54.9 60.3 65.8 65.7 66.4 66.9 68.5ro 51.2 58.8 65.4 65.0 64.8 65.1 68.8ru 42.3 59.4 63.0 63.1 66.6 66.2 69.4sl 46.0 57.8 63.1 64.0 65.3 65.4 68.8sq 41.0 55.4 60.3 60.4 62.1 61.7 67.3sv 57.2 63.1 69.8 69.6 69.3 68.9 72.4sw 35.7 52.3 57.9 57.5 60.9 60.6 65.3ta 37.2 53.0 55.4 55.7 60.7 60.9 65.8te 38.7 49.0 51.6 53.6 56.8 58.5 61.6th 49.4 60.0 63.5 66.5 65.2 65 71.5tl 48.4 55.7 64.1 64.2 65.2 64.8 67.5tr 46.7 58.5 63.7 63.4 62.7 62.8 69.4ur 38.9 51.2 60.4 60.6 62.2 61.9 64.6vi 46.9 55.1 59.0 59.2 63.0 63.3 67.6zhc 34.7 56.1 52.0 53.9 64.2 63.9 66.3zht 35.2 51.8 50.5 52.3 60.7 60.6 63.6\nAvg 43.8 55.9 61.0 61.6 63.2 63.2 67.7\nTable A1: EM accuracy comparison on MASSIVE\ndataset. Avg reports the EM accuracy averaged across\nthe 50 non-English languages\nLang Zero-Shot Few-Shot TAF TAF+Few-ShotLLM-T(top-k) LLM-T(top-p) Gold(skyline)\nde 54.4 62.8 75.0 75.1 73.7 74.0 78.5es 57.8 69.5 74.9 74.5 75.2 75.4 82.9fr 62.8 65.9 78.0 78.5 79.7 79.6 80.8hi 42.3 55.3 63.0 63.9 72.5 72.3 78.5th 42.1 53.9 60.8 62.9 66.8 68.0 77.0en 84.1 84.0 85.2 85.0 85.2 85.1 85.4\nAvg 51.9 61.5 70.3 71.0 73.6 73.9 79.5\nTable A2: EM accuracy comparison on MTOP dataset.\nAvg reports the EM accuracy averaged across the 5 non-\nEnglish languages\nIn Table A1, we present detailed baseline com-\nparisons for all the 51 languages in the MASSIVE\ndataset. Zero-Shot, Few-Shot, TAF, and TAF+Few-\nShot are the baselines described in Section 4.LLM-\nT represents our method with top-k or top-p sam-\npling used while decoding the translated exam-\nples. Gold is the \"full-shot\" skyline which utilizes\nthe original human-translated datasets (§ 5.2). Ta-\nble A2 presents the same set of results for the six\nlanguages in the MTOP dataset.\nA.2 Role of decoding strategy and ﬁltering\nIn Table A3 we present results for different de-\ncoding strategies and role of ﬁltering inconsistent\nexamples as discussed in Section 5.3.\nDecoding de es fr hi th AvgStrategy\nGreedy 71.1 71.7 72.6 68.1 66.0 69.9\n+ Filtering 72.2 73.5 74.8 71.5 67.4 71.9\nTop-k Sampling(k = 40)\n(#samples)\n1 70.5 71.7 73.1 66.8 66.5 69.6\n2 72.3 72.7 75.7 68.7 67.3 71.3\n4 71.3 73.1 73.8 68.5 67.8 70.9\n8 71.1 72.5 74.2 69.3 67.5 70.9\nTop-k Sampling+ Filtering(k = 40)\n(#samples)\n1 72.4 74.4 78 70.9 66.1 72.4\n2 73.6 74.4 78.2 72.1 67.9 73.2\n4 73.4 75.3 78.8 71.4 67.1 73.2\n8 73.7 75.2 79.7 72.5 66.8 73.6\nTop-p Sampling(p = 0.95)\n(#samples)\n1 70.1 71.5 74.3 66.9 67.2 70.0\n2 71.4 72.1 74.5 68.8 67.2 70.8\n4 71.1 72.8 76.4 69.0 66.0 71.1\n8 71.9 72.7 74.2 70.0 68.4 71.4\nTop-p Sampling+ Filtering(p = 0.95)\n(#samples)\n1 72.0 75.2 78.9 71.6 68.1 73.2\n2 73.7 75.2 79.5 72.0 67.6 73.6\n4 73.4 75.3 79.0 72.1 67.7 73.5\n8 74.0 75.4 79.6 72.3 68.0 73.9\nTable A3: Impact of decoding strategy and ﬁltering:\nGenerating multiple translations per English example\nusing top-k or top-p sampling followed by ﬁltering in-\nconsistent examples offers superior downstream perfor-\nmance compared to using greedy decoding or sampling\njust one translation per example.\n2467",
  "topic": "Bootstrapping (finance)",
  "concepts": [
    {
      "name": "Bootstrapping (finance)",
      "score": 0.9165516495704651
    },
    {
      "name": "Parsing",
      "score": 0.7802255153656006
    },
    {
      "name": "Computer science",
      "score": 0.7621665596961975
    },
    {
      "name": "Computational linguistics",
      "score": 0.658319890499115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5333449840545654
    },
    {
      "name": "Natural language processing",
      "score": 0.5231296420097351
    },
    {
      "name": "Programming language",
      "score": 0.3690398931503296
    },
    {
      "name": "Linguistics",
      "score": 0.35198813676834106
    },
    {
      "name": "Philosophy",
      "score": 0.10804077982902527
    },
    {
      "name": "Mathematics",
      "score": 0.08252060413360596
    },
    {
      "name": "Econometrics",
      "score": 0.0
    }
  ]
}