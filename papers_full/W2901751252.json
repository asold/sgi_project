{
    "title": "Unsupervised Word Discovery with Segmental Neural Language Models.",
    "url": "https://openalex.org/W2901751252",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A5060303231",
            "name": "Kazuya Kawakami",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5111222692",
            "name": "Chris Dyer",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5014309771",
            "name": "Phil Blunsom",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2158266063",
        "https://openalex.org/W2096204319",
        "https://openalex.org/W2126449874",
        "https://openalex.org/W2110485445",
        "https://openalex.org/W2029948425",
        "https://openalex.org/W1909733559",
        "https://openalex.org/W2074546930",
        "https://openalex.org/W2116211107",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1585861384"
    ],
    "abstract": "We propose a segmental neural language model that combines the representational power of neural networks and the structure learning mechanism of Bayesian nonparametrics, and show that it learns to discover semantically meaningful units (e.g., morphemes and words) from unsegmented character sequences. The model generates text as a sequence of segments, where each segment is generated either character-by-character from a sequence model or as a single draw from a lexical memory that stores multi-character units. Its parameters are fit to maximize the marginal likelihood of the training data, summing over all segmentations of the input, and its hyperparameters are likewise set to optimize held-out marginal likelihood. To prevent the model from overusing the lexical memory, which leads to poor generalization and bad segmentation, we introduce a differentiable regularizer that penalizes based on the expected length of each segment. To our knowledge, this is the first demonstration of neural networks that have predictive distributions better than LSTM language models and also infer a segmentation into word-like units that are competitive with the best existing word discovery models.",
    "full_text": "Learning to Discover, Ground and Use Words\nwith Segmental Neural Language Models\nKazuya Kawakami♠♣ Chris Dyer♣ Phil Blunsom♠♣\n♠Department of Computer Science, University of Oxford, Oxford, UK\n♣DeepMind, London, UK\n{kawakamik,cdyer,pblunsom}@google.com\nAbstract\nWe propose a segmental neural language\nmodel that combines the generalization power\nof neural networks with the ability to discover\nword-like units that are latent in unsegmented\ncharacter sequences. In contrast to previous\nsegmentation models that treat word segmen-\ntation as an isolated task, our model uniﬁes\nword discovery, learning how words ﬁt to-\ngether to form sentences, and, by condition-\ning the model on visual context, how words’\nmeanings ground in representations of non-\nlinguistic modalities. Experiments show that\nthe unconditional model learns predictive dis-\ntributions better than character LSTM models,\ndiscovers words competitively with nonpara-\nmetric Bayesian word segmentation models,\nand that modeling language conditional on vi-\nsual context improves performance on both.\n1 Introduction\nHow infants discover words that make up their ﬁrst\nlanguage is a long-standing question in develop-\nmental psychology (Saffran et al., 1996). Machine\nlearning has contributed much to this discussion\nby showing that predictive models of language\nare capable of inferring the existence of word\nboundaries solely based on statistical properties\nof the input (Elman, 1990; Brent and Cartwright,\n1996; Goldwater et al., 2009). However, there\nare two serious limitations of current models of\nword learning in the context of the broader prob-\nlem of language acquisition. First, language ac-\nquisition involves not only learning what words\nthere are (“the lexicon”), but also how they ﬁt to-\ngether (“the grammar”). Unfortunately, the best\nlanguage models, measured in terms of their abil-\nity to predict language (i.e., those which seem ac-\nquire grammar best), segment quite poorly (Chung\net al., 2017; Wang et al., 2017; K ´ad´ar et al.,\n2018), while the strongest models in terms of\nword segmentation (Goldwater et al., 2009; Berg-\nKirkpatrick et al., 2010) do not adequately account\nfor the long-range dependencies that are manifest\nin language and that are easily captured by recur-\nrent neural networks (Mikolov et al., 2010). Sec-\nond, word learning involves not only discovering\nwhat words exist and how they ﬁt together gram-\nmatically, but also determining their non-linguistic\nreferents, that is, their grounding. The work that\nhas looked at modeling acquisition of grounded\nlanguage from character sequences—usually in\nthe context of linking words to a visually experi-\nenced environment—has either explicitly avoided\nmodeling word units (Gelderloos and Chrupała,\n2016) or relied on high-level representations of\nvisual context that overly simplify the richness\nand ambiguity of the visual signal (Johnson et al.,\n2010; R¨as¨anen and Rasilo, 2015).\nIn this paper, we introduce a single model that\ndiscovers words, learns how they ﬁt together (not\njust locally, but across a complete sentence), and\ngrounds them in learned representations of nat-\nuralistic non-linguistic visual contexts. We ar-\ngue that such a uniﬁed model is preferable to a\npipeline model of language acquisition (e.g., a\nmodel where words are learned by one character-\naware model, and then a full-sentence grammar\nis acquired by a second language model using\nthe words predicted by the ﬁrst). Our prefer-\nence for the uniﬁed model may be expressed in\nterms of basic notions of simplicity (we require\none model rather than two), and in terms of the\nContinuity Hypothesis of Pinker (1984), which ar-\ngues that we should assume, absent strong evi-\ndence to the contrary, that children have the same\ncognitive systems as adults, and differences are\ndue to them having set their parameters differ-\nently/immaturely.\nOur model depends crucially on two compo-\nnents. The ﬁrst is, as mentioned, a lexical mem-\narXiv:1811.09353v2  [cs.CL]  18 Jun 2019\nory. This lexicon stores pairs of a vector (key) and\na string (value) the strings in the lexicon are con-\ntiguous sequences of characters encountered in the\ntraining data; and the vectors are randomly initial-\nized and learned during training. The second com-\nponent is a regularizer (§4) that prevents the model\nfrom overﬁtting to the training data by overusing\nthe lexicon to account for the training data.1\nOur evaluation ( §5–§7) looks at both lan-\nguage modeling performance and the quality of\nthe induced segmentations, in both unconditional\n(sequence-only) contexts and when conditioning\non a related image. First, we look at the seg-\nmentations induced by our model. We ﬁnd that\nthese correspond closely to human intuitions about\nword segments, competitive with the best exist-\ning models for unsupervised word discovery. Im-\nportantly, these segments are obtained in models\nwhose hyperparameters are tuned to optimize val-\nidation (held-out) likelihood, whereas tuning the\nhyperparameters of our benchmark models using\nheld-out likelihood produces poor segmentations.\nSecond, we conﬁrm ﬁndings (Kawakami et al.,\n2017; Mielke and Eisner, 2018) that show that\nword segmentation information leads to better lan-\nguage models compared to pure character mod-\nels. However, in contrast to previous work, we re-\nalize this performance improvement without hav-\ning to observe the segment boundaries. Thus, our\nmodel may be applied straightforwardly to Chi-\nnese, where word boundaries are not part of the\northography.\nAblation studies demonstrate that both the lex-\nicon and the regularizer are crucial for good per-\nformance, particularly in word segmentation—\nremoving either or both signiﬁcantly harms per-\nformance. In a ﬁnal experiment, we learn to model\nlanguage that describes images, and we ﬁnd that\nconditioning on visual context improves segmen-\ntation performance in our model (compared to the\nperformance when the model does not have access\nto the image). On the other hand, in a baseline\nmodel that predicts boundaries based on entropy\nspikes in a character-LSTM, making the image\navailable to the model has no impact on the quality\nof the induced segments, demonstrating again the\n1Since the lexical memory stores strings that appear in the\ntraining data, each sentence could, in principle, be generated\nas a single lexical unit, thus the model could ﬁt the train-\ning data perfectly while generalizing poorly. The regularizer\npenalizes based on the expectation of the powered length of\neach segment, preventing this degenerate solution from being\noptimal.\nvalue of explicitly including a word lexicon in the\nlanguage model.\n2 Model\nWe now describe the segmental neural language\nmodel (SNLM). Refer to Figure 1 for an illustra-\ntion. The SNLM generates a character sequence\nx = x1,...,x n, where each xi is a character\nin a ﬁnite character set Σ. Each sequence x is\nthe concatenation of a sequence of segments s =\ns1,..., s|s|where |s|≤ nmeasures the length of\nthe sequence in segments and each segment si ∈\nΣ+ is a sequence of characters,si,1,...,s i,|si|. In-\ntuitively, each si corresponds to one word. Let\nπ(s1,..., si) represent the concatenation of the\ncharacters of the segmentss1 to si, discarding seg-\nmentation information; thus x = π(s). For exam-\nple if x = anapple, the underlying segmenta-\ntion might be s = an apple (with s1 = an and\ns2 = apple), or s = a nap ple, or any of the\n2|x|−1 segmentation possibilities for x.\nThe SNLM deﬁnes the distribution overx as the\nmarginal distribution over all segmentations that\ngive rise to x, i.e.,\np(x) =\n∑\ns:π(s)=x\np(s). (1)\nTo deﬁne the probability of p(s), we use the chain\nrule, rewriting this in terms of a product of the se-\nries of conditional probabilities, p(st |s<t). The\nprocess stops when a special end-sequence seg-\nment ⟨/S⟩is generated. To ensure that the summa-\ntion in Eq. 1 is tractable, we assume the following:\np(st |s<t) ≈p(st |π(s<t)) = p(st |x<t), (2)\nwhich amounts to a conditional semi-Markov\nassumption—i.e., non-Markovian generation hap-\npens inside each segment, but the segment genera-\ntion probability does not depend on memory of the\nprevious segmentation decisions, only upon the\nsequence of characters π(s<t) corresponding to\nthe preﬁx character sequence x<t. This assump-\ntion has been employed in a number of related\nmodels to permit the use of LSTMs to represent\nrich history while retaining the convenience of dy-\nnamic programming inference algorithms (Wang\net al., 2017; Ling et al., 2017; Graves, 2012).\n2.1 Segment generation\nWe modelp(st |x<t) as a mixture of two models,\none that generates the segment using a sequence\nnC a uy o l o ko ta\n…\no\nl\no\no\nk\no\n</w> \nk\nl\n<w> l \nl o \nl o o k …\nap app appl apple lo loo look \nl o o k \n…\nlooks looke looked \nFigure 1: Fragment of the segmental neural language model while evaluating the marginal likelihood of a sequence.\nAt the indicated time, the model has generated the sequence Canyou, and four possible continuations are shown.\nmodel and the other that generates multi-character\nsequences as a single event. Both are conditional\non a common representation of the history, as is\nthe mixture proportion.\nRepresenting history To represent x<t, we\nuse an LSTM encoder to read the sequence\nof characters, where each character type σ ∈\nΣ has a learned vector embedding vσ. Thus\nthe history representation at time t is ht =\nLSTMenc(vx1 ,..., vxt ). This corresponds to the\nstandard history representation for a character-\nlevel language model, although in general, we as-\nsume that our modelled data is not delimited by\nwhitespace.\nCharacter-by-character generation The ﬁrst\ncomponent model, pchar(st |ht), generates st by\nsampling a sequence of characters from a LSTM\nlanguage model over Σ and a two extra special\nsymbols, an end-of-word symbol ⟨/W⟩ /∈Σ and\nthe end-of-sequence symbol⟨/S⟩discussed above.\nThe initial state of the LSTM is a learned trans-\nformation of ht, the initial cell is 0, and different\nparameters than the history encoding LSTM are\nused. During generation, each letter that is sam-\npled (i.e., each st,i) is fed back into the LSTM in\nthe usual way and the probability of the character\nsequence decomposes according to the chain rule.\nThe end-of-sequence symbol can never be gener-\nated in the initial position.\nLexical generation The second component\nmodel, plex(st |ht), samples full segments from\nlexical memory. Lexical memory is a key-value\nmemory containing Mentries, where each key,ki,\na vector, is associated with a value vi ∈Σ+. The\ngeneration probability of st is deﬁned as\nh′\nt = MLP(ht)\nm = softmax(Kh′\nt + b)\nplex(st |ht) =\nM∑\ni=1\nmi[vi = st],\nwhere [vi = st] is 1 if the ith value in memory is\nst and 0 otherwise, and K is a matrix obtained by\nstacking the k⊤\ni ’s. This generation process assigns\nzero probability to most strings, but the alternate\ncharacter model can generate all of Σ+.\nIn this work, we ﬁx the vi’s to be subsequences\nof at least length 2, and up to a maximum length\nLthat are observed at least F times in the training\ndata. These values are tuned as hyperparameters\n(See Appendix C for details of the experiments).\nMixture proportion The mixture proportion,\ngt, determines how likely the character genera-\ntor is to be used at time t (the lexicon is used\nwith probability 1 −gt). It is deﬁned by as gt =\nσ(MLP(ht)).\nTotal segment probability The total generation\nprobability of st is thus\np(st |x<t) = gtpchar(st |ht)+\n(1 −gt)plex(st |ht).\n3 Inference\nWe are interested in two inference questions: ﬁrst,\ngiven a sequence x, evaluate its (log) marginal\nlikelihood; second, given x, ﬁnd the most likely\ndecomposition into segments s∗.\nMarginal likelihood To efﬁciently compute the\nmarginal likelihood, we use a variant of the for-\nward algorithm for semi-Markov models (Yu,\n2010), which incrementally computes a sequence\nof probabilities, αi, where αi is the marginal like-\nlihood of generating x≤i and concluding a seg-\nment at time i. Although there are an exponential\nnumber of segmentations ofx, these values can be\ncomputed using O(|x|) space and O(|x|2) time as:\nα0 = 1, α t =\nt−1∑\nj=t−L\nαjp(s = xj:t |x<j).\n(3)\nBy letting xt+1 = ⟨/S⟩, then p(x) = αt+1.\nMost probable segmentation The most proba-\nble segmentation of a sequencex can be computed\nby replacing the summation with a max operator\nin Eq. 3 and maintaining backpointers.\n4 Expected length regularization\nWhen the lexical memory contains all the sub-\nstrings in the training data, the model easily over-\nﬁts by copying the longest continuation from the\nmemory. To prevent overﬁtting, we introduce a\nregularizer that penalizes based on the expecta-\ntion of the exponentiated (by a hyperparameter β)\nlength of each segment:\nR(x,β) =\n∑\ns:π(s)=x\np(s |x)\n∑\ns∈s\n|s|β.\nThis can be understood as a regularizer based on\nthe double exponential prior identiﬁed to be ef-\nfective in previous work (Liang and Klein, 2009;\nBerg-Kirkpatrick et al., 2010). This expectation\nis a differentiable function of the model parame-\nters. Because of the linearity of the penalty across\nsegments, it can be computed efﬁciently using\nthe above dynamic programming algorithm under\nthe expectation semiring (Eisner, 2002). This is\nparticularly efﬁcient since the expectation semir-\ning jointly computes the expectation and marginal\nlikelihood in a single forward pass. For more de-\ntails about computing gradients of expectations\nunder distributions over structured objects with\ndynamic programs and semirings, see Li and Eis-\nner (2009).\n4.1 Training Objective\nThe model parameters are trained by minimizing\nthe penalized log likelihood of a training corpusD\nof unsegmented sentences,\nL=\n∑\nx∈D\n[−log p(x) + λR(x,β)].\n5 Datasets\nWe evaluate our model on both English and Chi-\nnese segmentation. For both languages, we used\nstandard datasets for word segmentation and lan-\nguage modeling. We also use MS-COCO to evalu-\nate how the model can leverage conditioning con-\ntext information. For all datasets, we used train,\nvalidation and test splits. 2 Since our model as-\nsumes a closed character set, we removed vali-\ndation and test samples which contain characters\nthat do not appear in the training set. In the En-\nglish corpora, whitespace characters are removed.\nIn Chinese, they are not present to begin with. Re-\nfer to Appendix A for dataset statistics.\n5.1 English\nBrent Corpus The Brent corpus is a standard\ncorpus used in statistical modeling of child lan-\nguage acquisition (Brent, 1999; Venkataraman,\n2001).3 The corpus contains transcriptions of ut-\nterances directed at 13- to 23-month-old children.\nThe corpus has two variants: an orthographic\none (BR-text) and a phonemic one ( BR-phono),\nwhere each character corresponds to a single En-\nglish phoneme. As the Brent corpus does not have\na standard train and test split, and we want to tune\nthe parameters by measuring the ﬁt to held-out\ndata, we used the ﬁrst 80% of the utterances for\ntraining and the next 10% for validation and the\nrest for test.\nEnglish Penn Treebank (PTB) We use the\ncommonly used version of the PTB prepared by\nMikolov et al. (2010). However, since we removed\nspace symbols from the corpus, our cross entropy\nresults cannot be compared to those usually re-\nported on this dataset.\n5.2 Chinese\nSince Chinese orthography does not mark spaces\nbetween words, there have been a number of ef-\nforts to annotate word boundaries. We evalu-\nate against two corpora that have been manually\nsegmented according different segmentation stan-\ndards.\n2The data and splits used are available at\nhttps://s3.eu-west-2.amazonaws.com/\nk-kawakami/seg.zip.\n3https://childes.talkbank.org/derived\nBeijing University Corpus (PKU) The Beijing\nUniversity Corpus was one of the corpora used\nfor the International Chinese Word Segmentation\nBakeoff (Emerson, 2005).\nChinese Penn Treebank (CTB) We use the\nPenn Chinese Treebank Version 5.1 (Xue et al.,\n2005). It generally has a coarser segmentation\nthan PKU (e.g., in CTB a full name, consisting of\na given name and family name, is a single token),\nand it is a larger corpus.\n5.3 Image Caption Dataset\nTo assess whether jointly learning about meanings\nof words from non-linguistic context affects seg-\nmentation performance, we use image and caption\npairs from the COCO caption dataset (Lin et al.,\n2014). We use 10,000 examples for both training\nand testing and we only use one reference per im-\nage. The images are used to be conditional context\nto predict captions. Refer to Appendix B for the\ndataset construction process.\n6 Experiments\nWe compare our model to benchmark Bayesian\nmodels, which are currently the best known un-\nsupervised word discovery models, as well as\nto a simple deterministic segmentation criterion\nbased on surprisal peaks (Elman, 1990) on lan-\nguage modeling and segmentation performance.\nAlthough the Bayeisan models are shown to able\nto discover plausible word-like units, we found\nthat a set of hyperparameters that provides best\nperformance with such model on language mod-\neling does not produce good structures as reported\nin previous works. This is problematic since there\nis no objective criteria to ﬁnd hyperparameters\nin fully unsupervised manner when the model is\napplied to completely unknown languages or do-\nmains. Thus, our experiments are designed to as-\nsess how well the models infers word segmenta-\ntions of unsegmented inputs when they are trained\nand tuned to maximize the likelihood of the held-\nout text.\nDP/HDP Benchmarks Among the most effec-\ntive existing word segmentation models are those\nbased on hierarchical Dirichlet process (HDP)\nmodels (Goldwater et al., 2009; Teh et al., 2006)\nand hierarchical Pitman–Yor processes (Mochi-\nhashi et al., 2009). As a representative of these,\nwe use a simple bigram HDP model:\nθ·∼DP(α0,p0)\nθ·|s ∼DP(α1,θ·) ∀s ∈Σ∗\nst+1 |st ∼Categorical(θ·|st ).\nThe base distribution, p0, is deﬁned over strings in\nΣ∗∪{⟨/S⟩}by deciding with a speciﬁed prob-\nability to end the utterance, a geometric length\nmodel, and a uniform probability over Σ at a each\nposition. Intuitively, it captures the preference\nfor having short words in the lexicon. In addi-\ntion to the HDP model, we also evaluate a sim-\npler single Dirichlet process (DP) version of the\nmodel, in which the st’s are generated directly as\ndraws from Categorical(θ·). We use an empiri-\ncal Bayesian approach to select hyperparameters\nbased on the likelihood assigned by the inferred\nposterior to a held-out validation set. Refer to Ap-\npendix D for details on inference.\nDeterministic Baselines Incremental word seg-\nmentation is inherently ambiguous (e.g., the let-\nters the might be a single word, or they might\nbe the beginning of the longer word theater).\nNevertheless, several deterministic functions of\npreﬁxes have been proposed in the literature as\nstrategies for discovering rudimentary word-like\nunits hypothesized for being useful for bootstrap-\nping the lexical acquisition process or for improv-\ning a model’s predictive accuracy. These range\nfrom surprisal criteria (Elman, 1990) to sophisti-\ncated language models that switch between mod-\nels that capture intra- and inter-word dynamics\nbased on deterministic functions of preﬁxes of\ncharacters (Chung et al., 2017; Shen et al., 2018).\nIn our experiments, we also include such de-\nterministic segmentation results using (1) the\nsurprisal criterion of Elman (1990) and (2) a\ntwo-level hierarchical multiscale LSTM (Chung\net al., 2017), which has been shown to pre-\ndict boundaries in whitespace-containing charac-\nter sequences at positions corresponding to word\nboundaries. As with all experiments in this paper,\nthe BR-corpora for this experiment do not contain\nspaces.\nSNLM Model conﬁgurations and Evaluation\nLSTMs had 512 hidden units with parameters\nlearned using the Adam update rule (Kingma\nand Ba, 2015). We evaluated our models with\nbits-per-character (bpc) and segmentation accu-\nracy (Brent, 1999; Venkataraman, 2001; Goldwa-\nter et al., 2009). Refer to Appendices C–F for de-\ntails of model conﬁgurations and evaluation met-\nrics.\nFor the image caption dataset, we extend the\nmodel with a standard attention mechanism in the\nbackbone LSTM ( LSTMenc) to incorporate im-\nage context. For every character-input, the model\ncalculates attentions over image features and use\nthem to predict the next characters. As for image\nrepresentations, we use features from the last con-\nvolution layer of a pre-trained VGG19 model (Si-\nmonyan and Zisserman, 2014).\n7 Results\nIn this section, we ﬁrst do a careful compari-\nson of segmentation performance on the phone-\nmic Brent corpus (BR-phono) across several dif-\nferent segmentation baselines, and we ﬁnd that\nour model obtains competitive segmentation per-\nformance. Additionally, ablation experiments\ndemonstrate that both lexical memory and the pro-\nposed expected length regularization are necessary\nfor inferring good segmentations. We then show\nthat also on other corpora, we likewise obtain seg-\nmentations better than baseline models. Finally,\nwe also show that our model has superior perfor-\nmance, in terms of held-out perplexity, compared\nto a character-level LSTM language model. Thus,\noverall, our results show that we can obtain good\nsegmentations on a variety of tasks, while still hav-\ning very good language modeling performance.\nWord Segmentation (BR-phono) Table 1 sum-\nmarizes the segmentation results on the widely\nused BR-phono corpus, comparing it to a variety\nof baselines. Unigram DP, Bigram HDP, LSTM\nsuprisal and HMLSTM refer to the benchmark\nmodels explained in §6. The ablated versions\nof our model show that without the lexicon\n(−memory), without the expected length penalty\n(−length), and without either, our model fails to\ndiscover good segmentations. Furthermore, we\ndraw attention to the difference in the performance\nof the HDP and DP models when using subjec-\ntive settings of the hyperparameters and the empir-\nical settings (likelihood). Finally, the deterministic\nbaselines are interesting in two ways. First, LSTM\nsurprisal is a remarkably good heuristic for seg-\nmenting text (although we will see below that its\nperformance is much less good on other datasets).\nSecond, despite careful tuning, the HMLSTM of\nChung et al. (2017) fails to discover good seg-\nments, although in their paper they show that when\nspaces are present between, HMLSTMs learn to\nswitch between their internal models in response\nto them.\nFurthermore, the priors used in the DP/HDP\nmodels were tuned to maximize the likelihood as-\nsigned to the validation set by the inferred poste-\nrior predictive distribution, in contrast to previous\npapers which either set them subjectively or in-\nferred them (Johnson and Goldwater, 2009). For\nexample, the DP and HDP model with subjective\npriors obtained 53.8 and 72.3 F1 scores, respec-\ntively (Goldwater et al., 2009). However, when\nthe hyperparameters are set to maximize held-out\nlikelihood, this drops obtained 56.1 and 56.9. An-\nother result on this dataset is the feature unigram\nmodel of Berg-Kirkpatrick et al. (2010), which\nobtains an 88.0 F1 score with hand-crafted fea-\ntures and by selecting the regularization strength\nto optimize segmentation performance. Once the\nfeatures are removed, the model achieved a 71.5\nF1 score when it is tuned on segmentation perfor-\nmance and only 11.5 when it is tuned on held-out\nlikelihood.\nP R F1\nLSTM surprisal (Elman, 1990) 54.5 55.5 55.0\nHMLSTM (Chung et al., 2017) 8.1 13.3 10.1\nUnigram DP 63.3 50.4 56.1\nBigram HDP 53.0 61.4 56.9\nSNLM (−memory, −length) 54.3 34.9 42.5\nSNLM (+memory, −length) 52.4 36.8 43.3\nSNLM (−memory, +length) 57.6 43.4 49.5\nSNLM (+memory, +length) 81.3 77.5 79.3\nTable 1: Summary of segmentation performance on\nphoneme version of the Brent Corpus (BR-phono).\nWord Segmentation (other corpora) Table 2\nsummarizes results on the BR-text (orthographic\nBrent corpus) and Chinese corpora. As in the\nprevious section, all the models were trained to\nmaximize held-out likelihood. Here we observe\na similar pattern, with the SNLM outperforming\nthe baseline models, despite the tasks being quite\ndifferent from each other and from the BR-phono\ntask.\nWord Segmentation Qualitative Analysis We\nshow some representative examples of segmenta-\ntions inferred by various models on the BR-text\nP R F1\nBR-text\nLSTM surprisal 36.4 49.0 41.7\nUnigram DP 64.9 55.7 60.0\nBigram HDP 52.5 63.1 57.3\nSNLM 68.7 78.9 73.5\nPTB\nLSTM surprisal 27.3 36.5 31.2\nUnigram DP 51.0 49.1 50.0\nBigram HDP 34.8 47.3 40.1\nSNLM 54.1 60.1 56.9\nCTB\nLSTM surprisal 41.6 25.6 31.7\nUnigram DP 61.8 49.6 55.0\nBigram HDP 67.3 67.7 67.5\nSNLM 78.1 81.5 79.8\nPKU\nLSTM surprisal 38.1 23.0 28.7\nUnigram DP 60.2 48.2 53.6\nBigram HDP 66.8 67.1 66.9\nSNLM 75.0 71.2 73.1\nTable 2: Summary of segmentation performance on\nother corpora.\nand PKU corpora in Table 3. As reported in Gold-\nwater et al. (2009), we observe that the DP mod-\nels tend to undersegment, keep long frequent se-\nquences together (e.g., they failed to separate arti-\ncles). HDPs do successfully prevent oversegmen-\ntation; however, we ﬁnd that when trained to opti-\nmize held-out likelihood, they often insert unnec-\nessary boundaries between words, such as yo u .\nOur model’s performance is better, but it likewise\nshows a tendency to oversegment. Interestingly,\nwe can observe a tendency tends to put boundaries\nbetween morphemes in morphologically complex\nlexical items such as dumpty ’s, and go ing. Since\nmorphemes are the minimal units that carry mean-\ning in language, this segmentation, while incor-\nrect, is at least plasuible. Turning to the Chinese\nexamples, we see that both baseline models fail to\ndiscover basic words such as山间(mountain) and\n人们(human).\nFinally, we observe that none of the models\nsuccessfully segment dates or numbers containing\nmultiple digits (all oversegment). Since number\ntypes tend to be rare, they are usually not in the\nlexicon, meaning our model (and the H/DP base-\nlines) must generate them as character sequences.\nLanguage Modeling Performance The above\nresults show that the SNLM infers good word seg-\nmentations. We now turn to the question of how\nwell it predicts held-out data. Table 4 summa-\nrizes the results of the language modeling experi-\nments. Again, we see that SNLM outperforms the\nBayesian models and a character LSTM. Although\nthere are numerous extensions to LSTMs to im-\nprove language modeling performance, LSTMs\nremain a strong baseline (Melis et al., 2018).\nOne might object that because of the lexicon,\nthe SNLM has many more parameters than the\ncharacter-level LSTM baseline model. However,\nunlike parameters in LSTM recurrence which are\nused every timestep, our memory parameters are\naccessed very sparsely. Furthermore, we observed\nthat an LSTM with twice the hidden units did not\nimprove the baseline with 512 hidden units on\nboth phonemic and orthographic versions of Brent\ncorpus but the lexicon could. This result suggests\nmore hidden units are useful if the model does not\nhave enough capacity to ﬁt larger datasets, but that\nthe memory structure adds other dynamics which\nare not captured by large recurrent networks.\nMultimodal Word Segmentation Finally, we\ndiscuss results on word discovery with non-\nlinguistic context (image). Although there is much\nevidence that neural networks can reliably learn\nto exploit additional relevant context to improve\nlanguage modeling performance (e.g. machine\ntranslation and image captioning), it is still un-\nclear whether the conditioning context help to dis-\ncover structure in the data. We turn to this ques-\ntion here. Table 5 summarizes language modeling\nand segmentation performance of our model and a\nbaseline character-LSTM language model on the\nCOCO image caption dataset. We use the Elman\nEntropy criterion to infer the segmentation points\nfrom the baseline LM, and the MAP segmenta-\ntion under our model. Again, we ﬁnd our model\noutperforms the baseline model in terms of both\nlanguage modeling and word segmentation accu-\nracy. Interestingly, we ﬁnd while conditioning on\nimage context leads to reductions in perplexity in\nboth models, in our model the presence of the im-\nage further improves segmentation accuracy. This\nsuggests that our model and its learning mecha-\nnism interact with the conditional context differ-\nently than the LSTM does.\nTo understand what kind of improvements in\nsegmentation performance the image context leads\nto, we annotated the tokens in the references with\npart-of-speech (POS) tags and compared relative\nExamples\nBR-text\nReference are you going to make him pretty this morning\nUnigram DP areyou goingto makehim pretty this morning\nBigram HDP areyou go ingto make him p retty this mo rn ing\nSNLM are you go ing to make him pretty this morning\nReference would you like to do humpty dumpty’s button\nUnigram DP wouldyoul iketo do humpty dumpty ’s button\nBigram HDP would youlike to do humptyd umpty ’s butt on\nSNLM would you like to do humpty dumpty ’s button\nPKU\nReference 笑声 、 掌声 、 欢呼声 ， 在 山间 回荡 ， 勾 起 了 人们 对 往事 的 回忆 。\nUnigram DP 笑声 、 掌声 、 欢呼 声 ，在 山 间 回荡 ， 勾 起了 人们 对 往事 的 回忆 。\nBigram HDP 笑 声、 掌声 、 欢 呼声 ，在 山 间 回 荡， 勾 起了 人 们 对 往事 的 回忆 。\nSNLM 笑声、 掌声 、 欢呼声 ， 在 山间 回荡 ， 勾起 了 人们 对 往事 的 回忆 。\nReference 不得 在 江河 电缆 保护区 内 抛锚 、 拖锚 、 炸鱼 、 挖沙 。\nUnigram DP 不得 在 江河电缆 保护 区内抛锚、 拖锚 、炸鱼、挖沙 。\nBigram HDP 不得 在 江 河电缆 保护 区内 抛 锚、拖 锚 、 炸鱼、 挖沙 。\nSNLM 不得 在 江河 电缆 保护区 内 抛锚 、 拖锚、 炸鱼 、 挖沙 。\nTable 3: Examples of predicted segmentations on English and Chinese.\nBR-text BR-phono PTB CTB PKU\nUnigram DP 2.33 2.93 2.25 6.16 6.88\nBigram HDP 1.96 2.55 1.80 5.40 6.42\nLSTM 2.03 2.62 1.65 4.94 6.20\nSNLM 1.94 2.54 1.56 4.84 5.89\nTable 4: Test language modeling performance (bpc).\nimprovements on recall between SNLM (−image)\nand SNLM ( +image) among the ﬁve POS tags\nwhich appear more than 10,000 times. We ob-\nserved improvements on ADJ ( +4.5%), NOUN\n(+4.1%), VERB (+3.1%). The improvements on\nthe categories ADP ( +0.5%) and DET ( +0.3%)\nare were more limited. The categories where we\nsee the largest improvement in recall correspond\nto those that are likelya priori to correlate most re-\nliably with observable features. Thus, this result is\nconsistent with a hypothesis that the lexican is suc-\ncessfully acquiring knowledge about how words\nidiosyncratically link to visual features.\nSegmentation State-of-the-Art The results re-\nported are not the best-reported numbers on the\nEnglish phoneme or Chinese segmentation tasks.\nAs we discussed in the introduction, previous\nwork has focused on segmentation in isolation\nfrom language modeling performance. Models\nthat obtain better segmentations include the adap-\ntor grammars (F1: 87.0) of Johnson and Goldwa-\nter (2009) and the feature-unigram model (88.0)\nbpc↓ P ↑ R ↑ F1↑\nUnigram DP 2.23 44.0 40.0 41.9\nBigram HDP 1.68 30.9 40.8 35.1\nLSTM (−image) 1.55 31.3 38.2 34.4\nSNLM (−image) 1.52 39.8 55.3 46.3\nLSTM (+image) 1.42 31.7 39.1 35.0\nSNLM (+image) 1.38 46.4 62.0 53.1\nTable 5: Language modeling (bpc) and segmentation\naccuracy on COCO dataset. +image indicates that the\nmodel has access to image context.\nof Berg-Kirkpatrick et al. (2010). While these re-\nsults are better in terms of segmentation, they are\nweak language models (the feature unigram model\nis effectively a unigram word model; the adap-\ntor grammar model is effectively phrasal unigram\nmodel; both are incapable of generalizing about\nsubstantially non-local dependencies). Addition-\nally, the features and grammars used in prior work\nreﬂect certain English-speciﬁc design considera-\ntions (e.g., syllable structure in the case of adap-\ntor grammars and phonotactic equivalence classes\nin the feature unigram model), which make them\nquestionable models if the goal is to explore what\nmodels and biases enable word discovery in gen-\neral. For Chinese, the best nonparametric mod-\nels perform better at segmentation (Zhao and Kit,\n2008; Mochihashi et al., 2009), but again they are\nweaker language models than neural models. The\nneural model of Sun and Deng (2018) is similar to\nour model without lexical memory or length regu-\nlarization; it obtains 80.2 F1 on the PKU dataset;\nhowever, it uses gold segmentation data during\ntraining and hyperparameter selection, 4 whereas\nour approach requires no gold standard segmen-\ntation data.\n8 Related Work\nLearning to discover and represent temporally\nextended structures in a sequence is a funda-\nmental problem in many ﬁelds. For exam-\nple in language processing, unsupervised learn-\ning of multiple levels of linguistic structures\nsuch as morphemes (Snyder and Barzilay, 2008),\nwords (Goldwater et al., 2009; Mochihashi et al.,\n2009; Wang et al., 2014) and phrases (Klein and\nManning, 2001) have been investigated. Recently,\nspeech recognition has beneﬁted from techniques\nthat enable the discovery of subword units (Chan\net al., 2017; Wang et al., 2017); however, in\nthat work, the optimally discovered character se-\nquences look quite unlike orthographic words. In\nfact, the model proposed by Wang et al. (2017)\nis essentially our model without a lexicon or the\nexpected length regularization, i.e., ( −memory,\n−length), which we have shown performs quite\npoorly in terms of segmentation accuracy. Fi-\nnally, some prior work has also sought to dis-\ncover lexical units directly from speech based\non speech-internal statistical regularities (Kam-\nper et al., 2016), as well as jointly with ground-\ning (Chrupała et al., 2017).\n9 Conclusion\nWord discovery is a fundamental problem in lan-\nguage acquisition. While work studying the prob-\nlem in isolation has provided valuable insights\n(showing both what data is sufﬁcient for word\ndiscovery with which models), this paper shows\nthat neural models offer the ﬂexibility and perfor-\nmance to productively study the various facets of\nthe problem in a more uniﬁed model. While this\nwork uniﬁes several components that had previ-\nously been studied in isolation, our model assumes\naccess to phonetic categories. The development\nof these categories likely interact with the devel-\nopment of the lexicon and acquisition of seman-\ntics (Feldman et al., 2013; Fourtassi and Dupoux,\n4https://github.com/\nEdward-Sun/SLM/blob/\nd37ad735a7b1d5af430b96677c2ecf37a65f59b7/\ncodes/run.py#L329\n2014), and thus subsequent work should seek to\nunify more aspects of the acquisition problem.\nAcknowledgments\nWe thank Mark Johnson, Sharon Goldwater, and\nEmmanuel Dupoux, as well as many colleagues at\nDeepMind, for their insightful comments and sug-\ngestions for improving this work and the resulting\npaper.\nReferences\nTaylor Berg-Kirkpatrick, Alexandre Bouchard-C ˆot´e,\nJohn DeNero, and Dan Klein. 2010. Painless un-\nsupervised learning with features. In Proc. NAACL.\nMichael R Brent. 1999. An efﬁcient, probabilistically\nsound algorithm for segmentation and word discov-\nery. Machine Learning, 34(1):71–105.\nMichael R Brent and Timothy A Cartwright. 1996.\nDistributional regularity and phonotactic constraints\nare useful for segmentation. Cognition, 61(1):93–\n125.\nWilliam Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly.\n2017. Latent sequence decompositions. In Proc.\nICLR.\nGrzegorz Chrupała, Lieke Gelderloos, and Afra Al-\nishahi. 2017. Representations of language in a\nmodel of visually grounded speech signal. In Proc.\nACL.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In Proc. ICLR.\nJason Eisner. 2002. Parameter estimation for proba-\nbilistic ﬁnite-state transducers. In Proc. ACL.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nThomas Emerson. 2005. The second international Chi-\nnese word segmentation bakeoff. In Proc. SIGHAN\nWorkshop.\nNaomi H. Feldman, Thomas L. Grifﬁths, Sharon Gold-\nwater, and James L. Morgan. 2013. A role for the\ndeveloping lexicon in phonetic category acquisition.\nPsychological Review, 120(4):751–778.\nAbdellah Fourtassi and Emmanuel Dupoux. 2014. A\nrudimentary lexicon and semantics help bootstrap\nphoneme acquisition. In Proc. EMNLP.\nLieke Gelderloos and Grzegorz Chrupała. 2016. From\nphonemes to images: levels of representation in a re-\ncurrent neural model of visually-grounded language\nlearning. In Proc. COLING.\nSharon Goldwater, Thomas L Grifﬁths, and Mark John-\nson. 2009. A Bayesian framework for word segmen-\ntation: Exploring the effects of context. Cognition,\n112(1):21–54.\nAlex Graves. 2012. Sequence transduction with\nrecurrent neural networks. arXiv preprint\narXiv:1211.3711.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nMark Johnson, Katherine Demuth, Michael Frank, and\nBevan K. Jones. 2010. Synergies in learning words\nand their referents. In Proc. NIPS.\nMark Johnson and Sharon Goldwater. 2009. Improving\nnonparameteric bayesian inference: experiments on\nunsupervised word segmentation with adaptor gram-\nmars. In Proc. NAACL, pages 317–325.\n´Akos K ´ad´ar, Marc-Alexandre C ˆot´e, Grzegorz\nChrupała, and Afra Alishahi. 2018. Revisiting\nthe hierarchical multiscale LSTM. In Proc.\nCOLING.\nHerman Kamper, Aren Jansen, and Sharon Goldwater.\n2016. Unsupervised word segmentation and lexi-\ncon induction discovery using acoustic word embed-\ndings. IEEE Transactions on Audio, Speech, and\nLanguage Processing, 24(4):669–679.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to create and reuse words in open-\nvocabulary neural language modeling. In Proc.\nACL.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proc. ICLR.\nDan Klein and Christopher D Manning. 2001. Dis-\ntributional phrase structure induction. In Workshop\nProc. ACL.\nZhifei Li and Jason Eisner. 2009. First-and second-\norder expectation semirings with applications to\nminimum-risk training on translation forests. In\nProc. EMNLP.\nPercy Liang and Dan Klein. 2009. Online EM for un-\nsupervised models. In Proc. NAACL.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. In Proc. ECCV, pages\n740–755.\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tom ´aˇs Ko ˇcisk´y, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2017. Latent predictor\nnetworks for code generation. In Proc. ACL.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In Proc. ICLR.\nSebastian J. Mielke and Jason Eisner. 2018. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. In Proc. NAACL.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Proc.\nInterspeech.\nDaichi Mochihashi, Takeshi Yamada, and Naonori\nUeda. 2009. Bayesian unsupervised word segmen-\ntation with nested Pitman–Yor language modeling.\nIn Proc. ACL.\nSteven Pinker. 1984. Language learnability and lan-\nguage development. Harvard University Press.\nOkko R ¨as¨anen and Heikki Rasilo. 2015. A joint\nmodel of word segmentation and meaning acquisi-\ntion through cross-situational learning. Psychologi-\ncal Review, 122(4):792–829.\nJenny R Saffran, Richard N Aslin, and Elissa L New-\nport. 1996. Statistical learning by 8-month-old in-\nfants. Science, 274(5294):1926–1928.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2018. Neural language modeling\nby jointly learning syntax and lexicon. In Proc.\nICLR.\nK. Simonyan and A. Zisserman. 2014. Very deep con-\nvolutional networks for large-scale image recogni-\ntion. CoRR, abs/1409.1556.\nBenjamin Snyder and Regina Barzilay. 2008. Unsuper-\nvised multilingual learning for morphological seg-\nmentation. In Proc. ACL.\nZhiqing Sun and Zhi-Hong Deng. 2018. Unsupervised\nneural word segmentation for Chinese via segmental\nlanguage modeling. In Proc. EMNLP.\nYee-Whye Teh, Michael I. Jordan, Matthew J. Beal,\nand Daivd M. Blei. 2006. Hierarchical Dirichlet\nprocesses. Journal of the American Statistical As-\nsociation, 101(476):1566–1581.\nAnand Venkataraman. 2001. A statistical model for\nword discovery in transcribed speech. Computa-\ntional Linguistics, 27(3):351–372.\nChong Wang, Yining Wan, Po-Sen Huang, Abdelrah-\nman Mohammad, Dengyong Zhou, and Li Deng.\n2017. Sequence modeling via segmentations. In\nProc. ICML.\nXiaolin Wang, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2014. Empirical study of unsuper-\nvised Chinese word segmentation methods for SMT\non large-scale corpora. In Proc. ACL.\nNaiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta\nPalmer. 2005. The Penn Chinese treebank: Phrase\nstructure annotation of a large corpus. Natural lan-\nguage engineering, 11(2):207–238.\nShun-Zheng Yu. 2010. Hidden semi-Markov models.\nArtiﬁcial Intelligence, 174(2):215–243.\nHai Zhao and Chunyu Kit. 2008. An empirical com-\nparison of goodness measures for unsupervised Chi-\nnese word segmentation with a uniﬁed framework.\nIn Proc. IJCNLP.\nA Dataset statistics\nTable 6 summarizes dataset statistics.\nB Image Caption Dataset Construction\nWe use 8000, 2000 and 10000 images for\ntrain, development and test set in order of in-\nteger ids specifying image in cocoapi 5 and use\nﬁrst annotation provided for each image. We\nwill make pairs of image id and annotation\nid available from https://s3.eu-west-2.\namazonaws.com/k-kawakami/seg.zip.\nC SNLM Model Conﬁguration\nFor each RNN based model we used 512 dimen-\nsions for the character embeddings and the LSTMs\nhave 512 hidden units. All the parameters, includ-\ning character projection parameters, are randomly\nsampled from uniform distribution from −0.08 to\n0.08. The initial hidden and memory state of the\nLSTMs are initialized with zero. A dropout rate of\n0.5 was used for all but the recurrent connections.\nTo restrict the size of memory, we stored sub-\nstrings which appeared F-times in the training\ncorpora and tuned F with grid search. The maxi-\nmum length of subsequences Lwas tuned on the\nheld-out likelihood using a grid search. Tab. 7\nsummarizes the parameters for each dataset. Note\nthat we did not tune the hyperparameters on seg-\nmentation quality to ensure that the models are\ntrained in a purely unsupervised manner assuming\nno reference segmentations are available.\nD DP/HDP Inference\nBy integrating out the draws from the DP’s, it is\npossible to do inference using Gibbs sampling di-\nrectly in the space of segmentation decisions. We\nuse 1,000 iterations with annealing to ﬁnd an ap-\nproximation of the MAP segmentation and then\nuse the corresponding posterior predictive distri-\nbution to estimate the held-out likelihood assigned\nby the model, marginalizing the segmentations us-\ning appropriate dynamic programs. The evaluated\nsegmentation was the most probable segmentation\naccording to the posterior predictive distribution.\nIn the original Bayesian segmentation work, the\nhyperparameters (i.e., α0, α1, and the components\nof p0) were selected subjectively. To make com-\nparison with our neural models fairer, we instead\nused an empirical approach and set them using the\n5https://github.com/cocodataset/cocoapi\nheld-out likelihood of the validation set. However,\nsince this disadvantages the DP/HDP models in\nterms of segmentation, we also report the original\nresults on the BR corpora.\nE Learning\nThe models were trained with the Adam update\nrule (Kingma and Ba, 2015) with a learning rate of\n0.01. The learning rate is divided by 4 if there is no\nimprovement on development data. The maximum\nnorm of the gradients was clipped at 1.0.\nF Evaluation Metrics\nLanguage Modeling We evaluated our models\nwith bits-per-character (bpc), a standard evalua-\ntion metric for character-level language models.\nFollowing the deﬁnition in Graves (2013), bits-\nper-character is the average value of −log2 p(xt |\nx<t) over the whole test set,\nbpc = −1\n|x|log2 p(x),\nwhere |x|is the length of the corpus in characters.\nThe bpc is reported on the test set.\nSegmentation We also evaluated segmentation\nquality in terms of precision, recall, and F1 of\nword tokens (Brent, 1999; Venkataraman, 2001;\nGoldwater et al., 2009). To get credit for a word,\nthe models must correctly identify both the left\nand right boundaries. For example, if there is a\npair of a reference segmentation and a prediction,\nReference: do you see a boy\nPrediction: doyou see a boy\nthen 4 words are discovered in the prediction\nwhere the reference has 5 words. 3 words in the\nprediction match with the reference. In this case,\nwe report scores as precision = 75.0 (3/4), recall\n= 60.0 (3/5), and F1, the harmonic mean of preci-\nsion and recall, 66.7 (2/3). To facilitate compari-\nson with previous work, segmentation results are\nreported on the union of the training, validation,\nand test sets.\nSentence Char. Types Word Types Characters Average Word Length\nTrain Valid Test Train Valid Test Train Valid Test Train Valid Test Train Valid Test\nBR-text 7832 979 979 30 30 29 1237 473 475 129k 16k 16k 3.82 4.06 3.83\nBR-phono 7832 978 978 51 51 50 1183 457 462 104k 13k 13k 2.86 2.97 2.83\nPTB 42068 3370 3761 50 50 48 10000 6022 6049 5.1M 400k 450k 4.44 4.37 4.41\nCTB 50734 349 345 160 76 76 60095 1769 1810 3.1M 18k 22k 4.84 5.07 5.14\nPKU 17149 1841 1790 90 84 87 52539 13103 11665 2.6M 247k 241k 4.93 4.94 4.85\nCOCO 8000 2000 10000 50 42 48 4390 2260 5072 417k 104k 520k 4.00 3.99 3.99\nTable 6: Summary of Dataset Statistics.\nmax len (L) min freq (F) λ\nBR-text 10 10 7.5e-4\nBR-phono 10 10 9.5e-4\nPTB 10 100 5.0e-5\nCTB 5 25 1.0e-2\nPKU 5 25 9.0e-3\nCOCO 10 100 2.0e-4\nTable 7: Hyperparameter values used."
}