{
  "title": "STAR: Sparse Transformer-based Action Recognition",
  "url": "https://openalex.org/W3178676225",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2065720840",
      "name": "Shi, Feng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lee, Chonghan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125638754",
      "name": "Qiu Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116243516",
      "name": "Zhao, Yizhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2332317912",
      "name": "Shen Tian-Yi",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Muralidhar, Shivran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120511635",
      "name": "Han Tian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202042597",
      "name": "Zhu, Song-Chun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2943878792",
      "name": "Narayanan, Vijaykrishnan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3138121279",
    "https://openalex.org/W2085731903",
    "https://openalex.org/W2048821851",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W2143267104",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2906943923",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2991516293",
    "https://openalex.org/W2128310833",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W2603861860",
    "https://openalex.org/W3092754310",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W2767286248",
    "https://openalex.org/W3049455300",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W2799006218",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2618253408",
    "https://openalex.org/W3035050855",
    "https://openalex.org/W2940457086",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2918342466"
  ],
  "abstract": "The cognitive system for human action and behavior has evolved into a deep learning regime, and especially the advent of Graph Convolution Networks has transformed the field in recent years. However, previous works have mainly focused on over-parameterized and complex models based on dense graph convolution networks, resulting in low efficiency in training and inference. Meanwhile, the Transformer architecture-based model has not yet been well explored for cognitive application in human action and behavior estimation. This work proposes a novel skeleton-based human action recognition model with sparse attention on the spatial dimension and segmented linear attention on the temporal dimension of data. Our model can also process the variable length of video clips grouped as a single batch. Experiments show that our model can achieve comparable performance while utilizing much less trainable parameters and achieve high speed in training and inference. Experiments show that our model achieves 4~18x speedup and 1/7~1/15 model size compared with the baseline models at competitive accuracy.",
  "full_text": "STAR: Sparse Transformer-based Action Recognition\nFeng Shi1, Chonghan Lee2, Liang Qiu1, Yizhou Zhao1, Tianyi Shen2, Shivran Muralidhar2, Tian\nHan3, Song-Chun Zhu1, Vijaykrishnan Narayanan2\n1University of California Los Angeles\n2The Pennsylvania State University\n3Stevens Institute of Technology\nAbstract\nThe cognitive system for human action and behavior has\nevolved into a deep learning regime, and especially the ad-\nvent of Graph Convolution Networks has transformed the\nﬁeld in recent years. However, previous works have mainly\nfocused on over-parameterized and complex models based\non dense graph convolution networks, resulting in low efﬁ-\nciency in training and inference. Meanwhile, the Transformer\narchitecture-based model has not yet been well explored for\ncognitive application in human action and behavior estima-\ntion. This work proposes a novel skeleton-based human ac-\ntion recognition model with sparse-attention on the spatial\ndimension and segmented linear attention on the temporal di-\nmension of data. Our model can also process variable length\nof video clips grouped as a single batch. Experiments show\nthat our model can achieve comparable performance while\nutilizing much less trainable parameters and achieve high\nspeed in training and inference. Experiments show that our\nmodel achieves 4∼18× speedup and 1\n7 ∼ 1\n15 model size com-\npared with the baseline models at competitive accuracy.\nIntroduction\nHuman action recognition plays a crucial role in many real-\nworld applications, such as holistic scene understanding,\nvideo surveillance, and human-computer interaction (Htike\net al. 2014; Choi et al. 2008). In particular, skeleton-based\nhuman action recognition has attracted much attention in re-\ncent years and has shown its effectiveness. The skeleton rep-\nresentation contains a time series of 2D or 3D coordinates of\nhuman key-joints, providing dynamic body movement in-\nformation that is robust to variations of light conditions and\nbackground noises in contrast to raw RGB representation.\nEarlier skeleton-based human action recognition methods\nfocus on designing hand-crafted features extracted from the\njoint coordinates (Vemulapalli, Arrate, and Chellappa 2014;\nWang et al. 2012) and aggregating learned features using\nRNNs and CNNs (Yong Du, Wang, and Wang 2015; Xie\net al. 2018; Zhang et al. 2017; Liu, Tu, and Liu 2017; Ke\net al. 2017). However, these methods rarely explore the re-\nlations between body joints and result in unsatisfactory per-\nformance. Recent methods focus on exploring the natural\nconnection of human body joints and successfully adopted\nthe Graph Convolutional Networks (GCNs), especially for\nCopyright © 2021, Under review, conﬁdential, All rights reserved.\nnon-Euclidean domain data, similar to Convolutional Neural\nNetworks (CNNs) but executing convolutional operations to\naggregate the connected and related joints’ features. Yan et\nal. (Yan, Xiong, and Lin 2018) proposed a ST-GCN model\nto extract discriminative features from spatial and temporal\ngraphs of body joints. Following the success of ST-GCN,\nmany works proposed optimizations to ST-GCN to improve\nthe performance and network capacity (Shi et al. 2019; Li\net al. 2019; Liu et al. 2020).\nHowever, the existing GCN-based models are often im-\npractical in real-time applications due to their vast compu-\ntational complexity and memory usage. The baseline GCN\nmodel, e.g., ST-GCN, consists of more than 3.09 million pa-\nrameters and costs at least 16.2 GFLOPs to run inference on\na single action video sample (Yan, Xiong, and Lin 2018).\nDGNN, which is composed of incremental GCN modules,\neven contains 26 million model parameters. (Shi et al. 2019)\nSuch high model complexity leads to difﬁculty in model\ntraining and inference, makes the model not suitable for de-\nployment on edge devices. Furthermore, these GCN-based\nmodels process ﬁxed-size action videos by padding repet-\nitive frames and zeros to match the maximum number of\nframes and persons depicted in the videos. These additional\npaddings increase the latency and memory required hinder-\ning their adoption in real-time and embedded applications.\nThis paper proposes a sparse transformer-based action\nrecognition (STAR) model as a novel baseline for skeleton\naction modeling to address the above shortcomings. Trans-\nformers have been a popular choice in natural language pro-\ncessing. Recently, they have been employed in computer vi-\nsion to attain competitive results compared to convolutional\nnetworks, while requiring fewer computational resources to\ntrain (Dosovitskiy et al. 2020; Huang et al. 2019). Inspired\nby these Transformer architectures, our model consists of\nspatial and temporal encoders, which apply sparse atten-\ntion and segmented linear attention on skeleton sequences\nalong the spatial and temporal dimension, respectively. Our\nsparse attention module along the spatial dimension per-\nforms sparse matrix multiplications to extract correlations of\nconnected joints, whereas previous approaches utilize dense\nmatrix multiplications where most of the entries are ze-\nros, causing extra computation. The segmented linear atten-\ntion mechanism along temporal dimension further reduces\nthe computation and memory usage by processing variable\narXiv:2107.07089v1  [cs.CV]  15 Jul 2021\nlength of sequences. We also apply segmented positional en-\ncoding to the data embedding to provide the concept of time-\nseries ordering along the temporal dimension of variable-\nlength skeleton data. Additionally, segmented context at-\ntention performs weighted summarization across the entire\nvideo frames, making our model robust compared to GCN-\nbased models with their ﬁxed-length receptive ﬁeld on the\ntemporal dimension.\nCompared to the baseline GCN model (ST-GCN), our\nmodel (STAR) achieves higher performance with much\nsmaller model size on the two datasets, NTU RGB+D 60\nand 120. The major contributions of this work are listed be-\nlow:\n• We focus on designing an efﬁcient model purely\nbased on self-attention mechanism. We propose sparse\ntransformer-based action recognition (STAR) model that\nprocess variable length of skeleton action sequence with-\nout additional preprocessing and zero paddings. The ﬂex-\nibility of our model is beneﬁcial for real-time applications\nor edge platforms with limited computational resources.\n• We propose a sparse self-attention module that efﬁciently\nperforms sparse matrix multiplications to capture spatial\ncorrelations between human skeleton joints.\n• We propose a segmented linear self-attention module\nthat effectively captures temporal correlations of dynamic\njoint movements across time dimension.\n• Experiments show that our model is 5 ∼7×smaller than\nthe baseline models while providing 4 ∼18×execution\nspeedup.\nRelated works\nSkeleton-Based Action Recognition\nRecently, skeleton-based action recognition has attracted\nmuch attention since its compact skeleton data represen-\ntation makes the models more efﬁcient and free from the\nvariations in lighting conditions and other environmental\nnoises. Earlier methods to skeleton-based action modeling\nhave mainly worked on designing hand-crafted features and\nrelations between joints (). Recently, by looking into the in-\nherent connectivity of the human body, Graph Convolutional\nNetworks (GCNs), especially, ST-GCNs have gained mas-\nsive success in getting satisfactory results in this task. The\nmodel consists of spatial and temporal convolution mod-\nules similar to conventional convolutional ﬁlters used for\nimages (Yan, Xiong, and Lin 2018). The graph adjacency\nmatrix encodes the skeleton joints’ connections and extracts\nhigh-level spatial representations from the skeleton action\nsequence. On the temporal dimension, 1D convolutional ﬁl-\nters facilitate extracting dynamic information.\nMany following works have proposed improvements to\nST-GCN to improve the performance. Li et al. (Li et al.\n2019) proposed AS-GCN, which leveraged the potential of\nadjacency matrices to scale the human skeleton’s connectiv-\nity. Furthermore, they generated semantic links to capture\nbetter structural and action semantics with additional infor-\nmation aggregation. Lei et al. (Shi et al. 2019a) proposed\nDirected Graph Neural Networks (DGNNs), which incorpo-\nrate joint and bone information to represent the skeleton data\nas a directed acyclic graph. Liu et al. (Liu et al. 2020) pro-\nposed a uniﬁed spatial-temporal graph convolution module\n(G3D) to aggregate information across space and time for\neffective feature learning.\nSome studies have been focusing on the computational\ncomplexity of GCN-based methods. Cheng et al. (Cheng\net al. 2020) proposed Shift-GCN, which leverages shift\ngraph operations and point-wise convolutions to reduce the\ncomputational complexity. Song et al. (Song et al. 2020)\nproposed multi-branch ResGCN that fuses different spatio-\ntemporal features from multiple branches and used resid-\nual bottleneck modules to obtain competitive performance\nwith less number of parameters. Compared to these meth-\nods, our spatial and temporal self-attention modules have\nseveral essential distinctions: our model can process vari-\nable length of skeleton sequence without preprocessing with\nzero-paddings. Our model can retrieve global context on the\ntemporal dimension by applying self-attention to the input\nsequence’s entire frames.\nTransformers and Self-Attention Mechanism\nVaswani et al. (Vaswani et al. 2017) ﬁrst introduced Trans-\nformers for machine translation and have been the state-of-\nthe-art method in various NLP tasks. For example, GPT and\nBERT (Radford et al. 2018; Devlin et al. 2019) are currently\nthe Transformer-based language models that have achieved\nthe best performance. The core component of Transformer\narchitectures is a self-attention mechanism that learns the re-\nlationships between each element of a sequence. In contrast\nto recurrent networks that process sequence in a recursive\nfashion and are limited to attention on short-term context,\ntransformer architectures enable modeling long dependen-\ncies in sequence. Furthermore, the multi-head self-attention\noperations can be easily parallelized. Recently, Transformer-\nbased models have attracted much attention in the computer\nvision community. Convolution operation has been the core\nof the conventional deep learning models for computer vi-\nsion tasks. However, there are downfalls to the operation.\nThe convolution operates on a ﬁxed-sized window, which\nonly captures short-range dependencies. The same applies to\nGCNs where the Graph Convolution operation is incapable\nof capturing long-range relations between joints in both spa-\ntial and temporal dimensions.\nVision Transformer (ViT) (Dosovitskiy et al. 2020) is the\nﬁrst work to completely replace standard convolutions in\ndeep neural networks on large-scale image recognition tasks.\nHuang et al. (Huang et al. 2019) explored the sparse atten-\ntion to study the trade-off between computational efﬁciency\nand performance of a Transformer model on the image clas-\nsiﬁcation task. A recent study (Plizzari, Cannici, and Mat-\nteucci 2020) proposed a hybrid model consists of the Trans-\nformer encoder and GCN modules on the skeleton-based hu-\nman action recognition task. Nevertheless, no prior study has\ncompletely replaced GCNs with the Transformer architec-\nture to the best of our knowledge.\nMethodology\nIn this section, we present the algorithms used in our model\nand the relevant architecture of our model.\nSection depicts the sparse multi-head self-attention\n(MHSA) mechanism used in spatial Transformer encoder\nmodule; Section introduces the novel data format and the\nrelevant linear multi-head self-attention (MHSA) mecha-\nnism for temporal Transformer encoder; Section shows the\noverall framework of our model and related auxiliary mod-\nules.\nQ \nK \ni \nj \ni j \nArtificial AttentionReal Attention\nNumber of joints \nFigure 1: Illustration of our Sparse attention module: Given\nthe queries Q and the keys K of the skeleton embedding,\nfeature vectors of joint iand j are correlated with attention\nweight αi,j The solid black line on the skeleton represents\nthe physical connection of human skeleton. The dashed line\nconnecting two joints represents the artiﬁcial attention of\njoints.\nSpatial domain: Sparse MHSA\nThe crucial component of our spatial transformer encoder\nis the sparse multi-head self-attention module. GCN-based\nmodels and previous Transformer models, such as ST-GCN\nand ST-TR, utilize dense skeleton representation to aggre-\ngate the features of neighboring nodes. This dense adja-\ncency matrix representation contains 625 entries for the\nNTU dataset, while the actual number of joint connections\nrepresenting the skeletons is only 24. It means that 96% of\nthe matrix multiplications are unnecessary calculations for\nzero entries. So we propose a sparse attention mechanism,\nwhich only performs matrix multiplications on the sparse\nnode connections. This allows each joint to only aggregate\nthe information from its neighboring joints based on the at-\ntention coefﬁcients, which are dynamically assigned to the\ncorresponding connections.\nThe joint connections are based on the topology of skele-\nton, which is a tree structure. The attentions inherited from\nthis topology are seen as physical attention (or real atten-\ntion), as illustrated in Figure 1. To augment the attending\nﬁeld, we also artiﬁcially add more links between joints ac-\ncording to the logical relations of body parts, and we call\nthese artiﬁcially created attentions as artiﬁcial attention, as\nthe dashed yellow arrows shown in Figure 1. For simplic-\nity, suppose that the skeleton adjacency matrix isA, then the\nartiﬁcial links for additional spatial attention are obtained\nthrough A2 and A3. Hence, in our model, the spatial atten-\ntion maps are evaluated based on the topology representation\nof A+ A2 + A3.\nThe sparse attention is calculated according to the con-\nnectivity between joints. As described in below equations:\nafter the embedding in Equation 1, the joint-to-joint atten-\ntion between a pair of connected joints is computed ﬁrst by\nan exponential score of the dot product of the feature vectors\nof these two joints (Equation 2), then the score is normalized\nby the sum of exponential scores of all neighboring joints as\ndescribed in Equation 3.\nQ= XWq,K = XWk,V = XWv (1)\nαi,j = ⟨qi,kj⟩∑\nn∈N(i) ⟨qi,kn⟩ (2)\nv′\ni =\n∑\nj∈N(i)\nαi,jvj, or V′= AV (3)\nwhere Q, K, and V are queries, keys, and values in\nTransformer’s terminology, respectively; and qi = Q(i),\nkj = K(j), vj = V(j), and ⟨q,k⟩= exp\n(\nqT k√\nd\n)\n. Finally,\nwe obtain attention mapsAas multi-dimension (multi-head)\nsparse matrices sharing the identical topology described by\na single adjacency matrix (including links for the artiﬁcial\nattention), where attention coefﬁcients are A(i,j) = αi,j.\nThe sparse operation can be fulﬁlled with tensor gathering\nand scattering operations for parallelism.\nTemporal domain: Segmented Linear MHSA\nbatch size =  \nconcatenate into one tensor \nremove zero frames and remove zero-valued person\nbatch indices\nframes\nFigure 2: Illustration of our data format used in our frame-\nwork: Previous works used the upper data format, which has\nﬁxed-sized time and person dimensions. Our work adopts\nnew data format on the bottom, which has combined batch,\nperson, and time dimensions into a single variable length se-\nquence.\nThe most apparent drawbacks in the previous approaches\n(Yan, Xiong, and Lin 2018; Shi et al. 2019b) are utilizing (1)\nthe ﬁxed number of frames for each video clip and (2) zero-\nﬁlling for the non-existing second person. The ﬁrst draw-\nback constrains their scalability to process video clips longer\nthan the predeﬁned length and their ﬂexibility on a shorter\nvideo clip. The second drawback due to the zero’s participa-\ntion in computation causes latency degradation. Moreover,\na signiﬁcant amount of memory space is allocated to those\nzero-valued data during the computation. So we propose a\ncompact data format to bypass these drawbacks. Also, we\npropose Segmented Linear MHSA to process our compact\ndata format.\nVariable Frame Length Data Format The Figure 2\nshows the comparison between our data format and the for-\nmat used by previous works. In the data format adopted by\nprevious works, longer videos are cut off to the predeﬁned\nlength and shorter videos are padded with repeated frames.\nFurthermore, the frames with a single person are all zero-\npadded to match the ﬁxed number of persons. The upper\ndata format from Figure2 illustrates the NTU RGB+D data\nformat used by previous works. In each ﬁxed-length video\nV(i), P(i)\n1 and P(i)\n2 represent two persons. In NTU RGB+D\n120 dataset, only 26 out of 120 actions are mutual actions,\nwhich means that the second person’s skeleton is just zeros\n(P(i)\n2 = 0 in Figure 2) in most data samples. In contrast\nto the previous data format, the proposed format maintains\nthe original length of each video clip. Additionally, when a\nvideo clip contains two persons, we concatenate them along\nthe frame dimension. Instead of keeping an individual di-\nmension for a batch of video clips, we further concatenate\nthe video clips in a batch along the frame dimension, and\nthe auxiliary vector stores the batch indices to indicate to\nwhich video clip a frame belongs, as shown in the bottom\ndata format of Figure 2. Moreover, given the new dimen-\nsions (N, V, C) as shown in Figure 2, where N is the total\nnumber of frames after concatenating the video clips along\nthe temporal dimension and V is the number of skeleton’s\njoints, we regard dimension N as the logical batch size for\nspatial attention and dimension V as the logical batch size\nfor temporal attention.\nSegmented Linear Attention With the new data format\nintroduced in the previous section, we propose a novel linear\nmulti-head attention tailored for this data format. We call\nit a Segmented Linear Attention. As stated in the previous\nsections, Transformers are originally designed for sequential\ndata. In the human skeleton sequence, each joint’s dynamic\nmovement across the frames can be regarded as a time series.\nTherefore, the 3D coordinates, i.e., (x,y,z ), of every joint\ncan be processed individually through the trajectory along\nthe time dimension, and the application of attention extracts\nthe interaction among time steps represented by frames.\nLinear Attention. Standard dot product attention mech-\nanism (Vaswani et al. 2017) (Equation 4) with the global\nreceptive ﬁeld of N inputs are prohibitively slow due to\nthe quadratic time and memory complexity O(N2). The\nquadratic complexity also makes Transformers hard to train\nand limits the context. Recent research toward the lin-\nearized attention mechanism derives the approximation of\nthe Softmax-based attention. The most appealing ones are\nlinear Transformers (Katharopoulos et al. 2020; Choroman-\nski et al. 2021; Shen et al. 2021) based on kernel func-\ntions approximating the Softmax. The linearized Transform-\ners can improve inference speeds up to three orders of mag-\nnitude without much loss in predictive performance (Tay\net al. 2020). Given the projected embeddings Q, K, and V\nfor input tensors of queries, keys, and values, respectively,\naccording to the observation from the accumulated value\nV′\ni ∈Rd for the query Qi ∈Rd in position i, dis the chan-\nnel dimension, the linearized attention can be transformed\nfrom Equation 4 to Equation 5, the computational complex-\nity is reduced to O(Nd), when d is much smaller than N,\nthe computational complexity is approaching linear O(N):\nV′\ni =\n∑N\nj=1 ⟨Qi,Kj⟩Vj\n∑N\nj=1 ⟨Qi,Ki⟩\n(4)\nV′\ni =\nφ(Qi)T ∑N\nj=1 φ(Kj)VT\nj\nφ(Qi)T ∑N\nj=1 φ(Kj)\n= φ(Qi)T U\nφ(Qi)T Z\nU =\nN∑\nj=1\nφ(Kj)VT\nj , Z =\nN∑\nj=1\nφ(Kj)\n(5)\nwhere φ(·) is the kernel function. In work of (Katharopou-\nlos et al. 2020), kernel function is simply simulated with\nELU, φ(x) = elu(x) + 1; while (Choromanski et al.\n2021) introduces the Fast Attention via Orthogonal Ran-\ndom Feature (FA VOR) maps as the kernel function,φ(x) =\nc√\nM f(Wx + b)T , where c > 0 is a constant, and W ∈\nRM×d is a Gaussian random feature matrix, and M is the\ndimensionality of this matrix that controls the number of\nrandom features.\nSegmented Linear Attention. Since we concatenate the\nvarious length of video clips within a single batch along\nthe time dimension, directly applying linear attention will\ncause the cross clip attention, leading to irrelevant informa-\ntion taken into account from one video clip to another, as\nshown in Figure 4. Therefore, we consider the frames of a\nvideo clip arranged as a segment, and then we design the\nsegmented linear attention by reformulating Equation 5 with\nsegment index. Therefore, for each Vi in segment Sm, we\nsummarize\nV′\ni∈Sm =\nφ(Qi∈Sm)T ∑\nj∈Sm φ(Kj)VT\nj\nφ(Qi∈Sm)T ∑\nj∈Sm φ(Kj)\n= φ(Qi∈Sm)T USm\nφ(Qi∈Sm)T ZSm\nUSm =\n∑\nj∈Sm\nφ(Kj)VT\nj , Z Sm =\n∑\nj∈Sm\nφ(Kj)\n(6)\nwhere Sm is the m-th segment, and the reduction oper-\nation ∑\nj∈Sm f(x) can be easily implemented through the\nindexation to segments; and with help of the gathering and\nscattering operations (Fey 2021a), the segmented linear at-\ntention maintains the highly-paralleled computation. Figure\n3 illustrates the comparison of different attention operations.\n... \n.... \n.... \n(a) standard attention (b) linearized attention(c) segmented linear attention\nFigure 3: Illustration of Different Attention Operations: (a) Standard attention is obtained from Softmax(QKT )V with a\ncomplexity of O(n2). (b) Linearized attention φ(Q)(φ(KT )V) with kernel function φ(·) reduces the complexity to O(n), (c)\nwe extend the linearized attention (b) to process segments of sequences.\n0 0  0\n1  1  1    1  1\n2..2\n3 ... 3\n... \n... \n... .... \nirrelevant\nattention cross\nvideo clips\nthe attention map of video clip \nframe indices\nin a batch\nFigure 4: Segmented Attention: directly applying the lin-\nearized attention to our new data format will calculate un-\nexpected attention between the two irrelevant video clips,\nwhich is error-prone. Therefore, we use segmented attention\ncorresponding to each video sequence.\nSTAR Framework\nIn this work, we propose the Sparse-Transformer Ac-\ntion Recognition (STAR) framework. Figure 5 (c) shows\nthe overview of our STAR framework. The STAR frame-\nwork is built upon several Spatial-Temporal Transformer\nblocks (ST-block) followed by context-aware attention and\nMLP head for classiﬁcation. Each ST-block comprises two\npipelines: the spatial Transformer encoder and the tempo-\nral Transformer encoder. Each Transformer encoder consists\nof several key components, including the multi-head self-\nattention (MHSA), skip connection(AND & Norm part in\nFigure 5 (c)), and feed-forward network. The spatial Trans-\nformer encoder utilizes sparse attention to capture the topo-\nlogical correlation of connected joints for each frame. The\ntemporal Transformer encoder utilizes the segmented lin-\near attention to capture the correlation of joints along the\ntime dimension. The output sum from the two encoder lay-\ners is fed to the context-aware attention module to perform\nweighted summarization on the sequence of frames. Posi-\ntional encoding is also utilized before ST-block to provide\nthe context of ordering on the input sequence. Below is a\nbrief introduction to each of them.\nContext-aware attention In previous works (Yan, Xiong,\nand Lin 2018; Shi et al. 2019b), before connecting to the\nﬁnal fully-connected layer for classiﬁcation, summarizing\nthe video clip embedding along the temporal dimension is\nimplemented by global average pooling. Alternatively, we\nutilize a probabilistic approach throughcontext-aware atten-\ntion, which is extended from the work of (Bai et al. 2019), to\nenhance this step’s robustness, as demonstrated in Figure 6.\nDenote an input tensor embedding of video clip Sm as V ∈\nRF×N×D, for F is the number of frames in video clip Sm,\nN is the number of joints of skeleton, and each joint pos-\nsessing Dfeatures, where vi ∈RN×D is the embedding of\nframe iof V. First, aglobal context c∈RN×D is computed,\nwhich is a simple average of embedding of frames followed\nby a nonlinear transformation: c= tanh\n(\n1\nF W∑F\ni∈Sm vi\n)\n,\nwhere W ∈RD×D is a learnable weight matrix. The con-\ntext cprovides the global structural and feature information\nof the video clip that is adaptive to the similarity between\nframes in video clip Sm, via learning the weight matrix.\nBased on c, we can compute one attention weight for each\nframe. For frame i, to make its attention an aware of the\nglobal context, we take the inner product between cand its\nembedding. The intuition is that, frames similar to the global\ncontext should receive higher attention weights. A sigmoid\nfunction σ(x) = 1\n1+exp(−x) is applied to the result to en-\nsure the attention weights is in the range (0,1). Finally, the\nvideo clip embedding v′ ∈RN×D is the weighted sum of\nvideo clip embeddings, v′ = ∑F\ni∈Sm aivi. The following\nequations summarize the proposed context-aware attentive\nST-Transformer block \n+ \ncontext\nattention\nTemporal Transformer Encoder\nAdd & Norm\nFeed Forward\nAdd & Norm\nSegmented\nLinear MHSA\nTemporal Transformer\nEncoder Layer\nSpatial Transformer Encoder\nAdd & Norm\nSparse MHSA\nAdd & Norm\nFeed Forward\nSpatial Transformer\nEncoder Layer\n(a) Temporal Transformer\nEncoder Layer: the attention\nmodule is a Segmented Linear\nMHSA\nContext\nAttention\nMLP head\n(b) Spatial Transformer Encoder\nLayer: The attention module is a\nSparse MHSA\n(c) The architecture of our\nmodel: consists of N  ST -\nTransformer block\nFigure 5: Illustration of the overall pipeline of our approach (STAR)\nA batch of video clips with variable length of frames\n is the context-aware attention \nof -th frame of -th video clip \nFigure 6: The context-aware attention is utilized to summa-\nrize each video clip.\nmechanism:\nc= tanh\n\n1\nFW\nF∑\nj∈Sm\nvj\n\n= tanh\n(1\nF\n(\nVT ·1\n)\nW\n)\nv′=\nF∑\ni∈Sm\nσ\n\nvT\ni\n\ntanh\n\n1\nFW\nF∑\nj∈Sm\nvj\n\n\n\n\n\nvi\n=\nF∑\ni∈Sm\nσ\n(\nvT\ni c\n)\nvi = [σ(Vc)]T V\n(7)\nPositional Encoding As the attention mechanism is order-\nagnostic to the permutation in the input sequence (Vaswani\net al. 2017; Tsai et al. 2019) and treats the input as an un-\nordered bag of element. Therefore, an extra positional em-\nbedding is necessary to maintain the data order, i.e., time-\nseries data are in the inherently sequential ordering. Then\nthese positional embedding are participating the evaluation\nof the attention weight and value between token iand j in\nthe input sequence.\nSegmented Sequential Positional Encoding However,\nas we arrange the variable-length video clips into a batch\nalong the temporal dimension, it is not feasible to directly\napply positional encoding to the whole batch. Therefore,\nwe introduce the segmented positional encoding where each\nvideo clip gets its positional encoding according to batch in-\ndices. An example of such encoding is shown in Figure 7.\nStructural Positional Encoding. we also attempt to ap-\nply the structural positional encoding, e.g., tree-based po-\nsitional encoding (Shiv and Quirk 2019; Omote, Tamura,\nand Ninomiya 2019), to the spatial dimension, i.e., the tree\ntopology of skeleton. Experiments show that the current ap-\nproach which we used cannot improve our model’s perfor-\nmance signiﬁcantly. Hence, to reduce our model’s complex-\nity, we decide not to apply the structural positional encoding\nfor this work and leave it for future research.\n0 20 40 60 80\n0\n2\n4\n6\n8\n10\n12\n14\n0\n1\nFigure 7: Illustration of Segmented Positional Encoding for\na batch of 4 video clips. x-axis represents the number of\nframes and y-axis represents the feature dimension.\nExperiments\nIn this section, we conduct experiments and ablation stud-\nies to verify the effectiveness and efﬁciency of our proposed\nsparse spatial and segmented linear temporal self-attention\noperations. The comparison has been made with ST-GCN,\nthe baseline GCN model, and ST-TR, one of the state-of-\nthe-art hybrid model, which have utilized full attention op-\neration coupled with graph convolutions. The correspond-\ning analysis demonstrates the potential of our model and the\npossible room for improvements.\nDatasets\nIn the experiments, we evaluate our model on two largest\nscale 3D skeleton-based action recognition datasets, NTU-\nRGB+D 60 and 120.\nNTU RGB+D 60 This dataset contains 56,880 video clips\ninvolving 60 human action classes. The samples are per-\nformed by 40 volunteers and captured by three Microsoft\nKinect v2 cameras (Shahroudy et al. 2016). It contains\nfour modalities, including RGB videos, depth sequences, in-\nfrared frames, and 3D skeleton data. Our experiments are\nonly conducted with the 3D skeleton data. The length of the\nNTU-60 NTU-120\nMethod X-subject X-view X-subject X-setup\nST-GCN 81.5 88.3 72.4 71.3\nST-TR 88.7 95.6 81.9. 84.1\nSTAR-64 (ours) 81.9 88.9 75.4 78.1\nSTAR-128 (ours) 83.4 89.0 78.3 80.2\nTable 1: Comparison of models’ accuracy on NTU RGB+D 60 and 120 datasets\nModel CUDA time (ms) num. of parameters GMACs\nST-GCN 333.89 3.1M 261.49\nST-TR 1593.05 6.73M 197.55\nSTAR-64 (ours) 86.54 0.42M 15.58\nSTAR-128 (ours) 191.23 1.26M 73.33\nTable 2: Comparison of models’ efﬁciency\naction samples vary from 32 frames to 300 frames. In each\nframe, there are at most 2 subjects and each subject con-\ntains 25 3D joint coordinates. The dataset follows two eval-\nuation criteria, which are Cross-Subject and Cross-View. In\nthe Cross-View evaluation (X-View), there are 37,920 train-\ning samples captured from camera 2 and 3 and 18,960 test\nsamples captured from camera 1. In the Cross-Subject eval-\nuation (X-Sub), there are 40,320 training samples from 20\nsubjects and 26,560 test samples from the rest. We follow\nthe original two benchmarks and report the Top-1 accuracy\nas well as the proﬁling metrics.\nNTU RGB+D 120 The dataset (Liu et al. 2019) extends\nfrom NTU RGB+D 60 and is currently the largest dataset\nwith 3D joint annotations. It contains 57,600 new skeleton\nsequences representing 60 new actions, a total of 114,480\nvideos involving 120 classes of 106 subjects captured from\n32 different camera setups. The dataset follows two crite-\nria, which are Cross-Subject and Cross-Setup. In the Cross-\nSubject evaluation, similar to the previous dataset, splits\nsubjects in half to training and testing dataset. In the Cross-\nSetup evaluation, the samples are divided by the 32 camera\nsetup IDs, where the even setup IDs are for training and the\nodd setup IDs for testing. Similar to the previous dataset,\nthere is no preprocessing to set the uniform video length for\nall the samples. We follow the two criteria and report the\nTop-1 accuracy and the proﬁling metrics.\nUnlike GCN-based models, where the length of all the\nsamples and the number of subjects need to be ﬁxed (e.g.\n300 frames and 2 subjects), our model can process varying\nlength of input samples and of the number of subjects. So no\nfurther preprocessing with padding is done on the samples.\nConﬁguration of experiments\nImplementation details . As the original Transformer\nframework (Vaswani et al. 2017) employs the uniﬁed model\nsize dfor every layer, we follow the same notion and keep\nthe hidden channel size uniform across the attention heads\nand the feedforward networks. We run the experiments with\ntwo different hidden channel sizes, 64 and 128 for our Trans-\nformer encoders (STAR-64 and STAR-128), respectively.\nThe hidden channel size of the MLP head is also propor-\ntional to that of the attention heads. Our model consists of\n5 layers, each layer comprises one spatial encoder and one\ntemporal encoder in parallel and the output sum from the\ntwo encoders is fed to the next layer. Drop rates are set to\n0.5 for every module. We also replace the ReLU non-linear\nactivation funciton with SiLU (or Swish) (Elfwing, Uchibe,\nand Doya 2018; Ramachandran, Zoph, and Le 2017) to in-\ncrease the stability of gradients in back-propagation phase\n(GELU or SELU also bring similar effect). Our model is\nimplemented with the deep learning framework PyTorch\n(Paszke et al. 2019) and its extension PyTorch Geometric\n(Fey and Lenssen 2019). The scattering/gathering opera-\ntions and sparse matrix multiplications are based on PyTorch\nScatter (Fey 2021a) and PyTorch Sparse (Fey 2021b), re-\nspectively.\nTraining setting . The maximum number of training\nepochs is set to 100. We used the Adam optimizer (Kingma\nand Ba 2014) with β1 = 0.9, β2 = 0.98 and ϵ = 10−9.\nFollowing the setting of the original Transformer paper\n(Vaswani et al. 2017), the learning rate is adjusted through-\nout the training:\nlrate= d−0.5 ·min(t−0.5,t ·w−1.5) (8)\nwhere d is the model dimension, s is step number, and w\nis the warmup steps. According to the equation 8, the learn-\ning rate linearly increases for the ﬁrst w training steps and\nthen decreases proportionally to the inverse square root of\nthe step number. We keep the original settings for the base-\nline models in their papers (Yan, Xiong, and Lin 2018; Pliz-\nzari, Cannici, and Matteucci 2020), and use their codes pro-\nvided online. All our training experiments are performed on\na system with two GTX TITAN X GPUs and a system with\none TITAN RTX GPU, while the inferences are executed on\na single GPU.\nResults and Analysis\nWe evaluate the accuracy and the efﬁciency of the baseline\nGCN model (ST-GCN), our model (STAR) and the hybrid\nmodel (ST-TR), which utilize both transformer and GCN\nframeworks.\nMACs Parameters Latency\nST-GCN\nConv2d: 260.4 GMACs\nBatchNorm2d: 737.3 MMACs\nReLU: 184.3 MMACs\nConv2d: 3.06M\nBatchNorm2d: 6.4K\nLinear: 15.4K\nConv2d: 149.92ms\nBatchNorm2d: 19.92ms\nReLU: 4.49ms\nST-TR\nConv2d: 810.57 GMACs\nMatMul: 161.1 GMACs\nBatchNorm2d: 138.4 MMACs\nConv2d: 2.7M\nBatchNorm2d: 10.5K\nLinear: 30.8K\nConv2d: 692.39ms\nMatMul: 161.38ms\nBatchNorm2d: 38.97ms\nSTAR-64\nMatMul(attention): 24.4 GMACs\nMul(sparse): 12.3 GMACs\nLinear: 6.2 GMACs\nLinear:83.2K\nLayerNorm: 1.3K\nMatMul: 25.27ms\nMul: 12.81ms\nLinear:6.53ms\nTable 3: The breakdown analysis and top-3 components in each metrics\nAccuracy We ﬁrst evaluate the effectiveness of our Trans-\nformer encoder based model compared to ST-TR and ST-\nGCN models. Each model’s accuracy is evaluated with the\nNTU RGB+D 60 and 120 testing datasets. As shown in the\nTable 1, our model outperforms ST-GCN in both cross-view\n(cross-set) and cross-subject benchmarks of the two dataset.\nOur model achieves 3.6 ∼7.7 percent lower accuracy com-\npared to ST-TR, which heavily relies on convolution-based\nkey components inherited from ST-GCN and utilizes them in\nboth spatial and temporal pipelines. Our model yields mod-\nest performance compared to the state-of-the-art models in\nNTU RGB+D 60 and 120 when trained from scratch. The\nFigure 11 shows that there exists a performance gap between\nthe training and testing. Transformer architectures’ lack of\ninductive biases, especially translation equivariance and lo-\ncality that are essential to convolution networks, could re-\nsult in weak generalization. In NLP, Transformers are usu-\nally pretrained on a large corpus of text and ﬁne-tuned on\na smaller task-speciﬁc dataset to boost the performance. We\nwould like to conduct extensive experiments on pre-training\nand ﬁne-tuning our model on a larger dataset in the future\nto improve the accuracy comparable to those of the state-\nof-the-art models. For our future study, we want to address\neffective generalization methods for Transformer models,\nwhich resolves overﬁtting issues and improve the overall\nperformance.\nEfﬁciency In this section, we evaluate the efﬁciency of the\ndifferent models. As shown in Table 2, our model (STAR) is\nsigniﬁcantly efﬁcient in terms of model size, the number of\nmultiply-accumulate operations (GMACs) and the latency.\nEach metric for the different models is evaluated by run-\nning inference with sample dataset. Our model is fed with\nthe original skeleton sequence of varying length. The other\ntwo models are fed with ﬁx-sized skeleton sequence padded\nto 300 frames and 2 persons. We use the ofﬁcial proﬁler of\nPyTorch (v1.8.1) (Paszke et al. 2019), and Flops-Proﬁler of\nDeepSpeed (Rasley et al. 2020) to measure the benchmarks.\nThe results are summarized with the following metrics:\nMACs. The number of Multiply-Accumulate (MAC) op-\nerations is used to determine the efﬁciency of deep learn-\ning models. Each MAC operation is counted as two ﬂoat-\ning point operations. With the same hardware conﬁguration,\nmore efﬁcient models require fewer MACs than other mod-\nels to fulﬁll the same task. As shown in Table 2, both of\nour model with different channel sizes execute only 1\n3 ∼ 1\n17\namount of GMACs (i.e., Giga MACs) compared to ST-GCN\nand ST-TR models, respectively.\nModel size. Model size is another metric to measure the\nefﬁciency of a machine learning model. Given the same\ntask, smaller model delivering the same or very close perfor-\nmance is preferable. Smaller model is not only beneﬁcial for\nthe higher speedup and less memory accesses but also gives\nbetter energy consumption, especially for embedded sys-\ntems and edge devices with scarce computational resources\nand small storage volume. The column of the number of pa-\nrameters in Table 2 depicts the size of the models, these pa-\nrameters are trainable weights in the model. Among all the\nmodel, STAR possesses the smallest model size, 0.42M and\n1.26M for STAT-64 and STAR-128, respectively.\nBreakdown analysis . The breakdown analysis is used\nto identify potential bottlenecks within different models\n(STAR-64, ST-TR, and ST-GCN). Table 3 provides the de-\ntailed proﬁling results for the top-3 computation modules\nthat are dominant in each models. According to Figure 8,\n9 and 10, the convolution operations cost signiﬁcant num-\nber of MAC operations and lead to computation bound.\nST-GCN and ST-TR mainly consist of the convolution op-\nerations followed by batch normalization, which requires\nrelatively large computational resources. Our Transformer\nmodel is based on sparse and linear attention mechanisms. It\nonly produces relatively small attention weights from sparse\nattention; and performs low-rank matrix multiplication for\nlinear attention (O(n)). This replaces huge dynamic weights\nof attention coefﬁcients from the standard attention mech-\nanism, which has a quadratic time and space complexity\n(O(n2)).\nAblation Study\n. In this section, we evaluate the effectiveness and efﬁciency\nof our sparse self-attention operation in spatial encoder com-\npared to the standard transformer encoder with full-attention\noperation. Table 4 and Table 5 show that our model with\nsparse self-attention operation achieves higher accuracy on\nboth X-subject and X-view benchmarks and use signiﬁ-\ncantly less number of GMACs and runtime. This shows that\nadditional correlations of distant joints calculated by full\nattention do not improve the performance but rather con-\ntribute noise to the prediction. To handle such issue, learn-\nable masks, consistent with adjacency matrix of skeleton,\n0 5 10 15 20 25\nST-GCN\nST-TR\nSTAR\nConv2d:260400.0 BatchNorm2d:737.0 ReLU:184.0\nConv2d:81570.0 BatchNorm2d:38.4 ReLU:19.2\nLinear:6200.0 SiLU:22.0\nMACs breakdown for top-3 modules (bar length taken log(KMACs))\nFigure 8: The breakdown of MACs for top-3 modules\n0 2 4 6 8 10 12\nST-GCN\nST-TR\nSTAR\nConv2d:3060.0 BatchNorm2d:6.4 Linear:15.4\nConv2d:7000.0 BatchNorm2d:1.5 Linear:30.8\nLinear:83.2 LayerNorm:1.3\n# of parameters breakdown for top-3 modules (bar length taken log(K))\nFigure 9: The breakdown of # parameters for top-3 modules\n0 2 4 6 8 10 12 14 16\nST-GCN\nST-TR\nSTAR\nConv2d:149.9 BatchNorm2d:19.9 ReLU:4.5\nConv2d:692.4 BatchNorm2d:39.0 MatMul:161.4\nLinear:6.5 Mul:12.8 MatMul:25.3\nlatency breakdown for top-3 modules (bar length taken log(us))\nFigure 10: The breakdown of latency for top-3 modules\n0 20 40 60 80 100\nEpoch\n20\n40\n60\n80\n100Accuracy\nTrain\nT est\n0 20 40 60 80 100\nEpoch\n20\n40\n60\n80\n100Accuracy\nTrain\nT est\nFigure 11: The training and testing curve for STAR-64(left) and STAR-128(right) on NTU-RGB+D 60 X-subject benchmark.\ncan be integrated to the full attention calculation to avoid\naccuracy degradation. But it requires extra computation in-\nvolving learnable masks.\nNTU-60 NTU-120\nMethod X-subject X-view X-subject X-setup\nSTAR (sparse) 83.4 84.2 78.3 78.5\nSTAR (full) 80.7 81.9 77.4 77.7\nTable 4: Classiﬁcation accuracy comparison between Sparse\nattention and Full attention on the NTU RGB+D 60 Skeleton\ndataset.\nModel CUDA time (ms) GMACs\nSTAR-sparse 105.7 15.58\nSTAR-full 254.7 73.33\nTable 5: Efﬁciency comparison between Sparse attention\nand Full attention on the NTU RGB+D 60 Skeleton dataset.\nConclusion\nIn this paper, we propose an efﬁcient Transformer-based\nmodel with sparse attention and segmented linear atten-\ntion mechanisms applied on spatial and temporal dimen-\nsions of action skeleton sequence. We demonstrate that our\nmodel can replace graph convolution operations with the\nself-attention operations and yield the modest performance,\nwhile requiring signiﬁcantly less computational and mem-\nory resources. We also designed compact data representation\nwhich is much smaller than ﬁxed-size and zero padded data\nrepresentation utilized by previous models. This work was\nsupported in part by Semiconductor Research Corporation\n(SRC).\nReferences\nBai, Y .; Ding, H.; Bian, S.; Chen, T.; Sun, Y .; and Wang, W.\n2019. SimGNN: A Neural Network Approach to Fast Graph\nSimilarity Computation. WSDM ’19, 384–392. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450359405. doi:10.1145/3289600.3290967. URL\nhttps://doi.org/10.1145/3289600.3290967.\nCheng, K.; Zhang, Y .; He, X.; Chen, W.; Cheng, J.; and\nLu, H. 2020. Skeleton-Based Action Recognition with Shift\nGraph Convolutional Network. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nChoi, J.; Cho, Y .-i.; Han, T.; and Yang, H. S. 2008. A View-\nBased Real-Time Human Action Recognition System as an\nInterface for Human Computer Interaction. In Wyeld, T. G.;\nKenderdine, S.; and Docherty, M., eds., Virtual Systems and\nMultimedia, 112–120. Berlin, Heidelberg: Springer Berlin\nHeidelberg. ISBN 978-3-540-78566-8.\nChoromanski, K. M.; Likhosherstov, V .; Dohan, D.; Song,\nX.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J. Q.; Mohi-\nuddin, A.; Kaiser, L.; Belanger, D. B.; Colwell, L. J.; and\nWeller, A. 2021. Rethinking Attention with Performers.\nIn International Conference on Learning Representations .\nURL https://openreview.net/forum?id=Ua6zuk0WRH.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , 4171–4186. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics. doi:10.18653/v1/N19-1423. URL https://www.\naclweb.org/anthology/N19-1423.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929 .\nElfwing, S.; Uchibe, E.; and Doya, K. 2018. Sigmoid-\nweighted linear units for neural network function approx-\nimation in reinforcement learning. Neural Networks 107:\n3–11.\nFey, M. 2021a. PyTorch Scatter. URL https://github.com/\nrusty1s/pytorch scatter.\nFey, M. 2021b. PyTorch Sparse. URL https://github.com/\nrusty1s/pytorch sparse.\nFey, M.; and Lenssen, J. E. 2019. Fast Graph Representation\nLearning with PyTorch Geometric. In ICLR Workshop on\nRepresentation Learning on Graphs and Manifolds.\nHtike, K. K.; Khalifa, O. O.; Mohd Ramli, H. A.; and\nAbushariah, M. A. M. 2014. Human activity recognition\nfor video surveillance using sequences of postures. In The\nThird International Conference on e-Technologies and Net-\nworks for Development (ICeND2014), 79–82. doi:10.1109/\nICeND.2014.6991357.\nHuang, Z.; Wang, X.; Huang, L.; Huang, C.; Wei, Y .; and\nLiu, W. 2019. Ccnet: Criss-cross attention for semantic seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 603–612.\nKatharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F.\n2020. Transformers are RNNs: Fast Autoregressive Trans-\nformers with Linear Attention. In III, H. D.; and Singh,\nA., eds., Proceedings of the 37th International Conference\non Machine Learning , volume 119 of Proceedings of Ma-\nchine Learning Research , 5156–5165. PMLR. URL http:\n//proceedings.mlr.press/v119/katharopoulos20a.html.\nKe, Q.; Bennamoun, M.; An, S.; Sohel, F.; and Boussaid, F.\n2017. A New Representation of Skeleton Sequences for 3D\nAction Recognition 4570–4579. doi:10.1109/CVPR.2017.\n486.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 .\nLi, M.; Chen, S.; Chen, X.; Zhang, Y .; Wang, Y .; and Tian,\nQ. 2019. Actional-Structural Graph Convolutional Net-\nworks for Skeleton-Based Action Recognition. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nLiu, H.; Tu, J.; and Liu, M. 2017. Two-Stream 3D Convolu-\ntional Neural Network for Skeleton-Based Action Recogni-\ntion .\nLiu, J.; Shahroudy, A.; Perez, M.; Wang, G.; Duan, L.-Y .;\nand Kot, A. C. 2019. Ntu rgb+ d 120: A large-scale bench-\nmark for 3d human activity understanding. IEEE transac-\ntions on pattern analysis and machine intelligence 42(10):\n2684–2701.\nLiu, Z.; Zhang, H.; Chen, Z.; Wang, Z.; and Ouyang, W.\n2020. Disentangling and Unifying Graph Convolutions\nfor Skeleton-Based Action Recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 143–152.\nOmote, Y .; Tamura, A.; and Ninomiya, T. 2019.\nDependency-Based Relative Positional Encoding for\nTransformer NMT. In Proceedings of the International\nConference on Recent Advances in Natural Language\nProcessing (RANLP 2019) , 854–861. Varna, Bulgaria:\nINCOMA Ltd. doi:10.26615/978-954-452-056-4 099.\nURL https://www.aclweb.org/anthology/R19-1099.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury,\nJ.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.;\nAntiga, L.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito,\nZ.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner,\nB.; Fang, L.; Bai, J.; and Chintala, S. 2019. PyTorch:\nAn Imperative Style, High-Performance Deep Learning\nLibrary. In Wallach, H.; Larochelle, H.; Beygelz-\nimer, A.; d'Alch ´e-Buc, F.; Fox, E.; and Garnett, R.,\neds., Advances in Neural Information Processing Sys-\ntems 32 , 8024–8035. Curran Associates, Inc. URL\nhttp://papers.neurips.cc/paper/9015-pytorch-an-imperative-\nstyle-high-performance-deep-learning-library.pdf.\nPlizzari, C.; Cannici, M.; and Matteucci, M. 2020. Spa-\ntial temporal transformer network for skeleton-based action\nrecognition. arXiv preprint arXiv:2008.07404 .\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by generative\npre-training .\nRamachandran, P.; Zoph, B.; and Le, Q. V . 2017. Searching\nfor activation functions. arXiv preprint arXiv:1710.05941 .\nRasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y . 2020.\nDeepSpeed: System Optimizations Enable Training Deep\nLearning Models with Over 100 Billion Parameters. In Pro-\nceedings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining , KDD ’20,\n3505–3506. New York, NY , USA: Association for Com-\nputing Machinery. ISBN 9781450379984. doi:10.1145/\n3394486.3406703. URL https://doi.org/10.1145/3394486.\n3406703.\nShahroudy, A.; Liu, J.; Ng, T.-T.; and Wang, G. 2016. Ntu\nrgb+ d: A large scale dataset for 3d human activity analysis.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 1010–1019.\nShen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2021. Ef-\nﬁcient Attention: Attention with Linear Complexities. In\nWACV. IEEE.\nShi, L.; Zhang, Y .; Cheng, J.; and Lu, H. 2019. Skeleton-\nBased Action Recognition With Directed Graph Neural Net-\nworks. In 2019 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR) , 7904–7913. doi:\n10.1109/CVPR.2019.00810.\nShi, L.; Zhang, Y .; Cheng, J.; and Lu, H. 2019a. Skeleton-\nbased action recognition with directed graph neural net-\nworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 7912–7921.\nShi, L.; Zhang, Y .; Cheng, J.; and Lu, H. 2019b. Two-Stream\nAdaptive Graph Convolutional Networks for Skeleton-\nBased Action Recognition. In CVPR.\nShiv, V .; and Quirk, C. 2019. Novel positional encod-\nings to enable tree-based transformers. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ´e-Buc, F.; Fox,\nE.; and Garnett, R., eds., Advances in Neural Informa-\ntion Processing Systems , volume 32. Curran Associates,\nInc. URL https://proceedings.neurips.cc/paper/2019/ﬁle/\n6e0917469214d8fbd8c517dcdc6b8dcf-Paper.pdf.\nSong, Y .-F.; Zhang, Z.; Shan, C.; and Wang, L. 2020.\nStronger, Faster and More Explainable: A Graph Convo-\nlutional Baseline for Skeleton-Based Action Recognition.\nIn Proceedings of the 28th ACM International Confer-\nence on Multimedia (ACMMM) , 1625–1633. New York,\nNY , USA: Association for Computing Machinery. ISBN\n9781450379885. doi:10.1145/3394171.3413802. URL\nhttps://doi.org/10.1145/3394171.3413802.\nTay, Y .; Dehghani, M.; Bahri, D.; and Metzler, D. 2020. Ef-\nﬁcient Transformers: A Survey.\nTsai, Y .-H. H.; Bai, S.; Yamada, M.; Morency, L.-P.; and\nSalakhutdinov, R. 2019. Transformer Dissection: An Uni-\nﬁed Understanding for Transformer’s Attention via the Lens\nof Kernel. In EMNLP. ACL.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, u.; and Polosukhin, I. 2017. At-\ntention is All You Need. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing Sys-\ntems, NIPS’17, 6000–6010. Red Hook, NY , USA: Curran\nAssociates Inc. ISBN 9781510860964.\nVemulapalli, R.; Arrate, F.; and Chellappa, R. 2014. Hu-\nman Action Recognition by Representing 3D Skeletons as\nPoints in a Lie Group. In 2014 IEEE Conference on\nComputer Vision and Pattern Recognition , 588–595. doi:\n10.1109/CVPR.2014.82.\nWang, J.; Liu, Z.; Wu, Y .; and Yuan, J. 2012. Mining\nactionlet ensemble for action recognition with depth cam-\neras. In 2012 IEEE Conference on Computer Vision and\nPattern Recognition, 1290–1297. doi:10.1109/CVPR.2012.\n6247813.\nXie, C.; Li, C.; Zhang, B.; Chen, C.; Han, J.; and Liu, J.\n2018. Memory Attention Networks for Skeleton-based Ac-\ntion Recognition. 1639–1645. doi:10.24963/ijcai.2018/227.\nYan, S.; Xiong, Y .; and Lin, D. 2018. Spatial temporal graph\nconvolutional networks for skeleton-based action recogni-\ntion. In Proceedings of the AAAI conference on artiﬁcial\nintelligence, volume 32.\nYong Du; Wang, W.; and Wang, L. 2015. Hierarchical recur-\nrent neural network for skeleton based action recognition.\nIn 2015 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 1110–1118. doi:10.1109/CVPR.2015.\n7298714.\nZhang, P.; Lan, C.; Xing, J.; Zeng, W.; Xue, J.; and Zheng,\nN. 2017. View adaptive recurrent neural networks for high\nperformance human action recognition from skeleton data.\nIn Proceedings of the IEEE International Conference on\nComputer Vision, 2117–2126.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6069703698158264
    },
    {
      "name": "Star (game theory)",
      "score": 0.5596078038215637
    },
    {
      "name": "Action (physics)",
      "score": 0.4600730836391449
    },
    {
      "name": "Computer science",
      "score": 0.41262954473495483
    },
    {
      "name": "Electrical engineering",
      "score": 0.22819817066192627
    },
    {
      "name": "Physics",
      "score": 0.21448823809623718
    },
    {
      "name": "Engineering",
      "score": 0.2001446783542633
    },
    {
      "name": "Astrophysics",
      "score": 0.11351880431175232
    },
    {
      "name": "Voltage",
      "score": 0.11209222674369812
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}