{
  "title": "Predicting Discourse Trees from Transformer-based Neural Summarizers",
  "url": "https://openalex.org/W3167799121",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5007294216",
      "name": "Wen Xiao",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5053635382",
      "name": "Patrick Huber",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5049259877",
      "name": "Giuseppe Carenini",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2158211888",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W3171719338",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W2069684104",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W2950415592",
    "https://openalex.org/W2045738181",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970507703",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2152992673",
    "https://openalex.org/W2138909885",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2877801623",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4300009529",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W182831726",
    "https://openalex.org/W2963899396",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3098123193",
    "https://openalex.org/W2251803607",
    "https://openalex.org/W3016557243",
    "https://openalex.org/W2944833000",
    "https://openalex.org/W2121103874",
    "https://openalex.org/W2951481240",
    "https://openalex.org/W2052449326",
    "https://openalex.org/W2970008578",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W1894075015",
    "https://openalex.org/W2970263339",
    "https://openalex.org/W3099609223"
  ],
  "abstract": "Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4139–4152\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4139\nPredicting Discourse Trees from Transformer-based Neural Summarizers\nWen Xiao, Patrick Huber, Giuseppe Carenini\nDepartment of Computer Science\nUniversity of British Columbia\nVancouver, BC, Canada, V6T 1Z4\n{xiaowen3, huberpat, carenini}@cs.ubc.ca\nAbstract\nPrevious work indicates that discourse infor-\nmation beneﬁts summarization. In this paper,\nwe explore whether this synergy between dis-\ncourse and summarization is bidirectional, by\ninferring document-level discourse trees from\npre-trained neural summarizers. In particu-\nlar, we generate unlabeled RST-style discourse\ntrees from the self-attention matrices of the\ntransformer model. Experiments across mod-\nels and datasets reveal that the summarizer\nlearns both, dependency- and constituency-\nstyle discourse information, which is typically\nencoded in a single head, covering long- and\nshort-distance discourse dependencies. Over-\nall, the experimental results suggest that the\nlearned discourse information is general and\ntransferable inter-domain1.\n1 Introduction\nExtractive summarization is a common and im-\nportant task within the area of Natural Language\nProcessing (NLP) , which can be useful in a multi-\ntude of diverse real-life scenarios. Current extrac-\ntive summarizers typically use exclusively neural\napproaches, in which the importance of extracted\nunits (i.e., sentences or clauses) and relationship be-\ntween them are learned by the model from a large\namount of data (e.g., Liu and Lapata (2019b)).\nInspired by previous work in pre-neural times,\nindicating that discourse information, especially\ndiscourse trees according to the Rhetorical Struc-\nture Theory (RST) (Mann and Thompson, 1988),\ncan beneﬁt the summarization task (Marcu, 1999),\nseveral very recent neural summarizers have tried\nto explicitly encode discourse information to sup-\nport summarization. Overall, it seems that adding\nthese encodings, consistent with pre-neural results,\nis beneﬁcial. In particular, injecting discourse\nhas been shown to either improve performance\n1The code can be found in https://github.com/\nWendy-Xiao/summ_guided_disco_parser\non the extractive summarization task itself (Xu\net al., 2020), or allow for a substantial reduction\nin the number of the summarizer’s parameters,\nwhile keeping competitive performance (Xiao et al.,\n2020).\nThe central hypothesis we are exploring in this\npaper is whether the synergy between discourse\nparsing and summarization is bidirectional. In\nother words, we examine if summarization is a\nuseful auxiliary task to infer discourse structures.\nLiu et al. (2019b) performed a preliminary inves-\ntigation of this conjecture, showing that structural\ninformation can be inferred from attention mech-\nanisms while training a neural model on auxiliary\ntasks. However, they did not perform any compari-\nson against ground-truth discourse trees. Further,\nrecent work showed that discourse trees implicitly\ninduced during training are oftentimes trivial and\nshallow, not representing valid discourse structures\n(Ferracane et al., 2019).\nIn this paper, we address these limitations by\nexplicitly exploring the relationship between sum-\nmarization and discourse parsing through the infer-\nence of document-level discourse trees from pre-\ntrained summarization models, comparing the re-\nsults against ground-truth RST discourse trees. Be-\nsides Liu et al. (2019b), our idea and approach are\ninspired by recent works on extracting syntactic\ntrees from pre-trained language models (Wu et al.,\n2020) or machine translation approaches (Raganato\nand Tiedemann, 2018), as well as previous work\non knowledge graph construction from pre-trained\nlanguage models (Wang et al., 2020). Speciﬁcally,\nwe generate full RST-style discourse trees from\nself-attention matrices of a pre-trained transformer-\nbased summarization model. We use three different\ntree-aggregation approaches (CKY (Jurafsky and\nMartin, 2014), Eisner (Eisner, 1996) and CLE (Chu\nand Liu, 1965; Edmonds, 1967)), generating a set\nof constituency and dependency trees representing\ndiverse discourse-related attributes.\n4140\nOur proposal is thereby addressing one of the\nkey limitations in discourse parsing, namely the\nlack of large training corpora. We aim to overcome\nthis limitation by generating a large number of rea-\nsonable quality discourse trees from a pre-trained\nsummarization model, similar in spirit to what Hu-\nber and Carenini (2020) did with sentiment. Admit-\ntedly, the discourse information captured with our\napproach is summarization task-speciﬁc, however,\nour generated discourse treebank can be combined\nwith further task-dependent treebanks (e.g. from\nsentiment) to train more powerful discourse parsers\nin a multitask framework.\nGenerally speaking, the ability to infer discourse\ntrees as a “by-product\" of the summarization task\ncan also be seen as a form of unsupervised dis-\ncourse parsing, where instead of leveraging pre-\ntrained language models like in Kobayashi et al.\n(2019), we exploit a pre-trained neural summarizer.\nWe empirically evaluate our method on three\ndatasets with human RST-style annotations, cover-\ning different text genres. Multiple experiments\nshow that the summarization model learns dis-\ncourse information implicitly, and that more de-\npendency information are captured, compared to\nstructural (i.e., constituency) signals. Interestingly,\nan additional exploration of the attention matrices\nof individual heads suggests that, for all models,\nmost of the discourse information is concentrated\nin a single head, and the best performing head is\nconsistent across all datasets. We further ﬁnd that\nthe dependency information learned in the atten-\ntion matrix covers long distance discourse depen-\ndencies. Overall, the results are consistent across\ndatasets and models, indicating that the discourse\ninformation learned by the summarizer is general\nand transferable inter-domain.\n2 Related Work\nRhetorical Structure Theory (RST)(Mann and\nThompson, 1988) is one of the most popular the-\nories of discourse, postulating that a document\ncan be represented as a constituency tree, where\nleaves are clause-like Elementary Discourse Units\n(EDUs), and internal nodes combine their respec-\ntive children by aggregating them into a single,\njoint constituent. Each internal node also has a\nnuclearity attribute 2, representing the local im-\nportance of their direct child-nodes in the par-\nent context from the set of {Nucleus-Nucleus,\n2In this paper we do not consider rhetorical relations.\nNucleus-Satellite, Satellite-Nucleus}. “Nucleus\"\nchild-nodes thereby generally play a more impor-\ntant role when compared to a “Satellite\" child-node.\nAlthough standard RST discourse trees are encoded\nas constituency trees, they can be converted into\ndependency trees with near isomorphic transforma-\ntions. In this work, we infer both, constituency and\ndependency trees.\nOver the past decades, RST discourse parsing\nhas been mainly focusing on supervised models,\ntypically trained and tested within the same domain\nusing human annotated discourse treebanks, such\nas RST-DT (Carlson et al., 2002), Instruction-DT\n(Subba and Di Eugenio, 2009) or GUM (Zeldes,\n2017). The intra-domain performance of these su-\npervised models has consistently improved, with\na mix of traditional models by Joty et al. (2015)\nand Wang et al. (2017), and neural models (Yu\net al., 2018) reaching state-of-the-art (SOTA) re-\nsults. Yet, these approaches do not generalize well\ninter-domain (Huber and Carenini, 2020), likely\ndue to the limited amount of available training data.\nHuber and Carenini (2019) recently tackled this\ndata-sparsity issue through automatically generated\ndiscourse structures from distant supervision, show-\ning that sentiment information can be used to infer\ndiscourse trees. Improving on their initial results,\nHuber and Carenini (2020) published a large-scale,\ndistantly supervised discourse corpus (MEGA-DT),\nshowing that a parser trained on such treebank de-\nlivers SOTA performance on the more general inter-\ndomain discourse parsing task. In this paper, we\nalso tackle the data sparsity problem in discourse\nparsing, however, using a signiﬁcantly different\napproach. First, instead of relying on sentiment,\nwe leverage the task of extractive summarization.\nSecond, instead of a method for distant supervision,\nwe propose an unsupervised approach.\nThe area of unsupervised RST-style discourse\nparsing has been mostly underlooked in the past,\nwith recent neural approaches either taking advan-\ntage of pre-trained language models to predict dis-\ncourse (Kobayashi et al., 2019) or using pre-trained\nsyntactic parsers and linguistic knowledge (Nishida\nand Nakayama, 2020) to infer discourse trees in an\nunsupervsied manner. Similarly. our proposal only\nrelies on a pre-trained neural summarization model\nto generate discourse trees.\nRecent neural summarization modelsare typi-\ncally based on transformers (Liu and Lapata, 2019a;\nZhang et al., 2019). One advantage of these mod-\n4141\nels is that they learn the relationship between input\nunits explicitly using the dot-product self-attention,\nwhich allows for some degree of exploration of\nthe inner working of these complex and distributed\nmodels. Here, we investigate if the attention matri-\nces of a transformer-based summarizer effectively\ncapture discourse information (i.e., how strongly\nEDUs are related) and therefore can be used to\nderive discourse trees for arbitrary documents.\nMarcu (1999) pioneered the idea to directly ap-\nply RST-style discourse parsing to extractive\nsummarization, and empirically showed that RST\ndiscourse information can beneﬁt the summariza-\ntion task, by simply extracting EDUs along the\nnucleus path. This initial success was followed\nby further work on leveraging discourse parsing in\nsummarization, including McDonald (2007), Hirao\net al. (2013), and Kikuchi et al. (2014). More re-\ncently, the beneﬁts of discourse for summarization\nhave also been conﬁrmed for neural summarizers,\ne.g. in Xiao and Carenini (2019) and Cohan et al.\n(2018), using the structure of scientiﬁc papers (i.e.\nsections), and in Xu et al. (2020), successfully in-\ncorporating RST-style discourse and co-reference\ninformation in the BERTSUM summarizer (Liu\nand Lapata, 2019b).\nIn contrast to previous approaches demonstrating\nhow discourse can enhance summarization perfor-\nmance, we have recently shown that discourse en-\nables the speciﬁcation of simpler neural summariz-\ners, without affecting their performance (Xiao et al.,\n2020). In particular, by using a ﬁxed discourse-\nbased attention they achieve competitive results\ncompared to learnable dot-product self-attention\nmechanisms, as used in the original transformer\nmodel. Inspired by these ﬁndings, suggesting that\ntransformer-based summarization models learn ef-\nfective discourse representations, we explore if use-\nful discourse structures can be inferred from learnt\ntransformer self-attention weights.\nAdmittedly, Liu and Lapata (2018) and Liu et al.\n(2019b) presented preliminary work on inferring\ndiscourse structures from attention mechanisms,\nwhile training a neural model on auxiliary tasks,\nlike text classiﬁcation and summarization. How-\never, they did not perform any comparison against\nground-truth discourse trees as we do here. More\nimportantly, we employ a more explicit approach\nto infer discourse structures, not as part of the learn-\ning process, but extracting the structures after the\nsummarization model is completely trained and\napplied to new documents.\nWhile our focus is on discourse, extracting syn-\ntactic constituency and dependency treesfrom\ntransformer-based models has been recently at-\ntempted in both, machine translation and language\nmodelling. In machine translation, Mare ˇcek and\nRosa (2019) and Raganato and Tiedemann (2018)\nshow that trained translation models can capture\nsyntactic information within their attention heads,\nusing the CKY and CLE algorithms, respectively.\nIn pre-trained language models, Wu et al. (2020)\npropose a parameter-free probing method to con-\nstruct syntactic dependency trees based on a pre-\ntrained BERT model, only brieﬂy elaborating on\npossible applications to discourse. In contrast to\nour work, they do not directly use attention heads,\nbut instead build an impact matrix based on the dis-\ntance between token representations. Furthermore,\nwhile their BERT-based model cannot deal with\nlong sequences, our two-level encoder can effec-\ntively deal with sequences of any length, which is\ncritical in discourse.\n3 Our Model\n3.1 Framework Overview\nOur main goal is to show the ability of a previously\ntrained summarization model to be directly applied\nto the task of RST-style discourse parsing. Along\nthis line, we explore the relationship between infor-\nmation learned by the transformer-based sumarizer\nand the task of discourse parsing. We leverage\nthe synergies between units learned in the trans-\nformer model by following Xiao et al. (2020), previ-\nously proposing the use of a transformer document-\nencoder on top of a pretrained BERT EDU encoder.\nThis standard summarization model is presented\nin Figure 1 (left). In the transformer-based docu-\nment encoder, each head internally contains a self-\nattention matrix, learned during the training of the\nsummarization model, representing the relationship\nbetween EDUs (Figure 1 (center)). In this paper,\nwe analyze these learned self-attention matrices,\nnot only to conﬁrm our intuition that they contain\nrelevant discourse information, but also to compu-\ntationally exploit such information for discourse\nparsing. We therefore generate a set of different\n(constituency/dependency) discourse trees from the\nself-attention matrices, focusing on different at-\ntributes of discourse, as shown in Figure 1 (right).\nOur generated constituency trees only reveal the\ndiscourse tree structure without additional nucle-\n4142\nFigure 1: The pipeline of our whole method.\narity and relation attributes. More interestingly,\nwe complement the constituency interpretation of\nthe self-attention by additionally inferring a de-\npendency tree, also partially guided by discourse\nstructures, but mostly driven by the RST nuclearity\nattribute, which is shown to be more related to the\nsummarization task where the importance of the\ndifferent text spans is critical (Hirao et al., 2013).\nWe present and discuss the different parsing algo-\nrithms to extract discourse information from the\nself-attention matrix next.\n3.2 Parsing Algorithms\nFormally, for an input document D = {u1, .., un}\nwith n EDUs, each attention head returns an at-\ntention matrix A ∈Rn×n where entry Aij con-\ntains a score measuring how much the i-th EDU\nrelies on the j-th EDU. Given those bidirectional\nscores deﬁning the relationship between every two\nEDUs in a document, we build a tree such that\nEDU pairs with higher reciprocal attention scores\nare more closely associated in the resulting tree. In\nthe constituency case, this means that EDUs with\nhigher mutual attention should belong to sub-trees\non lower levels of the tree, while in the dependency\ncase this implies that the path between such EDUs\nshould contain less intermediate nodes. In essence,\nthese requirements can be formalized as searching\nfor the tree within the set of possible trees, which\nmaximizes a combined score.\n3.2.1 Constituency Tree (C-Tree) Parsing\nTo generate a constituency tree from the attention\nmatrix, we follow a large body of previous work in\ndiscourse parsing (e.g., Joty et al. (2015)), where\nconstituency discourse trees are generated using\nthe CKY algorithm (Jurafsky and Martin, 2014).\nSpeciﬁcally, we ﬁll a n ×n matrix P ∈Rn×n gen-\nerating the optimal tree in bottom-up fashion using\nthe dynamic programming approach according to:\nPij =\n\n\n\n0, i > j∑n\nk=1(Aki), i = j\nmaxj−1\nk=i(Pik + P(k+1)j\n+ avg(Ai:k,(k+1):j)\n+ avg(A(k+1):j,i:k))/2, i < j\nwhere Pij with i = j contains the overall impor-\ntance of EDU i, computed as the attention paid\nby others to unit i. Pij with i < j represents\nthe score of the optimal sub-tree spanning from\nEDU i to EDU j. We select the best combination\nof sub-trees k, such that the sum of the left sub-\ntree spanning [i : k] and the right one spanning\n[(k + 1) :j], along with the average score of con-\nnections between the two sub-trees is maximized.\nFigure 2: Example of CKY constituency parsing.\nFor example, to pick the structure of the sub-tree\nspanning EDUs [3 : 5](see Fig. 2), we need to\ndecide between the potential sub-tree aggregation\nof ((34)5) and (3(45)). The respective scores are\ncomputed based on the scores in green and blue\nblocks in both the CKY and the Attention Matrices.\nFollowing this algorithm, two sub-trees with a high\nattention score between them tend to be combined\non lower levels of the tree, indicating they are more\nrelated in the discourse tree.\nBesides the standard CKY algorithm described\n4143\nFigure 3: Chu-Liu-Edmonds Algorithm with sentence constraints\nabove, we also explore a hierarchical CKY ap-\nproach with sentence and paragraph constraints.\nSpeciﬁcally, we do not aggregate Pij if the span\n[i : j] crosses a sentences boundary where either\nsentence is incomplete. In the previous example, if\nEDU3 and EDU4 were in the same sentence, even\nif the score of the blue aggregation candidate was\nhigher, we would choose the green sub-tree aggre-\ngation. Plausibly, this hierarchical approach will\nperform better, since the ground-truth treebanks\nmostly contain sentences and paragraphs that are\ncovered by a complete discourse sub-trees.\n3.2.2 Dependency Tree (D-Tree) Parsing\nFor the dependency tree generation, we use the\nEisner (Eisner, 1996) and Chu-Liu-Edmonds al-\ngorithm (Chu and Liu, 1965; Edmonds, 1967) to\ngenerate projective and non-projective dependency\ntrees, respectively3. First, we convert the attention\nmatrix A into a fully connected graph G = (N, E),\nwhere N contains all the EDUs, and eij, indicating\nhow much the i-th EDU inﬂuences the j-th EDU,\ncorresponds to Aji, which is the attention that thej-\nth EDU pays to the i-th EDU. Based on this graph,\nwe apply the following algorithms:\nEisner Algorithm: We apply this dynamic pro-\ngramming algorithm to generate projective depen-\ndency trees. Thereby, we build a matrix P ∈\nRn×n×2×2, in which the ﬁrst and second dimen-\n3“Mixed\" approaches, dealing with mildly non-projective\ntrees (Kuhlmann and Nivre, 2006), are left for future work.\nsions contain the start and end indexes of sub-trees,\nsimilar to the CKY algorithm; while the third and\nfourth dimensions indicate whether the head is the\nstart or the end unit, and whether the sub-tree is\ncompleted. As done for constituency parsing, we\nalso use a hierarchical version of Eisner’s algo-\nrithm, in which we restrict inter-sentence connec-\ntions for incomplete sentence trees. Since the Eis-\nner algorithm can only generate projective depen-\ndency trees, it will be inaccurate for documents\nwith a non-projective discourse structure.\nChu-Liu-Edmonds (CLE) Algorithm: Origi-\nnally proposed as a recursive approach to ﬁnd\nthe maximum spanning tree of a graph given its\nroot, CLE can generate non-projective trees. In\nthe unconstrained case, we simply follow the stan-\ndard CLE algorithm, selecting the EDU with the\nhighest importance score, computed similar to Sec.\n3.2.1, i.e. root = argmaxi\n∑n\nk=1(Aki), as the\nroot. From there, the algorithm selects the “optimal\nedges\", i.e. the maximum in-edges for each node\nexcept the root, breaking the cycles recursively.\nAgain, as we did for CKY and Eisner, we also\napply the additional sentence constraint. Unlike for\nthe dynamic programming approaches, which build\nthe trees in a bottom-up fashion and can directly be\nconstrained to avoid cross-sentence aggregations\nof incomplete sentences, we need to substantially\nmodify CLE to allow for sentence constraints.\nIn particular, we ﬁrst build a sentence graph\nGs = {Ns, Es}from the EDU graph (Figure 3\n4144\n(b)), in which es\nSD = avgs∈S,d∈D esd, and record\nthe maximum edge corresponding to the edge be-\ntween sentences, i.e. argmaxs∈S,d∈D esd. After\nthat, we use the CLE algorithm within the sentence\ncontaining the root EDU as the root sentence to\nﬁnd the maximum spanning tree in Gs (Figure 3\n(c)). We then add the corresponding EDU edges\nto the ﬁnal tree (Figure 3 (d)). For example, the\nedge (s0, s1) in Gs corresponds to the EDU edge\n(e0, e2) in G. Next, we treat nodes with incoming\nedges from other sentences as the root of the sen-\ntence itself and run the CLE algorithm within each\nsentence (Figure 3 (e)). The ﬁnal tree (Figure 3 (f))\nis eventually formed as the combination of inter-\nsentence edges derived in sentence graph Gs and\nintra-sentence edges found within each sentence.\n4 Experiments and Analysis\n4.1 The Summarization Task\nIn order to show the generality of the discourse\nstructures learned in the summarization model, we\ntrain our summarizer across a variety of datasets\nand hyper-parameter settings. More speciﬁcally,\nwe train on two separate, widely-used news cor-\npora – CNN Daily Mail (CNNDM) (Nallapati et al.,\n2016) and NYT (Sandhaus, 2008) –, as well as un-\nder three hyper-parameter settings with different\nnumbers of layers and attention heads: (a) A sim-\nple model with 2 layers and a single head. (b) 6\nlayers with 8 heads each, proposed in the origi-\nnal transformer model(Vaswani et al., 2017). (c)\n2 layers with 8 heads each, constituting a middle\nground between the previous two settings. By con-\nsidering two corpora (CNNDM and NYT) and the\nthree settings, we train six models, which we call:\nCNNDM-2-1, CNNDM-6-8, CNNDM-2-8, NYT-\n2-1, NYT-6-8, NYT-2-84.\n4.2 Discourse Datasets\nThe quality of the attention-generated trees is as-\nsessed on three discourse datasets (see Table 1).\nRST-DT is the largest and most frequently used\nRST-style discourse treebank (Carlson et al., 2002),\ncontaining news articles from the Wall Street Jour-\nnal. Since this is the genre of both our summariza-\ntion training corpora, the experiments testing on\nthis dataset are intra-domain.\nInstruction-DT contains documents in the\nhome-repair instructions domain (Subba and Di Eu-\ngenio, 2009). We categorize the experiments on\n4Complete evaluation results for all six models are pre-\nsented in Appendix A.\nthis dataset as cross-domain.\nGUM contains documents from eight domains\nincluding news, interviews, academic papers and\nmore (Zeldes, 2017). Since the GUM corpus is\nmulti-domain, the performance on this dataset will\nreveal the generalizability of generated trees in a\nbroader sense.\nDataset # Docs #EDU/doc#Sent/doc#words/doc\nRST-DT 385 56.6 22.5 549\nInstruction 176 32.7 19.5 318\nGUM 127 107 45 874\nTable 1: Key RST-style discourse dataset dimensions.\nAll three discourse datasets contain ground-truth\nRST-style consituency trees. While all corpora con-\ntain potential non-binary sub-trees, Instruction-DT\nalso includes multi-root documents. To account\nfor these cases, we apply the right-branching bi-\nnarization following Huber and Carenini (2019).\nFurthermore, we convert constituency trees with\nnuclearity into ground truth dependency trees using\nthe algorithm proposed in Li et al. (2014) .\n4.3 Evaluation Metric\nTo evaluate how well the generated trees align with\nground-truth trees, we use RST Parseval Scores\nfor constituency trees and Unlabeled Attachment\nScore for dependency trees, measuring the ratio of\nmatched spans and the ratio of matched dependency\nrelations, respectively.\n4.4 Overall Results\nFor each model conﬁguration, we run a set of exper-\niments using the average attention matrix across all\nheads in a layer, i.e. Aavg = ∑\nh Ah/H, with H as\nthe number of heads. This initial setup is intended\nto provide insights into the discourse information\nlearned in each layer.\nThe results of the three tree-generation algo-\nrithms are shown in Table 2, 3 and 4 along with the\nperformance of a random baseline obtained by run-\nning the algorithms on 10 random matrices. Here,\nwe present the results of three selected models, lim-\nited to the performance of the ﬁrst two layers for\nthe 6-layer models, to allow for a direct compari-\nson to the 2-layer models5. Across evaluations, the\nlayer-wise performance within the same models are\nrather distinct, indicating that different properties\nare learned in the layers. This ﬁnding is in line\nwith previous work (Liu et al., 2019a), especially\n5Results for all six models can be found in Appendix B.\n4145\nModel No Cons. Sent Cons.\nAttn#0 Attn#1 Attn#0 Attn#1\nRST-DT\nCNNDM-2-1 61.2 59.7 76.2 74.6\nCNNDM-6-8 60.3 60.8 75.4 75.0\nNYT-6-8 62.4 62.2 76.7 75.6\nRandom 58.6±0.1 74.1±0.1\nInstruction\nCNNDM-2-1 61.1 59.8 71.4 ↓70.3\nCNNDM-6-8 60.3 61.2 71.2 70.9\nNYT-6-8 61.3 61.3 71.3 ↓70.0\nRandom 59.5±0.3 70.5±0.1\nGUM\nCNNDM-2-1 58.7 57.7 72.7 71.9\nCNNDM-6-8 58.9 59.3 72.4 72.7\nNYT-6-8 59.6 59.3 72.2 71.6\nRandom 57.5±0.1 71.5±0.2\nTable 2: RST Parseval Scores of generated con-\nstituency trees on the three datasets, expressed as ’Avg.\n±Std’. Green means the result is better than Random,\nand Red along with ↓means worse. Results for Ran-\ndom are obtained by applying the parser to random ma-\ntrices for 10 times. Attn#0/1 are the ﬁrst two layers.\ngiven that the performance of each layer is consis-\ntent across constituency and dependency parsing\noutputs for all datasets. Furthermore, the more lay-\ners the summarization model contains, the smaller\nthe performance gap between layers becomes. We\nbelieve that this could be caused by the discourse\ninformation being further spread across different\nlayers. Generally, we observe that models trained\non the CNNDM dataset perform better than mod-\nels trained on the NYT corpus, despite the larger\nsize of the NYT dataset. Plausibly, the superior\nperformance of our models trained on CNNDM\npotentially reﬂects a higher diversity within docu-\nments in the CNNDM dataset.\nComparing the constituency tree performance\nin Table 2 against the dependency tree results in\nTables 3 and 4, we can clearly see that the improve-\nment of the constituency parsing approach over the\nrandom baseline is much smaller than the improve-\nments for the generated dependency parse-trees.\nPresumably, this larger improvement for the de-\npendency trees is due to the fact that dependency\nrelationships (strongly encoding the nuclearity at-\ntribute) are more directly related to the summa-\nrization task than the plain structure information.\nThis is in line with previous work on applying de-\npendency trees to the summarization task (Hirao\net al., 2013; Xu et al., 2020) and indicates that the\nlearned attention matrices contain valid discourse\ninformation.\nModel No Cons. Sent Cons.\nAttn#0 Attn#1 Attn#0 Attn#1\nRST-DT\nCNNDM-2-1 23.7 ↓4.8 28.2 ↓18.2\nCNNDM-6-8 ↓7.9 20.5 ↓13.8 27.8\nNYT-6-8 15.7 12.5 24.3 ↓18.9\nRandom 11.2±0.2 20.3±0.2\nInstruction\nCNNDM-2-1 31.1 ↓4.4 29.3 ↓13.5\nCNNDM-6-8 ↓8.5 19.5 ↓9.9 22.0\nNYT-6-8 16.2 ↓12.1 22.8 ↓16.4\nRandom 13.1±0.3 19.3±0.4\nGUM\nCNNDM-2-1 21.3 ↓2.24 27.3 ↓16.1\nCNNDM-6-8 ↓4.7 15.8 ↓11.5 24.80\nNYT-6-8 12.6 ↓9.6 23.4 ↓17.1\nRandom 10.4±0.2 19.2±0.3\nTable 3: Unlabeled Attachment Scores of dependency\ntrees generated by the Eisner algorithm.\nModel No Cons. Sent Cons.\nAttn#0 Attn#1 Attn#0 Attn#1\nRST-DT\nCNNDM-2-1 21.6 ↓1.5 29.3 19.6\nCNNDM-6-8 7.3 17.3 ↓16.1 28.5\nNYT-6-8 13.7 10.6 25.0 21.1\nRandom 1.7±0.1 18.7±0.1\nInstruction\nCNNDM-2-1 28.1 ↓2.1 37.4 18.1\nCNNDM-6-8 6.9 15.9 ↓14.9 25.8\nNYT-6-8 14.8 9.8 25.4 21.1\nRandom 2.9±0.2 17.9±0.4\nGUM\nCNNDM-2-1 19.5 ↓0.7 28.8 17.9\nCNNDM-6-8 4.0 13.1 ↓14.9 25.4\nNYT-6-8 10.7 8.2 23.0 19.5\nRandom 0.9±0.05 17.0±0.2\nTable 4: Unlabeled Attachment Scores of dependency\ntrees generated by the CLE algorithm\nAs for the two approaches to dependency pars-\ning, although Eisner generally outperforms CLE,\nthe improvement over random trees is larger for\nCLE. We believe that this effect is due to the re-\nduced constraints imposed on the CLE algorithm,\nwhich is not limited to generate projective trees.\nConsidering all three methods, the results of the\nCLE-generated dependency tree seem most promis-\ning. A possible explanation is that both CKY and\nEisner build the discourse tree in a bottom-up fash-\nion with dynamic programming. This way, only\nlocal information is used on lower levels of the tree.\nOn the other hand, the CLE algorithm uses global\ninformation, potentially more aligned with the sum-\nmarization task, where all EDUs are considered to\npredict importance scores.\n4146\nFigure 4: The Unlabeled Attachment Score of trees generated by the attention matrix per head on three datasets\nunder two conditions with the CNNDM-6-8 model.\n4.5 Performance of Heads\nWhile all previous results rely on the average atten-\ntion matrices, we now analyze whether discourse\ninformation is evenly distributed across attention\nheads, or if a subset of the heads contains the ma-\njority of discourse related information.\nWe describe this analysis only for CLE for two\nreasons: (a) the summarization model seemingly\ncaptures more dependency-related discourse infor-\nmation than structure information; (b) compared\nwith Eisner, the CLE approach is more ﬂexible, by\nalso covering non-projective dependency trees.\nSince the results across all summarization mod-\nels are consistent, we only show the accuracy\nheatmap for the CNNDM-6-8 model on the three\nRST-style discourse datasets in Figure 4. Remark-\nably, for all three datasets, there is one head in\nthe model capturing the vast majority of discourse\ninformation, especially in the unconstrained case.\nFurthermore, the performance of the best single\nattention head is much better than the one of the\naverage attention matrix shown in section 4.4 (e.g.\n34.53 compared to 19.51 on the GUM dataset with-\nout sentence constraints). These intriguing ﬁndings\nwill be further explored in future work.\n4.6 Analysis of Generated Trees\nLocalness of Trees: To further verify that the\ngenerated trees are non-trivial, for instance simply\nconnecting adjacent EDUs, we analyze the qual-\nity of the trees produced with the second attention\nhead on the second layer, which is the top per-\nformer among all the heads shown in Figure 4.\nFirst, we separate all dependency relationships into\ntwo classes: local, holding between two adjacent\nEDUs, and distant, including all other relations\nbetween non-adjacent EDUs. Then we compute\nthe ratio of the correctly predicted dependencies\nwhich are local (Local Ratio Corr.), as well as the\nMeasurement(%)No Cons. Sent Cons.\nRST-DT\nLocal Ratio Corr.77.78 79.17\nLocal Ratio GT 53.22\nLocal Ratio Ours 46.52 58.35\nInstruction\nLocal Ratio Corr.81.15 84.90\nLocal Ratio GT 59.82\nLocal Ratio Ours 47.90 60.54\nGUM\nLocal Ratio Corr.77.99 80.20\nLocal Ratio GT 53.28\nLocal Ratio Ours 39.97 53.76\nTable 5: Measurements on the locality of the generated\ndependency trees, all numbers are in %. Corr. repre-\nsents all the correct predictions, GT the ground-truth\ntrees, and Ours the generated tree respectively.\nratio of local dependencies in the generated trees\n(Local Ratio Ours), and in the ground-truth trees\n(Local Ratio GT). The results of this analysis are\nshown in Table 5. For all datasets, the ratio of\ncorrectly predicted local dependencies (Local Ra-\ntio Corr.) (being > 50) is larger than the ratio for\ndistant relations, which appears reasonable, since\nlocal dependency predictions are easier to predict\nthan distant ones. Further, comparing (Local Ratio\nGT) and (Local Ratio Ours) without the sentence\nconstraint (ﬁrst column) shows that the number\nof local dependency relations in the ground-truth\ndiscourse trees is consistently larger than the pre-\ndicted number. This indicates that the discourse\ninformation learned in the attention matrices goes\nbeyond the oftentimes predominant local positional\ninformation. However, even without the sentence\nconstraint (ﬁrst column), when the CLE algorithm\ncan predict trees of any form, more than40% of the\nrelations are predicted as local, suggesting that the\nstandard CLE approach can already capture local\ninformation well.\nAdding the sentence constraint (second column),\n4147\nBranch Height Leaf Arc vac. (%)\nRST-DT\nOurs(Sent Cons)1.50 27.06 0.37 0.10 3%\nOurs(No Cons)1.74 25.76 0.49 0.12 3%\nGT Tree 2.10 8.19 0.51 0.13 2%\nInstruction\nOurs(Sent Cons)1.56 15.74 0.39 0.13 3%\nOurs(No Cons)1.80 14.35 0.50 0.14 3%\nGT Tree 1.59 8.49 0.41 0.15 1%\nGUM\nOurs(Sent Cons)1.61 44.94 0.40 0.05 0%\nOurs(No Cons)2.14 43.08 0.54 0.08 0%\nGT Tree 2.02 12.17 0.51 0.04 0%\nTable 6: Statistics of our generated trees and the gold\nstandard trees in terms of theaverage branch width, av-\nerage height, average leaf ratio (micro), average nor-\nmalized arc lengthof the trees and percentage of the\nVacuous trees.\nwe ﬁnd that the local dependency ratio of the gen-\nerated trees (Local Ratio Ours) further increases\nby more than 10% across all three datasets. This\nmakes intuitive sense, since the sentence constraint\nforces the generated trees to purely focus on local\naspects within each sentence. To sum up, we ﬁnd\nthat the learned attention matrices contains both\nlocal and distant dependency information, although\nlocal dependency predictions perform better.\nProperties of Trees: Following Ferracane et al.\n(2019), we structurally inspect the generated depen-\ndency trees, and compare them with the gold trees\non all three datasets. This comparison is presented\nin Table 6, showing theaverage branch width, aver-\nage height, average leaf ratio (micro)and average\nnormalized arc lengthof the trees as well as the\npercentage of vacuous treesin each dataset6.\nLooking at Table 6, it appears that our tree struc-\nture properties are similar to the ground-truth prop-\nerties in regards to all measures except the height\nof the tree, which indicates that our trees tend to be\ngenerally deeper than gold standard trees, despite\nhaving a similar branch width and leaf ratio. Fur-\nthermore, our trees are even deeper when using the\nsentence constraint. Plausibly, by forcing each sen-\ntence to have its own sub-tree can make shallower\ninter-sentential structures less likely. Exploring\npotential causes for the difference in tree-height,\npossibly due to the summarization task itself, are\nleft as future work.\n6A vacuous tree is a special tree in which the root is one\nof the ﬁrst two EDUs, with all nodes are children of the root.\n4.7 Additional Results on Model Sensitivity\nto Initialization and Summarizer Quality\nTo investigate whether the performance is consis-\ntent cross different random initializations, and to\nexplore the inﬂuence of the results with respect to\nthe quality of the summarizer, we perform addi-\ntional experiments with the ’CNNDM-6-8’ model7.\nOverall, we ﬁnd that the performance is rather sim-\nilar across random initializations. Interestingly, a\nsingle head consistently shows better performance\nthan all other heads across different initialization as\nwell as datasets; however, while the position of the\ntop-performing head is not always the same, it is\noften located in the second layer of the model. Re-\ngarding the second experiment exploring sensitivity\nto the summarizer quality, we create summarizers\nof increasing quality by providing more and more\ntraining. As expected, we ﬁnd that as the summa-\nrization model is trained for additional steps, more\naccurate discourse information is learnt, concen-\ntrated in a single head.\n5 Conclusions and Future Work\nWe present a novel framework to infer discourse\ntrees from the attention matrices learned in a\ntransformer-based summarization model. Experi-\nment across models and datsets indicates that both\ndependency and structural discourse information\nare learned, that such information is typically con-\ncentrated in a single head, and that the attention\nmatrix also covers long distance discourse depen-\ndencies. Overall, consistent results across datasets\nand models suggest that the learned discourse in-\nformation is general and transferable inter-domain.\nIn the future, we want to explore if simpler sum-\nmarizers like BERTSUM (Liu and Lapata, 2019b)\ncan also capture discourse info; speciﬁcally study-\ning if the importance of the heads corresponds\nto the captured discourse info, which may help\npruning summarization model by incorporating dis-\ncourse info, in spirit of Xiao et al. (2020).\nWith respect to dependency tree generation pos-\nsible improvements could come by looking for ad-\nditional strategies balancing between guidance and\nﬂexibility, as Kuhlmann and Nivre (2006) explore\nfor syntactic dependency parsing.\nTo address the problem of data sparsity in dis-\ncourse parsing, we want to synergistically leverage\nother discourse-related tasks, in addition to senti-\nment and summarization, like topic modeling.\n7More details can be found in Appendix C.\n4148\nAcknowledgments\nWe thank the anonymous reviewers and the UBC-\nNLP group for their insightful comments. This\nresearch was supported by the Language & Speech\nInnovation Lab of Cloud BU, Huawei Technolo-\ngies Co., Ltd.\nWe further acknowledge the support of the Natu-\nral Sciences and Engineering Research Council of\nCanada (NSERC).\nNous remercions le Conseil de recherches en sci-\nences naturelles et en génie du Canada (CRSNG)\nde son soutien.\nReferences\nLynn Carlson, Mary Ellen Okurowski, and Daniel\nMarcu. 2002. RST discourse treebank. Linguistic\nData Consortium, University of Pennsylvania.\nY . Chu and T. Liu. 1965. On the shortest arborescence\nof a directed graph.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. NAACL HLT 2018 - 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies - Proceedings of the Conference, 2:615–621.\nJack Edmonds. 1967. Optimum Branchings.\nJason M. Eisner. 1996. Three new probabilistic models\nfor dependency parsing: An exploration. In COL-\nING 1996 Volume 1: The 16th International Confer-\nence on Computational Linguistics.\nElisa Ferracane, Greg Durrett, Junyi Jessy Li, and\nKatrin Erk. 2019. Evaluating discourse in\nstructured text representations. arXiv preprint\narXiv:1906.01472.\nTsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,\nNorihito Yasuda, and Masaaki Nagata. 2013. Single-\nDocument Summarization as a Tree Knapsack Prob-\nlem. Technical report.\nPatrick Huber and Giuseppe Carenini. 2019. Pre-\ndicting discourse structure using distant supervision\nfrom sentiment. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2306–2316.\nPatrick Huber and Giuseppe Carenini. 2020. MEGA\nRST discourse treebanks with structure and nuclear-\nity from scalable distant sentiment supervision. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7442–7457, Online. Association for Computa-\ntional Linguistics.\nShaﬁq Joty, Giuseppe Carenini, and Raymond T Ng.\n2015. Codra: A novel discriminative framework\nfor rhetorical analysis. Computational Linguistics,\n41(3):385–435.\nDan Jurafsky and James H Martin. 2014. Speech and\nlanguage processing, volume 3. Pearson London.\nYuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-\nabu Okumura, and Masaaki Nagata. 2014. Single\ndocument summarization based on nested tree struc-\nture. 52nd Annual Meeting of the Association for\nComputational Linguistics, ACL 2014 - Proceedings\nof the Conference, 2:315–320.\nNaoki Kobayashi, Tsutomu Hirao, Kengo Naka-\nmura, Hidetaka Kamigaito, Manabu Okumura, and\nMasaaki Nagata. 2019. Split or merge: Which is bet-\nter for unsupervised rst parsing? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5801–5806.\nMarco Kuhlmann and Joakim Nivre. 2006. Mildly non-\nprojective dependency structures. In Proceedings\nof the COLING/ACL 2006 Main Conference Poster\nSessions, pages 507–514, Sydney, Australia. Associ-\nation for Computational Linguistics.\nSujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.\n2014. Text-level discourse dependency parsing. In\nProceedings of the 52nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 25–35, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2018. Learning struc-\ntured text representations. Transactions of the Asso-\nciation for Computational Linguistics, 6:63–75.\nYang Liu and Mirella Lapata. 2019a. Hierarchical\ntransformers for multi-document summarization. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n5070–5081, Florence, Italy. Association for Compu-\ntational Linguistics.\nYang Liu and Mirella Lapata. 2019b. Text Summa-\nrization with Pretrained Encoders. EMNLP-IJCNLP\n2019 - 2019 Conference on Empirical Methods in\nNatural Language Processing and 9th International\nJoint Conference on Natural Language Processing,\nProceedings of the Conference, pages 3730–3740.\n4149\nYang Liu, Ivan Titov, and Mirella Lapata. 2019b. Sin-\ngle document summarization as tree induction. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n1745–1755, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nWilliam C. Mann and Sandra A. Thompson. 1988.\nRhetorical Structure Theory: Toward a functional\ntheory of text organization.\nDaniel Marcu. 1999. Discourse Trees are Good Indica-\ntors of Importance in Text. Advances in Automatic\nText Summarization, pages 123–136.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nbalustrades to pierre vinken: Looking for syntax in\ntransformer self-attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 263–\n275, Florence, Italy. Association for Computational\nLinguistics.\nRyan McDonald. 2007. A study of global inference al-\ngorithms in multi-document summarization. In Pro-\nceedings of the 29th European Conference on IR Re-\nsearch, ECIR’07, page 557–564, Berlin, Heidelberg.\nSpringer-Verlag.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar GuÌ‡lçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nNoriki Nishida and Hideki Nakayama. 2020. Unsuper-\nvised discourse constituency parsing using viterbi\nem. Transactions of the Association for Computa-\ntional Linguistics, 8:215–230.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus.\nRajen Subba and Barbara Di Eugenio. 2009. An effec-\ntive discourse parser that uses rich linguistic infor-\nmation. In Proceedings of Human Language Tech-\nnologies: The 2009 Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics, pages 566–574. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 2017-Decem(Nips):5999–6009.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage models are open knowledge graphs.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA two-stage parsing method for text-level discourse\nanalysis. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 184–188.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. As-\nsociation for Computational Linguistics.\nWen Xiao and Giuseppe Carenini. 2019. Extractive\nsummarization of long documents by combining\nglobal and local context. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3011–3021, Hong Kong,\nChina. Association for Computational Linguistics.\nWen Xiao, Patrick Huber, and Giuseppe Carenini. 2020.\nDo we really need that many parameters in trans-\nformer for extractive summarization? discourse can\nhelp ! In Proceedings of the First Workshop on Com-\nputational Approaches to Discourse, pages 124–134,\nOnline. Association for Computational Linguistics.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.\n2020. Discourse-aware neural extractive text sum-\nmarization. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5021–5031, Online. Association for Computa-\ntional Linguistics.\nNan Yu, Meishan Zhang, and Guohong Fu. 2018.\nTransition-based neural rst parsing with implicit syn-\ntax features. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 559–570.\nAmir Zeldes. 2017. The GUM corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 5059–5069, Florence, Italy. Association for\nComputational Linguistics.\n4150\nA Performance of the Summarization\nTask\nTable.7 shows the performance of different sum-\nmarization models. In general, adding additional\nlayers and heads does not consistently increase the\nperformance on the summarization task itself.\nDataset #Layer #Head R-1 R-2 R-L\nCNNDM 2 1 40.92 18.69 37.85\nCNNDM 2 8 41.02 18.78 37.96\nCNNDM 6 8 41.03 18.69 37.86\nNYT 2 1 43.64 25.58 36.87\nNYT 2 8 44.11 26.08 37.34\nNYT 6 8 43.93 25.99 37.15\nTable 7: The in-domain performance of the summariz-\ners.\nB Full Results on Overall Tree Parsing\nWe show the overall results of all the six summa-\nrization models on constituency/dependency pars-\ning in Table.8, the results of three of them are\nshown in Table.2, Table.3 and Table.4 in the main\npaper.\nC Results on Sensitivity to Initialization\nand Summarizer Quality\nTo explore if the models with different random ini-\ntialization have consistent performances, we train 5\nmodels with 6 layers and 8 heads on the CNNDM\ndataset with different initialization, and the results\nof each layer for constituency/dependency parsing\nare shown in Table.9. We can ﬁnd that the results\nare relatively consistent across different initializa-\ntion, and additional exploration on the performance\nof all the heads (Fig.5) show that, with different\ninitialization of the model, there is consistently one\nhead containing most of the discourse information,\nbut the position of that head is not ﬁxed.\nWe further do the experiments on dependency\nparsing during training the summarizer, to see how\nthe performance changes as the summarizer be-\ncome better, and show the max and mean UAS\nover three datasets for all attention heads in the\n’cnndm-6-8’ model by the CLE algorithms after\ntraining for (0, 1k, 5k, 10k, 20k) steps in Fig.6.\nWe also show the heatmaps of the average UAS\nacross three datasets for all the heads in Fig.7. We\ncan ﬁnd that as the summarizer is trained for more\nsteps, more discourse information is learned, and\nit’s more concentrated in one head. Interestingly,\nthe mean UAS of dependency trees generated by\nCLE algorithm with sentence constraints show a\ndifferent trend, which may due to the concentra-\ntion of the discourse information on single head\nas the model trained for more steps, as it shows in\nFigure.7.\n4151\nModel Const Eisner CLE\nNo Cons. Sent Cons. No Cons. Sent Cons. No Cons. Sent Cons.\nRSTDT\nCNNDM-2-1 61.16 / 59.67 76.23 / 74.63 23.65/ 4.80 28.24/ 18.23 21.56/ 1.45 29.29/ 19.56\nCNNDM-2-8 62.65 / 59.75 76.42 / 74.28 22.09 / 8.40 26.23 / 21.29 20.31 / 6.13 26.57 / 22.67\nCNNDM-6-8 60.33 / 60.79 75.44 / 75.04 7.89 / 20.48 13.83 / 27.78 7.28 / 17.30 16.10 / 28.50\nNYT-2-1 60.27 / 60.23 75.57 / 75.29 9.76 / 14.84 23.18 / 20.61 6.18 / 12.68 21.06 / 21.73\nNYT-2-8 63.20/ 59.65 76.63 / 75.23 7.35 / 9.74 16.04 / 21.27 6.44 / 7.09 16.72 / 22.90\nNYT-6-8 62.42 / 62.17 76.65/ 75.58 15.74 / 12.51 24.30 / 18.90 13.71 / 10.59 25.04 / 21.14\nRandom 58.60 (0.1) 74.10 (0.1) 11.16 (0.2) 20.28 (0.2) 1.67 (0.08) 18.72 (0.11)\nInstruction\nCNNDM-2-1 61.06 / 59.84 71.39 / 70.29 31.07/ 4.39 29.33/ 13.45 28.06/ 2.08 37.38/ 18.12\nCNNDM-2-8 61.44 / 60.55 71.13 / 71.09 26.98 / 8.89 24.72 / 14.75 24.56 / 5.23 28.70 / 20.58\nCNNDM-6-8 60.32 / 61.22 71.24 / 70.88 8.53 / 19.51 9.93 / 21.96 6.92 / 15.85 14.93 / 25.78\nNYT-2-1 60.31 / 61.30 71.40 /71.43 10.67 / 21.15 18.99 / 21.19 7.82 / 17.59 21.30 / 24.54\nNYT-2-8 61.27 / 60.51 70.80 / 70.90 6.26 / 12.59 13.64 / 19.34 5.25 / 7.96 13.39 / 21.92\nNYT-6-8 61.32/ 61.27 71.30 / 70.03 16.22 / 12.14 22.79 / 16.37 14.81 / 9.81 25.44 / 21.10\nRandom 59.49 (0.3) 70.53 (0.1) 13.14 (0.33) 19.31 (0.44) 2.94 (0.24) 17.88 (0.42)\nGUM\nCNNDM-2-1 58.74 / 57.69 72.73/ 71.92 21.28/ 2.24 27.26/ 16.12 19.50/ 0.70 28.77/ 17.92\nCNNDM-2-8 59.98 / 58.43 72.69 / 71.95 19.45 / 4.98 25.00 / 19.25 18.03 / 2.92 25.07 / 20.40\nCNNDM-6-8 58.92 / 59.30 72.40 / 72.69 4.74 / 15.80 11.53 / 24.79 4.01 / 13.14 14.85 / 25.37\nNYT-2-1 57.81 / 58.84 71.95 / 72.23 5.64 / 12.84 19.94 / 20.19 2.92 / 9.79 18.23 / 19.68\nNYT-2-8 60.17/ 58.22 71.98 / 71.82 5.66 / 7.22 15.21 / 18.81 4.54 / 3.96 15.25 / 19.31\nNYT-6-8 59.62 / 59.25 72.19 / 71.56 12.58 / 9.61 23.35 / 17.14 10.67 / 8.23 22.99 / 19.53\nRandom 57.47 (0.1) 71.50 (0.2) 10.37 (0.23) 19.15 (0.26) 0.92 (0.05) 17.01 (0.2)\nTable 8: The RST Parseval Scores of generated constituency trees, Unlabeled Attachment Score of generated\ndependency trees by Eisner algorithm and CLE algorithm on the three datasets. The numbers in each cell are\nrepresented as the performance of (Layer 0 / Layer 1) the results of Random are obtained by applying the parser\non random generated matrices for 10 times, and are represented as ’Average (Std)’.\nModel Const Eisner CLE\nNo Cons. Sent Cons. No Cons. Sent Cons. No Cons. Sent Cons.\nRSTDT\nCNNDM-6-8 61.13 / 61.63 75.81 / 75.41 10.32 / 20.99 16.42 / 27.08 9.40 / 18.16 18.89 / 28.33\n(1.11) / (1.35) (0.26) / (0.34) (4.03) / (2.80) (3.62) / (1.37) (3.92) / (3.25) (4.19) / (1.59)\nRandom 58.6 (0.1) 74.10 (0.1) 11.16 (0.2) 20.28 (0.2) 1.67 (0.08) 18.72 (0.11)\nInstruction\nCNNDM-6-8 61.87 / 61.06 70.84 / 70.94 11.50 / 19.78 12.91 / 22.45 9.79 / 16.53 17.81 / 26.30\n(1.17) / (0.98) (0.51) / (0.34) (5.71) / (2.17) (4.39) / (1.48) (5.31) / (2.73) (4.70) / (1.64)\nRandom 59.49 (0.3) 70.53 (0.1) 13.14 (0.33) 19.31 (0.44) 2.94 (0.24) 17.88 (0.42)\nGUM\nCNNDM-6-8 58.19 / 58.71 72.28 / 72.48 7.62 / 15.69 14.62 / 24.02 6.77 / 13.23 17.32 / 25.13\n(0.82) / (0.97) (0.27) / (0.16) (3.97) / (2.87) (3.77) / (1.36) (3.96) / (3.29) (3.85) / (1.22)\nRandom 57.47 (0.1) 71.50 (0.2) 10.37 (0.23) 19.15 (0.26) 0.92 (0.05) 17.01 (0.2)\nTable 9: The average RST Parseval Scores of generated constituency trees, the average Unlabeled Attachment\nScores of generated dependency trees by the Eisner and CLE algorithms, on the three datasets with 5 random\ninitialization, the numbers in parenthesis are the standard deviation across different run\n4152\nFigure 5: The heatmap of average UAS across three discourse datasets for all attention heads in the models with\ndifferent initialization by the CLE algorithm.\nFigure 6: Max and Mean UAS of dependency trees generated by CLE algorithm on all attention heads (48) of\nthe model ’cnndm-6-8’, after training for (0,1,5,10,20,23) K steps on RST-DT(top), Instructional(middle) and\nGUM(bottom) datasets. The corresponding ROUGE scores are increasing.\nFigure 7: The heatmaps of the average UAS across the three discourse datasets for all the heads during training the\nsummarization model.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9292075634002686
    },
    {
      "name": "Computer science",
      "score": 0.7666232585906982
    },
    {
      "name": "Transformer",
      "score": 0.6537324786186218
    },
    {
      "name": "Natural language processing",
      "score": 0.6360036730766296
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5672941207885742
    },
    {
      "name": "Discourse analysis",
      "score": 0.5070888996124268
    },
    {
      "name": "Dependency (UML)",
      "score": 0.44799181818962097
    },
    {
      "name": "Style (visual arts)",
      "score": 0.43370410799980164
    },
    {
      "name": "Linguistics",
      "score": 0.322753369808197
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}