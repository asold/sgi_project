{
  "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models",
  "url": "https://openalex.org/W4389519999",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098778358",
      "name": "Jae-Young Choe",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A5102590178",
      "name": "Keonwoong Noh",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2122026184",
      "name": "Nayeon Kim",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A3199513887",
      "name": "Seyun Ahn",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2121091091",
      "name": "Woo-Hwan Jung",
      "affiliations": [
        "Hanyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2787423662",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4285137661",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W4206729293",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3203486519",
    "https://openalex.org/W4385572697"
  ],
  "abstract": "Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2101–2112\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExploring the Impact of Corpus Diversity on Financial Pretrained\nLanguage Models\nJaeyoung Choe1 ∗\n, Keonwoong Noh 2, Nayeon Kim 1 ∗\n, Seyun Ahn 2, Woohwan Jung1,3\n1 Department of Applied Artificial Intelligence, Hanyang University\n2 Division of Computer Science, Hanyang University\n3 Ramply Inc.\n{cjy9100, rohgw011, na2na8, tpdbs0907, whjung}@hanyang.ac.kr\nAbstract\nOver the past few years, various domain-\nspecific pretrained language models (PLMs)\nhave been proposed and have outperformed\ngeneral-domain PLMs in specialized areas such\nas biomedical, scientific, and clinical domains.\nIn addition, financial PLMs have been stud-\nied because of the high economic impact of\nfinancial data analysis. However, we found\nthat financial PLMs were not pretrained on\nsufficiently diverse financial data. This lack\nof diverse training data leads to a subpar gen-\neralization performance, resulting in general-\npurpose PLMs, including BERT, often outper-\nforming financial PLMs on many downstream\ntasks. To address this issue, we collected a\nbroad range of financial corpus and trained the\nFinancial Language Model (FiLM) on these\ndiverse datasets. Our experimental results con-\nfirm that FiLM outperforms not only existing\nfinancial PLMs but also general domain PLMs.\nFurthermore, we provide empirical evidence\nthat this improvement can be achieved even for\nunseen corpus groups.\n1 Introduction\nPretrained Language Models (PLMs) have been\nsuccessfully employed in various natural language\nprocessing tasks. This trend has been extended\nto further pretraining of PLMs on domain-specific\ncorpus in various fields such as biomedical (Lee\net al., 2020), scientific (Beltagy et al., 2019), and\nclinical (Alsentzer et al., 2019) domains.\nIn financial domain, Araci (2019); Yang et al.\n(2020); Liu et al. (2021); Loukas et al. (2022) pro-\nposed domain-specific PLMs based on BERT (De-\nvlin et al., 2019) model. However, each study tested\na small number of tasks, resulting in inadequate\nvalidation. We conduct extensive experiments to\nshow that the generalization performance of exist-\ning financial PLMs is unsatisfactory. Even general\ndomain PLMs such as BERT and RoBERTa (Liu\n∗Major in Bio Artificial Intelligence\net al., 2019) outperformed the financial PLMs on\nfinancial tasks.\nTo investigate the reasons for the low general-\nization performance, we categorize the financial\ncorpus into five groups and examine their use in\nexisting financial PLMs. This reveals that most ex-\nexisting PLMs are pretrained on a corpus consisting\nof only two of these corpus groups. This indicates\nthat the pretraining data for financial PLMs lacks\ndiversity, which might cause their low generaliza-\ntion performance across various financial tasks.\nMotivated by this observation, we collect a broad\nrange of corpus and analyze the impact of corpus\ndiversity on the performance of language models.\nOur investigations demonstrate that as a corpus be-\ncomes more diverse, the model’s performance im-\nproves. Incorporating a diverse corpus, even with\nfewer tokens, yields a better generalization perfor-\nmance than relying on numerous tokens from a non-\ndiverse corpus. Furthermore, when using diverse\ncorpora, the model exhibits robust generalization\nperformance on unseen tasks.\nWe train our Financial Language Model (FiLM)\non a corpus with diverse documents and evaluate\nit on six financial tasks, including recently intro-\nduced works (Chen et al., 2021; Loukas et al., 2022;\nShah et al., 2023). Our experimental results show\nthat FiLM outperforms not only existing financial\nPLMs but also general domain PLMs on most finan-\ncial tasks. In addition, we achieve an improvement\nof performance while reducing the number of to-\nkens trained and energy consumption. To the best\nof our knowledge, FiLM is the first model to sur-\npass RoBERTa in financial domain. We make our\nmodel and code publicly available on GitHub1 and\nHuggingface hub2 for continuous advancement in\nfinancial domain.\n1https://github.com/deep-over/FiLM\n2https://huggingface.co/HYdsl/FiLM\n2101\nGroup Name Description Financial PLMs # Tokens\nNews\nTRC2 Financial news stories from Reuters i, iv 227.3M\nInvesting.com Stock, options, commodity etc. News article - 130.8M\nNYtimes Economic articles from the New York Times - 75M\nEIA Commodity related news articles from EIA - 1.1M\nSEC filings Annual reports(10-K) and quarterly reports(10-Q) ii, iv, v 307.1M\nEarnings Call Earnings conference call transcripts ii, iv 1.6B\nPapers ArXiv A collection of abstracts of economic research papers - 42.1M\nAIHUB A collection of Korean economics research papers - 5.8M\nMISC Investopedia Economic glossary iv 5.3M\nFinWEB Finance, loans, and insurance related articles iii 2.8M\nA total of 10 datasets 2.4 B\nTable 1: Summary of the financial datasets used for pre-training the models. The ‘Financial PLMs’ column\nrepresents the financial PLMs that have utilized each respective dataset: i. Araci (2019), ii. Yang et al. (2020),\niii. Liu et al. (2021), iv. Shah et al. (2022), v. Loukas et al. (2022)\n(a) Pretraining datasets\n (b) Financial tasks\nFigure 1: Sentence embeddings visualization for both\ncorpus groups and financial tasks.\n2 Proposed method\nFirst, the datasets used to train the model are in-\ntroduced. Second, we describe the method for pre-\nprocessing the datasets. Finally, we describe FiLM\ntraining procedure.\n2.1 Pretraining datasets\nFor further pretraining the language models, we\ncollected a financial corpus from 10 sources. There\nare various financial documents with different char-\nacteristics. For instance, financial reports have a\nhigher prevalence of numerical information than\nother texts such as the abstracts of research in fi-\nnance (Loukas et al., 2022).\nAs shown in Table 1, we categorize the pretrain-\ning datasets into the five groups as follows:\n• News: The datasets are sourced from financial\nnews articles.\n• SEC filings: This dataset comprises financial\nreports (10-K, 10-Q) submitted to the U.S.\nSecurities and Exchange Commission (SEC).\n• Earnings call: This dataset comprises the in-\nformation and transcripts of earnings confer-\nence calls obtained from Seeking Alpha.\n• Papers: This group contains the abstracts of\nresearch papers in the field of economics.\n• MISC: It includes other datasets in the finan-\ncial domain.\nFor more detailed information on the corpus and\ngroups, please refer to Appendix A.1. Figure 1a\nshows the embeddings of sentences in the pretrain-\ning corpus. The color of each point represents the\ngroup to which the corresponding sentence belongs.\nThis indicates that sentences within the same group\nhave similar embeddings, thereby forming clusters.\nFor example, sentences in the financial news group\nare primarily distributed in the right and lower ar-\neas, whereas SEC filings are concentrated on the\ntop side.\nNone of the corpus groups was distributed over\nthe entire space. From this observation, we could\nconclude that it is essential to use data from all\ngroups for pretraining to enable the language model\nto learn diverse features.\nHowever, existing studies do not use all these\ngroups to further train language models. In addi-\ntion, most of them (Yang et al., 2020; Araci, 2019;\nLoukas et al., 2022) use only two groups of datasets.\nConsequently, these models might not achieve high\nperformance across the entire financial domain. To\nattain robust performance over the entire financial\ndomain, we employ all groups of datasets for pre-\ntraining.\n2.2 Preprocessing\nThe preprocessing for the pretraining involves two\nsteps: cleaning and deduplication. In the clean-\ning step, we remove HTML tags using Beautiful-\nSoup3 and unnecessary special characters such as\n3www.crummy.com/software/BeautifulSoup/\n2102\nnewline characters. Deduplication could improve\nthe efficiency of pretraining and the accuracy of\nPLMs (Lee et al., 2022). Thus, we remove du-\nplicate sentences from the datasets during prepro-\ncessing. For deduplication, we remove all but one\nsentence from each group of duplicate sentences\nin the corpus. The contents of the remaining doc-\numents were preserved if a sentence was removed\nduring deduplication. After deduplication, the to-\nken count decreased from 3.3B to 2.4B, and the\nsize of the training dataset reduced from 19.5GB\nto 14GB.\n2.3 Training procedure for FiLM\nTo train FiLM, we further pretrain RoBERTa on\nfinancial corpus presented in Section §2.1. Follow-\ning RoBERTa, we use the masked language model\nfor the pretraining task. In addition, we use the\nsame tokenization method and model input format\nas in RoBERTa. We further pretrain our model for\nonly one epoch because the performance saturates\nafter one epoch. We use a batch size of 16 and a\nlearning rate of 1e-5. We use the Adam optimizer\nand do not set any specific scheduler or warm-up\nsteps. Please refer to Table 5 for the hyperparame-\nter settings.\n3 Financial tasks for evaluation\nWe evaluate the performance of financial PLMs on\nthe following six financial NLP tasks:\n• FPB (Malo et al., 2014): This sentiment clas-\nsification task involves three categories: posi-\ntive, negative, and neutral. The dataset com-\nprises 4,840 sentences from financial news\narticles.\n• NER (Alvarado et al., 2015): The goal of this\nfinancial named entity recognition task is to\nidentify four types of named entities (PER,\nLOC, ORG, and MISC) within financial con-\ntracts reported to the SEC.\n• Headline (Sinha and Khandait, 2021): The ob-\njective of this task is to classify the impact of\nnews articles pertaining to gold commodities\nbased on the headline of the articles.\n• FiNER (Loukas et al., 2022) This is a numeric\nentity recognition task. The dataset comprises\nXBRL-tagged financial reports from publicly\ntraded companies.\n• FinQA (Chen et al., 2021): This is a financial\nquestion-answering task for evaluating numer-\nical reasoning and understanding. The dataset\nis based on profit and loss reports of S&P 500\ncompanies.\n• FOMC (Shah et al., 2023): This aims to clas-\nsify text generated by the Federal Open Mar-\nket Committee (FOMC) in order to assess its\nimpact on the financial markets. This dataset\nclassifies the policy stance as either \"Hawk-\nish\" or \"Dovish.\" Data collection for this task\nwas conducted up until October 15, 2022.\nFigure 1b shows the embeddings of the sentences\nsampled from the financial tasks. The colors of\neach point distinguish the six tasks and the gray\npoints represent the sentences from the pretraining\ndata. FiNER and FinQA are located on the top side\nbecause both tasks use SEC filings to create the\ndataset. Meanwhile, the Headline task is located\nfurther away from the other tasks due to its focus\non the gold commodity. In addition, the sentences\nin FPB, NER, and FOMC form their own clusters,\nseparate from the other datasets.\nAs observed, each financial NLP task has unique\naims and distinctive textual features. This implies\nthat the performance of language model has to be\nevaluated across a broad range of tasks. For more\ndetailed information about financial tasks, please\nrefer to Appendix B.\n4 Experiments\n4.1 Experiment setup\nWe compared the FiLM model with existing fi-\nnancial domain PLMs: FinBERT-A (Araci, 2019),\nFinBERT-Y (Yang et al., 2020), FLANG-BERT &\nFLANG-RoBERTa (Shah et al., 2022), and SEC-\nBERT (Loukas et al., 2022). Furthermore, we fine-\ntune general domain PLMs, BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019), to establish\nthe baselines. For detailed information on each\nmodel, please refer to Appendix C.\nFor all experiments, except for FiNER and\nFinQA, the results are computed based on the aver-\nage score across three seed values. For the standard\ndeviation obtained through all experiments, please\nrefer to Table 8.\nAll the models are trained on an RTX3090 GPU.\nFor the detailed settings for fine-tuning, please refer\nto Appendix B. The dataset and hyperparameters\nused to further pretrain FiLM are provided in Ap-\npendix A. Using this setup, the training of FiLM\non the entire dataset can be completed within 24\nhours, resulting in a high-performance language\nmodel while maintaining a low training cost.\n2103\nModel # Tokens FPB NER Headline FiNER FinQA FOMC\n(Financial Corpus)Accuracy F-1 F-1 F-1 F-1 Prog Acc Exe Acc F-1\nGeneral Domain PLMs\nBERT (Devlin et al., 2019) 83.30 81.73 75.09 89.54 79.40 51.09 53.10 63.81\nRoBERTa (Liu et al., 2019) 85.30 83.93 78.81 91.29 81.58 56.76 59.11 69.16\nExisting Financial Domain PLMs\nFinBERT-A (Araci, 2019) 237M 85.25 82.45 77.93 90.48 81.49 47.86 50.04 64.50\nFinBERT-Y (Yang et al., 2020) 4.9B 83.68 82.52 70.40 90.83 81.08 38.79 40.54 64.30\nFLANG-BERT (Shah et al., 2022)NA 84.76 83.12 75.58 91.06 81.52 49.17 51.43 64.93\nFLANG-RoBERTa (Shah et al., 2022)NA 83.86 82.19 71.36 90.46 80.77 30.68 32.17 68.02\nSEC-BERT (Loukas et al., 2022)3.1B 84.37 82.18 78.74 90.52 82.35 53.18 55.45 65.07\nProposed models\nFiLM 2.4B 86.25 84.48 79.78 91.79 82.02 58.85 61.38 69.60\nFiLM (5.5B) 5.5B 86.14 84.1178.82 91.74 82.39 59.37 61.64 69.16\nTable 2: Main results. FiLM (5.5B) is the model trained on an additional SEC filings dataset comprising 3.1B tokens\n(Appendix A). In each task, the best score is marked in bold, while the second best score is underlined. Shah et al.\n(2022) did not report the number of tokens but utilized approximately 2.78 million documents.\n4.2 Main results\nTable 2 reports the results of evaluating the per-\nformance of PLMs on financial tasks. Despite\nfurther training on financial corpus, existing fi-\nnancial PLMs often perform worse than gen-\neral domain PLMs on certain tasks. For exam-\nple, RoBERTa outperforms all existing financial\nPLMs on FPB, FinQA, and FOMC. In addition,\nBERT outperforms FinBERT-A, FinBERT-Y, and\nFLANG-BERT on FinQA tasks. This implies that\nfinancial PLMs exhibit a deficiency in their gener-\nalization capabilities, limiting their effectiveness\nacross a diverse spectrum of documents within the\nfinancial domain. However, our model, which in-\ncorporates a diverse range of financial corpus for\npretraining, has been equipped with robust gener-\nalization capabilities, enabling it to perform well\nacross various NLP tasks in the financial domain.\nAlthough our model is trained on fewer tokens\n(2.4B) than SEC-BERT (3.1B) and FinBERT-Y\n(4.9B), it outperforms existing PLMs on most fi-\nnancial tasks.\n4.3 Impacts of the diversity in the pretraining\ndataset\nWe investigate the variation in the performance of\nthe language model based on the number of cor-\npus groups used. To achieve this, we generate all\ncombinations of corpus groups and pretrain sep-\narate RoBERTa on each combination. Then, we\nevaluate each model by calculating the average F1\nscore across four downstream tasks: FPB, NER,\nHeadline, and FiNER. Figure 2 presents the aver-\nage F1 scores when the number of corpus groups\nvaried. This indicates that as the number of groups\nincreases, the model’s performance improves. This\nModel # GroupsNER FiNER FinQA\nF-1 F-1 Prog Acc\nFiLM [Ours](2.4B) 5 79.78 82.02 58.85\nw/o SEC filings(2.1B)4 76.51 81.94 57.54\nonly SEC filings(3.1B)1 75.51 81.91 57.45\nonly SEC filings(0.3B)1 75.30 81.64 57.10\nTable 3: Comparison of model performance on NER,\nFiNER, FinQA under different pre-training data set-\ntings.\nfinding suggests that a more diverse corpus leads\nto enhanced model performance.\n1 2 3 4\nNumber of corpus groups\n0.840\n0.845\n0.850\n0.855Average F1 score\nALL\nZERO\nFigure 2: Average F1 scores measured on four financial\ntasks, with varying the number of corpus groups for\npretraining.\n4.4 Performance extrapolation\n4.4.1 SEC filings dataset\nWe present empirical evidence that performance\nimprovements can be achieved even for unseen cor-\npus groups. To validate this, we select downstream\ntasks derived from SEC filings: NER, FiNER, and\nFinQA. Table 3 shows the performance of our\nmodel trained on a different pretraining corpus.\nThe model pretrained on the four corpus groups\nwithout SEC filings outperform the model further\ntrained on SEC filings only. Furthermore, despite\nthe use of a larger number (3.1B) of tokens derived\n2104\nModel # Tokens # Groups GPU(power) GPU Time Total Energy ConsumptionNER FiNER FinQA\nF-1 F-1 Prog Acc\nFiLM 2.4B 5 RTX3090(0.36kW) 23h 8.3kWh 79.78 82.02 58.85\n- only SEC filings 3.1B 1 RTX3090(0.36kW) 32h 11.2kWh 75.51 81.91 57.45\nFinBERT-Y 4.9B 2 Tesla P100(0.25kW) 192h 48.0kWh 70.40 81.08 38.79\nTable 4: Total energy consumptions for training FiLM and FinBERT-Y . The GPU time of FinBERT-Y is from Yang\net al. (2020).\nfrom SEC filings for further pretraining, this model\ndoes not exceed the performance of the model\ntrained using fewer (2.1B) tokens from four dif-\nferent corpus groups. This demonstrates that incor-\nporating diverse corpora can improve performance\nfor unseen tasks, which can be more important than\ndata volume. Furthermore, we anticipate that our\nmodel, denoted FiLM, may exhibit robust perfor-\nmance across a wide range of financial downstream\ntasks derived from unseen financial corpora.\n4.4.2 Macroeconomic perspective\nThe FOMC task is a macroeconomics-based task\ndesigned to predict changes in monetary policy\nstance (Shah et al., 2023). This dataset comprises\nsentences extracted from FOMC speeches, meet-\ning minutes, and press conferences. None of these\nsentences are included in pretraining dataset used\nfor FiLM. To substantiate that FiLM performs ro-\nbustly on unseen tasks, we evaluate models on the\nFOMC task. Table 2 represents that existing finan-\ncial PLMs underperform RoBERTa, as pointed out\nin Shah et al. (2023). Meanwhile, our FiLM model\noutperforms RoBERTa. This highlights that FiLM\nis the first model to surpass RoBERTa in financial\ndomain. For the results of all experiments of the\nFOMC task and their detailed explanation, please\nrefer to Table 9 and Appendix D, respectively.\n4.5 Cost-efficiency\nRecently, the environmental impacts of large lan-\nguage models have become a critical issue due\nto their significant energy consumption and car-\nbon emissions during the training model pro-\ncess (Strubell et al., 2019; Scao et al., 2022; Zeng\net al., 2022; Touvron et al., 2023). Since further\ntraining is required for developing domain-specific\nPLMs, additional resources are consumed. We pro-\nvide empirical evidence that when ensuring corpus\ndiversity, energy consumption is decreased while\nenhancing performance. Table 4 shows the total en-\nergy consumption for training a financial PLM. We\ncompute the electric energy required for training a\nmodel, using the following formula (Touvron et al.,\n2023): Energy (Wh) = GPU power (W) × GPU\ntime (h). Consequently, compared to FinBERT-Y,\nFiLM exhibits an 82% reduction in total energy\nconsumption while achieving an average perfor-\nmance of 10% gain.\nConclusion\nWe show that financial PLMs struggle with various\nfinancial tasks due to constrained pretraining data\ndiversity. To address this problem, we train our\nFiLM model using a broad spectrum of financial\ndata. We empirically show that FiLM outperforms\nother financial PLMs across six financial down-\nstream tasks. Specifically, FiLM works robustly\non unseen tasks and also attains superior perfor-\nmance on macroeconomics-based task. Further-\nmore, our experimental results indicate training on\ndiverse corpora reduces energy consumption, re-\nsulting in environmental benefits. Our study under-\nscores the significance of leveraging diverse data\nto train domain-specific language models.\nLimitations\nFinancial documents often attribute substantial sig-\nnificance to numerical data, more so than other\ntypes of documents. Therefore, we acknowledge\nthe necessity of an efficient technique for process-\ning numerical tokens, particularly for financial\nPLMs. However, when we tested several meth-\nods proposed in previous studies, we did not ob-\nserve any significant improvement. Techniques for\nhandling numeric tokens are excluded from our\nstudy; however, we highlight this as a valuable area\nfor future investigations. Finally, our FiLM is an\nencoder-based model that is not suitable for gener-\native tasks, such as financial news summarization.\nAcknowledgments\nThis work was supported by Institute of In-\nformation & communications Technology Plan-\nning & Evaluation(IITP) grant funded by the Ko-\nrea government(MSIT) (No. RS-2023-00261068,\n2105\nDevelopment of Lightweight Multimodal Anti-\nPhishing Models and Split-Learning Techniques\nfor Privacy-Preserving Anti-Phishing), (No.RS-\n2022-00155885, Artificial Intelligence Conver-\ngence Innovation Human Resources Development\n(Hanyang University ERICA)), and (2018-0-00192,\nthe National Program for Excellence in SW).\nThis work was supported by the National Re-\nsearch Foundation of Korea(NRF) grant funded\nby the Korea government(MSIT) (No. NRF-\n2022R1G1A1013549). Finally, we thank the re-\nviewers for their detailed feedback, which helped\nto improve the quality of this paper.\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, W A Redmond,\nand Matthew BA McDermott. 2019. Publicly avail-\nable clinical bert embeddings. NAACL HLT 2019,\npage 72.\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop 2015, pages 84–90.\nDogu Araci. 2019. Finbert: Financial sentiment analy-\nsis with pre-trained language models. arXiv preprint\narXiv:1908.10063.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3615–3620.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, et al.\n2021. Finqa: A dataset of numerical reasoning over\nfinancial data. arXiv preprint arXiv:2109.00122.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,\nand Jun Zhao. 2021. Finbert: A pre-trained finan-\ncial language representation model for financial text\nmining. In Proceedings of the twenty-ninth interna-\ntional conference on international joint conferences\non artificial intelligence, pages 4513–4519.\nLefteris Loukas, Manos Fergadiotis, Ion Androutsopou-\nlos, and Prodromos Malakasiotis. 2021. EDGAR-\nCORPUS: Billions of tokens make the world go\nround. In Proceedings of the Third Workshop on\nEconomics and Natural Language Processing, pages\n13–18, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nLefteris Loukas, Manos Fergadiotis, Ilias Chalkidis,\nEirini Spyropoulou, Prodromos Malakasiotis, Ion\nAndroutsopoulos, and Georgios Paliouras. 2022.\nFiNER: Financial numeric entity recognition for xbrl\ntagging. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4419–4431.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-\nlenius, and Pyry Takala. 2014. Good debt or bad\ndebt: Detecting semantic orientations in economic\ntexts. Journal of the Association for Information\nScience and Technology, 65(4):782–796.\nLeland McInnes, John Healy, and James Melville. 2018.\nUmap: Uniform manifold approximation and pro-\njection for dimension reduction. arXiv preprint\narXiv:1802.03426.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\n2106\nAgam Shah, Suvan Paturi, and Sudheer Chava. 2023.\nTrillion dollar words: A new financial dataset, task &\nmarket analysis. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6664–6679,\nToronto, Canada. Association for Computational Lin-\nguistics.\nRaj Sanjay Shah, Kunal Chawla, Dheeraj Eidnani,\nAgam Shah, Wendi Du, Sudheer Chava, Natraj Ra-\nman, Charese Smiley, Jiaao Chen, and Diyi Yang.\n2022. When flue meets flang: Benchmarks and\nlarge pre-trained language model for financial do-\nmain. arXiv preprint arXiv:2211.00083.\nAnkur Sinha and Tanmay Khandait. 2021. Impact of\nnews on the commodity market: Dataset and results.\nIn Advances in Information and Communication:\nProceedings of the 2021 Future of Information and\nCommunication Conference (FICC), Volume 2, pages\n589–601. Springer.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model\nfor financial communications. arXiv preprint\narXiv:2006.08097.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nA Pretraining information\nA.1 Datasets\n• TRC2: TThe Thomson Reuters Text Re-\nsearch Collection (TRC2) corpus comprises\n1,800,370 news stories published from 2008\nto 2009. For more information on the data and\nacquisition, please refer to https://trec.\nnist.gov/data/reuters/reuters.html\n• Investing.com4: Investing.com is a financial\nplatform and news website that offers a wide\nrange of information including stocks, fu-\ntures, options, and commodities. The his-\ntorical dataset is a compilation of news data\nsourced from Investing.com between 2009\nand 2020.02.\n• NYtimes 5: The New York Times is a well-\nknown newspaper worldwide. The NYTimes\nDataset comprises finance-related articles col-\nlected from the NYTimes website. The data\ncollection period spanned from 2005 to 2021.\n• EIA: The U.S. Energy Information Admin-\nistration (EIA) gathers, analyzes, and shares\nunbiased energy information to help create\nbetter policies, ensure efficient markets, and\nprovide a public understanding of the impact\nof energy on the economy and environment.\nWe collected news data only from the infor-\nmation provided by eia.gov.\n• SEC filings6: The SEC receives annual re-\nports (10-K) and quarterly reports (10-Q)\nfrom public US companies. These reports con-\ntains information about a company’s business\nand financial performance. We downloaded\nthe reports from SEC Edgar between 2020\nand 2021. For the experiment(Section 4.4),\nadditional data from 1993 to 2019 were in-\ncluded (Loukas et al., 2021).\n• Earnings call: Earnings conference calls are\nimportant in delivering corporate information.\nSeeking Alpha7 provides access to earnings\nconference call transcripts, and we collected\nthe data.\n• Arxiv8: This dataset is a collection of ab-\nstracts from economics research sourced from\narxiv.org\n• AIHUB: This dataset comprises a collection\nof Korean academic papers obtained and trans-\nlated by professional translators. Furthermore,\nprofessors specializing in translation studies\nreviewed the translated corpus. We used a sub-\nset of the corpus that focused on economics-\nrelated data. This research used datasets from\n‘The Open AI Dataset Project (AI-Hub, South\n4https://www.kaggle.com/datasets/gennadiyr/\nus-equities-news-data\n5https://www.nytimes.com/section/business/\neconomy\n6https://www.sec.gov/\n7https://seekingalpha.com/\n8https://arxiv.org/search/econ\n2107\nKorea)’. All data information can be accessed\nthrough ’AI-Hub’(www.aihub.or.kr).\n• FinWEB9: The dataset is a website that pro-\nvides economic knowledge and information\non finance, loans, and products. We collected\ndata by crawling websites.\n• Investopedia10: The dataset is a financial web-\nsite that functions as an economic dictionary,\nsimilar to Wikipedia, and provides definitions\nof economic terms. We collected all the eco-\nnomic terms available on the website.\nA.2 Hyperparameters for pretraining\nTable 5 shows the hyperparameters used to further\npretrain FiLM.\nHyperparameters FiLM\nInitializaion RoBERTa-base\nLearning rate 1e-5\nBatch size 16\nMax epochs 1\nMax length 512\nWeight decay 0\nWarmup steps 0\nTable 5: Pretraining hyperparameters setting.\nB Fine-tuning methodologies\nWe introduce the methods applied for finetuning\nPLMs on financial tasks as well as the experimen-\ntal settings used. All tasks followed the methods\nand datasets proposed in each study. The headline\ntask was performed using a sentiment classification\nmethod proposed by the authors 11. Table 6 lists\nthe descriptions, dataset sizes, and metrics for each\ntask. We followed the finetuning method of BERT.\nFurthermore, classification tasks (FPB, Headline,\nFOMC) were finetuned using sequence classifica-\ntion, which involved passing the [CLS] token rep-\nresentation for processing. The entity recognition\n(NER, FiNER) task follows token classification\nfinetuning using word representations. FinQA fol-\nlows the question answering system introduced by\nChen et al. (2021). Table 7 provides the parameters\nused to finetune each task are provided. FiNER12\nand FinQA13 can be accessed and reviewed on the\nofficial GitHub repository provided.\n9https://www.finweb.com/\n10https://www.investopedia.com/\n11https://www.kaggle.com/datasets/ankurzing/\nsentiment-analysis-in-commodity-market-gold\n12https://github.com/nlpaueb/finer\n13https://github.com/czyssrs/FinQA\nC Existing financial PLMs\nWe provide a summary of the financial PLMs used\nin previous research and organize the huggingface\nhub model used for the experiments. In the list\nbelow, items marked with \"✓\" indicate models that\nhave undergone further training, while those with-\nout the mark are models trained from scratch.\n• FinBERT-A ✓ (Araci, 2019) is the initial fi-\nnancial PLM. It is trained using the TRC2\ndataset. FinBERT-A focuses on the FPB\ntask and demonstrates superior performance\ncompared with the original BERT. https:\n//huggingface.co/ProsusAI/finbert\n• FinBERT-Y (Yang et al., 2020) used more\nthan three datasets compared with FinBERT-\nA and validated its performance across\nthree additional tasks. In addition, in-\nstead of using the traditional BERT tok-\nenizer and pretraining for the financial ap-\nproach, FinBERT-Y generates and applies a\nfinancial vocabulary, resulting in improved\nperformance. https://huggingface.co/\nyiyanghkust/finbert-tone\n• FinBERT-L (Liu et al., 2021) collected a gen-\neral domain corpus (3.3B tokens) and a fi-\nnancial corpus (12.7B tokens) to train the fi-\nnancial BERT model. Unlike the traditional\nfurther pretraining approach, FinBERT-L em-\nploys multitask self-supervised pretraining\nduring the training process. However, because\nthe proposed model is not publicly available,\na comparative experiment could not be con-\nducted.\n• SEC-BERT (Loukas et al., 2022) is the first\nto introduce the FiNER task. The BERT\nmodel was pretrained exclusively using the\nSEC filings. This study emphasized con-\nstructing vocabulary solely from the finan-\ncial reports dataset, with a specific focus on\nnumerical tokens found in financial reports.\nIn addition, SEC-BERT proposes a method\nfor substituting numeric tokens in the con-\ntext of FiNER. https://huggingface.co/\nnlpaueb/sec-bert-base\n• FLANG-BERT & FLANG-RoBERTa✓ (Shah\net al., 2022) is the first to create a bench-\nmark dataset for financial tasks by aggregat-\ning a diverse range of tasks. In addition,\n2108\nName Task Dataset Size MetricTrain Valid Test\nFPB Sentiment classification 3,391 726 726 Accuracy & F-1\nNER Named entity recognition 932 232 302 F-1\nHeadline News headlines classification 7,989 1,141 2,282 F-1\nFiNER Numeric entity recognition 900,384 112,494 108,378 F-1\nFinQA Question answering 6,251 883 1,147 Accuracy(Prog & Exe)\nFOMC Sentiment classification 1,588 396 496 F-1 (Combined-S)\nTable 6: Summary of financial tasks and their main aspects.\nHyperparam FPB NER Headline FOMC FiNER FinQA\nLearning rate {1e-3, 1e-4, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5} 1e-5 2e-5\nBatch size {16, 32, 64} {8, 16} {64, 128} {8, 16, 32} 16 16\nMax epochs 20 100 100 100 30 300\nMax length {64, 128} 512 64 256 200 512\nTable 7: Hyperparameter settings for each downstream task.\nthis study investigated and applied pretraining\nmethods optimized for economics during the\nfinetuning process. https://huggingface.\nco/SALT-NLP/FLANG-BERT\nD Results of the FOMC task\nIn this section, we provide supplementary expla-\nnations for the results presented in Table 9. The\nFOMC task is composed of datasets that are cate-\ngorized into three types:\n• Meeting Minutes (MM): These are reports de-\nrived from the FOMC’s eight annually sched-\nuled meetings.\n• Press Conference Transcripts (PC): These in-\nclude prepared remarks as well as the Q&A\nsessions between the Federal Reserve Chair\nand the press.\n• Speeches (SP): This category comprises any\nspeeches delivered by officials of the Federal\nReserve.\n• Additionally, there is a \"Combined\" category\nthat merges all three aforementioned types.\nTexts with a \"-S\" indicator signify that they have\nbeen split. This is because the FOMC often em-\nploys neutral sentences to maintain market stability\nand minimize excessive reactions. To address this\nissue, the authors of this study employed a rule-\nbased approach to split sentences that exhibit a\nneutral stance. All scores were obtained by strictly\nfollowing the settings provided in the GitHub link14\nin the Shah et al. (2023). Numbers in parentheses\nindicate the standard deviation.\n14https://github.com/gtfintechlab/\nfomc-hawkish-dovish\nFiLM demonstrates superior performance across\nall types with the exception of \"PC\". When com-\npared to RoBERTa-base, there is an improvement\nof +1.6 in performance in the \"Combined\". No-\ntably, there are substantial increases in performance\nmetrics for \"MM-S\" and \"SP\", with \"MM-S\" show-\ning a 3.98% improvement and \"SP\" a 4.69% im-\nprovement.\nE Comparison with a financial LLM\nWu et al. (2023) introduced the BloombergGPT,\na language model with 50 billion parameters\ntrained on a large-scale financial corpus. Ta-\nble 10 presents the results of the BloombergGPT\nand encoder-based PLMs on FPB and NER. For\nthe BloombergGPT, we report the numbers pro-\nvided in this study (Wu et al., 2023). Note\nthat BloombergGPT was evaluated by conduct-\ning 5-shot learning on the FPB and 20-shot learn-\ning on the NER. Remarkably, FiLM, BERT, and\nRoBERTa outperformed BloombergGPT, which\nhad many more parameters and required a higher\ncost. This demonstrates the limitations of using\nlarge language models for in-context learning in\nfinancial domain.\nF Visualization of financial domain\ndatasets\nTo examine the distribution of sentences from fi-\nnancial domain datasets in the embedding space\nin Figure 1, we sampled 10,000 sentences from\neach pretraining dataset and 500 sentences from\neach downstream task. Then, we generate embed-\nding representations for sampled sentences using\nthe approach proposed in Reimers and Gurevych\n2109\nModel FPB NER Headline FOMC\nStd(acc) Std(F-1) Std(F-1) Std(F-1) Std(F-1)\nBERT 1.302 0.682 1.479 1.852 1.49\nRoBERTa 0.736 0.908 0.734 0.961 1.385\nFinBERT-A 0.758 1.540 0.897 1.087 1.102\nFinBERT-Y 1.284 1.014 1.734 0.927 1.482\nFLANG-BERT 0.202 1.197 0.951 0.429 1.661\nFLANG-RoBERTa 0.835 0.914 5.002 1.233 0.509\nSEC-BERT 0.986 0.676 1.642 0.637 2.778\nFiLM 0.605 0.702 1.528 1.387 1.802\nFiLM (5.5B) 0.724 0.854 1.446 1.507 1.58\nTable 8: The standard deviation of model performance for FPB, NER, Headline, and FOMC.\nModel MM MM-S PC PC-S SP SP-S Combined Combined-S\nBERT 57.82\n(5.182)\n63.42\n(2.705)\n45.33\n(9.604)\n53.26\n(3.686)\n61.93\n(3.072)\n61.92\n(0.843)\n62.60\n(1.154)\n63.81\n(1.49)\nRoBERTa 66.96\n(4.619)\n66.19\n(4.048)\n54.08\n(0.944)\n54.55\n(9.701)\n65.08\n(2.036)\n66.99\n(1.676)\n69.04\n(0.784)\n69.16\n(1.385)\nFinBERT-A 59.98\n(3.786)\n65.01\n(2.654)\n45.04\n(5.62)\n51.33\n(5.262)\n65.59\n(2.341)\n62.82\n(3.419)\n63.93\n(2.625)\n64.50\n(1.102)\nFinBERT-Y 58.39\n(3.351)\n60.24\n(1.093)\n46.36\n(8.636)\n52.45\n(16.101)\n63.40\n(0.599)\n61.87\n(1.175)\n59.41\n(3.081)\n64.30\n(1.482)\nFLANG-BERT 61.54\n(5.214)\n66.71\n(1.601)\n51.74\n(8.891)\n47.03\n(7.254)\n62.35\n(2.973)\n62.57\n(0.874)\n63.39\n(1.261)\n64.93\n(1.661)\nFLANG-RoBERTa 62.30\n(2.813)\n67.18\n(1.711)\n51.47\n(2.595)\n49.44\n(2.265)\n65.18\n(1.754)\n63.53\n(1.553)\n64.82\n(2.853)\n68.02\n(0.509)\nSEC-BERT 64.46\n(8.053)\n67.64\n(3.455)\n53.67\n(2.238)\n44.51\n(14.825)\n67.10\n(2.015)\n65.11\n(3.114)\n68.10\n(0.525)\n65.06\n(2.778)\nFiLM (2.4B) 67.54\n(4.062)\n70.17\n(1.682)\n52.27\n(3.322)\n54.09\n(2.2)\n69.77\n(2.118)\n68.66\n(1.901)\n70.64\n(1.077)\n69.60\n(1.802)\nTable 9: Results of the FOMC task.\nModel Params FPB NER\nF-1 F-1\nBloombergGPT 50B 51.07 60.82\nBERT 110M 81.73 75.09\nRoBERTa 110M 83.93 78.81\nFiLM (2.4B tokens) 110M 84.48 79.79\nTable 10: Results on FPB and NER for BloombergGPT,\nBERT, RoBERTa, and FiLM.\nIntra-group\ndistance\nNews 3.529\nSEC filings 1.819\nEarnings call 2.557\nPapers 2.568\nMISC 3.378\nTable 11: Mean distances among sentence embeddings\nwithin a corpus group.\n(2019). To visualize sentence embeddings, we\nreduce the dimensionality of embeddings using\nUMAP (McInnes et al., 2018).\nG Quantitative analysis for the corpus\ngrouping\nTo confirm whether the corpus groups we divided\nhave distinct characteristics, we visualized sentence\nembeddings for each group in Figure 1a. Further-\nmore, in this section, we aim to establish the unique\nfeatures of each group through quantitative analy-\nsis. We calculate the mean distances between sen-\ntence embeddings within a corpus group (Table 11)\nand across corpus groups (Table 12). Inter-group\ndistances are generally larger than intra-group dis-\ntances. Thus, sentences within the same group\nhave certain similarities. This result supports our\nclaim that creating training data using diverse cor-\npus groups is a more effective approach. This is\nbecause the distinct characteristics of each group\ncontribute to a more comprehensive and varied\nlearning experience.\nH Effectiveness of the deduplication\nWe compared the results before and after apply-\ning the preprocessing step to the proposed method\n(Section 2.2). Specifically, we compare the model’s\nperformance trained on the entire corpus before pre-\n2110\nNews SEC filings Earnings call Papers MISC\nInter-group\ndistance\nNews — 5.062 3.574 3.542 4.933\nSEC filings 5.062 — 3.832 4.763 3.910\nEarnings call 3.754 3.832 — 2.917 3.518\nPapers 3.542 4.763 2.917 — 4.008\nMISC 4.933 3.910 3.518 4.008 —\nTable 12: Mean distances among sentence embeddings across corpus groups.\nModel # Tokens FPB NER Headline FiNER FinQA FOMC\n(Financial Corpus) Accuracy F-1 F-1 F-1 F-1 Prog Acc Exe Acc F-1\nBefore 3.3B 86.70 83.38 79.16 89.74 81.95 58.50 60.68 67.89\nFiLM[After] 2.4B 86.25 84.48 79.78 91.79 82.02 58.85 61.38 69.60\nTable 13: Compare the performance of before and after preprocessing models.\nprocessing, referred to as the ‘Before’ model, with\nthe performance of our FiLM model trained using\nthe proposed method after preprocessing. Table 13\npresents the results comparison. In addition, we\ncompare the number of duplicate sentences before\nand after preprocessing for each dataset, as shown\nin Table 14.\nName Original\nsentences\nDuplicate\nsentences\nDuplicate\nratio\nSEC filings 13,969,168 5,565,936 39.84\nEarnings call 77,864,753 1,789,634 2.30\nTRC2 7,784,537 1,297,580 16.67\nNYtimes 3,424,448 69,926 2.04\nPapers 1,777,151 8,589 0.48\nEIA 45,496 1,966 4.32\nInvesting.com 220,890 423 0.19\nFinWEB 144,456 390 0.27\nAIHUB 278,054 6 0.00\nInvestopedia 238,344 0 0.00\nTotal 105,747,297 8,734,450 8.26\nTable 14: Comparison of original sentences and dupli-\ncate sentences.\nI Similarity between corpus groups and\ndownstream tasks\nIn this section, we conduct a qualitative analysis\nusing corpus embeddings to measure the similar-\nity between corpus groups and tasks. The Fig-\nure 1a embedding map illustrates that even within\nthe same Financial Domain Dataset, each corpus\nhas distinct characteristics, leading to a wide dis-\ntribution in the embedding space. Furthermore,\nto confirm similar corpus groups for each down-\nstream task in financial domain, we calculate both\nthe vocabulary overlap ratio and distances in the\nembedding space. Figure 3 shows the vocabulary\nratio between the corpus groups and downstream\ntasks. The top 1,000 most frequent unigrams are\nused to calculate the ratio. Figure 4 shows the two\nFigure 3: V ocabulary overlap ratio between pretraining\nand downstream task datasets.\nclosest corpus groups for each downstream task\nin the embedding space. ● markers represent sen-\ntence embeddings in corpus groups, while ✕ mark-\ners indicate sentence embeddings in downstream\ntask. The mean distance of sentence embeddings\nbetween each corpus group and downstream task\nare calculated, and the two nearest groups are se-\nlected based on these distances. We discover that\nthere are slight differences between corpus groups\nidentified as similar in the embedding space and\nthose identified as similar based on the vocabulary\noverlap ratio. For instance, we noted that the News\ncorpus group demonstrated similarity to FPB in vo-\ncabulary overlap, but the SEC-filings corpus group\nshowed similarity to FPB within the embedding\nspace. This indicates that multiple factors should\nbe considered comprehensively when assessing the\nsimilarity between the datasets.\n2111\nFigure 4: Two nearest corpus groups to each downstream dataset in embedding space.\n2112",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6633987426757812
    },
    {
      "name": "Language model",
      "score": 0.5648782849311829
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5499550104141235
    },
    {
      "name": "Generalization",
      "score": 0.5480216145515442
    },
    {
      "name": "Machine learning",
      "score": 0.5122910737991333
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.44129207730293274
    },
    {
      "name": "Finance",
      "score": 0.4249071478843689
    },
    {
      "name": "Natural language processing",
      "score": 0.3728969693183899
    },
    {
      "name": "Economics",
      "score": 0.12494844198226929
    },
    {
      "name": "Mathematics",
      "score": 0.09076890349388123
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    }
  ],
  "cited_by": 3
}