{
    "title": "Optimizing vitiligo diagnosis with ResNet and Swin transformer deep learning models: a study on performance and interpretability",
    "url": "https://openalex.org/W4394989200",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5101191328",
            "name": "Fan Zhong",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5045867877",
            "name": "Kaiqiao He",
            "affiliations": [
                "Air Force Medical University",
                "Xijing Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5021272610",
            "name": "Mengqi Ji",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5000924621",
            "name": "Jianru Chen",
            "affiliations": [
                "Air Force Medical University",
                "Xijing Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5047013876",
            "name": "Tianwen Gao",
            "affiliations": [
                "Air Force Medical University",
                "Xijing Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5101672357",
            "name": "Shuli Li",
            "affiliations": [
                "Air Force Medical University",
                "Xijing Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5100702174",
            "name": "Junpeng Zhang",
            "affiliations": [
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A5100646085",
            "name": "Chunying Li",
            "affiliations": [
                "Air Force Medical University",
                "Xijing Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2733302840",
        "https://openalex.org/W2039775328",
        "https://openalex.org/W2766234204",
        "https://openalex.org/W2072629969",
        "https://openalex.org/W2012945985",
        "https://openalex.org/W1971953839",
        "https://openalex.org/W2013172515",
        "https://openalex.org/W4223918474",
        "https://openalex.org/W1996828958",
        "https://openalex.org/W4292830232",
        "https://openalex.org/W4311959437",
        "https://openalex.org/W3120253959",
        "https://openalex.org/W4313582012",
        "https://openalex.org/W4313562095",
        "https://openalex.org/W2903060508",
        "https://openalex.org/W4310989423",
        "https://openalex.org/W4210634901",
        "https://openalex.org/W2894555402",
        "https://openalex.org/W4313527340",
        "https://openalex.org/W3122512728",
        "https://openalex.org/W3216717225",
        "https://openalex.org/W2607941059",
        "https://openalex.org/W2395501683",
        "https://openalex.org/W4220686584",
        "https://openalex.org/W4206693420",
        "https://openalex.org/W2270754115",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3207810346",
        "https://openalex.org/W4283210562",
        "https://openalex.org/W1559136377",
        "https://openalex.org/W3121143135",
        "https://openalex.org/W3098990209",
        "https://openalex.org/W2786147899",
        "https://openalex.org/W2581082771",
        "https://openalex.org/W4387099612",
        "https://openalex.org/W2806853752",
        "https://openalex.org/W2937742783",
        "https://openalex.org/W4385804141",
        "https://openalex.org/W3046954229",
        "https://openalex.org/W3001669684",
        "https://openalex.org/W4313529702",
        "https://openalex.org/W2511730936",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3196057788",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W4223589960",
        "https://openalex.org/W4296836108",
        "https://openalex.org/W4296622899",
        "https://openalex.org/W4313197623",
        "https://openalex.org/W3125548117",
        "https://openalex.org/W1973855708",
        "https://openalex.org/W4316659316",
        "https://openalex.org/W2963946669"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports\nOptimizing vitiligo diagnosis \nwith ResNet and Swin \ntransformer deep learning \nmodels: a study on performance \nand interpretability\nFan Zhong 1, Kaiqiao He 2, Mengqi Ji 1, Jianru Chen 2, Tianwen Gao 2, Shuli Li 2, \nJunpeng Zhang 1* & Chunying Li 2*\nVitiligo is a hypopigmented skin disease characterized by the loss of melanin. The progressive nature \nand widespread incidence of vitiligo necessitate timely and accurate detection. Usually, a single \ndiagnostic test often falls short of providing definitive confirmation of the condition, necessitating \nthe assessment by dermatologists who specialize in vitiligo. However, the current scarcity of such \nspecialized medical professionals presents a significant challenge. To mitigate this issue and enhance \ndiagnostic accuracy, it is essential to build deep learning models that can support and expedite the \ndetection process. This study endeavors to establish a deep learning framework to enhance the \ndiagnostic accuracy of vitiligo. To this end, a comparative analysis of five models including ResNet \n(ResNet34, ResNet50, and ResNet101 models) and Swin Transformer series (Swin Transformer \nBase, and Swin Transformer Large models), were conducted under the uniform condition to identify \nthe model with superior classification capabilities. Moreover, the study sought to augment the \ninterpretability of these models by selecting one that not only provides accurate diagnostic outcomes \nbut also offers visual cues highlighting the regions pertinent to vitiligo. The empirical findings reveal \nthat the Swin Transformer Large model achieved the best performance in classification, whose AUC, \naccuracy, sensitivity, and specificity are 0.94, 93.82%, 94.02%, and 93.5%, respectively. In terms \nof interpretability, the highlighted regions in the class activation map correspond to the lesion \nregions of the vitiligo images, which shows that it effectively indicates the specific category regions \nassociated with the decision-making of dermatological diagnosis. Additionally, the visualization of \nfeature maps generated in the middle layer of the deep learning model provides insights into the \ninternal mechanisms of the model, which is valuable for improving the interpretability of the model, \ntuning performance, and enhancing clinical applicability. The outcomes of this study underscore \nthe significant potential of deep learning models to revolutionize medical diagnosis by improving \ndiagnostic accuracy and operational efficiency. The research highlights the necessity for ongoing \nexploration in this domain to fully leverage the capabilities of deep learning technologies in medical \ndiagnostics.\nKeywords Vitiligo, Swin transformer, Dermoscopic images, Class activation mapping\nSkin diseases present a substantial healthcare challenge worldwide, with vitiligo standing out as one of the \nprevalent conditions. It is a dermatological condition characterized by the progressive loss of melanocytes, \nresulting in depigmentation of the skin. The progressive nature of vitiligo can profoundly impact patients’ physical \nand psychological well-being1. Consequently, prompt and accurate diagnosis is pivotal for facilitating effective \ntreatment interventions.\nOPEN\n1College of Electrical Engineering, Sichuan University, Chengdu, China. 2Department of Dermatology, Xijing \nHospital, Fourth Military Medical University, Xi’an, China.  *email: junpeng.zhang@gmail.com ; lichying@\nfmmu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nVarious diagnostic methods, including dermoscopy, wood lamp examination, skin CT scans, and skin \nbiopsies, are utilized in the diagnosis of skin conditions. Dermoscopy, in particular, affords comprehensive \ninsights into the status of melanocytes and the distinctive characteristics of vitiligo  patches2,3. By recognizing the \npigment cell loss or reduction and identifying structural changes within areas of depigmentation, it contributes \nto the diagnosis of vitiligo. Apart from this, skin biopsy and histological examination can also be employed to \nevaluate the condition of pigment cells and confirm the presence of vitiligo. Nevertheless, since skin biopsy is \ninvasive, it is not used for routine diagnosis. Typically, the clinical diagnosis of vitiligo relies on a combination \nof physical examination, dermoscopy, and wood lamp examination. There is no single diagnostic test that \nconclusively confirms vitiligo, which requires the involvement of a dermatologist with expertise in vitiligo.\nUnfortunately, there is a shortage of dermatologists and an unequal distribution of medical resources. In \nsome remote areas, even non-dermatologists have to undertake the diagnosis and treatment of vitiligo due to \nmedical resource constraints, despite their limited knowledge and training in this field. Although dermatology \ntextbooks can be used as reference material, accurate identification and diagnosis is still the main challenge for \nthese laypersons. As a result, rates of misdiagnosis and underdiagnosis remain high, and diagnostic accuracy \nranges from 24 to 70%4–7. Therefore, the development of accurate and efficient Artificial Intelligence (AI) -assisted \ndiagnostic tools is crucial for analyzing vitiligo dermoscopy images. The AI-assisted diagnostic tools hold the \npotential to furnish dermatologists with precise classification results, thereby contributing to the accuracy of \nvitiligo diagnosis. This technology also serves to mitigate potential errors stemming from limited expertise, \nespecially in the context of non-dermatologists.\nThe major attention of AI-assisted diagnostic tools is to achieve more accurate classification of medical images. \nAs early as 1959, AI-assisted diagnostic tools have been used in medicine. Initially, some traditional machine \nlearning models are widely used for dermatological classification problems, such as Support Vector Machines \n(SVM)8, K-nearest neighbor (KNN) 9, and Naive  Bayes10. Unfortunately, these machine learning models are \nheavily reliant on the quality of manual feature extraction, which poses challenges in simultaneously achieving \nmore precise classification results and lower system complexity. Furthermore, the utilization of hand-crafted \n features11 in these models significantly hampers both the performance and  generalisability12 of the models when \napplied to dermoscopic images.\nIn contrast to traditional machine learning methods, deep learning has shown superior performance and \nhas attracted more attention. Its effectiveness has been prominently demonstrated in various medical image-\nprocessing  applications13–15. The adoption of automatic feature extraction has made it becoming more and more \npopular in the dermatological image classification  field16–18. As early as 2017, deep learning architectures have \nbeen proposed and utilized in the ISIC 2017 Dermoscopy Image Segmentation Challenge for dermatological \nclassification, segmentation, and detection  tasks19–21. Notably, ResGANet has exhibited outstanding performance \nin medical image classification tasks in comparison to state-of-the-art backbone  models22. Moreover, ResGANet \nhas demonstrated the ability to enhance performance in medical image segmentation tasks by combining it with \nvarious segmentation networks. Therefore, deep learning-based methods can effectively overcome the limitations \nassociated with traditional machine learning methods.\nHowever, it is crucial to acknowledge the notable challenge known as the “black box” problem in deep \nlearning methods. Despite the relative simplicity of the mathematical theory, the output mechanism is difficult \nto understand. In the field of medical image processing, the interpretability of classification results is crucial. \nHowever, the existence of the “black box” problem prevents physicians and researchers from understanding the \nlogic and mechanisms of implementing these  methods23. This lack of interpretability undermines the reliability of \ndeep learning methods, thus limiting their use in clinical practice. To break through this constraint, visualization \ntools and techniques can be used to increase the transparency of the model and enhance interpretability. \nSeveral researchers have explored the application of weakly supervised semantic segmentation. This method \nutilizes image-level labeling information, such as identifying the presence or absence of a lesion, to infer the \nsegmentation results of the lesion region. This method significantly improves the interpretability of medical \nimage classification  results24–26, since the segmented lesion regions correspond to the visual observations of \nthe physician. Generally, this methodology is acknowledged for its capacity to obviate the requirement for \nmanual processing of segmentation masks employed as training labels, resulting in substantial time and effort \n conservation27. Nevertheless, the delineation of ground truth remains imperative during the training of the \nsegmentation network, necessitating manual creation by domain experts.\nThis paper focuses on a AI-assisted diagnostic system based on deep learning methods, using dermoscopic \nimages for the detection and assessment of vitiligo. The objective of the system is to provide a diagnostic outcome \nof vitiligo and a visual diagnostic report that highlights potential areas associated with the disease. For this \npurpose, a set of networks has been trained, and the top five deep learning models with the most favorable \nresults were ultimately selected for comparison. These models belong to the Residual Network (ResNet) and Swin \nTransformer network series. The results reveal that the Swin Transformer, which is a series of image classification \nmodels based on Transformer architecture, attains the highest accuracy in vitiligo classification. This model \neffectively handles global dependencies in images through hierarchical attention mechanisms and cross-stage \nconnectivity mechanisms. Diverging from conventional semantic segmentation techniques prevalent in medical \nimage processing and weakly supervised semantic segmentation methods, the deep learning models used in \nthis paper were exclusively trained by using disease category labels. With the overall training process, the deep \nlearning-based method achieves unsupervised learning for the regions of interest(ROI).\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nMaterials and methods\nA. Materials\nThe study obtained approval from the Ethics Committee at the Fourth Military Medical University, following the \nprinciples outlined in the Declaration of Helsinki. The dermoscopic image dataset utilized for both model train-\ning and testing was sourced from the Department of Dermatology at Xijing Hospital, affiliated with the Fourth \nMilitary Medical University. In accordance with confidentiality regulations and exclusions, no additional clinical \ndata from patients were collected. The dataset consisted of a total of 4320 dermoscopic images, representing eight \ndistinct hypopigmented skin diseases. Among these, 2678 images were specifically associated with vitiligo, while \nthe remaining 1642 images represented seven other pigmented skin diseases distinct from vitiligo. These seven \nhypopigmented dermatoses were identified as pityriasis alba, pityriasis versicolor, marshall white syndrome, \nanemic nevus, idiopathic guttate hypomelanosis, amelanotic nevus, and hypomelanosis of Ito. Considering that \nthe primary objective of this study was to distinguish vitiligo through dermoscopic images, which constitutes a \ndichotomous problem, the other seven pigmented dermatoses were collectively grouped. To enhance the assess-\nment of the model, the dataset was divided into training and test sets in a 7:3 ratio. Within the training dataset, \nthere are 1875 images depicting cases of vitiligo and 1150 images of non-vitiligo cases. As for the test dataset, it \ncomprises 803 images representing vitiligo cases and 492 images non-vitiligo cases. All images were captured \nin the RGB color space and resized to a standardized dimension of 1280*960 pixels for both model training \nand testing. Considering the bias of the dataset on vitiligo conditions, an attention mechanism is introduced in \nthe model to focus more on the features of non-vitiligo images, thus balancing the bias in the training process.\nB. Data preprocessing and data enhancement\nIn order to improve the richness of the data during the subsequent training of the network, and thus improve the \nanti-interference ability and generalization of the model, conventional preprocessing methods such as filtering, \nsegmentation, hair removal, etc., are not applied to the raw data. Considering that the lighting conditions may be \ndifferent at the time of data acquisition, color constancy processing is used in this study to attenuate this effect.\nThe goal of color constancy is to transform the image acquired under an unknown light source so that the \nprocessed image is close to the image acquired under a standard light source. Typically, color constancy process-\ning can be accomplished in two separate steps. First, the estimation of the light source in RGB space is accom -\nplished. The estimated light source is then used to transform the image to minimize the effect of the light source. \nThe Shades of Gray method, which is the most commonly used method for dermatoscopic image processing, \nemploys Minkowski’s paradigm for light source estimation. It uses Minkowski’s paradigm to estimate the light \nsource. The value of P can be changed automatically, and when P = 1, the method degenerates to the GrayWorld \nalgorithm. When P  = ∞, the equation is equivalent to finding the maximum value of f(x), which is equivalent \nto the MaxRGB method. In this study, the value of P is set as 6. The steps of the calculation and the Minkowski \nparadigm used are shown below:\n(1) Substitute the data of each channel into the Minkowski paradigm to find the Min distance of each channel; \n(2) Substitute the data of the whole image into the Minkowski paradigm to find the Min distance of the whole; \n(3) Calculate the ratio of the correction according to the distance of the whole and the distance of each chan-\nnel; (4) Perform the correction of the ratio of each channel, and check whether there is any value exceeding the \nthreshold value, and set it as 255 for the ones exceeding the threshold value of 255. All images are preprocessed \nto replace the original images, and by reading the label file, the images can be mapped to the corresponding \ndermatologic category.\nAfter the preprocessing was completed, to further improve the model performance and results, we used a \ndata augmentation technique in each training iteration to make some minor changes to the data as a preliminary \nstep before batch sampling. This data augmentation strategy employed in this study encompassed four distinct \nmethods, namely random rotation, random brightness, random contrast, and random saturation. Additionally, to \nmaintain the original image dimensions, any empty spaces resulting from the data augmentation procedures were \nfilled with black pixels. Visual illustrations of the input images following the application of the data augmentation \nprocedures are presented in Fig. 1.\n(\n∫\n(f(X )P dX )∫\ndX )\n1\np = ke\nFigure 1.  Three sample images following data augmentation procedures.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nC. Overview of the ResNet networks\nThe ResNet network architecture includes preprocessing layers, and residual units, as well as a fully connected \nlayer and a Softmax layer. The key innovation of ResNet resides in its incorporation of residual connectivity, which \neffectively addresses the challenges of gradient vanishing and exploding encountered during the training of deep \nnetworks, by establishing direct interconnections between layers. In conventional deep networks, the increase \nin the number of layers results in a degradation of the gradient, thereby creating challenges in network training. \nIn order to enhance the network’s interpretability and visual comprehension, a Class activation mapping (CAM) \nmodule is incorporated into the ResNet architecture (as depicted in Fig. 2). The CAM module plays a crucial role \nin comprehending how the network allocates attention to different categories and identifies significant image \nregions. This network architecture facilitates both image classification and the generation of category activation \nmaps simultaneously. This capability allows the network not only to predict the input image’s category but also \nto provide visual interpretations of the classification outcomes. To enhance the analysis and comprehension \nof the feature extraction process within the network, as well as to support research and applications in feature \nvisualization and analysis, the hook technique is utilized in conjunction with ResNet. This technique involves the \nextraction of feature output maps from intermediate layers of the neural network by registering hook functions \nduring the forward propagation process. Through the implementation of the hook technique, the feature outputs \nof the middle layer can be acquired, thus enabling thorough exploration and analysis of the network’s feature \nextraction capabilities.\nD. Overview of the Swin transformer networks\nSwin  Transformer28 has been proposed by Microsoft Research in 2021 as a deep neural network model based \non the Transformer architecture. Its primary objective is to extend the application of the Transformer model \ninto the realm of image processing by incorporating a layered window attention mechanism. In contrast to \ntraditional Transformers, the Swin Transformer employs a Shifted-Window-based Multi-head Self-Attention \n(SW-MSA) module. This module is instrumental in modeling images at various granularities, contributing to the \nmodel’s enhanced performance in capturing diverse features within the image data. This design allows the Swin \nTransformer to effectively process large-sized images while maintaining computational complexity low during \ninference. Furthermore, the Swin Transformer consists of multiple Transformer layers, forming a deep network \nstructure. To improve the model’s local perception, an interaction layer is introduced. This layer facilitates the \ninformation exchange and interaction between different windows.\nFigure 2.  A brief overview of the ResNet framework with the addition of the CAM module.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nIn comparison to the Vision Transformer (VIT), the Swin Transformer introduces a hierarchical structure \nreminiscent of a convolutional neural network (CNN), marking a significant enhancement over VIT. Another \nnotable improvement involves replacing the multi-headed self-attention (MSA) module with a SW-MSA mod-\nule. Summarizing these two improvements as hierarchical feature mapping and SW-MSA. Hierarchical feature \nmapping requires the work of downsampling, which is commonly used in image recognition before pooling \noperations. Instead of traditional pooling, the Swin Transformer employs Patch Merging for downsampling, \nreducing both height (H) and width (W) by half, and channels (C) by four times. These modifications effectively \naddress VIT’s challenges in fine-grained tasks and excessive complexity, respectively. In terms of the skeleton, \nthe structural skeleton of VIT is still used for the design and continues to process input image data using the \nsame patched.\nIn terms of scale, the Swin Transformer provides various model specifications tailored to different tasks and \nresource constraints. The three main types of Swin Transformers are Base, Large, and Tiny, and the two specific \nscales used in this study are Base and Large.\nE. Swin transformer attention module\nThe attention mechanism serves as a computational model designed to identify and assign weights to relevant \nelements within a sequence or set. This process involves computing an attention weight by learning the relation-\nship between a query (Q), a key (K), and a value (V). Subsequently, this weight is applied to the corresponding \nvalue to produce a weighted representation. In practical applications, the Self-Attention mechanism has found \nextensive use. The MSA is an extended version of the self-attentive mechanism, aiming to enhance the repre-\nsentation capability of the model. It applies the attention mechanism to multiple attention heads (i.e., multiple \nQ, K, and V). Each attention head within the MSA mechanism is capable of learning distinct weights, allowing \nit to focus on different information within the input sequence. Finally, the outputs from these multiple attention \nheads are combined or aggregated to produce the final representation vector. MSA is extensively employed in \nTransformer models to capture global dependencies present in the input sequence. And the Swin Transformer \nmainly contains Window-Based Multiple Self-Attention (W-MSA) and SW-MSA.\nThe input features are partitioned into multiple equally sized, non-overlapping windows in W-MSA, each \ntreated as an individual attention head. In this way, the number of windows directly corresponds to the number \nof attention heads. For each window, W-MSA utilizes self-attention to compute dependencies between various \npositions within the window. This involves calculating the similarity between Q and K (usually using dot product \nattention or scaled dot product attention). Consequently, the attention weights for different positions within the \nwindow are determined, and the aggregation of values within the window is weighted using these attentional \nweights. This process produces the feature representation within the window. Its main objective is to tackle the \nproblem of excessive memory usage linked to VIT’s Self-Attention mechanism. The computational complexity \nof the self-attention mechanism is illustrated in Fig.  3 for both MSA and W-MSA. W-MSA effectively reduces \nFigure 3.  Computational complexity of self-attention mechanisms for MSA and W-MSA.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nthe complexity of MSA from O(n2) to O(n) , alleviating the memory footprint constraints associated with VIT. \nNonetheless, W-MSA has its limitations, and confining attention to the window introduces the challenge of \nglobal attention loss. To address this problem, the Swin Transformer integrates the SW-MSA module following \nthe W-MSA module. This window-shifting approach introduces essential inter-window connections, thereby \nimproving the network’s overall performance. The SW-MSA module splits the image into non-overlapping blocks \nand computes the attention of each block with the neighboring blocks, which enables the model to pay more \nattention to the local information of the image. In order to solve the problem of mismatched attention due to \ninteractive movement between windows, a mask matrix is added. For each window, a mask matrix is designed \nseparately, in which the mask matrix is assigned to -100 for the part that should not be computed, so that after \nsubsequent Softmax computation, it will eventually become 0, which is equivalent to playing a filtering role. In \naddition, the SW-MSA module can establish long-distance dependencies between different locations through \nthe multi-head self-attention mechanism, which helps to capture the correlation information between different \nlocations in the image and improves the model’s ability to perceive the global information of the image.\nF. Class activation mapping module\nTo visualize the classification results, a CAM module is incorporated before the final output layer, comprising a \nglobal average pooling (GAP) layer and a dense layer. For instance, as depicted in Fig. 4, the CAM module takes \nthe output feature map of the last residual module as input. It then applies GAP to the feature map, resulting in a \nfixed-length vector. This vector undergoes dot-product computation with the weights in the final fully connected \nlayer, yielding activation values for each category. These activation values can be interpreted as the significance \nof the region associated with a particular category within the input image. By weighting and summing the input \nfeature mapping values of the last layer of the residual unit, the CAM can be obtained. In the calculation, it is \nassumed that fk (x, y) represents the activation of the spatial position coordinate point (x, y) in the last residual \ncell of channel k . With channel k , the result of the GAP that has been performed is denoted as Fk , and Fk is \n1\nH ∗W\n∑\nx,y fk(x,y) . Hence, given the category c , the result of classifier Sc is:\nwhere w c\nk represents the weight of the model for channel k in the final dense layer corresponding to category \nc . It follows that w c\nk is important for the final class judgment, and each position element in CAM is defined in \ncategory c is:\nIn the case where the input image is classified as target class c , the CAM identifies the significance of each \nlocation pixel (x, y) on feature map’s spatial grid. Up-sampling the CAM to match the size of the original input \nimage, the most relevant region to the target category c can be identified.\nG. Intermediate layer feature map output module\nApplying the Hook technique to ResNet enables the extraction of feature output maps from the intermediate \nlayers of the neural network, facilitating a deeper analysis of the network’s feature extraction process. The Hook \nfunction is applied by registering it on a specific layer or module within ResNet. This Hook function is a custom \ncallback function. During the forward propagation of the input image through ResNet, the registered Hook \nfunction is triggered, capturing the output feature maps of the specified layer or module in accordance with \n(1)Sc =\n∑\nk\nw c\nkFk =\n∑\nk\nw c\nk\n1\nH ∗W\n∑\nx,y\nfk(x,y) = 1\nH ∗W\n∑\nx,y\n∑\nk\nw c\nkfk(x,y)\n(2)CAM c(x, y) =\n∑\nk\nw c\nkfk(x, y)\nFigure 4.  Graphical diagram of the CAM module mechanism.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nthe specified instructions. These feature maps can then be employed for subsequent analysis, visualization, or \nfurther processing.\nInformed consent\nInformed consent was obtained from all subjects involved in the study.\nResults\nIn this section, all five proposed deep learning models are evaluated for their performance on a real pathology \nimage set. Our experiments are structured into three parts: first, we conduct training and testing of these five deep \nlearning models on the target dataset to identify the most effective model for vitiligo classification. Second, we \nanalyze the visual interpretation to determine if the internal weighting parameters will provide valuable informa-\ntion for vitiligo diagnosis. Finally, we visualize the output feature maps for the intermediate layers of ResNet, Swin \nTransformer Base, and Swin Transformer Large. This visualization enables us to observe the response regions of \nneurons and the important features during the feature extraction process.\nThe classification performance was evaluated based on accuracy (ACC), sensitivity (SEN), specificity (SPE), \nprecision (PRE), and F1-score with vitiligo considered as the positive example. These metrics were computed \nusing True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) respectively. ACC \nserves as a measure of the overall correctness of predictions, irrespective of whether they pertain to positive \nor negative samples. It reflects the ratio of correct predictions to the total predictions made. SEN represents \nthe proportion of tests that accurately detect true instances of disease, essentially capturing the true positive \nrate. On the other hand, SPE denotes the proportion of tests that accurately identify non-diseased individuals, \nconstituting the true negative rate. PRE represents the proportion of samples with a positive prediction that is \nactually positive. F1-score is a weighted average of PRE and Recall. PRE reflects the model’s ability to discriminate \nbetween negative samples, and the higher the PRE, the better the model’s ability to distinguish between negative \nsamples; Recall reflects the model’s ability to recognize positive samples, the higher the Recall, the better the \nmodel’s ability to recognize positive samples. The F1-score is a combination of the two, with higher F1-scores \nindicating a more robust model.\nIn addition to these quantitative metrics, we generated receiver operating characteristic (ROC) curves for \nthese five models. These curves are employed for a binary categorization task to distinguish between vitiligo and \nnon-vitiligo skin diseases.\nA. Performance of five models\nThe ROC curves of the five models used in this research were analyzed, and the final results of the test set are \npresented in Fig.  5. Notably, in the case of AUC, the Swin Transformer Large model outperforms the other \nmodels, achieving a value of 94%. AUC is a significant performance metric, indicating the reliability of prediction \noutcomes, especially for binary classifiers. Furthermore, to provide a more detailed assessment, a confusion \nmatrix was utilized to quantify and visualize the performance of five models (Fig.  6). The rows and columns of \nthe matrix correspond to the predicted and actual classes, respectively, where 1 represents vitiligo and 0 represents \nother skin diseases that are not vitiligo. It is noteworthy that the Swin Transformer Large model exhibited the \nhighest sensitivity at 94.02%%, and also a commendable specificity at 93.5%. Conversely, the Swin Transformer \nBase model demonstrated a slight reduction in its ability to accurately classify both negative and positive samples, \nwith specificity and sensitivity values of 93.09% and 92.53%, respectively. Among the networks in the ResNet \nseries, ResNet34 emerged as the top performer with a classification accuracy of 89.26%. After assessing the \nperformance of five models, it was evident that the Swin Transformer Large model had the highest accuracy of \n93.82% (as indicated in Table 1). Consequently, among the five models, Swin Transformer Large stands out as \nthe preferred choice for the diagnosis of vitiligo based on dermatoscopic images.\nFurthermore, to ensure easy replication and validation of the research methodology, we analyze the proposed \nmethodology in comparison with some state-of-the-art (SOTA) methods that have performed  well29,30. Consid-\nering that most of the recent advances in the use of dermoscopic images have been aimed at bridging the gap \nbetween clinical and dermoscopic  images31,32.  Both29,30 used datasets of clinical images.\n(3)ACC = TP + TN\nTP + FP+ TN + FN\n(4)SEN = Recall= TP\nTP + FN\n(5)SPE = TN\nTN + FP\n(6)PRE = TP\nTP + FP\n(7)F 1−score = 2 × Pr ecision× Recall\nPr ecision+ Recall\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nThe comparative analysis results, as presented in Table  2, reveal that the model introduced in this paper \nexhibits inferior performance when compared to the method proposed  in29 concerning the public dataset. \nNotably, the observed maximum accuracy difference is approximately 4%. This discrepancy could be attributed \nto variations in dataset characteristics. Specifically, the public dataset predominantly comprises skin images \nfrom non-smooth regions such as arms, whereas the dermoscopic images utilized in our study predominantly \nfeature smoother regions. This dissimilarity in dataset composition emerges as a potential contributing factor \nto the discerned algorithmic differences.\nMoreover, it is noteworthy that the proposed method demonstrates a marginally superior performance on the \npersonal dataset compared to the method  in30. This nuanced improvement could be indicative of the adaptability \nand efficacy of our proposed approach in handling the specific attributes inherent to the personal dataset. Further \nanalysis and exploration of these dataset-specific intricacies are warranted to comprehensively understand the \nobserved performance variations between the proposed method and existing methods. In summation, our mod-\nels obtain good results on several types of datasets, which validate the stability and generalization of the models.\nB. Interpretive visuals generated by the CAM module\nThere are some instances of visual interpretation that are extracted from ResNet’s CAM module (Fig.  7). The \noriginal images corresponding to these results are also included for further analysis. In these visualizations, red \nareas indicate regions where the network is activated, while blue areas signify regions with no activation. The \ndarker red color indicates that the region has a higher contribution value to the model discrimination. Notably, \nthe activation is concentrated in areas associated with skin lesions, from the figure, it can be seen that there are \ntwo bases of discrimination when the model makes judgments: one is based on whether the area of the white \nspots is large and continuous and combined with the edge characteristics of the lesion area, and the other is \nbased on the color difference for differentiation, i.e., the difference in pigmentation between the lesion area and \nthe other normal skin areas. According to the comprehensive analysis conducted by experienced dermatologists, \nthe model captures the two bases and features of clinical diagnosis, i.e. edge and pigmentation, in the judgment. \nThe interpretability of the class activation map cues is in great agreement with the physician’s clinical experience. \nThis suggest that the CAM-generated heat maps are capable of highlighting category-specific areas of interest \nat critical diagnostic points. When analyzing the class activation maps, we observed that activations were not \npresented in all regions associated with vitiligo. The distribution density of activations did not exactly correspond \nto the features of vitiligo. This may due to the fact that the activation layers selected when generating the class \nactivation maps may not provide enough information to accurately reflect the key features of vitiligo. Different \nlevels of activation may highlight different image features. This results in failure to capture features associated \nwith vitiligo at specific levels.\nHowever, despite these limitations, we would like to acknowledge and emphasize the insights provided by \nclass activation maps in terms of visual interpretation. By localizing key regions for classification, we can guide \nvitiligo diagnostic decisions. Although the distribution of activations may not exactly match the reality of the \nlesion, this analysis still provides us with information about which regions of the image the model is focusing \non, thus providing a strong guide to diagnosis.\nC. Visual explanations from feature maps\nBy utilizing Hooks to access feature maps from the neural network’s intermediate layer, the convolutional \nlayers are intercepted to obtain feature representations of the different layers. These feature maps serve multiple \npurposes, including visual interpretation, analysis, and enhancing the interpretability of deep learning models. In \nFigure 5.  ROC curves of five models.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nFig. 8a, the ResNet middle layer feature output image is displayed, with each grid in different layouts representing \na feature map. In Fig.  8b, the feature layer output images of Swin Transformer Large and Swin Transformer \nBase are showcased. Due to the depth and complexity of the models, each convolutional layer extracts features \nat different levels and abstraction. As a result, the feature maps output from multiple intermediate layers can \nsynthetically represent various visual information within the input image. This aids in better understanding the \ndecision-making process and performance of the deep learning model.\nFigure 6.  Confusion matrixes of five models on test set: (a) ResNet34; (b) ResNet50; (c) ResNet101; (d) Swin \nTransformer Base; (e) Swin Transformer Large.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nInstitutional review board statement\nWritten informed consent for the use of identifying images was obtained from all patients. The study was \napproved by the Ethics Committee of the Fourth Military Medical University in accordance to the Declaration \nof Helsinki Principles.\nDiscussion\nRecently, there has been a notable surge in interest surrounding the application of deep learning in medi-\ncal diagnostics. Particularly, deep learning has demonstrated exceptional capabilities in tasks associated with \nimage classification, with its application extending into the field of  dermatology33. Seung et al. proposed a clas-\nsification of clinical images encompassing 12 skin diseases using a deep learning algorithm, achieving a final \naverage classification accuracy of 90% 34. Andre et al. employed CNNs for skin cancer classification, achieving \nresults comparable to the expertise of all evaluated experts and demonstrating a similar level of competence as \n dermatologists35. However, this is only the effect observed in experimental studies. In real-world settings, the \nresults of models compared to experts need to be revisited and  explored36. Furthermore, several studies have \ndemonstrated the remarkable diagnostic and classification abilities of deep learning in tasks related to melanoma \nimage  analysis37–39. Which further promotes the development of deep learning in the field of dermatology.\nUp to now, only a limited number of studies have delved into the application of deep learning in the context \nof vitiligo, and most of them have relied on publicly available  datasets40. For example, Guo, L. et al. developed \nand validated a hybrid artificial intelligence (AI) model utilizing deep learning for the objective measurement \nand color analysis of vitiligo lesions. The accuracy achieved in detecting vitiligo lesions using the YOLO v3 \narchitecture was reported at 85.02%41. Another study proposed an effective intelligent classification system for \nvitiligo, which generated high-resolution vitiligo images under the wood lamp and demonstrated high precision \nin classifying these images, achieving a classification accuracy of 85.69%42. In comparison to other skin diseases, \nthe research on extensively trained vitiligo image datasets and high-precision diagnostic systems is still in its \nearly stages. In this study, an accurate diagnostic system with interpretable vitiligo dermoscopic images was \ndeveloped based on a deep learning model.\nFive deep learning models were selected for comparison in this study, primarily comprising two network \nstructures ResNet and Swin Transformer, along with their variants, ResNet34, ResNet50, ResNet101, Swin \nTable 1.  Performance of five models on test set.\nModels ACC (%) SEN (%) SPE (%) AUC PRE (%) F1-score (%)\nResNet34 89.26 90.04 88.01 0.90 92.46 91.23\nResNet50 88.49 88.54 88.41 0.88 92.58 90.51\nResNet101 87.18 87.92 85.98 0.88 91.10 89.48\nSwin transformer base 92.74 92.53 93.09 0.92 95.62 93.75\nSwin transformer large 93.82 94.02 93.50 0.94 95.93 94.97\nTable 2.  Performance of proposed models and other SOTA methods on different dataset.\nDataset Models ACC SEN SPE PRE F1-score\nPublic dataset collected from seven public dermatology atlas websites: DermNet, DermNet NZ, \nAtlasDerm, DermIS, SD-260, Kaggle, and DanDerm\n(a) Proposed models\nResNet34 84.43% 84.97% 83.60% 85.38% 85.17%\nResNet50 85.32% 85.45% 85.19% 86.67% 86.06%\nResNet101 86.82% 87.79% 85.71% 87.38% 87.58%\nSwin Transformer Base 90.80% 90.61% 91.01% 91.90% 91.25%\nSwin Transformer Large 92.78% 92.96% 92.59% 93.40% 93.18%\n(b)  Reference29\nVGG 96.77% 97.20% 96.30% 96.73% 96.80%\nResNet 95.27% 95.10% 95.60% 96.19% 95.80%\nDenseNet 96.27% 96.20% 96.10% 96.70% 96.20%\nPerson dataset provided by Department of Cosmetic Laser Surgery in the Hospital for Skin Disease \nand Institute of Dermatology, Chinese Academy of Medical Sciences (CAMS) & Peking Union \nMedical College\n(a) Proposed models\nResNet34 81.65% 82.04% 81.00% 87.72% 84.78%\nResNet50 82.40% 82.63% 82.00% 88.46% 85.45%\nResNet101 83.52% 84.43% 82.00% 88.68% 86.96%\nSwin Transformer Base 86.89% 87.43% 86.00% 91.82% 89.57%\nSwin Transformer Large 87.64% 88.02% 87.00% 91.88% 89.91%\n(b)  Reference30\nYOLO V3 85.02% 92.91% 72.00% 84.70% 88.62%\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nTransformer Large, and Swin Transformer Base. It is noteworthy that Swin Transformer is a relatively new \nnetwork. In previous studies, ResNets have been used widely for dermatological image segmentation and \nclassification and have demonstrated excellent  performance43–45. Swin Transformer is a deep learning model based \non Transformer architecture. It has found extensive applications in the field of medical image processing and has \nachieved remarkable results in computer vision tasks. Consequently, we chose to investigate the performance \nof these two widely used and innovative deep learning networks in the specific context of vitiligo diagnosis. The \nexperimental results indicate that among the ResNets, ResNet34 performs slightly better than ResNet50, while \nResNet101 exhibits the least favorable results. Generally, with an increase in network depth, the performance \nof ResNet gradually improves, as deeper structures tend to capture finer details and features in images more \neffectively. However, it’s noteworthy that on specific datasets or tasks, ResNet34 may outperform  ResNet5046, and \nResNet34 with fewer parameters could also be more robust and better generalized, particularly on smaller datasets \nwhere the data size is limited. It’s important to acknowledge that performance comparisons are influenced by \nvarious factors, and different studies may reach slightly different  conclusions47. Swin Transformer, proposed as a \nnovel model for computer vision in 2021, has demonstrated wide applicability in various tasks, including image \nsegmentation, restoration, and  reconstruction48–50. Swin Transformer has been used in medical image processing \nthrough its hierarchical structure and self-attention mechanism, which demonstrates a robust capability for \nfeature extraction and modeling. This presents promising opportunities for innovation in medical image analysis \nand diagnosis. Currently, there have been limited studies reporting the utilization of Swin Transformer in the \ncontext of medical  images51–53. As far as we are aware, our study stands out as one of the relatively few instances \nFigure 7.  Examples of visual interpretation.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nwhere the Swin Transformer has been applied to the analysis of dermoscopic images. As the optimal model in \nour task, Swin Transformer achieved an accuracy of 93.82% and 92.74% for both specifications, respectively. This \nindicates the potential of Swin Transformer in medical applications, warranting further research and exploration \nin the field of medicine.\nFigure 8.  Examples of feature maps for different channels extracted from: (a) ResNet; (b) Swin Transformer.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nConclusion\nVitiligo is a common hypopigmentation disease, and its final diagnosis usually requires a combination of \nprofessional doctors’ experience and test results from specialized instruments. With the help of computer \nvision and deep learning technology, it can provide an auxiliary means to help doctors diagnose vitiligo more \naccurately. The objective of this study is to evaluate the performance of multiple deep learning models using \nvitiligo image samples and subsequently identify five models with optimal diagnostic performance, including \nResNet34, ResNet50, ResNet101, Swin Transformer Base, and Swin Transformer Large. These models were then \nutilized to develop a vitiligo diagnostic system that not only provides a disease label but also generates a visual \ndiagnostic report displaying the possible regions associated with the disease. The outcomes produced by the CAM \nmodule effectively emphasize specific areas relevant to each class within diagnostic points, thereby assisting in \ndecision-making during the diagnosis of skin conditions. Additionally, the use of feature output maps from the \nmiddle layer of the neural network enhances the understanding of how the model processes input images for \ntasks such as classification or localization. This integrated visual and informational output helps to improve the \ninterpretability of the system, providing physicians with more comprehensive and in-depth insights that enhance \nconfidence and decision-making during the diagnostic process.\nThe results of this study demonstrate that deep learning techniques have achieved significant accuracy gains \nin vitiligo diagnosis and provide comprehensive visual and informative outputs for diagnostic results. This not \nonly emphasizes the excellence of deep learning models in vitiligo diagnosis but also suggests the potential value \nof their application in a broader diagnostic setting covering a wide range of dermatological conditions. This \nfinding highlights the great potential of deep learning techniques in the field of dermatological imaging and \nalso emphasizes the urgency of delving deeper into this area in the future. These encouraging results not only \nprovide more reliable diagnostic support for patients with vitiligo but also lay a solid foundation for advancing \nthe diffusion of deep learning techniques in real-world dermatologic diagnostic applications.\nData availability\nThe data presented in this study are available on request from the corresponding author. The data are not publicly \navailable due to privacy or ethical concerns.\nReceived: 18 September 2023; Accepted: 10 April 2024\nReferences\n 1. Boniface, K., Seneschal, J., Picardo, M. & Taïeb, A. Vitiligo: focus on clinical aspects, immunopathogenesis, and therapy. Clin. Rev. \nAllergy Immunol. 54, 52–67. https:// doi. org/ 10. 1007/ s12016- 017- 8622-7 (2018).\n 2. Thatte, S. S. & Khopkar, U. S. The utility of dermoscopy in the diagnosis of evolving lesions of vitiligo. Indian J. Dermatol. Venereol. \nLeprol. 80, 505–508. https:// doi. org/ 10. 4103/ 0378- 6323. 144144 (2014).\n 3. Kumar Jha, A., Sonthalia, S., Lallas, A. & Chaudhary, R. K. P . Dermoscopy in vitiligo: Diagnosis and beyond. Int. J. Dermatol. 57, \n50–54. https:// doi. org/ 10. 1111/ ijd. 13795 (2018).\n 4. Federman, D. G., Concato, J. & Kirsner, R. S. Comparison of dermatologic diagnoses by primary care practitioners and \ndermatologists. A review of the literature. Arch. Fam. Med. 8, 170–172. https:// doi. org/ 10. 1001/ archf ami.8. 2. 170 (1999).\n 5. Moreno, G., Tran, H., Chia, A. L. K., Lim, A. & Shumack, S. Prospective study to assess general practitioners’ dermatological \ndiagnostic skills in a referral setting. Australas. J. Dermatol. 48, 77–82. https:// doi. org/ 10. 1111/j. 1440- 0960. 2007. 00340.x (2007).\n 6. Tran, H., Chen, K., Lim, A. C., Jabbour, J. & Shumack, S. Assessing diagnostic skill in dermatology: A comparison between general \npractitioners and dermatologists. Australas. J. Dermatol. 46, 230–234. https:// doi. org/ 10. 1111/j. 1440- 0960. 2005. 00189.x (2005).\n 7. Federman, D. G. & Kirsner, R. S. The abilities of primary care physicians in dermatology: Implications for quality of care. Am. J. \nManag. Care 3, 1487–1492 (1997).\n 8. Wu, W .-J., Lin, S.-W . & Moon, W . K. An artificial immune system-based support vector machine approach for classifying ultrasound \nbreast tumor images. J. Digit Imaging 28, 576–585. https:// doi. org/ 10. 1007/ s10278- 014- 9757-1 (2015).\n 9. Uddin, S., Haque, I., Lu, H., Moni, M. A. & Gide, E. Comparative performance analysis of K-nearest neighbour (KNN) algorithm \nand its different variants for disease prediction. Sci. Rep. 12, 6256. https:// doi. org/ 10. 1038/ s41598- 022- 10358-x (2022).\n 10. Fesharaki, N.J., Pourghassem, H., 2012. Medical X-ray Images Classification Based on Shape Features and Bayesian Rule, in: \n2012 Fourth International Conference on Computational Intelligence and Communication Networks . Presented at the 2012 4th \nInternational Conference on Computational Intelligence and Communication Networks (CICN), IEEE, Mathura, Uttar Pradesh, \nIndia, pp. 369–373. https:// doi. org/ 10. 1109/ CICN. 2012. 145.\n 11. Celebi, M. E. et al.  A methodological approach to the classification of dermoscopy images. Computerized Med. Imaging Gr.  31, \n362–373. https:// doi. org/ 10. 1016/j. compm edimag. 2007. 01. 003 (2007).\n 12. Jeong, H. K., Park, C., Henao, R. & Kheterpal, M. Deep learning in dermatology: A systematic review of current approaches, \noutcomes, and limitations. JID Innovations 3, 100150. https:// doi. org/ 10. 1016/j. xjidi. 2022. 100150 (2023).\n 13. Alwakid, G., Gouda, W ., Humayun, M. & Sama, N. U. Melanoma detection using deep learning-based classifications. Healthcare \n10, 2481. https:// doi. org/ 10. 3390/ healt hcare 10122 481 (2022).\n 14. Jalali, Y ., Fateh, M., Rezvani, M., Abolghasemi, V . & Anisi, M. H. ResBCDU-Net: A deep learning framework for lung CT image \nsegmentation. Sensors 21, 268. https:// doi. org/ 10. 3390/ s2101 0268 (2021).\n 15. Hasan, Md. M., Islam, M. U., Sadeq, M. J., Fung, W .-K. & Uddin, J. Review on the evaluation and development of artificial \nintelligence for COVID-19 containment. Sensors 23, 527. https:// doi. org/ 10. 3390/ s2301 0527 (2023).\n 16. Li, Y . et al. Deep learning radiomic analysis of DCE-MRI combined with clinical characteristics predicts pathological complete \nresponse to neoadjuvant chemotherapy in breast cancer. Front. Oncol.  12, 1041142. https:// doi. org/ 10. 3389/ fonc. 2022. 10411 42 \n(2023).\n 17. Tschandl, P . et al. Expert-level diagnosis of nonpigmented skin cancer by combined convolutional neural networks. JAMA Dermatol \n155, 58. https:// doi. org/ 10. 1001/ jamad ermat ol. 2018. 4378 (2019).\n 18. Ravi, V . Attention cost-sensitive deep learning-based approach for skin cancer detection and classification. Cancers 14, 5872. \nhttps:// doi. org/ 10. 3390/ cance rs142 35872 (2022).\n 19. Kaur, R., GholamHosseini, H., Sinha, R. & Lindén, M. Melanoma classification using a novel deep convolutional neural network \nwith dermoscopic images. Sensors 22, 1134. https:// doi. org/ 10. 3390/ s2203 1134 (2022).\n 20. Tushar, F . I. Automatic skin lesion segmentation using grabcut in hsv colour space. arXiv preprint. 29.  https:// doi. org/ 10. 48550/ \narXiv. 1810. 00871 (2018).\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\n 21. Ruan, J., Xiang, S., Xie, M., Liu, T., Fu, Y . MALUNet: A Multi-Attention and Light-weight UNet for Skin Lesion Segmentation. 2022 \nIEEE International Conference on Bioinformatics and Biomedicine, 1150–1156. https:// doi. org/ 10. 1109/ BIBM5 5620. 2022. 99950 40 \n(2022).\n 22. Zhao, Z., Zeng, Z., Xu, K., Chen, C. & Guan, C. DSAL: Deeply supervised active learning from strong and weak labelers for \nbiomedical image segmentation. IEEE J. Biomed. Health Inform. 25, 3744–3751. https:// doi. org/ 10. 1109/ JBHI. 2021. 30523 20 (2021).\n 23. Cheng, J. et al. ResGANet: Residual group attention network for medical image classification and segmentation. Med. Image Anal. \n76, 102313. https:// doi. org/ 10. 1016/j. media. 2021. 102313 (2022).\n 24. Razzak, M. I., Naz, S. & Zaib, A. Deep learning for medical image processing: Overview, challenges and the future. Classification \nBioApps: Autom. Decis. Mak. https:// doi. org/ 10. 1007/ 978-3- 319- 65981-7_ 12 (2018).\n 25. Hassan, H. et al. Supervised and weakly supervised deep learning models for COVID-19 CT diagnosis: A systematic review. \nComput. Method. Progr. Biomed. 218, 106731. https:// doi. org/ 10. 1016/j. cmpb. 2022. 106731 (2022).\n 26. Wang, R. et al. Medical Image segmentation using deep learning: A survey. IET Image Process. 16, 1243–1267. https:// doi. org/ 10. \n1049/ ipr2. 12419 (2022).\n 27. Kumar, A. et al. Adapting content-based image retrieval techniques for the semantic annotation of medical images. Comput. Med. \nImaging Graph. 49, 37–45. https:// doi. org/ 10. 1016/j. compm edimag. 2016. 01. 001 (2016).\n 28. Liu Z, Lin Y , Cao Y , et al. Swin transformer: Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision. 2021: 10012–10022.\n 29. Zhang, L. et al. Design and assessment of convolutional neural network based methods for vitiligo diagnosis. Front. Med. 8, 754202 \n(2021).\n 30. Guo, L. et al. A deep learning-based hybrid artificial intelligence model for the detection and severity assessment of vitiligo lesions. \nAnn. Transl. Med. https:// doi. org/ 10. 21037/ atm- 22- 1738 (2022).\n 31. Kroemer, S. et al. Mobile teledermatology for skin tumour screening: Diagnostic accuracy of clinical and dermoscopic image \ntele-evaluation using cellular phones. Br. J. Dermatol. 164(5), 973–979 (2011).\n 32. Reiter, O. et al. The differences in clinical and dermoscopic features between in situ and invasive nevus-associated melanomas and \nde novo melanomas. J. Eur. Acad. Dermatol. Venereol. 35(5), 1111–1118 (2021).\n 33. Li, L.-F . et al. Deep learning in skin disease image recognition: A review. IEEE Access 8, 208264–208280. https:// doi. org/ 10. 1109/ \nACCESS. 2020. 30372 58 (2020).\n 34. Han, S. S. et al. Classification of the clinical images for benign and malignant cutaneous tumors using a deep learning algorithm. \nJ. Investigative Dermatol. 138, 1529–1538. https:// doi. org/ 10. 1016/j. jid. 2018. 01. 028 (2018).\n 35. Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115–118. https:// doi. org/ \n10. 1038/ natur e21056 (2017).\n 36. Menzies, S. W . et al. Comparison of humans versus mobile phone-powered artificial intelligence for the diagnosis and management \nof pigmented skin cancer in secondary care: a multicentre, prospective, diagnostic, clinical trial. The Lancet Digital Health, 5(10), \ne679–e691. https:// doi. org/ 10. 1016/ S2589- 7500(23) 00130-9 (2023).\n 37. Haenssle, H. A. et al.  Man against machine: Diagnostic performance of a deep learning convolutional neural network for \ndermoscopic melanoma recognition in comparison to 58 dermatologists. Ann. Oncol. 29, 1836–1842. https:// doi. org/ 10. 1093/ \nannonc/ mdy166 (2018).\n 38. Codella, N.C.F . et al., 2018. Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium \non biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC), in: 2018 IEEE 15th International \nSymposium on Biomedical Imaging (ISBI 2018). Presented at the 2018 IEEE 15th International Symposium on Biomedical Imaging \n(ISBI 2018), IEEE, Washington, DC, pp. 168–172. https:// doi. org/ 10. 1109/ ISBI. 2018. 83635 47.\n 39. Brinker, T. J. et al. Deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image \nclassification task. Eur. J. Cancer 113, 47–54. https:// doi. org/ 10. 1016/j. ejca. 2019. 04. 001 (2019).\n 40. Hillmer, D. et al. Evaluation of facial vitiligo severity with a mixed clinical and artificial intelligence approach. J. Investigative \nDermatol. 144(2), 351–357 (2024).\n 41. Guo, L. et al. A deep learning-based hybrid artificial intelligence model for the detection and severity assessment of vitiligo lesions. \nAnn. Transl. Med. 10, 590. https:// doi. org/ 10. 21037/ atm- 22- 1738 (2022).\n 42. Luo, W ., Liu, J., Huang, Y . & Zhao, N. An effective vitiligo intelligent classification system. J. Ambient. Intell. Human Comput. 14, \n5479–5488. https:// doi. org/ 10. 1007/ s12652- 020- 02357-5 (2023).\n 43. S, K., Inbarani, H.H., 2022. Ensemble Pre-Trained Deep Convolutional Neural Network Model for Classifying Medical Image \nDatasets, in: 2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS). Presented at the 2022 \nInternational Conference on Augmented Intelligence and Sustainable Systems (ICAISS), IEEE, Trichy, India, pp. 121–128. https:// \ndoi. org/ 10. 1109/ ICAIS S55157. 2022. 10011 089.\n 44. Al-masni, M. A., Kim, D.-H. & Kim, T.-S. Multiple skin lesions diagnostics via integrated deep convolutional networks for \nsegmentation and classification. Comput. Method. Progr. Biomed. 190, 105351. https:// doi. org/ 10. 1016/j. cmpb. 2020. 105351 (2020).\n 45. Mayall, F . G. et al. Artificial intelligence-based triage of large bowel biopsies can improve workflow. J. Pathol. Inform. 14, 100181. \nhttps:// doi. org/ 10. 1016/j. jpi. 2022. 100181 (2023).\n 46. Huang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. Densely connected convolutional networks. IEEE conference on \ncomputer vision and pattern recognition, 4700–4708. https:// doi. org/ 10. 48550/ ARXIV . 1608. 06993 (2023).\n 47. PMLR.Tan, M., Le, Q.V . EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. International conference on \nmachine learning, 6105–6114. https:// doi. org/ 10. 48550/ ARXIV . 1905. 11946 (2019).\n 48. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R. SwinIR: Image Restoration Using Swin Transformer. IEEE/CVF \ninternational conference on computer vision. 1833–1844. https:// doi. org/ 10. 48550/ ARXIV . 2108. 10257 (2021).\n 49. Cao, H. et al.  Swin-Unet: Unet-Like Pure Transformer for Medical Image Segmentation. In: Computer Vision – ECCV 2022 \nWorkshops. ECCV 2022. Lecture Notes in Computer Science, (eds Karlinsky, L., Michaeli, T. & Nishino, K.) 13803, 205–218. https:// \ndoi. org/ 10. 1007/ 978-3- 031- 25066-8_9 (2022).\n 50. Huang, J. et al. Swin transformer for fast MRI. Neurocomputing.  493, 281–304. https:// doi. org/ 10. 1016/j. neucom. 2022. 04. 051 \n(2022).\n 51. Peng, L. et al. Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin \ntransformer, and RegNet. Front. Microbiol. 13, 995323. https:// doi. org/ 10. 3389/ fmicb. 2022. 995323 (2022).\n 52. Chi, J. et al. CT image super-resolution reconstruction based on global hybrid attention. Comput. Biol. Med. 150, 106112. https:// \ndoi. org/ 10. 1016/j. compb iomed. 2022. 106112 (2022).\n 53. Liu, L. et al. An intelligent diagnostic model for melasma based on deep learning and multimode image input. Dermatol. Ther. 13, \n569–579. https:// doi. org/ 10. 1007/ s13555- 022- 00874-z (2023).\nAuthor contributions\nConceptualization, C.-Y .L. and J.-P .Z.; methodology, F .Z., J.-P .Z. and K.-Q.H.; software, M.-Q.J.; validation, C.-\nY .L., J.-P .Z. and K.-Q.H.; formal analysis, F .Z. and J.-R.C.; investigation, T.-W .G. and F .Z.; resources, C.-Y .L. and \nJ.-P .Z.; data curation, S.-L.L. and J.-R.C.; writing—original draft preparation, F .Z.; writing—review and editing, \n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:9127  | https://doi.org/10.1038/s41598-024-59436-2\nwww.nature.com/scientificreports/\nC.-Y .L. and J.-P .Z.; visualization, M.-Q.J. and T.-W .G.; supervision, J.-P .Z.; project administration, C.-Y .L. and \nJ.-P .Z.; funding acquisition, C.-Y .L. All authors have read and agreed to the published version of the manuscript.\nFunding\nThis research was supported by the National Natural Science Foundation of China’s Mathematics Tianyuan \nFoundation (12126606) and the R&D project of Pazhou Lab (Huangpu) (2023K0605).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.Z. or C.L.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}