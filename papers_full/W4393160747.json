{
  "title": "ExpeL: LLM Agents Are Experiential Learners",
  "url": "https://openalex.org/W4393160747",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2312012778",
      "name": "Andrew Zhao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2186372574",
      "name": "Daniel Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5102679539",
      "name": "Quentin Xu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3112307123",
      "name": "Matthieu Lin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2639424285",
      "name": "Yong-Jin Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2114281204",
      "name": "Gao Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2312012778",
      "name": "Andrew Zhao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2186372574",
      "name": "Daniel Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5102679539",
      "name": "Quentin Xu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3112307123",
      "name": "Matthieu Lin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2639424285",
      "name": "Yong-Jin Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2114281204",
      "name": "Gao Huang",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4385373842",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4282938007",
    "https://openalex.org/W6734897383",
    "https://openalex.org/W6743716967",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4378770584",
    "https://openalex.org/W4360819401",
    "https://openalex.org/W2141559645",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4378465306",
    "https://openalex.org/W4363671832",
    "https://openalex.org/W6810889274",
    "https://openalex.org/W4379255871",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W3092516542",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4378715768",
    "https://openalex.org/W2789566302",
    "https://openalex.org/W4383605209",
    "https://openalex.org/W6600001191",
    "https://openalex.org/W6601295022",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4283828996",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4385644214",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W6770432743"
  ],
  "abstract": "The recent surge in research interest in applying large language models (LLMs) to decision-making tasks has flourished by leveraging the extensive world knowledge embedded in LLMs. While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities. Moreover, state-of-the-art language models like GPT-4 and Claude are primarily accessible through API calls, with their parametric weights remaining proprietary and unavailable to the public. This scenario emphasizes the growing need for new methodologies that allow learning from agent experiences without requiring parametric updates. To address these problems, we introduce the Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences and extracts knowledge using natural language from a collection of training tasks. At inference, the agent recalls its extracted insights and past experiences to make informed decisions. Our empirical results highlight the robust learning efficacy of the ExpeL agent, indicating a consistent enhancement in its performance as it accumulates experiences. We further explore the emerging capabilities and transfer learning potential of the ExpeL agent through qualitative observations and additional experiments.",
  "full_text": "ExpeL: LLM Agents Are Experiential Learners\nAndrew Zhao1, Daniel Huang2, Quentin Xu2, Matthieu Lin2, Yong-Jin Liu2, Gao Huang1*\n1 Department of Automation, BNRist, Tsinghua University\n2 Department of Computer Science, BNRist, Tsinghua University\n{zqc21,huang-jy22,xgd22,lyh21}@mails.tsinghua.edu.cn,\n{liuyongjin,gaohuang}@tsinghua.edu.cn\nAbstract\nThe recent surge in research interest in applying large lan-\nguage models (LLMs) to decision-making tasks has flour-\nished by leveraging the extensive world knowledge embed-\nded in LLMs. While there is a growing demand to tailor\nLLMs for custom decision-making tasks, finetuning them\nfor specific tasks is resource-intensive and may diminish the\nmodel’s generalization capabilities. Moreover, state-of-the-\nart language models like GPT-4 and Claude are primarily ac-\ncessible through API calls, with their parametric weights re-\nmaining proprietary and unavailable to the public. This sce-\nnario emphasizes the growing need for new methodologies\nthat allow learning from agent experiences without requir-\ning parametric updates. To address these problems, we intro-\nduce the Experiential Learning (ExpeL) agent. Our agent au-\ntonomously gathers experiences and extracts knowledge us-\ning natural language from a collection of training tasks. At\ninference, the agent recalls its extracted insights and past ex-\nperiences to make informed decisions. Our empirical results\nhighlight the robust learning efficacy of the ExpeL agent, in-\ndicating a consistent enhancement in its performance as it ac-\ncumulates experiences. We further explore the emerging ca-\npabilities and transfer learning potential of the ExpeL agent\nthrough qualitative observations and additional experiments.\n1 Introduction\nA computer program is said to learn from experience\nE with respect to some class of tasks T and\nperformance measure P, if its performance at tasks in\nT, as measured by P, improves with experience E.\nTom Mitchell\nMachine learning research has long been captivated by\nthe potential of autonomous agents and their capabilities.\nIn recent times, incorporating large language models into\nthese agents (Wang et al. 2023a; Xi et al. 2023) has unveiled\na broad spectrum of applications, even extending beyond\nacademia (Yang et al. 2023a; Significant-Gravitas 2023).\nOne of the significant advantages of LLMs lies in their world\nknowledge, allowing them to be inherently versatile across\nvarious scenarios (Zhao et al. 2023b).\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nOn the one hand, previous works investigated finetun-\ning LLMs with a large number of environment interactions\n(Yao et al. 2023c) or with a large amount of human-labeled\ndatasets (Nakano et al. 2021; Shaw et al. 2023). This class\nof methods incurs high computational costs and needs ac-\ncess to the LLM’s parametric weights. Furthermore, fine-\ntuning an LLM restricts its functionalities and can hurt its\ngeneralization abilities (Du et al. 2022). On the other hand,\nprompting methods can augment an LLM with better se-\nquential decision-making planning abilities with only a few\nin-context examples (Hao et al. 2023; Lin et al. 2023b; Sun\net al. 2023). However, since current LLMs are bounded by\ncontext window size (Tworkowski et al. 2023), these agents\nhave no recollections of what they have seen, and therefore\nno learning can be done outside of a few demonstrations.So,\nhow can we strike a balance between these paradigms?\nWe present the Experiential Learning (ExpeL) agent as a\nsolution. Our agent autonomously gathers experiences from\na collection of training tasks through trial and error. From\nthese experiences, it derives natural language insights and\nemploys its own successful experiences as in-context exam-\nples during test time. Our agent’s learning process is analo-\ngous to a student studying for an exam and then taking it on\na single attempt, reflecting many real-world situations. Un-\nlike self-improvement methods like Reflexion (Shinn et al.\n2023), our approach emphasizes the importance of retaining\nexperiences across multiple tasks to enhance agent perfor-\nmance. Moreover, ExpeL learns without parameter updates,\nmaking it compatible with powerful closed-source models\nlike GPT-4 or Claude. Lastly, the experience-gathering step\ndoes not require a large amount of data or human labels.\nWe evaluated ExpeL on three vastly different domains and\nconsistently outperformed strong baselines. Additionally,\nwe showcased a transfer learning scenario where our agent\nthat accumulated knowledge from source tasks showed pos-\nitive forward transfer to target tasks. Finally, we highlighted\nsome unexpected emerged abilities the ExpeL agent gained.\nIn summary, our key contributions are as follows: (1) we\nintroduced ExpeL, a novel LLM agent that autonomously\nlearns from experience without gradient updates; (2) We\nevaluated ExpeL on a diverse set of tasks to showcase its\nlearning abilities and improvement on top of existing plan-\nning methods; (3) we showed a novel setting of transfer\nlearning for our LLM agent and demonstrated forward trans-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19632\nFigure 1: ExpeL Agent Overview. Left: ExpeL operates in three stages: (1) Collection of success and failure experiences into\na pool. (2) Extraction/abstraction of cross-task knowledge from these experiences. (3) Application of the gained insights and\nrecall of past successes in evaluation tasks. Right: (A) Illustrates the experience gathering process via Reflexion (Shinn et al.\n2023), enabling task reattempt after self-reflection on failures. (B) Illustrates the insight extraction step. When presented with\nsuccess/failure pairs or a list of L successes, the agent dynamically modifies an existing list of insights ˆι using operations ADD,\nUPVOTE, DOWNVOTE, and EDIT. This process has an emphasis on extracting prevalent failure patterns or best practices.\nferability from source tasks to target tasks. Lastly, we be-\nlieve that as planning algorithms and foundational models\ncontinue to improve, ExpeL’s paradigm stands to gain sig-\nnificant benefits from their enhanced performances.1\n2 Related Work\nPrompt-based Learning: Prompt-based learning refines la-\nbel prediction tasks by modifying the input context, facili-\ntating swift adaptation to new tasks with minimal data (Liu\net al. 2023a). This approach capitalizes on LLMs for an-\nswers without parameter tuning as they can be augmented\nusing in-context learning (Brown et al. 2020). LAMA\n(Petroni et al. 2019) and GPT-3 (Brown et al. 2020) are\nearly works that promoted this formulation. Efforts to reduce\nthe intricacies of prompt design include automatic reason-\ning chains for NLP (Kojima et al. 2022; Zhang et al. 2023).\nSimilarly, the ExpeL agent also autonomously learns from\nexperiences using extracted insights and self-generated in-\ncontext trajectories by altering the execution prompt.\nRetrieval Augmented Generation (RAG): Retrieval al-\nlows LLMs to access databases, mitigating hallucinations\n(Li et al. 2022; Wang, Yang, and Wei 2023; Rubin, Herzig,\nand Berant 2022; Liu et al. 2022). Retrieval has also been\nused to enhance the capabilities of decision-making agents\n(Humphreys et al. 2022; Zhao et al. 2023a). In contrast\n1Visit https://andrewzh112.github.io/#expel for prompts and\ndemos, and https://github.com/LeapLabTHU/ExpeL for code.\nto these works, we focus on retrieving the ExpeL agent’s\nself-generated experiences, thus reducing the dependency on\ngold examples and leveraging domain-specific corpus.\nPlanning for LLM Agents: Application of LLM agents\nin fields like robotics, natural sciences, game-playing, and\nworkflows has surged, with emphasis on their world knowl-\nedge in fewshot settings (Ha, Florence, and Song 2023;\nMu et al. 2023; Bran et al. 2023; Boiko, MacKnight, and\nGomes 2023; Yang et al. 2023b; Lin et al. 2023a; Nakano\net al. 2021; Wang et al. 2023b; Liu et al. 2023b). Moreover,\nLLMs have demonstrated promising zero/few-shot planning\nand reasoning capabilities in various configurations (Sumers\net al. 2023), including embodied environments and reason-\ning tasks (Huang et al. 2022; Yao et al. 2023a; Wei et al.\n2022b; Yao et al. 2023b; Gong et al. 2023).\nSelf-improvement and Memory for LLM Agents:Agents\nlike Reflexion showcase feedback-based improvement, yet\noften lack cross-task memory (Shinn et al. 2023). Other\nagents exhibit potential in persistent memory within multi-\nagent contexts (Park et al. 2023; Maas et al. 2023). Our\nExpeL agent combines these approaches, focusing on task-\nsolving while benefiting from self-generated in-context ex-\namples and abstracted insights from memory.\n3 Preliminaries\nComplex Interactive Tasks We work with complex inter-\nactive tasks where at each time step i ∈ {0, . . . , H}, the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19633\nagent receives an observation o ∈ O, and from its observa-\ntion history Ht decides to perform action a ∈ A. The ob-\njective of the agent is to achieve some goal g ∈ G. We only\ndeal with deterministic environments in this work.\nLarge Language Models A large language model is a\nstatistical model of the natural language, typically a neu-\nral network. In our setting, we use an autoregressive lan-\nguage model (Brown et al. 2020; Touvron et al. 2023b,a;\nChowdhery et al. 2023), which given an ordered list of ex-\nisting tokens x = {x1, x2, ..., xl−1}, outputs the probabil-\nity of the next token p(xl | x<l). An instruction-following\nLLM (Thoppilan et al. 2022; Chung et al. 2022; Wei et al.\n2022a) is typically finetuned on various NLP tasks that are\nformatted into instruction, input, response tuples (Taori et al.\n2023). Instruction-tuned models are better at following natu-\nral language instructions which alleviates the need for heavy\nprompt engineering (Wei et al. 2022a).\nReAct and Reflexion ReAct (Yao et al. 2023b) and Re-\nflexion (Shinn et al. 2023) are promising frameworks en-\nabling the aforementioned proficiency of LLMs in reason-\ning and self-improvement. ReAct explicitly intertwines ob-\nservations, actions, and thoughts, providing a foundation for\nrobust planning and reasoning capabilities. Building upon\nit, Reflexion introduces an additional reflective step before\nreattempting the subsequent trial of the same task, enhanc-\ning the model’s adaptive learning process.\n4 ExpeL: An Experiential Learning Agent\nRecent advancements in generative LLMs suggest an in-\ntriguing approach. Rather than altering the LLM param-\neters, adjusting the prompts may be more beneficial: this\nstrategy ensures that the LLM’s inherent common sense\nknowledge remains intact, allowing for superior general-\nization (Liu et al. 2023a). Furthermore, some of the most\npotent language models are proprietary. Thus, focusing on\nprompt-based methods seems promising as a way to har-\nness the strengths of these advanced LLMs. Additionally,\nprevious works on learning in LLM agents have primarily\nbeen trained on extensive human-labeled datasets (Lin et al.\n2023a; Shaw et al. 2023) or improved via iterative retries\n(Shinn et al. 2023) on a single task. A relatively less ex-\nplored area is facilitating agents to learn autonomously from\ntheir own experiences, similar to a student gaining insights\nfrom practicing for an exam. The student tackles practice\nproblems multiple times to derive insights. At the exam, the\nstudent rely solely on these insights and draw memories of\nsimilar problems to answer the questions with one attempt.\nWith this in mind, we wish to design an LLM agent that au-\ntonomously gathers experiences and extracts insights, then\nuses these cross-task insights and memories of similar tasks\nto aid its decision-making.\nWe aim to enhance a planning LLM agent, such as Re-\nAct, with learning abilities that allow it to improve through\ninter-task experiences without any parameter updates. In-\nspired by the cognitive abilities inherent in human learning,\nas well as the benefits observed in self-learning autonomous\nagents and the progress made in prompt-based methods, we\ndeveloped the Experiential Learning (ExpeL) agent. During\nthe training stage, the agent interacts with the environment,\ngathering experiences via trial and error. These experiences\nare stored in an experience pool (Lin 1992). From this pool,\nthe agent later extracts insights, similar to off-policy learn-\ning (Watkins and Dayan 1992), in which the agent can learn\nfrom experiences of a behavior policy. During the evaluation\nstage, the agent attempts unseen tasks with a single try, aug-\nmented with extracted insights and successful trajectories in\nits experience pool gathered from the training stage. Refer\nto Fig. 1 for detailed information on our agent framework.\n4.1 Gathering Experiences\nTo gather diverse experiences that can be useful to extract\ninformation from, we leverage Reflexion (Shinn et al. 2023)\nto continuously retry the training task at most Z times.\nIn particular, the agent will be given a training task tn\nat the z-th trial, fewshot examples Fmanual and past reflec-\ntions νn,z (initially, νn,0 is the empty string). At first, the\nagent will attempt the task with fewshot examples concate-\nnated with its current trajectory τn,0 as the context, and\nuse ReAct (Yao et al. 2023b) as the base planning algo-\nrithm, LLM ReAct(· |τn,0, Fmanual, νn,0). On the z-th trial,\nwhen the agent finishes the task or the maximum number\nof steps H is reached, the ExpeL agent’s experience pool\nB ingests the trajectory τn,z. Then, if the agent succeeds,\nit moves on to the next task. However, if the agent fails,\nit will look at its failed trajectory and self-reflect to pro-\nduce νn,z+1 = concat(νn,z, LLMreflect(τn,z)) to see where\nit can do better on the next retry, concatenated with the pre-\nvious reflections. In the next retry, the agent will augment its\ncontext with reflection νn,z+1, the input to the LLM policy,\nLLMReAct(· |τn,z+1, Fmanual, νn,z+1).\nTo highlight, this trial and error way of gathering expe-\nriences not only improves the chances of getting more pos-\nitive examples for experience recall during evaluation but\nalso allows for collecting valuable success/failure pairs used\nfor comparisons during insight extraction (Sec. 4.2). The\npseudo-code can be found in Alg. 1.\n4.2 Learning from Experiences\nHuman learning occurs mainly either by storing successful\ntrajectories in memory, which can be later recalled as spe-\ncific examples, or by extracting high-level insights from ex-\nperiences, enabling generalization to novel situations. Ex-\npeL considers both of these learning modes to boost task\nperformance. Concretely, an instruction I given to an LLM\nagent can be broken down into task specifications and few-\nshot examples. We can augment task specifications with an\nagent’s extracted insights from past experiences, where an\ninstruction-following LLM can be leveraged to follow them\nclosely. For fewshot examples, we can allow the agent to re-\ntrieve from its experience pool with top-krelevant examples\nto aid its decisions. Next, we detail our experience recall and\ninsight extraction mechanisms.\nSimilar Experiences as Demonstrations Works have\nshown that using in-context examples that are semantically\nsimilar to the task at hand results in better performance (Liu\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19634\net al. 2022). Moreover, when involved in a novel situation,\nhumans also recall from their memory similar tasks they’ve\nsolved as references when attempting the task (Kahneman\n2011). Motivated by these observations, we propose experi-\nence recall to retrieve successful trajectories from the expe-\nrience pool gathered during training based on task similarity.\nConcretely, we used the Faiss vectorstore (Johnson,\nDouze, and J ´egou 2019) as the experience pool, kNN re-\ntriever and all-mpnet-base-v2 (Song et al. 2020) em-\nbedder to obtain top-k successful trajectories that have the\nmaximum inner-product task similarity with the evaluation\ntask. The advantage of using task similarity as the retrieval\nrank is that if the agent repeats a task or does a task simi-\nlar to an existing successful trajectory from the experience\npool, the agent only needs to closely imitate the successful\ntrajectory and have less burden on ability extrapolation.\nLearning from Successes and Failures To leverage the\ndiverse outcomes gathered during the experience collection\nphase, we believe the agent should analyze experiences in\ntwo distinct ways. First, we let the agent compare a failed\ntrajectory with a successful trajectory for thesame task. This\ncomparison offers a concrete understanding of the agent’s\nshortcomings, highlighting the correct and incorrect actions.\nSecond, we let the agent identify patterns within a set of\nsuccessful trajectories from different tasks. This approach\nsheds light on common “good practices” that the agent can\nadopt to ensure success in evaluation tasks.\nFor the implementation, we give the agent’s instruction-\nfollowing LLMinsights several operators to apply on an ex-\nisting set of insights ˆι. We initialize the set of insights to\nan empty set ˆι = ∅ and iteratively provide the LLM with\nfail/success pairs or lists of L successes (created by sam-\npling without replacement) from the experience pool. The\noperations the LLM can perform are: ADD a new insight,\nEDIT the content of an existing insight, DOWNVOTE to dis-\nagree with an existing insight, or UPVOTE to agree with an\nexisting insight. A newly added insight will have an initial\nimportance count of two associated with it, and the count\nwill increment if subsequent operators UPVOTE or EDIT\nare applied to it and will decrement when DOWNVOTE is ap-\nplied to it. If an insight’s importance count reaches zero, it\nwill be removed. This particular design choice robustifies the\nprocess since even successful trajectories can be suboptimal\nand mislead the generated insights. The prompt template we\nused can be found in Fig. 2. We kept the maximum size for a\nlist of successes to L and used gpt-4-0613 as the default\nLLMinsights. Weempirically found that gpt-4-0613 is bet-\nter than gpt-3.5-turbo-0613 at following instructions\non how to use the insight extraction operators and halluci-\nnated less. Pseudo-code for this process can be found in Alg.\n2. Finally, ExpeL utilizes these generated insights ˆι in the\ntask inference phase, described next.\n4.3 Task Inference\nAfter the agent gathers experiences, extracts insights from\nthem, and sets up a vectorstore of successful trajectories, it\ncan proceed to the evaluation. For each task, the task specifi-\ncations will be augmented with the concatenation of the full\nFigure 2: Insight Extraction Prompt Template. The prompt\ntemplate ExpeL agents used for insight extraction. The same\ntemplate is used both for success/fail pairs (A, in yellow) and\nL-sized successes (B, in green).\nlist of extracted insights ˆι = concat(ι1, ι2, ι3, ...), and the\ntop-k trajectories with the highest task similarity will be re-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19635\ntrieved and used as fewshot in-context examples,Fsimilar tasks.\nFig. 3 shows an example prompt template structure, and a\npseudo-code for this step can be found in Alg. 3. We believe\nas the list of extracted insights grows, retrieval could be a\nfeasible solution to manage the context window size.\nFigure 3: Task Inference Prompt Template. We illustrate Ex-\npeL’s prompt template during evaluation. The areas with a\nwhite background are identical to the base ReAct agent’s\ninputs. We differ by (purple areas) having additional ex-\ntracted insights from past experience, and dynamically re-\ntrieved successfulin-context examplesfrom past experiences\nbased on task similarity.\n4.4 Transfer Learning\nAfter demonstrating how learning by using experiences\nfrom a training set can benefit an LLM agent in solving an\nunseen task in the same task distribution, we investigate an-\nother interesting setting where knowledge accumulated from\na source task distribution could be useful for a target task\ndistribution with minimal target task examples for the Ex-\npeL agent. Like most transfer learning settings, we assume\nthat the source and target tasks exhibit common knowledge.\nTherefore, experiences accumulated from source tasks can\nbenefit the agent in solving a new set of target tasks.\nSimilar to pretraining on source task and finetuning on tar-\nget task in transfer learning literature (Zhuang et al. 2020),\nwe propose to use the extracted insights ˆι from the source\ntask and fewshot examples from the target task to “finetune”\nthe insights so that they are more applicable in the target\ntask. We hypothesize that using target task fewshot examples\ncan better ground the insights into the target task and miti-\nAlgorithm 1: ExpeL - Experience Gathering\nInitialize:\nPolicy LLMReAct\nSelf-reflection model LLMreflect\nCollection of tasks Ttrain\nFewshot examples Fmanual\nExperience pool B ←Fmanual\nNumber of training tasks N\nMaximum retry number Z\nMaximum step number H\nCurrent task index n ← 1\nwhile task n ≤ N do\ntn ← Ttrain[n]\nReflection νn,0 ← “”\nfor trial z = 0to Z do\no0 ← env.reset(tn)\nInitialize trajectory τn,z ← o0\nfor timestep i = 0to H do\nai ← LLMReAct(ai | τn,z, Fmanual, νn,z)\noi+1, ri+1, done ← env.step(ai)\nτn,z ← τn,z ∪ {(oi, ai, oi+1, ri+1)}\nif done then\nbreak\nend if\nend for\nB ← B ∪τn,z\nif done or z = Z then\nn ← n + 1\nbreak\nelse\nνn,z+1 ← concat(νn,z + LLMreflect(τn,z))\nend if\nend for\nend while\nreturn B\nAlgorithm 2: ExpeL - Insight Extraction\nInitialize:\nExperience pool B (from Alg. 1)\nInsight extraction model LLMinsights\nSet of insights ˆι ← ∅\nDivide the successes in B into L-sized chunks:\nCsuccess = {{τsuccess\n1 , τsuccess\n2 , ...τsuccess\nL },\n{τsuccess\nL+1 , τsuccess\nL+2 , ...τsuccess\n2L }, ...}\nConstruct fail/success tuples of the same tasks in B:\nCcompare = {(τsuccess\n1 , τfail\n1,0), (τsuccess\n1 , τfail\n1,1), ...,\n(τsuccess\n2 , τfail\n2,0), ...}\nfor each ccompare in Ccompare do\nˆι ← LLMinsights(ccompare, ˆι)\nend for\nfor each csuccess in Csuccess do\nˆι ← LLMinsights(csuccess, ˆι)\nend for\nreturn ˆι\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19636\nAlgorithm 3: ExpeL - Evaluation\nInitialize:\nExpeL agent LLMExpeL\nText Embedder E\nExperience pool B (from Alg. 1)\nSet of insights ˆι (from Alg. 2)\nCollection of evaluation tasks Tevaluation\nNumber of evaluation tasks M\nNumber of fewshots k\nNumber of successes S ← 0\nfor task m = 1to M do\ntm ← Tevaluation[m]\no0 ← env.reset(tm)\nInitialize trajectory τm ← o0\nFsimilar tasks ← Faiss(tm, B, E, k)\nfor timestep i = 1to H do\nai ← LLMExpeL(ai | τm, Fsimilar tasks, ˆι)\noi+1, ri+1, done ← env.step(ai)\nτm ← τm ∪ {(oi, ai, oi+1, ri+1)}\nif done then\nbreak\nend if\nend for\nif ri+1 = 1then\nS ← S + 1\nend if\nend for\nreturn S\nM\ngate hallucinations. An example prompt template to “fine-\ntune” extracted insights from a source domain to tailor them\nto a target domain is illustrated in Fig. 4.\n4.5 ExpeL’s Strengths\nIn this section, we outline the key strengths of our frame-\nwork. First and foremost, ExpeL offers inherent inter-\npretability, as both the extracted experiences and successful\ntrajectories are presented in natural language. This design\nallows users to easily inspect, modify, or remove potentially\nharmful trajectories/insights — a challenge in finetuned\nmodels. Moreover, users can seamlessly add expert insights\nor trajectories to an ExpeL agent. Additionally, our learning\napproach is highly accessible; it demands less data, reduces\ncomputational resources, and is straightforward to imple-\nment. Furthermore, self-improvement methods like Reflex-\nion (Shinn et al. 2023) facilitate intra-task improvements,\nbut ExpeL enables inter-task learning. ExpeL does not rely\non retries during deployment, which certain domains re-\nquire. On the flexibility front, the ExpeL agent boasts a\nsignificant level of versatility. It is not restricted to spe-\ncific language models and complements existing strategies\naimed at enhancing LLM agent planning capabilities. More-\nover, when applied in conjunction with them, ExpeL might\neven improve the capabilities of finetuned agents. Another\nstrength lies in continuous improvement. Our method stands\nto benefit from the ongoing enhancements in foundational\nmodels. As an illustration, our experiments show that using\nFigure 4: Transfer Learning Finetuning Prompt Template.\nThe prompt template used to finetune knowledge from\nsource to target domain. Highlighted in grey should be for-\nmatted with concise descriptions of the tasks.\ngpt-4 to extract insights outperforms gpt-3.5-turbo\n(refer to Sec. 5.6). Lastly, we introduced a method for trans-\nferring extracted insights across domains using only a small\namount of finetuning examples, demonstrating the advan-\ntage of our approach in diverse settings with limited data.\n5 Experiments\n5.1 Experimental Setup\nIn line with ReAct (Yao et al. 2023b), the experiments\nare designed based on four text-based benchmarks: Hot-\npotQA (Yang et al. 2018), a knowledge-intensive dataset\nthat challenges an agent to perform reasoning and question\nanswering using the search tool Wikipedia Docstore API,\nALFWorld and WebShop (Shridhar et al. 2021; Yao et al.\n2022) that require the agent to perform interactive multi-step\ndecision-making tasks in respectively a household and an\nonline shopping website environments, and FEVER (Thorne\net al. 2018), that focuses on fact verification tasks using the\nsame API as HotpotQA which makes it suitable for knowl-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19637\nFigure 5: Main Results. Average task success rates (std. error in gray arrows) across three different domains: HotpotQA,\nALFWorld, and WebShop. ReAct and Act are used as baselines. ExpeL consistently outperforms the baselines on all domains,\nhighlighting the importance of learning from experience. Additionally, we compare ExpeL with ExpeL (retrieve-only) and\nExpeL (insights-only) to highlight that both insight extraction and task similarity retrieval are essential and synergistic.\nedge transfer (Sec. 5.4). All experiments use four-fold vali-\ndation, and we report the mean and standard error over the\nfolds. Following ReAct, for all environments, we use suc-\ncess rate as the evaluation metric: exact matching for Hot-\npotQA and FEVER, completing the task in time for ALF-\nWorld, and purchasing the item that matches all attributes\nfor WebShop. Some additional metrics are introduced when\nthe environment offers them: mean reward score r ∈ [0, 1]\nfor WebShop and a score breakdown per task type for ALF.\nWe use ReAct and Act as main baselines planning LLM\nagents (Yao et al. 2023b), where Act does not have the rea-\nsoning steps like ReAct. All agents, including ExpeL, used\ngpt-3.5-turbo-0613 when performing actions during\nevaluation. All text generations were done with temperature\n0 and greedy decoding. Imitation learning (IL) results were\ntaken from the ReAct paper (Yao et al. 2023b).\n5.2 Main Results\nThe primary findings of this study are presented in Fig. 5.\nIL-based method struggles to efficiently perform in Web-\nShop and ALFWorld, possibly due to their demand for more\nsubstantial prior and reasoning abilities, which conventional\ntrainings from scratch fail to provide. This limitation shows\nthe promise of leveraging knowledge-based language mod-\nels to address these challenges. The following claims were\nmade based on (1) a deep understanding of each environ-\nment; (2) extracted insights and retrievable in-context exam-\nples; and (3) statistics (e.g. number of invalid actions per\ntrial) of the runs.\nExperiential learning Augmenting agents with ab-\nstracted insights and the ability to recall successful trajecto-\nries improve performance across all environments compared\nto baseline agents. When restricting the ExpeL agent to\nonly one mode of learning (insights-only or retrieval-only),\nHotpotQA and ALFWorld environments demonstrate con-\ntrasting quantitative distinctions (36%/31% and 50%/55%\nfor HotpotQA and ALFWorld, respectively). The prominent\ninfluence of insights on HotpotQA can be due to its re-\nliance on analysing (Wikipedia results) abilities. This high-\nlights the need for general guidelines across various question\ntypes. Conversely, ALFWorld’s task completion, dependent\non specific action sets, is better derived from past experi-\nential trajectories. Furthermore, WebShop presents a unique\nchallenge, requiring both website-based reasoning (price\ncomparisons, query reformulation, etc.) and precise execu-\ntion of actions (searching, clicking, option selection, etc.).\nConsequently, the performance across these tasks shows a\nnear equilibrium, as reflected in both the success rate and\nscore (37%/38% and 0.675/0.67 for insights/retrieve-only\nrespectively). These observations highlight the synergistic\ninterplay between abstraction and recollection in experien-\ntial learning, with ExpeL showing a quantitative advantage\nover baseline/restricted learning mode agents.\nCross-task learning Another important finding we ob-\nserve is the comparison with the Reflexion agent (Shinn\net al. 2023). ExpeL matches Reflexion’s performance (40%\nat R3 vs. 39%) for HotpotQA and even outperforms it for\nALFWorld (54% at R3 vs. 59%) without repeated attempts.\nWhile Reflexion improves results by iteratively refining in-\nsights through repeated task execution (R1, R2, R3...), our\nExpeL agent leverages cross-task learning by accumulating\ntask experience. However, it is noteworthy that there remains\nroom for improvement in the context of WebShop tasks, ap-\nproaching the lower side of Reflexion’s success rates.\n5.3 Agent Behavioral Analysis\nIn this section, we highlight some observations made by\nmanually inspecting the trajectories of ReAct agents and Ex-\npeL agents, and by pinpointing possible causes of how some\nunexpected behaviors might have emerged. Please visit the\npaper’s webpage, https://andrewzh112.github.io/#expel, for\nfull trajectory demos illustrating the following findings.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19638\nHypothesis Formulation & Constraints Adaptation Af-\nter extracting the insights from experiences gathered in the\ntraining set, we noticed the agent subsequently gained the\nability to reassess its whole trajectory in the last stepsand\nconclusively end the task rather than expressing its inep-\ntitude in providing a solution. This ability was particu-\nlarly observed in HotpotQA where a likely influential in-\nsight was stating that the agent should “consider the answer\nmight be in the observations already made”. Therefore the\nagent would finish by proposing the most probable answer\ngiven its past observations rather than concluding with “Un-\nknown” or “Information not available”.\nWorld Model Belief Update We noticed our ExpeL agent\nupdated its beliefs through the insights and over its gained\nexperience. This belief thereby update enables the agent to\navoid unnecessary actions and increase efficiency in solv-\ning a given task. For example, in ALFWorld, the agent com-\npletely changed the priors it had in ReAct on the likely loca-\ntions of a pan (from drawers/countertops/cabinets to stove-\nburners). This behavior emerged from the extracted insight\nclaiming that “when searching for an item” it needs to “con-\nsider its nature and its typical usage”, leading the agent to\npromptly and accurately find the correct item at the first step\nwhile the ReAct agent could not find it in time.\nSelf-correction Although ReAct was sometimes not able\nto reassess its situation when attempting to solve a task, Ex-\npeL demonstrated its proficiency in identifying and recti-\nfying missteps. Notably, when incorrectly taking an object\nin ALFWorld, the agent has shown its ability to put it back\nand resume the task by searching for the proper object. This\nhighlights ExpeL’s capacity to recover from errors and stay\non course without hallucinating when completing tasks. This\nbehavior is possibly encouraged by the generated insight\n“reassess the situation and consider alternative actions” if\n“an attempt does not progress the task”.\n5.4 Transfer Learning\nIn this experiment, we use the HotpotQA dataset (Yang\net al. 2018) as source tasks and the FEVER dataset (Thorne\net al. 2018) as target tasks. Like the HotpotQA dataset, we\nequip the agent with the ability to navigate on Wikipedia\nusing a Docstore API; therefore, we hypothesize that some\nof the knowledge obtained from HotpotQA tasks should\nalso be beneficial when transferred to the FEVER tasks. We\nuse gpt-4-0613 for adapting the HotpotQA insights into\nFEVER insights. We use the same fewshot examples to fine-\ntune the insights as the ones that will be used during task\nexecution. We compare our ExpeL Transfer agent’s transfer\nlearning ability with (1) ReAct; (2) Act; and (3) an agent\nthat “finetunes” insights without task demonstrations. No-\ntice that since source and target tasks are inherently differ-\nent, we do not have an experience pool to retrieve from; thus,\nthe ExpeL Transfer agents use the existing fixed fewshot ex-\namples as in-context examples.\nTab. 1 showcases the transfer learning results. Both agents\nthat transferred knowledge from the source domain saw per-\nformance gains. Notably, the agent with a few in-context\nexamples had a more significant improvement than the one\nwithout, indicating the effectiveness of the proposed “fine-\ntuning” method in transfer learning scenarios.\nFEVER (SR %)\nAct 58 ± 0.0\nReAct 63 ± 0.4\nExpeL Transfer w/o Task Demos 65 ± 1.7\nExpeL Transfer 70 ± 0.7\nTable 1: Transfer Results. We transfer insights extracted\nfrom HotpotQA to FEVER. Act and ReAct are baseline\nagents, ExpeL w/o Task Demosdoes not utilize fewshot ex-\namples when altering the insights for the target task.\nR0 R1 R2 R3\nReAct+Reflexion\n40.3% 47.8% 52.2% 54.4%\nExpeL retrie\nve only 54.5% 57.5% 59.7% 60.4%\nExpeL+Reflexion 59.0%\n60.4% 63.4% 64.2%\nTable 2: Success Rate on ALFWorld with Reflexion Rounds.\nExpeL and Reflexion appear to be synergistic in the ALF-\nWorld environment. R1-R3 were obtained from failed R0\ncheckpoints.\n5.5 ExpeL with Task Reattempts\nWhile not being the central focus of our study, we present\npreliminary findings on the effectiveness of incorporating\ntask reattempts into the evaluation phase using ExpeL by\nresuming the failed checkpoints from R0. The performance\nof ExpeL combined with Reflexion, alongside two base-\nlines: ReAct/Reflexion and ExpeL without insights (ExpeL\nretrieve only), is detailed in Table 2. The results demonstrate\na notable improvement in the success rate when ExpeL is\npaired with Reflexion, with the success rate increasing as\nthe number of task reattempts grows.\n5.6 Ablation Studies\nOne main component of ExpeL is the agent’s ability to au-\ntonomously gather valuable experiences benefiting its own\nlearning. Therefore, we wish to investigate if the number of\nuseful experiences impacts the downstream performance of\nExpeL. We designed two different agents to compare our\nagent with. The first one only has access to initial fewshot\nexamples and extracts insights from them. The second gath-\ners experience using ReAct where the agent has no retries.\nThus, the agent will not only get less successful trajecto-\nries but will also not have any success/failure comparison\npairs during insights extraction. We conducted experiments\nin the HotpotQA environment and presented the results in\nFig. 6. As we can see, the agent that extracts insights from\nthe existing fewshots has no advantage compared to the Re-\nAct agent, illustrating that experience is essential for ExpeL\nto learn from. This was reflected in a significantly better per-\nformance for the two other agents having access to more ex-\nperience. Furthermore, the ExpeL agent with access to a di-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19639\nverse set of experiences (failure and success pairs obtained\nusing Reflexion) performs better than the agent using only\nReAct during experience gathering.\nFigure 6: Effects of Experience on Performance. We high-\nlight the correlation between the number of diverse expe-\nrience samples and the final performance. Concretely, we\ncompare ExpeL with (1) ReAct, (2) ExpeL that only has\naccess to fewshot examples, and (3) ExpeL that only uses\nReAct during the experience gathering step. It is evident\nthat extra autonomously collected experiences are essential\nto ExpeL’s success and that diversity of success/failure data\ngathered using Reflexion was superior to using ReAct only.\nNext, we will scrutinize the efficacy of the insight ex-\ntraction step of ExpeL. Since insights had the most signif-\nicant impact on the HotpotQA environment (Fig. 5), we per-\nformed the ablations on insights in this environment. We\nuse three dimensions to ablate the design choices for in-\nsight extraction by creating the following variants of Ex-\npeL agents: (1) human-crafted insights, which were man-\nually engineered by carefully studying the agent’s mis-\ntakes during the experience gathering step; (2) adding re-\nflections ν into the insights construction step in addition\nto using fail/success pairs and lists of successes; (3) us-\ning gpt-3.5-turbo-0613 as the LLMinsights. Results in\nTab. 3 show several significant findings: (1) learned insights\nby the agent are more advantageous than hand-crafted ones;\n(2) using reflections in addition to success/failure pairs and\nlists of successes is disadvantageous, possibly due to reflec-\ntions sometimes outputting hallucinations, therefore mis-\nleading the insight extraction stage; and (3) a better LLM is\nmore advantageous at improving ExpeL’s performance, sug-\ngesting our agent will enjoy free performance boosts with\nthe ever-improving nature of base foundation models.\nLastly, we investigated the design choice of using task\nsimilarity as the ranking score for retrieving successful in-\ncontext examples in ALFWorld. In particular, we use (1)\nreason similarity by retrieving top-k trajectories with the\nmost similar reasoning step as the latest reasoning step in\nthe current trajectory, and (2) randomly sampling successful\ntrajectories from the experience pool. We clearly observe in\nTab. 3 that retrieving with task similarity (ExpeL) performs\nthe best. Reason similarity is still advantageous but slightly\ndrops in performance, possibly due to dynamically chang-\ning fewshots during a single trajectory, causing instabilities.\nLastly, random sampling has a significant drop in perfor-\nmance, suggesting that our design choice of selecting the\nmost pertinent in-context example is advantageous.\nHotpotQA (SR %)\nReAct 28.0 ± 1.4\nHand-crafted insights 32.0 ± 1.1\nInsights with reflections 29.0 ± 0.4\ngpt-3.5-turbo insights 32.0 ± 0.4\nExpeL (ours) 39.0 ± 1.7\nALFWorld (SR %)\nReAct 40.0 ± 0.3\nReasoning similarity 48.5 ± 2.1\nRandom sampled 42.5 ± 0.8\nExpeL (ours) 59.0 ± 0.3\nTable 3: Ablations Results. Upper: Ablations on insight\nextraction. Hand-crafted insights enjoyed a performance\nboost over ReAct but were less effective than LLM-\ngenerated ones. Furthermore, adding reflections to the\ninsight-generating process hurt performance. Lastly, better\nLLM base models give better insights. Lower: Ablations on\nin-context examples selection strategy.Randomly selected\nbaseline has a significant drop in performance while ranking\nusing reason similarity also has a noticeable dip.\n6 Conclusion and Limitations\nLimitations In this work, we investigated tasks with tex-\ntual observation, which is limiting in real-world scenar-\nios. Thus, incorporating image observations will make our\nmethod more generally applicable. Using Vision-Language\nModels or captioning models to supplement the LLM to en-\nable image observations could be an interesting new avenue\nof research. Additionally, we investigated the efficacy of our\nmethod by using closed-source API LLMs, which can be\noff-limits in some applications. Exploring LLM agents us-\ning open-source LLMs should be another promising future\nwork (Zeng et al. 2023). Furthermore, since our extracted in-\nsights do not exceed the current LLM’s token limit, we can\nfit them into the agent’s context window. However, extra re-\ntrieval steps for insights might be needed for truly lifelong\nlearning agents to ensure a manageable context window size.\nLastly, unlike reinforcement learning methods, prompting\ntechniques lack theoretical underpinnings that could poten-\ntially impact the efficiency of the resulting policies. Future\nresearch should explore the integration of these approaches\nto yield more effective and optimal solutions.\nIn summary, we introduced ExpeL, a novel learning LLM\nagent that autonomously gathers experience from a set of\ntraining tasks to improve its abilities in solving evaluation\ntasks without access to model parameters. We demonstrated\nits learning abilities by showing its performance gain com-\npared to vanilla ReAct and Act agents. Furthermore, we in-\nvestigated a transfer learning scenario where extracting in-\nsights from a set of source tasks can benefit the ExpeL agent\nin solving a target task. Lastly, we presented several unex-\npected emerged abilities our agent developed at the end of\nits training. We believe that autonomously learning from ex-\nperience is essential for developing human-like intelligent\nagents, and our ExpeL agent is a step toward that goal.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19640\nAcknowledgements\nThis work is supported in part by the National Key R&D\nProgram of China (2022ZD0114900), the National Natu-\nral Science Foundation of China under Grants 62022048,\nU2336214, and 62332019, and the Guoqiang Institute of Ts-\ninghua University.\nReferences\nBoiko, D. A.; MacKnight, R.; and Gomes, G. 2023. Emer-\ngent Autonomous Scientific Research Capabilities of Large\nLanguage Models. arXiv preprint.\nBran, A. M.; Cox, S.; White, A. D.; and Schwaller, P.\n2023. ChemCrow: Augmenting Large-Language Models\nwith Chemistry Tools. arXiv preprint.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language Models are Few-Shot Learners.\nNeurIPS.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton,\nC.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko,\nS.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y .; Shazeer,\nN.; Prabhakaran, V .; Reif, E.; Du, N.; Hutchinson, B.;\nPope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.;\nYin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;\nMichalewski, H.; Garcia, X.; Misra, V .; Robinson, K.; Fe-\ndus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;\nSpiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omer-\nnick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\nA.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;\nWang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei,\nJ.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and\nFiedel, N. 2023. PaLM: Scaling Language Modeling with\nPathways. JMLR.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .;\nFedus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.;\net al. 2022. Scaling Instruction-Finetuned Language Mod-\nels. arXiv preprint.\nDu, M.; He, F.; Zou, N.; Tao, D.; and Hu, X. 2022. Shortcut\nLearning of Large Language Models in Natural Language\nUnderstanding: A Survey. arXiv preprint.\nGong, R.; Huang, Q.; Ma, X.; V o, H.; Durante, Z.; Noda, Y .;\nZheng, Z.; Zhu, S.-C.; Terzopoulos, D.; Fei-Fei, L.; et al.\n2023. MindAgent: Emergent Gaming Interaction. arXiv\npreprint.\nHa, H.; Florence, P.; and Song, S. 2023. Scaling Up and\nDistilling Down: Language-Guided Robot Skill Acquisition.\nIn CoRL. PMLR.\nHao, S.; Gu, Y .; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.;\nand Hu, Z. 2023. Reasoning with Language Model is Plan-\nning with World Model. arXiv preprint.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. In ICML. PMLR.\nHumphreys, P.; Guez, A.; Tieleman, O.; Sifre, L.; Weber, T.;\nand Lillicrap, T. 2022. Large-scale Retrieval for Reinforce-\nment Learning. NeurIPS.\nJohnson, J.; Douze, M.; and J ´egou, H. 2019. Billion-scale\nSimilarity Search with GPUs. IEEE Transactions on Big\nData.\nKahneman, D. 2011. Thinking, Fast and Slow. Farrar, Straus\nand Giroux.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large Language Models are Zero-Shot Reasoners.\nNeurIPS.\nLi, H.; Su, Y .; Cai, D.; Wang, Y .; and Liu, L. 2022. A Survey\non Retrieval-Augmented Text Generation. arXiv preprint.\nLin, B. Y .; Fu, Y .; Yang, K.; Ammanabrolu, P.; Brahman, F.;\nHuang, S.; Bhagavatula, C.; Choi, Y .; and Ren, X. 2023a.\nSwiftSage: A Generative Agent with Fast and Slow Think-\ning for Complex Interactive Tasks. NeurIPS.\nLin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.\n2023b. Text2Motion: From Natural Language Instructions\nto Feasible Plans. Autonomous Robots.\nLin, L.-J. 1992. Self-Improving Reactive Agents Based on\nReinforcement Learning, Planning and Teaching. Machine\nlearning.\nLiu, J.; Shen, D.; Zhang, Y .; Dolan, B.; Carin, L.; and Chen,\nW. 2022. What Makes Good In-Context Examples for GPT-\n3? In DeeLIO. Association for Computational Linguistics.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2023a. Pre-train, Prompt, and Predict: A Systematic Sur-\nvey of Prompting Methods in Natural Language Processing.\nACM Computing Surveys.\nLiu, X.; Yu, H.; Zhang, H.; Xu, Y .; Lei, X.; Lai, H.; Gu, Y .;\nDing, H.; Men, K.; Yang, K.; et al. 2023b. AgentBench:\nEvaluating LLMs as Agents. arXiv preprint.\nMaas; Carey; Wheeler; Saatchi; Billington; and Shamash.\n2023. To Infinity and Beyond: SHOW-1 and Showrunner\nAgents in Multi-Agent Simulations. arXiv preprint.\nMu, Y .; Zhang, Q.; Hu, M.; Wang, W.; Ding, M.; Jin,\nJ.; Wang, B.; Dai, J.; Qiao, Y .; and Luo, P. 2023. Em-\nbodiedGPT: Vision-Language Pre-Training via Embodied\nChain of Thought. NeurIPS.\nNakano, R.; Hilton, J.; Balaji, S. A.; Wu, J.; Ouyang, L.;\nKim, C.; Hesse, C.; Jain, S.; Kosaraju, V .; Saunders, W.;\nJiang, X.; Cobbe, K.; Eloundou, T.; Krueger, G.; Button, K.;\nKnight, M.; Chess, B.; and Schulman, J. 2021. WebGPT:\nBrowser-Assisted Question-Answering with Human Feed-\nback. arXiv preprint.\nPark, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang, P.;\nand Bernstein, M. S. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. InACM Symposium on User\nInterface Software and Technology.\nPetroni, F.; Rockt¨aschel, T.; Riedel, S.; Lewis, P.; Bakhtin,\nA.; Wu, Y .; and Miller, A. 2019. Language Models as\nKnowledge Bases? In EMNLP-IJCNLP. Association for\nComputational Linguistics.\nRubin, O.; Herzig, J.; and Berant, J. 2022. Learning To Re-\ntrieve Prompts for In-Context Learning. In NAACL. Associ-\nation for Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19641\nShaw, P.; Joshi, M.; Cohan, J.; Berant, J.; Pasupat, P.; Hu,\nH.; Khandelwal, U.; Lee, K.; and Toutanova, K. 2023. From\nPixels to UI Actions: Learning to Follow Instructions via\nGraphical User Interfaces. NeurIPS.\nShinn, N.; Cassano, F.; Gopinath, A.; Narasimhan, K. R.;\nand Yao, S. 2023. Reflexion: Language Agents with Verbal\nReinforcement Learning. In NeurIPS.\nShridhar, M.; Yuan, X.; Cˆot´e, M.-A.; Bisk, Y .; Trischler, A.;\nand Hausknecht, M. 2021. ALFWorld: Aligning Text and\nEmbodied Environments for Interactive Learning. In ICLR.\nSignificant-Gravitas. 2023. AutoGPT. https://github.com/\nSignificant-Gravitas/Auto-GPT.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T.-Y . 2020. MP-\nNet: Masked and Permuted Pre-training for Language Un-\nderstanding. NeurIPS.\nSumers, T. R.; Yao, S.; Narasimhan, K.; and Griffiths, T. L.\n2023. Cognitive Architectures for Language Agents. arXiv\npreprint.\nSun, H.; Zhuang, Y .; Kong, L.; Dai, B.; and Zhang, C. 2023.\nAdaPlanner: Adaptive Planning from Feedback with Lan-\nguage Models. NeurIPS.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-Following LLaMA Model. https:\n//github.com/tatsu-lab/stanford\nalpaca.\nThoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kul-\nshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du,\nY .; et al. 2022. LaMDA: Language Models for Dialog Ap-\nplications. arXiv preprint.\nThorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal,\nA. 2018. FEVER: a Large-scale Dataset for Fact Extraction\nand VERification. In NAACL.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023a. LLaMA: Open and Efficient Foun-\ndation Language Models. arXiv preprint.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open Foundation and Fine-Tuned\nChat Models. arXiv preprint.\nTworkowski, S.; Staniszewski, K.; Pacek, M.; Wu, Y .;\nMichalewski, H.; and Miło´s, P. 2023. Focused Transformer:\nContrastive Training for Context Scaling. In NeurIPS.\nWang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.;\nChen, Z.; Tang, J.; Chen, X.; Lin, Y .; et al. 2023a. A Sur-\nvey on Large Language Model Based Autonomous Agents.\narXiv preprint.\nWang, L.; Yang, N.; and Wei, F. 2023. Learning to Retrieve\nIn-Context Examples for Large Language Models. arXiv\npreprint.\nWang, S.; Liu, C.; Zheng, Z.; Qi, S.; Chen, S.; Yang,\nQ.; Zhao, A.; Wang, C.; Song, S.; and Huang, G. 2023b.\nAvalon’s Game of Thoughts: Battle Against Deception\nthrough Recursive Contemplation. arXiv preprint.\nWatkins, C. J.; and Dayan, P. 1992. Q-learning. Machine\nlearning.\nWei, J.; Bosma, M.; Zhao, V .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022a. Finetuned\nLanguage Models are Zero-Shot Learners. In ICLR.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\nE.; Le, Q. V .; Zhou, D.; et al. 2022b. Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models.\nNeurIPS.\nXi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y .; Hong, B.;\nZhang, M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2023. The\nRise and Potential of Large Language Model Based Agents:\nA Survey. arXiv preprint.\nYang, S.; Nachum, O.; Du, Y .; Wei, J.; Abbeel, P.; and Schu-\nurmans, D. 2023a. Foundation Models for Decision Making:\nProblems, Methods, and Opportunities. arXiv preprint.\nYang, Z.; Li, L.; Wang, J.; Lin, K.; Azarnasab, E.; Ahmed,\nF.; Liu, Z.; Liu, C.; Zeng, M.; and Wang, L. 2023b. MM-\nREACT: Prompting ChatGPT for Multimodal Reasoning\nand Action. arXiv preprint.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W.; Salakhut-\ndinov, R.; and Manning, C. D. 2018. HotpotQA: A Dataset\nfor Diverse, Explainable Multi-hop Question Answering. In\nEMNLP. Association for Computational Linguistics.\nYao, S.; Chen, H.; Yang, J.; and Narasimhan, K. 2022. Web-\nShop: Towards Scalable Real-World Web Interaction with\nGrounded Language Agents. In NeurIPS.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023a. Tree of Thoughts: Deliberate\nProblem Solving with Large Language Models. NeurIPS.\nYao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,\nK.; and Cao, Y . 2023b. ReAct: Synergizing Reasoning and\nActing in Language Models. In ICLR.\nYao, W.; Heinecke, S.; Niebles, J. C.; Liu, Z.; Feng, Y .; Xue,\nL.; Murthy, R.; Chen, Z.; Zhang, J.; Arpit, D.; Xu, R.; Mui,\nP.; Wang, H.; Xiong, C.; and Savarese, S. 2023c. Retro-\nformer: Retrospective Large Language Agents with Policy\nGradient Optimization.\nZeng, A.; Liu, M.; Lu, R.; Wang, B.; Liu, X.; Dong, Y .; and\nTang, J. 2023. AgentTuning: Enabling Generalized Agent\nAbilities for LLMs. arXiv preprint.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2023. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. In ICLR.\nZhao, A.; Zhu, E.; Lu, R.; Lin, M.; Liu, Y .-J.; and Huang, G.\n2023a. Augmenting Unsupervised Reinforcement Learning\nwith Self-Reference. arXiv preprint.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023b. A\nSurvey of Large Language Models. arXiv preprint.\nZhuang, F.; Qi, Z.; Duan, K.; Xi, D.; Zhu, Y .; Zhu, H.;\nXiong, H.; and He, Q. 2020. A Comprehensive Survey on\nTransfer Learning. Proceedings of the IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19642",
  "topic": "Experiential learning",
  "concepts": [
    {
      "name": "Experiential learning",
      "score": 0.7584323287010193
    },
    {
      "name": "Psychology",
      "score": 0.4872983694076538
    },
    {
      "name": "Experiential education",
      "score": 0.41581571102142334
    },
    {
      "name": "Mathematics education",
      "score": 0.34467780590057373
    },
    {
      "name": "Medical education",
      "score": 0.34047794342041016
    },
    {
      "name": "Pedagogy",
      "score": 0.32345566153526306
    },
    {
      "name": "Medicine",
      "score": 0.20731252431869507
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 57
}