{
    "title": "Syntax-Aware Retrieval Augmented Code Generation",
    "url": "https://openalex.org/W4389520043",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2106804058",
            "name": "Xiangyu Zhang",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A2105593998",
            "name": "Yu Zhou",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A1990782487",
            "name": "Guang Yang",
            "affiliations": [
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A2106900639",
            "name": "Taolue Chen",
            "affiliations": [
                "Birkbeck, University of London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3177813494",
        "https://openalex.org/W3175863856",
        "https://openalex.org/W4221166942",
        "https://openalex.org/W3198685994",
        "https://openalex.org/W4376167329",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4226242393",
        "https://openalex.org/W3155807546",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W2757592053",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W3174199721",
        "https://openalex.org/W4286750487",
        "https://openalex.org/W2890867094",
        "https://openalex.org/W3021931813",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2809170625",
        "https://openalex.org/W3034976548",
        "https://openalex.org/W2949215742",
        "https://openalex.org/W3170427498",
        "https://openalex.org/W4385572795",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4321855134",
        "https://openalex.org/W4322717205",
        "https://openalex.org/W2889467844",
        "https://openalex.org/W3102839769",
        "https://openalex.org/W4287649493",
        "https://openalex.org/W18991458",
        "https://openalex.org/W3170092793",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3089307846",
        "https://openalex.org/W4285108092",
        "https://openalex.org/W2242083635",
        "https://openalex.org/W4206251287",
        "https://openalex.org/W4389519352"
    ],
    "abstract": "Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose kNN-TRANX, a token-level retrieval augmented code generation method. kNN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate kNN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1291–1302\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSyntax-Aware Retrieval Augmented Code Generation\nXiangyu Zhang1 Yu Zhou1∗ Guang Yang1 Taolue Chen2∗\n1 Nanjing University of Aeronautics and Astronautics\n2 Birkbeck, University of London\n{zhangx1angyu, zhouyu, yang.guang}@nuaa.edu.cn\nt.chen@bbk.ac.uk\nAbstract\nNeural code generation models are nowadays\nwidely adopted to generate code from natural\nlanguage descriptions automatically. Recently,\npre-trained neural models equipped with token-\nlevel retrieval capabilities have exhibited great\npotentials in neural machine translation. How-\never, applying them directly to code generation\nexperience challenges: the use of the retrieval-\nbased mechanism inevitably introduces extra-\nneous noise to the generation process, result-\ning in even syntactically incorrect code. Com-\nputationally, such models necessitate frequent\nsearches of the cached datastore, which turns\nout to be time-consuming. To address these is-\nsues, we propose kNN-TRANX, a token-level\nretrieval augmented code generation method.\nkNN-TRANX allows for searches in smaller\ndatastores tailored for the code generation task.\nIt leverages syntax constraints for the retrieval\nof datastores, which reduces the impact of re-\ntrieve noise. We evaluate kNN-TRANX on\ntwo public datasets and the experimental re-\nsults confirm the effectiveness of our approach.\n1 Introduction\nNeural code generation aims to map the input natu-\nral language (NL) to code snippets using deep learn-\ning. Due to its great potential to streamline soft-\nware development, it has garnered significant atten-\ntions from both natural language processing and\nsoftware engineering communities. Various meth-\nods have been explored to facilitate code genera-\ntion (Yin and Neubig, 2018; Wang et al., 2021; Guo\net al., 2022). Recent progress in neural machine\ntranslation (NMT) shows that the non-parametrick-\nnearest-neighbour machine translation (kNN-MT)\napproach may significantly boost the performance\nof standard NMT models (Khandelwal et al., 2021)\nand other text generation models (Kassner and\nSchütze, 2020; Shuster et al., 2021) by equipping\n∗Corresponding author.\nNL Check if object obj is a string.\nkNN-MT all(isinstance(obj)\nThe correct one isinstance(obj, str)\nTable 1: An example code generated by kNN-MT.\nthe models with a token-level retriever. In par-\nticular, this neural-retrieval-in-the-loop approach\nfacilitates the integration of external knowledge\ninto the pre-trained model and provides a simple\nyet effective method to update the model by switch-\ning the retrieval datastore, without fine-tuning the\nmodel parameters.\nCan such neural-retrieval-in-the-loop approach\nbenefit neural code generation? Our preliminary\nexperiments reveal three main issues (cf. the exam-\nple in Table 1) if it is adopted outright. Firstly, the\nmodel performance may be negatively affected by\nthe noise in the retrieved knowledge. For example,\n\"all\" does not match the intention of the descrip-\ntion, but it is recognized as the target token by the\nretriever, resulting in the generation of incorrect\ncode. Secondly, the code generated by kNN-MT\ncannot guarantee syntactic correctness, as demon-\nstrated by the mismatching parentheses in the given\nexample. Thirdly, the token-level retrieval method\nrequires similarity search of the entire datastore\nat each time step of inference, which hinders the\ndeployment of such approach.\nIn this paper, we propose a novel code gener-\nation approach, i.e. kNN-TRANX, to overcome\nthe limitations of the neural-retrieval-in-the-loop\nparadigm for code generation tasks. The basic idea\nis to integrate symbolic knowledge to ensure the\nquality of the generated code and expedite the re-\ntrieval process. To achieve this, we leverage the\nsequence-to-tree (seq2tree) model to generate ab-\nstract syntax tree (AST), which is a hierarchical\ntree-like structure used to represent the code, rather\nthan generate target code snippet directly. This en-\nables us to use AST construction rules to guarantee\n1291\nthe syntactic correctness of the generated code and\nfilter out retrieval noise.\nWe design kNN-TRANX as a two-step process\n(cf. Figure 2). In the first step, we construct two\nseparated datastores, i.e., the syntactic datastore\nand the semantic datastore, based on the type of\nAST nodes. This allows us to determine the type\nof the next node to be predicted according to the\ngrammar rules and query a specific datastore. In\nthe second step, we utilize syntactic rules to filter\nout irrelevant knowledge and convert the similarity\nretrieval results of the current target token into a\nprobability distribution, i.e., the kNN probability.\nThis probability, together with probability from the\nneural network, yields the probability of the ac-\ntion to be used for AST generation via a learnable\nconfidence parameter. It can help to minimize re-\ntrieval noise and dynamically exploit combinations\nof the two probabilities, resulting in improved code\ngeneration performance.\nTo evaluate the effectiveness ofkNN-TRANX,\nwe perform experiments on two publicly avail-\nable code generation datasets (i.e., CoNaLa and\nDjango). The experimental results show a 27.6%\nimprovement in the exact match metric on the\nCoNaLa dataset and a 4.2% improvement in the\nBLEU metric on the Django dataset, surpassing\nfive state-of-the-art models under comparison. Ad-\nditionally, we conduct an experiment on model\ncanonical incremental adaptation, which updates\nkNN-TRANX by switching the datastore. The\nexperimental results demonstrate that our model\ncan achieve performance comparable to fully fine-\ntuned models and reduce the trainable parameters\nby 7,000 times.\n2 Background\nIn this section, we provide an overview of thekNN-\nMT paradigm and the seq2tree model.\n2.1 kNN-MT\nThe kNN-MT paradigm (Khandelwal et al., 2021)\nis a translation mechanism that enhances the qual-\nity of model generation by incorporating an ad-\nditional translation retriever. This allows NMT\nmodels to benefit from the retrieved knowledge.\nThe paradigm comprises two main parts, namely,\ndatastore building and model inferring.\nDatastore Building. The datastore consists of a\nset of key-value pairs, where the key is the decoder\nhidden state and the value is the corresponding\ntarget token. Formally, given a bilingual sentence\npair (x,y) from the training corpus (X,Y), a pre-\ntrained NMT model fNMT (·) generates the i-th\ncontext representation hi = fNMT (x,y<i), then\nthe datastore Dis constructed as follows.\nD= (K, V) =\n⋃\n(x,y)∈(X,Y)\n{(hi,yi), ∀yi ∈y}\nModel Inferring. During inference, at time step\ni, given the already generated token ˆy<i and the\ncontextual information ˆhi, the kNN-MT model gen-\nerates yi by retrieving the datastore, which can be\ncalculated as\npkNN(yi |x, y<i) ∝\n∑\n(hj,yj)\n1 yi=yjexp\n(−dj\nT\n)\n(1)\nwhere T is the temperature and dj indicates the l2\ndistance between query ˆhi and the retrieved key hj.\n2.2 Seq2tree Model\nThe purpose of the seq2tree code generation mod-\nels is to generate ASTs instead of directly out-\nputting code snippets. Compared to the sequence-\nto-sequence (seq2seq) models, the seq2tree models\nensure the syntactic correctness of the generated\ncode. Among the seq2tree models, BertranX (Beau\nand Crabbé, 2022) was recently proposed and rep-\nresented the state-of-the-art architecture. BertranX\nemploys BERT to process the input natural lan-\nguage and features a grammar-based decoder.\nFigure 1: Example of ASDL for Python. ASDL defines\na set of grammatical symbols, which are denoted in\norange and distinguished by a unique constructor name\nhighlighted in blue. Each rule assigns names to its\nfields or the symbols marked in black. The grammatical\nsymbols can be classified into two types: nonterminals\n(e.g., expr) and terminals or primitives (e.g., identifier).\nSome of the grammatical symbols may have qualifiers\n(*) that allow for zero or more iterations of the symbol.\nBertranX describes ASTs using sequences of\nactions based on ASDL (Wang et al., 1997) gram-\nmar, which gives concise notations for describing\nthe abstract syntax of programming languages (cf.\nFigure 1 as an example). With ASDL, BertranX\n1292\ndefines two distinct types of actions that generate\nASTs, i.e., PREDRULE and GENERATE. The first\ntype is used for initiating the generation of a new\nnode from its parent node, which we mark as syn-\ntactic nodes in this paper; the second type on the\nother hand, is used to produce terminal or primitive\nsymbols that we mark as semantic nodes.\n3 kNN-TRANX\nThe workflow of kNN-TRANX is depicted in Fig-\nure 2. It consists of two main components: datas-\ntore building and model inferring.\n3.1 Datastore Building\nGiven a pre-trained seq2tree model and the training\ndataset, we first parse code snippets to ASTs and\ngenerate all instances in the training corpus. This\nprocess allows us to capture and store the decoder\nrepresentations along with their corresponding tar-\nget tokens as key-value pairs. The actions that con-\nstitute an AST can be categorized into two groups:\nrules and primitives. These categories align with\nthe actions of GENERATE and PREDRULE, re-\nspectively. As shown in Figure 3, the two types\nof nodes have significant differences in terms of\ntype and quantity. Combining them into the same\ndatastore could potentially reduce retrieval accu-\nracy. Therefore, we employ separated datastores\nfor each node type, referred to as the syntactic and\nsemantic datastores respectively. Nodes represent-\ning the structural information (e.g., Expr and Call)\nare put into the syntactic datastore, while nodes\nrepresenting the semantic information (e.g., text\nand split) of the code are put into the semantic one.\nGiven an NL-code pair (x,y) from the training\ncorpus (X,Y), we first transform the code snippets\nYinto AST representations Z. Next, we calculate\nthe i-th context representation hi = fθ(x,z<i),\nwhere fθ(·) refers to the trained seq2tree model\nand z∈Z. The datastore is constructed by taking\nhi’s as keys and zi’s as values. Namely,\nD(gra) =\n(\nK, V(gra)\n)\n=\n⋃\n(x,z)∈(X,Z)\n{(hi,zi) |zi ∈z& zi ∈rules},\nand\nD(pri) =\n(\nK, V(pri)\n)\n=\n⋃\n(x,z)∈(X,Z)\n{(hi,zi) |zi ∈z& zi ∈primitives}.\nAs a result, two separated symbolic datastores\ncan be constructed based on the various types of\ntarget actions within the training set. Constructing\ndatastores in this manner is more effective than\nstoring both types of actions in a single datastore\nsince it helps reduce noise during retrieval. More-\nover, the subsequent token type can be determined\nbased on grammar rules, facilitating the retrieval\nof a specific datastore and accelerating the retrieval\nprocess.\n3.2 Model Inferring\nThe process of model inference can be divided into\nthree main phases, as shown in Figure 2. First,\nthe code fragment xis put into the trained model\nto generate the context representation hi and com-\npute the neural network distribution (pNN). Then,\nwe query the datastore using this representation to\nobtain the k-nearest-neighbor distribution (pkNN).\nFinally, we combine these two distributions to pre-\ndict the target token. In the subsequent sections,\nwe will discuss three pivotal components of kNN-\nTRANX: syntax-constrained token-level retrieval,\nmeta-k network, and confidence network.\nSyntax-constrained token-level retrieval. Given\nthe current context representation hi generated\nby the model, we first calculate the l2 distance\ndj = l2(hi,ˆhj) between the context representa-\ntion hi and each neighbor (ˆhj,ˆzj) in the datas-\ntore to determine the k nearest neighbors. Previ-\nous studies (Meng et al., 2022; Dai et al., 2023)\nhave restricted the search space based on poten-\ntial input-output patterns to improve decoding ef-\nficiency and reduce the impact of noise. However,\nthe restricted search space may also exclude some\nvaluable knowledge.\nTo mitigate this problem, our approach features\nsyntax-aware retrieval capability. In contrast to\nconventional seq2seq models, our model aims to\ngenerate ASTs that allow to incorporate symbolic\nknowledge and determine the retrieved tokens by\nmeans of syntactic rules. During the retrieval pro-\ncess, we can determine the type of the subsequent\ntoken based on the tokens already produced. If the\nnext token is expected to represent the syntactic\ninformation, we just retrieve the syntactic datastore\nto accelerate the retrieval process, and vice versa.\nAdditionally, we can also use the ASDL rules to\nexclude illegitimate tokens to reduce the amount\nof irrelevant information. For example, as seen\nin Figure 2, our model has already generated the\n1293\nFigure 2: Workflow of kNN-TRANX\n(a) Syntactic tokens\n (b) Semantic tokens\nFigure 3: t-SNE visualization of CoNaLa data features,\nwith dark colored dots indicating more frequently oc-\ncurring vocabulary.\nnode Expr in the previous time step. It should be\nnoticed that kNN-TRANX have retrieved Call and\nalias nodes according to the distances. However,\nthe child nodes of Expr do not support alias in the\nASDL grammar. In this way, we filter out these\nnodes from the search results to reduce noise and\navoid excluding valuable information.\nMeta-k network. We retrieve krelevant pieces of\nknowledge from the datastore, and then map the\ndistances between the query vector and the cached\nrepresentation as probabilities. Empirically, the\nnumber of retrievals, k, is crucial in our model\nbecause too few retrievals may result in valuable in-\nformation being ignored, while too many retrievals\nmay introduce noise. To alleviate this problem, we\nemploy the meta-k network (Zheng et al., 2021) to\ndynamically evaluate the weight of the retrieved\nknowledge. Meta-k network considers a range\nof values that are smaller than the upper bound\nK, instead of using a fixed value of k. Typically\nthe range is set as S = {0,1,2,4,··· ,2log2⌊K⌋}.\nTo evaluate the weight of each of the values,\nwe use distance dj and the count of distinct\nvalues in top j neighbors cj as features and\nobtain a normalized weight by pβ(k) =\nsoftmax (fβ([d1,...,d K; c1,...,c K])) where\nfβ(·) denotes the Meta-k Network. The prediction\nof kNN can be obtained by pkNN (zi|x,ˆz<i) =∑\nkr∈S\npβ(kr) ·pkrNN (zi|x,ˆz<i) where pkrNN\nindicates the kr-nearest-neighbor prediction results\ncalculated as Equation (1). In this way, the kNN\nmodel can expand the search space while reducing\nthe impact of the retrieval noise.\nConfidence network. In order to utilize the knowl-\nedge of the symbolic datastore while maintaining\nthe generalization ability of the neural network, we\ncombine two probability distributions by means\nof weighting. Previous studies have integrated\nthe distributions of kNN and NN by using a fixed\nor learnable parameter λto measure their respec-\ntive weights. Khandelwal et al. (2021) combine\nthe probability distributions using fixed weights,\nbut this approach fails to dynamically adjust the\nweights based on the distance retrieved. Zhu et al.\n(2023) adjust the weights based on the retrieval\ndistance, but they overlook the confidence of the\nneural network output. Khandelwal et al. (2021)\nutilize the probability of the neural network and\nretrieval distance as features to dynamically select\nthe value of λ which consider the confidence of\n1294\nthe two distributions, but this approach neglects the\ncorrelation between the two distributions.\nTo address this issue, we propose a confidence\nnetwork that estimates the confidence of both\nprobabilities based on kNN and NN distributions.\nIn addition, we incorporate the weights of each\nk-value as features into the confidence network,\nensuring that the model is aware of the number\nof tokens that require attention. As such our\nmodel can capture the relationship between\nthe two distributions. In cases where the two\ndistributions conflict, we assign a higher weight\nto the distribution with higher confidence. The\nconfidence λ is calculated from W as λi =\nS(W[pkNN (zi |x,ˆz<i) ;pNN (zi|x,ˆz<i) ;pβ(k)]),\nwhere S denotes the sigmoid activation function.\nThe final distribution at prediction zi is calcu-\nlated as a weighted sum of two distributions with\nλi, i.e., p(zi|x,ˆz<i) =λipkNN + (1−λi) pNN.\n4 Experiments\nIn this section, we first introduce the datasets and\nevaluation metrics. Then, we conduct a compre-\nhensive study and analysis on code generation and\nmodel canonical incremental adaptation.\n4.1 Datasets and evaluation metrics\nWe evaluate kNN-TRANX on two code genera-\ntion datasets, namely, CoNaLa dataset (Yin et al.,\n2018) and Django dataset (Oda et al., 2015). The\nCoNaLa dataset comprises 600k NL-code pairs col-\nlected from StackOverflow, out of which 2,879 NL\nwere rewritten by developers. This dataset con-\ntains questions that programmers encounter in their\nreal-world projects. On the other hand, the Django\ndataset consists of 18,805 examples, where each\nexample consists of one line of Python code accom-\npanied by corresponding comments. Compared to\nCoNaLa, approximately 70% of the examples in\nDjango are simple tasks that include variable as-\nsignment, method definition, and exception han-\ndling, easily inferred from the corresponding NL\npredictions. We employed BLEU (Papineni et al.,\n2002), CodeBLEU (Ren et al., 2020), and exact\nmatch (EM) metrics to assess the performance of\nour experiments.\n4.2 Code Generation\nImplementation details. We use BertranX as the\nbase seq2tree model for our experiments, which\nis trained on annotated data and 100k mined data.\nTo expedite the implementation, we leverage kNN-\nbox (Zhu et al., 2023), an open-source toolkit for\nbuilding kNN-MT, to implement kNN-TRANX.\nAs explained in Section 3, kNN-TRANX creates\ntwo datastores. Due to the considerable difference\nin vocabulary sizes between the two datastores, we\nconstruct separate settings for the syntactic and\nsemantic datastores. For the syntactic datastore,\nwe set the upper limit Krule to 4 to account for its\nlimited token variety. For the semantic datastore,\nwe set Kpri to 64. To train the meta-k network\nand confidence network, we employ the AdamW\noptimizer with a learning rate of 3e-4. To accelerate\nthe datastore retrieval process, we incorporate the\nFAISS library (Johnson et al., 2019) for similarity\nretrieval. All experiments are performed on a single\nNVIDIA 2080Ti.\nBaselines. We compare kNN-TRANX against five\nstate-of-the-art code generation models.\n• TRANX (Yin and Neubig, 2018) is a seq2tree\nmodel consisting of a bidirectional LSTM en-\ncoder for learning the semantic representation\nand a decoder for outputting a sequence of ac-\ntions for constructing the tree.\n• Reranker (Yin and Neubig, 2019) reorders a set\nof N-best candidates to improve the quality of\nthe generated results.\n• Ext-codegen (Xu et al., 2020) incorporates API\ndocumentation as external knowledge into the\nmodel, thus enabling data augmentation.\n• TAE (Norouzi et al., 2021) uses BERT and a\ntransformer decoder to auto-encoding monolin-\ngual data.\n• BertranX (Beau and Crabbé, 2022) uses BERT\nas an encoder and serves as the base model for\nour kNN-TRANX.\n• REDCODER (Parvez et al., 2021) retrieves rele-\nvant code from a retrieval database and provides\nthem as a supplement to code generation models.\n• CodeT5 (Wang et al., 2021) builds on the sim-\nilar architecture of T5 (Raffel et al., 2020) but\nincorporates code-specific knowledge to endow\nthe model with better code understanding.\nMain results.\nThe experimental results are presented in Table 2.\nOur proposed kNN-TRANX exhibits a superior\nperformance over BertranX on the CoNaLa dataset\nby 3.11 BLEU (9.1%), 2.95 CodeBLEU (8.2%),\nand 1.6 EM (27.6%). On the Django dataset, we\nobserved improvements of 3.34 BLEU (4.2%), 2.81\nCodeBLEU (3.6%), and 2.39 EM (3.0%). These re-\n1295\nModel CoNaLa Django\nBLEU CodeBLEU EM BLEU CodeBLEU EM\nTRANX 28.11 29.01 2.5 62.31 64.35 73.70\nReranker 30.11 28.98 2.8 73.26 71.29 80.18\nExt-codegen 32.26 33.23 3.0 - - -\nTAE 33.41 32.87 3.4 68.39 70.27 81.03\nBertranX 34.18 36.09 5.8 79.86 78.85 79.77\nREDCODER 35.12 36.82 6.4 75.36 73.01 78.82\nCodeT5 36.28 35.34 6.8 76.50 71.92 80.93\nkNN-TRANX 37.29 39.04 7.4 83.20 81.66 82.16\nTable 2: Comparative results of models trained on the CoNaLa and Django test datasets.\nFigure 4: The syntax match score of the generated code\non the CoNaLa dataset.\nsults indicate that token-level retrieval can improve\nthe performance of code generation models, and\nthe improvement is more evident for challenging\ntasks. To demonstrate that our model can gener-\nate code with more accurate syntax, we provide\nthe syntax match score (Ren et al., 2020) in Fig-\nure 4, which reflects the degree of syntax matching\nin code. The results indicate that our model out-\nperforms the baseline model in terms of syntax\naccuracy.\nAnalysis. We conduct an ablation study of kNN-\nTRANX as shown in Table 3. The experimental\nresults demonstrate that retrieval filtering method\ncan significantly enhance the performance of the\ncode generation models. For the method that com-\nbines NN distribution and kNN distribution, we\ncompared the method of measuring retrieval dis-\ntance proposed in adaptive kNN-box (Zhu et al.,\n2023). Experimental results show that our ap-\nproach of considering both distributions compre-\nhensively achieves better results. We also evaluate\nthe effect of placing both types of action in the\nsame datastore. The result shows that this approach\nsignificantly reduces the quality of the generated\ncode by introducing a substantial amount of noise.\nMoreover, we analyze the effect of Krule and Kpri\non the experimental results, as presented in Table 4.\nThe results align with our conjecture that retrieving\na small number of the syntactic nearest neighbors\nand a relatively large number of semantic entries\nleads to better code generation. Furthermore, Fig-\nure 5 shows that increasing the size of datastore\ncan improve the quality of generated code. Ad-\nditionally, Figure 5(a) and Figure 5(d) depict that\neven a small amount of syntactic knowledge stored\ncan achieve a high quality of code generation. In\ncontrast, the quality of generated code keeps im-\nproving as the size of semantic datastore increases.\nWe believe this is because semantic knowledge is\nrelatively scarce compared to syntactic knowledge,\nwhich can be demonstrated by Figure 3.\n4.3 Model Canonical Incremental Adaptation\nImplementation details. Although the recent pro-\nposed large-scale models such as Codex (Chen\net al., 2021) and GPT-4 (OpenAI, 2023) have\ndemonstrated its powerful code generation capabili-\nties, one of the challenges is that the trained models\nare difficult to update. Models need to be continu-\nously trained via incremental learning, which con-\nsumes huge computing resources. To make matters\nworse, incremental learning with new data can lead\nto catastrophic forgetting problems (Li and Hoiem,\n2016). To address this, we validate our model using\nincremental learning by only updating the datas-\ntore without adapting the model parameters. We\nuse BertranX†1 as our base model to simulate the\n1BertranX†is trained on annotated data and 5k mined data\non CoNaLa dataset.\n1296\nModel CoNaLa Django\nBLEU CodeBLEU EM BLEU CodeBLEU EM\nBertranX 34.18 36.09 5.8 79.86 78.85 79.77\nkNN-TRANX 37.29 39.04 7.4 83.20 81.66 82.16\nw/o noise filtering 36.51 38.23 7.0 81.32 79.92 81.54\nw/o Confidence Network 36.89 37.82 5.8 81.23 79.15 81.98\nw/o separate datastore 35.62 37.21 6.2 80.74 79.23 80.08\nTable 3: Ablation study of different strategies and networks on CoNaLa dataset and Django dataset.\nKrule Kpri BLEU CodeBLEU EM\n1 1 36.05 37.74 6.4\n1 16 36.33 37.46 7.0\n1 64 36.51 37.69 7.4\n1 128 36.63 37.84 7.2\n4 1 35.76 37.59 6.4\n4 16 37.09 38.72 7.0\n4 64 37.29 39.04 7.4\n4 128 37.06 38.80 7.2\n16 1 36.02 37.81 6.4\n16 16 37.01 38.70 7.2\n16 64 36.89 38.76 7.4\n16 128 37.34 38.99 7.2\nTable 4: We studied the impact of different K values\non the generation results on CoNaLa dataset. Krule ∈\n{1,4,16}and Kpri ∈{1,16,64,128}.\nscenario for incremental learning. Then, we update\nthe datastore using {0k, 10k, 50k, 95k} mined data\nin CoNaLa dataset respectively. In our experiment,\nwe update the datastore to perform efficient fine-\ntuning. Compared to the method of fine-tuning\nall parameters (which requires training 122,205k\nparameters), our method only needs to train 17k,\ngreatly reducing the GPU memory consumption\nrequired for training, and achieving comparable\nresults to fine-tuning all parameters.\nIt is worth noting that, compared to the previous\nkNN generative model, our model includes two\ndatastores for syntactic and semantic information,\nrespectively. There are 109 kinds of token in the\nsyntactic datastore, and the number of correspond-\ning datastore entries is 1,603k. However, the types\nof token in the semantic datastore may be infinite,\ndepending on the actual defined vocabulary, so the\nknowledge corresponding to each vocabulary is\nrelatively scarce, with only 518k entries in this ex-\nperiment. Figure 5 confirms that providing a small\namount of syntactic knowledge can improve the\nperformance of the model. Therefore, we consider\ntwo ways to update the datastores in our experi-\nments, i.e., updating both datastores and updating\nthe semantic datastore only.\nMain results. We adopte the same evaluation met-\nrics as code generation. As shown in Table 5,\nfirstly, without using additional datastore, kNN-\nTRANX†can outperform BertranX†. As the knowl-\nedge in the datastore is constantly updated, we\ncan see that kNN-TRANX has improved on three\nevaluation criteria. In terms of BLEU evaluation,\nkNN-TRANX†with 95k external data can achieve\nperformance comparable to that of training-based\nBertranX. Furthermore, we update only the seman-\ntic datastore, which can also be effective. We\nalso provide two examples to demonstrate how k-\nnearest-neighbor retrieval assists in model decision-\nmaking in Appendix A.1. It should be noted that\nthe CoNaLa dataset was obtained through data min-\ning, and the majority of the NL-code pairs obtained\nthrough mining are irrelevant, which greatly adds\nnoise to our retrieval. Therefore, we believe that\nkNN-TRANX can perform even better on more\nreliable data sources through incremental learning.\n5 Related Work\nCode generation. Code generation aspires to gen-\nerate target code through natural language to im-\nprove programmers’ development efficiency. Yin\nand Neubig (2018) propose TRANX to generate\nASTs instead of generating code snippets directly.\nBased on TRANX, Beau and Crabbé (2022) pro-\npose BertranX relying on a BERT encoder and a\ngrammar-based decoder. Poesia et al. (2022) pro-\npose a framework for substantially improving the\nreliability of pre-trained models for code genera-\ntion. Chen et al. (2022) propose CodeT to lever-\nage pre-trained language models to generate both\n1297\nCoNaLa dataset\n34\n35\n36\n37\n38\n39\n40\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU\n(a) Ratio of Syntactic Datastore\n34\n35\n36\n37\n38\n39\n40\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU (b) Ratio of Semantic Datastore\n34\n35\n36\n37\n38\n39\n40\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU (c) Ratio of both Datastores\nDjango dataset\n78\n79\n80\n81\n82\n83\n84\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU\n(d) Ratio of Syntactic Datastore\n78\n79\n80\n81\n82\n83\n84\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU (e) Ratio of Semantic Datastore\n78\n79\n80\n81\n82\n83\n84\n0% 20% 40% 60% 80% 100%\nBLEU\nCodeBLEU (f) Ratio of both Datastores\nFigure 5: The impact of datastore size on the BLEU and CodeBLEU scores of CoNaLa dataset and Django dataset.\nWe used three strategies to reduce the datastore size, which are reducing the storage of syntactic knowledge, semantic\nknowledge, and both types of knowledge.\nthe code snippets along with test cases, and select\nthe best code based on the number of test cases\npassed. Besides, researchers used pre-training\nmethods to incorporate more external knowledge\ninto the model, effectively improving its perfor-\nmance on downstream tasks (Feng et al., 2020;\nWang et al., 2021; Ahmad et al., 2021). Recently,\nlarge-scale pre-training models have demonstrated\nremarkable capabilities in code generation (Li et al.,\n2023; Wang et al., 2023; Touvron et al., 2023).\nRetrieval-augmented models. Retrieving and in-\ntegrating auxiliary sentences has shown effective-\nness in enhancing the generation of class models.\nFarajian et al. (2017) propose retrieving similar\nsentences from the training set to adapt to differ-\nent inputs. The work of Hayati et al. (2018) is\nbuilt on the neural network model of AST driver\nand generates code by searching action subtrees.\nParvez et al. (2021) propose using retrieved natu-\nral language code to improve the performance of\ncode generation and code translation models using\nREDCODER.\nRecently, Khandelwal et al. (2021) propose\nkNN-MT, a non-parametric paradigm for construct-\ning a datastore using a decoder representation as\na key and using the corresponding target charac-\nter as a value. The generation is done by retriev-\ning the top k neighbors as the result. Based on\nthis paradigm, a number of optimization methods\nhave also been proposed. Zheng et al. (2021) use a\nmeta-k network to dynamically adjust the retrieval\nweights. Jiang et al. (2022) improve the robustness\nof kNN-MT in terms of both model structure and\ntraining methods. Meng et al. (2022) propose Fast\nkNN-MT, which improves retrieval efficiency by\nconstructing multiple smaller datastores.\n6 Conclusion\nIn this paper, we proposekNN-TRANX. By provid-\ning syntactic and semantic datastores for seq2tree\nmodel, we are able to outperform the baselines.\nIn addition, we provide more knowledge for the\nmodel by switch the datastores without fine-tuning\nthe neural network. Experimental results show that\nkNN-TRANX exhibits competitive performance\nagainst learning-based methods through incremen-\ntal learning. In the future, we plan to construct\na smaller and more fine-grained syntactic datas-\n1298\nModel Only External BLEU CodeBLEU EM Training Cost Extra\nSemantics Data (FLOPs) Storage\nBertranX† - - 29.22 29.89 4.0 1.09 ·1017 -\nkNN-TRANX† - 0 32.25 32.08 4.2 1.11 ·1017 0.2G\nkNN-TRANX† False\n10k 32.95 32.87 4.4 1.14 ·1017 0.5G\n50k 33.71 33.58 4.4 1.23 ·1017 1.3G\n95k 34.04 34.69 4.8 1.35 ·1017 2.2G\nkNN-TRANX† True\n10k 32.28 32.14 4.2 1.14 ·1017 0.3G\n50k 33.25 33.87 4.2 1.23 ·1017 0.5G\n95k 33.89 34.43 4.6 1.35 ·1017 0.8G\nBertranX - - 34.18 36.09 5.8 1.55 ·1018 -\nTable 5: The results of model canonical incremental adaptation. BertranX† is trained on cleaned 2k and mined 5k\ndata. kNN-TRANX† is built on the same size data as a datastore on top of BertranX†. When Only Semantics is set\nto False, both datastores are updated simultaneously, while True means only semantic datastore is updated. External\ndata refers to the quantity of training data updating the datastore.\ntore to reduce the search space of the model and\naccelerate model inference.\nLimitations\nAlthough our proposed approach can enhance the\ngeneralizability of the seq2tree model and enable\nrapid updates by switching datastores, incorporat-\ning extra datastores necessitates a similarity search\nat each time step of inference. Even though only\none of the two datastores needs to be retrieved,\nthe inference time for the model may still increase\nconsiderably (cf. Appendix A.2). Furthermore,\nthe incorporation of additional datastores will con-\nsume storage space and may escalate with growing\ntraining data. Although the previous work has pro-\nposed approaches to reduce the content of datastore\nthrough pruning, this approach also leads to model\nperformance degradation.\nAcknowledgements\nThis work is supported by the National Natural\nScience Foundation of China (No. 61972197, No.\n62372232), the Natural Science Foundation of\nJiangsu Province (No. BK20201292), and the Col-\nlaborative Innovation Center of Novel Software\nTechnology and Industrialization. T. Chen is par-\ntially supported by an overseas grant from the State\nKey Laboratory of Novel Software Technology,\nNanjing University (KFKT2022A03), Birkbeck\nBEI School Project (EFFECT) and National Natu-\nral Science Foundation of China (No. 62272397).\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668,\nOnline. Association for Computational Linguistics.\nNathanaël Beau and Benoit Crabbé. 2022. The impact\nof lexical and grammatical processing on generating\ncode from natural language. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2204–2214, Dublin, Ireland. Association for\nComputational Linguistics.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022.\nCodet: Code generation with generated tests. CoRR,\nabs/2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\n1299\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nYu-Hsiu Dai, Zhirui Zhang, Qiuzhi Liu, Qu Cui, Wei-\nHong Li, Yichao Du, and Tong Xu. 2023. Simple\nand scalable nearest neighbor machine translation.\nArXiv, abs/2302.12188.\nM. Amin Farajian, Marco Turchi, Matteo Negri, and\nMarcello Federico. 2017. Multi-domain neural ma-\nchine translation through unsupervised adaptation. In\nProceedings of the Second Conference on Machine\nTranslation, pages 127–137, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. UniXcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 7212–7225, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nShirley Anugrah Hayati, Raphael Olivier, Pravalika Av-\nvaru, Pengcheng Yin, Anthony Tomasic, and Graham\nNeubig. 2018. Retrieval-based neural code gener-\nation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 925–930, Brussels, Belgium. Association for\nComputational Linguistics.\nHui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou,\nJie Zhou, Degen Huang, and Jinsong Su. 2022. To-\nwards robust k-nearest-neighbor machine translation.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5468–5477, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nNora Kassner and Hinrich Schütze. 2020. BERT-kNN:\nAdding a kNN search component to pretrained lan-\nguage models for better QA. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 3424–3430, Online. Association for Computa-\ntional Linguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nZhizhong Li and Derek Hoiem. 2016. Learning without\nforgetting. In Computer Vision - ECCV 2016 - 14th\nEuropean Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part IV, volume\n9908 of Lecture Notes in Computer Science, pages\n614–629. Springer.\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\naofei Sun, Tianwei Zhang, and Jiwei Li. 2022. Fast\nnearest neighbor machine translation. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 555–565, Dublin, Ireland. Association\nfor Computational Linguistics.\nSajad Norouzi, Keyi Tang, and Yanshuai Cao. 2021.\nCode generation from natural language with less prior\nknowledge and more monolingual data. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 776–785, Online.\nAssociation for Computational Linguistics.\nYusuke Oda, Hiroyuki Fudaba, Graham Neubig,\nHideaki Hata, Sakriani Sakti, Tomoki Toda, and\nSatoshi Nakamura. 2015. Learning to generate\npseudo-code from source code using statistical ma-\nchine translation. In 2015 30th IEEE/ACM Interna-\ntional Conference on Automated Software Engineer-\ning (ASE), pages 574–584.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMd Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,\nBaishakhi Ray, and Kai-Wei Chang. 2021. Retrieval\naugmented code generation and summarization. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 2719–2734, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nGabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Ti-\nwari, Gustavo Soares, Christopher Meek, and Sumit\nGulwani. 2022. Synchromesh: Reliable code gen-\neration from pre-trained language models. arXiv\npreprint arXiv:2201.11227.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\n1300\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. Codebleu: a method\nfor automatic evaluation of code synthesis. CoRR,\nabs/2009.10297.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nDaniel C. Wang, Andrew W. Appel, Jeffrey L. Korn,\nand Christopher S. Serra. 1997. The zephyr ab-\nstract syntax description language. In Proceedings\nof the Conference on Domain-Specific Languages,\nDSL’97, Santa Barbara, California, USA, October\n15-17, 1997, pages 213–228. USENIX.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare,\nNghi DQ Bui, Junnan Li, and Steven CH Hoi. 2023.\nCodet5+: Open code large language models for\ncode understanding and generation. arXiv preprint\narXiv:2305.07922.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nFrank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan\nVasilescu, and Graham Neubig. 2020. Incorporating\nexternal knowledge through pre-training for natural\nlanguage to code generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6045–6052, Online. Asso-\nciation for Computational Linguistics.\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan\nVasilescu, and Graham Neubig. 2018. Learning to\nmine parallel natural language/source code corpora\nfrom stack overflow. In Proceedings of the 40th\nInternational Conference on Software Engineering:\nCompanion Proceeedings, ICSE 2018, Gothenburg,\nSweden, May 27 - June 03, 2018 , pages 388–389.\nACM.\nPengcheng Yin and Graham Neubig. 2018. TRANX:\nA transition-based neural abstract syntax parser for\nsemantic parsing and code generation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 7–12, Brussels, Belgium. Association\nfor Computational Linguistics.\nPengcheng Yin and Graham Neubig. 2019. Reranking\nfor neural semantic parsing. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4553–4559, Florence, Italy.\nAssociation for Computational Linguistics.\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang,\nBoxing Chen, Weihua Luo, and Jiajun Chen. 2021.\nAdaptive nearest neighbor machine translation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 368–374,\nOnline. Association for Computational Linguistics.\nWenhao Zhu, Qianfeng Zhao, Yunzhe Lv, Shu-\njian Huang, Siheng Zhao, Sizhe Liu, and Jia-\njun Chen. 2023. knn-box: A unified framework\nfor nearest neighbor generation. arXiv preprint\narXiv:2302.13574.\nA Appendix\nA.1 Case Study on Model Canonical\nIncremental Adaptation\nWe showcase two selected instances from the\nCoNaLa dataset in Table 6. For each instance, we\npresent the output and probability distribution of\nthe model when generating incorrect behavior. We\nalso demonstrate how our method utilizes the kNN\nalgorithm to enhance the model’s decision-making\nprocess, thereby proving the effectiveness of our\nmethod. As shown in the first example, BertranX†\ndemonstrates the ability to construct code with ap-\npropriate syntax, but it inaccurately generates the\nprimitive legend. However, with the assistance of\nkNN-TRANX†, the final decision of the model\nchanges through retrieval, resulting in the desired\ncode. In the second example, BertranX † makes\nan error when generating the second token. kNN-\nTRANX†, utilizing an updated syntactic datastore,\nproduces code that is semantically close to the tar-\nget code. In contrast, kNN-TRANX †, which did\nnot update the syntactic datastore, has drawbacks\nin the generation of correct code due to insuffi-\ncient syntactic node information, despite having\nimproved retrieval efficiency and datastore size.\n1301\nNL: Plot dataframe df without a legend.\nTarget code: df.plot(legend=False)\nTarget action sequence: [Expr, Call, Attribute, Name, df, Load, plot, Load, Reduce, keyword, legend, NameConstant, ··· ]\nModel Predicted Code Predicted Action Sequence pNN pkNN λ p(y|x)\n♣\ndf.plot(kind=‘bar‘,\n[··· keyword, kind ··· ]\np(kind) = 0.63\n- -\np(kind) = 0.63\nlegend=0) p(legend) = 0.21 p(legend) = 0.21\n♢ df.plot(legend=False) [··· keyword, legend ··· ]\np(kind) = 0.63 p(kind) = 0.12\n0.38\np(kind) = 0.44\np(legend) = 0.21 p(legend) = 0.88 p(legend) = 0.46\n♠ df.plot(legend=False) [··· keyword, legend ··· ]\np(kind) = 0.63 p(kind) = 0.12\n0.41\np(kind) = 0.42\np(legend) = 0.21 p(legend) = 0.88 p(legend) = 0.48\nNL: Get rid of none values in dictionary kwargs.\nTarget code: res = {k: v for k, v in list(kwargs.items()) if v is not None}\nTarget action sequence: [Assign, Name, res, Store, Reduce, DictC, Name, k, Load, Name, v, Load, comprehension, ··· ]\nModel Predicted Code Predicted Action Sequence pNN pkNN λ p(y|x)\n♣ kwargs.values() [Expr, Call ··· ]\np(Call) = 0.43\n- -\np(Call) = 0.43\np(DictC) = 0.38 p(DictC) = 0.38\np(Subs) = 0.14 p(Subs) = 0.14\n♢\n{k: v for k, v in\n[Expr, DictC ··· ]\np(Call) = 0.43 p(Call) = 0.05\n0.47\np(Call) = 0.25\nlist(kwargs.items()) p(DictC) = 0.38 p(DictC) = 0.95 p(DictC) = 0.65\nif v is not None} p(Subs) = 0.14 p(Subs) = 0 p(Subs) = 0.07\n♠ kwargs.items()[0] [Expr, Subs ··· ]\np(Call) = 0.43 p(Call) = 0.21\n0.36\np(Call) = 0.35\np(DictC) = 0.38 p(DictC) = 0 p(DictC) = 0.14\np(Subs) = 0.14 p(Subs) = 0.79 p(Subs) = 0.37\nTable 6: The case study of model canonical incremental adaptation. ✓ represents the correct target code. ♣is the\ngenerated result of BertranX†. ♢and ♠are the generated results of updating or not updating the syntactic datastore\nwith 95k external data by kNN-TRANX†. The actions enclosed in the red boxes indicate the generated errors. We\npresent the probability distribution of model decisions during the generation of these erroneous tokens.\nModel Decoding Time (ms/code)\nBertranX 357.5(×1.00)\nkNN-MT 1258.4(×3.52)\nkNN-TRANX 722.2(×2.02)\nTable 7: Decoding time of different models.\nA.2 Decoding Time\nWe compared the decoding time of BertranX,kNN-\nMT, and kNN-TRANX using the CoNaLa test set.\nDuring decoding, the beam size was set to 15. The\nresults are presented in Table 7.\n1302"
}