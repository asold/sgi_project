{
  "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
  "url": "https://openalex.org/W4389519059",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2146808361",
      "name": "Zhuoyan Li",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2342819269",
      "name": "Hangxiao Zhu",
      "affiliations": [
        "Washington University in St. Louis"
      ]
    },
    {
      "id": "https://openalex.org/A2805758565",
      "name": "Zhuoran Lu",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2099962107",
      "name": "Ming Yin",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174519801",
    "https://openalex.org/W2971136144",
    "https://openalex.org/W4404826238",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W4318832809",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285310604",
    "https://openalex.org/W4288058308",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W4366549000",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W4226125322",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4285251400",
    "https://openalex.org/W4225006216",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4372046852",
    "https://openalex.org/W4366587430",
    "https://openalex.org/W3162670395",
    "https://openalex.org/W3023708404",
    "https://openalex.org/W4385570982",
    "https://openalex.org/W4226348722",
    "https://openalex.org/W3011199201",
    "https://openalex.org/W3167788848",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2133341045",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W4309618884",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W3212327893",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4306705078"
  ],
  "abstract": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10443–10461\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSynthetic Data Generation with Large Language Models for Text\nClassification: Potential and Limitations\nZhuoyan Li1, Hangxiao Zhu2, Zhuoran Lu1, Ming Yin1\n1Purdue University\n2Washington University in St. Louis\n{li4178, lu800, mingyin}@purdue.edu\nhangxiao@wustl.edu\nAbstract\nThe collection and curation of high-quality\ntraining data is crucial for developing text clas-\nsification models with superior performance,\nbut it is often associated with significant costs\nand time investment. Researchers have recently\nexplored using large language models (LLMs)\nto generate synthetic datasets as an alternative\napproach. However, the effectiveness of the\nLLM-generated synthetic data in supporting\nmodel training is inconsistent across different\nclassification tasks. To better understand fac-\ntors that moderate the effectiveness of the LLM-\ngenerated synthetic data, in this study, we look\ninto how the performance of models trained on\nthese synthetic data may vary with the subjec-\ntivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance\nlevel, is negatively associated with the perfor-\nmance of the model trained on synthetic data.\nWe conclude by discussing the implications of\nour work on the potential and limitations of\nleveraging LLM for synthetic data generation1.\n1 Introduction\nToday, machine-learning-powered text classifica-\ntion models have been widely applied in diverse\napplications such as detecting biased or toxic lan-\nguage on online platforms (Wiegand et al., 2019)\nand filtering spam emails (Jindal and Liu, 2007).\nHowever, the performance of these models largely\ndepends on the quality of the training data. This\nposes a substantial challenge in practice, especially\nwhen models need to be built for a novel task do-\nmain or to incorporate new classification categories,\nas the training data collection and curation process\nis often costly, time-consuming, and complex.\nMeanwhile, with the recent advancements in\nlarge language models (LLMs), researchers have\nstarted to explore the potential of utilizing LLMs\nfor generating synthetic data tailored to specific\n1The collected human annotations are available at\nhuggingface.co/datasets/xfleezy/human_annotation_emnlp23.\ntasks and augmenting the training data in low-\nresourced data settings (Kumar et al., 2020; Yoo\net al., 2021; Hartvigsen et al., 2022; Sahu et al.,\n2022). Most recently, a few studies also investi-\ngate into the feasibility of generating a synthetic\ndataset from scratch using LLMs to support zero-\nshot learning (Ye et al., 2022; Wang et al., 2021;\nTang et al., 2023; Gao et al., 2023). While LLM-\nbased data augmentation is often found to outper-\nform other data augmentation methods in boosting\nthe model performance, mixed results are reported\nregarding whether the LLM-generated synthetic\ndata can effectively support model training to en-\nable a level of model performance that is compara-\nble to models trained on the data collected in the\nreal world and carefully annotated. This leaves\nuncertainty for researchers and practitioners in de-\nciding whether to rely on LLMs for synthetic data\ngeneration or to proceed with the traditional data\ncollection and curation pipeline when they need to\nconstruct a text classification model for a new task.\nNaturally, one may wonderwhat factors might mod-\nerate the effectiveness of LLM-generated synthetic\ndata in facilitating successful model training.\nWe conjecture that one such factor could be the\nsubjectivity of classification tasks. Indeed, lan-\nguage is inherently subjective and interpretive (Ben-\nveniste, 1971; Wiebe et al., 2004). Previous re-\nsearch has showed that people often perceive the\nsame text in different ways because of their per-\nsonal biases and perspectives (Sap et al., 2021; Li\net al., 2022; Gordon et al., 2022). Thus, achiev-\ning high model performance for classification tasks\nwith high subjectivity seems to impose a greater\ndemand on the training data in reflecting the rich-\nness and nuances present in human language, and\nthe extent to which LLM-generated synthetic data\ncan acompolish this objective is unclear.\nThus, in this paper, we formally evaluate the\neffectiveness of LLM (i.e., the cutting-edge GPT-\n3.5-Turbo model) in generating synthetic data to\n10443\nsupport model training for different text classifica-\ntion tasks. We adopt two approaches for synthetic\ndata generation—a zero-shot setting in which the\nLLM is directly prompted to generate text instances\nwith different labels of interests, and a few-shot\nsetting in which a few real-world data instances\nare provided as examples to guide the LLM in\ngenerating the synthetic data. We conduct two\nevaluation studies, each corresponding to one di-\nmension of subjectivity—the first study examines\nthe effectiveness of the synthetic data on 10 types\nof classification tasks and explores how it varies\nwith the task-level subjectivity (i.e., whether this\ntype of classification task is subjective); the second\nstudy concerns that given a specific classification\ntask, how the performance of a model trained on\nsynthetic data changes with the instance-level sub-\njectivity (i.e., whether people tend to disagree with\neach other on the label of this task instance). Our\nfindings suggest that across the 10 types of classifi-\ncation tasks that we have considered in this study,\nmodels trained on the LLM-generated synthetic\ndata generally perform worse than those trained on\nthe real-world data, yet guiding LLM’s synthetic\ndata generation process with a small amount of\nreal-world data (i.e., as done in the few-shot data\ngeneration setting) can improve the effectiveness of\nthe data generated. Moreover, we find that the per-\nformance of models trained on the LLM-generated\nsynthetic data is very close to those trained on the\nreal-world data for tasks with low subjectivity (e.g.,\nnews topic classification, spam email detection),\nwhile the performance decrease is much bigger on\ntasks with high subjectivity (e.g., humor or sar-\ncasm detection). Finally, even within the same type\nof classification task, models trained on the LLM-\ngenerated synthetic data tend to exhibit a higher\nlevel of performance on those task instances with\nlower subjectivity, for which human annotators ex-\nhibit a higher level of agreement in their annotation.\nTogether, our study provides important experi-\nmental evidence regarding the potential and limi-\ntations of using LLMs to generate synthetic data\nfor text classification tasks. We conclude by dis-\ncussing the implications, limitations, and future\nwork of our study.\n2 Related Work\nGenerative AI in synthetic data generation. Re-\ncent advancements in generative AI have motivated\nnumerous studies to explore the potential of lever-\naging generative models to create synthetic data\nfor training machine learning models, especially\nfor computer vision (CV) and natural language\nprocessing (NLP) tasks. In the realm of CV , sev-\neral works have utilized GAN-based models (Kar-\nras et al., 2019) or diffusion models (Nichol et al.,\n2021) to generate synthetic data for image recogni-\ntion (Besnier et al., 2020; He et al., 2022) or object\nsegmentation (Zhang et al., 2021). Similarly, in the\nNLP field, researchers have also probed into the ca-\npacity of language models in generating synthetic\ndata for various text classification tasks (Kumar\net al., 2020; Chung et al., 2023; Sahu et al., 2022;\nYoo et al., 2021; Ye et al., 2022; Wang et al., 2021),\nwith mixed results reported regarding the effective-\nness of the synthetic data generated. In this study,\nwe aim to obtain a better understanding ofwhen the\nsynthetic data generated by language models can\nlead to effective model training, and we focus on\nexploring the role of task subjectivity in moderating\nthe effectiveness of the synthetic data.\nLarge language models. Based on the Trans-\nformer architecture (Vaswani et al., 2017), large\nlanguage models (LLMs) have facilitated remark-\nable progress in the field of natural language pro-\ncessing. The utilization of bidirectional contexts\nin the BERT model (Devlin et al., 2018) has re-\nsulted in superior performance across a wide range\nof tasks. Building on this, OpenAI’s GPT series,\ncomprising of models like GPT-2 (Radford et al.,\n2019), the colossal GPT-3 (Brown et al., 2020)\nwith an impressive 175 billion parameters and the\nmost recent GPT-4 (OpenAI, 2023), pushed the\nboundaries of possibilities of LLMs. These mod-\nels exhibit remarkable proficiency in generating\nhigh-quality human-like text (Clark et al., 2021;\nDou et al., 2021; Zhou et al., 2023), showcasing\ncapabilities in rudimentary reasoning (Wei et al.,\n2021), translation (Brown et al., 2020), scientific\nsynthetic data generation (Hämäläinen et al., 2023),\nand code generation (Mcnutt et al., 2023). In this\nstudy, we focus on leveraging the cutting-edge GPT-\n3.5-Turbo model2 to explore its capabilities and\nlimitations in synthesizing data for text classifica-\ntion tasks with different subjectivity levels.\n2We used GPT-3.5-Turbo as the foundational model to\ngenerate synthetic data because at the time of this study, an\nofficial API for the more advanced GPT-4 model was not yet\navailable from OpenAI.\n10444\n3 Methodolgy\nIn this section, we outline the procedure we have\nfollowed when leveraging the large language model\nto generate the synthetic training data for text clas-\nsification. We consider two data generation settings\nin this study, i.e., the zero-shot setting and the few-\nshot setting.\n3.1 Zero-shot Synthetic Data Generation\nUnder the zero-shot synthetic data generation set-\nting, given a text classification task, we assume\nthat the real-world data in the form of “text-label\npairs” do not exist. Thus, in order to obtain syn-\nthetic training data for the text classification task,\ntwo sequential prompts are constructed and sup-\nplied to the pretrained large language model (i.e.,\nthe GPT-3.5-Turbo model). First, a customized\n“context prompt” relevant to the targeted domain of\ninterest is used to set the context. For example, in\nthe case of the IMDB movie review classification\ntask (Maas et al., 2011), the customized context\nprompt used is “Imagine you are a movie reviewer\non the IMDB platform”. This prompt aims to en-\ncourage the LLM to generate synthetic data that\nresemble the real texts produced in the targeted\ndomain. After the context is set, a second prompt,\ni.e., the “data generation prompt”, is provided to\nthe LLM, instructing the model to generate texts\nwith a specific style, label (with respect to the clas-\nsification task of interest), and word limit. For\nexample, for the IMDB movie review classification\ntask, the style of the text is a movie review, and\nthe label is a targeted sentiment conveyed by the\nreview (i.e., “positive” or “negative”). To further\nenhance the diversity of the generated data, after\nthe generation of every ndata points (i.e., texts of\ntargeted styles along with their labels) 3, we pro-\nvide a “diversity prompt” to the LLM—“Can you\nprovide something more diverse compared to the\npreviously generated data?”—aiming to increase\nthe diversity of the synthetic data generated.\n3.2 Few-shot Synthetic Data Generation\nUnder the few-shot synthetic data generation set-\nting, we assume that a small amount of real-world\ndata are available for the text classification task.\nThese data points can then serve as the examples\n3To increase data diversity while maintaining a reasonable\ndata generation speed, n is set to 10 for generating short texts\n(i.e., texts with a maximum length of 30 words), and 1 for\ngenerating longer paragraphs.\nfor the large language model in the data generation\nprocess, which can potentially provide LLM with\ninsights of the patterns exhibited in the real-world\ndata. We again start the data generation process by\nusing a context prompt to set the context. However,\ndifferent from that in the zero-shot setting, here,\neach time before we instruct the LLM to generate\na piece of text, we first provide the model with a\nfew randomly sampled real-world data instances\n(including both the text and the label) as the exam-\nples. To keep the LLM from merely rephrasing the\nprovided examples, an additional prompt is used to\nimpose a constraint on the LLM in generating the\nsynthetic data (i.e., “You should imitate the exam-\nple I have provided, but you cannot simply modify\nor rewrite the example I have given.”).\nFor more details about prompts used for gener-\nating data for each type of text classification task,\nplease refer to the App. D.\n4 Evaluation I: Comparison Across\nDifferent Types of Tasks\nIn our first evaluation study, we investigate into how\nwell the synthetic data generated by LLM under\nboth zero-shot and few-shot settings can support\neffective model training for different types of text\nclassification tasks. We are especially interested in\ncomparing the model performance between those\ntrained on the real-world data and on the LLM-\ngenerated synthetic data, and in understanding how\nthe performance of those models trained on the\nLLM-generated synthetic data varies with the sub-\njectivity of the text classification task.\n4.1 Datasets and Tasks\nWe experiment with 10 representative datasets\ncovering a variety of text classification tasks:\nAG’s news (Zhang et al., 2015b), IMDB reviews\n(Maas et al., 2011), SMS spam (Almeida et al.,\n2011), Financial phrase bank (Malo et al., 2014),\nReddit emotion (Demszky et al., 2020), Rela-\ntion classification (Gao et al., 2019), Tweet irony\nspeech (Van Hee et al., 2018), Tweet emotions (Mo-\nhammad et al., 2018), Sarcasm news (Misra and\nArora, 2023, Misra and Grover, 2021), and Humor\nspeech (Annamoradnejad and Zoghi, 2020). See\nApp. A.1 for detailed descriptions of datasets and\nthe corresponding text classification tasks. These\ndatasets are selected with the goal of spanning a\nwide range of task subjectivity in mind. For exam-\nple, we conjecture that classifying the news topic\n10445\ncategory (e.g., as that in the AG’s news dataset)\nis relatively objective, while determining whether\ntexts are humorous (e.g., as that in the Humor\nspeech dataset) is quite subjective (Veatch, 1998).\n4.2 Task-level Subjectivity Determination\nTo formally determine the subjectivity levels of dif-\nferent text classification tasks, we first conduct a\ncrowdsourced study to collect subjectivity judge-\nments from the crowd.\nStudy procedure. We adopt a comparative ap-\nproach to collect crowdsourced subjectivity judge-\nments in this study. Specifically, we recruited\ncrowd workers from Amazon Mechanical Turk\n(MTurk), and each worker was asked to complete\na sequence of 10 subjectivity judgement tasks. In\neach task, we randomly sampled a pair of text clas-\nsification tasks from the 10 tasks that we considered\nin this evaluation, and we presented to the worker\nthe task description, label description, and task ex-\namples for each task in the pair. Then, the worker\nwas asked to determine which text classification\ntask in the pair was more objective, with “objec-\ntivity” of a task defined as “the classification of a\npiece of text is based on clear, identifiable features\nin the text (e.g., keywords or phrases), and can be\ndone without being affected by any personal inter-\npretation of the text resulted from personal biases,\nemotions or beliefs.” The study was restricted to\nU.S. workers. Each worker was allowed to partic-\nipate only once and received a $1.2 payment. An\nattention check question was included in the study\nto validate the worker’s engagement, and only the\ndata from workers who successfully passed the at-\ntention check were considered valid.\nRanking task subjectivity. After excluding re-\nsponses from inattentive workers, a total of 540\npairwise subjectivity comparisons for the 10 tasks\nwere obtained from 54 workers. For each pair\nof tasks, we aggregated relative subjectivity judg-\nments made on this pair to determine which task\nwas perceived as more subjective (i.e., less objec-\ntive). To produce a ranking of the subjectivity of\nthe 10 tasks, we constructed a directed graph based\non the pairwise subjectivity comparisons—each\ntask was a node in this graph, and directed edges\nwere added between each pair of tasks, pointing\nfrom the one that was deemed as more subjective\n(on the aggregate level) to the one deemed as less\nsubjective. The topological sort algorithm (Cormen\net al., 2022) was then applied to this directed graph\nto obtain a linear ordering of the nodes. If a cycle\nwas detected within the graph, the corresponding\ntasks were considered to have the same level of\nsubjectivity and were merged into a single meta-\nnode before re-runing the algorithm. Our final task\nsubjectivity ranking results are shown in Table 1.\n4.3 Model Training\nGiven a text classification task, following the pro-\ncedures outlined in Sections 3.1 and 3.2, 3,000 syn-\nthetic data points were generated for each candidate\nlabel under both zero-shot and few-shot settings.\nWe then trained classification models using the real-\nworld training data provided by the original dataset,\nthe synthetic data generated under the zero-shot\nsettings, and the synthetic data generated under the\nfew-shot settings4, respectively. Specifically, we\nutilized the pre-trained BERT (Devlin et al., 2018)\nand RoBERTa (Liu et al., 2019) models from Hug-\ngingface’s transformers library (Wolf et al., 2020)\nas the encoders, and used the representation em-\nbeddings from the last layer of these models as the\ninput to our classification models. The classifica-\ntion model itself comprised a hidden layer of 768\nunits and an output layer, and it was fine-tuned with\na learning rate of 5e−5 and a batch size of 64. For\ndatasets that provided official partitions for training\nand test sets, we directly evaluated the classifica-\ntion model’s performance on the test sets. Other-\nwise, we randomly divided the dataset into training\n(70%), validation (5%), and test (25%) sets5. Mod-\nels’ performance was evaluated via Macro-F1 and\nAccuracy scores, and they were computed by com-\nparing the model’s predictions with the gold labels\nprovided in the test sets. To ensure the robustness\nof our results, all experiments were repeated three\ntimes, and the average performance across these\nrepetitions was reported.\n4.4 Evaluation Results\nTable 1 summarizes the comparative performance\nof classification models trained with different data.\nBelow, we highlight a few key observations we get\nfrom this comparison.\n4Under the few-shot setting, we randomly sampled 10%\nof the data points from the real-world training data provided\nin the original dataset as the example pool to guide the LLM’s\nsynthetic data generation process, but only the sythetic data\ngenerated were used to train the models.\n5To ensure a fair comparison, we maintained an equal\nsize for both the real-world and synthetic training data by\ndownsampling the dataset with a larger size.\n10446\nDataset Subjectivity BERT RoBERTaReal-world data Zero-shot setting Few-shot setting Real-world data Zero-shot setting Few-shot settingMacro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy ScoreAG ⋆ 95.3% 95.3% 89.3% (-6.0%) 89.3% (-6.0%) 91.5% (-3.8%) 91.6% (-3.7%) 94.6% 94.6% 88.6% (-6.0%) 88.6% (-6.0%) 92.9% (-1.7%) 92.9% (-1.7%)Relation⋆⋆ 98.6% 98.6% 92.4% (-6.2%) 92.7% (-5.9%) 96.4% (-2.2%) 96.4% (-2.2%) 97.0% 96.9% 91.4% (-5.6%) 91.6% (-5.3%) 94.1% (-2.9%) 94.1% (-2.8%)IMDB ⋆⋆⋆ 87.6% 87.6% 81.2% (-6.4%) 81.5% (-6.1%) 81.1% (-6.5%) 81.2% (-6.4%) 89.0% 89.0% 81.2% (-7.8%) 81.3% (-7.7%) 82.4% (-1.6%) 82.4% (-1.6%)SMS spam⋆⋆⋆⋆ 97.2% 98.8% 93.8% (-3.4%) 95.1% (-3.7%) 94.3% (-2.9%) 94.8% (-4.0%) 97.3% 98.8% 93.5% (-3.8%) 95.9% (-2.9%) 94.0% (-3.3%) 95.7% (-3.1%)Reddit emotion⋆⋆⋆⋆⋆ 93.7% 94.6% 72.7% (-21.0%) 74.4% (-20.2%) 81.9% (-11.8%) 82.0% (-12.6%) 91.3% 92.1% 77.9% (-13.4%) 78.1% (-14.0%) 87.5% (-3.8%) 87.7% (-4.4%)Tweet irony⋆⋆⋆⋆⋆ 72.2% 73.9% 63.4% (-8.8%) 63.6% (-10.3%) 81.5% (+9.3%) 81.9% (+8.0%) 74.0% 75.5% 57.8% (-16.2%) 59.1% (-16.4%) 83.3% (+9.3%) 83.7% (+8.2%)Tweet emotions⋆⋆⋆⋆⋆ 77.7% 81.1% 58.1% (-19.6%) 64.5% (-16.6%) 64.6% (-13.1%) 69.1% (-12.0%) 75.8% 78.9% 64.6% (-11.2%) 71.5% (-7.4%) 66.3% (-9.5%) 72.7% (-6.2%)Sarcasm⋆⋆⋆⋆⋆ 89.9% 90.3% 51.1% (-38.8%) 51.2% (-39.1%) 63.6% (-26.3%) 64.8% (-25.5%) 91.8% 92.0% 54.3% (-37.5%) 54.3% (-37.7%) 61.5% (-30.3%) 63.6% (-28.4%)Financial⋆⋆⋆⋆⋆ 83.2% 84.6% 48.2% (-35.0%) 60.7% (-23.9%) 70.6% (-12.6%) 74.2% (-10.4%) 85.0% 86.6% 58.5% (-26.5%) 70.3% (-16.3%) 75.0% (-10.0%) 78.9% (-7.7%)Humor speech⋆⋆⋆⋆⋆ 97.0% 97.0% 56.0% (-41.0%) 61.7% (-35.3%) 86.9% (-10.1%) 87.0% (-10.0%) 96.7% 96.7% 54.9% (-41.8%) 60.9% (-35.8%) 84.0% (-12.7%) 84.0% (-12.7%)\nTable 1: Comparing the performance of classification models trained on the LLM-generated synthetic data under the\nzero-shot or few-shot settings, with those trained with the original real-world data, in terms of Macro-F1 (%) and\nAccuracy Score (%). In the “Subjectivity” column, more \"⋆\" symbols indicate a higher level of task subjectivity.\nModels trained on the real-world data consis-\ntently outperform those trained on the synthetic\ndata. Our results indicate that models trained on\nthe original real-world data consistently outper-\nform their counterparts trained on the synthetic data\ngenerated under either zero-shot or few-shot set-\ntings, almost for every task. In particular, with the\nRoBERTa model, we observe that the average im-\nprovements of the model trained on the real-world\ndata over the models trained on zero-shot synthetic\ndata and few-shot synthetic data are 16.9% and\n6.7% in terms of Macro-F1, and 14.9% and 6.1%\nin terms of accuracy. Similar trends are observed\nwith the BERT model as well.\nGuiding LLM with real-world data examples\ncan boost the effectiveness of the synthetic data.\nWe also observe that models trained on those syn-\nthetic data generated under the few-shot settings\nalmost always outperform those trained on the syn-\nthetic data generated under the zero-shot settings.\nFor instance, for the BERT model, we see an aver-\nage increase of 10.6% and 8.8% in Macro-F1 and\naccuracy scores, respectively, across the 10 tasks in\nthe few-shot setting, as compared to the zero-shot\nsetting. Similarly, with the RoBERTa model, there\nis an average increase of 10.3% in Macro-F1 and\n8.9% in accuracy scores across the 10 tasks when\nthe real-world data are used as examples for LLM\nto mimic in the synthetic data generation process.\nFor more analysis of the few-shot synthetic data,\nplease see App. B.2 and B.3.\nSynthetic data support more effective model\ntraining for tasks that are less subjective.Finally,\nwe notice that for classification tasks with relatively\nlow levels of subjectivity (e.g., those in the AG’s\nnews, Relation classification, IMDB reviews, and\nSMS spam datasets), the performance difference\nbetween models trained on the synthetic data and\nthose trained on the real-world data is remarkably\nsmall. However, for tasks with high subjectivity,\nthe performance decrease resulted from the usage\nof the synthetic data is more significant—for in-\nstance, across the cluster of 6 tasks with the highest\nlevel of subjectivity in our evaluation, there is an\naverage decrease of 27.4% and 24.2% in Macro-F1\nand accuracy, respectively, comparing the BERT\nmodels trained on the zero-shot synthetic data with\nthose trained on the real-world data. In other words,\nfor text classification tasks that are highly objective,\nthere is great potential in training high-performing\nmodels simply based on synthetic data generated\nby LLMs, but the same method falls short in gen-\nerating synthetic data that can effectively support\nmodel training for highly subjective classifications.\nFor more robustness check of this finding (e.g., on\nmore datasets, when using different LLMs for data\ngeneration, when using alternative data generation\npipelines), see App. B.4–B.10 for details.\n4.5 Exploratory Analysis: Data Diversity\nTo explore the potential reasons underlying the\nmodel performance difference, we conducted an\nexploratory analysis on the diversity of the training\ndata. Following Rhys Cox et al. (2021), we used\nthe Remote Clique Score (i.e., the average mean\ndistance of a data instance to other instances) and\nthe Chamfer Distance Score (i.e., the average mini-\nmum distance of a data instance to other instances)\nto quantify the diversity of a set of data. For both\nmetrics, higher values indicate greater data diver-\nsity. As shown in Figure 1, we find that in general,\nthe real-world data appear to be more diverse than\nthe synthetic data generated under the few-shot set-\ntings, which in turn seem to be more diverse than\nthe zero-shot synthetic data. This might partially\nexplain why models trained on the real-world data\nand the few-shot synthetic data tend to outperform\nthose trained on the zero-shot synthetic data.\nIn addition, we also notice that compared to that\non the low subjectivity tasks (i.e., AG, Relation,\n10447\nDataset AG Relation IMDB SMS Spam Reddit Emotion Humor Speech Tweet Irony Sarcasm Tweet Emotions Finanical\nAverage Agreementa 0.80 (4.2) 0.78 (4.5) 0.76 (7.3) 0.73 (8.5) 0.69 (6.6) 0.68 (7.1) 0.68 (6.7) 0.64 (7.7) 0.64 (4.6) 0.57 (7.6)\nKrippendorff’sα 0.51 0.43 0.19 0.27 0.30 0.06 0.03 0.01 0.17 -0.03\nSubjectivity Level⋆ ⋆⋆ ⋆⋆⋆ ⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆\nTable 2: The average instance-level annotation agreement for different types of tasks, alongside the corresponding\ntask-level subjectivity. Numbers in parentheses in the first row represent the average number of annotations received\nper task instance. Higher values for both the average agreement aand Krippendorff’s αindicate a higher degree\ninter-annotator agreement.\n(a) Remote Clique\n (b) Chamfer Distance\nFigure 1: Comparing the diversity of the real-world data\nand the synthetic data.\nIMDB, Spam), the differences in data diversity\nbetween the real-world data and the synthetic data\nseem to be more salient on the high subjectivity\ntasks (i.e., the other 6 tasks), especially in terms\nof the Chamfer Distance Score. In fact, a t-test\nshows that the decrease of the Chamfer Distance\nScore in the zero-shot synthetic data compared to\nthe real data is significantly larger for the high\nsubjectivity tasks than for the low subjectivity tasks\n(p< 0.01). This suggests that for tasks with high\nsubjectivity, such as interpreting humor or sarcasm\nin language, LLMs may not be able to generate data\ninstances that can cover the full spectrum of real-\nlife scenarios, which may limit the performance of\nmodels trained on the synthetic data.\n5 Evaluation II: Comparison Across\nDifferent Task Instances\nIn the previous section, we have discovered that\nthe subjectivity of a task can adversely affect the\nperformance of classification models trained on the\nLLM-generated synthetic data. However, even for\nthe same type of task, the classification for each\nindividual task instance may exhibit different levels\nof subjectivity as well. Naturally, one may won-\nder whether models trained on the LLM-generated\nsynthetic data may show different performance on\ntask instances of different subjectivity. We aim to\nexplore the answers to this question in this section.\n5.1 Instance-level Subjectivity Determination\nGiven a text classification task and a specific text in-\nstance, we consider the degree ofagreement among\nannotators on the label of this text as a proxy for\nthe subjectivity of this instance—a lower level of\nagreement means that annotators hold more diver-\ngent views, hence the task may have a higher level\nof subjectivity. Thus, to formally quantify the sub-\njectivity of different instances for different tasks,\nwe again conduct a crowdsourced study to collect\ninstance-level annotations.\nStudy procedure. We again considered the 10\ntypes of text classification tasks as that in the first\nevaluation study. For each type of task, we ran-\ndomly sampled 50 text instances per category from\nthe test set to compose our “evaluation dataset” for\nthat task. We then recruited U.S. workers from\nMTurk to complete annotation tasks for those in-\nstances in our evaluation dataset. Specifically, each\nworker was randomly assigned to one type of text\nclassification tasks. After going through a brief in-\nstruction of the assigned task, the worker was asked\nto complete 20 classification tasks of the assigned\ntype to get a payment of $1.2, where the texts pre-\nsented in these 20 tasks were randomly sampled\nfrom the evaluation dataset for the assigned type of\ntask. Again, we included two attention check ques-\ntions in our study to filter out inattentive workers.\nWe ensured that each task instance received at least\nthree annotations from unique MTurk workers.\nComputing instance subjectivity. Based on an-\nnotations we obtained from attentive workers, we\nquantify the subjectivity level of each task instance\nusing the fraction of annotators who agree with the\nmajority label for the task instance, that is:\nai = maxy∈Y\n∑Ki\nk=1 1(rk\ni = y)\nKi\n(1)\nwhere Y = {1,··· ,Y }is the set of all possible\nlabels, Ki is the total number of annotators who la-\nbeled instance i, and rk\ni is the k-th annotator’s anno-\ntation on instance i. Intuitively, a lower value of ai\n10448\n(a) AG\n (b) Relation\n (c) IMDB Reviews\n (d) SMS Spam\n (e) Reddit Emotion\n(f) Sarcasm News\n (g) Humor Detection\n (h) Tweet Emotions\n (i) Tweet Irony Speech\n (j) Financial Phrasebank\nFigure 2: Changes in the accuracy of the BERT model trained on zero-shot synthetic data as the instance-level\nannotation agreement threshold varies. The solid blue line in each plot is the linear regression fitted on the data,\nand the R-squared score quantifies the goodness of fit. The Spearman’s ρassesses the strength of rank correlation\nbetween the instance-level agreement threshold and the model accuracy for each task. Higher values for both R-\nsquared and Spearman’s ρ, ideally close to 1, indicate a stronger monotonic relationship between the instance-level\nsubjectivity and the model accuracy.\nsuggests that consensus is less likely to be reached\namong annotators on instancei, thus instance imay\nhave a higher level of subjectivity. In Table 2, we\nreport the average values ofai (i.e., a) for instances\nin the evaluation datasets of different types of tasks,\nalong with the average inter-annotator agreement\non each task instance (as measured by the Krip-\npendorff’s α) as well as the task-level subjectivity\nlevel for different types of tasks. We can see that\na closely aligns with the Krippendorff’s α, and\ntasks with higher levels of subjectivity also exhibit\na higher value of ain general, indicating that ai\ncan potentially serve as a reasonable proxy for the\nsubjectivity of each task instance.\n5.2 Evaluation Results\nWe now look into whether models trained on the\nLLM-generated synthetic data exhibit different per-\nformance on instances with different levels of sub-\njectivity, and we focus on the models trained on\nzero-shot synthetic data in this evaluation. Specifi-\ncally, given a classification task, we trained a BERT\nmodel using the zero-shot synthetic data and com-\nputed its accuracy on the subset of task instances\nin the evaluation dataset whose instance-level an-\nnotation agreement (i.e., ai) exceeds a threshold γ,\nand we repeated this computation for many times\nas we varied the value of γ.\nFigure 2 illustrates how the model accuracy\nvaries with the instance-level annotation agreement\nthreshold γ for different types of tasks. For most\ntasks (except for the tasks in the Scarcasm News\nand Finanical Phrasebank datasets), we observe a\nstrong monotonically increasing relationship be-\ntween γand the model accuracy, with correlations\nbetween them (i.e., β) being positive and values of\nthe Spearman’s rank correlation coefficient ρoften\nexceeding 0.85. Since increasing the instance-level\nannotation agreement threshold γeffectively filters\nout task instances with high subjectivity, this ob-\nservation suggests that models trained on synthetic\ndata indeed tend to have varying performance on\ndifferent instances—even within the same type of\ntasks, these models still perform better on those\ntask instances with low subjectivity.\nAs a comparison, we also investigate into\nwhether models trained on the real-world data ex-\nhibit similar behaviors. The detailed results are\nreported in App. C. On the high level, while we\nalso observe the trend that these models’ perfor-\nmance appears to increase as the instance-level task\nsubjectivity decreases, such relationship is usually\nweaker than that illustrated in the models trained\non the synthetic data (e.g., βand ρare smaller).\n6 Conclusions and Discussions\nIn this paper, we present an initial exploration into\nfactors that moderate the effectiveness of LLM-\ngenerated synthetic data for facilitating the training\nof text classification models. Our results show that\n10449\nthe performance of the models trained on synthetic\ndata decreases both for classification tasks with\nhigher levels of subjectivity and on task instances\nwith higher subjectivity. In this section, we provide\nsome potential explanations for the observations of\nour study, and discuss the implications, limitations,\nand future directions of our work.\n6.1 Why subjectivity adversely impacts the\neffectiveness of the synthetic data?\nWe provide a few explanations for why task sub-\njectivity is found to be negatively associated with\nthe performance of models trained on the LLM-\ngenerated synthetic data. First, highly subjective\ntasks often require a deep understanding of nuanced\nhuman emotions and contextual subtleties, as well\nas the ability to discern and accurately interpret dif-\nferent perspectives. As such, LLMs may encounter\nlimitations in generating data that can capture the\nextensive range and complexity of real-life use of\nlanguage. Indeed, as shown in our exploratory\nanalysis in Section 4.5, the diversity of the LLM-\ngenerated synthetic data appears to be particularly\nlimited on tasks with high subjectivity, when com-\npared to the real-world data. This implies that one\npotential way to improve the effectiveness of syn-\nthetic data on high subjectivity tasks is to increase\nthe data diversity and ensure the synthetic data can\nbetter reflect real-world data distributions.\nSecond, specific to the relationship between the\ninstance-level subjectivity and model performance,\nwe note that the “gold label” of a task instance\nis usually decided by a majority vote within a\ngroup of annotators. This means that the gold label\nmay not represent the perspective of each individ-\nual (Goyal et al., 2022), and they are sometimes\n“biased” themselves depending on the annotator\ndecomposition (Li et al., 2022). Thus, it may be\nchallenging for LLMs to generate synthetic data\nto recover such potentially biased “majority view,”\nespecially if the LLMs are trained to maintain neu-\ntrality. Alternatively, one may ask for subjective\ntask instances that humans can hardly reach any\nconsensus on, whether the “gold label” is really\nthe only “correct” label? If not, a rethinking of\nhow to develop and evaluate models for these task\ninstances is urgently needed.\n6.2 Explaining a few exceptions\nIn Table 1, we surprisingly find that on the Tweet\nirony detection tasks, models trained on the few-\nshot synthetic data even outperform models trained\non the real-world data. One plausible explanation\nis that the nature of generating irony texts for so-\ncial media involves a creative writing task with few\nlanguage formality constraints, and recent research\nsuggests that LLMs have the potential to exhibit\ncomparable creativity with human writers in such\ntask (Franceschelli and Musolesi, 2023). Another\nexception we find is in Section 5.2—for the Fi-\nnancial Phrasebank and Scarcasm datasets, unlike\nother tasks, the effectiveness of the models trained\non the synthetic data do not vary much with the\ninstance-level task subjectivity. We conjecture that\nthis can be caused by some task-specific proper-\nties. On the Financial Phasebank dataset, accurate\nsentiment analysis requires the understanding of\nspecialized terminology related to finance. Simi-\nlarly, the Sarcasm detection task aims at identifying\nsarcasm in news headlines from selected sources\nand requires the comprehension on political top-\nics. Thus, on these tasks, LLMs might not be fully\nequipped with the necessary domain knowledge\nto create effective synthetic data under the zero-\nshot setting. In fact, as shown in Figure 2, models\ntrained on the zero-shot synthetic data have very\nlow performance on these two datasets, regardless\nof the subjectivity levels of task instances.\n6.3 Limitations and future work\nWe acknowledge that task subjectivity may not be\nthe only factor that moderates the effectiveness of\nthe LLM-generated synthetic data. Future studies\ncan look into the potential moderating role of other\nfactors, such as language formality and the require-\nment for domain-specific knowledge. Our reliance\non crowd workers in determining task subjectivity\nmay introduce some variability due to their lack\nof linguistic expertise. Most of our evaluation is\nalso based on the GPT-3.5-Turbo model only. It\nis important to note that the conclusions we get\nhere may not generalize to other LLMs (e.g., the\nmore advanced GPT-4), considering the continuous\nimprovements of LLMs in generating human-like\ntexts.\nOur findings suggest that incorporating real-\nworld data examples into the synthetic data genera-\ntion process can increase the data diversity and\nboost the performance of the resulting models.\nThus, future work can explore strategies that lever-\nage human intelligence, such as feedback or direct\nintervention in the generation process, to further\nenrich the diversity of synthetic data (Chung et al.,\n2023) and to identify the most “informative” type\n10450\nof data instance to generate. Finally, the signifi-\ncant correlation between the subjectivity of tasks\nor instances and the performance of models trained\non synthetic data also suggests the potential to uti-\nlize the performance of such models as a proxy for\napproximating task or instance subjectivity, or to\nestimate the reliability of gold labels.\nReferences\n2022. Chatgpt review. Accessed on Kaggle. Avail-\nable from: https://www.kaggle.com/datasets/\nsaloni1712/chatgpt-app-reviews.\n2022. Tweet sentiment emotions. Ac-\ncessed on Kaggle. Available from: https:\n//www.kaggle.com/datasets/ankitkumar2635/\nsentiment-and-emotions-of-tweets .\nall MiniLM-L6-v2. 2023. sentence-transformers/all-\nminilm-l6-v2. Accessed on Hugging Face Model\nHub. Available from: https://huggingface.co/\nsentence-transformers/all-MiniLM-L6-v2 .\nTiago A. Almeida, Jose Maria Gomez Hidalgo, and\nAkebo Yamakami. 2011. Contributions to the study\nof sms spam filtering: New collection and results. In\nProceedings of the 2011 ACM Symposium on Docu-\nment Engineering (DOCENG’11).\nIssa Annamoradnejad and Gohar Zoghi. 2020. Colbert:\nUsing bert sentence embedding for humor detection.\narXiv preprint arXiv:2004.12765.\nBBC. 2022. Accessed on Hugging Face. Avail-\nable from: https://huggingface.co/datasets/\nSetFit/bbc-news.\nEmile Benveniste. 1971. Subjectivity in language.\nProblems in general linguistics, 1:223–30.\nVictor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu\nCord, and Patrick Pérez. 2020. This dataset does\nnot exist: training models from generated images.\nIn ICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1–5. IEEE.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJohn Joon Young Chung, Ece Kamar, and Saleema\nAmershi. 2023. Increasing diversity while main-\ntaining accuracy: Text data generation with large\nlanguage models and human interventions. arXiv\npreprint arXiv:2306.04140.\nElizabeth Clark, Tal August, Sofia Serrano, Nikita\nHaduong, Suchin Gururangan, and Noah A Smith.\n2021. All that’s’ human’is not gold: Evaluating hu-\nman evaluation of generated text. arXiv preprint\narXiv:2107.00061.\nThomas H Cormen, Charles E Leiserson, Ronald L\nRivest, and Clifford Stein. 2022. Introduction to\nalgorithms. MIT press.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A Dataset of Fine-Grained Emo-\ntions. In 58th Annual Meeting of the Association for\nComputational Linguistics (ACL).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A Smith, and Yejin Choi. 2021. Is gpt-3\ntext indistinguishable from human text? scarecrow:\nA framework for scrutinizing machine text. arXiv\npreprint arXiv:2107.01294.\nPaul Ekman et al. 1999. Basic emotions. Handbook of\ncognition and emotion, 98(45-60):16.\nGiorgio Franceschelli and Mirco Musolesi. 2023. On\nthe creativity of large language models. arXiv\npreprint arXiv:2304.00008.\nJiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan\nLiang, Zhenguo Li, and Lingpeng Kong. 2023. Self-\nguided noise-free data generation for efficient zero-\nshot learning. In The Eleventh International Confer-\nence on Learning Representations.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0:\nTowards more challenging few-shot relation classi-\nfication. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6251–6256, Hong Kong, China. Association for Com-\nputational Linguistics.\nMitchell L Gordon, Michelle S Lam, Joon Sung Park,\nKayur Patel, Jeff Hancock, Tatsunori Hashimoto, and\nMichael S Bernstein. 2022. Jury learning: Integrat-\ning dissenting voices into machine learning models.\nIn Proceedings of the 2022 CHI Conference on Hu-\nman Factors in Computing Systems, pages 1–19.\nNitesh Goyal, Ian D Kivlichan, Rachel Rosen, and Lucy\nVasserman. 2022. Is your toxicity my toxicity? ex-\nploring the impact of rater identity on toxicity annota-\ntion. Proceedings of the ACM on Human-Computer\nInteraction, 6(CSCW2):1–28.\nPerttu Hämäläinen, Mikke Tavast, and Anton Kunnari.\n2023. Evaluating large language models in gener-\nating synthetic hci research data: A case study. In\nProceedings of the 2023 CHI Conference on Human\n10451\nFactors in Computing Systems, CHI ’23, New York,\nNY , USA. Association for Computing Machinery.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxigen: A large-scale machine-generated dataset for\nadversarial and implicit hate speech detection. arXiv\npreprint arXiv:2203.09509.\nRuifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing\nZhang, Philip Torr, Song Bai, and Xiaojuan Qi. 2022.\nIs synthetic data from generative models ready for im-\nage recognition? arXiv preprint arXiv:2210.07574.\nNitin Jindal and Bing Liu. 2007. Review spam detection.\nIn Proceedings of the 16th international conference\non World Wide Web, pages 1189–1190.\nTero Karras, Samuli Laine, and Timo Aila. 2019. A\nstyle-based generator architecture for generative ad-\nversarial networks. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion, pages 4401–4410.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. arXiv preprint arXiv:2003.02245.\nZhuoyan Li, Zhuoran Lu, and Ming Yin. 2022. Towards\nbetter detection of biased language with scarce, noisy,\nand biased annotations. In Proceedings of the 2022\nAAAI/ACM Conference on AI, Ethics, and Society ,\npages 411–423.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and\nP. Takala. 2014. Good debt or bad debt: Detecting se-\nmantic orientations in economic texts. Journal of the\nAssociation for Information Science and Technology,\n65.\nAndrew M Mcnutt, Chenglong Wang, Robert A De-\nline, and Steven M. Drucker. 2023. On the design\nof ai-powered code assistants for notebooks. In Pro-\nceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems, CHI ’23, New York,\nNY , USA. Association for Computing Machinery.\nRishabh Misra and Prahal Arora. 2023. Sarcasm detec-\ntion using news headlines dataset. AI Open, 4:13–18.\nRishabh Misra and Jigyasa Grover. 2021. Sculpting\nData for ML: The first act of Machine Learning.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. Semeval-\n2018 task 1: Affect in tweets. In Proceedings of the\n12th international workshop on semantic evaluation,\npages 1–17.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Jus-\ntifying recommendations using distantly-labeled re-\nviews and fine-grained aspects. In Proceedings of\nthe 2019 conference on empirical methods in natural\nlanguage processing and the 9th international joint\nconference on natural language processing (EMNLP-\nIJCNLP), pages 188–197.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. 2021. Glide: To-\nwards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint\narXiv:2112.10741.\nOpenAI. 2023. Gpt-4 technical report.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSamuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Chris-\ntian von der Weth, and Brian Y . Lim. 2021. Directed\ndiversity: Leveraging language embedding distances\nfor collective creativity in crowd ideation. In Pro-\nceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, CHI ’21, New York,\nNY , USA. Association for Computing Machinery.\nGaurav Sahu, Pau Rodriguez, Issam H. Laradji, Parmida\nAtighehchian, David Vazquez, and Dzmitry Bah-\ndanau. 2022. Data augmentation for intent classi-\nfication with off-the-shelf large language models.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna,\nXuhui Zhou, Yejin Choi, and Noah A Smith. 2021.\nAnnotators with attitudes: How annotator beliefs\nand identities bias toxic language detection. arXiv\npreprint arXiv:2111.07997.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and\nXia Hu. 2023. Does synthetic data generation of\nllms help clinical text mining? arXiv preprint\narXiv:2303.04360.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. Semeval-2018 task 3: Irony detection in en-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50.\n10452\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nThomas C Veatch. 1998. A theory of humor.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJanyce Wiebe, Theresa Wilson, Rebecca Bruce,\nMatthew Bell, and Melanie Martin. 2004. Learn-\ning subjective language. Computational linguistics,\n30(3):277–308.\nMichael Wiegand, Josef Ruppenhofer, and Thomas\nKleinbauer. 2019. Detection of abusive language:\nthe problem of biased datasets. In Proceedings of\nthe 2019 conference of the North American Chap-\nter of the Association for Computational Linguistics:\nhuman language technologies, volume 1 (long and\nshort papers), pages 602–608.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. Zerogen: Efficient zero-shot learning via\ndataset generation. arXiv preprint arXiv:2202.07922.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyeong Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. arXiv preprint arXiv:2104.08826.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015a.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015b.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-\nFrancois Lafleche, Adela Barriuso, Antonio Torralba,\nand Sanja Fidler. 2021. Datasetgan: Efficient labeled\ndata factory with minimal human effort.\nJiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G\nParker, and Munmun De Choudhury. 2023. Synthetic\nlies: Understanding ai-generated misinformation and\nevaluating algorithmic and human solutions. CHI\n’23, New York, NY , USA. Association for Computing\nMachinery.\n10453\nA Appendices\nA.1 Descriptions of tasks in Main Study\nAG’s News: This task involves classifying news\narticles from the subset of AG’s News Topic\nClassification dataset into one of thee categories:\nWorld, Sports and Sci/Tech. The AG’s News Topic\nClassification dataset, collected from over 2,000\nnews sources by the academic news search engine,\nComeToMyHead, consists of a training set of\n120,000 instances and a test set of 7,600 instances.\nRelation Classification: This task requires the\nidentification of the relationships between two\nentities within a given sentence. In this study,\nwe focus on four relations: ‘country’, ‘league’,\n‘screenwriter’, and ‘tributary’. The dataset\ncomprises English text sourced from Wikipedia\nand supplemented with crowdsourced English\nannotations. Each relation has 700 instances. As\nthe dataset does not provide an official division\ninto train, validation, and test sets, we randomly\nallocated the dataset into train (70%), validation\n(5%), and test (25%) sets. In our evaluation, this\nprocess was repeated three times, with the average\nperformance reported.\nIMDB Reviews: This task requires classifying\nthe sentiment of movie reviews from the IMDB\nplatform into one of two categories: positive (pos)\nor negative (neg). The dataset comprises 50,000\nmovie reviews evenly split, with 25,000 designated\nfor training and 25,000 for testing.\nSMS Message Spam: This task involves the\nclassification of SMS messages from the SMS\nSpam Collection v.1 dataset into either ‘ham’\n(legitimate) or ‘spam’ categories. The training\ndataset contains 5,574 English messages, each\nlabeled according to its legitimacy. As the dataset\ndoes not provide an official division into train,\nvalidation, and test sets, we randomly divided the\ndataset into train (70%), validation (5%), and test\n(25%) sets. In our evaluation, this process was\nrepeated three times, with the average performance\nreported.\nFinancial Phrasebank: This task entails the\nclassification of finance-related sentences into\none of three categories—positive, negative, or\nneutral—based on the sentiment expressed by the\nsentence. The dataset comprises 4,840 English\nsentences sourced from financial news articles. As\nthe dataset does not provide an official division\ninto train, validation, and test sets, we randomly\nallocated the dataset into train (70%), validation\n(5%), and test (25%) sets. In our evaluation, this\nprocess was repeated three times, with the average\nperformance reported.\nReddit Emotion: The Reddit Emotion is the\nsubset of the Go Emotions dataset. The Go Emo-\ntions dataset is comprised of 58,009 comments\ncollected from Reddit, and each comment has been\nannotated with respect to 28 emotion categories. In\nthis task, we focus on three basic emotions (Ekman\net al., 1999): joy, sadness, and surprise.\nTweet Irony Speech: The task involves classifying\ntweets into two categories: irony, non-irony. The\ndataset, which is composed of English-language\ntweets, has been manually annotated for these\nspecific categories. The distribution of the data\nincludes a training set of 2,862 instances and a test\nset of 784 instances.\nTweet Emotion: The task involves classifying\ntweets into four emotion categories: anger,\njoy, optimism, sadness. Each tweet in this\nEnglish-language dataset has been annotated by\nhuman reviewers with respect to these emotional\ncategories. The dataset is partitioned into a training\nset of 3,257 instances and a test set of 1,421\ninstances.\nSarcasm News Headlines: This task requires\ndistinguishing between sarcastic and non-sarcastic\nnews headlines. The dataset comprises 26,709\nheadlines from two news sources: TheOnion,\nrepresenting sarcasm, and HuffPost, representing\nnon-sarcasm. As the dataset does not provide an\nofficial division into train, validation, and test\nsets, we randomly allocated the dataset into train\n(70%), validation (5%), and test (25%) sets. In our\nevaluation, this process was repeated three times,\nwith the average performance reported.\nHumor Speech Detection: This task involves\ndiscerning humorous from non-humorous content\nfor short texts. The dataset, specifically curated\nfor humor detection, is composed of 200,000\ninstances, balanced between humorous and non-\nhumorous data. It is divided into a training set of\n160,000 instances and a test set of 40,000 instances.\nA.2 Descriptions of tasks in Robustness Check\nBBC News: This task involves classifying BBC\nnews articles into one of 5 categories: business,\nentertainment, politics, sport or tech. The dataset\ncontains 2225 articles. As the dataset does not\nprovide an official division into train, validation,\n10454\nand test sets, we randomly allocated the dataset\ninto train (70%), validation (5%), and test (25%)\nsets. In our evaluation, this process was repeated\nthree times, with the average performance reported.\nAmazon Review: This task contains classifying\nthe customer review text in the Amazon into one\nof two categories: positive (pos) or negative (neg).\nGiven the size of original dataset, we randomly\nsample 10000 instances for evaluation.\nSST-2: This task involves classifying single\nsentences extracted from movie reviews into one\nof two categories: positive (pos) or negative (neg).\nThe dataset is partitioned into a training set of\n67,349 instances and a test set of 1821 instances.\nYelp Review: The objective of this task is to\nclassify Yelp app reviews into either positive\n(pos) or negative (neg) sentiments. The focus is\nparticularly on the extreme reviews: those rated\nwith five stars (pos) and those with just one star\n(neg). The dataset is split into a training set with\n260,000 instances and a test set comprising 20,000\ninstances.\nChatGPT Review: The objective of this task is to\nclassify user reviews from the ChatGPT mobile\napp on iOS into either positive (pos) or negative\n(neg) sentiments. The focus is particularly on the\nextreme reviews: those rated with five stars (pos)\nand those with just one star (neg). The dataset\nis comprised of 2292 instances. As the dataset\ndoes not provide an official division into train,\nvalidation, and test sets, we randomly allocated\nthe dataset into train (70%), validation (5%), and\ntest (25%) sets. In our evaluation, this process was\nrepeated three times, with the average performance\nreported.\n2022 Tweet Emotion: The task involves classi-\nfying tweets into four emotion categories: anger,\njoy, optimism, sadness. The tweets in this task\nare posted during first three quarters of 2022\n(Jan 1–Sept 30). The dataset is comprised of\n25,000 instances. As the dataset does not provide\nan official division into train, validation, and\ntest sets, we randomly allocated the dataset into\ntrain (70%), validation (5%), and test (25%)\nsets. In our evaluation, this process was repeated\nthree times, with the average performance reported.\nB Evaluation I: Comparison Across\nDifferent Types of Tasks (Additional\nResults)\nB.1 Convergence Analysis\nFigure B.1 illustrates the training curves of classifi-\ncation models across the 10 types of tasks. We find\nthat compared to the training curves derived from\nthe real-world data, models trained on the synthetic\ndata exhibit a faster convergence rate and a greater\npropensity to overfit. This indicates that under both\nzero-shot and few-shot settings, the synthetic data\ngenerated by the LLM may lack a degree of diver-\nsity and falls short in fully capturing the complex\npatterns found in the real world language contexts.\nB.2 Potential of Few-shot Synthetic Data for\nData Augmentation\nIn the main text, the model performance we report\nfor the “few-shot synthetic data” are based on mod-\nels that are trained only on the synthetic data. As\nwe assume that a small amount of real-world data\nare available under the few-shot data generation\nsetting, a natural question to ask is whether the\nfew-shot synthetic data can be used to augment\nthe real-world data (which are used as the exam-\nples in the synthetic data generation process) and\nimprove the model performance. Answering this\nquestion, Table B.1 compares the performance of\nclassification models trained only on the limited\nset of real-world data (i.e., those used as example\nto guide LLM in generating synthetic data), only\non the few-shot synthetic data generated, and on\nthe combination of both data. We find that the\ncomparison between the performance of models\ntrained exclusively on the limited real-world data\nand models trained exclusively on few-shot syn-\nthetic data is task-dependent. However, when the\nfew-shot synthetic data is combined with the small\nset of real-world data, the resulting model can out-\nperform the model trained only on the real-world\ndata for many tasks. This highlights the potential of\nthe few-shot synthetic data for data augmentation.\nB.3 Similarity between the Synthetic Data\nand the Real Data\nIn the few-shot setting, we utilized real-world data\nexamples to guide the generation of synthetic data.\nTo quantify the similarity between the real-world\ndata examples and the few-shot synthetic data gen-\nerated, we employed a pre-trained Sentence Trans-\nformer model (all MiniLM-L6-v2, 2023) to convert\n10455\n(a) AG’s News\n (b) Relation\n (c) IMDB Reviews\n (d) SMS Spam\n (e) Financial Phrasebank\n(f) Reddit Emotion\n (g) Sarcasm News\n (h) Humor Detection\n (i) Tweet Emotions\n (j) Tweet Irony Speech\nFigure B.1: The training curves for classification models trained with the real-world data, the zero-shot synthetic\ndata, and the few-shot synthetic data.\nTask BERT RoBERTareal synthetic real + synthetic real synthetic real+ syntheticMacro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy Score Macro-F1 Accuracy ScoreAG 93.1% 93.2% 91.5% (-1.6%) 91.6% (-1.6%) 93.1% (+0.0%) 93.1% (-0.1%) 93.6% 93.6% 92.9% (-0.7%) 92.9% (-0.7%) 93.4% (-0.2%) 93.5% (-0.1%)Relation 96.8% 96.8% 96.4% (-0.4%) 96.4% (-0.4%) 96.7% (-0.1%) 96.8% (+0.0%) 97.6% 97.6% 94.1% (-3.5%) 94.1% (-3.5%) 97.1% (-0.5%) 97.3% (-0.3%)IMDB 77.4% 78.6% 81.1% (+3.7%) 81.2% (+2.6%) 80.2% (+2.8%) 80.1% (+1.5%) 75.7% 76.1% 82.4% (+6.7%) 82.4% (+6.3%) 81.0% (+5.3%) 81.1% (+5.0%)SMS Spam 98.2% 98.2% 94.3% (-3.9%) 94.8% (-3.4%) 98.1% (-0.1%) 98.2% (+0.0%) 98.1% 98.1% 94.0% (-4.1%) 95.7% (-2.4%) 98.1% (+0.0%) 98.1% (+0.0%)Reddit Emotion 92.5% 92.5% 81.9% (-10.6%) 82.0% (-10.5%) 91.8% (-0.7%) 91.8% (-0.7%) 91.7% 91.8% 87.5% (-4.2%) 87.7% (-4.1%) 90.4% (-1.3%) 90.8% (-1.0%)Tweet Irony 67.3% 68.2% 81.5% (+14.2%) 81.9% (+13.7%) 81.2% (+13.9%) 81.5% (+13.3%) 66.4% 67.2% 83.3% (+16.9%) 83.7% (+16.5%) 80.8% (+14.4%) 81.3% (+14.1%)Tweet Emotion 64.5% 64.5% 64.6% (+0.1%) 69.1% (+4.6%) 70.4% (+5.9%) 70.5% (+6.0%) 72.2% 72.5% 66.3% (-5.9%) 72.7% (+0.2%) 73.4% (+1.2%) 73.5% (+1.0%)Sarcasm 76.1% 78.3% 63.6% (-12.5%) 64.8% (-13.5%) 77.5% (+1.4%) 76.4% (-1.9%) 72.4% 72.5% 61.5% (-10.9%) 63.6% (-8.9%) 72.9% (+0.5%) 73.2% (+0.7%)Financial 72.5% 75.1% 70.6% (-1.9%) 74.2% (-0.9%) 74.6% (+2.1%) 76.3% (+1.2%) 76.9% 78.2% 75.0% (-1.9%) 78.9% (+0.7%) 78.4% (+1.5%) 80.1% (+1.9%)Humor Speech 94.8% 94.7% 86.9% (-7.9%) 87.0% (-7.7%) 93.3% (-1.5%) 93.3% (-1.4%) 95.3% 95.3% 84.0% (-11.3%) 84.0% (-11.3%) 94.6% (-0.7%) 94.6% (-0.7%)\nTable B.1: Comparing the performance of classification models trained using three types of data: a small amount\nof the real-world data used as the examples for guiding LLM in synthetic data generation (i.e., “real”), few-\nshot synthetic data generated by the LLM (i.e., “synthetic”), and a combination of both (“real+synthetic”). The\nperformance is measured in terms of Macro-F1 (%) and Accuracy Score (%).\ntexts into vector embeddings. We then computed\nthe cosine similarity between the embeddings of\nreal-world examples and the embeddings of the\nthe synthetic texts. The consine similarity metric\nranges from -1 to 1, and we rescaled it to the in-\nterval of [0, 1], with 1 representing the highest\nlevel of similarity. Then, for each real-world ex-\nample, we obtained its mean similarity with the\ntop 5 most similar synthetic texts in the synthetic\ndata and then computed the average mean simi-\nlarity scores across all real-world examples within\neach type of classification tasks. As a reference, we\nalso conducted the same computation between the\nreal-world examples and the synthetic data gener-\nated under the zero-shot settings, and results of the\nsimilarity comparisons are shown in Figure B.2.\nVisually, we find a consistent trend that the few-\nshot synthetic data has a higher level of similarity\nwith the real-world examples compared to the zero-\nshot synthetic data. We then performed t-tests on\neach classification task to determine whether the\ndifference of the average cosine similarity scores\nfor the zero-shot and few-shot synthetic data is\nsignificant. The results are shown in Table B.2,\nwhich indicates that the difference is statistically\nsignificant for all but the IMDB review classifica-\ntion task. In other words, the few-shot synthetic\ndata is more similar to the real-world data than the\nzero-shot synthetic data, which may partly explain\nwhy models trained on the few-shot synthetic data\ntend to outperform models trained on the zero-shot\nsynthetic data.\nB.4 Additional Results on Additional Tasks\nwith Low Subjectivity\nIn the main paper, six out of the ten text classi-\nfication tasks we examined turn out to have the\nhighest level of subjectivity based on the crowd-\nsourced annotations we collected. To further vali-\ndate our observation that LLM-generated synthetic\ndata appear to be more powerful for training accu-\nrate models for less subjective tasks, we conducted\nadditional experiments on a few more datasets\nwhich represent less subjective text classification\n10456\nFigure B.2: Average top 5 cosine similarity between the\nreal and synthetic data\nDataset p-value\nAG News p <0.001\nRelation p <0.001\nIMDB p <0.1\nSpam p <0.001\nFinancial p <0.001\nReddit Emotion p <0.001\nSarcasm p <0.001\nHumor p <0.001\nTweet Emotion p <0.001\nTweet Irony p <0.001\nTable B.2: T-test results for the similarity comparison.\ntasks. This includes the BBC News (BBC, 2022),\nSST-2 movie review (Socher et al., 2013), Ama-\nzon US review (Ni et al., 2019), and Yelp review\ndataset (Zhang et al., 2015a). We compared the\nperformance of BERT models trained on real data\nwith those trained on zero-shot synthetic data. As\nindicated in Table B.3, the average performance\ndifference between real-world data and zero-shot\nsynthetic data is only 4.2%. This gap is notably\nsmaller than what is observed for tasks with greater\nsubjectivity, reinforcing the finding that the subjec-\ntivity of a task is correlated with the effectiveness\nof LLM-generated synthetic data.\nB.5 Additional Results of Two Post-2022\nDatasets\nIn the main paper, we evaluated the effectivness\nof LLM-generated synthetic data for text classifi-\ncations on 10 datasets, and all these datasets are\ncollected before 2022. To address the concern\nDataset BBC news Amazon review SST-2 Yelp\nReal data 93.6 91.8 89.2 94.3\nZero-shot data 91.2 87.7 86.4 87.8\nTable B.3: Comparing the performance of classification\nmodels trained on the LLM-generated synthetic data\nunder the zero-shot with those trained with the original\nreal-world data, in terms of Macro-F1 (%)\nDataset ChatGPT App Review 2022 Tweet Emotion\nReal 79.4 68.9\nZero-shot 73.3 53.5\nFew-shot 76.5 58.8\nTable B.4: Comparing the performance of Bert classifi-\ncation models trained on the GPT-3.5 turbo-generated\nsynthetic data under the zero-shot or few-shot settings,\nwith those trained with the original real-world data, in\nterms of Macro-F1 (%).\nthat the LLM we used (i.e., OpenAI’s GPT-3.5-\nturbo) may have been exposed to these datasets\nin its training process, thus it may simply memo-\nrize some of the data instances and provide them\nas the synthetically-generated data, we conducted\nan additional study on two post-2022 datasets,\ni.e., ChatGPT App reviews (cha, 2022) (to reflect\nless subjective tasks) and the 2022 Tweet Emo-\ntion dataset (twe, 2022) (to reflect more subjective\ntasks). On these two datasets, we repeated the zero-\nshot and few-shot data generation processes with\nthe GPT-3.5-turbo model, used the resulting data to\ntrain Bert-based models, and compared the models’\nperformance on the test datasets using Macro-F1\nscores. As shown in Table B.4, the results confirm\nour earlier findings: synthetic data is more effec-\ntive in less subjective tasks like ChatGPT reviews,\nand using real examples to guide synthetic data\ngeneration in the few-shot setting can improve the\nefficacy of the synthetic data generated.\nB.6 Additional Results When Changing the\nVolume of Synthetic Data\nTo see whether training models with more synthetic\ndata can enhance classification performance, we\nconducted a study on several datasets, using GPT-\nturbo-3.5 as the LLM to generate synthetic data and\nBert as the base model for classification. We varied\nthe size of synthetically generated training dataset\nfrom half to a maximum of three times as that of\nthe real data. As shown in Table B.5, our observa-\ntions indicate that, unlike in low-resource settings,\nsimply varying the training data size between 0.5\nand 3 times of that of the real data using unfiltered\n10457\nRatio 0.5 1 1.5 2 2.5 3\nSMS 92.9 93.6 93.4 93.2 93.1 91.8\nRelation 92.5 92.1 91.6 91.4 92.2 91.8\nTweet Emotion52.6 57.8 58.9 57.4 56.3 56.5\nSarcasm 56.2 51.4 49.6 47.2 45.8 43.7\nFinancial 45.1 55.3 51.2 49.5 48.7 46.4\nTable B.5: Comparing the performance of Bert classi-\nfication models trained on varying size of the GPT-3.5\nturbo-generated synthetic data under the zero-shot set-\nting in terms of Macro-F1 (%).\nsynthetic data does not consistently enhance the\nmodel’s performance across different tasks.\nB.7 Additional Results When Using Other\nLLMs\nTo examine whether our findings hold true for\ndecoder-based models as well as models that are\nreasonably large, we conducted the same evaluation\nstudies using the GPT2-large (774M) and Llama2\n(7B) models. We conducted this evaluation on 6\nselected datasets from the entire set of 10 datasets\nwhich covered different levels of subjectivity. As\nindicated in Table B.6, we observed that models\ntrained on the LLM-generated synthetic data only\nexhibits slight variations among different LLMs\nfor each respective task. The overall trend remains\nconsistent: the effectiveness of synthetic data tends\nto be higher for tasks with lower subjectivity.\nB.8 Additional Results of Improved Data\nGeneration Pipeline\nTo see how adopting different data generation\npipelines may affect the effectiveness of the syn-\nthetic data for text classification tasks with dif-\nferent subjectivity levels, we conducted an addi-\ntional study in which we follow the data generation\npipelines, SunGen (Gao et al., 2023), to collect syn-\nthetic data. As shown in Table B.7, while SunGen\ndoes offer an improvement compared to directly\nprompting LLMs for zero-shot synthetic data gener-\nation, the effectiveness of SunGen compared to the\nreal data is still influenced by the task subjectivity\nlevel.\nB.9 Additional Results of Using Synthetic\nData as Examples in Few-shot Setting\nGiven the effectiveness of the synthetic data gen-\nerated in the few-shot settings, one may wonder\nways to make few-shot data generation possible\neven without real-world textual examples. To this\nend, we conducted an additional study by using\nthe synthetic data produced by the GPT-3.5-turbo\nmodel in the zero-shot setting as the guiding exam-\nples in the few-shot setting to generate data. As\nillustrated in Table B.8, we found that for tasks with\nvarying levels of subjectivity, using synthetic data\nas examples in the prompt for further synthetic data\ngeneration (referred to as “second-prompt”) leads\nto a larger performance degradation compared to\ndata generated in a single zero-shot round.\nB.10 Additional Results for Directly\nPrompting LLMs for Text Classification\nWhile LLMs are capable of generating high-quality\nsynthetic data through prompting, their direct clas-\nsification performance can sometimes lag behind\nthat of smaller models trained on this synthetic data.\nAs shown in Table B.9, for many tasks, directly\nprompting the GPT-3.5-turbo model for classifi-\ncation often yields poorer results compared to a\nsmaller model trained on the synthetic data. This\ndiscrepancy might arise because the prompt con-\nstraints defining the label space for the LLM can\nsometimes be too lax, making accurate classifica-\ntion challenging.\nC Evaluation II: Comparison Across\nDifferent Task Instances (Additional\nResults)\nIn order to investigate how models trained on the\nreal-world data perform across task instances of\nvarying subjectivity, we used BERT as the foun-\ndational model for training a classification model\nwith the real-world data. As depicted in Figure C.1,\nwe observed that compared to models trained on\nzero-shot synthetic data, the performance of mod-\nels trained on the real-world data is less affected by\nthe subjectivity of the task instance (i.e., β and ρ\nare smaller), except for that on the Scarcasm News\nand Financial Phrasebank datasets.\nD Additional Details on the Generation of\nSynthetic Data\nThe prompts we used to generate synthetic data un-\nder both the zero-shot setting and the few-shot set-\nting are shown in the Table D.1 and the Table D.2.\n10458\nDataset AG IMDB SMS Tweet Emotion Humor Speech Tweet Irony\nSubjectivity Level⋆ ⋆⋆⋆ ⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆\nReal data 95.3 87.6 97.2 77.7 97.0 72.2\nGPT2-Large 86.5 80.9 86.4 52.2 51.5 60.8\nLlama 2 88.7 82.4 88.5 59.1 57.2 63.1\nGPT-3.5 turbo 89.3 81.2 93.8 58.5 56.0 63.4\nTable B.6: Comparing the performance of Bert classification models trained on synthetic data generated by various\nLLMs within a zero-shot setting using Macro-F1 (%) as the metric.\nDataset AG IMDB SMS Tweet Emotion Humor Speech Tweet Irony\nSubjectivity Level⋆ ⋆⋆⋆ ⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆\nReal data 95.3 87.6 97.2 77.7 97.0 72.2\nSunGen 91.7 84.7 94.5 61.8 59.9 64.6\nZero-shot 89.3 81.2 93.8 58.5 56.0 63.4\nTable B.7: Comparing the performance of Bert classification models trained on synthetic data generated by the\nSunGen pipeline and our zero-shot pipeline using Macro-F1 (%) as the metric.\nDataset AG IMDB SMS Tweet Emotion Humor Speech Tweet Irony\nSubjectivity Level⋆ ⋆⋆⋆ ⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆\nReal data 95.3 87.6 97.2 77.7 97.0 72.2\nZero-shot 89.3 81.2 93.8 58.5 56.0 63.4\nSecond-Prompt 87.1 86.9 81.1 55.9 53.8 61.9\nTable B.8: Comparing the performance of Bert classification models trained on the zero-shot synthetic data and the\nfew-shot synthetic data where the synthetic data is used as the guiding examples (“second-promot”) generated by\nthe GPT-3.5 turbo in terms of Macro-F1 (%).\nDataset AG IMDB SMS Tweet Emotion Humor Speech Tweet Irony\nSubjectivity Level⋆ ⋆⋆⋆ ⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆ ⋆⋆⋆⋆⋆\nReal data 95.3 87.6 97.2 77.7 97.0 72.2\nDirect Prompt 86.5 82.8 89.4 54.3 59.2 61.1\nZero-shot 89.3 81.2 93.8 58.5 56.0 63.4\nTable B.9: Performance comparisons in terms of Macro-F1 (%) between “direct prompt” and “zero-shot data\ngeneration” using GPT-3.5 turbo. For the zero-shot synthetica data and real data, we adopted the Bert model as the\nbase for classification.\n(a) AG\n (b) Relation\n (c) IMDB Reviews\n (d) SMS Spam\n (e) Reddit Emotion\n(f) Sarcasm News\n (g) Humor Detection\n (h) Tweet Emotions\n (i) Tweet Irony Speech\n (j) Financial Phrasebank\nFigure C.1: Changes in the accuracy of the BERT model trained on real-world data as the instance-level annotation\nagreement threshold varies. The solid blue line in each plot is the linear regression fitted on the data, and the\nR-squared score quantifies the goodness of fit. The Spearman’s ρassesses the strength of rank correlation between\nthe instance-level agreement threshold and the model accuracy for each task. Higher values for both R-squared and\nSpearman’s ρ, ideally close to 1, indicate a stronger monotonic relationship between the instance-level subjectivity\nand the model accuracy.\n10459\nTask Zero-shot/Few-shot\nAG\nContext Prompt: Now you are a journalist writing news articles. You are given a topic and must write a\ncorresponding news article for it. You are also given a length requirement. You must ensure your news\nmeets the length requirement.\nData Generation Prompt: Can you write a news report with the topic {label}? The length requirement\nis: {num_words} words. Please be creative and write unique news articles.\nRelation\nContext Prompt: Now you are a Wikipedia editor. You need to generate new records for describing the\nrelation between entities. You are given a relation type, as well as a sentence describing the relationship.\nYou must write a sentence to describe the specified relationship between the two entities that you came\nup with.\nData Generation Prompt: Give me one pair of entities, which have the relation: {label}, and generate a\nsentence which contains the pair of entities that have the relation: {label}. The description of the relation\nis: {label_description}.\nIMDB\nContext Prompt: Now you are a movie critic. You need to have delicate emotions, unique perspectives,\nand a distinctive style. You are going to write a highly polar review for a movie and post it on IMDB.\nYou are given a movie genre/style and a length requirement. You must come up with a movie that\ncorresponds to the genre/style and write a review that meets the length requirement.\nData Generation Prompt: Write a film review for a {genre} movie to express {pos_or_neg} feedback.\nEach review should have {num_of_words} words. Be sure to express your personal insights and feelings.\nPlease be creative and write unique movie reviews.\nSMS spam\nContext Prompt (Spam): Now you are a person who is planning to send a spam SMS message. You\nmust be as creative as possible to diversify your messages. Ensure your language is conversational and\ncolloquial. Notice that scammers, in order to make people believe them, will make their spam SMS\nmessages look like people’s daily conversations or very formal and serious content. You also need\nto imitate these contents. Context Prompt (Ham): Now you are a person who is planning to send a\nSMS message. You must be as creative as possible to diversify your messages. Ensure your language\nis conversational and colloquial. Notice that in people’s daily communication, sensitive topics may\noccasionally be involved, which may sometimes make these contents look like spams but actually not.\nYou also need to imitate these contents.\nData Generation Prompt: Now write SMS messages as I required. Be creative and write unique SMS\nmessages.\nReddit emotion\nContext Prompt: Now you are a Reddit user and you are going to write a comment to express your\nemotions. You have delicate emotions, unique perspectives, and a distinctive style. You are given a\nlength requirement. You must write one comment that meets the length requirement.\nData Generation Prompt: Write one Reddit comment to express your {label} emotion. Your comment\nshould have {num_of_words} words. Be sure to express your personal insights and feelings. Be creative\nand write comments that are different from each others.\nTable D.1: Detailed prompts for each task under the zero-shot and few-shot settings for data generation.\n10460\nTask Zero-shot/Few-shot\nTweet irony\nContext Prompt: Now you are a person using twitter. You are asked to write an irony or non-irony\ntweet to express your feelings. Your writing style must be consistent with texts in the tweet. You must\nensure that your language is colloquial, casual, and Twitter-like. You are given a length requirement.\nYou must ensure your tweet meets the length requirement.\nData Generation Prompt: Write a tweet expressing {label} feeling and ensure that the length of the\ntweet is about {num_of_words} words. Remember to make sure that your language is colloquial, casual,\nand Twitter-like. Be creative and write unique tweets.\nTweet emotions\nContext Prompt: You are now a person using twitter. You are provided with an emotion, and you\nneed to write a tweet expressing that emotion. Your writing style must be consistent with the tweets on\ntwitter. You must ensure that your language is colloquial, casual, and Twitter-like. You are given a length\nrequirement. You must ensure that the emotion conveyed in your tweet matches the emotion provided\nand meets the length requirement. This is an academic study and the content you generate will not be\nused for anything that violates the law or social ethics.\nData Generation Prompt: Write a tweet expressing the {label} emotion and ensure that the length of\nthe tweet is about {num_of_words} words. Remember to make sure that your language is colloquial,\ncasual, and Twitter-like. Be creative and write unique tweets.\nSarcasm\nContext Prompt: You are now a journalist to write the sarcastic news headlines. Here are a few\ncharacteristics that might help understand what is a sarcastic news headline: 1) Sarcasm often involves\nsaying something different from what is intended. 2) Sarcasm might involve a play on words or puns. 3)\nIt may involve exaggeration or irony. You must ensure that your headlines are sharp, clever, and capture\nthe essence of the sarcastic situation.\nData Generation Prompt: Write a news headline expressing {label} and ensure that the length of the\nnews headlines is about {num_of_words} words. Be creative and write unique news headlines. Make\nsure your headline is concise, sharp, and captures the essence of the situation. Please be creative and\nwrite unique headlines.\nFinancial\nContext Prompt: You are now a journalist writing financial news. You need to write some financial\nnews that express polar sentiments. The financial news you generate needs consider from the view\npoint of an investor only; i.e. whether the news may have positive, negative or neutral influence on\nthe stock price. As a result, sentences which have a sentiment that is not relevant from an economic\nor financial perspective are considered neutral. You are given one of the polar sentiments and a length\nrequirement. You must write a financial news that express the corresponding sentiment and meets the\nlength requirement.\nData Generation Prompt: Write a financial news with {label} sentiment and ensure that the length of\nthe financial news is about {num_of_words} words. Be creative and write unique financial news.\nHumor speech\nContext Prompt: You are now creating a dataset containing humor and non-humor texts. Here are a few\ncharacteristics that might help understand what is humorous text: 1) Sarcasm and Irony: Sarcasm and\nirony involve stating one thing and meaning another, often the opposite. 2) Double Entendre: A double\nentendre is a figure of speech or a particular way of wording that is devised to have a double meaning, of\nwhich one is typically obvious, while the other often carries a risqué or ironic connotation. 3) Parody\nand Satire: Both involve imitating and exaggerating the features of a particular language style, genre,\nor piece of content to humorous effect. 4) Absurdity and Nonsense: Language that describes absurd\nor nonsensical scenarios can often be funny. This includes non-sequiturs, in which conclusions do not\nfollow from their premises, and other forms of illogical statements.\nData Generation Prompt: Write a {label} short text and ensure that the length of the short text is about\n{num_of_words} words. Be creative and write unique short text.\nTable D.2: Detailed prompts for each task under the zero-shot and few-shot settings for data generation (Continued).\n10461",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7600716352462769
    },
    {
      "name": "Synthetic data",
      "score": 0.7234642505645752
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5605247020721436
    },
    {
      "name": "Task (project management)",
      "score": 0.538635790348053
    },
    {
      "name": "Machine learning",
      "score": 0.5111361145973206
    },
    {
      "name": "Language model",
      "score": 0.49086838960647583
    },
    {
      "name": "Training set",
      "score": 0.48236292600631714
    },
    {
      "name": "Data modeling",
      "score": 0.47962865233421326
    },
    {
      "name": "Data quality",
      "score": 0.4232792258262634
    },
    {
      "name": "Natural language processing",
      "score": 0.3792383074760437
    },
    {
      "name": "Data science",
      "score": 0.3272060751914978
    },
    {
      "name": "Database",
      "score": 0.10024803876876831
    },
    {
      "name": "Engineering",
      "score": 0.08235400915145874
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Metric (unit)",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204465549",
      "name": "Washington University in St. Louis",
      "country": "US"
    }
  ]
}