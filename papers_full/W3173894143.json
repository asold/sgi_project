{
  "title": "MergeDistill: Merging Language Models using Pre-trained Distillation",
  "url": "https://openalex.org/W3173894143",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2992392264",
      "name": "Simran Khanuja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118424058",
      "name": "Melvin Johnson",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2248318254",
      "name": "Partha Talukdar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2905933322",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3126822054",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3136221257",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W3115778530",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W3029674355",
    "https://openalex.org/W3021805648",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2900356614",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3124253258",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W3209051700",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2963777589",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W3094612218",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2891844856",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2998653650",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W3085479580",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2995541765",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3034457371"
  ],
  "abstract": "Pre-trained multilingual language models (LMs) have achieved state-of-the-art results in cross-lingual transfer, but they often lead to an inequitable representation of languages due to limited capacity, skewed pre-training data, and sub-optimal vocabularies.This has prompted the creation of an ever-growing pretrained model universe, where each model is trained on large amounts of language or domain specific data with a carefully curated, linguistically informed vocabulary.However, doing so brings us back full circle and prevents one from leveraging the benefits of multilinguality.To address the gaps at both ends of the spectrum, we propose MERGEDISTILL, a framework to merge pre-trained LMs in a way that can best leverage their assets with minimal dependencies, using task-agnostic knowledge distillation.We demonstrate the applicability of our framework in a practical setting by leveraging pre-existing teacher LMs and training student LMs that perform competitively with or even outperform teacher LMs trained on several orders of magnitude more data and with a fixed model capacity.We also highlight the importance of teacher selection and its impact on student model performance.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2874–2887\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2874\nMERGE DISTILL : Merging Pre-trained Language Models using\nDistillation\nSimran Khanuja\nGoogle Research\nIndia\nsimrankh@google.com\nMelvin Johnson\nGoogle Research\nUSA\nmelvinp@google.com\nPartha Talukdar\nGoogle Research\nIndia\npartha@google.com\nAbstract\nPre-trained multilingual language models\n(LMs) have achieved state-of-the-art results\nin cross-lingual transfer, but they often lead\nto an inequitable representation of languages\ndue to limited capacity, skewed pre-training\ndata, and sub-optimal vocabularies. This has\nprompted the creation of an ever-growing pre-\ntrained model universe, where each model is\ntrained on large amounts of language or do-\nmain speciﬁc data with a carefully curated, lin-\nguistically informed vocabulary. However, do-\ning so brings us back full circle and prevents\none from leveraging the beneﬁts of multilin-\nguality. To address the gaps at both ends of\nthe spectrum, we propose M ERGE DISTILL , a\nframework to merge pre-trained LMs in a way\nthat can best leverage their assets with mini-\nmal dependencies, using task-agnostic knowl-\nedge distillation. We demonstrate the applica-\nbility of our framework in a practical setting by\nleveraging pre-existing teacher LMs and train-\ning student LMs that perform competitively\nwith or even outperform teacher LMs trained\non several orders of magnitude more data and\nwith a ﬁxed model capacity. We also highlight\nthe importance of teacher selection and its im-\npact on student model performance.\n1 Introduction\nWhile current state-of-the-art multilingual lan-\nguage models (LMs) (Devlin et al., 2019; Conneau\net al., 2020) aim to represent 100+ languages in\na single model, efforts towards building monolin-\ngual (Martin et al., 2019; Kuratov and Arkhipov,\n2019) or language-family based (Khanuja et al.,\n2021) models are only increasing with time (Rust\net al., 2020). A single model is often incapable of\neffectively representing a diverse set of languages,\nevidence of which has been provided by works\nhighlighting the importance of vocabulary curation\nand size (Chung et al., 2020; Artetxe et al., 2020),\nFigure 1: Previous works (left) typically focus on\ncombining ﬁne-tuned models derived from a single\npre-trained model using distillation. We propose\nMERGE DISTILL to combine pre-trained teacher LMs\nfrom multiple monolingual/multilingual LMs into a sin-\ngle multilingual task-agnostic student LM.\npre-training data volume (Liu et al., 2019a; Con-\nneau et al., 2020), and the curse of multilinguality\n(Conneau et al., 2020). Language speciﬁc mod-\nels alleviate these issues with a custom vocabulary\nwhich captures language subtleties1 and large mag-\nnitudes of pre-training data scraped from several\ndomains (Virtanen et al., 2019; Antoun et al., 2020).\nHowever, building language speciﬁc LMs brings\nus back to where we started, preventing us from\nleveraging the beneﬁts of multilinguality like zero-\nshot task transfer (Hu et al., 2020), positive trans-\nfer between related languages (Pires et al., 2019;\nLauscher et al., 2020) and an ability to handle code-\nmixed text (Pires et al., 2019; Tsai et al., 2019).\nWe need an approach that encompasses the best of\nboth worlds, i.e., leverage the capabilities of the\npowerful language-speciﬁc LMs while still being\nmultilingual and enabling positive language trans-\n1For example, in Arabic, (Antoun et al., 2020) argue that\nwhile the deﬁnite article “Al”, which is equivalent to “the” in\nEnglish, is always preﬁxed to other words, it is not an intrinsic\npart of that word. While with a BERT-compatible tokenization\ntokens will appear twice, once with “Al-” and once without\nit, AraBERT ﬁrst segments the words using Farasa (Abdelali\net al., 2016) and then learns the vocabulary, thereby alleviating\nthe problem.\n2875\nFigure 2: Overview of MERGE DISTILL : The input to MERGE DISTILL is a set of pre-trained teacher LMs and pre-\ntraining transfer corpora for all the languages we wish to train our student LM on. Here, we combine four teacher\nLMs comprising of three monolingual (trained on English, Spanish and Korean respectively) and one multilingual\nLM (trained on English and Hindi). The student LM is trained on English, Spanish, Hindi and Korean. Pre-training\ntransfer corpora for each language is tokenized and masked using their respective teacher LMs vocabulary. We\nthen obtain predictions for each masked word in each language, by evaluating all of their respective teacher LMs.\nFor example, we evaluate English masked examples on both the monolingual and multilingual LM as shown. The\nstudent’s vocabulary is a union ofall teacher vocabularies. Hence, the input, prediction and label indices obtained\nfrom teacher evaluation are now mapped to the student vocabulary, and input to the student LM for training. Please\nrefer to Section 3.1 for details.\nfer.\nIn this paper, we use knowledge distillation (KD)\n(Hinton et al., 2015) to achieve this. In the con-\ntext of language modeling, KD methods can be\nbroadly classiﬁed into two categories: task-speciﬁc\nand task-agnostic. In task-speciﬁc distillation, the\nteacher LM is ﬁrst ﬁne-tuned for a speciﬁc task\nand is then distilled into a student model which can\nsolve that task. Task-agnostic methods perform dis-\ntillation on the pre-training objective like masked\nlanguage modeling (MLM) in order to obtain a\ntask-agnostic student model. Prior work has either\nused task-agnostic distillation to compress single-\nlanguage teachers (Sanh et al., 2019; Sun et al.,\n2020) or used task-speciﬁc distillation to combine\nmultiple ﬁne-tuned teachers into a multi-task stu-\ndent (Liu et al., 2019b; Clark et al., 2019). The\nformer prevents positive language transfer while\nthe latter restricts the student’s capabilities to the\ntasks and languages in the ﬁne-tuned teacher LMs\n(as shown in Figure 1).\nWe focus on the problem of merging multiple\npre-trained LMs into a single multilingual student\nLM in the task-agnostic setting. To the best of our\nknowledge, this is the ﬁrst effort of its kind, and\nmakes the following contributions:\n• We propose MERGE DISTILL , a task-agnostic\ndistillation approach to mergemultiple teacher\nLMs at the pre-training stage, to train a strong\nmultilingual student LM that can then be ﬁne-\ntuned for any task on all languages in the stu-\ndent LM. Our approach is more maintainable\n(fewer models), compute efﬁcient and teacher-\narchitecture agnostic (since we obtain ofﬂine\npredictions).\n• We use MERGE DISTILL to i) combine mono-\nlingual teacher LMs into a single multilingual\nstudent LM that is competitive with or outper-\nforms individual teachers, ii) combine multi-\nlingual teacher LMs, such that the overlapping\nlanguages can learn from multiple teachers.\n• Through extensive experiments and analysis,\nwe study the importance of typological simi-\nlarity in building multilingual models, and the\nimpact of strong teacher LM vocabularies and\npredictions in our framework.\n2876\n2 Related Work\nLanguage Model pre-training has evolved\nfrom learning pre-trained word embeddings\n(Mikolov et al., 2013) to contextualized word\nrepresentations (McCann et al., 2017; Peters\net al., 2018; Eriguchi et al., 2018) and to the most\nrecent Transformer-based (Vaswani et al., 2017)\nLMs (Devlin et al., 2019; Liu et al., 2019a) with\nstate-of-the-art results on various downstream NLP\ntasks. Most commonly, these LMs are pre-trained\nwith the MLM objective (Taylor, 1953) on large\nunsupervised corpora and then ﬁne-tuned on\nlabeled data for the task at hand. Concurrently,\nmultilingual LMs (Lample and Conneau, 2019;\nSiddhant et al., 2020; Conneau et al., 2020; Chung\net al., 2021), trained on massive amounts of\nmultilingual data, have surpassed cross-lingual\nword embedding spaces (Glava ˇs et al., 2019;\nRuder et al., 2019) to achieve state-of-the-art in\ncross-lingual transfer. While Pires et al. (2019);\nWu and Dredze (2019) highlight their cross-lingual\nability, several limitations have been studied.\nConneau et al. (2020) highlight the curse of\nmultilinguality. Hu et al. (2020) highlight that\neven the best multilingual models do not yield\nsatisfactory transfer performance on the XTREME\nbechmark covering 9 tasks and 40 languages.\nImportantly, Wu and Dredze (2020) and Lauscher\net al. (2020) observe that these models signiﬁcantly\nunder-perform for low-resource languages as\nrepresentation of these languages in the vocabulary\nand pre-training corpora are severely limited.\nLanguage-speciﬁc LMs are becoming increas-\ningly popular as issues with multilingual language\nmodels persist. As language identiﬁcation systems\nare extended to 1000+ languages (Caswell et al.,\n2020), increasing capacity for a single model to\nuniformly represent all languages is prohibitive.\nOften, practitioners prefer to have a model\nperforming well on a subset of languages that\ntheir application calls for. To address this, the\ncommunity continues its efforts in building strong\nmulti-domain language models using linguistic\nexpertise. A few examples of these are AraBERT\n(Antoun et al., 2020), CamemBERT (Martin et al.,\n2020), and FinBERT (Virtanen et al., 2019).2\nKnowledge Distillation in pre-trained LMs has\n2(Nozza et al., 2020) maintain an ever-growing list of\nBERT models here\nmost commonly been used for task-speciﬁc model\ncompression of a teacher into a single-task stu-\ndent (Tang et al., 2019; Kaliamoorthi et al., 2021).\nThis has been extended to perform task-speciﬁc\ndistillation of multiple single-task teachers into one\nmulti-task student (Clark et al., 2019; Liu et al.,\n2020; Turc et al., 2019). In the task-agnostic sce-\nnario, prior work has focused on distilling a single\nlarge teacher model into a student model leverag-\ning teacher predictions (Sanh et al., 2019) or inter-\nnal teacher representations (Sun et al., 2020, 2019;\nWang et al., 2020) with the goal of model compres-\nsion. To the best of our knowledge, this is the ﬁrst\nattempt to perform task-agnostic distillation from\nmultiple teachers into a single task-agnostic stu-\ndent. In the context of neural machine translation,\nTan et al. (2019) come close to our work where they\nattempt to combine multiple single language-pair\nteacher models to train a multilingual student. How-\never, our work differs from theirs in three key as-\npects: 1) our students are task-agnostic while theirs\nare task-speciﬁc, 2) we can leverage pre-existing\nteachers while they cannot, and 3) we support teach-\ners with overlapping sets of languages while they\nonly consider single language-pairs teachers.\n3 M ERGE DISTILL\nNotations: Let K denote the set of languages we\ntrain our student LM on and T denote the set of\nteacher LMs input to MERGE DISTILL 3. Conse-\nquently, Tk denotes the set of teacher LMs trained\non language k, where |Tk|≥ 1 ∀k ∈K.\n3.1 Workﬂow\nAn overview of MERGE DISTILL is presented in\nFigure 2. Here we detail each step involved in\ntraining the student LM from multiple teacher LMs.\nStep 1: Input\nThe input to MERGE DISTILL is a set of pre-trained\nteacher LMs and pre-training transfer corpora for\nall the languages we wish to train our student LM\non. With reference to Figure 2, the student LM is\ntrained on K ={English (en), Spanish (es), Hindi\n(hi), Korean (ko)}. We combine four teacher LMs\ncomprising of three monolingual and one multi-\nlingual LM. The monolingual LMs are trained on\nEnglish (Men), Spanish (Mes), and Korean (Mko)\nwhile the multilingual LM is trained on English\n3Note that T can comprise of monolingual or multilingual\nmodels\n2877\nand Hindi (Men,hi). Therefore, for each language,\nthe corresponding set of teacher LMs (Tk) can be\ndeﬁned as: [Ten = {Men,Men,hi},Tes = {Mes},\nThi = {Men,hi},Tko = {Mko}]. First, the pre-\ntraining transfer corpora is tokenized and masked\nfor each language using their respective teacher\nLM’s tokenizer. For the language with two\nteachers, English, we tokenize each example using\nboth the teacher LMs.\nStep 2: Ofﬂine Teacher LM Evaluation\nWe now obtain predictions and logits for each\nmasked, tokenized example in each language, by\nevaluating their respective teacher LMs. For En-\nglish, we obtain predictions from both Men and\nMen,hi on their respective copies of each training\nexample. In an ideal situation, we believe that\nmultiple strong teachers can present a multi-view\ngeneralisation to the student as each teacher learns\ndifferent features in training. Let x denote a se-\nquence of tokens where xm = {x1,x2,x3...xn}\ndenote the masked tokens, and x−m denote the\nnon-masked tokens. Let v be the vocabulary of\nstudent LM θs. In the conventional case of learning\nfrom gold labels, we minimize the cross-entropy\nof student logit distribution for a masked word xmi ,\nwith the one-hot label vj, given by:\nP(xmi ,vj)= 1(xmi =vj) ×log p(xmi =vj|x−m; θs)\n(1)\nWith the teacher evaluations, we obtain predictions\n(and corresponding logits) of the teacher for the\nmasked tokens. Let us denote the teacher output\nprobability distribution (softmax over logits) for\ntoken xmi by Q(xmi |x−m; θt). Therefore, in addi-\ntion to the loss from gold labels, we minimize the\nentropy between the student logits and the teacher\ndistribution, given by :\nˆP(xmi ,vj)=Q(x mi =vj|x−m; θt)×\nlog p(xmi =vj|x−m; θs) (2)\nIt is extremely burdensome (both memory and\ntime) to load multiple teacher LMs and obtain\npredictions during training. Hence, we ﬁrst store\nthe top-k logits for each masked word ofﬂine,\nloading and normalizing them during student LM\ntraining, similar to (Tan et al., 2019). Additionally,\nobtaining ofﬂine predictions gives one the freedom\nto use expensive teacher LMs without increasing\nthe student model training costs and makes our\nframework teacher-architecture agnostic.\nStep 3: Vocab Mapping\nA deterrent in attempting to distill from multiple\npre-trained teacher LMs is that each LM has its\nown vocabulary. This makes it non-trivial to uni-\nformly process an input example for consumption\nby both the teacher and student LMs. Our student\nmodel’s vocabulary is the union of all teacher\nLM vocabularies. In the vocab mapping step, the\ninput indices , prediction indices , and the gold\nlabel indices, obtained after evaluation from each\nteacher LM are processed using a teacher→student\nvocab map. This converts each teacher token index\nto its corresponding student token index, ready for\nconsumption by the student model. For simplicity,\neach teacher and student LM uses WordPiece\ntokenization (Schuster and Nakajima, 2012; Wu\net al., 2016) in all our experiments.\nStep 4: Student LM Training\nThe processed input indices , prediction indices,\nand gold label indices can now be used to train\nthe multilingual student LM. In training, exam-\nples from different languages are shufﬂed together,\neven within a batch. We train the student LM with\nthe MLM objective. Let LMLM denote the MLM\nloss from gold labels. Therefore, with reference to\nEquation 1 :\nLMLM(xm|x−m) =−1\nn\nn∑\ni=1\n|v|∑\nj=1\nP(xmi ,vj)\nIn addition to learning from gold labels, we use\nteacher predictions as soft labels and minimize the\ncross entropy between student and teacher distribu-\ntions. Let LKD denote the KD loss from a single\nteacher LM. With reference to Equation 2:\nLKD(xm|x−m) =−1\nn\nn∑\ni=1\n|v|∑\nj=1\nˆP(xmi ,vj);\nThe total loss across all languages is minimized, as\nshown below:\nLALL =\nK∑\nk=1\nλ(LTk\nKD) + (1−λ)Lk\nMLM\nIn the case of multiple teacher LMs, we have n\ntokenized instances for a given example (where n\ndenotes the number of teachers for a particular lan-\nguage). In this case, each example in English has\n2878\ntwo copies – one tokenized using Men and another\nusing Men,hi. Thus, we explore two possibilities of\ntraining in this multi-teacher scenario :\n• Include all the copies in training. Here the\nmodel is exposed to n different teacher LM\npredictions, each presenting a multi-view gen-\neralisation to the student LM.\n• Include the best copy in training. The best\ncopy is the one having minimum teacher LM\nloss for a given example. Here the model is\nonly exposed to the best teacher LM predic-\ntions for each example.\n4 Experiments\nIn this section, we aim to answer the following\nquestions :\n1) How effective is MERGE DISTILL in combining\nmonolingual teacher LMs, to train a multilingual\nstudent LM that leverages the beneﬁts of multi-\nlinguality while performing competitively with\nindividual teacher LMs? (Section 4.2)\n2) How effective is MERGE DISTILL in combining\nmultilingual teacher LMs, trained on an overlap-\nping set of languages, such that each language can\nbeneﬁt from multiple teachers? (Section 4.3)\n3) How important are the teacher LM vocabulary\nand predictions in MERGE DISTILL ? Further, can\nMERGE DISTILL enable pre-trained zero-shot\ntransfer? (Section 4.4)\n4.1 Setup\nData: For all our experiments, we use Wikipedia\ndata as pre-training transfer corpora to train\nthe student model, irrespective of the data used\nin training individual teacher LMs. We use\nα= 0.7 for exponential smoothing of data across\nlanguages, similar to mBERT (Devlin et al., 2019).\nModel Size : Since transformer-based models\nperform better as capacity increases (Conneau\net al., 2020; Arivazhagan et al., 2019), we keep the\nnumber of parameters close to mBERT (∼178M)\nby appropriately modifying the vocabulary\nembedding size (like Lan et al. (2019)) to isolate\nthe positive effects of learning from teacher LMs.\nStudent Language Language Family Model\nStudentsimilar\nEnglish Indo-European BERT(Devlin et al., 2019)German Indo-European DeepSet(Chan et al., 2020)Italian Indo-European ItalianBERT(Schweter, 2020b)Spanish Indo-European BETO(Ca˜nete et al., 2020)\nStudentdissimilar\nArabic Afroasiatic AraBERT(Antoun et al., 2020)English Indo-European BERT(Devlin et al., 2019)Finnish Uralic FinBERT(Virtanen et al., 2019)Turkish Turkic BERTurk(Schweter, 2020a)Chinese Sino-Tibetan ChineseBERT(Devlin et al., 2019)\nTable 1: Monolingual BERT Models used as teacher\nLMs. Please refer to Section 4.2 for details.\nDistillation Parameters : We have two hyper-\nparameter choices here: 1) k in top-k logits - as\nit increases, we observe that while performances\nremain similar, storing k >8 number of predic-\ntions for each masked word ofﬂine signiﬁcantly\nincreases resource requirements4. Hence, we set\nk=8 in all our experiments. 2) the value of λ in\nthe loss function, which decides the proportion of\nteacher loss, is annealed through training similar to\nClark et al. (2019).\nEvaluation Metrics: We report F1 scores for struc-\ntured prediction tasks (NER, POS), accuracy (Acc.)\nscores for sentence classiﬁcation tasks (XNLI,\nPAWS-X), and F1/Exact Match (F1/EM) scores\nfor question answering tasks (XQuAD, MLQA,\nTyDiQA). We also report a task-speciﬁc relative\ndeviation from teachers (RDT) (in %) averaged\nacross all languages ( n). For each task, RDT is\ncalculated as:\nRDT(S,{T1,..., Tn}) =100\nn\nn∑\ni=1\n(PTi −PS)\nPTi\n(3)\nwhere PTi and PS are performances of the ith\nteacher and student LMs, respectively.\n4.2 Monolingual Teacher LMs\nPre-training: In this experiment, we use pre-\nexisting monolingual teacher LMs, as shown in\nTable 1, to train a multilingual student LM on\nthe union of all teacher languages. In this setup,\n|Tk|= 1∀k ∈K, i.e., each language can learn\nfrom its respective monolingual teacher LM only.\nOur teacher selection and setup follows a\ntwo-step process. First, we aim to select languages\nhaving pre-trained monolingual LMs available,\nand evaluation sets across a number of downstream\ntasks. This makes us choose teacher LMs for :\nArabic (ar), Chinese (zh), English (en), Finnish (ﬁ),\n4More details in Appendix A.4\n2879\nLanguage Model NER UDPOS QA\nF1 F1 F1/EM\nEnglish BERT 89.5 96.6 87.1/78.6\nStudentsimilar 89.8 96.3 89.8/82.1\nGerman DeepsetBERT 93.0 98.3 -\nStudentsimilar 93.9 98.3 -\nItalian ItalianBERT 94.5 98.6 73.5/61.6\nStudentsimilar 95.2 98.6 75.8/63.8\nSpanish BETO 94.2 99.0 74.9/56.6\nStudentsimilar 94.7 98.9 76.5/58.4\nRDT(%) +0.6 -0.1 +2.8/+3.7\nArabic AraBERT 94.3 96.3 83.1/68.6\nStudentdissimilar 93.7 96.4 81.3/66.6\nChinese ChineseBERT 83.0 96.9 81.8/81.8\nStudentdissimilar 82.6 96.8 80.8/80.8\nEnglish BERT 89.5 96.6 87.1/78.6\nStudentdissimilar 89.5 96.3 88.6/80.7\nFinnish FinBERT 94.4 97.9 81.0/68.8\nStudentdissimilar 94.4 95.5 77.7/65.9\nTurkish BERTurk 95.2 95.6 76.7/59.8\nStudentdissimilar 95.4 92.9 76.2/59.1\nRDT(%) -0.2 -1.1 -1.3/-1.4\nTable 2: Results for monolingual teacher LMs and mul-\ntilingual students on downstream tasks as described in\nSection 4.2. Relative deviations of 5% or less from\nteacher (i.e., RDT ≥−5%) are marked in bold. We\nﬁnd that Studentsimilar outperforms individual teacher\nLMs, with a maximum gain of upto +2.8/+3.7% for\nQA, while Studentdissimilar is competitive with teacher\nLMs, with a maximum drop of -1.3/-1.4% for QA.\nPlease refer to Section 4.2 for details.\nGerman (de), Italian (it), Spanish (es), and Turkish\n(tr). Second, as previous work has evidenced\npositive transfer between related languages in a\nmultilingual setup (Pires et al., 2019; Wu and\nDredze, 2020), we further group the chosen teacher\nLMs based on language families as shown in Table\n1, where:\ni) Studentsimilar is trained on four closely\nrelated languages from the Indo-European family –\nde, en, es and it.\nii) Studentdissimilar is trained on languages\nfrom different language families – ar, en, ﬁ, tr and\nzh.\nBoth student LMs have a BERT-base architec-\nture. Studentsimilar has a vocabulary size of\n99,112 with a total of 162M parameters, while\nStudentdissimilar has a vocabulary size of 180,996\nwith a total of 225M parameters. We keep a batch\nsize of 4096 and train for 250,000 steps with a\nmaximum sequence length of 512.\nFine-tuning: We evaluate both the teacher\nStudent LanguageTeacher LM Student LM% of DataTokens Tokens\nStudentsimilar\nEnglish 3300M 2285M 69.25%\nGerman 23723M 847M 3.57%\nItalian 13139M 506M 3.85%\nSpanish 3000M 639M 21.31%\nTotal 43162M 4277M 9.9%\nStudentdissimilar\nArabic 8600M 135M 1.58%\nEnglish 3300M 2285M 69.25%\nFinnish 3000M 83M 2.77%\nTurkish 4405M 60M 1.36%\nChinese 71M 71M 100.00%\nTotal 19376M 2634M 13.6%\nTable 3: Number of Tokens (in Millions) in the teacher\n(Table 1) and student LMs as described in Section 4.2\nand student LMs on three downstream tasks with\nin-language ﬁne-tuning for each task5 :\ni) Named Entity Recognition (NER): We use the\nWikiAnn (Pan et al., 2017; Rahimi et al., 2019)\ndataset for all languages.\nii) Part-of-Speech Tagging(UDPOS): We use the\nUniversal Dependencies v2.6 (Zeman et al., 2020)\ndataset for all languages.\niii) Question Answering (QA): We use DRCD for\nzh (Shao et al., 2018), TQuAD6 for tr, SQuADv1.1\n(Rajpurkar et al., 2016) for en, SQuADv1.1-\ntranslated for it (Croce et al., 2018) and es (Carrino\net al., 2020) and the TyDiQA-GoldP dataset (Clark\net al., 2020) for ar and ﬁ.\nResults: We report results of our teacher and\nstudent LMs in Table 2. Overall, we ﬁnd that\nStudentsimilar outperforms individual teacher mod-\nels on NER (+0.6%) and QA (+2.8/3.7%) while\nperforming competitively on UDPOS (-0.1%).\nStudentdissimilar is competitive with the teacher\nLMs with only small differences of up to 1.3/1.4%\n(QA), as shown in Table 2. For each language,\nwe ﬁnd Studentsimilar is either competitive or\noutperforms its respective teacher LM. Our re-\nsults provide evidence for positive transfer across\nlanguages in two ways. First, we observe that\nStudentsimilar outperforms Studentdissimilar for\nthe common language - English. Given that the\nEnglish teacher (BERT) and the pre-training trans-\nfer corpora 7 is common for both student LMs,\n5More details in Appendix A.3\n6https://tquad.github.io/turkish-nlp-qa-dataset\n7In fact, we can hypothesize that Studentdissimilar sees\nmore English tokens as compared to Studentsimilar because\nthe Non-English languages in Studentdissimilar are relatively\nlow resourced (a sum total of 349M unique tokens) in compar-\nison to Studentsimilar (a sum total of 1992M unique tokens)\n2880\nLanguages Model Teacher PANX UDPOS PAWSX XNLI XQUAD MLQA TyDiQAAvg.F1 F1 Acc. Acc. F1/EM F1/EM F1/EM\nMuRIL Languages\nmBERT - 58.8 68.5 93.4 66.2 70.3/57.5 65.0/50.8 62.5/52. 69.2\nMuRIL - 76.9 74.5 95.0 74.4 77.7/64.2 73.6/58.6 76.1/60.2 78.3\nStudentMuRIL MuRIL 69.3 72.3 95.4 71.9 75.7/62.1 72.0/56.3 70.7/59.2 75.3\nStudentmBERTmBERT 38.1 52.1 93.5 64.8 56.9/44.8 51.1/39.7 41.6/33.9 56.9\nStudentBothall mBERT+MuRIL67.9 72.3 94.5 71.1 76.1/62.9 70.4/55.5 70.8/55.3 74.7\nStudentBothbest mBERT+MuRIL68.5 71.5 93.9 70.7 77.7/64.3 70.8/55.6 70.6/58.4 74.8\nRDT(StudentMuRIL,mBERT)(%) +17.9 +5.6 +2.1 +8.6 +7.7/+8 +10.8/+10.8 +13.1/+12.3 +8.8\nRDT(StudentMuRIL,MuRIL)(%) -9.9 -3 +0.4 -3.4 -2.6/-3.3 -2.2/-3.9 -7.1/-1.7 -3.8\nNon MuRIL Languages\nmBERT - 63.5 71.1 80.2 65.9 62.2/47.1 59.7/41.4 60.4/46.1 66.1\nStudentMuRIL mBERT 63.9 72.8 83.3 68.7 66.5/51.2 63.1/44.4 61.7/45.0 68.6\nStudentmBERTmBERT 64.6 72.1 84.0 68.8 64.5/49.0 61.1/42.7 58.9/44.1 67.7\nStudentBothall mBERT 64.1 72.6 83.9 68.1 61.3/47.1 60.5/42.2 59.7/44.0 67.2\nStudentBothbest mBERT 63.3 72.6 83.2 67.2 66.0/50.6 61.4/43.2 62.4/46.5 68.0\nRDT(StudentMuRIL,mBERT)(%) +0.6 +2.4 +3.9 +4.3 +6.9/+8.7 +5.7/+7.2 +2.2/-2.4 +3.8\nTable 4: Results for multilingual teacher and student LMs on the XTREME benchmark. We compare perfor-\nmances of three student LM variants as described in Section 4.3 to the two teachers mBERT and MuRIL. Relative\ndeviations of 5% or less from teacher (i.e.,RDT ≥−5%) are marked in bold. Overall, we ﬁnd thatStudentMuRIL\nperforms the best among all student variants and report itsRDT (in %) (Equation 3) from the two teachers. Please\nrefer to Section 4.3 for a detailed analysis.\nwe can attribute this gain to the fact that En-\nglish is trained with linguistically and typolog-\nically similar languages in Studentsimilar. Sec-\nond, Studentsimilar outperforms its teacher LMs\nwhile Studentdissimilar is competitive for all lan-\nguages. These two results across all languages\npoint towards Studentsimilar beneﬁting from a pos-\nitive transfer across similar languages. In Table 3,\nwe observe that Studentsimilar is trained on 9.9%\nof the total unique tokens seen by its respective\nteacher LMs and Studentdissimilar lies close with\n13.6%. Despite this huge disparity in pre-training\ncorpora, student LMs are competitive with their\nteachers. This encouraging result proves that even\nwith very limited data, MERGE DISTILL enables\none to combine strong monolingual teacher LMs\nto train competitive student LMs that can leverage\nthe beneﬁts of multilinguality.\n4.3 Multilingual Teacher LMs\nPre-training: In this experiment, we make use\nof pre-existing multilingual models: mBERT and\nMuRIL. mBERT is trained on 104 languages and\nMuRIL covers 12 of these (11 Indian languages +\nEnglish): Bengali (bn), English (en), Gujarati (gu),\nHindi (hi), Kannada (kn), Malayalam (ml), Marathi\n(mr), Nepali (ne), Punjabi (pa), Tamil (ta), Telugu\n(te), and Urdu ( ur), with higher performance for\nthese languages on the XTREME benchmark. We\ntrain the student model on all 104 languages. In\nthis case, the MuRIL Languages (MuL) have two\nas shown in Table 3\nteachers (mBERT and MuRIL) and theNon-MuRIL\nLanguages (Non-MuL) can learn from mBERT\nonly. Therefore, while we only use mBERT\nas the teacher LM for Non-MuL across all ex-\nperiments, we consider three possibilities for MuL :\ni) StudentMuRIL: We only use MuRIL as\nthe teacher LM and each input training example is\ntokenized using MuRIL.\nii) StudentmBERT: We only use mBERT as the\nteacher LM and each input training example is\ntokenized using mBERT.\niii) StudentBoth: As highlighted in Section 3,\nwe consider two possibilities to incorporate both\nteacher LM predictions in training:\n• StudentBoth all: Tokenize each input exam-\nple using mBERT and MuRIL separately and\ninclude both copies in training.\n• StudentBoth best: Tokenize each input ex-\nample using mBERT and MuRIL separately\nand include only the best copy in training. The\nbest copy is the one having minimum teacher\nLM loss for the example.\nNote, it is non-trivial to tokenize each example in a\nway that is compatible with all teacher LMs. One\nmust resort to tokenization using an intersection of\nvocabularies which is sub-optimal.\nAll the student LMs use a BERT-base architecture\nand have a vocabulary size of 288,973. We reduce\nour embedding dimension to 256 as opposed to\n2881\nModel V ocabulary Labels PANX UDPOS PAWSX XNLI XQUAD MLQA TyDiQA Avg.\nSM1 mBERT Gold 63.2 73.0 94.8 71.2 70.2/57.9 65.1/51.3 60.8/48.7 71.2\nSM2 mBERT∪MuRIL Gold 69.3 73.9 95.3 71.2 76.2/63.171.1/56.0 70.9/56.075.4\nSM3 mBERT∪MuRIL Gold+Teacher69.3 72.3 95.4 71.9 75.7/62.172.0/56.3 70.7/59.275.3\nSM2100k mBERT∪MuRIL Gold 65.5 72.3 94.3 67.5 72.3/58.2 66.9/51.5 62.5/51.9 71.6\nSM3100k mBERT∪MuRIL Gold+Teacher 71.2 73.5 93.1 69.6 76.4/62.9 69.1/53.9 68.6/54.9 74.5\nTable 5: Importance of teacher vocabulary and predictions in MERGE DISTILL . We observe maximum perfor-\nmance gains, by changing the vocabulary from mBERT in SM1 to (mBERT∪MuRIL) vocabulary in SM2. Here,\nSM3 is the standard StudentMuRIL. We also observe that SM3 100k, trained for 20% of the total training steps, is\ncompetitive to SM3 and signiﬁcantly outperforms SM2 100k, highlighting the importance of teacher LM predic-\ntions in a limited data scenario. Please see Section 4.4 for details.\n768 to bring down the model size to be around\n160M, comparable to mBERT (178M). We keep a\nbatch size of 4096 and train for 500,000 steps with\na maximum sequence length of 512.\nFinetuning: We report zero-shot performance for\nall languages in the XTREME (Hu et al., 2020)\nbenchmark8.\nResults: We report results of our teacher and\nstudent LMs in Table 4. Overall, we ﬁnd that\nStudentMuRIL performs the best among all student\nvariants. For Non-MuL, StudentMuRIL beats the\nteacher (mBERT) by an average relative score of\n3.8%. For MuL, StudentMuRIL beats one teacher\n(mBERT) by 8.8%, but underperforms the other\nteacher (MuRIL) by 3.8%. There can be two fac-\ntors at play here. MuRIL is trained on monolingual\nand parallel data 9 while the student LMs only see\n∼22% of unique tokens in comparison. MuRIL\nalso has different language sampling strategies\n(α= 0.3 as opposed to 0.7 in our setting, where\na lower αvalue upsamples more rigorously from\nthe tail languages), which have a signiﬁcant role to\nplay in multilingual model performances (Conneau\net al., 2020). We also observe a signiﬁcant drop in\nStudentmBERT’s performance for MuL when com-\npared to the other student LM variants. This might\nbe because the input is tokenized using the mBERT\ntokenizer which prevents learning from MuRIL to-\nkens in the student vocabulary. For StudentBoth,\nwe do not observe much of a difference between\nStudentBoth all and StudentBoth best. This obser-\nvation may differ with one’s choice of teacher LMs\ndepending on how well it performs for a particular\nlanguage. In our case, we don’t observe much of a\ndifference in incorporating mBERT predictions for\nMuL.\n8More details in Appendix A.3\n9More details in Appendix A.2\n4.4 Further Analysis\nThe importance of vocabulary and teacher LM\npreditions: In Table 4, we see that StudentMuRIL\nsigniﬁcantly outperforms mBERT for MuL,\ndespite both being trained on Wikipedia corpora,\nand having comparable model sizes. With regard\nto MuL, StudentMuRIL differs from mBERT in\ntwo main aspects – i) StudentMuRIL’s vocabulary\nis a union of mBERT and MuRIL vocabularies. ii)\nStudentMuRIL is trained with additional MuRIL\npredictions as soft labels. To disentangle the\nrole both these factors play in StudentMuRIL’s\nimproved performance, we train two models :\ni) SM1 is trained exactly like StudentMuRIL, but\nwith mBERT vocabulary and on gold labels.\nii) SM2 is trained using StudentMuRIL’s vocabu-\nlary (mBERT ∪MuRIL) but on gold labels only,\nwithout teacher predictions.\nThe results are summarized in Table 5. Note,\nwe refer to StudentMuRIL as SM3. Overall, we\nobserve a ∼4.2% gain in average performance\nfor SM2 over SM1. This clearly highlights that\ngiven ﬁxed data and model capacity, LM training\nsigniﬁcantly beneﬁts by incorporating a strong\nteacher’s vocabulary.\nFurthermore, we also observe that SM2 and SM3\nachieve competitive performances despite SM3\nbeing additionally trained on teacher LM labels. To\nmotivate the need for teacher predictions, Hinton\net al. (2015) argue that when soft targets have high\nentropy, they provide much more information per\ntraining case than hard targets and can be trained\non much less data than the original cumbersome\nmodel. In our case, we hypothesize that training\non 500,000 steps exposes the model to sufﬁcient\ndata for it to generalize well enough and mask the\nbeneﬁts of teacher LM predictions. To validate this,\nwe evaluate the performances of SM2 and SM3,\n2882\n20% into training (i.e. 100,000 steps / 500,000\ntotal steps) as shown in Table 5. We observe a\n∼2.9% gain in average performance for SM3\nover SM2, clearly highlighting the importance of\nteacher LM predictions in a limited data scenario.\nThis is especially important when one has access\nto very limited monolingual data and a strong\nteacher LM for a particular language.\nPre-trained zero-shot transfer: Interestingly,\nStudentMuRIL performs the best on almost all\ntasks for Non-MuL. This hints at positive transfer\nfrom strong teachers to languages that the teacher\ndoes not cover at all, due to the shared multilin-\ngual representations.10 This would mean that learn-\ning from strong teachers can improve the student\nmodel’s performance in a zero-shot manner on re-\nlated languages not covered by the teacher. This\nwould make MERGE DISTILL highly beneﬁcial for\nlow-resource languages that do not have a strong\nteacher or limited gold data. We leave this explo-\nration to future work.\n5 Conclusion\nIn this paper we address the problem of merging\nmultiple pre-trained teacher LMs into a single mul-\ntilingual student LM by proposing MERGE DIS-\nTILL , a task-agnostic distillation method. To the\nbest of our knowledge, this is the ﬁrst attempt of its\nkind. The student LM learned by MERGE DISTILL\nmay be further ﬁne-tuned for any task across all\nof the languages covered by the teacher LMs. Our\napproach results in better maintainability (fewer\nmodels) and is compute efﬁcient (due to ofﬂine\npredictions). We use MERGE DISTILL to i) com-\nbine monolingual teacher LMs into one student\nmultilingual LM which is competitive with the\nteachers, thereby demonstrating positive cross-\nlingual transfer, and ii) combine multilingual LMs\nto train student LMs that learn from multiple teach-\ners. Through experiments on multiple benchmark\ndatasets, we show that student LMs learned by\nMERGE DISTILL perform competitively or even\noutperform teacher LMs trained on orders of mag-\nnitude more data. We disentangle the positive im-\npact of incorporating strong teacher LM vocabu-\n10For example, if you want to train a multilingual model\ncovering English and a closely related low-resource language\nfor which there exists no strong teacher, it may be possible\nto improve performance for the low resource language using\nteacher predictions for English only, due to a shared embed-\nding space and possibly shared sub-words.\nlaries and learning from teacher LM predictions,\nhighlighting the importance of the latter in a lim-\nited data scenario. We also ﬁnd that MERGE DIS-\nTILL enables positive transfer from strong teachers\nto languages not covered by them (i.e. zero-shot\ntransfer). Our work bridges the gap between the\nuniverse of language-speciﬁc models and massively\nmultilingual LMs, incorporating beneﬁts of both\ninto one framework.\n6 Acknowledgements\nWe would like to thank the anonymous reviewers\nfor their insightful and constructive feedback. We\nthank Iulia Turc, Ming-Wei Chang, and Slav Petrov\nfor valuable comments on an earlier version of this\npaper.\nReferences\nAhmed Abdelali, Kareem Darwish, Nadir Durrani, and\nHamdy Mubarak. 2016. Farasa: A fast and furious\nsegmenter for arabic. In Proceedings of the 2016\nconference of the North American chapter of the as-\nsociation for computational linguistics: Demonstra-\ntions, pages 11–16.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nCasimiro Pio Carrino, Marta R. Costa-juss `a, and Jos ´e\nA. R. Fonollosa. 2020. Automatic Spanish transla-\ntion of SQuAD dataset for multi-lingual question\nanswering. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 5515–\n5523, Marseille, France. European Language Re-\nsources Association.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and\nAnkur Bapna. 2020. Language ID in the wild:\nUnexpected challenges on the path to a thousand-\nlanguage web text corpus. In Proceedings of\n2883\nthe 28th International Conference on Computa-\ntional Linguistics , pages 6588–6608, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nJos´e Ca ˜nete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge P ´erez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nBranden Chan, Stefan Schweter, and Timo M ¨oller.\n2020. German’s next language model.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. In International Conference on\nLearning Representations.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4536–4546, Online. Association for Computational\nLinguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nKevin Clark, Minh-Thang Luong, Urvashi Khandel-\nwal, Christopher D. Manning, and Quoc V . Le. 2019.\nBAM! born-again multi-task networks for natural\nlanguage understanding. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5931–5937, Florence, Italy.\nAssociation for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nDanilo Croce, Alexandra Zelenanska, and Roberto\nBasili. 2018. Neural learning for question answer-\ning in italian. In AI*IA 2018 – Advances in Arti-\nﬁcial Intelligence, pages 389–402, Cham. Springer\nInternational Publishing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAkiko Eriguchi, Melvin Johnson, Orhan Firat, Hideto\nKazawa, and Wolfgang Macherey. 2018. Zero-\nshot cross-lingual classiﬁcation using multilin-\ngual neural machine translation. arXiv preprint\narXiv:1809.04686.\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun,\nand Jingjing Liu. 2020. Filter: An enhanced fu-\nsion method for cross-lingual language understand-\ning. arXiv preprint arXiv:2009.05166.\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 710–721,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. arXiv preprint arXiv:2003.11080.\nPrabhu Kaliamoorthi, Aditya Siddhant, Edward Li, and\nMelvin Johnson. 2021. Distilling large language\nmodels into tiny and effective students using pqrnn.\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani,\nSavya Khosla, Atreyee Dey, Balaji Gopalan,\nDilip Kumar Margam, Pooja Aggarwal, Rajiv Teja\nNagipogu, Shachi Dave, Shruti Gupta, Subhash\nChandra Bose Gali, Vish Subramanian, and Partha\nTalukdar. 2021. Muril: Multilingual representations\nfor indian languages.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nrussian language. arXiv preprint arXiv:1905.07213.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual Language Model Pretraining. In Proceed-\nings of NeurIPS 2019.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glava ˇs. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\n2884\nHairong Liu, Mingbo Ma, Liang Huang, Hao Xiong,\nand Zhongjun He. 2019a. Robust neural machine\ntranslation with joint textual and phonetic embed-\nding. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 3044–3049, Florence, Italy. Association for\nComputational Linguistics.\nLinqing Liu, Huan Wang, Jimmy Lin, Richard Socher,\nand Caiming Xiong. 2020. Mkd: a multi-task knowl-\nedge distillation approach for pretrained language\nmodels.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4487–4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSu´arez, Yoann Dupont, Laurent Romary, ´Eric Ville-\nmonte de la Clergerie, Djam ´e Seddah, and Beno ˆıt\nSagot. 2019. Camembert: a tasty french language\nmodel. arXiv preprint arXiv:1911.03894.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Proceedings of NIPS\n2017, pages 6294–6305.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. Advances in neural information processing sys-\ntems, 26:3111–3119.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020.\nWhat the [mask]? making sense of language-speciﬁc\nbert models. arXiv preprint arXiv:2003.02912.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1946–1958.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual word embedding\nmodels. Journal of Artiﬁcial Intelligence Research ,\n65:569–631.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2020. How good is\nyour tokenizer? on the monolingual performance\nof multilingual language models. arXiv preprint\narXiv:2012.15613.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nStefan Schweter. 2020a. Berturk - bert models for turk-\nish.\nStefan Schweter. 2020b. Italian bert and electra mod-\nels.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. Drcd: a chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nAditya Siddhant, Melvin Johnson, Henry Tsai, Naveen\nArivazhagan, Jason Riesa, Ankur Bapna, Orhan Fi-\nrat, and Karthik Raman. 2020. Evaluating the\nCross-Lingual Effectiveness of Massively Multilin-\ngual Neural Machine Translation. In Proceedings of\nAAAI 2020.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\n2885\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\nYan Liu. 2019. Multilingual neural machine trans-\nlation with knowledge distillation. arXiv preprint\narXiv:1902.10461.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciﬁc knowledge from bert into simple neural net-\nworks. arXiv preprint arXiv:1903.12136.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quarterly,\n30(4):415–433.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical bert models for sequence labeling.\narXiv preprint arXiv:1909.00100.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish. arXiv preprint arXiv:1912.07076.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nDaniel Zeman, Joakim Nivre, and Mitchell Abrams\net al. 2020. Universal dependencies 2.6.\nLINDAT/CLARIAH-CZ digital library at the\nInstitute of Formal and Applied Linguistics (´UFAL),\nFaculty of Mathematics and Physics, Charles\nUniversity.\nA Appendix\nA.1 Knowledge Distillation\nWe train our LMs with the MLM objective. Let\nx denote a sequence of tokens where xm =\n{x1,x2,x3...xn}denote the masked tokens, and\nx−m denote the non-masked tokens. Let v be the\nvocabulary of LMθ. The log-likelihood loss (cross-\nentropy with one-hot label) can be formulated as\nfollows:\nLMLM(xm|x−m) =−1\nn\nn∑\ni=1\n|v|∑\nk=1\nP(xmi ,k);\nP(xmi ,k) =1(xmi = k)logp(xmi = k|x−m; θ)\nIn a distillation setup, the student is trained to not\nonly match the one-hot labels for masked words,\nbut also the probability output distribution of the\nteacher t. Let us denote the teacher output probabil-\nity distribution for token xmi by Q(xmi |x−m; θt).\nThe cross entropy between the teacher and student\ndistributions then serves as the distillation loss :\nLKD(xm|x−m) =−1\nn\nn∑\ni=1\n|v|∑\nk=1\nˆP(xmi ,k);\nˆP(xmi ,k) = Q(xmi = k|x−m; θt)\nlogp(xmi = k|x−m; θ)\nThe total loss is then deﬁned as :\nLALL = λLKD + (1−λ)LMLM\nWith the addition of the teacher, the target distri-\nbution is no longer a single one-hot label, but a\nsmoother distribution with multiple words having\nnon-zero probabilities which yields in a smaller\nvariance in gradients (Hinton et al., 2015). Intu-\nitively, a single masked word can have several valid\npredictions, which appropriately ﬁt the context.\n2886\nTeacher LanguageTeacher LM Student LM% of DataTokens Tokens\nMuRIL\nBengali 1181M 27M 2.30%\nEnglish 6986M 2816M 40.30%\nGujarati 173M 7M 3.90%\nHindi 2368M 38M 1.61%\nKannada 196M 15M 7.64%\nMalayalam 337M 14M 4.17%\nMarathi 274M 8M 3.02%\nNepali 231M 5M 2.16%\nPunjabi 141M 9M 6.45%\nTamil 769M 26M 3.34%\nTelugu 331M 30M 8.99%\nUrdu 722M 23M 3.21%\nTotal 13709M 3018M 22%\nTable 6: Number of Tokens (in Millions) in the\nteacher (MuRIL) and student LMs as described in Sec-\ntion 4.3. Note, we only show the MuRIL Languages\nhere because for Non-MuRIL Languages, the teacher\n(mBERT) and student variants are trained on the same\ndata.\nA.2 Pre-training Details\nA.2.1 Monolingual Teacher LMs\nWe pre-train our student models using the BERT\nbase architecture. Studentsimilar has a vocabulary\nsize of 99112 and a model size of 162M parameters.\nStudentdiﬀerent has a vocabulary size of 180996\nand a model size of 225M parameters. We keep a\nbatch size of 4096 and train for 250k steps with a\nmaximum sequence length of 512. We use TPUs,\nand it takes around 1.5 days to pre-train each stu-\ndent LM.\nA.2.2 Multilingual Teacher LMs\nWe pre-train our student models using the BERT\nbase architecture. All student LMs have a\nvocabulary size of 288973. Hence, we reduce our\nembedding dimension to 256 as opposed to 768\nto bring down the model size to be around 160M,\ncomparable to mBERT (178M). We keep a batch\nsize of 4096 and train for 500k steps with a maxi-\nmum sequence length of 512. We use TPUs, and\nit takes around 3 days to pre-train each student LM.\nWe present pre-training data statistics for\nMuRIL and the student LMs in Table 6. Here\nwe only include the monolingual data statistics,\nbut MuRIL is additionally trained on parallel\ntranslated and transliterated data.\nTask Batch Learning No. of Warmup Max. seq.\nRate Epochs Ratio Length\nNER 32 3e-5 10 0.1 256\nPOS 32 3e-5 10 0.1 256\nQA 32 3e-5 10 0.1 384\nTable 7: Hyperparameter Details for each ﬁne-tuning\ntask in Section 4.2\nA.3 Fine-tuning Details\nA.3.1 Monolingual Teacher LMs\nData Statistics We evaluate our monolingual\nteacher LMs and multilingual student LMs, as\ndescribed in Section 4.2, on three tasks as follows:\ni) Named Entity Recognition (NER): We\nuse the WikiAnn (Pan et al., 2017; Rahimi\net al., 2019) dataset for all languages. Each\nlanguage comprises of a train/dev/test split of\n20000/10000/10000 tokens. Speciﬁcally, we use\nthe huggingface re-packaged implementation of\nthe dataset11.\nii) Part-of-Speech tagging (POS): We use\nthe Universal Dependencies v2.6 (Zeman et al.,\n2020) dataset for all languages. Detailed statistics\nfor each language can be found in Table 9.\nSpeciﬁcally, we use the huggingface re-packaged\nimplementation of the dataset12.\niii) Question Answering (QA): We use the\nTyDiQA dataset (Clark et al., 2020) for ar and\nﬁ, SQuADv1.1 (Rajpurkar et al., 2016) for en,\nSQuAD-translated for it (Croce et al., 2018) and es\n(Carrino et al., 2020), DRCD for zh (Shao et al.,\n2018) and TQuAD13 for tr. Detailed statistics for\neach language can be found in Table 10. Note,\nwe use the dev set as our test sets, since most\ndatasets only have a train/dev split. We use 10%\nof randomly shufﬂed training examples as our dev\nsets.\nHyperparameter Details: We use the same hy-\nperparameters for ﬁne-tuning all teacher and stu-\ndent LMs, as shown in Table 7. We report results\non the best-performing checkpoint for the valida-\ntion set. The performance of the best checkpoint\non validation sets are shown in Table??\n11https://huggingface.co/datasets/wikiann\n12https://huggingface.co/datasets/universal dependencies\n13https://tquad.github.io/turkish-nlp-qa-dataset\n2887\nLanguages Model PANX UDPOS PAWSX XNLI XQUAD MLQA TyDiQA\nMuRIL Languages\nmBERT 58.8 68.5 93.4 66.2 70.3/57.5 65.0/50.8 62.5/52.7\nMuRIL 76.9 74.5 95.0 74.4 77.7/64.2 73.6/58.6 76.1/60.2\nk=8 69.3 72.3 95.4 71.9 75.7/62.1 72.0/56.3 70.7/59.2\nk=128 67.5 72.8 94.4 70.7 75.5/61.9 71.1/56.1 70.2/55.4\nk=512 69.2 77.2 94.7 71.3 75.6/61.8 72.3/56.9 68.5/53.9\nNon MuRIL Languages\nmBERT 63.5 71.1 80.2 65.9 62.2/47.1 59.7/41.4 60.4/46.1\nk=8 63.9 72.8 83.3 68.7 66.5/51.2 63.1/44.461.7/45.0\nk=128 63.7 72.8 83.4 67.9 66.1/51.1 61.4/43.4 62.6/46.7\nk=512 64.8 73.3 82.7 67.4 65.7/50.7 63.6/44.958.7/44.8\nAll Languages\nmBERT 62.5 70.6 82.0 65.9 63.7/49.0 61.2/44.1 61.1/48.3\nk=8 65.0 72.7 85.0 69.3 68.2/53.2 65.6/47.864.7/49.7\nk=128 64.5 72.8 85.0 68.4 67.9/53.0 64.2/47.1 65.2/49.6\nk=512 65.7 74.0 84.4 68.2 67.5/52.8 66.1/48.362.0/47.8\nTable 8: Results of the best performing student model StudentMuRIL for different top-k values\nLanguage Dataset Examples (Train/Dev/Test)\nArabic AR PADT 6075/909/680\nChinese ZH GSD 3997/500/500\nEnglish EN EWT 12543/2002/2077\nGerman DE HDT 15305/18434/18459\nFinnish FI FTB 14981/1875/1867\nItalian IT ISDT 13121/564/482\nSpanish ES ANCORA 14305/1654/1721\nTurkish TR IMST 3664/988/983\nTable 9: Universal Dependencies v2.6 overview for\neach language, used in Section 4.2\nLanguage Dataset Examples (Train/Test)\nArabic TyDiQA-GoldP 14805/921\nChinese DRCD 26936/3524\nEnglish SQuADv1.1 87599/10570\nGerman - -\nFinnish TyDiQA-GoldP 6855/782\nItalian SQuADv1.1-translated 87599/10570\nSpanish SQuADv1.1-translated 87595/10570\nTurkish TQuAD 8308/892\nTable 10: Question Answering datasets , used in Sec-\ntion 4.2\nA.3.2 Multilingual Teacher LMs\nData Statistics We evaluate all the teacher\n(mBERT and MuRIL) and student (StudentMuRIL,\nStudentmBERT and StudentBoth) LMs on the\nXTREME (Hu et al., 2020) benchmark. We ﬁne-\ntune the pre-trained models on English training\ndata for the particular task, except TyDiQA, where\nwe use additional SQuAD v1.1 English training\ndata, similar to (Fang et al., 2020). All results are\ncomputed in a zero-shot setting.\nHyperparameter Details We use the same\nhyperparameters for ﬁne-tuning all teacher and\nstudent LMs, as shown in Table 11. We report\nresults on the best-performing checkpoint for the\nTask Batch Learning No. of Warmup Max. seq.\nRate Epochs Ratio Length\nPANX 32 2e-5 10 0.1 128\nUDPOS 64 5e-6 10 0.1 128\nPAWSX 32 2e-5 5 0.1 128\nXNLI 32 2e-5 3 0.1 128\nXQuAD 32 3e-5 2 0.1 384\nMLQA 32 3e-5 2 0.1 384\nTyDiQA 32 3e-5 2 0.1 384\nTable 11: Hyperparameter Details for each task in\nXTREME\neval set.\nA.4 Different top-k values\nWe present results for StudentMuRIL trained with\ndifferent top-k values from teacher predictions in\nTable 8. We observe that while performances re-\nmain similar for higher values of k, storage be-\ncomes increasingly expensive. Hence, we stick to\na value of k=8 in all our experiments.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.846193790435791
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.800757884979248
    },
    {
      "name": "Language model",
      "score": 0.6995819807052612
    },
    {
      "name": "Vocabulary",
      "score": 0.6744503378868103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5828743577003479
    },
    {
      "name": "Machine learning",
      "score": 0.5550715923309326
    },
    {
      "name": "Merge (version control)",
      "score": 0.5114149451255798
    },
    {
      "name": "Natural language processing",
      "score": 0.4744728207588196
    },
    {
      "name": "Training set",
      "score": 0.4672546088695526
    },
    {
      "name": "Transfer of learning",
      "score": 0.45665016770362854
    },
    {
      "name": "Model selection",
      "score": 0.449036568403244
    },
    {
      "name": "Labeled data",
      "score": 0.4292387068271637
    },
    {
      "name": "Information retrieval",
      "score": 0.19618776440620422
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}