{
  "title": "Pre-trained transformer-based language models for Sundanese",
  "url": "https://openalex.org/W4226134572",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3203258813",
      "name": "Wilson Wongso",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A3201891178",
      "name": "Henry Lucky",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A1877050847",
      "name": "Derwin Suhartono",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A3203258813",
      "name": "Wilson Wongso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3201891178",
      "name": "Henry Lucky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1877050847",
      "name": "Derwin Suhartono",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W3118017378",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4255393055",
    "https://openalex.org/W6825285808",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2607658476",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2915800638"
  ],
  "abstract": null,
  "full_text": "Pre‑trained transformer‑based language \nmodels for Sundanese\nWilson Wongso* , Henry Lucky and Derwin Suhartono \nIntroduction\nRecently, there has been a multitude of advances in the field of natural language, espe -\ncially since the release of the Transformer [1] architecture. Before Transformers, RNNs \n(Recurrent neural networks) [2] like LSTMs (Long short-term memory networks) [3], \nand GRUs (Gated recurrent units) [4] were prevalent due to their network architecture \ndesign that suited the domain of natural language in the form of texts. However, most of \nthese older architectures struggled to model long-term sequential dependency and are \noften slow to train due to the inability of parallelization.\nOn the other hand, the  Transformer solves this issue by being parallelizable dur -\ning training and extensively applying the attention mechanism to better model long \nsequences of inputs. Several modern architectures were then derived from the origi -\nnal Transformer model, such as the OpenAI GPT (Generative Pre-Training) [5], BERT \n(Bidirectional Encoder Representations from Transformers) [6], and subsequently vari -\nants like GPT-2 [7], GPT-3 [8], and RoBERTa (Robustly Optimized BERT Pre-training \nApproach) [9].\nFor instance, the GPT architecture stacks decoder blocks of the original Transformer \narchitecture, while BERT stacks encoder blocks of the original Transformer and modifies \nthem to become naturally bidirectional by design. Due to this discrepancy, GPT-based \nAbstract \nThe Sundanese language has over 32 million speakers worldwide, but the language \nhas reaped little to no benefits from the recent advances in natural language under-\nstanding. Like other low-resource languages, the only alternative is to fine-tune exist-\ning multilingual models. In this paper, we pre-trained three monolingual Transformer-\nbased language models on Sundanese data. When evaluated on a downstream text \nclassification task, we found that most of our monolingual models outperformed larger \nmultilingual models despite the smaller overall pre-training data. In the subsequent \nanalyses, our models benefited strongly from the Sundanese pre-training corpus size \nand do not exhibit socially biased behavior. We released our models for other research-\ners and practitioners to use.\nKeywords: Sundanese Language, Transformers, Natural Language Understanding, \nLow-resource Language\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nWongso et al. Journal of Big Data            (2022) 9:39  \nhttps://doi.org/10.1186/s40537‑022‑00590‑7\n*Correspondence:   \nwilson.wongso001@binus.\nac.id \nComputer Science \nDepartment, School \nof Computer Science, \nBina Nusantara University, \nJakarta 11840, Indonesia\nPage 2 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nmodels are normally pre-trained as causal/generative language models, whereas BERT-\nbased models are pre-trained as masked language models. Once these models have been \npre-trained, they can similarly be fine-tuned to downstream natural language under -\nstanding tasks. This scheme allowed for breakthroughs in tasks like text classification, \nquestion-answering, natural language inference, among many others.\nHowever, most of these recent successes occur in the domain of high-resource lan -\nguages like English, Chinese, and Spanish where data are abundant. Low-resource lan -\nguages, conversely, have reaped little benefits despite the recent advances; attributing \nto the lack of data and existing work [10]. The best alternative is to fine-tune a multilin -\ngual model whose corpus contains that specific low-resource language. However, previ -\nous studies [11, 12] showed that multilingual models often perform poorly compared to \nmonolingual models of low-resource languages.\nTo that, we propose the pre-training of various Transformer-based language models \non a low-resource language, namely Sundanese. The Sundanese language is Indonesia’s \nthird-most spoken language [13], with over 32 million speakers worldwide and is the \n52nd most spoken language in the world [14]. It is the official language in larger regions \nof Banten and West Java, as well as a minority language in various parts of Jakarta, Lam -\npung, Central Java, and other nearby provinces in Indonesia [15]. Linguists usually clas -\nsify the language into as many as 8 dialects [16], with the Priangan dialect being the most \nprominent of them all.\nMoreover, a previous study [17] showed that the Sundanese language remains the main \ncommunication tool for adults and parents living in those regions. In the same paper, the \nauthors concluded that there is a desire to preserve the Sundanese language in the fast-\ngrowing modern technological world as a cultural identity of the people. Another study \n[18] similarly noted that a great deal of Indonesian social media activity in platforms like \nTwitter originates from West Java, where the Sundanese language is most often used. \nWith that many speakers and a huge market, it would be beneficial to have Sundanese \nlanguage models that could be applied to various business processes.\nContrarily, there is very limited Sundanese corpus to be used for pre-training. Hence, \non top of the pre-existing multilingual corpora like OSCAR (Open Super-large Crawled \nAggregated Corpus) [19], CC-100 [20, 21], and C4 [22], Sundanese documents from \nWikipedia were also used during the pre-training of our language models to cater to the \nscarcity of data availability.\nHaving been pre-trained, these models were thus fine-tuned to a downstream task of \nclassifying emotions from tweets and were subsequently benchmarked against existing \ntraditional machine learning algorithms and multilingual Transformer models such as \nthe multilingual BERT [6] and the RoBERTa variant of XLM (Cross-lingual Language \nModel Pre-training) [20].\nOnce these models have been fully pre-trained and fine-tuned, they are subsequently \nreleased in the HuggingFace Transformers Model Hub 1, in hopes of encouraging other \nresearchers to work on the field of Sundanese language modeling with open access to \nour models.\n1 https:// hf. co/ models? filter= su.\nPage 3 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nIn the following sections, we will continue by discussing the background of Sundanese \nlanguage modeling in greater detail, followed by the methodology used and experiments \nperformed during the pre-training of our models, and finally, various analyses, discus -\nsions, and conclusions thereafter.\nRelated works\nTransformer architectures\nBefore the advent of deep learning [23] and Transformers [1] following thereafter, natu -\nral language is one domain that computers have always struggled to model. This under -\nlying issue prevails due to the fact that the rules of natural languages are often complex \nand vary from one language to another. Language modeling by means of traditional \nmethods like statistical or rule-based models would often crumble when new, unseen \ninputs are fed into these models, given their inability to generalize.\nOn the contrary, deep learning models need not be taught the rules of natural language \nexplicitly. Instead, by using methods like supervised learning, these models could gain \nan understanding of sequences of texts given the ground truth/label. A few of the earliest \ndeep learning architectures developed to learn sequences of inputs include Recurrent \nneural networks [2], Long short-term memory networks [3], and Gated Recurrent Units \n[4]. Unlike feed-forward neural network architectures, a recurrent neural network has \ntemporal information passed over time by considering the output of previous time steps.\nHowever, these temporal models are still relatively inefficient to train despite the \nsophisticated modifications because of their sequential behavior by design. Transform -\ners [1], on the other hand, completely remove the idea of recurrence and fully substitute \nit with the attention mechanism given by the following equation:\nwhere Q, K, and V represent the query, key, and value vectors respectively. They are not \nonly effective to capture relations between words, but are also efficient to train on accel -\nerators like GPUs (Graphical Processing Units) and recently, TPUs (Tensor Processing \nUnits).\nFollowing the release and prominence of Transformers [1], OpenAI then developed \na language model that builds on top of the original Transformer architecture, as well as \nextending the training schemes proposed in ULMFiT (Universal Language Model Fine-\nTuning) [24] and ELMo (Embeddings from Language Models) [25]. Instead of utiliz -\ning LSTMs [3] as found in ULMFiT [24], the proposed OpenAI GPT model applies the \ndecoder blocks of the Transformer architecture. Moreover, the learning process starts by \nfirst pre-training the model with the objective of generating the next word in a sequence, \ngiven a current prompt. This way, the model could learn the context of words in a given \nsequence before being tasked to solve downstream problems. This pre-training objective \nalso greatly leverages the widespread availability of unlabelled data as the process is per -\nformed in an unsupervised manner.\nAfterward, the pre-trained model is thus fine-tuned in a supervised manner to a \ndownstream task where labels are finally required. For example, if the model were to \n(1)Attention(Q,K ,V ) = softmax\n(\nQK ⊤\n√\ndk\n)\nV ,\nPage 4 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nbe fine-tuned as a text classifier model, the causal/generative language model head is \nswapped with a linear model projecting to the number of possible classes. With the \nusual cross-entropy loss function, the then-language model could now be trained as a \ntext classifier model. This fine-tuning regime could similarly be applied to other various \ndownstream tasks alike.\nThe successor to the first GPT model [5] is then called GPT-2 [7], where the latter is \nsimply ten times as large as the former in terms of the number of parameters and was \ntrained on an even larger pre-training dataset. The GPT-2 model still retains the same \narchitectural design and training scheme as GPT.\nBERT [6] similarly leverages the original Transformer architecture just as GPT did. \nHowever, rather than taking the decoder blocks, BERT utilizes the encoder blocks of the \nTransformer model. Moreover, BERT is naturally bidirectional by design, whereas the \npreviously aforementioned models learn to generate sequences from left to right. Due \nto this difference by design, BERT cannot be trained with a generative/causal objective. \nInstead, several words in a sequence are replaced with a special masked token which \nBERT has to learn to fill with the right word. This way, BERT learns the bidirectional \ncontext of words as a masked language model, unlike GPT, through a similar means of \nunsupervised learning. Aside from its mask-filling objective, BERT also has a next sen -\ntence prediction objective that learns to classify whether a pair of sequences follows \neach other. Like GPT, BERT can also be fine-tuned into downstream tasks like text clas -\nsification, question-answering, etc.\nRoBERTa [9] works upon the existing architecture of BERT [6] and argues that BERT \nis not only under-trained, but could also be robustly improved with several modifica -\ntions. For instance, RoBERTa removes the need to use next sentence prediction as a pre-\ntraining objective, as well as uses dynamic masking instead of BERT’s static masking. \nThat is, instead of feeding a sequence where the masked token is in the same position \nin every epoch (static), RoBERTa sees multiple versions of the same sequence with the \nmasked token in different positions (dynamic). By further feeding RoBERTa with an even \nlarger pre-training dataset, the authors argue that RoBERTa would outperform BERT in \nmost cases.\nSundanese language modeling\nPrior to the era of deep learning models, the field of Sundanese language modeling relied \nmostly on traditional machine learning algorithms. For example, various techniques \nwere proposed to classify Sundanese texts, including the emotional classification of \ntweets [18, 26] and speech level/register-classification of Sundanese texts [27].\nThose experiments involved the usage of traditional single-task machine learning algo-\nrithms like K-nearest neighbors, Naive Bayes classifier, and Support Vector Machines. \nAlthough they were able to attain a relatively decent classification result with over 90% \naccuracy, these models are only built for the specific task of text classification. They are \ntherefore inapplicable to other language modeling tasks like named-entity recognition, \npart-of-speech tagging, and extractive question-answering.\nMore flexible approaches include the usage of multilingual Transformer-based \nmodels such as the mBERT (multilingual BERT) model [6], or the large cross-lingual \nXLM model [28]. Both of these models claim to support multiple languages, including \nPage 5 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nSundanese, since a slight percentage of their respective pre-training corpus consists of \nSundanese texts. Multilingual models like them may be applicable to cases where the \ntarget language is of high-resource, unlike Sundanese.\nA more recent study similarly trained a multilingual BART (Bidirectional and Auto-\nRegressive Transformers) model [29] called IndoBART [30] that pre-trains on Indone -\nsian, Javanese, and Sundanese corpus. Their authors showed that the IndoBART model \nis able to perform sequence-to-sequence tasks like summarization and neural machine \ntranslation on the Sundanese language as well.\nHowever, there have been studies [12] which show that monolingual models are gen -\nerally more performant than multilingual models due to the differing sizes of pre-train -\ning data and a more accurate tokenization scheme [11]. This is very much apparent in \npre-trained monolingual models in various languages, such as IndoBERT [31] for Indo -\nnesian, PhoBERT [32] for Vietnamese, WangchanBERTa [33] for Thai, whereby these \nmonolingual models constantly outperform their multilingual counterparts in down -\nstream tasks.\nTherefore, given the evidence and the various studies which claim the superiority of \nmonolingual models over multilingual models for the case of low-resource languages, it \nis thus preferable to pre-train a monolingual language model whenever possible.\nSundanese language model pre‑training\nIn this section, we will present the pipeline of the pre-training process. This includes the \nconfigurations of architectures, the pre-training corpus used, how pre-processing was \nperformed, the optimization scheme, and the pre-training results.\nArchitectures\nConsidering the varying performances of different Transformer models, we pre-trained \nthree different models based on the architectural designs of OpenAI GPT-2 [7], BERT \n[6], and RoBERTa [9]. All three of these models are of the base size and utilize GELU \n(Gaussian Error Linear Unit) [34] as their activation functions. They all have 12 layers, \n12 heads, an embedding dimension and hidden size of 768, and an intermediate size of \n3,072. Additional details about the models’ configurations are shown in Table 1.\nAdditionally, Sundanese GPT-2 is pre-trained as a causal language model, whereas \nSundanese BERT and Sundanese RoBERTa are pre-trained as masked language models. \nHowever, the NSP (next sentence prediction) objective from BERT was removed during \npre-training, leaving merely the MLM (masked language modeling) objective. RoBERTa, \non the other hand, was trained according to the originally proposed paper, which simi -\nlarly removes the need to use NSP .\nTable 1 Architecture configurations of proposed Sundanese models\nModel #Params Vocabulary size Language modeling task\nSundanese GPT-2 124M 50,257 Causal/Generative\nSundanese BERT 110M 30,522 Masked\nSundanese RoBERTa 124M 50,265 Masked\nPage 6 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nData\nThe dataset used during pre-training consists of Sundanese subsets of multilingual com -\nmon-crawl corpora of OSCAR [19], CC-100 [20, 21], and C4 [22]. They are of the size \n141KB, 15MB, and 464MB, respectively. Seeing how Transformer models heavily depend \non a large pre-training monolingual corpus [11], additional Sundanese documents from \nWikipedia were added to the pre-training corpus. 68,920 documents Wikipedia texts \nwere collected in June 2021 from Wikidump, with a size of 279MB after parsing. This \naccumulates to a sum of 758MB of pre-training text, with 10% of the dataset held out for \nevaluation purposes.\nThis hence makes our pre-training corpus consist mainly of informally written docu -\nments with varying fluency levels. Nonetheless, the aim of using such a corpus is to gain \nas many vocabularies as possible, as well as to provide a diverse list of genres of texts.\nPre‑processing\nThere is a need to pre-process the pre-training corpus such that it can be learned by the \nmodel. As the different architectures require different tokenizers and collation schemes, \nthere are slight differences in the pre-processing step for each model. Nonetheless, they \nfollow a similar pre-processing pipeline that many other pre-trained Transformer mod -\nels follow, as shown in Figure  1. The following subsections will explain this in greater \ndetail.\nTokenization\nThe tokenizers used for each of the models accords to their respective original papers. \nFor instance, both the Sundanese GPT-2 and RoBERTa models leveraged the Byte-\nlevel BPE (Byte-Pair Encoding) tokenizer [35], though with different vocabulary sizes as \nFig. 1 Data Pre-processing Pipeline\nPage 7 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nshown in Table 1. This type of tokenizer relies on the pre-tokenization of the raw data -\nsets, which in our case, happens naturally as Sundanese texts are separated by whites -\npace. The Byte-level BPE tokenizer learns to merge byte-base characters based on their \noccurring frequencies until the desired vocabulary size has been reached. This allows the \ntokenizer to tokenize every sequence without having to use the < unk > token, which \ncorresponds to the special token representing unknown characters.\nThe Sundanese BERT, on the other hand, utilized the WordPiece tokenizer [36] that \nworks very similarly to the BPE tokenizer. That is, the former begins by creating a vocab-\nulary with every present character in the raw dataset and then learns to merge these \ncharacters into subwords. Instead of merging based on the frequency of occurrence like \nthe BPE tokenizer does, WordPiece tokenizer creates subwords of characters that are \nmost likely to be used as training data once included in the resultant vocabulary.\nGrouping and collation\nOnce these tokenizers have been trained on the pre-training corpus, the raw texts are \nsubsequently encoded into their respective numerical token representations. Addition -\nally, special tokens, attention masks, and token type identifiers are added to facilitate \ntraining.\nAfterward, these texts ought to be grouped into sequences of uniform lengths. The \nSundanese BERT and RoBERTa models were initialized to handle a maximum sequence \nlength of 128, while the Sundanese GPT-2 model could take a longer block size of 512 \ntokens as input.\nFinally, data collation was performed to accord to the different models’ language mod-\neling task. Sundanese GPT-2, which learns to generate texts, simply set the next token in \nsequence as the labels, i.e., shifting the texts to obtain ground truth. On the other hand, \nthe masked language models, BERT and RoBERTa, require a more sophisticated data \ncollation technique that involves the masking of tokens.\nThere is a slight discrepancy in the masking technique of BERT and RoBERTa, where \nthe former does static and the latter does dynamic masking. Regardless, each of them \nfollows their original masking strategy proposed in [6] and [9] respectively, with a mask -\ning probability of 15%. The masked tokens are thus the labels for the models to predict.\nOptimization\nIn the pre-training of our models, the AdamW optimizer [37] was used with values \nβ1 = 0.9 , β2 = 0.999 , and ǫ = 10−8 . There was a slight difference in the learning rate and \nweight decay of the different models, as outlined in Table  2. A weight decay factor of 0.1 \nwas added to the pre-training of Sundanese GPT-2 to better regularize the model, as it \nTable 2 Learning rate and weight decay of each of the pre-trained Sundanese models\nModel Learning rate Weight decay\nSundanese GPT-2 1 × 10−4 0.1\nSundanese BERT 2 × 10−4 0.0\nSundanese RoBERTa 2 × 10−4 0.0\nPage 8 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nwas prone to overfitting. The other two models, on the other hand, did not exhibit over -\nfitting behavior, hence the 0.0 weight decay factor.\nAll three models were trained for 50 epochs, with a batch size of 64 per device, and \nwere paired with a linearly decaying scheduler with a warm-up step of 1,000. The learn -\ning rate decreased linearly per optimization step and decayed all the way to zero at the \nend of training. Since the pre-training was performed on an 8-core TPU, this brings the \ntotal effective batch size to 512.\nExperiments\nSubsequent experiments were conducted according to the setup presented in the pre -\nvious section, Sundanese Language Model Pre-Training. We implemented the GPT-\n2, BERT, and RoBERTa models provided by HuggingFace’s Transformers library. The \nframework allows for easy integration with deep learning frameworks like Flax [38] and \nPyTorch [39], as well as compatibility to run on a GPU or a multi-core TPU accelerator.\nPre‑training\nThe pre-training process was done entirely on a TPU with 8 cores, using the Hugging -\nFace library that runs on top of Flax. As explained above, the Sundanese GPT-2 was \ntrained as a causal language model, whereas the Sundanese BERT and RoBERTa were \ntrained as masked language models. These models trained on 90% of the pre-training \ncorpus that makes up the training subset, followed by an evaluation on the remaining \n10%. Table 3 depicts the pre-training results of our models after 50 epochs.\nIn spite of the varying pre-training results, the three models are incomparable due to \ntheir different language modeling tasks. Furthermore, it is more relevant to compare \nthese models on a downstream task where the overall measure of performance is more \nrepresentative of the model’s capability.\nEvaluation\nWe fine-tuned these models to a downstream task of emotional classification of Sunda -\nnese tweets to better compare our pre-trained models with the existing baseline models.\nDownstream dataset\nThe dataset used for the downstream purpose of text classification consists of Sundanese \ntweets collected by [18]. It contains 2,518 tweets with four possible emotions of sadness, \njoy, fear, and anger. The authors proposed multiple machine learning-based solutions, \nwith a one-vs-one, linear SVC (C-Support Vector Classifier) being the most promi -\nnent algorithm. Various text pre-processing techniques were performed, including case \nTable 3 Pre-training results of Sundanese language models\nModel Training loss Evaluation loss Evaluation \nperplexity\nSundanese GPT-2 2.436 3.610 36.97\nSundanese BERT 2.860 2.845 17.20\nSundanese RoBERTa 1.965 1.952 7.04\nPage 9 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nfolding, stopword filtering, stemming, and tokenizing. They also utilized TF-IDF (Term \nFrequency-Inverse Document Frequency) as the feature extractor and their proposed \nsolution serves as the baseline results for our models.\nHowever, a slight tweak in the dataset splitting was made as the authors originally used \n10-fold cross-validation. Rather than doing so, 10% of the tweets were held out for evalu-\nation purposes and the same subset was used for all the fine-tuning experiments.\nSince this dataset is collected from Twitter, it is expected that most of the tweets may \ncontain informal, colloquial slang words and abbreviations commonly found in main -\nstream social media. Normally, these words have to be pre-processed and converted \nback to their official, standardized forms. However, because our pre-training datasets \nwere largely derived from common crawl data, we expect that these types of words have \nbeen included to our models’ respective vocabularies and that they were able to capture \nthe syntactic meaning of these unofficial words.\nFine‑tuning setup\nThere are various methods to fine-tune causal and masked language models as text clas -\nsifiers. ULMFiT [24] suggests a two-step fine-tuning of their LSTM model to better cap -\nture in-domain data. Likewise, BERTFiT [40] investigates the best method to fine-tune a \nChinese BERT model. In the end, we decided to conduct the usual fine-tuning scheme of \nreplacing the language model head with additional linear layers and only training for one \nadditional phase instead of two.\nOur pre-trained models were compared against the baseline method presented in [18], \nmultilingual BERT [6], XLM-RoBERTa [20], as well as IndoBERT Base Phase 1 [31]. The \nsame text pre-processing scheme was applied to the classification dataset – without data \ncollation – using the respective tokenizers of each model and a sequence length of 128.\nLike the pre-training stage, the AdamW optimizer [37] was used to fine-tune the deep \nlearning models, with the same values of β1 , β2 , and ǫ as shown in the previous section, \nOptimization. All deep learning models were fine-tuned for 10 epochs, with the varying \nhyperparameter choices reflected in Table  4. Also, the same linearly decaying scheduler \nthat brought the learning rate down to zero was used, except without warm-up steps. \nThe model checkpoint with the highest F1-score is loaded at the end of training.\nWith this setup, fine-tuning experiments were conducted using HuggingFace’s imple -\nmentation of Transformer models for sequence classification. However, instead of \nrunning on top of JAX, PyTorch was used as the backend framework. Moreover, the \nexperiments involving Sundanese monolingual classifiers were carried out on a single \nTable 4 Hyperparameters used to fine-tune pre-trained models\nModel Learning rate Weight decay Batch size\nSundanese GPT-2 1 × 10−5 0.01 16\nSundanese BERT 4 × 10−5 0.01 8\nSundanese RoBERTa 2 × 10−5 0.01 16\nIndoBERT [31] 2 × 10−5 0.0 16\nmBERT [6] 2 × 10−5 0.0 16\nXLM-RoBERTa [20] 2 × 10−5 0.0 16\nPage 10 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nNVIDIA Tesla T4 GPU, while the rest used TPUs – both of which were accessed via \nGoogle Colaboratory.\nResults\nThe following section will lay out the classification results of fine-tuning to Sundanese \ntweets. There were four main types of models which we fine-tuned, starting with the \nbaseline linear SVC with TF-IDF approach as proposed in [18], our pre-trained mono -\nlingual Sundanese models, multilingual Transformer models, and IndoNLU’s IndoBERT \nmodel [31] which was trained on an Indonesian corpus. Table  5 shows the various fine-\ntuning results as well as the number of parameters per model. Both the accuracy and \nF1-score were calculated on the evaluation subset, which makes up 10% of the original \ndataset.\nAmong the fine-tuned models, our Sundanese RoBERTa managed to obtain the \nhighest accuracy score of 98.41% and F1-macro score of 98.43%. It, therefore, outper -\nformed the baseline method, the IndoBERT [31] base model, as well as both mBERT \n[6] and XLM-RoBERTa [20] – where the latter two are larger in terms of the number of \nparameters.\nMoreover, the Sundanese BERT model performed just slightly below the mBERT \nmodel while still outperforming the much larger XLM-RoBERTa. On the other hand, the \nSundanese GPT-2 model could not even be on-par with the baseline method of linear \nSVC. Overall, these results are in line with their respective papers’ conclusions, whereby \nRoBERTa [9] can indeed exceed the results of BERT [6], whereas GPT-2 [7] remains less \neffective compared to BERT.\nLikewise, despite being smaller in the number of parameters and overall pre-training \ncorpus size, both Sundanese BERT and RoBERTa were able to surpass – or perform \nsimilarly to – the larger multilingual models. It should be noted that the monolingual \nmodels were trained on a larger pre-training Sundanese corpus, whereas the multilin -\ngual corpus used to pre-train the multilingual models contained a very small percentage \nof Sundanese texts. These results are hence parallel with the ideas proposed in [11].\nFinally, the baseline linear SVC method with TF-IDF feature extractor as proposed in \n[18] provided a relatively high baseline compared to Transformer models of very large \nTable 5 Evaluation result of Sundanese Twitter emotion classification\nModel #Params Accuracy F1‑Macro\nBaseline\n Linear SVC with TF-IDF [18] 30K 96.43 96.36\nSundanese (Ours)\n Sundanese GPT-2 124M 94.84 94.75\n Sundanese BERT 110M 96.82 96.75\n Sundanese RoBERTa 124M 98.41 98.43\nIndonesian\n IndoBERT [31] 124M 96.43 96.42\nMultilingual\n mBERT [6] 167M 96.83 96.79\n XLM-RoBERTa [20] 278M 95.24 95.22\nPage 11 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nsizes. We suspect that this ability is achievable due to the sophisticated pre-processing \nrules applied to the raw dataset prior to tokenization and the high dimensionality of fea -\ntures after extracting through TF-IDF.\nAnalysis and findings\nImpact of pre‑training corpus size\nFrom the results shown in Table 5, it is apparent, yet unsurprising, that the performance \nof Transformer-based models is not always proportional to their number of parameters. \nInstead, as suggested by [11], the difference in performance between monolingual and \nmultilingual models depends on the size of the pre-training corpus and the ability of \ntheir tokenizers to adapt to the target language. Therefore, it is much more suitable to \ncompare the different models’ pre-training corpus size over their respective number of \nparameters.\nLike the comparison method presented in [11], we compared the pre-training corpus \nsize of our Sundanese language models, mBERT [6], and XLM-RoBERTa [20], against \ntheir downstream performance, as shown in Figure  2. The Sundanese pre-training data -\nset discussed in the previous section, Data, contains about 89.8M words. As for the mul -\ntilingual models, only the Sundanese subsets of their respective pre-training corpus were \ntaken into account.\nMoreover, as the mBERT model was trained on entire Wikipedia dumps, there is not \nan exact number that represents the number of words present in the Sundanese subset \nused for the pre-training of mBERT. We resorted to finding an upper-bound estimate of \nFig. 2 Downstream performance of Sundanese and multilingual models on tweet emotion classification in \nterms of their number of parameters. The area of the points corresponds to their pre-training corpus size (in \nmillion words)\nPage 12 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \n6.22M words through the statistics provided in Wikimedia 2, like the authors of [11] did. \nOn the other hand, there are 10M words present in the Sundanese subset of the CC-100 \ndataset [21] used to pre-train XLM-RoBERTa [20] as readily shown in the original paper.\nThis shows that the pre-training corpus size of a language model significantly affects \nits final performance on downstream tasks. Although the multilingual models are much \nlarger in terms of the number of parameters and were pre-trained on significantly larger \nmultilingual corpora like Wiki-100 and CC-100, they may not consistently outperform \nsmaller monolingual models. Factors such as the varying ability of different tokenizers to \nadapt to a target language like Sundanese should similarly be considered.\nSimilarity of Indonesian and Sundanese corpus\nAs shown in Table  5, the IndoBERT model [31] was somehow able to perform just \nslightly below our Sundanese BERT model, despite the former being trained on mostly \nIndonesian data. This could mean a few things; either there are token overlaps between \nthe Sundanese and Indonesian tokenizers or that the Indonesian language is highly \nrelated to the Sundanese language on its own. Both possibilities were investigated.\nThe IndoBERT model [31] used the SentencePiece tokenizer [41] to encode their pre-\ntraining corpus called Indo4B, collected from various Indonesian data. As none of our \nmodels used the same tokenizer, the closest would be Sundanese BERT’s tokenizer which \nwas based on WordPiece [36]. Both tokenizers had about the same number of tokens, \nnamely 30,521 and 30,522 tokens for the Indonesian and Sundanese BERT, respectively. \nThe respective vocabularies were then compared and overlapping tokens were counted, \nto which we found only 12,935 overlapping tokens out of roughly 30,000 tokens. Since \nthere are only less than half overlapping tokens out of the entire vocabulary of both \ntokenizers, this possibility seemed quite unlikely.\nOn the flip side, it may just be that the Indonesian language is very much related to the \nSundanese language by design. Although the measure of similarity between these two \nlanguages isn’t trivial, it should be understood that the Indonesian language was created \nwith the influence of regional languages like Sundanese, which is the third most spoken \nlanguage in the nation [13]. Likewise, both languages stem from the same language fam -\nily of Malayo-Sumbawan, a subgroup of Malayo-Polynesian languages [42]. Therefore, \ndespite the lack of direct word overlaps as evident in the comparison of tokens, the two \nlanguages may exhibit an intrinsically similar linguistic structure which IndoBERT is \nable to model as well.\nWe suspect that the IndoBERT model [31] may hence be applicable as an alternative \nto the multilingual models for the case of regional languages in Indonesia, which are \nstrongly related to the national language.\nLimitations and bias\nPrevious studies [43, 44] have indicated social biases present in large language models \nlike BERT [6] and OpenAI GPT-3 [8]. This may include bias towards a certain gender, \nethnicity, or beliefs. In this section, we aim to empirically test whether our Sundanese \n2 https:// meta.m. wikim edia. org/ wiki/ List_ of_ Wikip edias. Accessed on August 8, 2021.\nPage 13 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nmodels exhibit biased behavior towards a certain gender. Our Sundanese RoBERTa lan -\nguage model was tested for this purpose.\nAlthough the Sundanese language does not have gender-specific pronouns, unlike \nEnglish and Chinese, daily informal texts found in social media websites may exhibit \nstereotypical occupations and/or roles found in the local Indonesian culture, as pointed \nout in [45]. This bias may be found in the pre-training corpus of our language models, \nespecially noting that it does consist of common crawl data, i.e., public data available on \nthe internet. Hence, although our models are relatively performant in terms of solving \ndownstream tasks, they do carry over whatever intrinsic biases are found in the original \ndata.\nInspired by [44], template prompts in the form of “[NOUN] [VERB] [MASK].”, \nwere prepared in Sundanese. For instance, [VERB] is replaced by verbs like saurang  \n(is a/an), damel salaku (works as a/an), ngalakukeun (do/does), etc., while [NOUN] is \nreplaced by gender-specific nouns and honorifics like lalaki  (man), awéwé (woman), \nbapak (mister), ibu  (ma’am), saudara (male sibling), saudari  (female sibling), etc. As a \ncontrol, [NOUN] is also replaced by gender-neutral nouns like urang éta (that person), \npagawe éta (that employee), kakak (older sibling), and anjeun (gender-neutral pronoun).\nThen, these prompts were passed to the model for it to predict the masked token and \nsubsequently, the top 10 predictions for each prompt were collected. 24 prompts were \nprepared for each gender category, yielding a sum of 240 predictions in total per gender. \nTable 6 shows the seven most common predictions for each gender category.\nTable 6 Seven most common predictions for each gender category made by the Sundanese \nRoBERTa model. English equivalents of the predicted Sundanese tokens are also provided\nGender Prediction Prediction (in English) Frequency\nMale bapak father 14\nlalaki man 10\nawéwé woman 7\nibu mother 7\nconto example 5\nsato animal 5\natlit athlete 5\nFemale awéwé woman 12\nlalaki man 7\npikaseurieun funny 7\nibu mother 7\nconto example 5\natlit athlete 4\nSMP secondary school 4\nNeutral dokter doctor 5\nconto example 5\nguru teacher 4\njalma creature 4\nprofesional professional 3\nIndonesia Indonesia 3\nurang person 3\nPage 14 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \nThe gender-neutral prompts seem to generally return gender-free predictions. Moreo-\nver, they mostly returned reasonable predictions such as bapak  and ibu  for male and \nfemale respectively, while the rest remained relatively neutral descriptions of a person. \nInterestingly, the same results may appear in either gender category despite being speci -\nfied of a specific gender in the prompt.\nDiscussion and future directions\nWhile the monolingual Sundanese language models we have pre-trained are generally \non-par with large multilingual models, it may be beneficial to add even more pre-train -\ning data to ensure a consistent state-of-the-art performance in downstream Sundanese \ntasks. This could mean adding more and richer data from various Sundanese sources \nsuch as online news outlets, social media, and formal Sundanese documents.\nMoreover, to encourage more work and establish a robust benchmark for the field of \nSundanese language modeling, there is a need to create a benchmark like GLUE (Gen -\neral Language Understanding Evaluation) [46], such that a more diverse set of tasks are \ntaken into consideration when measuring the performance of pre-trained models. How -\never, this might be among the more difficult set of issues due to the scarcity of labeled \nSundanese data. An alternative would be to translate closely related benchmark datasets, \nlike IndoNLU [31], for instance.\nApplications to other regional languages\nGiven that the method we proposed in this paper proved to be applicable to a low-\nresource language like Sundanese, the same could be done to other regional languages \nin Indonesia if a sufficiently large pre-training corpus has been collected. [20] recom -\nmended a minimum of a few hundred MB of pre-training text data if we were to pre-\ntrain a Transformer-based language model like BERT [6].\nMoreover, a previous study such as [47] applied a cross-lingual Transformer-based lan-\nguage model for multiple African regional languages with a minimally-sized pre-training \ndataset. It was shown that despite the small data, the model was still able to learn the \ndifferent languages and is transferable to numerous downstream tasks. Therefore, we \nhypothesize that other nearby regional languages in Indonesia can similarly benefit from \npre-trained Transformer-based language models in terms of developing practical text-\nbased applications.\nConclusion\nWe have pre-trained three different Sundanese, Transformer-based language models, \nnamely GPT-2, BERT, and RoBERTa, using limited pre-training data. Afterward, we \nevaluated these models by fine-tuning them to a downstream task of emotion classifica -\ntion of Sundanese tweets and found that our RoBERTa model significantly improved the \nresults of larger multilingual models due to the discrepancy in pre-training corpus size, \nwhile our BERT model performed slightly better than XLM-RoBERTa. An investigation \nof social biases present in our language model was also conducted, to which the model \nseems to return neutral results.\nPage 15 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \nAbbreviations\nBART:: Bidirectional and Auto-Regressive Transformers;; BERT:: Bidirectional Encoder Representations from Transformers;; \nELMo:: Embeddings from Language Models;; GELU:: Gaussian Error Linear Unit;; GLUE:: General Language Understanding \nEvaluation;; GPT:: Generative Pre-Training;; GPU:: Graphical Processing Units;; GRU:: Gated Recurrent Unit;; LSTM:: Long \nShort-Term Memory;; MBERT:: Multilingual BERT;; MLM:: Masked Language Modeling;; NSP:: Next Sentence Prediction;; \nOSCAR:: Open Super-large Crawled Aggregated Corpus;; RNN:: Recurrent Neural Network;; RoBERTa:: Robustly Optimized \nBERT Pre-training Approach;; SVC:: C-Support Vector Classifier;; TF-IDF:: Term Frequency-Inverse Document Frequency;; \nTPU:: Tensor Processing Units;; ULMFiT:: Universal Language Model Fine-Tuning;; XLM:: Cross-lingual Language Model \nPre-training..\nAcknowledgements\nWe would like to thank Bina Nusantara University for facilitating and supporting this entire research process.\nAuthor Contributions\nWW contributed as the research principal in this work as well as the technical issues. HL contributed to technical issues. \nDS advised all processes for this work. Regarding the manuscript, WW, HL, and DS wrote and revised the manuscript. All \nauthors read and approved the final manuscript.\nAuthors’ information\nWilson Wongso is a third-year undergraduate Computer Science student from Bina Nusantara University, Indonesia. \nHis research interests include natural language processing, especially in the domain of low-resource languages and \nIndonesia-related languages.\nHenry Lucky is a faculty member of Bina Nusantara University, Indonesia. He is currently pursuing an M.S. degree in \ncomputer science at Bina Nusantara University, Indonesia. His research interest includes stock prediction and natural \nlanguage processing, especially natural language generation.\nDerwin Suhartono is faculty member of Bina Nusantara University, Indonesia. He got his Ph.D. degree in computer sci-\nence from Universitas Indonesia in 2018. His research fields are natural language processing. Recently, he is continually \ndoing research in argumentation mining and personality recognition. He is actively involved in the Indonesia Association \nof Computational Linguistics (INACL), a national scientific association in Indonesia. He has professional memberships in \nACM, INSTICC, and IACT. He also takes the role of a reviewer in several international conferences and journals.\nFunding\nAll of this work is fully supported by Bina Nusantara University.\nAvailability of data and materials\nThe datasets used for this study are available on request to the corresponding author.\nDeclarations\nEthics approval and consent to participate\nNot applicable. \nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 14 September 2021   Accepted: 28 March 2022\nReferences\n 1. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin, I. Attention is all you need. \n2017; arXiv preprint arXiv: 1706. 03762.\n 2. Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back-propagating errors. nature. \n1986;323(6088):533–6.\n 3. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80.\n 4. Cho K, van Merrienboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H, Bengio Y. Learning phrase representa-\ntions using RNN encoder-decoder for statistical. Mach Transl. 2014;1406:1078.\n 5. Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training \n2018.\n 6. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language under-\nstanding 2019; 1810.04805.\n 7. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. \nOpenAI blog. 2019;1(8):9.\n 8. ...Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P , Neelakantan A, Shyam P , Sastry G, Askell A, Agarwal \nS, Herbert-Voss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu J, Winter C, Hesse C, Chen M, Sigler E, \nLitwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, Radford A, Sutskever I, Amodei D. Language models are \nfew-shot learners. Adv Neurl Inf Process Syst. 2020;2005:14165.\nPage 16 of 17Wongso et al. Journal of Big Data            (2022) 9:39 \n 9. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov, V. Roberta: A robustly opti-\nmized bert pretraining approach. 2019; arXiv preprint arXiv: 1907. 11692.\n 10. Ruder S. Why you should Do NLP beyond english. 2020;http:// ruder. io/ nlp- beyond- engli sh\n 11. Rust P , Pfeiffer J, Vulić I, Ruder S, Gurevych I. How good is your tokenizer? On the monolingual performance of multi-\nlingual language models 2021; 2012.15613\n 12. Virtanen A, Kanerva J, Ilo R, Luoma J, Luotolahti J, Salakoski T, Ginter F, Pyysalo S. Multilingual is not enough: BERT for \nFinnish. 2019;1912:07076.\n 13. Badan Pusat Statistik Kewarganegaraan, Suku Bangsa, agama, dan Bahasa Sehari-hari Penduduk Indonesia: Hasil \nSensus Penduduk 2010. http:// www. bps. go. id/ websi te/ pdf_ publi kasi/ water mark% 20_ Kewar ganeg araan ,% 20Suku% \n20Ban gsa,% 20Aga ma% 20dan% 20Bah asa_ 281211. pdf.\n 14. Ethnologue: what are the top 200 most spoken languages? SIL International, Dallas, TX, USA 2021. https:// www. \nethno logue. com/ guides/ ethno logue 200\n 15. Ministry of Education, R. Culture, (Indonesia) T. Data Bahasa di Indonesia. https:// petab ahasa. kemdi kbud. go. id/ datab \nahasa. php\n 16. Wurm SA, Hattori S. Language Atlas of the Pacific Area. Australian Academy of the Humanities (distributed by Geo-\ncenter, Stuttgart 80)\n 17. Haerudin D. The role of parents in sundanese language preservation. In: Proceedings of the 1st international confer-\nence on innovation in education (ICoIE 2018). Atlantis Press; 2019/01. p. 27–32.https:// doi. org/ 10. 2991/ icoie- 18. \n2019.7.\n 18. Putra OV, Wasmanson FM, Harmini T, Utama S.N. Sundanese twitter dataset for emotion classification. In: 2020 inter-\nnational conference on computer engineering, network, and intelligent multimedia (CENIM) (CENIM 2020), Online \n2020\n 19. Ortiz Suárez PJ, Sagot B, Romary L. Asynchronous pipelines for processing huge corpora on medium to low \nresource infrastructures. Leibniz-Institut für Deutsche Sprache, Mannheim 2019. https:// doi. org/ 10. 14618/ ids- pub- \n9021.http:// nbn- resol ving. de/ urn: nbn: de: bsz: mh39- 90215\n 20. Conneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzmán F, Grave E, Ott M, Zettlemoyer L, Stoyanov V. \nUnsupervised cross-lingual representation learning at scale. In: Proceedings of the 58th annual meeting of the asso-\nciation for computational linguistics. Association for Computational Linguistics, Online 2020. p. 8440–8451 https:// \ndoi. org/ 10. 18653/ v1/ 2020. acl- main. 747.https:// aclan tholo gy. org/ 2020. acl- main. 747.\n 21. Wenzek G, Lachaux M-A, Conneau A, Chaudhary V, Guzmán F, Joulin A, Grave,E. CCNet: Extracting high quality \nmonolingual datasets from web crawl data. In: Proceedings of the 12th language Resources and Evaluation Confer-\nence, pp. 4003–4012. European Language Resources Association, Marseille, France 2020. https:// aclan tholo gy. org/ \n2020. lrec-1. 494\n 22. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ. Exploring the limits of transfer learn-\ning with a unified text-to-text transformer. J Mach Learn Res. 2020;21(140):1–67.\n 23. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.\n 24. Howard J, Ruder S. Universal language model fine-tuning for text classification. arXiv preprint arXiv: 1801. 06146 \n2018.\n 25. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L. Deep contextualized word representa-\ntions. In: Proceedings of NAACL 2018.\n 26. Cahyono Y, Saprudin S. Analisis sentiment tweets berbahasa sunda menggunakan naive bayes classifier dengan \nseleksi feature chi squared statistic. J Inf Univ Pamulang. 2019;4(3):87–94.\n 27. Sutedi A, Kurniadi D, Baswardono W. Sundanese language level detection using rule-based classification: case stud-\nies on twitter\n 28. Lample G, Conneau A. Cross-lingual language model pretraining. 2019; arXiv preprint arXiv: 1901. 07291\n 29. Liu Y, Gu J, Goyal N, Li X, Edunov S, Ghazvininejad M, Lewis M, Zettlemoyer L. Multilingual denoising pre-training for \nneural machine translation. Trans Assoc Comput Linguist. 2020;8:726–42.\n 30. Cahyawijaya S, Winata GI, Wilie B, Vincentio K, Li X, Kuncoro A, Ruder S, Lim ZY, Bahar S, Khodra ML, Purwarianti \nA, Fung, P . IndoNLG: Benchmark and resources for evaluating Indonesian natural language generation 2021; \n2104.08200\n 31. Wilie B, Vincentio K, Winata GI, Cahyawijaya S, Li X, Lim ZY, Soleman S, Mahendra R, Fung P , Bahar S, Purwarianti A. \nIndoNLU: Benchmark and resources for evaluating Indonesian natural language understanding 2020; 2009.05387\n 32. Nguyen DQ, Nguyen A.T. PhoBERT: Pre-trained language models for Vietnamese 2020; 2003.00744\n 33. Lowphansirikul L, Polpanumas C, Jantrakulchai N, Nutanong S. WangchanBERTa: pretraining transformer-based Thai \nlanguage models 2021; 2101.09635\n 34. Hendrycks D, Gimpel K. Gaussian error linear units (GELUs). 2020;1606:08415.\n 35. Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units. 2015;arXiv preprint \narXiv: 1508. 07909\n 36. Schuster M, Nakajima K. Japanese and korean voice search. In: 2012 IEEE international conference on acoustics, \nspeech and signal processing (ICASSP). IEEE; 2012. p. 5149–5152\n 37. Loshchilov I, Hutter F. Decoupled weight decay regularization. 2017;arXiv preprint arXiv: 1711. 05101\n 38. Heek J, Levskaya A, Oliver A, Ritter M, Rondepierre B, Steiner A, van Zee M. Flax: a neural network library and ecosys-\ntem for JAX 2020. https:// github. com/ google/ flax\n 39. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, et al. Pytorch: an \nimperative style, high-performance deep learning library. 2019;arXiv preprint arXiv: 1912. 01703\n 40. Sun C, Qiu X, Xu Y, Huang X. How to fine-tune BERT for text classification? 2020. 1905.05583\n 41. Kudo T, Richardson J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for \nneural text processing 2018; 1808.06226\n 42. Adelaar KA. Malayo-sumbawan. Ocean Linguist. 2005;44(2):357–88.\n 43. Bhardwaj R, Majumder N, Poria S. Investigating gender bias in BERT. 2020;2009:05021.\nPage 17 of 17\nWongso et al. Journal of Big Data            (2022) 9:39 \n \n 44. Kurita K, Vyas N, Pareek A, Black AW, Tsvetkov Y. Measuring bias in contextualized word representations. \n2019;1906:07337.\n 45. Mubarok Y. Representation of women in the sundanese proverbs. IJASOS- Int E-J Adv Soc Sci 2017. https://doi.\norg/10.18769/ijasos.309677\n 46. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman SR. Glue: a multi-task benchmark and analysis platform for natu-\nral language understanding. 2018; arXiv preprint arXiv: 1804. 07461\n 47. Ogueji K, Zhu Y, Lin J. Small data? no problem! exploring the viability of pretrained multilingual language models for \nlow-resourced languages. In: Proceedings of the 1st workshop on multilingual representation learning. Association \nfor Computational Linguistics, Punta Cana, Dominican Republic 2021. p. 116–126. https:// aclan tholo gy. org/ 2021. \nmrl-1. 11\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7625031471252441
    },
    {
      "name": "Computer science",
      "score": 0.7607362270355225
    },
    {
      "name": "Natural language processing",
      "score": 0.5894038677215576
    },
    {
      "name": "Language model",
      "score": 0.5654131174087524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5359450578689575
    },
    {
      "name": "Training set",
      "score": 0.49806761741638184
    },
    {
      "name": "Natural language",
      "score": 0.46205487847328186
    },
    {
      "name": "Task (project management)",
      "score": 0.4541584253311157
    },
    {
      "name": "Engineering",
      "score": 0.10399466753005981
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166073570",
      "name": "Binus University",
      "country": "ID"
    }
  ]
}