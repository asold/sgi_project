{
    "title": "Knowledge-Aware Language Model Pretraining",
    "url": "https://openalex.org/W3040558716",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4291326157",
            "name": "Rosset, Corby",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746807616",
            "name": "Xiong Chenyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2575637519",
            "name": "Phan Minh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108846051",
            "name": "Song Xia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3019377260",
            "name": "Bennett, Paul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222630530",
            "name": "Tiwary, Saurabh",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2987283559",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3019416653",
        "https://openalex.org/W2972260442",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2997012196",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W3006188107",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W2710956079",
        "https://openalex.org/W2988615798",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3034569646",
        "https://openalex.org/W3016309009",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3004092718",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2951048068",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2604950042",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3008686018",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2476140796",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2889938809"
    ],
    "abstract": "How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entity-extended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in the hidden representations through edge probing.We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.",
    "full_text": "arXiv:2007.00655v2  [cs.CL]  4 Feb 2021\nKnowledge-A ware Language Model Pretraining\nCorby Rosset, Chenyan Xiong, Minh Phan, Xia Song, Paul Bennett, Saurabh Tiwary\nMicrosoft Corporation\ncorbin.rosset, chenyan.xiong, phan.minh,\nxiaso, paul.n.bennett, satiwary@microsoft.com\nAbstract\nHow much knowledge do pretrained language models hold? Rece nt research ob-\nserved that pretrained transformers are adept at modeling s emantics but it is un-\nclear to what degree they grasp human knowledge, or how to ens ure they do so.\nIn this paper we incorporate knowledge-awareness in langua ge model pretraining\nwithout changing the transformer architecture, inserting explicit knowledge layers,\nor adding external storage of semantic information. Rather , we simply signal the\nexistence of entities to the input of the transformer in pret raining, with an entity-\nextended tokenizer; and at the output, with an additional en tity prediction task.\nOur experiments show that solely by adding these entity sign als in pretraining, sig-\nniﬁcantly more knowledge is packed into the transformer par ameters: we observe\nimproved language modeling accuracy, factual correctness in LAMA knowledge\nprobing tasks, and semantics in the hidden representations through edge probing.\nW e also show that our knowledge-aware language model (KALM) can serve as a\ndrop-in replacement for GPT -2 models, signiﬁcantly improv ing downstream tasks\nlike zero-shot question-answering with no task-related tr aining.\n1 Introduction\nThe strong effectiveness and rich generalization ability o f pretrained language models (PLMs) [1, 2,\n3, 4, 5] have raised many questions about what is captured in t ransformer networks and why. Recent\nexplorations found the pretrained language models may “red iscover” the linguistic pipeline at vari-\nous transformer layers [6], can serve as implicit knowledge bases for relation extraction [7], perform\nsoft reasoning tasks [8, 9], and conduct some language tasks reasonably in a fully unsupervised, zero-\nshot, fashion [3, 10]. With sufﬁciently large amount of para meters, i.e. several billions, and enough\ntask-speciﬁc supervision, the pretrained language models can even directly generate answers for nat-\nural language questions, at the same accuracy with state-of -the-art reading comprehension systems,\nwithout using any context documents or knowledge graphs [11 ].\nImpressive as they are, language models are still far from re ady to serve as an “unsupervised multi-\ntask learner” that learns knowledge directly from human lan guage and generalizes to downstream\nlanguage tasks [3]. There are notable gaps of language model s’ performances on downstream tasks\nbetween models with [11, 12] and without [3] large amounts of task-speciﬁc ﬁne-tuning. The lan-\nguage models still (de-)generate dull, factually-incorre ct, or dream-like text when used in natural\nlanguage generation [13, 14, 15, 16]. These challenges ofte n necessitate over-parameterization [17],\ngrounding on external structural semantics [14, 16, 18, 19] , or large amount of task-speciﬁc ﬁne-\ntuning [11], which are costly, complicated, and not always f easible for every language task.\nOne potential limitation of these language models is their s tyle of pretraining, e.g, auto-regressive\nlanguage modeling [3] or masked language modeling [1], wher ein transformer networks process a\nsequence of words and are asked to predict the next/masked wo rds. There is no explicit guidance\nto the transformers that humans prefer them to capture corre ct, real-world information. As a result,\nPreprint. Under review .\nall the knowledge captured in these pretrained language mod els is only signaled by patterns of co-\noccuring words in the input sequence that is learned implici tly during pretraining.\nIn this paper, instead of creating bigger models or adding kn owledge-speciﬁc architectures, we\npropose to more efﬁciently leverage the existing parameter s in the standard transformer language\nmodel, by simply making them aware of the various forms an ent ity can manifest itself as, and its role\nin the surrounding text. More speciﬁcally, this knowledge- awareness is communicated via the input\nfed to PLMs and in the output expected from them during pretra ining. For input-awareness, we use\nan entity-name (surface form) dictionary that tokenizes wo rd spans to their most popularly referred-\nto entity, e.g., as fuzzy frequency-based entity annotatio ns [20], and serve these entity tokens as a\nparallel input channel along with the word tokens. For outpu t-awareness, in addition to the language\nmodeling objective, we add an entity prediction task that gu ides the model to distinguish the correct\nentity from various negative distractions. The two objecti ves together explicitly guide the language\nmodel to predict not only the correct words, but also the corr ect entity behind those words during\npretraining, without changing the network architecture.\nBy adding knowledge awareness to GPT -2 style auto-regressi ve language models, our pretrained\nlanguage model, “Knowledge-A ware Language Model” (KALM), shows signiﬁcantly improved\nhandling of knowledge-sensitive tasks. In the LAMA knowled ge probing tasks [7], KALM out-\nperforms its entity-unaware baseline, GPT -2, by about 25% a cross all tasks at both base and large\ntransformer sizes. Our 24 layer KALM (Large) is even compara ble with the 17 Billion parameter\nGPT -2 on some tasks. It more accurately captures commonsens e knowledge, factual semantics, and\nalso relation semantics in these LAMA tests. The knowledge s ignals also aid generic language un-\nderstanding: we have observed better language modeling per plexity and word prediction accuracy\nwith KALM too.\nThe advantages in language modeling also transfer to downst ream tasks. In zero-shot question\nanswering, the exact match accuracy of the answers generate d by KALM are 20%-100% better\nthan those of an equivalent GPT -2 model. W e did not use any tas k-speciﬁc supervision or additional\ngradient updates, relying solely on the unsupervised knowl edge learned in KALM. W e only feed\nin a few example question-answer pairs as templates to forma t how generated answers should look.\nInjecting rich knowledge signals leads to improvements app roximately equal to those gained by\ndoubling the transformer layers, indicating that PLMs can b e trained more efﬁciently – growing the\nparameters exponentially is not the only way to improve lang uage understanding.\nT o better understand pretraining and the advantage of knowl edge awareness, we leverage the edge\nprobe technique [6, 21] and dissect what is learned in the rep resentations at various gradient step\nnumbers throughout pretraining. W e observe that the auto-r egressive transformers start to learn the\nbasis of language in the beginning of the pretraining, and gr adually learns more complex semantics\nin the process; adding knowledge-awareness greatly accele rates learning of higher level semantics,\ne.g., coreferences and entity types, and helps the model per form better in those more complicated\ntasks.\n2 Pretraining Knowledge-A ware Language Models\nIn this section we ﬁrst present preliminaries in language mo deling and then how we add knowledge-\nawareness in their pretraining.\n2.1 Preliminary\nIn this paper, without loss of generality, we mainly focus on the auto-regressive language modeling.\nConsidering the text X as a sequence of tokens (words or sub-words): X = {w1, ..., wi, ..., wn},\nthe classical unidirectional factorization of language pr obabilities [22, 23] describes:\np(X) =\n∏\ni\np(wi|w<i), (1)\nwhere w<i refers to all the tokens appear before i. This conditional probability can be parameterized\nin various ways. An effective choice is to use the uni-direct ional transformer, as done in GPT -2 [3]:\np(wi|w<i) =transformer(wi|w<i).\n2\nThe language modeling task provides a large amount of data to pretrain very deep transformer net-\nworks [5, 24, 25]. Scaling up the transformer parameter size s will lead to signiﬁcant improvements\nin the language model capability: with wider and deeper tran sformer layers, it is observed that trans-\nformer language models start to output more complicated sem antics beyond lexical and syntactic\npatterns [6, 8, 7]. On the other hand, a roughly log-linear re lationship between transformer size and\noutput quality has been established, e.g. doubling the qual ity requires ten times more parameters and\ntraining data [3, 17, 10]. Even in industry, the marginal gai n of increasing parameters will eventually\nbe outweighed by the cost to train and serve such models.\n2.2 Knowledge-A ware Pretraining\nAs shown in Eqn. 1, the pretraining is solely at the token leve l. All the semantics in PLMs are\ncaptured by the transformer indirectly; there is no explicit requirement in pretraining to better capture\nknowledge – yet we expect them capture knowledge beneath the raw word sequences implicitly, e.g.,\nto generate factually correct statements.\nIn this work we mitigate this discrepancy by making transfor mer networks aware of knowledge in\nlanguage model pretraining. Instead of stacking more layer s or adding external knowledge storage,\nwe present a knowledge-aware language modeling (KALM) fram ework that packs more information\ninto the same amount of transformer parameters. The ﬁrst ste p to introduce knowledge awareness\nis an entity tokenizer that forms an additional entity token sequence [26] to signa l the existence of\nentities in the input and output of the pretraining process.\nEntity T okenizer .An entity tokenizer segments the text sequence into entity i ds using a surface\nform dictionary, which maps word-ngrams to entities: wi:i+k\ndict look up\n− − − − − − → ei, where ei is the most\npopular entity referred by the word k-gram wi:i+k, and ei = null if wi is not part of any known\nentity surface names.\nThis simple dictionary look-up can be conducted efﬁciently , similar to the (sub)word tokenizer. Si-\nmultaneously, the text is tokenized into two channels – a wor d-entity duet token sequence [26]:\nXduet =\n{ {w1, ..., wi, ..., wT } W ord Sequence;\n{e1, ..., ei, ..., eT } Entity Sequence. (2)\nThe two sequences are aligned position by position. If multi ple (sub)words together form an entity\nname, the corresponding entity id is duplicated in each posi tion corresponding to these words. For\nexample, the name “United States” at wi:i+2 is mapped to entity “USA ” in ei and ei+1.\nInstead of enlisting a more precise entity linker or supervi sions from entity labels [16, 18], a fuzzy\nfrequency-based dictionary look up places higher expectat ions on the model to use clues in the text\nto jointly build token and entity representations that bett er reﬂect how language conveys knowledge.\nUsing a highly tuned entity linker would propagate its own bi ases into the transformer.\nKnowledge-A ware Input. Just as there is an input embedding for every word token, we al low the\nmodel to learn an entity embedding for each entity:\n⃗ ei = Embeddinge(ei) ∈ Rde , (3)\n⃗ wi = Embeddingw(wi) ∈ Rdw . (4)\nThe two embeddings are combined to form the knowledge aware i nput:\n⃗ti = ⃗ wi + Lineart(⃗ ei), Lineart ∈ Rde× dw . (5)\nAll the embeddings are randomly initialized and learned in p retraining.\nKnowledge-A ware Output.The knowledge-aware input is fed into standard transformer layers, the\nsame as the word-only input. Then in pretraining, besides th e next-word prediction task, we also\nemploy a next-entity prediction task to further incorporat e knowledge-awareness.\nSpeciﬁcally, we use one output head for the word probability , one for the entity, and share all trans-\nformer layers between words and entities. If there are L transformer layers and hL\ni is the output of\nthe ﬁnal layer’s i-th token, the loss for position i is computed as\n3\nle(ei|t<i) =max(0, s( ⃗hi\nL\n, ⃗ ei) − s( ⃗hi\nL\n, ⃗ e− ) +λ), (6)\ns( ⃗hi\nL\n, ⃗ ej ) =cos(Linear( ⃗hi\nL\n), ⃗ ej ), (7)\n⃗hi\nL\n= transformerL(t<i). (8)\nThe transformers in Eqn. 8 are stacked multiple times simila r to GPT -2. The scoring function in\nEqn. 7 projects the hidden state into the embedding space wit h a Linear layer and takes the cosine\nsimilarity with an arbitrary entity ej . Assuming that position i is linked by our entity linker to ei,\nwe carefully choose a corresponding negative entity e− to contrast with ei using margin loss with\nmargin λ in Eqn. 6.\nPretraining. The knowledge-aware input and output are incorporated in th e standard multi-task set\nup for our KALM knowledge-aware language model pretraining :\nlKALM (Xduet ) =\n∑\ni\nlw(p(wi|t<i)) +αle(ei|t<i). (9)\nIt combines the language modeling loss lw() – cross-entropy as in standard PLMs – with the entity\nprediction loss le(), where α is a hyper-parameter to balance the two.\nInference. At inference time – whenever generating output text – KALM us e the word prediction\nhead p(wi|t<i), which is consistent with GPT -2. The entity prediction task is an auxiliary task\nthat guides the model to attend to the entity semantics durin g pretraining; in inference only the\nshared transformer representations is used upon the input w ord and entity tokens. Compared with\nthe standard GPT -2 style PLMs, the architecture of KALM only differs with an enlarged tokeniza-\ntion vocabulary with additional entity tokens, and their en tity embeddings before the input to the\ntransformer network. The transformer architecture and its layer conﬁgurations are kept consistent.\n3 Probing Language Models\nThis section describes the techniques we use to probe the kno wledge capability incorporated in the\nweights of pretrained language models, including LAMA Know ledge Probing [7], Edge Probing [6,\n21], and the zero-shot performance on downstream tasks [3].\nKnowledge Probe. Petroni et al. [7] developed the LAMA knowledge probing test which evaluates\nwhether the language model can predict the factually correc t token in “ﬁll-the-blank” cloze state-\nments. For example, the model is considered to include the co rresponding commonsense knowledge\n“isCapbleOf ” if it can predict the token “ﬂy” or “eat” for the cloze test “birds can\n”.\nThe LAMA cloze statements were semi-manually constructed f rom four knowledge sources:\nGoogle-RE (Wikipedia Relations), T -REx (Wikidata Relatio ns), ConceptNet (Commonsense Re-\nlations), and SQuAD (Questions on Wikipedia). The knowledg e graph triples were transformed\nto statements using manually deﬁned templates. The SQuAD qu estions are manually rewritten to\nstatements. LAMA only keeps cloze statements where the miss ing word is a single token w .r.t the\ntokenizer of the model in question, to avoid the inﬂuence of d ecoding. Sometimes the missing word\ncan appear in the middle of a sentence, but for auto-regressi ve language models such as GPT -2 and\nTransformer-XL [27], LAMA only evaluates on those at the end . W e refer to their paper for more\ndetails [7].\nEdge Probe. Besides looking at the token-level predictions, T enney et a l. [6, 21] develop the edge\nprobing” tasks to study the information (e.g., syntactic, s emantic, or long range structures) in the\nlearned hidden representations. Similar to the evaluation protocol in representation learning [28],\nthe edge probing technique uses the PLM’s hidden representa tions on one or multiple text spans\n(e.g., hi:i+k) as ﬁxed feature representations to linear classiﬁers on va rious linguistic/semantic tasks\n– A better performance indicates stronger knowledge capabi lity of the PLM in the corresponding\ntask.\nIn total eight core NLP tasks are included in edge probing [21 ]: part-of-speech tagging (POS),\nconstituent labeling (Consts.), dependency parsing (Deps .), named entity typing (Entities), semantic\nrole labeling (SRL), coreference (Coref.), semantic probe -role (SPR), and relation classiﬁcation\n4\nT able 1: Size of evaluation sets. The\nnumber of relation types in LAMA are\nin brackets. All evaluations are zero-\nshot on their ofﬁcial testing data.\nDataset Items\nLanguage Modeling\nWikiT ext-103 (tokens) 270k\nLambada 5.1k\nLAMA\nGoogle-Re 4.6k\nT -Rex 1-1 (2) 937\nT -Rex N-1 (23) 20k\nT -Rex N-M (16) 13k\nConceptNet (16) 11k\nSQuAD (Statements) 305\nZero-Shot QA\nTrivia QA 11k\nNatural Questions (Short) 3.7k\nW ebQuestions 2.0k\nT able 2: Speciﬁcations of language models: pa-\nrameters in the network layers ( Net.P .#), in em-\nbeddings ( E.P .#), number of layers ( L.#), and hid-\nden Dimension. GPT -2 from OpenAI is marked by\n(OAI). Note that embeddings are looked up in con-\nstant time; the network capacity are mostly deﬁned\nby Net.P .#[17].\nModel Net.P .# E.P .# L.# Dim\nBase\nGPT -2 90M 38M 12 768\nKALM 90M 458M 12 768\nLarge\nGPT -2 (OAI) 304M 51M 24 1024\nGPT -2 304M 51M 24 1024\nKALM 304M 471M 24 1024\neXntra Large\nGPT -2 XL (OAI) 1.46B 80M 48 1600\nGPT -2 1.5B 1.46B 80M 48 1600\nGPT -2 17B 16.9B 214M 78 4256\n(Relations). The ﬁrst four tasks are considered more local, syntactical, while the later four involve\n“higher-order” and/or long-range phenomena [6].\nZero-Shot Evaluation. Radford et al. [3] demonstrate that their GPT -2 language mod els can be\nviewed as “unsupervised multitask learners” that perform d ownstream tasks without using any task\nspeciﬁc pretraining or ﬁne-tuning [3]. Their deep GPT -2 mod els, though far from fully-supervised\nPLM’s performance, also provide meaningful zero-shot resu lts on several downstream tasks, such as\nquestion answering, summarization, and machine translati on. These zero-shot results show promise\nthat one centralized deep model can conduct many real world t asks without requiring much human\nannotation or task-speciﬁc ﬁnetuning. The new GPT -3 is anot her step in this direction, but at the\ncost of 175 Billion parameters [10]). In the short-term, thi s zero-shot evaluation is a good strategy\nto directly evaluate the knowledge captured in language mod el parameters.\nW e focus on the zero-shot QA setting in Radford et al. [3], and include all the three datasets used by\nRoberts et al. [11]: Trivia QA [29], Natural Questions [30], and W eb Questions [31]. In zero-shot\nsetting, the PLMs are directly asked to generate the answer f or the question; no task related infor-\nmation is provided for training, nor any context documents o r knowledge graph of these questions\nare used. All the knowledge has to come from the model parameters optim ized on the pretraining\ncorpus alone.\nThe only additional information available to the PLMs is the task format as context during inference.\nSeveral example question-answer pairs were concatenated t o the front of the question, to notify the\nmodel to generate answers [3]. In total we use eight manually written dummy QA pairs (listed in\nappendix), e.g. “Question: how many wheels does a semi truck have? Answer: 18”, and prepend\nthem to each question of the testing dataset. All our models t ake the input in the format of\nQuestion: dummy Q \\ n Answer: dummy A \\ n\\ n Question: testing Q \\ n Answer:\nand generates an answer without any additional training bes ides knowledge-aware pretraining.\n4 Experimental Methodologies\nPretraining with Knowledge A wareness. All models in this study are pretrained on a dataset\nsimilar to OpenW ebT ext 1; it contains about 30 billion tokens after ﬁltering and dedu plication.\nThe entity tokenizer in KALM uses the entity dictionary from the CMNS Linker [20], which was\nharvested from the F ACC1 and F AKBA entity annotations [32]. W e keep surface forms with fre-\nquency at least 1k, and entities with at least 100 links – in to tal about 1.4M entities. The tokenizer\n1 https://github.com/NVIDIA/Megatron-LM/tree/master/tools/openwebtext\n5\nT able 3: Results on language modeling tasks and LAMA knowled ge probing tasks. The LAMA\nnumbers are average Precision@1.\nLanguage Modeling LAMA Knowledge Probing\nWiki-103 Lambada G-Re T -REx C-Net Squad\nModel Perplex. last word T otal 1-1 N-1 N-M T otal T otal T otal\nBase (∼ 100M)\nGPT -2 20.85 33.73 3.99 24.17 14.19 16.72 15.66 7.67 4.55\nKALM 22.51 40.26 3.27 44.70 24.95 25.07 25.96 8.61 6.64\nLarge (∼ 300M)\nGPT -2 (OAI) 22.50 42.98 3.71 56.03 19.77 19.93 21.6 10.86 8.04\nGPT -2 20.46 42.63 4.90 45.95 19.28 18.59 20.31 9.72 5.94\nKALM 17.05 49.14 5.41 63.18 25.74 27.15 28.12 10.70 11.89\neXntra Large (>1B)\nGPT -2 1.5B (OAI) 17.37 51.23 4.30 62.31 21.53 19.61 22.77 12.28 11.54\nGPT -2 1.5B 14.68 56.72 6.48 65.04 24.04 21.51 25.06 12.79 11.54\nGPT -2 17B 10.21 67.98 8.77 76.82 29.60 27.14 30.95 14.39 22.38\nis forward-greedy by mapping the longest (at most 4-grams) s urface form to its most popular en-\ntity [20]. W e choose this frequency-based dictionary looku p to favor higher entity coverage, linking\nspeed, and recognition of entity polymorphism, while relyi ng on the language model pretraining to\nhandle any inaccuracy in the linking [26].\nEvaluations. The language modeling evaluations include perplexity on Wi kiT ext-103 [33] and ac-\ncuracy in LAMBADA last-word prediction [34], implemented t he same as Megatron-LM [24].\nThe LAMA knowledge probes use their ofﬁcial setting [7] 2 . Particularly we use their set-up to\nevaluate auto-regressive LMs such as GPT [7]. Our results ar e directly comparable with GPT -2 but\nnot BER T as the two use different tokenizers. In zero-shot QA evaluations, the ofﬁcial evaluation\nsplits of Trivia QA [29], Natural Questions (Short Answer Se tting) [30], and W eb Questions [31] are\nused. Besides exact match (EM), we also show whether the gene rated answers contains the ground\ntruth answer (cover-EM). The edge probe tasks use their ofﬁcial implementation [21] 3 . W e probe the\nlast layer of PLMs at every 1k pretraining gradient steps fro m 5k to 10k, and again at convergence.\nCompared Methods. W e mainly compare with the autoregressive GPT -2 architectu re, and pretrain\nour own version GPT -2 in-house, following the implementati on of Megatron-LM [24]. All else\nremaining equal, we pretrain KALM exactly as in-house GPT -2 networks, except with knowledge\nawareness. The statics of all the evaluation tasks are liste d in T able 1. The architecture speciﬁcation\nof these models are listed in T able 2. W e measured that the KAL M models had a forward-pass\nruntime that was a constant added to that of an equivalent GPT -2; doubling the number of layers\ndoes not change this constant look up cost.\nImplementation Details. W e trained our models on either 32 or 64 Nvidia v100 GPUs in DGX -2\npods for 3-6 days using the DeepSpeed ZeRO optimizer [35]. In all cases, we used the same linear\nlearning rate warm-up over 3.2k steps to a maximum of 1.5e-4 w ith batches of 512 sequences each of\nlength 1024, with cosine learning rate decay to a minimum of 1 e-5 over at most 300k steps. Models\nwere trained until convergence around 200-250k steps (4-5 e pochs). Dropout of 0.1 was applied on\nthe input tokens and the attention layers; the model had ℓ-2 regularization of 0.01. W e initialized all\nmatrices with a uniform normal.\nFor KALM we added an additional dropout on the linked entitie s, replacing them with the null\nentity 10% of the time. W e also sampled negatives for the outp ut entity prediction task according\nto the following regimen: 1% of the time the negative was the null entity, 49% it was a random\nentity chosen from among all 1.4M, and the remaining 50% was a “difﬁcult” negative chosen from\nthe 100 nearest Trans-E neighbors of the positive entity. Al l our models are pretrained from scratch\nwith entity embedding dimension de = 300. More details can be found in Appendix.\n2 https://github.com/facebookresearch/LAMA\n3 https://github.com/ jsalt18-sentence-repl/jiant\n6\nT able 4: Zero-shot question answering performance of diffe rent models for three different question\nanswering benchmarks. EM is exact match percent, and cover- EM is the percent counting whether\nthe correct answer is a substring of the generated answer. Al l generated answers were generated by\na greedy decoding of at most 20 tokens.\nTrivia QA Natural Questions W eb Questions\nModel EM cover-EM EM cover-EM EM cover-EM\nFully Supervised\nT5 Base [11] 29.1 n.a. 27.0 n.a. 29.1 n.a.\nT5 Large [11] 35.9 n.a. 29.8 n.a. 32.2 n.a.\nT5 11B [11] 50.1 n.a. 34.5 n.a. 37.4 n.a.\nZero-Shot\nGPT -2 Base 3.44 4.77 0.81 1.24 2.08 2.92\nKALM Base 5.87 7.16 1.75 2.13 3.53 4.79\nGPT -2 Large 7.32 9.05 3.48 4.26 4.79 6.20\nKALM Large 11.68 13.34 4.34 5.07 6.56 9.48\nGPT -2 1.5B 17.78 21.59 6.08 7.95 6.20 12.65\nGPT -2 17B 42.32 47.56 14.34 17.30 12.15 21.68\n5 Evaluation Results\nThis section present evaluations on language modeling task s and the probe tasks.\n5.1 Language Modeling and Knowledge Probing Results\nIn T able 3 we show both the WikiT ext (perplexity) and Lambada (last word accuracy), adjacent to the\nLAMA knowledge probe (precision at 1). In general, the in-ho use GPT -2 results, with more training\ndata, are slightly better than their Open AI implementation s of equivalent architecture. W e also\nconsistently see that KALM has around 15%-20% improvement o n LAMBADA accuracy. More\nimportantly, on LAMA which directly tests factual correctn ess, KALM consistently improves over\nGPT -2 by margins of 40-80%, even approaching performance of a GPT -2 model with 5-fold more\ntransformer parameters.\nThe improvements from KALM are most noticeable on T -Rex, whi ch tests the model’s skill on\nhundreds of knowledge base relations like “X is owned by\n”. The gain is even more signiﬁcant\non more complex relations (N-1 and N-M), where GPT -2 is confu sed the most. KALM Large even\nmatches the accuracy of GPT -2 17B on the most difﬁcult N-M rel ations, using only 2% transformer\nparameters. Simply by being aware of the existence of entities in pretra ining, KALM enjoys signif-\nicantly better effectiveness and parameter efﬁciency in le arning how to capture factual knowledge\nin standard transformer layers.\n5.2 Zero-Shot T ask Performances\nIn T able 4 we compare the zero-shot question answering perfo rmance of GPT -2 and KALM, as\nwell as T5 ﬁne-tuned in a supervised fashion. Remarkably, ev en though the largest GPT -2 model\nwas only trained on unsupervised text prediction tasks, it s till performs nearly as well as the fully-\nsupervised T5 11B model on Trivia QA, conﬁrming that indeed, larger language models do capture\nsome knowledge in an unsupervised way. Y et, the captured kno wledge seems more on the “trivial\nside” as GPT -2 performs much worse on NQ and WQ, which are more similar to what real users\nask in search engines. This observation still holds even in t he newly-announced GPT -3 with 175\nBillion parameters [10], which has 30 EM on NQ, versus 44.5 in the RAG model which uses “only”\nhundreds of millions parameters [12].\nThough far from perfect, KALM signiﬁcantly improves the zer o-shot accuracy on all three QA\ndatasets. It more efﬁciently packs practical knowledge int o its parameters that can answer human\nquestions: the 12L KALM Base is very close to the zero-shot ac curacy of the 24L GPT -2 Large.\nThis indicates that one day pretrained language models may b e able to capture complex semantics\nin real world tasks without needing orders of magnitude more parameters.\nKALM has greater capabilities without sacriﬁcing speed. It s extra entity embeddings do not in-\ncrease run time much – they only introduce constant time matr ix lookup operations after a linear\n7\n5k 10k ... 200k\n70.0\n74.0\n78.0\nF1 Relations\n5k 10k ... 200k\n80.0\n82.5\nF1 SPR\n5k 10k ... 200k\n92.0\n93.0\nF1 Coref.\n5k 10k ... 200k\n84.0\n86.0\nF1 SRL\n5k 10k ... 200k\n92.0\n94.0\nF1 Entities\n5k 10k ... 200k\n91.0\n92.0\nF1 Deps.\n5k 10k ... 200k\n74.0\n76.0\n78.0\nF1 Consts.\n5k 10k ... 200k\n94.0\n95.0\nF1 POS\nGPT-2 Base\nKALM Base\nFigure 1: Edge probing results on eight NLP tasks. The base ve rsions of GPT -2 and KALM are\nprobed, along different training steps (x-axies), using mi cro-averaged F1 scores (y-axes).\ntime entity tokenization – this is one of our motivations of ﬁ rst adding the knowledge “awareness”\nto standard transformers before proceeding to specialized knowledge layers or external “memory”\nmodules.\n5.3 Probing Knowledge A ware Pretraining\nIn Figure 1 we show the edge probing results during the langua ge model pretraining in terms of F1\non the eight NLP tasks [21]. Theses tasks are ordered followi ng the observed “NLP Pipeline” order\nin BER T layers [6]. T asks thought to require more sophistica tion or long range semantics are shown\nfurther left. Intuitively, the “easier” tasks that focus on local syntax, e.g., POS, are learned earlier in\nthe pretraining process, while the “harder” ones, especial ly those require long range dependencies,\nsuch as relation extraction and coreference, are captured l ater in the pretraining. Transformers seems\nto learn what words are before learning how they interact to f orm higher levels of meaning.\nThe hidden representations learned by KALM are more informa tive in all eight categories at con-\nvergence. KALM starts slower in the beginning of pretrainin g (at 5k steps), as it needs to learn the\nadditional entity embeddings from scratch; then the extra k nowledge awareness quickly help KALM\nsurpass GPT -2 after 5k steps. The beneﬁts of KALM are more app arent on knowledge-related tasks,\nsuch as Entity typing, Relations, and Semantic Proto-Roles .\n6 Related W ork\nAre deep transformers just memorizing the corpus, e.g., usi ng as many as one parameter per two\ntraining tokens [10], or do they abstract concepts and knowl edge from the data [3, 5, 11, 36]? Many\nobserved that the pretrained transformer weights do includ e certain forms of logic and semantic\nknowledge [6, 7, 9, 37]: GPT -2 and GPT -3 can perform downstre am tasks they never saw in training\nin a zero-shot fashion [3, 10]; T5 with task-speciﬁc ﬁne-tun ing can accurately answer some questions\nwithout using the background document or knowledge graph [5 , 11].\nY et it is also observed these enormous pretrained transform ers often “dream” and produce lan-\nguage that might look right at ﬁrst glance, but actually incl ude biased [38], repetitive [13, 15],\nuseless [39], or even worse, misleading information [14]. T o address this, previous research mainly\ngrounds the language model to some forms of explicit semanti c structures, for example, additional\nentity-oriented transformer layers [16, 18], or explicitl y stored knowledge graph triples [19, 40, 41].\nGrounding provides a “safety net” for language models to loo k up a semantic facts in their external\nstorage, instead of relying on their transformer parameter s to incorporate correctness.\n7 Conclusion and Future Directions\nKALM improves the knowledge learning capability of languag e models by signaling to the language\nmodel the existence of entities during pretraining. This si mple knowledge-awareness signiﬁcantly\n8\nimproves the parameter efﬁciency of transformers in pretra ining. In our experiments on various\ntasks, KALM often matches the effectiveness accuracy of the “knowledge-unware” GPT -2 that uses\ntwice as many transformer layers as KALM.\nThe fact that, adding knowledge awareness leads to improvem ents almost equal to those from dou-\nbling the transformer layers, indicates language models ca n be pretrained with better parameter\nefﬁciency. Brute-force blowing-up the model parameter num bers exponentially is not the only way\nto achieve a better language model – better intelligence is n ot only created by network sizes but can\nalso by proper optimization, better understanding of pretr aining, model architecture design, and the\ninductive biases within it.\n9\nBroader Impact\nWith the dramatic impacts of pretrained language models in A I, there are also concerns on the\nuncanny valley stage of their information reliability, as w ell as their limited democratization due to\nhigh demand in computing resource. KALM, as a pretrained lan guage model itself, moves one step\nforward to address both concerns.\nAddress misinformation output by language models. One motivation of KALM is to improve\nthe factual correctness of pretrained language models. W e h ave observed the strong uncanny valley\nphenomena with those PLMs: many generated outputs from PLMs are perfectly correct syntactically,\nbut include many factual or semantic errors. More over, thes e factual errors often appear believable\nto many and their misinformation may only be noticeable by do main experts. KALM provides a\nsimple and effective way to improve the inherent factual cor rectness of pretrained language models.\nW e also deliberately make sure we keep the transformer archi tecture unchanged, so that others can\nbuild on existing proven technology, and to encourage other s to investigate how to train transformers\nto serve humanity’s desire for factual correctness and ethi cs.\nHelp democratize AI with reduced resource requirement.The exponential growth of the comput-\ning resource used to pretrain deep transformer language mod els is unsustainable in multiple fronts.\nWith PLMs growing to hundreds of billions of parameters, not only conducting research in pretrain-\ning will become (if not already) a privilege, the ability to ﬁ ne-tune or even apply such enormous\nmodels may only be limited to a few institutions. KALM shows t hat blowing-up the model size is\nnot the only way to improve language models: there are ways th e broader community can access\nand contribute to language model pretraining, for instance , by addressing how to teach them and\nhow to inﬂuence their behavior to output correct and ethical text. At smaller scales, truly effective\nand generalizable techniques would likely beneﬁt larger, m ore powerful models.\nReferences\n[1] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Ling uistics: Human Language T echnologies ,\npages 4171–4186, 2019.\n[2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Jos hi, Danqi Chen, Omer Levy , Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov . RoBERTa: A Robustl y Optimized BERT Pretraining Approach.\narXiv preprint arXiv:1907.11692, 2019.\n[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 2019.\n[4] Zhilin Y ang, Zihang Dai, Yiming Y ang, Jaime Carbonell, R uss R Salakhutdinov , and Quoc V Le. XLNet:\nGeneralized Autoregressive Pretraining for Language Unde rstanding. In Advances in Neural Information\nProcessing Systems, pages 5754–5764, 2019.\n[5] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee , Sharan Narang, Michael Matena, Y anqi Zhou,\nW ei Li, and Peter J Liu. Exploring the limits of transfer lear ning with a uniﬁed text-to-text transformer.\narXiv preprint arXiv:1910.10683, 2019.\n[6] Ian T enney , Dipanjan Das, and Ellie Pavlick. Bert redisc overs the classical nlp pipeline. arXiv preprint\narXiv:1905.05950, 2019.\n[7] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patr ick Lewis, Anton Bakhtin, Y uxiang Wu, and\nAlexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages 2463–2473, 2019.\n[8] Peter Clark, Oyvind T afjord, and Kyle Richardson. Trans formers as soft reasoners over language. arXiv\npreprint arXiv:2002.05867, 2020.\n[9] Peter Clark, Oren Etzioni, Tushar Khot, Bhavana Dalvi Mi shra, Kyle Richardson, Ashish Sabharwal,\nCarissa Schoenick, Oyvind T afjord, Niket T andon, Sumithra Bhakthavatsalam, et al. From’f’to’a’on the\nny regents science exams: An overview of the aristo project. arXiv preprint arXiv:1909.01958, 2019.\n10\n[10] T om B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbia h, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, S andhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T om Henighan, Rewon Child, Aditya Ramesh , Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray , Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, I lya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. 2020.\n[11] Adam Roberts, Colin Raffel, and Noam Shazeer. How much k nowledge can you pack into the parameters\nof a language model? arXiv preprint arXiv:2002.08910, 2020.\n[12] Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, W en-tau Yih, Tim Rocktäschel, et a l. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020.\n[13] Margaret Li, Stephen Roller, Ilia Kulikov , Sean W ellec k, Y -Lan Boureau, Kyunghyun Cho, and Jason\nW eston. Don’t say that! making inconsistent dialogue unlik ely with unlikelihood training. arXiv preprint\narXiv:1911.03860, 2019.\n[14] Robert Logan, Nelson F . Liu, Matthew E. Peters, Matt Gar dner, and Sameer Singh. Barack’s wife hillary:\nUsing knowledge graphs for fact-aware language modeling. I n Proceedings of ACL , pages 5962–5971,\n2019.\n[15] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Y ejin Choi. The curious case of neural text degen-\neration. arXiv preprint arXiv:1904.09751, 2019.\n[16] Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwa rtz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. Knowledge enhanced contextual word representation s. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP), pages 43–54, 2019.\n[17] Jared Kaplan, Sam McCandlish, T om Henighan, T om B Brown , Benjamin Chess, Rewon Child, Scott\nGray , Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling la ws for neural language models. arXiv\npreprint arXiv:2001.08361, 2020.\n[18] Thibault Févry , Livio Baldini Soares, Nicholas FitzGe rald, Eunsol Choi, and T om Kwiatkowski. Entities\nas experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202, 2020.\n[19] Hiroaki Hayashi, Zecong Hu, Chenyan Xiong, and Graham N eubig. Latent relation language models. In\nProceedings of AAAI 2020, 2020.\n[20] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsb erg. Entity Linking in Queries: Efﬁciency\nvs.Effectiveness. In European Conference on Information Retrieval, pages 40–53, 2017.\n[21] Ian T enney , Patrick Xia, Berlin Chen, Alex W ang, Adam Po liak, R Thomas McCoy , Najoung Kim, Ben-\njamin V an Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing for\nsentence structure in contextualized word representation s. arXiv preprint arXiv:1905.06316, 2019.\n[22] Y oshua Bengio, Réjean Ducharme, Pascal Vincent, and Ch ristian Jauvin. A neural probabilistic language\nmodel. Journal of machine learning research, 3(Feb):1137–1155, 2003.\n[23] Frederick Jelinek. Interpolated estimation of markov source parameters from sparse data. 1980.\n[24] Mohammad Shoeybi, Mostofa Patwary , Raul Puri, Patrick LeGresley , Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language m odels using gpu model parallelism. arXiv\npreprint arXiv:1909.08053, 2019.\n[25] Corby Rosset. Turing-nlg: A 17-billion-parameter lan guage model by microsoft.\nhttps://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-micr osoft/,\n2020.\n[26] Chenyan Xiong, Jamie Callan, and Tie-Y an Liu. W ord-ent ity duet representations for document rank-\ning. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, Shinjuku, T okyo, Japan, August 7-11, 2017, pages 763–772, 2017.\n[27] Zihang Dai, Zhilin Y ang, Yiming Y ang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov .\nTransformer-XL: Attentive Language Models beyond a Fixed- Length Context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988, 2019.\n11\n[28] Kaiming He, Haoqi Fan, Y uxin Wu, Saining Xie, and Ross Gi rshick. Momentum contrast for unsuper-\nvised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.\n[29] Mandar Joshi, Eunsol Choi, Daniel S W eld, and Luke Zettl emoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.\n[30] T om Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld , Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton L ee, et al. Natural questions: a benchmark for\nquestion answering research. Transactions of the Association for Computational Linguistics, 7:453–466,\n2019.\n[31] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Li ang. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language\nprocessing, pages 1533–1544, 2013.\n[32] Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag S ubramanya. Facc1: Freebase annotation of\nclueweb corpora, version 1. Note: http://lemurproject. org/clueweb09/F ACC1/Cited by, 5, 2013.\n[33] Stephen Merity , Caiming Xiong, James Bradbury , and Ric hard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843, 2016.\n[34] Luong Hoang, Sam Wiseman, and Alexander Rush. Entity tr acking improves cloze-style reading compre-\nhension. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\n2018.\n[35] Samyam Rajbhandari, Jeff Rasley , Olatunji Ruwase, and Y uxiong He. Zero: Memory optimizations\ntoward training trillion parameter models, 2019.\n[36] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton B akhtin, Y uxiang Wu, Alexander H Miller, and\nSebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\n[37] Kevin Clark, Urvashi Khandelwal, Omer Levy , and Christ opher D. Manning. What does BERT look at?\nan analysis of BERT’s attention. In Proceedings of the 2019 ACL W orkshop BlackboxNLP: Analyzin g\nand Interpreting Neural Networks for NLP, pages 276–286, August 2019.\n[38] Moin Nadeem, Anna Bethke, and Siva Reddy . Stereoset: Me asuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456, 2020.\n[39] Corbin Rosset, Chenyan Xiong, Xia Song, Daniel Campos, Nick Craswell, Saurabh Tiwary , and Paul\nBennett. Leading conversational search by suggesting usef ul questions. In Proceedings of The W eb\nConference 2020, pages 1160–1170, 2020.\n[40] Houyu Zhang, Zhenghao Liu, Chenyan Xiong, and Zhiyuan L iu. Grounded conversation generation as\nguided traverses in commonsense knowledge graphs. In Proceedings of ACL, 2020.\n[41] Sungjin Ahn, Heeyoul Choi, T anel Pärnamaa, and Y oshua B engio. A neural knowledge language model.\narXiv preprint arXiv:1608.00318, 2016.\n12\nA Appendix\nA.1 More Implementation Details\nCMNS Entity T okenizationW e performed entity linking in linear time with a forward-gr eedy approach. This\nmeans that the linker looks at all consecutive groups of up to four space-delimited words and repeatedly query-\ning variants of that window in the dictionary (variants such as lowercase, uppercase, capital case, and without\npunctuation). W e ﬁrst query all the variants of the 4-word wi ndow before moving down to 3 words, and so on\ndown to 1 word, before incrementing the window position and r esetting it to 4. Whenever there is a hit in the\ndictionary , we link all the word tokens in the candidate ment ion span to the entity with the highest frequency ,\nand then we move the window up past the words that were hit. Not e that the entity linking is done in the “word\nspace” meaning it looks at space-delimited words, but we tak e great care to ensure that all the tokens in the\n“sub-word” space that comprise the matched entity string ar e labeled as well. In all our experiments, we used a\nBPE subword vocabulary , but this technique will work for any standard word or subword vocabulary .\nZero-shot QA Context T emplate. W e normalize both gold and predicted answers by lowercasing them. W e\nﬁlter Natural Questions to those with short answers as in the T5 paper [11]: we ignore questions whose answers\nare longer than 5 tokens. W e use the unﬁltered version of the T rivia QA dataset. For both Natural Questions\nand Trivia QA datasets, we report the performance on the vali dation sets because the test sets with labels are\nnot publicly available.\nFor each question in any QA dataset, we preﬁx the test questio n with a template of eight example questions.\nThe exact template we use is:\nquestion: where is the Lincoln Memorial located? \\n answer: Washington, DC, USA\n\\n \\n ’\nquestion: How long is the Nile river \\n answer: 6250 miles \\n \\n\nquestion: who was elected president of the united states in 1 928? \\n answer:\nHerbert Hoover \\n \\n\nquestion: what year did the September 11th attack occur? \\n answer: 2001 \\n \\n\nquestion: what elements of the periodic table are liquid at r oom temperature? \\n\nanswer: bromine, mercury \\n \\n\nquestion: which pigment helps plant absorb energy from ligh t? \\n answer:\nchlorophyll \\n \\n\nquestion: who was the commander of the japanese navy for the m ajority of World War\nII? \\n answer: Isoroku Yamamoto \\n \\n\nquestion: name of a famous highway without speed limits? \\n answer: Autobahn \\n\n\\n\nquestion: how many wheels does a semi truck have? \\n answer: 18 \\n \\n\nKALM with Input entities only . W e found that a KALM model that only had knowledge awareness a t the\ninputs (no entity contrasting loss at the output) had qualit y that was in between that of a knowledge-unaware\nGPT -2 and our full KALM. For a 12 layer input-only model, the w ikitext perplexity was 26.45 and the lambada\naccuracy was 36.03. On the zero-shot question answering tas ks, the triviaQA exact match was 4.5%, the natural\nquestions EM was 1.5%, and lastly the webquestions EM was 2.7 %.\n13"
}