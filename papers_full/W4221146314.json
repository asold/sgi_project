{
  "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
  "url": "https://openalex.org/W4221146314",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5108128673",
      "name": "Jiayi Wang",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A5055860272",
      "name": "Rongzhou Bao",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A5070962435",
      "name": "Zhuosheng Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A5100457332",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3101449015",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963491027",
    "https://openalex.org/W2954978443",
    "https://openalex.org/W2766108848",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2970078867",
    "https://openalex.org/W3169948074",
    "https://openalex.org/W3099729825",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2963384482",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2964232431",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3173596483",
    "https://openalex.org/W3117433489",
    "https://openalex.org/W2982054702",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3116387889",
    "https://openalex.org/W2970449623",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W3127569499",
    "https://openalex.org/W2612372205",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2951004499",
    "https://openalex.org/W3035164976",
    "https://openalex.org/W3174150157",
    "https://openalex.org/W3015001695"
  ],
  "abstract": "Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 905 - 915\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDistinguishing Non-natural from Natural Adversarial Samples for More\nRobust Pre-trained Language Model\nJiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao∗\nDepartment of Computer Science and Engineering, Shanghai Jiao Tong University\nKey Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\nwangjiayi_102_23@sjtu.edu.cn, rongzhou.bao@outlook.com\nzhangzs@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nRecently, the problem of robustness of pre-\ntrained language models (PrLMs) has received\nincreasing research interest. Latest studies on\nadversarial attacks achieve high attack success\nrates against PrLMs, claiming that PrLMs are\nnot robust. However, we ﬁnd that the ad-\nversarial samples that PrLMs fail are mostly\nnon-natural and do not appear in reality. We\nquestion the validity of current evaluation of\nrobustness of PrLMs based on these non-\nnatural adversarial samples and propose an\nanomaly detector to evaluate the robustness\nof PrLMs with more natural adversarial sam-\nples. We also investigate two applications of\nthe anomaly detector: (1) In data augmen-\ntation, we employ the anomaly detector to\nforce generating augmented data that are dis-\ntinguished as non-natural, which brings larger\ngains to the accuracy of PrLMs. (2) We ap-\nply the anomaly detector to a defense frame-\nwork to enhance the robustness of PrLMs. It\ncan be used to defend all types of attacks\nand achieves higher accuracy on both adversar-\nial samples and compliant samples than other\ndefense frameworks. The code is available\nat https://github.com/LilyNLP/Distinguishing-\nNon-Natural.\n1 Introduction\nPre-trained language models (PrLMs) have\nachieved state-of-the-art performance across a wide\nvariety of natural language understanding tasks\n(Devlin et al., 2018; Liu et al., 2019a; Clark et al.,\n2020). Most works of PrLMs mainly focus on\ndesigning stronger model structures and training\nobjectives to improve the accuracy or training ef-\nﬁciency. However, in real industrial applications,\nthere exist noises that can mislead the predictions\nof PrLMs (Malykh, 2019), which raise potential\nsecurity risks and limit the application efﬁcacy of\n*Corresponding author. This work was supported in part\nby the Key Projects of National Natural Science Foundation\nof China under Grants U1836222 and 61733011.\nPrLMs in practice. To solve this challenge, stud-\nies around the robustness of PrLMs have received\nincreasing research interest. Recent studies demon-\nstrated that, due to the lack of supervising signals\nand data noises in the pre-training stage, PrLMs\nare vulnerable to adversarial attacks, which can\ngenerate adversarial samples to fool the model\n(Zhang et al., 2020). A variety of attack algo-\nrithms have been proposed to use spelling errors\n(Li et al., 2019), synonym substitutions (Jin et al.,\n2020), phrase insertions (Le et al., 2020) or sen-\ntence structure reconstructions (Zhao et al., 2018)\nto generate adversarial samples. Some of these\nattack algorithms have achieved an over 90% at-\ntack success rate on PrLMs (Li et al., 2020; Garg\nand Ramakrishnan, 2020). Thus they claim that\nexisting PrLMs are not robust.\nHowever, we investigate the adversarial samples\non which PrLMs fail, and ﬁnd that most of them are\nnot natural and ﬂuent, thus can be distinguished by\nhumans. These samples are unlikely to appear in\nreality and are against the principle that adversarial\nsamples should be imperceptible to humans (Zhang\net al., 2020). Therefore it is not reasonable to judge\nthe robustness of PrLMs based on these non-natural\nadversarial samples. By adopting a PrLM-based\nanomaly detector and a two-stage training strategy,\nwe empirically demonstrate that most of the non-\nnatural adversarial samples can be detected by the\nmachine. Furthermore, we adopt the anomaly score\n(the output probability of the anomaly detector)\nas a constraint metric to help adversarial attacks\ngenerate more natural samples. Under this new\nconstraint of generating natural samples, the attack\nsuccess rates of existing attack methods sharply\ndecrease. These experimental results demonstrate\nthat the robustness of PrLMs is not as fragile as\nprevious works claimed.\nThen we explore two application scenarios of\nthe anomaly detector. Firstly, we wonder whether\nthe anomaly detection can generalize to other appli-\n905\ncations using artiﬁcially modiﬁed sentences. Thus\nwe think of the data augmentation scenario. The\nobjective of data augmentation is to increase the\ndiversity of training data without explicitly collect-\ning new data (Wei and Zou, 2019). For an original\nsequence and a data augmentation technique, there\nexist many possible augmented sequences. We ap-\nply the anomaly detector to select among these\npossibilities the augmented sequence that can bring\nmore diversity into training data. For each original\nsequence, we continuously generate augmented se-\nquences until the anomaly detector distinguishes\none as anomaly. The augmented data under this\nconstraint can further increase the prediction accu-\nracy of PrLMs than ordinary data augmentation.\nSecondly, we integrate the anomaly detector into\na defense framework to enhance the robustness of\nPrLMs. Inspired by the defense methods in the\ncomputer vision domain (Liu et al., 2019b; Das\net al., 2017; Raff et al., 2019) which apply transfor-\nmations like JPEG-based compression to mitigate\nthe adversarial effect, we use textual transforma-\ntions to restore adversarial samples. We consider a\ncandidate set of transformation functions including\nback translation, MLM suggestion, synonym swap,\nadverb insertion, tense change, and contraction.\nFor the input sequence that is detected as an adver-\nsarial sample, we randomly apply ktransformation\nfunctions from the candidate set to the sequence.\nWe send the ktransformed sequences to the PrLM\nclassiﬁer to get their prediction scores. The ﬁnal\nprediction is based on the average of these k pre-\ndiction scores. Empirical results demonstrate that\nthis defense framework achieves higher accuracy\nthan other defense frameworks on both adversar-\nial samples and compliant samples (By compliant\nsamples, we mean the non-adversarial samples in\noriginal datasets).\n2 Related Work\nThe study of the robustness of PrLMs is based on\nthe competition between adversarial attacks and de-\nfenses. Adversarial attacks ﬁnd the adversarial sam-\nples where PrLMs are not robust, while defenses\nenhance the robustness of PrLMs by utilizing these\nadversarial samples or modifying model structure\nagainst the attack algorithm.\n2.1 Adversarial Attacks\nProblem Formulation Adversarial attacks gen-\nerate adversarial samples against a victim modelF,\nwhich is a PrLM-based text classiﬁer in this paper.\nGiven an input sequence X, the victim model F\npredicts its label F(X) = y. The corresponding\nadversarial sample Xadv should alter the prediction\nof the victim model and meanwhile be similar to\noriginal sequence:\nF(Xadv) ̸= F(X)\ns.t. d (Xadv,X) <σ,\n(1)\nwhere d() measures the size of perturbations, and\nσis a predeﬁned threshold.\nClassiﬁcation of attacks Adversarial attacks\ncan be conducted in both white-box and black-box\nscenarios. In the white-box scenario (Meng and\nWattenhofer, 2020), adversarial attacks can access\nall information of their victim models. In the black-\nbox scenario, adversarial attacks can only get the\noutput of the victim models: if they get predic-\ntion scores, they are score-based attacks (Jin et al.,\n2020); if they get the prediction label, they are\ndecision-based attacks (Wallace et al., 2020).\nAccording to the granularity of perturbations,\ntextual attacks can be classiﬁed into character-level,\nword-level, and sentence-level attacks. Character-\nlevel attacks (Gao et al., 2018) introduce noises\nby replacing, inserting, or deleting a character in\nseveral words. Word-level attacks substitute sev-\neral words by their synonyms to fool the model\n(Jin et al., 2020; Garg and Ramakrishnan, 2020).\nSentence-level attacks generate adversarial samples\nby paraphrasing the original sentence (Iyyer et al.,\n2018) or using a generative adversarial network\n(GAN) (Zhao et al., 2018).\nMetrics to constrain perturbations To evaluate\nthe robustness of PrLMs, it is important that the\nadversarial samples are within a perturbation con-\nstraint. An adversarial sample must have similar\nsemantic meaning to the original sample, while\nsyntactically correct and ﬂuent as a natural lan-\nguage sequence. Existing attack methods adopt the\nfollowing metrics to realize this requirement:\n(1) Semantic Similarity: semantic similarity is\nthe most popular metric used in existing attack\nworks (Jin et al., 2020; Li et al., 2020). They\nuse Universal Sentence Encoder (USE) (Cer et al.,\n2018) to encode original sentence and adversarial\nsentence into vectors and use their cosine similarity\nto deﬁne semantic similarity.\n(2) Perturbation Rate: perturbation rate is of-\nten used in word-level attacks (Jin et al., 2020) (Li\n906\nInput \nSequence\nX\nAnomaly \nDetector\nfd\nX\nX\nPrLM\nClassifier\nfc\ntrans1(X)\ntrans2(X)\n.\n.\n.\ntransk(X)\nPrLM\nClassifier\nfc\nOutput \nLabel\n()0 . 5dfX \n() ()def cf X f X\n()0 . 5dfX \n   1,...()def ck iif X f trans X\nRandomly select k \ntransformations\nBack Translation\nMLM Suggestion\nAdverb Insertion\nTense Change\nSynonym Swap\n...\nFigure 1: Defense framework.\net al., 2020) to indicate the rate between the number\nof modiﬁed words and total words.\n(3) Number of Increased Grammar Errors: it\nis the number of increased grammatical errors in\nthe adversarial sample compared to the original\nsample. This metric is used in (Maheshwary et al.,\n2020), (Li et al., 2021) and is calculated using Lan-\nguageTool (Naber, 2003).\n(4) Levenshtein Distance: levenshtein distance\nis often used in character-level attacks (Gao et al.,\n2018). It refers to the number of editing operations\nto convert one string to another.\n2.2 Adversarial Defenses\nThe objective of adversarial defenses is to design\na model which can achieve high accuracy on both\ncompliant and adversarial samples. One direction\nof adversarial defenses is adversarial training. By\naugmenting original training data with adversarial\nsamples, the model is trained to be more robust to\nthe perturbations seen in the training stage (Good-\nfellow et al., 2015). However, it is impossible to\nexplore all potential perturbations within a limited\nnumber of adversarial samples. Empirical results\ndemonstrate that the improvement of robustness\nbrought by adversarial training alone is quite lim-\nited when faced with strong dynamic attacks (Jin\net al., 2020; Maheshwary et al., 2020).\nAnother direction is modifying the model struc-\nture against a speciﬁc type of adversarial attack.\nFor character-level attacks, ScRNN (Pruthi et al.,\n2019) leverages an RNN semi-character architec-\nture to identify and restore the modiﬁed characters.\nFor word-level attacks, DISP (Zhou et al., 2019)\nutilizes a perturbation discriminator followed by\nan embedding estimator to restore adversarial sam-\nples. For sentence-level attacks, DARCY (Le et al.,\n2021) greedily searches and injects multiple trap-\ndoors into the model to catch potential UniTrigger\nattacks (Wallace et al., 2019).\nCertiﬁed robustness is a particular branch of de-\nfense whose aim is to ensure that the model pre-\ndictions are unchanged within a perturbation scope.\nFor example, (Jia et al., 2019) and (Huang et al.,\n2019) certify the robustness of the model when\ninput word embeddings are perturbed within the\nconvex hull formed by the embeddings of its syn-\nonyms. However, certiﬁed robustness is hard to\nscale to deep networks and harms the model’s accu-\nracy on compliant samples due to the looser outer\nbound.\n3 Methods\n3.1 Anomaly Detector\nWe adopt a PrLM-based binary classiﬁer as the\nanomaly detector to distinguish adversarial samples\nfrom compliant samples. For an input sequence X,\nX is ﬁrstly separated into sub-word tokens with a\nspecial token [CLS] at the beginning. A PrLM\nthen encodes the tokens and generates a sequence\nof contextual embeddings {h0,h1,h2,...,h n}, in\nwhich h0 ∈RH is the contextual representation\nof [CLS]. For text classiﬁcation tasks, h0 is used\nas the aggregate sequence representation which\ncontains the sentence-level information. So the\nanomaly detector leverages h0 to predict the prob-\nability that X is labeled as class ˆyd (if X is adver-\nsarial sample, ˆyd = 1; if X is compliant sample,\n907\nFigure 2: Examples of transformation functions used in the defense framework.\nFigure 3: Examples of adversarial samples generated by four adversarial attacks.\nˆyd = 0) by a logistic regression with softmax:\nyd = softmax(Wd(dropout(h0)) +bd). (2)\nAnd we use the binary cross entropy loss func-\ntion to train the anomaly detector :\nlossd = −yd ∗log ˆyd −(1−yd)∗log(1−ˆyd). (3)\nWe adopt a two-stage training strategy for the\nanomaly detector. In the ﬁrst stage, we generate\nthe \"artiﬁcial samples\" using the same way each\nattack modiﬁes the sentence (details of how attacks\nmodify the sentences are described in Section 4.2).\nBut the artiﬁcial samples are not required to alter\nthe prediction result of the PrLM, so the modiﬁ-\ncation is only applied once. For example, to gen-\nerate artiﬁcial samples simulating the word-level\nattack TextFooler, we substitute a portion of words\nby their synonyms in a synonym set according to\nWordNet. The training data consist of original sam-\nples (labeled as 0) in the train set and their corre-\nsponding artiﬁcial samples (labeled as 1). We train\nthe detector on these data so that it can learn to dis-\ntinguish artiﬁcially modiﬁed sequences from natu-\nral sequences. In the second stage, we generate the\nadversarial samples (labeled as 1) from the original\nsamples (labeled as 0) in the train set, and train the\nanomaly detector to distinguish adversarial samples\nfrom original samples. In this way, the detector can\ndistinguish non-naturally modiﬁed examples, and\nespecially the adversarial ones among them. The\nexperimental results in section 5.1 demonstrate that\nthe anomaly detector can accurately distinguish\nadversarial samples from compliant samples.\nTask Dataset Train Test Avg Len\nClassiﬁcation\nMR 9K 1K 20\nSST2 67K 1.8K 20\nIMDB 25K 25K 215\nEntailment MNLI 433K 10K 11\nTable 1: Dataset statistics.\n3.2 Evaluation of Robustness under Anomaly\nScore Constraint\nExisting adversarial samples have applied some\nthresholds to limit the anomaly of adversarial sam-\nples. However, the generated adversarial samples\nare still not natural, indicating that existing metrics\nare not effective enough. In order to measure the\nrobustness of PrLMs with more natural adversar-\nial samples, we use a new metric: anomaly score,\nto constrain the perturbations. Given a sentence\nX, we leverage the probability that X is adversar-\nial sample predicted by anomaly detector as the\nanomaly score of X:\nScore(X) =Prob( ˆyd = 1|X). (4)\nFor existing attacks, we add a threshold on\nanomaly score to enforce the attacks to generate\nmore natural and undetectable adversarial samples.\n908\nMR SST2 IMDB MNLI\nTPR. FPR. F1. TPR. FPR. F1. TPR. FPR. F1. TPR. FPR. F1.\nDeepWordBug 96.2 1.3 97.4 98.5 3.7 97.4 94.4 1.6 96.3 97.6 9.2 94.4\nTextFooler 80.2 3.8 87.2 90.6 18.9 86.5 83.6 2.6 89.8 87.6 11.0 88.2\nBERT-Attack 72.6 4.0 81.9 86.5 12.8 87.1 87.2 3.2 91.6 86.4 13.0 86.7\nSCPN 94.5 4.1 95.2 94.6 12.6 88.2 - - - 93.0 13.4 90.0\nTable 2: Performance of anomaly detector trained on each dataset and each attack method.\nMR SST2 IMDB MNLI\nw/o Cons. w Cons. w/o Cons. w Cons. w/o Cons. w Cons. w/o Cons. w Cons.\nDeepwordbug 82.2 8.5 78.3 2.8 74.2 23.2 76.8 25.2\nTextFooler 80.5 35.2 61.0 31.4 86.6 40.4 86.5 38.3\nBERT-Attack 84.7 13.9 87.2 11.5 87.5 18.9 89.8 15.2\nTable 3: The attack success rate of attacks using BERT as victim model without and with the anomaly score\nconstraint on MR, SST2, IMDB, MNLI.\nThe attack problem formulation now becomes:\nF(Xadv) ̸= F(X)\ns.t. d (Xadv,X) <σ,\nScore(Xadv) <0.5,\n(5)\nwhere d() measures the perceptual difference be-\ntween Xadv and X. Each attack has its own deﬁni-\ntion of d() and threshold σ. And we add on a new\nconstraint that the anomaly score ofXadv should be\nsmaller than 0.5. We investigate the robustness of\nPrLMs under the constraint of anomaly score and\nﬁnd that PrLMs are more robust than previously\nclaimed.\n3.3 Application in Data Augmentation\nIn data augmentation, PrLM is trained on original\nsentences and their artiﬁcially augmented sentences\nto improve the diversity of training data. We con-\nsider random synonym substitution as the augmen-\ntation technique for experiments. For an original\nsequence of nwords, we randomly select p% ∗n\nwords and substitute them with their synonyms to\nform the augmented sequence. For each replaced\nword, the replacing synonym is randomly selected\namong its smost similar synonyms. So we will\nhave in total Cp%∗n\nn ∗sp%∗n possible augmented se-\nquences. In order to select the augmented sequence\nthat can bring more diversity into training data, we\napply the anomaly detector to select the augmented\nsequence that is distinguished as anomaly. For each\noriginal sequence, we continuously apply random\nsynonym substitution to form candidate augmented\nsequences until the detector distinguishes one as\nanomaly.\n3.4 Application in Enhancing Robustness\nThere are two ways to apply the anomaly detector\nin enhancing the robustness of PrLMs: (1) detect\nand then directly block the adversarial samples; (2)\ndistinguish the adversarial samples and conduct\noperations on them to make the PrLMs give correct\npredictions. The ﬁrst application is trivial so we\nexplore the second way.\nWe propose a defense framework as shown in\nFigure 1. We ﬁrstly build a transformation func-\ntion set containing ttransformation function can-\ndidates: Back Translation (translate the original\nsentence into another language and translate it back\nto original language); MLM Suggestion (mask sev-\neral tokens in the original sentence and use masked\nlanguage model to predict the masked tokens); Ad-\nverb Insertion (insert adverbs before verbs); Tense\nChange (change the tense of verbs into another\ntense); Synonym Swap (swap several words with\ntheir synonyms according to WordNet), Contrac-\ntion (contract or extend the original sentence by\ncommon abbreviations). We implement these trans-\nformation functions based on (Wang et al., 2021)\n1. The examples of these transformation functions\nare displayed in Figure 2.\nFor each input sequence X, we apply the\nanomaly detector fd to identify whether it is ad-\nversarial (fd(X) >0.5) or not (fd(X) <0.5). If\nthe X is recognized as compliant sample, it will\nbe directly sent to the PrLM classiﬁer fc to get the\nﬁnal output probability of the defense framework:\nfdef (X) =fc(X). If X is recognized as adversar-\nial sample, we will randomly select ktransforma-\ntion functions from the transformation candidate\n1https://github.com/textﬂint/textﬂint\n909\nBERT RoBERTa ELECTRA\nw/o Cons. w Cons. w/o Cons. w Cons. w/o Cons. w Cons.\nDeepwordbug 82.2 8.5 83.8 10.4 79.4 7.9\nTextFooler 80.5 35.2 67.6 36.3 63.6 33.6\nBERT-Attack 84.7 13.9 73.7 17.4 70.8 14.2\nTable 4: The attack success rate of attacks without and with the anomaly score constraint using different PrLMs\nas victim models on MR.\nNo Augmentation Augmentation\nAugmentation w/o Selection w Selection\nBERT 86.4 87.1 88.3\nRoBERTa 88.3 89.1 89.5\nELECTRA 90.1 90.2 90.4\nTable 5: The accuracy of no augmentation, after the\ndata augmentation without and with the selection of de-\ntector on MR.\nset and apply them to X. We send the k trans-\nformed sequences transi(X),i ∈{1,...,k }to the\nPrLM classiﬁer to get their prediction probabilities\nfc(transi(X)),i ∈{1,...,k }, and the ﬁnal predic-\ntion probability of the defense framework is the\nexpectation over the k transformed probabilities\nfdef (X) =Ei∈{1,...,k}(fc(transi(X))).\nSince the detector is not perfect, there always\nexist a small number of compliant samples that\nare misclassiﬁed into adversarial samples. In order\nto minimize the harm to the accuracy of PrLMs\non compliant samples, during the training stage of\nPrLMs, we augment the training data with their\ntransformed data. In this way, the PrLMs are more\nstable to transformations on compliant samples,\nand data augmentation itself also brings gains to\nthe accuracy of PrLMs.\n4 Experimental Implementation\n4.1 PrLMs\nWe investigate three PrLMs: BERT BASE (Devlin\net al., 2018), RoBERTaBASE (Liu et al., 2019a) and\nELECTRABASE (Clark et al., 2020). The PrLMs\nare all implemented in their base-uncased version\nbased on PyTorch 2: they each have 12 layers, 768\nhidden units, 12 heads and around 100M parame-\nters. For most experiments on attacks and defenses,\nwe use BERTBASE as the victim model for an easy\ncomparison between our results and those of previ-\nous works.\n2https://github.com/huggingface\n4.2 Adversarial Attacks\nWe investigate four adversarial attacks from charac-\nter level, word level to sentence level. Examples of\nadversarial samples generated by these four attacks\nare demonstrated in Figure 3.\nCharacter-level attack For character-level at-\ntack, we consider Deepwordbug, which applies\nfour types of character-level modiﬁcations (substi-\ntution, insertion, deletion and swap) to words in the\noriginal sample. Edit distance is used to constrain\nthe similarity between original and adversarial sen-\ntences.\nWord-level attack We select two classic word-\nlevel attack methods: TextFooler (Jin et al., 2020)\nand BERT-Attack (Li et al., 2020). They both\nsort the words in the original sample by impor-\ntance scores, and then substitute the words in or-\nder with their synonyms until the PrLM is fooled.\nTextFooler selects the substitution word from a syn-\nonym set of the original word according to Word-\nNet (Mrkši´c et al., 2016). BERT-Attack masks\nthe original word and uses the masked language\nmodel (MLM) to predict the substitution word. Se-\nmantic similarity and perturbation rate are used to\nconstrain the perturbation size.\nSentence-level attack We select SCPN 3 (Iyyer\net al., 2018) to generate sentence-level adversar-\nial samples. SCPN applies syntactic transforma-\ntions to original sentences and automatically labels\nthe sentences with their syntactic transformations.\nBased on these labeled data, SCPN trains a neural\nencoder-decoder model to generate syntactically\ncontrolled paraphrased adversarial samples. Se-\nmantic similarity is used to ensure that the semantic\nmeaning remains unchanged.\n4.3 Datasets\nExperiments are conducted on four datasets: SST2\n(Socher et al., 2013), MR (Pang and Lee, 2005),\nIMDB (Maas et al., 2011), MNLI (Nangia et al.,\n3https://github.com/thunlp/OpenAttack\n910\nMR SST2 IMDB MNLI\nw/o Def. w Def. w/o Def. w Def. w/o Def. w Def. w/o Def. w Def.\nDeepWordBug 16.3 57.5 19.7 62.3 24.3 81.4 18.7 70.3\nTextFooler 16.7 66.8 36.2 73.3 12.4 90.3 11.3 69.2\nBERT-Attack 13.3 61.5 12.8 65.2 11.8 85.9 9.5 65.4\nSCPN 64.2 74.3 70.8 81.5 - - 66.9 75.0\nTable 6: The adversarial accuracy with and without defense using BERT as victim model.\nMR SST2 IMDB MNLI\nw/o Def. 86.4 92.6 92.4 84.0\nw Def. 87.0 92.6 92.5 84.0\nTable 7: The original accuracy with and without de-\nfense using BERT as victim model.\n2017), covering two major NLP tasks: text classiﬁ-\ncation and natural language inference (NLI). The\ndataset statistics are displayed in Table 1.\nFor text classiﬁcation task, we use three datasets\nwith average text lengths from 20 to 215 words\nin English: (1) SST2 (Socher et al., 2013): a\nphrase-level binary sentiment classiﬁcation dataset\non movie reviews; (2) MR (Pang and Lee, 2005):\na sentence-level binary sentiment classiﬁcation\ndataset on movie reviews; (3) IMDB (Maas et al.,\n2011) : a document-level binary sentiment clas-\nsiﬁcation dataset on movie reviews. For the NLI\ntask, we use MNLI (Nangia et al., 2017), a widely\nadopted NLI benchmark with coverage of the tran-\nscribed speech, popular ﬁction, and government\nreports. When attacking the NLI task, we keep the\noriginal premises unchanged and generate adver-\nsarial hypotheses.\n4.4 Experimental Setup\nThe hyperparameter kin the defense framework is\n3. For the victim PrLMs under attack, we ﬁne-tune\nPrLMs on the training set of each dataset. For the\nanomaly detector, we use BERTBASE as the base\nPrLM and ﬁne-tune it on the training data indicated\nin Section 3.1. For the data augmentation, we ﬁne-\ntune PrLMs on the augmented training set of each\ndataset. During the ﬁne-tuning of all these PrLMs,\nwe use AdamW (Loshchilov and Hutter, 2018) as\nour optimizer with a learning rate of 3e-5 and a\nbatch size of 16. The number of training epochs is\nset to 5. To avoid randomness, we report the results\nof applications in data augmentation and defense\nframework based on the average of 3 runs.\n5 Experimental Results\n5.1 Anomaly Detector\nWe consider three metrics to evaluate the perfor-\nmance of the anomaly detector:F1 score(F1); True\nPositive Rate (TPR): the percentage of adversarial\nsamples that are correctly identiﬁed; False Positive\nRate (FPR): the percentage of compliant samples\nthat are misidentiﬁed as adversarial. The experi-\nmental results are shown in Table 2. The results of\nSCPN on the IMDB dataset are unavailable since\nSCPN cannot tackle document-level texts. Empiri-\ncal results demonstrate that the anomaly detector\ncan achieve an average F1 score over 90%, an aver-\nage TPR over 88%, and an average FPR less than\n10% for adversarial attacks from character-level,\nword-level to sentence-level.\n5.2 Evaluation of Robustness under Anomaly\nScore Constraint\nWe now conduct different types of attacks under\nthe constraint that the anomaly score of generated\nadversarial samples should be less than 0.5. Ta-\nble 3 compares the attack success rate of different\nattacks with and without the anomaly score con-\nstraint when the victim PrLM is BERT. We can\nobserve a sharp decrease in attack success rate with\nthe new constraint for all levels of attacks. This re-\nsult is surprising in that the attackers examined are\ndynamic. Despite their iterative attempts to attack\nthe model, the attackers fail to generate a natural\nadversarial sample that can bypass the anomaly\ndetector.\nTo ensure that this phenomenon holds for other\nPrLMs, we conduct experiments on RoBERTa and\nELECTRA. As shown in Table 4, the attack suc-\ncess rates also drop markedly under the constraint\nof anomaly score for these PrLMs. These empir-\nical results demonstrate that PrLMs are more ro-\nbust than previous attack methods have claimed,\ngiven that most of the adversarial samples gener-\nated by previous attacks are non-natural and de-\ntectable. However, there still exist a little portion\n911\nMR SST2 IMDB MNLI\nOrig% Adv% Orig% Adv% Orig% Adv% Orig% Adv%\nNo Defense 86.4 16.7 92.6 36.2 92.4 12.4 84.0 11.3\nAdv Training 85.4 35.2 92.1 48.5 92.2 34.3 82.3 33.5\nDISP 82.0 42.1 91.1 69.8 91.7 81.9 76.3 35.2\nSAFER 79.0 55.3 90.8 75.1 91.3 88.1 82.1 54.7\nOurs 87.0 66.8 92.6 73.3 92.5 90.3 84.0 69.2\nTable 8: The performance of our defense framework compared with other word-level defenses using BERT as\nPrLM and TextFooler as attack. Orig% is the original accuracy and Adv% is the adversarial accuracy.\nof undetectable adversarial samples that can suc-\ncessfully mislead PrLMs.\n5.3 Application in Data Augmentation\nWe consider the random synonym substitution that\nsubstitutes 30% words with their synonyms se-\nlected in 50 most similar words. Table 5 com-\npares the accuracy after data augmentation without\nand with the selection of anomaly detector. We\ncan observe a further increase in accuracy with the\nselection of the anomaly detector. However, the\nstronger the PrLM is, the smaller the increase is.\n5.4 Application in Enhancing Robustness of\nPrLMs\nWe evaluate the performance of the defense frame-\nwork based on original accuracy and adversarial\naccuracy. The original accuracy is the prediction\naccuracy of the defense framework on original com-\npliant samples. The adversarial accuracy is the ac-\ncuracy of the defense framework after the attack.\nHere we consider the situation that the attack algo-\nrithm can iteratively generate adversarial samples\nagainst our defense framework until it succeeds or\nexceeds the upper limit of attempts.\nTable 6 shows the adversarial accuracy with\nand without the defense using BERT as the vic-\ntim PrLM. We can see a large improvement in\nthe adversarial accuracy with the defense for all\nlevels of attacks. Table 7 shows the original accu-\nracy with and without the defense. We ﬁnd that\nthe original accuracy gets even higher with the de-\nfense. This is because with anomaly detection, the\ntransformations are only applied to detected ad-\nversarial examples. For the very few compliant\nsentences that are detected by mistake as anomaly\nand then applied transformations, the data augmen-\ntation in the training stage has trained the PrLMs\nto be stable to transformations on compliant sam-\nples. Besides, the data augmentation alone brings\nan increase to the original accuracy. Therefore the\nproposed framework does not harm and even in-\ncreases the prediction accuracy for non-adversarial\nsamples, which is important in real application sce-\nnarios.\nSince word-level attacks are the most inﬂuential\nand widely-used type of attack, we compare the\nperformance of our defense framework with sev-\neral state-of-the-art word-level defenses (adversar-\nial training, DISP, SAFER) while facing TextFooler\nas the attack model. DISP (Zhou et al., 2019) de-\ntects and restores adversarial examples by leverag-\ning a perturbation discriminator and an embedding\nestimator. SAFER (Ye et al., 2020) smooths the\nclassiﬁer by averaging the outputs of a set of ran-\ndomized examples. As shown in Table 8, although\nDISP and SAFER are especially designed for word-\nlevel attacks, our defense framework outperforms\nthem in most cases on both original accuracy and\nadversarial accuracy.\n6 Discussion\nThere are two trade-offs for the defense framework:\n(1) The trade-off between original accuracy and\nadversarial accuracy. If we abandon the anomaly\ndetector and apply random transformations to all\ninput sequences, then the adversarial accuracy can\nfurther increase by 5-7%, but the original accuracy\nwill decrease by 1-3%. Since in real applications it\nis not reasonable to sacriﬁce too much precision for\npossible security problems, we adopt the anomaly\ndetector to preserve the original accuracy. However,\nby developing a stronger detector with a higher\nTPR, the defense framework has the potential to\nachieve higher adversarial accuracy.\n(2) The trade-off between training efﬁciency and\noriginal accuracy. To preserve the original accu-\nracy, we apply data augmentation in the training\nstage of the defense framework to make it more\nstable to transformations on compliant samples.\nHowever, the training cost is now multiplied by\nthe size of the transformation set n(n= 6in the\nexperimental realization). If we abandon the data\naugmentation in training stage, the training efﬁ-\n912\nciency of the defense framework is the same as\nthe vanilla ﬁne-tuning of PrLM, but the original\naccuracy will decrease by 0.5-1.5%.\nA limitation of our work is that the attacks we\nexamined are black-box or grey-box attacks, but\ndo not include white-box (gradient-based) attacks.\nHowever, since more than 75% of the existing tex-\ntual attacks are not based on gradient 4, the de-\nfense framework is effective for the majority of\nattacks. We will investigate white-box attacks in\nfuture works.\n7 Conclusion\nIn this study, we question the validity of the cur-\nrent evaluation of robustness of PrLMs based on\nnon-natural adversarial samples, and propose an\nanomaly detector to evaluate the robustness of\nPrLMs with more natural adversarial samples. To\nincrease the precision of PrLMs, we employ the\nanomaly detector to select the augmented data that\nare distinguished as anomaly to introduce more di-\nversity in the training stage. The data augmentation\nafter selection brings larger gains to the accuracy of\nPrLMs. To enhance the robustness of PrLMs, we\nintegrate the anomaly detector to a defense frame-\nwork using expectation over randomly selected\ntransformations. This defense framework can be\nused to defend all levels of attacks, while achieving\nhigher accuracy on both adversarial samples and\ncompliant samples than other defense frameworks\ntargeting speciﬁc levels of attack.\nReferences\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nNilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen,\nFred Hohman, Li Chen, Michael E. Kounavis, and\nDuen Horng Chau. 2017. Keeping the bad guys out:\nProtecting and vaccinating deep learning with jpeg\ncompression.\n4https://github.com/textﬂint/textﬂint\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yan-\njun Qi. 2018. Black-box generation of adversarial\ntext sequences to evade deep learning classiﬁers. In\n2018 IEEE Security and Privacy Workshops (SPW),\npages 50–56.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBae: Bert-based adversarial examples for text clas-\nsiﬁcation.\nIan J. Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2015. Explaining and harnessing adversar-\nial examples.\nPo-Sen Huang, Robert Stanforth, Johannes Welbl,\nChris Dyer, Dani Yogatama, Sven Gowal, Krish-\nnamurthy Dvijotham, and Pushmeet Kohli. 2019.\nAchieving veriﬁed robustness to symbol substitu-\ntions via interval bound propagation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4083–4093, Hong\nKong, China. Association for Computational Lin-\nguistics.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and\nPercy Liang. 2019. Certiﬁed robustness to adversar-\nial word substitutions.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classiﬁcation\nand entailment. Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, 34(05):8018–8025.\nThai Le, Noseong Park, and Dongwon Lee. 2021. A\nsweet rabbit hole by DARCY: Using honeypots to\ndetect universal trigger’s adversarial attacks. InPro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 3831–\n3844, Online. Association for Computational Lin-\nguistics.\nThai Le, Suhang Wang, and Dongwon Lee. 2020. Mal-\ncom: Generating malicious comments to attack neu-\nral fake news detection models.\nDianqi Li, Yizhe Zhang, Hao Peng, Liqun Chen, Chris\nBrockett, Ming-Ting Sun, and Bill Dolan. 2021.\nContextualized perturbation for textual adversarial\nattack.\n913\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting\nWang. 2019. Textbugger: Generating adversarial\ntext against real-world applications. Proceedings\n2019 Network and Distributed System Security Sym-\nposium.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: Adversar-\nial attack against BERT using BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6193–6202.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi\nWang, and Wujie Wen. 2019b. Feature distillation:\nDnn-oriented jpeg compression against adversarial\nexamples.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nRishabh Maheshwary, Saket Maheshwary, and Vikram\nPudi. 2020. Generating natural language attacks in\na hard label black box setting.\nValentin Malykh. 2019. Robust to noise models in nat-\nural language processing tasks. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 10–16, Florence, Italy. Association for Com-\nputational Linguistics.\nZhao Meng and Roger Wattenhofer. 2020. A geometry-\ninspired attack for generating natural language ad-\nversarial examples. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 6679–6689, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nNikola Mrkši´c, Diarmuid Ó Séaghdha, Blaise Thom-\nson, Milica Gaši ´c, Lina M. Rojas-Barahona, Pei-\nHao Su, David Vandyke, Tsung-Hsien Wen, and\nSteve Young. 2016. Counter-ﬁtting word vectors to\nlinguistic constraints. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–148, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nD. Naber. 2003. A Rule-Based Style and Grammar\nChecker. GRIN Verlag.\nNikita Nangia, Adina Williams, Angeliki Lazaridou,\nand Samuel R Bowman. 2017. The repeval 2017\nshared task: Multi-genre natural language inference\nwith sentence representations. In RepEval.\nBo Pang and Lillian Lee. 2005. Seeing stars: Ex-\nploiting class relationships for sentiment categoriza-\ntion with respect to rating scales. In Proceed-\nings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 115–\n124, Ann Arbor, Michigan. Association for Compu-\ntational Linguistics.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lip-\nton. 2019. Combating adversarial misspellings with\nrobust word recognition. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5582–5591, Florence, Italy.\nAssociation for Computational Linguistics.\nEdward Raff, Jared Sylvester, Steven Forsyth, and\nMark McLean. 2019. Barrage of random transforms\nfor adversarially robust defense. In 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 6521–6530.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Lin-\nguistics.\nEric Wallace, Mitchell Stern, and Dawn Song. 2020.\nImitation attacks and defenses for black-box ma-\nchine translation systems. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5531–5546,\nOnline. Association for Computational Linguistics.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, et al. 2021.\nTextﬂint: Uniﬁed multilingual robustness evaluation\ntoolkit for natural language processing. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing: System Demonstrations , pages 347–355,\nOnline. Association for Computational Linguistics.\n914\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\nmentation techniques for boosting performance on\ntext classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nMao Ye, Chengyue Gong, and Qiang Liu. 2020.\nSAFER: A structure-free approach for certiﬁed ro-\nbustness to adversarial word substitutions. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3465–\n3475, Online. Association for Computational Lin-\nguistics.\nWei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi,\nand Chenliang Li. 2020. Adversarial attacks on\ndeep-learning models in natural language process-\ning: A survey. ACM Trans. Intell. Syst. Technol. ,\n11(3).\nZhengli Zhao, Dheeru Dua, and Sameer Singh. 2018.\nGenerating natural adversarial examples. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nYichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei\nWang. 2019. Learning to discriminate perturbations\nfor blocking adversarial attacks in text classiﬁcation.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 . Association for\nComputational Linguistics.\n915",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.9331410527229309
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.891202986240387
    },
    {
      "name": "Detector",
      "score": 0.7177226543426514
    },
    {
      "name": "Computer science",
      "score": 0.7071122527122498
    },
    {
      "name": "Natural language",
      "score": 0.5958542823791504
    },
    {
      "name": "Anomaly detection",
      "score": 0.5527049899101257
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5476545095443726
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4363323450088501
    },
    {
      "name": "Machine learning",
      "score": 0.3868681788444519
    },
    {
      "name": "Data mining",
      "score": 0.3844150900840759
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32411298155784607
    },
    {
      "name": "Geography",
      "score": 0.06151267886161804
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "cited_by": 7
}