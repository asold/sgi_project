{
  "title": "Language Modelling for Source Code with Transformer-XL",
  "url": "https://openalex.org/W3046762077",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Dowdell, Thomas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A325290382",
      "name": "Zhang, Hongyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2921792613",
    "https://openalex.org/W2565684518",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2903594448",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2605202003",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2308618763",
    "https://openalex.org/W2955426500",
    "https://openalex.org/W2140609933",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2148190602",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2402619042",
    "https://openalex.org/W2165747537",
    "https://openalex.org/W2201171388",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2497764072",
    "https://openalex.org/W2967096374",
    "https://openalex.org/W2557805692",
    "https://openalex.org/W2018389835",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2795753518"
  ],
  "abstract": "It has been found that software, like natural language texts, exhibits \"naturalness\", which can be captured by statistical language models. In recent years, neural language models have been proposed to represent the naturalness of software through deep learning. In this paper, we conduct an experimental evaluation of state-of-the-art neural language models for source code, including RNN-based models and Transformer-XL based models. Through experiments on a large-scale Python code corpus, we find that the Transformer-XL model outperforms RNN-based models (including LSTM and GRU models) in capturing the naturalness of software, with far less computational cost.",
  "full_text": "Language Modelling for Source Code with Transformer-XL\nThomas Dowdell\nUniversity of Newcastle\nCallaghan, NSW, Australia\ntomjamesdowdell@gmail.com\nHongyu Zhang\nUniversity of Newcastle\nCallaghan, NSW, Australia\nhongyu.zhang@newcastle.edu.au\nABSTRACT\nIt has been found that software, like natural language texts, exhibits\n\"naturalness\", which can be captured by statistical language models.\nIn recent years, neural language models have been proposed to\nrepresent the naturalness of software through deep learning. In\nthis paper, we conduct an experimental evaluation of state-of-the-\nart neural language models for source code, including RNN-based\nmodels and Transformer-XL based models. Through experiments\non a large-scale Python code corpus, we find that the Transformer-\nXL model outperforms RNN-based models (including LSTM and\nGRU models) in capturing the naturalness of software, with far less\ncomputational cost.\nKEYWORDS\nLanguage Modeling, software naturalness, Transformer-XL\n1 INTRODUCTION\nOver years of software development, a large amount of source code\nhas been accumulated online. Through large-scale mining, it has\nbeen found that source code contains many repetitive, statistical\nregularities, which are also termed as \"naturalness\" of source code\n[17][4][5][6][32]. The naturalness hypothesis has been confirmed\nby empirical evidence [8][10][15][19][27]. Researchers have also\napplied natural language processing techniques, such as Language\nModeling, to model the naturalness of source code [17][19] [30] [31].\nThe constructed language models can be used in many practical\nprogramming tasks such as code completion [23][17], syntax error\nfixing [1][7][24], and API recommendation [14].\nLanguage Models (LMs) are probability distributions over strings.\nGiven a sequence of tokens xT , language modelling is defined\nas predicting the token xt given the previous tokens from x1 to\nxt−1, i.e. maxθ P(xt |xt−1, xt−2, ..., xt−1, θ), where θ are the train-\nable model variables. Multiple language models for source code\nhave already been introduced [15] [22] [28]. A classical language\nmodeling method is the N-gram model. Recent research uses RNN-\nbased models (including LSTM and GRU models) [14][15][23][24]\nand find that RNN-based models superseded N-gram models.\nTransformer models [29] have been recently introduced, and\nhave outperformed RNNs for natural language processing tasks,\nbecause of their ability to track long-range dependencies and their\nparallelizable nature. Many Transformer variants have been in-\ntroduced, notably the Transformer-XL[ 11], which is capable of\ntracking remarkably long-range dependencies.\nIn this paper, we investigate the effectiveness of the Transformer-\nXL based language model for source code, where the model is\ntrained to predict the next token, given a sequence of source code\ntokens. Our experimental results on a Python dataset show that\nthe Transformer-XL models largely outperform the state-of-the-art\nRNN-based models. This strongly suggests that the long-range anal-\nysis of the Transformer-XL is helpful for modelling software nat-\nuralness. The Transformer-XL model also contained significantly\nless time to train in comparison to both the LSTM and GRU models.\n2 TRANSFORMER-BASED LANGUAGE\nMODELS FOR SOURCE CODE\n2.1 The Transformer Model\nAlthough LSTM [18] and GRU [9] models are considered the pinna-\ncle of RNN-based models, both have noted problems with long-term\ndependencies. The Transformer model [29] has been introduced to\novercome the limitations of RNN-based models. Transformer mod-\nels do not calculate the results sequentially, but instead calculate\nthe results in a parallel manner using a self-attention mechanism,\nalso allowing a Transformer-based model to be trained in a shorter\namount of time. Transformers can achieve better results with less\ncomputational cost than the RNN-based models on a variety of\nnatural language tasks [13].\nFigure 1: The Transformer-XL model, as specified in [31].\nThe key to the TransformerâĂŹs success is an attention block,\ndefined as the self-attention mechanism, and a feedforward (FFD)\nblock, both of which are calculated in parallel. The attention block\narXiv:2007.15813v1  [cs.LG]  31 Jul 2020\nallows a model to view all data across an entire sequence, unim-\npeded, in a computationally efficient manner, while the feedfor-\nward block analyzes the data in a sequence-independent manner.\nTransformer-based neural networks take less time to train than the\nRNNs, require less memory and achieve a lower per-token loss [29].\nA single layer of the Transformer can be defined, mathematically,\nfor the lth layer, as:\nx0 = inputs\nattl = Sel f −Attention _Mechanism (xl−1)\nxl = AddNorm (attl , xl−1)\nf f dl = FFD (xl )\nxl = AddNorm (f f dl , xl )\nThe input sequence of tokens is defined as inputs. The functions\nAddNorm, FFD and Self-Attention_Mechanism are defined as:\nAddNorm (x, y)= layernorm (x)+ y\nFFD (xl )= Wl, 2 ∗act(Wl, 1 ∗xl + bl, 1)+ bl, 2\nSel f−Attention _Mechanism (xl )= WO ∗sof tmax(Q ∗KT /\n√\ndk )∗V\nQ, K, V = WQ ∗xl , WK ∗xl , WV ∗xl\nThe variables Wl,1, W l,2, W Q, W K, W V are all trainable weight\nmatrices, the variables bl,1 and b l,2 are trainable bias vectors and\nfunction act is an user-defined non-linear activation function. In\nall models used in this paper, the GELU activation function [16] is\nused.\nThe softmax function is applied over the output to yield a cate-\ngorical probability distribution over each token. At the time-step t,\nthe Transformer calculates the input token for time-step t + 1 .\n2.2 The Transformer-XL Model\nThe Transformer-XL [11] [31] analyzes data in an identical man-\nner to a vanilla transformer, except that, for each sequence, the\nresults calculated for each âĂŸlayerâĂŹ of the transformer are\nsaved and re-inputted, in a gradient-free manner, for the next se-\nquence. This allows the Transformer-XL to âĂŸseeâĂŹ across previ-\nous sequences, allowing for a greater amount of input information\nand therefore greater expressiveness and accuracy. This, conceptu-\nally, makes sense, given that a model with greater memory would\nbe capable of analysing this larger memory to generate a better\nunderstanding of the underlying source code.\nFormally, Transformer-XL redefines the equation that generates\nthe values Q, K, V of the attention mechanism as follows:\nQ = WQ x\nK, V = WK,V [SG(xt−1)∗x]\nwhere [ * ] refers to concatenation, SG refers to the stop-gradient\nfunction and xt-1 refers to the input to the attention mechanism\nfrom the previous sequence.\nFigure 1 shows the structure of Transformer-XL model, as\nspecified in [31]. With the exception of the memory-specific self-\nattention mechanism, the Transformer-XL layer is identical to the\nTransformer layer. The self-attention is normalized using layer nor-\nmalization and a residual connection, followed by an FFD network,\nas specified in Section 2.1 above.\n3 EXPERIMENTS\n3.1 Data Collection and Preprocessing\nTo perform the evaluation, we create a dataset composed of Python\nsource code collected from Github. The dataset contains over 17,000\nPython files, which total over 3 million individual lines of code. We\nuse two different forms of tokenization; character-level tokeniza-\ntion and subword-level tokenization. Character-level tokenization\ncontains 141M tokens, while subword tokenization contains 88M\ntokens.\nWe did not use word-level tokenization for this task. This is\nbecause, unlike natural language, source code does not have an\nexplicit vocabulary. Therefore, there is no way to accurately and\neffectively model all possible words inside a source code file. How-\never, the source code can be effectively modelled using characters\nand subwords[25]. For example, the source code print(x + 3 if x ==\n0) can be tokenized into a sequence of subwords, [print, (, x, +, 3, if,\nx, ==, 0, ) ].\nWe explicitly set, for subword tokenization, the vocabulary size\nhyperparameter to 1000. We chose this vocabulary size because, as\nprevious authors have noted [21], a large vocabulary is responsible\nfor one of the most computationally expensive aspects of the model.\nBy setting the vocabulary size to 1000, the model can tokenize the\nmost common words as a single token, but break down less common\nwords into a series of subwords. This allows the model to express\nany input information without unacceptable computational com-\nplexity and information loss. Therefore, subword-level tokenization\ndoes not suffer from the out-of-vocabulary (OOV) problem.\nGiven this dataset, the model is trained to functionally predict\nthe next token in the line, without being able to âĂŸpeek’ forward\nthrough the line. We specify that the model is trained on a sequence\nthat includes 256 tokens, and the memory from the previous se-\nquence.\nWe split the collected data in the training, validation and testing\ndata using an 80%/10%/10% split. We were careful to avoid as much\ncode duplication as possible, which has been previously shown\nto be a common problem for many source code-based machine\nlearning models [3]. In order to do this, we tested that each file\nin the training dataset did not reappear in either the validation or\ntesting dataset. Further, for each validation and test file, we tested\nto make sure that the number of lines in each file was not repeated\nin the training dataset above a certain threshold. If the file was over\nthe threshold, then it was removed from the dataset. We chose 25%\nas the threshold.\n3.2 Model Training\nEach model is trained for 50 epochs, where each epoch contains 512\niterations. The learning rate is set to a linear-warmup from 1e-6 to\n5e-4 for the first 5120 iterations, and a cosine-decay rate back to\n1e-6 for the following iterations. The optimizer used is the Adam\nOptimizer [20]. We clip the gradient norm to 0.1, which we found\nwas essential for training. We noted that if the gradients were not\nclipped, then all models would produce inferior results.\n2\nBoth RNN models and the Transformer-XL model have a hidden\nsize of 512 units, and have a dropout rate set to 0.1 for training.\nInitially, we trained each model with a depth of 4 layers. RNN-based\nmodels do not typically include many more layers due to the RNN’s\nmassive computational cost. However, Transformer-based models\ntypically have many more layers, anywhere from 12 [29] to 72 [26]\nlayers.\nThe depth of a model is considered an essential aspect of a\nmodel’s ability to learn, but we noted that increasing the number of\nRNN layers significantly slows down training, but this was not the\ncase for the Transformer-XL model. Considering previous papers[2]\nhave shown that Transformer-based models achieve noticeably\nsuperior results by increasing the depth for language modelling, we\ndecided to further experiment by increasing the number of layers\nto 8.\nThe experiments were primarily conducted on a single server\nand a single K40 NVIDIA Tesla GPU, alongside 128GB of RAM.\n3.3 Evaluation Metrics\nThe evaluation metric used for character-level experiments in this\npaper is the BPC (bytes-per-character) metric [2], where the BPC is\ncalculated per-token. This metric is chosen because BPC is a func-\ntion of test-entropy, which effectively displays how âĂŸconfident’ a\nmodel is about guessing the correct answer. The lower the BPC the\nmore confident the model is when guessing the correct answer and\nthe more effective the overall model. The cross-entropy equation\ncan be defined as, given, at time-step t, the predicted output yt and\nthe target output zt:\nlosst = −1 ∗zt ∗loд(yt )\nThe BPC is defined as:\nbpc = loss\nloд(2)\nFor subword-level experiments, the metric is the perplexity\nrather than the BPC. Perplexity is defined as:\nperplexit y = eloss\nLow perplexity is desirable since the perplexity is the exponen-\ntial of the entropy. Multiple previous papers have stated that, for\ncharacter-level analysis, BPC is the metric of choice but perplexity\nis the metric to be used for both word-level and subword-level\nanalysis [2][13][31].\n3.4 Experimental Results\nThe results from all experiments are presented in Table 1. We tested\neach model 3 times and presented the average results of each model.\nEach model was initialized randomly, and the random-seed is dif-\nferent for each experiment. We noted that each model, when tested\nwith the same data, performed similarly across different runs and\nseeds. The validation data loss, for both the character-level and\nBPC-level analysis, can be seen in Figure 2 and Figure 3.\nThe LSTM model, which has been used as the previous neu-\nral source code modelling work [ 12], achieved the highest per-\ncharacter BPC (1.8706) and per-subword Perplexity (6.4552). The\nGRU model achieved reliably better results than the LSTM (BPC\nModel Character-Level Subword-Level\nTest BPC Test Perplexity\nLSTM (4 Layer) 1.8706 6.4552\n(0.0053) (0.0774)\nGRU (4 Layer) 1.2758 6.1135\n(0.002) (0.0146)\nTransformer-XL 1.1506 2.7278\n(4 Layer) (0.0018) (0.0005)\nTransformer-XL 1.1297 2.7185\n(8 Layer) (0.0063) (0.0208)\nTable 1: The test BPC and Perplexity of each of the three\nmodels, where better models output lower values. The num-\nbers in brackets are standard deviation across the multiple\nruns. Across both tokenization schemes, the Transformer-\nXL model outperforms both RNN models.\nFigure 2: The BPC for the LSTM, GRU and Transformer-\nXL model when analysing character-level source code. The\nTransformer-XL, across the entire run, outperforms the\nRNN models. We chose to display the best-performing\nmodel for the LSTM, GRU and Transformer-XL.\n1.2758 and Perplexity 6.1135), but only by a small margin. The\nTransformer-XL model, on the other hand, could reliably outper-\nform both the LSTM and GRU models by a substantial margin, for\nboth the 4-layer and 8-layer models. The 8-layer Transformer-XL\nmodel, which performed better then all others, achieved a per-\ncharacter BPC of 1.1297 and per-subword perplexity of 2.7185,\nwhich is lower than all other models. It is worth noting that the\n4-layer Transformer-XL outperformed both RNN models as well.\nThe superior results of the 4-layer Transformer-XL model, in\ncomparison to the RNN models, is likely a sign that the Transformer-\nXL model can extract more rich and useful features than both RNN\nmodels.\nWe also found that an 8-layer Transformer-XL model outper-\nformed a 4-layer Transformer-XL model, but not by the same mar-\ngin that a Transformer-XL outperforms the RNN. This 8-layer\nTransformer-XL, despite having twice the depth of the RNN mod-\nels, was trained in less than half of the time required by the RNN\nmodels, even when trained on identical hardware.\n3\nFigure 3: The Perplexity for the LSTM, GRU and\nTransformer-XL model when analysing subword-level\nsource code. The Transformer-XL outperforms the RNN\nmodels.\nWe note that the training time for each model depends on the\nhardware that the model is trained on. For example, an RNN model\nthat is trained on a quicker GPU will be trained faster than a more\nefficient model on a more slow GPU. In order to reflect this, we\nrecorded the normalized training time, where each model was\ntrained on the same GPU and the time to train was normalized, to\nbetter reflect the length of training for each model in comparison\nto the other models. The normalized training time for each model\ncan be seen in Table 2, from which we can see that Transformer-XL\nmodels require much less time to train than the two RNN models\n(LSTM and GRU).\nIn summary, our experiments show that the Transformer-XL\nmodel outperformed both RNN models when the layers are all set\nto 4, and the Transformer-XL further improves results when there\nare 8 layers instead of 4. Furthermore, even though the 8-layer\nTransformer-XL model has twice the depth and more trainable\nparameters, it takes less than half of the time to train and achieves\nnotably superior results.\n4 DISCUSSION\nOur experimental results suggest that the Transformer-XL model\nis a substantially superior model to either the LSTM or GRU model\nfor modelling source code. This is because the RNN models cannot\neasily be trained for more layers without incurring massive com-\nputational cost, and therefore the RNN models produce inferior\nresult [29]. The Transformer-XL model, on the other hand, can pro-\nduce noticeably superior results by increasing the layers without an\nunacceptable computational cost. Therefore, the Transformer-XL\ncan achieve substantially superior using results in practice. We\nsuggest using Transformer-XL in future research work on language\nmodelling for source code.\nPrevious work on natural language processing has shown that\nmodels that are trained on language modelling based tasks can\nachieve state-of-the-art results on downstream NLP tasks, such as\nsentiment analysis and question-answering [13]. We would hypoth-\nesize that downstream source code related tasks can be performed\nModel Normalized\nTraining Time\nLSTM (4 Layer) 4.2\nGRU (4 Layer) 3.58\nTransformer-XL 1\n(4 Layer)\nTransformer-XL 1.39\n(8 Layer)\nTable 2: The normalized training time for each model, where\neach unit is the normalized length of time for training a\nmodel over a single iteration. The 4-layer Transformer-XL\nmodel requires the shortest training time, followed by the\n8-layer Transformer-XL model. It is worth noting that the\n8-layer Transformer-XL, despite having twice the depth of\nRNNs, takes less than half of the time to train.\nin a similar way. Pre-training the models to perform language mod-\nelling allows a model to “learn\" the underlying statistical behaviour\n(naturalness) of source code, and this understanding can be lever-\naged to effectively perform downstream tasks.\nFuture work could focus on applying language models to the\nsource code related downstream tasks such as automatic code com-\npletion. These models could also be pushed further. For example,\nthese models could be easily retrained to perform more compli-\ncated tasks such as automatic error detection or automatic bug\nfixing. These tasks cannot be expressed in an identical manner to\nlanguage modelling, in a manner similar to automatic code com-\npletion. However, the software naturalness that the model learns\ncan be leveraged to better perform these downstream tasks. Future\nwork should focus on testing the effectiveness of these downstream\ntasks, and whether a language model can be trained to improve\nthese tasks effectively.\n5 CONCLUSION\nThis paper has shown that Transformer-based language models\nlargely outperform RNN-based models for source code modelling.\nThe Transformer-XL model achieves substantially better results\nwith less than half the complexity and training time.\nBased on this work, we recommend that Transformer-XL can\nbe adopted for language modelling tasks relating to source code.\nFollowing the examples set in natural language processing [ 13],\nthis model can be further applied to other downstream source code\nrelated tasks, such as automatic code completion and automatic\nbug fixing.\nOur tool and experimental data are publicly available at https:\n//github.com/Anon-111/Source-Code-Modelling.\nREFERENCES\n[1] Umair Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit Gul-\nwani. 2018. Compilation of Error Repair: For the Student Programs, from the\nStudent Programs. In 40th International Conference on Software Engineering: Soft-\nware Engineering Education and Training Track (ICSE-SEETâĂŹ18) . ACM, 78–87.\nhttps://dl.acm.org/citation.cfm?doid=3183377.3183383\n[2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018.\nCharacter-Level Language Modeling with Deeper Self-Attention. arXiv preprint\narXiv:1808.04444v2 (2018).\n[3] Miltiadis Allamanis. 2018. The Adverse Effects of Code Duplication in Machine\nLearning Models of Code. arXiv preprint arXiv:1812.06468 (2018).\n4\n[4] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2014. Learn-\ning Natural Coding Conventions. arXiv preprint arXiv:1402.4182 (2014).\n[5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.\nA survey of machine learning for big code and naturalness. ACM Computing\nSurveys (CSUR) 51, 4 (2018), 81.\n[6] Miltiadis Allamanis and Charles Sutton. 2013. Mining Source Code Repositories\nat Massive Scale using Language Modeling. In 2013 10th Working Conference on\nMining Software Repositories (MSR) . IEEE, 207–216.\n[7] Sahil Bhatia and Rishabh Singh. 2016. Automated Correction for Syntax Errors\nin Programming Assignments using Recurrent Neural Networks. arXiv preprint\narXiv:1603.06129 (2016).\n[8] Avishkar Bhoopchand, Tim Rockt aschel, Earl Barr, and Sebastian Riedel. 2016.\nLearning Python Code Suggestion with a Sparse Pointer Network. arXiv preprint\narXiv:1611.08307 (2016).\n[9] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase\nRepresentations using RNN Encoder-Decoder for Statistical Machine Translation.\narXiv preprint arXiv:1406.1078 (2014).\n[10] Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. 2017.\nSynthesizing benchmarks for predictive modeling. In Proceedings of the 2017\nInternational Symposium on Code Generation and Optimization (CGO ’17) . ACM,\n86–99. 3049843\n[11] Zihing Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models Beyond a\nFixed-Length Context. arXiv preprint arXiv:1901.02860 (2019).\n[12] Hoa Khanh Dam, Truyen Tran, and Trang Pham. 2016. A Deep Language Model\nModel for Software Code. arXiv preprint arXiv:1608.02715 (2016).\n[13] Jacob Delvin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\npre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[14] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep\nAPI Learning. In Proceedings of the 2016 24th ACM SIGSOFT International Sympo-\nsium on Foundations of Software Engineering (FSE 2016) . Association for Comput-\ning Machinery, New York, NY, USA, 631âĂŞ642. https://doi.org/10.1145/2950290.\n2950334\n[15] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. 2017. DeepFix:\nFixing Common C Language Errors by Deep Learning. In Proceedings of the\nThirty-First AAAI Conference on Artificial Intelligence (AAAI’17) . AAAI Press,\n1345–1351. http://dl.acm.org/citation.cfm?id=3298239.3298436\n[16] Dan Hendrycks and Kevin Gimpe. 2018. Gaussian Error Linear Units (GELUs).\narXiv preprint arXiv:arXiv:1606.08415v3 (2018).\n[17] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.\n2012. On the Naturalness of Software. In Proceedings of the 34th International\nConference on Software Engineering (ICSE ’12) . IEEE Press, Piscataway, NJ, USA,\n837–847. http://dl.acm.org/citation.cfm?id=2337223.2337322\n[18] Sepp Hochreiter and JÃĳrgen Schmidhuber. 1997. Long Short-Term Memory. In\nNeural Computation . 1735–1780. https://www.mitpressjournals.org/doi/10.1162/\nneco.1997.9.8.1735\n[19] Rafael-Michael Karampatsis and Charles A. Sutton. 2019. Maybe Deep Neural\nNetworks are the Best Choice for Modeling Source Code. CoRR abs/1903.05734\n(2019). arXiv:1903.05734 http://arxiv.org/abs/1903.05734\n[20] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[21] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. arXiv preprint arXiv:1909.11942 (2019).\n[22] Ali Mesbah, Andrew Rice, Emily Johnston, Nick Glorioso, and Edward Aftandilian.\n2019. DeepDelta: learning to repair compilation errors. In Proceedings of the\n2019 27th ACM Joint Meeting on European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering (ESEC/FSE 2019) . ACM,\n925–936. https://dl.acm.org/citation.cfm?id=3340455\n[23] Veselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code Completion with\nStatistical Language Models. SIGPLAN Not. 49, 6 (June 2014), 419âĂŞ428. https:\n//doi.org/10.1145/2666356.2594321\n[24] E. A. Santos, J. C. Campbell, D. Patel, A. Hindle, and J. N. Amaral. 2018. Syn-\ntax and sensibility: Using language models to detect and correct syntax errors.\nIn 2018 IEEE 25th International Conference on Software Analysis, Evolution and\nReengineering (SANER) . 311–322. https://doi.org/10.1109/SANER.2018.8330219\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural Machine\nTranslation of Rare Words with Subword Units. arXiv preprint arXiv:1509.07909\n(2015).\n[26] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jard Casper,\nand Bryan Catanzaro. 2019. Megatrom-LM: Training Multi-Billion Parameter Lan-\nguage Models Using Model Parallelism. arXiv preprint arXiv:arXiv:1909.08053v3\n(2019).\n[27] Giriprasad Sridhara, Emily Hill, Divya Muppaneni, Lori Pollock, and K Vijay-\nShanker. 2010. Towards automatically generating summary comments for java\nmethods. In Proceedings of the IEEE/ACM international conference on Automated\nsoftware engineering. ACM, 43–52.\n[28] Zhaopeng Tu, Zheopeng Su, and Premkumar Devanbu. 2014. On the localness of\nsoftware. In Proceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering . 269–280.\n[29] Ashish Vaswani, Noam Sheezer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\nGomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.arXiv\npreprint arXiv:1706.03762 (2017).\n[30] Martin White, Christopher Vendome, Mario Linares-Vásquez, and Denys Poshy-\nvanyk. 2015. Toward Deep Learning Software Repositories. In Proceedings of the\n12th Working Conference on Mining Software Repositories (MSR ’15) . IEEE Press, Pis-\ncataway, NJ, USA, 334–345. http://dl.acm.org/citation.cfm?id=2820518.2820559\n[31] Zhilin Yang, Zihing Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\nQuoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. arXiv preprint arXiv:1906.08237 (2019).\n[32] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu. 2019. A Novel\nNeural Source Code Representation Based on Abstract Syntax Tree. In 2019\nIEEE/ACM 41st International Conference on Software Engineering (ICSE) . 783–794.\nhttps://doi.org/10.1109/ICSE.2019.00086\n5",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5260681509971619
    },
    {
      "name": "Computer science",
      "score": 0.398938924074173
    },
    {
      "name": "Programming language",
      "score": 0.3826799988746643
    },
    {
      "name": "Engineering",
      "score": 0.23253914713859558
    },
    {
      "name": "Electrical engineering",
      "score": 0.15860122442245483
    },
    {
      "name": "Voltage",
      "score": 0.04644376039505005
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78757542",
      "name": "University of Newcastle Australia",
      "country": "AU"
    }
  ]
}