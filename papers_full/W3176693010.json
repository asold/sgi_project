{
  "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation",
  "url": "https://openalex.org/W3176693010",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2740518062",
      "name": "Ruidan He",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2111528926",
      "name": "Lin-Lin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132756360",
      "name": "Hai Ye",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2242520679",
      "name": "Qingyu Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128421073",
      "name": "Bosheng Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103479347",
      "name": "Liying Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147395767",
      "name": "Jia-Wei Low",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127260490",
      "name": "Luo Si",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287692509",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W2973047874",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3023528699",
    "https://openalex.org/W3094491777",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2964352358",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W2946296745",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2777662428",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3120490999",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2912811302",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2009284521",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W3100311862",
    "https://openalex.org/W2964067969",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962933129",
    "https://openalex.org/W3003289092",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W2995998574",
    "https://openalex.org/W3023911605",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W2948337921",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W3127622310"
  ],
  "abstract": "Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jiawei Low, Lidong Bing, Luo Si. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2208–2222\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2208\nOn the Effectiveness of Adapter-based Tuning for\nPretrained Language Model Adaptation\nRuidan He∗1, Linlin Liu∗12†, Hai Ye∗3, Qingyu Tan13†, Bosheng Ding12†,\nLiying Cheng14†, Jia-Wei Low12†, Lidong Bing1, Luo Si1\n1DAMO Academy, Alibaba Group 2Nanyang Technological University\n3National University of Singapore 4Singapore University of Technology and Design\n{ruidan.he,linlin.liu,qingyu.tan}@alibaba-inc.com\n{bosheng.ding,liying.cheng,jiawei.low}@alibaba-inc.com\nyeh@comp.nus.edu.sg {l.bing,luo.si}@alibaba-inc.com\nAbstract\nAdapter-based tuning has recently arisen as an\nalternative to ﬁne-tuning. It works by adding\nlight-weight adapter modules to a pretrained\nlanguage model (PrLM) and only updating the\nparameters of adapter modules when learning\non a downstream task. As such, it adds only a\nfew trainable parameters per new task, allow-\ning a high degree of parameter sharing. Prior\nstudies have shown that adapter-based tun-\ning often achieves comparable results to ﬁne-\ntuning. However, existing work only focuses\non the parameter-efﬁcient aspect of adapter-\nbased tuning while lacking further investiga-\ntion on its effectiveness. In this paper, we\nstudy the latter. We ﬁrst show that adapter-\nbased tuning better mitigates forgetting issues\nthan ﬁne-tuning since it yields representations\nwith less deviation from those generated by\nthe initial PrLM. We then empirically com-\npare the two tuning methods on several down-\nstream NLP tasks and settings. We demon-\nstrate that 1) adapter-based tuning outperforms\nﬁne-tuning on low-resource and cross-lingual\ntasks; 2) it is more robust to overﬁtting andless\nsensitive to changes in learning rates.\n1 Introduction\nLarge scale pretrained language models (PrLMs)\n(Devlin et al., 2019; Liu et al., 2019; Conneau et al.,\n2020a; Brown et al., 2020) have achieved state-of-\nthe-art results on most natural language processing\n(NLP) tasks, where ﬁne-tuning has become a dom-\ninant approach to utilize PrLMs. A standard ﬁne-\ntuning process copies weights from a PrLM and\ntunes them on a downstream task, which requires a\nnew set of weights for each task.\nAdapter-based tuning (Houlsby et al., 2019;\nBapna and Firat, 2019) has been proposed as a\n∗ ∗Equally Contributed\n††Linlin, Qingyu, Bosheng, Liying, and Jia-wei are under\nthe Joint PhD Program between Alibaba and their correspond-\ning universities.\nmore parameter-efﬁcient alternative. For NLP,\nadapters are usually light-weight modules inserted\nbetween transformer layers (Vaswani et al., 2017).\nDuring model tuning on a downstream task, only\nthe parameters of adapters are updated while the\nweights of the original PrLM are frozen. Hence,\nadapter-based tuning adds only a small amount\nof parameters for each task, allowing a high de-\ngree of parameter-sharing. Though using much\nless trainable parameters, adapter-based tuning has\ndemonstrated comparable performance with full\nPrLM ﬁne-tuning (Houlsby et al., 2019; Bapna and\nFirat, 2019; Stickland and Murray, 2019).\nExisting work mostly focuses on the parameter-\nefﬁcient aspect of adapters and attempt to derive\nuseful applications from that, which is still the\ncase in most recent works: R ¨uckl´e et al. (2020)\nexplore methods to further improve the parame-\nter and computation efﬁciency of adapters; Pfeif-\nfer et al. (2020a) combine knowledge from multi-\nple adapters to improve the performance on down-\nstream tasks; Artetxe et al. (2020) and Pfeiffer\net al. (2020c) leverage the modular architecture\nof adapters for parameter-efﬁcient transfer to new\nlanguages or tasks, and Wang et al. (2020) utilize\nthe same property for knowledge injection.\nBesides parameter-efﬁciency, the unique char-\nacteristic of adapter-based tuning, with alternat-\ning frozen and learnable layers, might be directly\nuseful for improving model performances. How-\never, this has not yet been discussed in the prior\nwork. In this paper, we ﬁrst empirically demon-\nstrate that adapter-based tuning better regularizes\ntraining than ﬁne-tuning by mitigating the issue\nof forgetting. We show that it yields representa-\ntions with less deviation from those generated by\nthe original PrLM. Next, to see what this prop-\nerty of adapters will help when adapting PrLMs,\nwe compare the performance of ﬁne-tuning and\nadapter-based tuning on a wide range of datasets\n2209\nSelf-attention\nFeed-forward\nAdapter\nFeed-forward\nAdapter\n+\n+\nLayer Norm\n+\nTransformer Layer Adapter\nLayer Norm\nFigure 1: The structure of the adapter adopted\nfrom Houlsby et al. (2019). N is the number of trans-\nformer layers.\nand NLP tasks. Extensive experiments and anal-\nysis are conducted in different settings, including\nlow-resource and high-resource, monolingual and\ncross-lingual.\nOur main ﬁndings can be summarized as fol-\nlows:\n• For monolingual adaptation, adapter-based\ntuning yields better results in low-resource\nsettings, especially when the task is more\ndomain-speciﬁc. With increasing training\nsamples, the performance gain over ﬁne-\ntuning is less signiﬁcant (§3).\n• Adapter-based tuning tends to outperform\nﬁne-tuning on zero-shot cross-lingual tasks\nunder different amounts of training data (§4).\n• Adapter-based tuning demonstrates higher sta-\nbility and better generalization ability. It is\nless sensitive to learning rates compared to\nﬁne-tuning (§5).\n2 Adapter Better Regularizes Tuning\n2.1 Adapter-based Tuning\nWhen adapting a pretrained language\nmodel (PrLM), adapter-based tuning inserts\nlight-weight neural networks (adapters) between\nthe transformer layers of the PrLM, and only\nupdates the parameters of the adapters on a down-\nstream task, but keeps the ones of the PrLM frozen.\nUnlike ﬁne-tuning which introduces an entire\nnew model for every task, one great advantage\nof adapter-based tuning is generating a compact\n2 4 6 8 10 12\nBERT Layer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0RSA Similarity\nRepresentation Space Comparison (SST-2)\nFine-tune vs Base\nAdapter vs Base\nFigure 2: Comparison of the representations obtained\nat each layer before ( Base) and after adapter-based\ntuning or ﬁne-tuning on BERT-base using Representa-\ntional Similarity Analysis (RSA). 5000 tokens are ran-\ndomly sampled from the dev set for computing RSA.\nA higher score indicates that the representation spaces\nbefore and after tuning are more similar.\nmodel with only a few trainable parameters added\nper task.\nHoulsby et al. (2019) have extensively studied\nthe choices of adapter architectures and where they\nshould be inserted into PrLMs. They ﬁnd that a\nstack of down- and up-scale neural networks works\nwell which only introduces a small amount of extra\nparameters to the network. This design inspires\nmost of the following work (Pfeiffer et al., 2020a,c;\nBapna and Firat, 2019). As shown in Figure 1,\nthe adapter maps an input hidden vector h from\ndimension dto dimension mwhere m < d, and\nthen re-maps it to dimension d. We refer m as\nthe hidden size of the adapter. A skip-connection\nis employed inside the adapter network such that\nif the parameters of the projection layers are near\nzeros, the adapter module approximates an identity\nfunction. Formally, given the input hidden vector\nh, the output vector h′is calculated as:\nh′= f2(tanh f1(h)) + h (1)\nin which f1(·) and f2(·) are the down- and up-\nprojection layers. At each transformer layer, two\nadapters are inserted right after the self-attention\nand the feed-forward layers respectively. During\nadapter tuning, only the parameters of the adapters,\nthe normalization layers, and the ﬁnal classiﬁca-\ntion layer are updated. We use the above described\nadapter conﬁguration in all of our experiments,\nsince it is adopted in most prior work with few\nmodiﬁcations.\n2210\n2.2 Representation Similarity\nFine-tuning large-scale PrLMs on downstream\ntasks can suffer from overﬁtting and bad gener-\nalization issues (Dodge et al., 2020; Phang et al.,\n2018). Recently, Lee et al. (2020) propose Mixout\nto regularize the ﬁne-tuning of PrLMs. They show\nthat Mixout avoids catastrophic forgetting and sta-\nbilizes the ﬁne-tuning process by encouraging the\nweights of the updated model to stay close to the\ninitial weights. Since adapter-based tuning does\nnot update the weights of PrLMs at all, we suspect\nthat it has a similar effect of alleviating the issue\nof catastrophic forgetting. Since the weights of the\nPrLM are the same before and after adapter-based\ntuning, to verify this, we use Representational Sim-\nilarity Analysis (RSA) (Laakso and Cottrell, 2000)\nto assess the similarity of tuned representations to\nthose without tuning at each transformer layer.\nRSA has been widely used to analyze the simi-\nlarity between two neural network outputs (Abnar\net al., 2019; Chrupała and Alishahi, 2019; Mer-\nchant et al., 2020), which works by creating two\ncomparable sets of representations by inputting a\nsame set of nsamples to the two models. For each\nset of representations, a n×npairwise similarity1\nmatrix is calculated. The ﬁnal RSA similarity score\nbetween the two representation space is computed\nas the Pearson correlation between the ﬂattened up-\nper triangulars of the two similarity matrices. We\nuse a subset of GLUE tasks (Wang et al., 2018)\nfor our analysis. Given a task, we ﬁrst perform\nadapter-based tuning and ﬁne-tuning to adapt a\nBERT-base model (Morg) to the target task, which\nyields models Madapt and Mft respectively (See\nAppendix A.2 for training details). Then we pass\nsentences (or sentence-pairs depend on the task)\nfrom the development set to Morg, Madapt, and\nMft respectively. We extract representations at\neach layer from the three models and select the\ncorresponding representations of 5k randomly sam-\npled tokens 2 (n = 5000) for evaluation. Note\nthat the same set of tokens is used for all mod-\nels. Finally, we compare the representations ob-\ntained from Madapt or Mft to those from Morg\nusing RSA.\nFigure 2 plots the results on STS-2, results of\nother tasks demonstrate a similar trend and can be\nfound in Appendix A.3. For both ﬁne-tuning and\nadapter-based tuning, we observe that the repre-\n1Cosine similarity is used\n2We skip [PAD], [CLS], [SEP] for token selection.\nsentation change generally arises in the top lay-\ners of the network, which is consistent with previ-\nous ﬁndings that higher layers are more task rele-\nvant (Howard and Ruder, 2018). It can be clearly\nobserved that compared to ﬁne-tuning, adapter-\nbased tuning yields representations with less devia-\ntion from those of BERT-base at each layer, which\nveriﬁes our claim that adapter-based tuning can\nbetter regularize the tuning process by mitigating\nthe forgetting problem. Apparently, this property\nof adapter tuning comes from that it freezes all\nthe parameters of PrLMs. And because of the skip-\nconnection in the adapter, the hidden representation\nout of the adapter can mimic the input representa-\ntion, in this way, some of the original knowledge\nof PrLMs (before injecting adapters) can be pre-\nserved.\nSince we ﬁnd that adapter-based tuning better\nregularizes the learning process, the next question\nis how this property will help to improve the per-\nformance when adapting PrLMs to downstream\ntasks. We conduct extensive experiments to investi-\ngate this. The remainder of this paper is organized\nas follows. We compare ﬁne-tuning and adapter-\nbased tuning on monolingual text-level adaptation\ntasks in §3, followed by cross-lingual adaptation in\n§4. Further analysis about the training stability and\ngeneralization capabilities is shown in §5.\n3 Monolingual Adaptation\nIn this section, we ﬁrst experiment with eight\ndatasets as used in Gururangan et al. (2020) in-\ncluding both high- and low-resource tasks ( §3.1).\nWe refer this set of tasks as Task Adaptation Eval-\nuation (TAE). We observe that adapter-based tun-\ning consistently outperforms ﬁne-tuning on low-\nresource tasks, while they perform similarly on\nhigh-resource tasks. We further conﬁrm the effec-\ntiveness of adapters in low-resource settings on the\nGLUE benchmark (Wang et al., 2018) (§3.2).\n3.1 TAE\nTAE consists of four domains (biomedical, com-\nputer science, news text, and AMAZON reviews)\nand eight classiﬁcation tasks (two in each domain),\nwhose domain diversity makes it suitable to as-\nsess the adaptation effectiveness of different ap-\nproaches. Detailed data statistics are displayed in\nAppendix A.1. We consider tasks with fewer than\n5k training examples as low-resource tasks and the\nothers as high-resource tasks.\n2211\nlow-resource high-resource\nModel CHEMPROT ACL-ARC SCIERC HYP. RCT AGNEWS HELPFUL. IMDB\n(4169) (1688) (3219) (515) (180k) (115k) (115k) (20k)\nRoBa.-ft† 81.91.0 63.05.8 77.31.9 86.60.9 87.20.1 93.90.2 65.13.4 95.00.2\nRoBa.-ft∗ 81.70.8 65.03.6 78.51.8 88.93.3 87.00.1 93.70.2 69.10.6 95.20.1\nRoBa.-adapter256 82.90.6 67.54.3 80.80.7 90.44.2 87.10.1 93.80.1 69.00.4 95.70.1\nRoBa.-ft+TAPT† 82.60.4 67.41.8 79.31.5 90.45.2 87.70.4 94.50.1 68.51.9 95.50.1\nRoBa.-ft+TAPT∗ 82.50.3 66.55.1 79.70.8 91.30.8 87.40.1 94.00.2 70.31.1 95.40.1\nRoBa.-adapter256+TAPT 83.50.5 70.02.1 81.10.2 90.03.5 87.20.1 94.00.1 68.80.8 95.80.0\nTable 1: Average results across ﬁve random seeds with standard deviations as subscripts on TAE. micro-F1 is\nreported for CHEMPROOT and RCT, and macro-F1 is reported for the other tasks. Results with “ †” are taken\nfrom Gururangan et al. (2020). Results with “*” are reproduced by us. Numbers in () indicate the training size.\nExperimental Setup We perform supervised\nﬁne-tuning on RoBERTa-base as our baseline\n(RoBa.-ft). For adapter-based tuning, we set\nthe hidden size m of adapters to 256 ( RoBa.-\nadapter256). We also present the results of adding\ntask-adaptive pretraining(+TAPT) (Gururangan\net al., 2020). In this setting, before ﬁne-tuning\nor adapter-based tuning, the model was trained\nwith a masked language modeling (MLM) objec-\ntive on the training texts (without labels) of the\ntask. Note that in RoBa.-adapter256+TAPT, we\nalso use adapter-based tuning for TAPT where only\nthe weights of adapters are updated at the TAPT\nstage. This is to evaluate whether adapter-based\ntuning can work with unsupervised learning ob-\njectives. We follow the experimental settings in\nGururangan et al. (2020) for TAPT. For ﬁne-tuning\nand adapter-based tuning, we train models for 20\nepochs to make sure they are sufﬁciently trained\nand save the checkpoint after each training epoch.\nWe select the checkpoint that achieves the best\nscore on the validation set for evaluation on the test\nset. The batch size is set to 16 for both methods.\nThe learning rate is set to 2e-5 for ﬁne-tuning, and\n1e-4 for adapter-based tuning. See Appendix A.2\nfor the hyperparameter selection process and more\ntraining details.\nResults Table 1 presents the comparison results.\nWe report the average result over 5 runs with dif-\nferent random seeds. On four low-resource tasks,\nadapter-based tuning consistently outperforms ﬁne-\ntuning and improves the average result by 1.9%.\nAdapter-based tuning alone without TAPT even out-\nperforms ﬁne-tuning with TAPT. Besides, adding\nTAPT before adapter-based tuning further improves\nthe performance on 3 out of 4 low-resource tasks,\nwhich suggests that adapter-based tuning works\nwith both supervised and unsupervised objectives.\n2k 4k 8k 16k 32k 64k all\n# of training samples\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87Test Performance\nRCT\nFine-tune\nAdapter\n2k 4k 8k 16k 32k 64k all\n# of training samples\n0.89\n0.90\n0.91\n0.92\n0.93\n0.94\nAGNEWS\nFine-tune\nAdapter\nFigure 3: Test performance w.r.t the number of train-\ning examples. Reported results are averages across ﬁve\nruns with different random seeds.\nAnother ﬁnding is that when trained on high-\nresource tasks, both methods achieve similar re-\nsults. To verify the effects of training size, on\nhigh-resource tasks, we plot the performances with\nvarying numbers of training examples in Figure 3.\nThe trend is consistent with our existing observa-\ntions – adapter-based tuning achieves better results\nwhen the training set is small while ﬁne-tuning will\ngradually catch up with an increasing number of\ntraining examples.\n3.2 GLUE Low-resource Adaptation\nTo further validate that adapters tend to general-\nize better than ﬁne-tuning under low-resource set-\ntings, we follow Zhang et al. (2021) to study low-\nresource adaptation using eight datasets from the\nGLUE benchmark (Wang et al., 2018) which cov-\ners four types of tasks: natural language inference\n(MNLI, QNLI, RTE), paraphrase detection (MRPC,\nQQP), sentiment classiﬁcation (SST-2) and linguis-\ntic acceptability (CoLA). Appendix A.1 provides\ndetailed data statistics and descriptions.\nExperimental Setup For each dataset, we sim-\nulate two low-resource settings by randomly sam-\npling 1k and 5k instances from the original training\n2212\nModel CoLA MNLI m MNLImm MRPC QNLI QQP RTE SST-2 STS-B Avg.\n1k\nBERT-ft 41.4 4.0 57.43.2 60.33.2 83.61.2 80.50.3 69.80.7 62.51.1 87.80.4 85.50.9 69.91.7\nBERT-adapter64 42.92.6 61.60.9 64.10.8 84.80.7 80.50.9 70.32.0 62.51.3 88.00.7 86.10.3 71.21.1\nBERT-adapter64−256 43.62.9 61.60.9 64.10.8 84.80.7 81.00.2 76.80.7 65.32.0 88.00.7 86.30.2 72.41.0\nRoBa.-ft 45.4 2.8 71.20.9 72.90.9 88.40.7 84.00.7 75.01.1 67.02.7 89.00.8 88.50.4 75.71.2\nRoBa.-adapter64 47.72.5 71.00.8 71.90.8 88.90.9 83.20.5 74.70.3 67.72.2 90.01.4 88.40.2 76.01.1\nRoBa.-adapter64−256 47.72.5 71.80.8 73.01.1 89.20.7 83.50.4 75.10.1 68.70.8 90.50.2 88.60.2 76.40.8\n5k\nBERT-ft 54.42.4 69.60.8 71.21.1 - 85.0 0.7 74.71.8 - 88.6 1.0 88.70.7 76.01.2\nBERT-adapter64 54.11.5 71.30.5 73.00.4 - 85.3 0.3 74.21.3 - 89.1 0.2 88.90.1 76.60.6\nBERT-adapter64−256 54.11.5 71.30.5 73.20.4 - 85.30.3 74.90.4 - 89.10.2 88.90.1 76.70.5\nRoBa.-ft 55.7 1.7 79.50.4 80.30.4 - 87.10.5 78.11.3 - 91.4 0.5 90.60.1 80.40.7\nRoBa.-adapter64 56.81.2 80.20.3 80.60.2 - 86.5 0.7 78.21.0 - 92.2 0.5 90.40.2 80.70.6\nRoBa.-adapter64−256 57.41.6 80.20.3 80.50.2 - 86.9 0.6 78.30.9 - 92.20.5 90.40.2 80.80.6\nTable 2: Results on GLUE 1k and 5k low resource settings as described in §3.2. Results of MRPC and RTE in 5k\nsetting are omitted as their training data is less than 5k. CoLA is evaluated using Matthew’s Correlation. MRPC\nand QQP are evaluated using F1 score. STS-B is evaluated using Spearman’s correlation. The other tasks are\nevaluated using accuracy. We report averages across ﬁve random seeds, with standard deviations as subscripts.\ndata as the new training sets. In each setting, we\ndraw another 1k samples from the remaining train-\ning set as the validation set and instead use the\noriginal validation set as the test set, since the orig-\ninal GLUE test sets are not publicly available 3.\nWe perform ﬁne-tuning on BERT-base (BERT-\nft) and RoBERTa-base (RoBa.-ft) respectively as\nour baselines. We set the learning rate to 2e-5 and\nthe batch size to 16 for BERT and RoBERTa ﬁne-\ntuning experiments (See Appendix A.2 for details).\nFor adapters, we only tune its hidden sizes in {64,\n128, 256 }, setting the learning rate to 1e-4 and\nbatch size to 16 as the same used in §3.1.\nResults Table 2 presents the comparison results.\nFor adapter-based tuning, we report two results on\neach task. One is obtained with the optimal hid-\nden size which varies per dataset, and the other\nis obtained with the size of 64. We observe that\nadapter-based tuning outperforms ﬁne-tuning most\nof the time under both 1k and 5k settings. In partic-\nular, the performance gain is more signiﬁcant in 1k\nsetting, where on average across all tasks, adapter-\nbased tuning outperforms ﬁne-tuning by 2.5% and\n0.7% on BERT and RoBERTa respectively.\n3.3 Discussions\nOne consistent observation from §3.1 and §3.2\nis that adapters tend to outperform ﬁne-tuning on\n3Users are limited to a maximum of two submissions per\nday to obtain test results, which is inconvenient for a large\nnumber of runs\ntext-level classiﬁcation tasks when the training set\nis small, but with more training samples, the ben-\neﬁt of adapters is less signiﬁcant. In low-resource\nsetting, ﬁne-tuning has more severe overﬁtting\nproblem, since it has much more tunable parame-\nters compared to adapter-tuning, so adapter-tuning\nworks better than ﬁne-tuning. However, in high-\nresource setting, overﬁtting is not a big issue and\nmodel capacity counts more. Obviously, the model\ncapacity under ﬁne-tuning is larger than that under\nadapter-tuning since ﬁne-tuning can update much\nmore model parameters.\nWhen comparing the improvements of adapter\ntuning over ﬁne-tuning on tasks from TAE (§3.1)\nand GLUE (§3.2), we ﬁnd that the improvement\nis more signiﬁcant on low-resource tasks from\nTAE – on RoBERTa-base, the average improve-\nment brought by adapters is 1.9% across four low-\nresource tasks from TAE, while the average im-\nprovement on GLUE is 0.7% and 0.4% in 1k and\n5k settings respectively. As indicated in Gururan-\ngan et al. (2020), the TAE dataset is more domain-\nspeciﬁc and has less overlap with the corpus used\nfor RoBERTa-base pretraining, one intuitive ex-\nplanation for this observation is that ﬁne-tuning\nhas more severe forgetting and overﬁtting issues\nin domain adaptation where the target domain is\ndissimilar to the source domain in pretraining, thus\nadapter-based tuning is more preferable in this sce-\nnario.\n2213\nPOS NER XNLI\nModel All Target Distant All Target Distant All Target Distant\nXLMR-ft (Hu et al., 2020) 73.80 73.14 64.34 65.40 64.87 58.21 79.24 78.56 76.73\nXLMR-ft (reproduced) 74.29 73.61 64.90 63.85 63.32 56.85 79.28 78.64 77.03\nXLMR-adapter256 75.82 75.20 68.05 66.40 65.95 59.01 80.08 79.43 77.60\nTable 3: Zero-shot cross-lingual results. Accuracy is reported for POS tagging and XNLI. F1 is reported for NER.\nAll is the average test result of all languages.Targetis the average test result of all target languages except English.\nDistant is the average test result of the languages not in the Indo-European family.\n5% 10% 20%\nModel All Target Distant All Target Distant All Target Distant\nXLMR-ft 75.76 75.09 73.12 76.73 76.07 74.21 78.28 77.64 75.84\nXLMR-adapter64 76.09 75.47 73.78 77.52 76.94 75.10 78.68 78.07 76.39\nTable 4: Accuracy on XNLI with different amount of training data. We only compare XLMR-ft to XLMR-\nadapter64 in this set of experiments as XLMR-adapter64 is more light-weight.\n4 Cross-lingual Adaptation\nIn this section, we further compare ﬁne-tuning and\nadapter-based tuning in the zero-shot cross-lingual\ntransfer setting. All experiments in this section are\nbased on XLM-R-large (Conneau et al., 2020a),\na recent SOTA multilingual PrLM covering 100\nlanguages. We conduct evaluations on a set of\nmultilingual tasks from XTREME (Hu et al., 2020),\nincluding Universal Dependencies v2.5 tree banks\n(UD-POS) (Nivre et al., 2018), Wikiann NER (Pan\net al., 2017), and cross-lingual natural language\ninference (XNLI) (Conneau et al., 2020b). UD-\nPOS contains 34 languages, Wikiann NER contains\n40 languages, and XNLI contains 15 languages. We\nrefer the reader to Hu et al. (2020) for additional\ndetails about the datasets.\nExperimental Setup On each task, we perform\nhyperparameter tuning on the English development\nset. For both ﬁne-tuning and adapter-based tun-\ning, we use batch size 32, and tune the learning\nrates in {1e-5, 2e-5, 3e-5, 4e-5, 5e-5}. For adapter-\nbased tuning, we further tune the hidden sizes in\n{64, 128, 256}and ﬁnd size 256 often performs the\nbest. We train and select models with the English\ntraining and development sets and then evaluate the\ntuned models on test sets of all languages. See Ap-\npendix A.2 for hyperparameter and training details.\nResults Table 3 summarizes the results. To better\ncompare cross-lingual transfer to different groups\nof languages, we present the average results of\nall languages ( All), the target languages except\nEnglish (Target), and the Non-Indo-European lan-\nguages (Distant). It can be observed that adapter-\nbased tuning signiﬁcantly outperforms ﬁne-tuning\nModel TAE low GLUE1k XNLIfull XNLI5%\nﬁnetune 78.52 69.86 78.64 75.09\nAdapter64 77.20 71.20 79.01 75.47\nAdapter128 79.29 71.09 79.24 75.83\nAdapter256 80.41 71.06 79.43 75.45\nTable 5: Average test results with different adapter hid-\nden sizes. Results of GLUE1k are based on BERT-base.\nTAElow denotes low resource tasks from TAE.\non all three settings for each task. Speciﬁcally,\nadapter-based tuning outperforms the reported ﬁne-\ntuning results (Hu et al., 2020) on Targetand Dis-\ntant by 2.06% and 3.71% on UD-POS, 1.08% and\n0.8% on Wikiann NER, and 0.87% and 0.87% on\nXNLI. See Appendix A.3 for detailed results on\neach language.\nNote that UD-POS, Wikiann NER, and XNLI\nare all high-resource tasks, with 20k, 20k, and\n400k training samples respectively. Unlike mono-\nlingual tasks, adapters achieve consistent perfor-\nmance gains even under high-resource settings on\ncross-lingual tasks. We suspect that the ability to\nmitigate forgetting is more useful in cross-lingual\nscenarios since the model knowledge of the target\nlanguages only comes from pretraining. Adapter-\nbased tuning can better maintain the knowledge.\nWe further investigate the effectiveness of adapter-\nbased tuning on XNLI with smaller training sets.\nTable 4 summarizes the results when trained on\n5%, 10%, and 20% of the original training sets. In\nall settings, adapters still demonstrate consistent\nimprovements over ﬁne-tuning.\n2214\n0.0\n0.1\n0.2\n0.3\n0.4acc.\nCoLA (1k)\nFine-tune\nAdapter 0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nMNLI (1k)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6acc.\nCoLA (5k)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nMNLI (5k)\n2e-5 4e-5 6e-5 8e-5 1e-4\nlearning rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5acc.\nCoLA (1k)\n2e-5 4e-5 6e-5 8e-5 1e-4\nlearning rate\n0.4\n0.5\n0.6\n0.7\nMNLI (1k)\n2e-5 4e-5 6e-5 8e-5 1e-4\nlearning rate\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6acc.\nCoLA (5k)\n2e-5 4e-5 6e-5 8e-5 1e-4\nlearning rate\n0.74\n0.76\n0.78\n0.80\nMNLI (5k)\nFigure 4: Box plots of test performance distribution over 20 runs across different learning rates. The upper/bottom\nresults are based on Bert-base/RoBERETa-base. Note that the ﬁne-tuning results with learning rates larger than\n4e-5 on RoBERTa. MNLI 5k are all zeros, which are outside of the range and not shown in the subplot.\n5 Analysis\nAdapter Hidden Size The hidden size m4 is the\nonly adapter-speciﬁc hyperparameter. As indicated\nin Houlsby et al. (2019), the hidden size provides\na simple means to trade off performance with pa-\nrameter efﬁciency. Table 5 shows the performance\nwith different hidden sizes, from which we ﬁnd that\nincreasing the hidden size may not always lead to\nperformance gains. For monolingual low-resource\nadaptation, TAE tasks prefer a larger hidden size,\nwhile the results on GLUE are similar across differ-\nent hidden sizes. We suspect that this is due to that\nTAE datasets are more dissimilar to the pretraining\ncorpus, which requires relatively more trainable\nparameters to learn the domain-speciﬁc knowledge.\nOn XNLI, a larger hidden size helps improve the\nperformance when the full data is used. However,\nwhen only 5% training data is used, increasing the\nhidden size does not yield consistent improvements.\nThe results indicate that the optimal hidden size de-\npends on both the domain and the training size of\nthe task.\nLearning Rate Robustness We compare the two\ntuning methods in terms of their stability w.r.t the\nlearning rate. Figure 4 shows the performance dis-\ntributions on CoLA and MNLI under 1k and 5k\nsettings. The learning rates are varied in {2e-5, 4e-\n5, 6e-5, 8e-5, 1e-4}. Each box in the plot is drawn\nfrom the results of 20 runs with different random\nseeds. We observe that ﬁne-tuning yields larger\nvariances when increasing the learning rates. It\noften collapses with learning rates larger than 4e-5\n4The fraction of adapter parameters w.r.t. BERT-base\n(110M parameters) is 2%, 4%, and 6% when m is set to\n64, 128, and 256. The fraction w.r.t. XLMR-large (550M\nparameters) is 1%, 2%, and 3%, respectively.\n0.5\n1.0\n1.5eval. loss\nCoLA\nFine-tune\nAdapter\n10k steps\n0.5\n1.0eval. loss\nMRPC\n0.4\n0.6\n0.8\n1.0eval. loss\nQNLI\n60k steps\n0.4\n0.6\n0.8eval. loss\nSST-2\nFigure 5: Loss on the dev set w.r.t training steps. Re-\nsults are based on BERT-base. The original training\nand dev sets from GLUE are used for this analysis.\nEval acc. Mean (Best)\nFine-tune Adapter\nCoLA 54.27 (61.99) 58.27 (62.07)\nMRPC 84.53 (87.50) 85.28 (87.25)\nQNLI 89.39 (90.63) 90.41 (91.16)\nSST-2 90.21 (92.66) 91.01 (92.20)\nTable 6: Mean (Best) results on the dev set across all\nevaluation steps.\nwhen RoBERTa-base is used. Adapter-based tun-\ning is more stable across a wider range of learning\nrates.\nOverﬁtting and Generalization Here, we ﬁrst\nstudy the robustness of adapter-based tuning to\noverﬁtting. We use CoLA, MRPC, QNLI, and SST-\n2 with their original training and development sets\nfor our analysis. The CoLA and MRPC contain\n8.5k and 3.7k training samples and are regarded\nas low-resource tasks. The QNLI and SST-2 con-\n2215\n2\n 1\n 0 1 20\n1\n2\n3\n4\n5eval. loss\nCoLA\nFine-tune\nAdapter\n0\n1\n2\n3\n4\n5\n2\n 1\n 0 1 20\n1\n2\n3\n4\n5 SST-2\n0\n1\n2\n3\n4\n5\neval. loss\nFigure 6: Loss landscapes. BERT-base is used.\ntain 104k and 67k training samples and are used as\nhigh-resource tasks. We train the two low-resource\ntasks for 10k steps, and the high resource tasks for\n60k steps with a batch size of 16. We use BERT-\nbase for all experiments. Figure 5 plots the loss\ncurves on dev sets w.r.t training steps. We observe\nthat models with ﬁne-tuning can easily overﬁt on\nboth low- and high-resource tasks. Adapter-based\ntuning is more robust to overﬁtting. Additional re-\nsults on accuracy w.r.t. training steps and a similar\nanalysis on XNLI are in Appendix A.3.\nWe also present the mean and best dev results\nacross all evaluation steps in Table 6, where we\nperform an evaluation step every 20 training steps.\nThe mean results of adapter-based tuning consis-\ntently outperform those of ﬁne-tuning. The differ-\nences between the mean and the best values are\nalso smaller with adapter-based tuning. The results\nsuggest that the performance of adapters is more\nstable over ﬁne-tuning along the training process.\nTraining neural networks can be viewed as\nsearching for a good minima in the non-convex\nlandscape deﬁned by the loss function. Prior\nwork (Hochreiter and Schmidhuber, 1997; Li et al.,\n2018) shows that the ﬂatness of a local minima cor-\nrelates with the generalization capability. Thus, we\nfurther show the loss landscapes of the two tuning\nmethods. Following Hao et al. (2019), we plot the\nloss curve by linear interpolation between θ0 and\nθ1 with function f(α) = L(θ0 + α·(θ1 −θ0)),\nwhere θ0 and θ1 denote the model weights before\nand after tuning. L(θ) is the loss function and α\nis a scalar parameter. In our experiments, we set\nthe range of αto [−2,2] and uniformly sample 20\npoints. Figure 6 shows the loss landscape curves\non CoLA and SST based on BERT-base. It shows\nthat the minimas of adapter-based tuning are more\nwide and ﬂat, which indicates that adapter-based\ntuning tends to generalize better.\nCompare to Mixout The focus of this paper is\nto answer the question – besides being parameter-\nModel CoLA MRPC QNLI SST-2\nﬁnetune 41.39 83.56 80.51 87.84\nﬁnetune-mixout 42.35 84.00 80.03 87.71\nAdapter64 42.93 84.79 80.54 88.02\nAdapter64-mixout 42.52 83.80 80.67 87.66\nTable 7: Comparison with Mixout. Results are based\non BERT-base under 1k settiing. Average results across\n5 random seeds are reported.\nefﬁcient, when would adapter-based tuning be\nmore effective than ﬁne-tuning for PrLM adapta-\ntion? Thus, we only use ﬁne-tuning as our primary\nbaseline in previous sections. Here, for the sake\nof curiosity, we further compare adapter-based tun-\ning to ﬁne-tuning regularized by mixout (Lee et al.,\n2020) on a subset of GLUE tasks, since mixout\nsimilarly regularizes the learning process by miti-\ngating the forgetting issue. Speciﬁcally, it replaces\nall outgoing parameters from a randomly selected\nneuron to the corresponding parameters of the ini-\ntial model without tuning, such that it reduces di-\nvergence from the initial model. Following the sug-\ngestions in the paper, we conduct experiments by\nreplacing all dropout modules in the network with\nmixout and set the mixout probability to 0.9. From\nthe results in Table 7, we ﬁnd that using adapter-\nbased tuning alone yields the best results in most\ncases. Applying mixout to ﬁne-tuning improves the\nperformance on CoLA and MRPC only. However,\napplying it to adapters instead tends to degrade the\nperformance. We suspect that this is because the\nnumber of trainable parameters of adapters is very\nfew to begin with. Hence, further replacing a large\npercentage of them with their initial weights may\nweaken the learning ability.\n6 Related Work\nFine-tuning pretrained large scale language mod-\nels has proven its effectiveness on a wide range of\nNLP tasks (Devlin et al., 2019; Liu et al., 2019;\nConneau et al., 2020a; Brown et al., 2020). How-\never, ﬁne-tuning requires a new set of weights for\neach task, which is parameter inefﬁcient. Adapter-\nbased tuning is proposed to deal with this prob-\nlem (Houlsby et al., 2019). Most previous work\nhas demonstrated that it achieves comparable per-\nformance to ﬁne-tuning (Bapna and Firat, 2019;\nPfeiffer et al., 2020b,a,c; R¨uckl´e et al., 2020; Wang\net al., 2020; Guo et al., 2020). However, exist-\ning work mostly focuses on the parameter-efﬁcient\naspect while overlooks the effectiveness.\n2216\nFine-tuning PrLMs in a low-resource setting has\nbeen studied for a while (Dodge et al., 2020; Lee\net al., 2020; Phang et al., 2018; Jiang et al., 2020;\nZhang et al., 2021). Previous work points out that\nwith large-scale parameters, ﬁne-tuning on a few\nsamples can lead to overﬁtting and bad general-\nization, which causes the results unstable. Phang\net al. (2018) ﬁnd that pretraining on an intermedi-\nate task can improve ﬁne-tuning outcomes. Jiang\net al. (2020) improve the robustness of ﬁne-tuning\nby controlling the model complexity and prevent-\ning aggressive updating. On the other hand, catas-\ntrophic forgetting can appear when transferring\na pretrained neural networks (French, 1999; Mc-\nCloskey and Cohen, 1989; Goodfellow et al., 2013),\nwhere the learned knowledge from pretraining is\nlost when adapting to downstream tasks. This phe-\nnomenon often appears in NLP tasks (Mou et al.,\n2016; Arora et al., 2019). To relieve this problem of\nadapting pretrained language models, Howard and\nRuder (2018) gradually unfreeze the layers start-\ning from the last layer and Sun et al. (2019) ﬁnd\nassigning lower learning rate to the bottom layers\ncan improve the performance. Lee et al. (2020) reg-\nularize learning by encouraging the weights of the\nupdated model to stay close to the initial weights.\nAghajanyan et al. (2021) regularize ﬁne-tuning by\nintroducing noise to the input which is similar to\nadversarial training for ﬁne-tuning studied in Zhu\net al. (2020). Mosbach et al. (2021) point out that\nthe instability of ﬁne-tuning lies in the optimizer\nand propose to revise the Adam optimizer by re-\nplacing it with a de-bias version. Chen et al. (2020)\npropose a mechanism to recall the knowledge from\npretraining tasks.\n7 Conclusion\nPrior work often focuses on the parameter-efﬁcient\naspect while overlooks the effectiveness of adapter-\nbased tuning. We empirically demonstrate that\nadapter-based tuning can better regularize the learn-\ning process. We conduct extensive experiments to\nverify its effectiveness and conclude that 1) it tends\nto outperform ﬁne-tuning on both low-resource and\ncross-lingual tasks; 2) it demonstrates higher sta-\nbility under different learning rates compared to\nﬁne-tuning. We hope our study will inspire more\nfuture work on PrLM adaptation based on adapters\nand other methods that only tune part of the PrLM\nparameters.\nAcknowledgements\nLinlin Liu would like to thank the support from\nInterdisciplinary Graduate School, Nanyang Tech-\nnological University.\nReferences\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nJelle Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity and stability analysis of\nneural language models and brains. In Proceedings\nof the ACL-Workshop on Analyzing and Interpreting\nNeural Networks for NLP.\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021. Better ﬁne-tuning by reducing representa-\ntional collapse. In Proceedings of ICLR.\nGaurav Arora, Afshin Rahimi, and Timothy Baldwin.\n2019. Does an LSTM forget more than a cnn? an\nempirical study of catastrophic forgetting in NLP. In\nProceedings of ALTA.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of ACL.\nAnkur Bapna and Orhan Firat. 2019. Simple, scalable\nadaptation for neural machine translation. In Pro-\nceedings of EMNLP-IJCNLP.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Sys-\ntems.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of EMNLP.\nGrzegorz Chrupała and Afra Alishahi. 2019. Correlat-\ning neural and symbolic representations of language.\nIn Proceedings of ACL.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL.\n2217\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Hol-\nger Schwenk, Ves Stoyanov, Adina Williams, and\nSamuel R. Bowman. 2020b. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nEMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. CoRR.\nRobert M French. 1999. Catastrophic forgetting in con-\nnectionist networks. Trends in cognitive sciences,\n3(4):128–135.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron\nCourville, and Yoshua Bengio. 2013. An empirical\ninvestigation of catastrophic forgetting in gradient-\nbased neural networks. CoRR.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorpo-\nrating BERT into parallel sequence decoding with\nadapters. In Proceedings of NeurIPS.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the EMNLP-IJCNLP.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Flat\nminima. Neural Comput., 9(1):1–42.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of ICML.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of ACL.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nisation. In Proceedings of ICML.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings ACL.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nAarre Laakso and Garrison Cottrell. 2000. Content\nand cluster analysis: assessing representational sim-\nilarity in neural systems. Philosophical psychology,\n13(1):47–76.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to ﬁnetune\nlarge-scale pretrained language models. In Proceed-\nings of ICLR.\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and\nTom Goldstein. 2018. Visualizing the loss landscape\nof neural nets. In Proceedings of NeurIPS.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. CoRR.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109–165. El-\nsevier.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT em-\nbeddings during ﬁne-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of ﬁne-tuning\nBERT: misconceptions, explanations, and strong\nbaselines. In Proceedings of ICLR.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable are\nneural networks in NLP applications? In Proceed-\nings of EMNLP.\nJoakim Nivre, Rogier Blokland, Niko Partanen, and\nMichael Rießler. 2018. Universal dependencies 2.2.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of ACL.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterfusion: Non-destructive task composition\nfor transfer learning. CoRR.\nJonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aish-\nwarya Kamath, Ivan Vulic, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020b.\nAdapterhub: A framework for adapting transform-\ners. In Proceedings of EMNLP: System Demonstra-\ntions.\n2218\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020c. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. CoRR.\nJason Phang, Thibault F ´evry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. CoRR.\nAndreas R ¨uckl´e, Gregor Geigle, Max Glockner,\nTilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2020. Adapterdrop: On the efﬁciency of\nadapters in transformers. CoRR.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand PALs: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In Proceedings of\nICML.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune BERT for text classiﬁcation?\nIn Proceedings of CCL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. CoRR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q.\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT ﬁne-tuning. In Proceedings of ICLR.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced ad-\nversarial training for natural language understanding.\nIn Proceedings of ICLR.\n2219\nA Appendix\nA.1 Datasets\nTAE Table 8 presents the data statistics of the\nTAE datasets we used in §3.1.\nGLUE Table 9 presents the statistics and descrip-\ntions of GLUE tasks. In §3.2, to investigate the ef-\nfectiveness in low-resource scenarios, we simulate\ntwo low-resource settings by randomly sampling\n1k and 5k examples respectively from each of the\noriginal training set as the new training sets. In\neach setting, we draw 1k samples from the remain-\ning training set as our validation set and use the\noriginal validation set as held-out test set since the\noriginal GLUE test sets are not publicly available.\nFor the RSA analysis in §2 and the analysis of\noverﬁtting and generalization in §5, we use the\noriginal training and development sets for analysis\npurpose, as this better reveals the behaviors under\nboth high- and low- resource settings.\nA.2 Experimental Details\nImplementation We use language model imple-\nmentations from HuggingFace Transfromers li-\nbrary (Wolf et al., 2019). Our adapter implemen-\ntation is also based on that. Following standard\npractice (Devlin et al., 2019), we pass the ﬁnal\nlayer [CLS] token representation to a task-speciﬁc\nfeedforward layer for prediction on downstream\ntasks. Each experiment was performed on a single\nv100 GPU. We use the Adam optimizer (Kingma\nand Ba, 2015) with a linear learning rate scheduler.\nTraining Details on TAE and GLUE For both\nﬁne-tuning and adapter-based tuning, we train mod-\nels for a ﬁxed number of epochs, and select models\nwith the best validation performances on epoch end\nfor evaluation.\nFor ﬁne-tuning, on TAE we follow the learning\nrate and batch size as suggested by Houlsby et al.\n(2019). On GLUE, we tune learning rates in {1e-5,\n2e-5, 3e-5, 4e-5, 5e-5}and batch sizes in {16, 32}\nto select the best conﬁguration across tasks.\nFor adapters, on TAE, we set the batch size the\nsame as used in ﬁne-tuning, and tune learning rates\nin {2e-5, 5e-5,1e-4, 2e-4}and adapter’s hidden size\nin {64, 128, 256}to select the best conﬁguration\nacross all tasks. On GLUE, we keep the learning\nrate and batch size the same as used in TAE, and\ntune the adapter’s hidden sizes in{64, 128, 256}\nfor each task. We use the same hyperparameter\nsetting for all our analysis experiments with GLUE\ntasks as well.\nTable 10 presents the detailed hyperparameter\nsettings for TAE and GLUE.\nTraining Details on Xtreme Tasks For UD-\nPOS, Wikiann NER, and XNLI, we use batch size\n32, and tune learning rates in {1e-5, 2e-5, 3e-5,\n4e-5, 5e-5}on each task. We tune the adapter’s hid-\nden sizes in {64, 128, 256}to select the best value\nacross all tasks. We use the English training and\ndevelopment sets of each task for hyperparameter\ntuning. Table 11 presents the detailed settings.\nA.3 Additional Results\nRSA Figure 7 presents additional Representa-\ntional Similarity Analysis (RSA) plots on three\nGLUE tasks as mentioned in §2. We further con-\nduct RSA to show the deviation of representation\nspace before and after tuning (with English train-\ning set) on three distant languages (zh, ja, th) from\nthe cross-lingual NER task. Figure 8 presents the\nresults.\nAccuracy w.r.t Training Steps Figure 9 shows\nthe change of accuracy with increasing training\nsteps on four GLUE tasks. The results again in-\ndicate that adapter-based tuning is more robust to\noverﬁtting.\nOverﬁtting Analysis on XNLI We train XLMR-\nlarge with 10% of the original English training data\nof XNLI, and plot the average loss and accuracy\ncurves on development sets across all target lan-\nguages except English in Figure 10. The plots\ndemonstrate similar trends as shown in the plots of\nGLUE tasks (Figure 5 and Figure 9), where models\nwith ﬁne-tuning are easily overﬁtted and adapter-\nbased tuning is more robust to overﬁtting.\nDetailed Cross-lingual Results Table 12 and Ta-\nble 13 presents the cross-lingual POS tagging re-\nsults and the cross-lingual NER results on each\nlanguage respectively. Table 14 presents detailed\nresults on XNLI when trained with full data. 15\npresents detailed XNLI results when trained on 5%,\n10%, and 20% of training data.\n2220\n2 4 6 8 10 12\nBERT Layer\n0.2\n0.4\n0.6\n0.8\n1.0RSA Similarity\nRepresentation Space Comparison (CoLA)\nFine-tune vs Base\nAdapter vs Base\n2 4 6 8 10 12\nBERT Layer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0RSA Similarity\nRepresentation Space Comparison (MRPC)\nFine-tune vs Base\nAdapter vs Base\n2 4 6 8 10 12\nBERT Layer\n0.2\n0.4\n0.6\n0.8\n1.0RSA Similarity\nRepresentation Space Comparison (MNLI)\nFine-tune vs Base\nAdapter vs Base\nFigure 7: Comparison of the representations obtained at each layer before (Base) and after adapter-based tuning or\nﬁne-tuning on BERT-base using Representational Similarity Analysis (RSA). The original training and dev sets of\nCoLA, MRPC, and MNLI are used for this analysis. 5000 tokens are randomly sampled from the dev set of each\ntask for computing RSA. A higher score indicates that the representation spaces before and after tuning are more\nsimilar.\n14 16 18 20 22 24\nXLMR Layer\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8RSA Similarity\nRepresentation Space Comparison (NER (zh))\nFine-tune vs Base\nAdapter vs Base\n14 16 18 20 22 24\nXLMR Layer\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8RSA Similarity\nRepresentation Space Comparison (NER (ja))\nFine-tune vs Base\nAdapter vs Base\n14 16 18 20 22 24\nXLMR Layer\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6RSA Similarity\nRepresentation Space Comparison (NER (th))\nFine-tune vs Base\nAdapter vs Base\nFigure 8: Comparison of the representations obtained at each of the top 12 layers (layer 13-24) before ( Base) and\nafter adapter-based tuning or ﬁne-tuning on XLMR-large using Representational Similarity Analysis (RSA). We\nshow results on 3 distant languages from the Wikiann NER task. 5000 tokens are randomly sampled from the dev\nset of each language for computing RSA. A higher score indicates that the representation spaces before and after\ntuning are more similar.\n10k steps\n0.2\n0.3\n0.4\n0.5\n0.6eval. result\nCoLA\nFine-tune\nAdapter\n10k steps\n0.70\n0.75\n0.80\n0.85\nMRPC\nFine-tune\nAdapter\n60k steps\n0.80\n0.85\n0.90\nQNLI\nFine-tune\nAdapter\n60k steps\n0.86\n0.88\n0.90\n0.92\nSST-2\nFine-tune\nAdapter\nFigure 9: Accuracy on the dev set w.r.t training steps. Results are based on BERT-base. The original training and\ndev sets from GLUE are used for this analysis. We can observe that for both high resource (QNLI and SST-2) and\nlow-resource (CoLA and MRPC) tasks, adapter-based tuning is more robust to overﬁtting.\n0 2000 4000 6000 8000 10000 12000 14000\n# of training steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2eval. loss\nFine-tune\nAdapter\n0 2000 4000 6000 8000 10000 12000 14000\n# of training steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70eval. accuracy\nFine-tune\nAdapter\nFigure 10: Change of the average dev loss (left) and accuracy (right) across all target languages of XNLI except\nEnglish with increasing training steps. The results are obtained when trained on 10% of the XNLI training data.\n2221\nDomain Task Label Type # Train # Dev # Test # Class\nBIOMED CHEMPROT relation classiﬁcation 4169 2427 3469 13\nRCT abstract sent. roles 180040 30212 30135 5\nCS ACL-ARC citation intent 1688 114 139 6\nSCIERC relation classiﬁcation 3219 455 974 7\nNEWS HYPERPARTISAN partisanship 515 65 65 2\nAGNEWS topic 115000 5000 7600 4\nREVIEWS HELPFULNESS review helpfulness 115251 5000 25000 2\nIMDB review sentiment 20000 5000 25000 2\nTable 8: Data statistics of Task Adaptation Evaluation (TAE) tasks.\nTask Description # Train # Dev # Class\nCoLA linguistic acceptability classiﬁcation 8.5k 1042 2\nMNLI textual entailment classiﬁcation 392k 9816/9833 3\nMRPC paraphrase classiﬁcation 3.7k 409 2\nQNLI textual entalment classiﬁcation 104k 5464 2\nQQP quora question paris classiﬁcation 363k 404k 2\nRTE textual entailment classiﬁcation 2.5k 278 2\nSST-2 sentiment classiﬁcation 67k 873 2\nSTS-B semnatic textual similarity (regression) 5.7k 1501 -\nTable 9: Data statistics of GLUE tasks.\nTAE GLUE\nHyperparameter ﬁne-tuning adapter ﬁne-tuning adapter\nnumber of epochs 10 10 20 20\nbatch size 16 16 16 16\nlearning rate 2e-5 1e-4 2e-5 1e-4\ndropout 0.1 0.1 0.1 0.1\nfeedforward layer 1 1 1 1\nfeedforward nonlnearity layer 1 1 1 1\nclassiﬁcation layer 1 1 1 1\nTable 10: Hyperparameters for ﬁne-tuning and adapter-based tuning for experiments on TAE and GLUE.\nPOS NER XNLI\nHyperparameter ﬁne-tune adapter ﬁne-tune adapter ﬁne-tune adapter\nnumber of epochs 5 5 5 5 5 5\nbatch size 32 32 32 32 32 32\nlearning rate 2e-5 5e-5 2e-5 5e-5 1e-5 4e-5\ndropout 0.1 0.1 0.1 0.1 0.1 0.1\nfeedforward layer 1 1 1 1 1 1\nfeedforward nonlnearity layer 1 1 1 1 1 1\nclassiﬁcation layer 1 1 1 1 1 1\nTable 11: Hyperparameters for ﬁne-tuning and adapter-based tuning for experiments on UD-POS, Wikiann NER,\nand XNLI.\n2222\nen af ar bg de el es et eu fa ﬁ fr he hi hu id it\nIndo-European yes yes no yes yes yes yes no no yes no yes no yes no no yesXLMR-ft† 96.10 89.80 67.50 88.10 88.50 86.30 88.30 86.50 72.50 70.60 85.80 87.20 68.30 56.80 82.60 72.40 89.40XLMR-ft∗ 96.15 89.26 69.12 88.33 88.79 87.42 88.34 87.38 73.70 71.05 86.56 87.24 67.86 75.48 83.49 72.67 89.07XLMR-adapter256 95.89 89.30 70.50 88.79 88.48 86.44 88.99 87.31 74.84 71.94 85.99 88.74 67.32 69.63 83.11 73.31 90.16\nja kk ko mr nl pt ru ta te th tl tr ur vi yo zh avg\nIndo-European no no no yes yes yes yes no no no no no yes no no no -XLMR-ft† 15.90 78.10 53.90 80.80 89.50 87.60 89.50 65.20 86.60 47.20 92.20 76.30 70.30 56.80 24.60 25.70 73.80XLMR-ft∗ 21.34 78.86 53.84 85.24 89.75 87.98 89.75 64.34 85.65 43.12 93.03 76.65 69.43 58.10 23.92 28.60 74.29XLMR-adapter256 38.53 78.47 53.35 86.45 89.86 88.82 90.21 64.31 85.38 55.88 91.10 76.21 63.46 59.38 24.28 55.76 75.82\nTable 12: Zero-shot cross-lingual POS tagging accuracy on the test set of each target language. Results with “ †”\nare taken from (Hu et al., 2020). Results with “∗” are reproduced by us.\nen ar he vi id jv ms tl eu ml ta te af nl de el bn hi mr ur\nIndo-European yes no no no no no no no no no no no yes yes yes yes yes yes yes noXLMR-ft† 84.7 53 56.8 79.4 53 62.5 57.1 73.2 60.9 67.8 59.5 55.8 78.9 84 78.8 79.5 78.8 73 68.1 56.4XLMR-ft∗ 84.62 43.72 54.08 77.19 52.26 58.37 69.78 72.21 62.08 65.78 56.92 52.31 77.64 84.26 77.95 77.23 76.25 71.01 64.14 54.15XLMR-adapter256 83.87 51.89 56.59 78.02 53.53 63.24 62.65 71.57 64.96 68.30 59.57 54.93 79.43 84.88 79.38 80.51 78.99 73.17 72.74 72.36\nfa fr it pt es bg ru ja ka ko th sw yo my zh kk tr et ﬁ hu\nIndo-European yes yes yes yes yes yes yes no no no no no no no no no no no no noXLMR-ft† 61.9 80.5 81.3 81.9 79.6 81.4 69.1 23.2 71.6 60 1.3 70.5 33.6 54.3 33.1 56.2 76.1 79.1 79.2 79.8XLMR-ft∗ 61.13 79.07 81.05 79.61 68.76 81.18 71.46 18.31 68.93 57.99 1.47 69.95 41.26 51.32 25.82 49.83 78.94 78.03 78.63 79.32XLMR-adapter256 60.39 81.21 81.79 82.61 76.12 82.50 69.76 21.41 70.55 61.37 2.47 68.90 38.18 60.48 31.11 51.34 81.89 80.36 80.86 82.06\nTable 13: Zero-shot cross-lingual NER F1 on the test set of each language. Results with “ †” are taken from (Hu\net al., 2020). Results with “∗” are reproduced by us.\nModel en ar bg de el es fr hi ru sw th tr ur vi zh\nXLMR-ft† 88.7 77.2 83 82.5 80.8 83.7 82.2 75.6 79.1 71.2 77.4 78.0 71.7 79.3 78.2\nXLMR-ft∗ 88.28 78.34 82.73 82.07 81.34 83.63 81.93 75.33 79.04 71.59 76.67 78.36 71.86 79.32 78.80\nXLMR-adapter256 89.22 78.62 83.59 83.47 82.39 84.69 83.27 76.42 79.74 72.21 77.84 78.80 72.27 79.32 79.34\nTable 14: Zero-shot XNLI accuracy on the test set of each language when trained with full data. Results with “ †”\nare taken from (Hu et al., 2020). Results with “∗” are reproduced by us.\nModel en ar bg de el es fr hi ru sw th tr ur vi zh\n5% training data\nXLMR-ft 85.09 73.53 78.7 79.58 77.26 80.13 79.36 72.07 76.52 67.8 72.53 74.53 68.3 75.24 75.74\nXLMR-adapter64 84.77 73.95 78.76 79.02 78.08 80.55 79.48 72.01 76.54 68.76 73.83 75.56 68.6 75.94 75.50\n10% training data\nXLMR-ft 85.96 75.04 79.78 79.82 78.72 80.99 80.25 73.23 77.28 68.08 74.43 75.7 69.54 76.02 76.16\nXLMR-adapter64 85.74 76.78 80.27 80.77 79.72 81.87 81.13 73.87 78.42 69.3 74.25 77.08 69.54 77.30 76.82\n20% training data\nXLMR-ft 87.26 76.48 81.07 82.03 80.47 82.55 81.53 75.06 78.04 69.96 76.00 77.36 70.75 77.74 77.94\nXLMR-adapter64 87.24 78.00 81.87 82.15 80.47 82.65 81.53 75.00 78.74 70.87 75.94 78.44 70.51 78.70 78.16\nTable 15: Zero-shot XNLI accuracy on the test set of each language when trained on 5%, 10%, 20% of training\ndata respectively.",
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.8684567213058472
    },
    {
      "name": "Computer science",
      "score": 0.7058818340301514
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5621471405029297
    },
    {
      "name": "Language model",
      "score": 0.47874635457992554
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4563385546207428
    },
    {
      "name": "Natural language processing",
      "score": 0.4482380449771881
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3965797424316406
    },
    {
      "name": "Speech recognition",
      "score": 0.32440218329429626
    },
    {
      "name": "Programming language",
      "score": 0.32424044609069824
    },
    {
      "name": "Linguistics",
      "score": 0.32269564270973206
    },
    {
      "name": "Operating system",
      "score": 0.12461921572685242
    },
    {
      "name": "Psychology",
      "score": 0.10253763198852539
    },
    {
      "name": "Philosophy",
      "score": 0.07560586929321289
    },
    {
      "name": "Neuroscience",
      "score": 0.056427985429763794
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I152815399",
      "name": "Singapore University of Technology and Design",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 112
}