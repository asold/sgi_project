{
  "title": "Transformers as Soft Reasoners over Language",
  "url": "https://openalex.org/W3006188107",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1912941100",
      "name": "Peter Clark",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A662704055",
      "name": "Oyvind Tafjord",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2122050085",
      "name": "Kyle Richardson",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2995359496",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W2952862139",
    "https://openalex.org/W1279140112",
    "https://openalex.org/W2078508369",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2111353076",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2986920492",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2963117013",
    "https://openalex.org/W2964222271",
    "https://openalex.org/W1520957705",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2786395785",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W59020575",
    "https://openalex.org/W2132453167",
    "https://openalex.org/W2884967713",
    "https://openalex.org/W2119409989",
    "https://openalex.org/W2997686430",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W2969159802",
    "https://openalex.org/W51796616",
    "https://openalex.org/W2947000318",
    "https://openalex.org/W2998370259",
    "https://openalex.org/W2914245851",
    "https://openalex.org/W1536183618",
    "https://openalex.org/W2968433472",
    "https://openalex.org/W2903190877",
    "https://openalex.org/W2950246755",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2154474435",
    "https://openalex.org/W2321696644",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W2577946330",
    "https://openalex.org/W2995129620",
    "https://openalex.org/W2228826686"
  ],
  "abstract": "Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.",
  "full_text": "Transformers as Soft Reasoners over Language\nPeter Clark, Oyvind Tafjordand Kyle Richardson\nAllen Institute for AI, Seattle, W A\nfpeterc,oyvindt,kylerg@allenai.org\nAbstract\nBeginning with McCarthy’s Advice Taker (1959),\nAI has pursued the goal of providing a system with\nexplicit, general knowledge and having the system\nreason over that knowledge. However, expressing\nthe knowledge in a formal (logical or probabilistic)\nrepresentation has been a major obstacle to this re-\nsearch. This paper investigates a modern approach\nto this problem where the facts and rules are pro-\nvided as natural language sentences, thus bypassing\na formal representation. We train transformers to\nreason (or emulate reasoning) over these sentences\nusing synthetically generated data. Our models,\nthat we call RuleTakers, provide the ﬁrst empiri-\ncal demonstration that this kind of soft reasoning\nover language is learnable, can achieve high (99%)\naccuracy, and generalizes to test data requiring sub-\nstantially deeper chaining than seen during training\n(95%+ scores). We also demonstrate that the mod-\nels transfer well to two hand-authored rulebases,\nand to rulebases paraphrased into more natural lan-\nguage. These ﬁndings are signiﬁcant as it suggests\na new role for transformers, namely as limited “soft\ntheorem provers” operating over explicit theories in\nlanguage. This in turn suggests new possibilities\nfor explainability, correctability, and counterfactual\nreasoning in question-answering.1\n1 Introduction\nAI has long pursued the goal of giving a system explicit\nknowledge, and having itreason over that knowledge to reach\nconclusions, dating back to the earliest years of the ﬁeld, e.g.,\nMcCarthy’s Advice Taker (1959), and Newell and Simon’s\nLogic Theorist (1956). While this has resulted in impres-\nsive applications (e.g., [Metaxiotis et al., 2002 ]), building\nand reasoning over the required formal representations has\nalso proved challenging [Musen and Van der Lei, 1988 ]. In\nthis work, we explore a modern approach to this goal, and ask\nwhether transformers can be trained to reason (or emulate rea-\nsoning) using rules expressed in language, thus bypassing a\n1 A live demo and all our datasets are available at\nhttps://allenai.org/data/ruletaker\n(Input Facts:)Alan is blue. Alan is rough. Alan is young.\nBob is big. Bob is round.\nCharlie is big. Charlie is blue. Charlie is green.\nDave is green. Dave is rough.\n(Input Rules:)Big people are rough.\nIf someone is young and round then they are kind.\nIf someone is round and big then they are blue.\nAll rough people are green.\nQ1: Bob is green. True/false? [Answer: T]\nQ2: Bob is kind. True/false? [F]\nQ3: Dave is blue. True/false? [F]\nFigure 1: Questions in our datasets involve reasoning with rules.\nThe inputs to the model are the context (facts + rules) and a ques-\ntion. The output is the T/F answer to the question. Here the under-\nlying reasoning for the true fact (Q1) is: Bob is big, therefore rough\n(rule1) therefore green (rule4). Note that the facts + rules themselves\nchange for different questions in the datasets.\nformal representation. If so, new opportunities for question-\nanswering, explainability, correctability, and counterfactual\nreasoning may become possible.\nThis goal is quite distinct from question-answering as\nselecting an answer span in a passage, today’s prevailing\nparadigm, e.g., [Rajpurkar et al., 2016]. Rather, we want the\nsystem to reason over the provided rules to ﬁnd conclusions\nthat follow. Our goal is also distinct from that of inducing\nrules from examples, e.g., given instances of family relation-\nships, inducing that a parent’s parent is a grandparent [Sinha\net al., 2019], something that transformers are already known\nto do well. Rather, here we provide rules explicitly, and wish\ntransformers to draw appropriate conclusions, as illustrated\nin Figure 1. Here, rather than inducing rules from examples,\nour task involves learning to emulate a reasoning algorithm.\nWe provide the ﬁrst demonstration that this is possible,\ni.e., that transformers can reason with rules expressed in lan-\nguage. Our approach uses a broadly applicable training regi-\nmen: Characterize the desired behavior in a formal way, syn-\nthesize formal examples, generate linguistic equivalents, and\ntrain a model. The result suggests a new role for transform-\ners, namely as a kind of limited “soft theorem prover” over\nlanguage (Figure 2). This in turn may allow inspection and\ncontrol of the knowledge that the model is manipulating, with\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3882\nFigure 2: (a) Traditional formal reasoning applies a theorem prover\nto axioms in order to answer a question. (b) Our work here strives\nfor a linguistic analog, where a transformer serves as a “soft theorem\nprover” over knowledge expressed linguistically.\npotential beneﬁts for explanation, correctability, and counter-\nfactual reasoning.\nOur investigations here are in a limited setting: Rules are\nlinguistic expressions of conjunctive implications condition\n[^condition]* !conclusion, with the semantics of logic\nprograms with negation [Apt et al., 1988]; and reasoning is\nthe deduction of a statement’s truth according to these seman-\ntics. However, although there is still a potentially large gap\nto natural language inference (NLI),2 our approach also sug-\ngests a path to teaching machines to reason over broader lan-\nguage, with similar potential beneﬁts.\nWe leave open the question of whether the transformer is\nactually “reasoning”, and even what that might mean in a neu-\nral setting. Rather, we show that transformers can reliably\nemulate the i/o behavior of a formal reasoner, including ap-\nplied to test data requiring more reasoning than at training\ntime, two hand-authored rulebases, and rulebases rephrased\ninto more natural (crowdsourced) language.\nThe paper is organized to address the following questions,\nand contributes the following results:\n1. Can transformers learn to reason with rules?We\ntrain and test on rules expressed in (synthetic) language,\nand ﬁnd high (99%) accuracy, including on test ques-\ntions requiring a greater depth of reasoning than seen\nduring training (scoring up to 95%, Table 1).\n2. Can the trained model solve hand-authored reason-\ning problems? We ﬁnd the trained models are able to\nsolve ﬁve of six variants of two independently authored\nrule-based problems, zero shot (90%+ scores, Table 4).\n3. Do the results transfer to theories expressed in more\nnatural language? Models also perform well when\ntrained and tested on theories paraphrased into more nat-\nural (crowdsourced) language (98% score). The best\nearlier model can even partially solve these problems\nzero-shot (66% accuracy, Table 5).\n4. Can the model identify which facts an answer de-\npends on?We show that the model is largely able to do\nthis (94% F1), including perfect identiﬁcation for over\n70% of the questions. This is a ﬁrst step towards having\na model create an explanation for its conclusions. (Sec-\n2 NLI is informally deﬁned as making inferences from language\nthat “a person would typically infer” [Dagan et al., 2013], and in-\ncludes use of many linguistic forms, unstated background knowl-\nedge, and sometimes unsound inference steps.\ntion 4.5 and Figure 8).\n5. Can other neural architectures learn to reason?Our\nexperiments show a particular transformer (RoBERTa)\nis sufﬁcient for our tasks, but is it necessary? We show\nthat two other systems, BERT and ESIM (an LSTM-\nbased model) [Chen et al., 2017], are also able to learn\nthese tasks, albeit with lower scores (95% and 80% re-\nspectively, vs. 98%). This suggests that our results\nare not speciﬁc to RoBERTa or transformers, although\ntransformers learn the tasks more easily (Table 6).\n2 Related Work\nWhile our work is, to the best of our knowledge, the ﬁrst sys-\ntematic study of transformers directly reasoning with rules in\nlanguage, there are several datasets that make a ﬁrst step to-\nwards this by testing whether neural systems can apply a sin-\ngle rule in a particular situation. Task 15 in the bAbI dataset\n[Weston et al., 2016] tests whether a rule of the form “Xs\nare afraid of Ys” can be correctly applied, e.g., “Sheep are\nafraid of wolves. Gertrude is a sheep. What is Gertrude afraid\nof? A:wolves”. Similarly, the synthetic, conditional probes in\n[Richardson et al., 2020] test single rule application. In addi-\ntion, the datasets QuaRTz [Tafjord et al., 2019] and ROPES\n[Lin et al., 2019] involve applying general statements to a sit-\nuation, but also require many other reading comprehension\nskills, rather than speciﬁcally testing reasoning.\nAlthough our core datasets may seem similar to the bAbI\ndataset [Weston et al., 2016 ] in using synthetic data, our\nprobes are qualitatively different. Speciﬁcally, apart from\nbAbI Task 15 (above), the underlying rules needed to infer\nan answer in the bAbI tasks are implicit, while our concern\nhere is reasoning with explicit rule sets, potentially different\nfor each example (Figure 1).\nOur approach contrasts with prior efforts that attempt to\nsemantically parse language into a formal form, so that a for-\nmal reasoner can then be applied [Kamath and Das, 2019 ].\nDespite substantial research, semantic parsing remains chal-\nlenging, with few examples of systems that can reliably con-\nvert multi-sentence text into formal theories. Instead, we ex-\nplore reasoning with language directly, bypassing the seman-\ntic parsing task.\nOur work can be seen as evaluating transformers for (a sub-\nset of) Natural Logic [MacCartney and Manning, 2014], i.e.,\nformal inference over statements expressed in language. It\nis also related to textual entailment and Natural Language\nInference (NLI) [Manning and MacCartney, 2009], but with\nthe important difference that NLI also allowsunsupported in-\nferences that “a person would typically infer” [Dagan et al.,\n2013]. We discuss bridging the gap between our work and\nNLI in Section 5.3.\nSeveral researchers have developed methods for Neural\nTheorem Proving (NTP), combining symbolic and neural\nmethods to reason step-wise over language-derived struc-\ntures, e.g., [Weber et al., 2019]. Similarly, there has been\nwork on SAT solving [Selsam et al., 2019 ], approximate\n(DNF) model counting [Abboud et al., 2020], and formula\nembedding [Abdelaziz et al., 2020] to help solve formal rea-\nsoning problems. While our goals are similar, we do not im-\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3883\npose any structure on the neural reasoning process, instead\nwanting to know if the (i/o of the) reasoning process itself is\nlearnable, using knowledge expressed in language.\nOur task can perhaps best be viewed as one of algorithm\nemulation, here for systematic reasoning with rules. There\nhave been numerous other demonstrations that transformers\neither already know [Talmor et al., 2019; Richardson and\nSabharwal, 2019] or can learn to emulate other algorithms,\nincluding for semantic parsing [He and Choi, 2019 ], ma-\nchine translation [Wanget al., 2019], integration [Lample and\nCharton, 2019], and math [Saxton et al., 2019]. Here we in-\nvestigate a transformer’s ability to learn rule-based reasoning.\n3 Dataset Generation\nTo investigate a transformer’s ability to emulate rule-based\nreasoning, we generate ﬁve datasets requiring various depths\nof inference to answer the questions. Each example in a\ndataset is a triple (context,statement,answer), where context\nhas the form (fact*,rule*), statement is the question, namely\na declarative sentence to prove, andanswer is either T (true) if\nstatement deductively follows from the context, or F if it does\nnot (false under a closed-world assumption, CW A). Facts,\nrules, and the question statements are expressed in (synthetic)\nEnglish. Each example is essentially a (linguistic) standalone\nlogical theory with an “Is it true?” question posed against it.\n3.1 Overview\nTo generate each example, we ﬁrst generate a small theory\n(facts + rules) in logic, perform forward inference to de-\nrive all its implications, then select question statements from\nthose implications (answer=true), and from unproven (posi-\ntive) facts (answer=false, under the CW A). We generate ﬁve\ndatasets, each constrained by the maximum depth of infer-\nence required to prove the facts used in its questions (up to\ndepths D=0, D\u00141, D\u00142, D\u00143 and D\u00145 respectively). Depth\nD=0 means the true facts can be “proved” by simple lookup\nin the context (no inference). The ﬁfth dataset, called DMax,\ncontains questions up to depth 5, and is used to test general-\nization to depths unseen in training on the other four datasets.\n3.2 Theory Generation\nTheories contain two types of facts:\n\u000fattributes is(ei; aj ) e.g., is(Alan,Big).\n\u000frelations rk(ei; ek) e.g., eats(Dog,Rabbit).\nThe is() predicate assigns attributes to entities, while therk()\npredicates relate two entities. Like people names, the symbols\nDog, Rabbit, etc. also denote speciﬁc entities, i.e., denote\n“the dog”, “the rabbit”, etc. Rules are of the form:\ncondition [^condition]* !conclusion.\nThe ﬁrst condition is a predicate whose ﬁrst argument is a\nvariable,3 and second argument is an attribute or entity. For\neach subsequent condition and the conclusion, they are also\npredicates whose ﬁrst argument is either the same variable or\na previously mentioned entity, and the second argument is a\n3 Or with 20% probability, an entity, in order to include some\nfully grounded rules in the datasets.\nThe bald eagle does not eat the dog. The cat chases the dog.\nThe cat eats the bald eagle. The cat is nice. The cat likes the dog.\nThe cat likes the rabbit. The dog is furry.\nThe rabbit chases the bald eagle. The rabbit eats the bald eagle.\nIf someone does not eat the cat then they do not eat the dog.\nIf someone likes the bald eagle then they do not like the rabbit.\nIf someone eats the bald eagle and they do not eat the rabbit\nthen they are furry.\nIf someone is furry then they like the cat.\nQ1. The bald eagle likes the cat. True/false? [F]\nQ2. The rabbit likes the cat. True/false? [T]\nQ3. The bald eagle is furry. True/false? [F]\nFigure 3: An example of a rulebase and 3 questions using relations\nwith negation. The reasoning for the [T] answer is: The rabbit eats\nthe bald eagle (given), therefore the rabbit is furry (rule3), therefore\nthe rabbit likes the cat (rule4).\nnew attribute or entity. (In this way, rules are constrained to\nhave at most one variable. Rules are implicitly universally\nquantiﬁed over that variable). For example, the formal form\nof the ﬁrst rule in Figure 1 looks:\n// If someone is young and round then they are kind.\nis(?X,Young) ^is(?X,Round) !is(?X,Kind).\nEach theory contains 1-16 facts and 1-9 rules generated at\nrandom. We generate two types of theory:\n1. Type 1 uses only the is() predicate, with 4 entities\nfAlan,Bob,...gand 7 (non-mutually-exclusive) attributes\nfBlue,Rough,Young,...g, drawn randomly from pools of\n10 names and 14 attributes respectively.\n2. Type 2 uses is() and 3 other predicates flikes(),\nchases(); :::g, 4 entities fCat,Dog,BaldEagle,...g, and 5\nattributes fBig,Furry,...g, drawn randomly from pools of\nsize 6, 10, and 10 respectively.\nWe also generate a version of each that adds negation (not) in\nthe facts and rule conditions/conclusions (negation-as-failure\nfor conditions, strong negation for conclusions). Figure 1 is\nan example of Type 1, without negation. Figure 3 is an ex-\nample of Type 2, with negation. Each dataset contains 100k\nexamples (25k of each Type \u0002without/with negation). Data\nis randomly split 70/10/20 into train/dev/test partitions, en-\nsuring no overlap of theories between each partition.\n3.3 Forward Inference\nGiven a randomly generated theory (facts+rules), we perform\nexhaustive forward inference to ﬁnd all its implications, not-\ning their proof(s). (As the domains are ﬁnite, the number of\nimplications are ﬁnite too). For semantics, we treat the rule-\nbase as a logic program, and infer the minimal, supported\nanswer set implied by the program [Apt et al., 1988]. Nega-\ntions in the rules’ conditions are treated as negation as failure\n(NAF), and we ensure that the rulebase is stratiﬁed to avoid\nambiguity and cycles [Bidoit and Froidevaux, 1991 ]. Infer-\nence is performed layerwise to ﬁnd the minimal supported\nmodel, and inconsistent and unstratiﬁed rulebases are dis-\ncarded. We also check that inference proceeds to the depth\nrequired, e.g., for the D\u00143 dataset, at last one fact must re-\nquire depth 3 inference to infer it for all its theories.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3884\n3.4 Question Generation and English Synthesis\nFor each theory, we generate several questions with answer\n‘true’ by selecting from the inferred facts, one at each depth\nof inference from 0 to the dataset’s target depth (e.g., for the\nD\u00142 dataset, we generate 3 ‘true’ questions at depthsd = 0, 1,\nand 2 for each theory). For each ‘true’ question we also gen-\nerate a ‘false’ question by negating a conclusion proven at the\nsame depth. We then generate the same number of questions\nusing facts that are unproven (false under a closed-world as-\nsumption), drawing equally from unproven, instantiated pos-\nitive rule conclusions or other unproven positive facts. Half\nare used as questions labeled as false (via the CW A), and for\ndiversity, half are ﬂipped by negating the fact and changing\nthe label to true (i.e., “f ? False” becomes “Not f? True”).\nThus a theory for depth d has (up to) 4(d+1) questions, with\nan equal balance of true and false answers. Each question is\nalso annotated with the inference depth needed to answer it.\nFinally the theories and questions are converted into (syn-\nthetic) English, using simple natural language templates plus\nrules to improve ﬂuency (e.g., using pronouns). We use three\ntemplates (randomly selected per rule): “If condition [and\ncondition]* then conclusion.”, “All attribute* peoplejthings\nare attribute.”, and “attribute* peoplejthings are attribute.”,\nthe last two only applicable to rules involving just attributes.\nExamples are shown in Figures 1 and 3.\n4 Experiments\n4.1 Models\nWe conduct all our experiments (bar Section 4.6) using\nRoBERTa-large, additionally ﬁne-tuned on the RACE dataset\n[Lai et al., 2017]. We use ﬁxed hyperparameters (learning\nrate etc), inheriting the settings from RoBERTa on RACE\n[Liu et al., 2019].\nWe train RoBERTa to predict true/false (i.e., binary clas-\nsiﬁcation) for each question statement. Questions are sup-\nplied to RoBERTa as: [CLS] context [SEP] statement [SEP],\nwhere context is the theory (facts+rules, expressed in lan-\nguage) and statement is the fact to try and prove. The [CLS]\noutput token is projected to a single logit. A logit score of\n>0 is treated as predicting true, otherwise the answer is false.\nTraining is performed using cross-entropy loss. For evalua-\ntion, we measure accuracy. (The test data has an equally bal-\nance of TRUE/FALSE answers, hence the baseline of random\nguessing is 50%).\n4.2 Can RoBERTa Answer Reasoning Questions?\nWe train and test RoBERTa models on each of our datasets\nD=0, D\u00141, D\u00142, D\u00143, and DMax, containing problems re-\nquiring reasoning up to depths 0, 1, 2, 3, and 5 respectively.\nWe then test the models on the DMax dataset, that includes\nproblems at depths greater than the other datasets. The re-\nsults are shown in Table 1. The results suggest the following\nﬁndings:\n1. RoBERTa is able to master the test data almost per-\nfectly (99% accuracy, row 1) even though the speciﬁc\nreasoning problems (facts+rules) in each test question\nare distinct from those in the training set.\nTable 1: Accuracy of models (Mod0,...) trained and tested on the ﬁve\ndatasets (“Test (own)” row), and tested on all, and different slices, of\nthe DMax test set. The boxed area indicates test problems at depths\nunseen during training.\n2. The Depth=0 model, Mod0, only trained on lookup\nquestions, is (unsurprisingly) unable to answer ques-\ntions requiring reasoning(column Mod0).4\n3. As we train with increasingly deep inference, the mod-\nels’ ability to generalize improves. The D\u00142 model\n(questions involving problems up to depth 2) achieves\n71.1% on Depth=3 problems, while the D\u00143 model\ngeneralizes wellright up to the maximum depth tested\n(e..g, 97.6% for Depth=5 problems).\nWe additionally test the robustness of the models’ answers\nby perturbing the original theories. Speciﬁcally, for each test\nfact f that is true, we test whether removing a sentence that\nis part of the proof of f causes the prediction to (desirably)\nﬂip from true to false. We call these sentences in the proof\ntree critical sentences, as the truth of f depends on them.\nConversely, removing anirrelevant sentence should cause no\nchange to the model’s prediction. As we know the original\nproof trees for each fact f in the dataset, we can identify the\ncritical and irrelevant sentences by simple inspection of those\ntrees.5 Typically, 1-6 sentences of the \u001915-20 sentences are\ncritical for proving each provable fact.\nWe test this using the no-negation 6 half of the DMax test\nset (\u001910k questions). In this partition, 5904 questions have\nproofs (are true). (The remaining questions are false under\nthe CW A). For each of these questions, we remove each of\nthe theory sentencessi in turn, and measure the prediction ac-\ncuracy on each result. As there are about 19 sentences/theory\non average, this results in 113978 “sentence removed” probes\n(of which 20746 have a critical sentence removed, and 93232\nhave an irrelevant sentence removed). Ideally, removing a\nsentence critical to a question f should ﬂip the model’s pre-\n4 In fact, we see an interesting learning artifact, namely Mod0\nscores worse than random (50%) at depths higher than 2. This arises\nbecause most questions at these depths are provably true facts, but\nMod0 learns to predict all facts are false except those explicitly given\n(as that is all it has seen at training time), hence systematically gets\nthese wrong.\n5 If there are multiple, alternative proofs for f, we deﬁne a criti-\ncal sentence as one that is used in all the proofs. To support this, we\ngenerate and record all possible proofs for each provable fact f\n6 With negation, the deﬁnition of critical sentence becomes more\ncomplex because the the theory is non-monotonic (i.e., removing a\nsentence may cause a fact to become true). Hence, we omit theories\nwith negation for this analysis.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3885\nOriginal Remove Remove Remove\nIrrelevant Critical Any\nAccuracy (test) 99.4 99.6 81.2 96.3\nTable 2: Accuracy on the DMax (no negation) subset, and all its\n(113k) perturbed (one context sentence removed) variants. The over-\nall accuracy (Remove Any, last column) is largely unchanged, but\nwith a drop for the subset where a critical sentence was removed.\nOriginal predictions for true (positive) facts:\nT F\nNew T 3895 (should have ﬂipped) 10 (incorrectly ﬂips)\nPred. F 16654 (correct ﬂips) 187 (becomes correct)\nTable 3: On the true questions that were originally answered cor-\nrectly (column 1), the predicted T answer should ﬂip to predicted\nF when a critical sentence is removed. In practice, we observe this\nhappens 81% of the time (16654/(16654+3895)).\ndiction from T to F, while removing a noncritical sentence\nshould leave the prediction unchanged as T. We also measure\noverall performance on the entire dataset of questions with\nperturbed theories.\nThe results are shown in Tables 2 and 3. We observe:\n1. The overall accuracy is largely unchangedon the full\ncollection of questions with perturbed theories, suggest-\ning robustness to these variants (last column, Table 2).\n2. For the (20k) questions where the prediction is expected\nto ﬂip from true to false, we see this ﬂip occurs 81% of\nthe time, Table 3. This suggestsmoderate robustnessto\nthis speciﬁc type of perturbation, although notably less\nthan for a formal theorem prover (that would make this\nﬂip 100% of the time). For the remaining (93k) ques-\ntions, the prediction (correctly) stays true over 99% of\nthe time (no Table).\n4.3 Performance on Hand-Authored Problems\nTo further test robustness and out-of-distribution perfor-\nmance, we test the trained models on two hand-authored\nreasoning problems, both including reasoning with negation,\nwritten independently of our datasets. Note that these new\ndatasets are used purely as test sets (no training on them,\ni.e., zero-shot performance); their vocabulary of entities, at-\ntributes, and predicates (except for is()) are all new to the\nmodels at test time. The two test datasets are as follows:\nBirds. The “birds” rulebase is a well-known logic problem\nillustrating the use of “abnormality” predicates [McCarthy,\n1984]. We entered Sergot’s formulation of it 7 verbatim (bar\nsyntax), and generated a series of test questions using the\nsame procedure as earlier. Figure 4 illustrates the problem (in\nrestricted English, exactly as presented to our model) and four\nexample questions. We created two linguistic expressions of\nthe formal theory, Birds1 and Birds2. Birds2 is shown in Fig-\nure 4, while Birds1 is identical except “can/cannot ﬂy” is re-\nplaced with “is/is not ﬂying” to make the negation (“not”)\nmore explicit (this turns out not to matter). Questions require\nreasoning up to depth 1.\n7https://www.doc.ic.ac.uk/\u0018mjs/teaching/KnowledgeRep491/\nExtendedLP 491-2x1.pdf, p5\nIf someone is a bird and not abnormal then they can ﬂy.\nIf someone is an ostrich then they are a bird.\nIf someone is an ostrich then they are abnormal.\nIf someone is an ostrich then they cannot ﬂy.\nIf someone is a bird and wounded then they are abnormal.\nIf someone is wounded then they cannot ﬂy.\nArthur is a bird. Arthur is not wounded. Bill is an ostrich.\nColin is a bird. Colin is wounded.\nDave is not an ostrich. Dave is wounded.\nQ1.Arthur can ﬂy. True/false?[T] Q2.Bill can ﬂy. True/false?[F]\nQ3.Colin can ﬂy. True/false?[F] Q4.Dave can ﬂy. True/false?[F]\nFigure 4: Sergot’s “birds” puzzle includes reasoning about abnor-\nmality predicates. The dataset contains these and other questions\nabout the single theory.\nThe circuit has a switch.\nThe switch is on.\nThe circuit has a light bulb.\nIf a circuit has a switch and the switch is on\nthen the circuit is complete.\nIf a circuit does not have a switch then the circuit is complete.\nIf a circuit is complete then a current runs through the circuit.\nIf a current runs through a circuit and the circuit has a light bulb\nthen the light bulb is glowing.\nIf a current runs through a circuit and the circuit has a bell\nthen the bell is ringing.\nIf a current runs through a circuit and the circuit has a radio\nthen the radio is playing.\nQ1. The circuit is not complete. True/false? [F]\nQ2. The light bulb is glowing. True/false? [T]\nQ3. The radio is playing. True/false? [F]\nFigure 5: The simple Electricity2 rulebase, an example circuit, and 3\nquestions about the circuit. (Circuit diagram is for illustration only).\nElectricity. We also created a small rulebase about an elec-\ntrical circuit, describing the conditions for an appliance to\nfunction. We created 4 variants of increasing complexity,\ncontaining 5, 6, 11, and 12 rules respectively. For each rule-\nbase, we generate different scenarios (the facts) by randomly\nselecting from possible ground facts. Questions are then gen-\nerated against each scenario using the same procedure as ear-\nlier, resulting in 4 test sets. Figure 5 shows the Electric-\nity2 rulebase with an example scenario plus three questions.\nQuestions against the four rulebases require inference up to\ndepth 2, 3, 3, and 4 respectively.\nResults\nThe results are in Table 4, tested using the earlier trained mod-\nels. Note that these new problems and vocabularies were un-\nseen during training (i.e., are zero-shot). We observe:\n1. The “birds” problems are solved (almost) perfectlyby\nall but the non-reasoning (Mod0) model (MMax gets one\nquestion wrong on Birds1).\n2. The MMax model (trained on DMax) solves all but one\nof these datasets with 90%+ scores.\nThese are two point demonstrations that the trained models\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3886\nTable 4: Accuracy of the earlier models tested on hand-crafted rule-\nbases (zero shot, no ﬁne-tuning). Note that the models were only\ntrained on the earlier datasets (e.g., Figures 1 and 3), and thus the\nnew rulebases’ entities, attributes, and predicates (baris()) are com-\npletely unseen until test time.\ncan be used to solve novel reasoning problems with high reli-\nability (90%+ in all but one case).\nWe see one surprising anomaly also: the models trained\nwith deeper reasoning depths do slightly worse on Electric-\nity4 than the depth 1 model, Mod1. From investigation, we\nﬁnd almost all failing questions at higher depths are those\nwhere the queried fact f is an unsatisﬁed rule conclusion\n(hence should be false), in particular when the ﬁrst argument\nof f is not the ﬁrst argument of one of the rule’s conditions.\nBecause of the way the original dataset was generated, exam-\nples similar to this are very rare in the training data, possibly\ncausing this anomaly. More generally this illustrates that even\nwhen trained on a diversity of problems, the trained model\ncan have unanticipated blind spots.\n4.4 Reasoning with Paraphrased Rules\nOur experiments so far have been with synthetic language,\nbut our ultimate goal is to reason over full natural language.\nTo test transfer to more natural linguistic forms, we generated\na new dataset of 40k examples, using crowdworkers to para-\nphrase our theories. Of course, this only tests robustness to\nparaphrasing, not to abitrary natural language. Nevertheless,\nit is a small ﬁrst step in this direction.\nTo generate our data, we follow a similar approach to\n[Sinha et al., 2019]. For this experiment, we used Type 1\ntheories without negation, i.e., the same form as in Figure 1.\nDataset Generation\nTo generate the new dataset, called ParaRules, we ﬁrst gener-\nated a novel collection of 10k theories (facts+rules) expressed\nin synthetic language, as before, then extracted the “fact\ngroups” and rules from each. A “fact group” is all the facts in\na theory about a particular person, e.g., (from Figure 1) “Alan\nis blue. Alan is rough. Alan is young.”, while a rule is just the\noriginal “If...then...” sentence. We then asked crowdworkers\nto creatively re-express the fact-groups and rules, shown to\nthem in English, in their own words. For example, the earlier\nfact-group might be rewritten as: “Alan is on the young side,\nbut rough. He often feels rather blue.”. Rewritten fact-groups\nwere then turned into templates by variabilizing the person\nname. Turkers also rephrased each rule (no variabilization\nneeded). Rephrasings were automatically checked to make\nsure that all the key attributes were mentioned (and no others\nincluded), and rejected otherwise.\nAlan, who is round, red, kind, and also green, tends to be rather\nblue. In the snow sits Bob, crying from being cold. Charlie has\ngreen teeth and rough skin. People also notice his blue eyes.\nA quite nice person who is red and green is also big.\nAny big, kind person that turns red is cold to the touch.\nYoung, kind people have a habit of being nice.\nA kind person will certainly be young.\nQ1. Dave is nice. True/false? [F]\nQ2. Charlie is big. True/false? [F]\nQ3. Alan is nice. True/false? [T]\nFigure 6: A paraphrased theory in the ParaRules dataset. The rea-\nsoning for the true answer here is: Alan is kind (given), therefore\nyoung (rule4), therefore nice (rule3).\nTable 5: Accuracy with rules paraphrased into more natural lan-\nguage (ParaRules), without ﬁne-tuning (zero shot) and with (last\ncolumn only). The strongest zero-shot model (MMax) partially\nsolves (66.6%) this problem zero-shot, with strongest performance\nfor depth 0 and 1 inferences.\nWe use these to assemble the new ParaRules dataset of 40k\nquestions against \u00192k theories expressed in the paraphrased\nlanguage. To build each theory, facts were collected by ran-\ndomly sampling and instantiating fact-group templates with\npeople’s names, and rules were randomly sampled. An ex-\nample is shown in Figure 6. The train, dev, and test sets were\ngenerated using different partitions of the templates, to ensure\nthat no templates were shared between partitions.\nAs we kept track of the corresponding logic underlying\neach fact group and rule, we can then generate questions as\nbefore: Exhaustively forward-chain on the (logic version of)\nthe theory, discard if a contradiction is hit or reasoning is of\ninsufﬁcient depth (we require at least depth 3 reasoning), and\nthen for each depth select inferred and non-inferred facts as\ntrue/false questions as before.\nResults\nWe ran the earlier trained models on the ParaRules test par-\ntition (no ﬁne-tuning, i.e., zero shot). The results are shown\nin Table 5. The strongest model, MMax, partially solves this\ndataset with a score of 66.6%, higher for questions requiring\nless inference, and lower for questions requiring more infer-\nence. (The below-random scores for D=0 reﬂect the same ar-\ntifact as earlier, namely predicting everything as false except\nfor facts explicitly given. See Footnote 4).\nNote that these results are for zero-shot, with no model ex-\nposure to the paraphrased data during training. In contrast,\nwe also trained a model usingboth of the D\u00143 and ParaRules\ntraining partitions. The resulting model (last column Table 5)\nhas an accuracy of 98.8% on ParaRules test (even though the\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3887\nFigure 7: In this (abbreviated) example, the model has correctly\nidentiﬁed the sentences critical to the answer (shown in green). Per-\nfect identiﬁcation occurs for over 70% of the provable answers (See\nFigure 8 for a full histogram).\n0.0 0.2 0.4 0.6 0.8 1.00\n1000\n2000\n3000\n4000Figure 8: Counts of the F1 scores for predicting which sentences are\ncritical to the proofs of questions in DMax (test, no negation subset).\nFor over 70% of the questions, the model predicts critical sentences\nperfectly (F1=1.0), with high F1 in the remaining case.\nParaRules test rewordings are distinct from train and dev),\nshowing near-perfect performance is learnable. Although a\nlimited study, this suggests that our ﬁndings may extend to\nrulebases expressed in more natural language.\n4.5 Generating Explanations\nIn Section 4.2, we tested (for the no-negation theories)\nwhether removing a theory sentence si caused the prediction\nfor a true fact f to ﬂip to false, and found that sentences caus-\ning a ﬂip were very often (98%) part of the original proof of\nf (i.e., critical sentences), while sentences that did not were\nnot (97%). Using that data about which removed sentences\ncaused a ﬂip, we can build a map of the theory paragraph\nshowing which sentences the model considers critical to a\nconclusion, a potentially ﬁrst step to providing an explana-\ntion for the model’s answers (see Figure 7).\nWe can quantify this “explanatory” performance by mea-\nsuring the per-proof scores of predicted vs. actual critical\nsentences for each question, measuring the precision, recall,\nand F1 scores for each question in turn. The (macro)average\nP/R/F1 scores are P=98.7, R=86.9, and F1=92.4, suggesting\na high degree of reliability in predicting sentences critical to\na proof. (This is essentially an alternative view on the earlier\nrobustness data, viewed from a per-proof perspective). A his-\ntogram of the F1 scores is shown in Figure 8, indicating per-\nfect critical sentence identiﬁcation for over 70% of the ques-\ntions, and high F1 for the remaining questions. This suggests\nthe model has some knowledge of the dependencies between\nthe context sentences and a particular conclusion.\n4.6 Other Architectures\nTo what extent are our results speciﬁc to RoBERTa? To ex-\nplore this, we also trained BERT and ESIM (an LSTM-based\nmodel for natural language inference) [Chen et al., 2017] on\nour datasets. As a sanity check we also ran the decomposable\nattention model (DECOMP) on our data[Parikh et al., 2016].\nThe results are shown in Table 6.\nWe observe that the strongest BERT model trained up to\ndepth 3 (Mod3) masters the dataset that includes higher in-\nference depths (DMax) with 95%+ accuracy, while ESIM’s\nTable 6: Transformers (RoBERTa,BERT) are sufﬁcient but not\nstrictly necessary for this task, although other architectures (ESIM)\ndo not score as well.\nscores are lower (\u001980%). Note that unlike RoBERTa and\nBERT, ESIM was not pre-trained on large amounts of text,\nperhaps contributing to its lower scores. This suggests that\nour results are not speciﬁc to RoBERTa or transformers, al-\nthough transformers seem to learn the tasks more easily. As\nexpected, DECOMP does not do well (random score is 50%),\nsuggesting the datasets are not trivially solvable.\nFinally, to explore the role of pretraining, we generated a\nversion of the D\u00143 dataset in which every word was (sys-\ntematically) replaced by a random word, so that there was\nno grammaticality in the theories. After training, RoBERTa\nscores 83.3% on the test partition, substantially below the\noriginal 99.3%, suggests that pretrained knowledge is play-\ning an important role.\n5 Discussion and Future Work\nAlthough our demonstrations have been in a limited setting,\nthe implications of being able to predictably reason with lan-\nguage are signiﬁcant. With further advances, we may poten-\ntially be able to:\n\u000fauthor theories in English (e.g., Figure 5), thus sidestep-\nping the intricacies of formal languages and offering\nnew opportunities for easy creation and maintenance of\nknowledge.\n\u000fhave the machine apply general knowledge, e.g., from\nWikipedia, to explainably solve novel problems\n\u000fteach our AI when it makes a mistake, by providing\nthe missing facts and/or correcting the erroneous ones\nit used (“instructable systems”).\n\u000freason about counterfactual situations. For example, we\nmight describe a world in which plastic is a type of\nmetal, and see how the conductivity of objects change.\nThis useful capability has previously been out of scope\nfor transformers.\nOur RuleTaker models demonstrate these capabilities in a\nnarrow setting. We now discuss additional steps needed to\nachieve these goals more broadly.\n5.1 Extending The Theory Language\nWhile we have shown that transformers can emulate a form\nof deductive reasoning, our demonstrations have been with\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3888\nsmall theory sizes (< 20 facts, < 10 rules), small domains (<\n100 possible ground facts), and with a limited rule language\n(at most one variable that is universally quantiﬁed over). Ex-\npanding the expressiveness of the rule language would en-\nhance the model’s utility. For example, we have not yet ex-\nplored using multi-variable rules such as “If a person’s father\nis a second person, and the second person’s father is a third\nperson, then the ﬁrst person’s grandfather is the third person,”\nlimiting what can be stated (e.g., rules of transitivity). Sim-\nilarly there are other forms of reasoning we would like to\ntrain the model to handle, e.g., taxonomic inheritance, rea-\nsoning with disjunctive conclusions, and handling functional\nrelations (“A country has exactly one capital”). This again\nrequires characterizing the semantics of such statements, and\ngenerating training data showing the valid conclusions.\nMore generally, there are many natural language state-\nments whose formal meaning is less clear (e.g., “Most birds\nﬂy”, “It often rains in Seattle in winter.”). To apply our\nmethodology to statements with more complex semantics\nwould require new training data, either synthesized from a\nricher formal representation and model of inference, 8 or col-\nlected from people.\n5.2 Generating Training Data\nWe assume that our synthetic training data is sufﬁciently rep-\nresentative of the real problems that the model will eventually\nbe used for. However, it is possible that the generation pro-\ncedure under-represents or misses some important types of\ntheory, potentially giving the model a “blind spot” on novel\nproblems if it is unable to fully generalize. (A minor example\nof this was the MMax results on Electricity4, last paragraph\nof Section 4.3). It would be valuable to ﬁnd ways to charac-\nterize the different types of inference problems in the space,\nand design training curricula to ensure they are systematically\ncovered and/or the model is able to generalize to them. Ad-\nversarial approaches to generation, where the generator learns\nto create theories that are hard for a partially trained model,\nmay be useful in this context, e.g., [Kalyan et al., 2019].\n5.3 Natural Language Inference (NLI)\nWe have shown that transformers can perform deductive in-\nference over English statements. However, human reasoning\nover language - natural language inference (NLI) - is not al-\nways deductive. In particular, NLI allows forunsupported in-\nferences that “a person would typically infer” [Dagan et al.,\n2013], while we have used a precise model of inference in\nwhich all of a rule’s conditions need to be proven true in or-\nder for the conclusion to follow. Our model may still be quite\nfar from that required for fully natural reasoning over lan-\nguage. For example, we would like our model to still proceed\nif there are gaps in the explicitly provided knowledge, pro-\nviding the missing knowledge is “obvious” (and not contra-\ndicted by the explicitly provided facts), perhaps by leveraging\nits pretrained knowledge. Similarly, our model’s treatment of\nnegation as failure (NAF) sometimes clashes with intuitions\nabout NLI, for example given (just) “If my car does not have\n8 If one even exists - formal reasoning is still far from modeling\nall of natural language inference.\ngas then it is not working.” our model will conclude (given\nnothing else) that “My car is not working.” as it cannotprove\nthat “My car has gas.”.\nThis raises a fundamental tension about the nature of the\nreasoning we ultimately desire: We want reasoning to be\nrigorous (conclusions justiﬁed by the information provided),\nbut also “soft” (tolerant of phrasing differences and common-\nsense knowledge gaps), and strictly speaking these two goals\nare in conﬂict. Our experiments with Turk-authored language\nillustrates tolerance of phrasing differences, which we view\nas desirable, although in a strict deductive sense it is unjusti-\nﬁed to conclude (say) “A person is green” from “Charlie has\ngreen teeth” (Figure 6). Similarly we would like the model to\ntolerate minor, unstated taxonomic gaps, for example given\n“Buildings have roofs” conclude “My house has a roof”, even\nif “Houses are buildings” is not explicitly stated (butnot con-\nclude that result if it is explicitly stated that “Houses are not\nbuildings”). Characterizing which inferences should be de-\nductive vs. which can be assumed in NLI, and training a\nmodel to combine explicitly stated knowledge with implicit\n(pretrained) knowledge, remain signiﬁcant open challenges.\n6 Conclusion\nJust as McCarthy advocated 60 years ago for machines rea-\nsoning (“taking advice”) in logic, we have shown (in a re-\nstricted setting) that machines can by trained to reason over\nlanguage. While we have assumed a particular semantics of\ninference, the methodology we have used is general: Charac-\nterize the desired behavior in a formal way, synthesize exam-\nples, generate linguistic equivalents, and train a model. The\nresult, at least within our experiments, appears to be both nat-\nural and robust, in a way distinct from working with the orig-\ninal formalization.\nThe ability to reason (or emulate reasoning) over rules\nexpressed in language has potentially far-reaching implica-\ntions. For example, rules might be easily authored by a per-\nson, sidestepping some of the intricacies of a formal lan-\nguage (a simple kind of “programming in English”); or they\ncould be retrieved from natural sources (e.g., science texts,\nWikipedia). Similarly, if the answer is wrong, the user\nmay be able to directly teach the system by providing gen-\neral missing knowledge (or correcting erroneous knowledge)\nthat can then also be used for new problems - a step to-\nwards instructable algorithms. Finally, the mechanism opens\nthe door to neural counterfactual reasoning. For example,\nwe can modify the earlier “birds” rulebase to describe a\nworld in which birds typically don’t ﬂy, but where ostriches\ncan ﬂy, and see the consequences. To encourage further\nprogress, an interactive demo and all our datasets are avail-\nable at https://allenai.org/data/ruletaker\nAcknowledgements\nThanks to Chitta Baral, Jonathan Berant, Oren Etzioni, Matt\nGardner, Ashish Sabharwal, and Alon Talmor for comments\non earlier drafts.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3889\nReferences\n[Abboud et al., 2020] R. Abboud, I. Ceylan, and T. Luk-\nasiewicz. Learning to reason: Leveraging neural networks\nfor approximate dnf counting. In AAAI, 2020.\n[Abdelaziz et al., 2020] Ibrahim Abdelaziz, Veronika Thost,\nMaxwell Crouse, and Achille Fokoue. An experimental\nstudy of formula embeddings for automated theorem prov-\ning in ﬁrst-order logic. arXiv, 2002.00423, 2020.\n[Apt et al., 1988] K. Apt, H. Blair, and A. Walker. Towards\na theory of declarative knowledge. In Foundations of De-\nductive Databases and Logic Programming., 1988.\n[Bidoit and Froidevaux, 1991] N. Bidoit and C. Froidevaux.\nGeneral logical databases and programs: Default logic se-\nmantics and stratiﬁcation. Inf. Comput., 91:15–54, 1991.\n[Chen et al., 2017] Qian Chen, Xiao-Dan Zhu, Zhen-Hua\nLing, Si Wei, Hui Jiang, and Diana Inkpen. Enhanced lstm\nfor natural language inference. In ACL, 2017.\n[Dagan et al., 2013] Ido Dagan, Dan Roth, Mark Sammons,\nand Fabio Zanzotto. Recognizing Textual Entailment:\nModels and Applications. Morgan and Claypool, 2013.\n[He and Choi, 2019] Han He and Jinho D. Choi. Establish-\ning strong baselines for the new decade: Sequence tag-\nging, syntactic and semantic parsing with bert. ArXiv,\nabs/1908.04943, 2019.\n[Kalyan et al., 2019] Ashwin Kalyan, Oleksandr Polozov,\nand Adam Kalai. Adaptive generation of program-\nming puzzles. Technical report, Georgia Tech, 2019.\n(https://openreview.net/forum?id=HJeRveHKDH).\n[Kamath and Das, 2019] Aishwarya Kamath and Rajarshi\nDas. A survey on semantic parsing. In AKBC’19, 2019.\n[Lai et al., 2017] G. Lai, Q. Xie, H. Liu, Y . Yang, and\nE. Hovy. RACE: Large-scale reading comprehension\ndataset from examinations. In EMNLP, 2017.\n[Lample and Charton, 2019] G. Lample and F. Charton.\nDeep learning for symbolic mathematics. In ICLR, 2019.\n[Lin et al., 2019] Kevin Lin, Oyvind Tafjord, Peter Clark,\nand Matt Gardner. Reasoning over paragraph effects in\nsituations. In Proc. MRQA Workshop (EMNLP’19), 2019.\nalso arXiv:1908.05852.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: a robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 2019.\n[MacCartney and Manning, 2014] Bill MacCartney and\nChris Manning. Natural logic and natural language\ninference. Computing Meaning, 47:129–147, 2014.\n[Manning and MacCartney, 2009] Christopher D. Manning\nand Bill MacCartney. Natural language inference. Stan-\nford University, 2009.\n[McCarthy, 1959] John W. McCarthy. Programs with com-\nmon sense. In Proc. Tedding Conf. on the Mechanization\nof Thought Processes, pages 75–91, 1959.\n[McCarthy, 1984] J. McCarthy. Applications of circumscrip-\ntion to formalizing commonsense. In NMR, 1984.\n[Metaxiotis et al., 2002] Kostas S Metaxiotis, Dimitris Ask-\nounis, and John Psarras. Expert systems in production\nplanning and scheduling: A state-of-the-art survey. Jour-\nnal of Intelligent Manufacturing, 13(4):253–260, 2002.\n[Musen and Van der Lei, 1988] Mark A Musen and Johan\nVan der Lei. Of brittleness and bottlenecks: Challenges\nin the creation of pattern-recognition and expert-system\nmodels. In Machine Intelligence and Pattern Recognition,\nvolume 7, pages 335–352. Elsevier, 1988.\n[Newell and Simon, 1956] A. Newell and H. Simon. The\nlogic theory machine-a complex information processing\nsystem. IRE Trans. Information Theory, 2:61–79, 1956.\n[Parikh et al., 2016] Ankur Parikh, Oscar T¨ackstr¨om, Dipan-\njan Das, and Jakob Uszkoreit. A decomposable attention\nmodel for natural language inference. In EMNLP, 2016.\n[Rajpurkar et al., 2016] P. Rajpurkar, J. Zhang, K. Lopyrev,\nand P. Liang. SQuAD: 100,000+ questions for machine\ncomprehension of text. In EMNLP, 2016.\n[Richardson and Sabharwal, 2019] Kyle Richardson and\nAshish Sabharwal. What does my qa model know? de-\nvising controlled probes using expert knowledge. ArXiv,\nabs/1912.13337, 2019.\n[Richardson et al., 2020] Kyle Richardson, Hai Hu,\nLawrence S Moss, and Ashish Sabharwal. Probing\nnatural language inference models through semantic\nfragments. In AAAI’20, 2020.\n[Saxton et al., 2019] David Saxton, Edward Grefenstette,\nFelix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. In ICLR, 2019.\n[Selsam et al., 2019] Daniel Selsam, Matthew Lamm,\nBenedikt B ¨unz, Percy Liang, Leonardo de Moura, and\nDavid L. Dill. Learning a SAT solver from single-bit\nsupervision. In ICLR, 2019.\n[Sinha et al., 2019] K. Sinha, S. Sodhani, J. Dong, J. Pineau,\nand W. Hamilton. CLUTRR: a diagnostic benchmark for\ninductive reasoning from text. In EMNLP, 2019.\n[Tafjord et al., 2019] Oyvind Tafjord, Matt Gardner, Kevin\nLin, and Peter Clark. Quartz: An open-domain dataset of\nqualitative relationship questions. In EMNLP, 2019.\n[Talmor et al., 2019] A. Talmor, Y . Elazar, Y . Goldberg, and\nJ. Berant. oLMpics - on what language model pre-training\ncaptures. ArXiv, abs/1912.13283, 2019.\n[Wang et al., 2019] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li,\nD. Wong, and L. Chao. Learning deep transformer models\nfor machine translation. In ACL, 2019.\n[Weber et al., 2019] Leon Weber, Pasquale Minervini,\nJannes M ¨unchmeyer, Ulf Leser, and Tim Rockt ¨aschel.\nNlprolog: Reasoning with weak uniﬁcation for question\nanswering in natural language. In ACL, 2019.\n[Weston et al., 2016] J. Weston, A. Bordes, S. Chopra, and\nT. Mikolov. Towards AI-Complete question answering: A\nset of prerequisite toy tasks. In ICLR, 2016.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3890",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7539524435997009
    },
    {
      "name": "Counterfactual thinking",
      "score": 0.6525524258613586
    },
    {
      "name": "Natural language",
      "score": 0.6114497184753418
    },
    {
      "name": "Transformer",
      "score": 0.5865216851234436
    },
    {
      "name": "Forward chaining",
      "score": 0.519076406955719
    },
    {
      "name": "Question answering",
      "score": 0.5101625919342041
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49409249424934387
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.48652806878089905
    },
    {
      "name": "Natural language understanding",
      "score": 0.477181077003479
    },
    {
      "name": "Probabilistic logic",
      "score": 0.4480288326740265
    },
    {
      "name": "Natural language processing",
      "score": 0.40030765533447266
    },
    {
      "name": "Expert system",
      "score": 0.27703213691711426
    },
    {
      "name": "Psychology",
      "score": 0.11540943384170532
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ],
  "cited_by": 16
}