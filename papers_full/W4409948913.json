{
    "title": "Quantifying Gender Bias in Large Language Models Using Information-Theoretic and Statistical Analysis",
    "url": "https://openalex.org/W4409948913",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2223739356",
            "name": "Imran Mirza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5101499071",
            "name": "Akbar Anbar Jafari",
            "affiliations": [
                "University of Tartu"
            ]
        },
        {
            "id": "https://openalex.org/A2168245208",
            "name": "Cagri Ozcinar",
            "affiliations": [
                "University of Tartu"
            ]
        },
        {
            "id": "https://openalex.org/A2528826811",
            "name": "Gholamreza Anbarjafari",
            "affiliations": [
                "Estonian Business School"
            ]
        },
        {
            "id": "https://openalex.org/A2223739356",
            "name": "Imran Mirza",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5101499071",
            "name": "Akbar Anbar Jafari",
            "affiliations": [
                "University of Tartu"
            ]
        },
        {
            "id": "https://openalex.org/A2168245208",
            "name": "Cagri Ozcinar",
            "affiliations": [
                "University of Tartu"
            ]
        },
        {
            "id": "https://openalex.org/A2528826811",
            "name": "Gholamreza Anbarjafari",
            "affiliations": [
                "Estonian Business School"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4396788662",
        "https://openalex.org/W4401386758",
        "https://openalex.org/W3021159925",
        "https://openalex.org/W4407690577",
        "https://openalex.org/W4402372001",
        "https://openalex.org/W4396667188",
        "https://openalex.org/W4400585758",
        "https://openalex.org/W4280599741",
        "https://openalex.org/W4229367922",
        "https://openalex.org/W1995875735",
        "https://openalex.org/W3207316473",
        "https://openalex.org/W4390355332",
        "https://openalex.org/W4285210452",
        "https://openalex.org/W4394947904",
        "https://openalex.org/W6859477064",
        "https://openalex.org/W4393213892",
        "https://openalex.org/W4389519898",
        "https://openalex.org/W4402670268",
        "https://openalex.org/W4385574250",
        "https://openalex.org/W6852874933",
        "https://openalex.org/W4387994739",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6803757056",
        "https://openalex.org/W4398186377",
        "https://openalex.org/W4406458080",
        "https://openalex.org/W6869753673",
        "https://openalex.org/W4403488230",
        "https://openalex.org/W4401857670",
        "https://openalex.org/W3212244639",
        "https://openalex.org/W4380353763"
    ],
    "abstract": "Large language models (LLMs) have revolutionized natural language processing across diverse domains, yet they also raise critical fairness and ethical concerns, particularly regarding gender bias. In this study, we conduct a systematic, mathematically grounded investigation of gender bias in four leading LLMs—GPT-4o, Gemini 1.5 Pro, Sonnet 3.5, and LLaMA 3.1:8b—by evaluating the gender distributions produced when generating “perfect personas” for a wide range of occupational roles spanning healthcare, engineering, and professional services. Leveraging standardized prompts, controlled experimental settings, and repeated trials, our methodology quantifies bias against an ideal uniform distribution using rigorous statistical measures and information-theoretic metrics. Our results reveal marked discrepancies: GPT-4o exhibits pronounced occupational gender segregation, disproportionately linking healthcare roles to female identities while assigning male labels to engineering and physically demanding positions. In contrast, Gemini 1.5 Pro, Sonnet 3.5, and LLaMA 3.1:8b predominantly favor female assignments, albeit with less job-specific precision. These findings demonstrate how architectural decisions, training data composition, and token embedding strategies critically influence gender representation. The study underscores the urgent need for inclusive datasets, advanced bias-mitigation techniques, and continuous model audits to develop AI systems that are not only free from stereotype perpetuation but actively promote equitable and representative information processing.",
    "full_text": null
}