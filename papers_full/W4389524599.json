{
  "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
  "url": "https://openalex.org/W4389524599",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097808136",
      "name": "Tao Lei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2109999465",
      "name": "Michiel de Jong",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2124070775",
      "name": "Santiago Ontañón",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2036183448",
      "name": "Siddhartha Brahma",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3156685601",
      "name": "Yury Zemlyanskiy",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2322051720",
      "name": "David Uthus",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2885677207",
      "name": "Mandy Guo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5002725719",
      "name": "James Lee-Thorp",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4224699826",
      "name": "Yun-Hsuan Sung",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A632247528",
      "name": "Sumit Sanghai",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153603860",
    "https://openalex.org/W4280652569",
    "https://openalex.org/W4385570209",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285595056",
    "https://openalex.org/W4308760184",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4308163580",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4225390052",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W2962865973",
    "https://openalex.org/W3175515348",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4288028629",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W3171494313",
    "https://openalex.org/W3207796810",
    "https://openalex.org/W4226441939",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3201977280",
    "https://openalex.org/W4309955236",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W4365441218",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4385573804",
    "https://openalex.org/W4293718192"
  ],
  "abstract": "Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, Sumit Sanghai. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5085–5100\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCOLT5: Faster Long-Range Transformers with Conditional Computation\nJoshua Ainslie∗, Tao Lei, Michiel de Jong, Santiago Ontañón\nSiddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo\nJames Lee-Thorp, Yi Tay, Yun-Hsuan Sung, Sumit Sanghai\nGoogle Research\nAbstract\nMany natural language processing tasks bene-\nﬁt from long inputs, but processing long doc-\numents with Transformers is expensive -- not\nonly due to quadratic attention complexity but\nalso from applying feedforward and projec-\ntion layers to every token. However, not all\ntokens are equally important, especially for\nlonger documents. We propose C OLT5, a\nlong-input Transformer model that builds on\nthis intuition by employing conditional compu-\ntation, devoting more resources to important\ntokens in both feedforward and attention lay-\ners. We show that C OLT5 achieves stronger\nperformance than L ONG T5 with much faster\ntraining and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover,\nCOLT5 can effectively and tractably make use\nof extremely long inputs, showing strong gains\nup to 64k input length.\n1 Introduction\nMany natural language processing tasks, such as\nsummarization (Cohan et al., 2018) or question an-\nswering over long documents (Joshi et al., 2017),\nrequire machine learning models to encode long-\nform text. Processing long documents with a Trans-\nformer model is computationally expensive, both\nbecause attention cost scales quadratically with in-\nput length and because feedforward and attention\nprojection layers have to be applied to each input\ntoken.\nOver the past few years, many “efﬁcient Trans-\nformer” approaches have been proposed that re-\nduce the cost of the attention mechanism over long\ninputs (Child et al., 2019; Ainslie et al., 2020; Belt-\nagy et al., 2020; Zaheer et al., 2020; Wang et al.,\n2020; Tay et al., 2021; Guo et al., 2022). However,\nespecially for larger models, the feedforward and\nprojection layers actually make up the majority of\n∗Author contributions are outlined in Appendix A. Corre-\nspondence author: jainslie@google.com.\nFigure 1: An overview of a C OLT5 Transformer layer\nwith conditional computation. All tokens are processed\nby light attention and MLP layers, whileqrouted query\ntokens perform heavier attention over v routed key-\nvalue tokens and m routed tokens are processed by a\nheavier MLP.\nthe computational burden and can render process-\ning long inputs intractable.\nThis paper presents COLT5 (Conditional\nLongT5), a new family of models that, building on\ntop of LONG T5 (Guo et al., 2022), enables fast pro-\ncessing of long inputs by combining architecture\nimprovements for both attention and feedforward\nlayers. COLT5 is based on the intuition that some\ntokens are more important than others, and we can\nachieve better quality for lower cost by devoting\nmore computation to important tokens. Moreover,\nthe fraction of important tokens is likely to dimin-\nish with document length, allowing for tractable\nprocessing of long documents.\nIn particular, COLT5 divides each feedforward\nlayer and each attention layer into a light branch\n5085\n200 400 600 800\n42\n44\n46\nB\nL\nXL\nB\nL\nXL\nTime per sample (ms)\nAverage perf\nInference\n0 500 1,000 1,500 2,000 2,500\n42\n44\n46\nB\nL\nXL\nB\nL\nXL\nTime per sample (ms)\nFine-tuning\nLONG T5 COLT5\nFigure 2: COLT5 achieves stronger performance than L ONG T5 at any speed. Average performance on all\ndatasets as a function of inference and ﬁne-tuning time per sample (ms) for L ONG T5 and C OLT5 Base, Large,\nand XL models. L ONG T5 does not use MQA, but we report speed as though it had for a conservative baseline.\nwhich is applied to all tokens and a heavy branch\nwhich is applied to a set of important tokens, se-\nlected speciﬁcally for that input and component.\nThe light feedforward branch has lower hidden di-\nmension than standard LONG T5 while the heavy\nfeedforward branch has higher hidden dimension.\nThe light attention branch has fewer heads and ap-\nplies only local attention, while the heavy attention\nbranch performs full attention over another sepa-\nrately selected set of important tokens. Figure 1\nprovides an overview of the COLT5 conditional\nmechanism.\nFinally, COLT5 also includes two other mod-\niﬁcations to the LONG T5 architecture. COLT5\nadds multi-query cross-attention (Shazeer, 2019),\nsigniﬁcantly speeding up inference. COLT5 also\nemploys the UL2 (Tay et al., 2022) pre-training ob-\njective, which we demonstrate allows for in-context\nlearning over long inputs.\nWe show that COLT5 performs much faster ﬁne-\ntuning and inference with similar or better model\nquality, improving over LONG T5 on arXiv summa-\nrization (Cohan et al., 2018) and TriviaQA question\nanswering (Joshi et al., 2017) datasets and achiev-\ning SOTA on the SCROLLS benchmark (Shaham\net al., 2022). Moreover, COLT5 achieves further\ngains in quality and speed for tasks with extremely\nlong inputs (64k tokens), with less-than-linear scal-\ning of “focus” tokens.\n2 Background\nTransformer FLOPs COLT5 follows an exten-\nsive line of work in attempting to reduce the com-\nputational cost of Transformer models, particularly\nover long inputs. The computational burden of\nTransformer models has several distinct elements,\nand different approaches focus on reducing the cost\nof different components. For that reason, it is help-\nful to start by providing a breakdown of the compu-\ntational cost of Transformer components. Table 1\nshows the FLOPs1 for each component of a Trans-\nformer encoder layer (Kaplan et al., 2020).\nEncoder Layer Component Flops\nVanilla self-attention computation 2n2d\nAttention QKV and output projections 4nd2\nFeedforward layer 8nd2\nLONG T5 local attention computation 2nwd\nLONG T5 global attention computation n2\n8 d\nTable 1: Computational cost of encoder layer trans-\nformer components measured in FLOPs. nis the input\nlength, dis the model dimensionality, and wis the size\nof the local attention window.\nSparse attention The ﬁrst challenge of applying\na Transformer to a long input is that the FLOPs\nof the self-attention mechanism scales quadrati-\ncally in the input length, becoming intractable for\nlong inputs. A large body of work focuses on re-\nducing self-attention cost, restricting attention be-\ntween a subset of inputs (Child et al., 2019; Ainslie\net al., 2020; Beltagy et al., 2020; Zaheer et al.,\n2020; Wang et al., 2020; Guo et al., 2022) or to\na subset of layers (Zemlyanskiy et al., 2021). In\nLONG T5 (Guo et al., 2022), the most closely re-\nlated model to COLT5, tokens attend within a lo-\n1Each multiply-add is counted as a single FLOP.\n5086\ncal window as well as to a mean-pooled summary\nrepresentation for each block of 16 tokens in the\ninput. LONG T5 attention leads to sharply reduced\n(though still non-negligible) FLOPs (Table 1).\nConditional computation After applying a\nsparse attention mechanism, the feedforward and\nattention projection layers account for the major-\nity of the FLOPs. These costs scale with the\nlength of the input, such that processing long in-\nputs is still prohibitively expensive. A common\napproach to reduce the remaining cost is to employ\nsome form of conditional computation, avoiding\napplying all model parameters to the entire input.\nCALM (Schuster et al., 2022) applies a varying\nnumber of decoder layers to each decoded token,\noutputting a token early if the model is conﬁdent in\nits prediction. Mixture-of-Experts models (Shazeer\net al., 2017; Fedus et al., 2021; Zoph et al., 2022)\nroute inputs through a small proportion of expert\nsub-modules, bringing to bear only the parame-\nters most relevant to the input. In the context of\nretrieval-augmented models, numerous works re-\nrank retrieved passages by their relevance to the\nquery and process only the highest scoring pas-\nsages (Mao et al., 2021; Wang et al., 2018; Yu\net al., 2022) and vary the number of processed pas-\nsages depending on model conﬁdence (Kratzwald\nand Feuerriegel, 2018; Varshney et al., 2022). Con-\ncurrent work CoDA (Lei et al., 2023) employs a\nrelated conditional computation mechanism, de-\nsigned for efﬁcient adaptation rather than modeling\nlong documents.\nDevice utilization FLOPs do not tell the whole\nstory, as modeling choices can inﬂuence the effec-\ntive speed of operations achieved by accelerators.\nFor long text inputs, autoregressive decoder infer-\nence is very slow due to memory bandwidth con-\nstraints from repeatedly loading the long sequence\nof keys and values (Shazeer, 2019; de Jong et al.,\n2022). Shazeer (2019) introduces multi-query at-\ntention (MQA), sharing heads for keys and values\nto reduce memory bandwidth overhead. Pope et al.\n(2022) studies how to shard large models, espe-\ncially in the context of MQA, to obtain optimal\ndevice utilization and therefore speed.\nTraining objectives T5 introduced the span cor-\nruption objective (Raffel et al., 2020), a modi-\nﬁcation of masked language modeling (Devlin\net al., 2019). LONG T5 made use of the PEGA-\nSUS (Zhang et al., 2020) sentence reconstruc-\ntion objective for improved summarization perfor-\nmance. Tay et al. (2022) proposes UL2, a mixture\nof span corruption, preﬁx, and causal language\nmodeling, and shows that it leads to strong perfor-\nmance on both short-output and generative tasks.\n3 C OLT5\n3.1 Conditional computation\nAs discussed in the previous section, a large propor-\ntion of Transformer FLOPs arise from feedforward\nand projection layers that scale with the length of\nthe input sequence. Therefore, LONG T5 training\nand inference on long documents remains expen-\nsive.\nCOLT5 further reduces the cost of processing\nlong documents through conditional computation,\nfollowing the intuition that some tokens are more\nimportant and therefore beneﬁt more than others\nfrom heavy computation. First, some types of to-\nkens may inherently require less computation, such\nas ﬁller words and punctuation. Second, especially\nin long documents, large parts of the input may not\nbe relevant to the current question, task, or process-\ning stage.\nThe COLT5 conditional computation mecha-\nnism consists of three components: routing mod-\nules, conditional feedforward layers, and condi-\ntional attention layers. All tokens are processed by\nstandard, lightweight attention and feedforward lay-\ners. Routing modules additionally select important\ntokens from an input at each attention or feedfor-\nward layer, and a heavy conditional layer applies\nadditional computation to routed tokens. This sec-\ntion describes each component in detail. Figure 1\nprovides an overview of the COLT5 conditional\ncomputation mechanism, and Table 2 compares\nCOLT5 and LONG T5 FLOPs.\nModel Encoder Layer Flops\nT5 12nd2 + 2n2d\nLONG T5 12nd2 + n2\n8 d\nCOLT5 71\n4 nd2 + n2\n84 d\nTable 2: COLT5 uses signiﬁcantly fewer FLOPs\nthan L ONG T5. Comparison of approximate encoder\nlayer total FLOPs between T5, L ONG T5, and C OLT5.\nCOLT5 FLOPs rounded to readable fractions.\nRouting In order to separately select important\ntokens for each component in each layer, we need\n5087\na learnable and tractable routing function. We\nfollow the simple three-step mechanism from Lei\net al. (2023): (1) multiply inputs with a learned\nembedding to obtain routing scores, (2) normalize,\nand (3) select the top-khighest scoring inputs.\nLet Xi be the representation of token i, and u\na d-dimensional learnable embedding. Then the\nrouting score of token iis\nsi = Xi · u\nWe select the top-khighest scoring inputs. In order\nto provide a learning signal to the scoring embed-\nding, we make sure the contribution of the routed\ntokens to the layer update isscaled according to the\nrouting score, as will be seen later. To provide a bet-\nter distributed signal to all tokens, we also globally\nnormalize the routing scores to sum up to the num-\nber of desired routed tokens using a generalized\nsoftmax, resulting in normalized scores ˜si. Each\nCOLT5 layer has three independent routers, one\neach for the feedforward layer, attention queries,\nand attention key-values.\nConditional Feedforward Intuitively, some to-\nken representations may beneﬁt from more pro-\ncessing than others. The COLT5 conditional feed-\nforward layer applies an additional high-capacity\nfeedforward layer to selected tokens. In particular,\nlet Xi be the model state of the ith token and ˜si\ndenote the normalized routing score (set to 0 for\nnon-routed tokens). Then the feedforward update\nfor COLT5 is given by\nXi = Xi + FFdLight(Xi) + ˜si · FFdHeavy(Xi)\nThe light and heavy feedforward branches differ\nonly in their hidden dimension, with the light\nbranch having smaller hidden dimension than\nthe standard T5 feedforward layer and the heavy\nbranch larger. Let ndenote the number of input to-\nkens, mthe number of selected tokens, and rL and\nrH the ratios of light and heavy hidden dimension\nto standard T5 hidden dimension. Then the FLOPs\nof the COLT5 layer are given by\nFLOPsFFd = 8nrLd2\n  \nLight branch\n+ 8mrHd2\n  \nHeavy branch\nWe set the light and heavy ratios as rL = 1\n2 and\nrH = 4, half and quadruple the standard T5 hid-\nden dimension respectively. For our main exper-\niments, a fraction 1\n16 of tokens are routed to the\nFigure 3: An overview of the COLT5 attention pattern.\nThe light branch performs local attention for each to-\nken. In the higher capacity heavy branch q selected\nquery tokens (2 in the ﬁgure) attend to vseparately se-\nlected key and value tokens (4 in the ﬁgure).\nheavy branch. As a result the approximate FLOPs\nfrom the COLT5 feedforward layer equals\nFLOPsFFd = 4 nd2\n  \nLight branch\n+ 2 nd2\n  \nHeavy branch\nconsuming 75% of the FLOPs of a standard T5\nfeedforward layer.\nConditional Attention COLT5 conditional at-\ntention operates on the intuition that most tokens\nhave simple, local interactions, but some tokens\nbeneﬁt from heavier processing and long-range in-\nteractions. The COLT5 conditional attention layer\napplies an additional high-capacity attention layer\nthat attends from selected query tokens to selected\nkey-value tokens. Let ˜sq\ni denote the normalized\nrouting query score for token i, and ˜skv the key-\nvalue scores for all tokens (set to 0 if not routed).\nThen the attention update for COLT5 is given by\nXi = Xi +ALight(Xi,X) + ˜sq\ni ·AHeavy(Xi,˜skvX)\nThe light and heavy branches differ in the number\nof heads and tokens attended to: the light branch\nhas fewer heads and attends to a local context win-\ndow, while the heavy branch has more heads and\nattends to all routed key-value tokens. Separately\nselecting query and key-value tokens also allows\nthe model to differentiate between tokens that re-\nquire additional information and those that possess\n5088\nModel Avg Speed TQA NQA QAS QuAL CNLI arXiv SumS QMS GovR\ninf fn F1 F1 F1 EM EM R gm Rgm Rgm Rgm\nLONG T5-B 43.1 0.6 / 7.4 3.7 82.2 23.0 46.6 37.9 85.6 35.4 19.2 20.4 37.7\nCOLT5-B 42.4 11.2 6.5 82.4 23.3 42.1 36.5 86.5 35.3 18.7 18.4 37.9\nLONG T5-L 45.3 0.3 / 3.0 1.3 84.2 27.2 52.3 40.6 87.3 35.7 19.1 21.4 39.5\nCOLT5-L 45.3 5.0 2.0 84.5 27.7 49.8 39.9 88.7 35.9 20.5 21.0 39.7\nLONG T5-XL 46.6 0.2 / 1.2 0.4 85.3 29.3 53.1 46.0 88.2 35.9 19.4 21.3 40.5\nCOLT5-XL 47.4 2.3 0.5 86.1 31.1 53.9 48.1 88.4 36.1 20.0 22.5 40.5\nTable 3: Performance comparison of C OLT5 and L ONG T5 Base, Large and XL models on question-answering\ndatasets TriviaQA (TQA), NarrativeQA (NQA), QASPER (QAS), and QuALITY (QuAL), NLI dataset Con-\ntractNLI (CNLI), and summarization datasets arXiv, SummScreenFD (SumS), QMSum (QMS), and GovReport\n(GovR). SCROLLS results are on leaderboard test set where C OLT5-XL achieves SOTA. Average speed is re-\nported in samples per second for inference (inf) and ﬁne-tuning (fn). L ONG T5 does not use MQA but inference\nspeed is reported without/with MQA for conservative baseline. R gm stands for the geometric mean of ROUGE-\n1,2,L. Similar to SCROLLS, we take a simple average across all datasets even though the datasets use different\nperformance metrics.\nsuch information. Figure 3 shows the COLT5 at-\ntention pattern. Let q,v be the number of selected\nquery and key-value tokens, wthe size of the lo-\ncal attention window and rL,rH the proportion of\nlight and heavy heads relative to standard T5. Then\nthe FLOPs of the COLT5 attention layer are given\nby\nFLOPsAtt = 4 n· rLd2\n  \nLocal projection\n+ 2nw· rLd  \nLocal attention\n+ 2q· rHd2 + 2v· rHd2\n  \nGlobal projection\n+ 2qv· rHd  \nGlobal attention\nWe set the light and heavy head ratios as rL = 1\n4\nand rH = 3\n4 , keeping the total number of heads\nacross the light and heavy branches equal to stan-\ndard T5 heads. For our main experiments a fraction\n1\n16 query tokens and 1\n8 key-value tokens are routed\nto the heavy branch, so q= n\n16 and v= n\n8 . Ignor-\ning local attention computation, we approximate\nattention FLOPS by2\nFLOPsAtt ≈ nd2\n\nLocal proj.\n+ 1\n4nd2\n  \nGlobal proj.\n+ 1\n84n2d\n  \nGlobal att.\nwith less than half projection FLOPs and order-of-\nmagnitude smaller quadratic length scaling com-\npared to LONG T5. Table 2 shows total FLOPs for\nthe COLT5 layer. In general, we set q = mand\nv = 2m, and use mto summarize the number of\nrouted tokens going forward.\n2Global projection and attention FLOPs rounded to read-\nable fractions, exact values are 9\n32 and 3\n256 . Complexity as-\nsumes constant fraction of routed tokens; we show we can do\nbetter in practice for extremely long inputs.\n3.2 Multi-query Attention\nConditional computation effectively reduces the\ncomputational cost of the encoder. However, for\nencoder-decoder models with long inputs the ma-\njority of inference time is spent in the decoder due\nto memory bandwidth constraints (Shazeer, 2019;\nde Jong et al., 2022). Most of the overhead is\ncaused by repeatedly reading all the input token\nkeys and values from memory for every output to-\nken that is autoregressively decoded during cross\nattention. Multi-query attention (Shazeer, 2019)\n(MQA) allows all query heads to share a single key\nand value head, alleviating this bottleneck. Accord-\ningly, we apply MQA in cross-attention layers for\nmuch faster inference. Note however that MQA\ndoes not improve training speed since target tokens\nare processed in parallel during training, avoiding\nthis memory bandwidth bottleneck.\n3.3 UL2\nThe UL2 pre-training objective (Tay et al., 2022)\ncombines different denoising objectives, extending\nthe span corruption pre-training used in T5 to a\nvariety of noise rates / average span lengths and\nadding a preﬁx language modeling objective more\nsimilar to typical decoder-only model pre-training.\nUL2 has been shown to lead to improved in-context\nlearning. We train COLT5 on UL2 instead of PE-\nGASUS (Zhang et al., 2020), endowing COLT5\nwith in-context learning capabilities.\n4 Experiments\nIn order to evaluate COLT5, we perform the fol-\nlowing experiments: (1) our main results com-\n5089\npare COLT5 and LONG T5 on a collection of long\ninput datasets using input length of 16k tokens;\n(2) we evaluate COLT5 on extremely long in-\nputs up to 64k tokens and compare scaling against\nLONG T5; (3) demonstrate COLT5’s few-shot ca-\npability, investigating how performance changes as\ninput length and number of shots increase, (4) per-\nform a series of ablations to understand the effect\nof individual COLT5 components, and (5) inves-\ntigate empirical routing patterns. The remainder\nof the section outlines our experimental setup, and\nthen describes each of the experiments above.\n4.1 Experimental setup\nConﬁgurations COLT5 is based on the T5.1.1\narchitecture (Raffel et al., 2020), implemented\nwith JAX (Bradbury et al., 2018), Flax (Heek et al.,\n2020), and Flaxformer3. Following LONG T5, we\nexperiment with Base, Large, and XL model sizes.\nCOLT5 models use the same embedding dimen-\nsion, number of layers, and total attention heads as\ncorresponding LONG T5 models of the same size,\nwith more overall parameters (but less compute)\ndue to the conditional branch. See Appendix B for\nadditional details on model conﬁguration.\nPre-training We pre-train COLT5 for 1M steps\non the C4 dataset (Raffel et al., 2020) using a vari-\nant of the UL2 objective (Tay et al., 2022) with\nbatch size 256, input length 4096, and output length\n910. In particular, our mixture contains four objec-\ntives in equal proportion: preﬁx-LM with noise rate\n0.5, and span corruption (Raffel et al., 2020) with\nnoise rate 0.15 and average span lengths 3, 8, and\n64. We use the Adafactor optimizer (Shazeer and\nStern, 2018) with the T5.1.1 inverse square root\nlearning rate schedule and no dropout. COLT5 is\ntrained with the T5X (Roberts et al., 2022) frame-\nwork. For pre-training, we route m= 512tokens,\n1\n8 th of the input length.\nFine-tuning For ﬁne-tuning we use a constant\nlearning rate of 0.001, batch size 128, and dropout\nrate 0.1 for all tasks. Main results use input length\nof 16384 for all datasets other than ContractNLI,\nwhich uses 8192. Question answering datasets use\noutput length 128 and summarization datasets use\noutput length 512, except for GovRep which uses\noutput length 1024. We route m = 1024tokens,\n1\n16 th of the input length. We train until convergence\n3https://github.com/google/ﬂaxformer\nand select the checkpoint with the highest dev per-\nformance. We use greedy decoding for inference.\nData We evaluate COLT5 on TriviaQA (Joshi\net al., 2017), arXiv (Cohan et al., 2018),\nand the SCROLLS benchmark (Shaham et al.,\n2022). SCROLLS contains question-answering\ndatasets: NarrativeQA (Ko ˇciský et al., 2018),\nQASPER (Dasigi et al., 2021), and QuAL-\nITY (Pang et al., 2021), an NLI dataset: Con-\ntractNLI (Koreeda and Manning, 2021), and sum-\nmarization datasets: SummScreenFD (Chen et al.,\n2022), QMSum (Zhong et al., 2021), and Gov-\nReport (Huang et al., 2021). Table 4 provides\nan overview of the size and input length for each\ndataset.\nDataset Type Samples Median 90%\nTriviaQA QA 157,053 8,858 28,956\narXiv Sum 215,913 8,519 20,170\nNarrativeQA QA 71,187 57,829 176,862\nQASPER QA 5,692 5,472 8,657\nQuALITY QA 6,737 7,171 8,276\nContractNLI NLI 10,319 2,148 4,485\nSummScreen Sum 4,348 9,046 15,172\nQMSum Sum 1,810 14,197 27,761\nGovRep Sum 19,402 8,841 18,835\nTable 4: Median and 90th percentile input length by\ndataset measured in SentencePiece tokens.\nTiming We report time per sample per TPUv4\nchip, as measured by xprof (Google, 2020). For\ninference we use a single TPUv4 with batch size 16\nor the largest that ﬁts in memory. For ﬁne-tuning\nwe proﬁle with 8 TPUv4 chips, sharded separately\nfor each model to maximize throughput.\n4.2 Main results\nFigure 2 compares the quality-speed trade-off for\nLONG T54 and COLT5, showing that COLT5 is\nbetter at any speed. For 16k input length, COLT5\nmatches or exceeds LONG T5 quality for Large and\nXL with 35-75% training speedup and 50-100% in-\nference speedup on top of the order-of-magnitude\ninference speedup from MQA. Encoder speedups\nare even greater (Appendix D). COLT5-XL also\nachieves SOTA performance on the SCROLLS\nbenchmark. Table 3 contains all main results.\n5090\n100 200 300 400 500 600\n24\n26\n28\n30\n32\n8k\n16k\n32k\n8k\n16k\n32k\n64k\nTime per sample (ms)\nF1\nLONG T5\nCOLT5\nFigure 4: COLT5 effectively scales to extremely long\ninputs, achieving stronger performance and faster\nspeed than L ONG T5. F1 on NarrativeQA as a func-\ntion of inference time per sample for L ONG T5 and\nCOLT5 Large models using varying input lengths.\n4.3 Scaling to extremely long inputs\nWe hypothesize that the advantage of COLT5 over\nLONG T5 strengthens with input length, as the frac-\ntion of important tokens decreases and COLT5 can\nroute a greater proportion of important tokens to\nthe heavy branch. Figure 4 compares the quality-\nspeed trade-off for LONG T5 and COLT5 on Nar-\nrativeQA, sweeping over input length rather than\nmodel size. The number of routed tokens is 1\n16 th\nof the input length, except that we do not increase\nrouted tokens going from 32k to 64k, so at 64k\nwe route only 1\n32 nd of the input length. COLT5\nachieves both stronger performance and faster in-\nference speed at all input lengths and is able to\neffectively make use of extremely long inputs. We\nnote that COLT5 achieves large quality gains by\ngoing from 32k to 64k tokens even while keeping\nthe number of routed tokens constant, providing\nmore evidence for our hypothesis.\n4.4 In-context learning\nModels trained on the UL2 objective have shown\nstrong few-shot in-context learning (ICL) capa-\nbilities5 even at smaller sizes (Tay et al., 2022).\nCOLT5 enables tractable inference with long in-\nputs. Here, we leverage this for scaling the number\nof examples used for in-context learning.\n4Note that LONG T5 does not use MQA, but for proﬁling\nwe add MQA to LONG T5 for a conservative baseline.\n5We initially evaluated ICL for models pre-trained with\nPEGASUS but found performance to be nearly 0.\n0.1 0.2 0.3\n1k\n2k\n4k\n8k\n16k\nNaturalQ\n0.05 0.1\nTriviaQA\nFigure 5: COLT5 can use its long-input capability\nto beneﬁt from more shots for in-context learning.\nFew-shot exact match for C OLT5-Large on Natural\nQuestions and TriviaQA dev sets as a function of in-\nput tokens, ﬁtting as many examples as possible. Each\nexample contains question, context, and answer. Inputs\nlength used are 1024, 2048, 4096, 8192, 16384.\nWe test the above hypothesis by evaluating\nfew-shot learning performance on Natural Ques-\ntions (Kwiatkowski et al., 2019) and TriviaQA as\na function of input length, using as many exam-\nples as ﬁt in the context. We consider the open\nbook setting, such that each example consists of\nquestion, context document, and answer. Table 5\nshows the number of examples by input length. We\nevaluate on the full dev set, randomly sampling\nexamples from the training set for each dev sample\nuntil no further examples ﬁt in the input length. We\nfound that COLT5 can perform in-context learning\nonly up to the input length it was trained on, so\nfor these experiments we continued pre-training\na COLT5-Large model on input length 16384 for\nanother 100k steps. For the same reason we route\nm= 512tokens as in pre-training.\nFigure 5 displays COLT5 few-shot performance\nas a function of input length, showing that COLT5\nis able to apply its long-input capabilities to extract\ninformation from increasing numbers of examples.\nDataset 1024 2048 4096 8192 16384\nNQ 0.1 0.7 1.7 3.4 5.6\nTriviaQA 1.6 2.3 3.8 7.0 9.8\nTable 5: Average number of Natural Questions and\nTriviaQA few-shot examples that ﬁt in input length.\n4.5 Ablations\nThis section studies the effect of different choices\nin the COLT5 recipe. Table 6 contains results of a\nseries of experiments that change a single compo-\n5091\nAblation Model Avg Inf TQA NQA QAS QuAL CNLI arX SumS QMS GovR\nS/s F1 F1 F1 EM EM R gm Rgm Rgm Rgm\nBaseline C OLT5-B 42.5 11.2 82.4 23.1 38.3 36.6 87.8 35.3 19.3 20.5 39.4\nRouting Static 40.5 11.6 79.7 19.2 34.2 34.5 86.4 34.9 18.1 18.9 38.8\nShare QKV 42.0 11.8 82.1 21.9 37.5 36.2 87.0 35.2 18.2 20.4 39.7\nAttention v=all 42.5 9.4 82.4 22.3 38.6 37.2 87.8 35.3 19.1 20.3 39.8\nv=q 42.3 11.5 82.5 22.5 37.3 37.0 85.9 35.2 19.0 20.5 39.7\nRouted\nTokens\nm=512 41.6 12.2 81.9 22.1 37.3 35.4 84.6 35.2 18.9 19.5 39.6\nm=1536 42.9 10.4 82.6 23.5 39.8 37.5 87.5 35.4 19.4 20.8 40.0\nEncoder L ONG T5-B 42.1 7.4 82.0 21.4 38.4 35.8 88.0 35.5 18.7 20.4 38.5\nDecoder Multi-head 42.9 0.7 82.7 22.9 40.2 35.8 87.7 35.5 19.7 21.2 40.3\nObjective PEGASUS 42.8 11.2 82.6 22.6 40.5 37.3 87.3 35.3 19.6 20.8 39.6\nTable 6: C OLT5 ablations evaluated on validation sets. Each experiment modiﬁes a component of the C OLT5\nrecipe for C OLT5-Base. Static routing divides the input into equal-length blocks and selects the ﬁrst token in\neach block to be routed. Shared QKV routing shares routing decisions for queries and keys/values. In v=all the\nrouted queries attend to the entire input, while v=q selects the same number of key and value tokens as query\ntokens. m=512 and m=1536 use different numbers of routed tokens. L ONG T5-B uses a L ONG T5 encoder while\nretaining other parts of the COLT5 training recipe such as MQA and the UL2 objective. Multi-head refers to using\nmulti-head cross-attention. The ﬁnal ablation replaces the UL2 objective with PEGASUS as in L ONG T5.\nnent for COLT5 Base.\nRouting First, we note that static routing --\nevenly distributing routed tokens over the input\n-- leads to massive drop in performance. The impor-\ntance of routing provides evidence that the model\nlearns to devote capacity to important tokens and\nthe advantage of COLT5 is not merely a result of\nadditional parameters. Sharing routing decisions\nfor query and KV tokens should be compared with\nv=q, and leads to a modest reduction in quality and\nincrease in speed.\nThe optimal number of routed tokens represents\na trade-off between improved performance and\ncomputational cost of applying heavier layers. Ta-\nble 6 shows strong gains going from 512 to 1024\n(baseline) routed tokens and diminishing returns\nfor further increases.\nAttention COLT5 relies on routing to identify\nnot only tokens that can beneﬁt from important in-\nformation elsewhere in the input, but also which\ntokens contain such important information. We\nstudy whether COLT5 is successful in this task by\ncomparing performance with two different atten-\ntion settings -- v=all, in which routed tokens attend\nto the entire input, and v=q, which uses equal num-\nber of routed keys and values as queries, rather than\ntwice as many. COLT5 appears to occupy a sweet\nspot, as using fewer routed key-values modestly de-\ncreases performance at similar speed but attending\nto all inputs barely helps at sharply increased cost.\nMLP Query KV\n0.2\n0.4\n0.6\nRouted proportion\nOther\nAnswer\nQuestion\nFigure 6: Proportion of tokens routed for answer (string\nmatch), question, and other tokens by routing compo-\nnent for COLT5 Large model, averaged over examples\nin TriviaQA dev set and all layers of model.\nOther We compare COLT5 to LONG T5 with\nmulti-query cross-attention, conﬁrming that\nLONG T5 indeed does not achieve an unexpected\nquality gain from MQA, and our conservative\nassumptions in Figures 2, 4 are valid. Next, we\nevaluate multi-head cross-attention for COLT5,\nﬁnding that it leads to modestly improved COLT5\nperformance. However, as MHA exhibits order-\nof-magnitude slower inference, MQA is clearly\nfavored. Finally, PEGASUS appears to ﬁne-tune\nslightly better than UL2, though the difference is\nsmall and UL2 enables few-shot learning.\n5092\n4.6 Routing analysis\nIt is interesting to ask whether COLT5 routed to-\nkens line up with what we consider intuitively\nimportant tokens in each document. We investi-\ngate this question by studying routing patterns of a\nLarge COLT5 model ﬁne-tuned on TriviaQA. We\ndivide tokens into three categories: (1) question\ntokens, (2) answer tokens, and (3) other tokens.\nFigure 6 shows the average fraction of each type of\ntoken that is routed through the heavy path for MLP\nand attention layers on TriviaQA. We note that\nquestion and answer tokens are signiﬁcantly more\nlikely to be routed than other tokens, for feedfor-\nward as well as attention queries and keys/values.\nAppendix F presents more detailed routing analy-\nsis; e.g., semantically important tokens are much\nmore likely to be selected in later layers.\n5 Conclusion\nWe propose COLT5, a new model for long-range\ninputs that employs conditional computation for\nhigher quality and faster speed. COLT5 has light\nfeedforward and attention layers that apply to the\nentire input, as well as heavy branches that are ap-\nplied only to a subset of important tokens selected\nby a learned router. We show that COLT5 achieves\nstronger performance at any speed compared to\nLONG T5 on a variety of long-input datasets, and\ncan effectively and efﬁciently make use of ex-\ntremely long inputs up to 64k tokens.\nLimitations\nCOLT5 applies conditional computation only in the\nencoder. Applying conditional computation in the\ndecoder is more complicated; the routing method\nin COLT5 is not causal, so it isn’t applicable when\ngenerating token by token. Since decoder-only\nmodels and applications with long outputs have\nbecome more popular recently, this is a strong limi-\ntation of the current approach. Although the rout-\ning method in COLT5 could potentially be applied\nto the input context in a decoder-only model, we\ndidn’t investigate this setup.\nCOLT5 is specialized towards long sequences\nand has to be trained from scratch. For large-scale\ntraining and deployment, it is desirable to either\ntrain a single model that can handle both short and\nlong sequences, or develop a long-input architec-\nture that can be adapted from an existing large\nmodel.\nAcknowledgements\nWe would like to thank Srinadh Bhojanapalli, Luke\nVilnis, Zachary Fisher, Jianmo Ni, Tal Schuster,\nVaclav Cvicek, Sudeep Gandhe, Bhargav Kanagal,\nKenton Lee, Ming-Wei Chang, Afroz Mohiuddin,\nRaphael Hoffmann, and others at Google Research\nfor helpful advice and discussion.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. arXiv preprint arXiv:2004.08483.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin\nGimpel. 2022. SummScreen: A dataset for ab-\nstractive screenplay summarization. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8602–8615, Dublin, Ireland. Association for\nComputational Linguistics.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 615–621,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599–4610, On-\nline. Association for Computational Linguistics.\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,\nNicholas FitzGerald, Sumit Sanghai, Fei Sha, and\n5093\nWilliam Cohen. 2022. FiDO: Fusion-in-decoder op-\ntimized for stronger performance and faster infer-\nence. arXiv preprint arXiv:2212.08153.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nGoogle. 2020. Proﬁle your model with cloud tpu\ntools. https://cloud.google.com/tpu/docs/\ncloud-tpu-tools. Accessed: 2022-11-11.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago\nOntañón, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang. 2022. LongT5: Efﬁcient text-to-text trans-\nformer for long sequences. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2022 ,\npages 724–736, Seattle, United States. Association\nfor Computational Linguistics.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efﬁcient attentions for long\ndocument summarization. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1419–1436, On-\nline. Association for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361.\nTomáš Ko ˇciský, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, Gábor Melis, and\nEdward Grefenstette. 2018. The NarrativeQA read-\ning comprehension challenge. Transactions of the\nAssociation for Computational Linguistics , 6:317–\n328.\nYuta Koreeda and Christopher Manning. 2021. Con-\ntractNLI: A dataset for document-level natural lan-\nguage inference for contracts. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1907–1919, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nBernhard Kratzwald and Stefan Feuerriegel. 2018.\nAdaptive document retrieval for deep question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018 ,\npages 576–581. Association for Computational Lin-\nguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: a benchmark for question answer-\ning research. Trans. Assoc. Comput. Linguistics ,\n7:452–466.\nTao Lei, Junwen Bai, Siddhartha Brahma, Joshua\nAinslie, Kenton Lee, Yanqi Zhou, Nan Du, Vin-\ncent Y . Zhao, Yuexin Wu, Bo Li, Yu Zhang, and\nMing-Wei Chang. 2023. Conditional adapters:\nParameter-efﬁcient transfer learning with fast infer-\nence. In Advances in Neural Information Processing\nSystems.\nYuning Mao, Pengcheng He, Xiaodong Liu, Ye-\nlong Shen, Jianfeng Gao, Jiawei Han, and Weizhu\nChen. 2021. Reader-guided passage reranking\nfor open-domain question answering. In Findings\nof the Association for Computational Linguistics:\nACL/IJCNLP 2021, Online Event, August 1-6, 2021,\nvolume ACL/IJCNLP 2021 of Findings of ACL ,\npages 344–350. Association for Computational Lin-\nguistics.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen,\nVishakh Padmakumar, Johnny Ma, Jana Thompson,\nHe He, and Samuel R. Bowman. 2021. QuAL-\nITY: Question answering with long input texts, yes!\narXiv preprint arXiv:2112.08608.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\nJacob Devlin, James Bradbury, Anselm Levskaya,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. 2022. Efﬁciently scaling transformer in-\nference. arXiv preprint arXiv:2211.05102.\nYujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu,\nZhuyun Dai, Siddhartha Brahma, Iftekhar Naim,\nTao Lei, and Vincent Y Zhao. 2022. Multi-\nvector retrieval as sparse alignment. arXiv preprint\narXiv:2211.01267.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\n5094\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Sebas-\ntian Goodman, Livio Baldini Soares, Haitang Hu,\nSasha Tsvyashchenko, Aakanksha Chowdhery, Jas-\nmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo\nNi, Andrew Chen, Kathleen Kenealy, Jonathan H.\nClark, Stephan Lee, Dan Garrette, James Lee-\nThorp, Colin Raffel, Noam Shazeer, Marvin Ritter,\nMaarten Bosma, Alexandre Passos, Jeremy Maitin-\nShepard, Noah Fiedel, Mark Omernick, Brennan\nSaeta, Ryan Sepassi, Alexander Spiridonov, Joshua\nNewlan, and Andrea Gesmundo. 2022. Scaling up\nmodels and data witht5x and seqio. arXiv preprint\narXiv:2203.17189.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa De-\nhghani, Dara Bahri, Vinh Q Tran, Yi Tay, and Don-\nald Metzler. 2022. Conﬁdent adaptive language\nmodeling. arXiv preprint arXiv:2207.07061.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nScrolls: Standardized comparison over long lan-\nguage sequences. ArXiv, abs/2201.03533.\nNoam Shazeer. 2019. Fast transformer decoding:\nOne write-head is all you need. arXiv preprint\narXiv:1911.02150.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural net-\nworks: The sparsely-gated mixture-of-experts layer.\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings . OpenRe-\nview.net.\nNoam Shazeer and Mitchell Stern. 2018. Adafac-\ntor: Adaptive learning rates with sublinear memory\ncost. In Proceedings of the 35th International Con-\nference on Machine Learning, ICML 2018, Stock-\nholmsmässan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Re-\nsearch, pages 4603–4611. PMLR.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022. Unify-\ning language learning paradigms. arXiv preprint\narXiv:2205.05131.\nNeeraj Varshney, Man Luo, and Chitta Baral.\n2022. Can open-domain QA reader utilize exter-\nnal knowledge efﬁciently like humans? CoRR,\nabs/2211.12707.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R 3:\nReinforced ranker-reader for open-domain question\nanswering. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5981–5988. AAAI Press.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022. Kg-ﬁd: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022 , pages\n4961–4974. Association for Computational Linguis-\ntics.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntañón, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33:17283–17297.\nYury Zemlyanskiy, Joshua Ainslie, Michiel de Jong,\nPhilip Pham, Ilya Eckstein, and Fei Sha. 2021.\nReadtwice: Reading very large documents with\nmemories. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5189–5195.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5905–5921, Online. Association for Computational\nLinguistics.\n5095\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du,\nYanping Huang, Jeff Dean, Noam Shazeer, and\nWilliam Fedus. 2022. St-moe: Designing stable and\ntransferable sparse expert models. arXiv preprint\narXiv:2202.08906.\n5096\nModel Layers Model dim MLP light dim MLP heavy dim Heads light Headsheavy Params\nLONG T5-B 12 768 2048 N/A 12 N/A 248m\nCOLT5-B 12 768 1024 8096 4 8 433m\nLONG T5-L 24 1024 2816 N/A 16 N/A 783m\nCOLT5-L 24 1024 1408 11264 4 12 1462m\nLONG T5-XL 24 2048 5120 N/A 32 N/A 2850m\nCOLT5-XL 24 2048 2560 20480 8 24 5297m\nTable 7: Hyperparameters for L ONG T5 and C OLT5 models. T5.1.1 hyperparameters match L ONG T5. C OLT5\nparameters are sparsely accessed as a result of conditional computation, so parameter counts do not reﬂect compute,\nand for a given model size COLT5 is in fact faster than LONG T5 despite having more parameters.\nA Contributions\nJoshua led the project, developed the initial con-\nditional attention mechanisms, and conducted\nmost experimental ablations. Tao developed the\nheavy/light formulation for heterogeneous condi-\ntional computation, comprising the routing and con-\nditional feedforward mechanisms, and iterated with\nJoshua on initial experiments demonstrating fea-\nsibility. Michiel helped to scope the paper, per-\nformed most of the writing, and oversaw speed\nbenchmarking. Santiago designed and conducted\nall the few-shot experiments, initiated the routing\nanalysis visualization, and integrated UL2 into the\ncodebase. Siddhartha developed the separate rout-\ning for query and key/value tokens in the condi-\ntional attention component and demonstrated the\nresulting quality improvements. Yury designed and\nconducted all experiments for inputs larger than\n16k tokens, demonstrating favorable scaling up to\n64k. David integrated all SCROLLS tasks into\nthe codebase and ran early experiments, especially\ncomparing UL2 with PEGASUS. Mandy devel-\noped the leaderboard comparisons with LongT5\nand helped run several experiments. James advised\non and ran early comparisons with MoE conditional\ncomputation. Yi advised on the adaptation of UL2\nto 4k input length pre-training. Finally, Yun-Hsuan\nand Sumit provided guidance and support for the\nproject overall.\nB Model Hyperparameters\nTable 7 shows LONG T5 and COLT5 hyperparam-\neters, including parameter counts. For LONG T5,\nwe report numbers for the TGlobal conﬁguration,\nwhich match T5.1.1. Notice that COLT5’s parame-\nter counts are larger due to using conditional com-\npute. Similar to other conditional compute archi-\ntectures such as mixture-of-experts, computational\ncost does not necessarily increase with parameter\ncount.\nWe use the same 127-token local radius for\nCOLT5 as LONG T5. This results in a local atten-\ntion window wof 255 since 127 tokens are attended\nto the left and 127 to the right.\nC Routing Normalization\nHyperparameters\nTo normalize the routing scores for differentiable\ntop-ktoken selection, we use the iterative soft top-\nkalgorithm from Lei et al. (2023) and Qian et al.\nModel Average 16k in, 128 out 16k in, 512 out 16k in, 1024 out 8k in, 128 out\nEnc Tot Enc Tot Enc Tot Enc Tot Enc Tot\nLONG T5-B 77 136 84 98 84 165 84 296 27 39\nCOLT5-B 29 90 30 45 30 113 30 256 18 30\nLONG T5-L 164 329 173 222 179 392 179 799 66 100\nCOLT5-L 70 201 73 103 73 250 73 578 45 69\nLONG T5-XL 390 870 412 557 423 1081 423 2065 166 290\nCOLT5-XL 177 439 185 239 185 525 185 1253 115 163\nTable 8: Comparison of total and encoder inference time per sample (ms) for L ONG T5 and C OLT5 Base, Large,\nand XL models at different input and output lengths. Average time per sample is computed as a weighted average\nover input and output lengths, weighted by the number of tasks in our evaluation that use the corresponding setting\n(4 for 16k/128, 3 for 16k/512, and one each for 16k/1024 and 8k/128).\n5097\nModel arXiv SummScreenFD QMSum GovRep\nR-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nLONG T5-B 47.4 21.4 43.5 34.8 9.3 20.7 35.1 11.1 23.4 59.3 30.1 33.0\nCOLT5-B 47.5 21.3 43.6 35.6 9.7 21.0 34.6 10.9 23.0 60.2 31.0 32.8\nLONG T5-L 47.9 21.7 43.8 35.3 9.1 20.8 35.9 12.0 24.1 61.4 32.5 34.1\nCOLT5-L 48.4 21.7 44.3 35.7 10.1 21.4 36.8 12.6 24.7 61.8 32.7 34.4\nLONG T5-XL 48.2 21.8 44.1 36.6 10.3 21.5 37.0 12.5 24.7 61.8 33.2 34.8\nCOLT5-XL 48.4 22.0 44.3 36.3 10.0 21.5 37.4 13.0 25.1 62.2 33.3 34.9\nTable 9: Full performance comparison with Rouge-1, Rouge-2, and Rouge-L metrics of C OLT5 and L ONG T5\nBase, Large, and XL models on summarization dev sets. Results based on checkpoint that maximizes R gm as in\nTable 3.\n(2022) with ϵ = 1.0 and 50 iterations. During\ntraining we allow the top9\n8 ktokens to have nonzero\nweight instead of just the top kin order to provide\na slightly improved training signal.\nD Additional Experimental Results\nTable 8 compares LONG T5 and COLT5 inference\nspeed in more detail, splitting off encoder and total\ntime per sample. Since COLT5 applies conditional\ncomputation only in the encoder, encoder speed\ngains are larger than overall speed gain, and total\nspeed gains are largest for shorter output length.\nTrade-offs are even more in the favor of COLT5\nwhen paired with other decoder optimizations.\nTable 9 shows full (Rouge-1, Rouge-2, Rouge-L)\nresults for summarization datasets.\nE Computational Resources\nFor pre-training we generally used 128 TPUv4\nchips for Base and 256 TPUv4 chips for Large\nand XL. Pre-training took approximately 2.5 days\nfor Base, 3.7 days for Large, and 12.8 days for XL.\nFor ﬁne-tuning we generally used 64, 128, and 256\nTPUv4 chips for Base, Large, and XL, respectively,\nwith training time varying with dataset size.\nF Routing Analysis\nIn this section we take a closer look at the routing\nmechanisms in COLT5. There are three routing\nprocesses in each layer of COLT5: (1) Routing\nof attention keys and values (“KV-routing”), (2)\nrouting of attention queries (“Q-routing”) and (3)\nrouting of MLP tokens (“MLP-routing”). For sim-\nplicity, we will say that a token is selected, when it\nis routed to the heavy alternative (of either MLP or\nattention). We are interested in understanding what\ntokens are selected and whether these mechanisms\nselect similar or different tokens in each layer.\nWhich tokens are selected We divide input to-\nkens into three categories: (1) question tokens, (2)\nanswer tokens (found via simple normalized string\nmatch of the ground truth answer), and (3) other to-\nkens. Figure 7 shows the proportion of each token\ntype that is routed by a ﬁne-tuned COLT5-Large\nmodel on the TriviaQA dev set, by layer and rout-\ning component.\n0 10 20\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nRouting proportion\nMLP\n0 10 20\n0\n0.2\n0.4\n0.6\n0.8\nLayer\nQuery\n0 10 20\n0.2\n0.4\n0.6\n0.8\n1\nLayer\nKV\nOther Answer Question\nFigure 7: Proportion of tokens routed for answer (string match), question, and other tokens by routing component\nand layer for COLT5 Large model, averaged over examples in TriviaQA dev set.\n5098\nThe same is true \nfor tokens around \nthe correct answer \n(“papageno” in \nthis example).\nQuestion is \nheavily routed to \nthe expensive \nalternative by last \nlayers of the \nmodel.\nFigure 8: Visualization of token routing weights for some fragments of an example on TriviaQA.\nEarlier we showed that question and answer to-\nkens are more likely to be selected, but separat-\ning routing decisions by layer reveals interesting\npatterns. At early layers question and answer to-\nkens are only modestly more likely to be selected,\nwith routing probability sharply increasing at later\nlayers and peaking in the last layer. This makes\nintuitive sense: in early layers the model has not\nyet had the opportunity to identify which tokens\nand parts of the document are important. However,\nthe increase is not monotonic and there is strong\nvariation between layers. This variation may im-\nply that different layers focus on different types of\ntokens, or that some routing components do not\nsuccessfully learn to identify important tokens.\nTo gain a better insight into this, Figure 8 vi-\nsualizes routing on two sample fragments from a\nTriviaQA example (notice that, given the large in-\nput length used in COLT5, we do not show the\ncomplete example in the ﬁgure). The two frag-\nments shown correspond to the beginning of the\nexample (where the question is located), and the\npart of the context surrounding the correct answer.\nWe have added a colored background to the ﬁgure,\nwhere each of the three CMY channels are mapped\nto the KV-routing weights in different layers of the\nmodel. Cyan corresponds to layer 1, Magenta to\nlayer 12, and Yellow to layer 24. As we can see,\nquestion and answer are heavily yellow colored,\nshowing those tokens are selected in the last layer.\nCorrelation between routing processes. Table\n10 shows the Pearson correlation coefﬁcient be-\ntween the routing weights of the different routing\nmechanisms in each layer in aCOLT5 Large model\n(MLP-routing correlation with KV-routing, MLP-\nrouting with Q-routing, and KV-routing with Q-\nrouting). We show numbers for both the pre-trained\ncheckpoint, as well as a ﬁne-tuned model on Trivi-\naQA. As we can see, the routing of keys/values and\nrouting of queries is highly correlated at all layers\nexcept the ﬁrst two, while the routing of tokens\nin the MLP has lower correlation to the other two\nprocesses. Interestingly correlation between MLP\nand attention routing increases in the last layers of\nthe model.\n5099\nPre-trained Fine-tuned\nMLP-KV MLP-Q KV-Q MLP-KV MLP-Q KV-Q\n1 -0.06 -0.06 -0.09 -0.06 -0.09 -0.26\n2 0.27 0.52 0.04 0.27 0.39 0.02\n3 -0.05 -0.03 0.75 0.05 -0.01 0.69\n4 0.05 0.09 0.76 0.18 0.14 0.72\n5 0.02 -0.01 0.75 0.22 0.26 0.68\n6 0.02 -0.01 0.78 0.31 0.33 0.70\n7 0.02 0.00 0.73 0.26 0.27 0.70\n8 0.00 -0.02 0.44 0.11 -0.07 0.29\n9 0.13 0.11 0.74 0.36 0.40 0.70\n10 -0.06 -0.08 0.08 -0.15 -0.15 0.12\n11 -0.05 -0.07 0.31 -0.08 -0.03 0.18\n12 -0.04 -0.08 0.27 0.03 0.00 0.28\n13 -0.10 -0.09 0.87 -0.13 -0.03 0.72\n14 -0.04 -0.05 0.76 -0.06 -0.12 0.67\n15 0.53 0.64 0.69 0.51 0.55 0.67\n16 0.08 0.12 0.63 0.06 0.57 0.24\n17 0.28 0.30 0.65 0.27 0.32 0.69\n18 0.28 0.02 0.84 0.31 0.20 0.76\n19 0.45 0.77 0.59 0.19 0.38 0.64\n20 0.30 0.39 0.64 0.38 0.47 0.62\n21 0.05 -0.04 0.49 0.18 0.11 0.47\n22 0.05 0.00 0.69 0.21 0.16 0.68\n23 0.39 0.33 0.68 0.60 0.79 0.69\n24 0.43 0.39 0.59 0.57 0.63 0.65\nTable 10: Pearson correlation coefﬁcient between the\nrouting weights of the different routing mechanisms in\neach layer in a C OLT5 Large model. We show num-\nbers for both the pre-trained checkpoint, as well as a\nﬁne-tuned model on TriviaQA. Blue bars visualize pos-\nitive correlation, whereas red bars visualize negative\ncorrelation.\n5100",
  "topic": "Computation",
  "concepts": [
    {
      "name": "Computation",
      "score": 0.4921686053276062
    },
    {
      "name": "Transformer",
      "score": 0.4599084258079529
    },
    {
      "name": "Engineering",
      "score": 0.3830161690711975
    },
    {
      "name": "Computer science",
      "score": 0.32802388072013855
    },
    {
      "name": "Electrical engineering",
      "score": 0.22046124935150146
    },
    {
      "name": "Algorithm",
      "score": 0.16891637444496155
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 22
}