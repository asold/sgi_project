{
    "title": "Verbal lie detection using Large Language Models",
    "url": "https://openalex.org/W4390058537",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3211022172",
            "name": "Riccardo Loconte",
            "affiliations": [
                "IMT School for Advanced Studies Lucca"
            ]
        },
        {
            "id": "https://openalex.org/A2100207294",
            "name": "Roberto Russo",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A2734517785",
            "name": "Pasquale Capuozzo",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A20707900",
            "name": "Pietro Pietrini",
            "affiliations": [
                "IMT School for Advanced Studies Lucca"
            ]
        },
        {
            "id": "https://openalex.org/A2110316339",
            "name": "Giuseppe Sartori",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A3211022172",
            "name": "Riccardo Loconte",
            "affiliations": [
                "IMT School for Advanced Studies Lucca"
            ]
        },
        {
            "id": "https://openalex.org/A2100207294",
            "name": "Roberto Russo",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A2734517785",
            "name": "Pasquale Capuozzo",
            "affiliations": [
                "University of Padua"
            ]
        },
        {
            "id": "https://openalex.org/A20707900",
            "name": "Pietro Pietrini",
            "affiliations": [
                "IMT School for Advanced Studies Lucca"
            ]
        },
        {
            "id": "https://openalex.org/A2110316339",
            "name": "Giuseppe Sartori",
            "affiliations": [
                "University of Padua"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2089428492",
        "https://openalex.org/W2056242694",
        "https://openalex.org/W4311049239",
        "https://openalex.org/W2529276895",
        "https://openalex.org/W2089594500",
        "https://openalex.org/W2113335527",
        "https://openalex.org/W1973635730",
        "https://openalex.org/W2313695998",
        "https://openalex.org/W2159822720",
        "https://openalex.org/W4328048663",
        "https://openalex.org/W1677438756",
        "https://openalex.org/W2181556729",
        "https://openalex.org/W105812634",
        "https://openalex.org/W2067996923",
        "https://openalex.org/W4212809732",
        "https://openalex.org/W2410465342",
        "https://openalex.org/W2040849715",
        "https://openalex.org/W89279510",
        "https://openalex.org/W2251411520",
        "https://openalex.org/W6815833287",
        "https://openalex.org/W2755857139",
        "https://openalex.org/W2750623459",
        "https://openalex.org/W2804621770",
        "https://openalex.org/W2757817663",
        "https://openalex.org/W4319655871",
        "https://openalex.org/W2091034860",
        "https://openalex.org/W2807123141",
        "https://openalex.org/W2149344011",
        "https://openalex.org/W4237260681",
        "https://openalex.org/W2010056161",
        "https://openalex.org/W1509980967",
        "https://openalex.org/W1979855527",
        "https://openalex.org/W2301534821",
        "https://openalex.org/W3184516577",
        "https://openalex.org/W3011505293",
        "https://openalex.org/W2020107139",
        "https://openalex.org/W2915755524",
        "https://openalex.org/W2123991517",
        "https://openalex.org/W2765401873",
        "https://openalex.org/W4282019701",
        "https://openalex.org/W3161354620",
        "https://openalex.org/W2795454945",
        "https://openalex.org/W3120923022",
        "https://openalex.org/W4308146474",
        "https://openalex.org/W2547604310",
        "https://openalex.org/W2250650831",
        "https://openalex.org/W2124821580",
        "https://openalex.org/W3015228357",
        "https://openalex.org/W2035896792",
        "https://openalex.org/W4320490910",
        "https://openalex.org/W4318925766",
        "https://openalex.org/W1974991592",
        "https://openalex.org/W4385570577",
        "https://openalex.org/W2016487189",
        "https://openalex.org/W1965443741",
        "https://openalex.org/W2054776631"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports\nVerbal lie detection using Large \nLanguage Models\nRiccardo Loconte 1*, Roberto Russo 2, Pasquale Capuozzo 3, Pietro Pietrini 1 & \nGiuseppe Sartori 3\nHuman accuracy in detecting deception with intuitive judgments has been proven to not go above \nthe chance level. Therefore, several automatized verbal lie detection techniques employing Machine \nLearning and Transformer models have been developed to reach higher levels of accuracy. This study \nis the first to explore the performance of a Large Language Model, FLAN-T5 (small and base sizes), \nin a lie-detection classification task in three English-language datasets encompassing personal \nopinions, autobiographical memories, and future intentions. After performing stylometric analysis \nto describe linguistic differences in the three datasets, we tested the small- and base-sized FLAN-T5 \nin three Scenarios using 10-fold cross-validation: one with train and test set coming from the same \nsingle dataset, one with train set coming from two datasets and the test set coming from the third \nremaining dataset, one with train and test set coming from all the three datasets. We reached state-\nof-the-art results in Scenarios 1 and 3, outperforming previous benchmarks. The results revealed also \nthat model performance depended on model size, with larger models exhibiting higher performance. \nFurthermore, stylometric analysis was performed to carry out explainability analysis, finding \nthat linguistic features associated with the Cognitive Load framework may influence the model’s \npredictions.\nLie detection involves the process of determining the veracity of a given communication. When producing \ndeceptive narratives, liars employ verbal strategies to create false beliefs in the interacting partners and are thus \ninvolved in a specific and temporary psychological and emotional  state1. For this reason, the Undeutsch hypoth-\nesis suggests that deceptive narratives differ in form and content from truthful  narratives2. This topic has always \nbeen under constant investigation and development in the field of cognitive psychology, given its significant and \npromising applications in the forensic and legal  setting3. Its potential pivotal role is in determining the honesty \nof witnesses and potential suspects during investigations and legal proceedings, impacting both the investigative \ninformation-gathering process and the final decision-making  level4.\nDecades of research have focused on identifying verbal cues for deception and developing effective methods \nto differentiate between truthful and deceptive narratives, with such verbal cues being, at best, subtle and typically \nresulting in both naive and expert individuals performing just above chance  levels5,6. A potential explanation \ncoming from social psychology for this unsatisfactory human performance is the intrinsic human inclination \nto the truth bias 7, i.e., the cognitive heuristic of presumption of honesty, which makes people assume that an \ninteraction partner is truthful unless they have reasons to believe  otherwise8,9. However, it is worth mentioning \nthat a more recent study challenged this solid result, finding that instructing participants to rely only on the \nbest available cue, such as the detailedness of the story, enabled them to consistently discriminate lies from the \ntruth with accuracy ranging from 59 to 79%10. This finding moves the debate on (1) the proper number of cues \nthat judges should combine before providing their veracity judgment -with the suggestion that the use-the-best \nheuristic approach is the most straightforward and accurate- and thus on (2) the diagnosticity level of this cue.\nMore recently, the issue of verbal lie detection has also been tackled by employing computational techniques, \nsuch as stylometry. Stylometry refers to a set of methodologies and tools from computational linguistic and \nartificial intelligence that allow to conduct quantitative analysis of linguistic features within written texts to \nuncover distinctive patterns that can infer and characterize authorship or other stylistic  attributes11–13. Albeit \nwith some limitations, stylometry has been proven to be effective in the context of lie  detection14,15. The main \nadvantage is the possibility of coding and extracting verbal cues independently from human judgment, hence \nreducing the problem of inter-coder agreement, as researchers using the same technique for the same data will \nextract the same  indices15.\nOPEN\n1Molecular Mind Lab, IMT School for Advanced Studies Lucca, Piazza San Francesco 19, 55100 Lucca, LU, \nItaly. 2Department of Mathematics “Tullio Levi-Civita”, University of Padova, Padova, Italy. 3Department of \nGeneral Psychology, University of Padova, Padova, Italy. *email: riccardo.loconte@imtlucca.it\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nAlongside this trend, several recent studies have explored computational analysis of language in different \ndomains, such as fake  news16,17, transcriptions of court  cases18–20, evaluations of deceptive product  reviews21–23, \ninvestigations into cyber-crimes 24, analysis of autobiographical  information25, and assessments of deceptive \nintentions regarding future  events26. Taken together, most of those studies focused on the usage of Machine \nLearning and Deep Learning algorithms combined with Natural Language Processing (NLP) techniques to detect \ndeception from verbal cues automatically (see Constâncio et al. 27 for a systematic review of the computerized \ntechniques employed in lie-detection studies).\nMore recently, a great step in advance has been made in the field of AI and NLP with the advent of Large \nLanguage Models (LLMs). LLMs are Transformer-based language models with hundreds of millions of param-\neters trained on a large collection of corpora (i.e., pre-training phase)28. Thanks to this pre-training phase, LLMs \nhave proven to capture the intricate patterns and structures of language and develop a robust understanding of \nsyntax, semantics, and pragmatics, being able to generate coherent text resembling human natural language. In \naddition, once pre-trained, these models can be fine-tuned on specific tasks using smaller task-specific datasets. \nFine-tuning refers to the process of continuing the training of a pre-trained model on a new dataset, allowing it to \nadapt its previously learned knowledge to the nuances and specificities of the new data, thereby achieving state-\nof-the-art  results28. Common tasks for LLMs fine-tuning include NLP tasks, such as language translation, text \nclassification (e.g., sentiment analysis), question-answering, text summarization, and code generation. Therefore, \nLLMs excel at a wide range of NLP tasks, as opposed to models uniquely trained for one specific  task28. However, \nto the best of our knowledge, despite the extreme flexibility of LLMs, the procedure of fine-tuning an LLM on \nsmall corpora for a lie-detection task has remained unexplored.\nRelated works in the psychology field\nAmong previous psychological frameworks aimed at identifying reliable cues of verbal deception, the Distanc-\ning framework, the Cognitive Load (CL) theory, the Reality Monitoring (RM) framework, and the Verifiability \nApproach (V A) have been extensively studied, gaining empirical support for their efficacy not only from primary \nresearch but also from meta-analytic studies.\nThe Distancing framework of deception states that liars tend to distance themselves from their narratives as \na mechanism to handle the negative emotions experienced while lying by using fewer self-references (e.g., \"I,\" \n\"me\") and employing more other-references (e.g., \"he,\" \"they\")3,29.\nThe CL framework states that liars consume more cognitive resources while fabricating their fake responses, \nchecking their congruency with other fabricated information, and maintaining credibility and consistency in \nfront of the  examiner30, resulting in shorter, less elaborate, and less complex statements. A meta-analysis 31 \nfound that approaches based on CL theories produce higher accuracy rates in detecting deception than standard \napproaches.\nThe RM framework bases its assumptions on the memory characteristics literature hypothesizing that truth-\nful recollections are based on experienced events, while deceptive recollections are based on imagined  events32. \nTherefore, RM derives its predictions about truthful narratives from sensory, spatial, and temporal information \nand from emotions and feelings experienced during the event. On the contrary, predictions about deceptions are \ndrawn from the number of cognitive operations (e.g., thoughts and reasonings)33–35. The total RM scores appear \nto be diagnostic (d = 0.55) in the detection accuracy of  truthfulness36,37 (see  also38 for an extensive review of verbal \nlie-detection methods). More recently, the RM framework was investigated through concreteness in  language39. \nIn this study, one underlying and partially supported assumption was the truthful concreteness hypothesis, \nwhich suggests that truthful statements usually consist of concrete, specific, and contextually relevant details. \nIn contrast, deceptive or false statements often include more abstract and less specific information, being more \nassociated with the RM criterion of cognitive operations.\nThe V A in verbal lie detection suggests that truthful statements are more likely to be verifiable than false \nor deceptive statements, as liars avoid mentioning details that could be verified with independent evidence to \nconceal their  deception40,41. Verifiable details may be represented by activities involving or witnessed by identi-\nfied individuals, documented through video or photographic evidence, or leaving digital or physical traces (e.g., \nphone calls or receipts)40,41.\nNotably, these frameworks offer detectable linguistic cues that can be readily identified using NLP techniques \nand have been extensively studied in this sense.\nHouch et al.14 conducted a meta-analysis of studies on computer-based lie detection, with most of the included \nstudies relying on the Linguistic Inquiry and Word Count software (LIWC)42,43. LIWC is the gold standard tool \nfor studying lexical diversity and text semantic content. Given a text, LIWC calculates the percentage of total \nwords corresponding to more than 100 categories in the dictionary related to different psychosocial dimensions, \nwhich have been validated by human evaluators using rigorous procedures. Among Houch’s meta-analysis find-\nings, LIWC metrics reflecting Distancing, CL, and RM frameworks of deception found support from the results \nand can detect verbal deception through computerized techniques.\nUsually, for distancing metrics, researchers compute the number of self and other-references by summing \nthe frequency of first-person pronouns in contrast with second and third-person  pronouns3,29. When employing \nCL theory in texts, researchers usually employ and analyze statistics about the number of words and sentences, \nthe readability, and the complexity of  texts12–14. RM is often investigated with  LIWC26,44–46. Schutte et al.47 pro-\nvided evidence that human coding of perceptual and contextual details in discriminating lies from truths is \nnot conclusively superior, thereby highlighting the potential advantages of automated techniques. Additionally, \nrecent studies extracted verifiable details by using named-entity recognition (NER), proving to be an effective \nautomatized procedure for the detection of deception in hotel reviews 23 as well as in participants’ intentions on \ntheir weekend  plans48.\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nThe promising results in applying NLP techniques for psychological research suggest the possibility of com-\nbining metrics from different psychological frameworks in a new theory-based stylometric analysis, offering the \npossibility to investigate verbal lie detection from multiple perspectives in one shot.\nRelated works in the AI field\nPrevious works from the AI field have applied machine learning and deep learning models in a binary classifica-\ntion task for data-driven verbal deception detection.\nKleinberg and  Verschuere49 developed a database of future intentions to investigate whether combining \nmachine and human judgments may improve accuracy in predicting deception. While finding that human judg-\nment impairs automated deception detection accuracy, the authors implemented two machine learning models \n(i.e., vanilla random forest) trained respectively on LIWC and Part-of-Speech features (e.g., frequency of names, \nadjectives, adverbs, verbs) reaching an accuracy of 69% (95% CI: 63–74%) and 64 (95% CI: 58%, 69%), respec -\ntively. On the same dataset, Ilias et al. 50 evaluated six deep-learning models, including combinations of BERT \n(and RoBERTa), MultiHead Attention, co-attentions, and Transformers models. The best accuracy reached was \n70.61% (± 2.58%) using a BERT with co-attention model. The authors also provided explainaibility analysis to \nunderstand how the models reached their decisions using a combination of LIME (a tool used to explain deep \nlearning predictions in more straightforward and understandable terms by showing which specific words of the \ntext influenced the outcome) and LIWC.\nCapuozzo et al. 51 developed a new cross-domain and cross-language dataset of opinions, asking English-\nspeaking and Italian-speaking participants to provide truthful or deceptive opinions on five different topics. After \nencoding the texts with FastText word-embedding, they trained Transformers models in multiple scenarios using \n10-fold cross-validation, with averaged accuracy ranging from 63% (± 8.7%) in the “within-topic” scenario to a \nhigh of 90.1% (± 0.16%) in the “author-based” scenario.\nIn contrast, Sap et al. 52 developed a new dataset of narratives generated from memories and imagination \nand used an LLM (GPT-3) to compute a new metric called “sequentiality” . Sequentiality is a metric of narrative \nflow that compares the probability of a sentence with and without its preceding story context. While providing \ninsights into the cognitive processes of storytelling with an innovative computational approach, the authors did \nnot employ a fine-tuning procedure for an LLM to classify different narratives.\nThe findings in the AI domain indicate that as the model’s complexity increases, there is a heightened accuracy \nin predicting deception from texts. However, this increase in accuracy often comes at the expense of explain-\nability for these predictions. LLMs are currently among the most cutting-edge models capable of handling vast \namounts and complexities of linguistic data, and the lack of literature on fine-tuning LLMs for lie-detection tasks \nprovides worthwhile reasons to investigate this area.\nAims and hypotheses of the study\nThe main objectives and hypothesis of this study are outlined as follows:\n• Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, \n1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n• Hypothesis 2): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of \ndeception;\n• Hypothesis 3): Fine-tuning an LLM on a multiple-context dataset enables the model to obtain successful \npredictions on a multi-context test set;\n• Hypothesis 4): Model performance depends on model size, with larger models showing higher accuracy;\n• Hypothesis 5a): The linguistic style distinguishing truthful from deceptive statements varies across different \ncontexts, 5b) and can be a significant feature for model prediction.\nTo test Hypothesis 1a, we fine-tuned an open-source LLM, FLAN-T5, using three datasets: personal opinions \n(the Deceptive Opinions  dataset51), autobiographical experiences (the Hippocorpus  dataset52) and future inten-\ntions (the Intention  dataset49). Given the extreme flexibility of LLMs, this approach is hypothesized to detect \ndeception from raw texts above the chance level. To test the advantage of our approach compared to classical \nmachine and deep learning models (Hypothesis 1b), we decided to compare the results with two benchmarks, \nfurther described in the Methods and Materials section.\nWith regards to Hypotheses 2 and 3, according to empirical evidence, classical machine learning models tend \nto experience a decline in performance when trained and tested on the aforementioned  scenarios53–55. In contrast, \nLLMs have acquired a comprehensive understanding of language patterns during the pre-training phase. We \nposit that a fine-tuned LLM is capable of generalizing its learning across various contexts. Related to Hypothesis \n4, we believe this generalization ability is further enhanced in larger models, as their size is associated with a \nmore sophisticated representation of language.\nFinally, to test Hypothesis 5, we introduced a new theory-based stylometric approach, named DeCLaRatiVE \nstylometry, to extract linguistic features related to the psychological frameworks of  Distancing29, Cognitive \n Load31, Reality  Monitoring32, and Verifiability  Approach40,41, providing a pragmatic set of criteria to extract \nfeatures from utterances. We will apply DeCLaRatiVE stylometry to compare truthful and deceptive statements \nin the three aforementioned datasets in order to explore potential differences in terms of linguistic style. Our \nhypothesis suggests that the linguistic style distinguishing truthful from deceptive statements may vary across the \nthree datasets, as these types of statements originate from distinct contexts. We also applied the D eCLaRatiVE \nstylometry technique to provide explainability analysis of the top-performing model.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nMethods and materials\nDatasets\nThree datasets were employed for this study: the Deceptive Opinions  dataset51, from now on Opinion Data-\nset, the Hippocorpus  dataset52, from now on Memory Dataset, and the Intention dataset 49. For each dataset, \nparticipants were required to provide genuine or fabricated statements in three different domains: personal \nopinions on five different topics (Opinion dataset), autobiographical experiences (Memory dataset), and future \nintentions (Intention Dataset). Notably, the specific topic within each domain was counterbalanced among liars \nand truth-tellers. A more detailed description of each dataset is available in Supplementary Information as well \nas in the method section of each original article.\nTable 1 displays an example of truthful and deceptive statements about opinions, memories, and intentions. \nTable 2 reports descriptive statistics for each dataset, both overall and when grouped by truthful and deceptive \nsets of statements. These statistics include the minimum, maximum, average, and standard deviation of word \ncounts. Word counts were computed after text tokenization using spaCy, a Python library for text processing. \nAdditionally, Table 2 provides Jaccard similarity index values between truthful and deceptive vocabulary sets. \nJaccard’s index was derived by calculating the intersection (common words) and union (total words) of these \nTable 1.  Truthful and deceptive example statements about opinions, memories, and intentions. In brackets, \nthe topic assigned to the participant in the deceptive condition to fabricate the narrative.\nTruthful Deceptive\nOpinion\n(Abortion)\nWhile I am morally torn on the issue, I believe that \nultimately it is a woman’s body and she should be able to \ndo with it as she pleases. I belive people should not dehu-\nmanize the fetus tough, to make themselves feel better. The \ndecision about laws regarding this issue should be left up to \nthe states to decide. To combat this problem, birth control \nshould be easily accessible\nAbortion is the termination of a life and should not be \nal- lowed. If a fetus has made it to the point of being able to \nsurvive “on its own” outside its mother’s body, what right \ndo we have to cut its life short. If the mother’s life is in \ndanger, she already chose that she was willing to sacrifice \nher life to have a child when she consented to procreating\nMemory\n(My boyfriend and I went to a concert together and had \na great time. We met some of my friends there and really \nenjoyed ourselves watching the sunset.)\nThe day started perfectly, with a great drive up to Denver \nfor the show. Me and my boyfriend didn’t hit any traffic \non the way to Red Rocks, and the weather was beautiful. \nWe met up with my friends at the show, near the top of the \ntheater, and laid down a blanket. The opener came on, and \nwe danced our butts off to the banjoes and mandolins that \nwere playing on-stage. We were so happy to be there. That’s \nwhen the sunset started. It was so beautiful. The sky was \na pastel pink and was beautiful to watch. That’s when Phil \nLesh came on, and I just about died. It was the happiest \nmoment of my life, seeing him after almost a decade of not \nseeing him. I was so happy to be there, with my friends and \nmy love. There was nothing that could top that night. We \ndrove home to a sky full of stars and stopped at an overlook \nto look up at them. I love this place I live. And I love live \nmusic. I was so happy\nConcerts are my most favorite thing, and my boyfriend \nknew it. That’s why, for our anniversary, he got me tickets \nto see my favorite artist. Not only that, but the tickets were \nfor an outdoor show, which I love much more than being \nin a crowded stadium. Since he knew I was such a big fan \nof music, he got tickets for himself, and even a couple of \nmy friends. He is so incredibly nice and considerate to me \nand what I like to do. I will always remember this event \nand I will always cherish him. On the day of the concert, I \ngot ready, and he picked me up and we went out to a res-\ntaurant beforehand. He is so incredibly romantic. He knew \nexactly where to take me without asking. We ate, laughed, \nand had a wonderful dinner date before the big event. We \narrived at the concert and the music was so incredibly \nbeautiful. I loved every minute of it. My friends, boyfriend, \nand I all sat down next to each other. As the music was \nslowly dying down, I found us all getting lost just staring \nat the stars. It was such an incredibly unforgettable and \nbeautiful night\nIntention\n(Going swimming with my daughter)\nWe go to a Waterbabies class every week, where my \n16-month-old is learning to swim. We do lots of activities \nin the water, such as learning to blow bubbles, using floats \nto aid swimming, splashing and learning how to save them-\nselves should they ever fall in. I find this activity important \nas I enjoy spending time with my daughter and swimming \nis an important life skill\nI will be taking my 8-year-old daughter swimming this \nSaturday. We’ll be going early in the morning, as it’s gener-\nally a lot quieter at that time, and my daughter is always \nup early watching cartoons anyway (5 am!). I’m trying to \nteach her how to swim in the deep end before she starts \nher new school in September as they have swimming les-\nsons there twice a week\nTable 2.  Summary statistics of the number of words for each dataset and truthful and deceptive set of \nstatements. Jaccard Similarity Index and its qualitative interpretation in brackets refers to the similarity \nbetween truthful and deceptive vocabulary sets for each dataset.\nDataset (total number) Min–Max number of words Average number of words (SD)\nJaccard similarity Index (qualitative \ninterpretation)\nAll opinions (2500) 6–338 59.05 (30.66)\n0.35\n(low similarity)Truthful opinions (1250) 7–338 66.74 (31.95)\nDeceptive opinions (1250) 6–232 51.36 (27.24)\nAll intentions (1640) 15–251 50.44 (30.11)\n0.34\n(low similarity)Truthful intentions (783) 15–206 47.04 (28.36)\nDeceptive intentions (857) 15–251 53.55 (31.31)\nAll memories (5506) 22–625 255.24 (92.36)\n0.34\n(low similarity)Truthful memories (2770) 22–625 269.78 (94.14)\nDeceptive memories (2736) 22–609 240.51 (88.12)\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\ntwo  sets50,56. The resulting index ranges from 0 to 1, with 0 indicating a completely different vocabulary between \nthe two sets, and 1 indicating a completely identical vocabulary between the two sets. We reported the Jaccard \nsimilarity index to provide a measure of similarity or overlap between the word choices of truthful and decep -\ntive statements within the respective datasets. Supplementary Information offers a detailed methodology for \ncalculating the Jaccard similarity index.\nFLAN-T5\nWe adopted FLAN-T5, an LLM developed by Google researchers and freely available through HuggingFace \nPython’s library Transformers (https:// huggi ngface. co/ docs/ trans forme rs/ model_ doc/ flan- t5). HugginFace is \na company that provides free access to state-of-the-art LLMs through Python API. Among the available LLMs, \nwe chose FLAN-T5 because of its valuable trade-off between computational load and goodness of the learned \nrepresentation. FLAN-T5 is the improved version of MT-5, a text-to-text general model capable of solving many \nNLP tasks (e.g., sentiment analysis, question answering, and machine translation), which has been improved by \npre-training57. The peculiarity of this model is that every task they were trained on is transformed into a text-\nto-text task. For example, while performing sentiment analysis, the output prediction is the string used in the \ntraining set to label the positive or negative sentiment of each phrase rather than a binary integer output (e.g., \n0 = positive; 1 = negative). Hence, their power stands in both the generalized representation of natural language \nlearned during the pre-training phase and the possibility of easily adapting the model to a downstream task with \nlittle fine-tuning without adjusting its architecture.\nDeCLaRatiVE stylometric analysis\nThis study employed stylometric analysis to achieve two primary objectives. First, we aimed to describe the \nlinguistic features that distinguished the three datasets before initializing the fine-tuning process. Second, we \nconducted explainability analysis to gain insights into the role of linguistic style that differentiated truthful and \ndeceptive statements in the model’s classification process. For this purpose, a new framework that we referred to \nas DeCLaRatiVE stylometry was adopted, which involved the extraction of 26 linguistic features in conjunction \nwith the psychological frameworks of D istancing29, Cognitive Load30,31, Reality Monitoring32,34, and VErifi-\nability  Approach40,41. A full list of the 26 linguistic features with a short description is shown in Table  3. This \ncomprehensive approach enabled the analysis of verbal cues of deception from a multidimensional perspective.\nFeatures associated with the CL framework consisted of statistics about the length, readability, and complex-\nity of the  text14,58–60 and were extracted using the Python library TEXTSTAT. Features related to the Distancing \nTable 3.  List and short description of the 26 linguistic features pertaining to the DeCLaRatiVE Stylometry \ntechnique.\nLabel Description\nnum_sentences Total number of sentences\nnum_words Total number of words\nnum_syllables Total number of syllables\navg_syllabes_per_word Average number of syllables per word\nfk_grade Index of the grade level required to understand the text\nfk_read Index of the readability of the text\nAnalytic LIWC summary statistic analyzing the style of the text in term of analytical thinking (0–100)\nAuthentic LIWC summary statistic analyzing the style of the text in term of authenticity (0–100)\nTone Standardized difference (0–100) of ‘tone_pos’—‘tone_neg’\ntone_pos Percentage of words related to a positive sentiment (LIWC dictionary)\ntone_neg Percentage of words related to a negative sentiment (LIWC dictionary)\nCognition Percentage of words related to semantic domains of cognitive processes (LIWC dictionary)\nmemory Percentage of words related to semantic domains of memory/forgetting (LIWC dictionary)\nfocuspast Percentage of verbs and adverbs related to the past (LIWC dictionary)\nfocuspresent Percentage of verbs and adverbs related to the present (LIWC dictionary)\nfocusfuture Percentage of verbs and adverbs related to the future (LIWC dictionary)\nSelf-reference Sum of LIWC categories ‘i’ + ‘we’\nOther-reference Sum of LIWC categories ‘shehe’ + ‘they’ + ‘you’\nPerceptual details Sum of LIWC categories ‘attention’ + ‘visual’ + ‘auditory’ + ‘feeling’\nContextual Embedding Sum of LIWC categories ‘space’ + ‘motion’ + ‘time’\nReality Monitoring Sum of Perceptual details + Contextual Embedding + Affect—Cognition\nConcreteness score Mean of concreteness score of words\nPeople Unique named-entities related to people: e.g., ‘Mary’ , ‘Paul’ , ‘ Adam’\nTemporal details Unique named-entities related to time: e.g., ‘Monday’ , ‘2:30 PM’ , ‘Christmas’\nSpatial details Unique named-entities related to space: e.g., ‘airport’ , ‘Tokyo’ , ‘Central park’\nQuantity details Unique named-entities related to quantities: e.g., ‘20%’ , ‘5 $’ , ‘first’ , ‘ten’ , ‘100 m’\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nand RM framework were computed using  LIWC42,43, the gold standard software for analyzing word usage. Using \nthe English dictionary, we scored each text along with all the categories present in LIWC-22. LIWC scoring was \ncomputed on tokenized text using the English dictionary. The selection of the LIWC categories related to the \nDistancing and RM framework was guided by previous research on computerized verbal lie-detection29,49,50,52,56 \nand a recent  metanalysis14. RM was also investigated through linguistic concreteness of  words39. To determine \nthe average level of concreteness for each statement, we utilized the concreteness annotation dataset developed \nby Brysbaert et al. 61. For the calculation of concreteness scores, a preprocessing pipeline was applied to textual \ndata using the Python library SpaCy: text was converted to lowercase and tokenized; then stop words were \nremoved, and the remaining content words were lemmatized. These content words were then cross-referenced \nwith the annotated concreteness dataset to assign the respective concreteness value when a match was found. \nThe concreteness score for each statement was then computed as the average of the concreteness scores for all \nthe content words in that statement. For what concerns verifiable details, they were estimated by the frequency \nof unique named entities. Named entities were extracted with the NER technique using Python’s library SpaCy \nthrough the Transformer algorithm for English language (en_core_web_trf, https://  spacy. io/ models/ en# en_ \ncore_ web_ trf).\nFurther details on how the 26 linguistic features were computed are provided in the Supplementary \nInformation.\nExperimental set-up\nIn this section, we describe the methodology that we applied in this work. As a first step, we wanted to perform \na descriptive linguistic analysis of our datasets, trying to provide a response to Hypothesis 5a), i.e., whether the \nlinguistic style distinguishing truthful from deceptive statements varies across different contexts. To achieve this \nresult, we employed the DeCLaRatiVE stylometric analysis. As a second step, we proceeded to test the capacity \nof the FLAN-T5 model to be fine-tuned on a Lie Detection task. To do so, we provided three scenarios to verify \nthe following hypothesis:\n• Hypothesis 1a): Fine-tuning an LLM can effectively classify the veracity of short narratives from raw texts, \n1b) outperforming classical machine learning and deep learning approaches in verbal lie detection.\n• Hypothesis 2): Fine-tuning an LLM on deceptive narratives enables the model to also detect new types of \ndeception;\n• Hypothesis 3): Fine-tuning an LLM on a multiple-context dataset enables the model to obtain successful \npredictions on amulti-context test set;\n• Hypothesis 4): Model performance depends on model size, with larger models showing higher accuracy;\nWe expected hypotheses 1a, 1b, 3, and 4 to be verified, while we did not have any a priori expectation for the \nsecond hypothesis. The scenarios are described below:\n1. Scenario 1: The model was fine-tuned and tested on a single dataset. This procedure was repeated for each \ndataset with a different copy of the same model each time (i.e., the same parameters before the fine-tuning \nprocess) (Fig. 1). This Scenario assesses the model’s capacity to learn how to detect lies related to the same \ncontext and responds to Hypothesis 1a;\n2. Scenario 2: The model was fine-tuned on two out of the three datasets and tested on the remaining unseen \ndataset. As for the previous Scenario, this procedure was iterated three times, employing separate instances \nof the same model, each time with a distinct combination of dataset pairings (Fig. 2). This Scenario assesses \nhow the model performs on samples from a new context to which it has never been exposed during the \ntraining phase and provides a response for Hypothesis 2;\n3. Scenario 3: We first aggregated the three train and test sets from Scenario 1. Then, we fine-tuned the model \non the aggregated datasets and tested the model on the aggregated test sets (Fig.  1). This Scenario assesses \nthe capacity of the model to learn and generalize from samples of truthful and deceptive narratives from \nmultiple contexts and provides a response for Hypothesis 3.\nIn Scenarios 1 and 3, each experiment underwent a 10-fold cross-validation. N-fold cross-validation is a \nstatistical method used to estimate the performance of a model by dividing the dataset into n partitions (n  = 10 \nfor this study). For each partition i, we created a training set composed of the remaining n−1 partitions using the \ni partition as a test set (i.e., 90% of the data belongs to the training set, and 10% of the remaining data belongs \nto the test set). For each iteration, performance metrics are computed on the test set, stored, and then averaged. \nThis procedure ensures an unbiased performance estimation and allows a fair comparison between different \nmodels. For our study, we employed identical train-test splits within scenarios 1 and 3 and for both model sizes \nto guarantee a fair performance comparison. The average test accuracy from each fold and its corresponding \nstandard deviation are presented as performance metrics. Conversely, in Scenario 2, each pairing combination \nunderwent fine-tuning using the entire two paired datasets as a training set, while the model’s performance was \nassessed using the complete unseen dataset as a test set.\nNotably, the Opinion dataset was developed to have each participant’s truthful and deceptive statements for \na total of five opinions. Therefore, we treated each opinion as a separate sample. In order to avoid the model \nexhibiting inflated performance on the test set as a result of learning the participants’ linguistic style, we adopted \nthe following precautionary measure. Specifically, we ensured an exclusive division of participants between the \n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\ntraining and test sets, such that any individual who had their opinions assigned to the training set did not have \ntheir opinions assigned to the test set, and vice versa.\nTogether, Scenarios 2 and 3 provide evidence about the generalized capabilities of the fine-tuned FLAN-T5 \nmodel in a lie-detection task when tested on unseen data and on a multi-domain dataset. Furthermore, we tested \nwhether model performance may depend on model sizes. Therefore, we first fine-tuned the small-sized version \nof FLAN-T5 in every scenario, and then we repeated the same experiments in every scenario with the base-sized \nversion, providing a response for Hypothesis 4.\nTo test Hypothesis 1b, i.e., to test the advantage of our approach when compared to classical machine learning \nmodels, we decided to compare the results with two benchmarks:\n1. A basic approach consisting of a bag-of-words (BoW) encoder plus a logistic regression  classifier62 (following \nthe experimental procedure of Scenario 1);\nFigure 1.  Visual illustration of the Scenarios 1 and 3.\nFigure 2.  Visual illustration of the Scenario 2.\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\n2. A literature baseline based on previous studies providing accuracy metrics on the same datasets using a \nmachine learning or a deep learning  approach49–51. For the Opinion dataset -characterized by opinions on \nfive different topics per subject- we compared our results to the performance obtained  in51 with respect to \ntheir “within-topic” experiments because our approach is equivalent to theirs, with the only difference that \nwe addressed all the topics in one model.\nAs a final step, we conducted an explainability analysis to investigate the differences in linguistic style between \nthe truthful and deceptive statements that were correctly classified and misclassified by the model. This procedure \naimed to provide a response to Hypothesis 5b, i.e., whether the model takes into account the linguistic style of \nstatements for its final predictions. To achieve this result, we employed the DeCLaRatiVE stylometric analysis.\nIn Fig. 3, we provided a flow chart of the whole experimental set-up.\nFine-tuning strategy\nFine-tuning of LLMs consists of adapting a pre-trained language model to a specific task by further training the \nmodel on task-specific data, thereby enhancing its ability to generate contextually relevant and coherent text in \nline with the desired task  objectives57. We fine-tuned FLAN-T5 in its small and base size using the three datasets \nand following the experimental set-up described above. We approached the lie-detection task as a binary clas -\nsification problem, given that the three datasets comprised raw texts associated with a binary label, specifically \ninstances classified as truthful or deceptive.\nTo the best of our knowledge, no fine-tuning strategy is available in the literature for this novel downstream \nNLP task. Therefore, our strategy followed an adaptation of the Hugginface’s guidelines on fine-tuning an LLM \nfor translation. Specifically, we chose the same optimization strategy used to pre-train the original model and \nthe same loss function.\nNotably, the classification task between deceptive and truthful statements has never been performed during \nthe FLAN-T5 pre-training phase, nor is it included in any of the tasks the model has been pre-trained on. There-\nfore, we performed the same experiments, described in the Experimental set-up section, multiple times with \ndifferent learning rate values (i.e., 1e−3, 1e−4, 1e−5), and we finally chose the configuration shown in Table  4, \nwhich yielded the best performance in terms of accuracy. All experiments and runs of the three scenarios were \nconducted on Google Colaboratory Pro + using their NVIDIA A100 Tensor Core GPU.\nStatistical procedure for descriptive linguistic analysis\nAfter applying the DeCLaRatiVE stylometry technique, we obtained a stylistic vector of 26 linguistic features \nfor each text of the three datasets.\nIn order to assess the significance of the observed differences between the groups, a permutation t-test was \n employed63. This non-parametric method involves pooling all observations and then randomly redistribut-\ning them into two groups, preserving the original group sizes. The test statistic of interest (i.e., the difference \nin means) is then computed for these permuted groups. By repeating this process thousands of times (i.e., \nn = 10,000), we generated a test statistic distribution under the null hypothesis of no difference between the \ngroups. The observed test statistic from the actual data was then compared to this distribution to compute a \np-value, indicating the likelihood of observing such a difference if the null hypothesis was true. The advantage \nof using a permutation t-test is that no assumption about the distribution of data is needed. This analysis was \nconducted in Python using SciPy and Pingouin library.\nFigure 3.  Visual illustration of the whole experimental set-up. The Opinion, Memory, and Intention dataset \nunderwent Descriptive Linguistic Analysis using DeCLaRatiVE stylometry. A baseline model consisting of \nBag of Words (BoW) and Logistic Regression (Scenario 1) was also established for the three datasets. Then, the \nFLAN-T5 model in small and base versions was fine-tuned across Scenarios 1, 2, and 3. Finally, an Explainability \nAnalysis was conducted on the top-performing model using DeCLaRatiVE stylometry to interpret the results.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nFor the Memory and Intention dataset, we computed a permutation t-test (n  = 10,000) for independent \nsamples for the 26 linguistic features to outline significant differences among the truthful and deceptive texts.\nFor the Opinion dataset, our analysis proceeded as follows. Firstly, we computed the DeCLaRatiVE stylom-\netry technique for all the subjects’ opinions. This resulted in a 2500 (opinions) × 26 (linguistic features) matrix. \nThen, since each subject provided five opinions (half truthful and half deceptive), we averaged the stylistic vec-\ntor separately for the truthful and deceptive sets of opinions. This procedure allowed us to obtain two different \naveraged stylistic vectors for the same subject, one for the truthful opinions and one for the deceptive opinions. \nImportantly, this averaging process enabled us to obtain results that are independent of the topic (e.g., abortion or \ncannabis legalization) and the stance taken by the subject (e.g., in favor or against that particular topic). Finally, \nwe validated the statistical significance of these differences by conducting a paired sample permutation test \n(n = 10,000). Results for each dataset were corrected for multiple comparisons with Holm-Bonferroni correction.\nThe effect size was expressed by Common Language Effect Size (CLES) with a confidence interval of 95% \n(95% CI), which is a measure of effect size that is meant to be more intuitive in its understanding by providing \nthe probability that a specific linguistic feature, in a picked-at-random truthful statement, will have a higher \nscore than in a picked-at-random deceptive  one64. The null value for the CLES is the chance level at 0.5 (in a \nprobability range from 0 to 1) and indicates that, when sampled, one group will be greater than the other, with \nequal chance. Cohen’s d effect size with 95% CI was also computed to add interpretation.\nStatistical procedure for explainability analysis\nTo examine whether the linguistic style of the input statements exerted an influence on the resulting output of the \nmodel and to provide explanations for the wrong classification outputs, we applied a DeCLaRatiVE stylometric \nanalysis of statements correctly classified and misclassified by the top-performing model identified in Scenario \n3 (FLAN-T5 base).\nTo this aim, during each iteration from cross-validation, we paired the sentences belonging to the test set \nand their actual labels with the labels predicted by the model. After the cross-validation ended, for each of the \nten folds and for each of the 26 linguistic features of the sentences that composed the test set for that fold, we \nperformed a non-parametric permutation t-test for independent samples (n = 10,000) for the following com-\nparison of interest:\na. Truthful statements misclassified as deceptive (False Negatives), with deceptive statements misclassified as \ntruthful (False Positives);\nb. Statements correctly classified as deceptive (True Negatives) vs. truthful statements misclassified as deceptive \n(False Negatives);\nc. Statements correctly classified as truthful (True Positives) vs. deceptive statements misclassified as truthful \n(False Positives).\nd. Truthful versus deceptive statements correctly classified by the model (True Positives vs. True Negatives).\nTo compute the effect size, we computed the average of the CLES and Cohen’s d effect size scores with their \nrespective 95% CI obtained from each fold.\nResults\nDescriptive linguistic analysis\nThis section outlines the results of the descriptive linguistic analysis in terms of DeCLaRatiVE stylometric \nanalysis to compare the three datasets on linguistic features.\nTable 4.  FLAN-T5 hyperparameters configuration for the small- and base-sized version. The initial learning \nrate for every scenario was 5e−4 for the small model and 5e−5 for the base model. This choice was motivated \nby preliminary experiment results, with the smaller model, but not the base model, generally performing better \nwith higher learning rates. The weight decay coefficient was set to 0.01 in all models and Scenarios. The batch \nsize was set to 2 for computational reasons, specifically to avoid running out of available memory, even though \nit is known that a larger batch size usually leads to better performance. Finally, the number of epochs was set to \n3 after preliminary experiments showing the maximum test accuracy after the third epoch without overfitting.\nModel Hyperparameter Value\nFLAN-T5 small\nLearning rate 5e−4\nWeight decay coefficient 0.01\nBatch size 2\nNumber of Epochs 3\nFLAN-T5 base\nLearning rate 5e−5\nWeight decay coefficient 0.01\nBatch size 2\nNumber of Epochs 3\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nFigure 4.  Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the \nsignificant linguistic features that survived post-hoc corrections in the Opinion dataset. The CLES estimates \nrepresent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful opinions (sky \nblue) than in deceptive ones (salmon). The CLES for truthful opinions are sorted in descending order, while the \nCLES for deceptive opinions are sorted in ascending order.\nFigure 5.  Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the \nsignificant linguistic features that survived post-hoc corrections in the Memory dataset. The CLES estimates \nrepresent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful memories (sky \nblue) than in deceptive ones (salmon). The CLES for truthful memories are sorted in descending order, while \nthe CLES for deceptive memories are sorted in ascending order.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nFor the three datasets, Figs.  4, 5, and 6 show the differences in the number, the type, the magnitude of the \nCLES effect size, and the direction of the effect for the linguistic features that survived post-hoc corrections. \nTo make an example of these differences, the concreteness score of words (‘concr_score’) presented the larg-\nest CLES within the Intention dataset towards the truthful statements (Fig.  6), while in the Opinion dataset, \nit showed the largest CLES towards the deceptive statements (Fig.  4). Overall, the Intentions dataset displayed \nfewer significant differences in linguistic features among truthful and deceptive statements than the Opinion \nand Memory datasets. In Table S5 (Supplementary Information), we reported, for all the linguistic features and \nthe three datasets, all the statistics, the corrected p-values, the effect-size scores expressed by CLES and Cohen’s \nD with 95% CI, and the direction of the effect.\nPerformance on the lie-detection classification task\nThis section presents the performance, in terms of averaged accuracy (and standard deviation) of the 10-folds, \non the test sets after the last epoch of the small and base model in all the Scenarios.\nScenario 1\nIn Table 6 are depicted the test accuracies for the FLAN-T5 model, categorized by dataset and model size in \nScenario 1. In each case, the base model, on average, outperformed the small model, with the Memory dataset \nshowing the largest improvement of 4% and the Intention dataset showing just a 0.06% increase in average \naccuracy. These results indicate that the larger model size generally leads to improved performance across the \nthree datasets, with higher accuracy observed in the base version.\nScenario 2\nThis scenario aimed to investigate our fine-tuned LLM’s generalization capability across different deception \ndomains. As presented in Table  5, the test accuracy for the three experiments in this scenario significantly \ndropped to the chance level, showing that the model, in any case, was able to learn a general rule to detect lies \ncoming from different contexts.\nScenario 3\nIn Scenario 3, we tested the accuracy of the FLAN-T5 small and base version on the aggregated Opinion, \nMemory, and Intention datasets. The small-sized FLAN-T5 achieved an average test accuracy of 75.45% (st. \ndev. ± 1.6), while the base-sized FLAN-T5 exhibited a higher average test accuracy of 79.31% (st. dev. ± 1.3). \nIn other words, the base-sized model outperformed the small model by approximately four percentage points.\nResults in Table 6 show the disaggregated performance on individual datasets between the small and base \nFLAN-T5 models in Scenario 3, with a comparison to their counterparts in Scenario 1. These comparisons show \nthat FLAN-T5-small in Scenario 3 exhibited worse performance than in Scenario 1. Instead, in Scenario 3, the \nFigure 6.  Horizontal stacked bar chart presenting the Common Language Effect Size (CLES) estimates for the \nsignificant linguistic features that survived post-hoc corrections in the Intention dataset. The CLES estimates \nrepresent the probability (ranging from 0 to 1) of finding a specific linguistic feature in truthful intentions (sky \nblue) than in deceptive ones (salmon). The CLES for truthful intentions are sorted in descending order, while \nthe CLES for deceptive intentions are sorted in ascending order.\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nbase model barely outperformed its counterparts of Scenario 1 on the Opinion and Intention datasets by less \nthan 1% and slightly underperformed its counterpart of Scenario 1 on the Memory dataset.\nWe identified the top-performing model as the FLAN-T5 base in Scenario 3 because of its higher accuracy \nin the overall performance. The averaged confusion matrix of the 10 folds for this model is depicted in Fig.  7.\nNotably, in any case, we were able to outperform both the bag of word + logistic regression classifier baseline \nand the performance achieved on the same datasets in previous  studies49–51.\nExplainability analysis\nThis section aims to gain a deeper understanding of the top-performing model identified in Scenario 3 (FLAN-\nT5 base) through a DeCLaRatiVE stylometric analysis of statements correctly classified and misclassified by the \nmodel. The purpose of this analysis was to examine whether the linguistic style of the input statements exerted an \ninfluence on the resulting output of the model and to provide explanations for the wrong classification outputs. \nFor this analysis, we compared:\na. Truthful statements misclassified as deceptive (False Negatives), with deceptive statements misclassified as \ntruthful (False Positives);\nb. Statements correctly classified as deceptive (True Negatives) vs. truthful statements misclassified as deceptive \n(False Negatives);\nc. Statements correctly classified as truthful (True Positives) vs. deceptive statements misclassified as truthful \n(False Positives).\nd. Truthful vs. deceptive statements correctly classified by the model (True Positives vs. True Negatives).\nThe statistically significant features reported survived post-hoc correction for multiple comparisons in each \nfold. Overall, for comparison a), b), and c), we observed no statistically significant differences (p  < 0.05) in any \nlinguistic features for most of the splits with the only exception of:\n1) ‘fk_read’ in fold 1 (t = 5.30; p = 0.04, CLES = 0.63 [0.55, 0.71], d = 0.46 [0.18, 0.75]) and ‘Reality Monitoring’in \nfold 6 (t = 4.74; p = 0.047, CLES = 0.62 [0.54, 0.70], d = 0.46 [0.17, 0.75]) for the a) comparison;\n2) ‘Reality Monitoring’in fold 6 (t  = −3.39, p  = 0.04, CLES  = 0.40 [0.34, 0.46], d  = −0.34 [−0.55, −0.13]) and \n‘Reality Monitoring’ (t = −3.16 p = 0.04, CLES = 0.41 [0.34, 0.47], d  = −0.34 [−0.56, −0.12]) and ‘Contextual \nEmbedding’ (t = −2.11; p = 0.01, CLES = 0.39 [0.33, 0.45], d = −0.42 [−0.63, −0.2]) in fold 7 the b) comparison;\nTable 5.  Test accuracy of FLAN-5 models in scenario 2 (three combination of train sets). The performance \ncomparison is among the small and base version of the FLAN-T5 model in the three combination of train set: \nopinion + memory, opinion + intention, memory + intention.\nTrain set Test set Model size Test accuracy\nOpinion + Memory Intention\nFLAN-T5 small 55.37\nFLAN-T5 base 55.67\nOpinion + Intention Memory\nFLAN-T5 small 55.37\nFLAN-T5 base 54.23\nMemory + Intention Opinion\nFLAN-T5 small 53.12\nFLAN-T5 base 49.40\nTable 6.  Test acccuracy of the FLAN-T5 models in Scenarios 1 and 3 for the three datasets. Reported values \nare means ± standard deviation of the 10 folds. Best results per evaluation metric are in bold. The literature \nbaseline for the Opinion dataset refers to the average accuracy and standard deviation from all within-topic \naccuracies from FastText Embedding +  Transformer51. The literature baseline for the Intention dataset refers \nto the accuracy from Vanilla Random Forest using LIWC features (confidence interval in square brackets)49, \nthe averaged accuracy and standard deviation from RoBERTa + Transformers + Co-Attention model and \nBERT + co-attention  model50 respectively.\nModel Opinion Memory Intention\nBag-of-words baseline 76.16 ± 2.9% 57.57 ± 7.66% 67.07 ± 3.18%\nLiterature baseline 65.16 ± 5.7% –\n69.00 [63; 74] %\n69.86 ± 2.34%\n70.61 ± 2.58%\nFLAN-T5 small—Scenario 1 80.64 ± 2.03% 76.87 ± 2.06% 71.46 ± 3.65%\nFLAN-T5 base—Scenario 1 82.60 ± 3.01% 80.61 ± 1.41% 71.52 ± 2.21%\nFLAN-T5 small—Scenario 3 79 ± 2.11% 75.67 ± 1.90% 69.32 ± 3.75%\nFLAN-T5 base—Scenario 3 82.72 ± 2.39% 79.87 ± 1.60% 72.25 ± 2.86%\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\n3) ‘num_syllables‘(t = 76.87, p = 0.01, CLES = 0.64 [0.57, 0.7], d = 0.46 [0.27, 0.7]) and ‘word_counts’(t = 59.63, \np = 0.01, CLES = 0.64 [0.57, 0.71], d = 0.46 [0.21, 0.7]) in fold 9 for the c) comparison.\nConversely, for the d) comparison, several significant features emerged in all the folds and survived correc -\ntions for multiple comparisons. Figure 8 depicts the CLES effect size scores of linguistic features, sorted accord-\ning to the number of times they were found to be significant among the ten folds. The top six features in Fig.  8 \nrepresented a cluster of linguistic features related to the Cognitive Load framework.\nDiscussion\nIn the present research, we investigated the efficacy of a Large Language Model, specifically FLAN-T5 in its small \nand base version, in learning and generalizing the intrinsic linguistic representation of deception across differ -\nent contexts. To accomplish this, we employed three datasets encompassing genuine or fabricated statements \nregarding personal opinions, autobiographical experiences, and future intentions.\nDescriptive linguistic analysis\nDescriptive linguistic analysis was performed to compare the three datasets on linguistic features by exploring \nthe differences in the D eCLaRatiVE style, i.e., analyzing 26 linguistic features extracted from the psychologi-\ncal frameworks of D istancing, Cognitive Load, Reality monitoring, and VErifiability approach. This analysis \naimed to test Hypothesis 5a, which postulates a variation in the linguistic style that differentiates truthful from \ndeceptive statements across varying contexts (i.e., personal opinions vs. autobiographical memories vs. future \nintentions). The results from this analysis confirmed our hypothesis, showing that the linguistic features exhibit-\ning statistically significant differences between truthful and deceptive statements indeed varied across datasets. \nThis variation was observed in terms of the total number and type of features, the magnitude of the effect size \n(from very small to medium), and the direction of the effect. In the following paragraphs, the interpretation of \nthe significant linguistic features of each dataset will be discussed.\nOpinions\nAfter analyzing truthful and deceptive opinions using the DeCLaRatiVE stylometry, different linguistic features—\nrelated to the theoretical frameworks of CL, RM, and Distancing—were found to be significant.\nFigure 7.  Averaged confusion matrix of the top-performing model identified as FLAN-T5 base in Scenario \n3. In each square, the results obtained represent the average (and standard deviation) from the test set of each \niteration of the 10-fold cross-validation.\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nIn line with the CL framework, we observed that truthful opinions were characterized by greater complexity, \nverbosity, and more authenticity in linguistic  style14,31.\nFor features related to the RM framework, truthful opinions were characterized by a lesser number of con -\ncrete words and a greater number of cognitive words, as also previously  shown55; in contrast, deceptive opinions \nshowed higher scores in the concreteness of words, contextual details, and reality monitoring. These differ -\nences may reflect on one side the reasoning processes that truth-tellers engage in evaluating the pros and cons \nof abstract and controversial concepts (e.g., abortion), while for deceivers, it may be indicative of difficulty in \nabstraction, resulting in faked opinions that sound more grounded in reality.\nFinally, in line with previous literature on distancing  framework29,65 and deceptive  opinions20,55, deceivers \nutilized more other-related word classes (‘Other-reference’) and fewer self-related words (‘Self-reference’), con-\nfirming that individuals may tend to avoid personal involvement when expressing deceptive statements.\nMemories\nFollowing the analysis of truthful and deceptive narratives of autobiographical memories through DeCLaRatiVE \nstylometry, various linguistic features associated with the theoretical frameworks of CL, RM, V A, and Distancing \nwere found to be significant.\nAs for opinions, according to the CL framework, truthful narratives of autobiographical memories exhibited \nhigher levels of complexity and verbosity and appeared to be more analytical in  style14,31.\nIn accordance with the RM  framework32–37, posing that truthful memory accounts tend to reflect the percep-\ntual processes involved while experiencing the event while fabricated accounts are constructed through cognitive \noperations, we found genuine memories exhibiting higher scores in memory-related words and the number of \nwords associated with spatial and temporal information (‘Contextual Embedding’), as well as an overall higher \nRM score. Conversely, we found deceptive memories showing higher scores in words related to cognitive pro -\ncesses (e.g., reasoning, insight, causation). Furthermore, in line with Kleinberg’s truthful concreteness hypothesis \n39, truthful memories were overall characterized by words with higher scores of concreteness.\nAlong with the V A, truthful memories contained more verifiable details, as indicated by the greater num-\nber of named entities about times and  locations23,48. Notably, we found this effect although participants lied in \na low-stake scenario. However, deceptive memories were unexpectedly characterized by a higher number of \nself-references and named entities of ‘People’ . This result is in contrast with previous literature on distancing \n framework14,29. One possible explanation of this significant but small effect is that liars may try to increase their \ncredibility by fostering a sense of social connection.\nIntentions\nUpon examining truthful and deceptive statements of future intentions through DeCLaRatiVE stylometry, sev-\neral linguistic features were found to be significant. Our findings are consistent with previous research claiming \nthat genuine intentions contain more ‘how-utterances’ , i.e., indicators of careful planning and concrete descrip-\ntions of activities. In contrast, false intentions are characterized by ‘why-utterances’ , i.e., explanations and reasons \nFigure 8.  Linguistic features in Truthful and Deceptive statements that were accurately classified by FLAN-T5 \nbase in Scenario 3. The bar plot shows the averaged Common Language Effect Size among the ten folds of \nlinguistic features that survived post-hoc corrections. Linguistic features are sorted in descending order \naccording to the number of times they were found to be significant among the 10 folds (displayed at the side of \neach bar). Linguistic features higher on average in truthful texts are shown in sky blue, while those higher on \naverage in deceptive texts are shown in salmon.\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nfor why someone planned an activity or for doing something in a certain  way48. Indeed, we found true intentions \nwere more likely to provide concrete and distinct information about the intended action, grounding their state-\nments in real-world experiences and providing temporal and spatial references. Additionally, true intentions \nwere characterized by a more analytical style and a greater presence of numerical entities. In contrast, false \nintentions exhibited a higher number of cognitive words and expressions and were temporally oriented toward \nthe present and past.\nFurthermore, we found evidence in line with the claim that liars may over-prepare their  statements48, as \nindicated by higher verbosity. Finally, in contrast with the distancing  framework14,29, we found a significantly \nhigher proportion of self-references and mentions of people in deceptive statements. However, the effect size for \nthis finding was small. As for deceptive memories, one possible interpretation is that liars may attempt to appear \nmore credible by creating a sense of social connection.\nLie detection task\nIn order to test the capacity of the FLAN-T5 model to be fine-tuned on a Lie Detection task, we developed three \nscenarios.\nIn Scenario 1, we tested whether fine-tuning LLMs can effectively classify the veracity of short statements \nbased on raw texts with performance highly above the chance level (Hypothesis 1a). To this aim, we fine-tuned \nFLAN-T5 in its small version to perform lie detection as a classification task. We repeated this procedure for \nthe three datasets (i.e., opinions vs. memories vs. intentions). This fine-tuning process yielded promising results \nconfirming our hypothesis, with an average accuracy of 80.64% (st. dev. ± 2.03%) for the Opinion dataset, 76.87% \n(st. dev. ± 2.06%) for the Memory dataset, and 71.46% (st. dev. ± 3.65%) for the Intention dataset.\nIn Scenario 2, we tested whether fine-tuning an LLM on deceptive narratives enables the model to detect new \ntypes of deception (Hypothesis 2). To verify this hypothesis, we fine-tuned FLAN-T5 (small version) on two \ndatasets and tested on the third one (e.g., train: opinion  + memory; test: intention). Our findings show that the \nmodel performed at chance level in all three combinations of this Scenario, suggesting that there are no universal \nrules the model can learn to distinguish truthful from deceptive statements, enabling a generalization of the \ntask across different contexts. Indeed, as shown in the Descriptive Linguistic Analysis section, the three datasets \ndiffered significantly in terms of the content and the linguistic style by which truthful and deceptive narratives \nare delivered. Therefore, the model struggled to identify a specific pattern of linguistic deception and appeared \nto engage a domain-specific learning, tailoring its classification capabilities to that specific domain of deception.\nIn Scenario 3, we tested whether fine-tuning an LLM on a multiple-context dataset enables the model to \nobtain successful predictions on a multi-context test set (Hypothesis 3). At this aim, we fine-tuned and tested \nFLAN-T5 (small version) with the three aggregated datasets (i.e., opinion + memory + intention). The small-sized \nFLAN-T5 achieved an average accuracy of 75.45% (st. dev. ± 1.6). Additionally, the disaggregated performance \non individual datasets compared to their counterpart in Scenario 1 exhibited solely a small decrease in accuracy \n(around 1%). These findings confirmed our hypothesis, providing evidence of LLMs’ ability to generalize when \nfine-tuned and texted on a multi-context dataset, in contrast to previous empirical evidence showing a decline \nin performance in machine learning models on the same  scenarios53–55.\nTo test whether the model performance increases when employing larger models (Hypothesis 4), we repeated \nthe same experiments in Scenarios 1, 2, and 3 with the base version of FLAN-T5.\nIn Scenario 1, we found that the base version of FLAN-T5 provided higher accuracy than the small version. \nIn Scenario 3, the base version of the model achieved an average accuracy of 79.31% (st. dev. ± 1.3), outper -\nforming the small model by approximately four percentage points. Additionally, this increase in the general \naccuracy did not compromise the performance on any individual dataset when compared to what achieved by \nthe smaller model or by the FLAN-T5 base in Scenario 1. In contrast, the base version of FLAN-T5 in Scenario \n2 still obtained performance around the chance level.\nOn one hand, the findings obtained from the base model in Scenarios 1 and 3 confirmed the hypothesis that \nthe model size does influence the performance, likely because a bigger model is able to learn a better representa-\ntion of linguistic patterns of genuine and deceptive narratives. Specifically, in Scenario 3, the FLAN-T5 base, with \nits larger size, possessed the capability to comprehend and integrate the features of the three distinct datasets \naltogether, thereby maintaining consistent performance across all individual datasets. In contrast, the smaller \nFLAN-T5 in Scenario 3 seemed to relinquish certain specialized abilities that are beneficial for specific datasets \nto classify deception across different contexts.\nOn the other hand, findings from Scenarios 2 and 3 (with small and base FLAN-T5) showed that LLMs, \ndespite having acquired a comprehensive understanding of language patterns, still require exposure to prior \nexamples to accurately classify deceptive texts within different domains.\nFinally, to test whether our approach outperforms classical machine learning and deep learning approaches \nin verbal lie detection (Hypothesis 1b), we compared the results obtained from FLAN-T5 in its small and base \nversions with the performance of a simpler baseline of a logistic regressor based on BoW  embedding 62 and \nof Transformer models previously employed in the literature on the  Opinion51 and Intention  datasets49,50.\nSpecifically, when comparing the Memory dataset to the logistic regression baseline, there was a 32% increase \nin performance. This improvement might be attributed to the longer and more complex nature of the stories in \nthe Memory dataset, which challenges the effectiveness of more straightforward methods like logistic regression \nbased on BoW in a lie detection task. In contrast, LLMs already possess a robust language representation; thus, \nfine-tuning LLMs leverages this representation, tailoring their NLP proficiency specifically for a lie detection \ntask, yielding higher accuracy.\nThe performance gained by fine-tuning LLMs was less pronounced for the Opinion and Intention datasets. \nFor the Opinion dataset, this could be due to the relative ease of classification in these datasets, where simpler \n16\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nmodels can already achieve good performance, leaving a smaller margin for improvement. Nonetheless, the \ndifference between our approach and the baselines is not negligible. In the Opinion dataset, we outperformed \nthe literature baseline of a Transformer model trained from scratch by 17% accuracy and surpassed our logistic \nregression baseline by six percentage points. For the Intention dataset, our approach showed a 5-percentage \npoint improvement over the logistic regression baseline and around 1–2% improvement over the best literature \nbaseline. Notably, the best literature baseline for the Intention dataset (averaged accuracy: 70.61 ± 2.58%) used a \nsimilar approach to ours in terms of the type of model used, involving a Transformer-based model (BERT + Co-\nattention), which may explain the narrower performance gap.\nBesides the differences in performance, the main advantage of our approach is its simplicity and flexibility \ncompared to those used in previous  studies49–51. Fine-tuning an LLM leverages an existing encoding of lan-\nguage that effortlessly handles any type of statement, unlike logistic regression based on BoW or training a new \nTransformer-based model from scratch. Taking all these aspects together, fine-tuning LLMs resulted in being \nmore advantageous in terms of feasibility, flexibility, and performance accuracy.\nExplainability analysis\nTo improve the explainability of the performance collected, we investigated whether the linguistic style that \ncharacterizes truthful and deceptive narratives could have a role in the model’s final predictions (Hypothesis \n5b). For this aim, we applied a D eCLaRatiVE stylometric analysis on statements that were correctly classified \nand misclassified by the top-performing model identified in Scenario 3 (i.e., FLAN-T5 base).\nIn the misclassified sample, truthful and deceptive statements did not differ significantly for any linguistic \nfeature extracted with the D eCLaRatiVE stylometry technique. The only exception was fold 1, which showed \nsignificant differences in the text’s readability score, and fold 6, which showed significant differences in ’Real -\nity Monitoring’ scores. No significant differences were detected in each fold in linguistic features between decep-\ntive statements that were correctly classified as deceptive (True Negatives) and truthful statements that were \nmisclassified as deceptive (False Negatives), with the exception of ‘Reality Monitoring’ in folds 6 and 7 and \n‘Contextual Embedding’ score in fold 7. Finally, truthful statements that were correctly classified as truthful (True \nPositives) and deceptive statements that were misclassified as truthful (False Positives) exhibited no significant \ndifferences, except for the number of syllables and number of words in the fold 9. We argue that the observation \nof significant differences in selected linguistic features across specific folds is more indicative that these findings \nmay not be generalizable and are likely influenced by the particular fold under analysis. When taken together, \nmost of the analyzed folds showed a substantial overlap in linguistic style. Consequently, the model might have \nexhibited poor classification performance for those statements because, while deceptive, they showed a linguistic \nstyle resembling truthful statements and vice-versa.\nIn contrast, correctly classified statements displayed several significant differences between truthful and \ndeceptive statements. Notably, the top six linguistic features in Fig. 8 resulted in statistical significance in at least \n6 out of 10 folds. The fact that we found a consistent pattern of linguistic features in correctly classified state-\nments but not in misclassified statements provides evidence for our hypothesis, suggesting that the linguistic \nstyle of statements does have a role in the model’s final predictions. More in detail, the top-six linguistic features \ndepicted in Fig. 8 represent a cluster of linguistic cues associated with the CL  framework31, specifically low-level \nfeatures related to the length, complexity, and analytical style of the texts that may have enabled the distinction \nbetween truthful and deceptive statements. The fact that linguistic cues of CL survived among the several features \navailable -in a mixed dataset of utterances reflecting opinions, memories, and intentions- raises the question of \nwhether CL cues may be more generalizable than other cues that are, in contrast, more specific to a particular \ntype of deception.\nConclusion, limitations, and further work\nAt the time of writing and to the best of our knowledge, this is the first study involving the use of an LLM for a \nlie-detection task.\nLLMs are Transformer-based models trained on large corpora of text that have proven to generate coherent \ntext in human natural language and have extreme flexibility in a wide range of NLP  tasks28. In addition, these \nmodels can be further fine-tuned on specific tasks using smaller task-specific datasets, achieving state-of-the-art \n results28. In this study, we tested the ability of a fine-tuned LLM (FLAN-T5) on lie-detection tasks.\nFirst, given the extreme flexibility of LLM, we tested whether fine-tuning a LLM is a valid procedure to detect \ndeception from raw texts above chance level and outperform the classical machine and deep learning approaches. \nWe found that fine-tuning FLAN-T5 on a single dataset is a valid procedure to obtain a state-of-the-art accuracy, \nas proved by the fact that this procedure outperformed the baseline model (BoW + logistic regression) and previ-\nous works that applied machine and deep learning techniques on the same  datasets49–51,62.\nSecond, we wanted to investigate whether fine-tuning an LLM on deceptive narratives enables the model to \nalso detect new types of deceptive narratives. Findings from Scenario 2 disconfirms this hypothesis, suggesting \nthat the model requires previous examples of different deceptive narratives to provide adequate accuracy in this \nclassification task.\nThird, we investigated whether it is possible to successfully fine-tune an LLM on a multiple-context dataset. \nResults from Scenario 3 confirm that fine-tuned LLM may provide adequate accuracy in detecting deception \nfrom different contexts. We also found that fine-tuning on multiple datasets can increase the performance with \nrespect to when fine-tuned on a single dataset.\nFurthermore, we hypothesized that the model performance may depend on the model size, given that the \nlarger the model, the better the model forms its inner representation of language. Results from Scenario 1 and 3 \nconfirmed that the base-sized model of FLAN-T5 provides higher accuracy than the small-sized version.\n17\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\nFinally, with our experiments, we introduced the D eCLaRatiVE stylometry technique, a new theory-based \nstylometric approach to investigate deception in texts from four psychological frameworks (Distancing, Cognitive \nLoad, Reality Monitoring, and Verifiability approach). We employed the D eCLaRatiVE stylometry technique \nto compare the three datasets on linguistic features and we found that fabricated statements from different con-\ntexts exhibit different linguistic cues of deception. We also employed the  DeCLaRatiVE stylometry technique \nto conduct an explainability analysis and investigate whether the linguistic style by which truthful or deceptive \nnarratives are delivered is a feature that the model takes into account for its final prediction. At this aim, we com-\npared correctly classified and misclassified statements by the top-performing model (FLAN-T5 base in Scenario \n3), finding that correctly classified statements share linguistic features related to the cognitive load theory. In \ncontrast, truthful and deceptive misclassified statements do not present significant differences in linguistic style.\nGiven the results achieved, we highlight the importance of a diversified dataset to achieve a generalized good \nperformance. We also considered crucial the balance between the diversity of the dataset and the size of the LLM, \nsuggesting that the more diverse the dataset is, the bigger the model required to achieve higher-level accuracy. \nThe main advantage of our approach consists of its applicability to raw text without the need for extensive train-\ning or handcrafted features.\nDespite the demonstrated success of our model, three significant limitations impact the ecological validity \nof our findings and their practical application in real-life scenarios.\nThe first notable limitation pertains to the narrow focus of our study, which concentrated solely on lie detec-\ntion within three specific contexts: personal opinions, autobiographical memories, and future intentions. This \nrestricted scope limits the possibility of accurately classify deceptive texts within different domains. A second \nlimitation is that we exclusively considered datasets developed in experimental set-ups designed to collect genu-\nine and completely fabricated narratives. However, individuals frequently employ embedded lies in real-life \nscenarios, in which substantial portions of their narratives are true, rather than fabricating an entirely fictitious \nstory. Finally, the datasets employed in this study were collected in experimental low-stake scenarios where \nparticipants had low incentives to lie and appear credible. Because of all the above issues, the application of our \nmodel in real-life contexts may be limited, and caution is advised when interpreting the results in such situations.\nThe limitations addressed in this study underscore the need for future research to expand the applicability and \ngeneralizability of lie-detection models for real-life settings. Future works may explore the inclusion of new data-\nsets, trying different LLMs (e.g., the most recent GPT-4), different sizes (e.g., FLAN-T5 XXL version), and dif -\nferent fine-tuning strategies to investigate the variance in performance within a lie-detection task. Furthermore, \nour fine-tuning approach completely erased the previous capabilities possessed by the model; therefore, future \nworks should also focus on new fine-tuning strategies that do not compromise the model’s original capabilities.\nData availability\nFor the Opinion dataset, we obtained full access after contacting the corresponding author. The Memory dataset \nis downloadable at the link: https:// msrop  endata. com/ datas ets/ 0a83f 6f- a759- 4a17- aaa2- fac8 45773 18. The \nintention dataset is publicly available at the link: https:// osf. io/ 45z7e/.\nCode availabitity\nAll the Colab Notebooks to perform linguistic analysis on the three datasets, fine-tune the model in the three \nScenarios, and conduct explainability analysis is available at https:// github. com/ robec oder/ Verba lLieD etect ionWi \nthLLM. git.\nReceived: 29 June 2023; Accepted: 16 December 2023\nReferences\n 1. Walczyk, J. J., Harris, L. L., Duck, T. K. & Mulay, D. A social-cognitive framework for understanding serious lies: Activation-\ndecision-construction-action theory. New Ideas Psychol. 34, 22–36. https:// doi. org/ 10. 1016/j. newid eapsy ch. 2014. 03. 001 (2014).\n 2. Amado, B. G., Arce, R. & Fariña, F . Undeutsch hypothesis and criteria based content analysis: A meta-analytic review. Eur J Psychol \nAppl Legal Context 7, 3–12. https:// doi. org/ 10. 1016/j. ejpal. 2014. 11. 002 (2015).\n 3. Vrij, A. et al. Verbal lie detection: Its past, present and future. Brain Sciences 12, 1644. https:// doi. org/ 10. 3390/ brain sci12 121644 \n(2022).\n 4. Vrij, A. & Fisher, R. P . Which lie detection tools are ready for use in the criminal justice system?. J. Appl. Res. Mem. Cognit. 5, \n302–307. https:// doi. org/ 10. 1016/j. jarmac. 2016. 06. 014 (2016).\n 5. DePaulo, B. M. et al. Cues to deception. Psychol. Bull. 129, 74–118. https:// doi. org/ 10. 1037/ 0033- 2909. 129.1. 74 (2003).\n 6. Bond, C. F . Jr. & DePaulo, B. M. Accuracy of deception judgments. Personal. Soc. Psychol. Rev. 10, 214–234. https:// doi. org/ 10. \n1207/ s1532 7957p spr10 03_2 (2006).\n 7. Levine, T. R., Park, H. S. & McCornack, S. A. Accuracy in detecting truths and lies: Documenting the “veracity effect” . Commun. \nMonogr. 66, 125–144. https:// doi. org/ 10. 1080/ 03637 75990 93764 68 (1999).\n 8. Levine, T. R. Truth-default theory (TDT). J. Lang. Soc. Psychol. 33, 378–392. https:// doi. org/ 10. 1177/ 02619 27x14 535916 (2014).\n 9. Street, C. N. H. & Masip, J. The source of the truth bias: Heuristic processing?. Scand. J. Psychol. 56, 254–263. https:// doi. org/ 10. \n1111/ sjop. 12204 (2015).\n 10. Verschuere, B., et al. The use-the-best heuristic facilitates deception detection. Nat. Hum. Behav.  7, 718–728. https:// doi. org/ 10. \n1038/ s41562- 023- 01556-2 (2023)\n 11. Chen, X., Hao, P ., Chandramouli, R., and Subbalakshmi, K. P . Authorship similarity detection from email messages. In International \nWorkshop On Machine Learning and Data Mining In Pattern Recognition. Editor P . Perner (New Y ork, NY: Springer), 375–386. \nhttps:// doi. org/ 10. 1007/ 978-3- 642- 23199-5_ 28 (2011).\n 12. Chen, H. Dark web: Exploring and mining the dark side of the web. In 2011 European Intelligence and Security Informatics Confer-\nence, 1–2. IEEE (2011).\n 13. Daelemans, W . Explanation in computational stylometry. In Computational Linguistics and Intelligent Text Processing, 451–462. \nSpringer, Berlin. https:// doi. org/ 10. 1007/ 978-3- 642- 37256-8_ 37 (2013).\n18\nVol:.(1234567890)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\n 14. Hauch, V ., Blandón-Gitlin, I., Masip, J. & Sporer, S. L. Are computers effective lie detectors? A meta-analysis of linguistic cues to \ndeception. Personal. Soc. Psychol. Rev. 19, 307–342. https:// doi. org/ 10. 1177/ 10888 68314 556539 (2015).\n 15. Tomas, F ., Dodier, O., & Demarchi, S. Computational measures of deceptive language: Prospects and issues. Front. Commun.  7 \nhttps:// doi. org/ 10. 3389/ fcomm. 2022. 792378 (2022).\n 16. Conroy, N. K., Rubin, V . L. & Chen, Y . Automatic deception detection: Methods for finding fake news. Proc. Assoc. Inf. Sci. Technol. \n52, 1–4. https:// doi. org/ 10. 1002/ pra2. 2015. 14505 20100 82 (2015).\n 17. Pérez-Rosas, V ., Kleinberg, B., Lefevre, A., & Mihalcea, R. Automatic detection of fake news. arXiv preprint arXiv: 1708. 07104  \n(2017).\n 18. Fornaciari, T. & Poesio, M. Automatic deception detection in Italian court cases. Artif. Intell. Law 21, 303–340. https:// doi. org/ 10. \n1007/ s10506- 013- 9140-4 (2013).\n 19. Y ancheva, M., & Rudzicz, F . Automatic detection of deception in child-produced speech using syntactic complexity features. In \nProceedings of the 51st Annual Meeting of the Association for Computational Linguistics 1, 944–953, (2013).\n 20. Pérez-Rosas, V ., & Mihalcea, R. Experiments in open domain deception detection. In Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing. https:// doi. org/ 10. 18653/ v1/ d15- 1133 (2015).\n 21. Ott, M., Choi, Y ., Cardie, C., & Hancock, J. T. Finding deceptive opinion spam by any stretch of the imagination. arXiv preprint \narXiv: 1107. 4557 (2011).\n 22. Fornaciari, T., & Poesio, M. Identifying fake Amazon reviews as learning from crowds. In Proceedings of the 14th Conference of the \nEuropean Chapter of the Association for Computational Linguistics. https:// doi. org/ 10. 3115/ v1/ e14- 1030n (2014).\n 23. Kleinberg, B., Mozes, M., Arntz, A. & Verschuere, B. Using named entities for computer-automated verbal deception detection. \nJournal of forensic sciences 63, 714–723. https:// doi. org/ 10. 1111/ 1556- 4029. 13645 (2017).\n 24. Mbaziira, A. V ., & Jones, J. H. Hybrid text-based deception models for native and Non-Native English cybercriminal networks. In \nProceedings of the International Conference on Compute and Data Analysis. https:// doi. org/ 10. 1145/ 30932 41. 30932 80 (2017).\n 25. Levitan, S. I., Maredia, A., & Hirschberg, J. Linguistic cues to deception and perceived deception in interview dialogues. In Pro -\nceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, 1. https:// doi. org/ 10. 18653/ v1/ n18- 1176 (2018).\n 26. Kleinberg, B., Nahari, G., Arntz, A., & Verschuere, B. An investigation on the detectability of deceptive intent about flying through \nverbal deception detection. Collabra: Psychol. 3. https:// doi. org/ 10. 1525/ colla bra. 80 (2017).\n 27. Constâncio, A. S., Tsunoda, D. F ., Silva, H. de F . N., Silveira, J. M. da, & Carvalho, D. R. Deception detection with machine learning: \nA systematic review and statistical analysis. PLOS ONE, 18, e0281323. https:// doi. org/ 10. 1371/ journ al. pone. 02813 23 (2023).\n 28. Zhao, W . X., et al. A survey of large language models. arXiv preprint arXiv: 2303. 18223. (2023).\n 29. Newman, M. L., Pennebaker, J. W ., Berry, D. S. & Richards, J. M. Lying words: Predicting deception from linguistic styles. Personal. \nSoc. Psychol. Bull. 29, 665–675. https:// doi. org/ 10. 1177/ 01461 67203 02900 5010 (2003).\n 30. Monaro, M. et al. Covert lie detection using keyboard dynamics. Sci Rep 8, 1976. https:// doi. org/ 10. 1038/ s41598- 018- 20462-6 \n(2018).\n 31. Vrij, A., Fisher, R. P . & Blank, H. A cognitive approach to lie detection: A meta-analysis. Legal Criminol. Psychol. 22(1), 1–21. \nhttps:// doi. org/ 10. 1111/ lcrp. 12088 (2015).\n 32. Johnson, M. K. & Raye, C. L. Reality monitoring. Psychol. Rev. 88, 67–85. https:// doi. org/ 10. 1037/ 0033- 295x. 88.1. 67 (1981).\n 33. Sporer, S. L. The less travelled road to truth: Verbal cues in deception detection in accounts of fabricated and self-experienced \nevents. Appl. Cognit. Psychol. 11(5), 373–397. https:// doi. org/ 10. 1002/ (SICI) 1099- 0720(199710) 11:5% 3c373:: AID- ACP461% 3e3.0. \nCO;2-0 (1997).\n 34. Sporer, S. L. Reality monitoring and detection of deception in The Detection of Deception in Forensic Contexts (Cambridge University \nPress.), 64–102. https:// doi. org/ 10. 1017/ cbo97 80511 490071. 004 (2004).\n 35. Masip, J., Sporer, S. L., Garrido, E. & Herrero, C. The detection of deception with the reality monitoring approach: A review of the \nempirical evidence. Psychol. Crime Law 11(1), 99–122. https:// doi. org/ 10. 1080/ 10683 16041 00017 26356 (2005).\n 36. Amado, B. G., Arce, R., Fariña, F . & Vilariño, M. Criteria-Based Content Analysis (CBCA) reality criteria in adults: A meta-analytic \nreview. Int. J. Clin. Health Psychol. 16(2), 201–210. https:// doi. org/ 10. 1016/j. ijchp. 2016. 01. 002 (2016).\n 37. Gancedo, Y ., Fariña, F ., Seijo, D., Vilariño, M. & Arce, R. Reality monitoring: A meta-analytical review for forensic practice. Eur. \nJ. Psychol. Appl. Legal Context 13(2), 99–110. https:// doi. org/ 10. 5093/ ejpal c2021 a10 (2021).\n 38. Vrij, A. et al. Verbal lie detection: its past, present and future. Brain Sci.  12(12), 1644. https:// doi. org/ 10. 3390/ brain sci12 121644 \n(2022).\n 39. Kleinberg, B., van der Vegt, I., & Arntz, A. Detecting deceptive communication through linguistic concreteness. Center for Open \nScience. https:// doi. org/ 10. 31234/ osf. io/ p3qjh (2019).\n 40. Nahari, G., Vrij, A. & Fisher, R. P . Exploiting liars’ verbal strategies by examining the verifiability of details. Legal Criminol. Psychol. \n19, 227–239. https:// doi. org/ 10. 1111/j. 2044- 8333. 2012. 02069.x (2012).\n 41. Vrij, A., & Nahari, G. The verifiability approach. In Evidence-Based Investigative Interviewing (pp. 116–133). Routledge. https://  \ndoi. org/ 10. 4324/ 97813 15160 276-7 (2019).\n 42. Pennebaker, J. W ., Francis, M. E., & Booth, R. J. Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum \nAssociates, 71, 2001 (2001).\n 43. Boyd, R. L., Ashokkumar, A., Seraj, S., & Pennebaker, J. W . The development and psychometric properties of LIWC-22. Austin, \nTX: University of Texas at Austin, 1–47. (2022).\n 44. Bond, G. D. & Lee, A. Y . Language of lies in prison: Linguistic classification of prisoners’ truthful and deceptive natural language. \nAppl. Cognit. Psychol. 19(3), 313–329. https:// doi. org/ 10. 1002/ acp. 1087 (2005).\n 45. Bond, G. D. et al. ‘Lyin’ Ted’ , ‘crooked hillary’ , and ‘Deceptive Donald’: Language of lies in the 2016 US presidential debates. Appl. \nCognit. Psychol. 31(6), 668–677. https:// doi. org/ 10. 1002/ acp. 3376 (2017).\n 46. Bond, G. D., Speller, L. F ., Cockrell, L. L., Webb, K. G., & Sievers, J. L. ‘Sleepy Joe’ and ‘Donald, king of whoppers’: Reality monitor-\ning and verbal deception in the 2020 U.S. presidential election debates. Psychol. Rep.  003329412211052. https:// doi. org/ 10. 1177/ \n00332 94122 11052 12 (2022).\n 47. Schutte, M., Bogaard, G., Mac Giolla, E., Warmelink, L., Kleinberg, B., & Verschuere, B. Man versus Machine: Comparing manual \nwith LIWC coding of perceptual and contextual details for verbal lie detection. Center for Open Science. https:// doi. org/ 10. 31234/ \nosf. io/ cth58 (2021).\n 48. Kleinberg, B., van der Toolen, Y ., Vrij, A., Arntz, A. & Verschuere, B. Automated verbal credibility assessment of intentions: The \nmodel statement technique and predictive modeling. Appl. Cognit. Psychol. 32, 354–366. https:// doi. org/ 10. 1002/ acp. 3407 (2018).\n 49. Kleinberg, B., & Verschuere, B. How humans impair automated deception detection performance. Acta Psychol., 213, https:// doi. \norg/ 10. 1016/j. actpsy. 2020. 103250 (2021).\n 50. Ilias, L., Soldner, F ., & Kleinberg, B. Explainable verbal deception detection using transformers. arXiv preprint arXiv: 2210. 03080 \n(2022).\n 51. Capuozzo, P ., Lauriola, I., Strapparava, C., Aiolli, F ., & Sartori, G. DecOp: A multilingual and multi-domain corpus for detecting \ndeception in typed text. In Proceedings of the 12th Language Resources and Evaluation Conference, 1423–1430 (2020).\n 52. Sap, M. et al. Quantifying the narrative flow of imagined versus autobiographical stories. Proc. Natl. Acad. Sci. 119(45), \ne2211715119. https:// doi. org/ 10. 1073/ pnas. 22117 15119 (2022).\n19\nVol.:(0123456789)Scientific Reports |        (2023) 13:22849  | https://doi.org/10.1038/s41598-023-50214-0\nwww.nature.com/scientificreports/\n 53. Hernández-Castañeda, Á., Calvo, H., Gelbukh, A. & Flores, J. J. G. Cross-domain deception detection using support vector net -\nworks. Soft Comput. 21, 585–595. https:// doi. org/ 10. 1007/ s00500- 016- 2409-2 (2016).\n 54. Pérez-Rosas, V ., & Mihalcea, R. Cross-cultural deception detection. In Proceedings of the 52nd Annual Meeting of the Association \nfor Computational Linguistics 2. https:// doi. org/ 10. 3115/ v1/ p14- 2072 (2014).\n 55. Mihalcea, R., & Strapparava, C. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings \nof the ACL-IJCNLP 2009 conference short papers 309–312. https:// doi. org/ 10. 3115/ 16675 83. 16676 79 (2009).\n 56. Ríssola, E. A., Aliannejadi, M., & Crestani, F . Beyond modelling: Understanding mental disorders in online social media. In \nAdvances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14–17, 2020, \nProceedings, Part I 42 (pp. 296–310). Springer (2020).\n 57. Chung, H. W ., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv: 2210. 11416. (2022).\n 58. Zhou, L., Burgoon, J. K., Nunamaker, J. F . & Twitchell, D. Automating linguistics-based cues for detecting deception in text-based \nasynchronous computer-mediated communications. Group Decis. Negot. 13, 81–106. https:// doi. org/ 10. 1023/b: grup. 00000 11944. \n62889. 6f (2004).\n 59. Solà-Sales, S., Alzetta, C., Moret-Tatay, C. & Dell’Orletta, F . Analysing deception in witness memory through linguistic styles in \nspontaneous language. Brain Sci. 13, 317. https:// doi. org/ 10. 3390/ brain sci13 020317 (2023).\n 60. Sarzynska-Wawer, J., Pawlak, A., Szymanowska, J., Hanusz, K. & Wawer, A. Truth or lie: Exploring the language of deception. PLOS \nONE 18, e0281179. https:// doi. org/ 10. 1371/ journ al. pone. 02811 79 (2023).\n 61. Brysbaert, M., Warriner, A. B. & Kuperman, V . Concreteness ratings for 40 thousand generally known English word lemmas. Behav \nRes 46, 904–911. https:// doi. org/ 10. 3758/ s13428- 013- 0403-5 (2014).\n 62. Lin, Y . C., Chen, S. A., Liu, J. J., & Lin, C. J. Linear Classifier: An Often-Forgotten Baseline for Text Classification. arXiv preprint \narXiv: 2306. 07111 (2023).\n 63. Moore, J. H. Bootstrapping, permutation testing and the method of surrogate data. Phys. Med. Biol. 44(6), L11 (1999).\n 64. McGraw, K. O. & Wong, S. P . A common language effect size statistic. Psychol. Bull. 111, 361. https:// doi. org/ 10. 1037/ 0033- 2909. \n111.2. 361 (1992).\n 65. Hancock, J. T., Curry, L. E., Goorha, S. & Woodworth, M. On lying and being lied to: A linguistic analysis of deception in computer-\nmediated communication. Discourse Process. 45, 1–23. https:// doi. org/ 10. 1080/ 01638 53070 17391 81 (2007).\nAcknowledgements\nWe would like to thank Bruno Verschuere and Bennett Kleinberg for sharing the full version of their Intention \ndataset with us.\nAuthor contributions\nG.S. conceptualized the research. R.L., R.R., P .C., and G.S. designed the research. P .C. shared the updated version \nof the Deceptive Opinion Dataset. R.L. performed the descriptive linguistic analysis and explainability analysis. \nR.R. developed and implemented the fine-tuning strategy. R.L. and R.R. wrote the paper. P .P . and G.S. supervised \nall aspects of whole the research and provided critical revisions.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 50214-0.\nCorrespondence and requests for materials should be addressed to R.L.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}