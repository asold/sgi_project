{
    "title": "Code Prediction by Feeding Trees to Transformers",
    "url": "https://openalex.org/W3014797428",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2941426465",
            "name": "Kim Seohyun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2367368710",
            "name": "Zhao Jin-man",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3085565345",
            "name": "Tian, Yuchi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2187946078",
            "name": "Chandra Satish",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2949737566",
        "https://openalex.org/W2116272605",
        "https://openalex.org/W2533695286",
        "https://openalex.org/W2971351900",
        "https://openalex.org/W1994573369",
        "https://openalex.org/W2893554781",
        "https://openalex.org/W2963499994",
        "https://openalex.org/W2963935794",
        "https://openalex.org/W2950898568",
        "https://openalex.org/W2143861926",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3099302725",
        "https://openalex.org/W3109966548",
        "https://openalex.org/W2963393993",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3004658838",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2962995178",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964303497",
        "https://openalex.org/W2964150020",
        "https://openalex.org/W3005598269",
        "https://openalex.org/W3083954092",
        "https://openalex.org/W2969101956",
        "https://openalex.org/W2626639386",
        "https://openalex.org/W2444132761",
        "https://openalex.org/W2887364112",
        "https://openalex.org/W2963015915",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2516621648",
        "https://openalex.org/W2971270287",
        "https://openalex.org/W2740130862",
        "https://openalex.org/W3015810308",
        "https://openalex.org/W2238673293",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2513722160",
        "https://openalex.org/W2897133944",
        "https://openalex.org/W2753108589",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2947477827",
        "https://openalex.org/W3008282111",
        "https://openalex.org/W2344444819",
        "https://openalex.org/W3100026183",
        "https://openalex.org/W2964645190",
        "https://openalex.org/W2739349903",
        "https://openalex.org/W3035300716",
        "https://openalex.org/W2963374347",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3021206621",
        "https://openalex.org/W2996086147",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3036286896",
        "https://openalex.org/W3108032709",
        "https://openalex.org/W2954451301",
        "https://openalex.org/W2979104824",
        "https://openalex.org/W3013745307",
        "https://openalex.org/W2963676655",
        "https://openalex.org/W2952802110",
        "https://openalex.org/W2266912522",
        "https://openalex.org/W2751295852",
        "https://openalex.org/W2886490473",
        "https://openalex.org/W3105398568",
        "https://openalex.org/W3015001695",
        "https://openalex.org/W2995259046",
        "https://openalex.org/W2962700793"
    ],
    "abstract": "We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3 system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.",
    "full_text": "Code Prediction by Feeding Trees to Transformers\nSeohyun Kim†\nFacebook Inc.\nU.S.A.\nskim131@fb.com\nJinman Zhao†\nUniversity of Wisconsin-Madison\nU.S.A.\njz@cs.wisc.edu\nYuchi Tian\nColumbia University\nU.S.A.\nyuchi.tian@columbia.edu\nSatish Chandra\nFacebook Inc.\nU.S.A.\nschandra@acm.org\nAbstract—Code prediction, more speciﬁcally autocomplete, has\nbecome an essential feature in modern IDEs. Autocomplete is\nmore effective when the desired next token is at (or close to)\nthe top of the list of potential completions offered by the IDE at\ncursor position. This is where the strength of the underlying\nmachine learning system that produces a ranked order of\npotential completions comes into play.\nWe advance the state-of-the-art in the accuracy of code\nprediction (next token prediction) used in autocomplete systems.\nOur work uses Transformers as the base neural architecture. We\nshow that by making the Transformer architecture aware of the\nsyntactic structure of code, we increase the margin by which a\nTransformer-based system outperforms previous systems. With\nthis, it outperforms the accuracy of several state-of-the-art next\ntoken prediction systems by margins ranging from 14% to 18%.\nWe present in the paper several ways of communicating\nthe code structure to the Transformer, which is fundamentally\nbuilt for processing sequence data. We provide a comprehensive\nexperimental evaluation of our proposal, along with alternative\ndesign choices, on a standard Python dataset, as well as on\nFacebook internal Python corpus. Our code and data preparation\npipeline will be available in open source.\nIndex Terms—code embedding, code prediction, autocomplete\nI. I NTRODUCTION\nA. Code Prediction\nThe idea of code prediction in general is to predict some\ncode element, given code surrounding the point of prediction.\nCode prediction is commonly used in an IDE for autocom-\nplete, where based on the code already written up to the\ndeveloper’s cursor position, the IDE offers the most likely\nnext tokens, perhaps as a drop down list to choose from as\nshown in IDE views in Fig 1. Other forms of code prediction\ncould predict missing tokens at arbitrary code locations or\npredict larger units of code; in this paper we will concern\nourselves with prediction of the immediate next token at the\ncursor position.\nConsider the Python code fragment shown in Fig 1. Suppose\na developer has written code up to string following by a\ndot. At this point, it will be helpful for the IDE to prompt\nthe developer with attribute names that are likely to follow,\npreferably, with atoi ranked at the top because in this case\nthat is the correct next token.\nDevelopers have come to rely on autocomplete in their\nIDEs for multiple reasons. First, and most obviously, it saves\nthe effort of typing in the next token(s) in the IDE. For\nthis reason alone, most modern IDEs come with at least\n†Both authors contributed equally to this research.\n(a) Type-based, alphabetical\n(b) SeqRNN\n(c) TravTrans\nFig. 1: Screenshots of ranked autocomplete predictions from\nthree different models–Type-based alphabetical, SeqRNN, and\nTravTrans, as would appear in an IDE. A type-based autocom-\nplete tool such as Jedi that sorts choices alphabetically ranks\n“atoi” low. SeqRNN, a RNN-based model predicts it as the\nsecond result. TravTrans, a Transformer-based model predicts\nit as the ﬁrst result. Fewer keystrokes are needed to choose\nthe correct answer as we go from left to right.\nsome autocomplete facility for the languages they support.\nNotice that a top-ranked suggestion is often selectable by\nhitting a tab—as would be the case in Fig 1(c)—whereas\nthe lower ranked suggestions have to be selected by scrolling\n(Fig 1(a,b)), which is more effort. Thus, providing the right\ncompletion at the top of the list, or if not, among the top few,\nis important.\nSecond, autocomplete is also a powerful code discovery\nmechanism. For instance, a developer might not know the\nname of an API call they need off the top of their head, but is\narXiv:2003.13848v4  [cs.SE]  8 Mar 2021\nable to choose among the choices shown by an autocomplete\ntool. Without assistance from IDE, the developer might need\nto change their mental context, go to Stack Overﬂow or some\nother web site, and come back to the IDE. However, the code\ndiscovery assistance from autocomplete works only when a\ncontextually appropriate code suggestion is offered among the\ntop choices in the list, because developers do not have the time\nto go through a comprehensive list of completions.\nB. Machine Learning for Code Prediction\nIt is clear that effective autocomplete requires the intended\nnext token to be predicted at the top of the list, or as\nclose to the top as possible. Type-based autocomplete (e.g.\nEclipse1 and Jedi 2) returns a list of type-compatible names,\nbut does naive ranking: alphabetically or with simple count-\nbased statistics. For instance, the autocomplete model used\nin the IDE (Jedi, which ranks type-compatible suggestions\nalphabetically) shown in Figure 1(a) ranks atoi fairly low.\nThe example shows why one of the earliest attempts of code\nprediction, using type-based methods, is not very effective.\nAdditionally, for dynamic languages, it is extremely difﬁcult\nto gather an accurate type-compatible list of tokens that could\noccur in a context.\nThese limitations have motivated the use of machine learn-\ning for code prediction, as machine learning methods are able\nto base their predictions on the naturalness [1] of code. Early\napproaches adapted n-gram language models on linearized\nsource code tokens [2], [3]. More recently, deep neural net-\nworks have been applied to code prediction, surpassing n-\ngram models. The most common neural technique for code\nprediction is Recurrent Neural Networks (RNNs) [4] and\ntheir variants [5]–[10], where the code represented as a linear\nsequence is fed as input to the model. Fig 1(b) shows how an\nRNN model does better than a non-ML alphabetical ranking\nFig 1(a) in showing the expected item closer to the top.\nResearchers have also investigated using the syntactic struc-\nture of code for prediction, as opposed to seeing code as\ntext: both using probabilistic graphical models (probabilis-\ntic context-free grammars [11] and probabilistic higher-order\ngrammars [12]–[14]), as well as using deep learning [15].\nC. Is this a solved problem?\nAlthough predictive models cannot be expected to be per-\nfect, the accuracy of the current state-of-the-art methods leaves\nsubstantial margin to be improved. For instance, a typical\nRNN-based method provides less than 37% mean reciprocal\nrank (this equates to the correct answer being in the top\n(37%)−1 ≈2.7 results, Sec IV-C) on the py150 benchmark.\nImproving this metric is exactly the goal of this paper.\nWe report that our techniques are able to suggest the correct\nnext tokens at ranks better — showing correct result to the\ndeveloper by 0.5 to 1 ranks higher — than those achieved by\nprevious methods (MRR increase of 14% to 18%.)\n1https://www.eclipse.org/pdt/help/html/working with code assist.htm\n2https://github.com/davidhalter/jedi\n...\nip = socket.gethostbyname (host)\n[port, request_size, num_requests, num_conns] = map (\nstring.atoi, sys.argv[2:]\n)\nchain = build_request_chain(num_requests, host, request_size)\n...\nFig. 2: Running example of Python code. The code snippet 3is\nfrom the py150 dataset [16].\nAs a concrete example: Table I shows the ranks of the\nvarious non-punctuation tokens to be predicted for the code\nin Figure 2, using various recent methods, as well as for our\nwork. Speciﬁcally, the rank of atoi is predicted at rank 1\nonly by the new methods we propose in this paper.\nD. Feeding Trees to Transformers\nTo improve the accuracy of next token predicted, a number\nof alternatives come to mind. First, we could strengthen\nthe neural architecture alone. For instance, researchers have\nsuggested adding attention to RNNs [5], [17] to compensate\nfor loss of signal on long range dependence. Without long-\nrange dependence a model would be making a (potentially sub-\noptimal) decision only on the most recent tokens. Transformers\nhandle long-range dependencies better. In the NLP community,\nTransformers have achieved state-of-the-art results [18]–[20],\noutperforming RNNs, for a variety of NLP tasks such as\nlanguage modeling, question answering, and sentence entail-\nment. For us, since training Transformers did not take much\nmore resources that training RNNs (Table IV), we decided to\nproceed with Transformers.\nAn orthogonal way to improve the accuracy is to enable\nthe machine learning system to “see” more code structure.\nRaychev et al. [14] had found that—for the code prediction\nproblem—a non-neural but AST-aware engine could outper-\nform RNNs. In the same spirit, Alon et al. [21] had found—for\ncode summarization problem (though not for code prediction\nin their paper)—that embedding the AST structure of code\nvastly outperformed purely sequence-based methods.\nInspired by the above, we explore how to leverage code\nstructure while using Transformers on code. This is not\nobvious; we cannot simply jam an AST into the Transformer,\nwhich is a sequence processing model. We explore two models\nthat represent two ways to capture the (partial) structure\nof an AST: one, based on decomposing the tree into paths\n(PathTrans), and the other, based on a tree traversal order\n(TravTrans). We also investigated a variant of TravTrans that\ntakes into account even more tree structure.\nE. Key Results\nTab II compares the new models we introduce in this paper\n(in bold), with three state-of-the-art models from previous\nwork (Sec II-A discusses these models.) We report results\nbased on training and evaluating on the py150 [16] dataset, and\n3data/JeremyGrosser/supervisor/src/supervisor/medusa/test/test 11.py\nToken value ip socket get*Name host map string atoi sys argv 2 chain\nPrevious SeqRNN >10 >10 3 2 7 >10 2 1 1 3 >10\nwork Deep3 >10 9 4 >10 >10 >10 >10 6 1 7 >10\nCode2Seq >10 2 1 3 >10 >10 8 1 1 3 >10\nSeqTrans >10 1 1 6 >10 >10 1 10 1 1 >10\nOur PathTrans 10 2 1 1 8 1 2 1 1 1 >10\nwork TravTrans >10 1 5 1 4 1 1 1 1 1 >10\nTABLE I: Ranks for the predictions for the leaf nodes listed in Fig 3. >10 means the model did not get the right answer in\nthe top 10 results.\nModel Sequence AST\nTransformer SeqTrans (54.9%) TravTrans (58.0%)\nPathTrans (55.1%)\nAttention Code2Seq [15] (43.7%)\nRNN SeqRNN [22] (36.6%)\nDecision tree Deep3 [14] (43.9%)\nTABLE II: Overview of the models considered in this paper.\nModels in bold font are models from this work; SeqTrans feeds\nsource tokens in linear order to the Transformer. The numbers\nin parenthesis denote accuracy (see Sec V.) It is clear that the\naccuracy increases as models uses information from the AST\n(columns), and as more sophisticated neural architectures are\nused (rows).\nfor each, we report accuracy in mean reciprocal rank (MRR)\nas a percentage (see Sec IV).\nOur best model TravTrans, which communicates the tree\nstructure to the Transformer, signiﬁcantly outperforms all\nprevious models for code prediction, with improvements in\nreciprocal ranks:\n• from 43.9% to 58.0% when comparing a non-neural tree\nbased model Deep3 [14] vs. TravTrans;\n• from 43.6% to 58.0% when comparing Code2Seq [15]\nvs. TravTrans;\n• from 36.6% to 54.9% when comparing an RNN imple-\nmentation SeqRNN vs. TravTrans 4;\nWe also evaluated our trained model on a dataset selected\nfrom a Python code repository internal to a Facebook, and\nfound the relative beneﬁts of the Transformer models to be\nsimilar to those on the py150 dataset. This indicates that the\nrelative advantage of Transformer models carries over to other\ndatasets. (Sec V, Table VIII)\nDeep learning models can be rather opaque as to their\nworking. To better understand whether TravTrans is taking ad-\nvantage of the tree structure, we employed saliency maps [23]\nto examine where the model focuses its attention when making\na prediction. We found that indeed, the model tends to focus\non the most relevant parts of the tree, starting from the parent\nnode (Sec VI).\nF . Contributions\nWe move the state-of-the-art forward in accurate next token\nprediction, a capability increasingly expected in modern IDEs.\n4comparing TravTrans against SeqRNN is slightly different than comparing\nit against Deep3 or Code2Seq, hence the difference in MRR. We discuss in\ndetail in Sec V.\n• We describe ways of using Transformers for the task\nof next token prediction, especially ways that proﬁtably\ncommunicate the syntactic structure of code. (Sec II).\nAlthough there have been previous work in applying\nTransformers in the context of code (code summariza-\ntion [24], code correction [25], and code translation [26]),\nthis paper is the among the ﬁrst to explore and evaluate\nTransformers for code (next token) prediction.\n• We present a systematic comparison of our proposed\nmodels with the most effective models from prior work 5\nthat are applicable to next token prediction, on a widely\navailable Python dataset py150. The ﬁndings clearly\nindicate a 14% to 18% gain in accuracy with our best\nmodel relative to prior state-of-the-art.\n• We provide a preliminary attribution study in an attempt\nto understand the prediction given by our best performing\nmodel, TravTrans. This study (Sec VI) indicates that\nTravTrans indeed conditions its predictions on the most\npertinent tokens in the context. To our knowledge, this\nkind of model interpretability analysis for autocomplete\nis also a ﬁrst.\nOur overall conclusion is that Transformer based models\nover ASTs provide the best prediction power for autocomplete.\nG. Outline\nSec II provides background on the previous models that we\nuse in our evaluation, along with a primer on Transformers.\nSec III explains the Transformer-based models of our own\ncreation. Sec IV describes our datasets and implementation.\nSec V presents our quantitative results. Sec VI takes a closer\nlook into why our models worked well (or did not). Sec VII\nlists some threats to validity. Sec VIII discusses prior related\nwork in the ﬁeld of code prediction and Transformers. We\nconclude the paper with our future work in Sec IX.\nII. B ACKGROUND\nIn this section, we deﬁne the code prediction task we\nexamine in this work, followed by details of previous state-\nof-the-art methods of code prediction we use for comparison.\nWe end the section with a brief introduction to the original\nTransformer model. We will refer to the nodes of the AST for\nFig 2, as shown in Fig 3.\n5Incidentally, this paper is also the ﬁrst to compare the three prior works\non a common dataset.\nFig. 3: Part of the AST for the example in Fig 2. The leaf\n(terminal) nodes have values and the interior (non-terminal)\nnodes have types.\nA. Code Prediction Task\nCode prediction task studied in this work is to predict the\nnext code unit given the partial program up to the point of\nprediction. Let p∗(unit |ctx) be the empirical distribution of\ncode unit given the partial program context ctx. Our task is to\nlearn to approximate p∗ using a machine learning model M.\nIn our proposals, M will be some Transformer Trans . The\nlearned distribution can be viewed as\np(unit |ctx) =M(ctx; θ),\nwhere θrepresents the trainable parameters of the model. We\ntrain the models by minimizing the KL-divergence between p\nand p∗, or equivalently, minimizing the cross-entropy loss l\nover all code prediction locations.\nWe explore several ways of representing partial program and\npredicting different kinds of code units. When using source\ncode token as code unit and representing partial program as\nsequence of source code tokens (SeqTrans), the problem aligns\nwith the traditional notion of language modeling – predicting\nthe next token in a sequence given all previous tokens: p(ti |\nt1, . . . , ti−1) where ti is the i-th token.\nMore interestingly, we explore various representations of a\npartial program to better utilize its AST information. The intu-\nition is that the more we can utilize the syntactic information\nprovided by the AST, the better we can predict the next token.\nThe next section discusses three models used in previous work.\nB. SeqRNN\nFor next token prediction, a popular method is to feed\nthe source sequence tokens into an RNN (or LSTM) [8]–\n[10], [22]. An RNN embeds the input tokens into a vector:\nxt = emb(wt), where wt is the source token seen at the\nt’th time step. The hidden state ht+1 at the (t + 1)-th time\nFig. 4: Fragment of a TGEN program encoding a decision tree\non the left (bold words are the steps that comprise a path), with\nthe corresponding paths shown on the AST on the right.\nstep is computed as ht+1 = rnn (xt, ht), where rnn is a\ntrainable RNN unit. The last hidden state is then fed through\na classiﬁcation layer.\nThe pertinent point to note is that the hidden state ht\nencodes the knowledge of not just the current token, but\nof last several (and theoretically all) previous tokens via the\npropagation of information in previous hidden states.\nIn our experiments, we feed the source code tokens into an\nLSTM and call this model SeqRNN.\nC. Deep3\nRaychev et al. [14] presented a system, Deep3, based on a\nlearned decision tree combined with count-based probabilities\nat the leaves of the decision tree.\nFig 4 shows part of a learned decision tree, written in\nthe form of program in a specialized language called TGEN.\nGiven an AST t and a starting node n, a TGEN program\nwalks certain paths in t starting from n. For example, Up\nWriteValue (line 1) goes to the parent of n and records\nthe label. At the end of a TGEN program is a probability\ndistribution for the possible values of the starting node. For\nexample, starting with node 29, the TGEN program predicts\n“atoi” with 60%, “split” with 30%, etc.\nA TGEN program is learned—on a speciﬁc corpus—by\na genetic search procedure that simultaneously selects paths\nand grows the decision tree from the training data, with an\nentropy minimization objective. In this paper, we use their\npretrained model [27] as well as their Python dataset [16] for\nour experiments.\nD. Code2Seq\nCode2Seq is a model by Alon et al. [15] that embeds code\nsnippets by embedding AST paths in a neural network.\nAt a high-level, given an AST, Code2Seq creates path\nrepresentations for all leaf-to-leaf paths. For example, Fig 5\nshows three leaf-to-leaf paths for nodes 22-29 from the full\nAST (Fig 3). For each path, a path representation is created\nwith: 1. the starting leaf value, tokenized by snake and camel\ncase, 2. the path itself, and 3. the ending leaf value, also\ntokenized. 1 and 3 are embedded using LSTMs, and 2 is em-\nbedded using bi-directional LSTMs. These three embeddings\nFig. 5: Example of an input for Code2Seq, which consists\nof leaf-to-leaf path representations given a partial AST. A\npath representation is made of tokenized starting tokens,\npath, and tokenized ending tokens. If the path ends with the\ntarget node (in this example, atoi), the value is replaced by\n<placehholder>.\nare concatenated and then fed through a feed forward network.\nFinally, all of the path represention embeddings in the AST\nare combined using a simple attention mechanism.\nIn Code2Seq, a decoder is then used to solve a code\nsummarization task: given a method body, how well can\nCode2Seq generate the correct method name? The training\nproposed in [15] is not well suited for next token prediction.\nIn code summarization, a set of leaf-to-leaf paths needs to be\ncreated one time for a method. By contrast, in code prediction,\na new set of leaf-to-leaf paths has to be created for each point\nof prediction.\nFor example, to predict atoi (node 29) in Fig 3, we\nmust ﬁrst create an representative embedding for the partially\ncompleted AST up to node 29, using all leaf-to-leaf paths\navailable up to node 29. Paths that end in atoi are also used,\nwith atoi replaced with a placeholder token to prevent infor-\nmation leak (e.g. Paths 2 and 3 in Fig 5). The representative\nembedding is then fed through a classiﬁcation layer to generate\npredictions.6\nBy treating each point of prediction as a separate data\npoint (compared to a language model, where one sequence\nis considered one data point), the number of training data\npoints, along with the effort to create them makes Code2Seq\ncomputationally very expensive.\nE. A Primer on Transformers\nHere we present a brief introduction of Transformers. Read-\ners familiar with Transformers can skip ahead to Section III.\nTransformers belong to a class of deep neural networks\nthat are designed for sequence processing. In Transformers,\ninformation from any previous location of the sequence can\ndirectly affect the encoding of the next token, through a\nmechanism called self-attention, which helps greatly improve\nthe connectivity in long sequences.\nTo be precise, a Transformer is a stack of Attention blocks\n(AttnBlk) preceded by an input embedding layer (Emb) and\n6Note that this is different than the generative decoder that Code2Seq uses.\nFig. 6: Schematic of a GPT2 Transformer. The self-attention\nlayer is able to consider all tokens in the input up to the\npoint of prediction. Here the self-attention box depicts the\ninformation ﬂow when predicting next token after the ”.”; see\nTable III for where the numbers come from.\nfollowed by a classiﬁcation layer (Clsfr), where AttnBlk is\nrepeated nblock times.\nTrans (ctx) =Clsfr(AttnBlk(. . .(AttnBlk(Emb(ctx))) . . .))\nSee Fig 6 for a schematic of a Transformer with embedding\nlayer, a stack of (here 6) attention blocks, and ﬁnally a\nclassiﬁcation layer.\nThe self-attention layer—which constitutes the main part of\nan attention block—is the crux of the model. The intuition\nhere is to attend to the elements in the input sequence in\nproportion of their relevance to the location being predicted.\nFor example, take an example input token sequence [“map”,\n“(”, “string”, “.”], and the target next token being “atoi.”\nIt is ﬁrst fed through the initial embedding layer to give:\nE = [emap, e(, estring, e.]. Then, we feed E to three fully-\nconnected networks ( Wq, Wk, Wv) to create query, key, and\nvalue embeddings:\nQ= EWq, K= EWk, V = EWv,\nFig 6 depicts the vectors Q comprised of its elements\nqmap, q(, qstring, q. (in green), and likewise for K and V.\nThe self-attention then works by querying keys k using\nqueries q and then using the result to summarize values v\nthrough the attention function:\nAttn(Q, K, V) =softmax\n(\nQK⊤\n√dk\n)\nV\nwhere dk is the dimension of key vectors. Here, QK⊤is used\nto determine which token relationships are the most important,\nresulting in a matrix of size n ×n, where n is the length of\nthe input sequence. Each row is then normalized and passed\nthrough a softmax layer. Table III shows an example of the\nself-attention weights. Looking at the last row, we can see that\nmost of the attention is given to “.”, meaning it has a greater\n... map ( string .\nmap 0.9\n( 0.6 0.1\nstring 0.1 0.1 0.7\n. 0.2 0.1 0.2 0.4\nTABLE III: Example matrix for the numerical self-attention\nscores after taking the softmax over the normalized values\nof QK⊺. For example, the entry against (string, map) is\nobtained by multiplying qstring with kmap, after softmax and\nnormalization. Note that the rows listed here do not sum up to\nexactly 1 since there are previous tokens in the input sequence\nthat are not shown in this matrix.\nfactor in predicting the next token “atoi”. Note how the matrix\nis a lower triangular matrix: this is because self-attention can-\nnot be applied to tokens that have not been seen before. After\nmultiplying this matrix with the value vector, in our example,\nAttn(Q, K, V) = [0.2 ∗vmap, 0.1 ∗v(, 0.2 ∗vstring, 0.4 ∗v.].\nTransformers also uses multiple heads of these self-attention\nblocks, called multi-headed attention, which enables the model\nto simultaneously consider different ways of attending to\nprevious information within one block and also across other\nblocks. In our implementation, we omit positional encoding\n(see Sec IV-B.) For more details, please refer to [28] and [20].\nThe next sections discuss various ways of feeding code\nfragments into this Transformer architecture.\nIII. O UR WORK : TRANSFORMER -BASED MODELS\nThe question that interests us is: can Transformer-based\nmodels also beneﬁt from syntactic structure, and if so, how\ncan we communicate the syntactic structure to Transformer?\nIn this section, we ﬁrst begin with a model SeqTrans that\nuses a Transformer to take source code tokens as input. Then\nwe introduce two other models, PathTrans and TravTrans, that\nuse more syntactic information obtained from the AST.\n1) SeqTrans: Our ﬁrst model is to apply a Transformer\nover source token sequences, which can be easily obtained by\napplying a tokenizer. Here the input is the partial program\nrepresented as source token sequences and the output is a\nsource code token. This is a straightforward application of\nthe original Transformer design, and functions as a baseline\nfor our later attempts that take on more AST information. The\nmodel can be written as\no= Trans\n(\n(et)t∈src seq\n)\nwhere ois a distribution over all possible tokens, and et is the\nembedding for source token t for every t in the source token\nsequence. As we show in the experiments, SeqTrans turns out\nto be an already strong model as a direct comparison to the\nbaseline RNN model SeqRNN.\nSince Transformers are originally designed as a sequential\nmodel, the challenge becomes ﬁnding ways to convey AST\ninformation to Transformers. In the next subsection, we will\nvary the inputs and the outputs to the Transformer, but the\nprinciples of operation will remain the same as in SeqTrans.\nFig. 7: Example of an input to the PathTrans model. It takes\nall leaf nodes, along with its path to root, to make the next\nleaf token prediction. The root paths are embedded using an\nLSTM, and the leaf tokens are embedded using an embedding\nlayer. These two embeddings are added together to create\na path representation embedding, which is then fed to the\nTransformer as shown in Fig 6. The classiﬁcation layer of\nthe Transformer outputs leaf tokens.\n2) PathTrans: PathTrans enhances SeqTrans by exposing\ntree structure to the Transformer via root-paths. A root-path\nis the path from the leaf node t to the root of the AST by\ntraversing up its ancestors, recording all the nodes along with\nit, and thus a sequence of internal AST nodes. Fig 7 shows an\nexample of an input datapoint for predicting node 29 of Fig 3.\nThe root-paths are ﬁrst fed into a an LSTM 7 added with the\nembedding of the leaf node, and is fed through a Transformer:\no= Trans\n(\n(et + pt)t∈leaf seq\n)\nwhere o is a distribution over all possible leaf nodes,\nand et is the embedding for AST node t and pt =\nLSTM\n(\n(eu)u∈Rootpath(t)\n)\nis the summarized representation\nof root-path from t for every leaf node t in the leaf sequence\nleaf seq. 8 The hope here is that the root-paths captures the\nlocal syntactical information and thus can help the prediction.\nSince the points of prediction are the leaf nodes of the AST,\nthe loss is taken over only the leaf AST nodes, and it predicts\nonly leaf tokens.\n3) TravTrans: As a Transformer naturally only takes a\nsequence as input, we provide the AST nodes as a sequence\nin pre-order traversal, or a depth-ﬁrst-search (DFS) order.\nFor Fig 3, for node 29, the previous nodes in DFS order\nwould be: [..., “Call”, “NameLoad”, “map”, “AttributeLoad”,\n“NameLoad”, “string”, “Attr”].\nThe TravTrans model can be written as:\no= Trans\n(\n(et)t∈AST seq\n)\n7We could have used a Transformer to embed the path sequences in lieu\nof an LSTM, but since the path sequences are short (capped at 13 tokens)\nenough for LSTMs to perform adequately well, we decided to use an LSTM.\nSee Sec IV-B for details.\n8+ is used here following the convention of Transformer computations and\nto keep the embedding dimension the same for every component.\nwhere ois a distribution over all possible tokens, and et is the\nembedding for AST token t for every t in the partial program\nrepresented as a AST token sequence AST seq in DFS order.\n4) Capturing even more AST structure?: TravTrans\npresents the tree nodes in a pre-determined order, but still does\nnot retain detailed structural relationship between nodes. For\nexample, consider the sequence of nodes 26 - 28 in Fig 3. This\nwould be represented as [“NameLoad”, “string”, “attr”], the\nthree nodes appearing consecutively in DFS order. Looking at\nthe AST, we can see that the relations between (“NameLoad”\n& “string”, and “string” & “attr”) are actually quite different:\n“NameLoad” is one node up from “string”, while “string” is\ntwo nodes up and one node down from “attr”.\nWe create a new model, TravTrans+ that enhances Trav-\nTrans by capturing these richer path-based relations. Similarly\nto Hellendoorn et al. [29], we enhance the self attention block\nof the Transformer with a matrix Rthat captures the (unique)\npath needed to reach from a to b. This path is represented\nabstractly only in terms of up and down moves:\nUDpath(a, b) =UiDj\nwhere i, and j are the number of up and down nodes,\nrespectively, node a has to travel to reach node b. For example,\nUDpath(29, 27) = U2D2 for node 29 and 27 in Fig 3. R is\nintroduced to the Transformer by replacing the Attn function\nwith the following Attn TreeRel function.\nAttnTreeRel(Q, K, V, R) =softmax\n(\nR⊙(QK⊤)√dk\n)\nV\nwhere ⊙is element-wise product. This provides a way for\nthe self attention to consider the previous tokens, taking into\naccount the AST relationship between pairs of nodes as well.\nIV. I MPLEMENTATION AND DATASETS\nA. Dataset\nWe train our models using the py150 dataset used in [14].\nThe dataset consists of 150k Python 2 source code ﬁles from\nGitHub repositories, along with their parsed ASTs, split into\n100k for training and 50k for evaluation. In this work, we\nslightly modify the AST to ensure that the internal nodes\nonly carry syntactic types and the leaf nodes only carry token\nvalues. To incorporate large trees (greater than 1000 nodes,\nwhich is the limit we chose for transformer window), we\ndeploy a technique adopted by [30], which slices a large tree\ninto shorter segments with a sliding window to maintain part\nof the previous context.\nWe evaluate our models on two evaluation datasets:\n• py150: We use the evaluation dataset used in [14], which\nconsists of 50k Python ASTs. After the modiﬁcations\nmentioned above, there are 16,003,628 leaf nodes.\n• internal: We also created an evaluation dataset consisting\nof 5000 Python ﬁles from a code repository internal\nto Facebook. With this dataset, we can evaluate how\nour trained model can generalize to a different dataset,\neven if the code comes from disjoint projects. After the\nmodiﬁcations, there are 1,669,085 leaf nodes.\nB. Implementation\na) Transformers: For the models that use Transformers\n(SeqTrans, PathTrans, TravTrans, TravTrans+), we adapt the\nPytorch implementation 9 of GPT-2 small [20]. We use six\nTransformer blocks, six heads in each block, n ctx = 1000,\nand embedding dim = 300. We borrow other hyperparam-\neters from [20]. We limit the token vocabulary size to 100k,\nwhich covers over 90% of the tokens used in the training\ndataset. For PathTrans, we limit the maximum length of the\npath from leaf node to root to be 13, which covers over 90%\nof the nodes. For any path longer than 13, we keep the nodes\nclosest to the leaf, and truncate the nodes near the root.\nIn our implementation, we did not use positional encod-\ning [28] or positional embedding [20] to provide extra po-\nsitional information over elements since our early trials with\nLeafSeq suggested positional embedding is rather hurting than\nhelping. This is also supported by the claims in [31] that posi-\ntional encoding does not help for language modeling. Recently,\n[26] tried to introduce tree structures to Transformer models\nvia positional encoding. However, their relative improvement\nis small compared to what we see with tree-relational prior in\nSection V.\nb) RNN: For SeqRNN, we adapt the PyTorch LSTM\nexample implementation 10. We use embedding dimension\ndmodel = 300, with dropout = 0.5 and n layers = 1. We\nmaintain the same vocabulary size at 100k.\nc) Code2Seq: For Code2Seq, we used a PyTorch adapta-\ntion of the publicly released model 11, using the same hyperpa-\nrameters, except changing the vocab size to 100k. For selecting\n200 (max number of paths) paths per AST, we ﬁrst picked\npaths that ended with the target (to maximize the amount of\nlocal context). Since for each prediction point in the AST, a\nnew set of leaf-to-leaf paths have to be generated, the data\nprocessing for Code2Seq takes a substantial amount of time\n(magnitude of days worth of time).\nWe trained all models on Nvidia Tesla V100 (using 4 GPUs\nat a time) until the loss converged, with all of the parameters\nrandomly initialized. We used the Adam optimizer with the\nlearning rate set to 1e-3. Implementation details regarding\nnumber of epochs until convergence, training time (minutes\nper epoch), inference time (to evaluate over the py150 dataset),\nand model size, are listed in Table IV.\nd) Deep3: For the Deep3 model, since the authors have\nshared only the model and not the training algorithm, we used\nthe model pretrained on py150.\nC. Evaluation Metric\nWe evaluate the models on next token prediction for the\nleaf tokens. We report numbers for all leaf token predictions,\nas well as breaking down into more interesting categories:\nattribute access, numeric constant, name (variable, module),\nand function parameter name.\n9https://github.com/graykode/gpt-2-Pytorch.\n10https://github.com/pytorch/examples/tree/master/word language model\n11https://github.com/tech-srl/code2seq\nPrior work Our work\nSeqRNN Deep3 Code2Seq SeqTrans PathTrans TravTrans\nNum epochs 9 n/a 10 9 16 11\nTraining time (min / epoch) 45 n/a 210 45 45 60\nInference time (min) 40 75 45 40 20 50\nModel size (MB) 233 n/a 149 163 280 163\nTABLE IV: Implementation details for all the models - number of epochs until convergence, training time (minutes per epoch),\ninference time (to evaluate over the py150 dataset), and model size (state dict of PyTorch - the learnable parameters of the\nmodel). Note that some information about Deep3 is not available since the authors have shared only the model.\nTo measure performance on these tasks, we use mean\nreciprocal rank (MRR). The rank is deﬁned as\nMRR = 1\nn\nn∑\ni=1\n1\nranki\n(1)\nwhere n is the number of predicting locations and ranki is\nthe rank of the correct label given by the model for the ith\ndata point. We present MRR as a percentage, in keeping with\nprior work [7], [32].\nWhile Acc@1 only gives score when the correct label\nis ranked at the top, MRR also give scores when the true\nlabel is not ranked as the top, but among top few prediction.\nComparing to the hit-or-miss style metric (Acc@1), this is\ncloser to the realistic scenario when completion suggestions\nare presented to developers. With this practical perspective and\nfor ease of computation, we only consider ranki ≤10 for each\nlocation i (all ranki > 10 will have a score of 0). We share our\ndata processing scripts and model implementations at https:\n//github.com/facebookresearch/code-prediction-transformer.\nV. E VALUATION\nRQ1: Given a source token sequence, does a Trans-\nformer work better than an RNN, as in previous work?\nComparing SeqRNN and TravTrans, we ﬁnd that a Trans-\nformer does work better than an RNN. For the py150 dataset,\nwe can see a signiﬁcant improvement in MRR for predicting\nall leaf tokens in Table V, from 36.6% to 50.1% for the\nSeqRNN and SeqTrans models, respectively. The same holds\nfor comparing on the internal dataset, as shown in Table VI:\n23.8% vs 36.5%. Consistent improvements can be seen for\nspeciﬁc types of leaf tokens.\nPrior work Our work\nApplications SeqRNN SeqTrans TravTrans\n(type + value)\nAttribute access 39.3% 55.9% 60.4%\nNumeric constant 40.6% 55.9% 57.0%\nName (variable, module) 38.2% 54.1% 62.7%\nFunction parameter name 57.7% 66.2% 64.8%\nAll leaf tokens 36.6% 50.1% 54.9%\nTABLE V: MRR of various types of next token predictions\nfor py150. To fairly compare these models, TravTrans makes\ntwo predictions - one for the leaf node and then one for its\nparent internal node (see RQ3 for details).\nRQ2: Do the Transformer models on tree outperform\nprevious work on AST-based prediction?\nPrior work Our work\nApplications SeqRNN SeqTrans TravTrans\n(type + value)\nAttribute access 26.4% 41.0% 44.5%\nNumeric constant 32.2% 51.7% 53.0%\nName (variable, module) 25.0% 39.3% 47.2%\nFunction parameter name 45.5% 54.3% 51.4%\nAll leaf tokens 23.8% 36.5% 40.7%\nTABLE VI: MRR of various types of next token predictions\nfor internal dataset. TravTrans makes two predictions - one for\nthe leaf node and one for its parent internal node (see RQ3\nfor details).\nWe compare Deep3 and Code2Seq against PathTrans,\nand TravTrans (Table VII). Overall, we found that both\ntransformer-based models, achieve better scores than both\nDeep3 and Code2Seq for all leaf tokens as well as for speciﬁc\ntypes of leaf tokens. Our best performing model, TravTrans,\nimproves Deep3’s MRR by 14.1% (from 43.9% to 58.0%), and\nCode2Seq’s MRR by 14.4% (from 43.7% to 58.0%). Similar\nresults can be seen for the internal dataset (Table VIII).\nWe also compared the accuracy on AST internal predictions,\ncomparison Deep3 and TravTrans, as they are the only models\nwith this capability. In Table IX we see that TravTrans im-\nproves accuracy over Deep3 across the board. Table IX show\nnon-terminal value prediction for both py150 and internal\ndataset, respectively. We see that TravTrans is outperforms\nDeep3 for all internal node types as well. 12\nPrior work Our work\nApplications Deep3 Code2Seq PathTrans TravTrans\nAttribute access 45.3% 39.3% 57.2% 60.5%\nNumeric constant 53.2% 49.5% 59.1% 63.5%\nName (variable, module) 48.9% 45.8% 63.5% 66.6%\nFunction parameter name 58.1% 56.8% 65.8% 67.2%\nAll leaf nodes 43.9% 43.7% 55.1% 58.0%\nTABLE VII: MRR of various types of next token predictions\nfor py150.\nRQ3: Does adding syntactic structure help? Comparing\nSeqTrans and TravTrans, we conﬁrm that adding syntactic\nstructure does help.\nComparing SeqTrans against TravTrans on leaf nodes fairly\nis a bit tricky. A source code token we are looking at here\n12We do not include Code2Seq comparison for non-terminal node predic-\ntions due to the overhead required to prepare and process the dataset. Since\nthe main part of the paper was on leaf token prediction, and we have shown\nthat TravTrans performs signiﬁcantly better than Code2Seq, we did not deem\nit essential to include the results on non-terminal value predictions.\nPrior work Our work\nApplications Deep3 Code2Seq PathTrans TravTrans\nAttribute access 38.5% 26.4% 41.5% 44.7%\nNumeric constant 46.5% 45.4% 56.1% 61.5%\nName (variable, module) 41.0% 31.2% 48.0% 50.7%\nFunction parameter name 50.6% 39.3% 52.1% 53.3%\nAll leaf nodes 31.6% 31.0% 40.8% 43.9%\nTABLE VIII: MRR of various types of next token value\nprediction for internal dataset.\nDataset py150 internal\nApplications Deep3 TravTrans Deep3 TravTrans\nFunction call 81.6% 88.5% 78.2% 86.0%\nAssignment 76.5% 78.9% 78.5% 79.7%\nReturn 52.8% 67.8% 59.9% 72.2%\nList 59.4% 76.0% 40.8% 63.1%\nDictionary 66.3% 15.0% 39.8% 23.5%\nRaise 35.0% 63.3% 33.5% 59.3%\nAll types 81.9% 87.3% 79.9% 87.7%\nTABLE IX: MRR of various type predictions for py150 and\ninternal dataset.\ncontains both syntactic type and lexical value, which translates\nto two nodes in the AST. For example, the source code token\n“2” is equivalent to an internal node with value “Num” and\nits child leaf node “2”. Thus, for a fair comparison, TravTrans\nhas to make two predictions – one for the leaf node and one\nfor its parent internal node. For example, given the sequence\n“y =”, and the next prediction should be “2”, TravTrans must\npredict both the internal ”Num” node as well as the leaf ”2”\nnode. For evaluation, we implemented a local beam search to\nchoose the pair with the maximum joint probability. Table V\nshows that TravTrans still performs better than SeqRNN and\nSeqTrans (except for predicting function parameter name) for\npredicting the leaf tokens.\nIt is important to note that even with this correction, it is\ntricky to completely fairly compare the accuracy of SeqTrans\nagainst TravTrans model on leaf nodes. The main issue here is\nthat the contexts used to predict a leaf value is different in the\ntwo models, because of different order of seeing information.\nConsider the case of a code fragment x = y + z, and let us\nsay we need to predict what comes after the “=”. In the case\nof source token based prediction, the predictor has seen “x =”\nand would be expected to produce the next token (“y”). In an\nAST, one would ﬁrst construct an ”Assign” internal node, with\nthe right child to be ﬁlled next, and the next prediction would\nbe actually an interior node ”BinOpPlus”. The “y” would be\npredicted as the left child of BinOpPlus. In a context in which\nApplications TravTrans TravTrans+\nAttribute access. 60.5% 61.2%\nNumeric constant 63.5% 63.5%\nName (variable, module) 66.6% 67.7%\nFunction parameter name 67.2% 67.0%\nAll leaf nodes 58.0% 58.8%\nAll internal nodes 87.3% 91.9%\nTABLE X: MRR TravTrans compared against its variant\nTravTrans+ that incorporates more tree structure.\nthe AST has been augmented with the BinOpPlus node, the\nprediction of “y” has more information compared to what a\nsource token model did immediately after the “=”. This makes\na direct comparison of token predictions difﬁcult. One way to\nachieve better parity between the models would be to use an\nin-order traversal instead of a pre-order traversal; we chose\nthe latter because we want to do top-down generation, which\nwill be useful in future AST generative models.\nWe also compare TravTrans with its variant TravTrans+ that\nextends the model to incorporate even more tree structure, as\nmentioned in Sec III. Table X shows a slight increase in MRR:\n58.0% vs 58.8% for all leaf nodes, and 87.3% to 91.9% for\ninternal nodes. Due to mixed beneﬁt on leaf node predictions,\nwe continue to treat TravTrans as our ﬂagship model. But this\ndoes hint at the existence of more powerful models, if we\nincorporate more tree structure.\nVI. M ODEL INSPECTION\nDespite their wide adoption in many areas of computing,\ndeep learning models have been repeatedly shown suscepti-\nble to adversarial examples [33]–[35]. Interpretability studies\n[36]–[39] that provide human-understandable justiﬁcations,\nhave thus become an important property in building safe\nand trustworthy AI systems. With the growing adoption of\nneural models for software engineering tasks, the same con-\ncern raises. Recent discoveries of unexpected behaviours of\ndeep learning models of code [40]–[42] calls attention for\nsuch studies. Besides of helping understand the behaviour of\ndeep learning models, interpretability methods have also been\nused to directly improve the process of software engineering.\nFor example, [43], [44] used LIME [45], a model-agnostic\nexplainability method, to pinpoint the defective lines of code\nfrom defect detection models.\nAs a crucial ﬁrst step towards interpretability, in this section,\nwe reveal what TravTrans has learned that leads to its good\npredicative power. We found that TravTrans has learned to\nattribute the prediction to relevant previous tokens. While this\nis not a principled study, we hope it sheds light on a possible\napproach to perform a complete model inspection study.\nAlthough our Transformer-based models heavily relies on\nattentions, direct visualizations of weights in individual at-\ntention heads did not yield clear insights as the attentions\nare stacked across layers and multiple attention heads are in\neffect in each layer (see Sec II-E). We thus turn to gradient-\nbased saliency maps [23], [46], [47], for a more comprehensive\naccount of the inﬂuence of each input token. Following [23],\nthe inﬂuence of each input token is computed by taking the\npartial derivative of the loss function with respect to the\nembedding vector of that token. Fig 8 is the saliency map\nwe got for TravTrans, which visualizes the magnitudes of the\ngradients fall at each input token ( x-axis) when the model\npredicts a particular output ( y-axis). Intuitively, the larger the\nvalue for a particular token, the more sensitive the output is\nto the variations at that input token.\nWe ﬁrst observe that the parent AST node (the internal\nnode right above the leaf) is generally important in pre-\nFig. 8: Inﬂuence of previous nodes in value prediction of the\nexample in Fig 3 by TravTrans. x-axis is labeled with the\ninput values. y-axis is labeled with the values to be predicted.\nColor indicates the model’s prediction is correct or wrong.\ndicting the next leaf node value: the last token usually has\na high inﬂuence. More generally, we found that TravTrans\ntends to attribute more towards the internal nodes that are\ndirectly followed by relevant leaf nodes. For example, we can\nsee the prediction of atoi is inﬂuenced by (the locations\nprevious to) string, map and num_requests. It is not\nshown in the ﬁgure but the predictions of sys and argv\nbefore 2 are inﬂuenced by the previous occurrence of the\nsame values followed by 0 and 1. The prediction of host,\ngethostbyname are inﬂuenced by (the location previous to)\nip. For the position of gethostbyname, TravTrans predicts\nsocket with probability 0.31, with the correct answer ranked\nsecond with slightly lower probability 0.24. All above suggest\nthat TravTrans is capable of picking up information from\nrelevant previous code locations. The reason TravTrans tends\nto focus more on the parent nodes may be that all relevant\ninformation for the next token are already summarized in the\nhidden representation at the location previous to the token (aka\nparent nodes for the leaf nodes).\nOn an orthogonal note, we also observe that for many\npredicting locations, the magnitude of gradients are very small,\nsuggesting the robustness of the model in the sense that it is\nless sensitive to minor perturbations of the input sequence.\nVII. T HREATS TO VALIDITY\na) Out-of-vocabulary (OOV) handling: One of the chal-\nlenges in next-token prediction is predicting tokens that are\nrare or unseen in the training corpus.\nWhen trained on py150, 21% and 35% of prediction loca-\ntions observe OOV tokens for the py150 and the internal test\nset, respectively. None of the models in our main evaluation\nhandle OOV predictions and thus are unable to correctly\npredict for such locations.\nHandling of OOV tokens, while important, is orthogonal\nto core model design, and is outside the scope of our cur-\nrent work. Adding OOV handling to a model should further\nimprove its performance, as suggested by previous works [5],\n[7], [48]. Moreover, a quick comparison (see Sec VIII) against\nPointerMixture from [5] found that TravTrans already has an\nedge even without OOV handling for the py150 dataset. We\nthus do not expect OOV handling to reverse our ﬁndings.\nb) Training Corpus: While larger Python corpora have\nappeared, py150 is still sizable at 500MB; we do not expect\nthe larger corpora to reverse our ﬁndings.\nc) Python speciﬁcity: We have only carried out evalua-\ntions on Python, and have not demonstrated that our results\nwould carry over (in trends) to other languages. The Deep3\npaper did ﬁnd their results (in trends) to roughly carry over\nfrom Python to JavaScript.\nd) Model comparison: To bound the scope of this paper,\nwe limit our investigation to techniques that do not require\nany compiler support beyond constructing the AST; thus, we\nexclude ways to communicate program analysis results such\nas def-use information, as in [49]. We also limit our study to\nthe Transformer architecture, and purposely exclude the graph\nneural networks (GNN), because there is already an active\ndebate between GNN and Transformer for NLP applications\nthat transcends code prediction.\nVIII. R ELATED WORK\nWe compared our models extensively with three baselines:\nSeqRNN, Deep3 and Code2Seq, both qualitatively in Sec II\nand quantitatively in Sec V. Before proceeding to a broader\ndiscussion, we brieﬂy compare our work with the attentive\npointer mixture model (PointerMixture) from Li at al. [5] as\nanother plausible baseline. We are particularly interested in\nthis model, because it (1) works with serialized AST node\nsequences, (2) uses attention mechanism on top of LSTM,\nand (3) has reportedly achieved better accuracy than Deep3’s\nresults.\nThe similarities and differences in relation to our work are\nas follows. For (1), the preorder traversal of the AST they used\nis similar to ours. They did not separate types and values into\ndifferent nodes as we do, which makes their sequences more\ncompact than ours. For (2), their attention is applied within\na ﬁxed-sized window on top of LSTM outputs, whereas our\nTransformer-based models use stacked layers of attentions as\nthe key mechanism for computing throughout the network.\n[5] also used “parent attention”, which is based on the parent-\nchild relation in the AST. In comparison, our PathTrans and\nTravTrans+ extend far beyond the direct parent-child relation\nby making use of paths.\nFor (3), the accuracy reported in [5] included the predictions\nof null values for internal nodes, whereas our numbers only\nconsider leaf nodes with non-empty values. To fairly compare\nthe predicative power between our proposal and theirs, we\nimplemented PointerMixture in our setting. 13 Note that Point-\nerMixture uses a pointer network [50] which decides for each\n13To maintain consistency, the hidden size and the embedding sizes were\nset to 300, and the vocab size was increased to 100k. The model was trained\nfor 8 epochs.\nDataset py150 internal\nApplications PointerMixture TravTrans OOV Rate (%) PointerMixture TravTrans OOV Rate (%)\nAttribute access 54.2% 60.5% 19.3% 50.4% 44.7% 33.8%\nNumeric constant 50.6% 63.5% 10.3% 40.4% 61.5% 6.3%\nName (variable, module) 49.5% 66.6% 15.5% 42.1% 50.7% 31.4%\nFunction parameter name 59.7% 67.2% 7.7% 55.2% 53.3% 18.7%\nAll leaf nodes 51.9% 58.0% 21.1% 46.3% 43.9% 34.8%\nTABLE XI: MRR of PointerMixture compared against TravTrans for various types of next token prediction for py150 and\ninternal dataset. The out-of-vocabulary (OOV) rates for internal dataset is much higher compared to py150 dataset.\npoint of prediction whether to use the LSTM output or to\ncopy from a previous location, whereas our TravTrans has no\nmeans for handling out-of-vocabulary (OOV) tokens (i.e. all\npredictions requiring an OOV token are incorrect.)\nWe found on the basis of py150 dataset that PointerMixture\noutperforms SeqRNN, as well as the other baselines of Deep3\nand Code2Seq. However, TravTrans outperforms PointerMix-\nture, even though all the OOV predictions count as wrong\nfor TravTrans! For the internal dataset, PointerMixture model\ndoes better than TravTrans for some prediction types. This is\nexpected: we trained the model for illustration on a different\ndataset (py150) than the one for evaluation (internal), causing\na signiﬁcantly higher percentage of OOV predictions. In actual\nusage, we would retrain our models on the internal dataset. The\nresults are shown in Table XI, focusing on PointerMixture vs.\nTravTrans.\nSince we have already introduced a brief history of au-\ntocomplete in the paper, this section will focus on other\nexplorations in the Transformer and code prediction space.\na) Code Prediction: In this paper, we viewed the context\nof prediction as code that appears strictly before the cursor [4]–\n[6], [11]–[14]. Among other ﬂavors of code prediction are the\nones where code after the prediction location, when available,\nis taken into account [9], [49], [51], [52], e.g. when completing\na “hole” in a program, or correcting a misused local variable\ninstance. Another dimension of work considers prediction at\nvarying granularities of predictions, e.g. from characters [53]\nto subtokens [7]) to AST fragments (e.g. sub-ASTs [52]). In\nthe context of autocomplete, we believe that subtokens or BPE\nare indeed a promising future direction, as we mentioned in\nSec IX.\nThere have also been work discussing the practical implica-\ntions of applying a code prediction tool into production. [54],\n[55] state that synthetic benchmarks are not representative\nof real-world data, and accuracy of the models drops when\nevaluated on real-world data. [56] discusses approaches to\nmake models more lightweight to allow for faster computa-\ntions and less memory usage in an IDE. A user evaluation\nof an autocomplete tool with stronger ML models is outside\nthe scope of this paper. We would also like to point out that\nadvantages in idealistic settings often transfer to advantages in\nmore practical settings, as conﬁrmed by the results reported\nin the above two works (Table II and III in [54], and Table 2\nin [56]).\nb) Transformers: Other than next token prediction,\nTransformers have been used recently for code summariza-\ntion [24]. Furthermore, there has been a surge of interest\nsince 2019 in extending Transformer models to handle beyond\nsequential structures for NLP [57]–[59]. It has been shown that\ntaking tree structure into account helped code correction [25]\nand code translation [26].\nThere is practical interest, outside of academic literature, in\nthe topic of code prediction using Transformers. Galois [60]\nis an open source project that uses GPT-2 [20] for code\nprediction. TabNine™ published a blog post [61] in July 2019\nmentioning the use of GPT-2 in their code prediction but re-\nvealed no technical detail. Recently a team from Microsoft also\npublished their ongoing efforts [62] on applying Transformers\nfor code autocomplete. We believe we are among the ﬁrst to\nsystematically evaluate Transformers for code prediction and\ncompare them to previous models.\nc) Deep learning techniques over Code, beyond Code\nPrediction: There are many other uses of deep learning\ntechniques for code, beyond code prediction. These include\ntechniques for code summarization [21], bug ﬁnding [49], re-\npair [63] and many others. An interesting aspect of this body of\nwork is in the different ways in which they represent a program\nas an input to a neural architecture. These representations have\nranged from linear token sequence (as for code prediction [5],\n[7], [32]), to paths in an AST [15], [21], [52], and sometimes\neven ways to convey static analysis information to the neural\nnetwork [24], [49], [51], [64].\nIX. C ONCLUSION AND FUTURE WORK\nIn this paper, we presented ways to using the Transformer\nfor code prediction. We showed that the Transformer outper-\nforms existing models for code prediction, and when supplied\nwith code’s structural information, we are able to get even\nbetter predictive power. Attribution study show that our best\nmodel tends to focus on relevant code locations for prediction.\nIn the future, one avenue we wish to continue working on is\nhandling out-of-vocabulary words better. Source code presents\na difﬁculty shared with NLP in handling large vocabularies and\nrare words. The token/word to be predicted in test data may\nnot appear in the training data. This is even more challenging\nwhen predicting identiﬁers, such as method names, variable\nnames, as developers can come up with arbitrary identiﬁer\nnames. Possible mitigation includes copying mechanism [48],\n[51], [65] and open-vocabulary models [7], [66].\nThis paper focused on predicting the next token, as it is\nalready a challenging task. In future, we also want to explore\npredicting multiple tokens at a time, i.e. autocompleting entire\nexpressions.\nREFERENCES\n[1] A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, “On the\nnaturalness of software,” Communications of the ACM , vol. 59, no. 5,\npp. 122–131, 2016.\n[2] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, “A survey\nof machine learning for big code and naturalness,” ACM Computing\nSurveys (CSUR), vol. 51, no. 4, pp. 1–37, 2018.\n[3] T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “A\nstatistical semantic language model for source code,” in Proceedings\nof the 2013 9th Joint Meeting on Foundations of Software\nEngineering, ser. ESEC/FSE 2013. New York, NY , USA: Association\nfor Computing Machinery, 2013, p. 532–542. [Online]. Available:\nhttps://doi.org/10.1145/2491411.2491458\n[4] C. Liu, X. Wang, R. Shin, J. E. Gonzalez, and D. Song, “Neural code\ncompletion,” 2016. [Online]. Available: https://openreview.net/forum?\nid=rJbPBt9lg\n[5] J. Li, Y . Wang, M. R. Lyu, and I. King, “Code completion with neural\nattention and pointer networks,” in Proceedings of the 27th International\nJoint Conference on Artiﬁcial Intelligence, ser. IJCAI’18. AAAI Press,\n2018, p. 4159–25.\n[6] F. Liu, L. Zhang, and Z. Jin, “Modeling programs hierarchically with\nstack-augmented LSTM,” Journal of Systems and Software , p. 110547,\n2020. [Online]. Available: https://doi.org/10.1016/j.jss.2020.110547\n[7] R.-M. Karampatsis, H. Babii, R. Robbes, C. Sutton, and A. Janes, “Big\ncode != big vocabulary: Open-vocabulary models for source code,” in\nInternational Conference on Software Engineering (ICSE) , 2020.\n[8] A. Svyatkovskiy, Y . Zhao, S. Fu, and N. Sundaresan, “Pythia: AI-assisted\ncode completion system,” Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , Jul\n2019. [Online]. Available: http://dx.doi.org/10.1145/3292500.3330699\n[9] V . Raychev, M. Vechev, and E. Yahav, “Code completion with\nstatistical language models,” in Proceedings of the 35th ACM SIGPLAN\nConference on Programming Language Design and Implementation ,\nser. PLDI ’14. New York, NY , USA: Association for Computing\nMachinery, 2014, p. 419–428. [Online]. Available: https://doi.org/10.\n1145/2594291.2594321\n[10] G. A. Aye and G. E. Kaiser, “Sequence model design for code comple-\ntion in the modern IDE,” arXiv preprint arXiv:2004.05249 , 2020.\n[11] M. Allamanis and C. Sutton, “Mining idioms from source code,” in\nProceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering , 2014, pp. 472–483.\n[12] P. Bielik, V . Raychev, and M. Vechev, “PHOG: probabilistic model\nfor code,” in International Conference on Machine Learning , 2016, pp.\n2933–2942.\n[13] V . Raychev, P. Bielik, M. Vechev, and A. Krause, “Learning\nprograms from noisy data,” in Proceedings of the 43rd Annual\nACM SIGPLAN-SIGACT Symposium on Principles of Programming\nLanguages, ser. POPL ’16. New York, NY , USA: Association\nfor Computing Machinery, 2016, p. 761–774. [Online]. Available:\nhttps://doi.org/10.1145/2837614.2837671\n[14] V . Raychev, P. Bielik, and M. Vechev, “Probabilistic model for code\nwith decision trees,” in Proceedings of the 2016 ACM SIGPLAN\nInternational Conference on Object-Oriented Programming, Systems,\nLanguages, and Applications , ser. OOPSLA 2016. New York, NY ,\nUSA: Association for Computing Machinery, 2016, p. 731–747.\n[Online]. Available: https://doi.org/10.1145/2983990.2984041\n[15] U. Alon, O. Levy, and E. Yahav, “code2seq: Generating sequences from\nstructured representations of code,” in International Conference\non Learning Representations , 2019. [Online]. Available: https:\n//openreview.net/forum?id=H1gKYo09tX\n[16] “150k python dataset,” 2016. [Online]. Available: https://eth-sri.github.\nio/py150\n[17] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing\nsource code using a neural attention model,” in Proceedings of\nthe 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Berlin, Germany: Association\nfor Computational Linguistics, Aug. 2016, pp. 2073–2083. [Online].\nAvailable: https://www.aclweb.org/anthology/P16-1195\n[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[19] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\nM. Zhou, and H.-W. Hon, “Uniﬁed language model pre-training\nfor natural language understanding and generation,” in Advances in\nNeural Information Processing Systems , 2019, pp. 13 042–13 054.\n[Online]. Available: https://papers.nips.cc/paper/9464-uniﬁed-language-\nmodel-pre-training-for-natural-language-understanding-and-generation\n[20] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” inOpenAI Blog,\n2019. [Online]. Available: https://openai.com/blog/better-language-\nmodels/\n[21] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, “code2vec: Learning\ndistributed representations of code,” Proceedings of the ACM on\nProgramming Languages, vol. 3, no. POPL, pp. 1–29, 2019. [Online].\nAvailable: https://doi.org/10.1145/3290353\n[22] V . J. Hellendoorn and P. Devanbu, “Are deep neural networks the best\nchoice for modeling source code?” in Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering , 2017, pp. 763–773.\n[23] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside convolutional\nnetworks: Visualising image classiﬁcation models and saliency maps,”\narXiv preprint arXiv:1312.6034 , 2013.\n[24] V . J. Hellendoorn, C. Sutton, R. Singh, and P. Maniatis, “Global\nrelational models of source code,” in International Conference\non Learning Representations , 2020. [Online]. Available: https:\n//openreview.net/forum?id=B1lnbRNtwr\n[25] J. Harer, C. Reale, and P. Chin, “Tree-Transformer: A transformer-\nbased method for correction of tree-structured data,” arXiv preprint\narXiv:1908.00449, 2019.\n[26] V . Shiv and C. Quirk, “Novel positional encodings to enable\ntree-based transformers,” in Advances in Neural Information\nProcessing Systems , 2019, pp. 12 058–12 068. [Online]. Avail-\nable: https://papers.nips.cc/paper/9376-novel-positional-encodings-to-\nenable-tree-based-transformers\n[27] “Pretrained probabilistic models for code,” 2017. [Online]. Available:\nhttps://github.com/eth-sri/ModelsPHOG\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nu. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings\nof the 31st International Conference on Neural Information Processing\nSystems, ser. NIPS’17. Red Hook, NY , USA: Curran Associates Inc.,\n2017, p. 6000–6010.\n[29] V . J. Hellendoorn, C. Sutton, R. Singh, P. Maniatis, and D. Bieber,\n“Global relational models of source code,” in International conference\non learning representations , 2019. [Online]. Available: https://\nopenreview.net/forum?id=B1lnbRNtwr\n[30] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, “Character-\nlevel language modeling with deeper self-attention,” in Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol. 33, no. 01, 2019,\npp. 3159–3166.\n[31] K. Irie, A. Zeyer, R. Schl ¨uter, and H. Ney, “Language modeling with\ndeep transformers,” Interspeech 2019 , Sep 2019. [Online]. Available:\nhttp://dx.doi.org/10.21437/Interspeech.2019-2225\n[32] V . J. Hellendoorn and P. T. Devanbu, “Are deep neural networks the best\nchoice for modeling source code?” in Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering , 2017.\n[33] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning\nin computer vision: A survey,” IEEE Access, vol. 6, pp. 14 410–14 430,\n2018.\n[34] A. Chakraborty, M. Alam, V . Dey, A. Chattopadhyay, and D. Mukhopad-\nhyay, “Adversarial attacks and defences: A survey,” arXiv preprint\narXiv:1810.00069, 2018.\n[35] W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, “Adversarial attacks\non deep-learning models in natural language processing: A survey,”ACM\nTransactions on Intelligent Systems and Technology (TIST) , vol. 11,\nno. 3, pp. 1–41, 2020.\n[36] S. Chakraborty, R. Tomsett, R. Raghavendra, D. Harborne, M. Alzantot,\nF. Cerutti, M. Srivastava, A. Preece, S. Julier, R. M. Rao et al. ,\n“Interpretability of deep learning models: a survey of results,” in 2017\nIEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced &\nTrusted Computed, Scalable Computing & Communications, Cloud &\nBig Data Computing, Internet of People and Smart City Innovation\n(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI). IEEE, 2017,\npp. 1–6.\n[37] Q.-S. Zhang and S.-C. Zhu, “Visual interpretability for deep learning: a\nsurvey,”Frontiers of Information Technology & Electronic Engineering,\nvol. 19, no. 1, pp. 27–39, 2018.\n[38] D. V . Carvalho, E. M. Pereira, and J. S. Cardoso, “Machine learning\ninterpretability: A survey on methods and metrics,” Electronics, vol. 8,\nno. 8, p. 832, 2019.\n[39] X. Huang, D. Kroening, W. Ruan, J. Sharp, Y . Sun, E. Thamo, M. Wu,\nand X. Yi, “A survey of safety and trustworthiness of deep neural\nnetworks: Veriﬁcation, testing, adversarial attack and defence, and\ninterpretability,” Computer Science Review , vol. 37, p. 100270, 2020.\n[40] K. Wang and M. Christodorescu, “Coset: A benchmark for evaluating\nneural program embeddings,” arXiv preprint arXiv:1905.11445 , 2019.\n[41] N. Yefet, U. Alon, and E. Yahav, “Adversarial examples for\nmodels of code,” Proceedings of the ACM on Programming\nLanguages, vol. 4, no. OOPSLA, Nov. 2020. [Online]. Available:\nhttps://doi.org/10.1145/3428230\n[42] G. Ramakrishnan, J. Henkel, Z. Wang, A. Albarghouthi, S. Jha, and\nT. Reps, “Semantic robustness of models of source code,” arXiv preprint\narXiv:2002.03043, 2020.\n[43] J. Jiarpakdee, C. Tantithamthavorn, H. K. Dam, and J. Grundy, “An\nempirical study of model-agnostic techniques for defect prediction\nmodels,” IEEE Transactions on Software Engineering , pp. 1–1, 2020.\n[44] S. Wattanakriengkrai, P. Thongtanunam, C. Tantithamthavorn, H. Hata,\nand K. Matsumoto, “Predicting defective lines using a model-agnostic\ntechnique,”IEEE Transactions on Software Engineering, no. 01, pp. 1–1,\nsep 5555.\n[45] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should i trust you?”:\nExplaining the predictions of any classiﬁer,” in Proceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining , ser. KDD ’16. New York, NY , USA: Association\nfor Computing Machinery, 2016, p. 1135–1144. [Online]. Available:\nhttps://doi.org/10.1145/2939672.2939778\n[46] D. Smilkov, N. Thorat, B. Kim, F. Vi ´egas, and M. Wattenberg,\n“Smoothgrad: removing noise by adding noise,” arXiv preprint\narXiv:1706.03825, 2017.\n[47] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep\nnetworks,” in Proceedings of the 34th International Conference on\nMachine Learning - Volume 70 , ser. ICML’17. JMLR.org, 2017, p.\n3319–3328.\n[48] M. Allamanis, H. Peng, and C. Sutton, “A convolutional attention\nnetwork for extreme summarization of source code,” in Proceedings\nof The 33rd International Conference on Machine Learning , ser.\nProceedings of Machine Learning Research, M. F. Balcan and\nK. Q. Weinberger, Eds., vol. 48. New York, New York, USA:\nPMLR, 20–22 Jun 2016, pp. 2091–2100. [Online]. Available:\nhttp://proceedings.mlr.press/v48/allamanis16.html\n[49] M. Allamanis, M. Brockschmidt, and M. Khademi, “Learning\nto represent programs with graphs,” in International Conference\non Learning Representations , 2018. [Online]. Available: https:\n//openreview.net/forum?id=BJOFETxR-\n[50] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” arXiv\npreprint arXiv:1506.03134, 2015.\n[51] M. Brockschmidt, M. Allamanis, A. L. Gaunt, and O. Polozov,\n“Generative code modeling with graphs,” in International Conference\non Learning Representations , 2019. [Online]. Available: https:\n//openreview.net/forum?id=Bke4KsA5FX\n[52] U. Alon, R. Sadaka, O. Levy, and E. Yahav, “Structural language\nmodels of code,” in Proceedings of the 37th International Conference\non Machine Learning , ser. Proceedings of Machine Learning Research,\nH. D. III and A. Singh, Eds., vol. 119. PMLR, 13–18 Jul 2020, pp. 245–\n256. [Online]. Available: http://proceedings.mlr.press/v119/alon20a.html\n[53] P. Bielik, V . Raychev, and M. Vechev, “Program synthesis for\ncharacter level language modeling,” in International Conference\non Learning Representations , 2016. [Online]. Available: https:\n//openreview.net/forum?id=ry sjFqgx\n[54] V . J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, “When\ncode completion fails: A case study on real-world completions,” in Pro-\nceedings of the 41st International Conference on Software Engineering ,\n2019, p. 960–970.\n[55] G. A. Aye, S. Kim, and H. Li, “Learning autocompletion from real-\nworld datasets,” in Proceedings of the ACM/IEEE 43rd International\nConference on Software Engineering: Software Engineering in Practice.\n[Online]. Available: https://arxiv.org/abs/2011.04542\n[56] A. Svyatkovskiy, S. Lee, A. Hadjitoﬁ, M. Riechert, J. Franco, and\nM. Allamanis, “Fast and memory-efﬁcient neural code completion,”\n2020.\n[57] Y . Wang, H.-Y . Lee, and Y .-N. Chen, “Tree transformer: Integrating tree\nstructures into self-attention,” in Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 2019, pp. 1060–1070. [Online]. Available: https:\n//www.aclweb.org/anthology/D19-1098/\n[58] M. Ahmed, M. R. Samee, and R. E. Mercer, “You only need attention\nto traverse trees,” in Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics , 2019, pp. 316–322.\n[Online]. Available: https://www.aclweb.org/anthology/P19-1030/\n[59] X.-P. Nguyen, S. Joty, S. Hoi, and R. Socher, “Tree-structured\nattention with hierarchical accumulation,” in International Conference\non Learning Representations , 2020. [Online]. Available: https:\n//openreview.net/forum?id=HJxK5pEYvr\n[60] “Galois autocompleter,” https://github.com/iedmrc/galois-autocompleter.\n[61] TabNine, “Autocompletion with deep learning,” TabNine Blog , Jul\n2019. [Online]. Available: https://tabnine.com/blog/deep\n[62] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, IntelliCode\nCompose: Code Generation Using Transformer . New York, NY , USA:\nAssociation for Computing Machinery, 2020, p. 1433–1443.\n[63] M. Vasic, A. Kanade, P. Maniatis, D. Bieber, and R. Singh, “Neural\nprogram repair by jointly learning to localize and repair,” arXiv preprint\narXiv:1904.01720, 2019.\n[64] Y . Yang and C. Xiang, “Improve language modelling for code com-\npletion through learning general token repetition of source code,” in\n31st International Conference Software Engineering and Knowledge\nEngineering, 2019, pp. 667–777.\n[65] P. Fernandes, M. Allamanis, and M. Brockschmidt, “Structured\nneural summarization,” in International Conference on Learning\nRepresentations, 2019. [Online]. Available: https://openreview.net/\nforum?id=H1ersoRqtm\n[66] M. Cvitkovic, B. Singh, and A. Anandkumar, “Open vocabulary\nlearning on source code with a graph-structured cache,” in Proceedings\nof the 36th International Conference on Machine Learning , ser.\nProceedings of Machine Learning Research, K. Chaudhuri and\nR. Salakhutdinov, Eds., vol. 97. Long Beach, California, USA:\nPMLR, 09–15 Jun 2019, pp. 1475–1485. [Online]. Available:\nhttp://proceedings.mlr.press/v97/cvitkovic19b.html"
}