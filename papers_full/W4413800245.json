{
  "title": "How Good Are Large Language Models at Supporting Frontline Healthcare Workers in Low-Resource Settings – A Benchmarking Study &amp; Dataset",
  "url": "https://openalex.org/W4413800245",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2772671756",
      "name": "Samuel Rutunda",
      "affiliations": [
        "Digital Science (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2963208684",
      "name": "Gwydion Williams",
      "affiliations": [
        "Programs for Assessment of Technology in Health Research Institute",
        "Program for Appropriate Technology in Health"
      ]
    },
    {
      "id": "https://openalex.org/A5119466213",
      "name": "Kleber Kabanda",
      "affiliations": [
        "Digital Science (United States)"
      ]
    },
    {
      "id": null,
      "name": "Francis Nkurunziz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119466215",
      "name": "Solange Uwiduhaye",
      "affiliations": [
        "University Teaching Hospital of Butare"
      ]
    },
    {
      "id": "https://openalex.org/A5119466216",
      "name": "Eulade Rugegamanzi",
      "affiliations": [
        "University Teaching Hospital of Butare"
      ]
    },
    {
      "id": "https://openalex.org/A5119466217",
      "name": "Cyprien Nshimiyimana",
      "affiliations": [
        "Revolution Research"
      ]
    },
    {
      "id": "https://openalex.org/A2944526878",
      "name": "Vaishnavi Menon",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A5066937667",
      "name": "Mira Emmanuel-Fabula",
      "affiliations": [
        "Feminist Archive North"
      ]
    },
    {
      "id": "https://openalex.org/A2034410391",
      "name": "Alastair K. Denniston",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128335348",
      "name": "Liu Xiaoxuan",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A5093677089",
      "name": "Emery Hezagira",
      "affiliations": [
        "Rwanda Biomedical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2594388025",
      "name": "Bilal A Mateen",
      "affiliations": [
        "Birmingham City University"
      ]
    },
    {
      "id": "https://openalex.org/A2772671756",
      "name": "Samuel Rutunda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2963208684",
      "name": "Gwydion Williams",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119466213",
      "name": "Kleber Kabanda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119466215",
      "name": "Solange Uwiduhaye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119466216",
      "name": "Eulade Rugegamanzi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119466217",
      "name": "Cyprien Nshimiyimana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2944526878",
      "name": "Vaishnavi Menon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5066937667",
      "name": "Mira Emmanuel-Fabula",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128335348",
      "name": "Liu Xiaoxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093677089",
      "name": "Emery Hezagira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2594388025",
      "name": "Bilal A Mateen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4405461579",
    "https://openalex.org/W4390971012",
    "https://openalex.org/W6912093287"
  ],
  "abstract": "Abstract Large language models (LLMs) have demonstrated strong performance in medical contexts; however, existing benchmarks often fail to reflect the real-world complexity of low-resource health systems accurately. This study developed a dataset of 5,609 clinical questions contributed by 101 community health workers (CHWs) across four Rwandan districts and compared responses generated by five large language models (LLMs) (Gemini-2, GPT-4o, o3 mini, Deepseek R1, and Meditron-70B) with those from local clinicians. A subset of 524 question-answer pairs was evaluated using a rubric of 11 expert-rated metrics, scored on a five-point Likert scale. Gemini-2 and GPT-4o were the best performers (achieving mean scores of 4.49 and 4.48 out of 5, respectively, across all 11 metrics). All LLMs significantly outperformed local clinicians (ps &lt; 0.001) across all metrics, with Gemini-2, for example, surpassing local GPs by an average of 0.83 points on every metric (range: 0.38 – 1.10). While performance degraded slightly when LLMs communicated in Kinyarwanda, the LLMs remained superior to clinicians and were over 500 times cheaper per response. These findings support the potential of LLMs to strengthen frontline care quality in low-resource, multilingual health systems.",
  "full_text": "How Good Are Large Language Models at Supporting \nFrontline Healthcare Worker s in Low-Resource Settings \n– A Benchmarking Study & Dataset  \n \nAuthors: Samuel Rutunda1*, Dr. Gwydion Williams2*, Kleber Kabanda1, Francis Nkurunziz1, Dr. \nSolange Uwiduhaye3, Dr. Eulade Rugegamanzi3, Cyprien Nshimiyimana4, Dr. Vaishnavi Menon5, \nMira Emmanuel-Fabula6, Prof. Alastair K. Denniston5, Dr. Xiaoxuan Liu5, Emery Hezagira7, Prof. Bilal \nA. Mateen2,5  \n \n*Joint first authors \n \n1 Digital Umuganda, Rwanda \n2 PATH, United Kingdom \n3 Butaro Teaching Hospital, Rwanda \n4 Rwanda Centre for the Fourth Industrial Revolution, Rwanda \n5 University of Birmingham, United Kingdom \n6 FATH, Switzerland \n7 Rwanda Biomedical Centre, Rwanda \n \nCorresponding Author:  \nProf. Bilal A Mateen \nbmateen@path.org \n437 N 34th Street  \nSeattle, WA 98103,  \nUSA.  \n \nWord count:  1,882 \nNumber of boxes: 1 \nNumber of figures:  1 \n \nKeywords:  Artificial Intelligence, Large Language Model, Rwanda, Community Health Worker, \nEvaluation, Clinical Decision Support \n \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract  \n \nLarge language models (LLMs) have demonstrated strong performance in medical contexts; however, \nexisting benchmarks often fail to reflect the real-world complexity of low-resource health systems \naccurately. This study developed a dataset of 5,609 clinical questions contributed by 101 community \nhealth workers (CHWs) across four Rwandan districts and compared responses generated by five large \nlanguage models (LLMs) (Gemini-2, GPT-4o, o3 mini, Deepseek R1, and Meditron-70B) with those from \nlocal clinicians. A subset of 524 question-answer pairs was evaluated using a rubric of 11 expert-rated \nmetrics, scored on a five-point Likert scale. Gemini-2 and GPT-4o were the best performers (achieving \nmean scores of 4.49 and 4.48 out of 5, respectively, across all 11 metrics). All LLMs significantly \noutperformed local clinicians (ps < 0.001) across all metrics, with Gemini-2, for example, surpassing local \nGPs by an average of 0.83 points on every metric (range: 0.38 – 1.10). While performance degraded \nslightly when LLMs communicated in Kinyarwanda, the LLMs remained superior to clinicians and were \nover 500 times cheaper per response. These findings support the potential of LLMs to strengthen \nfrontline care quality in low-resource, multilingual health systems. \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nLarge language models (LLMs) have consistently demonstrated expert-level performance on \npostgraduate medical examinations such as the USMLE1, and in navigating clinical vignettes that \napproximate real-world scenarios with similar levels of accuracy2. However, these assessments fail to \nreflect the complexities of tiered health systems commonly found in low- and middle-income countries \n(LMICs). In these settings, frontline care is often delivered by narrowly trained community health workers \n(CHWs); diagnostic and therapeutic resources may be scarce; and healthcare delivery frequently occurs \nin non-English language environments, posing additional linguistic challenges3. \n \nThe development of the AfriMedQA dataset addressed a critical gap by creating the world’s first large-\nscale English-language African medical multiple-choice question (MCQ) dataset4. The accompanying \nbenchmarking study revealed performance differences on 'African' questions compared to \nMedQA/USMLE-derived questions5. The significance of such datasets lies not only in representational \nequity but also in their ability to encode the meaningful differences in disease burden, clinical \npresentation, and healthcare infrastructure that likely explain (in part) the observed geography-based \nvariation in LLM performance. However, benchmarking datasets that mirror the realities of healthcare in \nresource-limited settings remain scarce. This scarcity hinders our understanding of whether existing LLMs \nare suitable for these contexts and limits opportunities to de-risk deployments through in-silico testing. \n \nTo address this, we engaged 101 CHWs across four Rwandan districts (Gicumbi, Gakenke, Nyanza, and \nNgoma) to generate open-ended clinical questions (i.e, vignettes) based on typical patient encounters. \nCHW demographic characteristics are detailed in Supplementary Table 1. Participants were encouraged \nto submit at least 60 questions over three weeks via a custom data collection app ('Mbaza'), developed \nby Digital Umuganda, a Rwandan technology company. Questions were submitted as voice recordings \nin Kinyarwanda, transcribed using a speech-to-text model developed by Digital Umuganda6, and \nsubsequently cleaned and screened for quality and relevance by trained local nurses. \n \nOut of 7,143 questions submitted, 1534 were excluded based on quality criteria, resulting in 5,609 \naccepted entries. Questions were mapped to 18 domains that aligned with the 14 government-defined \nCHW work packages (Supplementary Figure 3). Multiple category assignments were allowed per \nquestion. The most common category was “Other” (n = 1,613), followed by “Malaria” (n = 1,133) and \n“Maternal and Newborn Health” (n = 802). The least frequent categories included “Emergency \nResponse to Epidemics” (n = 56), “Gender-Based Violence” (GBV) (n = 72), and “Adolescent and Sexual \nReproductive Health” (n = 215). Quality assessment and categorisation were completed by a group of \nlocal nurses (demographics in Supplementary Table 2). \n \nFollowing data collection, a group of Rwandan general practitioners and senior nurses (demographics \nprovided in Supplementary Table 3) was recruited to generate clinician responses to each question, \nsimulating the ideal response a Community Health Worker (CHW) might receive if such a ‘tele-advice’ \nservice existed. Questions and clinician responses were translated into English or Kinyarwanda to create \na fully bilingual dataset. These (translation) outputs were subsequently reviewed and edited by \nprofessional linguists. In parallel, responses to the questions were generated using five LLMs: Gemini-2-\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nFlash, GPT-4o, o3-mini-high, Deepseek-R1, and Meditron-70B. An illustrative example of a question, \nalong with a paired clinician and LLM response, is provided in Box 1. \nBox 1: An illustrative (mental health -related) query and a pair of responses (clinician & LLM) . Note \nthat the indiv iduals depicted  and discussed  here are illustrative, and are not real people.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nA fifty-two-year-old woman, weighing thirty-eight kilograms, has been speaking \nloudly during our discussions to commemorate the genocide against the Tutsi \nin 1994. I have taken her to a quiet place. Help me. Since I see she has a \ntrauma issue, what other help can I provide her?”\nWell, if you have really failed to make her come back to normal state, then that woman might be having a \npsychiatric condition (mental disorder) called Post traumatic stress disorder(PTSD) that happens to people \nwho have encountered multiple traumatic events that occurred anytime in the past. and also most patients \nwith PTSD have other mental disorder like Major depressive disorder, anxiety disorder, bipolar disorder, \ntherefore she needs a psychiatrist for better management that includes pharmacotherapy & psychotherapy.”\n“\nOkay, Murakoze cyane (thank you very much) for recognizing her distress and taking her to a quiet place. \nThat was a very important first step. It's understandable that the commemorations can trigger traumatic \nmemories, especially considering the weight of the past. Given her age and low weight, and the context of \nthe commemorations, let’s consider a few things:\nIMMEDIATE ACTIONS:\nSafety and Comfort: Make sure she feels safe and secure. Your presence \nand calm demeanor are already helpful. Offer her water and a comfortable \nplace to sit or lie down.\nGrounding Techniques: Encourage her to focus on her senses. \nYou can ask her:\n\"What are five things you can see?\"\n\"What are four things you can touch?\"\n\"What are three things you can hear?\"\n\"What are two things you can smell?\"\n\"What is one thing you can taste?\"\nBreathing Exercises: Guide her in slow, deep breathing. Encourage her \nto breathe in deeply through her nose, hold it for a few seconds, and \nexhale slowly through her mouth. This can help calm her nervous system.\nADDRESSING POTENTIAL TRAUMA:\nListen Without Judgment: Allow her to share what she’s comfortable \nsharing, without pressure. Do not interrupt or offer unsolicited advice. Just \nlisten attentively and acknowledge her feelings. A simple \"Ndashima ko \nubimbwira (Thank you for telling me)\" can be powerful.\nAvoid Triggering Questions: Be very careful not to ask specific questions \nabout the genocide events. Let her lead the conversation.\nValidate Her Feelings: Tell her it’s okay to feel the way she’s feeling. Her \nreactions are understandable given the circumstances and her past \nexperiences. For example, you could say, \"Birumvikana ko wumva utyo (It's \nunderstandable that you feel that way).\"\nCheck for Physical Needs: Her loud speaking and distress could also be \nrelated to physical needs. Is she hungry, thirsty, or in pain? Given her low \nweight, she might be malnourished, which can exacerbate emotional distress.\nIMPORTANT CONSIDERATIONS FOR RWANDA:\n\"Kwibuka\" (Remembrance): Be aware of the specific cultural \nsensitivities surrounding the genocide commemorations.\n\"Gacaca\" Courts Legacy: The Gacaca courts, while aiming for \nreconciliation, may have also caused further trauma for some. Be mindful \nof this history.\nTraditional Healing: While referring to formal mental health services, also \nbe respectful of traditional healing practices that may be helpful to her.\nResource limitations: Given her low weight, check if she has access to \nfood. If not refer her to a nutritionist and find out whether there are any \noptions for helping her access basic food.\nSafety and fairness: Be careful when talking to the patient to observe \nconfidentiality. Do not do anything to aggravate her situation.\nSELF-CARE FOR YOU:\nSupporting someone experiencing trauma can be emotionally draining. \nRemember to take care of yourself. Talk to a colleague or supervisor \nabout your experience and seek support if needed.\nWhat specific resources do you have access to in your community? \nKnowing that will help me give you more targeted advice.\nThis is a difficult situation, and you are doing a great job by being present \nand supportive. Keep up the compassionate work!”\nFURTHER SUPPORT AND REFERRAL:\nMental Health Services: This is crucial. Rwanda has made efforts to \nprovide mental health services. Find out what resources are available \nin your community. This might include:\n• Health Centers (Ibitaro Nto): Many health centers have trained \nnurses who can provide basic mental health support.\n• District Hospitals (Ibitaro by'Akarere): These hospitals typically \nhave doctors and mental health professionals.\n• Specialized Mental Health Facilities: If possible, referral to a \nspecialized facility might be necessary. Inquire about the closest \noptions.\n• Community-Based Programs: Are there any local organizations \noffering trauma counseling or support groups?\nNutritional Support: Given her low weight, assess her nutritional \nstatus. Is she able to access food regularly? Connect her with \nresources like food distribution programs or agricultural support \ninitiatives, if available.\nSocial Support: Connect her with her family, friends, or community \nmembers who can offer ongoing support. Social isolation can worsen \ntrauma symptoms.\nExplain Confidentiality: Assure her that what she shares with you will \nbe kept confidential, unless there is a risk of harm to herself or others.\n“\nResponse from Clinician\nQuestion from CHW\n“\nResponse from Gemini-2 (LLM)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nA random subset of 524 question-response pairs was selected for expert evaluation (demographics for \nexpert evaluators in Supplementary Table 4). Evaluators assessed each response (generated by clinicians \nand the five LLMs) using an 11-item rubric based on the framework introduced in Google’s Med-PaLM-2 \nevaluation1, employing a five-point Likert scale (see Online Methods). The first 416 questions were \nevaluated based on the prompts and responses in English. The remaining 108 were conducted entirely \nin Kinyarwanda. Any unresolved disagreement between evaluators (defined as a difference >1 between \nscores given by evaluators for the same question-response pair) would lead to affected questions being \nremoved from the final dataset 15 English questions and 3 Kinyarwanda questions were removed from \nthe final dataset on this basis, yielding a final set of 506 questions. The results of the evaluation are \npresented in Figures 1a and 1b. \n \nWhen prompted in English, the top-performing LLMs, Gemini-2 and GPT-4o, achieved average \nperformance scores (out of 5) of 4.56 (SD 0.58) and 4.53 (SD 0.68), respectively. The o3 mini model \nperformed comparably on most metrics (mean 4.49, SD 0.58) but underperformed on 'omission of \nimportant information’ (falling 0.21 and 0.19 points behind Gemini-2 and GPT-4o, respectively). \nDeepseek R1 (mean 4.16, SD 1.06) and Meditron-70B (mean 3.99, SD 0.86) had markedly lower \nperformance. Pairwise comparisons between all models and human clinicians are summarised in \nSupplementary Figure 1. Notably, all LLMs significantly outperformed local clinicians (ps < 0.001) across \nall metrics, with Gemini-2, for example, surpassing local GPs by an average of 0.83 points (range: 0.38 – \n1.10). Evaluators appeared to favour the LLMs’ structured and comprehensive responses over the \nclinicians’ briefer answers. This brevity likely contributed to clinicians scoring well on 'absence of \nirrelevant content' (GPs: mean 4.02, SD 0.99; nurses: mean 3.99, SD 0.99) but poorly on 'omission of \nimportant information' (GPs: mean 3.38, SD 0.91; nurses: 3.28, SD 0.89). \n \nPerformance varied by CHW work package. Clinicians demonstrated the most significant variation \nbetween their best (GPs: Water, Sanitation, and Hygiene: 3.99; nurses: Maternal & Newborn Health: \n4.08) and worst (GPs & nurses: Family Planning, 3.42 & 3.24) topics. In contrast, Gemini-2 exhibited only \na 0.31-point drop from its highest (Mental Health, 4.63) to its lowest (Water, Sanitation, and Hygiene, \n4.32) scoring topics. Frequencies of each topic area within the 506-question subset are provided in \nSupplementary Figure 4; corresponding performance data are in Supplementary Figure 5. \n \nFor the 105 Kinyarwanda-prompted questions, Meditron produced unusable outputs; therefore, it was \nexcluded from this analysis. Of the remaining four models, performance decreased by a mean of 0.15 \npoints across all metrics compared to their English-prompted outputs (Figure 1), yet remained superior \nto clinician responses (ps < 0.001; Supplementary Figure 2). Performance data per LLM, metric, and \nlanguage (including all relevant pairwise comparisons) are provided in Supplementary Figures 1, 2, and \n7. Topic-level performance trends were consistent with the English subset. \n \nClinicians had the option to respond in either English or Kinyarwanda, based on personal preference. \nLanguage preference broadly aligned with professional background; general practitioners preferred \nEnglish (likely due to their language of training), while nurses favoured Kinyarwanda (see Supplementary \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nTable 3). We observed no significant differences in the scores received by clinicians who opted to \nrespond in English versus Kinyarwanda (p = 1.000). \n \nOverall, the study demonstrates that LLMs can provide high-quality, on-demand clinical advice to CHWs \nthat outperforms local experts, even when operating in low-resource, non-English language settings. \nHowever, it is worth remembering that the workflow here (i.e., Q&A) does not fully reflect the complexity \nof day-to-day practice, nor does it guarantee that other human factors (e.g., CHWs not complying with \nthe advice) would not undermine the translation of these results into patient-level benefits if an LLM-\nbased clinical decision support system were to be deployed in this manner. \n \nThe latter result, pertaining to the language in which the LLM was prompted, aligns with prior findings \nshowing performance improvements when prompting LLMs with high-quality English translations instead \nof less-well-represented languages7. However, the additional cost and latency associated with \nintegrating professional linguists or machine translation APIs must be considered. For many use cases, \nthe modest performance reduction related to native language input may be offset by the operational \nadvantages of direct Kinyarwanda interaction. \n \nA cost analysis highlights the economic benefits of LLMs. Clinician-generated answers cost an average of \n$5.43 (general practitioners) or $3.80 (nurses) per question – likely an overestimate due to consulting \npremiums. In contrast, LLM responses cost an average of $0.0035 in English and $0.0044 in \nKinyarwanda, which we found to be a significant increase (p < .001; illustrated in Figure 1c). This was \ndriven by significantly higher (p < .001) token counts per response in Kinyarwanda (mean of 1173) than in \nEnglish (mean of 905), despite similar word counts and semantically identical queries. This is consistent \nwith prior findings that non-English languages use more tokens for equivalent content8, and highlights \nthe need for improved tokenisation methods for less well-represented languages.  \n \nFinally, while human expert evaluations remain the gold standard for research-based LLM assessment in \nhealthcare, our findings highlight their long-term unsustainability as an oversight mechanism for real-\nworld deployments. Even if we use 10% of the evaluator’s costs (who were paid $9.17 per assessment), \nthese costs become impractical at scale. Considering a national-level rollout, for example, Rwanda has \n~60,000 CHWs; even one question every working day from every CHW would cost over $13 million per \nannum to quality assure. This doesn’t even factor in the potential impact of multi-turn interactions. \nFuture research should prioritize the development and validation of automated evaluation strategies9, \nand explore how these can be operationalized for real-time performance monitoring post-deployment. \n \nIn conclusion, these results suggest great promise for LLM-based clinical decision support tools in \nsupporting frontline healthcare workers to deliver a higher standard of care in low-resource settings. \nConfirmation of this potential role for LLMs requires in-field studies, and prospective evaluation of the \nimpacts on healthcare outcomes, which are currently underway10,11. \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nFigure 1: Evaluation scores (and costs) for all models and clinicians on the 11 evaluation criteria when prompted in English & Kinyarwanda  \n \n \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nOnline Methods  \n \nThis study was conducted in four districts across Rwanda: Gicumbi (Byumba Health Centre) and Gakenke \n(Nganzo Health Centre) in the Northern Province, Nyanza (Nyanza Health Centre) in the Southern \nProvince, and Ngoma (Kibungo Health Centre) in the Eastern Province. Below, we outline the process by \nwhich we generated the benchmarking dataset and describe our evaluation of both human and model \nperformance. \n \nDataset Generation  \nDataset generation happened in four phases. First, we recruited Rwandan community health workers \n(CHWs) to generate vignettes that captured representative cases they would encounter in the field. \nSecond, local nurses assessed the quality of those vignettes (and rejected any that failed to meet set \nstandards) and categorised them by health area. Third, local linguists translated approved vignettes from \nKinyarwanda into English. Fourth, and finally, local clinicians generated responses to the vignettes, which \nwere themselves translated to provide bilingual (English & Kinyarwanda) responses. Below is a more \ndetailed explanation of each step. \n \n1. Vignette Generation by CHWs \nTo generate vignettes representative of patients encountered by frontline health workers in Rwanda, we \nrecruited 101 community health workers (CHWs) across four districts (demographic data for CHWs by \ncounty are provided in Supplementary Table 1). CHWs were contacted and recruited based on \nrecommendations from the Rwanda Biomedical Centre (RBC), the para-statal implementation arm of the \nMinistry of Health, which manages the community health programme. Specifically, we relied on input \nfrom the head of the community health programme at RBC, Dr Emery Hezagira (also an author of this \nmanuscript). Participation was voluntary, and CHWs were not paid directly for their time. However, all \ntravel costs to and from training (see below) were covered, and smartphones were provided to all \nparticipating CHWs to support data collection. \n \nOnce recruited, all participating Community Health Workers (CHWs) were invited to a district-specific \ntraining workshop held in December 2024. During these one-day workshops, participants were trained \nto generate vignettes using an adaptation of the ‘Situation, Background, Assessment, and \nRecommendation’ (SBAR12) framework. This involved instructing CHWs to describe how a patient \npresented (situation, e.g., their symptoms, age, gender, and weight), any relevant contextual information \nor clinical history (background, e.g., any relevant pre-existing conditions), their analysis of the situation \nand the options they considered in response (assessment), the actions they took or recommended others \ntake (recommendation), and any questions they have regarding the case for trained clinicians. CHWs \nwere instructed to submit their vignettes via a custom-built mobile application, ‘Mbaza’, developed by \nDigital Umuganda specifically for this project. Vignettes were to be submitted via voice recording, and \nto ensure that recorded vignettes were of a high quality, CHWs were given the following additional \ninstructions: \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \n1. Background noise check: Ensure that you are in a quiet environment with no background noise \nin the audio. \n2. Microphone check: Ensure that your microphone is properly working. \n3. Medical Categorization: Ensure that all questions asked are within the 14 work packages/18 \nclinical domains.  \n4. Language & Terminology Standardization: Ensure that the terminology used in the questions is \nclear, concise, and consistent with local healthcare practices and languages. \n5. Clarity: Make sure that the questions are clear and understandable by both the LLM and the \nGeneral practitioners/nurses who will respond. \n6. Completeness: Ensure the questions are complete and do not lack essential details such as the \npatient's age, gender, and name of the disease/issue. \n \nAll training (including the above instructions) were delivered in Kinyarwanda, and the content provided \nhere has been translated into English for documentation purposes. \n \nFollowing training, all participating Community Health Workers (CHWs) generated vignettes by \nsubmitting voice recordings while working in their communities over a three-week period, with each \nCHW aiming to produce at least 60 vignettes (though many exceeded this target). This yielded 7143 \nvignettes to be assessed and categorised in the next stage (described below). \n \n2. Assessment & Categorisation by Nurses \nSix nurses (three male, three female) were recruited to assess and categorise the 7,143 vignettes \ngenerated by all participating CHWs. To be eligible to participate, nurses needed (i) to have at least 3 \nyears of clinical experience in community health and patient management, (ii) to be bilingual English-\nKinyarwanda speakers, and (iii) basic familiarity with digital devices and applications. Nurses were \nrecruited by a senior doctor based at the Butaro District Hospital, who was previously known to Digital \nUmuganda and leveraged their network to advertise the opportunity (author SU). Interested nurses then \ncompleted an application form, and those who were deemed eligible were recruited to the study. All \nparticipating nurses were paid 697 Rwandan Francs (~0.48 USD) per vignette processed. \n \nOnce recruited, nurses received targeted training sessions (which were delivered by Digital Umuganda \nover 2 days) on how to assess and categorise vignettes, which included practical simulations to ensure \nuniformity in approach. For each vignette, nurses would listen to the original audio recording and review \na machine-translated transcript of the recording (transcripts were generated using the Digital \nUmuganda-maintained Mbaza speech-to-text model, and nurses could correct transcriptions upon \nreview) before assessing the vignette for quality. Nurses were instructed to reject vignettes from \ninclusion in the final dataset if they failed to follow the SBAR tool described above, lacked sufficient \ninformation for a clinician to provide a sound response to the question posed, or if the audio recording \nwas incomplete or inaudible. Nurses were not asked to record the reason for exclusion. A total of 1534 \nvignettes were rejected, leaving 5609 for categorisation. Nurses would then categorise each vignette \nthat passed quality assessment into one/more of 18 medical domains, which align with the 14 ‘work \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \npackages’ that Rwandan CHWs are expected to provide care (including an “Other” option was also \navailable if the vignette included elements that did not fit neatly within the 17 explicit domains). All \n5,609 vignettes were categorised, and the distribution of vignettes per category is shown in \nSupplementary Figure 3. This was all completed within a custom-built annotation platform (which again \nwas developed by Digital Umuganda specifically for this project) and was completed over 3 weeks. \n \n3. Transcription & Translation by Linguists \nTo facilitate the accurate transcription of the 5,609 categorised vignettes, we recruited eight linguists \nwho would work under the supervision of two supervising linguists with previous experience of working \nwith Digital Umuganda on English-Kinyarwanda NLP workflows. The two supervising linguists were \nexisting collaborators of Digital Umuganda, and they shared the opportunity to participate in the study \nwith other linguists in their network. Interested linguists completed an application form, which included \nassessments of their ability to conduct bidirectional English-Kinyarwanda translation. The eight highest-\nscoring applicants (as judged by the supervising linguists) were recruited for the study. Linguists were \npaid 1,629 Rwandan Francs (~$ 1.13 USD) per vignette reviewed. \n \nInitial speech-to-text transcription was performed using Digital Umuganda’s Kinyarwanda speech-to-text \nmodel6. All transcriptions were then reviewed by the eight linguists, who listened to the audio recording \nwhile reviewing the transcribed text and correcting any errors that they found. Supervising linguists then \nreviewed a randomly sampled 10% of all transcripts reviewed by each linguist to ensure quality and \nconsistency. This process was completed within a custom-built web-app developed by Digital \nUmuganda. \n \nVerified transcriptions were initially translated using Digital Umuganda’s machine translation tool (Mbaza \nMT)13. The first 2784 questions were translated using Mbaza MT. It was then determined that GPT4o was \ncapable of effectively translating the text; thus, the remainder of the questions were translated using this \ntool. \n \n GPT-4o was prompted with the following text:  \n“Provide only the direct translation of the text below. Do not include any explanations, \nnotes, or additional context.”. \n \nThis prompt was included because although Gemini-2 anecdotally produced better translations, it was \nincapable (despite prompting) of not responding to the questions alongside the translation, which \nGPT4o (with prompting) was capable of doing.  \n \nTranslations were verified by our team of linguists using the same procedure as for transcription \nverification: they compared the Kinyarwanda transcription with the English translation, listened to the \noriginal audio recording if necessary to resolve any misunderstandings, and corrected any errors. \nSupervising linguists again reviewed a randomly sampled 10% of the translations reviewed by each \nlinguist to ensure quality and consistency. For the original Mbaza MT solution, out of the 2,784 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \ntranslations examined, only 586 were not edited or corrected by the linguists at all. For GPT4o, 2711 \nwere not modified by the linguists, suggesting a much higher quality initial output. Given that every \ntranslation was reviewed and, where necessary, edited by a linguist, we have a strong prior that the tool \nused should not impact the downstream assessment of the responses.  \n \nThis process yielded linguist-verified transcriptions and translations of all 5,609 vignettes, providing a \nfully bilingual dataset of real questions generated by Rwandan community health workers (CHWs). \n \n4. Response Generation by Clinicians \nFinally, to generate responses to the questions posed in the vignettes, we recruited six senior nurses \nwith at least five years of clinical experience and 14 general practitioners (GPs) with 2 to 5 years of \nclinical experience. All nurses were recruited by the same senior doctor, who managed the recruitment \nprocess for vignette assessment and categorization, using the same procedure, but with more stringent \neligibility criteria (i.e., requiring that participating nurses had at least 5 years of clinical experience). \nGeneral practitioners (GPs) were recruited by the Director of Clinical Services at the Butaro District \nHospital (author ER), with all participating GPs working at the same hospital. All senior nurses and GPs \nwere paid 5,488 Rwandan Francs (approximately 3.80 USD) and 7,835 Rwandan Francs (approximately \n5.43 USD), respectively, per vignette answered. \nmFigure 1: The Response and Evaluation Web Application  \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nLegend: The Response and Evaluation Web Portal presented clinicians with the original audio \nrecording of the question alongside the transcription of that recording in a language of their choosing \n(Top). Having listened to and read the question, clinicians would respond by either typing in the text \nbox provided or by recording their speech (which could later be edited). Once happy with their \nresponse, they would click Next, and then be prompted to rate the question along four dimensions \n(Relevance, Clarity, Actionability, and Completeness). The same platform was used for the human \nexpert evaluation described later (Bottom). Note that the people discussed here are illustrative, and \ndo not correspond to real cases/patients. \n \nA dedicated web platform was created by Digital Umuganda to facilitate response generation, as for the \nother activities (see mFigure 1). Clinicians received each vignette and question in both audio and text \nformats, with the text format available in both Kinyarwanda and English. Upon accessing the question, \nclinicians first selected their preferred language for responding (responses could be given in either \nEnglish or Kinyarwanda). Once selected, the audio recording in Kinyarwanda could be played, and the \ncorresponding text was displayed in their chosen language. See Supplementary Table 3 for the \ndistribution of preferred clinical language. \n \nClinicians were encouraged to listen to the original voice recording before responding to it. To ensure \nthey understood the vignette, clinicians were asked to summarise the question posed in their own words \nbefore providing a detailed response. To facilitate ease of use, the platform included a speech-to-text \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nmodel, allowing clinicians to dictate their responses, which would then be automatically transcribed (and \nclinicians could then edit). Finally, once submitted, each response was translated into the alternate \nlanguage (again using GPT-4o for translation). This process yielded a fully bilingual set of 5,422 \nquestion-answer pairs – see mTable 1 for a summary of the distribution of clinician types (i.e., GP or \nnurse) across the question categories. Responses were not generated for the complete set of 5,609 \ncategorised questions because a target of 5,000 questions was set, and clinicians were instructed to stop \nonce that target had been reached (although there was some delay, hence the increased number of \n5,422). \n \nmTable 1 – Percentage of questions answered by GPs and Nurses for each category \nCategory \n% Answered \nby GPs \n& Answered \nby Nurses Total \nAdolescent Sexual and Reproductive Health (ASRH) 61.8% 38.2% 34 \nBehaviour Change Communication (BCC) 63.3% 36.7% 49 \nCommunity-Based Provision Family Planning (CBP/FP) 70.7% 29.3% 41 \nDrug Management 72.1% 27.9% 68 \nEarly Childhood Development (ECD) 62.3% 37.7% 53 \nEmergency Response 57.7% 42.3% 26 \nFirst Aid 60.0% 40.0% 35 \nGender Based Violence (GBV) 66.7% 33.3% 33 \nHIV 69.4% 30.6% 36 \nIntegrated Community Case Management (ICCM) 74.5% 25.5% 51 \nMalaria 71.6% 28.4% 95 \nMaternal & Newborn Health 61.5% 38.5% 52 \nMental Health 78.0% 22.0% 41 \nNon-Communicable Diseases (NCDs) 60.0% 40.0% 35 \nNutrition 60.8% 39.2% 51 \nTuberculosis 69.4% 30.6% 36 \nWater, Sanitation, and Hygiene (WASH) 57.5% 42.5% 40 \nOther 76.2% 23.8% 101 \n \n \nAfter providing their responses, clinicians were prompted to rate the quality of each vignette (on a 5-\npoint Likert scale) for four criteria: \n1. Relevance: how directly the question relates to the patient’s specific condition or the public \nhealth concern. Highly relevant questions enable clinicians to quickly grasp context and respond \naccurately. \n2. Clarity: how easily the question is understood by healthcare providers. \n3. Actionability: whether the question can clearly lead to practical steps in patient care. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \n4. Completeness: whether the question captures all essential information needed for clinicians to \nfully address the medical issue. \n \nThis rating step was included to capture additional insights regarding the quality and utility of the \nquestions received from CHWs. We found that 90% of all vignettes received ratings of 3 or higher across \nall dimensions. This indicates that most questions submitted by CHWs were both relevant and \nadequately comprehensive in terms of providing the necessary information for clinicians to respond. \n \nmFigure 5 – Histogram of the number of questions achieving each score for each of the four rating \ncategories. \n \n \nLLM Response Generation  \nWe generated responses to all 5422 vignettes in the final dataset from five LLMs: Gemini-2-Flash, GPT-\n4o, o3 mini, Deepseek R1, and Meditron-70B. For all but Meditron-70B, responses were generated \nusing the relevant APIs by supplying the models with each vignette and the prompt shown in mBox 1. \nSince Meditron-70B is an open-source model, Digital Umuganda hosted the model in a private cloud \ninstance (utilising two A100 GPUs within Google Cloud Platform), while maintaining the same prompting \nstrategy. \n \nmBox 1: LLM Instruction Prompt \n \nRole & Context  \nYou are a knowledgeable doctor,supporting Community Health Workers (CHWs) in Rwanda. You focus \non providing accurate health advice, practical tips, and emotional support. You answer in the same \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nlanguage the question is asked (e.g., Kinyarwanda, English). You adapt to local conditions, respect \ncultural practices, and draw on global best practices. If uncertain, suggest ways to check information \nrather than giving incorrect details. \n \nInstructions  \nOffer reliable medical guidance in a concise way. \nInclude local Rwandan health guidelines where possible. \nIf needed, give motivational or emotional support to CHWs. \nConsider resource-limited settings and propose creative solutions. \nEmphasize safety, fairness, confidentiality, and kindness. \n \nAll models, except Meditron, were prompted natively in English and Kinyarwanda, i.e., both the original \nKinyarwanda transcription and the English transcription of each vignette were provided to solicit one \nresponse in each language from each model.  \n \nThe cost of each response generated by each model was measured by applying relevant tokenisers \n(o200k_base for GPT-4o and o3 mini; LlamaTokenizerFast for Deepseek R1 and Meditron-70B; and \nGemma’s SentencePiece-based tokeniser for Gemini-2) to tokenise all vignettes supplied as prompts \nand all responses received from each model. The input and output token counts were then combined \nwith the per-token costs for each model to calculate the inference cost for each question-answer pair. \n \nComparing Human and Model Performance  \nTo compare the responses generated by local clinicians with those generated by the five LLMs included \nin this study, we recruited a panel of local experts to evaluate response quality, and then we analysed \ndifferences in how human clinicians and each of our models performed. \n \n1. Human Evaluation \nSix clinicians were recruited to evaluate a set of 506 question-answer pairs. Clinicians were recruited by \nthe Director of Clinical Services at Butaro District Hospital (Author ER), specifically targeting senior \ndoctors with at least three years of clinical experience. They were paid 13,235 Rwandan Francs (~$ 9.17 \nUSD) per question-answer pair evaluated. Question-answer pairs were sampled randomly to select 416 \ncases that would be evaluated in English (i.e., the vignette and the human/model-generated response \nwould be presented in English), and 108 that would be evaluated in Kinyarwanda (i.e., the vignette and \nthe human/model-generated response would be presented in Kinyarwanda). Since question-answer pairs \nwere sampled at random, some of the 416 cases that would be evaluated in English were originally \nresponded to by human clinicians in Kinyarwanda, with those responses later machine-translated into \nEnglish (and vice versa for the 108 cases evaluated in Kinyarwanda) – this was accounted for in our \nanalysis (see below). \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nEvaluating clinicians used an adaptation of the Med-PaLM-2 evaluation framework1 to evaluate each \nquestion-answer pair. The full evaluation framework used can be found in the ‘Supplementary Evaluation \nFramework Description’, but in brief, clinicians rated each response on 11 dimensions:   \n1. Alignment with Medical Consensus: Does the response align with established medical \nguidelines, evidence-based practices, and expert consensus? \n2. Question Comprehension: Does the response accurately understand and address the question \nasked? \n3. Knowledge Recall: Is the information provided accurate, relevant, and reflective of an expert-\nlevel knowledge base? \n4. Logical Reasoning: Is the response logically structured, with a clear and coherent rational \nprogression of ideas? \n5. Inclusion of Irrelevant Content: Does the response include unnecessary or unrelated information \nthat could distract from the question at hand? \n6. Omission of Important Information: Does the response omit any critical information that would \ncompromise its quality, accuracy, or safety? \n7. Possible Extent of Harm: If the user were to follow this response, how severe could the potential \nharm be (e.g., misdiagnosis, incorrect treatment, or unsafe advice)? \n8. Possible Likelihood of Harm: How likely is it that the response could lead to harm if followed? \n9. Clear Communication: Is the response presented in a clear, professional, and understandable \nmanner? Is the structure and tone appropriate for the intended audience? \n10. Understanding of Local Context: Does the response take into account regional, cultural, and \nresource-specific factors relevant to the local setting in Rwanda? \n11. Potential for Demographic Bias: To what extent does the response avoid bias based on \ndemographic factors such as age, gender, race, ethnicity, or socioeconomic status? \n \nThe evaluation itself was conducted by dividing the clinicians into two groups of three clinicians each, \nwith one clinician in each group designated as a supervisor and the other two as evaluators. Each group \nassessed 262 question-answer pairs (i.e., half of the full sample). Initially, evaluators would independently \nevaluate six responses (i.e., the five models and one human response) generated for the same question. \nThey would then convene to discuss their evaluations and resolve any disagreements in their scoring \n(disagreement defined as a difference of >1 on the 5-point Likert scale for each dimension). This process \nyielded two sets of independent scores for each response generated for each vignette, with each pair of \nper-dimension scores being within one Likert-point of one another. Disagreement could not be resolved \nfor 15 English question-answer pairs and 3 Kinyarwanda question-answer pairs; these cases were \nremoved from the dataset and all subsequent analyses.  \n \n2. Statistical Analysis \nThe primary question we sought to answer was whether and how the quality of responses depended on \nthe source, specifically, how humans and the five models compared, as evaluated by expert human \nclinicians. We also sought to understand whether the profession of the responding clinicians (i.e., \nwhether they were senior nurses or junior GPs), the language used to respond (i.e., whether nurses/GPs \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nresponded in English or Kinyarwanda), or the language used to evaluate (i.e., whether evaluating \nclinicians evaluated English or Kinyarwanda question-answer pairs) had any effect on performance. \n \nTo answer these questions, we conducted an Aligned Rank Transform (ART) ANOVA14, which is a non-\nparametric factorial procedure that “aligns” the data by subtracting estimated effects for each term, then \nranks the aligned values so that a standard ANOVA performed on those ranks yields valid tests of main \neffects and interactions without assuming normality or interval scaling. Because alignment isolates each \neffect before ranking, the method maintains Type I error control and statistical power comparable to that \nof parametric ANOVA, even with small samples or skewed, ordinal outcomes (such as the Likert-scale \nrating used in our evaluation framework). \n \nWe tested a fully saturated statistical model, which included main effects for evaluation dimension (i.e., \nwhich of the 11 evaluation dimensions an individual score pertained to), responder (i.e., which of junior \nGP, senior nurse, GPT-4o, o3-mini-high, Gemini-2-Flash, Meditron-70B, or Deepseek-R1 generated the \nresponse under evaluation), and evaluation language (i.e., the language of the question-answer pair \npresented to evaluating clinicians), along with all interaction terms. \n \nWe then conducted pairwise comparisons between the levels of all significant main effects and \ninteractions on the aligned-ranked marginal means with Tukey’s honest significance difference (HSD) \ntest. Because the ART procedure isolates each factorial contrast before ranking, the aligned ranks meet \nthe independence and equal-variance requirements of HSD, allowing us to control the family-wise error \nrate. This approach yields adjusted p-values for all pairwise contrasts within each significant main effect \nor interaction, providing a rigorous yet interpretable basis for reporting differences we found among \nevaluation dimensions, responders, and languages.  \n \nFinally, we analyzed differences in the cost of generating responses in English versus Kinyarwanda for \nthe three models that could do so (excluding Meditron-70B and Deepseek, as they were unable to \noperate natively in Kinyarwanda). For all API-accessed models, the cost of each response was computed \nby tokenizing the input and output text with the appropriate tokenizers and then applying the \ninput/output token costs to the resulting number of tokens. Costs in cents per million tokens, for input \nand output respectively, were: GPT4o (2.5, 10), Gemini-2-Flash (0.5, 2), o3 Mini-High (1.1, 4.4), and \nDeepSeek R1 (0.55, 2.19). For humans, the cost of each response was the amount paid to the junior GP \nor senior nurse who generated it. Costs were analyzed using a standard two-way ANOVA, which \nincluded main effects for model and language, as well as the interaction between them, all of which were \nsignificant. In brief, we hypothesized that generating responses in Kinyarwanda would be more \nexpensive due to larger token counts for the same semantic content (when compared with English \ntranslations of the same content). \n \nThe complete set of analytical code is archived in a fully open repository, available at: \nhttps://github.com/PATH-AI-Initiative/RwandaBenchmarking\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nReferences  \n \n[1] Singhal K, et al. Nature Medicine. 2025, Jan 8:1-8. \n \n[2] Goh E, et al. Nature Medicine. 2025, Feb 5:1-6. \n \n[3] Idriss-Wheeler D, et al. PLOS global public health. 2024, Jan 18;4(1):e0002799. \n \n[4] Olatunji T, et al. Available at: https://github.com/intron-innovation/AfriMed-QA. Accessed on 24 May \n2025. \n \n[5] Olatunji T, et al. arXiv preprint arXiv:2411.15640. 2024, Nov 23. \n \n[6] Elamin M, Chanie Y, Ewuzie P, Rutunda S.. In 4th Workshop on African Natural Language Processing \n2023 Apr. \n \n[7] Alhanai T, et al. In Proceedings of the AAAI Conference on Artificial Intelligence 2025, Apr 11. \n \n[8] Ahia O, et al. arXiv preprint arXiv:2305.13707. 2023, May 23. \n \n[9] Johri S, et al. Nature Medicine. 2025, Jan 2:1-0. \n \n[10] Menon V, et al. Available at: 10.5281/zenodo.15493572 \n \n[11] Mateen BA, Nature Medicine. 2025, [In Press]. \n \n[12] Joint Commission, 2008. available at \nhttps://www.jointcommission.org/at_home_with_the_joint_commission/sbar_%E2%80%93_a_powerful_t\nool_to_help_improve_communication/. Accessed 24 May 2025. \n \n[13] Digital Umuganda. Hugging Face. 2024. Available at: \nhttps://huggingface.co/DigitalUmuganda/Quantized_Mbaza_MT_v1 \n \n[14] Wobbrock, J.O., Findlater, L., Gergle, D. and Higgins, J.J., In Proceedings of the SIGCHI conference \non human factors in computing systems. 2011.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nEthics Approval \nThe study was deemed exempt from review by the Rwanda National Ethics Committee. PATH’s research \ndetermination committee also reviewed the scope and confirmed it was not human subjects research \nsubject to IRB approval. \n \nData Availability Statement  \nThe subset of 524 questions, answers, and individual evaluation results that comprise this benchmarking \nstudy is available upon reasonable request and will otherwise be made available upon publication of this \nwork in a peer-reviewed journal. The full dataset has been donated to the Rwanda Biomedical Centre \n(RBC), the parastatal delivery arm of the Rwandan Ministry of Health, and is hosted in a secure data \nenvironment. It will be made available to researchers on request and based on an assessment of ‘fair \nvalue exchange’ by stakeholders, to ensure that the indigenous population that generated the \ninformation benefits from its exploitation. This arrangement was specifically designed to ensure \nadherence to the CARE principles. The Centre for the Fourth Industrial Revolution, as the innovation lab \nfor the Rwandan Government, serves as the primary point of contact for researchers seeking to access \nthis data. Prospective users should contact ‘info@c4ir.rw’ to request access. \n \nPatient and Public Involvement Statement  \nPatients were not directly involved in this study. \n \nAuthor Contributions Statement  \nBAM conceptualized the study, and secured funding for it. CN and MEF managed the project. SR, GW, \nCN, MEF, VM, XL, AD, and BAM developed the methodology. EH, SR, KK, FN, SU, ER and other \nmembers of the broader Digital Umuganda team (acknowledged) developed the tools, recruited \nrelevant participants, for the study and collected the data. SR, GW, and members of the broader Digital \nUmuganda team (acknowledged) performed the data analysis. SR, MEF, GW, and BAM drafted the \noriginal manuscript. All authors contributed to review, editing and approval of the final manuscript. \n \nCompeting Interests Statement  \nThe authors declare no competing interests.  \n \nFunding Statement  \nThis research was supported by the Gates Foundation (INV-068056). The funders had no role in the \nstudy design, data collection and analysis, the decision to publish, or the preparation of the manuscript.  \n \nAcknowledgments  \nWe thank the Ministry of Health and the Ministry of ICT for their support, as well as the numerous \ncommunity health workers (CHWs) and clinicians who participated in the study. Additionally, we’d like to \nacknowledge the efforts of the broader Digital Umuganda team (Boris Ishimwe Mugisha, Michel \nMivumbi Patrick, Saad Byiringiro, Celestin Niyindagiriye, Cedric Mugisha, Ali Nengo, Emmanuel \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint \nIgirimbabazi, Gilbert Nzabonimpa), as well as the C4IR leadership (Crystal Rugege & Alain Ndayishimiye) \nfor their contributions in operationalizing and undertaking this study. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted August 28, 2025. ; https://doi.org/10.1101/2025.08.26.25333975doi: medRxiv preprint ",
  "topic": "Rubric",
  "concepts": [
    {
      "name": "Rubric",
      "score": 0.8118910789489746
    },
    {
      "name": "Benchmarking",
      "score": 0.687404215335846
    },
    {
      "name": "Likert scale",
      "score": 0.6030632853507996
    },
    {
      "name": "Metric (unit)",
      "score": 0.5659646391868591
    },
    {
      "name": "Health care",
      "score": 0.5383250713348389
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.44096386432647705
    },
    {
      "name": "Medicine",
      "score": 0.40917399525642395
    },
    {
      "name": "Psychology",
      "score": 0.3598668575286865
    },
    {
      "name": "Computer science",
      "score": 0.2825821042060852
    },
    {
      "name": "Business",
      "score": 0.22166875004768372
    },
    {
      "name": "Marketing",
      "score": 0.13157117366790771
    },
    {
      "name": "Economics",
      "score": 0.11914956569671631
    },
    {
      "name": "Economic growth",
      "score": 0.11324471235275269
    },
    {
      "name": "Developmental psychology",
      "score": 0.11174291372299194
    },
    {
      "name": "Mathematics education",
      "score": 0.09900465607643127
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210161281",
      "name": "University Teaching Hospital of Butare",
      "country": "RW"
    },
    {
      "id": "https://openalex.org/I79619799",
      "name": "University of Birmingham",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210148345",
      "name": "Rwanda Biomedical Center",
      "country": "RW"
    }
  ]
}