{
  "title": "Generating Better Items for Cognitive Assessments Using Large Language Models",
  "url": "https://openalex.org/W4385571498",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4320548982",
      "name": "Antonio Laverghetta Jr.",
      "affiliations": [
        "University of South Florida"
      ]
    },
    {
      "id": "https://openalex.org/A343778178",
      "name": "John Licato",
      "affiliations": [
        "University of South Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4220685986",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2901906577",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3115314063",
    "https://openalex.org/W2024509488",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3084223432",
    "https://openalex.org/W650350307",
    "https://openalex.org/W2055445475",
    "https://openalex.org/W4310993626",
    "https://openalex.org/W3096869521",
    "https://openalex.org/W4308885736",
    "https://openalex.org/W1555938369",
    "https://openalex.org/W2904197112",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4321004017",
    "https://openalex.org/W2791246286",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W4382502700",
    "https://openalex.org/W2757978590",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2064889271",
    "https://openalex.org/W4287854553",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W2102925141",
    "https://openalex.org/W4287854734",
    "https://openalex.org/W2067745708",
    "https://openalex.org/W3173736278",
    "https://openalex.org/W3098467034",
    "https://openalex.org/W4286561208",
    "https://openalex.org/W3184970977",
    "https://openalex.org/W2005364767",
    "https://openalex.org/W2061935394",
    "https://openalex.org/W4295942986",
    "https://openalex.org/W2989682635",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1985936397",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W3000065748",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385573244",
    "https://openalex.org/W3186095502"
  ],
  "abstract": "Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.",
  "full_text": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 414–428\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\nGenerating Better Items for Cognitive Assessments Using Large Language\nModels\nAntonio Laverghetta Jr.and John Licato\nUniversity of South Florida\nDepartment of Computer Science and Engineering\nTampa, FL, USA\n{alaverghett,licato}@usf.edu\nAbstract\nWriting high-quality test questions (items) is\ncritical to building educational measures but\nhas traditionally also been a time-consuming\nprocess. One promising avenue for alleviat-\ning this is automated item generation, whereby\nmethods from artificial intelligence (AI) are\nused to generate new items with minimal hu-\nman intervention. Researchers have explored\nusing large language models (LLMs) to gen-\nerate new items with equivalent psychomet-\nric properties to human-written ones. But can\nLLMs generate items withimproved psychome-\ntric properties, even when existing items have\npoor validity evidence? We investigate this us-\ning items from a natural language inference\n(NLI) dataset. We develop a novel prompting\nstrategy based on selecting items with both the\nbest and worst properties to use in the prompt\nand use GPT-3 to generate new NLI items. We\nfind that the GPT-3 items show improved psy-\nchometric properties in many cases, whilst also\npossessing good content, convergent and dis-\ncriminant validity evidence. Collectively, our\nresults demonstrate the potential of employing\nLLMs to ease the item development process\nand suggest that the careful use of prompting\nmay allow for iterative improvement of item\nquality.\n1 Introduction\nAI is having increasingly profound impacts on ed-\nucational and psychological measurement (Chen\net al., 2020; Tavast et al., 2022). Technologies built\non AI and machine learning, including educational\ndata mining (Romero and Ventura, 2020), intelli-\ngent tutoring systems (Mousavinasab et al., 2021),\ndeep item response theory (Cheng et al., 2019), and\ndeep knowledge tracing (Piech et al., 2015), among\nothers (Asfahani, 2022, inter-alia) are transforming\neducational and psychological measurement, and\nthis trend seems likely to continue.\nOne promising educational application of large\nlanguage models (LLMs) is for the automatic gen-\neration of test items (AIG). Writing high-quality\ntest items is critical to building effective educa-\ntional assessments, but has also traditionally been\na time-consuming process, as items must be devel-\noped by experts and undergo numerous rounds of\nreview (Bandalos, 2018). There has been signifi-\ncant research interest in using AIG to create high-\nquality items with minimal intervention to speed\nup the test development process (Prasetyo et al.,\n2020). Prior work has demonstrated that LLMs can\ngenerate items with at least face validity (i.e, they\nappear valid based on item content) for both non-\ncognitive (Götz et al., 2023) and cognitive (Attali\net al., 2022) constructs. Careful psychometric anal-\nysis of items generated from such models has also\nrevealed that they are just as valid and reliable as\ntheir human written counterparts (Lee et al., 2023).\nAlthough promising, this research has largely fo-\ncused on generating items for constructs that have\nbeen well-studied, using items already known to\nhave strong validity evidence. Suppose an educator\nwishes to develop a test for a new construct where\nexisting items may have only undergone pretest-\ning. Or suppose the educator wishes to use a new\ntype of item for a well-established domain (e.g, a\ntest of algebraic reasoning that uses a novel item\nformat). In either case, the items will likely have\nlimited validity evidence, and much time would\nneed to be spent revising the items to improve their\npsychometric properties before they can be used.\nIn this work, we ask: can LLMs be used to gener-\nate valid and reliable items even in these scenarios\nwhere existing items have only limited validity ev-\nidence? If so, LLM-based AIG could be used to\niteratively improve the psychometric properties of\nitems, explore the underlying construct space, and\nshed light on what makes a good item.\nWe explore this using GPT-3 (Brown et al., 2020)\nand focus on generating items that test for natural\nlanguage inference (NLI) (Dagan et al., 2006; Bow-\nman et al., 2015). NLI is an important cognitive\n414\nconstruct in NLP research which, to our knowledge,\nhas only undergone limited psychometric analysis\nin human participants (Laverghetta Jr. et al., 2021).\nWe develop a novel prompting strategy that uses\nthe psychometric properties of items, calculated\nusing prior human responses, to select the most\ninformative examples to send to the model to maxi-\nmize the quality of the generated examples. Our\nmain contributions are as follows:\n1. We develop a novel prompting strategy for\ngenerating items by selecting items to include\nas context based on the psychometric proper-\nties they possess, focusing primarily on item\ndiscrimination.\n2. Using GPT-3 we test our approach using the\nGLUE broad coverage diagnostic (Wang et al.,\n2018), a popular cognitive task in NLP re-\nsearch. We perform an extensive analysis of\nthe psychometric properties of the generated\nitems and find that those from GPT-3 show\nstronger evidence for validity and reliability\nthan those written by humans in most cases.\n2 Related Work\n2.1 Automated Item Generation\nPsychometricians have explored how to automate\nitem generation for decades (Prasetyo et al., 2020).\nEarly attempts focused on developing item models,\nwhich are systems that can interchange certain key-\nwords in the item while keeping other parts of it\nconstant (Bejar et al., 2002). While item models\nare theoretically justified and very likely to produce\npsychometrically valid items, developing them re-\nquires a great deal of manual effort, as both the\nitem stem and other components must still be man-\nually written. Furthermore, item models are limited\nin the diversity of content they can generate. These\ndrawbacks have motivated recent work to investi-\ngate using LLMs as the item generator. von Davier\n(2018) was one of the first to explore this and used\nrecurrent neural networks to generate items for a\npersonality assessment. The advent of the trans-\nformer (Vaswani et al., 2017; Devlin et al., 2019;\nBrown et al., 2020) led to the creation of LLMs\nwhich could generate much more coherent and se-\nmantically accurate text, leading to further interest\nin LLM-based AIG. Götz et al. (2023) generated\na large number of personality items using GPT-2\n(Radford et al., 2019), and showed that at least\nsome of these items passed face validity checks.\nMaertens et al. (2021) developed a test for mis-\ninformation susceptibility, using LLM-generated\nitems. Hernandez and Nie (2022) developed a sys-\ntem for the automatic generation and validation of\ntest items, using autoregressive LLMs for genera-\ntion and autoencoding LLMs for validation. Lee\net al. (2023) extensively evaluated the psychomet-\nric properties of GPT-3 generated personality items,\nincluding analysis of internal structure, differential\nitem functioning, and reliability. They concluded\nthat the validity evidence for machine-generated\nitems was just as strong, if not stronger than, for\nhuman-written ones. While much work has fo-\ncused on non-cognitive assessments, others have\nexplored LLM-based AIG for educational assess-\nments. Notably, Chan et al. (2022) used the BERT\n(Devlin et al., 2019) LLM to generate grammar\nreading exercises. Zou et al. (2022) and Rathod\net al. (2022) used transformers to generate true/-\nfalse and reading comprehension questions. At-\ntali et al. (2022) used transformer-based LLMs to\ngenerate items for the Duolingo English Test. Zu\net al. (2023) used a combination of finetuning and\nprompt-based learning to train GPT-2 to generate\ndistractors for fill-in-the-blank vocabulary items.\nA common theme throughout these works is the\nfocus on well-studied assessments, and the use of\nitems that have already been psychometrically vali-\ndated in the prompt. Their goal is thus to generate\nitems that maintain existing psychometric proper-\nties, which is different from our goal of generating\nitems with improved properties.\n2.2 Synthetic Data Generation in NLP\nWhen it comes to gathering high-quality data, NLP\nresearchers have concerns that overlap with those\nfaced by the measurement community. Training ex-\namples for popular NLP tasks, including NLI (Bow-\nman et al., 2015), and question answering (QA)\n(Rajpurkar et al., 2016), have historically been cre-\nated using crowd-sourced annotations, which is\nboth expensive and time-consuming. The incred-\nibly rapid progress of LLMs in recent years also\nmeans that many once challenging datasets quickly\nbecome outdated as new models are developed\n(Ott et al., 2022). There has been significant re-\nsearch interest in using LLMs to generate synthetic\ntraining data, forgoing the need to run annotation\nstudies (Schick and Schütze, 2021). Prior work\nhas explored LLM-based data augmentation for\nQA (Duan et al., 2017), paraphrase identification\n415\n(Nighojkar and Licato, 2021), and NLI (Liu et al.,\n2022). Typically, this line of research relies on\ninformation-theoretic metrics of item quality, for\nexample, dataset maps (Swayamdipta et al., 2020)\nto evaluate the newly generated items. Most rele-\nvant to our work is the study by Liu et al. (2022),\nwho developed a system for using GPT-3 to au-\ntomatically generate NLI items. However, their\napproach does not employ methods of assessing va-\nlidity and reliability commonly used in educational\nmeasurement and instead relies on information-\ntheoretic measures of item quality. Our goal is to\ngenerate items with improved validity and reliabil-\nity in both human and LLM populations, using the\npsychometric properties of the items as the opti-\nmization target.\n3 Generation of Test Items\nThe General Language Understanding Evaluation\n(GLUE) (Wang et al., 2018) is a benchmark de-\nsigned to measure broad linguistic constructs in\nLLMs. Included in GLUE is a diagnostic set,AX,1\nwhich is meant to be a challenge set for diagnosing\nfaults in LLMs. Items on AX are framed as NLI:\ngiven a premise ( p) and hypothesis ( h), a model\nmust determine whether pentails, contradicts, or\nis neutral with respect to h (Dagan et al., 2006;\nBowman et al., 2015). Items were written by NLP\nexperts, inspired by categories taken from the Fra-\nCas suite (Cooper et al., 1996), and are based on\nsentences from a variety of artificial and naturalistic\ncontexts. Wang et al. (2018) reported strong inter-\nrater reliability when labeling a random sample\nof AX items, and AX has been used successfully\nto evaluate many new LLMs (Brown et al., 2020;\nRaffel et al., 2020; Chowdhery et al., 2022), which\nsuggests the diagnostic has good predictive validity.\nFurthermore, Laverghetta Jr. et al. (2021) previ-\nously ran human studies on a subset of items from\nAX, targeting those testing for propositional struc-\nture (PS), quantifiers (Q), morphological negation\n(MN), and lexical entailment (LE). Table 1 shows\nexample AX items from these categories. They\nfound that LLMs strongly predicted item difficul-\nties and inter-item correlations in human responses\nacross these categories, indicating good convergent\nvalidity for AX as a test of reasoning in both pop-\nulations. Collectively, these results demonstrate\na surface level of validity for the AX items (i.e,\n1AX being the notation for the diagnostic on the GLUE\nleaderboard.\nCategory p h\nPS The cat sat on the mat. The cat did not sit\non the mat.\nLE The water is too hot. The water is\ntoo cold.\nMN The new console\nis cheap.\nThe new console\nisn’t cheap.\nQ Several are available. All are available.\nTable 1: Examples of NLI items from eachAXcategory.\nMN and Q items have been trimmed and paraphrased\nto fit in one line, but still fall into their respective cate-\ngories.\nface validity); the items appear to function well in\npreliminary human studies and have been used suc-\ncessfully to find faults within LLM reasoning, but\nextensive analysis of their psychometric properties\nhas yet to be performed. This makes AX a good\nassessment to use for our experiments, as we want\nitems that have not undergone extensive psychome-\ntric development, and hence may not have strong\nvalidity as measures of the construct in question.\nOur goal is to use LLMs to generate new items\nfor AX, such that the psychometric properties of\nboth the items and the test as a whole are improved.\nFormally, given an LLM M and a prompt pthat\ncontains one or more items that have a psychomet-\nric property θ, we seek to sample new items ifrom\nM that lead to an improvement in θ:2\ni∼M(p) |θi >θp (1)\nWhere i and p are assumed to test for the\nsame construct (e.g., NLI). Prior work has demon-\nstrated that when LLMs are given existing items as\nprompts, they can generate new items that match\nthe construct measured by those items (Liu et al.,\n2022; Lee et al., 2023). We build on this approach\nby designing prompts to instruct LLMs to generate\nnew items for a particular construct, that possess\na desired psychometric property. Figure 1 shows\none of the prompts we developed. The model is\ninstructed to generate only items that match the\ntarget property, and we use items from only one\ncategory at a time. We use item discrimination as\nthe target property in our experiments. Discrimina-\ntion refers to the ability of an item to separate high\nfrom low-ability test takers (Bandalos, 2018) and\nis computed using the item-to-total correlation (the\ncorrelation between the responses to a single item\nand total scores across all items). An item that is\n2Note that θi >θp should be taken to mean that the psy-\nchometric properties of iare improved relative to p, and not\nnecessarily that they are numerically greater.\n416\nI need to generate new NLI\nitems for a given trait.\nHere are some examples:\n###\nTrait: High Discrimination\nItems (3):\n[ITEMS]\n###\nTrait: Low Discrimination\nItems (3):\n[ITEMS]\n###\nTrait: High Discrimination\nNew Items (5):\nFigure 1: Prompt structure using the “simple” prompt\nformat. Additional newlines have been added to keep\ntext within margins.\nhighly discriminating will predict total scores and\nthus should be maximized. Our use of discrimina-\ntion was based on preliminary analysis of the data\nfrom Laverghetta Jr. et al. (2021), which indicated\nthat at least one item in every category had nega-\ntive discrimination. In general, items with negative\ndiscrimination are regarded as problematic and pos-\nsibly erroneous, and should not be included in cog-\nnitive assessments (Bandalos, 2018), which makes\nimproving the discrimination of the AX items a\nnatural optimization target. We use existing human\nwritten items as examples of the desired property\nin the prompt, selecting the top k items with the\nhighest discrimination as “high discrimination” and\nthe bottom kitems with the lowest discrimination\nas “low discrimination”. 3 We set k = 3 in our\nexperiments, as we found larger values caused the\ndifference in discrimination to become negligible.\nBy providing examples of both good and bad items,\nwe hope to teach the model general characteristics\nof high-quality items.4\nWe use GPT-3 (Brown et al., 2020) as our item\ngenerator, given its strong performance across\nmany NLP tasks, the presence of an easy-to-use\nand inexpensive API, and the success prior work\nhas had in using GPT-3 to generate non-cognitive\n(Lee et al., 2023) and NLI (Liu et al., 2022) items.\n3Properties are calculated using SPSS version 28. We use\nonly the categories from Table 1.\n4Note that our approach has strong conceptual similarities\nto prior work in few-shot item selection for in-context learning\n(e.g. Walsh et al., 2022), in that the psychometric properties\nof the items are essentially used to select which shots to use.\nWe set temperature to 1 for all experiments, to en-\ncourage diversity in the generated items, and use a\nmaximum token limit of 300. We explore the effect\nof varying other key hyperparameters:\n• Top P:This parameter is based on nucleus\nsampling (Holtzman et al., 2019) and deter-\nmines what fraction of log probabilities to\nconsider when sampling, with larger values\nallowing more unlikely completions to be sam-\npled. Prior work in LLM-based AIG has dif-\nfered on this setting; some have used a value\nabove 0.5 (Lee et al., 2023) and others a value\nat or below 0.5 (Liu et al., 2022). We there-\nfore choose to experiment with both 0.5 and\n1, as we theorized setting a higher value could\nlead to more diverse generations, but also in-\ncrease the risk the items would lack construct\nvalidity.\n• Prompt Type:We use a “simple” prompt fol-\nlowing the structure shown in Figure 1. How-\never, because the AX categories are highly\nspecific, we reasoned that providing addi-\ntional context about the categories may im-\nprove generation accuracy. We thus also ex-\nperiment with “elaborated” prompts, which\ninclude additional information about each cat-\negory, taken from the appendix on AX.5\nWe left all other hyperparamters at their defaults.\nWe use the text-davinci-003 endpoint,6 and\nqueried the API in December 2022. We generate\n400 items, 100 for each category, and 25 for each\nhyperparameter combination (prompt type and top\np). We remove any duplicate items, items where\nthe model did not generate a valid label, and items\nthat match verbatim an item from AX.\nFollowing best practices in scale development\n(Worthington and Whittaker, 2006) we conduct a\ncontent review on the generated items. Four Ph.D.\nstudents with prior publications in NLP, NLI, or\npsychometric AI were asked to rate the quality of\nthe GPT-3 items. We ask our annotators to rate the\nrelevance of the items for measuring the category,\nthe clarity of the items (in terms of whether they\nhave spelling or grammatical errors), whether the\nitems have potentially harmful content, and their\n5https://gluebenchmark.com/diagnostics\n6Prompts and generated items for repro-\nducing our results are available on Github:\nhttps://github.com/Advancing-Machine-Human-Reasoning-\nLab/gpt3-item-generation/tree/main\n417\ncertainty in their annotations. Before beginning\nthe study, we gave annotators detailed instructions\nthey were asked to review in advance, including\ninformation about the AX categories, how to an-\nswer each of the ratings, and example ratings. We\ninstructed annotators to rate items as “Completely\nirrelevant” if either the label was incorrect or the\nitem did not match the target category. We followed\nstandard practices in NLI research for determining\nwhat the correct label should be (Bowman et al.,\n2015), which all our annotators were informed of.\nIn particular, annotators always assumedpand hre-\nferred to the same event or situation (Bowman et al.,\n2015). For determining category membership, we\nfollow the definitions of each AX category pro-\nvided by Wang et al. (2018), and developed a sim-\nple code book for determining this. The majority\nof the annotations were done synchronously in a\nfour-hour annotation session. Per recommended\npractices for content analysis, each item was rated\nby every annotator (Putka et al., 2008). Annotators\nwere encouraged to discuss items with each other\nand come to an agreement on what ratings should\nbe used. Further details on the content review, in-\ncluding an example of the annotation interface, can\nbe found in Appendix A.\nFor a generated item to pass the content review,\nwe determined that all annotators must rate the\nitem as very clear, either relevant or very relevant,\nthat the item contained no harmful content, and\nthat annotators were either sure or very sure of\ntheir predictions. Of the 400 items, 92 met these\ncriteria across all categories, with at least 15 in\nevery category passing. We sampled 15 at random\nfrom each category, balanced for the label, to obtain\nthe GPT-3 generated items. In total, 60 items were\nsampled.\n4 Experiments\nWe determined in Section 3 that GPT-3 can gen-\nerate AX items that possess at least face validity\nevidence. But are these items really more valid\nand reliable measures of basic linguistic reasoning,\ngiven that we designed our prompts to induce this?\nTo study this, we recruited human participants on\nAmazon Mechanical Turk7 to complete both the\nGPT-3 items and the original human-written items.\n102 participants residing in the United States,\nwho had completed at least 50 HITs (human intelli-\ngence tasks) with an acceptance rate of at least 90%,\n7https://www.mturk.com\nwere recruited to take part in the study. We use the\nattention check items and quality control protocol\nfrom Laverghetta Jr. et al. (2021) to validate that\nour workers participated in good faith. Workers\nfirst completed an onboarding HIT where they were\ngiven five attention check items, whose format was\nidentical to the AX items but by design, they were\nmuch easier to solve. This was meant to familiarize\nworkers with the task and ensure they would likely\ngive good response data. Workers who passed the\nonboarding then completed two more HITs, each\ncontaining half the GPT-3 items, and then two fi-\nnal HITs, each containing half the human-written\nitems, and each of these HITs contained six atten-\ntion checks spread evenly throughout the survey.\nEach worker’s submission was evaluated on every\nsurvey, and we followed the protocols developed by\nLaverghetta Jr. et al. (2021) to determine whether\nwork should be accepted or rejected. Briefly, work-\ners needed to get at least 60% accuracy on the\nsurvey, or at least 66% on the attention checks, and\nprovide a justification for each response to show\nthat they were truly paying attention. Further de-\ntails on the protocol and payment structure for the\nhuman studies are included in Appendix B.\nWe ultimately gathered data from 18 participants\nand base the following analysis on this sample.\nBroadly, our goal is to compare the psychomet-\nric properties of the GPT-3 written items to the\nhuman-written items, focusing specifically on item\ndifficulty, item discrimination, reliability (assessed\nusing internal consistency), and convergent and\ndiscriminant validity. These are all important prop-\nerties to analyze when establishing the validity and\nreliability of a new assessment (Bandalos, 2018),\nand when assessed using a measurement frame-\nwork known as classical test theory (CTT), can be\ncomputed using small sample sizes. CTT essen-\ntially posits that an individual’s true proficiency on\na cognitive task (their true score) can be decom-\nposed into an observed (actual) score they obtain\nand an error term that represents the measurement\nerror (Rust and Golombok, 2014). Note that this\nerror is assumed to be random, and not systematic.\nMethods from CTT for assessing both validity and\nreliability are hence based on analysis of observed\nscores, and correlations between observed scores,\nwhere the observed scores are simply accuracy on\nthe task:\nobservedscore = correctanswers\nallanswers (2)\n418\nFigure 2: Mean item difficulties for each category, mea-\nsured using total scores. Lower values indicate lower\ntotal scores, and hence more difficult items.\nAlthough more sophisticated measurement the-\nories have been developed (Embretson and Reise,\n2013), they typically rely on latent variable model-\ning and require much larger sample sizes. Further-\nmore, in practice, establishing validity and reliabil-\nity under CTT is often a first step in validating new\nassessments (Bandalos, 2018), which we believe\njustifies our focus on CTT in the present study.\n4.1 Analysis of Item Properties\nFigure 3: Mean item-to-total correlations for each cate-\ngory. Higher values indicate items are more predictive\nof a participant’s total score, and hence are more dis-\ncriminating.\nWe begin by comparing mean item difficulties\n(Figure 2) and mean item discriminations (Figure\n3) for both human and GPT-3 written items. Diffi-\nculty is based on the participants’ observed scores,\nand is equivalent to accuracy. Classical psychomet-\nrics dictates that items should have difficulties at\napproximately the midpoint between chance and\nFigure 4: Coefficient αfor item responses in each cate-\ngory, comparing human-written to GPT-3 written items.\nErrors bars are 95% confidence intervals computed us-\ning Feldt’s method (Feldt et al., 1987). Higher values\nindicate better reliability and stronger validity evidence.\nperfect scores (Lord, 1952), which in our case is\nroughly 70%. We again use item-to-total correla-\ntion to measure discrimination, and recall that item\ndiscrimination should be positive, with high values\nindicating better discrimination. We find that GPT-\n3 items are consistently closer to the optimal diffi-\nculty level than human-written items. GPT-3 items\nare also more discriminating than human-written\nones, though a notable exception is for LE, where\nthe GPT-3 items are noticeably less discriminating.\nAs LE tests for all forms of lexical entailment, and\nis a much more broadly scoped construct than the\nothers, lower discrimination is expected (Clark and\nWatson, 1995), though this does not fully explain\nthe rather sizeable drop.\n4.2 Internal Consistency Reliability\nItems on cognitive assessments should exhibit\nstrong reliability, meaning that participants with\nsimilar ability levels should also respond in a simi-\nlar fashion. A widely used measure of reliability is\ncoefficient α(Tavakol and Dennick, 2011), defined\nas:\nα= k\nk−1(1 −\n∑k\ni=1 σ2\nyi\nσ2x\n) (3)\nWhere kis the total number of items, σ2\nx is the\nvariance of total scores across all items, and σ2\nyi\nis the variance of total scores for item i. αranges\nfrom −∞to 1, and will be negative when there\nis greater within-subject variability than between-\nsubject variability. Reliability should thus be maxi-\nmized. We compute αfor both GPT-3 and human\n419\nFigure 5: Results from the MTMM matrix, computed\nusing Pearson correlations with total scores. Bluer col-\nors indicate stronger correlation.\nwritten items, doing so separately for each cate-\ngory, using the Pingouin Python library (Vallat,\n2018). Reliabilites with 95% confidence intervals\nare shown in Figure 4. Across all categories, GPT-\n3 produces items with similar or better reliabilities\ncompared to human-written items. MN is a special\ncase, as αfor this category dips into the negative\nrange, indicating poor validity evidence, though\neven in this case the GPT-3 items show much better\nreliability overall. Thus, the GPT-3 items appear\nto elicit more consistent responses among human\nparticipants.\n4.3 Convergent and Discriminant Validity\nEvidence\nThe multi-trait multi-method (MTMM) matrix is a\nclassic technique for evaluating the construct valid-\nity of measures and is often used when evaluating\nnew instruments (Campbell and Fiske, 1959). The\nMTMM matrix shows the correlations between dif-\nferent cognitive constructs (the traits) when they\nare measured using different measurement tech-\nniques (the methods). In this framework, validity\nis defined in terms of the strength of the correlation\nbetween different trait / method combinations. In\ngeneral, different methods should be strongly cor-\nrelated when measuring the same trait (monotrait-\nheteromethod), and different traits measured us-\ning the same method should be weakly correlated\n(heterotrait-monomethod), per the definitions of\nconvergent and discriminant validity (Campbell\nand Fiske, 1959).\nWe use this approach to evaluate the convergent\nand discriminant validity of the GPT-3 items. We\ntreat the AX category as the trait, and the method\nused to generate items (human written or gener-\nated by GPT-3) as the method and compute Pear-\nson correlations between all possible combinations\nof trait and method, using the participant’s total\nscores. Additionally, we check for significance\nusing Bonferroni corrected p-values of 0.002.8 Re-\nsults are shown in Figure 5. Significant monotrait-\nheteromethod correlations were found for PS ( ρ\n= 0.75, p << 0.001) but not for Q ( ρ = 0.72, p\n< 0.01), MN ( ρ = 0.06, p < 0.5) or LE ( ρ =\n0.20, p < 0.5) All heterotrait-monomethod cor-\nrelations were insignificant (p > 0.1), except for\nbetween PS and Q. For human-written items, the\ncorrelation was found to be significant ( ρ= 0.81,\np << 0.001), but not for GPT-3 written items (ρ\n= 0.16, p < 0.5). Collectively, these results indi-\ncate strong evidence for the discriminant validity\nof the GPT-3 items, given the lack of significant\nheterotrait-monomethod correlations. Evidence for\nconvergent validity is strong for PS, and to a lesser\nextent Q,9 but not for either MN or LE. Thus, the\nvalidity evidence for GPT-3 written items is just\nas strong, if not stronger, than for human-written\nitems.\n4.4 Analysis of Local Item Dependency\nRecall that CTT assumes that measurement errors\nare due purely to random chance, and systematic\nerror is not easily accounted for. One way this\ncan be violated is from a phenomenon called local\nitem dependence (LID). LID occurs between pairs\nof items, often whenever information needed to\nsolve the items is interrelated. For example, LID is\noften a concern on reading comprehension assess-\nments, because items that refer to the same text can\ninadvertently introduce local dependency on the\ncommon stimulus (Attali et al., 2022). Importantly,\nLID indicates that errors on items are interrelated in\na way other than proficiency on the construct, and\nhence imply systematic error in the measurement.\nAs Attali et al. (2022) notes, LID is an even\ngreater concern in the context of AIG, as GPT-3\nmay have generated items in a programmatic and\nsomewhat redundant fashion. Perhaps as an artifact\nof how AX was constructed, we also found many\nhuman-written items had highly similar linguistic\nstructures, which we reasoned could cause GPT-\n8Rounded to three decimal places.\n9The monotrait-hetheromethod correlations for Q were\nstrong, even though they did not meet the Bonferrioni-\ncorrected significance level.\n420\nFigure 6: Density plots (computed using kernel density estimation) of partial Pearson correlations computed for\neach category, controlling for the participants’ total scores per category. Item pairs where one or both items have 0\nvariance are excluded. Partial correlations greater than 0.3 indicate LID, and distributions which peak closer to 0\nhave fewer item pairs with LID.\n3 to generate items based on a common stimulus,\nwhich might inadvertently introduce LID. We thus\nfollow Attali et al.’s protocol and, for each category\nand for both the human-written and GPT-3 written\nitems, we compute the partial correlations between\nall pairs of items in each category, controlling for\ntotal scores. Following prior work (Christensen\net al., 2017; Attali et al., 2022), we use a thresh-\nold of 0.3 correlation or higher as indicating LID,\nand we plot the density distributions of the partial\ncorrelations in each category. Results are shown in\nFigure 6. We find that, even with the human-written\nitems, LID appears to be present in all categories\nexcept for MN, though even in this case we ob-\nserve strong anti-correlations. It does not appear,\nhowever, that the GPT-3 items have made LID sig-\nnificantly worse. Distributions are often similar\nbetween the item types, and in some cases, GPT-3\ndistributions appear closer to zero, indicating fewer\npairs with LID. We thus surmise that LID is no\ngreater a concern for GPT-3 written items than it\nwas for human-written items.\n4.5 Scaling Up to GPT-4\nOpenAI’s most recent LLM, GPT-4,10 was released\nafter the completion of our testing of the GPT-3\nitems. Given the large gains in performance re-\nported for GPT-4 across myriad tasks, we chose\nto perform preliminary analysis on the quality of\nitems generated by GPT-4, this time running only\nthe content review.11 We use the same content ex-\nperts and follow an identical protocol for the review.\nWe chose not to generate items for MN, due to the\nvery poor validity evidence for items in this cat-\negory. Hyperparameters and prompts remain the\n10https://openai.com/research/gpt-4\n11Due to time constraints, we could not run a more detailed\nanalysis on the GPT-4 written items, and leave this to future\nwork.\nsame, and we use the gpt-4 endpoint in the API.\nTo keep results as comparable as possible across\nmodels, we chose not to use the system context or\nother chat features provided for GPT-4, and instead\nadminister the prompts in a single shot. We gen-\nerate 18 items per category, totaling 54 across the\nthree categories tested. After running deduplication\nand dropping items with invalid labels, we admin-\nister the remaining items to our content experts.\nWe were specifically interested in whether our ex-\nperts would report the GPT-4 items as being any\nmore relevant for measuring the target construct as\ncompared to GPT-3. We graph the annotator distri-\nbutions for PS in Figure 7, and show results for LE\nand Q in Appendix C. Surprisingly, we find results\nfrom GPT-4 to be mixed. Although GPT-4 gen-\nerates a larger fraction of items labeled as either\n“Relevant” or “Very relevant” for Q, it generates\nfewer such items for LE and PS. As GPT-4 is de-\nsigned to function more like a chatbot than GPT-3,\nit is possible our prompts need to be restructured\nto make better use of the model’s capabilities, but\nmore experiments are needed to explore this.\n5 Discussion and Conclusion\nCollectively, our results demonstrate that LLMs\ncan generate items with superior validity evidence,\neven for constructs that have undergone limited\npsychometric analysis. GPT-3 items were found\nto have better discrimination and reliability, while\nmaintaining strong convergent, discriminant, and\ncontent validity. LID, while confirmed to be\npresent in both item types, appeared no worse and\nperhaps slightly better in GPT-3 items. These pos-\nitive results, while clearly present for PS and Q,\nwere less clear for MN and LE, and validity evi-\ndence as a whole appeared strongest for the cate-\ngories testing the most narrowly scoped constructs.\n421\nFigure 7: Distribution of annotator relevance scores\n(checking that the item both has a correct label and\nmatches the category) for both GPT-3 and GPT-4 items,\non items from the PS category. A lower percentage\nof items marked as “Completely irrelevant” indicates\nstronger evidence of the content validity of items gener-\nated using that model.\nThough promising, our results come with lim-\nitations that should be addressed in future work.\nThe small sample size we collected makes it diffi-\ncult to assess the generalizability of our findings.\nThis also prevented us from running any analysis\nof internal structure or differential item function-\ning (DIF) using methods from factor analysis or\nitem response theory, as these models require large\nsample sizes (Min and Aryadoust, 2021). As items\ngenerated by GPT-3 should contain no DIF and\nhave similar factor structures as items written by\nhumans, these are important analyses to explore\nin future work. We also did not examine the di-\nversity of the generated items, in other words, how\nthoroughly the model explored the construct space.\nIt is a well-known problem in psychometrics that\nhaving too many similarly worded items can inflate\nthe reliability and reduce the validity of a measure\n(Clark and Watson, 1995), and our results may have\nbeen susceptible to this. A related problem is ensur-\ning that the distribution of labels in the generated\nitems remains balanced, and while we took steps to\naccount for this, we did find that the distribution of\nGPT-3 items was somewhat unbalanced. For exam-\nple, there were far fewer neutral items than either\nentailment or contradiction. Improving the prompt\ndesign to account for diversity and other psychome-\ntric properties simultaneously is a fruitful direction\nfor future work. Our experiment with GPT-4, while\ndisappointing, was also quite limited and should be\nexpanded upon. We deliberately kept the prompt\ndesign as similar as possible between the two mod-\nels, to avoid possible confounds. Making effective\nuse of the system query and changing the structure\nof the prompts to suit a conversational style could\nlead to much better results, however. Finally, al-\nthough we believe NLI is a good task to use for\ninitial experimentation, we also acknowledge that\nit is significantly different from the tasks of interest\nin education (e.g., question answering), and future\nwork should explore our approach on tasks with\nstronger educational applications.\nLLMs have the potential to greatly ease the bur-\nden of scale development, and transform educa-\ntional and psychological measurement. Our results\ncontribute to the growing field of LLM-based au-\ntomated item generation, and demonstrate the po-\ntential these methods have for generating valid and\nreliable items at a scale that would have previously\nbeen impossible. Further research, combining our\napproach with more advanced prompting strategies,\nor zero-shot parameter estimation, could conceiv-\nably lead to a system that generates high-quality\nitems in a fully autonomous fashion, which would\ntransform the practice of writing and validating test\nitems.\nLimitations\nWe emphasize that our research is exploratory and\nthe generated items we produced should not be\nused for making critical evaluations of cognitive\nskillsets in either humans or LLMs. As discussed in\nSection 5, our small sample size makes it difficult\nto draw broad conclusions about the generalizabil-\nity of our findings, and practical considerations\nregarding the annotation study limited our ability\nto thoroughly explore the prompt space. While we\nchose GPT-3 due to its ease of use and the fact\nthat most psychometricians would likely be aware\nof it, we also acknowledge that OpenAI has re-\nleased few details on how this model is trained or\nupdated, which hampers the reproducibility of our\nresults. We also acknowledge that more recent Ope-\nnAI LLMs, including ChatGPT and GPT-4, have\nbeen released since this work is completed, and that\nour preliminary experiments using GPT-4 do not\ngive us a full understanding of the capabilities of\nthis model. However, given that we were still able\nto perform detailed experiments using the GPT-3\nitems, and these items proved to have superior va-\nlidity evidence across multiple trials, we do not\nbelieve the existence of more recent LLMs negates\n422\nour results. Finally, it is also well known that LLMs\ncan produce biased, toxic, or other forms of harm-\nful text content (Liang et al., 2021). While we took\nsteps to account for this in our content review, fu-\nture work must keep this possibility in mind and\ncarefully analyze generated items for potentially\nharmful content. A related problem is the risk of\nGPT-3 items propagating disadvantages against his-\ntorically marginalized groups. For example, the\nitems may have relied on cultural context or other\ninformation that would give an unfair advantage\nto certain populations. Given that we lacked a\nsufficient sample size and did not collect person-\nally identifiable information from participants, we\ncould not run DIF analysis to check for this, and\ncannot state definitively that DIF is not present.\nAcknowledgements\nWe would like to thank Logan Fields, Animesh\nNighojkar, and Zaid Marji for assisting us with\nthe content review. Part of this research was spon-\nsored by the DEVCOM Analysis Center and was\naccomplished under Cooperative Agreement Num-\nber W911NF-22-2-0001. The views and conclu-\nsions contained in this document are those of the au-\nthors and should not be interpreted as representing\nthe official policies, either expressed or implied, of\nthe Army Research Office or the U.S. Government.\nThe U.S. Government is authorized to reproduce\nand distribute reprints for Government purposes\nnotwithstanding any copyright notation herein.\nReferences\nAhmed M Asfahani. 2022. The impact of artificial intel-\nligence on industrial-organizational psychology: A\nsystematic review. The Journal of Behavioral Sci-\nence, 17(3):125–139.\nYigal Attali, Andrew Runge, Geoffrey T. LaFlair, Kevin\nYancey, Sarah Goodwin, Yena Park, and Alina A.\nvon Davier. 2022. The interactive reading task:\nTransformer-based automatic item generation. Fron-\ntiers in Artificial Intelligence, 5.\nDeborah L Bandalos. 2018. Measurement theory and\napplications for the social sciences. Guilford Publi-\ncations.\nIsaac I Bejar, René R Lawless, Mary E Morley,\nMichael E Wagner, Randy E Bennett, and Javier Re-\nvuelta. 2002. A feasibility study of on-the-fly item\ngeneration in adaptive testing. ETS Research Report\nSeries, 2002(2):i–44.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632–\n642.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDonald T Campbell and Donald W Fiske. 1959.\nConvergent and discriminant validation by the\nmultitrait-multimethod matrix. Psychological bul-\nletin, 56(2):81.\nSophia Chan, Swapna Somasundaran, Debanjan Ghosh,\nand Mengxuan Zhao. 2022. Agree: A system for gen-\nerating automated grammar reading exercises. arXiv\npreprint arXiv:2210.16302.\nXieling Chen, Haoran Xie, Di Zou, and Gwo-Jen\nHwang. 2020. Application and theory gaps dur-\ning the rise of artificial intelligence in education.\nComputers and Education: Artificial Intelligence ,\n1:100002.\nSong Cheng, Qi Liu, Enhong Chen, Zai Huang, Zhenya\nHuang, Yiying Chen, Haiping Ma, and Guoping Hu.\n2019. Dirt: Deep learning enhanced item response\ntheory for cognitive diagnosis. In Proceedings of the\n28th ACM international conference on information\nand knowledge management, pages 2397–2400.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Bang Christensen, Guido Makransky, and Mike\nHorton. 2017. Critical values for yen’s q 3: Identifi-\ncation of local dependence in the rasch model using\nresidual correlations. Applied psychological mea-\nsurement, 41(3):178–194.\nLee Anna Clark and David Watson. 1995. Constructing\nvalidity: Basic issues in objective scale development.\nPsychological Assessment, 7(3):309.\nRobin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox,\nJohan Van Genabith, Jan Jaspars, Hans Kamp, David\nMilward, Manfred Pinkal, Massimo Poesio, et al.\n1996. Using the framework. Technical report, Tech-\nnical Report LRE 62-051 D-16, The FraCaS Consor-\ntium.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluat-\ning Predictive Uncertainty, Visual Object Classifi-\ncation, and Recognising Tectual Entailment: First\n423\nPASCAL Machine Learning Challenges Workshop,\nMLCW 2005, Southampton, UK, April 11-13, 2005,\nRevised Selected Papers, pages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNan Duan, Duyu Tang, Peng Chen, and Ming Zhou.\n2017. Question generation for question answering.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n866–874, Copenhagen, Denmark. Association for\nComputational Linguistics.\nSusan E Embretson and Steven P Reise. 2013. Item\nresponse theory. Psychology Press.\nLeonard S Feldt, David J Woodruff, and Fathi A Salih.\n1987. Statistical inference for coefficient alpha. Ap-\nplied psychological measurement, 11(1):93–103.\nFriedrich M Götz, Rakoen Maertens, Sahil Loomba,\nand Sander van der Linden. 2023. Let the algorithm\nspeak: How to use neural networks for automatic\nitem generation in psychological scale development.\nPsychological Methods.\nIvan Hernandez and Weiwen Nie. 2022. The ai-ip: Min-\nimizing the guesswork of personality scale item de-\nvelopment through artificial intelligence. Personnel\nPsychology.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nAntonio Laverghetta Jr., Animesh Nighojkar, Jamshid-\nbek Mirzakhalov, and John Licato. 2021. Can trans-\nformer language models predict psychometric proper-\nties? In Proceedings of *SEM 2021: The Tenth Joint\nConference on Lexical and Computational Semantics,\npages 12–25, Online. Association for Computational\nLinguistics.\nPhilseok Lee, Shea Fyffe, Mina Son, Zihao Jia, and\nZiyu Yao. 2023. A paradigm shift from “human\nwriting” to “machine generation” in personality test\ndevelopment: an application of state-of-the-art nat-\nural language processing. Journal of Business and\nPsychology, 38:163–190.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understanding\nand mitigating social biases in language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research , pages 6565–6576.\nPMLR.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: Worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 6826–6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nFrederic M Lord. 1952. The relation of the reliability\nof multiple-choice tests to the distribution of item\ndifficulties. Psychometrika, 17(2):181–194.\nRakoen Maertens, Friedrich Götz, Claudia R Schnei-\nder, Jon Roozenbeek, John R Kerr, Stefan Stieger,\nWilliam Patrick McClanahan III, Karly Drabot, and\nSander van der Linden. 2021. The misinformation\nsusceptibility test (mist): A psychometrically vali-\ndated measure of news veracity discernment.\nShangchao Min and Vahid Aryadoust. 2021. A sys-\ntematic review of item response theory in language\nassessment: Implications for the dimensionality of\nlanguage ability. Studies in Educational Evaluation,\n68:100963.\nElham Mousavinasab, Nahid Zarifsanaiey, Sharareh\nR. Niakan Kalhori, Mahnaz Rakhshan, Leila Keikha,\nand Marjan Ghazi Saeedi. 2021. Intelligent tutor-\ning systems: a systematic review of characteristics,\napplications, and evaluation methods. Interactive\nLearning Environments, 29(1):142–163.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143,\nOnline. Association for Computational Linguistics.\nAnimesh Nighojkar and John Licato. 2021. Improv-\ning paraphrase detection with the adversarial para-\nphrasing task. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 7106–7116, Online. Association for\nComputational Linguistics.\nSimon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan\nBrauner, and Matthias Samwald. 2022. Mapping\nglobal dynamics of benchmark creation and satura-\ntion in artificial intelligence. Nature Communica-\ntions, 13(1):6793.\nChris Piech, Jonathan Bassen, Jonathan Huang, Surya\nGanguli, Mehran Sahami, Leonidas J Guibas, and\nJascha Sohl-Dickstein. 2015. Deep knowledge trac-\ning. Advances in neural information processing sys-\ntems, 28.\nSeptian Eko Prasetyo, Teguh Bharata Adji, and Indriana\nHidayah. 2020. Automated item generation: Model\nand development technique. pages 64–69. IEEE.\n424\nDan J Putka, Huy Le, Rodney A McCloy, and Tirso\nDiaz. 2008. Ill-structured measurement designs in\norganizational research: Implications for estimating\ninterrater reliability. Journal of Applied Psychology,\n93(5):959.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nManav Rathod, Tony Tu, and Katherine Stasaski. 2022.\nEducational multi-question generation for reading\ncomprehension. In Proceedings of the 17th Work-\nshop on Innovative Use of NLP for Building Edu-\ncational Applications (BEA 2022), pages 216–223,\nSeattle, Washington. Association for Computational\nLinguistics.\nCristobal Romero and Sebastian Ventura. 2020. Educa-\ntional data mining and learning analytics: An updated\nsurvey. Wiley Interdisciplinary Reviews: Data Min-\ning and Knowledge Discovery, 10(3):e1355.\nJohn Rust and Susan Golombok. 2014. Modern psy-\nchometrics: The science of psychological assessment.\nRoutledge.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951.\nStephen Stark, Oleksandr S Chernyshenko, and Nigel\nGuenole. 2011. Can subject matter experts’ rat-\nings of statement extremity be used to streamline\nthe development of unidimensional pairwise pref-\nerence scales? Organizational Research Methods,\n14(2):256–278.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. Association for Computa-\ntional Linguistics.\nMohsen Tavakol and Reg Dennick. 2011. Making sense\nof cronbach’s alpha. International journal of medical\neducation, 2:53.\nMikke Tavast, Anton Kunnari, and Perttu Hämäläinen.\n2022. Language models can generate human-like\nself-reports of emotion. In 27th International Con-\nference on Intelligent User Interfaces, pages 69–72.\nRaphael Vallat. 2018. Pingouin: statistics in python. J.\nOpen Source Softw., 3(31):1026.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nMatthias von Davier. 2018. Automated item genera-\ntion with recurrent neural networks. Psychometrika,\n83:847–857.\nReece Walsh, Mohamed H Abdelpakey, Mohamed S\nShehata, and Mostafa M Mohamed. 2022. Auto-\nmated human cell classification in sparse datasets us-\ning few-shot learning. Scientific Reports, 12(1):2924.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nRoger L Worthington and Tiffany A Whittaker. 2006.\nScale development research: A content analysis and\nrecommendations for best practices. The counseling\npsychologist, 34(6):806–838.\nBowei Zou, Pengfei Li, Liangming Pan, and Ai Ti Aw.\n2022. Automatic true/false question generation for\neducational purpose. In Proceedings of the 17th\nWorkshop on Innovative Use of NLP for Building\nEducational Applications (BEA 2022), pages 61–70,\nSeattle, Washington. Association for Computational\nLinguistics.\nJiyun Zu, Ikkyu Choi, and Jiangang Hao. 2023. Auto-\nmated distractor generation for fill-in-the-blank items\nusing a prompt-based learning approach. Psychologi-\ncal Testing and Assessment Modeling, 65(2):55–75.\nA Details on Content Review\nContent review ratings were collected via\nQualtrics.12 We developed five items to ask our\nexperts:\n12https://www.qualtrics.com\n425\nFigure 8: The annotation interface for the content review.\n1. Item Relevance: This question concerned\nthe usefulness of the item for measuring\nthe construct. Experts could rate items as\n“Completely irrelevant”, “Somewhat relevant”,\n“Relevant”, or “Very relevant”. At a basic\nlevel, items needed have both a correct label\nand test for the target category. If either of\nthese were false, experts were instructed to\nrate the item as “Completely irrelevant”. Ex-\nperts were instructed to rate items as “Some-\nwhat relevant” if the prior checks passed, but\nknowledge of the category was not critical to\nsolving the item. An example of this would\nbe an item from MN where the negated clause\ndoes not change at all from pto h. If knowl-\nedge of the category was critical, and all prior\nchecks passed, experts were instructed to rate\nthe item as “Relevant”. “Very relevant” was\nreserved for items that experts judged as be-\ning highly discriminating, which we included\nbased on prior work demonstrating experts\ncan effectively evaluate latent properties of\nitems (Stark et al., 2011). We left the exact\njudgment of what constituted a highly discrim-\ninating item up to the discretion of the ex-\nperts, and we encouraged them to discuss this\nand reach an agreement for each item deemed\n“Very relevant”.\n2. Item Clarity:This question concerned how\nclear the wording of the item is, and whether\nit contains spelling or grammatical errors.\nExperts could rate items as ”Not clear, ma-\njor revisions”, “Somewhat clear, some revi-\nsions”, “Clear, slight revisions”, and “Very\nclear, no revisions”. “Not clear, major revi-\nsions” was reserved for cases where items\ncontained any spelling or grammatical errors.\nThis also included cases with unterminated\npunctuation (e.g, an opening ‘(’ that was not\nclosed). Both “Somewhat clear, some revi-\nsions” and “Clear, slight revisions” were re-\nserved for cases where the prose of the item\nwas unorthodox (e.g, GPT-3 generated an odd\nword choice or an unusual phrase). Experts\nwere instructed to rate “Very clear, no revi-\nsions” if items were both grammatically cor-\nrect and contained no unusual wording that\nmade the item needlessly difficult to under-\nstand.\n426\n3. Potentially Harmful Content:This was in-\ncluded to ensure that GPT-3 did not generate\noffensive or otherwise harmful content in the\nitems, though we did not expect this to be an\nissue in general as AX items were written in\na fairly neutral tone and avoided covering con-\ntroversial social issues or explicitly targeting\nidentified subgroups. Experts were instructed\nto check if the items contained any content\nrelated to race, ethnicity, religion, or other\nidentifiable characteristics that might be con-\nsidered offensive to members of those groups.\nImportantly, AX does contain items related\nto U.S. politics circa 2018 that we reasoned\nmight lead to toxic generations regarding po-\nlitical ideology. We made experts aware of\nthis but instructed them toonly rate such items\nas harmful if the content explicitly attacked a\npolitical ideology or its adherents. There were\nonly two options for this item, “yes” or “no”.\n4. Annotator Certainty:Finally, using a four-\npoint Likert scale, we asked annotators to rate\nhow sure they were of their ratings.\nFigure 8 shows the annotation interface. Experts\nwere given the full item content and the label gen-\nerated by GPT-3, as well as additional data about\nthe hyperparameters used which they did not need\nto refer to. They were free to move back and forth\nwithin the survey and revise their responses later\nif they wished. Most annotations were completed\nin a synchronous session, and all annotators be-\ngan their work in this session to ensure the task\ninstructions were clear and to train them on how to\nrate each item. Importantly, we did not ask raters\nto edit any item content to improve its quality, as\nwe were interested in the quality of GPT-3 written\nitems without human intervention.\nFor determining category membership, we devel-\noped a codebook based on the presence of certain\nkeywords in the item content, and either p or h\nneeded to contain at least one of these keywords to\npass content validity. For example, for Q, either p\nor hneeded to contain either a universal (all, none)\nor existential (some, many, most, etc.) quantifier\nin natural language to pass. We developed an ini-\ntial list of keywords based on both the appendix\ncovering AX in Wang et al. (2018), and by manu-\nally inspecting the items in each category to locate\nadditional keywords. During the content review,\nexperts could also suggest additional keywords,\nand if all annotators agreed, these new keywords\nwere added to the codebook. Table 2 show all the\nkeywords used across categories. LE was the only\ncategory that did not follow this protocol for de-\ntermining category membership. As LE tests for\nall forms of entailment at the word level, there is\nno predetermined list of keywords that can be used\nto determine LE membership. Therefore, for LE,\nwe used the rule that pand hmust differ by only\none word, with the only exception being if other\nwords needed to be changed to keep the sentences\ngrammatically correct.\nB Details on Human Study\nWe follow many of the same protocols from\nLaverghetta Jr. et al. (2021) for conducting our hu-\nman study. In particular, they employed attention\ncheck NLI items taken from the ChaosNLI dataset\n(Nie et al., 2020), which collected 100 human rat-\nings to a subset of SNLI (Bowman et al., 2015) and\nMultiNLI (Williams et al., 2018) items. Only items\nwhich at least 90% of the workers agreed on the\ncorrect label were used, and hence they are presum-\nably quite easy to answer correctly. In addition,\nLaverghetta Jr. et al. (2021) also asked workers\nto justify their response to each item, which was\nused as an additional check to ensure workers were\npaying attention during the task. We follow their\nprotocol and check that workers do not copy text\nfrom the item as their justification, that the justifi-\ncation is not used multiple times, that it is clearly\nrelated to the item content, 13 and that the justifi-\ncation is not a nonsensical word or phrase (e.g,\n“good” or “nice question”). Collectively, the fol-\nlowing quality control procedure was used for each\nsurvey:\n1. Submissions with duplicate IP addresses or\nworker IDs were dropped.\n2. Submissions with less than 40% accuracy, or\nless than 60% with less than 66% on attention\nchecks, were dropped.\n3. Submissions whose justifications did not meet\nthe above criteria were also dropped.\nAll other submissions were accepted, and at each\nstage passing workers were given qualifications to\nproceed to the next survey. If however, workers\n13In some instances, workers appeared to copy text from ex-\nternal websites that was completely unrelated to the question.\n427\nCategory Keywords\nMN un −, non− ir−, dis−, im−, il−, in−, −n’t, not, never, no\nPS un −, non− ir−, dis−, im−, il−, in−, −n’t, not, no, and, or, if\nQ all, no, some, many, most, none, every, several, each, one other, only, nearly all, the , part of\nTable 2: Keywords used to determine category membership. Leading and trailing “−” indicate suffixes and prefixes,\nrespectively.\nfailed a given stage, they were not allowed to pro-\nceed. In total, we administered five separate HITs\nand used Qualtrics to gather all responses. Workers\nwere paid $8.00 for each HIT, except for the initial\nonboarding HIT, where they were paid $0.10,14 and\nhad one hour to complete each HIT. Workers were\ntold they would be compensated for each survey\ncompleted successfully, to encourage consistently\nhigh-quality work. Workers gave informed con-\nsent to participate prior to beginning each HIT, and\ncould withdraw at any time. Workers could ap-\npeal any rejections made, however, we also clearly\nstated submissions would be checked for quality\ncontrol purposes, and may be dropped if evidence\nof bad-faith responses was found. All work was\ndone anonymously; workers were not asked to pro-\nvide us with any personally identifiable information\nat any stage.\nFinally, we also considered extending\nLaverghetta Jr. et al.’s protocol to check for\nAI-generated text for the explanations, in case\nworkers attempted to use ChatGPT or another\nLLM during the survey. We examined several\ndetectors for AI-written text, including one\ndeveloped by OpenAI.15 However, we found that\ncurrently available models require too much text\nto be helpful for our study. Participants were\nasked to only briefly explain their thought process\nwith at most one sentence, which was far too\nshort for current detectors to make a classification.\nTherefore, we did not include any check for\nAI-generated text, but we strongly encourage\nfuture work to consider this and investigate other\npossible safeguards against workers cheating on\nthe task using LLMs.\nC Additional Results from GPT-4\nFigures 9 and 10 compare the annotator relevance\nscores between GPT-3 and GPT-4 items, for LE\nand Q.\n14This HIT contained only 5 items and was meant to be\nfinished quickly.\n15https://openai.com/blog/new-ai-classifier-for-indicating-\nai-written-text\nFigure 9: Distribution of annotator relevance scores for\nLE.\nFigure 10: Distribution of annotator relevance scores\nfor Q.\n428",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6646993160247803
    },
    {
      "name": "Inference",
      "score": 0.5359298586845398
    },
    {
      "name": "Discriminant validity",
      "score": 0.5315836071968079
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5142852663993835
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5063079595565796
    },
    {
      "name": "Cognition",
      "score": 0.4945337474346161
    },
    {
      "name": "Process (computing)",
      "score": 0.4931589961051941
    },
    {
      "name": "Natural language processing",
      "score": 0.4704165756702423
    },
    {
      "name": "Machine learning",
      "score": 0.4466104507446289
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4304264783859253
    },
    {
      "name": "Psychometrics",
      "score": 0.3932177722454071
    },
    {
      "name": "Psychology",
      "score": 0.3349175453186035
    },
    {
      "name": "Clinical psychology",
      "score": 0.20443275570869446
    },
    {
      "name": "Programming language",
      "score": 0.08109444379806519
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Internal consistency",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ]
}