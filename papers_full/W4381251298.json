{
  "title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System",
  "url": "https://openalex.org/W4381251298",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2023896611",
      "name": "Hu, Zhiyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1954544755",
      "name": "Feng Yue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230890300",
      "name": "Luu, Anh Tuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746959131",
      "name": "Hooi, Bryan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224200774",
      "name": "Lipani, Aldo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4221167095",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4385572445",
    "https://openalex.org/W4284708952",
    "https://openalex.org/W4385571593",
    "https://openalex.org/W4310558071",
    "https://openalex.org/W3035337525",
    "https://openalex.org/W3034441005",
    "https://openalex.org/W4287889972",
    "https://openalex.org/W4200253303",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W4385570526",
    "https://openalex.org/W3214585123",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4300513200",
    "https://openalex.org/W4365601361",
    "https://openalex.org/W3163474636",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4284695140",
    "https://openalex.org/W4297783677",
    "https://openalex.org/W4372346643",
    "https://openalex.org/W4321650181",
    "https://openalex.org/W4283789934",
    "https://openalex.org/W4385574182"
  ],
  "abstract": "Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.",
  "full_text": "Unlocking the Potential of User Feedback: Leveraging Large\nLanguage Model as User Simulator to Enhance Dialogue System\nZhiyuan Hu\nzhiyuan_hu@u.nus.edu\nNational University of Singapore\nYue Feng\nyue.feng.20@ucl.ac.uk\nUniversity College London\nAnh Tuan Luu\nanhtuan.luu@ntu.edu.sg\nNanyang Technology University\nBryan Hooi\nbhooi@comp.nus.edu.sg\nNational University of Singapore\nAldo Lipani\naldo.lipani@ucl.ac.uk\nUniversity College London\nABSTRACT\nDialogue systems and large language models (LLMs) have gained\nconsiderable attention. However, the direct utilization of LLMs as\ntask-oriented dialogue (TOD) models has been found to under-\nperform compared to smaller task-specific models. Nonetheless,\nit is crucial to acknowledge the significant potential of LLMs and\nexplore improved approaches for leveraging their impressive abil-\nities. Motivated by the goal of leveraging LLMs, we propose an\nalternative approach called User-Guided Response Optimization\n(UGRO) to combine it with a smaller TOD model. This approach\nuses LLM as an annotation-free user simulator to assess dialogue re-\nsponses, combining them with smaller fine-tuned end-to-end TOD\nmodels. By utilizing the satisfaction feedback generated by LLMs,\nUGRO further optimizes the supervised fine-tuned TOD model.\nSpecifically, the TOD model takes the dialogue history as input\nand, with the assistance of the user simulator‚Äôs feedback, gener-\nates high-satisfaction responses that meet the user‚Äôs requirements.\nThrough empirical experiments on two TOD benchmarks, we vali-\ndate the effectiveness of our method. The results demonstrate that\nour approach outperforms previous state-of-the-art (SOTA) results.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíUsers and interactive retrieval ; ‚Ä¢\nComputing methodologies ‚ÜíDiscourse, dialogue and pragmatics ;\n‚Ä¢ Human-centered computing ‚ÜíHuman computer interaction\n(HCI).\nKEYWORDS\nDialogue system, Large Language Model, User Simulation\nACM Reference Format:\nZhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, and Aldo Lipani. 2023.\nUnlocking the Potential of User Feedback: Leveraging Large Language\nModel as User Simulator to Enhance Dialogue System. In Proceedings of the\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0124-5/23/10. . . $15.00\nhttps://doi.org/10.1145/3583780.3615220\nHello, anything may I help you?\nHello,IwanttoreservearestaurantinEustonstreetOK, do you want to find a restaurant for eating?OK, what type of food do you want to reserve.OK,thereareChinesefood,IndianfoodandItaliancuisineetc.,whichonedoyouprefertoreserve\nGreat,IlikeItalianfood.Pleasegivemethedetailedinformation\nLLM\nEstimate user‚Äôs feedback on responses\nFigure 1: An example of using LLMs as a user simulator to\npredict user satisfaction scores in task-oriented dialogues.\n32nd ACM International Conference on Information and Knowledge Manage-\nment (CIKM ‚Äô23), October 21‚Äì25, 2023, Birmingham, United Kingdom. ACM,\nNew York, NY, USA, 5 pages. https://doi.org/10.1145/3583780.3615220\n1 INTRODUCTION\nIn recent years, there has been significant interest in dialogue sys-\ntems and their potential to improve human-computer interaction.\nThe development of LLMs has particularly contributed to the ad-\nvancement of general dialogue systems. However, adapting LLMs\nfor task-oriented dialogues (TODs) in specific domains, such as\nservice reservation, remains challenging. LLMs struggle to han-\ndle the intricacies of TODs as they require domain knowledge,\nbackground information, and context to generate appropriate re-\nsponses. Recent research [9, 15] has aimed to tackle this challenge\nusing few-shot or zero-shot learning. However, even when adapting\nAlpaca-LoRA-7B or ChatGPT, the BLEU score and Success metric\nfor TOD tasks remain significantly low compared with fine-tuned\nend-to-end models. An alternative option for fine-tuning LLMs is\nresource-intensive. Moreover, whether fine-tuning will result in\nsignificant improvements is not always guaranteed [ 1] and can\nvary depending on several factors, including the similarity between\npretraining tasks and fine-tuning tasks, the model size, and the\nduration of the fine-tuning process. 1 2\nHow should LLMs be integrated into TOD systems? Work in\npsychology has explored the dichotomy between two cognitive\nprocesses, System 1 (fast and intuitive) and System 2 (slow and\ndeliberative), which guides or corrects System 1 [10], Our model\n1Code is available at: https://github.com/zhiyuanhubj/UGRO-CIMK23.\n2The major work was conducted when first author was in University College London.\narXiv:2306.09821v2  [cs.CL]  19 Oct 2023\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, & Aldo Lipani\ndesign follows a similar separation of roles: we leverage the reason-\ning and knowledge of an LLM by using it as a source of feedback to\nguide a task-specific and thus ‚Äòintuitive‚Äô model. Toward this goal,\nour approach is to utilize LLMs as a user simulator. User simula-\ntion has emerged as a line of work which involves mimicking user\nbehaviour, including user query generation, system response satis-\nfaction prediction, and user action prediction [6, 11, 23, 29]. LLMs\npossess significant strengths, such as their remarkable understand-\ning, extensive knowledge, and reasoning capabilities. Therefore, it\nis natural to utilize LLMs as user simulators to assess responses\ngenerated by TOD systems using smaller fine-tuning end-to-end\nmodels and provide feedback on user satisfaction to enhance the\nTOD system.\nFigure 1 illustrates an example of a dialogue system response and\na user simulator. In this scenario, the system responds to the user\nby considering the dialogue history. The user simulator, powered\nby an LLM, predicts the satisfaction scores based on the system\nresponse candidates and previous dialogue history. By leveraging\nthese satisfaction feedback, we can select the most likely satisfying\nresponse for the user, thereby facilitating high-quality interactions.\nThe key aspect enabling this is the ability of the LLM to assign a\nsatisfaction score to each potential response candidate.\nHowever, due to limitations in dataset annotation scale, mod-\nelling user behaviour, and the capabilities of current user simulator\napproaches, the performance of previous research on user simu-\nlators is not yet qualified for practical applications. Consequently,\nwe explore two questions: 1) whether current LLMs can serve as an\neffective user simulator and 2) how we can leverage the user simu-\nlator‚Äôs feedback generated by LLMs to enhance the task-oriented\ndialogue system. To address these questions, we design suitable\nprompts and investigate the zero-shot and few-shot performance\nof LLM on user satisfaction prediction tasks. We propose our ap-\nproach, User-Guided Response Optimization (UGRO), to leverage\nthe user feedback to boost the TOD system. Initially, we train the\nTOD model using a domain-supervised dataset. Subsequently, the\nsatisfaction score predicted by the LLM will be utilized as a reward\nto optimize the fully supervised trained TOD system. This will\nbe accomplished by employing the Proximal Policy Optimization\n(PPO) algorithm, which allows us to capitalize on user feedback. In\nsummary, our contributions can be categorized into three aspects:\n‚Ä¢We investigate the performance of LLMs as a user simulator for\nproviding feedback in zero-shot and few-shot settings.\n‚Ä¢We propose a new model called User-Guided Response Opti-\nmization (UGRO) to harness the potential of user feedback and\nenhance dialogue systems.\n‚Ä¢Extensive experiments validate the effectiveness of our model, in-\ncluding quantitative performance evaluation, human evaluation,\nand a case study.\n2 RELATED WORK\nTOD systems facilitate tasks like hotel bookings or restaurant reser-\nvations. Some end-to-end models [ 8, 13] generate responses us-\ning only the dialogue context, while policy optimization methods\n[24, 25] use ground-truth dialogue states. Bang et al. [2], Lee [13]\nincorporate both text information and dialogue states to generate a\ndialogue response. Additionally, reinforcement learning methods\n[28] have also shown improvements in TOD systems. Hudeƒçek and\nDu≈°ek [9] show that specialized task-specific models still outper-\nform general LLMs in TOD tasks. Additionally, the integration of\nLLMs into domain-specific dialogue systems has also been explored\nby Li et al. [15], Snell et al. [21].\nRegarding user simulators, Sun et al. [23] proposes the user sat-\nisfaction estimation task. It leverages human annotations regarding\nturn-level satisfaction to train an estimator. The estimator is then\nutilized to perform automatic evaluation by simulating users. Deng\net al. [5], Kim and Lipani [11] employ the multi-task framework\nto enhance the performance of their user simulator. Feng et al. [7]\nuse schema-guided information, and Ye et al. [27] incorporate sat-\nisfaction dynamics to enhance user satisfaction prediction. [17, 26]\nexplore Multi-Scale Receptive Field Graph model and Variational-\nAutoencoder in unsupervised way to determine the emotion in a\nconversation. Additionally, [14] propose a new dialogue simulation\nmethod based on LLM in-context learning to construct the dataset\nautomatically.\n3 METHODOLOGY\n3.1 Overview\nWe begin by assessing the performance of LLMs as user simulators\nfor predicting response satisfaction. Subsequently, we fine-tune the\nTOD models through supervised training with response data and\noptimize it using the PPO algorithm and satisfaction rewards.\n3.2 LLM as User Simulator\nMotivation As mentioned earlier, the user simulation encom-\npasses query generation, response satisfaction prediction, and ac-\ntion prediction. Satisfaction prediction stands out as the most di-\nrect feedback for assessing quality and enhancing the TOD model.\nHence, in this work, our key hypothesis is that satisfaction predic-\ntion allows the capabilities of LLMs to be effectivelytransferred to a\nnew domain by prompting them in a suitable way to predict satisfac-\ntion scores in that domain. LLMs‚Äô strong understanding ability and\nrich knowledge enable them to comprehend the semantic aspects\nof domain dialogue. Moreover, their reasoning capability allows\nthem to provide a reasonable score with detailed explanations.\nAs illustrated in Figure 2, when using LLMs as a user simulator,\nthe dialogue history serves as the input, and the system response is\nconsidered as the last utterance. To provide the necessary context\nfor LLM, the input prompt consists of the task description, crite-\nria for satisfaction scores, data format introduction, 0-6 dialogue\nexamples, the dialogue to be assessed, and instructions. Notably,\nwe incorporate the idea of ‚ÄòZero-shot Chain-of-Thought‚Äô [12] by\nincluding an ‚Äòexplanation reason‚Äô slot in the instruction. This al-\nlows the LLM to provide a reliable satisfaction score along with its\nreasoning. The complete prompt used in this work can be found in\nour GitHub repository. The output of the user simulator consists of\nboth the explanation reason and the satisfaction score. The score is\nextracted using regular expressions.\nFormally, the TOD dataset, denoted as D= {ùíô,ùíö‚àó}, consists of\ndialogue history inputs ùíô and corresponding ground truth response\noutputs ùíö‚àó. The TOD model‚Äôs response is represented as ùíö, and the\nsatisfaction score predicted by the LLM denoted as ùë∫.\nùë∫ = ùêøùêøùëÄ (ùíô,ùíö) (1)\nUnlocking the Potential of User Feedback: Leveraging LLMs as User Simulators to Enhance Dialogue CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\nDialogue History\nb) Supervised Training of TOD Model \nc) User-Guided Response Optimization\nResponse Generator\nDialogue History\nResponse GeneratorUserSimulator\nTrain with user satisfaction feedback(PPO)\nuser satisfaction score \nDialogue History\na) LLM Serve as User Simulator \nUserSimulator\nPaStationofferscustomizablepastadishesfrom11amto2am.MayIhelpyoutoreserve?\nPaStationofferscustomizablepastadishesfrom11amto2am.MayIhelpyoutoreserve?\nHello, anything may I help you?Hello,IwanttoreservearestaurantinEustonstreetOK,thereareChinesefood,IndianfoodandItaliancuisineetc.,whichonedoyouprefertoreserve\nGreat,IlikeItalianfood,pleasegivemethedetailedinformation\nHello, anything may I help you?Hello,IwanttoreservearestaurantinEustonstreetOK,thereareChinesefood,IndianfoodandItaliancuisineetc.,whichonedoyouprefertoreserveGreat,IlikeItalianfood,pleasegivemethedetailedinformation\nHello, anything may I help you?Hello,IwanttoreservearestaurantinEustonstreetOK,thereareChinesefood,IndianfoodandItaliancuisineetc.,whichonedoyouprefertoreserveGreat,IlikeItalianfood,pleasegivemethedetailedinformation\nFigure 2: Model Architecture. TOD model undergoes supervised training initially and then is optimized based on user feedback.\n3.3 User-Guided Response Optimization\nUsing LLMs to enhance TOD models As discussed in our\nintroduction, the limitations of directly utilizing LLMs in TOD sys-\ntems necessitate a key design goal in our approach: employing\nLLMs as a user simulator to provide satisfaction feedback and lever-\naging this feedback to optimize the fine-tuning TOD model, so\nas to leverage the LLM‚Äôs knowledge, understanding and reason-\ning capabilities. This user feedback guides the TOD model toward\nproducing responses that better satisfy the user.\nIn order to train a task-oriented dialogue (TOD) system, we begin\nby performing supervised training on the TOD model using the\ncomplete training dataset, maximizing the log-likelihood:\nLTOD = ‚àíE(ùíô,ùíö‚àó)‚àºDlog ùëùTOD\n\u0000ùíö‚àó|ùíô\u0001 (2)\nOur objective is to enhance the quality of TOD system-generated\nresponses and improve user satisfaction. Thus, the optimization\ngoal during this phase is to achieve higher satisfaction scores. Math-\nematically, we aim to maximize the following objective:\nEùíô‚àºD,ùíö‚àºùëùTOD [R(ùíô,ùíö)] (3)\nwhere R(ùíô,ùíö)= ùë∫ = ùêøùêøùëÄ (ùíô,ùíö),ùíö ‚àºùëùTOD(¬∑| ùíô)\nThe aforementioned optimization process can be complex and\nunstable for the TOD model. Therefore, we approach the TOD\noptimization as a reinforcement learning problem and employ the\nproximal policy optimization (PPO) algorithm [20]. We initialize\nthe policy network ùúã0 using the TOD model, denoted as ùëùTOD.\nEùúã [ùëü]= Eùíô‚àºD,ùíö‚àºùúã (¬∑|ùíô)[ùëü (ùíô,ùíö)] (4)\nTo incorporate penalty rewards, we also utilize the KL-divergence\nand set the hyperparameter ùõΩ for training. Consequently, the final\nreward is calculated as follows:\nùëü (ùíô,ùíö)= ùêøùêøùëÄ (ùíô,ùíö)‚àíùõΩ log ùúã (ùíö |ùíô)\nùëùTOD(ùíö |ùíô) (5)\nImplementation Details In this work, ChatGPT is used as a user\nsimulator to assess the generated response and provide satisfaction\nscores. We utilize Flan-T5 (large version) [4] as the fine-tuning TOD\nmodel. This model has been trained extensively on instruction tasks\nand is well-suited for domain-supervised fine-tuning.\n4 EXPERIMENT RESULTS AND ANALYSIS\n4.1 Experimental Settings\nDataset In dialogue response generation, MultiWoZ 2.1 [3] and\nSchema Guided Dialogue (SGD) [19] are two typical TOD datasets,\nand they also have been annotated with satisfaction scores.\nEvaluation Metrics For evaluation, we use BLEU [ 18] and\nROUGE (F1 score of ROUGE-1, ROUGE-2, and ROUGE-L) [16] met-\nrics to assess the generation quality and semantic performance.\nCompared Prior Art We compare our model with prior strong\nbaselines, including HDNO [24], MTTOD [13], PPTOD [22], GALAXY\n[8], and TOATOD [2]. HDNO models the hierarchical structure\nbetween dialogue policy and natural language generation using\nan option framework [] (temporal abstraction for reinforcement\nlearning). MTTOD combines pretrained language models with a\nmulti-task learning framework for end-to-end TOD modelling by\nutilizing span prediction as an auxiliary task. PPTOD is a unified\nplug-and-play model for TOD. It employs a multi-task pre-training\nstrategy to learn primary TOD task completion skills from diverse\ndialogue corpora. GALAXY is a generative pretrained model that\nimproves dialogue generation by incorporating semi-supervised\nlearning and explicit policy injection. TOATOD is an end-to-end\nTOD system with task-specific adapters, learning independently\nfor tasks such as dialogue state tracking and response generation.\n4.2 User Simulator Performance and Analysis\nTable 1 demonstrates that ChatGPT performs well even in a zero-\nshot setting. Increasing the number of examples to 6 shows a signif-\nicant improvement in accuracy. The F1 score also exhibits enhance-\nment. Furthermore, ChatGPT‚Äôs ability to predict satisfaction scores\nis comparable to that of fine-tuned BERT models and significantly\nsuperior to the zero-shot BERT model. We also noticed that the\ndataset imbalance affects satisfaction prediction in ChatGPT. Rating\n3 dominates 90% of samples, while ratings 1 and 5 have less than\n1%. This poses challenges for accurate satisfaction classification.\nAlthough ChatGPT‚Äôs performance may only be comparable, its\nadvantage lies in its practicality as a user simulator. This annotation-\nfree approach addresses the issues of high annotation cost and\nhuman bias encountered in previous satisfaction score labelling.\nFurthermore, this method can be easily extended to new domains or\ntasks, even with minimal or zero examples, while still maintaining\ncomparable and practical performance.\nCIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom Zhiyuan Hu, Yue Feng, Anh Tuan Luu, Bryan Hooi, & Aldo Lipani\nTable 1: The user satisfaction prediction typically involves considering the user‚Äôs utterance alongside the system response.\nHowever, in our case, user satisfaction is assessed by ChatGPT solely based on the system response, following the approach in\n[23], which is the SOTA performance. We use their BERT method as the baseline, replicating their experimental setup.\nSetting Model MultiWoZ 2.1 SGD\nAcc P R F1 Acc P R F1\nSupervised Training HiGRU+ATTN 79.7 24.2 24.0 24.0 70.9 25.8 26.2 26.1\nBERT 85.4 31.6 26.9 27.1 77.8 28.9 26.9 27.0\nZero shot BERT 14.8 13.5 20.3 5.8 11.3 18.1 22.2 6.1\nChatGPT 61.1 21.6 19.1 19.1 51.6 22.6 26.6 19.4\nFew shot ChatGPT 78.1 22.4 20.8 21.1 72.6 26.6 23.1 23.6\nTable 2: Performance of system response generation. The BLEU performances of the MultiWOZ 2.1 dataset are sourced from\npublished papers, and ROUGE scores are calculated based on the generated responses uploaded to the leaderboard or provided\nby the authors. In the case of the SGD dataset performance, * denotes results implemented using official source codes.\nModel MultiWoZ 2.1 SGD\nBLEU ROUGE ROUGE-1 ROUGE-2 ROUGE-L BLEU ROUGE ROUGE-1 ROUGE-2 ROUGE-L\nHDNO[24] 18.9 28.2 34.6 17.0 32.9 - - - - -\nMTTOD[13] 21.0 30.2 36.9 18.6 35.0 5.7‚àó 18.6‚àó 23.8‚àó 9.2‚àó 22.7‚àó\nPPTOD[22] 19.2 31.2 37.8 19.6 36.3 9.7‚àó 22.0‚àó 27.3‚àó 12.6‚àó 26.1‚àó\nGALAXY[8] 20.0 30.2 36.7 18.7 35.1 - - - - -\nTOATOD[2] 17.1 29.1 35.3 18.0 34.0 - - - - -\nUGRO- 19.2 24.5 30.0 14.3 28.9 19.8 35.8 42.2 24.7 40.4\nUGRO 34.3 31.7 38.5 20.1 36.5 24.6 37.4 44.0 26.4 41.8\n4.3 Advancing TOD through User Feedback\nTable 2 shows UGRO‚Äôs superior performance compared to the pre-\nvious SOTA model on the MultiWoZ 2.1 dataset. UGRO achieves a\n13% BLEU improvement over the MTTOD model and sets a new\nSOTA in ROUGE, surpassing the PPTOD model. To validate the ef-\nfectiveness of user simulator feedback generated by LLMs, we also\nconduct an ablation study by performing UGRO- (Non-feedback\nUGRO) experiments. The results show a roughly 15% increase in\nBLEU and an average 7% improvement in ROUGE, highlighting the\nbenefits derived from user feedback.\nFurthermore, we implemented three baselines (MTTOD with\nthe highest BLEU score, PPTOD with the highest ROUGE results\nand UGRO-) from the MultiWoZ 2.1 dataset into the SGD dataset.\nBy comparing the performance of these baselines with the UGRO\nmodel, we can assess the generalization and stability of our method.\n4.4 Case Study\nIn Figure 3, we compare the responses generated by our model\nUGRO, PPTOD and MTTOD to better illustrate our motivation.\nUGRO demonstrates a better understanding of dialogue history,\nproviding clear and relevant responses while also requesting the\nnecessary information. Furthermore, unlike the ground truth re-\nsponse that simply asks for the ‚Äòdeparture‚Äô place, which has already\nbeen mentioned in the previous dialogue, the UGRO model provides\nboth the departure site and the day for the train, resulting in a more\nsatisfactory and higher-quality answer.\n4.5 Human Evaluation\nTo ensure a more comprehensive evaluation, we incorporate human\nassessment to measure the satisfaction level and semantic quality\nof the generated responses. For satisfaction assessment, we em-\nploy a 5-level scoring setting with defined criteria in [23]. Semantic\nquality is evaluated on a scale of 1 to 10, focusing on fluency and\ncoherence. We randomly select 50 responses from each MTTOD,\nPPTOD, and UGRO generation results in the MultiWoZ 2.1 dataset\nand obtained assessments from 5 annotators. Among the evalu-\nated models, UGRO demonstrates superior performance, with a\nI am planning a trip in Cambridge?\nHow can I help you today ?\nWhat is your destination ?\nWhere will you be going ?\nThere are [value_choice] trains leaving [value_departure]\non [value_day]. What time would you like to leave?\nI need to know what museums are in the area . can you\nsuggest 1 and let me know what area it is in please ?\nIs there a location preference ?\nI do not care what area it is . I was hoping you could\nmake a recommendation .\nWhat about [value_name] [value_type] in the\n[value_area] ?\nSounds great ! I also need a train leaving on Thursday\nfrom London kings cross .\nPPTOD\nMTTOD\nOkay , there are [value_choice] trains that meet your criteria . Would\nyou like to narrow it down further by departure site or arrival time ?\nUGRO\nGround\nTruth\nFigure 3: Dialogue responses generated by different models.\nsatisfaction score of 4.25, surpassing MTTOD (4.07) and PPTOD\n(4.09). Additionally, UGRO achieves the highest semantic quality\nscore of 7.99, compared to MTTOD (7.89) and PPTOD (7.87), further\nsolidifying its position as the top-performing model.\n5 CONCLUSION\nIn this study, we embark on an exploration of leveraging LLMs as\nuser simulators. We propose UGRO, a novel approach that optimizes\nthe fine-tuned TOD model by incorporating satisfaction feedback\nfrom the LLM-powered user simulator. Our objective is to lever-\nage the user simulator‚Äôs potential and seamlessly integrate it into\nfuture TOD models. By using LLMs as user simulators with appro-\npriate prompts, we achieve comparable performance to the previous\nSOTA model in predicting satisfaction scores. This annotation-free\nmethod can be extended to different domains. We validate UGRO\nby evaluating improvements in generated responses on the Mul-\ntiWoZ 2.1 and SGD datasets, along with a case study and human\nevaluation that demonstrate enhancements in user satisfaction and\nsemantic quality. Looking ahead, we aim to further explore different\nforms of interaction between the LLM and TOD systems, such as\nexplanatory reasons for satisfaction scores provided by LLM.\nUnlocking the Potential of User Feedback: Leveraging LLMs as User Simulators to Enhance Dialogue CIKM ‚Äô23, October 21‚Äì25, 2023, Birmingham, United Kingdom\nREFERENCES\n[1] Vinsen Marselino Andreas, Genta Indra Winata, and Ayu Purwarianti. 2021. A\ncomparative study on language models for task-oriented dialogue systems. In\n2021 8th International Conference on Advanced Informatics: Concepts, Theory and\nApplications (ICAICTA). IEEE, 1‚Äì5.\n[2] Namo Bang, Jeehyun Lee, and Myoung-Wan Koo. 2023. Task-Optimized\nAdapters for an End-to-End Task-Oriented Dialogue System. arXiv preprint\narXiv:2305.02468 (2023).\n[3] Pawe≈Ç Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva,\nStefan Ultes, Osman Ramadan, and Milica Ga≈°iƒá. 2018. MultiWOZ‚Äìa large-scale\nmulti-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv\npreprint arXiv:1810.00278 (2018).\n[4] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[5] Yang Deng, Wenxuan Zhang, Wai Lam, Hong Cheng, and Helen Meng. 2022.\nUser Satisfaction Estimation with Sequential Dialogue Act Modeling in Goal-\noriented Conversational Systems. In Proceedings of the ACM Web Conference 2022 .\n2998‚Äì3008.\n[6] Pierre Erbacher, Ludovic Denoyer, and Laure Soulier. 2022. Interactive query\nclarification and refinement via user simulation. In Proceedings of the 45th In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 2420‚Äì2425.\n[7] Yue Feng, Yunlong Jiao, Animesh Prasad, Nikolaos Aletras, Emine Yilmaz, and\nGabriella Kazai. 2023. Schema-Guided User Satisfaction Modeling for Task-\nOriented Dialogues. arXiv preprint arXiv:2305.16798 (2023).\n[8] Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu,\nPeng Jiang, Min Yang, Fei Huang, Luo Si, et al. 2022. Galaxy: A generative pre-\ntrained model for task-oriented dialog with semi-supervised learning and explicit\npolicy injection. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 36. 10749‚Äì10757.\n[9] Vojtƒõch Hudeƒçek and Ond≈ôej Du≈°ek. 2023. Are LLMs All You Need for Task-\nOriented Dialogue? arXiv preprint arXiv:2304.06556 (2023).\n[10] Daniel Kahneman. 2011. Thinking, fast and slow . macmillan.\n[11] To Eun Kim and Aldo Lipani. 2022. A multi-task based neural model to simulate\nusers in goal oriented dialogue systems. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n2115‚Äì2119.\n[12] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916 (2022).\n[13] Yohan Lee. 2021. Improving end-to-end task-oriented dialog system with a simple\nauxiliary task. In Findings of the Association for Computational Linguistics: EMNLP\n2021. 1296‚Äì1303.\n[14] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan.\n2022. Controllable Dialogue Simulation with In-context Learning. In Findings\nof the Association for Computational Linguistics: EMNLP 2022 . Association for\nComputational Linguistics, Abu Dhabi, United Arab Emirates, 4330‚Äì4347. https:\n//doi.org/10.18653/v1/2022.findings-emnlp.318\n[15] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng\nYan. 2023. Guiding Large Language Models via Directional Stimulus Prompting.\narXiv preprint arXiv:2302.11520 (2023).\n[16] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out . 74‚Äì81.\n[17] Donovan Ong, Jian Su, Bin Chen, Anh Tuan Luu, Ashok Narendranath, Yue Li,\nShuqi Sun, Yingzhan Lin, and Haifeng Wang. 2022. Is discourse role important\nfor emotion recognition in conversation?. In Proceedings of the AAAI Conference\non Artificial Intelligence , Vol. 36. 11121‚Äì11129.\n[18] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . 311‚Äì318.\n[19] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav\nKhaitan. 2020. Towards scalable multi-domain conversational agents: The schema-\nguided dialogue dataset. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 8689‚Äì8696.\n[20] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[21] Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. 2022. Context-\naware language modeling for goal-oriented dialogue systems. arXiv preprint\narXiv:2204.10198 (2022).\n[22] Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi\nZhang. 2021. Multi-task pre-training for plug-and-play task-oriented dialogue\nsystem. arXiv preprint arXiv:2109.14739 (2021).\n[23] Weiwei Sun, Shuo Zhang, Krisztian Balog, Zhaochun Ren, Pengjie Ren, Zhumin\nChen, and Maarten de Rijke. 2021. Simulating user satisfaction for the evaluation\nof task-oriented dialogue systems. In Proceedings of the 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval . 2499‚Äì\n2506.\n[24] Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. 2020. Modelling\nhierarchical structure between dialogue policy and natural language genera-\ntor with option framework for task-oriented dialogue system. arXiv preprint\narXiv:2006.06814 (2020).\n[25] Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, and Jianxing Yu. 2020. Multi-\ndomain dialogue acts and response co-generation.arXiv preprint arXiv:2004.12363\n(2020).\n[26] Jie Wei, Guanyu Hu, Luu Anh Tuan, Xinyu Yang, and Wenjing Zhu. 2023. Multi-\nScale Receptive Field Graph Model for Emotion Recognition in Conversations. In\nICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) . IEEE, 1‚Äì5.\n[27] Fanghua Ye, Zhiyuan Hu, and Emine Yilmaz. 2023. Modeling User Satisfaction\nDynamics in Dialogue via Hawkes Process. arXiv preprint arXiv:2305.12594\n(2023).\n[28] Xiao Yu, Qingyang Wu, Kun Qian, and Zhou Yu. 2022. Reinforced Language\nModeling for End-to-End Task Oriented Dialog. arXiv preprint arXiv:2211.16773\n(2022).\n[29] Saber Zerhoudi, Sebastian G√ºnther, Kim Plassmeier, Timo Borst, Christin Seifert,\nMatthias Hagen, and Michael Granitzer. 2022. The SimIIR 2.0 Framework: User\nTypes, Markov Model-Based Interaction Simulation, and Advanced Query Gen-\neration. In Proceedings of the 31st ACM International Conference on Information &\nKnowledge Management . 4661‚Äì4666.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7232693433761597
    },
    {
      "name": "Task (project management)",
      "score": 0.6923381686210632
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.5274897813796997
    },
    {
      "name": "Language model",
      "score": 0.42548075318336487
    },
    {
      "name": "Simulation",
      "score": 0.3740881681442261
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34152117371559143
    },
    {
      "name": "Engineering",
      "score": 0.10726600885391235
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}