{
    "title": "Performance of single-agent and multi-agent language models in Spanish language medical competency exams",
    "url": "https://openalex.org/W4410176137",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5008087407",
            "name": "Fernando Altermatt",
            "affiliations": [
                "Pontificia Universidad Católica de Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5083484581",
            "name": "Andrés Neyem",
            "affiliations": [
                "Agencia Nacional de Investigación y Desarrollo",
                "Pontificia Universidad Católica de Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5117469171",
            "name": "Nicolás I. Sumonte",
            "affiliations": [
                "Agencia Nacional de Investigación y Desarrollo",
                "Pontificia Universidad Católica de Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5033526663",
            "name": "Marcelo Mendoza",
            "affiliations": [
                "Agencia Nacional de Investigación y Desarrollo",
                "Millennium Institute for Integrative Biology",
                "Pontificia Universidad Católica de Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5003074639",
            "name": "Ignacio Villagrán",
            "affiliations": [
                "Pontificia Universidad Católica de Chile"
            ]
        },
        {
            "id": "https://openalex.org/A5033077117",
            "name": "Héctor J. Lacassie",
            "affiliations": [
                "Pontificia Universidad Católica de Chile"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4389148698",
        "https://openalex.org/W4391899415",
        "https://openalex.org/W4396718668",
        "https://openalex.org/W4404781813",
        "https://openalex.org/W4399911310",
        "https://openalex.org/W4319341091",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4391188705",
        "https://openalex.org/W4393069045",
        "https://openalex.org/W3134225570",
        "https://openalex.org/W4402480882",
        "https://openalex.org/W4388822338",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W6851275496",
        "https://openalex.org/W6846739993",
        "https://openalex.org/W4362656036",
        "https://openalex.org/W4321162379",
        "https://openalex.org/W4406892424",
        "https://openalex.org/W4387973771",
        "https://openalex.org/W4402670791",
        "https://openalex.org/W4395659193",
        "https://openalex.org/W4389156617",
        "https://openalex.org/W4404783265",
        "https://openalex.org/W4387829826",
        "https://openalex.org/W4393318738",
        "https://openalex.org/W4394677717",
        "https://openalex.org/W4399354087",
        "https://openalex.org/W3049647566",
        "https://openalex.org/W4407632178"
    ],
    "abstract": "Abstract Background Large language models (LLMs) like GPT-4o have shown promise in advancing medical decision-making and education. However, their performance in Spanish-language medical contexts remains underexplored. This study evaluates the effectiveness of single-agent and multi-agent strategies in answering questions from the EUNACOM, a standardized medical licensure exam in Chile, across 21 medical specialties. Methods GPT-4o was tested on 1,062 multiple-choice questions from publicly available EUNACOM preparation materials. Single-agent strategies included Zero-Shot, Few-Shot, Chain-of-Thought (CoT), Self-Reflection, and MED-PROMPT, while multi-agent strategies involved Voting, Weighted Voting, Borda Count, MEDAGENTS, and MDAGENTS. Each strategy was tested under three temperature settings (0.3, 0.6, 1.2). Performance was assessed by accuracy, and statistical analyses, including Kruskal–Wallis and Mann–Whitney U tests, were performed. Computational resource utilization, such as API calls and execution time, was also analyzed. Results MDAGENTS achieved the highest accuracy with a mean score of 89.97% (SD = 0.56%), outperforming all other strategies ( p &lt; 0 . 001). MEDAGENTS followed with a mean score of 87.99% (SD = 0.49%), and the CoT with Few-Shot strategy scored 87.67% (SD = 0.12%). Temperature settings did not significantly affect performance ( F 2 , 54 = 1 . 45, p = 0 . 24). Specialty-level analysis showed the highest accuracies in Psychiatry (95.51%), Neurology (95.49%), and Surgery (95.38%), while lower accuracies were observed in Neonatology (77.54%), Otolaryngology (76.64%), and Urology/Nephrology (76.59%). Notably, several exam questions were correctly answered using simpler single-agent strategies without employing complex reasoning or collaboration frameworks. Conclusions and relevance Multi-agent strategies, particularly MDAGENTS, significantly enhance GPT-4o’s performance on Spanish-language medical exams, leveraging collaboration to improve diagnostic accuracy. However, simpler single-agent strategies are sufficient to address many questions, high-lighting that only a fraction of standardized medical exams require sophisticated reasoning or multi-agent interaction. These findings suggest potential for LLMs as efficient and scalable tools in Spanish-speaking healthcare, though computational optimization remains a key area for future research.",
    "full_text": "Altermatt et al. BMC Medical Education          (2025) 25:666  \nhttps://doi.org/10.1186/s12909-025-07250-3\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nBMC Medical Education\nPerformance of single-agent \nand multi-agent language models in Spanish \nlanguage medical competency exams\nFernando R. Altermatt1*, Andres Neyem2,4, Nicolas Sumonte2,4, Marcelo Mendoza2,4,5, Ignacio Villagran2,3† and \nHector J. Lacassie1† \nAbstract \nBackground Large language models (LLMs) like GPT-4o have shown promise in advancing medical decision-making \nand education. However, their performance in Spanish-language medical contexts remains underexplored. This study \nevaluates the effectiveness of single-agent and multi-agent strategies in answering questions from the EUNACOM, \na standardized medical licensure exam in Chile, across 21 medical specialties.\nMethods GPT-4o was tested on 1,062 multiple-choice questions from publicly available EUNACOM preparation \nmaterials. Single-agent strategies included Zero-Shot, Few-Shot, Chain-of-Thought (CoT), Self-Reflection, and MED-\nPROMPT, while multi-agent strategies involved Voting, Weighted Voting, Borda Count, MEDAGENTS, and MDAGENTS. \nEach strategy was tested under three temperature settings (0.3, 0.6, 1.2). Performance was assessed by accuracy, \nand statistical analyses, including Kruskal–Wallis and Mann–Whitney U tests, were performed. Computational resource \nutilization, such as API calls and execution time, was also analyzed.\nResults MDAGENTS achieved the highest accuracy with a mean score of 89.97% (SD = 0.56%), outperforming all \nother strategies (p < 0.001). MEDAGENTS followed with a mean score of 87.99% (SD = 0.49%), and the CoT with Few-\nShot strategy scored 87.67% (SD = 0.12%). Temperature settings did not significantly affect performance (F2,54 = 1.45, \np = 0.24). Specialty-level analysis showed the highest accuracies in Psychiatry (95.51%), Neurology (95.49%), and Sur-\ngery (95.38%), while lower accuracies were observed in Neonatology (77.54%), Otolaryngology (76.64%), and Urology/\nNephrology (76.59%). Notably, several exam questions were correctly answered using simpler single-agent strategies \nwithout employing complex reasoning or collaboration frameworks.\nConclusions and relevance Multi-agent strategies, particularly MDAGENTS, significantly enhance GPT-4o’s perfor-\nmance on Spanish-language medical exams, leveraging collaboration to improve diagnostic accuracy. However, sim-\npler single-agent strategies are sufficient to address many questions, high-lighting that only a fraction of standardized \nmedical exams require sophisticated reasoning or multi-agent interaction. These findings suggest potential for LLMs \nas efficient and scalable tools in Spanish-speaking healthcare, though computational optimization remains a key area \nfor future research.\nKeywords Large language models, Medical decision-making, Spanish medical contexts, Medical AI., GPT-4o\n†Ignacio Villagran and Hector J. Lacassie contributed equally to this work.\n*Correspondence:\nFernando R. Altermatt\nfalterma@uc.cl\nFull list of author information is available at the end of the article\nPage 2 of 11Altermatt et al. BMC Medical Education          (2025) 25:666 \nBackground\nThe rapid evolution of large language models (LLMs) has \nrevolutionized the field of artificial intelligence, unlock -\ning capabilities with broad applications across numer -\nous disciplines [1–3]. In the medical domain, LLMs have \ndemonstrated significant promise in advancing medical \ndecision-making (MDM) by enhancing diagnostic accu -\nracy, personalizing therapeutic strategies, and optimizing \nresource utilization [4 –9]. For example, GPT-based sys -\ntems have been employed to generate clinical summaries, \nassist with complex diagnoses, and predict patient out -\ncomes from large datasets, fundamentally transforming \npatient care approaches [10–12]. In the context of medi -\ncal education, [13] assessed the performance of ChatGPT \nversions 3.5, 4, and 4 V on the EUNACOM, a key national \nmedical licensing exam in Chile. These evaluations dem -\nonstrated ChatGPT’s ability to pass the exam, albeit with \nvarying proficiency level across medical disciplines and \nmodel iterations. However, these results also underscored \nsignificant limitations, such as biases in AI training data \nand challenges in linguistic adaptability [13]. Language \nbarriers, in particular, remain a critical obstacle in apply -\ning LLMs to non-English contexts [14–16].\nSpanish, the second most spoken native language glob -\nally, represents a vast demographic that could greatly \nbenefit from innovations in AI-driven healthcare [17]. \nYet, current LLMs, predominantly trained on English-\nlanguage datasets, face substantial linguistic and cultural \nlimitations when applied to Spanish medical contexts. \nFor instance, studies like that of Guillen-Grima et  al. \n[18] show that GPT-4, despite achieving high scores on \nSpain’s Medical Residency Examination (MIR), struggled \nwith domain-specific nuances and multimodal questions. \nSimilarly, Vera [19] highlights the importance of linguistic \nconcordance in clinical communication, emphasizing the \nneed for AI systems that account for the complexities of \nSpanish medical terminology to improve patient-centered \ncare. These findings point to the need for robust adapta -\ntions of LLMs tailored to Spanish-speaking populations.\nThe architecture and configuration of LLMs also play \na critical role in their performance on high-stakes med -\nical tasks. Single-agent frameworks, such as GPT-4o, \nleverage advanced techniques like Chain-of-Thought \n(CoT) reasoning [20] and self-reflection [21– 26], and \nhave shown commendable performance in complex \nmedical exams such as the USMLE. However, single-\nagent models often falter when tasked with interdisci -\nplinary cases that demand collaborative reasoning [27, \n28]. In contrast, multiagent frameworks, exemplified \nby MEDAGENTS [29] and MDAGENTS [30], integrate \ndiverse perspectives through iterative discussions and \nvoting mechanisms among specialized agents. These \ncollaborative configurations mimic real-world medical \ndecision-making processes, enhancing robustness and \naccuracy, particularly in error-prone scenarios.\nIn this context, our study addresses a critical gap in \nthe evaluation of LLMs for Spanish-language medical \ncompetency exams. Specifically, we examine the perfor -\nmance of GPT-4o on the National Single Examination \nof Medical Knowledge (EUNACOM), a standardized \nassessment essential for medical licensure in Chile. \nThis exam presents a unique challenge as it evaluates \nspecialized medical knowledge in Spanish, offering a \nrigorous testbed for assessing the efficacy of LLMs in \nnon-English healthcare contexts.\nThrough a comparative analysis of single-agent and \nmultiagent configurations, we investigate key perfor -\nmance metrics, including accuracy, response time, and \ncomputational efficiency, as measured by API calls. \nAdvanced prompting techniques, such as CoT and \nMEDPROMPT, are employed for single-agent mod -\nels, while collaborative frameworks are used to evalu -\nate multiagent systems. This approach allows us to \nelucidate the strengths and limitations of these con -\nfigurations and to identify strategies that enhance the \nadaptability of LLMs in Spanish-speaking medical sce -\nnarios. By contributing to the broader discourse on \nadapting AI systems for diverse linguistic and cultural \ncontexts, this research seeks to advance the integration \nof LLMs into global healthcare.\nUltimately, this study bridges a critical gap in the \nunderstanding of LLM applications in Spanish medi -\ncal education and lays the groundwork for future \ninnovations in multilingual AI-driven healthcare. By \naddressing the linguistic and methodological challenges \ninherent in adapting LLMs for Spanish-speaking popu -\nlations, we aim to catalyze the development of inclu -\nsive and effective AI tools that align with the needs of \ndiverse healthcare systems worldwide.\nMethods\nIn this study, we investigated the performance of GPT-\n4o on the National Single Examination of Medical \nKnowledge (EUNACOM), a stringent medical licen -\nsure assessment administered in Chile. We evaluated \nthe model’s single-agent and multiagent configurations \nand examined how different prompting strategies and \ncollaborative framework influenced accuracy, consist -\nency, adaptability, and response times. Additionally, we \nassessed the impact of various temperature settings to \nunderstand how response variability and uncertainty \naffect performance in a Spanish-language, domain-spe -\ncific medical context.\nPage 3 of 11\nAltermatt et al. BMC Medical Education          (2025) 25:666 \n \nEUNACOM as a Benchmark for medical competency\nThe EUNACOM is a high-stakes, standardized test \ndesigned to validate the medical knowledge of physicians \npracticing in Chile. It encompasses a broad range of mul -\ntiple-choice questions (MCQs) covering internal medi -\ncine, surgery, pediatrics, obstetrics and gynecology, and \nother core medical domains. Its comprehensive scope \nand complexity make it an ideal benchmark for evaluat -\ning the capabilities of large language models in special -\nized, context-rich medical scenarios.\nFor this study, we curated a dataset of 1,062 publicly \navailable and previously administered EUNACOM ques -\ntions. All items underwent preprocessing, including \northographic standardization and exclusion of any image-\nbased or multimodal content, to ensure compatibility and \nconsistency. Answer choices were randomized for each \nrun to minimize ordering biases and enhance robustness. \nEach question in the dataset included a predefined cor -\nrect answer that served as the evaluation key. The assess -\nment is based on multiple-choice questions (MCQs); \ntherefore, no metrics are used to compare open-ended \ntext responses. It is a multiple-choice format The evalua -\ntion process was fully automated, comparing model-gen -\nerated answers against these established correct answers. \nThis careful curation and automated assessment method-\nology ensured that the resulting dataset provided a bal -\nanced, unbiased platform for model evaluation.\nExperimental design\nWe evaluated GPT-4o under two primary configurations: \na single-agent setting and a multiagent framework. Each \nconfiguration was tested across three temperature condi -\ntions (0.3, 0.6, and 1.3) to capture the effects of varying \nresponse diversity and uncertainty on performance. The \nchosen temperature values reflect a range of response \nbehaviors, from deterministic outputs to more explora -\ntory and diverse responses. At the lower end, a temper -\nature of 0.3 minimizes randomness, encouraging highly \ndeterministic and precise outputs that are critical for \nhigh-stakes tasks like medical licensure exams. This set -\nting evaluates the model’s intrinsic confidence and con -\nsistency. A medium temperature setting of 0.6 introduces \na balance between focused reasoning and some degree \nof exploration, simulating real-world scenarios where \nnuanced reasoning or multiple valid approaches may be \napplicable. Finally, a higher temperature of 1.3 maximizes \ndiversity in responses, fostering exploration and adapt -\nability, which is particularly useful for understanding the \nmodel’s robustness in handling ambiguous or unconven -\ntional questions. By using this range of temperatures, \nwe aimed to assess the model’s stability and adaptability \nunder varying degrees of response variability.\nEach configuration was tested twice at each tempera -\nture setting to ensure robust and reliable results. These \nexperiments provided a comprehensive evaluation of \nthe model’s capacity to adapt its reasoning processes \nacross deterministic and stochastic conditions.\nSingle‑agent configuration\nIn the single-agent setup, we explored various prompt -\ning techniques to assess the model’s capacity to handle \nspecialized medical knowledge. Zero-shot prompting \nwas used to present each question without additional \ncontext, relying solely on the model’s pretrained \nknowledge [31]. Few-shot prompting involved pro -\nviding a small set of example question–answer pairs \nbefore each query, which refined the model’s reason -\ning within the specific domain [32]. Advanced tech -\nniques incorporated chain-of-thought (CoT) reasoning \n[20], prompting the model to explicitly outline its rea -\nsoning steps. This approach was further coupled with \nself-reflection [21], allowing the model to review and \nrefine its responses. Additionally, combinations of CoT \nwith few-shot examples and the MEDPROMPT [33] \nmethod, designed specifically for medical contexts, \nwere evaluated.\nAll single-agent strategies were tested across the three \ntemperature settings, enabling a detailed analysis of the \neffects of controlled variability on accuracy and reliabil -\nity. A comprehensive description of these techniques is \nprovided in Appendix A.\nMultiagent configuration\nThe multiagent framework emulated a collaborative \ndiagnostic environment, featuring multiple instances \nof GPT-4o working in concert. The strategies included \nsimple voting, where six independent agents provided \nresponses, and the majority vote determined the final \nanswer. Weighted voting incorporated confidence-based \nweighting schemes, such as the Borda count [34], to \nreflect agent certainty in the consensus. Role-specific \nframeworks, such as MEDAGENTS, assigned agents \ndomain-specific roles (e.g., cardiologist, pediatrician), \nmimicking interdisciplinary collaboration in clinical \npractice [29]. The MDAGENTS configuration further \nadapted agent reasoning strategies to the diagnostic or \ntherapeutic context, enhancing the relevance and accu -\nracy of responses [30].\nEach multiagent strategy was tested across the same \ntemperature settings to directly compare their per -\nformance with single-agent configurations. Detailed \ndescriptions of the frameworks, including voting mecha -\nnisms and agent roles, are provided in Appendix B.\nPage 4 of 11Altermatt et al. BMC Medical Education          (2025) 25:666 \nLanguage model and implementation\nAll experiments employed OpenAI’s GPT-4o, a state-of-\nthe-art multilingual language model with demonstrated \nproficiency in Spanish. We used the base version with -\nout additional fine-tuning to provide an out-of-the-box \nevaluation. The responses were limited to 3,048 tokens to \nensure completeness while avoiding excessive verbosity. \nExperiments were conducted on a cloud-based platform \nvia Python 3.8 and the langchain library, leveraging the \nOpenAI API to facilitate reproducibility and scalability.\nEvaluation metrics and statistical analyses\nWe performed all analyses on the dataset of 1,062 \nEUNACOM questions (n = 1, 062). Accuracy, defined \nas the proportion of correctly answered questions, was \nthe primary outcome measure, reported as the mean ± \nstandard deviation (s.d.). Additionally, we recorded the \naverage number of API calls and the mean response time \nper query to assess computational resources and execu -\ntion speed.\nThe normality of the data distribution was tested using \nthe Shapiro–Wilk test, which indicated that the data were \nnot normally distributed (p < 0.05). As a result, nonpara -\nmetric statistical tests were employed. The Kruskal–Wal-\nlis test was used to compare accuracy across strategies \nand conditions, with an alpha level (α) set at 0.05. Post \nhoc pairwise comparisons were conducted using the \nMann–Whitney U test, with the Benjamini–Hochberg \nprocedure applied to correct for multiple comparisons.\nAll statistical analyses were performed in R (version \n4.2.2) using open-source packages. The statistical codes, \nsubsets of the data, and detailed p values are provided in \nAppendix C.\nEthical and regulatory considerations\nThis work adhered to established ethical guidelines \ngoverning standardized examination materials. All \nEUNACOM items were anonymized, and the dataset did \nnot contain any patient-identifying information. Since \nthis research did not involve human participants, institu -\ntional ethical review was not needed.\nReproducibility\nTo ensure robust and reproducible results, all the experi -\nmental conditions were standardized, and each con -\nfiguration was repeated twice at each temperature. \nAppendices A and B provide comprehensive documenta -\ntion of prompts and configurations, enabling independ -\nent verification and replication of the findings.\nResults\nOverall performance\nOur evaluation of GPT-4o on the EUNACOM exami -\nnation revealed substantial differences in performance \nacross the tested strategies and configurations (Table  1). \nMultiagent frameworks consistently outperform sin -\ngle-agent methods, with MDAGENTS emerging as the \ntop performer, as illustrated in Figure  1a. MDAGENTS \nachieved a mean accuracy of 89.97% (SD = 0.56%), sig -\nnificantly surpassing simpler approaches, including zero-\nshot (85.90%, SD = 0.32%) and self-reflection (85.38%, SD \n= 0.22%) (all adjusted p < 0.01).\nSingle-agent approaches that incorporate prompt engi -\nneering and guided reasoning—such as few-shot (86.88%, \nSD = 0.40%), MEDPROMPT (86.96%, SD = 0.44%), and \nCoT (86.86%, SD = 0.37%)—consistently outperform \nzero-shot and self-reflection (all adjusted p < 0.01 for key \ncomparisons). Notably, the CoT + few-shot configuration \nachieved exceptional consistency (SD = 0.12%), high -\nlighting the benefits of structured reasoning strategies.\nPairwise comparisons and statistical significance\nPairwise comparisons confirmed that advanced reason -\ning and multiagent approaches yield substantial benefits \nTable 1 Performance metrics for all evaluated strategies on the EUNACOM Exam. Mean scores, standard deviations (SD), API calls, and \nmean completion time (in sec- onds) are shown\nCategory Strategy Accuracy (Mean % ± SD) API Calls Time (s)\nSingle-agent COT + Few-Shot Few-Shot 87.67% ± 0.12% 86.88% ± 0.40% 1.00 1.00 1.74 1.61\nCoT MEDPROMPT 86.86% ± 0.37% 1.00 2.26\n86.96% ± 0.44% 1.00 2.95\nSELF-REFLECTION 85.38% ± 0.22% 2.65 4.15\nZERO-SHOT 85.90% ± 0.32% 1.00 1.53\nMDAGENTS 89.97% ± 0.56% 21.14 192.44\nMEDAGENTS 87.99% ± 0.49% 17.00 63.95\nMulti-agent VOTING 87.22% ± 0.31% 6.00 12.51\nBORDA COUNT 86.70% ± 0.18% 6.00 13.03\nWeighted Voting 86.68% ± 0.18% 6.00 12.43\nPage 5 of 11\nAltermatt et al. BMC Medical Education          (2025) 25:666 \n \nFig. 1 Performance metrics for all evaluated strategies on the EUNACOM exam\nPage 6 of 11Altermatt et al. BMC Medical Education          (2025) 25:666 \nover baseline techniques, as shown in Figure  1b. For \nexample, MDAGENTS outperforms zero-shot by an \naverage margin of 4.06 percentage points (adjusted p \n< 0.01) and self-reflection by 4.60 percentage points \n(adjusted p < 0.01). CoT + few-shot, MEDAGENTS, and \nother enhanced methods also achieved statistically sig -\nnificant improvements, further emphasizing the value of \nstructured prompting and collaborative reasoning.\nRobustness to temperature variation\nNeither the single-agent nor the multiagent strategies \nshowed significant sensitivity to temperature adjust -\nments (0.3, 0.6, 1.3), as evidenced by the Kruskal–Wallis \ntest results (p > 0.05 for all comparisons). This stabil -\nity underscores the robustness of the observed perfor -\nmance improvements, demonstrating their resilience to \nvariations in sampling stochasticity and enhancing their \nreliability across diverse operational contexts. The perfor-\nmance distributions under different temperature settings \nare visualized in Figure  2. Detailed statistical results, \nincluding pairwise comparisons and Kruskal–Wallis tests \nfor configurations, are provided in Appendix C.\nComputational considerations and consistency\nWhile multi-agent strategies offer superior accuracy, they \nrequire greater computational resources. In contrast to \nsingle-agent methods such as Zero-Shot and CoT, which \nrequire only a single API call and complete queries within \nseconds, MDAGENTS averages 21.14 API calls and \napproximately 192 seconds per experiment. CoT + few-\nshot combined strong accuracy (87.67%, SD = 0.12%) \nwith minimal computational overhead, representing an \noptimal balance between performance and efficiency. \nFigure 3 illustrates all configurations’ trade-offs between \naccuracy and computational requirements.\nPerformance by topic\nA detailed analysis of performance by medical specialty \nwas conducted to assess how each strategy performed \nacross different medical domains. Table  2 presents the \naverage accuracy of all the strategies in each medical \narea. The results indicate that the highest accuracies were \nachieved in specialties such as psychiatry (95.51%), neu -\nrology (95.49%), and surgery (95.38%). The accuracies of \nspecialties such as neonatology (77.54%), otolaryngol -\nogy (76.64%), and urology and nephrology (76.59%) were \nlower. These variations suggest that the model performs \nbetter in certain medical domains, possibly because of \nthe complexity of the subject matter or the availability of \ntraining data in those areas. Understanding performance \nby topic is crucial for identifying the strengths and limi -\ntations of the model in various medical contexts, as it can \nguide targeted improvements in areas where the model \nunderperforms. Appendix D provides further details on \nthe accuracy distributions by specialty.\nFig. 2 Boxplot of strategy performance across varying temperature settings. Stability across all configurations highlights the robustness \nof the methods\nPage 7 of 11\nAltermatt et al. BMC Medical Education          (2025) 25:666 \n \nKey insights\nThese findings highlight the substantial benefits of col -\nlaborative, multiagent frameworks such as MDAGENTS, \nwhich leverage distributed reasoning and specialized \nroles to achieve superior accuracy in complex medi -\ncal assessments. Moreover, strategies such as the CoT \n+ few-shot configuration demonstrate that carefully \ncrafted prompts can balance high reliability with mini -\nmal computational overhead. Although performance \nacross specialties generally remains strong, certain \ndomains continue to pose challenges, suggesting that \nfurther refinements may be necessary to ensure consist -\nently high accuracy in all medical areas. Moreover, the \ndemonstrated robustness to temperature variations and \nthe availability of efficient yet effective approaches under-\nscore the practical potential of these large language mod -\nels in real-world educational and clinical contexts.\nDiscussion\nThis study underscores the growing potential of large \nlanguage models (LLMs), specifically GPT-4o, to sup -\nport medical competency evaluations in Spanish-lan -\nguage contexts. By benchmarking the performance of \nboth single-agent and multiagent configurations on the \nEUNACOM—a high-stakes licensure exam—we demon -\nstrate that LLMs can serve as scalable, accurate tools for \neducational and diagnostic tasks. Each strategy presents \nFig. 3 Scatter plot of mean accuracy versus computational requirements (API calls) for all evaluated strategies\nTable 2 Average accuracy by medical specialty\nSpecialty Average \nAccuracy \n(%)\nCardiology 87.73\nSurgery 95.38\nDermatology 92.00\nEndocrinology 86.97\nGastroenterology 92.39\nGynecology 88.61\nHematology and Oncology 86.29\nInfectious Diseases 87.20\nNephrology 87.65\nNeonatology 77.54\nNeurology 95.49\nObstetrics 86.89\nOphthalmology 82.23\nOtolaryngology 76.64\nPediatrics 86.52\nPsychiatry 95.51\nRespiratory Medicine 80.80\nRheumatology 85.23\nPublic Health 80.66\nTraumatology 83.36\nUrology 88.17\nUrology and Nephrology 76.59\nPage 8 of 11Altermatt et al. BMC Medical Education          (2025) 25:666 \ndistinct advantages depending on task complexity, resource \nconstraints, and application setting.\nPerformance insights and implications\nOur results show that multiagent frameworks—particu -\nlarly MDAGENTS—significantly outperform single-\nagent strategies, achieving the highest mean accuracy \n(89.97%, SD = 0.56%). These findings support prior litera-\nture emphasizing the advantages of distributed reasoning \nin AI systems, where collaboration among specialized \nagents enhances interpretive depth and robustness [30]. \nBy simulating interdisciplinary clinical reasoning, multia-\ngent frameworks mirror real-world medical decision-\nmaking and offer superior performance in complex cases.\nNonetheless, certain single-agent approaches dem -\nonstrated competitive accuracy with substantially lower \ncomputational costs. The CoT + few-shot configura -\ntion, for instance, reached 87.67% accuracy (SD = 0.12%) \nwith minimal API usage, offering a practical alternative \nfor scenarios with limited computational infrastructure. \nImportantly, many exam questions were answered cor -\nrectly using simpler prompting techniques, suggesting \nthat complex reasoning is not universally required across \nthe EUNACOM. This highlights an opportunity to selec -\ntively apply advanced strategies where they are most \nimpactful—reserving collaborative methods for more \nambiguous or interdisciplinary cases.\nRobustness and generalizability\nThe consistency of performance across different temper -\nature settings (0.3, 0.6, 1.3) reinforces the robustness of \nboth single- and multiagent strategies. The lack of signifi-\ncant performance fluctuations suggests that GPT-4o can \nmaintain accuracy under variable stochastic conditions—\na crucial feature for high-stakes, real-world applications.\nHowever, performance varied across medical special -\nties. High accuracies in psychiatry, neurology, and sur -\ngery suggest these domains are well-represented in the \nmodel’s training data or better suited to LLM reasoning. \nConversely, lower performance in neonatology, otolar -\nyngology, and urology may reflect domain complexity, \nlimited data exposure, or nuanced terminology. These \ndiscrepancies highlight areas for targeted model refine -\nment or domain-specific tuning to ensure equitable per -\nformance across all specialties.\nComputational trade‑offs and practical applications\nWhile multiagent strategies yielded the best results, their \nimplementation comes at a computational cost. MDA -\nGENTS required an average of 21.14 API calls and 192 \nseconds per experiment, compared to just one call for \nZero-Shot or CoT strategies. These resource demands \nmay limit scalability, particularly in low-resource settings \nor real-time applications.\nGiven this trade-off, selecting an appropriate con -\nfiguration should be guided by use-case constraints. In \nhigh-stakes clinical simulations or policy development, \nmultiagent frameworks may be justified. In contrast, for \nmedical education or exam preparation, high-performing \nsingle-agent strategies (e.g., CoT + few-shot) may offer \na more efficient alternative without compromising accu -\nracy. This flexibility enables institutions to match LLM \ndeployment to infrastructure capabilities and pedagogi -\ncal goals.\nEducational applications\nLLMs offer transformative opportunities for medical \neducation, particularly in Spanish-speaking regions. \nGPT-4o and similar models can serve as interactive \ntutors, facilitating case-based learning and immediate \nfeedback—a pedagogical approach shown to improve \nclinical reasoning skills [35, 36]. Beyond tutoring, LLMs \ncan assist in generating high-quality multiple-choice \nquestions and answer rationales, supporting scalable and \nstandardized assessment development [37].\nOur research validates these educational applications \nthrough GPT-4o’s strong performance on the EUNA -\nCOM examination. The differential accuracy across spe -\ncialties identifies domains where these tools could be \nmost effectively implemented in Spanish-language cur -\nricula, while also highlighting areas requiring additional \neducator guidance.\nAs Li et  al. [38] emphasize, these technologies are \nredefining medical educator roles from conventional \nknowledge transmitters to learning navigators who guide \ncritical thinking and information evaluation. This transi -\ntion is especially valuable in Spanish speaking contexts, \nwhere LLMs can help address disparities in clinical edu -\ncational resources through virtual clinical scenarios and \npersonalized learning paths [38].\nOur comparison of prompting strategies offers practi -\ncal implementation pathways for educators. The strong \nperformance of computationally efficient approaches \nsuggests that even institutions with limited resources \ncan effectively implement LLM-assisted education—a \ncritical consideration for many Spanish-speaking medical \nschools.\nAdaptive learning environments, where AI-generated \ncontent dynamically responds to learner progress, can \npersonalize education in ways not previously possible. \nThis is especially valuable in resource-limited settings, \nwhere faculty shortages and grading inconsistencies may \nhinder quality instruction. By demonstrating reliable per-\nformance on a standardized Spanish-language exam, our \nPage 9 of 11\nAltermatt et al. BMC Medical Education          (2025) 25:666 \n \nstudy contributes to the validation of LLMs in such edu -\ncational roles.\nFuture research should explore the longitudinal impact \nof LLM-based tools on medical training, knowledge \nretention, and clinical performance in Spanish-speaking \nhealthcare systems, as well as their integration into for -\nmal curricula.\nLanguage considerations in Spanish medical contexts\nWe acknowledge that regional variations exist within the \nSpanish language, including differences in vocabulary, \nidioms, and local expressions. However, in the context \nof standardized medical examinations such as EUNA -\nCOM, the language used tends to be linguistically neu -\ntral. This neutrality is a characteristic feature of medical \ndiscourse, resulting from shared international curricula, \nscientific literature, and the technical nature of medical \nterminology, which largely transcends regional dialectical \nvariations.\nWhile the specific impact of Spanish dialectical varia -\ntions on medical examination performance has not been \nextensively studied, related research in other languages \nsuggests minimal effects. A randomized controlled trial \nby Kozato et. al [39]demonstrated that non-native Eng -\nlish accents had no statistically significant effect on \nexaminers’ scores in Objective Structured Clinical Exam -\ninations (OSCEs). As noted in their findings, examiners \nwere not biased either positively or negatively towards \n[non-native English accents] when providing checklist or \nglobal scores [39]. Although this study was conducted in \nEnglish, it suggests that in standardized medical assess -\nment contexts, linguistic variations may not significantly \nimpact performance outcomes.\nNevertheless, we recognize that in broader health -\ncare delivery contexts, particularly in direct patient care \nsettings, linguistic variations can be highly relevant. A \nrecent review [40] highlights that the use of local lan -\nguages in healthcare delivery improves metrics such as \npatient satisfaction, compliance with medical instruc -\ntions, and health improvement. This underscores the \nimportance of considering dialectical variations in clini -\ncal practice, even if their impact on standardized exami -\nnations may be limited.\nThe version of Spanish used in this study aligns with \nwhat is typically understood as Latin American Span -\nish—common across much of Central and South Amer -\nica, including regions such as Mexico and the Caribbean. \nGiven the standardized nature of the EUNACOM exam, \nwe believe the LLM’s ability to generalize across Spanish-\nspeaking regions is likely to remain robust, especially in \nformal, medical contexts. It should be noted that dialectal \nvariation, even within standardized Spanish, constitutes a \nlimitation of the study. This is particularly relevant given \nthat we do not currently have access to standardized \nexams from other Spanish-speaking countries to conduct \ncomparative analyses.\nFuture work may explore this question more directly \nby evaluating LLMs across distinct Spanish dialects and \nclinical terminologies used in different Spanish-speak -\ning countries. Such analyses would further enhance the \nadaptability and inclusiveness of AI models in global \nhealth education and practice, particularly as they \nextend beyond standardized examinations to clinical \napplications.\nConclusion\nThis study demonstrates the capability of GPT-4o in tack-\nling the challenges of Spanish-language medical licensing \nexaminations, specifically the EUNACOM. By evaluating \nsingle-agent and multiagent configurations, we provide \nvaluable insights into the trade-offs between accuracy, \ncomputational efficiency, and robustness.\nMultiagent frameworks, particularly MDAGENTS, \nemerged as the most accurate strategy, showcasing the \nadvantages of collaborative reasoning and specialized \nroles. However, single-agent configurations like CoT + \nfew-shot provide a compelling alternative, delivering high \naccuracy with minimal computational overhead. These \nfindings highlight the potential of LLMs to enhance \nmedical education and assessment while emphasizing \nthe need for optimizing models for practical and scalable \napplications.\nFuture research should focus on addressing the identi -\nfied gaps in performance across medical specialties and \nfurther refining multiagent frameworks to reduce the \ncomputational demands. Additionally, expanding the \ntraining datasets to better represent Spanish-language \nmedical contexts will be crucial for improving the mod -\nels’ linguistic and cultural adaptability.\nUltimately, this study contributes to the broader dis -\ncourse on leveraging LLMs for multilingual healthcare \napplications, advocating for inclusive and context-sen -\nsitive AI systems that can benefit diverse populations \nglobally.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12909- 025- 07250-3.\nSupplementary Material 1. Our article includes an appendix section as \nsupplementary material for additional context. \nAcknowledgements\nThe authors would like to acknowledge the support of the National Research \nand Development Agency (ANID) and the National Center for Artificial Intel-\nligence (CENIA) for providing the resources and funding necessary to conduct \nPage 10 of 11Altermatt et al. BMC Medical Education          (2025) 25:666 \nthis research. Special thanks are extended to the entire research team for their \ncollaboration and dedication throughout the study.\nF.R.A., N.I.S., and A.N. had full access to the data and take responsibility for the \nintegrity of the data and the accuracy of the data analysis. The lead author \naffirms that this manuscript is an honest, accurate, and transparent account of \nthe study being reported, and that no important aspects have been omitted.\nCode availability\nThe code used to generate or process the reported results can be obtained \nfrom the corresponding author upon reasonable request. Access to raw code \nand computational logs may be subject to approval and specific conditions to \nensure appropriate use.\nAuthors’ contributions\nF.R.A., A.N., and N.I.S. received the funding and conceptualized the study. \nF.R.A., A.N., N.I.S., and M.M. contributed to methodology development, data \nanalysis, and interpretation of the results. F.R.A. and N.I.S. were responsible for \ndrafting the manuscript. F.R.A., A.N., and M.M. critically revised the manuscript \nfor important intellectual content. All authors reviewed and approved the final \nmanuscript for submission.\nFunding\nThis work was partially supported by the National Research and Development \nAgency (ANID), FONDEF IDeA ID23I10319. The contributions of A.N., N.I.S., and \nM.M. were supported in part by the National Center for Artificial Intelligence \nunder Grant FB210017, Basal ANID.\nThe study sponsors had no role in the study design, data collection, analysis, \ninterpretation, or manuscript preparation. The investigators were independent \nfrom the funders.\nData availability\nThe datasets generated and/or analyzed during this study are partially avail-\nable. Experimental protocols, including detailed prompting strategies and \nconfigurations for both single-agent and multiagent setups, are included \nin the Appendix. The raw performance data (accuracy scores, API call logs, \nexecution times) and GPT-4 interaction transcripts are available from the cor-\nresponding author upon reasonable request. Access to these data requires a \nformal request outlining the intended use and adherence to data usage terms. \nThe statistical analysis results and the corresponding results are provided \nin the appendix. This study did not involve human subjects or clinical trials; \ntherefore, trial registration is not applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1 Division of Anesthesiology, School of Medicine, Pontificia Universidad \nCatólica de Chile, Marcoleta 377, 8320000 Santiago, RM, Chile. 2 Department \nof Computer Science, School of Engineering, Pontificia Universidad Católica \nde Chile, Vicuña Mackenna 6840, 7820436 Santiago, RM, Chile. 3 Department \nof Kinesiology, Health Sciences School, Pontificia Universidad Católica de \nChile, Vicuña Mackenna 6840, 7820436 Santiago, RM, Chile. 4 National Center \nfor Artificial Intelligence (CENIA), National Research and Development Agency \n(ANID), Vicuña Mackenna 6840, 7820436 Santiago, RM, Chile. 5 Millennium \nInstitute for Foundational Research On Data (IMFD), Pontificia Universidad \nCatólica de Chile, Vicuña Mackenna 6840, 7820436 Santiago, RM, Chile. \nReceived: 10 January 2025   Accepted: 28 April 2025\nReferences\n 1. Chowdhery A, et al. Palm: scaling language modeling with pathways. J \nMachine Learn Res 24, 1–113 (2023). URL http:// jmlr. org/ papers/ v24/ 22- \n1144. html.\n 2. Anil R, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403 \n(2023).\n 3. OpenAI et al. Gpt-4 technical report (2024). URL https:// arxiv. org/ abs/ \n2303. 08774. 2303. 08774.\n 4. Mu X, et al. Comparison of large language models in management \nadvice for melanoma: Google’s ai bard, bingai and chatgpt. Skin Health \nDis. 2024;4:ski2-313.\n 5. Doshi R, et al. Utilizing large language models to simplify radiology \nreports: a comparative analysis of chatgpt3. 5, chatgpt4. 0, google bard, \nand microsoft bing. medRxiv. 2023;5:2023–06.\n 6. Fan, Z. et al. Ai hospital: Interactive evaluation and collaboration of llms as \nintern doctors for clinical diagnosis. 2024. arXiv preprint arXiv:2402.09742.\n 7. Li J. et al. Agent hospital: A simulacrum of hospital with evolvable medi-\ncal agents. 2024. arXiv preprint arXiv:2405.02957.\n 8. Shi, W. et al. Ehragent: Code empowers large language models for few-\nshot complex tabular reasoning on electronic health records. Proceed-\nings of the ICLR 2024 Workshop on Large Language Model (LLM) Agents. \n2024.\n 9. Yan W. et al. Clinicallab: Aligning agents for multi-departmental clinical \ndiagnostics in the real world. 2024. arXiv preprint arXiv:2406.13890.\n 10. Rao A, et al. Evaluating chatgpt as an adjunct for radiologic decision-\nmaking. MedRxiv. 2023;7:2023–02.\n 11. Singhal K, et al. Large language models encode clinical knowledge. \nNature. 2023;620:172–80.\n 12. Han C, et al. Evaluation of gpt-4 for 10-year cardiovascular risk prediction: \nInsights from the uk biobank and koges data. Iscience. 2024;27:109022.\n 13. Rojas M, Rojas M, Burgess V, Toro-P´erez J, Salehi S. Exploring the perfor-\nmance of chatgpt versions 3.5, 4, and 4 with vision in the chilean medical \nlicensing examination: Observational study. JMIR Medical Education 10, \ne55048 (2024).\n 14. Camacho JCV, et al. Addressing linguistic barriers to care: evaluation of \nbreast cancer online patient educational materials for spanish-speaking \npatients. J Am Coll Radiol. 2021;18:919–26.\n 15. Lion KC, Lin Y-H, Kim T. Artificial intelligence for language translation: the \nequity is in the details. JAMA. 2024;332:1427–8.\n 16. Nicholas, G. & Bhatia, A. Lost in translation: large language models in non- \nenglish content analysis. 2023. arXiv preprint arXiv: 2306. 07377.\n 17. Instituto Cervantes. El espan˜ol en el mundo 2023 (Instituto Cervantes, \n2023). URL https:// www. cerva ntes. es/ sobre. insti tuto. cerva ntes/ prensa/ \nnota. prensa/ nota/ anuar io. htm. Segu´n el u´ltimo anuario del Instituto \nCervantes, ”el espan˜ol es la segunda lengua materna m´as hablada del \nmundo, con aproximada- mente 493 millones de hablantes nativos” . Este \ninforme se actualiza anualmente para reflejar los cambios demogr´aficos \ny lingu¨´ısticos.\n 18. Guillen-Grima F, et al. Evaluating the efficacy of chatgpt in navigating \nthe spanish medical residency entrance examination (mir): promising \nhorizons for ai in clinical medicine. Clin Pract. 2023;13:1460–87.\n 19. Vera AL. Enhancing medical spanish education and proficiency to bridge \nhealthcare disparities: a comprehensive assessment and call to action. \nCureus. 2023;15:11.\n 20. Wei J, et al. Chain-of-thought prompting elicits reasoning in large lan-\nguage models. Adv Neural Inf Process Syst. 2022;35:24824–37.\n 21. Madaan A, et al. Self-refine: iterative refinement with self-feedback. Adv \nNeural Inform Proc Syst. 2024;36:46534–94.\n 22. Welleck S, et al. Generating sequences by learning to self-correct. 2022. \narXiv preprint arXiv:2211.00053.\n 23. Kim G, Baldi P , McAleer S. Language models can solve computer tasks. \nAdv Neural Inform Proc Syst. 2024;36:39648–77.\n 24. Paul D, et al. Refiner: Reasoning feedback on intermediate representa-\ntions. 2023. arXiv preprint arXiv:2304.01904.\n 25. Ganguli D, et al. The capacity for moral self-correction in large language \nmodels. 2023. arXiv preprint arXiv:2302.07459.\n 26. Renze M, Guven E. Self-reflection in llm agents: Effects on problem-\nsolving performance. 2024. arXiv preprint arXiv:2405.06682.\n 27. Yang Z. Performance of multimodal gpt-4v on usmle with image: poten-\ntial for imaging diagnostic support with explanations. medRxiv. 2023.\nPage 11 of 11\nAltermatt et al. BMC Medical Education          (2025) 25:666 \n \n 28. Li’evin V, Hother CE, Motzfeldt AG, Winther O. Can large language models \nreason about medical questions? Patterns. 2024;5:3.\n 29. Tang X, et al. Medagents: Large language models as collaborators for \nzero-shot medical reasoning. 2023. arXiv preprint arXiv:2311.10537.\n 30. Kim Y, et al. Adaptive collaboration strategy for llms in medical decision \nmaking. 2024. arXiv preprint arXiv:2404.15155.\n 31. Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are \nzero-shot reasoners. Adv Neural Inf Process Syst. 2022;35:22199–213.\n 32. Brown, T. B. Language models are few-shot learners. 2020. arXiv preprint \narXiv:2005.14165.\n 33. Nori H, et al. Can generalist foundation models outcompete spe-\ncial-purpose tuning? case study in medicine. 2023. arXiv preprint \narXiv:2311.16452.\n 34. Zhao X, Wang K, Peng W. An electoral approach to diversify llm-\nbased multi-agent collective decision-making. 2024. arXiv preprint \narXiv:2410.15168.\n 35. Scherr R, Halaseh FF, Spina A, Andalib S, Rivera R. Chatgpt interactive \nmedical simulations for early clinical education: case study. JMIR Med \nEduc. 2023;9:e49877.\n 36. Artsi Y, et al. Large language models for generating medical examina-\ntions: systematic review. BMC Med Educ. 2024;24:354.\n 37. Wu Y, et al. Embracing chatgpt for medical education: explor-\ning its impact on doctors and medical students. JMIR Med Educ. \n2024;10:e52483.\n 38. Li Z, et al. Large language models and medical education: a paradigm \nshift in educator roles. Smart Learn Environ. 2024;11:1–11.\n 39. Kozato A, Patel N, Shikino K. A randomised controlled pilot trial of the \ninfluence of non-native English accents on examiners’ scores in OSCEs. \nBMC Med Educ. 2020;20:268.\n 40. Babalola AE, et al. The role of local languages in effective health service \ndelivery. Discover Public Health. 2025;22:59.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}