{
  "title": "Knowledge-Augmented Language Model and its Application to Unsupervised Named-Entity Recognition",
  "url": "https://openalex.org/W2938179091",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Liu, Angli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748019825",
      "name": "Du, Jingfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224909769",
      "name": "Stoyanov, Veselin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2610748790",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2889675133",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2798904449",
    "https://openalex.org/W2891734001",
    "https://openalex.org/W2148540243",
    "https://openalex.org/W2785349534",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W19862471",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2550448043",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2102267996",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2469104253",
    "https://openalex.org/W1533057952",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2525332836"
  ],
  "abstract": "Traditional language models are unable to efficiently model entity names observed in text. All but the most popular named entities appear infrequently in text providing insufficient context. Recent efforts have recognized that context can be generalized between entity names that share the same type (e.g., \\emph{person} or \\emph{location}) and have equipped language models with access to an external knowledge base (KB). Our Knowledge-Augmented Language Model (KALM) continues this line of work by augmenting a traditional model with a KB. Unlike previous methods, however, we train with an end-to-end predictive objective optimizing the perplexity of text. We do not require any additional information such as named entity tags. In addition to improving language modeling performance, KALM learns to recognize named entities in an entirely unsupervised way by using entity type information latent in the model. On a Named Entity Recognition (NER) task, KALM achieves performance comparable with state-of-the-art supervised models. Our work demonstrates that named entities (and possibly other types of world knowledge) can be modeled successfully using predictive learning and training on large corpora of text without any additional information.",
  "full_text": "Knowledge-Augmented Language Model and Its Application to\nUnsupervised Named-Entity Recognition\nAngli Liu\nFacebook AI\nanglil@cs.washington.edu\nJingfei Du\nFacebook AI\njingfeidu@fb.com\nVeselin Stoyanov\nFacebook AI\nves@fb.com\nAbstract\nTraditional language models are unable to ef-\nﬁciently model entity names observed in text.\nAll but the most popular named entities appear\ninfrequently in text providing insufﬁcient con-\ntext. Recent efforts have recognized that con-\ntext can be generalized between entity names\nthat share the same type (e.g., person or loca-\ntion) and have equipped language models with\naccess to an external knowledge base (KB).\nOur Knowledge-Augmented Language Model\n(KALM) continues this line of work by aug-\nmenting a traditional model with a KB. Un-\nlike previous methods, however, we train with\nan end-to-end predictive objective optimizing\nthe perplexity of text. We do not require\nany additional information such as named en-\ntity tags. In addition to improving language\nmodeling performance, KALM learns to rec-\nognize named entities in an entirely unsuper-\nvised way by using entity type information la-\ntent in the model. On a Named Entity Recog-\nnition (NER) task, KALM achieves perfor-\nmance comparable with state-of-the-art super-\nvised models. Our work demonstrates that\nnamed entities (and possibly other types of\nworld knowledge) can be modeled success-\nfully using predictive learning and training on\nlarge corpora of text without any additional in-\nformation.\n1 Introduction\nLanguage modeling is a form of unsupervised\nlearning that allows language properties to be\nlearned from large amounts of unlabeled text. As\ncomponents, language models are useful for many\nNatural Language Processing (NLP) tasks such as\ngeneration (Parvez et al., 2018) and machine trans-\nlation (Bahdanau et al., 2014). Additionally, the\nform of predictive learning that language model-\ning uses is useful to acquire text representations\nthat can be used successfully to improve a number\nof downstream NLP tasks (Peters et al., 2018; De-\nvlin et al., 2018). In fact, models pre-trained with\na predictive objective have provided a new state-\nof-the-art by a large margin.\nCurrent language models are unable to encode\nand decode factual knowledge such as the infor-\nmation about entities and their relations. Names\nof entities are an open class. While classes of\nnamed entities (e.g., person or location) occur fre-\nquently, each individual name (e.g, Atherton or\nZhouzhuang) may be observed infrequently even\nin a very large corpus of text. As a result, language\nmodels learn to represent accurately only the most\npopular named entities. In the presence of external\nknowledge about named entities, language models\nshould be able to learn to generalize across entity\nclasses. For example, knowing thatAlice is a name\nused to refer to a person should give ample infor-\nmation about the context in which the word may\noccur (e.g., Bob visited Alice).\nIn this work, we propose Knowledge Aug-\nmented Language Model (KALM), a language\nmodel with access to information available in a\nKB. Unlike previous work, we make no assump-\ntions about the availability of additional compo-\nnents (such as Named Entity Taggers) or annota-\ntions. Instead, we enhance a traditional LM with a\ngating mechanism that controls whether a particu-\nlar word is modeled as a general word or as a ref-\nerence to an entity. We train the model end-to-end\nwith only the traditional predictive language mod-\neling perplexity objective. As a result, our system\ncan model named entities in text more accurately\nas demonstrated by reduced perplexities compared\nto traditional LM baselines. In addition, KALM\nlearns to recognize named entities completely un-\nsupervised by interpreting the predictions of the\ngating mechanism at test time. In fact, KALM\nlearns an unsupervised named entity tagger that ri-\nvals in accuracy supervised counterparts.\narXiv:1904.04458v2  [cs.CL]  24 Jun 2019\nKALM works by providing a language model\nwith the option to generate words from a set of\nentities from a database. An individual word can\neither come from a general word dictionary as in\ntraditional language model or be generated as a\nname of an entity from a database. Entities in the\ndatabase are partitioned by type. The decision of\nwhether the word is a general term or a named en-\ntity from a given type is controlled by a gating\nmechanism conditioned on the context observed\nso far. Thus, KALM learns to predict whether the\ncontext observed is indicative of a named entity of\na given type and what tokens are likely to be enti-\nties of a given type.\nThe gating mechanism at the core of KALM\nis similar to attention in Neural Machine Trans-\nlation (Bahdanau et al., 2014). As in translation,\nthe gating mechanism allows the LM to represent\nadditional latent information that is useful for the\nend task of modeling language. The gating mech-\nanism (in our case entity type prediction) is la-\ntent and learned in an end-to-end manner to max-\nimize the probability of observed text. Experi-\nments with named entity recognition show that the\nlatent mechanism learns the information that we\nexpect while LM experiments show that it is ben-\neﬁcial for the overall language modeling task.\nThis paper makes the following contributions:\n•Our model, KALM, achieves a new state-\nof-the art for Language Modeling on several\nbenchmarks as measured by perplexity.\n•We learn a named entity recognizer without\nany explicit supervision by using only plain\ntext. Our unsupervised named entity recog-\nnizer achieves a performance on par with the\nstate-of-the supervised methods.\n•We demonstrate that predictive learning com-\nbined with a gating mechanism can be uti-\nlized efﬁciently for generative training of\ndeep learning systems beyond representation\npre-training.\n2 Related Work\nOur work draws inspiration from Ahn et al.\n(2016), who propose to predict whether the word\nto generate has an underlying fact or not. Their\nmodel can generate knowledge-related words by\ncopying from the description of the predicted fact.\nWhile theoretically interesting, their model func-\ntions only in a very constrained setting as it re-\nquires extra information: a shortlist of candidate\nentities that are mentioned in the text.\nSeveral efforts successfully extend LMs with\nentities from a knowledge base and their types,\nbut require that entity models are trained sepa-\nrately from supervised entity labels. Parvez et al.\n(2018) and Xin et al. (2018) explicitly model the\ntype of the next word in addition to the word itself.\nIn particular, Parvez et al. (2018) use two LSTM-\nbased language models, an entity type model and\nan entity composite (entity type) model. Xin\net al. (2018) use a similarly purposed entity typ-\ning module and a LM-enhancement module. In-\nstead of entity type generation, Gu et al. (2018)\npropose to explicitly decompose word genera-\ntion into sememe (a semantic language unit of\nmeaning) generation and sense generation, but re-\nquires sememe labels.Yang et al. (2016) propose\na pointer-network LM that can point to a 1-D or\n2-D database record during inference. At each\ntime step, the model decides whether to point to\nthe database or the general vocabulary.\nUnsupervised predictive learning has been\nproven effective in improving text understanding.\nELMo (Peters et al., 2018) and BERT (Devlin\net al., 2018) used different unsupervised objectives\nto pre-train text models which have advanced the\nstate-of-the-art for many NLP tasks. Similar to\nthese approaches KALM is trained end-to-end us-\ning a predictive objective on large corpus of text.\nMost unsupervised NER models are rule-based\n(Collins and Singer, 1999; Etzioni et al., 2005;\nNadeau et al., 2006) and require feature engi-\nneering or parallel corpora (Munro and Manning,\n2012). Yang and Mitchell (2017) incorporate a KB\nto the CRF-biLSTM model (Lample et al., 2016)\nby embedding triples from a KB obtained using\nTransE (Bordes et al., 2013). Peters et al. (2017)\nadd pre-trained language model embeddings as\nknowledge to the input of a CRF-biLSTM model,\nwhile still requiring labels in training. And Zhou\net al. (2018) are concerned only with zero-shot\nNER inference.\nTo the best of our knowledge, KALM is the ﬁrst\nunsupervised neural NER approach. As we dis-\ncuss in Section 5.4, KALM achieves results com-\nparable to supervised CRF-biLSTM models.\n3 Knowledge-Augmented Language\nModel\nKALM extends a traditional, RNN-based neural\nLM. As in traditional LM, KALM predicts prob-\nabilities of words from a vocabulary Vg, but it\ncan also generate words that are names of entities\nof a speciﬁc type. Each entity type has a sepa-\nrate vocabulary {V1,...,V K}collected from a KB.\nKALM learns to predict from context whether to\nexpect an entity from a given type and generalizes\nover entity types.\n3.1 RNN language model\nAt its core, a language model predicts a distribu-\ntion for a word yt+1 given previously observed\nwords ct := [y1,...,y t−1,yt]. Models are trained\nby maximizing the likelihood of the observed next\nword. In an LSTM LM, the probability of a word,\nP(yt+1|ct), is modeled from the hidden state of an\nLSTM (Hochreiter and Schmidhuber, 1997):\nP(yt+1 = i|ct) =\nexp(Wp\ni,: ·ht)\n|Vg|∑\nw=1\nexp(Wp\nw,: ·ht)\n(1)\nht,γt = lstm(ht−1,γt−1,yt) (2)\nwhere lstmrefers to the LSTM step function and\nhi, γi and yi are the hidden, memory and input\nvectors, respectively. Wp is a projection layer that\nconverts LSTM hidden states into logits that have\nthe size of the vocabulary |Vg|.\n3.2 Knowledge-Augmented Language Model\nKALM builds upon the LSTM LM by adding\ntype-speciﬁc entity vocabularies V1,V2,...,V K in\naddition to the general vocabularyVg. Type vocab-\nularies are extracted from the entities of speciﬁc\ntype in a KB. For a given word, KALM computes\na probability that the word represents an entity of\nthat type by using a type-speciﬁc projection matrix\n{Wp,j|j = 0,...,K }. The model also computes\nthe probability that the next word represents dif-\nferent entity types given the context observed so\nfar. The overall probability of a word is given by\nthe weighted sum of the type probabilities and the\nprobability of the word under the give type.\nMore precisely, let τi be a latent variable denot-\ning the type of word i. We decompose the proba-\nbility in Equation 1 using the type τt+1:\nP(yt+1|ct) =\nK∑\nj=0\nP(yt+1,τt+1 = j|ct)\n=\nK∑\nj=0\nP(yt+1|τt+1 = j,ct)\n·P(τt+1 = j|ct) (3)\nWhere P(yt+1|τt+1,ct) is a distribution of en-\ntity words of type τt+1. As in a general LM,\nit is computed by projecting the hidden state of\nthe LSTM and normalizing through softmax (eq.\n4). The type-speciﬁc projection matrix Wp,j is\nlearned during training.\nWe maintain a type embedding matrix We and\nuse it in a similar manner to compute the probabil-\nity that the next word has a given type P(τt+1|ct)\n(eq. 5). The only difference is that we use an ex-\ntra projection matrix, Wh to project ht into lower\ndimensions. Figure 1a illustrates visually the ar-\nchitecture of KALM.\nP(yt+1 = i|τt+1 = j,ct) =\nexp(Wp,j\ni,: ·ht)\n|Vj|∑\nw=1\nexp(Wp,j\nw,: ·ht)\n(4)\nP(τt+1 = j|ct) = exp(We\nj,: ·(Wh ·ht))\nK∑\nk=0\nexp(We\nk,: ·(Wh ·ht))\n(5)\n3.3 Type representation as input\nIn the base KALM model the input for word yt\nconsists of its embedding vector yt. We enhance\nthe base model by adding as inputs the embedding\nof the type of the previous word. As type informa-\ntion is latent, we represent it as the weighted sum\nof the type embeddings weighted by the predicted\nprobabilities:\nνt+1 =\nK∑\nj=0\nP(τt+1 = j|ct) ·We\nj,: (6)\n˜yt+1 = [yt+1; νt+1] (7)\nP(τt+1 = j|ct) is computed using Equation 5 and\nej is the type embedding vector.\n!\"#$\n%&'&\"()\n*+,-./0\n!\"\n!\"1234\n*+,-./0\n56&7(\n1234\n89:\n1234\n<<>\n!\"#>\n89:%&'&\"()\n*+,-./0*+,-./0\n?(ABC)?(?EB)?(1AC)?FGHIJ1AK) ?FGHIJ?EB) ?FGHIJABC)\n…\nL(\nLM\n(a) Basic model architecture of KALM.\n!\"#$\n%&'&\"()\n*+,-./0\n!\" !\"1234\n*+,-./0\n56&7(\n56&7(\n12341234\n89:\n1234\n<<>\n!\"#>\n89:%&'&\"()\n*+,-./0*+,-./0\n?(ABC)?(?EB)?(1AC)\n<\\<>\n?GHIJK1AL) ?GHIJK?EB) ?GHIJKABC)\n…\nM(\nMN\n(b) Adding type representation as input to KALM.\nFigure 1: KALM’s architectures\nAdding type information as input serves two\npurposes: in the forward direction, it allows\nKALM to model context more precisely based on\npredicted entity types. During back propagation,\nit allows us to learn latent types more accurately\nbased on subsequent context. The model enhanced\nwith type input is illustrated in Figure 1b.\n4 Unsupervised NER\nThe type distribution that KALM learns is latent,\nbut we can output it at test time and use it to pre-\ndict whether a given word refers to an entity or a\ngeneral word. We compute P(τt+1|ct) using eq. 5\nand use the most likely entity type as the named\nentity tag for the corresponding word yt+1.\nThis straightforward approach, however, pre-\ndicts the type based solely on the left context of the\ntag being predicted. In the following two subsec-\ntions, we discuss extensions to KALM that allow\nit to utilize the right context and the word being\npredicted itself.\n4.1 Bidirectional LM\nWhile we cannot use a bidirectional LSTM for\ngeneration, we can use one for NER, since the en-\ntire sentence is known.\nFor each word, KALM generates the hidden\nvectors hl,t and hr,t representing context coming\nfrom left and right directions, as shown in Equa-\ntions 8 and 9.\nhl,t,γl,t = lstml(hl,t−1,γl,t−1,[yl,t; νl,t]) (8)\nhr,t,γr,t = lstmr(hr,t+1,γr,t+1,[yr,t; νr,t])\n(9)\nWe concatenate the hidden vectors from the two\ndirections to form an overall context vectorht, and\ngenerate the ﬁnal type distribution using Equation\n5.\nTraining the bidirectional model requires that\nwe initialize the hidden and cell states from both\nends of a sentence. Suppose the length of a sen-\ntence is n. The the cross entropy loss is computed\nfor only then−2 symbols in the middle. Similarly,\nwe compute only the types of the n−2 symbols\nin the middle during inference.\n4.2 Current word information\nEven bidirectional context is insufﬁcient to predict\nthe word type by itself. Consider the following\nexample: Our computer models indicate Edouard\nis going to 1 The missing word can be either\na location (e.g., London), or a general word (e.g.,\nquit). In an NER task we observe the underlined\nwords: Our computer models indicate Edouard is\ngoing to London. In order to learn predictively, we\ncannot base the type prediction on the current to-\nken. Instead, we can use a prior type information\nP(τt|yt), pre-computed from entity popularity in-\nformation available in many KBs. We incorpo-\nrate the prior informationP(τt|yt) in two different\nways described in the two following subsections.\n4.2.1 Decoding with type prior\nWe incorporate the type prior P(τt|yt) directly by\ncombining it linearly with the predicted type:\nP(τt|cl,cr,yt) =P(yt|τt,cl,cr)\n2P(yt|cl,cr) ·P(τt|cl,cr)\n+ P(cl,cr|τt,yt)\n2P(cl,cr|yt) ·P(τt|yt)\n=α·P(τt|cl,cr)\n+ β·P(τt|yt) (10)\nThe coefﬁcients αand βare free parameters and\nare tuned on a small amount of withheld data.\n1All the examples are selected from the CoNLL 2003\ntraining set.\n4.2.2 Training with type priors\nAn alternative for incorporating the pre-computed\nP(τt|yt) is to use it during training to regularize\nthe type distribution. We use the following op-\ntimization criterion to compute the loss for each\nword:\nL=H(P(yi|cl,cr),P(ˆyi|cl,cr))\n+ λ·||KL(P(τi|cl,cr),P(τi|yi))||2 (11)\nwhere ˆyi is the actual word, H(.) is the cross en-\ntropy function, and KL(.) measures the KL di-\nvergence between two distributions. A hyper-\nparameter λ (tuned on validation data) controls\nthe relative contribution of the two loss terms.\nThe new loss forces the learned type distribu-\ntion, P(τi|cl,cr), to be close to the expected dis-\ntribution P(τi|yi) given the information in the\ndatabase. This loss is speciﬁcally tailored to help\nwith unsupervised NER.\n5 Experiments\nWe evaluate KALM on two tasks: language mod-\neling and NER. We use two datasets: Recipe used\nonly for LM evaluation and CoNLL 2003 used for\nboth the LM and NER evaluations.\n5.1 Data\nRecipe The recipe dataset 2 is composed of\n95,786 recipes, We follow the same preprocess-\ning steps as in Parvez et al. (2018) and divide the\ncrawled dataset into training, validation and test-\ning. A typical sentence after preprocessing looks\nlike the following: “in a large mixing bowl com-\nbine the butter sugar and the egg yolks”. The en-\ntities in the recipe KB are recipe ingredients. The\n8 supported entity types are dairy, drinks, fruits,\ngrains, proteins, seasonings, sides, and vegeta-\nbles. In the sample sentence above, the entity\nnames are butter, sugar, egg and yolks, typed as\ndairy, seasonings, proteins and proteins, respec-\ntively.\nCoNLL 2003 Introduced in Tjong Kim Sang\nand De Meulder (2003), the CoNLL 2003 dataset\nis composed of news articles. It contains text and\nnamed entity labels in English, Spanish, German\nand Dutch. We experiment only with the English\nversion. We follow the CoNLL labels and sep-\narate the KB into four entity types: LOC (loca-\n2Crawled from http://www.ffts.com/recipes.\nhtm\ntion), MISC (miscellaneous), ORG (organization),\nand PER (person).\nStatistics about the recipe and the CoNLL 2003\ndataset are presented in Table 1.\ntrain valid test\n#sent 61302 15326 19158\n#tok 7223474 1814810 2267797\ntrain valid test\n#sent 14986 3465 3683\n#tok 204566 51577 46665\nTable 1: Statistics of recipe and CoNLL 2003 datasets\nThe information about the entities in each of\nthe KBs is shown in Table 2. The recipe KB is\nprovided along with the recipe dataset 3 as a con-\nglomeration of typed ingredients. The KB used by\nCoNLL 2003 is extracted from WikiText-2. We\nﬁltered the entities which are not belonging to the\n4 types of CoNLL 2003 task.\ntype dairy drinks fruits grains\n#entities 80 84 110 158\ntype proteins seasonings sides vegetables\n#entities 316 180 140 156\ntype LOC MISC ORG PER\n#entity words 1503 1211 3005 5404\nTable 2: Statistics of recipe and CoNLL 2003 KBs\n5.2 Implementation details\nWe implement KALM by extending the AWD-\nLSTM4 language model in the following ways:\nVocabulary We use the entity words in Ta-\nble 2 to form V1,...,V K We extract 51,677 gen-\neral words in the recipe dataset, and 17,907 gen-\neral words in CoNLL 2003 to form V0. Identical\nwords that fall under different entity types, such\nas Washington in George Washington and Wash-\nington D.C., share the same input embeddings.\nModel The model has an embedding layer\nof 400 dimensions, LSTM cell and hidden states\nof 1,150 dimensions, and 3 stacked LSTM layers.\nWe scale the ﬁnal LSTM’s hidden and cell states\nto 400 dimensions, and share weights between\nthe projection layer Wp and the word embedding\nlayer. Each entity type in the knowledge base is\nrepresented by a trainable 100-dimensional em-\nbedding vector. When concatenating the weighted\n3The KB can be found in https://github.com/\nuclanlp/NamedEntityLanguageModel\n4https://github.com/salesforce/\nawd-lstm-lm\naverage of the type embeddings to the input, we\nexpand the input dimension of the ﬁrst LSTM\nlayer to 500. All trainable parameters are initial-\nized uniformly randomly between −0.1 and 0.1,\nexcept for the bias terms in the decoder linear\nlayer, which are initialized to 0.\nFor regularization, we adopt the techniques in\nAWD-LSTM, and use an LSTM weight dropout\nrate of 0, an LSTM ﬁrst-layers locked dropout\nrate of 0.3, an LSTM last-layer locked dropout\nrate of 0.4, an embedding Bernoulli dropout rate\nof 0.1, and an embedding locked dropout rate of\n0.65. Also, we impose L2 penalty on LSTM pre-\ndropout weights with coefﬁcient 1, and L2 penalty\non LSTM dropout weights with coefﬁcient 2, both\nadded to the cross entropy loss.\nOptimization We use the same loss penalty,\ndropout schemes, and averaged SGD (ASGD) as\nin Merity et al. (2017). The initial ASGD learn-\ning rate is 10, weight decay rate is 1.2 ×10−6,\nnon-monotone trigger for ASGD is set to 5, and\ngradient clipping happens at 0.25. The models are\ntrained until the validation set performance starts\nto decrease.\n5.3 Language modeling\nFirst, we test how good KALM is as a language\nmodel compared to two baselines.\n5.3.1 Baselines\n•AWD-LSTM (Merity et al., 2017) is the\nstate-of-the-art word-level language model as\nmeasured on WikiText-2 and Penn Treebank.\nIt uses ASGD optimization, a new dropout\nscheme and novel penalty terms in the loss\nfunction to improve over vanilla LSTM LMs.\n•Named-entity LM (NE-LM) (Parvez\net al., 2018) consists of a type model\nthat outputs P(τi+1|τi,τi−1,...) and\nan entity composite model that outputs\nP(yi+1|{yi,τi},{yi−1,τi−1},...). The type\nmodel is trained on corpora with entity type\nlabels, whereas the entity composite model\nhas an input for words and another input for\nthe corresponding types, and so needs to be\ntrained on both the labeled corpus and the\nunlabeled version of the same corpus. At\ninference time, a joint inference heuristic\naggregates type model and entity composite\nmodel predictions into a word prediction.\nSince both models require type labels as\ninput, each generation step of NE-LM\nrequires not only the previously generated\nwords [yi,yi−1,...], but also the type labels\nfor these words [τi,τi−1,...].\n5.3.2 Results\nFor language modeling we report word prediction\nperplexity on the recipe dataset and CoNLL 2003.\nPerplexity is deﬁned as the following.\nPP = e\n−\nN∑\nt=1\n1\nN log P(yt)\n=\nN\n√\nN∏\nt=1\n1\nP(yt) (12)\nWe use publicly available implementations to pro-\nduce the two baseline results. We also compare\nthe language models in the bidirectional setting,\nwhich the reference implementations do not sup-\nport. In that setting, we transform both models in\nNE-LM to be bidirectional.\nDiscussion Table 3 shows that KALM out-\nperforms the two baselines in both unidirectional\nand bidirectional settings on both datasets. The\nimprovement relative to NE-LM is larger in the\nunidirectional setting compared to the bidirec-\ntional setting. We conjecture that this is because in\nthat setting NE-LM trains a bidirectional NER in a\nsupervised way. The improvement relative to NE-\nLM is larger on CoNLL 2003 than on the recipe\ndataset. We believe that the inference heuristic\nused by NE-LM is tuned speciﬁcally to recipes\nand is less suitable to the CoNLL setting.\nWe also ﬁnd that training KALM on more unla-\nbeled data further reduces the perplexity (see Table\n4), and study how the quality of the KB affects the\nperplexity. We discuss both these results in Sec-\ntion 5.4.\n5.4 NER\nIn this section, we evaluate KALM in NER against\ntwo supervised baselines.\n5.4.1 Baselines\nWe train two supervised models for NER on the\nCoNLL 2003 dataset: a biLSTM and a CRF-\nbiLSTM. We replicate the hyperparameters used\nby Lample et al. (2016), who demonstrate the\nstate-of-the-art performance on this dataset. We\nuse a word-level model, and 100 dimensional\npretrained GloVe embeddings (Pennington et al.,\n2014) for initialization. We train for 50 epochs, at\nwhich point the models converge.\nmodel unidirectional bidirectional\nvalidation test validation test\nRecipe\nAWD-LSTM 3.14 2.99 1.98 2\nNE-LM 2.96 2.24 1.85 1.73\nKALM 2.75 2.20 1.85 1.71\nCoNLL 2003\nAWD-LSTM 5.48 5.94 4.85 5.3\nNE-LM 5.67 5.77 4.68 4.94\nKALM 5.36 5.43 4.64 4.74\nTable 3: Language modeling results on the recipe and CoNLL 2003 datasets\n5.4.2 Results\nWe evaluate the unsupervised KALM model under\nthe following conﬁgurations:\n•Basic: bidirectional model with aggregated\ntype embeddings fed to the input at the next\ntime step;\n•With type priors: using P(τt|yt) in the two\nways described in Section 4.2;\n•Extra data: Since KALM is unsupervised,\nwe can train it on extra data. We use the\nWikiText-2 corpus in addition to the original\nCoNLL training data.\nWikiText-2 is a standard language modeling\ndataset released with Merity et al. (2016). It con-\ntains Wikipedia articles from a wide range of top-\nics. In contrast, the CoNLL 2003 corpus con-\nsists of news articles. Table 4 show statistics\nabout the raw / characer level WikiText-2 and the\nCoNLL 2003 corpora. Despite the domain mis-\nmatch between the WikiText and CoNLL corpora,\nthe WikiText coverage of the entity words that ex-\nist in the CoNLL dataset is high. Speciﬁcally,\nmost of the person, location and organization en-\ntity words that appear in CoNLL either have a\nWikipedia section, or are mentioned in a Wiki ar-\nticle. Therefore, we expect that the addition of\nWikiText can guide the unsupervised NER model\nto learn better entity type regularities. Indeed, the\nresult presented in the rightmost column of Table\n4 shows that when adding WikiText-2 to CoNLL\n2003, the perplexity for the KALM model for the\nnews text of CoNLL 2003 is decreased: from 4.69\ndown to 2.29.\nWe show NER results in Table 5. The table lists\nthe F1 score for each entity types, as well as the\noverall F1 score.\nDiscussion Even the basic KALM model\nlearns context well – it achieves an overall F1\nscore of 0.72 for NER. This illustrates that KALM\nhas learned to model entity classes entirely from\nsurrounding context. Adding prior information as\nto whether a word represents different entity types\nhelps to bring the F1 score to 0.76.\nThe strength of an unsupervised model is that\nit can be trained on large corpora. Adding the\nWikitext-2 corpus improves the NER score of\nKALM to 0.84.\nTo give a sense of how the unsupervised mod-\nels compare with the supervised model with re-\nspect to training data size, we trained biLSTM and\nCRF-biLSTM on a randomly sampled subset of\nthe training data of successively decreasing sizes.\nThe resulting F1 scores are shown in Figure 2.\nOur best model scores 0.86, same as a CRF-\nbiLSTM trained on around 40% of the training\ndata. It is less than 0.03 behind the best supervised\nCRF-biLSTM. The best KALM model almost al-\nways scores higher than biLSTM without the CRF\nloss.\nLastly, we perform an ablation experiment to\ngauge how sensitive KALM is to the quality of\nthe knowledge base. Previous studies (Liu et al.,\n2016; Zhang et al., 2012) have shown that the\namount of knowledge retrieved from KBs can im-\npact the performance of NLP models such as rela-\ntion extraction systems substantially. In this ex-\nperiment, we deliberately corrupt the entity vo-\ncabularies V0,...,V K−1 by moving a certain per-\ncentage of randomly selected entity words from\nVi to the general vocabulary Vg. Figure 3 shows\nlanguage modeling perplexities on the validation\nset, and NER F1 scores on the test set as a func-\ntion of the corruption percentage. The language\nmodeling performance stops reacting to KB cor-\nruption beyond a certain extent, whereas the NER\nperformance keeps dropping as the number of en-\ntities removed from V1,V2,... increases. This re-\nsult shows the importance of the quality of the KB\nentity unique entity size LM\nratio ratio ratio perplexity\n92.80% 82.56% 2.62 2.29 : 4.69\nTable 4: Characterization of WikiText-2 relative to CoNLL 2003 training set. Entities extracted from WikiText\ncover 92.80% of the entities in CoNLL 2003 overall, and cover 82.56% of the unique entities. WikiText’s size is\n2.62 times as large. And adding WikiText to CoNLL training reduces the perplexity from4.69 to 2.29.\nLOC MISC ORG PER overall\nunsupervised\nbasic0 0.75 0.67 0.64 0.83 0.72\n+P(τ|y) in dec1 0.81 0.67 0.65 0.88 0.76\n+wiki-concat2 0.83 0.83 0.76 0.95 0.84\n+wiki-concat+P(τ|y) in dec3 0.84 0.84 0.77 0.96 0.86\nsupervised biLSTM 0.88 0.71 0.81 0.95 0.86\nCRF-biLSTM 0.90 0.76 0.84 0.95 0.89\nTable 5: Results of KALM (above the double line in the table) and supervised NER models (under the double\nline). Superscript annotations: 0: The basic bidirectional KALM with type embedding features as described in\nSection 4.1. 1: Adding P(τ|y) in decoding, as described in Section 4.2.1, where αand βare tuned to be 0.4 and\n0.6. 2: The basic model trained on CoNLL 2003 concatenated with WikiText-2. 3: Adding (τ|y) in decoding, with\nthe model trained on CoNLL 2003 concatenated with WikiText-2.\n0.2 0.4 0.6 0.8 1.0\n% of training\n0.81\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87\n0.88F1\nF1 v.s. Percentage of labeled training\nBiLSTM\nCRF-biLSTM\nKALM\nFigure 2: Unsupervised v.s. supervised NER\ntrained on different portions of training data\n0 5 10 15 20 25 30 35 40\n% of corrupted entities\n4.64\n4.66\n4.68\n4.70\n4.72\n4.74cross entropy loss\nLM\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nF1\nNER and LM performances v.s. Percentage of corrupted entities\nNER\nFigure 3: NER and LM performances on different\nportions of mislabeled entities\nto KALM.\n6 Conclusion\nWe propose Knowledge Augmented Language\nModel (KALM), which extends a traditional RNN\nLM with information from a Knowledge Base. We\nshow that real-world knowledge can be used suc-\ncessfully for natural language understanding by\nusing a probabilistic extension. The latent type in-\nformation is trained end-to-end using a predictive\nobjective without any supervision. We show that\nthe latent type information that the model learns\ncan be used for a high-accuracy NER system.\nWe believe that this modeling paradigm opens the\ndoor for end-to-end deep learning systems that can\nbe enhanced with latent modeling capabilities and\ntrained in a predictive manner end-to-end. In ways\nthis is similar to the attention mechanism in ma-\nchine translation where an alignment mechanism\nis added and trained latently against the overall\ntranslation perplexity objective. As with our NER\ntags, machine translation alignments are empiri-\ncally observed to be of high quality.\nIn future work, we look to model other types\nof world knowledge beyond named entities using\npredictive learning and training on large corpora of\ntext without additional information, and to make\nKALM more robust against corrupted entities.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel P ¨arnamaa, and\nYoshua Bengio. 2016. A neural knowledge lan-\nguage model. arXiv preprint arXiv:1608.00318.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information\nprocessing systems, pages 2787–2795.\nMichael Collins and Yoram Singer. 1999. Unsuper-\nvised models for named entity classiﬁcation. In\n1999 Joint SIGDAT Conference on Empirical Meth-\nods in Natural Language Processing and Very Large\nCorpora.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nOren Etzioni, Michael Cafarella, Doug Downey, Ana-\nMaria Popescu, Tal Shaked, Stephen Soderland,\nDaniel S Weld, and Alexander Yates. 2005. Un-\nsupervised named-entity extraction from the web:\nAn experimental study. Artiﬁcial intelligence ,\n165(1):91–134.\nYihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing\nXie, Maosong Sun, Fen Lin, and Leyu Lin. 2018.\nLanguage modeling with sparse product of sememe\nexperts. arXiv preprint arXiv:1810.12387.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\narXiv preprint arXiv:1603.01360.\nAngli Liu, Stephen Soderland, Jonathan Bragg,\nChristopher H Lin, Xiao Ling, and Daniel S Weld.\n2016. Effective crowd annotation for relation ex-\ntraction. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 897–906.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nRobert Munro and Christopher D Manning. 2012. Ac-\ncurate unsupervised joint named-entity extraction\nfrom unaligned parallel text. In Proceedings of the\n4th Named Entity Workshop, pages 21–29. Associa-\ntion for Computational Linguistics.\nDavid Nadeau, Peter D Turney, and Stan Matwin. 2006.\nUnsupervised named-entity recognition: Generating\ngazetteers and resolving ambiguity. In Conference\nof the Canadian Society for Computational Studies\nof Intelligence, pages 266–277. Springer.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2018. Building language\nmodels for text with named entities. arXiv preprint\narXiv:1805.04836.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nMatthew E Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\narXiv preprint arXiv:1705.00108.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the seventh conference on Natural\nlanguage learning at HLT-NAACL 2003-Volume 4 ,\npages 142–147. Association for Computational Lin-\nguistics.\nJi Xin, Hao Zhu, Xu Han, Zhiyuan Liu, and Maosong\nSun. 2018. Put it back: Entity typing with language\nmodel enhancement. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 993–998.\nBishan Yang and Tom Mitchell. 2017. Leveraging\nknowledge bases in lstms for improving machine\nreading. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , volume 1, pages 1436–\n1446.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2016. Reference-aware language models.\narXiv preprint arXiv:1611.01628.\nCe Zhang, Feng Niu, Christopher R ´e, and Jude Shav-\nlik. 2012. Big data versus the crowd: Looking for\nrelationships in all the right places. In Proceedings\nof the 50th Annual Meeting of the Association for\nComputational Linguistics: Long Papers-Volume 1,\npages 825–834. Association for Computational Lin-\nguistics.\nBen Zhou, Daniel Khashabi, Chen-Tse Tsai, and Dan\nRoth. 2018. Zero-shot open entity typing as type-\ncompatible grounding. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2065–2076.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9388101100921631
    },
    {
      "name": "Computer science",
      "score": 0.8500326871871948
    },
    {
      "name": "Entity linking",
      "score": 0.7979789972305298
    },
    {
      "name": "Natural language processing",
      "score": 0.7136212587356567
    },
    {
      "name": "Language model",
      "score": 0.7006916403770447
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6618325710296631
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6548227071762085
    },
    {
      "name": "Context (archaeology)",
      "score": 0.612646222114563
    },
    {
      "name": "Knowledge base",
      "score": 0.5446732044219971
    },
    {
      "name": "Task (project management)",
      "score": 0.5172682404518127
    },
    {
      "name": "Named entity",
      "score": 0.4887228012084961
    },
    {
      "name": "Topic model",
      "score": 0.4496966004371643
    },
    {
      "name": "Question answering",
      "score": 0.4178019165992737
    },
    {
      "name": "Information retrieval",
      "score": 0.3529794216156006
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}