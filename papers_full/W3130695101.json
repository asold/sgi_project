{
    "title": "TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation",
    "url": "https://openalex.org/W3130695101",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A698323896",
            "name": "Zhang Yun-dong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4300352678",
            "name": "Liu, Huiye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1897455964",
            "name": "Hu Qiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3106061934",
        "https://openalex.org/W2401231614",
        "https://openalex.org/W3014641072",
        "https://openalex.org/W2996290406",
        "https://openalex.org/W2008359794",
        "https://openalex.org/W2961348656",
        "https://openalex.org/W2963062226",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2915126261",
        "https://openalex.org/W2999580839",
        "https://openalex.org/W2803575519",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2560328367",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2774320778",
        "https://openalex.org/W2947263797",
        "https://openalex.org/W2021088830",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3092344722",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2885668531",
        "https://openalex.org/W2963946669",
        "https://openalex.org/W3105311500",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W2285968993",
        "https://openalex.org/W2979515228",
        "https://openalex.org/W2997286550",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W3124994365",
        "https://openalex.org/W2964043069",
        "https://openalex.org/W2884588972",
        "https://openalex.org/W2626778328"
    ],
    "abstract": "Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallel-in-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique - BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.",
    "full_text": "TransFuse: Fusing Transformers and CNNs for\nMedical Image Segmentation\nYundong Zhang‚ãÜ1, Huiye Liu‚ãÜ1,2 (B), and Qiang Hu 1\n1 Rayicer, Suzhou, China\nhuiyeliu@rayicer.com\n2 Georgia Institute of Technology, Atlanta, GA, USA\nAbstract. Medical image segmentation - the prerequisite of numerous\nclinical needs - has been signiÔ¨Åcantly prospered by recent advances in\nconvolutional neural networks (CNNs). However, it exhibits general lim-\nitations on modeling explicit long-range relation, and existing cures, re-\nsorting to building deep encoders along with aggressive downsampling\noperations, leads to redundant deepened networks and loss of localized\ndetails. Hence, the segmentation task awaits a better solution to improve\nthe eÔ¨Éciency of modeling global contexts while maintaining a strong\ngrasp of low-level details. In this paper, we propose a novel parallel-\nin-branch architecture, TransFuse, to address this challenge. TransFuse\ncombines Transformers and CNNs in a parallel style, where both global\ndependency and low-level spatial details can be eÔ¨Éciently captured in\na much shallower manner. Besides, a novel fusion technique - BiFusion\nmodule is created to eÔ¨Éciently fuse the multi-level features from both\nbranches. Extensive experiments demonstrate that TransFuse achieves\nthe newest state-of-the-art results on both 2D and 3D medical image\nsets including polyp, skin lesion, hip, and prostate segmentation, with\nsigniÔ¨Åcant parameter decrease and inference speed improvement.\nKeywords: Medical Image Segmentation¬∑ Transformers¬∑ Convolutional\nNeural Networks ¬∑ Fusion\n1 Introduction\nConvolutional neural networks (CNNs) have attained unparalleled performance\nin numerous medical image segmentation tasks [9,12], such as multi-organ seg-\nmentation, liver lesion segmentation, brain 3D MRI, etc., as it is proved to be\npowerful at building hierarchical task-speciÔ¨Åc feature representation by training\nthe networks end-to-end. Despite the immense success of CNN-based method-\nologies, its lack of eÔ¨Éciency in capturing global context information remains a\nchallenge. The chance of sensing global information is equaled by the risk of\neÔ¨Éciency, because existing works obtain global information by generating very\nlarge receptive Ô¨Åelds, which requires consecutively down-sampling and stacking\n‚ãÜ These authors contributed equally to this work.\narXiv:2102.08005v2  [cs.CV]  10 Jul 2021\n2 Y. Zhang et al.\nconvolutional layers until deep enough. This brings several drawbacks: 1) train-\ning of very deep nets is aÔ¨Äected by the diminishing feature reuse problem [23],\nwhere low-level features are washed out by consecutive multiplications; 2) local\ninformation crucial to dense prediction tasks, e.g., pixel-wise segmentation, is\ndiscarded, as the spatial resolution is reduced gradually; 3) training parameter-\nheavy deep nets with small medical image datasets tends to be unstable and\neasily overÔ¨Åtting. Some studies [29] use the non-local self-attention mechanism\nto model global context; however, the computational complexity of these mod-\nules typically grows quadratically with respect to spatial size, thus they may\nonly be appropriately applied to low-resolution maps.\nTransformer, originally used to model sequence-to-sequence predictions in\nNLP tasks [26], has recently attracted tremendous interests in the computer\nvision community. The Ô¨Årst purely self-attention based vision transformers (ViT)\nfor image recognition is proposed in [7], which obtained competitive results on\nImageNet [6] with the prerequisite of being pretrained on a large external dataset.\nSETR [32] replaces the encoders with transformers in the conventional encoder-\ndecoder based networks to successfully achieve state-of-the-art (SOTA) results\non the natural image segmentation task. While Transformer is good at modeling\nglobal context, it shows limitations in capturing Ô¨Åne-grained details, especially\nfor medical images. We independently Ô¨Ånd that SETR-like pure transformer-\nbased segmentation network produces unsatisfactory performance, due to lack\nof spatial inductive-bias in modelling local information (also reported in [4]).\nTo enjoy the beneÔ¨Åt of both, eÔ¨Äorts have been made on combining CNNs with\nTransformers, e.g., TransUnet [4], which Ô¨Årst utilizes CNNs to extract low-level\nfeatures and then passed through transformers to model global interaction. With\nskip-connection incorporated, TransUnet sets new records in the CT multi-organ\nsegmentation task. However, past works mainly focus on replacing convolution\nwith transformer layers or stacking the two in a sequential manner. To further\nunleash the power of CNNs plus Transformers in medical image segmentation, in\nthis paper, we propose a diÔ¨Äerent architecture‚Äî TransFuse, which runs shallow\nCNN-based encoder and transformer-based segmentation network in parallel, fol-\nlowed by our proposed BiFusion module where features from the two branches\nare fused together to jointly make predictions. TransFuse possesses several ad-\nvantages: 1) both low-level spatial features and high-level semantic context can\nbe eÔ¨Äectively captured; 2) it does not require very deep nets, which alleviates\ngradient vanishing and feature diminishing reuse problems; 3) it largely im-\nproves eÔ¨Éciency on model sizes and inference speed, enabling the deployment\nat not only cloud but also edge. To the best of our knowledge, TransFuse is the\nÔ¨Årst parallel-in-branch model synthesizing CNN and Transformer. Experiments\ndemonstrate the superior performance against other competing SOTA works.\n2 Proposed Method\nAs shown in Fig. 1, TransFuse consists of two parallel branches processing infor-\nmation diÔ¨Äerently: 1) CNN branch, which gradually increases the receptive Ô¨Åeld\nTransFuse: Fusing Transformers and CNNs for Medical Image Segmentation 3\nand encodes features from local to global; 2) Transformer branch, where it starts\nwith global self-attention and recovers the local details at the end. Features with\nsame resolution extracted from both branches are fed into our proposed BiFu-\nsion Module, where self-attention and bilinear Hadamard product are applied\nto selectively fuse the information. Then, the multi-level fused feature maps are\ncombined to generate the segmentation using gated skip-connection [20]. There\nAG\nLayer\nNorm\nLayer\nNorm\n+\nMSA\nMLP\n+\nEmbedded \nSequence\nZ\nBiFusion\nùêü2\nBiFusion\nùêü1\nBiFusion\nùêü0\n‚Ä¶\nLinear Projection\nTransformer\nTransformer\n‚Ä¶\nùê≠0\nùê≠1\nùê≠2\nTransformer Branch\nAG\n(H/4, W/4)\n(H/8, W/8)\n(H/4, W/4)\n(H/8, W/8)\n(H/16, W/16)\n ùê†0\nùê†1\nùê†2\nCNN Branch\nùêø ùê∫,head(ùê≠2)\n·àòùêü1\n·àòùêü2\nùêø ùê∫,head(ùêü0)\nùêø ùê∫,head(·àòùêü2)\nConv\nUpsample\nReshape\nPred. Head\nAG Attn. Gate\nùê≥ùêã\nFig. 1: Overview of TransFuse (best viewed in color): two parallel branches - CNN\n(bottom right) and transformer (left) fused by our proposed BiFusion module.\nare two main beneÔ¨Åts of the proposed branch-in-parallel approach: Ô¨Årstly, by\nleveraging the merits of CNNs and Transformers, we argue that TransFuse can\ncapture global information without building very deep nets while preserving\nsensitivity on low-level context; secondly, our proposed BiFusion module may si-\nmultaneously exploit diÔ¨Äerent characteristics of CNNs and Transformers during\nfeature extraction, thus making the fused representation powerful and compact.\nTransformer Branch.The design of Transformer branch follows the typ-\nical encoder-decoder architecture. SpeciÔ¨Åcally, the input image x ‚ààRH√óW√ó3 is\nÔ¨Årst evenly divided into N = H\nS √óW\nS patches, where S is typically set to 16. The\npatches are then Ô¨Çattened and passed into a linear embedding layer with output\ndimension D0, obtaining the raw embedding sequence e ‚ààRN√óD0 . To utilize\nthe spatial prior, a learnable positional embeddings of the same demension is\nadded to e. The resulting embeddings z0 ‚ààRN√óD0 is the input to Transformer\nencoder, which contains Llayers of multiheaded self-attention (MSA) and Multi-\nlayer Perceptron (MLP). We highlight that the self-attention (SA) mechanism,\nwhich is the core principal of Transformer, updates the states of each embedded\npatch by aggregating information globally in every layer:\nSA(zi) = softmax\n( qikT\n‚àöDh\n)\nv, (1)\n4 Y. Zhang et al.\nwhere [q,k,v] = zWqkv, Wqkv ‚ààRD0√ó3Dh is the projection matrix and vector\nzi ‚ààR1√óD0 ,qi ‚ààR1√óDh are the ith row of z and q, respectively. MSA is an ex-\ntension of SA that concatenates multiple SAs and projects the latent dimension\nback to RD0 , and MLP is a stack of dense layers (refer to [7] for details of MSA\nand MLP). Layer normalization is applied to the output of the last transformer\nlayer to obtain the encoded sequence zL ‚ààRN√óD0 . For the decoder part, we use\nprogressive upsampling (PUP) method, as in SETR [32]. SpeciÔ¨Åcally, we Ô¨Årst\nreshape zL back to t0 ‚ààR\nH\n16 √óW\n16 √óD0 , which could be viewed as a 2D feature map\nwith D0 channels. We then use two consecutive standard upsampling-convolution\nlayers to recover the spatial resolution, where we obtain t1 ‚ààR\nH\n8 √óW\n8 √óD1 and\nt2 ‚ààR\nH\n4 √óW\n4 √óD2 , respectively. The feature maps of diÔ¨Äerent scales t0, t1 and t2\nare saved for late fusion with corresponding feature maps of the CNN branch.\nCNN Branch. Traditionally, features are progressively downsampled to\nH\n32 √óW\n32 and hundreds of layers are employed in deep CNNs to obtain global\ncontext of features, which results in very deep models draining out resources.\nConsidering the beneÔ¨Åts brought by Transformers, we remove the last block\nfrom the original CNNs pipeline and take advantage of the Transformer branch\nto obtain global context information instead. This gives us not only a shallower\nmodel but also retaining richer local information. For example, ResNet-based\nmodels typically have Ô¨Åve blocks, each of which downsamples the feature maps\nby a factor of two. We take the outputs from the 4th ( g0 ‚ààR\nH\n16 √óW\n16 √óC0 ), 3rd\n(g1 ‚ààR\nH\n8 √óW\n8 √óC1 ) and 2nd (g2 ‚ààR\nH\n4 √óW\n4 √óC2 ) blocks to fuse with the results from\nTransformer (Fig. 1). Moreover, our CNN branch is Ô¨Çexible that any oÔ¨Ä-the-shelf\nconvolutional network can be applied.\nBiFusion Module.To eÔ¨Äectively combine the encoded features from CNNs\nand Transformers, we propose a new BiFusion module (refer to Fig. 1) that incor-\nporates both self-attention and multi-modal fusion mechanisms. SpeciÔ¨Åcally, we\nobtain the fused feature representation fi,i = 0,1,2 by the following operations:\nÀÜti = ChannelAttn(ti)\nÀÜbi = Conv(tiWi\n1 ‚äôgiWi\n2)\nÀÜ gi = SpatialAttn(gi)\nfi = Residual([ÀÜbi,ÀÜti,ÀÜ gi])\n(2)\nwhere Wi\n1 ‚ààRDi√óLi, Wi\n2 ‚ààRCi√óLi, |‚äô| is the Hadamard product and Conv\nis a 3x3 convolution layer. The channel attention is implemented as SE-Block\nproposed in [10] to promote global information from the Transformer branch.\nThe spatial attention is adopted from CBAM [30] block as spatial Ô¨Ålters to\nenhance local details and suppress irrelevant regions, as low-level CNN features\ncould be noisy. The Hadamard product then models the Ô¨Åne-grained interaction\nbetween features from the two branches. Finally, the interaction features ÀÜbi\nand attended features ÀÜti,ÀÜ gi are concatenated and passed through a Residual\nblock. The resulting feature fi eÔ¨Äectively captures both the global and local\ncontext for the current spatial resolution. To generate Ô¨Ånal segmentation, fis\nare combined using the attention-gated (AG) skip-connection [20], where we\nhave ÀÜfi+1 = Conv([Up(ÀÜfi),AG(fi+1,Up(ÀÜfi))]) and ÀÜf0 = f0, as in Fig. 1.\nLoss Function.The full network is trained end-to-end with the weighted\nIoU loss and binary cross entropy loss L = Lw\nIoU + Lw\nbce, where boundary pix-\nTransFuse: Fusing Transformers and CNNs for Medical Image Segmentation 5\nels receive larger weights [17]. Segmentation prediction is generated by a sim-\nple head, which directly resizes the input feature maps to the original reso-\nlution and applies convolution layers to generate M maps, where M is the\nnumber of classes. Following [8], We use deep supervision to improve the gra-\ndient Ô¨Çow by additionally supervising the transformer branch and the Ô¨Årst\nfusion branch. The Ô¨Ånal training loss is given by L = Œ±L\n(\nG,head(ÀÜf2)\n)\n+\nŒ≥L\n(\nG,head(t2)\n)\n+ Œ≤L\n(\nG,head(f0)\n)\n, where Œ±, Œ≥, Œ≤ are tunnable hyperparame-\nters and G is groundtruth.\n3 Experiments and Results\nData Acquisition. To better evaluate the eÔ¨Äectiveness of TransFuse, four\nsegmentation tasks with diÔ¨Äerent imaging modalities, disease types, target ob-\njects, target sizes, etc. are considered: 1) Polyp Segmentation, where Ô¨Åve public\npolyp datasets are used: Kvasir [14], CVC-ClinicDB [2], CVC-ColonDB [24],\nEndoScene [27] and ETIS [21]. The same split and training setting as described\nin [8,11] are adopted, i.e. 1450 training images are solely selected from Kvasir\nand CVC-ClinicDB while 798 testing images are from all Ô¨Åve datasets. Before\nprocessing, the resolution of each image is resized into 352 √ó352 as [8,11]. 2)\nSkin Lesion Segmentation, where the publicly available 2017 International Skin\nImaging Collaboration skin lesion segmentation dataset (ISIC2017) [5] is used 3.\nISIC2017 provides 2000 images for training, 150 images for validation and 600\nimages for testing. Following the setting in [1], we resize all images to 192 √ó256.\n3) Hip Segmentation, where a total of 641 cases are collected from a hospital\nwith average size of 2942 √ó2449 and pixel spacing as 0.143mm 4. Each image is\nannotated by a clinical expert and double-blind reviewed by two specialists. We\nresized all images into 352 √ó352, and randomly split images with a ratio of 7:1:2\nfor training, validation and testing. 4) Prostate Segmentation, where volumetric\nProstate Multi-modality MRIs from the Medical Segmentation Decathlon [22]\nare used. The dataset contains multi-modal MRIs from 32 patients, with a me-\ndian volume shape of 20 √ó320√ó319. Following the setting in [12], we reshape all\nMRI slices to 320 √ó320, and independently normalize each volume using z-score\nnormalization.\nImplementation Details.TransFuse was built in PyTorch framework [16] and\ntrained using a single NVIDIA-A100 GPU. The values of Œ±, Œ≤ and Œ≥ were set to\n0.5, 0.3, 0.2 empirically. Adam optimizer with learning rate of 1e-4 was adopted\n3 Another similar dataset ISIC2018 was not used because of the missing test set anno-\ntation, which makes fair comparison between existing works can be hardly achieved.\n4 All data are from diÔ¨Äerent patients and with ethics approval, which consists of 267\npatients of Avascular Necrosis, 182 patients of Osteoarthritis, 71 patients of Femur\nNeck Fracture, 33 patients of Pelvis Fracture, 26 patients of Developmental Dysplasia\nof the Hip and 62 patients of other dieases.\n6 Y. Zhang et al.\nand all models were trained for 30 epochs as well as batch size of 16, unless\notherwise speciÔ¨Åed.\nIn polyp segmentation experiments, no data augmentation was used except\nfor multi-scale training, as in [8,11]. For skin lesion and hip segmentation, data\naugmentation including random rotation, horizontal Ô¨Çip, color jittering, etc. were\napplied during training. A smaller learning rate of 7e-5 was found useful for skin\nlesion segmentation. Finally, we follow the nnU-Net framework [12] to train and\nevaluate our model on Prostate Segmentation, using the same data augmenta-\ntion and post-processing scheme. As selected pretrained datasets and branch\nbackbones may aÔ¨Äect the performance diÔ¨Äerently, three variants of TransFuse\nare provided to 1) better demonstrate the eÔ¨Äectiveness as well as Ô¨Çexibility of\nour approach; 2) conduct fair comparisons with other methods. TransFuse-S\nis implemented with ResNet-34 (R34) and 8-layer DeiT-Small (DeiT-S) [25] as\nbackbones of the CNN branch and Transformer branch respectively. Similarly,\nTransFuse-L is built based on Res2Net-50 and 10-layer DeiT-Base (DeiT-B),\nwhile TransFuse-L* uses ResNetV2-50 and ViT-B [7]. Note that ViTs and DeiTs\nhave the same backbone architecture and they mainly diÔ¨Äer in the pre-trained\nstrategy and dataset: the former is trained on ImageNet21k while the latter is\ntrained on ImageNet1k with heavier data augmentation.\nEvaluation Results TransFuse is evaluated on both 2D and 3D datasets to\ndemonstrate the eÔ¨Äectiveness. As diÔ¨Äerent medical image segmentation tasks\nserve diÔ¨Äerent diagnosis or operative purposes, we follow the commonly used\nevaluation metrics for each of the segmentation tasks to quantitatively analyze\nthe results. Selected visualization results of TransFuse-S are shown in Fig. 2.\nResults of Polyp Segmentation.We Ô¨Årst evaluate the performance of\nour proposed method on polyp segmentation against a variety of SOTA meth-\nods, in terms of mean Dice (mDice) and mean Intersection-Over-Union (mIoU).\nAs in Tab. 1, our TransFuse-S/L outperform CNN-based SOTA methods by a\nlarge margin. SpeciÔ¨Åcally, TransFuse-S achieves 5.2% average mDice improve-\nment on the unseen datasets (ColonDB, EndoSene and ETIS). Comparing to\nother transformer-based methods, TransFuse-L* also shows superior learning\nability on Kvasir and ClinicDB, observing an increase of 1.3% in mIoU com-\npared to TransUnet. Besides, the eÔ¨Éciency in terms of the number of parame-\nters as well as inference speed is evaluated on an RTX2080Ti with Xeon(R) Gold\n5218 CPU. Comparing to prior CNN-based arts, TransFuse-S achieves the best\nperformance while using only 26.3M parameters, about 20% reduction with re-\nspect to HarDNet-MSEG (33.3M) and PraNet (32.5M). Moreover, TransFuse-S\nis able to run at 98.7 FPS, much faster than HarDNet-MSEG (85.3 FPS) and\nPraNet (63.4 FPS), thanks to our proposed parallel-in-branch design. Similarly,\nTransFuse-L* not only achieves the best results compared to other Transformer-\nbased methods, but also runs at 45.3 FPS, about 12% faster than TransUnet.\nResults of Skin Lesion Segmentation.The ISBI 2017 challenge ranked\nmethods according to Jaccard Index [5] on the ISIC 2017 test set. Here, we use\nJaccard Index, Dice score and pixel-wise accuracy as evaluation metrics. The\nTransFuse: Fusing Transformers and CNNs for Medical Image Segmentation 7\nTable 1: Quantitative results on polyp segmentation datasets compared to pre-\nvious SOTAs. The results of [4] is obtained by running the released code and we\nimplement SETR-PUP. ‚Äò-‚Äô means results not available.\nMethods Kvasir ClinicDB ColonDB EndoScene ETIS\nmDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU\nU-Net [18] 0.818 0.746 0.823 0.750 0.512 0.444 0.710 0.627 0.398 0.335\nU-Net++ [33] 0.821 0.743 0.794 0.729 0.483 0.410 0.707 0.624 0.401 0.344\nResUNet++ [13] 0.813 0.793 0.796 0.796 - - - - - -\nPraNet [8] 0.898 0.840 0.899 0.849 0.709 0.640 0.871 0.797 0.628 0.567\nHarDNet-MSEG [11] 0.912 0.857 0.932 0.882 0.731 0.660 0.887 0.821 0.677 0.613\nTransFuse-S 0.918 0.8680.918 0.868 0.773 0.6960.902 0.833 0.733 0.659\nTransFuse-L 0.918 0.868 0.934 0.8860.744 0.676 0.904 0.838 0.737 0.661\nSETR-PUP [32] 0.911 0.854 0.934 0.885 0.773 0.690 0.889 0.814 0.726 0.646\nTransUnet [4] 0.913 0.857 0.935 0.887 0.781 0.699 0.893 0.824 0.731 0.660\nTransFuse-L* 0.920 0.870 0.942 0.897 0.781 0.706 0.894 0.826 0.737 0.663\nTable 2: Quantitative results on ISIC\n2017 test set. Results with backbones use\nweights pretrained on ImageNet.\nMethods Backbones Epochs Jaccard Dice Accuracy\nCDNN [31] - - 0.765 0.849 0.934\nDDN [15] ResNet-18 600 0.765 0.866 0.939\nFrCN [1] VGG16 200 0.771 0.871 0.940\nDCL-PSI [3] ResNet-101 150 0.777 0.857 0.941\nSLSDeep [19] ResNet-50 100 0.7820.878 0.936\nUnet++ [33] ResNet-34 30 0.775 0.858 0.938\nTransFuse-SR34+DeiT-S 30 0.7950.872 0.944\nTable 3: Results on in-house hip\ndataset. All models use pretrained\nbackbones from ImageNet and are\nof similar size ( ‚àº 26M). HD and\nASD are measured in mm.\nMethods Pelvis L-Femur R-Femur\nHD ASD HD ASD HD ASD\nUnet++ [33] 14.4 1.21 9.33 0.932 5.04 0.813\nHRNetV2 [28] 14.2 1.13 6.36 0.769 5.98 0.762\nTransFuse-S9.81 1.09 4.44 0.767 4.19 0.676\ncomparison results against leading methods are presented in Tab. 2. TransFuse-\nS is about 1.7% better than the previous SOTA SLSDeep [19] in Jaccard score,\nwithout any pre- or post-processing and converges in less than 1 /3 epochs. Be-\nsides, our results outperform Unet++ [33] that employs pretrained R34 as back-\nbone and has comparable number of parameters with TransFuse-S (26.1M vs\n26.3M). Again, the results prove the superiority of our proposed architecture.\nResults of Hip Segmentation.Tab. 3 shows our results on hip segmenta-\ntion task, which involves three human body parts: Pelvis, Left Femur (L-Femur)\nand Right Femur (R-Femur). Since the contour is more important in dianosis\nand THA preoperative planning, we use HausdorÔ¨Ä Distance (HD) and Average\nSurface Distance (ASD) to evaluate the prediction quality. Compared to the\ntwo advanced segmentation methods [33,28], TransFuse-S performs the best on\nboth metrics and reduces HD signiÔ¨Åcantly (30% compared to HRNetV2 as well\nas 34% compared to Unet++ on average), indicating that our proposed method\nis able to capture Ô¨Åner structure and generates more precise contour.\nResults of Prostate Segmentation.We compare TransFuse-S with nnU-\nNet [12], which ranked 1st in the prostate segmentation challenge [22]. We follow\nthe same preprocessing, training as well as evaluation schemes of the publicly\n8 Y. Zhang et al.\nTable 4: Quantitative results on prostate MRI segmentation. PZ, TZ stand for\nthe two labeled classes (peripheral and transition zone) and performance (PZ,\nTZ and mean) is measure by dice score.\nMethods PZ TZ Mean Params Throughput\nnnUnet-2d [12] 0.6285 0.8380 0.7333 29.97M 0.209s/vol\nnnUnet-3d full[12] 0.6663 0.8410 0.7537 44.80M 0.381s/vol\nTransFuse-S 0.6738 0.8539 0.7639 26.30M 0.192s/vol\nTable 5: Ablation study on parallel-in-\nbranch design. Res: Residual.\nIndex Backbones Composition Fusion Kvasir ColonDB\nE.1 R34 Sequential - 0.890 0.645\nE.2 DeiT-S Sequential - 0.889 0.727\nE.3 R34+DeiT-S Sequential - 0.908 0.749\nE.4 R34+VGG16 Parallel BiFusion 0.896 0.651\nE.5 R34+DeiT-S Parallel Concat+Res 0.912 0.764\nE.6 R34+DeiT-S Parallel BiFusion 0.918 0.773\nTable 6: Ablation study on BiFu-\nsion module. Res: Residual; TFM:\nTransformer; Attn: Attention.\nFusion Jaccard Dice Accuracy\nConcat+Res 0.778 0.857 0.939\n+CNN Spatial Attn 0.782 0.861 0.941\n+TFM Channel Attn 0.787 0.865 0.942\n+Dot Product 0.795 0.872 0.944\navailable nnU-Net framework5 and report the 5-fold cross validation results in\nTab. 4. We can Ô¨Ånd that TransFuse-S surpasses nnUNet-2d by a large margin\n(+4.2%) in terms of the mean dice score. Compared to nnUNet-3d, TransFuse-S\nnot only achieves better performance, but also reduces the number of parameters\nby ‚àº41% and increases the throughput by ‚àº50% (on GTX1080).\nAblation Study.An ablation study is conducted to evaluate the eÔ¨Äective-\nness of the parallel-in-branch design as well as BiFusion module by varying design\nchoices of diÔ¨Äerent backbones, compositions and fusion schemes. Aseen (Kvasir)\nand an unseen (ColonDB) datasets from polyp are used, and results are recorded\nin mean Dice. In Tab. 5, by comparing E.3 against E.1 and E.2, we can see that\ncombining CNN and Transformer leads to better performance. Further, by com-\nparing E.3 against E.5, E.6, we observe that the parallel models perform better\nthan the sequential counterpart. Moreover, we evaluate the performance of a\ndouble branch CNN model (E.4) using the same parallel structure and fusion\nsettings with our proposed E.6. We observe that E.6 outperforms E.4 by 2.2%\nin Kvasir and 18.7% in ColonDB, suggesting that the CNN branch and trans-\nformer branch are complementary to each other, leading to better fusion results.\nLastly, performance comparison is conducted between another fusion module\ncomprising concatenation followed by a residual block and our proposed BiFu-\nsion module (E.5 and E.6). Given the same backbone and composition setting,\nE.6 with BiFusion achieves better results. Additional experiments conducted on\nISIC2017 are presented in Tab. 6 to verify the design choice of BiFusion module,\nfrom which we Ô¨Ånd that each component shows its unique beneÔ¨Åt.\n5 https://github.com/MIC-DKFZ/nnUNet\nTransFuse: Fusing Transformers and CNNs for Medical Image Segmentation 9\nPolyp Segmentation\nSkin Lesion Segmentation\nHip Segmentation\nProstate Segmentation\nFig. 2: Results visualization on all three tasks (best viewed in color). Each row\nfollows the repeating sequence of ground truth (GT) and predictions (Pred).\n4 Conclusion\nIn this paper, we present a novel strategy to combine Transformers and CNNs\nwith late fusion for medical image segmentation. The resulting architecture,\nTransFuse, leverages the inductive bias of CNNs on modeling spatial correlation\nand the powerful capability of Transformers on modelling global relationship.\nTransFuse achieves SOTA performance on a variety of segmentation tasks whilst\nbeing highly eÔ¨Écient on both the parameters and inference speed. We hope that\nthis work can bring a new perspective on using transformer-based architecture.\nIn the future, we plan to improve the eÔ¨Éciency of the vanilla transformer layer as\nwell as test TransFuse on other medical-related tasks such as landmark detection\nand disease classiÔ¨Åcation.\nAcknowledgement. We gratefully thank Weijun Wang, MD, Zhefeng Chen,\nMD, Chuan He, MD, Zhengyu Xu, Huaikun Xu for serving as our medical ad-\nvisors on hip segmentation project.\n10 Y. Zhang et al.\nReferences\n1. Al-Masni, M.A., Al-Antari, M.A., et al.: Skin lesion segmentation in dermoscopy\nimages via deep full resolution convolutional networks. Computer methods and\nprograms in biomedicine (2018)\n2. Bernal, J., S¬¥ anchez, F.J., et al.: Wm-dova maps for accurate polyp highlighting in\ncolonoscopy: Validation vs. saliency maps from physicians. Computerized Medical\nImaging and Graphics (2015)\n3. Bi, L., Kim, J., et al.: Step-wise integration of deep class-speciÔ¨Åc learning for\ndermoscopic image segmentation. Pattern recognition (2019)\n4. Chen, J., Lu, Y., et al.: Transunet: Transformers make strong encoders for medical\nimage segmentation. arXiv preprint arXiv:2102.04306 (2021)\n5. Codella, N.C., Gutman, D., et al.: Skin lesion analysis toward melanoma detection:\nA challenge at the 2017 international symposium on biomedical imaging (isbi),\nhosted by the international skin imaging collaboration (isic). In: 2018 IEEE 15th\nInternational Symposium on Biomedical Imaging (ISBI 2018) (2018)\n6. Deng, J., Dong, W., et al.: Imagenet: A large-scale hierarchical image database.\nIn: 2009 IEEE conference on computer vision and pattern recognition (2009)\n7. Dosovitskiy, A., Beyer, L., et al.: An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n8. Fan, D.P., Ji, G.P., Zhou, T., Chen, G., Fu, H., Shen, J., Shao, L.: Pranet: Parallel\nreverse attention network for polyp segmentation. In: International Conference on\nMedical Image Computing and Computer-Assisted Intervention (2020)\n9. Hesamian, M.H., Jia, W., He, X., Kennedy, P.: Deep learning techniques for medical\nimage segmentation: achievements and challenges. Journal of digital imaging (2019)\n10. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition (2018)\n11. Huang, C.H., Wu, H.Y., Lin, Y.L.: Hardnet-mseg: A simple encoder-decoder polyp\nsegmentation neural network that achieves over 0.9 mean dice and 86 fps. arXiv\npreprint arXiv:2101.07172 (2021)\n12. Isensee, F., J¬® ager, P.F., et al.: Automated design of deep learning methods for\nbiomedical image segmentation. arXiv preprint arXiv:1904.08128 (2019)\n13. Jha, D., Smedsrud, P.H., Riegler, M.A., Johansen, D., De Lange, T., Halvorsen,\nP., Johansen, H.D.: Resunet++: An advanced architecture for medical image seg-\nmentation. In: 2019 IEEE International Symposium on Multimedia (ISM) (2019)\n14. Jha, D., Smedsrud, P.H., et al.: Kvasir-seg: A segmented polyp dataset. In: Inter-\nnational Conference on Multimedia Modeling (2020)\n15. Li, H., He, X., et al.: Dense deconvolutional network for skin lesion segmentation.\nIEEE journal of biomedical and health informatics (2018)\n16. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\nT., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-\nperformance deep learning library. Advances in neural information processing sys-\ntems 32, 8026‚Äì8037 (2019)\n17. Qin, X., Zhang, Z., Huang, C., Gao, C., Dehghan, M., Jagersand, M.: Basnet:\nBoundary-aware salient object detection. In: Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (2019)\n18. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention (2015)\nTransFuse: Fusing Transformers and CNNs for Medical Image Segmentation 11\n19. Sarker, M.M.K., Rashwan, H.A., et al.: Slsdeep: Skin lesion segmentation based\non dilated residual and pyramid pooling networks. In: International Conference on\nMedical Image Computing and Computer-Assisted Intervention (2018)\n20. Schlemper, J., Oktay, O., et al.: Attention gated networks: Learning to leverage\nsalient regions in medical images. Medical image analysis (2019)\n21. Silva, J., Histace, A., Romain, O., Dray, X., Granado, B.: Toward embedded detec-\ntion of polyps in wce images for early diagnosis of colorectal cancer. International\njournal of computer assisted radiology and surgery (2014)\n22. Simpson, A.L., Antonelli, M., et al.: A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithms. arXiv preprint\narXiv:1902.09063 (2019)\n23. Srivastava, R.K., GreÔ¨Ä, K., Schmidhuber, J.: Highway networks. arXiv preprint\narXiv:1505.00387 (2015)\n24. Tajbakhsh, N., et al.: Automated polyp detection in colonoscopy videos using shape\nand context information. IEEE transactions on medical imaging (2015)\n25. Touvron, H., Cord, M., et al.: Training data-eÔ¨Écient image transformers & distil-\nlation through attention. arXiv preprint arXiv:2012.12877 (2020)\n26. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. arXiv preprint arXiv:1706.03762 (2017)\n27. V¬¥ azquez, D., Bernal, J., et al.: A benchmark for endoluminal scene segmentation\nof colonoscopy images. Journal of healthcare engineering (2017)\n28. Wang, J., Sun, K., et al.: Deep high-resolution representation learning for visual\nrecognition. IEEE transactions on pattern analysis and machine intelligence (2020)\n29. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceed-\nings of the IEEE conference on computer vision and pattern recognition (2018)\n30. Woo, S., Park, J., et al.: Cbam: Convolutional block attention module. In: Pro-\nceedings of the European conference on computer vision (ECCV) (2018)\n31. Yuan, Y., Lo, Y.C.: Improving dermoscopic image segmentation with enhanced\nconvolutional-deconvolutional networks. IEEE journal of biomedical and health\ninformatics (2017)\n32. Zheng, S., Lu, J., et al.: Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. arXiv preprint arXiv:2012.15840 (2020)\n33. Zhou, Z., et al.: Unet++: Redesigning skip connections to exploit multiscale fea-\ntures in image segmentation. IEEE transactions on medical imaging (2019)"
}