{
  "title": "End-to-end Lane Shape Prediction with Transformers",
  "url": "https://openalex.org/W3099616417",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2107319612",
      "name": "Liu Ruijin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224256786",
      "name": "Yuan Zejian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989403455",
      "name": "Liu, Tie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3049740049",
      "name": "Xiong Zhiliang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2895707844",
    "https://openalex.org/W2032438872",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2981441441",
    "https://openalex.org/W2913960518",
    "https://openalex.org/W3006566272",
    "https://openalex.org/W2211466563",
    "https://openalex.org/W2964199920",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2745410201",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2963856865",
    "https://openalex.org/W3017943203",
    "https://openalex.org/W2083183663",
    "https://openalex.org/W2131076267",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2963223517",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963611454"
  ],
  "abstract": "Lane detection, the process of identifying lane markings as approximated curves, is widely used for lane departure warning and adaptive cruise control in autonomous vehicles. The popular pipeline that solves it in two steps -- feature extraction plus post-processing, while useful, is too inefficient and flawed in learning the global context and lanes' long and thin structures. To tackle these issues, we propose an end-to-end method that directly outputs parameters of a lane shape model, using a network built with a transformer to learn richer structures and context. The lane shape model is formulated based on road structures and camera pose, providing physical interpretation for parameters of network output. The transformer models non-local interactions with a self-attention mechanism to capture slender structures and global context. The proposed method is validated on the TuSimple benchmark and shows state-of-the-art accuracy with the most lightweight model size and fastest speed. Additionally, our method shows excellent adaptability to a challenging self-collected lane detection dataset, showing its powerful deployment potential in real applications. Codes are available at https://github.com/liuruijin17/LSTR.",
  "full_text": "End-to-end Lane Shape Prediction with Transformers\nRuijin Liu1 Zejian Yuan1 Tie Liu2 Zhiliang Xiong3\n1Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University, China\n2College of Information Engineering, Capital Normal University, China\n3Shenzhen Forward Innovation Digital Technology Co. Ltd, China\n1lrj466097290@stu.xjtu.edu.cn 1yuan.ze.jian@xjtu.edu.cn\n2liutiel@163.com 3leslie.xiong@forward-innovation.com\nAbstract\nLane detection, the process of identifying lane markings\nas approximated curves, is widely used for lane departure\nwarning and adaptive cruise control in autonomous vehi-\ncles. The popular pipeline that solves it in two steps—\nfeature extraction plus post-processing, while useful, is too\ninefﬁcient and ﬂawed in learning the global context and\nlanes’ long and thin structures. To tackle these issues, we\npropose an end-to-end method that directly outputs param-\neters of a lane shape model, using a network built with\na transformer to learn richer structures and context. The\nlane shape model is formulated based on road structures\nand camera pose, providing physical interpretation for pa-\nrameters of network output. The transformer models non-\nlocal interactions with a self-attention mechanism to cap-\nture slender structures and global context. The proposed\nmethod is validated on the TuSimple benchmark and shows\nstate-of-the-art accuracy with the most lightweight model\nsize and fastest speed. Additionally, our method shows\nexcellent adaptability to a challenging self-collected lane\ndetection dataset, showing its powerful deployment poten-\ntial in real applications. Codes are available at https:\n//github.com/liuruijin17/LSTR.\n1. Introduction\nVision-based lane marking detection is a fundamental\nmodule in autonomous driving, which has achieved remark-\nable performance on applications such as lane departure\nwarning, adaptive cruise control, and trafﬁc understand-\ning [11, 13, 20, 14]. In real applications, detecting lanes\ncould be very challenging. The lane marking is a long and\nthin structure with strong shape prior but few appearance\nclues [14]. Besides, lane markings vary in different types,\nlight changes, and occlusions of vehicles and pedestrians,\nwhich requires the global context information to infer the\nvacancy or occluded part. Moreover, the high running efﬁ-\nciency and transferring adaptability of algorithms are indis-\npensable for deployment on mobile devices [15].\nExisting methods [12, 14, 6, 15, 7] take advantage of\nthe powerful representation capabilities of convolution neu-\nral networks (CNNs) to improve the performance of lane\ndetection task by a large margin over traditional meth-\nods [11, 13, 20] which are based on hand-crafted fea-\ntures and Hough Transform. However, current CNNs-based\nmethods still are ﬂawed in addressing the aforementioned\nchallenges. The earlier methods [12, 15] typically ﬁrst gen-\nerate segmentation results and then employ post-processing,\nsuch as segment clustering and curve ﬁtting. These meth-\nods are inefﬁcient and ignore global context when learn-\ning to segment lanes [7, 14]. To tackle the context learn-\ning issue, some methods [14, 21, 4] use message passing\nor extra scene annotations to capture the global context for\nenhancement of ﬁnal performance, but these methods in-\nevitably consume more time and data cost [6]. Unlike these\nmethods, a soft attention based method [6] generates a spa-\ntial weighting map that distills a richer context without ex-\nternal consumes. However, the weighting map only mea-\nsures the feature’s importance, limiting its usage to con-\nsider the dependencies between features that support to in-\nfer slender structures. On the other hand, to improve the al-\ngorithm’s efﬁciency, [8] transfers the pipeline in object de-\ntection to detect lanes without the above segmentation pro-\ncedure and post-processing, but it relies on complex anchor\ndesigning choices and additional non-maximal suppression,\nmaking it even slower than most lane detectors. Recently,\na method [18] reframes the task as lane markings ﬁtting\nby polynomial regression, which achieves signiﬁcant efﬁ-\nciency but still has a large accuracy gap with other methods\ndue to the neglect of learning the global context.\nTo tackle these issues, we propose to reframe the lane\ndetection output as parameters of a lane shape model and\narXiv:2011.04233v2  [cs.CV]  28 Nov 2020\ndevelop a network built with non-local building blocks to\nreinforce the learning of global context and lanes’ slen-\nder structures. The output for each lane is a group of pa-\nrameters which approximates the lane marking with an ex-\nplicit mathematical formula derived from road structures\nand the camera pose. Given speciﬁc priors such as cam-\nera intrinsic, those parameters can be used to calculate the\nroad curvature and camera pitch angle without any 3D sen-\nsors. Next, inspired by natural language processing mod-\nels which widely employ transformer block [19] to explic-\nitly model long-range dependencies in language sequence,\nwe develop a transformer-based network that summarizes\ninformation from any pairwise visual features, enabling it\nto capture lanes’ long and thin structures and global con-\ntext. The whole architecture predicts the proposed outputs\nat once and is trained end-to-end with a Hungarian loss.\nThe loss applies bipartite matching between predictions and\nground truths to ensure one-to-one disorderly assignment,\nenabling the model to eliminate an explicit non-maximal\nsuppression process.\nThe effectiveness of the proposed method is validated in\nthe conventional TuSimple lane detection benchmark [1].\nWithout bells and whistles, our method achieves state-of-\nthe-art accuracy and the lowest false positive rate with the\nmost lightweight model size and fastest speed. In addition,\nto evaluate the adaptability to new scenes, we collect a large\nscale challenging dataset called Forward View Lane (FVL)\nin multiple cities across various scenes (urban and highway,\nday and night, various trafﬁc and weather conditions). Our\nmethod shows strong adaptability to new scenes even that\nthe TuSimple dataset does not contain, e.g., night scenes.\nThe main contributions of this paper can be summarized\nas follows:\n• We propose a lane shape model whose parameters\nserve as directly regressed output and reﬂect road\nstructures and the camera pose.\n• We develop a transformer-based network that con-\nsiders non-local interactions to capture long and thin\nstructures for lanes and the global context.\n• Our method achieves state-of-the-art accuracy with the\nleast resource consumption and shows excellent adapt-\nability to a new challenging self-collected lane detec-\ntion dataset.\n2. Related Work\nThe authors of [11] provide a good overview of the tech-\nniques used in traditional lane detection methods. Feature-\nbased methods usually extract low-level features (lane seg-\nments) by Hough transform variations, then use clustering\nalgorithms such as DBSCAN (Density Based Spatial Clus-\ntering of Applications with Noise) to generate ﬁnal lane\ndetections [13]. Model-based methods use top-down pri-\nors such as geometry and road surface [20], which describe\nlanes in more detail and shows excellent simplicity.\nIn recent years, methods based on deep neural networks\nhave been shown to outperform traditional approaches. Ear-\nlier methods [12, 15] typically extract dense segmentation\nresults and then employ post-processing such as segment\nclustering and curve ﬁtting. Their performances are lim-\nited by the initial segmentation of lanes due to the difﬁcul-\nties of learning such long and thin structures. To address\nthis issue, SCNN [14] uses message passing to capture a\nglobal context, exploiting richer spatial information to in-\nfer occluded parts. [21, 4] adopt extra scene annotations to\nguide the model’s training, which enhances the ﬁnal perfor-\nmance. Unlike them that need additional data and time cost,\nENet-SAD [6] applies a soft attention mechanism, which\ngenerates weighting maps to ﬁlter out unimportant features\nand distill richer global information. In contrast to infer-\nring based on dense segmentation results, PINet [7] extracts\na sparse point cloud to save the computations, but it also\nrequires inefﬁcient post-processing like outlier removal.\nIn contrast to these methods, our method directly outputs\nparameters of a lane shape model. The whole method works\nin an end-to-end fashion without any intermediate represen-\ntation or post-processing.\nIn literature, some end-to-end lane detectors have been\nproposed recently. Line-CNN [8] transfers the success of\nFaster-RCNN [16] into lane detection by predicting offsets\nbased on pre-designed straight rays (like anchor boxes),\nwhich achieves state-of-the-art accuracy. However, it in-\nevitably suffers the drawbacks of complex ad-hoc heuris-\ntics choices to design rays and additional non-maximal sup-\npress, making it even slower than most lane detectors. Poly-\nLaneNet [18] reframes the lane detection task as a polyno-\nmial regression problem, which achieved the highest efﬁ-\nciency. However, its accuracy still has a large gap with\nother methods, especially has difﬁculty in predicting lane\nlines with accentuated curvatures due to neglect of learning\nglobal information and ignorant of road structures model-\ning.\nIn this work, our approach also expects a parametric out-\nput but differs in that these parameters are derived from a\nlane shape model which models the road structures and the\ncamera pose. These output parameters have explicit physi-\ncal meanings rather than simple polynomial coefﬁcients. In\naddition, our network is built with transformer block [19]\nthat performs attention in modeling non-local interactions,\nenabling it to reinforce the capture of long and thin struc-\ntures for lanes and learning of global context information.\n3. Method\nOur end-to-end method reframes the output as parame-\nters of a lane shape model. Parameters are predicted by us-\ning a transformer-based network trained with a Hungarian\nﬁtting loss.\n3.1. Lane Shape Model\nThe prior model of the lane shape is deﬁned as a polyno-\nmial on the road. Typically, a cubic curve is used to approx-\nimate a single lane line on ﬂat ground:\nX = kZ3 + mZ2 + nZ+ b, (1)\nwhere k,m,n and b are real number parameters, k ̸= 0.\nThe (X,Z) indicates the point on the ground plane. When\nthe optical axis is parallel to the ground plane, the curve\nprojected from the road onto the image plane is:\nu= k′\nv2 + m′\nv + n′ + b′ ×v, (2)\nwhere k′,m′,n′,b′ are composites of parameters and cam-\nera intrinsic and extrinsic parameters, and (u,v) is a pixel\nat the image plane.\nIn the case of a tilted camera whose optical axis is at an\nangle of φto the ground plane, the curve transformed from\nthe untitled image plane to the tilted image plane is:\nu′ = k′ ×cos2 φ\n(v′ −fsin φ)2 + m′ cos φ\n(v′ −fsin φ) + n′\n+b′ ×v′\ncos φ −b′ ×ftan φ,\n(3)\nhere f is the focal length in pixels, and (u′,v′) is the cor-\nresponding pitch-transformed position. When φ = 0, the\ncurve function Eq. 3 will be simpliﬁed to Eq. 2. Details of\nthe derivation can be reviewed in Sec. 7.\nCurve re-parameterization. By combining parameters\nwith the pitch angle φ, the curve in a tilted camera plane\nhas the form of:\nu′ = k′′\n(v′ −f′′)2 + m′′\n(v′ −f′′) + n′ + b′′ ×v′ −b′′′, (4)\nhere, the two constant terms n′ and b′′′ are not integrated\nbecause they contain different physical parameters.\nApart from that, the vertical starting and ending offset\nα,β are also introduced to parameterize each lane line.\nThese two parameters provide essential localization infor-\nmation to describe the upper and lower boundaries of lane\nlines.\nIn real road conditions, lanes typically have a global con-\nsistent shape. Thus, the approximated arcs have a equal cur-\nvature from the left to the right lanes, sok′′,f′′,m′′,n′ will\nbe shared for all lanes. Therefore, the output for the t-th\nlane is re-parameterized to gt:\ngt = (k′′,f′′,m′′,n′,b′′\nt ,b′′′\nt ,αt,βt) (5)\nwhere t ∈{1,...,T }, T is the number of lanes in an im-\nage. Each lane only differs in bias terms and lower/upper\nboundaries.\n3.2. Hungarian Fitting Loss\nThe Hungarian ﬁtting loss performs a bipartite matching\nbetween predicted parameters and ground truth lanes to ﬁnd\npositives and negatives. The matching problem is efﬁciently\nsolved by the Hungarian algorithm. Then the matching re-\nsult is used to optimize lane-speciﬁc regression losses.\nBipartite matching.Our method predicts a ﬁxedNcurves,\nwhere N is set to be larger than the maximum number\nof lanes in the image of a typical dataset. Let us denote\nthe predicted curves by H= {hi|hi = (ci,gi)}N\ni=1, where\nci ∈{0,1}(0: non-lane, 1: lane). The ground truth lane\nmarking is represented by a sequence ˆ s= (ˆu′\nr,ˆv′\nr)R\nr=1,\nwhere rsequentially indexes the sample point within range\nRand ˆv′\nr+1 > ˆv′\nr. Since the number of predicted curves N\nis larger than the number of ground truth lanes, we consider\nthe ground truth lanes also as a set of size N padded with\nnon-lanes L=\n{\nˆli|ˆli = (ˆci,ˆsi)\n}N\ni=1\n. We formulate the bi-\npartite matching between the set of curves and the set of\nground truth lane markings as a cost minimization problem\nby searching an optimal injective function z : L→H , i.e.,\nz(i) is the index of curve assigned to ﬁtting ground-truth\nlane i:\nˆz= arg min\nz\nN∑\ni=1\nd\n(\nˆli,hz(i)\n)\n, (6)\nwhere dmeasures the matching cost given a speciﬁc permu-\ntation z between the i-th ground truth lane and a predicted\ncurve with index z(i). Following prior work (e.g., [17]),\nthis problem can be solved efﬁciently by the Hungarian al-\ngorithm.\nFor the prediction with index z(i), the probability of\nclass ˆci is deﬁned as pz(i) (ˆci), and the ﬁtting lane sequence\nis deﬁned as sz(i) = (u′\nri,ˆv′\nri)Ri\nr=1, where Ri is the length\nof i-th lane and u′\nri is calculated using Eq. 4 based on the\npredicted group of parameters gi. Then the matching cost d\nhas the form of:\nd= −ω1pz(i) (ˆci) +1 (ˆci = 1)ω2L1\n(ˆsi,sz(i)\n)\n+ 1 (ˆci = 1)ω3L1\n(\nˆαi,αz(i),ˆβi,βz(i)\n)\n,\n(7)\nwhere 1 (·) is an indicator function, ω1, ω2 and ω3 adjusts\nthe effect of the matching terms, and L1 is the commonly-\nused mean absolute error. We use the probabilities instead\nof log-probabilities following [3] because this makes the\nclassiﬁcation term commensurable to the curve ﬁtting term.\nRegression loss.The regression loss calculates the error for\nall pairs matched in the previous step with the form of:\nL=\nN∑\ni=1\n−ω1 log pˆz(i) (ˆci) +1 (ˆci = 1)ω2L1\n(ˆsi,sˆz(i)\n)\n+ 1 (ˆci = 1)ω3L1\n(\nˆαi,αˆz(i),ˆβi,βˆz(i)\n)\n,\n(8)\n4\n&-- \nc\nb\nb\nb b\nb b\nc1 2\n1 2 7\n1 2 7\n‘’ ‘’ ‘’ \n‘’‘ ‘’’ ‘’‘\nH L\n* Backbone\nCurve Parameters Bipartite Matching Loss \nTransformer \nEncoder\nTransformer \nDecoder\nc7\n...\n&1\nshared\nFFNs\n4F\n4E\nβ1 2 7\nα1 2 7α α\nβ β\nk‘’ f ‘’ m‘’ n‘\n...\n...\n4R\nFigure 1. Overall Architecture. The S, Se and Ep indicate ﬂattened feature sequence, encoded sequence and the sinusoidal positional\nembeddings which are all tensors with shape HW × C. The Sq, ELL and Sd represent query sequence, learned lane embedding and the\ndecoded sequence which are all in shapeN ×C. Different color indicate different output slots. White hollow circles represent ”non-lanes”.\n/g134/g134\n&1\n4 4F\n&-- \n/g134/g134\n4E\nQ\nK\nV\n4F\n&1\n/g134/g134\n4R\nTransformer Encoder Layer \ni\u0001\u0013\u0001\nTransformer Decoder Layer \ni\u0001\u0013\u0001\nO\n\nSelf-Attention\nSelf-Attention \nFFNs \nAdd & LayerNorm\nLinear \nSoftmax \nAttention \nAdd & LayerNorm\nAdd & LayerNorm\nLinear Linear \nSelf-Attention \nFFNs \nAdd & LayerNorm\nAdd & LayerNorm\nA\nFigure 2. Transformer Encoder and Decoder. The ⊕ and ⊙ repre-\nsent matrix addition and dot-product operations respectively.\nwhere ˆzis the optimal permutation calculated in Eq. 7. The\nω1,ω2, and ω3 also adjust the effect of the loss terms and\nare set to be the same values of coefﬁcients in Eq. 7.\n3.3. Architecture\nThe architecture shown in Fig. 1 consists of a backbone,\na reduced transformer network, several feed-forward net-\nworks (FFNs) for parameter predictions, and the Hungar-\nian Loss. Given an input image I, the backbone extracts a\nlow-resolution feature then ﬂattens it into a sequence S by\ncollapsing the spatial dimensions. The S and positional em-\nbedding Ep are fed into the transformer encoder to output\na representation sequence Se. Next, the decoder generates\nan output sequence Sd by ﬁrst attending to an initial query\nsequence Sq and a learned positional embedding ELL that\nimplicitly learns the positional differences, then computing\ninteractions with Se and Ep to attend to related features.\nFinally, several FFNs directly predict the parameters of pro-\nposed outputs.\nBackbone. The backbone is built based on a reduced\nResNet18. The original ResNet18 [5] has four blocks and\ndownsamples features by 16 times. The output channel\nof each block is ”64, 128, 256, 512”. Here, our reduced\nResNet18 cuts the output channels into ”16, 32, 64, 128” to\navoid overﬁtting and sets the downsampling factor as 8 to\nreduce losses of lane structural details. Using an input im-\nage I as input, the backbone extracts a low-resolution fea-\nture that encodes high-level spatial representations for lanes\nwith a size of H×W×C. Next, to construct a sequence as\nthe input of encoder, we ﬂatten that feature in spatial dimen-\nsions, resulting in a sequence S with the size of HW ×C,\nwhere HW denotes the length of the sequence and Cis the\nnumber of channels.\nEncoder. The encoder has two standard layers that are\nlinked sequentially. Each of them consists of a self-attention\nmodule and a feed-forward layer shown in Fig.2. Given the\nsequence S that abstracts spatial representations, the sinu-\nsoidal embeddings Ep based on the absolute positions [19]\nis used to encode positional information to avoid the per-\nmutation equivariant. The Ep has the same size as S. The\nencoder performs scaled dot-product attention by Eq. 9:\nA = softmax\n(QKT\n√\nC\n)\n, O = AV, (9)\nwhere the Q,K,V denote sequences of query, key and\nvalue through a linear transformation on each input row,\nand A represents the attention map which measures non-\nlocal interactions to capture slender structures plus global\ncontext, and O indicates the output of self-attention. The\noutput sequence of encoder Se with the shape of HW ×C\nis obtained by following FFNs, residual connections with\nlayer normalizations [2], and another same encoder layer.\nDecoder. The decoder also has two standard layers. Un-\nlike the encoder, each layer inserts the other attention mod-\nule, which expects the output of the encoder, enabling it to\nperform attention over the features containing spatial infor-\nmation to associate with the most related feature elements.\nFacing the translation task, the original transformer [19]\nshifts the ground truth sequence one position as the input of\nthe decoder, making it output each element of the sequence\nin parallel at a time. In our task, we just set the input Sq\nas an empty N ×C matrix and directly decodes all curve\nparameters at a time. Additionally, we introduce a learned\nlane embedding ELL with the size of N ×C, which serves\nas a positional embedding to implicitly learn global lane in-\nformation. The attention mechanism works with the same\nformula Eq. 9 and the decoded sequence Sd with the shape\nof N ×C is obtained sequentially like the way in the en-\ncoder. When training, intermediate supervision is applied\nafter each decoding layer.\nFFNs for Predicting Curve Parameters.The prediction\nmodule generates the set of predicted curves Husing three\nparts. A single linear operation directly projects the Sd\ninto N ×2 then a softmax layer operates it in the last di-\nmension to get the predicted label (background or lane)\nci,i ∈ {1,...,N }. Meanwhile, one 3-layer perceptron\nwith ReLU activation and hidden dimension C projects the\nSd into N ×4, where dimension 4 represents four groups\nof lane-speciﬁc parameters. The other 3-layer perceptron\nﬁrstly projects a feature into N×4 then averages in the ﬁrst\ndimension, resulting in the four shared parameters.\n4. Experiments\nDatasets. The widely-used TuSimple [1] lane detection\ndataset is used to evaluate our method. The TuSimple\ndataset consists of 6408 annotated images which are the\nlast frames of video clips recorded by a high-resolution\n(720×1280) forward view camera across various trafﬁc and\nweather conditions on America’s highways in the daytime.\nIt is split initially into a training set (3268), a validation set\n(358), and a testing set (2782). To evaluate the adaptive ca-\npability to new scenes, we introduce a much more complex\nself-collected dataset named Forward View Lane (FVL).\nThe FVL contains 52970 images with a raw resolution of\n720 ×1280. These images were collected by a monocular\nforward-facing camera typically located near the rear-view\nmirror in multiple cities across different scenes (urban and\nhighway, day and night, various trafﬁc and weather condi-\ntions). The FVL contains more challenging road conditions\nand will go public to help research for the community.\nEvaluation Metrics. To compare the performance against\nprevious methods, we follow the literature and calculate the\naccuracy using TuSimple metrics. The prediction accuracy\nis computed as Accuracy =\n∑\nvc TPr vc∑\nvc Gtvc\n, where TPrvc is\nthe number of true prediction points in the last frame of the\nvideo clip, and Gtvc is the number of ground truth points. A\npoint is considered as a true positive if its distance from the\nTable 1. Comparisons of accuracy (%) on TuSimple testing Set.\nThe number of multiply-accumulate (MAC) operations is given in\nG. The number of parameters (Para) is given in M (million). The\nPP means the requirement of post-processing.\nMethod FPS MACs Para PP Acc FP FN\nFastDraw [15] 90 - - ✓ 95.20 .0760 .0450\nSCNN [14] 7 - 20.72 ✓ 96.53 .0617 .0180\nENet-SAD [6] 75 - 0.98 ✓ 96.64 .0602 .0205\nPINet [7] 30 - 4.39 ✓ 96.70 .0294 .0263\nLine-CNN [8] 30 - - - 96.87 .0442 .0197\nPolyLaneNet [18] 115 1.784 4.05 - 93.36 .0942 .0933\nOurs 420 0.574 0.77 - 96.18 .0291 .0338\ncorresponding label point is within 20 pixels as the TuSim-\nple benchmark suggested [18]. Besides, false positives (FP)\nand false negatives (FN) rates are also reported [1].\nImplementation Details. The hyperparameter settings are\nthe same for all experiments except for the ablation study.\nThe input resolution is set to 360 ×640. The raw data are\naugmented by random scaling, cropping, rotating, color jit-\ntering, and horizontal ﬂipping. The learning rate is set to be\n0.0001 and decayed 10 times every 450k iterations. Batch\nsize is set as 16, and loss coefﬁcients ω1, ω2 and ω3 are set\nas 3, 5 and 2. The ﬁxed number of predicted curves N is\nset as 7, and the number of training iterations is set as 500k.\nAll those hyper-parameters are determined by maximizing\nthe performance on the TuSimple validation set.\nIn the following section, we treat PolyLaneNet [18] as\nthe baseline method since they also predict parametric out-\nput for lanes and provide amazingly reproducible codes\nand baseline models. Besides, to best show our perfor-\nmance, we also compare with other state-of-the-art methods\nPINet [7], Line-CNN [8], ENet-SAD [6], SCNN [14], Fast-\nDraw [15]. The proposed method was trained using both\nTuSimple training and validation set as previous works did.\nThe time unit compares the FPS performance, and we also\nreport MACs and the total number of parameters. All results\nare tested on a single GTX 1080Ti platform.\n4.1. Comparisons with State-of-the-Art Methods\nTab. 1 shows the performance on TuSimple benchmark.\nWithout bells and whistles, our methods outperforms the\nPolyLaneNet by 2.82% accuracy with 5 ×fewer parame-\nters and runs 3.6 ×faster. Compared to the state-of-the-art\nmethod Line-CNN, ours is only 0.69% lower in accuracy\nbut runs 14 ×faster than it. Compared with other two stages\napproaches, our method achieves competitive accuracy and\nthe lowest false positive rate with the fewest parameters and\nmuch faster speed. The high false positive rate would lead\nto more severe risks like false alarming and rapid changes\nthan missing detections in real applications [7]. In a word,\nour method has tremendous mobile deployment capabilities\nPolyLaneNet [18] Ours \nFigure 3. Qualitative comparative results on TuSimple test set. The ﬁrst row visualizes the predicted curves by the best model of ofﬁcially\npublic PolyLaneNet resources (red curves means these predictions are mismatched). The second row visualizes our predictions.\nTable 2. Quantitative evaluation of different shape models on\nTuSimple validation set (%).\nCurve Shape Consistency Acc FP FN\nQuadratic - 91.94 0.1169 0.0975\nQuadratic ✓ 93.18 0.1046 0.0752\nCubic - 92.64 0.1068 0.0868\nCubic ✓ 93.69 0.0979 0.0724\nthan any other algorithms.\nThe visualization of the lane detection results is given in\nFig. 3. By comparing the areas closer to the horizon, our\nmethod is capable of catching structures with fewer details,\nshowing an excellent performance on lane markings with\nlarge amplitude curvature than the baseline method. We\nattribute this to (1) the global lane shape consistency im-\nplicitly requires the model to learn consistent shapes across\nall supervision information from all lane markings; (2) the\nattention mechanism does capture non-local information,\nsupplementing the contextual information for the missing\ndetails, which helps capture slender structures. Subsequent\nablation experiments further corroborate these conclusions.\n4.2. Ablation Study\nInvestigation of Shape Model.To investigate the effect of\nthe lane shape model, we tested different shape models. The\ncomparison results are listed in Tab. 2. The header ’Curve\nShape’ denotes our hypothetical approximation of the lanes\non the roadway. Without shape consistency constraint, the\ni-th predicted curve has its own predicted shape parameters,\nk′′\ni ,f′′\ni ,m′′\ni ,n′\ni, regressed by a 3-layer perceptron without\naveraging. i∈{1,...,N }, N is the number of predictions.\nFrom Tab. 2, we ﬁnd that the best model is using cu-\nbic curve approximation with the shape consistency. The\nconsensus in the lane detection ﬁeld is that high order lane\nmodels always ﬁt lane markings better than simple mod-\nels [10, 22]. Besides, the shape consistency further im-\nAttention(280, 539) Attention(340, 1036)\n(290, 411) (280, 539)\n(520, 1031)\n(340, 1036)\nAttention(290, 411) Attention(520, 1031)\nFigure 4. Encoder attention maps for different sampling points.\nThe encoder seems to aggregate a lot contextual information and\ncapture slender structures.\nproves accuracy. We attribute this to the effect of sample\npoints equilibrium. In the TuSimple dataset, a single lane\nline close to the horizon is marked using far fewer points\nthan the one close to the camera, and this imbalance makes\nthe model more biased towards a better performance at the\nnearby areas [18]. To tackle this issue, the shape consis-\ntency requires the model to ﬁt the same curvature at areas\n\tB\n\u0001&GGFDUT\u0001PG\u0001\u0001%JGGFSFOU\u0001&ODPEFS\u00014J[FT \tC\n\u0001&GGFDUT\u0001PG\u0001\u0001%JGGFSFOU\u00010VUQVU\u00012VBOUJUJFT \nFigure 5. Quantitative evaluation of (a) encoders size and (b) num-\nber of predictions on TuSimple validation set (%). The decoder\nsize is ﬁxed to 2.\nAttention(Slot ID: 0) Attention(Slot ID: 1)\nAttention(Slot ID: 6) Attention(Slot ID: 3)\nID: 0 ID: 1\nID: 6\nID: 3\nFigure 6. Decoder attention maps for output slots. The decoder\nmainly focus on local structures.\nclose to the horizon, enabling it to use all remote points to\ninfer the same curvature which is appropriate for all lanes.\nNumber of encoder layers.To investigate the effects of\nperforming attention on spatial features, we tested differ-\nent numbers of encoder layers. From Fig. 5(a), without\nself-attention mechanism, the accuracy drops by 1.01%.\nMeanwhile, we found that the attention mechanism could\nbe overused. When the same accuracy of the training set\n( ≈96.2%) was observed, a larger number led to a degra-\ndation of the model generalization performance. It appears\nthat our model is approaching the capacity limit of the data’s\nexpressive ability.\nTable 3. Quantitative evaluation of decoder size and different de-\ncoder layer on TuSimple validation set (%). The encoder size is\nset to be 2.\nSize\nLayer 1 2 3 4 5 6\n2 93.55 93.69 - - - -\n4 92.52 93.08 93.15 93.15 - -\n6 92.70 93.07 93.05 93.13 93.14 93.16\nTo better understand the effect of the encoder, we visual-\nize the attention maps A of the last encoder layer in Fig. 4.\nWe tested a few points on the lane markings with differ-\nent conditions. The orange-red lane marking point is totally\noccluded, so the encoder seems to focus on its right unob-\nstructed lane line features. The peach-puff point is located\nwith clear lane markings, and its attention map shows a\nclear long and thin structure for the lane. Despite the lack of\nappearance clues in the local neighborhood of the light-blue\npoint, the encoder still identiﬁes a distinct slender structure\nby learning the global context (distant markings and nearby\nvehicles). Similarly, the green point misses many details,\nbut the encoder still identiﬁes a relevant long and thin struc-\nture by learning the global context.\nNumber of decoder layers.To investigate the performance\nof auxiliary losses, we changed the number of decoder lay-\ners. From Tab. 3, the output of the last layer is the highest\nin each conﬁguration, while as the layers become more nu-\nmerous, the overall performance gradually degrades due to\noverﬁtting.\nSimilarly to visualizing encoder attention, we analyze\nthe attention map for each output of the decoder in Fig. 6.\nWe observe that decoder attention mainly focuses on its\nown slender structures which help the model to separate\nspeciﬁc lane instances directly rather than using additional\nnon-maximal suppression.\nNumber of predicted curves.The predicted curves play a\nsimilar role as the anchor boxes [16], which generate pos-\nitive and negative samples based on some matching rules.\nThe difference is that we only ﬁnd one-to-one matching\nwithout duplicates, so the number of predicted curves de-\ntermines the number of negative samples. To investigate\nthe impact of positive and negative sample proportions, we\ntested from 5 (the TuSimple has up to 5 ground truth lane\nmarkings in one image) to 10 in increments of 1.\nFrom Fig. 5(b), the best size is 7. As the number of\npredicted curves gets smaller, the network gradually loses\nits generalization capability because the lack of negatives\nmakes the training inefﬁcient, resulting in degenerate mod-\nels [9]. Moreover, as the number increases, the performance\nalso degrades since lane curve ﬁtting needs a sufﬁcient num-\nber of positives. More negatives wrongly guide the model to\noptimizing loss by paying more attention to the classiﬁca-\nFigure 7. Qualitative transfer results on FVL dataset. Our method even estimates exquisite lane lines without ever seeing the night scene.\ntion for negatives, which weakens the performance of curve\nﬁtting for positives.\n4.3. Transfer Results on FVL Dataset\nFig. 7 demonstrates the qualitative transfer results on\nFVL datasets. Without any supervised training on FVL, we\nobserved that our model exhibit excellent transfer perfor-\nmance. We attribute this to: (1) our method does not need\nany prior processing which heavily rely on the data distri-\nbution, making them hard to transfer; (2) the transformer-\nbased network aggregates a richer context to focus on infor-\nmation that is more generalized to detect lane objects.\n5. Conclusion\nIn this work, we present an end-to-end lane detector that\ndirectly outputs parameters of a lane shape model. The lane\nshape model reﬂects the road structures and camera state,\nenabling it to enhance the interpretability of the output pa-\nrameters. The network built with transformer blocks efﬁ-\nciently learns global context to help infer occluded part and\ncapture the long and thin structures especially nearby the\nhorizon. The whole method achieves state-of-the-art lane\ndetection performance while requiring the least parameters\nand running time consumption. Meanwhile, our method\nadapts robustly to changes in datasets, making it easier to\ndeploy on mobile devices and more reliable. It would be in-\nteresting to address complex and ﬁne-grained lane detection\ntasks and introduce the tracking function in future work.\n6. Acknowledgement\nThis work was supported in part by the National\nKey Research and Development Program of China under\nGrant 2016YFB1001001 and in part by the National Nat-\nural Science Foundation of China (61976170, 91648121,\n61876112, 61603022).\n7. Appendix\nFor reference, derivation for Eq. 3 is as follows. Given\nEq. 1, according to the perspective projection, a pixel(u,v)\nin the image plane projects on to the point (X,Z) on the\nground plane by:\nX = u×fu ×Z; Z = H\nv×fv\n, (10)\nwhere fu is the width of a pixel on the focal plane divided\nby the focal length, fv is the height of a pixel on the fo-\ncal plane divided by the focal length, and H is the camera\nheight. Submitting Eq. 10 into Eq. 1 and performing some\npolynomial simpliﬁcation:\nu= k×H2\nfu ×fv\n2 ×v2 + m×H\nfu ×fv ×v + n\nfu\n+ b×fv ×v\nfu ×H ,\n(11)\nthen combining parameters together we can get Eq. 2.\nGiven the pitch angle φ, the transformation between a tilted\nand an untitled camera is:\nu= u′,[f\nv\n]\n=\n[ cos φ sin φ\n−sin φ cos φ\n][f′\nv′\n]\n,\n[\nf′\nv′\n]\n=\n[\ncos φ −sin φ\nsin φ cos φ\n][\nf\nv\n]\n,\n(12)\nwhere (u,v,f ) represents the location of a point in the un-\ntitled image plane, f is the focal length which is in pixels,\nand (u′,v′,f′) represents the pitch-transformed position of\nthat point. According to Eq. 12, we get:\nv= v′ −fsin φ\ncos φ (13)\nSubmitting u= u′ and Eq. 13 into Eq. 2, the curve function\nin the tilted image plane can be obtained with the form of\nEq. 3\nReferences\n[1] Tusimple benchmark. https://github.com/\nTuSimple/tusimple-benchmark, 2017.\n[2] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. CoRR, abs/1607.06450, 2016.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. CoRR,\nabs/2005.12872, 2020.\n[4] Mohsen Ghafoorian, Cedric Nugteren, N ´ora Baka, Olaf\nBooij, and Michael Hofmann. EL-GAN: embedding loss\ndriven generative adversarial networks for lane detection. In\nECCV Workshops (1) , volume 11129 of Lecture Notes in\nComputer Science, pages 256–272, 2018.\n[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770–778, 2016.\n[6] Yuenan Hou, Zheng Ma, Chunxiao Liu, and Chen Change\nLoy. Learning lightweight lane detection cnns by self atten-\ntion distillation. In ICCV, pages 1013–1021, 2019.\n[7] YeongMin Ko, Jiwon Jun, Donghwuy Ko, and Moongu Jeon.\nKey points estimation and point instance segmentation ap-\nproach for lane detection. CoRR, abs/2002.06604, 2020.\n[8] Xiang Li, Jun Li, Xiaolin Hu, and Jian Yang. Line-cnn: End-\nto-end trafﬁc line detection with line proposal unit. IEEE\nTrans. Intell. Transp. Syst., 21(1):248–258, 2020.\n[9] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\nand Piotr Doll´ar. Focal loss for dense object detection. IEEE\nTrans. Pattern Anal. Mach. Intell., 42(2):318–327, 2020.\n[10] Joel C. McCall and Mohan M. Trivedi. Video-based lane\nestimation and tracking for driver assistance: survey, system,\nand evaluation. IEEE Trans. Intell. Transp. Syst., 7(1):20–37,\n2006.\n[11] Sandipann P. Narote, Pradnya N. Bhujbal, Abbhilasha S.\nNarote, and Dhiraj Manohar Dhane. A review of recent ad-\nvances in lane detection and departure warning system. Pat-\ntern Recognit., 73:216–234, 2018.\n[12] Davy Neven, Bert De Brabandere, Stamatios Georgoulis,\nMarc Proesmans, and Luc Van Gool. Towards end-to-end\nlane detection: an instance segmentation approach. In Intel-\nligent Vehicles Symposium, pages 286–291, 2018.\n[13] Jianwei Niu, Jie Lu, Mingliang Xu, Pei Lv, and Xiaoke Zhao.\nRobust lane detection using two-stage feature extraction with\ncurve ﬁtting. Pattern Recognit., 59:225–233, 2016.\n[14] Xingang Pan, Jianping Shi, Ping Luo, Xiaogang Wang, and\nXiaoou Tang. Spatial as deep: Spatial CNN for trafﬁc scene\nunderstanding. In AAAI, pages 7276–7283, 2018.\n[15] Jonah Philion. Fastdraw: Addressing the long tail of lane\ndetection by adapting a sequential prediction network. In\nCVPR, pages 11582–11591, 2019.\n[16] Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun.\nFaster R-CNN: towards real-time object detection with re-\ngion proposal networks. IEEE Trans. Pattern Anal. Mach.\nIntell., 39(6):1137–1149, 2017.\n[17] Russell Stewart, Mykhaylo Andriluka, and Andrew Y . Ng.\nEnd-to-end people detection in crowded scenes. In CVPR,\npages 2325–2333, 2016.\n[18] Lucas Tabelini Torres, Rodrigo Ferreira Berriel, Thiago M.\nPaix˜ao, Claudine Badue, Alberto F. De Souza, and Thi-\nago Oliveira-Santos. Polylanenet: Lane estimation via deep\npolynomial regression. CoRR, abs/2004.10924, 2020.\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, pages 5998–\n6008, 2017.\n[20] Yue Wang, Dinggang Shen, and Eam Khwang Teoh. Lane\ndetection using spline model. Pattern Recognit. Lett. ,\n21(8):677–689, 2000.\n[21] Jie Zhang, Yi Xu, Bingbing Ni, and Zhenyu Duan. Geomet-\nric constrained joint lane segmentation and lane boundary\ndetection. In ECCV (1), volume 11205 of Lecture Notes in\nComputer Science, pages 502–518, 2018.\n[22] Kun Zhao, Mirko Meuter, Christian Nunn, Dennis M ¨uller,\nStefan M ¨uller-Schneiders, and Josef Pauli. A novel multi-\nlane detection and tracking system. In Intelligent Vehicles\nSymposium, pages 1084–1089, 2012.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7448294758796692
    },
    {
      "name": "Lane departure warning system",
      "score": 0.6383675336837769
    },
    {
      "name": "Transformer",
      "score": 0.5971984267234802
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5467570424079895
    },
    {
      "name": "Software deployment",
      "score": 0.5097858309745789
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5014219284057617
    },
    {
      "name": "Adaptability",
      "score": 0.47877752780914307
    },
    {
      "name": "Dynamic Bayesian network",
      "score": 0.4197736084461212
    },
    {
      "name": "Machine learning",
      "score": 0.37868279218673706
    },
    {
      "name": "Computer vision",
      "score": 0.3495648503303528
    },
    {
      "name": "Real-time computing",
      "score": 0.3351306915283203
    },
    {
      "name": "Data mining",
      "score": 0.32149747014045715
    },
    {
      "name": "Bayesian probability",
      "score": 0.22128349542617798
    },
    {
      "name": "Engineering",
      "score": 0.1412847936153412
    },
    {
      "name": "Voltage",
      "score": 0.09806323051452637
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I96852419",
      "name": "Capital Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 9
}