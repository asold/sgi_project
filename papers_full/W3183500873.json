{
  "title": "LIORI at SemEval-2021 Task 8: Ask Transformer for measurements",
  "url": "https://openalex.org/W3183500873",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5020540261",
      "name": "Adis Davletov",
      "affiliations": [
        "Katanov Khakass State University",
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)",
        "The Russian Presidential Academy of National Economy and Public Administration"
      ]
    },
    {
      "id": "https://openalex.org/A5021854452",
      "name": "Denis Gordeev",
      "affiliations": [
        "Katanov Khakass State University",
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)",
        "The Russian Presidential Academy of National Economy and Public Administration"
      ]
    },
    {
      "id": "https://openalex.org/A5047490968",
      "name": "Nikolay Arefyev",
      "affiliations": [
        "Katanov Khakass State University",
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)",
        "The Russian Presidential Academy of National Economy and Public Administration"
      ]
    },
    {
      "id": "https://openalex.org/A5097845389",
      "name": "Emil Davletov",
      "affiliations": [
        "Katanov Khakass State University",
        "Lomonosov Moscow State University",
        "National Research University Higher School of Economics",
        "Samsung (Russia)",
        "The Russian Presidential Academy of National Economy and Public Administration"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3092542022",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3183659444",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4320013820",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "This work describes our approach for subtasks of SemEval-2021 Task 8: MeasEval: Counts and Measurements which took the official first place in the competition. To solve all subtasks we use multi-task learning in a question-answering-like manner. We also use learnable scalar weights to weight subtasks’ contribution to the final loss in multi-task training. We fine-tune LUKE to extract quantity spans and we fine-tune RoBERTa to extract everything related to found quantities, including quantities themselves.",
  "full_text": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1249–1254\nBangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics\n1249\nLIORI at SemEval-2021 Task 8: Ask Transformer for measurements\nAdis Davletov\nRANEPA, Moscow, Russia\nLomonosov Moscow State University, Moscow, Russia\ndavletov-aa@ranepa.ru\nDenis Gordeev\nRANEPA, Moscow, Russia\ngordeev-di@ranepa.ru\nNikolay Arefyev\nLomonosov Moscow State University, Moscow, Russia\nSamsung Research Center Russia, Moscow, Russia\nHSE University, Moscow, Russia\nnick.arefyev@gmail.com\nEmil Davletov\nKatanov Khakas State University, Abakan, Russia\nedavletov@yandex.ru\nAbstract\nThis work describes our approach for subtasks\nof SemEval-2021 Task 8: MeasEval: Counts\nand Measurements which took the ofﬁcial ﬁrst\nplace in the competition. To solve all sub-\ntasks we use multi-task learning in a question-\nanswering-like manner. We also use learnable\nscalar weights to weight subtasks’ contribution\nto the ﬁnal loss in multi-task training. We ﬁne-\ntune LUKE to extract quantity spans and we\nﬁne-tune RoBERTa to extract everything re-\nlated to found quantities, including quantities\nthemselves.\n1 Introduction\nSemEval-2021 Task 8 consisted of ﬁve subtasks\nthat covered span extraction, classiﬁcation, and\nrelation extraction tasks. This paper presents so-\nlutions to all ﬁve of them which showed the best\nresults in the competition1.\nIn the subtask 1(A) participants were asked to re-\ntrieve Quantity (Q) spans from texts. For example,\nin the following text ”The soda can’s volume was\n355 ml.”, the system should retrieve ”355 ml”as\nQ span. The rest of the subtasks were to extract in-\nformation related to retrieved Quantities (Qs) from\nsubtask A.\nThe subtask 2(B) was to extract the Unit of mea-\nsurement (UoM) of the extracted Q and also to\nclassify it into 10 classes: HasTolerance, IsAp-\nproximate, IsCount, IsList, IsMean, IsMeanHas-\nTolerance, IsMeanIsRange, IsMedian, IsRange, Is-\nRangeHasTolerance. It should be noted that some\n1https://github.com/davletov-aa/meas-eval\nQs could be related to more than one type and there\nwere ones which didn’t belong to any type. The\nsubtask 3(C) was to extract Measured Entity (ME)\nand Measured Property (MP) spans. In the sub-\ntask 4(D) additional Qualiﬁer (Qlfr) spans, which\nhelped to validate or understand the extracted Q,\nwere asked to be extracted. And ﬁnally, sub-\ntask 5(E) was to extract relations betweenQs, MEs,\nMPs and Qlfrs.\nMore detailed information about the competition\ncould be found in the Harper et al. (2021)’s shared\ntask description paper.\n2 Related Work\nSpan extraction and classiﬁcation problems have\na long history of studies and are often studied as a\npart of Named Entity Recognition (NER). For ex-\nample, the NER dataset Ontonotes v5 (Weischedel\net al., 2013) contains such entities as ”Quantity”,\nwhich also includes measurements, and ”Money”.\nHowever, the general NER approach used in\nOntonotes or ConLL 2003 (Sang and De Meul-\nder, 2003) datasets is not so ﬁne-grained as the one\nthat is used in the task under study.\nMost state-of-the-art models for named en-\ntity recognition and relation extraction are based\non Transformer architecture by Vaswani et al.\n(2017). For example, the top three best models\nfor Ontonotes v5 according to paperswithcode.com\nuse BERT 2. BERT is a large pre-trained language\nmodel based on Attention (Devlin et al., 2019).\n2https://paperswithcode.com/task/named-entity-\nrecognition-ner\n1250\nFigure 1: Data example. It shows that one named entity may have several incoming and outcoming relations.\nBERT has a unique training procedure where the\nmodel is trained using Masked language objec-\ntive, where some tokens are replaced with a special\n’[MASK]’ token and the model should predict the\noriginal token. BERT also had an additional train-\ning objective - the model had to predict whether a\nsentence was random or it followed the ﬁrst sen-\ntence. However, some papers have investigated that\nBERT is undertrained and that training BERT on\nmore data and for a longer time might increase\nmodel performance. RoBERTa was one of the\nﬁrst and more inﬂuential papers of such kind (Liu\net al., 2019). RoBERTa modiﬁes BERT’s pretrain-\ning procedure. The RoBERTa model is trained\nlonger, with bigger batches over more data and\non longer sequences. RoBERTa’s authors have\nalso found that removing the next sentence pre-\ndiction objective from BERT matches or slightly\nimproves BERT performance. Researchers have\nalso suggested ways of leveraging the nature of the\ntask and adding some problem bias to named entity\nrecognition. Among such works, which is currently\nthe best performing for Ontonotes v5 and CoNLL\n2003 according to paperswithcode.com, is LUKE\n(Yamada et al., 2020). The authors of LUKE have\nadded a new language modeling task that consists\nof predicting randomly masked words and enti-\nties in an entity-annotated corpus retrieved from\nWikipedia. The authors have also expanded the self-\nattention mechanism to entity types and consider\nentity types when computing attention scores. The\nproposed approach allowed the authors to achieve\nstate-of-the-art results not only for named entity\nrecognition but for a bunch of other unrelated tasks\nsuch as SQuAD1.1 question answering.\nFor relation extraction, Transformer-based mod-\nels also outperform other approaches. A promising\napproach is treating relation extraction as a ques-\ntion answering problem. Among works implement-\ning this approach, we can mention (Cohen et al.,\n2020) where the authors restructured relation clas-\nsiﬁcation as a Question answering (QA) like span\nprediction problem. It allowed them to get state-\nof-the-art results for TACRED and SemEval 2010\ntask 8 datasets.\n3 System Description\n3.1 Data\nThe data provided by the organizers contained plain\ntext ﬁles and their annotations in tsv format. There\nwere in total 248 training texts, 65 trial ones, and\n135 for the evaluation phase. There were 2764,\n897, and 1620 annotated entities respectively. The\nﬁles have been approximately equally distributed\namong several domains: Agriculture, Astronomy,\nBiology, Chemistry, Computer Science, Earth Sci-\nence, Engineering, Materials Science, Mathemat-\nics, Medicine. Entities could have been labeled\ninto 5 classes: Q, ME, MP, UoM or Qlfr.\nAs input data in the competition was in the form\nof plain text extracts, we ﬁrst split them into sen-\ntences using PunktSentenceTokenizer and Punkt-\nTrainer from NLTK library (Bird et al., 2009). We\ntrained PunktTrainer on texts from the training set.\nWe did data augmentation by including text ex-\ntracts consisting of two sentences following each\nother for each text document. So if we had origi-\nnal sentences [s1, s2, s3, s4] we get an augmented\nset of texts [s1, s2, s3, s4, s1s2, s2s3, s3s4]. Then\nwe split each example into tokens using Regexp-\nTokenizer from the NLTK library with the follow-\ning \\w+|\\(|\\)|\\[|\\]|[-{.,]|\\S+ regu-\nlar expression. We used the train set for training\nand the trial set for development.\nAlso, we relabeled Qualiﬁer to QuantityQuali-\nﬁer (QQ), MeasuredEntityQualiﬁer (MEQ), and\nMeasuredPropertyQualiﬁer (MPQ). By this little\ntrick we solved the problem with examples having\nmultiple Qlfrs corresponding to either Q, ME, or\nMP.\n1251\nFigure 2: Architecture of the QuAnt system. It takes tokenized text with marked MeasuredEntity as input and\npredicts all needed spans and the class of the Quantity.\nHyperparam Value\ndropout 0.1\nweight decay 0.1\nwarmup proportion 0.1\nlr 1e-4\nlr scheduler linear warmup\noptimizer Adam\nepochs 50\nbs 128\nmax seq len 128\nmodel xlm-roberta-large\nmax grad norm 1.0\nvalidate per epoch 4\nTable 1: Training hyperparameters of QuAnt system,\nsubmitted to competition\n3.2 QuAnt System\nThe architecture of the QuAnt3 is shown in Figure 2.\nAs could be seen, our model uses the RoBERTa\nmodel to extract features for each example. It\nsolves all subtasks of the competition in a multi-\ntask question answering way. We ask our model\nto predict all BPE subword-level start and end po-\nsitions of spans (answers) related to Q (question).\nWe ask the model by inserting special tokens ” •”\nand ”/” around Q. Also, the model makes multi-\nclass multi-label predictions regarding the type of\nthe Quantity (QT).\nIt takes text extracts containing some Qs and\npositions of the Q regarding which it should make\npredictions.\nFor example, for the input text ”The soda can’s\n3QuAnt - the system deals with quantities in a question-\nanswering-like manner\nvolume was 355 ml.”and for subword-level posi-\ntions (6, 7) of the Q ”355 ml”, the model should\npredict the following start and end positions: (6, 7)\nfor Q (A), (7, 7) for UoM (B), (3, 3) for ME (C),\n(4, 4) for MP (C), (2, 2) for MEQ (D). Also, the\nmodel shouldn’t predict any label forQT (B).\n3.2.1 Extract Quantities\nSo, our approach needs quantity span information\nas input. And to get that information we went with\nﬁne-tuning the LUKE model (Yamada et al., 2020)\non the NER task to predict Q spans. We used the\ncode provided by the authors of the model. We\ntrained it on the augmented dataset in BIO format\nwith the following hyperparameters: maximum-\nentity-length, maximum-sequence-length, learning\nrate, and batch size were set to 64, 256, 1e-5, and\n4 respectively. We trained two models with the\nweight decay hyperparameter equal to 0.1 and 0.01\nfor 10 epochs. We were validating our models four\ntimes per epoch on the development set and saving\nthe top 3 best checkpoints during training, resulting\nin a total of 6 models.\nSo after two training runs, we got 6 trained mod-\nels. Using the development set we chose the best\ncombination of them for a simple word-level voting\nensemble.\n3.2.2 Extract Everything\nDuring training and validation, we use Q spans\nfrom the annotated set. During test prediction, we\nuse spans predicted by the ensemble of quantity\nextractors from the previous section. Because of\nour test time augmentation process, we had been\nable to get up to three entries per each Q: for the\nsentence containing it and for it with either its left\n1252\nor right context sentences.\nWe split tokenized examples into byte-pair-\nencoding (BPE) subwords with RoBERTaTok-\nenizer which resulted in the following RoBERTa\ninputs marked by symbols ”•” and ”/”Qs:\n[CLS] {Optional question preﬁx [EOS]}s1 ... •\nw / ... sn [EOS].\nTo vectorize QT, we use the output from the\nlast layer corresponding to [CLS] token. And to\npredict start and end probabilities for each subword\nof each span type we use outputs from the last layer.\nWe feed them to linear layers to predict QTs and\nspan starts and span ends.\nDuring training we optimize the following\nweighted loss:\nLtotal = −wQT\nbs\nbs∑\ni=1\nQTi\nk ∗log( ˆQT\ni\nk)−\n∑\nST ∈ST s\n(−wST\n2bs (\nbs∑\ni=1\nST i\nstart ∗log( ˆST\ni\nstart)+\n+\nbs∑\ni=1\nST i\nend ∗log( ˆST\ni\nend)))\n, where bs – batch size, [wQT ; wST |ST ∈\nSTs ] = softmax([wqt; wst|st ∈STs ]) – learn-\nable weights vector initialized with ones, QTi –\none-hot encoded QT of i-th example (which could\nbe zero vector in some cases and will not contribute\nduring training), ˆQT\ni\n– predicted QT probability\ndistribution, STs – set of following span types: [Q,\nME, MP, Qlfr, UoM, QQ, MEQ, MPQ], ST i\nstart\n– one-hot encoded start position of the correspond-\ning span. ˆST\ni\nstart – predicted start positions proba-\nbility distribution. The same goes for ST i\nend.\nWe trained our model without adding an optional\nquestion preﬁx to RoBERTa inputs. We used hy-\nperparameters from Table 1.\nAs our test predictions include duplicated pre-\ndictions for the same Q due to the test time aug-\nmentation, we remove identical predictions. Worth\nnoting, that there still might be duplicates left in\nthe case of different extracted values for the same\nQuantity. Because of this, our submitted results\nare higher than the results without test time data\naugmentation.\nSo, our model takes Q with its context as input\nand predicts its type and extracts various spans. For\nall of the subtasks except the subtask E we treated\nextracted answers as is. For the subtask E we used\nthe following rules to extract relations between Q,\nMP, ME and Qlfr (QQ, MEQ, MPQ):\n• (MP, HasQuantity, Q);\n• if there is MP then (ME, HasProperty, MP),\notherwise (ME, HasQuantity, Q);\n• (QQ, Qualiﬁes, Q), (MEQ, Qualiﬁes, ME),\n(MPQ, Qualiﬁes, MP);\n4 Experiments and Results\nIn this section, we report the results of our post-\nevaluation experiments.\nFirst, we experimented with base models. We\ntried different subtask weighting strategies. As we\nsolve the task in a multi-task way, we need to ag-\ngregate the losses of each subtask to optimize the\nﬁnal loss. And here, we tried to just average them\n(equal) or take the weighted sum using learnable\nweights (softmax, rsqr+log) vector W with the\nlength equal to the number of training subtasks. In\nthe case of softmax weighting strategy we just use\nsoftmax over the vector W. In the case ofrsqr+log,\nwe divide each subtask’s loss to its squared learn-\nable weight and sum with the logarithm of it. This\napproach of weighting subtasks in multi-task learn-\ning was introduced by Kendall et al. (2018).\nWe also experimented with data augmentation.\nBut unlike experiments we did in the evaluation\nperiod, here we didn’t do test time data augmenta-\ntion.\nAlso, we tried to concatenate the question preﬁx\nto an input example. We experimented with preﬁx\nFind measured entities and properties of marked\nquantity. We hoped it could give extra information\nto the model regarding the nature of the answer.\nTable 2 shows the best results for the develop-\nment dataset and Table 3 shows corresponding re-\nsults for the test dataset. Also, there are our ofﬁcial\nsubmission results.\nTable 2 shows that training time data augmen-\ntation improves the overall score. Also, we could\nsee that including preﬁx question did not improve\nthe overall scores of the models which use data\naugmentation. Yet we see the opposite picture\nfor the test set in Table 3. It can also be seen\nthat RoBERTa-large not necessarily outperforms\nRoBERTa base.\nWe see that using just the average sum of sub-\ntask’s losses demonstrates the best results.\nWe also tried to ﬁne-tune the large version of\nXLM-R with the best hyperparameters from base\n1253\nModel Aug WSch PQ O Q ME MP Qlfr UoM M HQ HP Qlfs\npost-evaluation phase results: roberta.base\nQA-v1 F equal F 45.2 98.9 32.5 36.2 15.2 73.9 66.1 42.4 21.2 9.3\nQA-v2 F equal T 45.6 98.9 33.2 36.1 15.7 74.3 73.0 42.9 22.6 11.3\nQA-v3 F rsqr+log F 45.3 98.9 32.3 35.4 15.2 74.7 70.1 41.8 22.0 12.9\nQA-v4 F rsqr+log T 45.8 98.9 33.6 37.8 13.4 73.0 67.9 46.1 22.5 9.7\nQA-v5 F softmax F 45.8 98.9 33.4 36.6 13.5 74.0 72.6 42.3 20.5 11.8\nQA-v6 F softmax T 45.6 98.9 32.5 37.3 16.2 75.3 75.5 45.6 21.8 11.3\nQA-v7 T equal F 47.7 98.9 33.1 35.6 16.1 74.3 77.7 40.3 22.7 9.6\nQA-v8 T equal T 46.1 98.9 32.8 35.5 11.1 73.7 76.4 38.9 21.2 9.3\nQA-v9 T rsqr+log F 47.6 98.9 32.4 36.8 18.7 73.9 78.3 40.0 21.9 12.6\nQA-v10 T rsqr+log T 46.1 98.9 33.1 36.3 14.3 73.9 78.2 42.0 22.3 10.2\nQA-v11 T softmax F 47.3 98.9 32.9 37.2 16.6 73.6 78.1 41.8 23.0 10.7\nQA-v12 T softmax T 46.9 98.9 34.5 35.2 13.6 73.6 76.3 39.9 24.2 10.0\npost-evaluation phase results: roberta.large\nQA-v1 T equal F 49.3 98.6 37.8 38.3 17.7 75.6 78.1 43.7 27.0 10.3\nQA-v2 T rsqr+log F 48.8 98.5 35.0 41.3 10.7 75.3 77.7 46.0 24.5 6.9\nQA-v3 T softmax F 48.9 98.5 37.9 38.1 17.8 73.7 78.4 44.4 27.2 10.3\nTable 2: Best Overlap F1 scores for the dev set. Aug – augmentation, WSch – weighting Scheme, PQ – Preﬁx\nQuestion, O - Overall, Q – Quantity, ME – Measured Entity, MP – Measured Property, Qlfr – Qualiﬁer, UoM –\nUnit, M – Modiﬁer, HQ – Has Quantity, HP – Has Property, Qlfs – Qualiﬁes.\nModel Aug WSch PQ O Q ME MP Qlfr UoM M HQ HP Qlfs\nevaluation phase results: roberta.large\nQA-v1 T softmax F 51.9 86.1 43.7 46.7 16.3 72.2 64.2 48.2 31.8 9.2\nevaluation phase results: best results of other competitors\n47.3 85.5 40.6 43.7 10.7 80.4 61.4 42.4 25.7 6.4\npost-evaluation phase results: roberta.base\nQA-v1 F equal F 44.8 84.7 38.8 38.1 15.4 66.9 52.0 39.5 24.3 8.5\nQA-v2 F equal T 43.5 84.7 36.3 34.5 12.3 66.9 57.0 37.6 21.9 5.6\nQA-v3 F rsqr+log F 45.1 84.7 39.3 39.7 11.9 67.2 50.9 41.7 24.3 7.8\nQA-v4 F rsqr+log T 45.2 84.7 39.3 40.1 13.8 67.0 48.8 41.7 24.8 7.1\nQA-v5 F softmax F 43.6 83.8 37.5 35.9 14.9 67.9 49.5 37.9 23.3 8.3\nQA-v6 F softmax T 42.7 84.7 37.6 32.5 10.2 67.0 55.9 34.4 20.4 5.8\nQA-v7 T equal F 44.6 84.2 37.4 37.9 9.8 67.4 57.2 39.7 23.6 6.2\nQA-v8 T equal T 45.7 84.7 39.4 38.8 12.5 66.7 58.8 41.9 24.2 7.4\nQA-v9 T rsqr+log F 45.9 84.4 39.0 38.3 18.7 66.1 58.7 41.5 24.1 10.6\nQA-v10 T rsqr+log T 47.1 84.7 39.9 42.0 12.5 68.2 59.6 44.4 26.6 7.4\nQA-v11 T softmax F 45.8 84.5 40.0 39.9 14.3 66.8 56.1 42.2 24.7 7.7\nQA-v12 T softmax T 46.0 84.7 39.9 39.3 12.3 67.4 56.2 41.9 26.3 6.6\npost-evaluation phase results: roberta.large\nQA-v1 T equal F 48.9 84.6 43.0 45.6 15.4 67.0 56.6 47.7 32.0 8.0\nQA-v2 T rsqr+log F 47.2 83.8 41.9 42.0 9.2 66.7 58.0 44.6 29.7 6.0\nQA-v3 T softmax F 47.6 84.6 41.9 41.8 15.4 66.5 59.9 44.9 29.7 8.3\nTable 3: Overlap F1 scores for the test set. Aug – augmentation, WSch – weighting Scheme, PQ – Preﬁx Question,\nO - Overall, Q – Quantity, ME – Measured Entity, MP – Measured Property, Qlfr – Qualiﬁer, UoM – Unit, M –\nModiﬁer, HQ – Has Quantity, HP – Has Property, Qlfs – Qualiﬁes.\nmodels. In the case of large models, again, equal\nweighting scheme demonstrated the best result.\nIn all our post-evaluation experiments we used\nthe same settings as in Table 1. We tried learning\nrates from [5e −5, 1e −4, 2e −4] and batch sizes\nfrom [32, 64, 128].\n5 Conclusion\nIn this paper, we introduced our solution to\nSemEval-2021 Task 8: MeasEval: Counts and Mea-\nsurements. Our approach was based on RoBERTa\nand LUKE models. We show that extracting mea-\nsurements from a text can be treated as a question-\nanswering task. In this work, we tried a set of\ndifferent models, hyperparameters, and weighting\nschemes and present their effect on the ﬁnal result.\nReferences\nSteven Bird, Ewan Klein, and Edward Loper. 2009.\nNatural language processing with Python: analyz-\ning text with the natural language toolkit. ” O’Reilly\nMedia, Inc.”.\nAmir DN Cohen, Shachar Rosenman, and Yoav Gold-\n1254\nberg. 2020. Relation extraction as two-way span-\nprediction. arXiv preprint arXiv:2010.04829.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCorey Harper, Jessica Cox, Curt Kohler, Antony Scerri,\nRon Daniel Jr., and Paul Groth. 2021. SemEval 2021\ntask 8: MeasEval – extracting counts and measure-\nments and their related contexts. In Proceedings\nof the Fifteenth Workshop on Semantic Evaluation\n(SemEval-2021), Bangkok, Thailand (online). Asso-\nciation for Computational Linguistics.\nAlex Kendall, Yarin Gal, and Roberto Cipolla. 2018.\nMulti-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 7482–7491.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nErik F Sang and Fien De Meulder. 2003. Intro-\nduction to the conll-2003 shared task: Language-\nindependent named entity recognition. arXiv\npreprint cs/0306050.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA, 23.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. Luke: Deep\ncontextualized entity representations with entity-\naware self-attention. In EMNLP.",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.8899232745170593
    },
    {
      "name": "Transformer",
      "score": 0.7299284934997559
    },
    {
      "name": "Computer science",
      "score": 0.7190364003181458
    },
    {
      "name": "Task (project management)",
      "score": 0.6341546773910522
    },
    {
      "name": "Ask price",
      "score": 0.6169923543930054
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5443375706672668
    },
    {
      "name": "Natural language processing",
      "score": 0.47683635354042053
    },
    {
      "name": "Machine learning",
      "score": 0.3206542730331421
    },
    {
      "name": "Physics",
      "score": 0.06912451982498169
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}