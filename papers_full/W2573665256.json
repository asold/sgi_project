{
  "title": "LanguageCrawl: a generic tool for building language models upon common Crawl",
  "url": "https://openalex.org/W2573665256",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2462914710",
      "name": "Szymon Roziewski",
      "affiliations": [
        "National Information Processing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4296485266",
      "name": "Marek Kozłowski",
      "affiliations": [
        "National Information Processing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4296485266",
      "name": "Marek Kozłowski",
      "affiliations": [
        "National Information Processing Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035016936",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W6691892052",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2131062488",
    "https://openalex.org/W2158403176",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2395418806",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W1615991656",
    "https://openalex.org/W2143017621",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2262562434",
    "https://openalex.org/W6691892961",
    "https://openalex.org/W1580899267",
    "https://openalex.org/W6992891273",
    "https://openalex.org/W2131164945",
    "https://openalex.org/W168564468",
    "https://openalex.org/W2462196471",
    "https://openalex.org/W6640862754",
    "https://openalex.org/W6685203456",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2250966211",
    "https://openalex.org/W2613992498",
    "https://openalex.org/W2270208168",
    "https://openalex.org/W2757554342",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W1565926589",
    "https://openalex.org/W1967932898",
    "https://openalex.org/W2169200297",
    "https://openalex.org/W2170716095",
    "https://openalex.org/W44059908",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2325227998",
    "https://openalex.org/W2344043511",
    "https://openalex.org/W2250716486",
    "https://openalex.org/W1956559956",
    "https://openalex.org/W2599398625",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2279392567",
    "https://openalex.org/W2949547296",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2250653840"
  ],
  "abstract": "Abstract The exponential growth of the internet community has resulted in the production of a vast amount of unstructured data, including web pages, blogs and social media. Such a volume consisting of hundreds of billions of words is unlikely to be analyzed by humans. In this work we introduce the tool LanguageCrawl , which allows Natural Language Processing (NLP) researchers to easily build web-scale corpora using the Common Crawl Archive—an open repository of web crawl information, which contains petabytes of data. We present three use cases in the course of this work: filtering of Polish websites, the construction of n-gram corpora and the training of a continuous skipgram language model with hierarchical softmax. Each of them has been implemented within the LanguageCrawl toolkit, with the possibility to adjust specified language and n-gram ranks. This paper focuses particularly on high computing efficiency by applying highly concurrent multitasking. Our tool utilizes effective libraries and design. LanguageCrawl has been made publicly available to enrich the current set of NLP resources. We strongly believe that our work will facilitate further NLP research, especially in under-resourced languages, in which the lack of appropriately-sized corpora is a serious hindrance to applying data-intensive methods, such as deep neural networks.",
  "full_text": "PROJECT NOTES\nLanguageCrawl: a generic tool for building language\nmodels upon common Crawl\nSzymon Roziewski1 • Marek Kozłowski1\nAccepted: 15 June 2021 / Published online: 5 August 2021\n/C211The Author(s) 2021\nAbstract The exponential growth of the internet community has resulted in the\nproduction of a vast amount of unstructured data, including web pages, blogs and\nsocial media. Such a volume consisting of hundreds of billions of words is unlikely\nto be analyzed by humans. In this work we introduce the tool LanguageCrawl,\nwhich allows Natural Language Processing (NLP) researchers to easily build web-\nscale corpora using the Common Crawl Archive—an open repository of web crawl\ninformation, which contains petabytes of data. We present three use cases in the\ncourse of this work: ﬁltering of Polish websites, the construction of n-gram corpora\nand the training of a continuous skipgram language model with hierarchical soft-\nmax. Each of them has been implemented within the LanguageCrawl toolkit, with\nthe possibility to adjust speciﬁed language and n-gram ranks. This paper focuses\nparticularly on high computing efﬁciency by applying highly concurrent multi-\ntasking. Our tool utilizes effective libraries and design. LanguageCrawl has been\nmade publicly available to enrich the current set of NLP resources. We strongly\nbelieve that our work will facilitate further NLP research, especially in under-\nresourced languages, in which the lack of appropriately-sized corpora is a serious\nhindrance to applying data-intensive methods, such as deep neural networks.\nKeywords Polish Web Corpus /C1Common Crawl /C1Word2Vec /C1Language Models /C1\nN-gram\n& Szymon Roziewski\nsroziewski@opi.org.pl\nMarek Kozłowski\nmkozlowski@opi.org.pl\n1 National Information Processing Institute, Warsaw, Poland\n123\nLang Resources & Evaluation (2021) 55:1047–1075\nhttps://doi.org/10.1007/s10579-021-09551-7\n1 Introduction\nThe internet is the largest and most diverse collection of textual information in\nhuman history, and, covers almost all known subjects and languages. This makes it\nan appealing resource for extraction of large-scale corpora for language modelling.\nHowever, until recently, it was highly unlikely that language researchers in\nacademia would have had access to the necessary infrastructure to process such a\nlarge amount of information when building a language corpus. With recent\nimprovements in computing power, storage availability and powerful, highly\nefﬁcient scalable processing and computing frameworks, it has now become feasible\nto build a large scale corpus using publicly available web-archives and commodity\nhardware—that which can be purchased from retailers, such as low-cost desktop\ncomputers.\nThe LanguageCrawl tool supports NLP researchers to readily form large-scale\ncorpora of a given language, ﬁltered directly from the Common Crawl Archive. The\nuser is then capable of building an n-gram collection and training neural-network-\nbased distribution model. We strongly believe that this work will prove beneﬁcial to\nthe linguistic community. The corpus gathered for a given language, precomputed\nn-grams and distributed word representations are valuable for many purposes, such\nas boosting the accuracy of speech recognition, spell checking or machine\ntranslation systems (Buck, Heaﬁeld, & van Ooyen, 2014).\nIn this work our primary objective is to illustrate some of use cases of\nLanguageCrawl. Straining out Polish websites from the Common Crawl Archive\nand subsequently building n-gram corpora along with the training continuous\nskipgram language model are the main functionalities. As far as we know, such a\nlarge-scale collection has not yet been formed from the Polish internet. We consider\nour results to be highly useful for further NLP research in the Polish language, and\nto enrich knowledge on the wider subject of NLP, by means of models and data. In\naddition, our collection of Polish websites is currently the largest open text\nrepository of Polish web-oriented language, counting more than 22 billion words.\nBy the time this paper is published, the source code and language models used will\nbe publicly available.\n2 Related work\n2.1 Data sets\nPolish reference textual data-sets have been built during last ﬁfteen years by several\ninstitutions, and the most popular of those are described below.\nThe National Corpus of Polish 1 (Przepiorkowski, 2012; Przepio´rkowski, Go´rski,\nLewandowska-Tomaszczyk, & Łaz´inski, 2008), (NCP) is a shared initiative of four\ninstitutions: the Institute of Computer Science at the Polish Academy of Sciences\n(the coordinator), the Institute of Polish Language at the Polish Academy of\n1 http://nkjp.pl/.\n1048 S. Roziewski, M. Kozłowski\n123\nSciences, the Polish Scientiﬁc Publisher, PWN, and the Department of Computa-\ntional and Corpus Linguistics at the University of Ło ´dz´. The initiative was\nregistered as a research and development project by the Polish Ministry of Science\nand Higher Education. The list of sources for the corpora contains classic literature,\ndaily newspapers, specialist periodicals and journals, transcripts of conversations,\nand a variety of short-lived and internet texts. The NCP dataset contains\napproximately 1.5 billion words. It is distributed under a GNU GPL license in\nTEI-compliant XML format.\nPELCRA\n2 (Polish and English Language Corpora for Research and Applications)\nis also a research group at the Department of English Language at the University of\nŁo´dz´, which was established in 1997. The main research interests and activities of\nthe group include corpus and computational linguistics, natural language process-\ning, information retrieval and information extraction. The PELCRA Reference\nCorpus of Polish was developed between 1996 and 2005 (between 2003 and 2005 as\na research project of the Polish State Committee for Scientiﬁc Research) to address\nthe demand for a large reference corpus of Polish language for research and\napplications. The corpus contains approximately 100 million words, including both\nwritten (90% of the corpus) and spoken, and mainly contemporary language (95%).\nThe corpus comprises texts related to the European Union: news articles from the\nweb page of the Community Research and Development Information Service, and\npress releases from the European Commission, the European Parliament and the\nEuropean Southern Observatory.\nThe Polish Sejm Corpus (PSC) is a collection of stenographic transcripts from\nsessions of the Polish parliament, totaling approximately 200-million words\n(Ogrodniczuk, 2012). It contains automatically generated annotation on various\nlevels encoded in TEI format. In large parts, the transcripts represent quasi-spoken\nspeech, which makes it a highly interesting resource.\nWikipedia, being one of the largest freely available text sources, has proved\nuseful for text mining purpose. The most recent dump after text extraction from the\nXML ﬁles and subsequent corpus pre-processing produced approximately 380-\nmillion tokens.\n3\nIn Bano´n et al. ( 2020), the authors report on methods for building parallel\ncorpora by web crawling. They make use of CommonCrawl for exploiting web\nstatistics of potential websites to run targeted crawling. To create the parallel\ncorpora, the document and sentence alignment are performed. The data sets were\nevaluated in machine translation tasks with BLEU scoring. They released corpora\ncontaining 223 million sentence pairs from around 150k website domains, for 23 EU\nlanguages. Although, the formed data set is highly imbalanced, 73% of sentence\npairs comprising of ﬁve languages: French, German, Spanish, Italian, and\nPortuguese. The Polish part of ParaCrawl is 3.7 GB in size, and contains\napproximately 45 and 666 million sentences and words, respectively. The size of\nour data set which we analyze in this work is 166 GB in size and is formed out of 22\nbillion words, and more than 1.3 billion sentences.\n2 http://pelcra.pl/.\n3 https://pl.wikipedia.org/wiki/Wikipedia:Statystyki.\nLanguageCrawl: A Generic Tool for Building... 1049\n123\nIn another study on internet web corpora based on CommonCrawl (Sua ´rez,\nSagot, & Romary, 2019), the ﬁltered, classiﬁed by language, shufﬂed at line level\ndata set is developed. The pre-trained FastText model is used for language\nrecognition. The resulting multilingual (166 languages) corpora are 6.3 TB in size,\nhave 800 billion words. The Polish segment of the OSCAR corpora consists of 6.7\nbillion words and is 47 GB in size.\nThere is a strong need for a Polish textual data-set according to the Web Society.\nWe decided subsequently to propose a freely-available tool for constructing such a\ndata-set and corresponding language models using the Common Crawl archive. The\nCommon Crawl archive is built by the non-proﬁt Common Crawl organisation\nfounded by Gil Elbaz in California. The purpose of Common Crawl is to enable\nwider access to web information by manufacturing and supporting an open web\ncrawl repository. The Common Crawl Archive\n4 (Mu¨hleisen & Bizer, 2012; Smith\net al., 2013) is a publicly available index of the web which contains petabytes of\nscraped web site data. The data is made available both as raw HTML and as text\nonly ﬁles. The Common Crawl corpus includes billions of pages of web crawl data\ncollected over eight years. Common Crawl offers the largest, most comprehen-\nsive—open repository of web crawl data on the cloud. Our Polish subset of\nCommonCrawl after pre-processing contains more than 22 billion words, making it\nthe largest among known open and readily accessible Polish text repositories.\n2.2 Language models\nVectors are common both in AI and in cognitive science. The representation of\ndocuments and queries as vectors in a high dimensional space was introduced by\nSalton and McGill (1983). The Vector Space was used to discover the frequency of\nspeciﬁc words as a clue for representing the information contained in documents.\nThe frequencies of words and phrases now play a crucial role in language\nmodelling.\nModels that assign probabilities to sequences of words are known as Language\nModels (LM)—a probability distribution over sequences of words. Given such a\nsequence of length m, an LM assigns a probability to the whole sequence. The\nsimplest model assigns probabilities to sequences of words, the n-gram. An n-gram\nis a sequence of n words: a unigram is a single word like machine; a bigram is a\ntwo-word sequence like machine learning, and a trigram is a three-word sequence\nlike recursive neural network. The term, n-gram is used to mean either the word\nsequence itself or the predictive model that assigns it a probability. The unigram\nlanguage model is commonly known as the bag-of-words model. Technically, an n-\ngram model is a collection of sequences of words with the corresponding\nfrequencies, in which probabilities are counted dynamically using those frequencies.\nHaving a method of estimating the relative likelihood of different phrases is useful\nin many natural language processing applications, especially ones that generate text\nas an output from speech recognition or generation, machine translation, or spelling\ncorrection (Manning, Raghavan, & Schu¨tze, 2008).\n4 http://commoncrawl.org/.\n1050 S. Roziewski, M. Kozłowski\n123\nThere are a handful of studies on using Common Crawl Data for n-gram\ngeneration, which corresponds to the concepts and entities explored in NLP. One\nsuch study is presented by Kanerva, Luotolahti, Laippala, and Ginter ( 2014), and\noffers an overview of possible applications of Common Crawl Data. The authors\nobtained both linear and syntactic n-gram collection from a Finnish language\ninternet crawl, and made them publicly available. Some technical issues were also\nhighlighted, speciﬁcally those of raw textual data processing and language\ndetection. Another study (Buck et al., 2014) concerning n-gram counts and\nlanguage models formed on the Common Crawl Archive is broader in the number of\nlanguages analyzed. In this study, the authors stress the importance of data\nduplication and normalization. Additionally, they compare perplexity of n-gram\nlanguage models trained on corpora provided by the constrained condition of the\n2014 Workshop on Statistical Machine Translation and report that the lowest\nperplexity was achieved on their model. Moreover, the authors report that adding\nthe language models presented in their work to a Machine Translation (MT) system\nimproves the BLEU score by between 0.5 and 1.5 BLEU points.\nIn Wołk, Wołk, and Marasek ( 2017) the authors build pentagram language\nmodels of contemporary Polish language based on the Common Crawl corpus. They\nclaim that their model is more efﬁcient than Google WEB1T n-gram counts (Islam\n& Inkpen, 2009), in terms of perplexity and machine translation. Throughout their\nwork, they focus on processing text-only ﬁles. It is stated that dealing with\nstructured HTML ﬁles is insufﬁciently beneﬁcial since it is non-trivial, and demands\na high number of normalization steps with excessive computing power included.\nThe language models were built on Common Crawl data and evaluated by a\nmeasure of perplexity on a few test sets. The perplexities obtained range between\n480 and 1471.\nIn Zio´łłko, Skurzok, and Michalska (2010) a few Polish corpora were studied,\nstatistics for n-grams were computed, and a tool for manual n-gram error correction\nwas proposed. The authors pre-processed the textual data more carefully with\nrespect to Polish diacritical marks. They also used a morphological analyzer for\nbetter word recognition. The n-gram model obtained is used for automatic speech\nrecognition.\nThe bag-of-words (bows) or bag-of-n-grams document representation reports\nsurprising accuracy for text classiﬁcation and other application, owing to its\nsimplicity and efﬁciency. However, the bows also have many disadvantages. The\nwords order is lost, and thus different sentences can have exactly the same\nrepresentation, in cases in which the same words are used in different orders. Even\nthough the bag-of-n-grams method considers the word order in short context, it\nsuffers from data sparsity and high dimensionality. Moreover, the bag-of-words and\nbag-of-n-grams methods offer little guidance on the semantics of the words or more\nformally the distances between the words. The performance is usually dominated by\nthe size of the bag-of-words. Thus, there are situations in which simple scaling up of\nthe basic techniques will not result in any signiﬁcant progress, and more advanced\ntechniques must be utilized.\nDue to the rapid development of machine learning techniques in recent years, it\nhas become possible to train more complex models on much larger data-sets, and\nLanguageCrawl: A Generic Tool for Building... 1051\n123\nthese models typically outperform the simple ones. The most successful concept is\nlikely that which uses distributed representations of words.\nDistributional Semantic Models (DSMs) have recently received increased\nattention, together with the rise of neural architectures for the scalable training of\ndense vector embeddings. One of the most prominent trends in NLP is the use of\nword embeddings (Levy & Goldberg, 2014; Levy, Goldberg, & Dagan, 2015),\nwhich are vectors whose relative similarities correlate with semantic similarity.\nSuch vectors are used both as an end in itself for computing similarities between\nterms, and as a representational basis for downstream NLP tasks, such as text\nclassiﬁcation, document clustering, tagging parts of speech, named entity recog-\nnition, and sentiment analysis. The attention dedicated to neural network based\ndistributional semantics also growing substantially. The main reason for this is the\nhighly promising approach of employing neural network language models\n(NNLMs) trained on large corpora to learn distributional vectors for words\n(Mikolov, Sutskever, Chen, Corrado, & Dean, 2013c). Neural network based\nlanguage models signiﬁcantly outperform n-gram models (Mikolov, Chen, Corrado,\n& Dean, 2013a).\nMikolov et al. ( 2013a), Le and Mikolov ( 2014) have recently introduced the\nskipgram and continuous-bag-of-words models, which act as efﬁcient methods for\nlearning high-quality vector representations of words from large amounts of\nunstructured text data. The word representations computed using neural networks\nare interesting because the learned vectors explicitly encode many linguistic\nregularities and patterns. The best known tool in this ﬁeld is currently Word2Vec,\n5\nwhich allows fast training on huge amounts of raw linguistic data. Word2Vec takes\na large corpus as its input, and builds a vector space, typically of several hundred\ndimensions, with each unique word in the corpus represented by corresponding\nvector in the space. In distributional semantics, words are usually represented as\nvectors in a multi-dimensional space. The semantic similarity between two words is\nthen trivially calculated as the cosine similarity between their corresponding\nvectors.\nWith regards to the Polish language, there are a few studies that make use of\nword embeddings.\nDuring the CLARIN project, which produces vector representations of Polish\nwords (Ke˛dzia, Czachor, Piasecki, & Kocon ´, 2016). Wawer ( 2015) utilizes\nWord2Vec in order to propose a novel method of computing Polish word sentiment\npolarity using feature sets composed of vectors created from word embeddings.\nPrevious work on Polish language sentiment dictionaries demonstrated the\nsuperiority of machine learning on vectors created from word contexts, particularly\nwhen compared to the semantic orientation of pointwise mutual information (SO-\nPMI) method. This paper demonstrates that this state-of-the-art method could be\nimproved upon when extending the vectors by word embeddings, obtained from\nskipgram language models upon the Polish language version of Wikipedia.\nIn morphologically complex languages such as Polish, many high-level tasks in\nNLP rely on accurate morphosyntactic analyses of the input. In Ustaszewski (2016),\n5 https://code.google.com/p/word2vec/.\n1052 S. Roziewski, M. Kozłowski\n123\nthe author investigates whether semi-supervised systems may alleviate the data\nsparsity problem exploiting neural network based distributional models. His\napproach uses word clusters obtained from large amounts of unlabeled text in an\nunsupervised manner in order to provide a supervised probabilistic tagger with\nmorphologically informed features. Evaluations on a number of datasets for the\nPolish language including the National Corpus of Polish, PELCRA, the Polish Sejm\nCorpus, and Wikipedia were conducted.\nEach of the papers discussed above utilize a Word2Vec model built upon\nstandard well-known Polish resources, but not upon the CommonCrawl archive.\nHowever, such surveys have been performed in other countries. The authors of\nGinter and Kanerva ( 2014) train a Word2Vec skipgram model on pentagrams both\nfrom the Finnish language corpus extracted from the Common Crawl dataset, and\nthe Google-Books n-gram collection. They concern primarily, but not exclusively,\nword similarity and translation tasks. In the ﬁrst task, semantic and syntactic queries\nare used to return word similarities. The second task involves testing the ability of\nWord2Vec representation in simple linear transformation from one language vector\nspace to another. Training Word2Vec models on a collection of n-grams has the\nadvantage of compactness: speedup for an English language n-gram corpus is\nincreased by nearly 400 times, which means that it is possible to do the\ncomputations even on a single machine.\nWe have decided to focus on the Polish language in this work, and present the\nfollowing:\n– LanguageCrawl (our NLP toolkit),\n– A Polish n-gram collection,\n– a Word2Vec model trained on Polish Internet Crawl, which is based on the\nCommon Crawl Archive.\nWe provide deeper statistical information about the Polish language, and present n-\ngram counts and their distribution, which is built on the basis of the huge Polish\nInternet Crawl, and is much greater in size than the corpora analysed in Zio ´łłko\net al. ( 2010). Additionally, this study improves on previous works by utilizing an\nactor model, which enables us to take advantage of all available cores by highly\nparallelizing the corpus building process. As a result, LanguageCrawl scales well\nwith an increase in the number of worker nodes.\n3 The proposed approach\n3.1 Source data\nThe Common Crawl Archive (Crouse, Nagel, Elbaz, & Malamud, 2008), which we\nprocessed using LanguageCrawl, is an open repository of textual web crawl\ninformation containing petabytes of data. The information crawl has been gathered\nsince 2008. It is accessible to anyone on the internet either via freely direct\ndownload or commercial Amazon S3 service, for which a deposit is demanded.\nLanguageCrawl: A Generic Tool for Building... 1053\n123\nThe crawl data is stored in the Web Archive (WARC) format. 6 The WARC\nformat retains and processes data from the Common Crawl Archive dump which\ncan be as large as hundreds of terabytes in size and contains billions of websites in a\nmore effective and manageable way. The raw crawl data is wrapped around the\nWARC format, ensuring a straightforward mapping to the crawl action. The HTTP\nrequest and response are stored, along with metadata information. In the case of\nHTTP response, the HTTP header information is stored. This allows a high number\nof inquisitive insights to be gathered. The website content collected takes the form\nof an HTML document. Another available extension is WAT Response format,\n7\nwhich is in the form of JavaScript Object Notation (JSON) ﬁle. In fact, WAT ﬁle\ncontent is directly mapped from the corresponding WARC ﬁle. The same process is\nfollowed for the third obtainable extension, WET Response format,\n8 with some\ndifference in shortage of header information and plaintext web content instead of\nHTML. As most NLP tasks require only textual data, and we have access to limited\nresources, under the inﬂuence of Wołk et al. ( 2017), we decided to construct our\ntool around WET ﬁles containing plaintext website content with a minimal amount\nof metadata. Our use-cases are based on corpora extracted from the January to July\n2015 Crawl Archive, which is approximately 866 TB in size, and contains\napproximately 1.12 trillion web pages. Since we consider only WET ﬁles, we\nfetched 68 TB of compressed textual data. Although the amount of data processed is\nenormous, the Polish language constitutes only a tiny fraction of it: we estimated its\nvalue to be approximately 0.3%.\nFig. 1 Functionality diagram. Data Fetching as a core module begins all processing and is followed by\noptional Language Detection. Afterwards, the fetched data is stored in a database. This early stage\ncontinues in a loop. Both facultative, Sentence Extractor and Spelling Corrector are capable of textual\ndata enhancement. At the end of the process, N-gram Building and Word2Vec Training are deployed. The\ncrossed-circle mark denotes an or operation\n6 https://www.loc.gov/preservation/digital/formats/fdd/fdd000236.shtml.\n7 https://commoncrawl.org/the-data/get-started/.\n8 https://commoncrawl.org/the-data/get-started.\n1054 S. Roziewski, M. Kozłowski\n123\nProcessing data from the Common Crawl Archive is a colossal task, which is\nseverely limited by the internet bandwidth connection, and a bottleneck for data\nfetching. In our case it took several for weeks to download enough data to construct\na reasonable Polish website corpora. Our LanguageCrawl toolkit provides highly\nconcurrent actor-based architecture for building a local Common Crawl Archive,\nalong with n-gram collection for a speciﬁed language. Other tasks such as textual\ndata processing, cleaning, and training Word2Vec on the corpora are accomplished\nwithin the same aforementioned, resource-efﬁcient system. The core of data\nfetching and the language detection module is implemented using the successful\nAkka framework based on the Actor Model paradigm. The LanguageCrawl tool is\nlaunched on an efﬁcient computer to perform these tasks.\n3.2 Modules\nLanguageCrawl consists of three core modules along with three submodules. The\nfoundational components of the system are essential for many of LanguageCrawl’s\nfeatures, including data fetching, n-gram building, and Word2Vec training. Figure 1\nshows a functionality diagram. The key and optional traits are shown in the colors\nblue and pink, respectively. Data Storage is separated to enforce the boundaries of\nthe Data Fetching abstraction layer. The ﬁrst stage of processing implemented in the\nData Fetching module is continually repetitive until all URLs linking to Common\nCrawl Archive resources have been processed. The deduplication process is\nperformed based on URLs hash comparison, in order to locate a duplicated web\npages (Zeman et al., 2017). In the next step, we invoke Sentence Extractor for the\ncleansing of textual crawl data. The aim of this procedure is to remove all text\nphrases which are both functional parts of a website and do not carry any\nmeaningful content. We may also use Spelling Corrector, which aims to ﬁx\ncharacter encoding problems within a single word. Subsequently, N-gram Builder is\ncalled, which forms the desired n-grams for speciﬁed ranks. We are then able to\nutilize Word2Vec Training, with the ability to implement a speciﬁc setup for\nparameters. Two training algorithms, CBOW (which is a continuous-bag-of-words\nmodel) and skipgram, may be deployed. The other factors that can be detailed are as\nfollows: the dimensionality of the feature vectors, the maximum distance between\npredicted and current word in a sentence, the number of worker threads, and\nskipping words with lower total frequency.\n3.3 Actor model\nEach monthly Common Crawl Archive contains data resources distributed into\nmany 140 MB gz-compressed ﬁles in order to facilitate data processing. Since\ntextual information resides in disjoint ﬁles, it is straightforward to process the data\nin parallel. Processing web-scale data requires not only the passing of millions of\nmessages concurrently, but also handling multiple failures, such as data store\nunavailability or network failures. That is why, we decided to use the Akka\nframework: a high-performing, resilient, actor-based runtime for managing\nLanguageCrawl: A Generic Tool for Building... 1055\n123\nconcurrency. We selected the Scala language for implementation of the Data\nFetching stage.\nThe theoretical underpinnings of the Actor Model, a mathematical model of\nconcurrent computation, are described in Hewitt, Bishop, & Steiger ( 1973). In\nAkka, actors are organized in a hierarchical structure known as the actor system,\nwhich is equipped with supervisor strategy—a straightforward mechanism for\ndeﬁning fault-tolerant service.\nAkka Framework\nThe major beneﬁt of using the Akka Framework\n9 is actor-based concurrency. The\nactors, like in a hierarchical organization, unaffectedly form a stratiﬁed structure.\nThey exist to the outside as actor references—objects which can be passed around at\nease and without limitation. One actor, which is responsible for a speciﬁc\nfunctionality within the program, might divide a larger task into smaller, more\nmanageable pieces. For this aim it creates descendant actors and manages them.\nEach actor must know its supervisor by means of its reference. After the completion\nof a task, the current actor sends a message back to its executive. The supervisor\nreceives a notice which indicates the state of the sent request, i.e. success or failure,\nand depending on that outcome, decides which routine to perform. The most\nsigniﬁcant aspect is that it is impossible to explore inside an actor and acquire an\nimage of its state from the outside, except when the actor intentionally releases that\ninformation itself. There are several supervision strategies, most of which are\nrelated to the nature of the work and the character of the failure. When a problem\noccurs during processing, the subordinate can be restarted with a cleansing of its\ninternal state, a child actor might be stopped permanently, or the failure may\nescalate leading to a supervisor fault. On the other hand, the secondary actor might\nbe resumed with its stored internal state. It is essential to see an actor as a part of a\nsuperintendence hierarchy, which has major implications on subordinates when\nhandling the failure. For example, resuming an actor will resume all of its\ndescendant actors, and the opposite is true when terminating an actor. When we\nrestart an actor, we can terminate and start all of its subordinates again, or preserve\nchild states to themselves. The creation and termination of actors occurs in an\nasynchronous manner, without blocking the supervisor. The actor object has several\n9 http://akka.io.\nFig. 2 Message passing diagram. FileMaster (M) sends message to its FileWorkers (w1; ... ; wN ) wrapped\nin the router wR. FileWorkers needs Bouncers ( b1; ... ; bM ) as helpers for language detection and writing\ndata to Cassandra Storage. bR is a router for Bouncer Actors. Db is Cassandra Storage\n1056 S. Roziewski, M. Kozłowski\n123\nuseful characteristics. Namely, it encapsulates state, owns behavior and commu-\nnicates with other actors by exchanging messages. The actor object embodies some\nvariables which reﬂects the possible states it might be in. This data makes an actor\nvaluable, and one which must be protected from misuse by other actors. Actor\nbehavior is an action to be taken in reaction to the message received. Actors\ncommunicate by passing messages which are generally immutable. Those\nnotiﬁcations are placed into the recipient’s mailbox.\nIn the Akka actor model, there is no guarantee that the same thread will execute\nan identical actor for another messages. Thus, it is strongly recommended to use\nonly immutable messages in order to prevent messages corruption. The actor’s\nobjective is to process messages, which can be thought of tasks to do wrapped\naround data. The mailbox model assumes each actor has only a mailbox, and brings\ntogether the sender and receiver. The letters are enqueued in time-order of\ndispatching. It means that messages from other actors might not have an order at\nruntime, since some stochasticity occurs during the distribution of actors across\nthreads. Passing a multiplicity of messages to the same receiver from the same actor\nwill result in the same sequence. When an actor terminates by failing without\nrestarting, stopping itself, or being stopped by its master, it will relieve all resources\nand pass all messages from its mailbox to the system’s dead letter mailbox. The\nactor’s mailbox is replaced within the actor reference by a system mailbox. The\nmain actor within the whole system is Actor System, which forms a hierarchical set\nof actors sharing the same conﬁguration. It ﬁrst creates descendant actors and\nmanages them, providing a certain conﬁguration which is common for the child\nactors. As a matter of fact, Actor System starts and ends the application and is the\nroot of the system.\nLanguageCrawl In LanguageCrawl, Actor System creates File Master, an actor\nresponsible for initiating File Workers and providing them with tasks to be\naccomplished. In fact, File Master is the only actor which manages and distributes\ntasks to its subordinates. It processes a ﬁle containing a list of links to WET ﬁles\nbelonging to a speciﬁc monthly Common Crawl Archive dump. Each crawl contains\n32000 URLs on average. Having fetched all of the links, File Master dispatches\nthese URL paths to its individual File Workers. In an effort to avoid context-\nswitching we decided to limit the number of File Workers to the number of virtual\ncores available in the cluster on which the program was run. Figure 2 shows a\nmessage passing diagram to explain in more detail all important processes which\noccur during runtime. In the MainAp Scala object we create Actor System, and\ndeﬁne the conﬁguration for the whole system and data-base connection. Actor\nSystem begins the processing ﬂow by sending a message Start Downloading to File\nMaster, specifying a ﬁle from which the URL links will be fetched and the number\nof File Workers to be initiated. File Master creates routers for both types of actors,\nFile Worker and Bouncer. A router can be considered as a pool containing available\nactors of a certain type. Afterwards, File Master processes the URLs linking to WET\nﬁles wrapped in a compressed archive from the Common Crawl Repository, creates\nits File Workers, and feeds each of them with one URL until all links are fetched.\nFile Master activates its subordinates by continually sending them the Process File\nmessage with a URL string. File Master asynchronously awaits until it receives the\nLanguageCrawl: A Generic Tool for Building... 1057\n123\nmessage, Processing Finished from File Worker, with additional information\nwrapped around an object. Subsequently, the master is capable of dispatching a new\norder to the freed worker. In the meantime, other workers process the URLs\nallocated to them by downloading data and extracting textual content from zipped\narchives. In order to speed up processing, an iterator pipeline is created. Several\noperations on streams occur, including unzipping and creating a buffer reader\niterator. Textual content is extracted on top of the pipeline. During iteration over\ntextual content, individual web documents are recognized by pattern matching. The\nopening fragment of metadata serves as an indicator for that purpose. Then, File\nWorkers wrap a textual WET ﬁle crawl around an object, and send specimens of\neach particular web page for language recognition to the available Bouncers. The\nnumber of Bouncers was seto to 36—slightly more than the number of workers.\nThere is little beneﬁt in increasing that value signiﬁcantly, but decreasing the\nnumber of workers leads to a lowering of processing efﬁciency. When a Bouncer\nreceives a Please Let Me In message with a document for language detection, it uses\nan external native library with a Scala wrapper for this purpose. It takes the ﬁrst 100\nwords from the content, and passes them on to the language detector. If the selected\nlanguage is within our interest, the whole document is transferred to the database.\nFor better non-locking performance, writing to storage is asynchronous. This was\nachieved by using Future traits. As Future is completed, two cases are handled:\nsuccess and failure. In the case of success, the number of written documents is sent\nto the File Worker: in the case of failure, the logger keeps track of information about\nthe issue.\nLanguage Detection Other than downloading of the data itself, language\ndetection constitutes the largest amount of executing time (Kanerva et al., 2014)\nduring the collection of crawl data. Thus, we selected the language detector with\nrespect to its speed and accuracy, along with the ease of utilizing it from the Scala\nframework. The language detection module uses a highly efﬁcient and accurate\npackage, optimized for space and speed, which is known as Compact Language\nDetector 2 (CLD2) package.\n10 The tool is based on a Naive Bayes classiﬁer, which\nis a probabilistic model. The ﬁrst 100 words from any document are used as the\ninput for CLD2. The package also has several embellishments for enhancement of\nthe algorithm. Web-speciﬁc words containing no language information were\ntruncated, such as page, click, link, copyright. Repetitive words that could affect the\nscoring, such as jpg, foo.png, bar.gif, were ﬁltered. The charset of text sent for\nlanguage recognition to CLD2 must be converted to UTF-8 character encoding. As\nan output, the language detector identiﬁes the top three estimated languages and\ntheir probabilities. Such an indicator might be signiﬁcantly beneﬁcial, especially\nwhen the web page under consideration is multilingual. CLD2 implementation\nincludes three alternative token algorithms to provide better prediction accuracy for\ncertain languages which have peculiar properties. In the majority of cases, language\nrecognition is achieved by quadrams scoring, whereas for semanto-phonetic writing\nsystems and syllabic alphabets, the analysis is conducted on the basis of single\nletters scoring. The term, quadram denotes a sequence of four adjacent letters.\n10 Language detection is conducted with the https://github.com/CLD2Owners/cld2.\n1058 S. Roziewski, M. Kozłowski\n123\nLowercased Unicode text consisting of marks and letters, without digits and HTML\ntags, is scored afterwards. The training corpus was constructed manually from\nselected textual web samples of each language, which were then automatically\nenlarged with precaution by a further 100 million web pages. Both letter sequences\nextraction and scoring use table-driven methods, schemes which allow the system to\nsearch for information quicker, simpler and more efﬁciently. The algorithm is\noptimized by means of computation speed and space, speciﬁcally Random-Access\nMemory (RAM).\nCLD2 is ten times more time efﬁcient than other language detectors (Lui &\nBaldwin, 2012), is capable of recognizing 70 languages, and is wrapped in 1.8 M of\nx86 code and data structures. The primary lookup table offers coverage of 50\nlanguages, relying on quadrams, and consists of 256 K four-byte entities. The\nlanguage detection performed on the average website containing 30 KB of HTML\nlasts only 1 millisecond on a current x86 CPU. Using the CLD2 package, one can\nidentify 83 world languages. In one large-scale multilingual study (Zeman et al.,\n2017), the authors used this library for language identiﬁcation.\nAfter content written in a predeﬁned language has been detected, Bouncers may\nperform optional spelling correction for words with character encoding problems\n11\nand subsequently enter the corrected content to the Cassandra Database.\nCassandra Storage We decided to take advantage of Apache Cassandra as the\nLanguageCrawl database, owing to its efﬁciency, scalability, and fault-tolerance.\nApache Cassandra is an open-source project developed by the Apache Software\nfoundation, which offers a host of interesting big-data-beneﬁcial features. It is a\ndistributed document database which supports a scaling trait, and is focused on\nlessening read/write latency (Rabl et al., 2012). Automatic data replication among\ncomputer nodes might be selected in the conﬁgurational setup. The adopted data\nmodel scheme is a hybrid between column-oriented and key-value pair models.\nCassandra ensures a handy Application Programming Interface (API) known as\nCassandra Query Language (CQL). A set of useful Cassandra connectors exists for\nmany programming languages. Working with Akka for Scala, we utilized an open-\nsource driver connector dedicated to Scala, developed by DataStax company.\n12\nAs our resources are limited, we built a two-node Cassandra Cluster for our\nstudy, containing two tables. The ﬁrst stores a list of processed WET ﬁle links, and\nthe second is the main table for crawl data. Having information about ﬁles already\ndownloaded allows the system to be fault-tolerant.\n4 Data set\nThe collected data-set contains nearly 55 million documents and more than 35\nbillion words, of which 12% and 5.3% have character encoding problems,\nrespectively. The whole size of the textual corpora exceeds 407 GB. We performed\n11 The corrected words should be the same length as the original. A dictionary of correct words was built\non a subset of literary texts taken from National Corpus of Polish (Przepio´rkowski, et al. 2008), NCP.\n12 https://github.com/datastax.\nLanguageCrawl: A Generic Tool for Building... 1059\n123\ndocument processing in order to extract valuable content. Entries containing more\nthan four sentences and without encoding problems were trained. Ultimately, 36.2%\nof the elements were omitted, resulting in a set containing 35 million documents, 22\nbillion words, and comprising 166 GB in total. We estimated the mean values for\ndocument length to be 741 words, or 4848 characters. A document in the data-set\nconsists of 44 sentences on average. In one multilingual study (Zeman et al., 2017),\nthe authors collected Polish Common Crawl corpora consisting of 5.2 billion\nwords—the corpus used in this work is four times that size.\nFigure 3 shows the top-ten primary domain distribution across the documents.\nThe domains are well-concentrated and form three large clusters. The top-three\ndomains contribute more than 90% of the whole set. Furthermore, one somewhat\nFig. 3 A pie chart showing the\ntop-ten primary domain\ndistribution across the\ndocuments. The ﬁrst ﬁve entries\nrepresent more than 97% of the\ntotal. The majority of the\ndocuments are directly related to\nPolish primary domains. In\nparticular, many entities with\ndomains such as com and org\nhave the subdomain pl e.g.\npl.wikipedia.org,\npl.tripadvisor.com\nFig. 4 A pie chart representing\nthe distribution of the top-ten\ndomains across all documents.\nThe ﬁrst ﬁve entities constitute\nmore than 80% of all domains\n1060 S. Roziewski, M. Kozłowski\n123\nsurprising fact can be observed: among the top-ten primary domains, there is one\nwhich has German address. However, as 1.5 million Polish citizens currently live in\nGermany, this anomaly can be justiﬁed.\nIn Fig. 4 we present the distribution of the top-ten domains for documents. There\nare two major document sources within the data-set, namely pl.tripadvisor.com and\npl.wikipedia.org. The ﬁrst ten domains correspond to more than 99.9% all\ndocuments within the corpora.\nWe utilized of the CLD2 library to differentiate languages, as we mentioned in\nthe section devoted to language detection. We provide a brief language distribution\nanalysis into the collected data-set in order to ensure its coherency. The procedure is\nsimple: we attempted to discover how many words to a particular language\ndictionary. We investigated all the vocabulary in the gathered corpora. Polish words\ncomprise up to 76% of the dictionary, whereas 17% and 3% of entities are\nunclassiﬁed and English language, respectively. Five other languages Spanish,\nFrench, German, Russian, and Czech constitutes 2.7% of all words in the data-set.\nEntities not belonging to any language consist of a few types: fused words,\nmisprinted words, or words which cannot be found in formal dictionary.\nFigure 5 shows the language distributions across all documents. On the x axis we\ncan see the document fractions. We selected several popular languages across the\nglobe which utilize Latin alphabets, such as English, Spanish, French, and German.\nTwo Slavic languages were added, due to their similarity to Polish language and\ntheir cultural proximity: Russian and Czech. Though the Russian language uses the\nCyrillic alphabet, it is analyzed nevertheless, since it is a popular choice for Polish\nforeign language learners. It can be observed that approximately 55% and 49% of\nthe documents contain 97.5% and 99% Polish words, respectively. On the other\nhand, 15% of the words in 30% of the entries were unclassiﬁed. The second most\nfrequent language within the data-set is English, with 5% of the documents\ncomprising nearly 40% English words. Entries with more than 50% of their words in\nFig. 5 Curves representing\nlanguage distributions over\ndocument fractions for Polish\nand six other languages are\npresented. In addition, a line for\nunknown (undetected)\nlanguages is shown. For the\nmost part, the whole data-set\nconsists of documents written in\ndetectable Polish (the content of\n55% of the documents\ncomprises 97.5% Polish words.\nThere is a considerable number\nof documents whose original\nlanguage remains unknown (in\n30% of documents, 15% of the\nwords were unclassiﬁed)\nLanguageCrawl: A Generic Tool for Building... 1061\n123\nother languages constitute less than 1% of the data-set. On the basis of this study, we\ncan justify that CLD2 language prediction efﬁcacy is well-founded.\n5 Experiments\n5.1 N-gram language model\nThis section offers deeper insights into the collection gathered, and presents the\noutcome of constructing n-gram corpora. Moreover, we report on several statistical\ncharacteristics, demonstrate examples of n-grams, and discuss the properties of the\ncorpora in terms of weight and quantity presented with speciﬁc graph charts for\nbetter understanding.\nBased on the Common Crawl Archive which had been scraped and processed, an\nn-gram model was constructed. After ﬁltering out non-Polish web page content, data\nnormalization was performed by lowercase conversion, sentence extraction using a\ntokenizer from the NLTK Python toolkit (Loper & Bird, 2002), removing all words\ncontaining non-alphanumerical characters, numbers, or encoding errors. We\nimplemented statistical post-analysis to truncate fused and misprinted words. Also,\nwords longer than twenty characters were erased before the creation of the language\nmodels. Polish diacritical marks and stop-words were preserved, since phrase search\nis enhanced by using them (Zio´łłko et al.,2010). After creating the n-gram model\nand data aggregation, all duplicated n-grams were removed. As a result, both n-\ngram set sizes and n-gram occurrences were reduced by approximately 54 % and\n60%, respectively (see both total # occurrences and collection size in Table 6 and 7).\nMeanwhile, eliminating fused and misprinted words diminished the data volume by\nanother 31 %. This indicated, that the raw crawl data was highly duplicated and\ncontained useless content in many cases. Thus, we emphasize how important pre-\nTable 1 Overview of samples of unique unigrams and their counts\nTop 10 Middle segment\nSample Count Sample Count\nJest (is) 82,731,908 Pokorne (humble) 3201\nHotele (hotels) 37,044,318 Ros ´linnos´ci (vegetation) 3200\nWie˛cej (more) 27,415,228 Łosi (moose in plural) 3200\nMa (to have in 3rd person) 22,112,466 Zabijano (to have been killed) 3200\nForum 22,070,464 Dojazdo ´wka (diminutive access) 3200\nWszystkie (all) 21,041,834 Krokowych (related to step, adj) 3200\nTego (this) 19,571,471 Pseudonimami (form of pseudonyms) 3200\nMo_ze (can/maybe) 18,877,289 Boguchwałowice (Polish small town) 3200\nMam (I have) 16,988,514 Pozdro ´wka (diminutive salute) 3200\nOdpowiedzi (answers) 16,829,727 Pograj (imperative of play) 3200\n1062 S. Roziewski, M. Kozłowski\n123\nprocessing is when working with the Common Crawl Archive. It is worth\ninvestigating how early deduplication, such as removing copyright notices,\ninﬂuences the resulting corpora. The N-gram Collection of the Polish Internet\nwas established based on processing and analysis of an open repository Common\nCrawl Archive. It is formed from sets of unigrams, bigrams, trigrams, tetragrams\nand pentagrams. The starting point of this extensive analysis was to sort the n-gram\ncollection in descending order with respect to entity occurrences, for each type of n-\ngram.\nWe provide n-gram examples of ten most frequently occurring entities for each\nn-gram type, and also a brief sample of unigrams, bigrams and trigrams from the\ncollection.\nTable 1 shows two lists of unique unigrams sorted by occurrence frequency. One\nlist contains the top ten n-grams, whereas the second is related to mid-frequency\nelements. We present examples of Polish n-grams along with their English\nequivalents. Stopwords frequently open the entries, so we have removed them from\nthe list to demonstrate some less obvious examples. The occurrences decline\nsigniﬁcantly when the index position increases. The tail of the collection is\nunproﬁtable, and is made of redundant words, mainly misprints and fused words,\ne.g. unigrams with frequency less than ten. Thus, they are not included in this table.\nIn Table 2 we present the ten most frequently chosen bigrams, not including\nstopwords. Most of the ten bigrams are formed from commonly used internet\nphrases, and their counts drop much slower than in the case of unigrams. The middle\nsegment consists mostly of somewhat idiomatic expressions. The tail of the\ncollection for rare bigrams is not shown.\nTable 3 contains two lists analogous to the previous examples. The chosen\nentities are also presented without stopwords. The trigram count values are\ncomparable. The most common of them are idiomatic phrases once again, and some\nof them are linked to corresponding bigrams from Table 2.\nTable 2 Overview of samples of unique bigrams and their counts\nTop 10 Middle segment\nSample Count Sample Count\nNapisz, recenzje (write review) 4,155,761 Zdje ˛cia, gołe (naked pictures) 3401\nPolityka, prywatnos´ci (privacy policy) 3,206,055 Frontera, hotele (frontera hotels) 3401\nAdres, e-mail (email address) 3,098,369 Jedna, dawka (single dose) 3401\nProsze˛, czekac´ (please wait) 2,826,672 Polska, ksiegarnia (Polish bookstore) 3401\nWie˛cej, informacji (more information) 2,736,751 Masz, ciekawy (you’ve interesting) 3401\nSłowa, kluczowe (keywords) 2,620,938 Dwudziestu, pie ˛ciu (25—numeral) 3401\nRestauracje, atrakcje (..., attractions) 2,553,696 Rodzenia, terapie (childbirth...) 3401\nPliko´w, cookies (ﬁles cookies) 2,535,537 Krakow, powiat (Cracow district) 3401\nInternet, explorer 2,475,434 Naszego, testu (of our test) 3401\nZobacz, wie˛cej (see more) 2,382,278 Football, gdansk (football Gdan ´sk) 3401\nLanguageCrawl: A Generic Tool for Building... 1063\n123\nThe top ten tetragrams and pentagrams are illustrated in Table 4 and 5,\nrespectively. It is noticeable that the top tetragrams are somewhat associated\nsemantically—a statement might be inferred from reading any of them. On the other\nhand, the most frequent pentagrams are generally composed of the top tetragrams\nand they are related to each other. The diversity in n-gram occurrences is hardly\nnoticeable.\nTable 6 yields an overview of the corpora by the means of the total occurrences\nof n-grams and sizes of corpora given in GB with respect to n-gram type. The\nnumber of all words within the crawl data extends beyond 22 billion entities and 166\nTable 3 Overview of samples of unique trigrams and their counts\nTop 10 Middle segment\nSample Count Sample Count\nWikipedii, wolnej, encyklopedii 2,136,564 Canonical, ltd, dowiedz 3302\nInternet, explorer, mozilla 2,021,722 Ceny, zdje ˛cia, wektory 3302\nExplorer, mozilla, ﬁrefox 2,021,543 Kontakt, twoja, opinia 3302\nWindows, internet, explorer 2,020,873 Zadawane, pytania, blog: 3302\nMozilla, ﬁrefox, google 2,020,341 Umowa, członkowska, licencja 3302\nWitryna, tripadvisor, mo _ze 2,020,211 Strony, english, deutsch 3302\nTripadvisor, moze, nie 2,020,131 z, komo ´rki, jezyk 3302\nNie, byc´, wyswietlana 2,020,082 Informacje, prawne, umowa 3302\nObsługiwane, sa˛, naste˛puja˛ce 2,019,952 Editorial, wideo, wszystkie 3302\nsa˛, naste˛puja˛ce, przegla˛darki 2,019,876 Je ˛zyk, strony, english 3302\nTable 4 Overview of samples\nof unique tetragrams and their\ncounts\nTop 10\nSample Count\nInternet, explorer, mozilla, ﬁrefox 2,080,495\nTripadvisor, mo _ze, nie, byc´ 2,080,275\nWitryna, tripadvisor, mo _ze, nie 2,080,143\nsa˛, naste˛puja˛ce, przegla˛darki, windows 2,079,846\nMozilla, ﬁrefox, google, chrome 2,079,428\nz, wikipedii, wolnej, encyklopedii 2,079,390\nZauwa_zylismy, ze, u _zywasz, nieobsługiwanej 2,078,256\nze, u _zywasz, nieobsługiwanej, przegla˛darki 2,078,189\nMo_ze, nie, byc´, wys´wietlana 2,077,754\nNie, byc´, wys´wietlana, poprawnie 2,077,235\n1064 S. Roziewski, M. Kozłowski\n123\nGB of uncompressed textual data. The full volume of all corpora exceeds 15 TB of\nunaggregated data.\nTable 7 summarizes the n-gram collection obtained after data aggregation. The\nPolish n-gram corpora, reaches nearly 17 million entries. It contains more than 3\nmillion unique words, and is nearly 700 MB in size. The numbers of occurrences for\nhigher-ranked n-grams are signiﬁcantly larger than in the case of unigrams.\nA survey of the length of the various n-grams may be an interesting prospect for\nlanguage researchers. We offer additional insights on the topic, beyond what can be\nfound in the typical dictionary, thanks to the larger size of the corpus we analyzed,\nthe statistical measures we provided and the unique textual data source we gathered\nTable 5 Overview of samples\nof unique pentagrams and their\ncounts\nTop 10\nSample Count\nTripadvisor, mo _ze, nie, byc´, wys´wietlana 2,223,587\nMo_ze, nie, byc´, wys´wietlana, poprawnie 2,223,498\nObsługiwane, sa˛, nastepujace, przegla˛darki, windows 2,223,438\nsa˛, naste˛puja˛ce, przegla˛darki, windows, internet 2,223,410\nNaste˛puja˛ce, przegla˛darki, windows, internet, explorer 2,223,381\nWindows, internet, explorer, mozilla, ﬁrefox 2,223,314\nPrzegla˛darki, windows, internet, explorer, mozilla 2,223,227\nInternet, explorer, mozilla, ﬁrefox, google 2,223,221\nExplorer, mozilla, ﬁrefox, google, chrome 2,222,997\nz, wikipedii, wolnej, encyklopedii, skocz 2,185,113\nTable 6 Summary of numbers\ntotal occurrences for all type n-\ngrams and sizes of corpora\nN-gram type Total # occurrences Collection size\nUnigram 22,335,478,164 0.26 TB\nBigram 19,969,363,630 1.82 TB\nTrigram 18,375,050,955 3.38 TB\nFour-gram 17,099,346,331 4.48 TB\nFive-gram 16,071,746,162 5,38 TB\nTotal 93,850,985,242 15.32 TB\nTable 7 Overview of numbers\nof unique n-grams and sizes of\ncorpora, for each n-grams type\nN-gram type Total # occurrences Collection size\nUnigram 3,122,459 50 MB\nBigram 2,614,875 90 MB\nTrigram 3,813,750 135 MB\nFour-gram 4,211,349 182 MB\nFive-gram 3,648,194 232 MB\nTotal 17,410,627 689 MB\nLanguageCrawl: A Generic Tool for Building... 1065\n123\nfrom websites. We believe that all of that may enrich understanding of the\ncharacteristics of Polish language, and shed new light on its analysis.\nTable 8 presents several statistics pertaining to the n-gram length for each n-gram\ntype, including the mean, standard error, median, and two percentiles: the 10th and\nthe 90th. This data infers that the average Polish word is eight letters long, which\nwould appear to be an overestimation due to the fact that a high number of fused\nwords remain undetected in the n-gram corpora. Thus, it is reasonable to assume\nthat all of our statistics are affected by this issue.\nTable 8 Statistics of unique n-\ngrams’ lengths N-gram type Mean SE Median 10th 90th\nUnigram 8.89 2.44 9 5 13\nBigram 15.13 3.36 15 10 20\nTrigram 23.20 5.46 23 15 31\nFour-gram 31.45 6.84 31 22 40\nFive-gram 38.71 8.12 39 28 50\nFig. 6 The n-gram curves show the counts of each n-gram type with respect to its length in characters,\nand visualizes the data from Table 6. The two maxima in the unigram line chart represent the stop-words\ncounts and the most frequent size of Polish word, respectively. The usual length of Polish word is six\ncharacters. The curves of the n-gram occurrence functions for bigrams, trigrams, tetragrams and\npentagrams form a right-skewed bell-curve, with long tails which widen as n-gram length increases. The\nchart curvatures lessen as the n-gram rank increases, and the maxima shift right by the value of 5\n1066 S. Roziewski, M. Kozłowski\n123\nFigure 6 shows n-gram occurrences for each n-gram type with respect to its\nlength. The chart is based on much broader data than in paper (Roziewski,\nStokowiec, & Sobkowicz, 2016), though the characteristics remain similar.\nA few interesting phenomena concerning Polish n-gram corpora can be observed\nhere: the number of occurrences have the same order of magnitude; for higher order\nn-grams, charts’ shapes resemble bell curves, which broaden as the n-gram rank\nincreases; the charts are more right-skewed and ﬂatten with greater n-gram order;\nand the mean and median shift as the n-gram length increases, which is caused by\nthe long right tail of the curves. The elongated tails for longer n-gram length may\nexist due to concatenation of undetected words. The chart depicting the distribution\nof unigrams is different from the others, as it demonstrates two local maxima. The\nﬁrst maximum may be the result of the existence of stop-words in the n-gram\ncorpora, whereas the second, should be the most frequent word length in the Polish\nlanguage based on Common Crawl Data, which is six characters. Stop-words occur\nfrequently in the corpora: roughly 44% of the 274 frequently-occurring unigrams in\nour corpora are stop-words. We have estimated this ratio by comparing the numbers\nof the most frequent unique unigrams (only stop-words), and the Polish stop-words,\nextracted from the NLTK Python tool (Loper & Bird, 2002). The average length of\nthose 44% of the unigrams is 3.32, which is reﬂected by the ﬁrst lower maximum in\nFig. 6. We can recognize that the maxima for higher-rank n-grams are shifted\nFig. 7 Summary of the instances of each n-gram type for the 100 most frequent n-grams. The line charts\ndemonstrate the variability in n-gram counts from the n-gram frequency table. The shapes of n-gram\noccurrence functions are steeper for unigrams, bigrams and trigrams, whereas for tetragrams and\npentagrams the curvature is ﬂatter, with long tails widening to the end of the n-gram collection. The\ngeneral trend is asymptotic downwards for the ﬁrst three charts, and a smooth decay for the remainder\nLanguageCrawl: A Generic Tool for Building... 1067\n123\nroughly by 6. We may infer from this fact that the average length of a Polish word is\napproximately that value, depending on the Polish n-gram corpora.\nThe charts from Fig. 7 present the occurrences of the top 100 n-grams for each n-\ngram rank analysed. The shapes of curves and the general characteristics are in\naccordance with results published in work (Roziewski et al., 2016) and remain\nintact.\nIt is noticeable that Zipf’s Law manifests itself in the asymptotic decay of n-gram\noccurrences, especially in the case of the ﬁrst three curves. The law states that the\nword occurrence in a corpora is inversely proportional to its index in the frequency\nlist. The curves representing tetragrams and pentagrams are ﬂatter. This may be\ncaused by the fact that those n-grams are more highly correlated to each other\namong the n-grams within a rank.\n5.2 Perplexity\nPerplexity is a measure used for the evaluation of language models. It indicates how\nwell a language model can predict a word sequence from the test set. It can be\nexpressed by the formula:\nPPðWÞ¼\nY\nN\ni¼1\nPðwijw1... wi/C0 1Þ/C0 N ; ð1Þ\nwhere N, W and wi are a sequence length, a word chain and wi ith word,\nrespectively.\nWe have not introduced any advanced smoothing methods as e.g. exponential\nsmoothing methods. We cope with the problem of Out Of Vocabulary tokens using\ndefault minimal probability of appearance i.e. we performed replacement OOV\ntokens with 1=N marginal value, where N is a number of all tokens in the training\nset.\nWe selected 10\n5 documents at random to construct both sets, both for language\nmodel creation and its evaluation. The language model is built on the former set,\nand the measurement of perplexity was performed on the latter. The training set is\nthree times bigger in size than the test set. We computed the perplexity measure\nwithout making use of smoothing.\nIn Table 9 the perplexities evaluated on the test set are shown. We calculated the\nmean, median, and standard error for the perplexities. The median value is far lower\nthan the mean value, and the standard deviation is high. A useful example can be\nfound in the prediction ranks. Each prediction’s rank is based on the probability\nTable 9 Perplexities for\ndifferent n-grams N-gram type Median Mean SE\nBigram 771 2582 4187\nTrigram 487 2145 4079\nFour-gram 383 2362 4432\nFive-gram 338 2519 4653\n1068 S. Roziewski, M. Kozłowski\n123\nestimated by the language model. In most cases, the prediction rank is 1, meaning\nthat the model performed the best. However, in some cases when the model fails, it\nfails signiﬁcantly, leading the perplexity mean and standard deviation values to rise\nsubstantially. Thus, the median value of the perplexity is effectively lower than its\nmean.\nIn Popel and Marecˇek (2010) the authors reported perplexities for bigrams and\ntrigrams ranging from 368 (Catalan) to 3632 (Czech) and 325 (Catalan) to 3530\n(Czech), respectively. Since Polish language is morphologically rich in a similar\nway to Czech, we can infer that our perplexities concur with Popel and Marec ˇek\n(2010). In Wołk et al. (2017), the perplexities for Polish language models based on\nthe Common Crawl corpus range between 480 and 1471. These are signiﬁcantly\nlower values. However, the exact perplexity measure—neither the mean nor the\nmedian—is mentioned. In fact, in Wołk et al. ( 2017) deduplication was applied for\nremoval of repeated sentences. Such an approach appears beneﬁcial. Another\nperplexity study was performed in Buck et al. ( 2014) with mean values of 209 and\n294 for the French and Spanish languages, respectively. In Ziolko and Skurzok\n(2011) the authors reported perplexity for Polish corpora ranging from 4258\n(literature) to 16436 (Wikipedia), though the computational details were omitted.\n5.3 Word2Vec\nWord2Vec computes the distributional vector representation of words, and has been\nshown to help learning algorithms boost their scores in natural language processing\ntasks (Mikolov et al., 2013c). Continuous bag-of-words and skipgram architectures\nyield vector representations of words, which are useful in various natural language\nprocessing applications such as machine translation, named-entity recognition, word\nsense disambiguation, tagging, and parsing. The skipgram model is a method of\nlearning word vector representations which is useful in predicting their context in\nthe same sentence. The aim of the skipgram model is to maximize the function of\naverage log-likelihood, given a sequence of training words w\n1; w2; ... ; wT :\n1\nT\nXT\nt¼1\nXk\nj¼/C0 k\nlog pðwtþjjwtÞ; ð2Þ\nwhere k represents the size of the training window. Each word w corresponds to two\nvectors uw and vw which are vector representations of w as word and sense\naccordingly. The probability of accurately predicting a word wi given wj is\ndescribed by the softmax model:\npðwijwjÞ¼ expðu>\nwi vwj Þ\nPV\nl¼1 expðu>\nl vwj Þ ð3Þ\nwhere V is the size of vocabulary. The skipgram model featured with softmax is\ncomputationally expensive: computing log pðwijwjÞ is proportional to vocabulary\nsize, which can reach one billion. To boost the efﬁciency of the model, we use\nLanguageCrawl: A Generic Tool for Building... 1069\n123\nhierarchical softmax with lower computation demands bounded by OðlogðVÞÞ as\nshown in Mikolov et al. ( 2013c).\nIn our service we used the Gensim Rˇehu˚rˇek and Sojka (2010) implementation of\nWord2Vec. Gensim implementation is fast enough to process the ﬁltered corpus in\nless than a day. We set up the window size to 5 and the vector representations\ndimension to 300. In an effort to reduce memory usage, our training pipeline takes\nadvantage of iterators. The model is trained in an on-line fashion, by fetching\ndocuments one after another from the database. The resulting model is approxi-\nmately 5 GB in size, which makes it possible to train it even on machines with\nmodest amounts of available RAM.\nIn Table 10 selected words with the entries closest to them in meaning are\npresented. The output is semantically coherent.\nA few examples of linguistic computations based on vector space representation\nare shown in Table 11. Thanks to Word2Vec we have established a direct link\nbetween the mathematical representation of a word and its semantic meaning.\nTable 10 Examples of\nsemantic similarities based on\nWord2Vec trained on the Polish\ninternet corpora\nWord Most similar Distance\nKro´l( king) Cesarz ( emperor) 0.73\nTusk (former PM) Donald ( his name) 0.74\nKobieta (woman) Dziewczyna ( girl) 0.80\nMe˛_zczyzna (man) Chłopak ( boy) 0.85\nDziewczyna (girl) Rozochocona ( horny) 0.82\nApple Tablety ( tablets) 0.79\nDublin Blessington 0.85\nSushi Pizza 0.76\nTable 11 Linguistic\nregularities in vector space Expression Nearest token Distance\nSamocho´d þ rower Motocykl 0.71\n(car þ bicycle)( Motocycle)\nJezioro þ las Bagno 0.68\n(lake þ forrest)( Swamp)\nptak - zwierze˛ þ samolot Mys ´liwiec 0.65\n(bird - animal þ airplane)( Fighter plane)\nsosna - ros´lina þ zwierze˛ _Zubr 0.60\n(pine - plant þ animal)( Aurochs)\nkro´l - me˛_zczyzna þ kobieta Kro ´lowa 0.58\n(king - man þ woman)( Queen)\ndobry - zły Najlepszy 0.58\n(good - bad)( best)\n1070 S. Roziewski, M. Kozłowski\n123\n5.4 FastText\nThe FastText algorithm is a variant of Word2Vec approach. The origin Word2Vec\nmodel has the limitation of representing a distinct vector to each word, even if\nwords are morphologically similar. The main difference is based on a new advance\nto the skipgram model, where each word is represented as a bag of character n-\ngrams (Bojanowski, Grave, Joulin, & Mikolov, 2017). Thus, a word is expressed as\na sum of these n-gram vector representations.\nThe problem to solve is independently predicting the presence (or absence) of\ncontext words for the word w\nt. In Bojanowski et al. ( 2017), given the word at\nposition t, the authors consider all context words as positive samples, and sample\nnegatives randomly from the dictionary. The negative log-likelihood function based\non the binary logistic loss is described by the formula:\nlog\n/C16\n1 þ e\n/C0 sðwt;wcÞ\n/C17\nþ\nX\nn2N t;c\nlog\n/C16\n1 þ esðwt;nÞ\n/C17\n; ð4Þ\nwhere c is a context position, N t;c is a set of negative samples from the vocabulary,\nn is a size of character n-grams, and s is a scoring function mapping pairs of (word,\ncontext) to scores in I R. The objective function ( 4) can be expressed as follows:\nXT\nt¼1\n/C20 X\nc2Ct\n‘ðsðwt; wcÞÞ þ\nX\nn2N t;c\n‘ð/C0 sðwt; nÞÞ\n/C21\n; ð5Þ\nwhere ‘ is a logistic loss function ‘ : x ! logð1 þ e/C0 xÞ. The score s can be obtained\nas the scalar product between word and context vectors:\nsðwt; wcÞ¼ uT\nwt vwc : ð6Þ\nThe scoring function given by ( 6) is related to Word2Vec skipgram model with\nnegative sampling (Mikolov et al., 2013c).\nIn the FastText model the function s is redeﬁned as\nsðw; cÞ¼\nX\ng2Gw\nzT\ng vc; ð7Þ\nwhere Gw /C26f1; ... ; Gg is a set of n-grams appearing in w, G is a size of a dictionary\nof n-grams, and zg is a vector representation for a given n-gram g. The word w is\nexpressed as the sum of the vector representations of its n-grams. The scoring\nfunction ( 7) allows to share the representations across words, with the ability to\nlearn vectors for rare words.\nThe FastText model can be trained in both ways, supervised and unsupervised. In\nJoulin, Grave, Bojanowski, and Mikolov ( 2017), the authors performed sentiment\nanalysis and tag prediction. To speed up the running time, they used a hierarchical\nsoftmax to compute the probability distribution over the given classes (Goodman,\n2001) with making use of the Huffman coding tree (Mikolov, Chen, Corrado, &\nDean, 2013b). The computational time lessen from O(kh)t o Oðh log\n2ðkÞÞ, where k is\nLanguageCrawl: A Generic Tool for Building... 1071\n123\nthe number of classes and h the dimension of the text representation. In the\nsentiment analysis task they achieved comparable results to recurrent neural\nnetwork LSTM-GRNN and outperformed convolutional neural network models,\nincluding CNN-GRNN (Tang, Qin, & Liu, 2015).\nSince our data set is not labeled, we make use of an unsupervised training\nmethod. We used the FastText implementation from Facebook Open Source. We\nused the same model’s parameters as for Word2Vec. The resulting FastText model\nis comparable in size to the Word2Vec model, though computation time was lower.\nIn Table 12, we present several nearest neighbours for given sample words. The\nresults seem to be semantically coherent, though there is one thing to be addressed.\nFor two given cases (Tusk, Dublin), the FastText model returned nearest neighbours\nwhich are types of Polish grammar cases. In particular, there is no such a possibility\nfor the English language, where cases in grammar do not exist. One can apply\nstemming and lemmatization to reduce those drawbacks, especially for languages\nhaving complex grammar and morphology.\nIn Table 13, a few analogies are shown which are intact with those presented in\nTable 11. Although the analogies computed by the Word2Vec model seem to be\nmore accurate for the Polish language which is morphologically complex.\n6 Possible applications, conclusions and future work\nIn this paper we have presented LanguageCrawl—a linguistic tool which allows\nresearchers to construct web-scale corpora from the Common Crawl Archive. We\nhave also described and demonstrated its features and use cases. The tool is capable\nTable 12 Examples of\nsemantic similarities based on\nFastText trained on the Polish\ninternet corpora\nWord Most similar Distance\nkro´l( king) władca ( ruler) 0.77\nTusk (former PM) Tuska ( in a different case ) 0.74\nkobieta (woman) dziewczyna ( girl) 0.75\nme˛_zczyzna (man) facet ( guy) 0.79\napple myapple 0.69\nDublin Dublinie ( in a different case ) 0.71\nSushi Nigiri 0.72\nTable 13 Word analogies in\nvector space generated by\nFastText\nExpression Nearest token Distance\nptak-zwierze˛ þ samolot Bombowiec 0.61\n(bird-animal þ airplane)( Bomber plane)\nMoskwa - Rosja þ Polska Warszawa 0.59\n(Moscow - Russia þ Poland)( Warsaw)\nkro´l-me˛_zczyzna þ kobieta Kro ´lowa 0.79\n(king-man þ woman)( Queen)\n1072 S. Roziewski, M. Kozłowski\n123\nof scraping the Common Crawl Archive with respect to a given language, and\nbuilding both n-gram and distributional models using Word2Vec on the crawl data.\nIt should be noted that the preprocessing of data when conducting analysis is\nessential. The n-gram model can be incorporated into speech recognition or a\nmachine translation system to boost its performance, as demonstrated in Buck et al.\n(2014) and Wołk et al. ( 2017). Researchers may beneﬁt from using a well-trained\nWord2Vec model on large-scale Polish corpora. The ability to use Elastic Search for\nfast-querying entities, return similar text to a certain query, and perform statistical\nanalyses of textual corpora might prove outstandingly beneﬁcial.\nStatistics over a large Polish internet corpus provide a number of interesting\ninsights. In this study, we have shown ﬁve Polish language n-gram type\ndistributions, estimated the mean length of Polish words, and established their\nstatistical characteristics.\nThe future direction of our work will involve equipping our toolkit with syntactic\nn-grams, and training Word2Vec on both linear and syntactic n-gram collections.\nWe plan to evaluate the language models constructed in the machine translation task\nusing the EUROPARL data set.\nAcknowledgements We are immensely grateful to Dr. Irina Temnikova for her precious remarks and\ninsights. We are grateful to Wojciech Stokowiec for his work as he is the author of the LREC’16\nconference paper, which we are expanding in this study.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as\nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line\nto the material. If material is not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\npermission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nReferences\nBano´n, M., Chen, P., Haddow, B., Heaﬁeld, K., Hoang, H., Espla-Gomis, M., Forcada, M.L., Kamran, A.,\nKirefu, F., & Koehn, P., et al. (2020). Paracrawl: Web-scale acquisition of parallel corpora. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp.\n4555–4567).\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword\ninformation. Transactions of the Association for Computational Linguistics, 5, 135–146.\nBuck, C., Heaﬁeld, K., & van Ooyen, B. (2014). N-gram counts and language models from the common\ncrawl. In Proceedings of the Language Resources and Evaluation Conference. Reykjavk, Icelandik,\nIceland.\nCrouse, S., Nagel, S., Elbaz, G., & Malamud, C. (2008). Common Crawl Foundation . http://\ncommoncrawl.org\nGinter, F., & Kanerva, J. (2014). Fast training of word2vec representations using n-gram corpora. In:\nE. Volodina, L. Borin, I. Pila ´n (eds.) Linköping Electronic Conference Proceedings . Uppsala\nUniversity.\nLanguageCrawl: A Generic Tool for Building... 1073\n123\nGoodman, J. (2001). Classes for fast maximum entropy training. In 2001 IEEE International Conference\non Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221) (Vol. 1, pp. 561–\n564). IEEE.\nHewitt, C., Bishop, P., & Steiger, R. (1973). A universal modular actor formalism for artiﬁcial\nintelligence. In Proceedings of the 3rd International Joint Conference on Artiﬁcial Intelligence ,\nIJCAI’73, pp. 235–245. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. http://dl.acm.\norg/citation.cfm?id=1624775.1624804\nIslam, A., & Inkpen, D. (2009). Managing the google web 1t 5-gram data set. pp. 1—5.\nJoulin, A., Grave, E., Bojanowski, P., & Mikolov, T. (2017). Bag of tricks for efﬁcient text classiﬁcation.\nIn Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics (Vol. 2, Short Papers, pp. 427–431). Association for Computational\nLinguistics, Valencia, Spain . https://www.aclweb.org/anthology/E17-2068\nKanerva, J., Luotolahti, J., Laippala, V., & Ginter, F. (2014). Syntactic n-gram collection from a large-\nscale corpus of internet ﬁnnish. In: G.G.K.D.J.V.J. Utka A. (Ed.), Frontiers in Artiﬁcial Intelligence\nand Applications (Vol. 268): Human Language Technologies—The Baltic Perspective, pp. 184–191.\nIOS Press.\nKe˛dzia, P., Czachor, G., Piasecki, M., & Kocon ´, J. (2016). Vector representations of polish words\n(Word2Vec method) . http://hdl.handle.net/11321/327\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In Proceedings of\nInternational Conference on Machine Learning .\nLevy, O., & Goldberg, Y. (2014). Neural word embedding as implicit matrix factorization. InAdvances in\nneural information processing systems (pp. 2177–2185).\nLevy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity with lessons learned from\nword embeddings. Transactions of the Association for Computational Linguistics , 3, 211–225.\nLoper, E., & Bird, S. (2002). NLTK: The natural language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for Teaching Natural Language Processing and\nComputational Linguistics (Vol. 1, ETMTNLP ’02, pp. 63–70). Association for Computational\nLinguistics, Stroudsburg, PA, USA . http://www.nltk.org/\nLui, M., & Baldwin, T. (2012). langid.py: An off-the-shelf language identiﬁcation tool. In Proceedings of\nthe 50th Annual Meeting of the Association for Computational Linguistics (pp. 25–30). Association\nfor Computational Linguistics.\nManning, C. D., Raghavan, P., & Schu¨tze, H. (2008). Introduction to information retrieval . Cambridge:\nCambridge University Press.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013a). Efﬁcient estimation of word representations in\nvector space. In Proceedings of ICLR Workshop .\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013b). Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781\nMikolov, T., Sutskever, I., Chen, K., Corrado, G.S., & Dean, J. (2013c). Distributed representations of\nwords and phrases and their compositionality. In: C. Burges, L. Bottou, M. Welling, Z. Ghahra-\nmani, K. Weinberger (eds.) Advances in neural information processing systems (Vol. 26, pp. 3111–\n3119). Curran Associates, Inc.\nMu¨hleisen, H., & Bizer, C. (2012). Web data commons-extracting structured data from two large web\ncorpora. LDOW, 937, 133–145.\nOgrodniczuk, M. (2012). The polish sejm corpus. In Proceedings of the 8th International Conference on\nLanguage Resources and Evaluation (pp. 2219–2223) (LREC 2012).\nPopel, M., & Marecˇek, D. (2010). Perplexity of n-gram and dependency language models (pp. 173–180).\nPrzepiorkowski, A. (2012). Narodowy Korpus Jezyka Polskiego. Wydawnictwo Naukowe PWN\nPrzepio´rkowski, A., Go´rski, R.L., Lewandowska-Tomaszczyk, B., & Łaz ´inski, M. (2008). Towards the\nNational Corpus of Polish. In Proceedings of the Sixth International Conference on Language\nResources and Evaluation , LREC 2008. ELRA, Marrakech.\nRabl, T., Sadoghi, M., Jacobsen, H., S., G.V., V., M.M., & Mankovskii, S. (2012). Solving big data\nchallenges for enterprise application performance management .\nRˇehu˚rˇek, R., & Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks (pp. 45–50).\nELRA, Valletta, Malta . http://is.muni.cz/publication/884893/en\nRoziewski, S., Stokowiec, W., & Sobkowicz, A. (2016). N-Gram Collection from a Large-Scale Corpus\nof Polish Internet (pp. 23–34). Cham: Springer. https://doi.org/10.1007/978-3-319-30315-4_3\n1074 S. Roziewski, M. Kozłowski\n123\nSalton, G., & McGill, M. (1983). Introduction to modern information retrieval. New York: McGraw-Hill.\nSmith, J. R., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A. (2013). Dirt\ncheap web-scale parallel text from the common crawl. ACL, 1, 1374–1383.\nSua´rez, P.J.O., Sagot, B., & Romary, L. (2019). Asynchronous pipeline for processing huge corpora on\nmedium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of\nLarge Corpora (CMLC-7) . Leibniz-Institut fu¨r Deutsche Sprache.\nTang, D., Qin, B., & Liu, T. (2015). Document modeling with gated recurrent neural network for\nsentiment classiﬁcation. In Proceedings of the 2015 conference on empirical methods in natural\nlanguage processing (pp. 1422–1432).\nUstaszewski, M. (2016). Data sparsity in highly inﬂected languages: the case of morphosyntactic tagging\nin polish.\nWawer, A. (2015). Sentiment dictionary reﬁnement using word embeddings. In International Symposium\non Methodologies for Intelligent Systems (pp. 186–193). Springer.\nWołk, K., Wołk, A., & Marasek, K. (2017). Big data language model of contemporary polish . pp. 389–\n395.\nZeman, D., Popel, M., Straka, M., Hajicˇ, J., Nivre, J., Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S.,\nPotthast, M., Tyers, F., Badmaeva, E., Gokirmak, M., Nedoluzhko, A., Cinkova ´, S., Hajicˇ jr., J.,\nHlava´cˇova´, J., Kettnerova´, V., Uresˇova´, Z., Kanerva, J., Ojala, S., Missila ¨, A., Manning, C.D.,\nSchuster, S., Reddy, S., Taji, D., Habash, N., Leung, H., de Marneffe, M.C., Sanguinetti, M., Simi,\nM., Kanayama, H., de Paiva, V., Droganova, K., Martı´nez Alonso, H., C¸o¨ltekin, C¸ ., Sulubacak, U.,\nUszkoreit, H., Macketanz, V., Burchardt, A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen, T.,\nAttia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit, S., Mandl, M., Kirchner, J., Alcalde, H.F.,\nStrnadova´, J., Banerjee, E., Manurung, R., Stella, A., Shimada, A., Kwak, S., Mendonc¸a, G., Lando,\nT., Nitisaroj, R., & Li, J. (2017). CoNLL 2017 shared task: Multilingual parsing from raw text to\nuniversal dependencies. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from\nRaw Text to Universal Dependencies (pp. 1–19). Association for Computational Linguistics,\nVancouver, Canada.\nZiolko, B., & Skurzok, D. (2011). N-Grams Model for Polish . https://doi.org/10.5772/16568.\nZio´łłko, B., Skurzok, D., & Michalska, M. (2010). Polish n-grams and their correction process. In 4th\nInternational Conference on Multimedia and Ubiquitous Engineering . IEEE, Cebu, Philippines.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published\nmaps and institutional afﬁliations.\nLanguageCrawl: A Generic Tool for Building... 1075\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8272531032562256
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5772752165794373
    },
    {
      "name": "Language model",
      "score": 0.5755264759063721
    },
    {
      "name": "The Internet",
      "score": 0.5498091578483582
    },
    {
      "name": "Human multitasking",
      "score": 0.5228386521339417
    },
    {
      "name": "Softmax function",
      "score": 0.5160045623779297
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5070648193359375
    },
    {
      "name": "Natural language processing",
      "score": 0.4636133313179016
    },
    {
      "name": "World Wide Web",
      "score": 0.4354051351547241
    },
    {
      "name": "Artificial neural network",
      "score": 0.34690678119659424
    },
    {
      "name": "Programming language",
      "score": 0.126947283744812
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210139285",
      "name": "National Information Processing Institute",
      "country": "PL"
    }
  ],
  "cited_by": 10
}