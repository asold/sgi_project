{
  "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
  "url": "https://openalex.org/W4285294723",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2947436776",
      "name": "Zhengxiao Du",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2103818385",
      "name": "Yujie Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097346119",
      "name": "Xiao Liu",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1993547713",
      "name": "Ming Ding",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2416947802",
      "name": "Jiezhong Qiu",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2107505891",
      "name": "Zhilin Yang",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098032575",
      "name": "Jie Tang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W3024131638",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2963600562",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3094045953",
    "https://openalex.org/W3103753836",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3106339673",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4394651747",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3106340866",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 320 - 335\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nGLM: General Language Model Pretraining\nwith Autoregressive Blank Inﬁlling\nZhengxiao Du∗1,2 Yujie Qian∗3 Xiao Liu1,2 Ming Ding1,2 Jiezhong Qiu1,2\nZhilin Yang†1,4 Jie Tang†1,2\n1Tsinghua University 2Beijing Academy of Artiﬁcial Intelligence (BAAI)\n3MIT CSAIL 4Shanghai Qi Zhi Institute\nzx-du20@mails.tsinghua.edu.cn yujieq@csail.mit.edu\n{zhiliny,jietang}@tsinghua.edu.cn\nAbstract\nThere have been various types of pretrain-\ning architectures including autoencoding mod-\nels (e.g., BERT), autoregressive models (e.g.,\nGPT), and encoder-decoder models (e.g., T5).\nHowever, none of the pretraining frameworks\nperforms the best for all tasks of three main cat-\negories including natural language understand-\ning (NLU), unconditional generation, and con-\nditional generation. We propose a General\nLanguage Model (GLM) based on autoregres-\nsive blank inﬁlling to address this challenge.\nGLM improves blank ﬁlling pretraining by\nadding 2D positional encodings and allowing\nan arbitrary order to predict spans, which re-\nsults in performance gains over BERT and T5\non NLU tasks. Meanwhile, GLM can be pre-\ntrained for different types of tasks by varying\nthe number and lengths of blanks. On a wide\nrange of tasks across NLU, conditional and\nunconditional generation, GLM outperforms\nBERT, T5, and GPT given the same model\nsizes and data, and achieves the best perfor-\nmance from a single pretrained model with\n1.25×parameters of BERT Large, demonstrat-\ning its generalizability to different downstream\ntasks.1\n1 Introduction\nLanguage models pretrained on unlabeled texts\nhave substantially advanced the state of the art in\nvarious NLP tasks, ranging from natural language\nunderstanding (NLU) to text generation (Radford\net al., 2018a; Devlin et al., 2019; Yang et al., 2019;\nRadford et al., 2018b; Raffel et al., 2020; Lewis\net al., 2019; Brown et al., 2020). Downstream task\nperformance as well as the scale of the parame-\nters have also constantly increased in the past few\nyears.\n*The ﬁrst two authors contributed equally.\n†Corresponding authors.\n1The code and pre-trained models are available at https:\n//github.com/THUDM/GLM\nAll [START] NLP tasks are generation tasks\nAll NLP tasks [END] are generation tasks\n× L\nFigure 1: Illustration of GLM. We blank out text spans\n(green part) and generate them autoregressively. (Some\nattention edges are omitted; cf. Figure 2.)\nIn general, existing pretraining frameworks can\nbe categorized into three families: autoregressive,\nautoencoding, and encoder-decoder models. Au-\ntoregressive models, such as GPT (Radford et al.,\n2018a), learn left-to-right language models. While\nthey succeed in long-text generation and show few-\nshot learning ability when scaled to billions of\nparameters (Radford et al., 2018b; Brown et al.,\n2020), the inherent disadvantage is the unidirec-\ntional attention mechanism, which cannot fully cap-\nture the dependencies between the context words\nin NLU tasks. Autoencoding models, such as\nBERT (Devlin et al., 2019), learn bidirectional con-\ntext encoders via denoising objectives, e.g. Masked\nLanguage Model (MLM). The encoders produce\ncontextualized representations that suit natural lan-\nguage understanding tasks, but could not be directly\napplied for text generation. Encoder-decoder mod-\nels adopt bidirectional attention for the encoder,\nunidirectional attention for the decoder, and cross\nattention between them (Song et al., 2019; Bi et al.,\n2020; Lewis et al., 2019). They are typically de-\nployed in conditional generation tasks, such as\ntext summarization and response generation. 2.\nT5 (Raffel et al., 2020) uniﬁes NLU and condi-\ntional generation via encoder-decoder models but\nrequires more parameters to match the performance\n2Unconditional generation refers to generating text as a lan-\nguage model without ﬁnetuning, while conditional generation\nrefers to sequence-to-sequence tasks.\n320\nof BRET-based models such as RoBERTa (Liu\net al., 2019) and DeBERTa (He et al., 2021).\nNone of these pretraining frameworks is ﬂexible\nenough to perform competitively across all NLP\ntasks. Previous works have tried to unify differ-\nent frameworks by combining their objectives via\nmulti-task learning (Dong et al., 2019; Bao et al.,\n2020). However, since the autoencoding and au-\ntoregressive objectives differ by nature, a simple\nuniﬁcation cannot fully inherit the advantages of\nboth frameworks.\nIn this paper, we propose a pretraining frame-\nwork named GLM (General Language Model),\nbased on autoregressive blank inﬁlling. We ran-\ndomly blank out continuous spans of tokens from\nthe input text, following the idea of autoencoding,\nand train the model to sequentially reconstruct the\nspans, following the idea of autoregressive pretrain-\ning (see Figure 1). While blanking ﬁlling has been\nused in T5 (Raffel et al., 2020) for text-to-text pre-\ntraining, we propose two improvements, namely\nspan shufﬂing and 2D positional encoding. Empiri-\ncally, we show that with the same amount of param-\neters and computational cost, GLM signiﬁcantly\noutperforms BERT on the SuperGLUE benchmark\nby a large margin of 4.6% – 5.0% and outperforms\nRoBERTa and BART when pretrained on a corpus\nof similar size (158GB). GLM also signiﬁcantly\noutperforms T5 on NLU and generation tasks with\nfewer parameters and data.\nInspired by Pattern-Exploiting Training (PET)\n(Schick and Schütze, 2020a), we reformulate NLU\ntasks as manually-crafted cloze questions that\nmimic human language. Different from the BERT-\nbased models used by PET, GLM can naturally\nhandle multi-token answers to the cloze question\nvia autoregressive blank ﬁlling.\nFurthermore, we show that by varying the num-\nber and lengths of missing spans, the autoregressive\nblank ﬁlling objective can pretrain language mod-\nels for conditional and unconditional generation.\nThrough multi-task learning of different pretraining\nobjectives, a single GLM can excel in both NLU\nand (conditional and unconditional) text genera-\ntion. Empirically, compared with standalone base-\nlines, GLM with multi-task pretraining achieves\nimprovements in NLU, conditional text generation,\nand language modeling tasks altogether by sharing\nthe parameters.\n2 GLM Pretraining Framework\nWe propose a general pretraining framework GLM\nbased on a novel autoregressive blank inﬁlling ob-\njective. GLM formulates NLU tasks as cloze ques-\ntions that contain task descriptions, which can be\nanswered by autoregressive generation.\n2.1 Pretraining Objective\n2.1.1 Autoregressive Blank Inﬁlling\nGLM is trained by optimizing an autoregressive\nblank inﬁlling objective. Given an input text x =\n[x1,··· ,xn], multiple text spans{s1,··· ,sm}are\nsampled, where each span si corresponds to a\nseries of consecutive tokens [si,1,··· ,si,li] in x.\nEach span is replaced with a single [MASK] to-\nken, forming a corrupted text xcorrupt. The model\npredicts the missing tokens in the spans from the\ncorrupted text in an autoregressive manner, which\nmeans when predicting the missing tokens in a\nspan, the model has access to the corrupted text\nand the previously predicted spans. To fully cap-\nture the interdependencies between different spans,\nwe randomly permute the order of the spans, simi-\nlar to the permutation language model (Yang et al.,\n2019). Formally, let Zm be the set of all possi-\nble permutations of the length-mindex sequence\n[1,2,··· ,m], and sz<i be [sz1 ,··· ,szi−1 ], we de-\nﬁne the pretraining objective as\nmax\nθ\nEz∼Zm\n[ m∑\ni=1\nlog pθ(szi|xcorrupt,sz<i)\n]\n(1)\nWe always generate the tokens in each blank fol-\nlowing a left-to-right order, i.e. the probability of\ngenerating the span si is factorized as:\npθ(si|xcorrupt,sz<i)\n=\nli∏\nj=1\np(si,j|xcorrupt,sz<i,si,<j) (2)\nWe implement the autoregressive blank inﬁlling\nobjective with the following techniques. The input\nx is divided into two parts: Part A is the corrupted\ntext xcorrupt, and Part B consists of the masked\nspans. Part A tokens can attend to each other, but\ncannot attend to any tokens in B. Part B tokens can\nattend to Part A and antecedents in B, but cannot\nattend to any subsequent tokens in B. To enable au-\ntoregressive generation, each span is padded with\nspecial tokens [START] and [END], for input and\n321\n× × × × ×\n× × × × ×\n× × × × ×\n× × × × ×\n× × × × ×\n× × × ×\n× × ×\n× ×\n×\n(a)  Sample spans from the input text\nPart A:\nPart B:\n(b)  Divide the input into Part A / Part B\nGLM\n(Transformer w/ masked self-attention)\n(c)  Generate the Part B spans autoregressively\nQuery\nKey\n(d)  Self-attention mask\nToken\nx 1 x 2 x 3 x 4 x 5 x 6\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 5 x 6 [E] x 3 [E]\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 1\nx 2\n[M]\nx 4\n[M]\n[S]\nx 5\nx 6\n[S]\nx 3\nTarget \u0000 \u0000 \u0000 \u0000 \u0000 x 5 x 6 [E] x 3 [E]\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx1 x2 x3 x4 x5 x6\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx5 x6 [E] x3 [E]\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx1\nx2\n[M]\nx4\n[M]\n[S]\nx5\nx6\n[S]\nx3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx1 x2 x3 x4 x5 x6\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx5 x6 [E] x3 [E]\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx1\nx2\n[M]\nx4\n[M]\n[S]\nx5\nx6\n[S]\nx3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx1 x2 x3 x4 x5 x6\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx5 x6 [E] x3 [E]\nx1 x2 [M] x4 [M] [S] x5 x6 [S] x3\nx1\nx2\n[M]\nx4\n[M]\n[S]\nx5\nx6\n[S]\nx3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx 1 x 2 x 3 x 4 x 5 x 6\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 5 x 6 [E] x 3 [E]\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 1\nx 2\n[M]\nx 4\n[M]\n[S]\nx 5\nx 6\n[S]\nx 3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx 1 x 2 x 3 x 4 x 5 x 6\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 5 x 6 [E] x 3 [E]\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 1\nx 2\n[M]\nx 4\n[M]\n[S]\nx 5\nx 6\n[S]\nx 3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx 1 x 2 x 3 x 4 x 5 x 6\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 5 x 6 [E] x 3 [E]\nx 1 x 2 [M] x 4 [M] [S] x 5 x 6 [S] x 3\nx 1\nx 2\n[M]\nx 4\n[M]\n[S]\nx 5\nx 6\n[S]\nx 3\nPosition 1 1 2 3 4 5 5 5 5 3 3\nPosition 2 0 0 0 0 0 1 2 3 1 2\nx1 x2 x3 x4 x5 x6\nx1 x2 [M]x4 [M][S] x5 x6 [S] x3\nx5 x6 [E]x3 [E]\nx1 x2 [M]x4 [M][S] x5 x6 [S] x3\nx1\nx2\n[M]\nx4\n[M]\n[S]\nx5\nx6\n[S]\nx3\nPosition 11 2 3 4 5 5 5 5 3 3\nPosition 20 0 0 0 0 1 2 3 1 2\nx1 x2 x3 x4 x5 x6\nx1 x2 [M]x4 [M][S]x5 x6 [S]x3\nx5 x6 [E]x3 [E]\nx1 x2 [M]x4 [M][S]x5 x6 [S]x3\nx1\nx2\n[M]\nx4\n[M]\n[S]\nx5\nx6\n[S]\nx3\nPosition 11 2 3 4 5 5 5 5 3 3\nPosition 20 0 0 0 0 1 2 3 1 2\nPosition 1\nPosition 2\nFigure 2: GLM pretraining. (a) The original text is[x1,x2,x3,x4,x5,x6]. Two spans[x3] and [x5,x6] are sampled.\n(b) Replace the sampled spans with [M] in Part A, and shufﬂe the spans in Part B. (c) GLM autoregressively\ngenerates Part B. Each span is prepended with [S] as input and appended with [E] as output. 2D positional\nencoding represents inter- and intra-span positions. (d) Self-attention mask. Grey areas are masked out. Part A\ntokens can attend to themselves (blue frame) but not B. Part B tokens can attend to A and their antecedents in B\n(yellow and green frames correspond to the two spans). [M] := [MASK], [S] := [START], and [E] := [END].\noutput respectively. In this way, our model auto-\nmatically learns a bidirectional encoder (for Part\nA) and a unidirectional decoder (for Part B) in a\nuniﬁed model. The implementation of GLM is\nillustrated in Figure 2.\nWe randomly sample spans of length drawn from\na Poisson distribution with λ= 3. We repeatedly\nsample new spans until at least 15% of the original\ntokens are masked. Empirically, we have found\nthat the 15% ratio is critical for good performance\non downstream NLU tasks.\n2.1.2 Multi-Task Pretraining\nIn the previous section, GLM masks short spans\nand is suited for NLU tasks. However, we are\ninterested in pretraining a single model that can\nhandle both NLU and text generation. We then\nstudy a multi-task pretraining setup, in which a\nsecond objective of generating longer text is jointly\noptimized with the blank inﬁlling objective. We\nconsider the following two objectives:\n• Document-level. We sample a single span\nwhose length is sampled from a uniform distri-\nbution over 50%–100% of the original length.\nThe objective aims for long text generation.\n• Sentence-level. We restrict that the masked\nspans must be full sentences. Multiple spans\n(sentences) are sampled to cover 15% of\nthe original tokens. This objective aims for\nseq2seq tasks whose predictions are often\ncomplete sentences or paragraphs.\nBoth new objectives are deﬁned in the same way\nas the original objective, i.e. Eq. 1. The only differ-\nence is the number of spans and the span lengths.\n2.2 Model Architecture\nGLM uses a single Transformer with several mod-\niﬁcations to the architecture: (1) we rearrange\nthe order of layer normalization and the resid-\nual connection, which has been shown critical for\nlarge-scale language models to avoid numerical\nerrors (Shoeybi et al., 2019); (2) we use a sin-\ngle linear layer for the output token prediction;\n(3) we replace ReLU activation functions with\nGeLUs (Hendrycks and Gimpel, 2016).\n2.2.1 2D Positional Encoding\nOne of the challenges of the autoregressive blank\ninﬁlling task is how to encode the positional infor-\nmation. Transformers rely on positional encodings\nto inject the absolute and relative positions of the\ntokens. We propose 2D positional encodings to\naddress the challenge. Speciﬁcally, each token is\nencoded with two positional ids. The ﬁrst posi-\ntional id represents the position in the corrupted\ntext xcorrupt. For the masked spans, it is the position\nof the corresponding [MASK] token. The second\npositional id represents the intra-span position. For\ntokens in Part A, their second positional ids are\n0. For tokens in Part B, they range from 1 to the\nlength of the span. The two positional ids are pro-\njected into two vectors via learnable embedding\ntables, which are both added to the input token\nembeddings.\nOur encoding method ensures that the model is\nnot aware of the length of the masked span when\n322\nCoronet has the best lines of all day cruisers.\nPositive\n<latexit sha1_base64=\"cb5S93r+RGEy3gKCUaUf2i3SjJQ=\">AAAB6HicbVDJSgNBEK2JW4xb1KMijUHwFGY8qMegF48JmAWSIfR0apI2PQvdPcIw5OjJiwdFvPoV+Q5vfoM/YWc5aPRBweO9KqrqebHgStv2p5VbWl5ZXcuvFzY2t7Z3irt7DRUlkmGdRSKSLY8qFDzEuuZaYCuWSANPYNMbXk/85j1KxaPwVqcxugHth9znjGoj1dJusWSX7SnIX+LMSalyOK59PRyNq93iR6cXsSTAUDNBlWo7dqzdjErNmcBRoZMojCkb0j62DQ1pgMrNpoeOyIlResSPpKlQk6n6cyKjgVJp4JnOgOqBWvQm4n9eO9H+pZvxME40hmy2yE8E0RGZfE16XCLTIjWEMsnNrYQNqKRMm2wKJgRn8eW/pHFWds7Lds2kcQUz5OEAjuEUHLiACtxAFerAAOERnuHFurOerFfrbdaas+Yz+/AL1vs31mKQqQ==</latexit>\ny\ngood\n<latexit sha1_base64=\"CZ8abL6hJorh4jxHBPAGp8NppAA=\">AAAB63icbVBNSwMxEJ2tX7V+VT16CS1CRSi7HtRj0YvHCvYD2qVk07QNTbJLki0sS/+CFwVFvPqHvPXfmG170NYHA4/3ZpiZF0ScaeO6Mye3sbm1vZPfLeztHxweFY9PmjqMFaENEvJQtQOsKWeSNgwznLYjRbEIOG0F4/vMb02o0iyUTyaJqC/wULIBI9hk0qSSXPSKZbfqzoHWibck5Vqpe/k6qyX1XvG72w9JLKg0hGOtO54bGT/FyjDC6bTQjTWNMBnjIe1YKrGg2k/nt07RuVX6aBAqW9Kgufp7IsVC60QEtlNgM9KrXib+53ViM7j1Uyaj2FBJFosGMUcmRNnjqM8UJYYnlmCimL0VkRFWmBgbT8GG4K2+vE6aV1Xvuuo+2jTuYIE8nEEJKuDBDdTgAerQAAIjeIY3eHeE8+J8OJ+L1pyznDmFP3C+fgCio5Dy</latexit>\nv ( y )\nGLM\n<latexit sha1_base64=\"cIlXHKTMHL8y94GI+KZXnlT1K7g=\">AAAB7XicbVDLSgNBEOyNrxhfUY9ehgQhIoRdD+ox6MVjBPOAZAmzk9lkzOzMMjMrLjH/4EEPinj1f7zlb5w8DppY0FBUddPdFcScaeO6Yyezsrq2vpHdzG1t7+zu5fcP6lomitAakVyqZoA15UzQmmGG02asKI4CThvB4HriNx6o0kyKO5PG1I9wT7CQEWysVI9L6dPjSSdfdMvuFGiZeHNSrBTapy/jSlrt5L/bXUmSiApDONa65bmx8YdYGUY4HeXaiaYxJgPcoy1LBY6o9ofTa0fo2CpdFEplSxg0VX9PDHGkdRoFtjPCpq8XvYn4n9dKTHjpD5mIE0MFmS0KE46MRJPXUZcpSgxPLcFEMXsrIn2sMDE2oJwNwVt8eZnUz8reedm9tWlcwQxZOIIClMCDC6jADVShBgTu4Rne4N2Rzqvz4XzOWjPOfOYQ/sD5+gFYz5H0</latexit>\np ( y | x )\nIt is really [MASK]\n<latexit sha1_base64=\"XknPsXXFT3s+dKVLzg736M6sfhc=\">AAAB9XicbVC7TsMwFL3hWcKrwMgSUSExVQkDsCAqWBiLRB9SGyrHcVqrjh3ZDlBF/Q8WBh5i5TPYWRB/g9N2gJYjWT465175+AQJo0q77rc1N7+wuLRcWLFX19Y3Notb23UlUolJDQsmZDNAijDKSU1TzUgzkQTFASONoH+R+41bIhUV/FoPEuLHqMtpRDHSRrppB4KFahCbK7sfdoolt+yO4MwSb0JKZx/2afLyZVc7xc92KHAaE64xQ0q1PDfRfoakppiRod1OFUkQ7qMuaRnKUUyUn41SD519o4ROJKQ5XDsj9fdGhmKVRzOTMdI9Ne3l4n9eK9XRiZ9RnqSacDx+KEqZo4WTV+CEVBKs2cAQhCU1WR3cQxJhbYqyTQne9JdnSf2w7B2V3Su3VDmHMQqwC3twAB4cQwUuoQo1wCDhAZ7g2bqzHq1X6208OmdNdnbgD6z3H7R5lks=</latexit>\nx\nFigure 3: Formulation of the sentiment classiﬁcation\ntask as blank inﬁlling with GLM.\nreconstructing them. It is an important difference\nas compared to other models. For example, XL-\nNet (Yang et al., 2019) encodes the original posi-\ntion so that it can perceive the number of missing\ntokens, and SpanBERT (Joshi et al., 2020) replaces\nthe span with multiple [MASK] tokens and keeps\nthe length unchanged. Our design ﬁts downstream\ntasks as usually the length of the generated text is\nunknown beforehand.\n2.3 Finetuning GLM\nTypically, for downstream NLU tasks, a linear clas-\nsiﬁer takes the representations of sequences or to-\nkens produced by pretrained models as input and\npredicts the correct labels. The practices are differ-\nent from the generative pretraining task, leading to\ninconsistency between pretraining and ﬁnetuning.\nInstead, we reformulate NLU classiﬁcation tasks\nas generation tasks of blank inﬁlling, following\nPET (Schick and Schütze, 2020a). Speciﬁcally,\ngiven a labeled example (x,y), we convert the in-\nput text x to a cloze question c(x) via a pattern\ncontaining a single mask token. The pattern is writ-\nten in natural language to represent the semantics\nof the task. For example, a sentiment classiﬁcation\ntask can be formulated as “{SENTENCE}. It’s\nreally [MASK]”. The candidate labels y ∈Y are\nalso mapped to answers to the cloze, called ver-\nbalizer v(y). In sentiment classiﬁcation, the labels\n“positive” and “negative” are mapped to the words\n“good” and “bad”. The conditional probability of\npredicting ygiven x is\np(y|x) = p(v(y)|c(x))∑\ny′∈Yp(v(y′)|c(x)) (3)\nwhere Yis the label set. Therefore the probability\nof the sentence being positive or negative is propor-\ntional to predicting “good” or “bad” in the blank.\nThen we ﬁnetune GLM with a cross-entropy loss\n(see Figure 3).\nFor text generation tasks, the given context con-\nstitutes the Part A of the input, with a mask token\nappended at the end. The model generates the text\nof Part B autoregressively. We can directly apply\nthe pretrained GLM for unconditional generation,\nor ﬁnetune it on downstream conditional generation\ntasks.\n2.4 Discussion and Analysis\nIn this section, we discuss the differences between\nGLM and other pretraining models. We are mainly\nconcerned with how they can be adapted to down-\nstream blank inﬁlling tasks.\nComparison with BERT (Devlin et al., 2019).\nAs pointed out by (Yang et al., 2019), BERT fails\nto capture the interdependencies of masked tokens\ndue to the independence assumption of MLM. An-\nother disadvantage of BERT is that it cannot ﬁll in\nthe blanks of multiple tokens properly. To infer the\nprobability of an answer of length l, BERT needs\nto perform lconsecutive predictions. If the length l\nis unknown, we may need to enumerate all possible\nlengths, since BERT needs to change the number\nof [MASK] tokens according to the length.\nComparison with XLNet (Yang et al., 2019).\nBoth GLM and XLNet are pretrained with autore-\ngressive objectives, but there are two differences\nbetween them. First, XLNet uses the original posi-\ntion encodings before corruption. During inference,\nwe need to either know or enumerate the length of\nthe answer, the same problem as BERT. Second,\nXLNet uses a two-stream self-attention mechanism,\ninstead of the right-shift, to avoid the information\nleak within Transformer. It doubles the time cost\nof pretraining.\nComparison with T5 (Raffel et al., 2020). T5\nproposes a similar blank inﬁlling objective to pre-\ntrain an encoder-decoder Transformer. T5 uses\nindependent positional encodings for the encoder\nand decoder, and relies on multiple sentinel tokens\nto differentiate the masked spans. In downstream\ntasks, only one of the sentinel tokens is used, lead-\ning to a waste of model capacity and inconsistency\nbetween pretraining and ﬁnetuning. Moreover, T5\nalways predicts spans in a ﬁxed left-to-right order.\nAs a result, GLM can signiﬁcantly outperform T5\non NLU and seq2seq tasks with fewer parameters\nand data, as stated in Sections 3.2 and 3.3.\nComparison with UniLM (Dong et al., 2019).\nUniLM combines different pretraining objectives\nunder the autoencoding framework by changing the\n323\nattention mask among bidirectional, unidirectional,\nand cross attention. However, UniLM always re-\nplaces masked spans with [MASK] tokens, which\nlimits its ability to model the dependencies between\nthe masked spans and their context. GLM feeds in\nthe previous token and autoregressively generates\nthe next token. Finetuning UniLM on downstream\ngeneration tasks also relies on masked language\nmodeling, which is less efﬁcient. UniLMv2 (Bao\net al., 2020) adopts partially autoregressive model-\ning for generation tasks, along with the autoencod-\ning objective for NLU tasks. Instead, GLM uniﬁes\nNLU and generation tasks with autoregressive pre-\ntraining.\n3 Experiments\nWe now describe our pretraining setup and the eval-\nuation of downstream tasks.\n3.1 Pretraining Setup\nFor a fair comparison with BERT (Devlin et al.,\n2019), we use BooksCorpus (Zhu et al., 2015) and\nEnglish Wikipedia as our pretraining data. We use\nthe uncased wordpiece tokenizer of BERT with 30k\nvocabulary. We train GLMBase and GLMLarge with\nthe same architectures as BERTBase and BERTLarge,\ncontaining 110M and 340M parameters respec-\ntively.\nFor multi-task pretraining, we train two Large-\nsized models with a mixture of the blank inﬁll-\ning objective and the document-level or sentence-\nlevel objective, denoted as GLMDoc and GLMSent.\nAdditionally, we train two larger GLM models of\n410M (30 layers, hidden size 1024, and 16 atten-\ntion heads) and 515M (30 layers, hidden size 1152,\nand 18 attention heads) parameters with document-\nlevel multi-task pretraining, denoted as GLM410M\nand GLM515M.\nTo compare with SOTA models, we also train\na Large-sized model with the same data, tokeniza-\ntion, and hyperparameters as RoBERTa (Liu et al.,\n2019), denoted as GLMRoBERTa. Due to resource\nlimitations, we only pretrain the model for 250,000\nsteps, which are half of RoBERTa and BART’s\ntraining steps and close to T5 in the number of\ntrained tokens. More experiment details can be\nfound in Appendix A.\n3.2 SuperGLUE\nTo evaluate our pretrained GLM models, we\nconduct experiments on the SuperGLUE bench-\nmark (Wang et al., 2019) and report the standard\nmetrics. SuperGLUE consists of 8 challenging\nNLU tasks. We reformulate the classiﬁcation tasks\nas blank inﬁlling with human-crafted cloze ques-\ntions, following PET (Schick and Schütze, 2020b).\nThen we ﬁnetune the pretrained GLM models on\neach task as described in Section 2.3. The cloze\nquestions and other details can be found in Ap-\npendix B.1.\nFor a fair comparison with GLM Base and\nGLMLarge, we choose BERT Base and BERTLarge\nas our baselines, which are pretrained on the same\ncorpus and for a similar amount of time. We report\nthe performance of standard ﬁnetuning (i.e. classiﬁ-\ncation on the [CLS] token representation). The per-\nformance of BERT with cloze questions is reported\nin Section 3.4. To compare with GLMRoBERTa, we\nchoose T5, BARTLarge, and RoBERTaLarge as our\nbaselines. T5 has no direct match in the number\nof parameters for BERTLarge, so we present the re-\nsults of both T5Base (220M parameters) and T5Large\n(770M parameters). All the other baselines are of\nsimilar size to BERTLarge.\nTable 1 shows the results. With the same amount\nof training data, GLM consistently outperforms\nBERT on most tasks with either base or large archi-\ntecture. The only exception is WiC (word sense dis-\nambiguation). On average, GLMBase scores 4.6%\nhigher than BERTBase, and GLMLarge scores 5.0%\nhigher than BERT Large. It clearly demonstrates\nthe advantage of our method in NLU tasks. In\nthe setting of RoBERTaLarge, GLMRoBERTa can still\nachieve improvements over the baselines, but with\na smaller margin. Speciﬁcally, GLMRoBERTa outper-\nforms T5Large but is only half its size. We also ﬁnd\nthat BART does not perform well on the challeng-\ning SuperGLUE benchmark. We conjecture this\ncan be attributed to the low parameter efﬁciency of\nthe encoder-decoder architecture and the denoising\nsequence-to-sequence objective.\n3.3 Multi-Task Pretraining\nThen we evaluate the GLM’s performance in a\nmulti-task setting (Section 2.1). Within one train-\ning batch, we sample short spans and longer\nspans (document-level or sentence-level) with\nequal chances. We evaluate the multi-task model\nfor NLU, seq2seq, blank inﬁlling, and zero-shot\nlanguage modeling.\nSuperGLUE. For NLU tasks, we evaluate mod-\nels on the SuperGLUE benchmark. The results\n324\nTable 1: Results on the SuperGLUE dev set.\nModel ReCoRD\nF1/Acc.\nCOPA\nAcc.\nWSC\nAcc.\nRTE\nAcc.\nBoolQ\nAcc.\nWiC\nAcc.\nCB\nF1/Acc.\nMultiRC\nF1a/EM Avg\nPretrained on BookCorpus and Wikipedia\nBERTBase 65.4 / 64.9 66.0 65.4 70.0 74.9 68.8 70.9 / 76.8 68.4 / 21.5 66.1\nGLMBase 73.5 / 72.8 71.0 72.1 71.2 77.0 64.7 89.5 / 85.7 72.1 / 26.1 70.7\nBERTLarge 76.3 / 75.6 69.0 64.4 73.6 80.1 71.0 94.8 / 92.9 71.9 / 24.1 72.0\nUniLMLarge 80.0 / 79.1 72.0 65.4 76.5 80.5 69.7 91.0 / 91.1 77.2 / 38.2 74.1\nGLMLarge 81.7 / 81.1 76.0 81.7 74.0 82.1 68.5 96.1 / 94.6 77.1 / 36.3 77.0\nGLMDoc 80.2 / 79.6 77.0 78.8 76.2 79.8 63.6 97.3 / 96.4 74.6 / 32.1 75.7\nGLMSent 80.7 / 80.2 77.0 79.8 79.1 80.8 70.4 94.6 / 93.7 76.9 / 36.1 76.8\nGLM410M 81.5 / 80.9 80.0 81.7 79.4 81.9 69.0 93.2 / 96.4 76.2 / 35.5 78.0\nGLM515M 82.3 / 81.7 85.0 81.7 79.1 81.3 69.4 95.0 / 96.4 77.2 / 35.0 78.8\nPretrained on larger corpora\nT5Base 76.2 / 75.4 73.0 79.8 78.3 80.8 67.9 94.8 / 92.9 76.4 / 40.0 76.0\nT5Large 85.7 / 85.0 78.0 84.6 84.8 84.3 71.6 96.4 / 98.2 80.9 / 46.6 81.2\nBARTLarge 88.3 / 87.8 60.0 65.4 84.5 84.3 69.0 90.5 / 92.9 81.8 / 48.0 76.0\nRoBERTaLarge 89.0 / 88.4 90.0 63.5 87.0 86.1 72.6 96.1 / 94.6 84.4 / 52.9 81.5\nGLMRoBERTa 89.6 / 89.0 82.0 83.7 87.7 84.7 71.2 98.7 / 98.2 82.4 / 50.1 82.9\nTable 2: Results of abstractive summarization on the CNN/DailyMail and XSum test sets.\nModel CNN/DailyMail XSum\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L\nBERTSumAbs (Liu and Lapata, 2019) 41.7 19.4 38.8 38.8 16.3 31.2\nUniLMv2 Base (Bao et al., 2020) 43.2 20.4 40.1 44.0 21.1 36.1\nT5 Large (Raffel et al., 2020) 42.5 20.7 39.8 40.9 17.3 33.0\nBART Large (Lewis et al., 2019) 44.2 21.3 40.9 45.1 22.3 37.3\nGLM RoBERTa 43.8 21.0 40.5 45.5 23.5 37.3\nare also shown in Table 1. We observe that with\nmulti-task pretraining, GLMDoc and GLMSent per-\nform slightly worse than GLMLarge, but still outper-\nform BERTLarge and UniLMLarge. Among multi-\ntask models, GLM Sent outperforms GLM Doc by\n1.1% on average. Increasing GLM Doc’s param-\neters to 410M (1.25 ×BERTLarge) leads to better\nperformance than GLMLarge. GLM with 515M pa-\nrameters (1.5×BERTLarge) can perform even better.\nSequence-to-Sequence. Considering the\navailable baseline results, we use the Gigaword\ndataset (Rush et al., 2015) for abstractive summa-\nrization and the SQuAD 1.1 dataset (Rajpurkar\net al., 2016) for question generation (Du et al.,\n2017) as the benchmarks for models pretrained\non BookCorpus and Wikipedia. Additionally, we\nuse the CNN/DailyMail (See et al., 2017) and\nXSum (Narayan et al., 2018) datasets for abstrac-\ntive summarization as the benchmarks for models\npretrained on larger corpora.\nThe results for models trained on BookCorpus\nand Wikipedia are shown in Tables 3 and 4. We\nobserve that GLM Large can achieve performance\nmatching the other pretraining models on the two\ngeneration tasks. GLMSent can perform better than\nGLMLarge, while GLMDoc performs slightly worse\nthan GLMLarge. This indicates that the document-\nlevel objective, which teaches the model to extend\nthe given contexts, is less helpful to conditional\ngeneration, which aims to extract useful informa-\ntion from the context. Increasing GLM Doc’s pa-\nrameters to 410M leads to the best performance on\nboth tasks. The results for models trained on larger\ncorpora are shown in Table 2. GLM RoBERTa can\nachieve performance matching the seq2seq BART\nmodel, and outperform T5 and UniLMv2.\nText Inﬁlling. Text inﬁlling is the task of pre-\ndicting missing spans of text which are consistent\n325\nTable 3: Results on Gigaword summarization.\nModel RG-1 RG-2 RG-L\nMASS 37.7 18.5 34.9\nUniLM Large 38.5 19.5 35.8\nGLM Large 38.6 19.7 36.0\nGLM Doc 38.5 19.4 35.8\nGLM Sent 38.9 20.0 36.3\nGLM 410M 38.9 20.0 36.2\nTable 4: Results on SQuAD question generation.\nModel BLEU-4 MTR RG-L\nSemQG 18.4 22.7 46.7\nUniLM Large 22.1 25.1 51.1\nGLM Large 22.4 25.2 50.4\nGLM Doc 22.3 25.0 50.2\nGLM Sent 22.6 25.4 50.4\nGLM 410M 22.9 25.6 50.5\nTable 5: BLEU scores on Yahoo text inﬁlling. †indi-\ncates the results from (Shen et al., 2020).\nMask ratio 10% 20% 30% 40% 50%\nBERT † 82.8 66.3 50.3 37.4 26.2\nBLM † 86.5 73.2 59.6 46.8 34.8\nGLM Large 87.8 76.7 64.2 48.9 38.7\nGLM Doc 87.5 76.0 63.2 47.9 37.6\nwith the surrounding context (Zhu et al., 2019;\nDonahue et al., 2020; Shen et al., 2020). GLM\nis trained with an autoregressive blank inﬁlling\nobjective, thus can straightforwardly solve this\ntask. We evaluate GLM on the Yahoo Answers\ndataset (Yang et al., 2017) and compare it with\nBlank Language Model (BLM) (Shen et al., 2020),\nwhich is a speciﬁcally designed model for text in-\nﬁlling. From the results in Table 5, GLM outper-\nforms previous methods by large margins (1.3 to\n3.9 BLEU) and achieves the state-of-the-art result\non this dataset. We notice that GLM Doc slightly\nunderperforms GLMLarge, which is consistent with\nour observations in the seq2seq experiments.\nLanguage Modeling. Most language model-\ning datasets such as WikiText103 are constructed\nfrom Wikipedia documents, which our pretraining\ndataset already contains. Therefore, we evaluate\nthe language modeling perplexity on a held-out\ntest set of our pretraining dataset, which contains\nabout 20M tokens, denoted as BookWiki. We also\nevaluate GLM on the LAMBADA dataset (Paperno\nUnidirectional Bidirectional\n20\n30\n40\n50\n60Accuracy\nLAMBADA\nGLMDoc\nGLMDoc – 2D\nGLM410M\nGLM515M\nGPTLarge\nUnidirectional Bidirectional\n8\n10\n12\n14\n16Perplexily\nBooks&Wiki Test\nFigure 4: Zero-shot language modeling results.\net al., 2016), which tests the ability of systems to\nmodel long-range dependencies in text. The task\nis to predict the ﬁnal word of a passage. As the\nbaseline, we train a GPTLarge model (Radford et al.,\n2018b; Brown et al., 2020) with the same data and\ntokenization as GLMLarge.\nThe results are shown in Figure 4. All the models\nare evaluated in the zero-shot setting. Since GLM\nlearns the bidirectional attention, we also evalu-\nate GLM under the setting in which the contexts\nare encoded with bidirectional attention. Without\ngenerative objective during pretraining, GLMLarge\ncannot complete the language modeling tasks,\nwith perplexity larger than 100. With the same\namount of parameters, GLM Doc performs worse\nthan GPTLarge. This is expected since GLM Doc\nalso optimizes the blank inﬁlling objective. In-\ncreasing the model’s parameters to 410M (1.25×of\nGPTLarge) leads to a performance close to GPTLarge.\nGLM515M (1.5×of GPTLarge) can further outper-\nform GPTLarge. With the same amount of param-\neters, encoding the context with bidirectional at-\ntention can improve the performance of language\nmodeling. Under this setting, GLM 410M outper-\nforms GPTLarge. This is the advantage of GLM\nover unidirectional GPT. We also study the con-\ntribution of 2D positional encoding to long text\ngeneration. We ﬁnd that removing the 2D posi-\ntional encoding leads to lower accuracy and higher\nperplexity in language modeling.\n326\nTable 6: Ablation study on the SuperGLUE dev set. (T5 ≈GLM – shufﬂe spans + sentinel tokens.)\nModel ReCoRD\nF1/Acc.\nCOPA\nAcc.\nWSC\nAcc.\nRTE\nAcc.\nBoolQ\nAcc.\nWiC\nAcc.\nCB\nF1/Acc.\nMultiRC\nF1a/EM Avg\nBERTLarge 76.3 / 75.6 69.0 64.4 73.6 80.1 71.0 94.8 / 92.9 71.9 / 24.1 72.0\nBERTLarge (reproduced) 82.1 / 81.5 63.0 63.5 72.2 80.8 68.7 80.9 / 85.7 77.0 / 35.2 71.2\nBERTLarge (cloze) 70.0 / 69.4 80.0 76.0 72.6 78.1 70.5 93.5 / 91.1 70.0 / 23.1 73.2\nGLMLarge 81.7 / 81.1 76.0 81.7 74.0 82.1 68.5 96.1 / 94.6 77.1 / 36.3 77.0\n– cloze ﬁnetune 81.3 / 80.6 62.0 63.5 66.8 80.5 65.0 89.2 / 91.1 72.3 / 27.9 70.0\n– shufﬂe spans 82.0 / 81.4 61.0 79.8 54.5 65.8 56.3 90.5 / 92.9 76.7 / 37.6 68.5\n+ sentinel tokens 81.8 / 81.3 69.0 78.8 77.3 81.2 68.0 93.7 / 94.6 77.5 / 37.7 76.0\nSummary. Above all, we conclude that GLM\neffectively shares model parameters across natu-\nral language understanding and generation tasks,\nachieving better performance than a standalone\nBERT, encoder-decoder, or GPT model.\n3.4 Ablation Study\nTable 6 shows our ablation analysis for GLM.\nFirst, to provide an apple-to-apple comparison with\nBERT, we train a BERTLarge model with our im-\nplementation, data, and hyperparameters (row 2).\nThe performance is slightly worse than the ofﬁcial\nBERTLarge and signiﬁcantly worse than GLMLarge.\nIt conﬁrms the superiority of GLM over Masked\nLM pretraining on NLU tasks. Second, we show\nthe SuperGLUE performance of GLM ﬁnetuned as\nsequence classiﬁers (row 5) and BERT with cloze-\nstyle ﬁnetuning (row 3). Compared to BERT with\ncloze-style ﬁnetuning, GLM beneﬁts from the au-\ntoregressive pretraining. Especially on ReCoRD\nand WSC, where the verbalizer consists of multi-\nple tokens, GLM consistently outperforms BERT.\nThis demonstrates GLM’s advantage in handling\nvariable-length blank. Another observation is that\nthe cloze formulation is critical for GLM’s perfor-\nmance on NLU tasks. For the large model, cloze-\nstyle ﬁnetuning can improve the performance by\n7 points. Finally, we compare GLM variants with\ndifferent pretraining designs to understand their\nimportance. Row 6 shows that removing the span\nshufﬂing (always predicting the masked spans from\nleft to right) leads to a severe performance drop on\nSuperGLUE. Row 7 uses different sentinel tokens\ninstead of a single [MASK] token to represent dif-\nferent masked spans. The model performs worse\nthan the standard GLM. We hypothesize that it\nwastes some modeling capacity to learn the differ-\nent sentinel tokens which are not used in down-\nstream tasks with only one blank. In Figure 4, we\nshow that removing the second dimension of 2D\npositional encoding hurts the performance of long\ntext generation.\nWe note that T5 is pretrained with a similar blank\ninﬁlling objective. GLM differs in three aspects:\n(1) GLM consists of a single encoder, (2) GLM\nshufﬂes the masked spans, and (3) GLM uses a\nsingle [MASK] instead of multiple sentinel tokens.\nWhile we cannot directly compare GLM with T5\ndue to the differences in training data and the num-\nber of parameters, the results in Tables 1 and 6 have\ndemonstrated the advantage of GLM.\n4 Related Work\nPretrained Language Models. Pretraining large-\nscale language models signiﬁcantly improves the\nperformance of downstream tasks. There are three\ntypes of pretrained models. First, autoencoding\nmodels learn a bidirectional contextualized encoder\nfor natural language understanding via denoising\nobjectives (Devlin et al., 2019; Joshi et al., 2020;\nYang et al., 2019; Liu et al., 2019; Lan et al., 2020;\nClark et al., 2020). Second, autoregressive mod-\nels are trained with a left-to-right language mod-\neling objective (Radford et al., 2018a,b; Brown\net al., 2020). Third, encoder-decoder models are\npretrained for sequence-to-sequence tasks (Song\net al., 2019; Lewis et al., 2019; Bi et al., 2020;\nZhang et al., 2020).\nAmong encoder-decoder models, BART (Lewis\net al., 2019) conducts NLU tasks by feeding the\nsame input into the encoder and decoder, and tak-\ning the ﬁnal hidden states of the decoder. Instead,\nT5 (Raffel et al., 2020) formulates most language\ntasks in the text-to-text framework. However, both\nmodels require more parameters to outperform au-\ntoencoding models such as RoBERTa (Liu et al.,\n2019). UniLM (Dong et al., 2019; Bao et al., 2020)\nuniﬁes three pretraining models under the masked\nlanguage modeling objective with different atten-\ntion masks.\nNLU as Generation. Previously, pretrained\nlanguage models complete classiﬁcation tasks for\n327\nNLU with linear classiﬁers on the learned rep-\nresentations. GPT-2 (Radford et al., 2018b) and\nGPT-3 (Brown et al., 2020) show that generative\nlanguage models can complete NLU tasks such\nas question answering by directly predicting the\ncorrect answers without ﬁnetuning, given task in-\nstructions or a few labeled examples. However,\ngenerative models require much more parameters\nto work due to the limit of unidirectional atten-\ntion. Recently, PET (Schick and Schütze, 2020a,b)\nproposes to reformulate input examples as cloze\nquestions with patterns similar to the pretraining\ncorpus in the few-shot setting. It has been shown\nthat combined with gradient-based ﬁnetuning, PET\ncan achieve better performance in the few-shot set-\nting than GPT-3 while requiring only 0.1% of its\nparameters. Similarly, Athiwaratkun et al. (2020)\nand Paolini et al. (2020) convert structured predic-\ntion tasks, such as sequence tagging and relation\nextraction, to sequence generation tasks.\nBlank Language Modeling. Donahue et al.\n(2020) and Shen et al. (2020) also study blank-\ning inﬁlling models. Different from their work,\nwe pre-train language models with blank inﬁlling\nobjectives and evaluate their performance in down-\nstream NLU and generation tasks.\n5 Conclusions\nGLM is a general pretraining framework for nat-\nural language understanding and generation. We\nshow that the NLU tasks can be formulated as con-\nditional generation tasks, and therefore solvable by\nautoregressive models. GLM uniﬁes the pretrain-\ning objectives for different tasks as autoregressive\nblank inﬁlling, with mixed attention masks and\nthe novel 2D position encodings. Empirically we\nshow that GLM outperforms previous methods for\nNLU tasks and can effectively share parameters for\ndifferent tasks.\nAcknowledgements\nThe work is supported by the NSFC for Distin-\nguished Young Scholar(61825602), and Beijing\nAcademy of Artiﬁcial Intelligence (BAAI).\nReferences\nBen Athiwaratkun, Cicero dos Santos, Jason Krone,\nand Bing Xiang. 2020. Augmented natural language\nfor generative sequence labeling. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 375–385.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. In ICML 2020 ,\nvolume 119, pages 642–652.\nBin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei\nWang, Songfang Huang, Fei Huang, and Luo\nSi. 2020. PALM: Pre-training an Autoencod-\ning&Autoregressive Language Model for Context-\nconditioned Generation. In EMNLP 2020 , pages\n8681–8691.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot\nLearners. In NeurIPS 2020.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\nTask 1: Semantic Textual Similarity Multilingual\nand Crosslingual Focused Evaluation. In Proceed-\nings of the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining Text Encoders as Discriminators Rather\nThan Generators. In ICLR 2020.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nUniversal: Language Speciﬁc Translation Evalua-\ntion for Any Target Language. InProceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In NAACL 2019, pages 4171–4186.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to ﬁll in the blanks. pages\n2492–2501.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In NeurIPS 2019, pages 13042–\n13054.\n328\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to Ask: Neural Question Generation for Reading\nComprehension. In ACL 2017, pages 1342–1352.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.\nio/OpenWebTextCorpus.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-\nenhanced bert with disentangled attention. ArXiv,\nabs/2006.03654.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving Pre-training by Representing\nand Predicting Spans. Trans. Assoc. Comput. Lin-\nguistics, 8:64–77.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In ICLR\n2020.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Trans-\nlation, and Comprehension. In ACL 2020 , pages\n7871–7880.\nChin-Yew Lin. 2004. ROUGE: A Package for Auto-\nmatic Evaluation of Summaries. pages 74–81.\nYang Liu and Mirella Lapata. 2019. Text Summariza-\ntion with Pretrained Encoders. In EMNLP 2019 ,\npages 3730–3740.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJoel Mackenzie, Rodger Benham, Matthias Petri, Jo-\nhanne R. Trippas, J. Shane Culpepper, and Alistair\nMoffat. 2020. CC-News-En: A Large English News\nCorpus. In CIKM 2020, pages 3077–3084.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t Give Me the Details, Just the Summary!\nTopic-Aware Convolutional Neural Networks for Ex-\ntreme Summarization. In EMNLP 2018 , pages\n1797–1807.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai, Ci-\ncero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2020. Structured Prediction as Translation\nbetween Augmented Natural Languages.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, San-\ndro Pezzelle, Marco Baroni, Gemma Boleda, and\nRaquel Fernández. 2016. The LAMBADA dataset:\nWord prediction requiring a broad discourse context.\nIn ACL 2016.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A Method for Automatic\nEvaluation of Machine Translation. In ACL 2002,\npages 311–318.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Regu-\nlarizing neural networks by penalizing conﬁdent out-\nput distributions. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Workshop Track Proceed-\nings.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018a. Improving Language Under-\nstanding by Generative Pre-Training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018b. Lan-\nguage models are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer. J. Mach. Learn. Res. , 21:140:1–\n140:67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow What You Don’t Know: Unanswerable Ques-\ntions for SQuAD. In ACL 2018, pages 784–789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP 2016,\npages 2383–2392.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In KDD 2020, pages\n3505–3506.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In EMNLP 2015, pages 379–\n389.\nTimo Schick and Hinrich Schütze. 2020a. Exploiting\nCloze Questions for Few Shot Text Classiﬁcation\nand Natural Language Inference. pages 255–269.\nTimo Schick and Hinrich Schütze. 2020b. It’s Not\nJust Size That Matters: Small Language Models Are\nAlso Few-Shot Learners. pages 2339–2352.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get To The Point: Summarization with\nPointer-Generator Networks. In ACL 2017, pages\n1073–1083.\n329\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi S. Jaakkola. 2020. Blank language models.\npages 5186–5198.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nCoRR, abs/1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In EMNLP 2013, pages 1631–1642.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: Masked Sequence to Se-\nquence Pre-training for Language Generation. In\nICML 2019, volume 97, pages 5926–5936.\nTrieu H. Trinh and Quoc V . Le. 2019. A\nSimple Method for Commonsense Reasoning.\narXiv:1806.02847 [cs].\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. SuperGLUE:\nA Stickier Benchmark for General-Purpose Lan-\nguage Understanding Systems. In NeurIPS 2019 ,\npages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In ICLR\n2019, pages 353–355.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In NAACL\n2018, pages 1112–1122.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In NeurIPS 2019, pages\n5754–5764.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017. Improved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In ICML 2017 , volume 70, pages\n3881–3890.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2020. PEGASUS: Pre-training with Ex-\ntracted Gap-sentences for Abstractive Summariza-\ntion. In ICML 2020, pages 11328–11339.\nWanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text\ninﬁlling. arXiv preprint arXiv:1901.00158.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In ICCV 2015, pages 19–\n27.\nA Pretraining Setting\nA.1 Datasets\nTo train GLM Base and GLM Large, we use Book-\nCorpus (Zhu et al., 2015) and Wikipedia used by\nBERT (Devlin et al., 2019).\nTo train GLMRoBERTa, we follow the pretraining\ndatasets of RoBERTa (Liu et al., 2019), which con-\nsist of BookCorups (Zhu et al., 2015),Wikipedia\n(16GB), CC-News (the English portion of the Com-\nmonCrawl News dataset3 76GB), OpenWebText\n(web content extracted from URLs shared on Red-\ndit with at least three upvotes(Gokaslan and Co-\nhen, 2019), 38GB) and Stories (subset of Common-\nCrawl data ﬁltered to match the story-like style of\nWinograd schemas (Trinh and Le, 2019), 31GB).\nThe Stories dataset is no longer publicly available4.\nTherefore, we remove the Stories dataset and re-\nplace OpenWebText with OpenWebText25 (66GB).\nThe CC-News dataset is not publicly available and\nwe use the CC-News-en published by (Mackenzie\net al., 2020). All the datasets used total 158GB of\nuncompressed texts, close in size to RoBERTa’s\n160GB datasets.\nA.2 Hyperparameters\nThe hyperparameters for GLMBase and GLMLarge\nare similar to those used by BERT. For trade-off\nof training speed and fair comparison with BERT\n(batch size 256 and 1,000,000 training steps), we\nuse batch size of 1024 and 200,000 training steps\nfor GLMLarge. Since GLM Base is smaller, we re-\nduce the number of training steps to 120,000 to\nspeed up pre-training. The hyperparameters for\nGLMDoc and GLMSent are the same as those of\nGLMLarge. The hyperparameters except Trans-\nformer architecture for GLM 410M and GLM515M\nare the same as those of GLM Large. The models\nare trained on 64 V100 GPUs for 200K steps with\nbatch size of 1024 and maximum sequence length\nof 512, which takes about 2.5 days for GLMLarge.\nTo train GLMRoBERTa, we follow most of the hy-\nperparameters of RoBERTa. The main difference\n3https://commoncrawl.org/2016/10/\nnews-dataset-available\n4https://github.com/tensorflow/models/\ntree/archive/research/lm_commonsense#\n1-download-data-files\n5https://openwebtext2.readthedocs.io/\nen/latest\n330\nTable 7: Hyperparameters for pretraining\nHyperparameters GLM Base GLM Large GLM RoBERTa\nNumber of Layers 12 24 24\nHidden size 768 1024 1024\nFFN inner hidden size 3072 4096 4096\nAttention heads 12 16 16\nAttention head size 64 64 64\nDropout 0.1 0.1 0.1\nAttention Dropout 0.1 0.1 0.1\nWarmup Steps 6k 8k 30K\nPeak Learning Rate 4e-4 2e-4 4e-4\nBatch Size 1024 1024 8192\nWeight Decay 0.1 0.1 0.01\nMax Steps 120k 200k 250k\nLearning Rate Decay Cosine Cosine Cosine\nAdam ϵ 1e-6 1e-6 1e-6\nAdam β1 0.9 0.9 0.9\nAdam β2 0.98 0.98 0.98\nGradient Clipping 1.0 1.0 1.0\nincludes: (1) Due to resource limit, we only pre-\ntrain GLM RoBERTa for 250,000 steps, which are\nhalf of RoBERTa and BART’s training steps, and\nclose to T5 in number of trained tokens. (2) We use\ncosine decay instead of linear decay for learning\nrate scheduling (3) We additionally apply gradient\nclipping with value 1.0.\nThe hyperparameters for all the pre-training set-\ntings are summarized in Table 7.\nA.3 Implementation\nOur pretraining implementation is based on\nMegatron-LM (Shoeybi et al., 2019) and Deep-\nSpeed (Rasley et al., 2020). We include our code in\nthe supplementary material. Due to the size limit of\nsupplementary material, we cannot include the pre-\ntrained models, but will make them public available\nin the future.\nB Downstream Tasks\nB.1 SuperGLUE\nThe SuperGLUE benchmark consists of 8 NLU\ntasks. We formulate them as blank inﬁlling tasks,\nfollowing (Schick and Schütze, 2020b). Table 8\nshows the cloze questions and verbalizers we used\nin our experiments. For 3 tasks (ReCoRD, COPA,\nand WSC), the answer may consist of multiple\ntokens, and for the other 5 tasks, the answer is\nalways a single token.\nWhen ﬁnetuning GLM on the SuperGLUE tasks,\nwe construct the input using the cloze questions\nin Table 8 and replace the blank with a [MASK]\ntoken. Then we compute the score of generating\neach answer candidate. For the 5 single-token tasks,\nthe score is deﬁned to be the logit of the verbal-\nizer token. For the 3 multi-token tasks, we use\nthe sum of the log-probabilities of the verbalizer\ntokens. Thanks to the autoregressive blank inﬁll-\ning mechanism we proposed, we can obtain all the\nlog-probabilities in one pass. Then we compute the\ncross entropy loss using the groundtruth label and\nupdate the model parameters.\nFor the baseline classiﬁers, we follow the stan-\ndard practice to concatenate the input parts of each\ntask (such as the premise and hypothesis for textual\nentailment, or the passage, question and answer\nfor ReCORD and MultiRC) and add a classiﬁca-\ntion layer on top of the [CLS] token representa-\ntion. We also implemented cloze-style ﬁnetuning\nfor the other pre-trained models, but the perfor-\nmance was usually similar to the standard classiﬁer,\nas we shown in the ablation study. Models with\nblank-inﬁlling objectives, such as T5 and our GLM,\nbeneﬁts more from converting the NLU tasks into\ncloze questions. Thus for T5 and GLM, we report\nthe performance after such conversion in our main\nresults.\n331\nTable 8: Cloze questions and verbalizers for the 8 SuperGLUE tasks used in our experiments.∗denotes the answer\ncontains multiple tokens.\nDataset Task Cloze Question Verbalizers\nReCoRD∗ Question answering [passage p] [cloze questionq] Answer candidates\nCOPA∗ Causal reasoning “[choicec1]” or “[choicec2]”? [premisep], so\n.\nc1 / c2\nWSC∗ Coreference resolution [sentence s] The pronoun ‘∗p∗’ refers to . Noun n\nRTE Textual entailment “[hypothesis h]”?| , “[premisep]” “yes” (entail-\nment), “no” (not\nentailment)\nBoolQ Question answering [passage p]. Question:q? Answer: . “yes” / “no”\nWiC Word sense disambiguation “[sentences1]” / “[sentences2]” Similar sense\nof [wordw]? .\n“yes” / “no”\nCB Textual entailment “[hypothesis h]”?| , “[premisep]” “yes” (entailment),\n“no” (contradiction),\n“maybe” (neutral)\nMultiRC Question answering [passagep]. Question:q? Is it [answera]? . “yes” / “no”\nB.2 Sequence-to-Sequence\nFot the text summarization task, we use the dataset\nGigaword (Rush et al., 2015) for model ﬁne-tuning\nand evaluation. We ﬁnetune GLM LARGE on the\ntraining set for 4 epochs with AdamW optimizer.\nThe learning rate has a peak value of 3e-5, warm-\nup over the 6% training steps and a linear decay.\nWe also use label smoothing with rate 0.1 (Pereyra\net al., 2017). The maximum document length is 192\nand the maximum summary length is 32. During\ndecoding, we use beam search with beam size of 5\nand remove repeated trigrams. We tweak the value\nof length penalty on the development set. The\nevaluation metrics are the F1 scores of Rouge-1,\nRouge-2, and Rouge-L (Lin, 2004) on the test set.\nFor the question generation task, we use the\nSQuAD 1.1 dataset (Rajpurkar et al., 2016) and\nfollow the dataset split of (Du et al., 2017). The\noptimizer hyperparameters are the same as those of\nabstractive summarization. The maximum passage\nlength is 464 and the maximum question length\nis 48. During decoding, we use beam search with\nbeam size 5 and tweak the value of length penalty\non the development set. The evaluation metrics are\nthe scores of BLEU-1, BLEU-2, BLEU-3, BLEU-\n4 (Papineni et al., 2002), METEOR (Denkowski\nand Lavie, 2014) and Rouge-L (Lin, 2004).\nResults of T5Large on XSum are obtained by run-\nning the summarization script provided by Hug-\ngingface transformers6. All the other results of\n6https://github.com/huggingface/\ntransformers/tree/master/examples/\npytorch/summarization\nbaselines on seq2seq tasks are obtained from the\ncorresponding papers.\nB.3 Text Inﬁlling\nWe follow (Shen et al., 2020) and evaluate text in-\nﬁlling performance on the Yahoo Answers dataset\n(Yang et al., 2017), which contains 100K/10K/10K\ndocuments for train/valid/test respectively. The av-\nerage document length is 78 words. To construct\nthe text inﬁlling task, we randomly mask a given ra-\ntio r∈{10% ···50%}of each document’s tokens\nand the contiguous masked tokens are collapsed\ninto a single blank. We ﬁnetune GLMLarge on the\ntraining set for 5 epochs with dynamic masking, i.e.\nthe blanks are randomly generated at training time.\nSimilar to the sequence-to-sequence experiments,\nwe use an AdamW optimizer with a peak learning\nrate 1e-5 and 6% warm-up linear scheduler.\nFor comparison with previous work, we use the\nsame test set constructed by (Shen et al., 2020).\nThe evaluation metric is the BLEU score of the in-\nﬁlled text against the original document. We com-\npare with two baselines: (1) BERT, which learns a\nleft-to-right language model to generate the masked\ntokens on top of the blank representation, and (2)\nBLM proposed by (Shen et al., 2020), which can\nﬁll in the blank with arbitrary trajectories.\nB.4 Language Modeling\nWe evaluate the model’s ability of language model-\ning with perplexity on BookWiki and accuracy on\nthe LAMBDA dataset (Paperno et al., 2016).\nPerplexity is an evaluation criterion that has been\n332\nwell studied for language modeling. Perplexity is\nthe exponentiation of the average cross entropy of\na corpus.\nPPL = exp(−1\nT\nT∑\nt=1\np(xt|x<t)) (4)\nwhere x<t = [x0,··· ,xt−1]. Since transformers\ncan only operate on a window of ﬁxed input size\nw, we cannot fully calculate p(xt|x<t) and can\nonly calculate p(xt|xt−w:t−1). Even calculating\nthis value for each token is prohibitively expensive,\nsince we need to conduct T evaluations of w-size\ncontexts. To improve evaluation efﬁciency, we\nadopt overlapping evaluation, where we advance\nthe sliding windows by some overlap oeach time\nand only compute the cross entropy loss for the last\notokens of the window. In our experiments we set\no= 256for all the models.\nLAMBDA is a cloze-style dataset to test the abil-\nity of long-range dependency modeling. Each ex-\nample is a passage consisting of 4-5 sentences with\nthe last word missing and the model is required to\npredict the last word of the passage. Since we use\nWordPiece tokenization, a word can be split into\nseveral subword units. We use teacher forcing and\nconsider the prediction correct only when all the\npredicted tokens are correct.\nC Results on Other NLU Benchmarks\nGLUE (Wang et al., 2018) is another widely-used\nNLU benchmark, including single sentence tasks\n(e.g. sentiment analysis (Socher et al., 2013)) and\nsentence pair tasks (e.g. text similarity (Cer et al.,\n2017) and natural language inference (Williams\net al., 2018; Dagan et al., 2005)). The benchmark is\nusually considered as less challenging than Super-\nGLUE. SQuAD (Rajpurkar et al., 2016, 2018) is an\nextractive question answering benchmark. We fur-\nther compare GLM with BERT on the two bench-\nmarks.\nThe results on GLUE and SQuAD are shown in\nTables 9 and 10. On the two benchmarks, GLM\ncan still outperform BERT with the same amount\nof parameters, but with a smaller margin.\nD Text Generation Samples\nWe show texts generated by GLMDoc given unseen\ncontexts randomly sampled from the test set. We\nuse top-krandom sampling with k = 40for gen-\neration and set maximum sequence length to 512.\nSome of the texts are cut short.\nExample D.1. The Wyoming State Legislature is\nthe legislative branch of the U.S. State of Wyoming.\nGLM: It consists of a state senate and a house\nof representatives. As of 2019, state senators are\nSteve Kish, Joe L. Williams, and Linda S. Bollens.\nMembers of the Wyoming State Legislature are\nelected from single-member districts representing\nthe majority of the state. The current state senate\nmembers are: In recent years, there have been four\nchanges to the senate. The most recent is the cre-\nation of a six-seat district that includes all or part of\nthe following: In the 2009 elections, the state sen-\nate members were elected to six-year terms. The\ncurrent state house members are: The Wyoming\nConstitution assigns certain powers to the governor.\nMost notably, the governor is president of the sen-\nate and governor. However, if the governor desires\nto appoint a member to the Wyoming state senate, a\nlaw authorizes the governor to do so. The governor\nof Wyoming holds no legislative power but has the\npower to veto lawmakers, which is not limited to\nthe veto of laws. Under the wyoming state con-\nstitution, the governor can veto the actions of the\nother members of the wyoming house of represen-\ntatives. The governor can also appoint members of\nthe wyoming senate. In addition, the governor can\nappoint members of the Wyoming house of repre-\nsentatives. Wyoming’s constitution provides that\nthe governor can appoint a member of the wyoming\nstate senate to the wyoming supreme court, and the\nchairman of the wyoming senate.\nExample D.2. Jonathan Terry is a television and\nﬁlm actor.\nGLM: He was a voice actor for the “X-Men\"\ncartoon series. He was also a voice actor for “the\nSimpsons\" as well as “the marvelous misadven-\ntures of superman. He also has voiced characters\nin “the legend of korra\" He has appeared on several\ntelevision series, including “the simpsons\", “the\nx-ﬁles\", “heroes\" and “the simpsons movie\" as the\ncharacter captain billy higgledy-pig, and the web\nseries “krusty mysteries\" as the character Colonel\nTrungus. He wrote for “the Guardian\" newspaper.\nJonathan Terry was born in London. Terry grew\nup in Surrey, England and attended the University\nof Sussex in the United Kingdom, graduating with\na degree in english literature. He was a guest lec-\nturer at King’s College London, and then took two\nyears of acting courses at the brit school of acting\nto prepare for his future career in the entertainment\nindustry. Terry ﬁrst appeared in the TV series “the\n333\nTable 9: Results on the GLUE dev set.\nModel MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B Avg\nBERTLarge 86.6 92.3 91.3 73.6 93.2 88.0 60.6 90.0 84.4\nGLMLarge 86.7 92.8 91.5 74.0 93.5 90.0 61.4 90.7 85.1\nTable 10: Results on the SQuAD v1.1/v2.0 dev sets.\nModel SQuAD v1.1\nEM/F1\nSQuAD v2.0\nEM/F1\nBERTBase 80.8 / 88.5 73.7 / 76.3\nGLMBase 81.5 / 88.6 74.7 / 77.8\nBERTLarge 84.1 / 90.9 79.0 / 81.8\nGLMLarge 85.4 / 91.6 80.3 / 83.3\nSimpsons\" as the character captain Billy Higgledy-\npig, but his character was only a one-time recurring\ncharacter in the series’ ﬁrst six seasons. He later ap-\npeared as a regular for the show’s ﬁnal six seasons,\nand has been a frequent guest in the show since.\nHe appeared in the ﬁrst few episodes of “\" as the\ncharacter major Jack Ryan. He has also appeared\nas part of the supporting cast of several episodes of\n“the secret life of pets\". He has also worked on “the\nsimpsons\" TV show since “the simpsons movie\",\nmost notably playing the roles of Captain Skeletor\nand the ghost of the same name. He plays char-\nacters in several ﬁlms, including “\", “\", “\" and “\".\nHe has appeared in music videos for the killers in\n1993, the pretenders in 1995, and in the TV shows\n“the royal\" and “the bill\".\nExample D.3. Corona was a station along the port\nWashington branch of the long island rail road in\nthe Corona section of queens, New York City. It\nwas one of two stations built by the ﬂushing rail-\nroad in Corona, this one having been at Grand Av-\nenue (later called National Avenue, now National\nStreet ) and 45th Avenue.\nGLM: It had a brick entrance building next to\nthe tracks, and one platform with 2 side platforms.\nThe station burned down and was rebuilt in 1908.\nThe station closed on september 15, 1927, with\nthe train service transferred from Grand Avenue to\n45th Avenue. There were plans to build a subway\nextension to Corona, but it was never built. In the\n1980s, two stations were constructed on the line,\nCorona Road and Corona Park. The new Corona\nstation opened in 1988, and the original Corona\nstation was demolished.\nExample D.4. Robert Lee Smith (born July 5,\n1938) is a former American football cornerback\nin the national football league. He played for the\nLos Angeles Rams (1962–1965) and the Detroit\nLions (1965–1966). He was also a kick and punt\nreturner.\nGLM: In his four-year NFL career, he played\nin 33 games and started 14, registering 62 career\ninterceptions. He completed his NFL career with\nﬁve interceptions. Smith was born in La Canada\nFlintridge, Michigan, in 1938. He attended Bishop\nO’ Dowd high school in Flintridge. Smith’ s fa-\nther was a pharmacist. He graduated from Michi-\ngan State University in 1958 with a degree in busi-\nness administration. Smith played for the Michi-\ngan Wolverines football team from 1959 to 1963.\nIn his two years as a reserve cornerback, he led\nthe conference in interceptions with ﬁve. In 1962,\nhe set the Wolverines’ all-time interception record\nwith 13, and was second overall in the 1962 sea-\nson’s Heisman Trophy voting. Smith also won\nthe Dick Butkus award as the nation’s outstand-\ning linebacker. In 1961, the “Los Angeles Times\"\nwrote that Smith “is an outstanding pass rusher,\nwith an average of almost 100 yards per punt re-\nturn.\" Smith was inducted into the university of\nMichigan athletic hall of honor in 1989 and the\nnational football foundation hall of fame in 1991.\nHe was elected to the Michigan sports hall of fame\nin 1995. Smith earned the honor because of his ac-\ncomplishments prior to his NFL career. He was one\nof four Michigan players honored as ﬁrst-overall\nselections in the 1964 NFL draft. The others were\nJoe Namath, Bill Nelsen, and Jerry Kramer. In\n1966, the NFL gave players $300,000 a season to\nplay football. After his rookie season, he was not\nselected to play in the 1966 pro bowl. On January\n13, 1966, the Rams traded smith to the Detroit Li-\nons for Paul Hornung, and later that year he was\ntraded to the Lions for Ray “the Lion\" Jones in\nexchange for Linebacker Jim “the Hawk\" Johnson.\nOn September 10, 1968, he was traded back to Los\nAngeles for a second round pick in the 1970 draft.\nHe was also traded to the St. Louis Cardinals for\n334\na second round pick in the 1970 draft. On June 2,\n1970 he was cut by the Cardinals. On November\n15, 1970, the Los Angeles Rams acquired Smith\nfrom the Lions in exchange for Linebacker Tony\nHarris. The Rams waived Smith during the Septem-\nber 1, 1972 offseason. Smith’s number at Michigan\nState was # 7 in 1969.\n335",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.7693629264831543
    },
    {
      "name": "Blank",
      "score": 0.7654673457145691
    },
    {
      "name": "Natural language processing",
      "score": 0.5265783667564392
    },
    {
      "name": "Linguistics",
      "score": 0.5077496767044067
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.44400909543037415
    },
    {
      "name": "Association (psychology)",
      "score": 0.4343975782394409
    },
    {
      "name": "Computer science",
      "score": 0.42473888397216797
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3847019672393799
    },
    {
      "name": "Psychology",
      "score": 0.32305216789245605
    },
    {
      "name": "Statistics",
      "score": 0.31167274713516235
    },
    {
      "name": "Mathematics",
      "score": 0.2819262146949768
    },
    {
      "name": "Philosophy",
      "score": 0.24577170610427856
    },
    {
      "name": "Epistemology",
      "score": 0.14666837453842163
    },
    {
      "name": "Engineering",
      "score": 0.12092062830924988
    },
    {
      "name": "Physics",
      "score": 0.07499492168426514
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210122302",
      "name": "ShangHai JiAi Genetics & IVF Institute",
      "country": "CN"
    }
  ],
  "cited_by": 794
}