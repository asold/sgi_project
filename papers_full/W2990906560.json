{
    "title": "Independent language modeling architecture for end-to-end ASR",
    "url": "https://openalex.org/W2990906560",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2223322168",
            "name": "Pham Van Tung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2188375617",
            "name": "Xu, Haihua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221532426",
            "name": "Khassanov, Yerbolat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2369010149",
            "name": "Zeng Zhi-ping",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966190765",
            "name": "Chng Eng Siong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743287313",
            "name": "Ni Chongjia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130961321",
            "name": "Ma Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1239907363",
            "name": "LI Haizhou",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2555428947",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2407080277",
        "https://openalex.org/W2627092829",
        "https://openalex.org/W2962780374",
        "https://openalex.org/W2193413348",
        "https://openalex.org/W2916997151",
        "https://openalex.org/W2575842049",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2102113734",
        "https://openalex.org/W2949640717",
        "https://openalex.org/W2526425061",
        "https://openalex.org/W2577366047",
        "https://openalex.org/W2964012862",
        "https://openalex.org/W2973072704",
        "https://openalex.org/W2402302915",
        "https://openalex.org/W854541894",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W1526236009",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2886180730",
        "https://openalex.org/W2766219058",
        "https://openalex.org/W2889028433",
        "https://openalex.org/W2892655153",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W6908809"
    ],
    "abstract": "The attention-based end-to-end (E2E) automatic speech recognition (ASR) architecture allows for joint optimization of acoustic and language models within a single network. However, in a vanilla E2E ASR architecture, the decoder sub-network (subnet), which incorporates the role of the language model (LM), is conditioned on the encoder output. This means that the acoustic encoder and the language model are entangled that doesn't allow language model to be trained separately from external text data. To address this problem, in this work, we propose a new architecture that separates the decoder subnet from the encoder output. In this way, the decoupled subnet becomes an independently trainable LM subnet, which can easily be updated using the external text data. We study two strategies for updating the new architecture. Experimental results show that, 1) the independent LM architecture benefits from external text data, achieving 9.3% and 22.8% relative character and word error rate reduction on Mandarin HKUST and English NSC datasets respectively; 2)the proposed architecture works well with external LM and can be generalized to different amount of labelled data.",
    "full_text": "INDEPENDENT LANGUAGE MODELING ARCHITECTURE FOR END-TO-END ASR\nVan Tung Pham1, Haihua Xu1, Yerbolat Khassanov1,2, Zhiping Zeng1, Eng Siong Chng1,\nChongjia Ni3, Bin Ma3 and Haizhou Li4\n1School of Computer Science and Engineering, Nanyang Technological University, Singapore\n2ISSAI, Nazarbayev University, Kazakhstan\n3Machine Intelligence Technology, Alibaba Group\n4Department of Electrical and Computer Engineering, National University of Singapore, Singapore\nABSTRACT\nThe attention-based end-to-end (E2E) automatic speech\nrecognition (ASR) architecture allows for joint optimization\nof acoustic and language models within a single network.\nHowever, in a vanilla E2E ASR architecture, the decoder\nsub-network (subnet), which incorporates the role of the lan-\nguage model (LM), is conditioned on the encoder output.\nThis means that the acoustic encoder and the language model\nare entangled that doesn’t allow language model to be trained\nseparately from external text data. To address this problem,\nin this work, we propose a new architecture that separates\nthe decoder subnet from the encoder output. In this way, the\ndecoupled subnet becomes an independently trainable LM\nsubnet, which can easily be updated using the external text\ndata. We study two strategies for updating the new architec-\nture. Experimental results show that, 1) the independent LM\narchitecture beneﬁts from external text data, achieving 9.3%\nand 22.8% relative character and word error rate reduction on\nMandarin HKUST and English NSC datasets respectively; 2)\nthe proposed architecture works well with external LM and\ncan be generalized to different amount of labelled data.\nIndex Terms— Independent language model, low-resource\nASR, pre-training, ﬁne-tuning, catastrophic forgetting.\n1. INTRODUCTION\nEnd-to-End (E2E) architecture has been a promising strategy\nfor ASR systems. In this strategy, a single network is em-\nployed to directly map acoustic features into a sequence of\ncharacters or words without the need of a pronunciation dic-\ntionary that is required by the conventional Hidden Markov\nModel based systems. Furthermore, the components of E2E\nnetwork can be jointly trained for a common objective crite-\nrion to achieve overall optimization. The main approaches for\nE2E ASR are attention-based encoder-decoder [1, 2, 3, 4, 5,\n6], Connectionist Temporal Classiﬁcation (CTC) [7, 8] and\nthe hybrid CTC/attention architectures [9, 10].\nThe training of an E2E system requires a large amount of\ntranscribed speech data, here denoted as labelled data, which\nis unavailable for low-resource languages. However, we note\nthat large external text data can easily be collected. In this\nwork, we focus on the use of external text data to improve the\nlanguage model (LM) of E2E ASR systems.\nIn a vanilla E2E architecture, the decoder sub-network\n(subnet) incorporates the role of the LM. Unlike traditional\nASR systems where the LM is separated and hence can easily\nbe trained with text-only data, the decoder subnet is condi-\ntioned on the encoder output. As a result, it is not straightfor-\nward to update the LM component of the vanilla E2E archi-\ntecture with the text data.\nTo address this problem, in this work, we introduce a new\narchitecture which separates the decoder subnet from the en-\ncoder output, making the subnet an explicit LM. In this way,\nthe subnet can easily be updated using the text data. A po-\ntential issue, named catastrophic forgetting [11], might occur\nwhen using external text to update the E2E network: the net-\nwork forgets what it has learnt from labelled data. We, there-\nfore, study the strategies that use both labelled and external\ntext data to update the E2E network1.\nThe paper is organized as follows. Section 2 describes\na vanilla architecture of E2E ASR systems. In Section 3, we\nﬁrst describe the proposed architecture, then present strategies\nto update the proposed architecture using external text data.\nSection 4 relates our proposed approach with prior work. Ex-\nperimental setup and results are presented in Section 5 and 6\nrespectively. Section 7 concludes our work.\n2. A V ANILLA E2E ASR ARCHITECTURE\nIn this section, we describe a vanilla attention-based 2 E2E\nASR architecture (denoted as A1), which is widely used in\nprior work [9, 10]. Let ﬁrst denote Pas the labelled data. Let\n<X, Y>∈P be a training utterance, where X is a sequence\n1Since both labelled and external text data are used, we actually allow\nentire E2E network to be updated.\n2In actual implementation, we use the hybrid CTC/attention architecture\n[9, 10]. However, since the CTC module is untouched, we do not mention it\nduring the rest of this paper for simplicity.\narXiv:1912.00863v1  [cs.CL]  25 Nov 2019\nof acoustic features and Y = {y1,y2,...,y |Y|}is a sequence\nof output units.\nThe E2E architecture consists of an encoder and an\nattention-based decoder which are shown in Fig. 1 (a). The\nencoder acts as an acoustic model which maps acoustic fea-\ntures into an intermediate representationh. Then, the decoder\nsubnet, which consists of an embedding, a Long Short-Term\nMemory (LSTM) and a projection layers, generates one out-\nput unit at each decoding step ias follows,\nci = attention(h,si−1) (1)\nsi = LSTM(si−1,ci,embedding(yi−1)) (2)\nP(yi |X,y<i) =softmax(projection(si)) (3)\nwhere ci is the context vector, si−1 and si are output hidden\nstates at time step i−1 and irespectively, embedding() and\nprojection() are embedding and projection layers respec-\ntively. The E2E network is normally trained in batch-mode\nwith a loss function as follows,\nLASR(θ) = 1\n|B|\n∑\n<X,Y>∈B\nlog P(Y |X,θ)\n= 1\n|B|\n∑\n<X,Y>∈B\n|Y|∑\ni=1\nlog P(yi |X,y<i,θ)\n(4)\nwhere y<i, B and θ denote the decoding output history, a\nbatch of data and model parameters respectively.\nAccording to Equation (1) and (2), the LSTM is condi-\ntioned on the context vector ci which depends on the encoder\noutput h. In the absence of acoustic features X, thus h, it is\nnot possible to update the E2E architecture appropriately us-\ning only text data. One way to alleviate such problem is to\nset ci by an all-zero vector [12]. Unfortunately, this method\nintroduces a mismatch between training phase and updating\nphase (with external text data) since during training ci is gen-\nerally not the all-zero vector.\n3. INDEPENDENTLY TRAINABLE LM SUBNET\nTo allow updating of LM with external text data, we ﬁrst in-\ntroduce (Section 3.1) a novel architecture that separates the\ndecoder subnet from the encoder output. The updating algo-\nrithm is described in Section 3.2.\n3.1. Decoupling LM subnet\nInspired by the idea of spatial attention [13] for image cap-\ntioning, we propose to decouple the LM subnet from the en-\ncoder output as shown in Fig. 1(b). In this architecture, de-\nnoted as A2, the decoding process is formally described as\nfollows,\nsi = LSTM(si−1,embedding(yi−1)) (5)\nci = attention(h,si) (6)\nP(yi |X,y<i) =softmax(projection(si) +\nprojection(ci)) (7)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n  \n \n \n \nattention LSTM \nsoftmax \nprojection \nencoder embedding \nyi \nsi \nci \nh \nyi-1 X si-1 \nencoder \nattention \nLSTM \nembedding \nprojection \nsoftmax \nX \n si-1 \n yi-1 \nsi \nyi \nh \nci \nprojection \nsum \nb) Proposed architecture (A2) a) Vanilla architecture (A1) \nFig. 1. (a) A vanilla E2E architectureA1 and (b) the proposed\narchitecture A2. In the proposed architecture, the subnet (in\nred-dash box) is a language model which can be easily up-\ndated using external text data.\nFrom Equation (5), the LSTM is only conditioned on the\nprevious decoding hidden state and previous decoding output.\nIn other words, the decoder subnet is a standard LM, hereafter\ndenoted as LM subnet. In this way, this subnet can be inde-\npendently updated with external text data.\n3.2. Updating the LM subnet with external text data\nOne issue when using external text, denoted as T, to improve\nLM is catastrophic forgetting. Speciﬁcally, when T is used,\nthe network forgets what it has learnt from P. To address\nsuch issue, we use both T and Pto update the entire E2E\nnetwork. Another issue is when should Tbe used, i.e. before\nor after the entire E2E ASR network is trained. We study two\nstrategies to update the network as presented in Fig. 2.\n \nStrategy 1 \nStep 1: Train E2E \nnetwork \nStep 2: Fine-tune \nE2E network \nStep 3: Fine-tune \nE2E network \nStep 1: Pre-train \nLM subnet \nStep 2: Continue \ntraining the entire \nE2E network \nStrategy 2 \nExternal \ntext \nLabelled \ndata \nFig. 2. Strategies to update the E2E architecture with external\ntext data.\nIn Strategy 1, the entire E2E network is ﬁrst trained using\nP. Then, in the second step, the network is ﬁne-tuned with\nboth T and P. Finally, the network is further ﬁne-tuned with\nthe data P. We empirically found that the last step improves\nthe system performance.\nIn Strategy 2, the LM subnet is pre-trained with the text\ndata T ﬁrst. Then, the entire E2E network is trained using\nboth T and P.\nIn the second step of both strategies, i.e. to use both T\nand Pto update the E2E network, the following loss function\nis used:\nLtotal(θ) = (1−α)LASR(θ) +αLLM (θd) (8)\nwhere α denotes an interpolation factor, θd denotes all LM\nsubnet parameters, LLM (θd) denotes the LM loss obtained\nwhen external text data is used, i.e.:\nLLM (θd) = 1\n|B1|\n∑\nY’∈B1\nlog P(Y’|θd)\n= 1\n|B1|\n∑\nY’∈B1\n|Y’|∑\ni=1\nlog P(yi |y<i,θd)\n(9)\nwhere B1 is a batch of external text data.\n4. COMPARISON WITH RELATED WORK\nThere have been several studies on how to use external text\ndata for E2E ASR. One of the ideas is to use the external text\ndata to build an external LM, then incorporate it into the infer-\nence process [7, 14] or employ it to re-score n-best output hy-\npotheses [1]. Our proposed approach, however, improves the\nlanguage modeling capability of an E2E system without using\nany external LM. Another idea is data synthesis. Speciﬁcally,\n[15] used a text-to-speech system while [16] used a pronun-\nciation dictionary and duration information to generate addi-\ntional inputs given the external text, which are then used to\ntrain a correction model [15] or another encoder [16]. Our\nproposed approach uses external text data without involving\nexternal systems, such as text-to-speech.\nIn natural language processing, exploiting text corpora to\nimprove E2E systems is also widely used. A popular ap-\nproach is to use a text corpus to pre-train entire E2E network\n[17, 18]. Such techniques are only applicable for tasks where\nboth input and output are in text format. Another approach is\nto pre-train only the decoder by simply removing the encoder\n[17, 19]. This is equivalent to zeroing out the context vector\n[12], which introduces a mismatch as discussed in Section 2.\nThe idea of separating the decoder from encoder output\nhas been introduced in image captioning research community\n[13]. To the best of our knowledge, this work is the ﬁrst at-\ntempt applying it on the ASR task.\n5. EXPERIMENTAL SETUP\n5.1. Corpora\nWe conduct experiments on LDC2005S15, which is the\nHKUST Mandarin Telephone Speech [20], and the National\nDatasets\nHKUST IMDA\n#utts Duration\n(hours) #utts Duration\n(hours)\nLabelled data (P) 22,500 20.2 15,000 20.6\nExternal text (T ) 158,605 - 1,547,399 -\nValidation 5,457 4.88 16,144 21.2\nTest 5,151 4.75 5,589 7.6\nTable 1. Data division for the HKUST and NSC corpora.\nSpeech Corpus (NSC) [21], which is a Singapore English\nmicrophone data set.\nThe HKUST corpus consists of 171.1 hours of Mandarin\nChinese conversational telephone speech from Mandarin\nspeakers in mainland China. It is divided into a training set of\n166.3 hours, and a test set of 4.8 hours. We split the training\ndata into 3 subsets: the ﬁrst two subsets are Pand T in this\npaper, while the remaining subset is used for validation. The\ndetailed information of these data sets is presented in Table 1.\nFor the labelled data P, we perform speed-perturbation based\ndata augmentation [22]. We report Mandarin character error\nrate (CER) on the test set.\nThe NSC corpus consists of 2,172.6 hours of English read\nmicrophone speech from 1,382 Singaporean speakers. We ex-\ntract data of 6 speakers as testing data. Similar to the HKUST\ncorpus, we split the remaining data into 3 subsets for P, T\nand validation. The detailed data division is shown in Table\n1. We also perform data augmentation on the labelled dataP.\nWe report word error rate (WER) on the test set.\n5.2. E2E conﬁguration\nWe use the ESPnet toolkit [23] to develop our E2E models.\nWe use 80 mel-scale ﬁlterbank coefﬁcients with pitch as input\nfeatures. The encoder consists of 6 layers VGG [24] and 6\nlayers BLSTM, each has 320 units. In this paper, we used\nthe location-aware attention mechanism [4]. Characters and\nByte-Pair Encoding (BPE) (500 units) are used as output units\nfor HKUST and NSC corpora respectively.\nWe set the batch size for training dataPas B = 30. Since\nT has many more utterances than P, we set the batch size\nfor text data as B1 = 150 and 300 for HKUST and NSC re-\nspectively. The optimizer is the AdaDelta algorithm [25] with\ngradient clipping [26]. We used λ = 0.1 for both corpora.\nDuring decoding, we used beam width 30 for all conditions.\n6. EXPERIMENTAL RESULTS\n6.1. Independent LM architecture and updating strate-\ngies\nIn this section, we compare the vanilla architecture A1 to the\nproposed architecture A2 when they are trained using labelled\ndata P. We then compare two updating strategies described\nin Section 3.2 when external text data T is used. To update\nA1 with the external text data, we set the context vector by an\nall-zero vector [12] as mentioned in Section 2. The results are\npresented in Fig. 3. We have following observations.\n• The proposed A2 consistently outperforms A1 on both\nHKUST and NSC corpora. Particularly, A2 outper-\nforms A1 by 1.8% relative (from 49.2% to 48.3%)\nCER and 4.3% relative (from 39.9% to 38.2%) WER\non HKUST and NSC corpora respectively.\n• With external text data, Strategy 1 leads to signiﬁcant\nerror rate reduction for both architectures. For example,\non the A2 architecture, at α = 0.9 we observe 14.4%\nrelative (from 38.2% to 32.7%) WER reduction for the\nNSC corpus. We also observe that A2 outperforms A1\nfor all cases which indicates thatA2 beneﬁts more from\nthe external text.\n• Strategy 2 generally outperforms Strategy 1 when they\nare applied on A2. At α = 0.7, Strategy 2 (denoted\nas A2-Strategy2-0.7) achieves the best results on two\ncorpora, i.e. 43.8% CER and 29.5% WER (which are\n9.3% relative CER and 22.8% relative WER reduction\nover A2) on HKUST and NSC respectively. We will\nuse A2-Strategy2-0.7 for experiments in the next sec-\ntion.\n6.2. Interaction with external LM\nIn this section, we ﬁrst show the interaction between the pro-\nposed independent LM architecture and external LM [7, 14].\nSpeciﬁcally, we train a Recurrent Neural Network LM (RNN-\nLM) as a 1-layer LSTM with 1000 cells for both corpora,\nthen integrate the RNN-LM into inference process of A2 and\nA2-Strategy2-0.7. We also examine the effect of varying\namount of labelled data Pfrom 20 hours to 60 hours. For\nthis experiment, we only conduct on NSC corpus since the\nHKUST is relatively small. Results are reported in Fig 4.\nWe observe that the external RNN-LM improves A2-\nStrategy2-0.7 by 0.2% absolute CER and 4.5% absolute\nWER on 20 hours of HKUST and NSC corpora respectively.\nThe results indicate that our proposed approach beneﬁts from\nthe external LM. Additionally, we observe consistent im-\nprovements at different amount of Pon NSC, which demon-\nstrate that our proposed architecture works well under differ-\nent amount of labelled data.\n7. CONCLUSIONS\nWe introduced a new architecture that separates the decoder\nsubnet from the encoder output so that it can be easily updated\nusing an external text data. Experimental results showed that\nthe new architecture not only outperforms the vanilla archi-\ntecture when only labelled data is used, but also beneﬁts from\nthe external text data. We studied two strategies to update the\nE2E network and found that by pre-training the subnet with\n(a) Results on the HKUST corpus\n(b) Results on the NSC corpus\nFig. 3. Comparison between the vanilla architecture A1 (blue\ncolor), and the proposed architecture A2 (red color). Solid\nlines denote the two architectures trained using only Pwhile\ndashed and dotted lines indicate these architectures updated\nwith external text data T at different values of the factor α\n(see Eq. (8)) using two strategies described in Section 3.2.\nFig. 4. ASR performance with and without external LM at\ndifferent amount of labelled data.\nthe text data then ﬁne-tuning the entire E2E network using\nboth labelled and text data, we achieve the best results. Fur-\nther analyses also showed that the proposed architecture can\nbe augmented with an external LM for further improvement\nand can be generalized with different amount of labelled data.\n8. ACKNOWLEDGEMENTS\nThis work is supported by the project of Alibaba-NTU Singa-\npore Joint Research Institute.\n9. REFERENCES\n[1] William Chan, Navdeep Jaitly, Quoc V . Le, and Oriol\nVinyals, “Listen, attend and spell: A neural network for\nlarge vocabulary conversational speech recognition,” in\nProc. of ICASSP, 2016, pp. 4960–4964.\n[2] Dzmitry Bahdanau et al., “End-to-end attention-based\nlarge vocabulary speech recognition,” in Proc. of\nICASSP, 2016, pp. 4945–4949.\n[3] Chung-Cheng Chiu et al., “State-of-the-art speech\nrecognition with sequence-to-sequence models,” in\nProc. of ICASSP, 2018, pp. 4774–4778.\n[4] Jan Chorowski et al., “Attention-based models for\nspeech recognition,” in Proc. of NIPS, 2015, pp. 577–\n585.\n[5] Jan Chorowski and Navdeep Jaitly, “Towards better de-\ncoding and language model integration in sequence to\nsequence models,” in Proc. of INTERSPEECH, 2017,\npp. 523–527.\n[6] Rohit Prabhavalkar et al., “A comparison of sequence-\nto-sequence models for speech recognition,” in Proc. of\nINTERSPEECH, 2017, pp. 939–943.\n[7] Alex Graves and Navdeep Jaitly, “Towards end-to-end\nspeech recognition with recurrent neural networks,” in\nProc. of ICML, 2014, pp. 1764–1772.\n[8] Dario Amodei et al., “Deep speech 2 : End-to-end\nspeech recognition in english and mandarin,” in Proc.\nof ICML, 2015, pp. 173–182.\n[9] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, “Joint\nctc-attention based end-to-end speech recognition using\nmulti-task learning,” in Proc. of ICASSP, 2017, pp.\n4835–4839.\n[10] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R\nHershey, and Tomoki Hayashi, “Hybrid ctc/attention\narchitecture for end-to-end speech recognition,” IEEE\nJournal of Selected Topics in Signal Processing, vol. 11,\nno. 8, pp. 1240–1253, 2017.\n[11] Benedikt Pf ¨ulb, Alexander Gepperth, S. Abdullah, and\nA. Kilian, “Catastrophic forgetting: still a problem for\ndnns,” CoRR, vol. abs/1905.08077, 2019.\n[12] Yu-An Chung, Yuxuan Wang, Wei-Ning Hsu, Yu Zhang,\nand R. J. Skerry-Ryan, “Semi-supervised training for\nimproving data efﬁciency in end-to-end speech synthe-\nsis,” CoRR, vol. abs/1808.10128, 2018.\n[13] J. Lu, C. Xiong, D. Parikh, and R. Socher, “Knowing\nwhen to look: Adaptive attention via a visual sentinel for\nimage captioning,” in Proc. of CVPR, 2017, pp. 3242–\n3250.\n[14] Takaaki Hori, Jaejin Cho, and Shinji Watanabe, “End-\nto-end speech recognition with word-based rnn lan-\nguage models,” in Proc. of HLT, 2018, pp. 389–396.\n[15] Jinxi Guo, Tara N. Sainath, and Ron J. Weiss, “A\nspelling correction model for end-to-end speech recog-\nnition,” in Proc. of ICASSP, 2019, pp. 5651–5655.\n[16] Adithya Renduchintala, Shuoyang Ding, Matthew\nWiesner, and Shinji Watanabe, “Multi-modal data aug-\nmentation for end-to-end asr,” in Proc. of INTER-\nSPEECH, 2018, pp. 2394–2398.\n[17] Andrew M Dai and Quoc V Le, “Semi-supervised se-\nquence learning,” in Proc. of NIPS, 2015, pp. 3079–\n3087.\n[18] Kaitao Song et al., “MASS: masked sequence to se-\nquence pre-training for language generation,” in Proc.\nof ICML, 2019, pp. 5926–5936.\n[19] Prajit Ramachandran, Peter J. Liu, and Quoc V . Le, “Un-\nsupervised pretraining for sequence to sequence learn-\ning,” in Proc. of EMNLP, 2017, pp. 383–391.\n[20] Yi Liu, Pascale Fung, Yongsheng Yang, Christopher\nCieri, Shudong Huang, and David Graff, “Hkust/mts:\nA very large scale mandarin telephone speech corpus.,”\nin Proc. of ISCSLP, 2006, pp. 724–735.\n[21] Jia Xin Koh et al., “Building the singapore english na-\ntional speech corpus,” in Proc. INTERSPEECH 2019,\n2019, pp. 321–325.\n[22] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and San-\njeev Khudanpur, “Audio augmentation for speech recog-\nnition,” in Proc. of INTERSPEECH, 2015, pp. 3586–\n3589.\n[23] Shinji Watanabe et al., “Espnet: End-to-end speech pro-\ncessing toolkit,” in Proc. of INTERSPEECH, 2018, pp.\n2207–2211.\n[24] Takaaki Hori, Shinji Watanabe, Yu Zhang, and William\nChan, “Advances in joint ctc-attention based end-to-end\nspeech recognition with a deep cnn encoder and rnn-\nlm,” in Proc. of INTERSPEECH, 2017, pp. 949–953.\n[25] Matthew D. Zeiler, “Adadelta: An adaptive learning rate\nmethod,” CoRR, vol. abs/1212.5701, 2012.\n[26] Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-\ngio, “Understanding the exploding gradient problem,”\nCoRR, vol. abs/1211.5063, 2012."
}