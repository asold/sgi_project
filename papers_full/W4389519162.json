{
  "title": "Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding",
  "url": "https://openalex.org/W4389519162",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2167101309",
      "name": "Bram van Dijk",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A4286840948",
      "name": "Tom Kouwenhoven",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A2023187090",
      "name": "Marco Spruit",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science",
        "Leiden University Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Max Johannes van Duijn",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385570173",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4319301677",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W4210489638",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4322006551",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W2082513494",
    "https://openalex.org/W3120325630",
    "https://openalex.org/W2171847059",
    "https://openalex.org/W3091868446",
    "https://openalex.org/W4221161799",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W3000621992",
    "https://openalex.org/W2316693467",
    "https://openalex.org/W4376583107",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W1997802248",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4280543132",
    "https://openalex.org/W4303579584",
    "https://openalex.org/W4301357669",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285242720",
    "https://openalex.org/W4283398191",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4383058631",
    "https://openalex.org/W1494996288",
    "https://openalex.org/W4312143890",
    "https://openalex.org/W4321074112",
    "https://openalex.org/W4301594491",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W2955088691",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4385571329",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4385571695",
    "https://openalex.org/W4225479391",
    "https://openalex.org/W4309419356",
    "https://openalex.org/W389281733",
    "https://openalex.org/W3196304422",
    "https://openalex.org/W4310923406",
    "https://openalex.org/W4378499145",
    "https://openalex.org/W3205553525",
    "https://openalex.org/W4362597819",
    "https://openalex.org/W4291991132",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3008252655",
    "https://openalex.org/W4244083614",
    "https://openalex.org/W4223947928",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W1495375933",
    "https://openalex.org/W3199748991",
    "https://openalex.org/W4319653860",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2013583778",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W4324108774",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W4200182652",
    "https://openalex.org/W4330337479"
  ],
  "abstract": "Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of 'real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12641–12654\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models: The Need for Nuance in Current Debates and a\nPragmatic Perspective on Understanding\nBram van Dijk1, Tom Kouwenhoven1, Marco Spruit1,2, and Max van Duijn1\n1Leiden Institute of Advanced Computer Science\n2Leiden University Medical Centre\n{b.m.a.van.dijk, t.kouwenhoven, m.r.spruit, m.j.van.duijn}\n@liacs.leidenuniv.nl\nAbstract\nCurrent Large Language Models (LLMs) are\nunparalleled in their ability to generate gram-\nmatically correct, fluent text. LLMs are ap-\npearing rapidly, and debates on LLM capacities\nhave taken off, but reflection is lagging behind.\nThus, in this position paper, we first zoom in\non the debate and critically assess three points\nrecurring in critiques of LLM capacities: i)\nthat LLMs only parrot statistical patterns in the\ntraining data; ii) that LLMs master formal but\nnot functional language competence; and iii)\nthat language learning in LLMs cannot inform\nhuman language learning. Drawing on empir-\nical and theoretical arguments, we show that\nthese points need more nuance. Second, we\noutline a pragmatic perspective on the issue of\n‘real’ understanding and intentionality in LLMs.\nUnderstanding and intentionality pertain to un-\nobservable mental states we attribute to other\nhumans because they have pragmatic value:\nthey allow us to abstract away from complex\nunderlying mechanics and predict behaviour ef-\nfectively. We reflect on the circumstances under\nwhich it would make sense for humans to simi-\nlarly attribute mental states to LLMs, thereby\noutlining a pragmatic philosophical context for\nLLMs as an increasingly prominent technology\nin society.\n1 Introduction\nThe performance of Large Language Models\n(LLMs) has recently reached high levels (see e.g.\nBommasani et al., 2021; Mahowald et al., 2023).\nLLMs are deep neural networks with a Transformer\narchitecture (Vaswani et al., 2017), trained to pre-\ndict masked words from context, using massive text\ndatasets.1 During training, LLMs learn to represent\n1Some LLMs additionally benefit from further fine-tuning,\ne.g. reinforcement learning from human feedback (Christiano\net al., 2017). Since evidence is emerging that most of LLMs’\ncapabilities are learned during pre-training (Gudibande et al.,\n2023; Ye et al., 2023; Zhou et al., 2023a), we abstract away\nfrom this aspect in this paper.\ninput syntactically in hierarchical form, and they\nalso learn semantic relations (Rogers et al., 2021),\nwhich are useful features in summarising, question-\nanswering, and translating text. Examples of recent\nLLMs are LLaMA (Touvron et al., 2023), GPT-3 &\n4 (Brown et al., 2020; OpenAI, 2023), and PaLM\n(Chowdhery et al., 2022).\nLLMs have sparked a lot of debate, inside and\noutside academia, around the question what their\nsuccesses and failures say about linguistic capac-\nities in AI systems, but also in humans. In the\nfirst part of this paper, we scrutinize three key\npoints recurring in arguments by experts criti-\ncal of LLM capacities (e.g. Bender and Koller,\n2020; Bender et al., 2021; Browning and LeCun,\n2022; Marcus, 2022; Shanahan, 2022; Mitchell and\nKrakauer, 2022; Bisk et al., 2020; Floridi, 2023;\nMahowald et al., 2023), although there are also\nexperts who are more optimistic on this matter\n(e.g. Sahlgren and Carlsson, 2021; Agüera y Ar-\ncas, 2022a,b; Berger and Packard, 2022; Cerullo,\n2022; Sejnowski, 2022; Piantadosi and Hill, 2022;\nPiantadosi, 2023). These points are:\n1. that all LLMs can do is predict next words;\n2. that LLMs can only master formal as opposed\nto functional language competence;\n3. that language learning in LLMs cannot inform\nhuman language learning.\nIn the first part of this paper, we aim to nuance these\npoints and show that they are hard to maintain in\nthe face of empirical work on LLMs and theoretical\narguments. In the second part, this leads us to de-\nvelop a pragmatist perspective on LLMs, for which\nwe draw on work by Daniel Dennett, Richard Rorty,\nand others. ‘Real’ language understanding and in-\ntentionality consist of attributions of unobservable\nmental states, that humans make on the basis of\nobservable behaviour. We do so because this has\npragmatic value: it simplifies complex underlying\nbiophysical processes and allows us to predict fu-\nture behaviour. Instead of asking whether LLMs\n12641\nhave ‘real’ understanding and intentionality, we\nask under what circumstances regarding LLM be-\nhaviour and their role in society, it is reasonable\nfor humans to make mental models of LLMs that\ninclude capacities like understanding and intention-\nality.\nIn sum, our aim is to contribute to a more re-\nalistic framework for understanding LLMs within\nacademia and beyond, which is better grounded in\nempirical and philosophical work. Since LLMs’\nimpact on research and society likely increases\nin the future, properly understanding them is key.\nStill, although we defuse various critiques of LLMs\nin this paper, it is not our purpose to advocate their\ndeployment without ongoing reflection on their im-\nplications. Examples include their environmen-\ntal impact (Bender et al., 2021), biases (Lucy and\nBamman, 2021), problems for educators (Sparrow,\n2022), and ethical issues in adding human feed-\nback (Perrigo, 2023), but these issues go beyond\nthe scope of this paper.\n2 Key points in debates on LLMs\nIn this section, we qualify three key points from\nthe debate regarding LLM capacities by drawing\non theoretical and empirical work.\n2.1 All LLMs can do is next word prediction\nShanahan (2022) claims that whenever we prompt\na LLM, for example with ‘The capital city of the\nNetherlands is ...’, we actually ask ‘Given the sta-\ntistical patterns you learned from dataset Y during\ntraining, what word is most likely to follow the pro-\nvided sequence?’, where the answer is likely ‘Ams-\nterdam’. According to this assumption, this is all\nthere is to it; we should not speak about the model’s\npotential topographical knowledge, nor should we\nsay that the model understands the question in any\nway comparable to how humans understand it. We\ncan see similar claims in Bender et al. (2021); Mar-\ncus (2022); Chomsky (2023); Floridi (2023), and\nin weaker form in Mitchell and Krakauer (2022).\nAlthough it is true that the good performance of\nLLMs on many tasks stems from a simple train-\ning objective, which is predicting masked words\nfrom context, we argue that this point overlooks the\ncomplex ways in which LLMs are able to represent\ninformation. During training, LLMs induce vari-\nous semantic and syntactic features that the model\nuses internally to represent the input in a manner\nthat can be extracted, for example, by analysing\nmodel weights or patterns of neuronal activation.\nAn example regarding syntax is that LLMs are able\nto hierarchically represent input (Hewitt and Man-\nning, 2019; Manning et al., 2020; Rogers et al.,\n2021; Mahowald et al., 2023). That is, they are\ncapable of internally parsing the example into syn-\ntactic chunks, such that the prepositional phrase\n(‘of The Netherlands’) provides information about\nthe noun phrase (‘The capital city’), which con-\nstitutes a clue to the answer (‘Amsterdam’). In\naddition, regarding semantics, the vector represen-\ntations of words that neural networks induce are\nshown to be context-sensitive and rich enough to\ncapture conceptual relations in line with human\njudgements (Reif et al., 2019; Grand et al., 2022;\nPiantadosi and Hill, 2022). Moreover, in our exam-\nple, ‘The Netherlands’ and ‘Amsterdam’ are likely\ngeometrically related in the vector space of a LLM,\nwhich provides a further clue regarding the answer.\nOur syntactic and semantic examples here are\nnot necessarily the way LLMs represent the rele-\nvant linguistic information; it is not trivial to extract\nrepresentations from LLMs (Rogers et al., 2021).\nThe point is rather that LLMs are capable of fur-\nther representing input in various ways that are\nnot reducible to either their training data or objec-\ntive (Piantadosi and Hill, 2022). While they are\nnot explicitly trained to represent input hierarchi-\ncally, or represent semantic relations, such proper-\nties emerge while becoming better at their relatively\nsimple stochastic training objective (Manning et al.,\n2020). On second thought, this should not be too\nsurprising, given that there is a lot of linguistic in-\nformation ‘hidden’ in the web text used to train\nLLMs, which LLMs (partially) reconstruct. Note\nthat this argument does not require LLMs to ‘really’\nunderstand or know their inputs or representations.\nThe assumption that LLMs can only echo sta-\ntistical regularities is important to qualify. So-\ncalled ‘underclaiming’ (downplaying what LLMs\ndo and learn), resonates more broadly in the aca-\ndemic sphere, which could hinder studying how\nLLMs work in detail (Bowman, 2022b), and ex-\nploring whether they are useful for studying ques-\ntions about human language usage. The assump-\ntion likely stems from the idea that probabilistic\nmodelling of language may successfully simulate\nor approximate linguistic facts (i.e. generate co-\nherent language), but does this via such a different\nroute that it cannot provide any further insight into\nhuman language (Norvig, 2012; Piantadosi, 2023).\n12642\nAlthough it may seem on the surface that LLMs are\njust language simulation machines, this overlooks\nall the linguistic complexity that is stored in the\nweights that are updated during training, with word\nprediction providing a powerful supervision signal.\nBeyond the potential impact of ‘underclaiming’ in\nacademic debates (Bowman, 2022a), this assump-\ntion could reinforce simplistic views of what LLMs\nare and why they are useful in society at large.\n2.2 LLMs can master formal aspects of\nlanguage, but not its function\nThe distinction between formal and functional lan-\nguage competence we draw on here stems from Ma-\nhowald et al. (2023): formal language competence\nconcerns employing information about linguistic\nrules and patterns in producing coherent output,\nwhereas functional language competence draws on\nfurther cognitive capacities, such as formal reason-\ning, intentional reasoning, and situation modelling.\nThe authors paraphrase this difference as the dif-\nference between being good at language and be-\ning good at thought; in their view, LLMs master\nlanguage but not thought. They motivate this dis-\ntinction with the finding that the two competences\nrecruit independent brain circuits, and discuss per-\nsons with aphasia as a concrete example: they can\nhave limited formal linguistic competence, yet still\nbe able to compose music, solve logic puzzles, rea-\nson about other persons’ mental states, thus, lever-\nage thought independently of language. Whether\none buys into its neural grounding or not, the dis-\ntinction between formal and formal language com-\npetence as such is a useful one to make, and we see\nsimilar oppositions in Bender and Koller (2020),\nwhere the distinction is made between LLMs’ mas-\ntery of linguistic form as opposed to extra-linguistic\nmeaning, and in Bisk et al. (2020); Browning and\nLeCun (2022); Floridi (2023).\n2.2.1 Disentangling language and thought\nHere we do not claim that LLMs have thought that\ncan be meaningfully separated from language, as\nwe are agnostic on this matter, but we question\nsome of the methods currently used to disclose\nthought.\nFirst of all, whereas persons with aphasia can\nbe tested on their capacity to employ thought in a\nway that is clearly independent of language (e.g.\ncomposing music), for LLMs this is not possi-\nble. For example, for the common sense reason-\ning and intentional reasoning (a.k.a. Theory of\nMind/ToM) humans do, two vital functional capac-\nities, benchmarks inevitably rely on presenting a\nparticular (social) situation using linguistic prompts\n(e.g. Collins et al., 2022; Creswell et al., 2022; Sap\net al., 2022; Binz and Schulz, 2023; Borji, 2023;\nKosinski, 2023; Ullman, 2023; van Duijn et al.,\n2023). Implicitly or explicitly, such works draw on\nthe assumption that in LLMs, there must be a dis-\ntinction between thought as internal symbolic sys-\ntem representing abstract relations, and language\nas a mapping between these representations and\ntheir outward expression in text. Although this is\nnot unreasonable to think, given that LLMs have\nmany emergent capacities for which they were not\nexplicitly trained (Section 2.1), currently we do\nnot know how much formal linguistic information\nLLMs leverage when performing such tasks. LLM\noutput could, for example, be the result of specific\nsemantic or syntactic relations with the input, while\nit seems unlikely that a human would approach\nsuch tasks in the same way. Thus, in assessing\nthought in LLMs, language and thought are con-\nfounded.\nThis issue has an analogy in testing thought in\nchildren. When for instance testing ToM, confound-\ning factors are always present, as the myriad tests\nof ToM that exist and the different modalities they\nsolicit (vision, speech, text) illustrate (Quesque and\nRossetti, 2020). General language and memory\nabilities of children are typically controlled with ad-\nditional tasks (Milligan et al., 2007); few tests exist\nthat rely on language alone (Beaudoin et al., 2020).\nStill, many seemingly superficial aspects shape per-\nformance on such tests, such as how questions are\nphrased (Siegal and Beattie, 1991; Beaudoin et al.,\n2020). The influence of superficial linguistic arte-\nfacts of tests can be controlled to some extent when\nconducting ToM tests with LLMs, for example, pre-\nventing memorization by rewriting tests such that\nthey are not in the training data (e.g. Shapira et al.,\n2023; Kosinski, 2023; van Duijn et al., 2023), but\nthis is only the beginning of disentangling language\nand thought in LLM output.\nMoreover, disentangling language and thought is\ndifficult, because for many cognitive test we have\nan idea of what the test operationalises, but not\nwhen they are used in the context of LLMs. For the\nToM context, one hallmark test is the ‘unexpected\ncontents’ test (Perner et al., 1987), where a Smar-\nties box with unexpected contents (e.g. a pencil) is\nshown to a child. The child is asked what a friend,\n12643\nunfamiliar with the box, would think its contents\nare, thereby asking it to manage two conflicting\nbeliefs: the false belief imputed to the friend, and\nits own belief about the box’ contents. This conflict\nmanagement, as instantiation of ToM ability, is ar-\nguably what the test operationalises, and something\nhumans know from subjective experience, which\nmakes it easier to understand what was measured\nwhen such tests are used on humans. Yet, this is far\nless clear when using such tests on LLMs.\n2.2.2 Thought is a continuum\nHere we assume, for the sake of argument, that\nwe can separate thought from language ability in\nLLMs with cognitive tests (which we questioned in\nSection 2.2.1). We further reflect on how various\ntests are currently being used to deny or affirm\nthought in LLMs. Again, we do not argue that\nLLMs have or lack thought here, but rather that we\nshould suspend conclusions, based on the issues\nraised below.\nQuite some cognitive tests currently employed\nwith LLMs make assumptions about thought that\nneed qualification. Many are designed so that\na LLM can only fail or succeed on them (e.g.\n(part of) tests employed by Sap et al., 2022; Borji,\n2023; Bubeck et al., 2023; Kosinski, 2023; Shapira\net al., 2023; Ullman, 2023). Yet, thought capac-\nity is better understood as a continuum (Beaudoin\net al., 2020; Sahlgren and Carlsson, 2021). That\nis, thought capacity is unequally distributed in hu-\nmans; people who excel in logic, may still be horri-\nble composers, or struggle to recognise other per-\nsons’ intentional states.\nIn addition, we are typically much more lenient\ntowards failure in exercising thought. In daily con-\ntexts, humans are generally susceptible to a host of\nmisplaced heuristics and formal errors (Haselton\net al., 2015; Dasgupta et al., 2022), but we gen-\nerally do not conclude from this that humans do\nnot have unique thought capacity. Sahlgren and\nCarlsson (2021) make a similar claim for language\nunderstanding: different language users are good at\ndifferent things, at different times, in different situ-\nations. Thus, it is unsurprising that disagreement\nexists among NLP-scholars about proper opera-\ntionalisations of various tasks in Natural Language\nUnderstanding, such as Natural Language Infer-\nence (Subramonian et al., 2023), a disagreement\nthat can also be anticipated for other cognitive tests.\nTo make our evaluations of thought in LLMs\nas compelling as possible, we can employ more\nsophisticated measures, and aim for more nuance\nin the interpretations of results, before we can deny\n(or affirm) thought in LLMs. Instead of focusing\nmerely on (average) success or failure, knowing\nthat a LLM has, for example, 51% confidence in\na wrong answer is already more informative than\njust knowing the LLM erred. Fortunately, work on\nmore nuanced evaluations of thought in LLMs is\nemerging (e.g. Collins et al., 2022; Binz and Schulz,\n2023). Alternatively, we could evaluate the model’s\nintermediate reasoning steps in solving a complex\nreasoning task, besides only the answer, when em-\nploying Chain-of-Thought prompting (Wei et al.,\n2022). Evidence is emerging that in the context of\nToM tests, more sophisticated prompting improves\nperformance (Moghaddam and Honey, 2023).\nFrom a methodological perspective, testing\nthought in LLMs and humans differs a lot because\nthe entities at issue differ a lot. Yet, we can improve\nthe comparability of testing. A single output of a\nLLM on a single test item likely does not yield a\ngood estimate of its capacities; in testing humans,\nwe typically ask multiple humans to do the same\nitem. Thus, we could for example initialize LLMs\nwith slightly higher temperature values multiple\ntimes on the same test item, to get a fuller view\non what LLMs can, and to obtain a larger sample\nof responses on which statistical tests are possible.\nAlthough we know that low temperatures settings\nmake models deterministic, not much evaluation\nof slightly higher temperature settings in relation\nto performance has been done, with the exception\nof Moghaddam and Honey (2023). In addition, we\nknow from work on knowledge extraction in LLMs,\nthat paraphrasing a particular input improves model\nperformance in retrieving knowledge and relations\n(Jiang et al., 2020). Paraphrasing test items is not\nonly a way to increase model performance, but\ncould also provide a way to increase our confidence\nin our estimates of thought capacity as indicated by\nLLM performance on multiple paraphrased items.\nLastly, from a more general perspective, failure\nof a LLM on a cognitive test does not imply that\nthe system does not have the mappings required\nto do the test; the tests could also be less suited to\nretrieve them (Bommasani et al., 2021).\n2.3 Language acquisition in LLMs cannot\ninform human language acquisition\nBisk et al. (2020) argue that, since children can-\nnot acquire a language by merely listening to the\n12644\nradio, it is likewise wrong to expect that LLMs\ncan acquire language by purely ingesting text from\nthe internet; similar claims are offered in Bender\nand Koller (2020); Chomsky (2023). This point\nrelies on the presumed poverty of ‘extralinguistic’\ninformation in the train data of LLMs. In language\nacquisition, children draw not only on linguistic\ninput but also on sense perception (e.g. seeing and\ntouching the world), motor experience (e.g. mov-\ning objects), and interaction with caretakers (e.g.\nfeedback). Also, children receive far less language\ninput compared to LLMs (Warstadt and Bowman,\n2022). Thus, if language acquisition in children\nand LLMs is so different from the start, it seems\nlanguage acquisition in LLMs cannot inform lan-\nguage acquisition in humans.\n2.3.1 LLMs as useful distributional models\nLLMs and children evidently differ a lot. Yet, as\nSahlgren and Carlsson (2021) formulate, LLMs\nare theoretically and practically our current ‘best\nbet’ for machines to acquire language understand-\ning, given the empirical work documenting LLM\nproficiency in many language tasks (see e.g. Bom-\nmasani et al., 2021; Wei et al., 2023). Like any sci-\nentific model they are wrong in some respects, but\nthey seem our current best distributional models to\nstudy specific aspects of language acquisition.\nFor example, Chang and Bergen (2022) use\nBERT and GPT-2 as distributional agents that ex-\nclusively learn from word co-occurrence statistics.\nThey employ a.o. word frequency, lexical class and\nword length, as known predictors of word acqui-\nsition in children, and predict word acquisition in\nLLMs and children to gauge the extent to which\nthese known effects in children can be accounted\nfor by statistical learning mechanisms. Chang and\nBergen (2022) show that language acquisition in\nLLMs and children differs in key respects (LLMs\nare more frequency-driven), but are also similar\n(learning in both takes longer for words embedded\nin longer utterances). As the authors note, distri-\nbutional models can be used in similar fashion to\nexplore the extent to which acquisition of seman-\ntics or syntax in children can be accounted for by\nstatistical learning.\nCevoli et al. (2023) provide another example\nby unravelling lexical ambiguity with BERT. The\nauthors show that psychological theories positing\ncomplex mechanisms for representing ambiguity,\nare not necessary to explain how such representa-\ntions are acquired, since they can be decoded from\ndistributional information in text. This illustrates\nanother key role LLMs can play in language ac-\nquisition: as Warstadt and Bowman (2022) note,\nLLMs in ablation studies can provide ‘proof of\nconcept’ whether target linguistic knowledge (e.g.\nverb-subject agreement in triply embedded clauses)\nis learnable in an ablated environment (e.g. with-\nout triply embedded clauses in the training data).\nSuch studies are helpful in identifying sufficient\nconditions for obtaining specific linguistic knowl-\nedge in language acquisition. Thus, the examples\nmentioned above echo the broader point made by\nvarious scholars that we should see LLMs as dis-\ntributional learners that show what linguistic phe-\nnomena are in principle learnable from statistical\ninformation in text (Contreras Kallens et al., 2023;\nEvanson et al., 2023; Wilcox et al., 2023).\nWarstadt and Bowman (2022) note that in lan-\nguage acquisition contexts, LLMs need to be less\nadvantaged to humans in one key aspect: the\namount of training data. That is, they need to be\nmade more ecologically valid. The latter is indeed\nimportant, and fortunately, work is emerging which\nshows that it is possible to train LLMs with more\nrealistic amounts of data, that at the same time per-\nform equally well in predicting human neural and\nbehavioural patterns as models trained with large\ndatasets (e.g. Hosseini et al., 2022; Wilcox et al.,\n2023). Yet, it is equally remarkable that LLMs have\nbecome so successful, despite being very disadvan-\ntaged as well (no multimodal input, no feedback in\nlearning, no sensorimotor input). We argue that it\nis not obvious how we should weigh such disadvan-\ntages and advantages in LLMs’ language learning.\nLLMs make wrong assumptions about language\nacquisition in key respects, but all scientific models\ndo this (Box, 1979; Baayen, 2008), while this does\nnot render such models useless: they can provide\na lower bound on what linguistic phenomena are\nlearnable in principle from distributional informa-\ntion.\n2.3.2 How poor is training data?\nHere we discuss the assumption mentioned in Sec-\ntion 2.3 that data used to train LLMs lacks ex-\ntralinguistic information required in language ac-\nquisition, by considering what extralinguistic in-\nformation LLMs learn to represent during training.\nSince humans use language to do a variety of things\n(Sahlgren and Carlsson, 2021), such as providing\nexplanations, describing all sorts of objects and\nprocesses, entertaining and convincing others, it is\n12645\nnatural to assume that LLMs are able to recover\nsome of the knowledge about e.g. properties of\nobjects in the world, communicative intents, and\nusers’ mental states. Recent work shows that LLMs\nare able to represent conceptual schemes for worlds\n(e.g. for direction) they have never observed (Pa-\ntel and Pavlick, 2022), thus it seems that LLMs\nhave a sufficiently rich conceptual structure to de-\ncode at least some of the extralinguistic information\npresent in text, as a surrogate grounding. Similarly,\nAbdou et al. (2021) show that internal represen-\ntations of LLMs show a topology of colours that\ncorrespond to human perceptual topology. In addi-\ntion, evidence emerges that LLMs are able to repre-\nsent communicative intents behind texts (Andreas,\n2022), and the ways LLMs represent semantic fea-\ntures of various object concepts aligns with humans\n(Hansen and Hebart, 2022). Moreover, studies in\nwhich LLMs are trained and tested on synthetic\ntasks, provide an even stricter scenario for testing\nwhether LLMs are able to decode emergent prop-\nerties from simple input. LLMs trained on simple\ninput such as lists of player moves in a board game,\nprove able to recover emergent properties such as\ngame rules, valid future moves, and board states\n(Li et al., 2022). For additional examples of ex-\ntralinguistic grounding, see Bowman (2023).\n3 A pragmatic philosophy of LLMs\nThis section sketches a more general, pragmatist\nphilosophical context for LLMs. Although LLMs\nare prominent in academia and society, philosophi-\ncal reflection is lagging behind. This is lamentable,\ngiven that LLMs and the way they are deployed\nraise pressing philosophical questions. Here we\ndevelop a pragmatist view on LLMs with the fol-\nlowing claims that we will motivated with reference\nto philosophical pragmatism:\n1. All three key points from the debate about\ncapacities of LLMs discussed above ultimately re-\nvolve around the issue of ‘real’ understanding and\nintentionality, but fail to address what that means;\n2. Once we try to explain what ‘real’ under-\nstanding and intentionality are, we find that these\n(and mental states more generally) are not acces-\nsible in others we interact with, irrespective of\nwhether they are humans or other kinds of systems;\n3. Attributing mental states to others has\nforemost pragmatic value, in that they help us to\nabstract underlying complexity away, predict be-\nhaviour, and obtain goals in the world;\n4. Given the increasing prominence of LLMs,\ninteracting with them in terms of mental state attri-\nbution will likely become more common, yet lacks\na comprehensive theory;\n5. This practice is fully explainable from a prag-\nmatist perspective, although in different commu-\nnities, such as the scientific community, different\npragmatist values may play a role, that makes this\npractice less acceptable for this community.\n3.1 Invoking ‘real’ understanding\nVarious scholars have claimed that LLMs are inca-\npable of ‘really’ understanding language and using\nintentionality like humans (e.g. Bender and Koller,\n2020; Bishop, 2021; Browning and LeCun, 2022;\nFloridi, 2023; Mahowald et al., 2023). Indeed, it\nseems that the critiques of LLMs as autocomplete\nsystems that do not know how language functions\nin the world, or as language learners that cannot\nlearn by drawing on such functions, implicitly in-\nvoke this claim. Still, in such critical works it is sel-\ndom made explicit what ‘real’ understanding or in-\ntentionality amounts to. These works often revolve\naround John Searle’s ‘Chinese Room’ thought ex-\nperiment (Searle, 1980). We illustrate this with the\nfollowing quote from Bender and Koller (2020):\n\"This means we must be extra careful\nin devising evaluations for machine un-\nderstanding, as Searle (1980) elaborates\nwith his Chinese Room experiment: he\ndevelops the metaphor of a “system” in\nwhich a person who does not speak Chi-\nnese answers Chinese questions by con-\nsulting a library of Chinese books accord-\ning to predefined rules. From the outside,\nthe system seems like it “understands”\nChinese, although in reality no actual un-\nderstanding happens anywhere inside the\nsystem.\" (p. 5188)\nThe argument presented in the quote is that, for\nany system, being able to deliver the expected out-\nput on a range of inputs is insufficient for having\n‘actual understanding’, where it is important to note\nthat the perspective of anyone interacting with the\nsystem is ‘from the outside’. The point of this\nthought experiment is that, although the idea of\n‘real’ understanding is implied, it is not explained,\nwhich makes the argument incomplete.\n12646\n3.2 Explaining ‘real’ understanding\nThe Chinese room argument appeals to a situation\nwhere we would grant that the system understands\nChinese: if the human in the system understands\nChinese. That is, this human would need to have\na set of mental states involving knowledge, be-\nliefs, and intentions, such that in producing out-\nput, the human does not draw on predefined rules,\nbut rather on its knowledge of Chinese, beliefs\nabout the desired output, and further communica-\ntive intent. This would constitute an example of\nwhat is meant by a human having ‘real’ understand-\ning. Nonetheless, this explanation cannot save the\nthought experiment as presented above, since it\nmakes no difference for anyone interacting with the\nsystem if we would replace the rule-abiding human\nwith the human as full mental agent, since from\nthe outside, there’s only the system’s behaviour to\nobserve, which does not change.\nThis distinction between what is observable, e.g.\nbehaviour, and what is inaccessible or unobserv-\nable, e.g. mental states, is a distinction known in\nthe philosophy of science (see e.g. Van Fraassen,\n1980; Churchland, 1985; Fodor, 1987), but it is\nalso at work in the empirical domain. For example,\nas Rabinowitz et al. (2018) note with reference to\nDennett (1991), we make mental models of others’\ninternal states that are inaccessible from the out-\nside, and that make ‘little to no reference’ to the\nunderlying mechanisms of the agent that produces\nthe observed behaviour. Our point here is that we\nare always confined to observable behaviours of\nother agents, regardless of whether they are hu-\nmans or machines. Whenever we claim that ‘real’\nunderstanding and intentionality is lacking in some\nother agent, we make a claim about states that are\nin principle inaccessible from the outside.\n3.3 Pragmatic value\nWe are nevertheless fully entitled to make ‘mental\nmodels’ of other humans, that is, attribute to them\nbelieves, desires, and intentions, because this is\nuseful in everyday interaction: it has clear prag-\nmatic value. This point is perhaps best known in\nthe form worked out by Daniel Dennett as the ‘in-\ntentional stance’: by attributing mental states to\nother humans, we abstract away from their under-\nlying biophysical complexity, while still having a\nground for anticipating future behaviour (Dennett,\n1989). If we see a person running towards a bus\nstop, attributing the desire to catch the bus makes\nthe behaviour intelligible and allows us to predict\nfurther behaviour, e.g. waving to the bus driver.\nSimilarly, attributing the set of mental states that\nconstitutes ‘real’ language understanding to other\nhumans, makes their behaviour intelligible, and\nsmooths our social interactions. This pragmatic\nperspective is closely related to the idea that men-\ntal states are key concepts for humans that have a\nstrong social justification (Rorty, 2009); they help\na community to achieve its goals in the world, and\nthat is all the justification we need to use them. Ex-\nactly because attributing mental states to others has\nsuch clear pragmatic value, we have sufficient rea-\nson to take them seriously. From this perspective,\nit is counterproductive to adopt a behaviourist (i.e.\ndenying their importance or existence) or essential-\nist (i.e. accepting them only if there is evidence\nthat they are ‘real’) attitude towards mental states.\n3.4 Pragmatic value and LLMs\nWe can make a similar claim for LLMs, even\nthough humans and LLMs differ. With regard to ob-\nservable behaviour, humans can deploy more subtle\nand multimodal observable behaviours compared\nto LLMs, like tone of voice, facial expressions,\ngestures, even unconsciously. So the observable\nbehaviour that underlies our mental models of hu-\nmans is arguably much richer, which gives us more\ndetails to work with when attributing mental states\nto others. At the same time, we should acknowl-\nedge that the way we interact with LLMs is strik-\ningly different from the way we interacted with\nartificially intelligent systems before. Their lan-\nguage output is grammatically correct, fluent, and\ncritically, increasingly well adapted to context, user,\nand input. This is starting to challenge assumptions\nabout what it is to be human and what it is to be\na machine, and what it is to communicate as a hu-\nman with an intelligent system that communicates\nin many ways like a human would do (Guzman and\nLewis, 2020). The increasing sophistication of in-\nteraction has led to humans viewing such systems\nas distinct, social entities, and as a consequence,\nhumans are triggered even more to attribute mental\nstates to such systems (see e.g. Guzman and Lewis,\n2020; Stuart and Kneer, 2021).\nIn the context of LLMs, attributing mental states\nto LLMs has often been addressed as oversensitive\nanthropomorphisation, with our mental models be-\ning illusions ‘in the eye of the beholder’ (Bender\net al., 2021). Such critiques overlook that making\n12647\nmental models of intelligent systems can have clear\npragmatic value, in that they abstract away from the\nunderlying complexities of LLMs, and at the same\ntime help us to predict and explain their behaviour,\nand achieve goals in the world. Obviously there are\ncomplex systems for which making mental mod-\nels make less sense, e.g. for a Mars rover, where\nour goal of landing it on Mars is better served by\nphysical models. On the other hand, our interaction\nwith LLMs as complex systems, as we show with\nexamples below, is often best served by attribut-\ning mental states to them ‘as if’ they were socially\nintelligent in the way we think other humans are.\nOur mental model about what an LLM ‘knows’\nor ‘wants’, can allow us, among other things, to\ncommunicate our requests succinctly (‘Do you\nknow how to do X’ in prompting), explain errors\n(‘The system confuses X for Y’), formulate a next\nstep in interaction (‘The system now expects input\nX’), or gauge reliability of output (‘How strong\nis your belief that X?’). And this need not apply\nonly to our own interaction with LLMs, but is also\nrelevant for explaining LLM behaviour to other\nhumans. LLMs optimized for dialogue, (e.g. Chat-\nGPT, PaLM2-chat), increasingly enable this form\nof interaction that involves mental state language.\nThis development should not surprise us, given\nthat language is a tool that has evolved for commu-\nnicating and manipulating mental states to achieve\ngoals in the world (Clark, 1996; Tomasello, 2003),\nfor example resolving conflict and working to-\ngether. Similarly, in child development, language\ncompetence and the ability to reason about mental\nstates strongly overlap (for an overview see Milli-\ngan et al., 2007). Furthermore, scholars argue that\nchildren in learning word meanings (for example\nfor verbs of perception like ‘to look’) do not just\nlearn abstract sign-object mappings (that interlocu-\ntor X literally perceives object Y), but foremost\ntheir pragmatic effects, which is for children typi-\ncally directing an interlocutor’s attention to various\nconcrete objects (Enfield, 2023; San Roque and\nSchieffelin, 2019), and such forms of joint atten-\ntion are a precursor to ToM (Tomasello et al., 1995).\nIn a similar vein, current research that focuses on\nways to have LLMs ‘reflect’ on their ‘confidence’\nin their assertions, or on uncertainty in their input,\ncan be understood in terms of the pragmatic value\nthis has for LLM users, that in the normal world\nalso deal with uncertainty in information (Zhou\net al., 2023b; Kadavath et al., 2022).\nGiven the increasing prominence of LLMs in\nsociety, we can expect that making mental models\nof LLMs will become more common. 2 We can\nalready see some examples where LLMs are specif-\nically used to impersonate individuals, for example,\nhelpdesk service agents (Brynjolfsson et al., 2023),\ninfluencers (Lorenz, 2023), deceased beloved ones\n(Pearcy, 2023), virtual friends (Marr, 2023), and\npersonal assistants (Chen, 2023). These may strike\none as rather worrying examples, but such devel-\nopments could have pragmatic value for humans\nin that LLMs can give them a sense of relationship\nor consolation. This is not to say that we advocate\nsuch deployment of LLMs, as they have many un-\naddressed ethical implications, but rather that there\nare conditions imaginable, in which mental state\nattribution to LLMs is explainable, justified, and\nhas pragmatic value. Here we rather want to stress\nthat the larger role LLMs (in whatever future form)\nwill likely play in society, demands a theory of our\ninteractions with them that does not simplify our\nbehaviour to ‘anthromorphisation’.\n3.5 Pragmatic value and science\nWe want to emphasise that pragmatic values can\nbe different in different communities, since they\nmay have different goals in the world. In a scien-\ntific community that attempts to describe/explain\nLLMs and their purported cognitive capacities in\nmore detail than is typically required in daily life,\nresearchers may balk at attributing mental states to\nLLMs. Yet, they should not do so because LLMs\ndo not have any ‘real’ understanding and intention-\nality, as we saw that this claim misses the point.\nMental states are not intended as literal accounts of\nthe underlying complexity of humans or machines,\nand the subjective experience associated with ‘real’\nunderstanding is not something we can access from\nthe outside, and therefore deny outright in other\nentities.\nA better reason, grounded in the pragmatist per-\nspective we offer here, seems to be that mental\nmodels made in everyday interaction may allow\nus to explain and predict behaviour, but lack other\npragmatic values critical in the scientific commu-\nnity. If mental states are to play a role in a scientific\ndescription or explanation of LLMs, then they must,\nfor example, also cohere with other currently ac-\ncepted theories/models; offer an elegant or simple\n2Note that our account is not intended to be normative,\nin that we are not claiming that humans should make mental\nmodels of LLMs\n12648\nexplanation, have a large scope, etc. (Van Fraassen,\n1980). Such values are pragmatic, because they do\nnot primarily depend on the relation a theory has\nwith the observable world; there is, for example,\nno reason to think that the world must be elegant or\nsimple because our theories are, or that phenomena\nin the world cohere because our theories cohere.\nThe upshot is that attributing mental states to\nLLMs may not cohere well with empirical work\non mental states in other fields that map them to\npatterns of neuronal activity in the brain, for which\nneurons in LLMs currently constitute at best only a\nloose analogy. Or it may not cohere with more theo-\nretical work that holds that the possibility of mental\nstates in machines entails a category mistake (as\nintroduced by Ryle (1950)), as mental states are\nproperties of beings such as humans, which fun-\ndamentally differ from machines. By considering\npragmatic values at play in different communities,\nwe are able to explain why, on a general level, at-\ntributing mental models to LLMs can be explain-\nable and justifiable, but at the same time could be\nless acceptable in the scientific community that has\ndifferent pragmatic values.\n4 Discussion\nAlthough in Section 3 we discuss LLM capacities\nand mental states mostly at a fundamental level,\nour arguments are also relevant for engineers work-\ning on concrete systems that employ LLMs. Such\nsystems will always require reflection on under-\nstanding and mental states in humans and machines,\nwhich our pragmatic outlook can inform. Our argu-\nments are agnostic about the explicit taxonomies\nand frameworks of the mental, which engineers\nmay develop and employ in such systems, as it is\nthe system’s behaviour that the pragmatist is typ-\nically most interested in, and it can be realised in\nvarious ways. In the design of LLMs, no such tax-\nonomies or frameworks exist (Kosinski, 2023; Trott\net al., 2023), but it is possible that systems that do\nhave them manifest equally complex behaviour. In\na similar vein, we can imagine training scenarios\nthat include visual (or other multi-modal) input\nas a proxy for grounding denotations of words in\nthe world, which would also make LLM behaviour\nmore sophisticated, as disambiguating input is ar-\nguably simpler with an additional information chan-\nnel. Evidence is emerging that enriching LLMs\nwith vision modules as surrogate grounding allows\nsuch models to learn new words more efficiently\n(Ma et al., 2023).\nA related point is that, although a pragmatic ac-\ncount of mental states in humans abstracts away\nfrom their complex underlying biophysical corre-\nlates, pragmatism does not entail that there is no\npoint in trying to disclose such correlates scien-\ntifically, with the aim of opening the black box.\nA biophysical account of mental states may have\npragmatic values for a community of scientists (see\nSection 3.5), and also broader pragmatic value for\nsociety in that it can help us to, for example, treat\ndysfunctional mental states better. This biophysical\naccount resides at a different level of explanation,\nand does not necessarily conflict with pragmatic\naccounts of the mental in general. In the case of\nLLMs, the pragmatist has similarly no principal\nissues with trying to find out what patterns of (artifi-\ncial) neuronal activation are correlated with mental\nstate content in LLM input and output.\n5 Conclusion\nThe goal of this paper was to provide further reflec-\ntion on LLMs in two ways. First, we scrutinised\nthree key points surfacing in recurring critiques on\nLLMs, and found that on empirical and theoretical\ngrounds, these points need more nuance. Our con-\nclusions are that LLMs are more than exploiters of\nstatistical patterns; that we need better measures\nfor evaluating thought competence in LLMs before\nwe can draw conclusions; and that LLMs have a\nrole to play in language acquisition, as our current\nbest distributional models.\nSecond, we provided a philosophical context\nfor LLMs from a pragmatist perspective. An un-\nresolved question underlying various critiques of\nLLMs, is whether they have something like ‘real’\nlanguage understanding and intentionality. We ar-\ngued that whether we attribute unobservable mental\nstates to other entities, including the set that would\nconstitute ‘real’ language understanding and inten-\ntionality, depends on how much pragmatic value\nthis has to us, not on whether mental states are\nactual properties of the entities at issue.\nLLMs (in whatever future form) will become\nmore prominent in the years to come. We hope\nto have contributed to a better understanding of\nwhat LLMs can(not) do, as well as to a philosoph-\nically informed understanding of our interaction\nwith LLMs that is more than a story of mere an-\nthropomorphisation.\n12649\nLimitations\nIn this paper, we addressed LLMs as the set of large\nTransformer-based neural networks that are trained\nwith cloze tasks, using large text datasets. Still,\nthere is some variation in this set, as LLMs can\nhave different sizes, different architectures, train-\ning datasets, methods for further fine-tuning, and\nso on. Up to this point, it is typically the case\nthat larger LLMs trained with more data obtain the\nbest performance on a variety of tasks, which also\nmakes that such larger LLMs are overrepresented\nin evaluations of general LLM capacities. Ope-\nnAI’s flagship models like GPT-3 and ChatGPT are\nLLMs that frequently recur in tests (although the\nwork of e.g. Shapira et al. (2023) is an exception).\nIn addition, new models are appearing at a fast\npace, such as LLaMA (Touvron et al., 2023), Fal-\ncon (Penedo et al., 2023), and PaLM2 (Anil et al.,\n2023). It remains to be seen how these new models\nfare on various tests, such as those for cognition,\nbut they are fairly similar regarding their neural\nnetwork architecture, training data, and training ob-\njective. At the same time, signs are emerging that\nOpenAI’s flagship models may be slowly deterio-\nrating with respect to their performance on writing\ncode and doing basic math (Chen et al., 2023).\nAll these developments together challenge the\nidea that there is something like ‘the’ LLM, which\nis a simplification we made in this paper that is not\ndoing complete justice to the large zoo of LLMs\nthat currently exists. In addition, the continuing\nupdates they are undergoing to make them derail\nless quickly, safer, less bias-driven, more efficient,\nand so on, also imply that they are a moving target\nin many discussions. These fast developments may\nalso limit the import of the arguments into the more\ndistant future, as it is hard to foresee for example\ndevelopments in different neural architectures and\ntraining regimes.\nAcknowledgements\nThis research was not possible without collab-\noration with dr. Max van Duijn’s research\nproject ‘A Telling Story’ (with project number\nVI.Veni.191C.051), which is financed by the Dutch\nResearch Council (NWO). We thank Li Kloostra\nfor helpful comments on earlier versions of this\npaper. Lastly, the authors thank three anonymous\nreviewers for their constructive feedback.\nReferences\nMostafa Abdou, Artur Kulmizev, Daniel Hershcovich,\nStella Frank, Ellie Pavlick, and Anders Søgaard.\n2021. Can Language Models Encode Perceptual\nStructure Without Grounding? A Case Study in\nColor. In Proceedings of the 25th Conference on\nComputational Natural Language Learning, pages\n109–132, Online. Association for Computational Lin-\nguistics.\nBlaise Agüera y Arcas. 2022a. Artificial neural net-\nworks are making strides towards consciousness. Ac-\ncessed on: 2023-01-28.\nBlaise Agüera y Arcas. 2022b. Do Large Language\nModels Understand Us? Daedalus, 151(2):183–197.\nJacob Andreas. 2022. Language models as agent mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 5769–5779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nH. Baayen. 2008. Analyzing linguistic data: A prac-\ntical introduction to statistics using R . Cambridge\nUniversity Press.\nCindy Beaudoin, Élizabel Leblanc, Charlotte Gagner,\nand Miriam H. Beauchamp. 2020. Systematic Re-\nview and Inventory of Theory of Mind Measures for\nYoung Children. Frontiers in Psychology, 10.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\nDangers of Stochastic Parrots: Can Language Mod-\nels Be Too Big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 610–623.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nJonah Berger and Grant Packard. 2022. Using natural\nlanguage processing to understand people and culture.\nAmerican Psychologist, 77(4):525.\nMarcel Binz and Eric Schulz. 2023. Using cognitive\npsychology to understand GPT-3. Proceedings of the\nNational Academy of Sciences, 120(6):e2218523120.\nMark J. Bishop. 2021. Artificial intelligence is stupid\nand causal reasoning will not fix it. Frontiers in\nPsychology, 11:2603.\n12650\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lapata,\nAngeliki Lazaridou, Jonathan May, Aleksandr Nis-\nnevich, Nicolas Pinto, and Joseph Turian. 2020. Ex-\nperience Grounds Language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718–8735,\nOnline. Association for Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,\nAli Malik, Christopher D. Manning, Suvir Mirchan-\ndani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-\npadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Rob\nReich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishnan Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E.\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2021. On the Opportuni-\nties and Risks of Foundation Models. arXiv preprint\narXiv:2108.07258.\nAli Borji. 2023. A Categorical Archive of ChatGPT\nFailures. arXiv preprint arXiv:2302.03494.\nSamuel Bowman. 2022a. The dangers of underclaim-\ning: Reasons for caution when reporting how NLP\nsystems fail. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7484–7499, Dublin,\nIreland. Association for Computational Linguistics.\nSamuel R. Bowman. 2022b. The Dangers of Under-\nclaiming: Reasons for Caution When Reporting How\nNLP Systems Fail. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7484–7499,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nSamuel R. Bowman. 2023. Eight things to know\nabout large language models. arXiv preprint\narXiv:2304.00612.\nGeorge E. Box. 1979. All models are wrong, but some\nare useful. Robustness in Statistics, 202(1979):549.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are Few-Shot Learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Browning and Yann LeCun. 2022. AI and The\nLimits Of Language. Accessed on: 2023-01-28.\nErik Brynjolfsson, Danielle Li, and Lindsey R Raymond.\n2023. Generative AI at work. Technical report, Na-\ntional Bureau of Economic Research.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMichael Cerullo. 2022. In Defense of Blake Lemoine\nand the Possibility of Machine Sentience in Lamda.\nBenedetta Cevoli, Chris Watkins, Yang Gao, and Kath-\nleen Rastle. 2023. Shades of meaning: Uncover-\ning the geometry of ambiguous word representa-\ntions through contextualised language models. arXiv\npreprint arXiv:2304.13597.\nTyler A. Chang and Benjamin K. Bergen. 2022. Word\nacquisition in neural language models. Transactions\nof the Association for Computational Linguistics ,\n10:1–16.\nBrian X. Chen. 2023. How ChatGPT and Bard Per-\nformed as My Executive Assistants. Accessed on:\n2023-10-15.\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nHow is ChatGPT’s behavior changing over time?\narXiv preprint arXiv:2307.09009.\nNoam Chomsky. 2023. The False Promise of Chat-GPT.\nAccessed on: 2023-01-30.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\n12651\nPaul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nReinforcement Learning from Human Preferences.\nIn Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc.\nPaul M. Churchland. 1985. The ontological status of\nobservables: in praise of the superempirical virtues.\nImages of science, pages 35–47.\nHerbert H. Clark. 1996. Using Language. Cambridge\nUniversity Press.\nKatherine M Collins, Catherine Wong, Jiahai Feng,\nMegan Wei, and Josh Tenenbaum. 2022. Structured,\nflexible, and robust: benchmarking and improving\nlarge language models towards more human-like be-\nhavior in out-of-distribution reasoning tasks. In Pro-\nceedings of the Annual Meeting of the Cognitive Sci-\nence Society, volume 44.\nPablo Contreras Kallens, Ross Deans Kristensen-\nMcLachlan, and Morten H. Christiansen. 2023.\nLarge language models demonstrate the potential of\nstatistical learning in language. Cognitive Science,\n47(3):e13256.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\nC. Y . Chan, Antonia Creswell, Dharshan Kumaran,\nJames L. McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning.\nDaniel Dennett. 1989. The Intentional Stance . MIT\nPress.\nDaniel C. Dennett. 1991. Two Contrasts: Folk Craft\nvs. Folk Science and Belief vs. Opinion. In John D.\nGreenwood, editor, The Future of Folk Psychology:\nIntentionality and Cognitive Science, pages 135–48.\nNick J. Enfield. 2023. Linguistic concepts are self-\ngenerating choice architectures. Philosophical Trans-\nactions of the Royal Society B, 378(1870):20210352.\nLinnea Evanson, Yair Lakretz, and Jean Rémi King.\n2023. Language acquisition: do children and lan-\nguage models follow similar learning stages? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 12205–12218, Toronto,\nCanada. Association for Computational Linguistics.\nLuciano Floridi. 2023. AI as Agency without Intel-\nligence: On ChatGPT, large language models, and\nother generative models. Philosophy & Technology,\n36(1):15.\nJerry A. Fodor. 1987. Psychosemantics: The problem of\nmeaning in the philosophy of mind, volume 2. MIT\npress.\nGabriel Grand, Idan Asher Blank, Francisco Pereira,\nand Evelina Fedorenko. 2022. Semantic projection\nrecovers rich human knowledge of multiple object\nfeatures from word embeddings. Nature human be-\nhaviour, 6(7):975–987.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms.\nAndrea L. Guzman and Seth C. Lewis. 2020. Artificial\nintelligence and communication: A human–machine\ncommunication research agenda. New Media & Soci-\nety, 22(1):70–86.\nHannes Hansen and Martin N Hebart. 2022. Seman-\ntic features of object concepts generated with gpt-3.\narXiv preprint arXiv:2202.03753.\nMartie G. Haselton, Daniel Nettle, and Paul W. Andrews.\n2015. The evolution of cognitive bias. The handbook\nof evolutionary psychology, pages 724–746.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEghbal A. Hosseini, Martin Schrimpf, Yian Zhang,\nSamuel Bowman, Noga Zaslavsky, and Evelina Fe-\ndorenko. 2022. Artificial neural network language\nmodels align neurally and behaviorally with humans\neven after a developmentally realistic amount of train-\ning. bioRxiv.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How Can We Know What Language\nModels Know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, Scott Johnston, Sheer El-Showk,\nAndy Jones, Nelson Elhage, Tristan Hume, Anna\nChen, Yuntao Bai, Sam Bowman, Stanislav Fort,\nDeep Ganguli, Danny Hernandez, Josh Jacobson,\nJackson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language Models (Mostly) Know\nWhat They Know. arXiv preprint arXiv:2207.05221.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nKenneth Li, Aspen K Hopkins, David Bau, Fernanda\nViégas, Hanspeter Pfister, and Martin Wattenberg.\n2022. Emergent world representations: Exploring a\n12652\nsequence model trained on a synthetic task. arXiv\npreprint arXiv:2210.13382.\nTaylor Lorenz. 2023. An influencer’s AI clone will be\nyour girlfriend for $1 a minute. Accessed on: 2023-\n05-30.\nLi Lucy and David Bamman. 2021. Gender and rep-\nresentation bias in GPT-3 generated stories. In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nZiqiao Ma, Jiayi Pan, and Joyce Chai. 2023. World-\nto-words: Grounded open vocabulary acquisition\nthrough fast mapping in vision-language models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 524–544, Toronto, Canada. As-\nsociation for Computational Linguistics.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artificial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences, 117(48):30046–30054.\nGary F. Marcus. 2022. Nonsense on Stilts. Accessed\non: 2023-01-30.\nBernard Marr. 2023. Artificial Intimacy: How Gen-\nerative AI Can Now Create Your Dream Girlfriend.\nAccessed on: 2023-10-10.\nKaren Milligan, Janet Wilde Astington, and Lisa Ain\nDack. 2007. Language and Theory of Mind: Meta-\nAnalysis of the Relation Between Language Ability\nand False-Belief Understanding. Child Development,\n78(2):622–646.\nMelanie Mitchell and David C Krakauer. 2022. The\nDebate Over Understanding in AI’s Large Language\nModels. arXiv preprint arXiv:2210.13966.\nShima Rahimi Moghaddam and Christopher J. Honey.\n2023. Boosting theory-of-mind performance in large\nlanguage models via prompting.\nPeter Norvig. 2012. Colorless green ideas learn furi-\nously: Chomsky and the two cultures of statistical\nlearning. Significance, 9(4):30–33.\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774.\nRoma Patel and Ellie Pavlick. 2022. Mapping Language\nModels to Grounded Conceptual Spaces. In Interna-\ntional Conference on Learning Representations.\nAimee Pearcy. 2023. How ChatGPT and Bard Per-\nformed as My Executive Assistants. Accessed on:\n2023-10-15.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nJosef Perner, Susan R Leekam, and Heinz Wimmer.\n1987. Three-year-olds’ difficulty with false belief:\nThe case for a conceptual deficit. British journal of\ndevelopmental psychology, 5(2):125–137.\nBilly Perrigo. 2023. Exclusive: OpenAI Used Kenyan\nWorkers on Less Than $2 Per Hour to Make ChatGPT\nLess Toxic. Accessed on: 2023-01-25.\nSteven Piantadosi and Felix Hill. 2022. Meaning with-\nout reference in large language models. In NeurIPS\n2022 Workshop on Neuro Causal and Symbolic AI\n(nCSI).\nSteven T. Piantadosi. 2023. Modern language mod-\nels refute chomsky’s approach to language. Ling-\nbuzz/007180.\nFrançois Quesque and Yves Rossetti. 2020. What do\ntheory-of-mind tasks actually measure? Theory and\npractice. Perspectives on Psychological Science ,\n15(2):384–396.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan\nZhang, S. M. Ali Eslami, and Matthew Botvinick.\n2018. Machine theory of mind. In Proceedings of\nthe 35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learning\nResearch, pages 4218–4227. PMLR.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nbert. Advances in Neural Information Processing\nSystems, 32.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the Asso-\nciation for Computational Linguistics, 8:842–866.\nRichard Rorty. 2009. Philosophy and the Mirror of\nNature. Princeton University Press.\nGilbert Ryle. 1950. \"The Concept of Mind\". The Uni-\nversity of Chicago Press.\nMagnus Sahlgren and Fredrik Carlsson. 2021. The Sin-\ngleton Fallacy: Why Current Critiques of Language\nModels Miss the Point. Frontiers in Artificial Intelli-\ngence, 4:682578.\n12653\nLila San Roque and Bambi B Schieffelin. 2019. Per-\nception verbs in context: Perspectives from Kaluli\n(Bosavi) child-caregiver interaction. Laura Speed,\nC. O’Meara, Lila San Roque, and Asifa Majid (eds.)\nPerception Metaphors, pages 347–368.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits of\nsocial intelligence in large LMs. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3762–3780, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJohn R. Searle. 1980. Minds, brains, and programs.\nBehavioral and brain sciences, 3(3):417–424.\nTerrence Sejnowski. 2022. Large language mod-\nels and the reverse turing test. arXiv preprint\narXiv:2207.14382.\nMurray Shanahan. 2022. Talking About Large Lan-\nguage Models. arXiv preprint arXiv:2212.03551.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023. Clever Hans or Neu-\nral Theory of Mind? Stress Testing Social Reasoning\nin Large Language Models.\nMichael Siegal and Karen Beattie. 1991. Where to\nlook first for children’s knowledge of false beliefs.\nCognition, 38(1):1–12.\nJeff Sparrow. 2022. ‘Full-on robot writing’: the artificial\nintelligence challenge facing universities. Accessed\non: 2023-01-25.\nMichael T. Stuart and Markus Kneer. 2021. Guilty\nartificial minds: Folk attributions of mens rea and\nculpability to artificially intelligent agents. Proceed-\nings of the ACM on Human-Computer Interaction ,\n5(CSCW2):1–27.\nArjun Subramonian, Xingdi Yuan, Hal Daumé III, and\nSu Lin Blodgett. 2023. It Takes Two to Tango: Nav-\nigating Conceptualizations of NLP Tasks and Mea-\nsurements of Performance. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2023 ,\npages 3234–3279, Toronto, Canada. Association for\nComputational Linguistics.\nMichael Tomasello. 2003. The key is social cognition.\nLanguage in mind: Advances in the study of language\nand thought, pages 44–57.\nMichael Tomasello et al. 1995. Joint attention as social\ncognition. Joint attention: Its origins and role in\ndevelopment, 103130:103–130.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nSean Trott, Cameron Jones, Tyler Chang, James\nMichaelov, and Benjamin Bergen. 2023. Do Large\nLanguage Models know what humans know? Cogni-\ntive Science, 47(7):e13309.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nMax J. van Duijn, Bram M.A. van van Dijk, Tom\nKouwenhoven, Werner de Valk, Marco Spruit, and\nPeter van der Putten. 2023. Theory of Mind in\nLarge Language Models: Examining Performance of\n11 State-of-the-Art models vs. Children Aged 7-10\non Advanced Tests. Proceedings of 27th Confer-\nence on Computational Natural Language Learningn\n(CoNNL).\nBas C. Van Fraassen. 1980. The scientific image. Ox-\nford University Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. Advances in neural information process-\ning systems, 30.\nAlex Warstadt and Samuel R Bowman. 2022. What\nArtificial Neural Networks Can Tell Us About Human\nLanguage Acquisition. In Algebraic Structures in\nNatural Language, pages 17–60. CRC Press.\nChengwei Wei, Yun-Cheng Wang, Bin Wang, and C-\nC Jay Kuo. 2023. An overview on language models:\nRecent developments and outlook. arXiv preprint\narXiv:2303.05759.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of Thought Prompting Elicits Reasoning in\nLarge Language Models. CoRR, abs/2201.11903.\nEthan Gotlieb Wilcox, Richard Futrell, and Roger Levy.\n2023. Using computational models to test syntactic\nlearnability. Linguistic Inquiry, pages 1–44.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,\nShichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong,\nYang Shen, et al. 2023. A comprehensive capability\nanalysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023a. Lima:\nLess is more for alignment.\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto.\n2023b. Navigating the Grey Area: Expressions of\nOverconfidence and Uncertainty in Language Mod-\nels.\n12654",
  "topic": "Salience (neuroscience)",
  "concepts": [
    {
      "name": "Salience (neuroscience)",
      "score": 0.45385298132896423
    },
    {
      "name": "Context (archaeology)",
      "score": 0.41692763566970825
    },
    {
      "name": "Epistemology",
      "score": 0.41265034675598145
    },
    {
      "name": "Psychology",
      "score": 0.354651540517807
    },
    {
      "name": "Cognitive science",
      "score": 0.34559276700019836
    },
    {
      "name": "Sociology",
      "score": 0.34447532892227173
    },
    {
      "name": "Cognitive psychology",
      "score": 0.2640495002269745
    },
    {
      "name": "History",
      "score": 0.13175228238105774
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}