{
  "title": "StyTr^2: Unbiased Image Style Transfer with Transformers",
  "url": "https://openalex.org/W3168654474",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5102924119",
      "name": "Yingying Deng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112699696",
      "name": "Fan Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067338071",
      "name": "Xingjia Pan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069298091",
      "name": "Weiming Dong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055760495",
      "name": "ChongyangMa",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5022636178",
      "name": "Changsheng Xu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W2604737827",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2125879936",
    "https://openalex.org/W3093370337",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034580487",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3037016570",
    "https://openalex.org/W3035057895",
    "https://openalex.org/W2969803502",
    "https://openalex.org/W2585235684",
    "https://openalex.org/W2545656684",
    "https://openalex.org/W3173452715",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2962772087",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963095605",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2583638424",
    "https://openalex.org/W3177361240",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2339754110",
    "https://openalex.org/W3177457352",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2099407246",
    "https://openalex.org/W3175433303",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2993087067",
    "https://openalex.org/W2963890275",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2963683323",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3037093687",
    "https://openalex.org/W2564755245",
    "https://openalex.org/W1999360130",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2997418011",
    "https://openalex.org/W2952335829",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W3035026630",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "The goal of image style transfer is to render an image with artistic features guided by a style reference while maintaining the original content. Due to the locality and spatial invariance in CNNs, it is difficult to extract and maintain the global information of input images. Therefore, traditional neural style transfer methods are usually biased and content leak can be observed by running several times of the style transfer process with the same reference style image. To address this critical issue, we take long-range dependencies of input images into account for unbiased style transfer by proposing a transformer-based approach, namely StyTr^2. In contrast with visual transformers for other vision tasks, our StyTr^2 contains two different transformer encoders to generate domain-specific sequences for content and style, respectively. Following the encoders, a multi-layer transformer decoder is adopted to stylize the content sequence according to the style sequence. In addition, we analyze the deficiency of existing positional encoding methods and propose the content-aware positional encoding (CAPE) which is scale-invariant and more suitable for image style transfer task. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed StyTr^2 compared to state-of-the-art CNN-based and flow-based approaches.",
  "full_text": "1\nStyTr2:Image Style Transfer with Transformers\nYingying Deng, Fan Tang, Weiming Dong Member, IEEE, Chongyang Ma, Xingjia Pan, Lei Wang,\nChangsheng Xu, Fellow, IEEE\nAbstract—The goal of image style transfer is to render an\nimage with artistic features guided by a style reference while\nmaintaining the original content. Owing to the locality in con-\nvolutional neural networks (CNNs), extracting and maintaining\nthe global information of input images is difﬁcult. Therefore,\ntraditional neural style transfer methods face biased content\nrepresentation. To address this critical issue, we take long-range\ndependencies of input images into account for image style transfer\nby proposing a transformer-based approach called StyTr 2. In\ncontrast with visual transformers for other vision tasks, StyTr 2\ncontains two different transformer encoders to generate domain-\nspeciﬁc sequences for content and style, respectively. Following\nthe encoders, a multi-layer transformer decoder is adopted to\nstylize the content sequence according to the style sequence. We\nalso analyze the deﬁciency of existing positional encoding meth-\nods and propose the content-aware positional encoding (CAPE),\nwhich is scale-invariant and more suitable for image style transfer\ntasks. Qualitative and quantitative experiments demonstrate the\neffectiveness of the proposed StyTr 2 compared with state-of-the-\nart CNN-based and ﬂow-based approaches. Code and models are\navailable at https://github.com/diyiiyiii/StyTR-2.\nI. I NTRODUCTION\nImage style transfer is an interesting and practical research\ntopic that can render a content image using a referenced style\nimage. Based on texture synthesis, traditional style transfer\nmethods [1], [2] can generate vivid stylized images, but are\ncomputationally complex due to the formulation of stroke\nappearance and painting process. Afterward, researchers focus\non neural style transfer based on convolutional neural networks\n(CNNs). Optimization-based style transfer methods [3]–[5] ren-\nder the input content images with learned style representation\niteratively. Following the encoder-transfer-decoder pipeline,\narbitrary style transfer networks [6]–[14] are optimized by\naligning second-order statistics of content images to style\nimages and can generate stylized results in a feed-forward\nmanner efﬁciently. However, these methods cannot achieve\nsatisfactory results in some cases due to the limited ability to\nmodel the relationship between content and style. To overcome\nthis issue, several recent methods [15]–[19] apply a self-\nattention mechanism for improved stylization results.\nThe aforementioned style transfer methods utilize CNNs to\nlearn style and content representations. Owing to the limited\nYingying Deng, Weiming Dong and Changsheng Xu are with School\nof Artiﬁcial Intelligence, University of Chinese Academy of Sciences,\nBeijing, China and NLPR, Institute of Automation, Chinese Academy\nof Sciences, Beijing, China (e-mail: {dengyingying2017, weiming.dong,\nchangsheng.xu}@ia.ac.cn).\nFan Tang is with Jilin University, Changchun, China (e-mail: tang-\nfan@jlu.edu.cn)\nXingjia Pan is with YouTu Lab, Tencent, Shanghai, China (e-mail:\nxjia.pan@gmail.com)\nChongyang Ma is with Kuaishou Technology, Beijing, China (e-mail:\nchongyangma@kuaishou.com).\nWoman with a Hat ,\nHenri Matisse,\n1905.\nconv1 1\nlayer 1\n{conv1 1,conv2 1}\nlayer 2\n{conv1 1,...,conv4 1}\nlayer 3\n(a)\n(b)\nFig. 1. Comparisons of intermediate layers using the leftmost image as the\ninput content and the style reference in a style transfer task. (a) Feature\nvisualizations of a pretrained VGG based on [3].(b) Feature visualizations of\nour transformer decoder.\nreceptive ﬁeld of convolution operation, CNNs cannot capture\nlong-range dependencies without sufﬁcient layers. However,\nthe increment of network depth could cause the loss of\nfeature resolution and ﬁne details [20]. The missing details can\ndamage the stylization results in aspects of content structure\npreservation and style display. As shown in Figure 1(a), some\ndetails are omitted in the process of convolutional feature\nextraction. An et al [21] recently show that typical CNN-based\nstyle transfer methods are biased toward content representation\nby visualizing the content leak of the stylization process, i.e.,\nafter repeating several rounds of stylization operations, the\nextracted structures of input content will change drastically.\nWith the success of transformer [22] in natural language\nprocessing (NLP), transformer-based architectures have been\nadopted in various vision tasks. The charm of applying\ntransformer to computer vision lies in two aspects. First, it is\nfree to learn the global information of the input with the help of\nthe self-attention mechanism. Thus, a holistic understanding can\nbe easily obtained within each layer. Second, the transformer\narchitecture models relationships in input shapes [23], and\ndifferent layers extract similar structural information [24] (see\nFigure 1(b)). Therefore, transformer has a strong representation\ncapability to capture precise content representation and avoid\nﬁne detail missing. Thus, the generated structures can be well-\npreserved.\nIn this work, we aim to eliminate the biased representation\nissue of CNN-based style transfer methods and propose a\nnovel image Style Transfer Transformer framework called\nStyTr2. Different from the original transformer, we design\ntwo transformer-based encoders in our StyTr 2 framework to\nobtain domain-speciﬁc information. Following the encoders,\nthe transformer decoder is used to progressively generate\narXiv:2105.14576v3  [cs.CV]  1 Apr 2022\n2\nthe output sequences of image patches. Furthermore, towards\nthe positional encoding methods that are proposed for NLP,\nwe raise two considerations: (1) different from sentences\nordered by logic, the image sequence tokens are associated\nwith semantic information of the image content; (2) for the\nstyle transfer task, we aim to generate stylized images of any\nresolution. The exponential increase in image resolution will\nlead to a signiﬁcant change of positional encoding, leading\nto large position deviation and inferior output quality. In\ngeneral, a desired positional encoding for vision tasks should\nbe conditioned on input content while being invariant to image\nscale transformation. Therefore, we propose a content-aware\npositional encoding scheme (CAPE) which learns the positional\nencoding based on image semantic features and dynamically\nexpands the position to accommodate different image sizes.\nIn summary, our main contributions include:\n• A transformer-based style transfer framework called\nStyTr2, to generate stylization results with well-preserved\nstructures and details of the input content image.\n• A content-aware positional encoding scheme that is scale-\ninvariant and suitable for style transfer tasks.\n• Comprehensive experiments showing that StyTr 2 outper-\nforms baseline methods and achieves outstanding results\nwith desirable content structures and style patterns.\nII. R ELATED WORK\na) Image style transfer: Gatys et al [3] ﬁnd that hi-\nerarchical layers in CNNs can be used to extract image\ncontent structures and style texture information and propose\nan optimization-based method to generate stylized images\niteratively. Some approaches [25], [26] adopt an end-to-end\nmodel to achieve real-time style transfer for one speciﬁc style.\nFor more efﬁcient applications, [27]–[29] combine multiple\nstyles in one model and achieve outstanding stylization results.\nMore generally, arbitrary style transfer gains more attention\nin recent years. Huang et al [6] propose an adaptive instance\nnormalization (AdaIN) to replace the mean and variance of\ncontent with that of style. AdaIN is widely adopted in image\ngeneration tasks [10], [11], [14], [30], [31] to fuse the content\nand style features. Li et al [7] design a whiten and colorization\ntransformation (WCT) to align the second-order statistics of\ncontent and style features. Moreover, many methods [21], [32],\n[33] also aim at promoting the generation effect in the premise\nof efﬁciency. Based on the CNNs model, [15]–[17], [19],\n[34] introduce self-attention to the encoder-transfer-decoder\nframework for better feature fusion. Chen et al [35] propose an\nInternal-External Style Transfer algorithm (IEST) containing\ntwo types of contrastive loss, which can produce a harmonious\nand satisfactory stylization effect. However, existing encoder-\ntransfer-decoder style transfer methods cannot handle the long-\nrange dependencies and may lead to missing details.\nb) Transformer for vision tasks: As an alternative to\nrecurrent and convolutional neural networks, transformer [22]\nis ﬁrst proposed for machine translation tasks and has been\nwidely used in various NLP tasks [36]–[41]. Inspired by the\nbreakthrough of transformer in NLP, many researchers have\ndeveloped vision transformers for various image/video related\ntasks [42], including object detection [43]–[45], semantic\nsegmentation [46], [47], image classiﬁcation [48]–[52], image\nprocessing and generation [20], [50], [53]. Compared with\nfully convolutional networks, transformer-based networks\ncan capture long-term dependencies of the input image by\nusing self-attention mechanisms. In this paper, we introduce\ntransformer-based structures for style transfer tasks which can\nbe seen as sequence-to-sequence generation of image patches.\nc) Positional encoding: Positional encoding is commonly\nused in transformer-based models to provide position informa-\ntion. There are two types of positional encoding are used:\nfunctional and parametric positional encoding. Functional\npositional encoding is calculated by pre-deﬁned functions, such\nas sinusoidal functions [22]. Parametric positional encoding\nis learned via model training [38]. To ensure translational-\ninvariance for the transformers, relative positional encod-\ning [54]–[57] considers the distance between tokens in the\nimage sequence. [58] and [59] further include positional\nencoding in CNN-based models as spatial inductive. In\nthis paper, we propose a content-aware positional encoding\nmechanism that is scale-invariant and more suitable for image\ngeneration tasks.\nIII. O UR METHOD\nTo leverage the capability of transformers to capture long-\nrange dependencies of image features for style transfer, we\nformulate the problem as a sequential patch generation task.\nGiven a content image Ic ∈ RH×W×3 and a style image\nIs ∈RH×W×3, we split both images into patches (similar\nto tokens in NLP tasks) and use a linear projection layer to\nproject input patches into a sequential feature embedding Ein\na shape of L×C, where L= H×W\nm×m is the length of E, m= 8\nis the patch size and C is the dimension of E. The overall\nstructure of our framework is shown in Figure 2.\nA. Content-Aware Positional Encoding\nWhen using a transformer-based model, the positional\nencoding ( PE) should be included in the input sequence to\nacquire structural information. According to [22], the attention\nscore of the i-th patch and the j-th patch is computed as:\nAi,j =((Ei + Pi)Wq)T ((Ej + Pj)Wk)\n=WT\nq ET\ni EjWk + WT\nq ET\ni PjWk\n+ WT\nq PT\ni EjWk + WT\nq PT\ni PjWk,\n(1)\nwhere Wq and Wk are parameter matrices for query and key\ncalculation, and Pi presents the i-th one-dimensional PE. In\n2D cases, the positional relative relation between the patch at\na pixel (xi,yi) and the patch at a pixel (xj,yj) is:\nP(xi,yi)T P(xj,yj)\n=\nd\n4 −1∑\nk=0\n[cos(wk(xj −xi)) + cos(wk(yj −yi))],\n(2)\nwhere wk = 1/100002k/128, d= 512. The positional relative\nrelation between two patches only depends on their spatial\ndistance. Accordingly, we raise two important questions. First,\n3\nDecoder\nTransformer\nEncoder \nV\nQ\nQ\nK\nK\nAdd&Norm\nAdd&Norm\nAdd&Norm\nFFN\nMulti-head Attention\nMulti-head Attention\nV\nTransformer\nEncoder \nStyle value&key bank \n⨁\nCAPE \n(a) Transformer decoder layer (b) StyTr2   Network \nTransformer decoder\nTransformer decoder layer\nLinear Projection\nLinear Projection\nStyle Content\nReshape conv + 2×\nconv + 2×\nconv + 2×\n51264\nHW \n5128 8\nH W \n1284 4\nH W \n642 2\nH W \n3H W \nDecoder\nTransfomer \nLayers\n(a) Decoder (b) Identity loss\nFig. 2. Overall pipeline of our StyTr 2 framework. We split the content and style images into patches and use a linear projection to obtain patch sequences.\nThen, the content sequences added with CAPE are fed into a content transformer encoder, while the style sequences are fed into a style transformer encoder.\nFollowing the two transformer encoders, a multi-layer transformer decoder is adopted to stylize the content sequences according to the style sequences. Finally,\nwe use a progressive upsampling decoder to obtain the ﬁnal output.\n01\n0      10123\n0       1       2       3\n(a) (b)\nFig. 3. Schematic diagram of content-aware positional encoding.\nfor an image generation task, should we take image semantics\ninto account when calculating PE? Traditional PE is designed\nfor sentences ordered by logic, but image patches are organized\nbased on the content. We denote the distance between two\npatches as d(·,·). On the right-hand side of Figure 3(a), the\ndifference between d((x0,y3),(x1,y3)) (the red and green\npatches) and d((x0,y3),(x3,y3)) (the red and cyan patches)\nshould be small because we expect similar content patches\nto have similar stylization results. Second, is the traditional\nsinusoidal positional encoding still suitable for vision tasks\nwhen the input image size expands exponentially? As shown\nin Figure 3(a), when an image is resized, the relative distance\nbetween patches (depicted by small blue rectangles) in the\nsame locations can change dramatically, which may be not\nsuitable for multi-scale methods in vision tasks.\nTo this end, we propose content-aware positional encoding\n(CAPE), which is scale-invariant and more suitable for style\ntransfer tasks. Different from sinusoidal PE which only\nconsiders the relative distance of patches, CAPE is conditioned\non the semantics of image content. We assume that using n×n\npositional encodings is adequate to represent the semantics of\nan image. For an image I ∈RH×W×3, we rescale the ﬁxed\nn×n positional encoding to H\nm ×W\nm , as shown in Figure\n3(b). In this way, various image scales will not inﬂuence the\nspatial relation between two patches. The CAPE of patch (x,y),\nnamely , PCA(x,y), is formulated as\nPL= Fpos(AvgPooln×n(E)),\nPCA(x,y) =\ns∑\nk=0\ns∑\nl=0\n(aklPL(xk,yl)), (3)\nwhere AvgPooln×n is the average pooling function, Fpos is\n1 ×1 convolution operation used as a learnable positional\nencoding function, PL is learnable PE following the sequence\nE, n is set to 18 in our experiments, akl is the interpolation\nweight, and s is the number of neighboring patches. Lastly,\nwe add PCAi to Ei as the ﬁnal feature embedding of the i-th\npatch at a pixel location (x,y).\nB. Style Transfer Transformer\na) Transformer encoder.: We capture long-range depen-\ndencies of image patches by using transformer based structure\nto learn sequential visual representations. Different from other\nvision tasks [43], [45], [53], the input of tjr style transfer\ntask comes from two different domains, corresponding to\nnatural images and artistic paintings, respectively. Therefore,\nStyTr2 has two transformer encoders to encode domain-speciﬁc\nfeatures, which are used to translate a sequence from one\ndomain to another in the next stage.\nGiven the embedding of an input content sequence Zc =\n{Ec1 +PCA1,Ec2 +PCA2,..., EcL +PCAL}, we ﬁrst feed it into\nthe transformer encoder. Each layer of the encoder consists of\na multi-head self-attention module (MSA) and a feed-forward\nnetwork (FFN). The input sequence is encoded into query ( Q),\nkey (K), and value ( V):\nQ= ZcWq, K = ZcWk, V = ZcWv, (4)\nwhere Wq,Wk,Wv ∈RC×dhead . The multi-head attention is\nthen calculated by\nFMSA(Q,K,V ) = Concat(Attention1(Q,K,V ),\n..., AttentionN (Q,K,V ))Wo, (5)\n4\nwhere Wo ∈RC×C are learnable parameters, N is the number\nof attention heads, and dhead = C\nN . The residual connections\nare applied to obtain the encoded content sequence Yc:\nY′\nc = FMSA(Q,K,V ) +Q,\nYc = FFFN(Y′\nc ) +Y′\nc , (6)\nwhere FFFN(Y′\nc ) = max(0 ,Y ′\nc W1 + b1)W2 + b2. Layer\nnormalization (LN) is applied after each block [22].\nSimilarly, the embedding of an input style sequence Zs =\n{Es1,Es2,..., EsL}is encoded into a sequence Ys following the\nsame calculation process, except that positional encoding is\nnot considered because we do not need to maintain structures\nof the input style in the ﬁnal output.\nb) Transformer decoder.: Our transformer decoder is used\nto translate the encoded content sequence Yc according to the\nencoded style sequence Ys in a regressive fashion. Different\nfrom the auto-regressive process in NLP tasks, we take all\nthe sequential patches as input at one time to predict the\noutput. As shown in Figure 3(a), each transformer decoder\nlayer contains two MSA layers and one FFN. The input of our\ntransformer decoder includes the encoded content sequence,\ni.e., ˆYc = {Yc1 + PCA1,Yc2 + PCA2,...,Y cL + PCAl}, and the\nstyle sequence Ys = {Ys1,Ys2,...,Y sL}. We use the content\nsequence to generate the query Q, and use the style sequence\nto generate the key K and the value V:\nQ= ˆYcWq, K = YsWk, V = YsWv. (7)\nThen, the output sequence X of the transformer decoder can\nbe calculated by\nX′′= FMSA(Q,K,V ) +Q,\nX′= FMSA(X′′+ PCA,K,V ) +X′′,\nX = FFFN(X′) +X′.\n(8)\nLayer normalization (LN) is also applied at the end of each\nblock [22].\nc) CNN decoder.: The output sequence X of the trans-\nformer is in a shape of HW\n64 ×C. Instead of directly\nupsampling the output sequence to construct the ﬁnal results,\nwe use a three-layer CNN decoder to reﬁne the outputs of\nthe transformer decoder following [46]. For each layer, we\nexpand the scale by adopting a series of operations including\n3 ×3 Conv + ReLU + 2×Upsample. Finally, we can obtain\nthe ﬁnal results in a resolution of H×W ×3.\nC. Network Optimization\nThe generated results should maintain the original content\nstructures and the reference style patterns. Therefore, we\nconstruct two different perceptual loss terms to measure the\ncontent difference between the output image Io and the input\ncontent image Ic, as well as the style difference between Io\nand the input style reference Is.\nWe use feature maps extracted by a pretrained VGG model\nto construct the content loss and the style loss following [6],\n[21]. The content perceptual loss Lc is deﬁned as\nLc = 1\nNl\nNl∑\ni=0\n∥φi(Io) −φi(Ic)∥2, (9)\nwhere φi(·) denotes features extracted from the i-th layer in a\npretrained VGG19 and Nl is the number of layers.\nThe style perceptual loss Ls is deﬁned as\nLs = 1\nNl\nNl∑\ni=0\n∥µ(φi(Io)) −µ(φi(Is))∥2\n+ ∥σ(φi(Io)) −σ(φi(Is))∥2,\n(10)\nwhere µ(·) and σ(·) denote the mean and variance of extracted\nfeatures, respectively.\nWe also adopt identity loss [15] to learn richer and more\naccurate content and style representations. Speciﬁcally, we take\ntwo of the same content (style) images into StyTr 2, and the\ngenerated output Icc(Iss) should be identical to the input Ic(Is).\nTherefore, we compute two identity loss terms to measure the\ndifferences between Ic(Is) and Icc(Iss):\nLid1 = ∥Icc −Ic∥2+∥Iss −Is∥2,\nLid2 = 1\nNl\nNl∑\ni=0\n∥φi(Icc) −φi(Ic)∥2+∥φi(Iss) −φi(Is)∥2.\n(11)\nThe entire network is optimized by minimizing the following\nfunction:\nL= λcLc + λsLs + λid1Lid1 + λid2Lid2. (12)\nWe set λc, λs, λid1, and λid2 to 10, 7, 50, and 1 to alleviate\nthe impact of magnitude differences.\nIV. E XPERIMENTS\nA. Implementation Details\nMS-COCO [60] is used as the content dataset and\nWikiArt [61] is used as the style dataset. In the training stage,\nall the images are randomly cropped into a ﬁxed resolution of\n256 ×256, while any image resolution is supported at the test\ntime. We adopt the Adam optimizer [62] and the learning rate is\nset to 0.0005 using the warm-up adjustment strategy [63]. We\nset the batch size to be 8 and train our network with 160,000\niterations.\nB. Comparisons with SOTA Methods\nWe compare our method with AdaIN [6], Avater [64],\nSANet [15], AAMS [18], MAST [16], MCC [17], ArtFlow [21],\nAdaAttN [19], IEST [35] and StyleFormer [34]. AdaIN, Avater,\nSANet, AAMS, and MAST are typical CNN-based image\nstylization approaches. MCC [17] is a video style transfer\nmethod but can be applied to images without damaging the\ngenerated results. ArtFlow [21] designs a ﬂow-based network\nto minimize image reconstruction error and recovery bias.\nAdaAttN [19] performs attentive normalization on a per-\npoint basis for feature distribution alignment. IEST [35] takes\nadvantage of contrastive learning and external memory to\nboost visual quality. StyleFormer [34] adopts the transformer\nmechanism into the traditional CNN-based encoder-decoder\npipeline. By contrast, we present a pure transformer-based\narchitecture to solve the issue of missing content details caused\nby convolutions.\n5\nResolution Ours StyleFormer IEST AdaAttN ArtFlow MCC MAST AAMS SANet Avatar AdaIN\n256 ×256 0.116 0.013 0.065 0.104 0.142 0.013 0.030 2.074 0.015 0.260 0.007\n512 ×512 0.661 0.026 0.092 0.213 0.418 0.015 0.096 2.173 0.019 0.470 0.008\nTABLE I\nAVERAGE INFERENCE TIME (IN SECONDS ) OF DIFFERENT METHODS AT TWO OUTPUT RESOLUTIONS .\nStyleFormerContent\nStyle\n AdaIN\nAvatar\nSANet\nAAMS\nOurs\n MAST\nMCC\nArtFlow\nAdaAttN\nIEST\nFig. 4. Qualitative comparisons of style transfer results using different methods.\nOurs StyleFormer IEST AdaAttN ArtFlow MCC MAST AAMS SANet Avatar AdaIN\nLc ↓ 1.91 2.86 1.97 2.29 2.13 2.38 2.46 2.44 2.44 2.84 2.34\nLs ↓ 1.47 2.91 3.47 2.45 3.08 1.56 1.55 3.18 1.18 2.86 1.91\nTABLE II\nQUANTITATIVE COMPARISONS . WE COMPUTE THE AVERAGE CONTENT AND STYLE LOSS VALUES OF RESULTS BY DIFFERENT METHODS TO MEASURE HOW\nWELL THE INPUT CONTENT AND STYLE ARE PRESERVED .THE BEST RESULTS ARE IN BOLD WHILE THE SECOND -BEST RESULTS ARE MARKED WITH AN\nUNDERLINE .\na) Timing information: Our model is trained on two\nNVIDIA Tesla P100 GPUs and two NVIDIA GeForce RTX\n3090 GPUs for approximately one day. In Table I, we compare\nthe inference time of different style transfer methods under\ntwo output resolutions using one Tesla P100.\nb) Qualitative evaluation: Figure 4 shows the visual\nresults of qualitative comparisons. Owing to the simpliﬁed\nalignment of mean and variance, the results of AdaIN [6] have\ninsufﬁcient style patterns. The stylized images present crack\nartifacts that affect the overall transfer quality. AAMS [18]\nfocuses on the main structure (referring to salient regions\nin the attention map) of the content image but ignores the\nother parts. Therefore, the secondary structures are not well\nmaintained. The patch-swap-based method leads to artifacts of\nover-blurry output. MCC [17] uses a transform formulation of\nself-attention, but the absence of non-linear operation limiting\nthe maximum value of network output results in an overﬂow\nissue around object boundaries. The ﬂow-based model has\nlimited capability of feature representation, thus the results\nof ArtFlow [21] generally have the problem of insufﬁcient or\ninaccurate style. The border of stylized images may present\nundesirable patterns due to numerical overﬂow. The per-point\nbasis of AdaAttN [19] leads to style degeneration, thus the\nstylized patterns in the generated results are not consistent\nwith the input reference. The visual quality of IEST [35]\noutperforms other approaches. However, the style of generated\nresults may not be consistent with the input style reference (the\n1st and 3rd rows). Following the CNN-based “encoder-decoder”\npipeline, results of StyleFormer [34] still tend to missing details.\nBy contrast, StyTr 2 leverages a transformer-based network,\n6\nwhich has better feature representation to capture long-range\ndependencies of input image features and to avoid missing of\ncontent and style details. Therefore, our results can achieve\nwell-preserved content structures and desirable style patterns.\nc) Quantitative evaluation: We calculate the content\ndifference between the generated results and input content\nimages as well as the style difference between the generated\nresults and input style images, as two indirect metrics of the\nstyle transfer quality. Intuitively, the smaller the difference the\nbetter the input content/style is preserved. We randomly select\n40 style images and 20 content images to generate 800 stylized\nimages. For each method, we compute the content difference\nbased on Eq. (9) and calculate the style difference following\nEq. (10). Table II shows the corresponding quantitative results.\nOverall, our method achieves the lowest content losses and\nIEST [35] is the second-best. However, as discussed in the\nqualitative evaluation above, the style loss of IEST is the highest\nbecause the style appearance of generated results is not far from\nthe input style reference. In terms of style loss, SANet [15] and\nStyTr2 outperform the other methods. Therefore, our results\ncan effectively preserve both the input content and the reference\nstyle simultaneously.\nC. Analysis of Content Leak\nThe content leak issue usually occurs in the stylization\nprocess because CNN-based feature representation may not\nsufﬁciently capture details in the image content. This type of\nartifact is easy to spot by human eyes after repeating several\nrounds of the same stylization process [21], which is formulated\nby\nIi\no = Gi(...G 2(G1(Ic,Is),Is) ...,I s), (13)\nwhere Gi is the generator for the i-th round and Ii\no is the\ncorresponding stylization result. To solve the content leak\nproblem, An et al [21] propose a reversible network to replace\nCNN-based models. However, strict reversibility may not be\nsuitable for generation tasks [65]. Furthermore, the robustness\nand generated visual effects of ArtFlow may be downgraded\ndue to limited capability of feature representation. By contrast,\nwe leverage the capability of transformer-based architecture\nto capture long-range dependencies. Thus, our method can\nsigniﬁcantly alleviate the content leak issue.\nWe compare StyTr 2 with CNN-based methods and ﬂow-\nbased model ArtFlow [21]. Figure 5 demonstrates the cor-\nresponding results after the 1st and the 20th rounds of the\nrepeating stylization process. As shown in the top row, the\ncontent structures generated by CNN-based methods after the\nﬁrst round are damaged to various degrees, but our result still\npresents clear content details. Although the results generated by\nArtFlow maintain clear content structures, the stylized effect is\nnot satisfactory (e.g., the marginal ﬂaws and inappropriate\nstyle patterns). The bottom row of Figure 5 shows that\nwith increasing rounds of the stylization process, the content\nstructures generated by CNN-based methods tend to be blurry,\nwhile the content structures generated by our method are remain\ndistinct. The same problem applies to StyleFormer, which also\nrelies on the CNN-based encoder-decoder pipeline. Therefore,\nour model captures precise content representation leading to\nsuperior style transfer results while effectively alleviating the\ncontent leak issue.\nD. Analysis of CAPE\nAs described in III-A, when calculating PE, we should take\nthe semantic information of content images into account. To\ncompare the proposed CAPE with sinusoidal PE which is not\nsemantics-aware, we show two cases where the input content\nimage has repetitive patterns or is simply collaged by repeating\none image four times. As shown in Figure 6, we can observe\ninconsistent stylized regions in the ﬁnal results when using\nsinusoidal PE. The input resolution is set to be 256 ×256,\nwhich is the same as the image resolution for training.\nMoreover, handling input resolution different from the\ntraining examples is generally challenging for a learning-based\nmethod. To this end, an ideal PE for vision tasks should be\nscale-invariant, but a drastic change of image resolution leads\nto a signiﬁcant difference in traditional PE. We compare our\nCAPE with sinusoidal PE in Figure 7. In the third row, the\ninput size is 512 ×512, which is twice the image resolution\nfor training. Consequently, the results present vertical track\nartifacts due to the large positional deviation. In the second\nrow, the input resolution is 256 ×256, which is the same\nas the training data. The corresponding results do not have\nthe issue of vertical tracks but are not satisfactory due to the\nsmall resolution. By contrast, our method supports any input\nresolution with CAPE by design. Therefore, our results in the\nlast row of Figure 7 present clear content structures and proper\nstylized patterns. Additional ablation studies are provided in\nour supplementary materials.\nTo verify whether CAPE could provide position information,\nwe show CAPEs with different inputs in Figure 8. Although two\nCAPEs are not the same, they have similar encoding behaviors\nsuch as highlighted diagonal, repeat, and periodic patterns as\nlearnable PE [48] and sinusoidal PE. Different from learnable\nPE where the encoding is conditioned on the whole dataset,\nour CAPE dynamically encodes different input and thus can\neasily generalize to various resolutions.\nE. User study\nWe conduct a user study to further compare our method.\nAdaAttN [19], ArtFlow [21], MCC [17], AAMS [18] and\nAdaIN [6] are selected as baselines. We reuse the images\nin the quantitative comparison and invite 100 participants to\nevaluate the results of different approaches. The participants\nare informed of the purpose and details of our user study. The\nparticipants comprise 52 males and 48 females, in the age\nrange of 19 ∼45. Given a content image and a style image, we\nshow the result generated by our approach and the output from\nanother randomly selected method for comparison and ask the\nuser to choose (1) which result has better stylization effects (2)\nwhich stylization result preserves the content structures better\nand (3) which stylization result transfers the style patterns more\nconsistently. No time limit is given for the selection process,\nbut the assessment time should be longer than 30 seconds\nfor each question. Each participant conducts 40 rounds of\n7\nContent\nRound 1\nStyle\nRound 20\nOurs StyleFormer IEST AdaAttN ArtFlow MCC MAST AAMS SANet Avatar AdaIN\nFig. 5. Visualization of the content leak issue. Top/bottom: results after running the same stylization process using a certain method after the 1 st and the 20 th\nround, respectively.\nContent Style Sinusoidal PE CAPE\nFig. 6. Comparisons of sinusoidal PE and CAPE using content images with\nrepetitive patterns.\nAdaAttN ArtFlow MCC AAMS AdaIN\nOverall 44.8% 25.7% 25.5% 18.2% 23.5%\nContent 45.7% 28.3% 20.6% 19.4% 31.7%\nStyle 23.6% 16.7% 27.8% 14.8% 13.5%\nTABLE III\nUSER STUDY RESULTS . EACH NUMBER REPRESENTS THE PERCENTAGE OF\nVOTES THAT THE CORRESPONDING METHOD IS PREFERRED TO OURS ,\nUSING THE CRITERIA OF OVERALL QUALITY , PRESERVATION OF CONTENT\nAND STYLE , RESPECTIVELY .\ncomparisons and we collect 4,000 votes for each question. We\ncount the votes that existing methods are preferred to ours and\nshow the statistical results in III. Our method is superior to\nother approaches in all three criteria of overall quality, content\npreservation, and style consistency.\nV. C ONCLUSION\nIn this work, we propose a novel framework called StyTr 2,\nfor image style transfer. Our StyTr 2 includes a content\ntransformer encoder and a style transformer encoder to capture\ndomain-speciﬁc long-range information. A transformer decoder\nis developed to translate the content sequences based on the\nreference style sequences. We also propose a content-aware\npositional encoding scheme that is semantics-aware and is\nsuitable for scale-invariant visual generation tasks. As the ﬁrst\nbaseline for style transfer using a visual transformer, StyTr 2\nalleviates the content leak problem of CNN-based models\nand provides fresh insight into the challenging style transfer\nproblem. At present, the test-time speed of our method is not\nas fast as some CNN-based approaches. Incorporating some\npriors from CNNs to speed up the computation would be an\ninteresting future approach.\nREFERENCES\n[1] A. A. Efros and W. T. Freeman, “Image quilting for texture synthesis and\ntransfer,” in Proceedings of Annual Conference on Computer Graphics\nand Interactive Techniques, 2001, pp. 341–346.\n[2] S. Bruckner and M. E. Gr ¨oller, “Style transfer functions for illustrative\nvolume rendering,” Computer Graphics Forum, vol. 26, no. 3, pp. 715–\n724, 2007.\n[3] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using\nconvolutional neural networks,” in IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2016, pp. 2414–2423.\n[4] Y . Li, N. Wang, J. Liu, and X. Hou, “Demystifying neural style transfer,”\nin International Joint Conference on Artiﬁcial Intelligence (IJCAI) , 2017.\n[5] E. Risser, P. Wilmot, and C. Barnes, “Stable and controllable neural\ntexture synthesis and style transfer using histogram losses,” arXiv preprint\narXiv:1701.08893, 2017.\n[6] X. Huang and B. Serge, “Arbitrary style transfer in real-time with adaptive\ninstance normalization,” in IEEE International Conference on Computer\nVision (ICCV), 2017, pp. 1501–1510.\n[7] Y . Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal\nstyle transfer via feature transforms,” in Advances Neural Information\nProcessing Systems (NeurIPS) , 2017, pp. 386–396.\n[8] Z. Wang, L. Zhao, H. Chen, L. Qiu, Q. Mo, S. Lin, W. Xing, and\nD. Lu, “Diversiﬁed arbitrary style transfer via deep feature perturbation,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020, pp. 7789–7798.\n[9] Y . Li, M.-Y . Liu, X. Li, M.-H. Yang, and J. Kautz, “A closed-form\nsolution to photorealistic image stylization,” in European Conference on\nComputer Vision (ECCV) , 2018, pp. 453–468.\n[10] T. Lin, Z. Ma, F. Li, D. He, X. Li, E. Ding, N. Wang, J. Li, and X. Gao,\n“Drafting and revision: Laplacian pyramid network for fast high-quality\nartistic style transfer,” in IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2021.\n[11] J. An, T. Li, H. Huang, L. Shen, X. Wang, Y . Tang, J. Ma, W. Liu, and\nJ. Luo, “Real-time universal style transfer on high-resolution images via\nzero-channel pruning,” arXiv preprint arXiv:2006.09029 , 2020.\n[12] M. Lu, H. Zhao, A. Yao, Y . Chen, F. Xu, and L. Zhang, “A closed-form\nsolution to universal style transfer,” IEEE/CVF International Conference\non Computer Vision (ICCV) , 2019.\n[13] J. An, H. Xiong, J. Huan, and J. Luo, “Ultrafast photorealistic style\ntransfer via neural architecture search,” in AAAI Conference on Artiﬁcial\nIntelligence (AAAI), vol. 34, 2020, pp. 10 443–10 450.\n[14] H. Wang, Y . Li, Y . Wang, H. Hu, and M.-H. Yang, “Collaborative\ndistillation for ultra-resolution universal style transfer,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n1860–1869.\n[15] D. Y . Park and K. H. Lee, “Arbitrary style transfer with style-attentional\nnetworks,” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019, pp. 5880–5888.\n[16] Y . Deng, F. Tang, W. Dong, W. Sun, F. Huang, and C. Xu, “Arbitrary style\ntransfer via multi-adaptation network,” in ACM International Conference\non Multimedia, 2020, pp. 2719–2727.\n[17] Y . Deng, F. Tang, W. Dong, H. Huang, C. Ma, and C. Xu, “Arbitrary\nvideo style transfer via multi-channel correlation,” in AAAI Conference\non Artiﬁcial Intelligence (AAAI) , 2021, pp. 1210–1217.\n[18] Y . Yao, J. Ren, X. Xie, W. Liu, Y .-J. Liu, and J. Wang, “Attention-aware\nmulti-stroke style transfer,” in IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2019, pp. 1467–1475.\n8\n \n \n256×256\nSinusoidal\nPE\n256×256\nCAPE\n512×512\nSinusoidal\nPE\n512×512\nCAPE\nStyle\nContent\nFig. 7. Comparisons of sinusoidal PE and CAPE using content images with two different resolutions.\nLearnable PE Sinusoidal PE CAPE\nFig. 8. Normalized attention scores of different types of PE.\n[19] S. Liu, T. Lin, D. He, F. Li, M. Wang, X. Li, Z. Sun, Q. Li, and E. Ding,\n“Adaattn: Revisit attention mechanism in arbitrary neural style transfer,”\nin Proceedings of the IEEE International Conference on Computer Vision,\n2021.\n[20] Y . Jiang, S. Chang, and Z. Wang, “Transgan: Two transformers can make\none strong gan,” arXiv preprint arXiv:2102.07074 , 2021.\n[21] J. An, S. Huang, Y . Song, D. Dou, W. Liu, and J. Luo, “ArtFlow:\nUnbiased image style transfer via reversible neural ﬂows,” in IEEE/CVF\nConferences on Computer Vision and Pattern Recognition (CVPR) , 2021,\npp. 862–871.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems (NeurIPS) , 2017.\n[23] S. Paul and P.-Y . Chen, “Vision transformers are robust learners,” arXiv\npreprint arXiv:2105.07581, 2021.\n[24] M. Raghu, T. Unterthiner, S. Kornblith, C. Zhang, and A. Dosovitskiy,\n“Do vision transformers see like convolutional neural networks?” arXiv\npreprint arXiv:2108.08810, 2021.\n[25] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time style\ntransfer and super-resolution,” in European Conference on Computer\nVision (ECCV). Springer, 2016, pp. 694–711.\n[26] C. Li and M. Wand, “Precomputed real-time texture synthesis with\nmarkovian generative adversarial networks,” in European Conference on\nComputer Vision (ECCV) , 2016, pp. 702–716.\n[27] D. Chen, L. Yuan, J. Liao, N. Yu, and G. Hua, “Stylebank: An explicit\nrepresentation for neural image style transfer,” in IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , 2017, pp. 1897–\n1906.\n[28] V . Dumoulin, J. Shlens, and M. Kudlur, “A learned representation for\nartistic style,” in International Conference on Learning Representations\n(ICLR), 2016.\n[29] M. Lin, F. Tang, W. Dong, X. Li, C. Xu, and C. Ma, “Distribution aligned\nmultimodal and multi-domain image stylization,” ACM Transactions on\nMultimedia Computing, Communications, and Applications, vol. 17, no. 3,\npp. 96:1–96:17, 2021.\n[30] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for\ngenerative adversarial networks,” in IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2019, pp. 4401–4410.\n[31] X. Huang, M.-Y . Liu, S. Belongie, and J. Kautz, “Multimodal unsuper-\nvised image-to-image translation,” in European Conference on Computer\nVision (ECCV), 2018, pp. 172–189.\n[32] Z. Wu, C. Song, Y . Zhou, M. Gong, and H. Huang, “Efanet: Exchange-\nable feature alignment network for arbitrary style transfer,” in AAAI\nConference on Artiﬁcial Intelligence (AAAI) , 2020, pp. 12 305–12 312.\n[33] J. Svoboda, A. Anoosheh, C. Osendorfer, and J. Masci, “Two-stage\npeer-regularized feature recombination for arbitrary image style transfer,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020, pp. 13 816–13 825.\n[34] X. Wu, Z. Hu, L. Sheng, and D. Xu, “Styleformer: Real-time arbitrary\nstyle transfer via parametric style composition,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2021, pp.\n14 618–14 627.\n[35] H. Chen, Z. Wang, H. Zhang, Z. Zuo, A. Li, W. Xing, D. Lu et al. ,\n“Artistic style transfer with internal-external learning and contrastive\nlearning,” Advances in Neural Information Processing Systems , vol. 34,\n2021.\n[36] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[37] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” in Advances in Neural Information Processing\nSystems (NeurIPS), 2020, pp. 1877–1901.\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , 2019, pp. 4171–4186.\n[39] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\n9\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.\n[40] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” Preprint, 2018.\n[41] N. Dai, J. Liang, X. Qiu, and X. Huang, “Style transformer: Unpaired\ntext style transfer without disentangled latent representation,” in Annual\nMeeting of the Association for Computational Linguistics(ACL) , 2019.\n[42] Y . Xu, H. Wei, M. Lin, Y . Deng, K. Sheng, M. Zhang, F. Tang, W. Dong,\nF. Huang, and C. Xu, “Transformers in computational visual media: A\nsurvey,” Computational Visual Media , vol. 8, no. 1, pp. 33–62, 2022.\n[43] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision (ECCV) , 2020, pp. 213–229.\n[44] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,” inInternational\nConference on Learning Representations (ICLR) , 2021.\n[45] Z. Dai, B. Cai, Y . Lin, and J. Chen, “Up-detr: Unsupervised pre-training\nfor object detection with transformers,” in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2021.\n[46] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng, T. Xi-\nang, P. H. Torr, and L. Zhang, “Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 2021.\n[47] Y . Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia, “End-\nto-end video instance segmentation with transformers,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 2021.\n[48] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition at\nscale,” in International Conference on Learning Representations (ICLR) ,\n2021.\n[49] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, M. Tomizuka, K. Keutzer, and\nP. Vajda, “Visual transformers: Token-based image representation and\nprocessing for computer vision,” arXiv preprint arXiv:2006.03677 , 2020.\n[50] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International Conference on\nMachine Learning (ICML) , 2020, pp. 1691–1703.\n[51] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted windows,”\nin IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021.\n[52] Y . Xu, Z. Zhang, M. Zhang, K. Sheng, K. Li, W. Dong, L. Zhang, C. Xu,\nand X. Sun, “Evo-ViT: Slow-fast token evolution for dynamic vision\ntransformer,” in AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2022.\n[53] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu,\nand W. Gao, “Pre-trained image processing transformer,” in IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 2021.\n[54] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\nposition representations,” in Annual Conference of the North American\nChapter of the Association for Computational Linguistics (NAACL) , 2018.\n[55] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n“Xlnet: Generalized autoregressive pretraining for language understanding,”\nin Advances in Neural Information Processing Systems (NeurIPS) , 2019.\n[56] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou,\nW. Li, and P. J. Liu, “Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer,” Journal of Machine Learning Research ,\nvol. 21, no. 140, pp. 1–67, 2020.\n[57] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\nwith disentangled attention,” in International Conference on Learning\nRepresentations (ICLR), 2021.\n[58] R. Xu, X. Wang, K. Chen, B. Zhou, and C. C. Loy, “Positional encoding\nas spatial inductive bias in gans,” in IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2021.\n[59] M. A. Islam, S. Jia, and N. D. B. Bruce, “How much position information\ndo convolutional neural networks encode?” in International Conference\non Learning Representations (ICLR) , 2020.\n[60] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in\ncontext,” in European Conference on Computer Vision (ECCV) , 2014,\npp. 740–755.\n[61] F. Phillips and B. Mackintosh, “Wiki art gallery, inc.: A case for critical\nthinking,” Issues in Accounting Education , vol. 26, no. 3, pp. 593–608,\n2011.\n[62] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[63] R. Xiong, Y . Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang,\nY . Lan, L. Wang, and T. Liu, “On layer normalization in the transformer\narchitecture,” in International Conference on Machine Learning (ICML) ,\n2020, pp. 10 524–10 533.\n[64] L. Sheng, Z. Lin, J. Shao, and X. Wang, “Avatar-net: Multi-scale zero-\nshot style transfer by feature decoration,” in Computer Vision and Pattern\nRecognition (CVPR), 2018 IEEE Conference on , 2018, pp. 1–9.\n[65] T. Park, A. A. Efros, R. Zhang, and J.-Y . Zhu, “Contrastive learning\nfor unpaired image-to-image translation,” in European Conference on\nComputer Vision, 2020.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7183898687362671
    },
    {
      "name": "Computer science",
      "score": 0.6848700642585754
    },
    {
      "name": "Locality",
      "score": 0.5581120252609253
    },
    {
      "name": "Encoder",
      "score": 0.5476937294006348
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5260748267173767
    },
    {
      "name": "Computer vision",
      "score": 0.3820687234401703
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3320804238319397
    },
    {
      "name": "Engineering",
      "score": 0.0927078127861023
    },
    {
      "name": "Voltage",
      "score": 0.07734894752502441
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}