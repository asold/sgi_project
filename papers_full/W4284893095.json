{
  "title": "3D Part Assembly Generation With Instance Encoded Transformer",
  "url": "https://openalex.org/W4284893095",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101554884",
      "name": "Rufeng Zhang",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5065667064",
      "name": "Tao Kong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100627299",
      "name": "Weihao Wang",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5090419388",
      "name": "Xuan Han",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A5010299064",
      "name": "Mingyu You",
      "affiliations": [
        "Tongji University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997180840",
    "https://openalex.org/W3010196549",
    "https://openalex.org/W2903545703",
    "https://openalex.org/W2961222043",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3109636218",
    "https://openalex.org/W6778980215",
    "https://openalex.org/W3185933454",
    "https://openalex.org/W2961368225",
    "https://openalex.org/W2968717979",
    "https://openalex.org/W3091444023",
    "https://openalex.org/W3205009189",
    "https://openalex.org/W2077678691",
    "https://openalex.org/W2161960196",
    "https://openalex.org/W2157988056",
    "https://openalex.org/W3003776782",
    "https://openalex.org/W6640963894",
    "https://openalex.org/W2612843093",
    "https://openalex.org/W3034195670",
    "https://openalex.org/W2985840885",
    "https://openalex.org/W3109125268",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W6748208425",
    "https://openalex.org/W3034917353",
    "https://openalex.org/W4297598254",
    "https://openalex.org/W3100891210",
    "https://openalex.org/W1959608418"
  ],
  "abstract": "It is desirable to enable robots capable of automatic assembly. Structural understanding of object parts plays a crucial role in this task yet remains relatively unexplored. In this paper, we focus on the setting of furniture assembly from a complete set of part geometries, which is essentially a 6-DoF part pose estimation problem. We propose a multi-layer transformer-based framework that involves geometric and relational reasoning between parts to update the part poses iteratively. We carefully design a unique instance encoding to solve the ambiguity between geometrically-similar parts so that all parts can be distinguished. In addition to assembling from scratch, we extend our framework to a new task called in-process part assembly. Analogous to furniture maintenance, it requires robots to continue with unfinished products and assemble the remaining parts into appropriate positions. Our method achieves far more than 10% improvements over the current state-of-the-art in multiple metrics on the public PartNet dataset. Extensive experiments and quantitative comparisons demonstrate the effectiveness of the proposed framework.",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022 1\n3D Part Assembly Generation with\nInstance Encoded Transformer\nRufeng Zhang†⋆, Tao Kong⋆, Weihao Wang, Xuan Han and Mingyu You ‡\nAbstract—It is desirable to enable robots capable of auto-\nmatic assembly. Structural understanding of object parts plays\na crucial role in this task yet remains relatively unexplored.\nIn this paper, we focus on the setting of furniture assembly\nfrom a complete set of part geometries, which is essentially\na 6-DoF part pose estimation problem. We propose a multi-\nlayer transformer-based framework that involves geometric and\nrelational reasoning between parts to update the part poses\niteratively. We carefully design a unique instance encoding to\nsolve the ambiguity between geometrically-similar parts so that\nall parts can be distinguished. In addition to assembling from\nscratch, we extend our framework to a new task called in-\nprocess part assembly. Analogous to furniture maintenance,\nit requires robots to continue with unﬁnished products and\nassemble the remaining parts into appropriate positions. Our\nmethod achieves far more than 10% improvements over the\ncurrent state-of-the-art in multiple metrics on the public PartNet\ndataset. Extensive experiments and quantitative comparisons\ndemonstrate the effectiveness of the proposed framework.\nIndex Terms—Assembly, Deep Learning for Visual Perception,\nAI-Enabled Robotics.\nI. I NTRODUCTION\nA\nUTOMATIC assembly has excellent potential in 3D\ncomputer vision and is also a desirable functionality\nfor many intelligent robot systems. Yet assembling a well-\nconnected and structurally-stable IKEA furniture from scratch,\neven for human beings, is a cumbersome and time-consuming\nprocess. The task of 3D part assembly involves an extremely\nlarge solution space when there is no instruction manual or\nstep-by-step video demonstration as guidance.\nGiven geometry information ( i.e., part point cloud) of each\npart, we focus on accurately estimating the 6-DoF posture of\nthese parts so that they can be composed into a complete\nobject ( e.g., furniture, see Fig. 1(a)). Compared with naive\npose estimation, it requires joint prediction of all partial poses\nin part assembly. As part of the whole, part poses follow a\nseries of rigid connections and relationships, such as symmetry\nand parallelism. Unlike many structure-aware shape modeling\nworks [1], [2], [3], [4] that specify the semantics or granularity\nManuscript received February, 24, 2022; Revised May, 23, 2022; Accepted\nJune, 20, 2022.\nThis paper was recommended for publication by Editor Cesar Cadena\nLerma upon evaluation of the Associate Editor and Reviewers’ comments.\nRufeng Zhang, Weihao Wang, Xuan Han and Mingyu You\nare with the College of Electronic and Information Engineering,\nFrontiers Science Center for Intelligent Autonomous Systems, Tongji\nUniversity, Shanghai, China. {cxrfzhang, wwhtju, hanxuan,\nmyyou}@tongji.edu.cn. Tao Kong is with ByteDance AI Lab,\nBeijing, China. kongtao@bytedance.com.\n†Intern at ByteDance. ⋆Equal contribution. ‡Corresponding author.\nDigital Object Identiﬁer (DOI): see top of this page.\n/gid00002/gid00034/gid00032/gid00041/gid00047\n2\n1\nA\n(/gid00028) /gid00017/gid00028/gid00045/gid00047/gid00001/gid00002/gid00046/gid00046/gid00032/gid00040/gid00029/gid00039/gid00052\n(/gid00029) /gid00010/gid00041-/gid00043/gid00045/gid00042/gid00030/gid00032/gid00046/gid00046/gid00001/gid00002/gid00046/gid00046/gid00032/gid00040/gid00029/gid00039/gid00052\n1\n2 3\n4\n5\n6\n/gid00002/gid00034/gid00032/gid00041/gid00047\n/gid00010/gid00041/gid00033/gid00032/gid00045/gid00032/gid00041/gid00030/gid00032\n/gid00010/gid00041/gid00033/gid00032/gid00045/gid00032/gid00041/gid00030/gid00032\n/gid00017/gid00039/gid00028/gid00030/gid00032/gid00001/gid00032/gid00028/gid00030/gid00035/gid00001/gid00043/gid00028/gid00045/gid00047/gid00001/gid00028/gid00030/gid00030/gid00042/gid00045/gid00031/gid00036/gid00041/gid00034\n/gid00047/gid00042/gid00001/gid00047/gid00035/gid00032/gid00001/gid00046/gid00047/gid00045/gid00048/gid00030/gid00047/gid00048/gid00045/gid00032/gid00001/gid00043/gid00045/gid00036/gid00042/gid00045/gid00001\n/gid00022/gid00041/gid00031/gid00032/gid00045/gid00046/gid00047/gid00028/gid00041/gid00031/gid00001/gid00047/gid00035/gid00032/gid00001/gid00043/gid00045/gid00042/gid00034/gid00045/gid00032/gid00046/gid00046\n/gid00028/gid00041/gid00031/gid00001/gid00033/gid00036/gid00041/gid00036/gid00046/gid00035/gid00001/gid00047/gid00035/gid00032/gid00001/gid00028/gid00046/gid00046/gid00032/gid00040/gid00029/gid00039/gid00052/gid00001\nFig. 1. Illustration of two basic tasks . (a) is the regular task which aims to\nassemble objects from scratch. (b) indicates the proposed in-process task that\nrequires both the understanding of unﬁnished furniture and the relationship\nmodeling of the remaining parts.\nof the input parts in advance, robots in our setting may lack\nthese strong priors of the provided furniture. In other words,\nthe number of candidate parts is arbitrary and the semantics of\neach part is unknown, which puts forward higher requirements\nfor the generalization and robustness of the robot system.\nTo achieve this, we present a transformer-based [5] frame-\nwork to predict the 6-DoF pose for each part, including\nrigid rotation and translation. In assembly, we employ the\nself-attention mechanism to capture the part relationship con-\nstraints and the part poses are gradually reﬁned in a coarse-\nto-ﬁne manner. Several works [6], [7], [8] have attempted to\ntackle this problem lately. For example, Li et al. [6] propose\nto assemble 3D parts guided by an additional image. And\nHuang et al. [7] construct a dynamically varying part graph\nto induce the 6-DoF poses. These algorithms consistently\nadvance the ﬁeld, yet the performance still fails in household\nscenario. They typically focus on the design of relational-\nreasoning network architecture while ignoring the instance\ndifference between parts. Take a chair with four legs for\nexample. On the one hand, although the legs share the same\ngeometry shape, they differ in practical assembly pose. The\ngeometry-identical input parts usually result in intra-class con-\nﬂicts and failure in accurate prediction of differentiated poses.\nOn the other hand, some parts may have similar geometrical\nstructures. The square-shaped part could be a chair seat or\narXiv:2207.01779v1  [cs.RO]  5 Jul 2022\n2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022\nchair back; distinguishing the geometric subtlety of inter-class\ndetails leads to a struggling problem.\nInspired by such ﬁndings, we propose Instance Encoding ,\nwhich considers both intra-class and inter-class (dis)similarity.\nCorrespondingly, inter-class encoding aims to make all parts\ndistinguishable while intra-class encoding emphasizes the rela-\ntionships among those identical parts. The encoding algorithm\nis designed to be independent of the order or attributes of the\ninput parts. It allows the model to attend relative discrepancy\neffortlessly. More details are presented in the later section. At\nthe core of our method, the encoded instance vector is attached\nto the self-attention module to alleviate the confusion between\nparts.\nBesides assembling from scratch, to continue with unﬁn-\nished products is also a crucial skill for robots to generalize the\napplicability in reality, such as furniture maintenance. We term\nthe new task as in-process part assembly (see Fig. 1(b)). Taking\na piece of in-process furniture and several unassembled parts\nas input, the robot needs to make a smooth and collision-free\nassembly. This task is particularly challenging as it requires\na prior structural understanding of in-process shape and then\nplacing the new parts in their appropriate locations. To this\nend, we further extend our framework using an encoder-\ndecoder paradigm. The unﬁnished furniture is fed to the\nencoder to generate feature memory and the remaining parts\nquery the information from the memory through the decoder.\nTo the best of our knowledge, our algorithm is the ﬁrst\nto assemble furniture with an in-process inventory, and the\nsurplus part.\nFollowing [7], [8], we demonstrate the effectiveness of\nour approach on the ﬁnest-grained part granularity in the\nlarge-scale PartNet [9] dataset. Instance Encoding increases\nthe network’s capacity to learn intricate structures, boosting\nthe performance tremendously. Overall, our model achieves\nfar more than 10% improvements in both part accuracy and\nconnectivity accuracy over SOTA model RGL-Net [8], which\nemploys a time-consuming progressive strategy. Extensive\nexperiments also address the critical aspects of our scheme,\nincluding the architectural design and the advantage/limit of\nInstance Encoding . The main contributions of this work can\nbe summarized as follows.\n• We present a novel transformer-based framework involv-\ning geometric and relational reasoning between parts for\n3D part assembly, surpassing existing best results by a\nlarge margin.\n• We propose to encode instance-aware information in\nthe task of part assembly. The coding vector takes into\naccount both intra-class and inter-class (dis)similarity,\nsigniﬁcantly improving the accuracy of part placement\nand part connectivity.\n• We introduce a novel task termed in-process part as-\nsembly, along with the learning-based solution. Different\nfrom existing frameworks that always require the robot to\nassemble from scratch, the new problem emphasizes the\nassembly from an intermediate state. And the extensibility\nof our framework considerably generalizes its applicabil-\nity in real life.\nII. R ELATED WORK\nAssembly-based 3D modelling. Automated part assembly is\na long-standing research challenge. Previous works involving\npart assembly could be roughly categorized into two groups.\nSeveral methods [10], [11], [12] emphasize motion planning,\nactuator control, and 6-DoF grasping in robotics. Speciﬁcally,\nLitvak et al. [10] propose a two-stage pose estimation pipeline\nto learn robotic assembly with depth images. Shao et al. [11]\nutilize ﬁxtures to help the robot learn manipulation skills au-\ntonomously. To ensure the uncertainty of robust sensorimotor\ncontrol, Zachares et al. [12] formulate multi-object assembly\nin a hierarchical way.\nThe second family of solutions focuses on the problem of\npose or joint estimation for part assembly, which has some\nsimilarities with approaches in the vision and graphics com-\nmunity. To our knowledge, [13] is the ﬁrst to construct new\n3D geometric objects by assembling parts from a large house-\nhold database. Several works [14], [15] use the probabilistic\ngraph to learn semantic and stylistic relationships between\ncomponents in a shape repository. More recently, Dubrovina et\nal. [16] propose to model the semantic structure-aware 3D\nshape for part-level shape manipulation. PAGENet [1] intro-\nduces a part-aware network to generate semantic parts and\nestimate joint poses sequentially.\nThese methods either rely on the third-party shape repos-\nitory to query parts, or assume strong priors of structure-\nvariable (i.e., the component is not rigid and can be arbitrarily\nscaled and distorted) or semantic-aware ( e.g., for a particular\nchair, it has four legs, a backrest, a seat and two arms). Here\nwe tackle the problem in a more practical setting. No shape\ndatabase is provided, no semantic information is known, no\npart structure could be tuned. Instead, it takes arbitrary parts\nas input without any semantic knowledge, and estimates per-\npart poses jointly for the plausible and structurally stable shape\nassembly. Several works [6], [7], [8] formulate a similar task.\nSpeciﬁcally, DGL-Net [7] iteratively reﬁnes the part poses\nwith a dynamic part graph. Narayan et al. [8] explore a\nprogressive strategy via the recurrent graph learning frame-\nwork. These algorithms solve the assembly task as graph-level\npredictions, while we employ Transformer [5] to model the\nstructural relationships. As far as we know, we are the ﬁrst\nto demonstrate the effectiveness of Transformer in the task of\n3D part assembly.\nStructure-aware shape generation. Deep generative mod-\nels [17], [18] are the powerful way to shape generation tasks\nand have achieved tremendous success in just a few years.\nFor example, GRASS [19] employs a generative recursive\nautoencoder to encode part structure in a hierarchical way.\nWang et al. [3] present a global-to-local GAN-based paradigm\nthat performs overall-structure generation and per-part seg-\nmentation. Moreover, [20] extends the learned semantically\nplausible variations into structure editing through a conditional\nvariational autoencoder (V AE). Taking semantic-aware parts\nas input, SAGNet [4] introduces a structure-aware generative\nmodel to jointly learn the part geometry as well as the pairwise\nrelationships between parts. [21] decomposes the global shape\nstructure and generates ﬁne-grained part geometries using a\nZHANG et al.: 3D PART ASSEMBLY GENERATION WITH INSTANCE ENCODED TRANSFORMER 3\n/gid00020/gid00032/gid00039/gid00033-/gid00002/gid00047/gid00047/gid00032/gid00041/gid00047/gid00036/gid00042/gid00041\n×\n/gid00017/gid00045/gid00032/gid00031/gid00036/gid00030/gid00047/gid00042/gid00045\n{( , )} =1\n/gid00020/gid00035/gid00028/gid00045/gid00032/gid00031/gid00001/gid00017/gid00042/gid00036/gid00041/gid00047/gid00015/gid00032/gid00047\n/gid00010/gid00041/gid00046/gid00047/gid00028/gid00041/gid00030/gid00032/gid00001/gid00006/gid00041/gid00030/gid00042/gid00031/gid00036/gid00041/gid00034\n... /gid00006/gid00041/gid00030/gid00042/gid00031/gid00032/gid00045\n...\n...\n...\n...\n1\n−1\n/gid00060-/gid00013/gid00028/gid00052/gid00032/gid00045\n/gid00017/gid00032/gid00045/gid00030/gid00032/gid00043/gid00047/gid00045/gid00042/gid000411 1\nFig. 2. The overall architecture of our method , which mainly consists of four modules: (a) shared PointNet for feature extraction, (b) transformer-encoder\nto reason the inter-part relationships, (c) MLP predictor for pose estimation and (d) Instance Encoding to make input parts distinguishable. Here ⨁denotes\nthe concatenate operation of part feature and encoded vector.\ntwo-level V AE. Recently, Mo et al. [22] propose to generate\n3D point cloud shapes with geometric variations conditioned\non the part-tree hierarchy. Most of these works directly map\nnew part shape from the latent code, while we focus on the\nrigid transformation of existing parts for part assembly.\nIII. O UR METHOD\nLet P= {pi}N\ni=1 represent a set of part point clouds, where\npi ∈Rnpc×3 and N denotes the number of parts which may\nvary for different 3D shapes. Our goal is to predict a set of\n6-DoF part poses T = {(Ri,ti)}N\ni=1 in SE(3) space, where\nRi ∈R4 and ti ∈R3 denotes the rigid rotation and translation\nfor each part, respectively. We use unit quaternion to represent\nrotation, i.e., ∥Ri∥2 = 1. And the complete shape can be\nassembled into S = T1(p1 )∪T2(p2 )∪···∪ TN (pN ), in which\nTi is the joint transformation of (Ri,ti).\nIn this work, we present a multi-layer transformer-based\nframework to assemble 3D shapes in a coarse-to-ﬁne manner.\nWe apply the self-attention module to reason the inter-part\nrelationships. And Instance Encoding is introduced to resolve\nambiguity between parts, in which case some parts may share\nsimilar geometries. In addition, we explore a memory-query\nparadigm to process the in-process inventory. The overall\npipeline is illustrated in Fig. 2.\nA. Instance Encoded Transformer\nOur system takes a set of 3D point clouds as input and\njointly estimates the poses of parts in the space of SE(3). A\npart cloud is an unordered set of npc points, where each point\nis associated with its 3-dimensional XYZ coordinates. We ﬁrst\nuse a shared PointNet [23] to extract the global permutation-\ninvariant feature fi\nfeat for each input part pi. Then these part\nfeatures are sent into transformer-encoder to model the inter-\npart relations, denoted as {fi\nenc}N\ni=1. We apply a simple feed-\nforward network (FFN) to make the ﬁnal pose estimation. As\nfor the task of in-process part assembly, the candidate part fj\ndec\nis processed with transformer-decoder and queries the adjacent\ninformation from the part encoder-memory {fi\nenc}N′\ni=1 (here\nN′<N ).\nInstance encoding. Since both the PointNet and Transformer\nextract features in a permutation-invariant way, it inevitably\nfalls into the dilemma of inter-part conﬂict, resulting in poses\nfailure. For example, the geometry of the four legs of a chair\nis identical, and it is difﬁcult for the network to distinguish\nwhich portion is in the upper-left corner and which one should\nbe the lower-right leg. Besides, some parts, such as the chair\nbackrest and chair seat, are very similar in appearance ( e.g.,\ntheir geometries are both square-shaped), and the robot may\nmake the opposite judgments.\nTo alleviate this confusion, we propose to encode each\npart with a unique instance vector, termed Instance En-\ncoding. Speciﬁcally, we ﬁrst cluster the input part clouds\n{pi}N\ni=1 into sets of geometrically-equivalent part classes\nC = {C1,C2,··· ,Ck}, in which C1 = {pi}N1\ni=1, C2 =\n{pi}N1+N2\ni=N1+1, etc. Following [6], we calculate axis-aligned\nbounding boxes (AABB) for all the parts and measure the\nsimilarity of these 3D boxes. The parts will be grouped\ntogether when the difference between the corresponding en-\nclosing boxes is less than a certain threshold ( e.g., 0.1). In\nthe case of a chair, the four legs are clustered together as\nthey have identical geometries. Note Cis a disjoint complete\nset, i.e., Ck ∩Cl = ∅ when k ̸= l and ∪K\nk=1Ck = P.\nInstance Encoding consists of two terms: inter-class encoding\nand intra-class encoding. In inter-class encoding, we treat each\npart as an individual category and encode it with a one-\nhot vector in the length of N. It ensures that each part has\na unique property, making it easy to distinguish. In intra-\nclass encoding, geometrically-equivalent parts are attached\nwith the same code, which enforces the connections among\nthese highly-correlated parts. Instance Encoding is simple but\nefﬁcient, and we demonstrate it is the key to distinguishing\nindiscernible parts in the task of part assembly. We detail the\ngeneration procedure in Algorithm 1.\nEncoder. We employ the transformer-encoder to learn the\nrelationships between parts. The encoder applies multiple self-\nattention layers that aggregate information ( e.g., geometry and\nposture) from the entire input sequence (here is a set of parts),\nthus making it an optimal option for part assembly. We follow\nthe standard formulation of transformer-encoder, and we refer\n4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022\nAlgorithm 1: Instance Encoding\nInput:\n3D point clouds, P= {pi}N\ni=1;\nGeometrically-equivalent classes, C= {Ck}K\nk=1.\nOutput:\nInstance encoding set, V= {}.\nfor i= 1; i≤N; i= i+ 1do\nvi\ninter = fone hot(i);\nk←1;\nwhile k≤K do\nif pi ∈Ck then\nvi\nintra = fone hot(k);\nvi ←(vi\ninter,vi\nintra);\nAdd vi to V;\nBreak;\nelse\nk←k+ 1;\nend\nend\nend\nthe reader to Vaswani et al. [5] for details. The positional\nencoding [24] is omitted as the input already contains the\ninformation about 3-dimensional XYZ coordinates. Instead,\nInstance Encoding is attached to the input of each attention\nlayer to make parts distinct.\nPredictor. We carry out a 3-layer perceptron for the ﬁnal pre-\ndiction. Speciﬁcally, the ﬁrst two fully-connected layers have\nthe channel of 256, followed by ReLU activation function.\nAnd the last linear projection layer is used to estimate the\npart pose Ti ∈R7, including the 4-dimensional rigid rotation\nand 3-dimensional translation. We apply tanh operation to\nthe translation vector, constraining the part center offset to (-\n1, 1). To ensure the output of unit Quaternion prediction, we\nnormalize the rotation vector so that ∥Ri∥2 = 1. The feed-\nforward network (FFN) works on all outputs of encoder layers\nand the parameters are shared. Following [7], [8], the previous\npredicted poses are attached to the next step for more coherent\npose evolution.\nDecoder. Actually, the aforementioned modules have already\nbeen well qualiﬁed for the task of part assembly. In this sub-\nsection, we extend the application of our framework to a new\ntask, termed in-process part assembly. Previous works always\nuse scattered parts and assemble them into a complete shape\nfrom scratch. While sometimes an object is already being\nprocessed, we only need to assemble the remaining parts.\nWe append the transformer-decoder to tackle this problem.\nThe decoder takes candidate parts as input, and queries the\nrelationship information from in-process shapes (these features\nhave been obtained through the encoder). The part queries are\ntransformed into output embeddings by the decoder. And they\nare then decoded into poses using the feed-forward predictor\n(described in the last subsection). With self- and encoder-\ndecoder attention over these embeddings, the model globally\nreasons about all parts together using pair-wise relations\nbetween them, while being able to assemble new parts to in-\n/gid00006/gid00041/gid00030/gid00042/gid00031/gid00032/gid00045/gid00005/gid00032/gid00030/gid00042/gid00031/gid00032/gid00045\n/gid00017/gid00028/gid00045/gid00047/gid00001/gid00014/gid00032/gid00040/gid00042/gid00045/gid00052\n...\n/gid00044/gid00048/gid00032/gid00045/gid00052/gid00001/gid00058\n/gid00044/gid00048/gid00032/gid00045/gid00052/gid00001/gid00059\n{ } =1\n−2\n−1\nFig. 3. An overview of in-process assembly pipeline . It consists of two\nbranches: encoder branch and decoder branch. The input includes a piece of\nunﬁnished object and several parts. The incomplete product is fed to encoder\nto generate part feature memory. And the remaining parts ﬁrst query the\nrelation information from memory and then output their poses.\nprocess shape smoothly. The pipeline is shown in Fig. 3.\nPartDrop. In a standard paradigm of part assembly, all parts\nare typically input to the encoder for relation modeling. How-\never, there is an information vulnerability in the in-process\nobject as it has no access to capture the holistic shape. If\nwe directly use the pre-trained part-assembly encoder, the\nmodel fails to support incomplete-shape memory because of\nthe information gap between these two tasks. Therefore, we\nadopt a simple solution to this problem. The input parts are\nrandomly dropped during training so that the model learns to\ncapture in-process shape features. PartDrop largely improves\nthe capacity of encoder in the case of in-process objects.\nB. Training and Losses\nIn the task of part assembly, solutions are usually not\nunique but have multiple potential options. The locations of\ngeometrically-equivalent parts are interchangeable, and the\ndecorative portion could be placed in any suitable corner.\nSimilar to [7], [8], we employ the Min-of-N (MoN) loss [25]\nfor uncertainty modeling, and random noise is incorporated to\nexplore structural diversity. We conduct Hungarian matching\nfor all the geometrically-equivalent part groups. Considering\nthe overall framework as Fand the ground-truth poses as F∗,\nwe deﬁne the MoN loss as:\nLMoN = min\nrj∼N(0,1)\n1≤j≤n\nL(F(P,rj),F∗(P)) (1)\nwhere rj is the IID random noise sampled from unit Gaussian\ndistribution N(0,1). Given a set of part point clouds P, F\nmakes n predictions by perturbing the input with n random\nvectors rj. Intuitively, it ensures at least one prediction as close\nas the ground-truth space. Following [7], [8], we set n = 5\nin the experiment and supervise the model with global and\npart-wise losses.\nWe apply the Euclidean loss to measure the distance be-\ntween the predicted translation ti and ground-truth translation\nt∗\ni for each part, formally,\nLt =\nN∑\ni=1\n∥ti −t∗\ni ∥2\n2 (2)\nZHANG et al.: 3D PART ASSEMBLY GENERATION WITH INSTANCE ENCODED TRANSFORMER 5\nAnd Chamfer distance (CD) loss is used to supervise rigid\nrotation:\nLr =\nN∑\ni=1\n( ∑\nx∈Ri(pi)\nmin\ny∈\nR∗\ni (pi)\n∥x−y∥2\n2 +\n∑\ny∈R∗\ni (pi)\nmin\nx∈\nRi(pi)\n∥x−y∥2\n2\n)\n(3)\nin which Ri(pi) and R∗\ni (pi) represent the rotated part points\npi using the estimated rotation Ri and the ground-truth R∗\ni , re-\nspectively. The loss searches the nearest neighbor in the other\nset for each point, enabling the tolerance to part symmetry.\nFinally, we deﬁne the global CD loss to learn the holistic\nassembled shape S:\nLs =\n∑\nx∈S\nmin\ny∈S∗\n∥x−y∥2\n2 +\n∑\ny∈S∗\nmin\nx∈S\n∥x−y∥2\n2 (4)\nwhere S is the assembled shape and S∗ denotes the ground\ntruth. The overall loss is deﬁned as follows:\nL= λtLt + λrLr + λsLs (5)\nWe extend the part assembly task to in-process assembly\ntask by simply adding a decoder module to our framework.\nThe parameters in encoder are frozen and initialized with the\npre-trained part-assembly model. We randomly sample one\npart in a complete shape and extract its feature using decoder.\nThe remains are input to encoder to generate in-process object\nmemory. The loss functions are similar to the ones in part\nassembly.\nC. Evaluation Metrics\nPart assembly. Similar to [7], [8], we make multiple predic-\ntions by adding different noises to the input parts and ﬁnd the\nmost similar shape to ground truth using Minimum Matching\nDistance (MMD) [26]. The quality of assembled shapes is\nevaluated with three metrics: shape Chamfer distance , part\naccuracy and connectivity accuracy. Shape Chamfer distance\n(SCD) can be directly obtained through Eq. (4). And we use\npart accuracy (PA) to measure the percentage of matched parts\nwithin a certain Chamfer distance threshold, formally,\nPA= 1\nN\nN∑\ni=1\n(( ∑\nx∈Ri(pi)\nmin\ny∈R∗\ni (pi)\n∥x−y∥2\n2\n+\n∑\ny∈R∗\ni (pi)\nmin\nx∈Ri(pi)\n∥x−y∥2\n2\n)\n<τp\n)\n, (6)\nwhere we set the threshold τp = 0.01.\nFinally, connectivity accuracy (CA) is introduced to evaluate\nthe quality of connections between adjacent parts. For each\nconnected-part pair (p∗\ni ,p∗\nj ) in the object space, we ﬁrstly ﬁnd\nthe point in part p∗\ni closest to p∗\nj and deﬁne it as contact point\nc∗\nij, so does c∗\nji. The contact-point pair (c∗\nij,c∗\nji) is transformed\ninto part canonical space as (cij,cji), correspondingly. We\nformulate the connectivity accuracy as Eq. (7):\nCA= 1\n|C|\n∑\n{cij,cji∈C}\n∥Ti(cij) −Tj(cji)∥2\n2 <τc (7)\nwhere Cis the set of all contact point pairs in a shape and\nτc = 0.01 denotes the threshold of Chamfer distance.\n/gid00008/gid00045/gid00042/gid00048/gid00041/gid00031/gid00001/gid00047/gid00045/gid00048/gid00047/gid00035 /gid00002/gid00046/gid00046/gid00032/gid00040/gid00029/gid00039/gid00052/gid00001/gid00058/gid00002/gid00046/gid00046/gid00032/gid00040/gid00029/gid00039/gid00052/gid00001/gid00059\nFig. 4. Multiple plausible assembly results . The assembled chairs are not\nexactly the same as the ground truth, yet their structure is reasonable.\nIn-process part assembly. We employ the same evaluation\nmetrics for the in-process part assembly task. We sample\none part from a complete shape in turn and measure the\nperformance of assembled part individually. The result is the\naverage of all candidate parts in a shape. Formally,\n1\nN\nN∑\ni=1\nD\n(\nTi(pi) ∪S∗\ni\n)\n(8)\nwhere S∗\ni denotes the ground-truth incomplete shape that\nexcludes part pi and Dcan be any metric described above.\nIV. E XPERIMENTS\nA. Dataset\nOur experiments are conducted on PartNet [9] dataset using\nthe metrics described above. PartNet has 24 categories with\nﬁne-grained part annotations, and we select the three largest\ncategories (Chair, Table, and Lamp) for both training and\nevaluation. In total, we have 6,323 chairs, 8,218 tables, 2,207\nlamps in ﬁnest-grained level, and we follow the default train/-\nvalidation/test splits (70%/10%/20%) in the dataset. Furthest\nPoint Sampling (FPS) is adopted to sample 1,000 points for\neach part cloud and we ﬁlter out the objects with more than\n20 parts. All input parts are normalized within the canonical\nspace using PCA.\nB. Implementation Details\nTrain details. We use the AdamW optimizer, initial learning\nrate 0.00015, weight decay 0.0001 with 1,000 epochs in all\nfor the task of part assembly. We use a mini-batch of 64 and\nall models are trained with 8 GPUs. The default number of\nencoder layers is 6 and intermediate supervision is applied to\nall the output poses of encoders to accelerate the convergence.\nFollowing [7], λt = 1, λr = 10, λs = 1.\nAs for in-process part assembly, we add a 6-layer decoder\nto learn the remaining parts. We initialize this model with a\npre-trained part-assembly model, then freeze all weights in\nencoder and train only the decoder head for 500 epochs.\nInference details. Unlike training, we only use the ﬁnal\nestimated poses to measure the quality of assembled shapes.\nTo make a fair comparison with the previous methods [7], [8],\nwe generate 10 possible shapes and ﬁnd the prediction most\nsimilar to ground truth using Minimum Matching Distance\n(MMD).\n6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022\nTABLE I\nPART ASSEMBLY RESULTS ON PARTNET DATASET .\nShape Chamfer Distance ↓ Part Accuracy ↑ Connectivity Accuracy ↑\nChair Table Lamp Chair Table Lamp Chair Table Lamp\nB-Global [1], [2] 0.0146 0.0112 0.0079 15.70 15.37 22.61 9.90 33.84 18.60\nB-LSTM [27] 0.0131 0.0125 0.0077 21.77 28.64 20.78 6.80 22.56 14.05\nB-Complement [28] 0.0241 0.0298 0.0150 8.78 2.32 12.67 9.19 15.57 26.56\nDGL-Net [7] 0.0091 0.0050 0.0093 39.00 49.51 33.33 23.87 39.96 41.70\nRGL-Net [8] 0.0087 0.0048 0.0072 49.06 54.16 37.56 32.26 42.15 57.34\nOurs 0.0054 0.0035 0.0103 62.80 61.67 38.68 48.45 56.18 62.62\nChair Table Lamp\nOurs\nDGL-Net\nGT\nFig. 5. Qualitative results. Our method has the ability to assemble various furniture.\nC. Main Results\nWe evaluate the algorithm on PartNet [9] and compare our\nresults with state-of-the-art methods in Table I. Our method\noutperforms all these approaches by a large margin, especially\nin the part and connectivity accuracy metrics. Speciﬁcally, we\nachieve 13% PA and 16% CA improvements on the chair\ncategory, 7% PA and 14% CA improvements on the table\nover SOTA model RGL-Net [8], individually. Instead of time-\nconsuming step-by-step learning part poses, we transform all\nparts at once. Instance Encoding signiﬁcantly reduces the\nconfusion between parts in parallel processing and increases\nthe model’s capacity to distinguish indiscernible parts. As\nshown in Fig. 4, we generate multiple assembled shapes for\nthe same set of parts (with different noises) and observe\nthat they sometimes behave differently in appearance, but all\nare reasonable. More visualizations are presented in Fig. 5.\nOur method is more likely to assemble the well-connected\nand structurally-stable furniture than DGL-Net [7]. However,\nthe method fails to deal with round-shape components or\njillion structurally-similar parts well. We detail several failing\nsamples in Fig. 6.\nD. Ablation Study\nIn this subsection, a series of ablation studies are conducted\nto verify the effectiveness of our method. We report all results\non the chair category.\nInstance encoding. As we discussed above, Instance En-\ncoding enables each input part uniquely while maintaining\nthe same constraints for geometrically-equivalent parts. The\nresults are shown in Table II. The baseline model without any\nencoding suffers from inter-part ambiguity and fails to achieve\na satisfactory result. This problem is greatly alleviated when\nintegrating either inter-class or intra-class encoding into the\nnetwork. And the performance is further improved when these\ntwo codes are employed together.\nTo further verify the effectiveness of Instance Encoding, we\ninsert it into a typical graph neural network. The model follows\nthe basic design in [7] and iteratively reasons the part poses\nand their relationships. As shown in Table III, The code vector\nboosts the performance to a large extent, which shows its\ngenerality and can be introduced into most relational-reasoning\nframeworks in assembly tasks. Furthermore, we demonstrate\nthat the transformer-based framework achieves better assembly\naccuracy than GNN. Unlike sparse graph prediction, Instance\nEncoding is more favourable for Transformer involving dense\nrelational reasoning.\nNumber of encoder layers. We evaluate the importance\nof self-attention relationship learning between input parts by\nvarying the number of encoder layers. As shown in Table IV,\nthe performance grows steadily with the increase of layers\nand reaches saturation at the sixth encoder layer. The speed is\nreported on a single NVIDIA GTX 1080 GPU. We choose 6\nlayers as the default conﬁguration.\nZHANG et al.: 3D PART ASSEMBLY GENERATION WITH INSTANCE ENCODED TRANSFORMER 7\nTABLE II\nTHE EFFECT OF INSTANCE ENCODING .\nencoding? SCD ↓ PA ↑ CA ↑\n- 0.0068 48.40 36.04\ninter-only 0.0055 59.93 42.66\nintra-only 0.0059 60.78 45.92\nins-enc 0.0054 62.80 48.45\nTABLE III\nDIFFERENT ARCHITECTURE WITH INSTANCE ENCODING .\narch w/ins-enc? SCD ↓ PA ↑ CA ↑\nGNN 0.0068 46.44 33.77\n\u0013 0.0063 58.14 43.19\nTransformer 0.0068 48.40 36.04\n\u0013 0.0054 62.80 48.45\nLoss function. We train several models to evaluate the impor-\ntance of different loss functions by turning them on and off.\nThe results are listed in Table V. The model performs poorly\nwhen Lt is removed, which indicates that part translation\naccounts for the most of assembly performance. As for the\nremaining losses, Lr helps learn rigid rotation for each part,\nand Ls ensures the holistic part assembly.\nInﬂuence of noise. Introducing noise into the model enables\nus to generate multiple shape variations. Here we monitor the\nimpact of noise by varying the dimension of random noise\nand measure its quality using part accuracy metric. Similar\nto [8], we use variability VE to quantify the variations, which\nis deﬁned as the gap between best and worst performance. And\nthe worst assembly performance is calculated using Maximum\nMatching Distance, opposite to Eq. (1). Mathematically, VE\nis written as:\nVE = max\nj∈[E]\nL(F(P, rj), F∗(P)) −min\nj∈[E]\nL(F(P, rj), F∗(P)) (9)\nwhere E is the iteration of predictions with the same input part\npoints and we set it to 10 here. The result is presented in Fig. 7.\nWe use the solid green line to indicate the best performance\nand the dashed blue line to depict the worst matching accuracy.\nIncreasing the noise dimension moderately allows our model\nto explore more structural varieties yet results in a decreasing\ninﬁmum. We ﬁx the noise dimension as 64 for all the furniture\ncategories as it gives the optimal performances.\nLimitation of instance encoding. We observe that the im-\nprovement of the lamp category over previous methods is\nlimited ( e.g., 13.74% vs. 1.12% on chair and lamp PA in\nTable I). Instance Encoding is designed to tackle the problem\nof inter-part confusion. However, as shown in Fig. 5, most part\ngeometries in a lamp are easily distinguished from each other,\nthus weakening the effect of Instance Encoding.\nE. In-process Part Assembly Task\nThe task requires 1) understanding the global posture of\nin-process furniture, 2) reasoning the relationship between\nfurniture and scattered parts, and 3) placing the candidate parts\nin their appropriate locations. And this function will signiﬁ-\ncantly generalize the applicability in reality, such as furniture\nTABLE IV\nABLATION STUDY OF ENCODER DEPTH .\nlayers SCD ↓ PA ↑ CA ↑ FPS\n2 0.0065 56.10 34.37 30.5\n3 0.0061 58.66 37.20 27.1\n4 0.0057 61.29 41.08 24.6\n6 0.0054 62.80 48.45 21.2\n8 0.0055 62.30 46.75 19.1\nChair Table\n Lamp\nFig. 6. Some failure cases. It is difﬁcult to ﬁnd the contact points of round-\nshape parts.\nTABLE V\nTHE INFLUENCE OF LOSS COMPONENTS .\nLt Lr Ls SCD ↓ PA ↑ CA ↑\n✓ ✓ 0.0070 29.04 29.44\n✓ ✓ 0.0056 59.51 43.07\n✓ ✓ 0.0058 61.07 45.39\n✓ ✓ ✓ 0.0054 62.80 48.45\nbest matchworst match\nFig. 7. Performance of our network on the varying dimension of random\nnoise. We still achieve the best performance even without MoN inference\n(56.1% at zero noise vs. 49.1% in RGL-Net [8]).\nmaintenance. Given a broken chair, instead of dismantling\na series of parts and then re-assemble the furniture as part\nassembly task, we can replace the damaged part with a new\none directly. Our framework has the potential for this task\nusing an encoder-decoder paradigm. The unﬁnished furniture\nis fed to encoder to generate feature memory and the remaining\nparts query the information from the memory through the\ndecoder. Compared to progressive assembly [8] involving the\ncomputational complexity of O(n2), the complexity in our\nmemory-query paradigm decreases to O(n), which dramati-\ncally improves assembly efﬁciency.\nWe employ PartDropscheme to enhance its ability to extract\n8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022\nTABLE VI\nTHE IMPORTANCE OF PART DROP IN IN -PROCESS ASSEMBLY .\ndrop-prob Incomplete Shape Regular Shape\nSCD ↓ PA ↑ CA ↑ SCD ↓ PA ↑ CA ↑\n0 0.0093 50.24 37.44 0.0059 61.33 40.38\n0.1 0.0075 54.23 42.44 0.0055 61.76 45.65\n0.2 0.0071 55.52 45.46 0.0054 62.80 48.45\n0.5 0.0068 55.70 44.78 0.0053 61.73 48.04\nfeatures from incomplete shapes. The input part is randomly\ndropped with a certain probability during training. To evaluate\nthe model’s capacity to reason about in-process inventory,\nwe sequentially remove one part from the input parts to\ngenerate the incomplete shape during the inference phase.\nWe calculate the average of all one-part-removed candidates\nin a complete shape as the evaluation result. Moreover, the\ncomplete 3D part clouds in the regular assembly task is termed\nas regular shape. The overall results are presented in Table VI.\nWe set the drop-prob to 0.2 as it works well on both\nincomplete and regular shapes. Interestingly, we observe it\nis also beneﬁcial to regular part assembly, particularly part\nconnection accuracy ( ∼8% improvement). It is essentially a\nkind of data augmentation and generates more complex data.\nThis operation helps our model to better understand ﬁne-\ngrained part connections. We evaluate the quality of in-process\nparts through Eq. (8) and achieve 62.36/52.95 in PA and CA,\nindividually.\nV. CONCLUSIONS\nIn this work, we present an instance-aware relational-\nreasoning framework for part assembly. We establish new\nstate-of-the-art results in the regular assembly task and further\nintroduce a practical problem called in-process assembly.\nGiven the simplicity, efﬁciency, and ﬁrm performance, we\nhope that our method can serve as a cornerstone for many\nautomated assembly tasks. Considering data efﬁciency and\ninteractive safety, present works mainly conduct assembly in\nsimulation and are still inapplicable in reality. In future work,\nwe are intended to bridge the gap between simulation and\nreality, and transfer the gained assembly skill to accomplish\nmore complicated tasks in the real world.\nREFERENCES\n[1] J. Li, C. Niu, and K. Xu, “Learning part generation and assembly for\nstructure-aware shape synthesis,” inProceedings of the AAAI Conference\non Artiﬁcial Intelligence , vol. 34, no. 07, 2020, pp. 11 362–11 369.\n[2] N. Schor, O. Katzir, H. Zhang, and D. Cohen-Or, “Componet: Learning\nto generate the unseen by part synthesis and composition,” in Proceed-\nings of the IEEE/CVF International Conference on Computer Vision ,\n2019, pp. 8759–8768.\n[3] H. Wang, N. Schor, R. Hu, H. Huang, D. Cohen-Or, and H. Huang,\n“Global-to-local generative model for 3d shapes,” ACM Transactions on\nGraphics (TOG), vol. 37, no. 6, pp. 1–10, 2018.\n[4] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang,\n“Sagnet: Structure-aware generative network for 3d-shape modeling,”\nACM Transactions on Graphics (TOG) , vol. 38, no. 4, pp. 1–14, 2019.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[6] Y . Li, K. Mo, L. Shao, M. Sung, and L. Guibas, “Learning 3d part\nassembly from a single image,” in European Conference on Computer\nVision. Springer, 2020, pp. 664–682.\n[7] J. Huang, G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. Guibas, and\nH. Dong, “Generative 3d part assembly via dynamic graph learning,”The\nIEEE Conference on Neural Information Processing Systems (NeurIPS) ,\n2020.\n[8] A. Narayan, R. Nagar, and S. Raman, “Rgl-net: A recurrent graph\nlearning framework for progressive part assembly,” in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision ,\n2022, pp. 78–87.\n[9] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su,\n“Partnet: A large-scale benchmark for ﬁne-grained and hierarchical\npart-level 3d object understanding,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2019, pp. 909–\n918.\n[10] Y . Litvak, A. Biess, and A. Bar-Hillel, “Learning pose estimation for\nhigh-precision robotic assembly using simulated depth images,” in 2019\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2019, pp. 3521–3527.\n[11] L. Shao, T. Migimatsu, and J. Bohg, “Learning to scaffold the de-\nvelopment of robotic manipulation skills,” in 2020 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2020, pp.\n5671–5677.\n[12] P. A. Zachares, M. A. Lee, W. Lian, and J. Bohg, “Interpreting contact\ninteractions to overcome failure in robot assembly tasks,” in 2021 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE,\n2021, pp. 3410–3417.\n[13] T. Funkhouser, M. Kazhdan, P. Shilane, P. Min, W. Kiefer, A. Tal,\nS. Rusinkiewicz, and D. Dobkin, “Modeling by example,” ACM trans-\nactions on graphics (TOG) , vol. 23, no. 3, pp. 652–663, 2004.\n[14] S. Chaudhuri, E. Kalogerakis, L. Guibas, and V . Koltun, “Probabilistic\nreasoning for assembly-based 3d modeling,” in ACM SIGGRAPH 2011\npapers, 2011, pp. 1–10.\n[15] P. Jaiswal, J. Huang, and R. Rai, “Assembly-based conceptual 3d\nmodeling with unlabeled components using probabilistic factor graph,”\nComputer-Aided Design, vol. 74, pp. 45–54, 2016.\n[16] A. Dubrovina, F. Xia, P. Achlioptas, M. Shalah, R. Groscot, and L. J.\nGuibas, “Composite shape modeling via latent space factorization,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 8140–8149.\n[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,”\nAdvances in neural information processing systems , vol. 27, 2014.\n[18] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[19] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and L. Guibas,\n“Grass: Generative recursive autoencoders for shape structures,” ACM\nTransactions on Graphics (TOG) , vol. 36, no. 4, pp. 1–14, 2017.\n[20] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. J. Mitra, and L. J. Guibas,\n“Structedit: Learning structural shape variations,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 8859–8868.\n[21] L. Gao, J. Yang, T. Wu, Y .-J. Yuan, H. Fu, Y .-K. Lai, and H. Zhang,\n“Sdm-net: Deep generative network for structured deformable mesh,”\nACM Transactions on Graphics (TOG) , vol. 38, no. 6, pp. 1–15, 2019.\n[22] K. Mo, H. Wang, X. Yan, and L. Guibas, “Pt2pc: Learning to generate 3d\npoint cloud shapes from part tree conditions,” in European Conference\non Computer Vision . Springer, 2020, pp. 683–701.\n[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on\npoint sets for 3d classiﬁcation and segmentation,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n652–660.\n[24] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le, “Attention\naugmented convolutional networks,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision , 2019, pp. 3286–3295.\n[25] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for\n3d object reconstruction from a single image,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2017, pp.\n605–613.\n[26] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning rep-\nresentations and generative models for 3d point clouds,” in International\nconference on machine learning . PMLR, 2018, pp. 40–49.\n[27] R. Wu, Y . Zhuang, K. Xu, H. Zhang, and B. Chen, “Pq-net: A generative\npart seq2seq network for 3d shapes,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp. 829–\n838.\n[28] M. Sung, H. Su, V . G. Kim, S. Chaudhuri, and L. Guibas, “Comple-\nmentme: Weakly-supervised component suggestions for 3d modeling,”\nACM Transactions on Graphics (TOG) , vol. 36, no. 6, pp. 1–12, 2017.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.513708233833313
    },
    {
      "name": "Computer science",
      "score": 0.4193674325942993
    },
    {
      "name": "Engineering",
      "score": 0.22956019639968872
    },
    {
      "name": "Electrical engineering",
      "score": 0.17233964800834656
    },
    {
      "name": "Voltage",
      "score": 0.061261773109436035
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    }
  ]
}