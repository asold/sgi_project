{
  "title": "GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text",
  "url": "https://openalex.org/W4385848978",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1924862783",
      "name": "Liu Pengfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352885024",
      "name": "Ren Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1994903354",
      "name": "Tao Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350565932",
      "name": "Ren Zhi-xiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4235558226",
    "https://openalex.org/W1966456689",
    "https://openalex.org/W4283761122",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4295846611",
    "https://openalex.org/W4255020100",
    "https://openalex.org/W4382652046",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4220798957",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4389888290",
    "https://openalex.org/W3208314443",
    "https://openalex.org/W4292945941",
    "https://openalex.org/W4306640168",
    "https://openalex.org/W4387195417",
    "https://openalex.org/W4380552032",
    "https://openalex.org/W4281553035",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W3211951295",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4319994933",
    "https://openalex.org/W2345198528",
    "https://openalex.org/W1935434993",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4307868887",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4324319985",
    "https://openalex.org/W3216130706",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2899070097",
    "https://openalex.org/W3206711231",
    "https://openalex.org/W4290876361",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4212837331",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224035735"
  ],
  "abstract": "Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.",
  "full_text": "GIT-Mol: A Multi-modal Large Language Model for Molecular\nScience with Graph, Image, and Text\nPengfei Liua,b, Yiming Ren a, Jun Tao b and Zhixiang Ren a\naPeng Cheng Laboratory, Shenzhen, 518055, Guangdong Province, China\nbSchool of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, 510006, Guangdong Province, China\nA R T I C L E I N F O\nKeywords:\nMolecular Representation\nMolecule Generation\nLarge Language Model\nMulti-modality\nA B S T R A C T\nLarge language models have made significant strides in natural language processing, enabling\ninnovative applications in molecular science by processing textual representations of molecules.\nHowever, most existing language models cannot capture the rich information with complex molecular\nstructures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model\nthat integrates the Graph, Image, and Text information. To facilitate the integration of multi-\nmodal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning\nall modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties\nprediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-\nto-language molecular translation strategy, our model has the potential to perform more downstream\ntasks, such as compound name recognition and chemical reaction prediction.\n1. Introduction\nMolecular science covers a broad spectrum of fields\nthat study the structures, properties, and interactions of\nmolecules. It is a interdisciplinary field that draws on chem-\nistry, physics, biology, and computer science. Molecular\nscience is pivotal in drug discovery applications, such as\ntarget identification and validation, structure-based drug\ndesign, and side effect prediction. However, most existing\nmethods of discovering new molecules or tweaking existing\nones can be time-consuming, expensive, and prone to fail-\nure (Rodrigues et al., 2016). More recently, computational\nmethods have shown significant advantages in molecule\ngeneration and tweaking (Bilodeau et al., 2022). These\ntechniques enable rapid identification and optimization of\npotential drug candidates. However, these computational\nmethods are limited by substantial computational demands.\nFortunately, artificial intelligence (AI) and deep learning\nhave emerged as powerful tools for molecular science.\nThese technologies can potentially revolutionize the field\nby improving speed, accuracy, and scalability in molecu-\nlar discovery and understanding. Large Language Models\n(LLMs) have made significant progress in Natural Language\nProcessing (NLP) and molecular science. MolT5 (Edwards\net al., 2022), based on the T5 (Raffel et al., 2020), which\nincludes capabilities of molecule captioning (Mol2Cap)\nand text-based molecule generation (Cap2Mol). LLMs like\nMolT5 help describe molecules in words and generate\nstructures from the text. However, these text-to-text models\ncan not fully use the advantages of molecular structure data\nand understand molecular images. To fuse and understand\nmulti-modal data, the multi-modal large language models\n(MLLMs) like CLIP (Radford et al., 2021), ALIGN (Jia\net al., 2021), and BEIT-3 (Wang et al., 2022c) have laid\n‚àóCorresponding author at: Peng Cheng Laboratory, Shenzhen, 518055,\nChina\n‚àó ‚àóE-mail addresses: renzhx@pcl.ac.cn\nthe groundwork for adaptive learning across image-text\nmodalities. In molecular science, SwinOCSR (Xu et al.,\n2022) is designed for image recognition, which significantly\naids in document comprehension and multi-modal drug\ndiscovery database construction (Wang et al., 2022b). In\naddition, MoleculeSTM (Liu et al., 2022) and MoMu (Su\net al., 2022) can combine the Simplified Molecular Input\nLine Entry System (SMILES) (Weininger, 1988) and graph\nrepresentations for molecular property prediction tasks.\nExisting language models excel in processing textual\nmolecular data, but processing molecular graphs and images\nis difficult, typically relying on processed vector encodings.\nThis limitation highlights the need for a modality data\nalignment process, where multi-modal models outperform\nlanguage models by integrating diverse data types for en-\nhanced feature representation. Moreover, the scalability of\nmolecular representation and generation models remains a\nsignificant challenge. Models capable of effectively fusing\nthree or more modalities are scarce, and the complexity of\nintegrating these diverse modalities, often with missing or\nincomplete data, calls for advanced modeling techniques.\nTo address the significant scalability challenges in\nmolecular science and to harness the potential of vast quan-\ntities of unlabeled multi-modal data, we have developed\nGIT-Mol, a robust neural network architecture. At the core\nof GIT-Mol is the GIT-Former, which incorporates an ad-\nvanced cross-attention mechanism. It is capable of mapping\ndata from various modalities into a unified multi-modal\nrepresentation. This approach not only enhances the model‚Äôs\nability to process and integrate diverse modalities efficiently\nbut also effectively bridges the scalability gap, providing a\npowerful tool for various molecular applications. Our main\ncontributions are as follows:\n‚Ä¢ To utilize the available large amount of unlabeled\nmulti-modal data, we develop a specialized multi-\nmodal large language model, GIT-Mol (700M), to\nPengfei Liu et al.: Preprint submitted to CIBM Page 1 of 14\narXiv:2308.06911v3  [cs.LG]  6 Feb 2024\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nFigure 1: An overview of GIT-Mol . (a) Internal Information , including sequence and graph structure representations,\nemphasizes inherent chemical properties and simple topology; (b) External Information, e.g., images and text descriptions,\nprovide richer details and help the human understanding; (c) GIT-Former Multi-modal Encoder , architecture and Pre-\ntrain Strategy of GIT -Former, GIT -Former aligns graph, image, and text with the target text modality (SMILES strings or\ncaptions) using self-attention and cross-attention. The learnable queries interact with each other and the various modalities\nthrough these attention layers. Xmodal-Text Matching (XTM) and Xmodal-Text Contrastive Learning (XTC) represent our\nself-supervised learning strategies tailored for specific modalities (X) and target text modalities; (d) Multi-modal Molecular\nTasks, in cross-modal tasks, GIT -Former generates different Embeddings based on various inputs, which MolT5 then decodes\ninto the target text modality and the MLP model for property prediction tasks.\ncover all three modalities in molecular science (graph,\nimage, and text) for molecule generation, molecule\ncaptioning, molecular image recognition, and molec-\nular property prediction.\n‚Ä¢ We present GIT-Former, a novel modality mixer\ndesigned with a cross-attention mechanism, enabling\nseamless fusion of three molecular modalities. Each\nfusion strategy within GIT-Former operates at the\nmolecule level, ensuring optimal flexibility and scal-\nability. The effectiveness of our multi-modal model\nis demonstrated in our extensive ablation studies,\nwhich provides an improvement of 10%-15% over the\nsingle-modality models.\n‚Ä¢ The quantitative evaluations indicate that GIT-Mol\nexhibits commendable performance and surpasses\nstate-of-the-art on certain metrics, as it outperforms\nPengfei Liu et al.: Preprint submitted to CIBM Page 2 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nthe baseline 20% in the cross-modal molecule gen-\neration tasks regarding the validity of the generated\nmolecules and by 5%-10% in molecular property\nprediction tasks.\nIn the subsequent sections, Section 2 primarily intro-\nduces multi-modal models and applications in molecular\nscience. Section 3 introduces the principles of our GIT-Mol\nmodel. Section 4 focuses on the experimental setup and\nresults for downstream tasks of the model. Sections 5, 6,\nand 7 respectively discuss the features of our model, the\nlimitations, and the conclusions.\n2. Related Works\n2.1. Multi-modal Large Language Models\nIn recent years, Large Language Models (LLMs) like\nthe GPT (Floridi and Chiriatti, 2020) family have received\nmore attention due to their performance and potential appli-\ncations, especially the ChatGPT(Radford et al., 2019) and\nGPT-4(OpenAI, 2023). Some variants of those models have\nbeen used in many scientific domains, such as BioGPT (Luo\net al., 2022), DrugGPT (Li et al., 2023c), and MolReGPT\n(Li et al., 2023b), which have been adapted for molecular\nscience tasks. Additionally, models like LLaMA and T5\nhave inspired a variety of variants and as the language model\nin multi-modal models.\nMulti-modal models have been a primary focus on\nimage captioning task (Stefanini et al., 2022). Modality-\nadaptive Learning models such as BEIT-3 (Wang et al.,\n2022c) and KOSMOS-1 (Huang et al., 2023) adopt the\nMixture of Experts (MoE) strategy (Bao et al., 2022) and the\nMAGNETO transformer (Wang et al., 2022a), enhancing\nlearning across different modalities using specialized en-\ncoders and shared self-attention modules. In contrast, Multi-\nmodal agents like Flamingo (Alayrac et al., 2022) and Gato\n(Reed et al., 2022) are crafted for real-world applications,\nleveraging multi-task and reinforcement learning to inter-\npret and act within intricate environments. Additionally,\nmodels such as Visual ChatGPT (Wu et al., 2023) employ\nthe Chain-of-Thoughts (CoT) strategy (Wei et al., 2022)\napproach, merging visual encoders with ChatGPT archi-\ntectures for enriched, visually-informed dialogues. Lastly,\nthe Cross-modal Learning paradigm, as seen in models\nlike MiniGPT-4 (Zhu et al., 2023), emphasizes modality\nfusion and alignment using techniques like the Q-Former in\nBLIP2 (Li et al., 2023a) or adapter, promoting the modality\nintegration.\nIn addition, text-image generation models like DALLE-\n2 (Ramesh et al., 2022) and UniDiffuser (Bao et al., 2023)\nemploy diffusion models (Yang et al., 2022) to generate\nimages from text. These models have potential applications\nacross art, design, and scientific visualization. Moreover,\ndiffusion models are also used in molecular research for\nmolecule structure graph generation tasks.\nIn multi-modal tasks, image-captioning and text-image\ngeneration stand out as significant directions. These models\nintegrate information across different modalities, highlight-\ning the potential of multi-modal learning in comprehen-\nsively understanding the connections in modalities.\n2.2. Multi-modal Model in Molecular Science\nIn molecular science, multi-modal models focus on\nmolecule-caption translation tasks and utilize multi-modal\nrepresentations for downstream tasks like molecular prop-\nerty and chemical reaction prediction.\nMolecule-Caption Translation, in this task, our model\ncan learn a shared semantic space from a dataset of molecules\npaired with their text descriptions. Like Text2Mol (Ed-\nwards et al., 2021), uses natural language descriptions to\nretrieve molecules. Moreover, models including KV-PLM\n(Zeng et al., 2022) and MolT5 (Edwards et al., 2022)\nsignificantly contribute to this area. KV-PLM builds a\nmachine reading system, pre-trained on the domain-specific\ncorpus, linking molecules and biomedical text. MolT5 is\na self-supervised framework enhancing molecule-caption\ntranslation tasks. In addition, MoleculeSTM and MoMu\nbridge molecular graphs and text data through contrastive\nlearning. MolReGPT applies a retrieval-based paradigm\nfor molecule-caption translation, leveraging LLMs like\nChatGPT without fine-tuning. These models, transforming\nbetween SMILES expressions and text descriptions, have\nthe potential for further advancement, with possibilities\nincluding better modality alignment, fusion strategies, and\nfine-tuning methods.\nMolecule Image Captioning , rule-based models such\nas MolVec2 (Peryea et al., 2019) and OSRA (Filippov and\nNicklaus, 2009), along with machine learning-based ones\nlike DECIMER (Rajan et al., 2021) and SwinOCSR. These\nmodels use image encoders like Vision Transformer (Doso-\nvitskiy et al., 2020) or ResNet (He et al., 2016) and process\nimage features using recurrent neural networks (RNNs) or\ntransformers (Vaswani et al., 2017) to decode into SMILES\nstrings. Unlike the general domain, their results mainly\nfocus on transforming images into SMILES strings.\nMolecular Property Prediction , GNN-based models\nsuch as GraphCL (Wang et al., 2022d) and GraphMAE (Hou\net al., 2022) are used, leveraging contrastive learning and\nself-supervised graph autoencoders respectively. Models\nlike Uni-mol (Zhou et al., 2023) and GraphMVP (Liu et al.,\n2021a) process 3D graph data effectively, while MoMu and\nMoleculeSTM combine SMILES and graph representations\nfor downstream tasks.\nThe current challenge lies in enhancing modal fusion\nand merging data from various sources like text, graphs,\nand images. While these tasks demonstrate the vast potential\nof multi-modal learning, there remains significant scope for\nimprovement, particularly in capturing the intricate rela-\ntionships between different data sources. To address these\nchallenges, our GIT-Mol model takes inspiration from the\nBLIP2‚Äôs Q-Former approach, centering around text modal-\nities while expanding the range of modal adaptability. For\nhandling image modality data, in addition to employing the\nSwin Transformer, we have integrated contrastive learning\nPengfei Liu et al.: Preprint submitted to CIBM Page 3 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nstrategies into our training regime. Moreover, our dataset\nfeatures more complex molecular structures compared to\nexisting models. In contrast to MoMu‚Äôs graph encoder,\nGIT-Mol not only employs contrastive learning but also\ncombines it with cross-attention and modal fusion methods.\nGIT-Mol is adept at mapping multi-modal tensors of\nvarying lengths into a fixed-length unified latent space\nusing the GIT-Former. Our model aligns at the entity level\nof molecules, providing a significant degree of flexibility.\nThis adaptability ensures that our model remains versatile\nand applicable even in different scenarios, such as when\nreplacing images or graphs with other modalities. These\nenhancements enable our model to more effectively capture\nand integrate the nuanced interplay between various modal-\nities, thereby addressing the critical challenges in the field\nand pushing the boundaries of molecular science research.\n3. Methodology\n3.1. Overview\nIn this work, we present GIT-Mol, a multi-modal large\nlanguage model for molecular science. As shown in Figure\n1, molecular information is categorized into internal and ex-\nternal forms in our research. Internal information, including\nmolecular SMILES and graph data, focuses on the intrinsic\nstructure and rules of molecules. External information com-\nprises molecular images and molecule captions, offering a\nmore interpretable perspective. Furthermore, GIT-Mol can\nfuse data from diverse sources and present a comprehensive\nview of molecules. Specifically, we propose GIT-Former,\na novel module capable of aligning all modalities into a\nunified latent space. It is designed to incorporate various\nmolecular data types, including graphs, images, and text. In\nthe pre-training phase, the model employs cross-attention\nand contrastive learning to align different modalities, en-\nriching our understanding of the molecular data. Each fusion\nstrategy within GIT-Former operates at the molecule level,\nensuring optimal flexibility and scalability. During the fine-\ntuning stage, the process is guided by prompt learning that\nadapts to various tasks. Furthermore, these multi-modal\nrepresentations can be directly processed through MLP to\nexecute molecular properties prediction.\n3.2. Data and Preprocessing Strategy\nData Modalities : The diversity of data modalities al-\nlows the model to learn and understand complex relation-\nships across different modalities, which improves the per-\nformance in molecular property prediction and generation\ntasks. Molecular information is categorized as internal and\nexternal information, as shown in Figure 1. Internal data\nlike SMILES strings and structural graphs are essential for\npredicting molecular properties and features. The SMILES\nstrings provide a textual representation of molecular struc-\ntures, concisely encoding vital connectivity and stereochem-\nistry details. Furthermore, molecular structured graphs offer\na topological view of molecules in two-dimensional space,\nwhere atoms are nodes and bonds are edges. In contrast, ex-\nternal data, including text descriptions and molecule images,\nare user-friendly and easy to interpret. Molecular captions\npresent textual descriptions that shed light on molecules‚Äô\ndistinct characteristics and properties, offering a natural\nlanguage context for the model. Furthermore, molecular\nimages visually showcase atomic structures and bonding\nschemes, providing intuitive input for our model‚Äôs molec-\nular analysis.\nDataset: We collected approximately 4.8 million chem-\nical compounds from the PubChem (Kim et al., 2019)\ndatabase, providing a robust training dataset for our model.\nThis dataset contains molecular images for image caption-\ning tasks and serves as a rich resource for self-supervised\nlearning with SMILES and molecular graph representations.\nIn addition, we use the standard ChEBI-20 dataset (Edwards\net al., 2021), consisting of 33,010 molecule-description\npairs, for fine-tuning and evaluation. While the molecule\ncaptions in some databases are less complex than those\nin ChEBI-20, it is crucial to have concise and accurate\ndescriptions of molecular characteristics. We constructed\na dataset from ChEBI (Hastings et al., 2016) and PubChem.\nPubChem has over 320,000 molecule-description pairs, but\nmany of them are too brief or contain non-informative\ncontent. To ensure high-quality captions, we analyzed the\nlength distribution of captions in the CHEBI-20 dataset. We\nestablished a criterion for selecting captions that are longer\nthan 96 characters and excluded those without any relevant\ninformation on their properties or functional groups.\nData Preprocessing: To ensure optimal model training\nand evaluation, we undertake several critical steps to refine\nraw data. Initially, we focus on data cleaning by rectifying\ninconsistencies, filling in missing values, or resolving data\nerrors. It guarantees a high-quality data input for the model.\nFurther, we remove compounds that feature low-frequency\natoms by analyzing atom counts. It ensures the model\nfocuses on patterns from prevalent and pertinent atomic\nstructures. Lastly, using the RDKit toolkit (Bento et al.,\n2020), we validate and enhance the dataset‚Äôs graph struc-\ntures. We refine our collection to include only high-quality\nand representative compounds. By rigorously preprocessing\nthe data, we assure data quality and bolster the model‚Äôs\nlearning efficiency and performance on targeted tasks.\n3.3. GIT-Former\nGIT-Former is an architecture that can map all modal-\nities into a unified latent space, designed based on the Q-\nFormer architecture in BLIP2 (Li et al., 2023a). It leverages\nthe strengths of various encoder and decoder models to suit\nthe specific characteristics of different data modalities, such\nas SMILES strings, captions, images, and structure graphs.\nThis section first introduces the model architecture. Then, it\ndelineates the pre-training and fine-tuning in two stages: (1)\nthe Xmodal-to-language representation learning stage with\nfrozen encoders and (2) the Xmodal-to-language generative\nlearning stage with the decoder. The GIT-Former model\narchitecture and training strategy are shown in Figure 1. In\nthe pre-training phase of GIT-Former, attention mechanisms\nand contrastive learning are used to align Xmodal data\nPengfei Liu et al.: Preprint submitted to CIBM Page 4 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nwith the target text modal. During the fine-tuning phase,\nXmodal data is mapped onto learnable fixed-length queries,\neffectively aligning diverse modalities into a latent space.\nGIT-Former model serves as a modality mixer to fuse\nmolecular data. Unlike existing Visual Language (VL) mod-\nels like BLIP2‚Äôs Q-Former, our approach can address the\ngraph and text modality translation tasks, providing the\nflexibility needed in bio-molecular multi-modal studies. To\nbetter fit our scientific scenario, we replace the BERT (De-\nvlin et al., 2018) model with SciBERT (Beltagy et al., 2019),\ntailored explicitly to scientific text, and enhance it with\ncross-attention mechanisms for graph and text modalities.\nAs shown in Figure 1, our model can adaptively adjust its\ntraining modules to achieve modality alignment effectively.\nMoreover, alternatives such as MolT5 can also be utilized as\nembedding and self-attention layers, providing flexibility to\nadapt to different molecular scenarios.\nEncoder and Decoder: MolT5 is an advanced language\nmodel to translate molecule and molecular textual descrip-\ntions. We adopt it to serve as our text encoder and decoder.\nFor image encoding, we adopt the Swin Transformer (Liu\net al., 2021b) from SwinOCSR, and for graph encoding, we\nselect the GIN model from the pre-trained MoMu model.\nThis multi-encoder and decoder setup equips our model\nwith the flexibility to adapt to the demands of each data\nmodality, enhancing the model‚Äôs performance and efficacy.\nCross-Attention Mechanism is central to GIT-Former.\nThis module facilitates the alignment of image and graph\nmodalities to text modalities, which encompass molecular\ncaptions and SMILES strings. This mechanism can be math-\nematically formulated as a multi-head attention operation,\nwhich captures inter-modal data relationships:\nMultiHead(ùëÑùëá ùë°, ùêæ, ùëâ) =Concat(head1, ‚Ä¶ , head‚Ñé)ùëäùëÇ (1)\nheadùëñ = Attention(ùëÑùëá ùë°ùëäùëÑùëñ, ùêæùëäùêæùëñ, ùëâ ùëäùëâ ùëñ) (2)\nIn the context of GIT-Former,ùëÑ represents queries, ùëÑùëá ùë°\nrepresents the queries from the target text modality (caption\nor SMILES string), ùêæ stands for keys, ùëâ denotes values,\nand ùëä refers to the learned weight matrices to project\nthe input into appropriate spaces. For source modalities in\nequation (2), ùëñ represent the graph, image, and source text\nmodality. For each source modality, GIT-Former computes\na separate set of cross-attention weights, aligning the ùëá ùë°\nmodality to each of the source modalities. This ensures that\nevery detail of the image or graph is precisely reflected\nin its corresponding textual representation. The attention\nmechanism can be visualized as:\nAttention(ùëÑùëá ùë°, ùêæ, ùëâ) =softmax\n(\nùëÑùëá ùë°ùêæùëá\n‚àö\nùëëùëò\n)\nùëâ (3)\nwhere ùëëùëò is the dimensionality of the queries and keys.\nBy leveraging this cross-attention framework, GIT-\nFormer efficiently discerns and encodes intricate relation-\nships and dependencies between modalities. This is shown\nin Figure 1, emphasizing the alignment between image,\ngraph and source text modalities to ùëá ùë°modality.\n3.4. Pre-training Strategy\nOur research involves various training strategies. In the\npre-training phase, we use frozen image and graph encoders\nand SciBERT for text encoding. Moreover, the GIT-Former\naligns each modality with the target text by self-supervised\nlearning, enhancing molecular translation tasks. We use a\nunique cross-attention method in pre-training to achieve\ninter-modal and contrastive learning.\nXmodal-Text Matching (XTM) aims to align differ-\nent modal representations with corresponding text. It is\na binary classification task that determines if a set of\ncross-modal texts matches. A bi-directional self-attention\nmask facilitates interaction between learnable queries and\nmodality embeddings. The learnable query is an initial-\nized, fixed-length tensor of zeros. The embeddings from\ndifferent modalities are mapped onto this fixed-length tensor\nthrough cross-attention mechanisms. This process results in\nthe generation of query embeddings enriched with multi-\nmodal information. These query embeddings are processed\nthrough a linear classifier to get a logit, and the average\nacross queries gives the final matching score. For the dif-\nferent modalities (X-source-modal and text-target-modal),\nwe convert the inputs into embeddings (ùê∏ùë• and ùê∏ùë°) and then\npair them with matched and mismatched samples, creating\na contrast for the model to recognize the correct combi-\nnations. Self-attention and cross-attention mechanisms are\nemployed to understand relations within and across modal-\nities, resulting in a unified embedding that contains multi-\nmodal information. This process can be represented with\nùëìùëéùë°ùë° as the function for both attention mechanisms.\nùê∏fused = ùëìatt(concat([ùê∏ùë•, ùê∏ùë°])) (4)\nLastly, the fused embeddings ùê∏fused are processed\nthrough a linear layer with a weight of ùëä , yielding a\npredictive score (logit). The cross-entropy loss between the\nlogit and actual labels (1 for match, 0 for mismatch) is\ncomputed. This loss is a metric for the model‚Äôs performance\non the binary classification task, and its optimization helps\nthe model effectively match information across different\nmodalities.\nlossxtm = cross_entropy(ùëä ‚ãÖ ùê∏fused, ùë¶) (5)\nXmodal-Text Contrastive Learning (XTC)aligns dif-\nferent information types with corresponding text represen-\ntations. This technique contrasts the similarity of matched\ncross-modal text against mismatches. The representation\nfrom the task modality is aligned with the text, and the\none with the highest similarity is selected as the cross-\nmodal data pair. In the XTC approach, we first employ\nthe GIT-Former to attain the learned representation of the\nXmodal input. We then extract the Xmodal features ùê∏ùë•\nfrom this representation. Subsequently, we compute the\nmutual information ùêº(ùê∏ùë•, ùê∏ùë°), individually for Xmodal and\nPengfei Liu et al.: Preprint submitted to CIBM Page 5 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\ntext representations ùê∏ùë°. Finally, we use cross-entropy to\ncalculate the loss function ùêøùë•ùë°ùëê. The operation ‚ãÖ denotes the\ndot product between the two embeddings. With this in place,\nthe mutual information is determined as:\nùêº(ùê∏ùë•, ùê∏ùë°) =ùê∏ùë• ‚ãÖ ùê∏ùë° (6)\nFinally, the cross-entropy loss ùêøùë•ùë°ùëê is computed as:\nùêøùë•ùë°ùëê = cross_entropy(ùêº(ùê∏ùë•, ùê∏ùë°), ùë¶) (7)\n3.5. Fine-tuning Strategy\nDuring the fine-tuning phase, we classify our tasks into\ntwo main categories: Modality Translation and Molecu-\nlar Property Prediction. Within the Modality Translation\ntask, as illustrated in Figure 1, our focus areas include\nmolecule captioning, molecule generation, and molecular\nimage recognition. To enhance the flexibility of this task,\nwe incorporate our prompt manager, which enables an\nany-to-language training mechanism, facilitating a more\nversatile modality translation process. In the Molecular\nProperty Prediction task, we fine-tune our model using\nlabeled SMILES and graph data from MoleculeNet (Wu\net al., 2018), specifically classification tasks.\nPrompt Tuning for Modality Translation : As shown\nin Figure 1, our model encodes data from each modality into\nembeddings, and then the GIT-Former maps embeddings\ninto a unified latent space. This allows seamless interac-\ntion and translation among different modalities within a\ncommon representation. Moreover, the framework supports\ntranslating information across any modality to the language\nmodality, providing a flexible setup for managing tasks such\nas prompt management, data loading, and model training.\nThis is achieved by designing various task types and corre-\nsponding prompts according to the specific modality.\nThe any-to-language molecular translation strategy fun-\ndamentally revolves around a text-centric pretraining and\nfinetuning approach. It can be implemented through a com-\nbination of prompts, GIT-Former, and Large Language\nModels (LLMs) to accomplish any-to-language tasks. The\nstrength of this strategy lies in its flexibility in modal\ntransformation and adaptability to various tasks. We test\ndirective and guiding prompts for each task. Finally, we em-\nploy the following prompt for the SMILES string generation\nand recognition task: \"Given the provided ùëñùëõùëùùë¢ùë°ùë†, gener-\nate the corresponding SMILES string.\" Once this prompt\nis fused with the GIT-Former embeddings, the combined\nrepresentation is channeled to the MolT5 decoder. Here, the\ntraining objective is to align the input data (a caption or an\nimage) with its pertinent SMILES data. This design ensures\nthat the model remains sensitive to the nuanced differences\nbetween modalities while still producing reliable SMILES\nrepresentations, bridging the gap between visual or struc-\ntural information and text.\nIn the molecule captioning task, our prompt choice is:\n\"This is the ùëñùëõùëùùë¢ùë°ùë† of the molecule, and the corresponding\ncaption is: \" Similar to the previously outlined procedure,\nupon amalgamation of this prompt with the GIT-Former\nembeddings, the aggregate is routed to the MolT5 decoder.\nThe overarching training goal in this scenario revolves\naround ensuring that the input data (SMILES string or\ngraph) aligns impeccably with its associated caption data.\nThis task reinforces the model‚Äôs ability to understand molec-\nular structures from various perspectives. It enriches its\ncompetency in generating descriptive text, demonstrating\nthe model‚Äôs broad applicability in multi-modal molecular\ninformatics.\nFine-tuning for Property Prediction Tasks : We use\nMolT5 as embedding and self-attention layers to process\nSMILES strings and employ contrastive learning to pre-\ntrain SMILES string and graph embeddings. Our approach\nemphasizes utilizing multi-modal data integrating modality\nrepresentations through attention mechanisms. After ob-\ntaining this enriched embedding, it is passed through a\nMulti-Layer Perceptron (MLP), which outputs the predicted\nmolecular properties.\nLanguage models have inherent strengths in understand-\ning context and relationships, our model is particularly apt\nfor benchmark tasks related to bio-activity and toxicity\nclassifications. The model‚Äôs ability to process and relate\ncomplex molecular data makes it well-suited for these\nspecific prediction challenges.\n3.6. The Essentiality of Architecture and Strategy\nIn molecular science, molecular data have multiple rep-\nresentation formats, such as molecular images, structural\ngraphs, SMILES strings, and physical properties. Most\nexisting models bridge two modalities into a unified space,\nlike images with SMILES strings or graph structures with\ncaptions. In summary, accommodating an expanded set of\nmodalities remains challenging for current models.\nArchitecture: GIT-Former provides a dynamic and scal-\nable solution tailored for multi-modal molecular data. The\nprowess of GIT-Former emanates from its cross-attention\nmechanism, allowing it to integrate and adapt to new modal-\nities within molecular representations effortlessly. The key\nfeature of the GIT-Former is its ability to map multi-modal\ntensors of varying lengths into a fixed-length unified latent\nspace. This mapping allows the model to align data at the\nmolecular entity level, offering significant flexibility. Such\nadaptability ensures that GIT-Mol remains versatile and\napplicable in various scenarios, including those involving\nmodalities beyond images and graphs.\nTraining strategies: The choice of XTC and XTM as\ntraining strategies, both strategies operate at the individual\nvector level, providing flexibility in model training. XTC\nis crucial for distinguishing between positive and nega-\ntive molecular samples, especially since minor molecular\nchanges can lead to significant shifts in bioactivity or drug\ninteractions. Meanwhile, XTM ensures that each molecular\nrepresentation aligns precisely with its corresponding text\nor description.\nMulti-modal Representation: GIT-Former can extract\ninformation from each modality and ensure that this infor-\nmation effectively integrates within a unified representation\nPengfei Liu et al.: Preprint submitted to CIBM Page 6 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nspace. It ensures a deeper understanding of the nuances\nwithin each modality. Moreover, when paired with prompt\nlearning and the pre-trained state-of-the-art MolT5 decoder,\nGIT-Mol effectively leverages this information in specific\ntasks, producing the corresponding molecular data.\n4. Results\nIn our experiments, we delve into crucial tasks related\nto molecular multi-modal translation, including molecule\ncaptioning (Sec 4.1), text-based de novo molecule gen-\neration (Sec 4.2), and molecular image recognition (Sec\n4.3). Additionally, we utilize the processed embeddings for\nmolecular property prediction tasks for the molecular multi-\nmodal representation (Sec 4.4). Additionally, we have insti-\ntuted ablation studies to ascertain the contributions of multi-\nmodal data, cross-attention, and prompt learning to the\noverall performance. To evaluate our model‚Äôs efficacy, we\ncompare embeddings pre-model and post-model processing,\naiming to understand the model‚Äôs impact on molecular data\n(Sec 4.5). Finally, we detail the training configurations and\nhyperparameters employed in our experiments (Sec 4.6).\n4.1. Molecule Captioning\nExperimental Setup : Molecule captioning primarily\nfocuses on generating textual descriptions of molecules\nbased on their inherent information, such as their SMILES\nstrings, graph representations, and other molecular features.\nTo evaluate the performance of our model, we conduct\nexperiments using a set of established Natural Language\nProcessing (NLP) metrics. These metrics include BLEU,\nwhich measures the n-gram overlap between generated\nand reference sentences; ROUGE, which emphasizes the\noverlap of various n-grams, word sequences, and word\npairs; and METEOR, a comprehensive metric that takes into\naccount precision, recall, synonymy, stemming, and word\norder. These collectively assess the quality and relevance\nof the generated text in molecule captioning. Within the\nframework of our GIT-Mol model, SciBERT and MolT5\nserve as language model components, and then we choose\nSciBERT and MolT5 as the baselines for our model. This\nselection is particularly pertinent in our Molecule Genera-\ntion experiments.\nMolecule Caption Dataset : Our team procured a sub-\nstantial dataset of 320,000 molecules accompanied by de-\ntailed descriptions from the PubChem Record Description\ncategory. We enhanced this dataset by conducting a thor-\nough crawl of the corresponding captions (up to a maximum\nof 3) from each molecule page. We augmented our dataset\nby combining it with the ChEBI-20 Dataset, thus forming a\nhighly comprehensive and informative multi-modal caption\ndataset.\nWe further carried out data cleaning to remove \"invalid\"\ndescriptions in the captions, such as those that merely\nspecify the source of the compound without any pertinent\ninformation about its properties or characteristics. However,\nwe opted to retain succinct descriptions that provide essen-\ntial information about the compound. This enriched dataset\nconsists of approximately 90,000 data points, incorporating\nmulti-modal data such as images of the molecules, SMILES\nstrings, and captions.\nResults and Observations : Our model demonstrates\nsuperior performance in generating high-quality, relevant\nmolecule captions. Detailed results are shown in Table 1.\nThis experiment underscores the potential of multi-modality\nand suggests areas for further optimization and explo-\nration.The GIT-Mol(graph) model delivers better perfor-\nmance than GIT-Mol(SMILES) across all metrics. It im-\nplies that graph representations might capture molecular\nstructures more effectively than SMILES strings. However,\nneither of them outperforms the multi-modal model. In\nthe ablation study, the employment of this multi-modal\nmodel resulted in an impressive 10%-15% improvement\nover single-modality models. It suggests that each modality\nbrings complementary information to the task. As for pre-\ntraining strategies, the model pre-trained only with XTM\nshows room for enhancement, underlining the effectiveness\nof the XTC strategy in boosting overall performance.\nCase Studies: We select a set of molecules, feed them\nto our model, and let it generate captions based on the\nlearned representations. The results are shown in Figure 2.\nFor each molecule, we present the ground truth, the captions\ngenerated by our model and the baseline model.\n4.2. Text-Based de novo Molecule Generation\nExperimental Setup : Text-Based de novo Molecule\nGeneration aims to produce molecular SMILES represen-\ntations based on provided textual captions of molecules. To\ngauge the accuracy and validity of the molecules generated\nby our model, we employ a mix of cheminformatics and\nNLP metrics:\n‚Ä¢ Fingerprint Tanimoto Similarity (FTS): We utilize\nthis to measure the chemical similarity between the\nground-truth molecules and the generated ones, con-\nsidering multiple fingerprint types: MACCS, RDK,\nand Morgan fingerprints. FTS provides a decimal\nvalue between 0 (no similarity) and 1 (perfect sim-\nilarity), offering a quantifiable measure of chemical\nresemblance.\n‚Ä¢ BLEU: Used primarily in NLP tasks, BLEU scores\nevaluate the overlap of n-grams between generated\nand reference SMILES strings, signifying the close-\nness of the generated SMILES strings to the actual\nones.\n‚Ä¢ Exact Match : It simply checks if the generated\nSMILES string matches the reference SMILES string.\nAn exact match score directly measures the model‚Äôs\nprecision in producing accurate molecular represen-\ntations.\n‚Ä¢ Levenshtein distance : This metric calculates the\nminimum number of single-character edits (i.e., in-\nsertions, deletions, or substitutions) needed to change\none SMILES string into another. A smaller distance\nPengfei Liu et al.: Preprint submitted to CIBM Page 7 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nModel BLEU-2 BLEU-4 ROUGH-1 ROUGH-2 ROUGH-L METEOR\nSciBERT 0.184 0.113 0.412 0.327 0.397 0.367\nMolT5-base 0.316 0.247 0.572 0.480 0.545 0.529\nGIT -Mol(SMILES) 0.264 0.176 0.477 0.374 0.451 0.430\nGIT -Mol(Graph) 0.290 0.210 0.540 0.445 0.512 0.491\nGIT -Mol(XTM) 0.264 0.187 0.521 0.421 0.494 0.471\nGIT -Molùëúùëù 0.312 0.237 0.556 0.468 0.535 0.525\nGIT -Mol 0.352 0.263 0.575 0.485 0.560 0.533\nTable 1\nMolecule captioning results. In the subsequent experimental results, ùëúùëù signifies models that do not utilize prompt learning,\nXTM represents models that only utilize XTM pre-training strategy.\nFigure 2: Study case of Molecule Caption. The GIT -Mol model exhibits precise chemical characterization, aligning closely\nwith ground truth information.\nindicates a closer similarity between the generated\nand reference SMILES string.\n‚Ä¢ Validity: We employ RDKit, a cheminformatics soft-\nware, to check the chemical validity of the generated\nSMILES string. By reporting the percentage of valid\nmolecules, we ensure that our model does not just\nproduce semantically accurate SMILES strings but\nalso chemically feasible ones.\nTo stay consistent with our methodology in the molecule\ncaptioning task, we employ MolT5-base as both the decoder\nand the baseline for comparison in our experimental setup.\nMolecule Generation Dataset: We use the same ChEBI-\n20 Dataset as MolT5 for fine-tuning our model on the\nmolecule generation task. It contains various molecular\nstructures represented in SMILES notation, providing a rich\nresource for training models to generate novel molecules.\nPengfei Liu et al.: Preprint submitted to CIBM Page 8 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nModel BLEU Exact Levenshtein MACCS FTS RDK FTS Morgan FTS Validity\nSciBERT 0.459 0.005 55.459 0.499 0.344 0.254 0.915\nMolT5-base 0.769 0.081 24.458 0.721 0.588 0.529 0.772\nGIT -Mol(XTM) 0.245 0.0 82.633 0.322 0.196 0.087 1.0\nGIT -Molùëúùëù 0.721 0.041 30.41 0.705 0.477 0.453 0.812\nGIT -Mol 0.756 0.051 26.315 0.738 0.582 0.519 0.928\nTable 2\nMolecule generation results . Our model performs similarly to MolT5-base for generating molecular SMILES strings but\nexcels in ensuring molecule validity. With a molecule validity of 92.8%, our model surpasses MolT5-base‚Äôs validity of 77.2%\nby over 20%.\nResults and Observations: Our model exhibits impres-\nsive results in generating chemically valid molecules that\nclosely match the ground-truth molecules regarding chem-\nical and structural characteristics. The results are shown\nin Table 2. Compared to the MolT5-base(caption) model,\nGIT-Mol(caption) slightly outperforms the MACCS FTS\nmetric and presents closely competitive results on the other\ntwo molecular similarity measures, RDK FTS and Morgan\nFTS. Notably, the performance of GIT-Mol(caption) on the\n‚ÄúValidity‚Äù metric reaches an impressive 0.928, significantly\nhigher than the other two models. This indicates that GIT-\nMol can generate a higher proportion of valid molecules\nwhile still preserving high molecular similarity. In the ab-\nlation study, the model pre-trained only with XTM displays\na higher propensity for overfitting, with a lack of answer\ndiversity and a tendency to generate homogenous molecular\nstructures. Additionally, we demonstrate the necessity of\nprompt learning in influencing the performance of this task.\nWe provide some examples of molecule generation based on\ntext descriptions and detailed analysis.\nCase Studies: In Figure 3, we showcase the outcomes\nfor every molecule instance. Our model, the baseline model,\nand GPT4 all generate SMILES strings, which are compared\nto the ground truth. This comparison allows for a visual\nevaluation of the quality and relevance of the generated\nmolecules, offering a better understanding of our model‚Äôs\neffectiveness.\n4.3. Molecule Image Recognition\nExperimental Setup : Molecular Image Recognition\nfocuses on converting 2D molecular images into their cor-\nresponding SMILES representations. We employ the iden-\ntical set of metrics as those used in \"Text-Based de novo\nMolecule Generation.\" For a fair and rigorous assessment,\nwe juxtapose our model with SwinOCSR, an acknowledged\nefficient model in molecular image captioning.\nMolecule Image Dataset: We employ the same dataset\nas the Molecule Caption Dataset utilized in the Molecule\nCaptioning Task. The distinction lies in the training data\ncomposition, where the pairings are between SMILES\nstrings and their corresponding molecular images rather\nthan textual descriptions.\nResults and Observations: The results are presented in\nTable 3. The empirical outcomes indicate that our model,\nwhether employing prompt learning or not, the GIT-Mol\nmodel outperforms the SwinOCSR model across all eval-\nuated metrics. It emphasizes the efficacy of the GIT-Mol\narchitecture in the molecule image recognition task. In the\nablation study, similar to findings in the molecule gener-\nation experiment, the model exclusively using the XTM\npre-training strategy not only overfitted more easily but\nalso converged more slowly, resulting in the generation of\nuniform molecular structures. Our model demonstrates the\nmodel‚Äôs extensibility and effectiveness of prompt learning.\n4.4. Molecular Property Prediction\nExperimental Setup : In our study, we use six essen-\ntial classification datasets related to molecular biological\nactivity from MoleculeNet (Wu et al., 2018) , including\nTox21, ToxCast, Sider, ClinTox, Bace, and BBBP. These\ndatasets contribute to understanding a molecular properties\nand effects. We ensure the model‚Äôs fairness and robustness\nby using a scaffold-based splitting approach for structuring\ntraining, validation, and test sets. This leads to diverse\nmolecule evaluations. The pre-trained model on PubChem\nprocesses the datasets into molecule embeddings, and we\noptimize using the Binary Cross-Entropy (BCE) loss func-\ntion.\nMoleculeNet for Molecular Property Prediction. In\nour study, we chose a few classification task datasets from\nMoleculeNet as our fine-tuning datasets. These datasets\ninclude Tox21, ToxCast, Sider, ClinTox, BBBP, and Bace,\neach covering different molecular property prediction tasks.\n‚Ä¢ Tox21: A collaborative project to identify potential\ntoxins, containing toxicity data for approximately\n7800 compounds across 12 different pathways.\n‚Ä¢ ToxCast: Contains biological activity data for about\n8600 environmental compounds from over 600 in\nvitro experiments, used to assess potential toxicity.\n‚Ä¢ Sider: A database of side effect information for\naround 1400 drugs, used for predicting potential drug\nside effects.\n‚Ä¢ ClinTox: Includes toxicity information for known\ndrugs and clinical toxicity predictions for unapproved\ncompounds, primarily used to predict clinical toxicity.\n‚Ä¢ BBBP: Utilized to predict whether compounds can\npenetrate the blood-brain barrier, aiding in under-\nstanding brain delivery capacity.\nPengfei Liu et al.: Preprint submitted to CIBM Page 9 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nFigure 3: Study case of Molecule Generation. GIT -Mol stands out for its ability to generate valid molecules. Besides, even\nif not identical to the ground truth, it still faithfully adhere to the features described in the textual captions.\nModel BLEU Exact Levenshtein MACCS FTS RDK FTS Morgan FTS Validity\nSwinOCSR 0.892 0.376 9.157 0.945 0.872 0.846 0.827\nGIT -Mol(XTM) 0.347 0 62.075 0.423 0.289 0.137 1\nGIT -Molùëúùëù 0.913 0.405 8.675 0.957 0.885 0.878 0.850\nGIT -Mol 0.924 0.461 6.575 0.962 0.906 0.894 0.899\nTable 3\nMolecule image recognition results . The results suggest that prompt learning aids in producing more accurate SMILES\nstrings from molecule images.\n‚Ä¢ Bace: Contains inhibitory activity information on\nbeta-secretase 1 (BACE1) for 1513 compounds, use-\nful in identifying potential candidate drugs for disease\ntreatment.\nResults and Observations : Our model performs com-\nmendably in predicting molecular properties, achieving high\nAUC scores across multiple runs. The results of our model\nand baselines are shown in Table 4. We demonstrate the\neffectiveness of multi-modal data combining graph and\nSMILES for molecular representation, which shows an\naverage performance improvement of 5%-10%.\n4.5. Embeddings Visualization\nGIT-Former Processing: We experimented on 80 molecules\nto create vector visualizations, as shown in Figure 4. The\nfirst image in Figure 4(a) shows the vector representations\ncreated by the encoders for each modality of molecular\ndata. The second image displays the vector representations\nwithout the image embeddings. Our analysis of these images\nreveals that the graph vectors are more concentrated in\nthe original vector representations, while the image rep-\nresentations are more diverse. Additionally, the SMILES\nand caption representations, both text-based, are not easily\ndistinguishable.\nPre-training Effects : In Figure 4(b), we display the\nvectors processed by an untrained GIT-Former. All vectors\nPengfei Liu et al.: Preprint submitted to CIBM Page 10 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nDataset Tox21 ‚Üë ToxCast ‚Üë Sider ‚Üë ClinTox ‚Üë BBBP ‚Üë Bace ‚Üë Avg\nMolecules 7831 8575 1427 1478 2039 1513 ‚Äì\nTask 12 617 27 2 1 1 ‚Äì\nKV-PLM 72.1¬±1.0 55.0¬±1.7 59.8¬±0.6 - 70.5¬±0.5 78.5¬±2.7 67.20\nGraphCL 75.1¬±0.7 63.0¬±0.4 59.8¬±1.3 77.5¬±3.8 67.8¬±2.4 74.6¬±2.1 69.64\nGraphMVP 74.9¬±0.5 63.1¬±0.2 60.2¬±0.13 79.1¬±2.8 70.8¬±0.5 79.3¬±1.5 71.23\nMoMu 75.6¬±0.3 63.4¬±0.5 60.5¬±0.9 79.9¬±4.1 70.5¬±2.0 76.7¬±2.1 71.1\nMole-BERT 76.8¬±0.5 64.3¬±0.2 62.8¬±1.1 78.9¬±3.0 71.9¬±1.6 80.8¬±1.4 72.58\nGIT -Mol(SMILES) 73.9¬±0.7 62.1¬±0.4 60.1¬±1.1 83.5¬±3.1 71.9¬±1.8 68.4¬±1.7 70.0\nGIT -Mol(Graph) 75.4¬±0.5 65.3¬±0.7 58.2¬±0.9 78.9¬±2.5 71.1¬±1.5 65.8¬±1.8 69.1\nGIT -Mol(G+S) 75.9¬±0.5 66.8¬±0.5 63.4¬±0.8 88.3¬±1.2 73.9¬±0.6 81.08¬±1.5 74.90\nTable 4\nResults for molecular property prediction (classification) . The combined use of SMILES and 2D graphs enhances our\nmulti-modal molecular representation, which outperforms both single-modal models and other multi-modal approaches.\nappear to move towards a uniform distribution. In contrast,\nFigure 4(c) shows the vectors processed by a pre-trained\nGIT-Former. The vector distribution shifts from dispersed to\nconcentrated, with the outermost layer being the graph em-\nbeddings, the image embeddings, and the SMILES strings\nand captions in the text modality. This occurs because our\nGIT-Former‚Äôs pre-training strategy involves aligning other\nmodalities to the text modality. The SMILES strings and\ncaptions can be further distinguished from the original\nvector representations.\nClustering and Distribution of atoms : The image in\nFigure 4(d) displays the distribution of the number of atoms\nin a molecule. The color changes from dark to light, indi-\ncating an increase in the number of atoms. The distribution\nof the number of atoms is similar when viewed on the\nsame vertical axis. Moving on to Figure 4(e), we conducted\nK-means clustering on the molecules. We observed the\ndistribution of C, N, and O atoms in Figure 4(f) based on\ndifferent groups. We can see some differences in the distri-\nbution among the different groups. However, by represent-\ning different molecules based on these differences, our GIT-\nFormer can differentiate between different modalities, data\ntypes within the same modality, and molecular properties\nwithin the same data type.\n4.6. Training Settings\nTo achieve optimal performance with our GIT-Former\nmodel, we carefully designed both pretraining and fine-\ntuning stages. This section provides an overview of the\ntraining configurations, detailing the encoder settings, hy-\nperparameter values, and the specific adjustments made\nfor different molecular tasks. By meticulously orchestrating\nthese settings, we ensure that GIT-Former can harness its\nfull potential in varied application scenarios.\n‚Ä¢ Pre-training and Hyperparameters : The encoder\nsettings freeze both the image and graph encoders.\nFor modality selection, 1-3 arbitrary modalities and\nthe target text modality are selected for pre-training.\nThe learning rate ranges from 1e-4 to 5e-5, and the\ntraining process is constrained to less than 20 epochs.\nThe batch size is determined by the number of input\nmodalities, ranging from 32 to 128 (the more input\nmodalities, the smaller the batch size).\n‚Ä¢ Optional Settings : There are options for replace-\nment in the GIT-Former model, such as replacing\nthe Embedding layer with other models like BERT\nor MolT5‚Äôs Encoder. Training strategies can also be\ntailored to the experiment, including Xmodal-Text\nMatching (XTM), Xmodal-Text Contrastive Learning\n(XTC), or both.\n‚Ä¢ Molecular Translation Task : The task type deter-\nmines the hyperparameters, including input and out-\nput modalities and batch size. The model has No\nparameters frozen, and the learning rate ranges from\n1e-4 to 5e-5. The model is trained for less than 20\nepochs, with a batch size between 32 and 64.\n‚Ä¢ Molecular Property Prediction Task: The task de-\ntermines specific hyperparameters, such as batch size.\nThe model‚Äôs Embedding layer utilizes MolT5-large.\nThe learning rate is again set between 1e-4 and 5e-5,\nwith less than 100 epochs in training. Patience is set\nto 10, and batch size ranges between 16 and 256.\n5. Discussion\nData Quality: In molecular science, the quality and di-\nversity of data are crucial. Multi-modal data provides a more\ncomprehensive perspective, enhancing model predictions.\nFurthermore, the datasets require benchmarks, validation\nsubsets, and metrics to gauge their representativeness and\nchallenges.\nModel Techniques: Large language models, like the\nGPT series, have achieved significant success in natural\nlanguage processing and multi-modal tasks. Applying pre-\ntrained models to specific molecular tasks and refining them\ncan be highly beneficial. Moreover, distillation techniques\ncan shrink models, fitting them for limited-resource settings,\nand data distillation targets training valuable subsets from\nlarge datasets. On the other hand, incorporating molecu-\nlar science knowledge into model structures can pave a\nPengfei Liu et al.: Preprint submitted to CIBM Page 11 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nFigure 4: Embeddings visualization. (a) Original vector representations from various molecular data modalities. (b) Vector\ndistribution is processed by an untrained GIT -Former, illustrating a tendency towards uniformity. (c) Hierarchical vector\ndistribution processed by a pre-trained GIT -Former showcasing the layered separation of modalities with the outermost layer\nrepresenting graph embeddings, followed by image embeddings, and innermost containing SMILES strings and captions from\nthe text modality. (d) Distribution of atoms in molecules, with color gradients indicating increasing atom numbers. (e) Results of\nK-means clustering applied to molecular data. (f) Distribution of C, N, and O atoms across different clusters. The pre-training\neffects demonstrate GIT -Former‚Äôs ability to differentiate among modalities, subtypes within a modality, and specific properties\nwithin a given data type.\nmore robust learning pathway, demanding specially de-\nsigned model layers or preprocessing steps.\nScientific Insights: Beyond vector visualization, visual\noutputs from models can aid scientists in comprehend-\ning decision-making processes. Attention mechanisms and\nother interpretability tools can reveal the data parts the\nPengfei Liu et al.: Preprint submitted to CIBM Page 12 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nmodel relies on for predictions and which structures in the\nmodel focus on specific knowledge areas.\nPractical Implications: The improvement of 5%-10%\nin accuracy for property prediction and a significant 20.2%\nenhancement in molecule generation validity demonstrate\nthe superior generalizability of our multi-modal approach.\nThe advancements benefit AI-aided drug discovery (AIDD).\nA model with higher predictive precision can speed up the\nprocess of drug discovery, enhancing both time and cost\nefficiency. In drug discovery, where expenses can reach\nbillions, even a 5% increase in accuracy could mean saving\nmillions, marking a substantial economic impact.\n6. Limitations\nWhile our model leverages the parameter-efficient train-\ning method of prompt learning, the speed of training is still a\nchallenge. To address this, especially as tasks become com-\nplex and the model size grows, we‚Äôre considering the use\nof Parameter-Efficient Fine-Tuning (PEFT) to reduce the\nnumber of training parameters and enhance performance.\nHowever, for tasks like molecular captioning and genera-\ntion, we face another hurdle: the absence of appropriate\nevaluation to measure the model‚Äôs ability. This limitation\nmakes it challenging to assess the model‚Äôs effectiveness in\nthese specific areas.\n7. Conclusions\nWe introduce GIT-Mol, a specialized multi-modal large\nlanguage model tailored for molecular science, overcom-\ning significant challenges in data collection, particularly\nin acquiring scarce molecular captions. Additionally, we\nfaced and overcame difficulties in training large models, em-\nploying advanced parallel training algorithms. Compared to\nbaseline models, it demonstrates relative advantages in tasks\nsuch as molecule captioning, generation, image captioning,\nand property prediction. In addition, a new multi-modal\ndata mixer, GIT-Former, is proposed for fusing multi-modal\nmolecular data. Our model outperforms existing methods\nand offers an any-to-language modality translation strategy,\nenhancing flexibility for various applications.\nFor future directions, we plan to integrate techniques\nsuch as Low-Rank Adaptation (LoRA) and Adapters to\nexpedite the training process. Moreover, we will explore the\nextent to which prompts enhance model performance. As\nfor new downstream tasks, such as compound name recog-\nnition and chemical reaction prediction, we will enhance the\nmodel‚Äôs flexibility and adaptability. These improvements\nare expected to significantly impact fields like molecular\nscience and drug development, potentially leading to more\nefficient and insightful research.\nData and Software Availability:\nThe data and software can be accessed at https://\ngithub.com/AI-HPC-Research-Team/GIT-Mol .\nIt is available for non-commercial use.\nFunding\nThis work is supported by grants from the National Nat-\nural Science Foundation of China (61902446, 62172456,\nand 91937302) and the Peng Cheng Cloud-Brain of Peng\nCheng Laboratory.\nCRediT authorship contribution statement\nPengfei Liu: Conceptualization, Methodology, Data cu-\nration, Model traning, Writing-original draft. Yiming Ren:\nModel traning, Writing-original draft. Jun Tao: Writing\nreview editing. Zhixiang Ren: Conceptualization, Formal\nanalysis, Supervision, Funding acquisition, Writing review\nediting.\nDeclaration of Competing Interest\nThe authors declare that they have no known competing\nfinancial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nAcknowledgments\nThe authors appreciate Yue Zhou from Peng Cheng Lab-\noratory for the technical advice. The research is supported\nby the Peng Cheng Cloud-Brain of Peng Cheng Laboratory.\nReferences\nAlayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y ., Lenc,\nK., Mensch, A., Millican, K., Reynolds, M., et al., 2022. Flamingo:\na visual language model for few-shot learning. Advances in Neural\nInformation Processing Systems 35, 23716‚Äì23736.\nBao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y ., Yue, G., Cao, Y ., Su,\nH., Zhu, J., 2023. One transformer fits all distributions in multi-modal\ndiffusion at scale. arXiv preprint arXiv:2303.06555 .\nBao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O.K., Aggarwal,\nK., Som, S., Piao, S., Wei, F., 2022. Vlmo: Unified vision-language\npre-training with mixture-of-modality-experts. Advances in Neural\nInformation Processing Systems 35, 32897‚Äì32912.\nBeltagy, I., Lo, K., Cohan, A., 2019. Scibert: A pretrained language model\nfor scientific text. arXiv preprint arXiv:1903.10676 .\nBento, A.P., Hersey, A., F√©lix, E., Landrum, G., Gaulton, A., Atkinson, F.,\nBellis, L.J., De Veij, M., Leach, A.R., 2020. An open source chemical\nstructure curation pipeline using rdkit. Journal of Cheminformatics ,\n1‚Äì16.\nBilodeau, C., Jin, W., Jaakkola, T., Barzilay, R., Jensen, K.F., 2022. Gener-\native models for molecular discovery: Recent advances and challenges.\nWiley Interdisciplinary Reviews: Computational Molecular Science 12,\ne1608.\nDevlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,\net al., 2020. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929 .\nEdwards, C., Lai, T., Ros, K., Honke, G., Cho, K., Ji, H., 2022. Translation\nbetween molecules and natural language, in: Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pp.\n375‚Äì413.\nEdwards, C., Zhai, C., Ji, H., 2021. Text2mol: Cross-modal molecule\nretrieval with natural language queries, in: Proceedings of the 2021\nPengfei Liu et al.: Preprint submitted to CIBM Page 13 of 14\nGIT -Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text\nConference on Empirical Methods in Natural Language Processing, pp.\n595‚Äì607.\nFilippov, I.V ., Nicklaus, M.C., 2009. Optical structure recognition software\nto recover chemical information: Osra, an open source solution.\nFloridi, L., Chiriatti, M., 2020. Gpt-3: Its nature, scope, limits, and\nconsequences. Minds and Machines 30, 681‚Äì694.\nHastings, J., Owen, G., Dekker, A., Ennis, M., Kale, N., Muthukrishnan,\nV ., Turner, S., Swainston, N., Mendes, P., Steinbeck, C., 2016. Chebi\nin 2016: Improved services and an expanding collection of metabolites.\nNucleic Acids Research 44, D1214‚ÄìD1219.\nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image\nrecognition, in: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 770‚Äì778.\nHou, Z., Liu, X., Cen, Y ., Dong, Y ., Yang, H., Wang, C., Tang, J.,\n2022. Graphmae: Self-supervised masked graph autoencoders, in:\nProceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining, pp. 594‚Äì604.\nHuang, S., Dong, L., Wang, W., Hao, Y ., Singhal, S., Ma, S., Lv, T.,\nCui, L., Mohammed, O.K., Liu, Q., et al., 2023. Language is not all\nyou need: Aligning perception with language models. arXiv preprint\narXiv:2302.14045 .\nJia, C., Yang, Y ., Xia, Y ., Chen, Y .T., Parekh, Z., Pham, H., Le, Q., Sung,\nY .H., Li, Z., Duerig, T., 2021. Scaling up visual and vision-language\nrepresentation learning with noisy text supervision, in: International\nConference on Machine Learning, PMLR. pp. 4904‚Äì4916.\nKim, S., Chen, J., Cheng, T., Gindulyte, A., He, J., He, S., Li, Q.,\nShoemaker, B.A., Thiessen, P.A., Yu, B., et al., 2019. Pubchem 2019\nupdate: improved access to chemical data. Nucleic Acids Research 47,\nD1102‚ÄìD1109.\nLi, J., Li, D., Savarese, S., Hoi, S., 2023a. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels. arXiv preprint arXiv:2301.12597 .\nLi, J., Liu, Y ., Fan, W., Wei, X.Y ., Liu, H., Tang, J., Li, Q., 2023b.\nEmpowering molecule discovery for molecule-caption translation with\nlarge language models: A chatgpt perspective. arXiv preprint\narXiv:2306.06615 .\nLi, Y ., Gao, C., Song, X., Wang, X., Xu, Y ., Han, S., 2023c. Druggpt:\nA gpt-based strategy for designing potential ligands targeting specific\nproteins. bioRxiv , 2023‚Äì06.\nLiu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang, J., Xiao, C.,\nAnandkumar, A., 2022. Multi-modal molecule structure-text model for\ntext-based retrieval and editing. arXiv preprint arXiv:2212.10789 .\nLiu, S., Wang, H., Liu, W., Lasenby, J., Guo, H., Tang, J., 2021a. Pre-\ntraining molecular graph representation with 3d geometry. arXiv\npreprint arXiv:2110.07728 .\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B.,\n2021b. Swin transformer: Hierarchical vision transformer using shifted\nwindows, in: Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 10012‚Äì10022.\nLuo, R., Sun, L., Xia, Y ., Qin, T., Zhang, S., Poon, H., Liu, T.Y .,\n2022. Biogpt: generative pre-trained transformer for biomedical text\ngeneration and mining. Briefings in Bioinformatics 23, bbac409.\nOpenAI, 2023. GPT-4 technical report. CoRR abs/2303.08774.\nPeryea, T., Katzel, D., Zhao, T., Southall, N., Nguyen, D.T., 2019. Molvec:\nOpen source library for chemical structure recognition, in: Abstracts of\nPapers of the American Chemical Society, Amer Chemical Soc 1155\n16TH ST, NW, W ASHINGTON, DC 20036 USA.\nRadford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,\nSastry, G., Askell, A., Mishkin, P., Clark, J., et al., 2021. Learning\ntransferable visual models from natural language supervision, in: In-\nternational Conference on Machine Learning, PMLR. pp. 8748‚Äì8763.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.,\n2019. Language models are unsupervised multitask learners. OpenAI\nblog 1, 9.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\nY ., Li, W., Liu, P.J., 2020. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine Learning\nResearch 21, 5485‚Äì5551.\nRajan, K., Zielesny, A., Steinbeck, C., 2021. Decimer 1.0: deep learning\nfor chemical image recognition using transformers. Journal of Chemin-\nformatics 13, 1‚Äì16.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M., 2022. Hierarchi-\ncal text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 .\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-\nMaron, G., Gimenez, M., Sulsky, Y ., Kay, J., Springenberg, J.T., et al.,\n2022. A generalist agent. arXiv preprint arXiv:2205.06175 .\nRodrigues, T., Reker, D., Schneider, P., Schneider, G., 2016. Counting on\nnatural products for drug design. Nature Chemistry 8, 531‚Äì541.\nStefanini, M., Cornia, M., Baraldi, L., Cascianelli, S., Fiameni, G., Cuc-\nchiara, R., 2022. From show to tell: A survey on deep learning-based\nimage captioning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 45, 539‚Äì559.\nSu, B., Du, D., Yang, Z., Zhou, Y ., Li, J., Rao, A., Sun, H., Lu, Z., Wen, J.R.,\n2022. A molecular multimodal foundation model associating molecule\ngraphs with natural language. arXiv preprint arXiv:2209.05481 .\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, ≈Å., Polosukhin, I., 2017. Attention is all you need. Advances in\nneural information processing systems 30.\nWang, H., Ma, S., Huang, S., Dong, L., Wang, W., Peng, Z., Wu, Y ., Bajaj,\nP., Singhal, S., Benhaim, A., et al., 2022a. Foundation transformers.\narXiv preprint arXiv:2210.06423 .\nWang, J., Shen, Z., Liao, Y ., Yuan, Z., Li, S., He, G., Lan, M., Qian,\nX., Zhang, K., Li, H., 2022b. Multi-modal chemical information\nreconstruction from images and texts for exploring the near-drug space.\nBriefings in Bioinformatics 23, bbac461.\nWang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal,\nK., Mohammed, O.K., Singhal, S., Som, S., et al., 2022c. Image as\na foreign language: Beit pretraining for all vision and vision-language\ntasks. arXiv preprint arXiv:2208.10442 .\nWang, Y ., Wang, J., Cao, Z., Barati Farimani, A., 2022d. Molecular con-\ntrastive learning of representations via graph neural networks. Nature\nMachine Intelligence 4, 279‚Äì287.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V .,\nZhou, D., et al., 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing\nSystems 35, 24824‚Äì24837.\nWeininger, D., 1988. Smiles, a chemical language and information system.\n1. introduction to methodology and encoding rules. Journal of Chemical\nInformation and Computer Sciences 28, 31‚Äì36.\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N., 2023. Visual\nchatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint arXiv:2303.04671 .\nWu, Z., Ramsundar, B., Feinberg, E.N., Gomes, J., Geniesse, C., Pappu,\nA.S., Leswing, K., Pande, V ., 2018. Moleculenet: a benchmark for\nmolecular machine learning. Chemical Science 9, 513‚Äì530.\nXu, Z., Li, J., Yang, Z., Li, S., Li, H., 2022. Swinocsr: end-to-end optical\nchemical structure recognition using a swin transformer. Journal of\nCheminformatics 14, 1‚Äì13.\nYang, L., Zhang, Z., Song, Y ., Hong, S., Xu, R., Zhao, Y ., Shao, Y ., Zhang,\nW., Cui, B., Yang, M.H., 2022. Diffusion models: A comprehensive\nsurvey of methods and applications. arXiv preprint arXiv:2209.00796 .\nZeng, Z., Yao, Y ., Liu, Z., Sun, M., 2022. A deep-learning system bridging\nmolecule structure and biomedical text with comprehension comparable\nto human professionals. Nature Communications 13, 862.\nZhou, G., Gao, Z., Ding, Q., Zheng, H., Xu, H., Wei, Z., Zhang, L.,\nKe, G., 2023. Uni-mol: A universal 3d molecular representation\nlearning framework, in: The Eleventh International Conference on\nLearning Representations. URL: https://openreview.net/\nforum?id=6K2RM6wVqKu.\nZhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M., 2023. Minigpt-4: En-\nhancing vision-language understanding with advanced large language\nmodels. arXiv preprint arXiv:2304.10592 .\nPengfei Liu et al.: Preprint submitted to CIBM Page 14 of 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.764319658279419
    },
    {
      "name": "Natural language processing",
      "score": 0.6060251593589783
    },
    {
      "name": "Modal",
      "score": 0.6004590392112732
    },
    {
      "name": "Artificial intelligence",
      "score": 0.538952648639679
    },
    {
      "name": "Graph",
      "score": 0.5369389653205872
    },
    {
      "name": "Translation (biology)",
      "score": 0.4602019488811493
    },
    {
      "name": "Language model",
      "score": 0.42251744866371155
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23550564050674438
    },
    {
      "name": "Chemistry",
      "score": 0.1269867718219757
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    }
  ],
  "cited_by": 8
}