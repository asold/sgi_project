{
    "title": "Language model fusion for streaming end to end speech recognition",
    "url": "https://openalex.org/W3154308281",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5065949254",
            "name": "Rodrigo Cabrera",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100359288",
            "name": "Xiaofeng Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5074978208",
            "name": "Mohammadreza Ghodsi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5026192467",
            "name": "Zebulun Matteson",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058585526",
            "name": "Eugene Weinstein",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5023098355",
            "name": "Anjuli Kannan",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1828163288",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2773781902",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2747467128",
        "https://openalex.org/W2577366047",
        "https://openalex.org/W3028075677",
        "https://openalex.org/W2617258110",
        "https://openalex.org/W2962760690",
        "https://openalex.org/W3015686596",
        "https://openalex.org/W2883416004",
        "https://openalex.org/W47568227",
        "https://openalex.org/W2748679025",
        "https://openalex.org/W2939111082",
        "https://openalex.org/W2955856352"
    ],
    "abstract": "Streaming processing of speech audio is required for many contemporary practical speech recognition tasks. Even with the large corpora of manually transcribed speech data available today, it is impossible for such corpora to cover adequately the long tail of linguistic content that's important for tasks such as open-ended dictation and voice search. We seek to address both the streaming and the tail recognition challenges by using a language model (LM) trained on unpaired text data to enhance the end-to-end (E2E) model. We extend shallow fusion and cold fusion approaches to streaming Recurrent Neural Network Transducer (RNNT), and also propose two new competitive fusion approaches that further enhance the RNNT architecture. Our results on multiple languages with varying training set sizes show that these fusion methods improve streaming RNNT performance through introducing extra linguistic features. Cold fusion works consistently better on streaming RNNT with up to a 8.5% WER improvement.",
    "full_text": "LANGUAGE MODEL FUSION FOR STREAMING END TO END SPEECH RECOGNITION\nRodrigo Cabrera, Xiaofeng Liu∗, Mohammadreza Ghodsi,\nZebulun Matteson, Eugene Weinstein, Anjuli Kannan\n{rodrigocabrera,ghodsi,zmatteson,weinstein, anjuli}@google.com, xiaofeng.liu@twosigma.com\nGoogle Inc.\nABSTRACT\nStreaming processing of speech audio is required for many\ncontemporary practical speech recognition tasks. Even with\nthe large corpora of manually transcribed speech data avail-\nable today, it is impossible for such corpora to cover ade-\nquately the long tail of linguistic content that’s important for\ntasks such as open-ended dictation and voice search. We seek\nto address both the streaming and the tail recognition chal-\nlenges by using a language model (LM) trained on unpaired\ntext data to enhance the end-to-end (E2E) model. We extend\nshallow fusion and cold fusion approaches to streaming Re-\ncurrent Neural Network Transducer (RNNT), and also pro-\npose two new competitive fusion approaches that further en-\nhance the RNNT architecture. Our results on multiple lan-\nguages with varying training set sizes show that these fusion\nmethods improve streaming RNNT performance through in-\ntroducing extra linguistic features. Cold fusion works con-\nsistently better on streaming RNNT with up to a 8.5% WER\nimprovement.\nIndex Terms— shallow fusion, cold fusion, RNNT, end-\nto-end, ASR\n1. INTRODUCTION\nEnd-to-end (E2E) models for automatic speech recognition\n(ASR) tasks have gained popularity because these models\npredict subword sequences from acoustic features with a\nsingle model, unlike classic ASR systems which have sep-\narate acoustic, pronunciation, and language model com-\nponents. The most common E2E architectures are either\nattention-based (e.g., listen, attention, and spell (LAS) [1]) or\nRNNT [2] models (see [3] for comparison).\nOverall, E2E models show comparable or better word er-\nror rate (WER) performance with a simpliﬁed system setup.\nWhile standard attention-based models must inspect the entire\ninput sequence before generating outputs, streaming-friendly\nmodiﬁcations of such models have been proposed, such as\nMoChA [4] and neural transducer [5].\nWhile these approaches have shown promise, the RNNT\narchitecture is an alternative E2E model that can natively\n∗Performed the work while at Google\npredict output sequences on the ﬂy (with unidirectional en-\ncoders), and thus is a natural choice for streaming appli-\ncations. RNNT models have also shown extremely strong\nresults., e.g., He et al. [6] presented an RNNT based real-\ntime steaming recognizer, which outperformed classic ASR\nmodels by a wide margin.\nClassic ASR models leverage unpaired text data with a\nseparately trained language model (LM) and second-pass\nrescoring model [7], but unpaired text data cannot be easily\nutilized when training E2E models. Although E2E models\nhave overall shown strong results, they have been shown to\nhave difﬁculty accurately modeling tail phenomena such as\nproper nouns, numerics, and accented speech [8, 9, 10, 11],\ndue to the requirement that they be trained on paired (speech-\ntranscript) data.\nRecent papers have proposed fusing E2E models with\nLMs trained with text data (usually referred to this as fusion),\nincluding shallow fusion [12, 13], deep fusion [12], cold fu-\nsion [14], component fusion [15], etc. Most experiments used\nneural LMs, and some used n-gram fst LMs [1, 16, 12, 17].\nSee [18] for comparison of some of these approaches. How-\never, these experiments were performed with standard non-\nstreaming attention models. Fusion approaches with stream-\ning E2E models have been unexplored, and it is unknown\nwhether the observe gains on attention-based models could\nbe translated to streaming models.\nIn this paper, we explore shallow fusion, cold fusion and\ntwo new fusion approaches unique to the streaming RNNT\nmodels in section 2, we detail the experimentation across\nmultiple languages of varying sizes of training data in section\n3, and we analyze the results in section 4. We show that while\nshallow fusion worked better than cold fusion for attention-\nbased models, cold fusion outperforms shallow fusion for\nRNNT models, with a WER reduction of up to 8.5%.\n2. METHODS\n2.1. RNN Transducer\nThe RNNT architecture proposed by Graves [2] consists of\nan encoder, a prediction network, and typically a joint net-\nwork. It directly predicts a sequence of words, subwords [19]\nor graphemes without using an external pronunciation model\narXiv:2104.04487v1  [cs.CL]  9 Apr 2021\nor language model. Like Connectionist temporal classiﬁca-\ntion (CTC), its target symbol set is augmented with a blank\nsymbol (⟨blank⟩) such that the RNNT model does not out-\nput a grapheme or subword symbol at every time frame. The\nRNNT outputs a symbol at each time frame t = 1,2,...,T\nwhere T is the total number of frames, the input of the en-\ncoder is the log-mel ﬁlterbank energies of dimension d. The\ninput of the prediction network is the last non- ⟨blank⟩ sym-\nbol during prediction. The joint network takes as input the\noutput vectors of both encoder and prediction networks and\noutputs logits which are then passed to the softmax layer to\npredict subword symbols. In this work, we focus on stream-\ning RNNT wordpiece models, where the encoder and predic-\ntion networks both use unidirectional LSTM layers, and the\npredicted symbols are wordpieces. During decoding, a blank\npenalty is usually added to adjust the posterior probability of\nthe ⟨blank⟩ symbol, and its value is usually optimized through\nparameter sweep.\n2.2. Fusion with RNN-LM\nThe recurrent neural network (RNN) based LM predicts the\nprobability of a symbol given the context, and is trained with\ntext-only data. It contains an embedding layer followed by a\nstack of unidirectional RNN layers. For a sequence of word-\npieces wT\n1 , the RNN-LM computes a probability:\nP(wT\n1 ) =\nT∏\ni=1\nP(wi|w1,w2,...,w i−1). (1)\nOne big difference between attention-based models and\nRNNT models is the existence of the ⟨blank⟩ symbol in\nRNNT models, and that symbols needs to be handled cor-\nrectly during fusion. While the RNN-LMs are trained with-\nout knowledge of the ⟨blank⟩ symbol, the LM probability\nof ⟨blank⟩ must be deﬁned during inference time. In this\nwork we make the language model’s probability of ⟨blank⟩\nbe equal to the RNNT probability,i.e., log Plm(⟨blank⟩) =\nlog Prnnt(⟨blank⟩), where log Plm and log Prnnt are the\nprobability of LM and RNNT, respectively. This means that\nwhen the RNNT model outputs ⟨blank⟩, the RNN-LM is not\nupdated and that the probability for the ⟨blank⟩ symbol after\nfusion remains unchanged.\n2.3. Shallow Fusion\nShallow fusion (SF), ﬁrst proposed by Gulchere, et al. [12],\nintegrates an external LM with the RNNT only during infer-\nence time, as shown in Fig 1a. The scores of RNNT and\nRNN-LM are log-linearly combined before the softmax layer.\nlog P(yt) = logPrnnt(yt) +βlog Plm(yt), (2)\nwhere βis the LM weight, andyt is the output symbol at time\nframe t.\n(a) Shallow fusion\n (b) Early shallow fusion\n(c) Cold fusion\n (d) Early cold fusion\nFig. 1: Illustrations of our different fusion styles.\nIn SF, the RNNT and the LM are trained independently\nwith different training data, and thus the approach is very\nmodular and it is easy to integrate the LM during inference\ntime. Note that shallow fusion performance is sensitive to\nthe LM weight βand to the RNNT’s blank penalty, and these\nvalues usually need to be optimized through sweeping when\nusing the LM.\n2.4. Early Shallow Fusion\nOne popular hypothesis [3] about the RNNT architecture is\nthat the prediction network is analogous to an LM. Inspired\nby this hypothesis we propose the early shallow fusion (ESF)\napproach as illustrated in Fig. 1b It fuses the outputs of the\npretrained LM with the RNNT prediction network using log-\nlinear interpolation before feeding to the joint network, i.e.,\nlPred\nt = DNN(hPred\nt )\nhESF\nt = lPred\nt + βlLM\nt\n(3)\nWhere hPred\nt is projected to have a logit output and lLM\nt is\nthe logit output of the external LM at time frame t, β is the\nLM weight, and hESF\nt is the interpolated vector that is fed to\nthe joint network. Similar to SF, the LM is used only during\ninference time and not for RNNT training. It’s possible to\nﬁne tune the RNNT model by further training it for extra steps\nwith the fused LM on supervised data.\n2.5. Cold Fusion\nIn cold fusion (CF) [20], the model is trained using a pre-\ntrained LM that remains ﬁxed when training the rest of the\nmodel. A ﬁne-grained gating approach is applied on the LM\nlogit instead of the hidden state to allow for ﬂexible swapping\nand to improve performance on out-of-domain scenarios as in\ncomponent fusion [15]. The model architecture is illustrated\nin Fig. 1c, deﬁned as:\nhLM\nt = DNN(lLM\nt )\ngt = σ(W[sjn\nt ; hLM\nt ] +b)\nsCF\nt = [sjn\nt ; gt ˙hLM\nt ]\nrCF\nt = DNN(sCF\nt )\nˆP(yt|x,y<t) =softmax(rCF\nt )\n(4)\nWhere lLM\nt is the logit output of the language model,gt is the\nﬁne-grained gate output parametrized by W, which controls\nthe importance of the contribution of the hidden state of the\nLM. sjn\nt is the state of the joint network, sCF\nt is the ﬁnal\nfused state, and DNN is a feedforward neural network. In\nour experiments we have been using a single layer of fully\nconnected network.\nTraining the DNN and sigmoid functions help the model\nlearn to use the LM when predicting a non- ⟨blank⟩ symbol,\nand only use the RNNT when predicting the ⟨blank⟩ sym-\nbol. In the CF approach, sweeping for LM weight and blank\npenalty is not needed.\n2.6. Early Cold Fusion\nWe propose the early cold fusion (ECF) approach as illus-\ntrated in Fig. 1d. ECF is inspired by cold fusion and fuses the\nLM by passing its logits through a DNN layer, the DNN\noutput is concatenated with the hidden states of the predic-\ntion network and fed into a gating layer before sending to the\njoint network. The LM is used during training and inference.\nhLM\nt = DNN(lLM\nt )\ngt = σ(W[hPred\nt ; hLM\nt ] +b)\nhECF\nt = [hPred\nt ; gt ˙hLM\nt ]\n(5)\n3. EXPERIMENT SETUP\n3.1. Model Architecture\nThe RNNT models and LMs in all experiments shared the\nsame architectures. For RNNT, the encoder network con-\nsists of 8 unidirectional LSTM layers, where each layer has\n2048 hidden units followed by a 640-dimensional projection\nlayer. A stacking layer is inserted after the second layer,\nwhich stacks the outputs of two neighboring frames with a\nstride of 2 for speed improvement. The prediction network\ncontains 2 unidirectional LSTM layers, where each layer has\n2048 hidden units and followed by a 640-dimensional pro-\njection layer. The outputs of the encoder and prediction net-\nworks were fed into the joint network. The joint network is\na feed-forward network with 640 hidden units, which accepts\ninput from both the encoder and the prediction networks. The\noverall RNNT model has 120 million parameters.\nThe RNN-LM model has 2 unidirectional LSTM layers,\neach with 2048 units, with an embedding layer of 128 dimen-\nsions. This model has 60 million parameters. Both the RNNT\nand LM models were trained with using wordpieces and a vo-\ncabulary size of 4096.\n3.2. Data\nWe performed experiments on three languages: Greek, Nor-\nwegian, and Sinhala. The training sets consisted of 6,700,\n3,500, and 160 hours of utterances respectively, which were\nanonymized and hand-transcribed, and are representative of\nGoogle’s voice search trafﬁc. These languages covered the\ncases with low to high amount of training data. During train-\ning, the utterances were artiﬁcially noisiﬁed using a room\nsimulator; the noise and reverberation had an average signal-\nto-noise ratio of 12dB [21]. The noise sources came from\nYouTube or noisy environmental recordings.\nFor the RNN-LM, during training the data were randomly\ndrawn from a mix of text sources with different weights: 0.6\nfor the same hand-transcribed data used in RNNT training,\nand 0.1 for each of YouTube search logs, Google search\nqueries, Maps search queries and crawled web documents.\nThis in total accounted for over 200 million text sentences\nfor Sinhala and over a billion sentences for Greek and Nor-\nwegian. The RNN-LMs are trained by minimizing the log\nperplexity over a held out set. For all the languages, the fused\nmodels were tested on anonymized voice search testsets,\nwhich were drawn from Google’s speech trafﬁc and were not\nused while training the model. For shallow fusion models,\nthe LM weight and blank penalty were also optimized by\nsweeping using these testsets.\n4. RESULTS\nTable 1: Log perplexity of the LMs over the VS testset.\n.\nLog Perplexity\nNorwegian 6.64\nGreek 6.75\nSinhala 11.92\nFor each of the three languages, the LM was trained once\nwith the text-only data, and applied in all the experiments.\nTable 1 shows the perplexities of the pretrained LMs when\npredicting the test sets. It can be seen that it was much higher\nfor Sinhala than Greek or Norwegian. This is expected be-\ncause the Sinhala LM training data were dominated by the\nnon-transcribed text sources which might be more different\nfrom the test sets.\n4.1. Shallow fusion and cold fusion\nTable 2 shows the performance in terms of WER on the test-\nsets for the baseline model, a modiﬁed RNNT model with\nlarger prediction network (LPN), shallow fusion (SF), and\nTable 2: Shallow fusion and cold fusion results.\nModel #params Norweigian Greek Sinhala\nBaseline 120M 24.5 14.5 69.4\nLPN 191M 24.4\n(-0.40%) N/A 70.0\n(+0.86%)\nSF 180M 24.2\n(-1.22%)\n14.4\n(-0.68%)\n69.3\n(-0.14%)\nCF 200M 22.4\n(-8.57%)\n14.0\n(-3.44%)\n69.0\n(-0.57%)\ncold fusion (CF). The LPN was identical to the baseline\nmodel except its prediction network had 6 LSTM layers in-\nstead of 2, such that the number of parameters was similar\nto the CF model. The LPN model on Norweigian showed\nslight improvement over the baseline model, suggesting that\njust increasing RNNT model size is not effective for lower\nresource languages. Overall, cold fusion performed signiﬁ-\ncantly better than shallow fusion. This is surprising as it’s the\nopposite pattern than what was seen with LAS models [18].\nShallow fusion gave a small gain (1.2%) for Norweigian, but\nhad no signiﬁcant improvement for Greek and Sinhala, while\ncold fusion improved on all three languages. This could be\nbecause the handling of ⟨blank⟩ probability in SF was not\nideal, while CF can beneﬁt from the extra parameters and\nthe co-training of the RNNT model with the LM where the\nRNNT model learned to adapt to the fused LM.\nNotably for Norwegian, cold fusion showed 8.57% gains,\nwhich was much larger than that for Greek (3.44%). This\ncould be because the RNNT model for Greek was trained with\na much larger data set than Norwegian. The large train set\ncovers more linguistic features compared to the coverage in\nNorwegian, and as a result the gain brought by extra text-data\nwas smaller. For Sinhala, both methods didn’t show signiﬁ-\ncant improvement. Our hypothesis is that the training set for\nSinhala was too small, causing the model to overﬁt quickly\nduring training; and that the linguistic features of the trained\nLM are too different from the test sets as manifested in the\nLM perplexity shown in Table 1. These results impliy that the\namount of training data is critical for RNNT performance.\nTable 3 shows two examples of the top hypothesis gener-\nated by the baseline RNNT model and cold fusion in Greek.\nThe hypotheses were transliterated to Latin for better under-\nstanding, and the differences between the hypotheses of the\ntwo models were highlighted. The last row shows the English\ntranslations of the transcript truth. The fused model correctly\nrecognized the utterances while the baseline model failed to\nproduce hypotheses that makes sense. This was likely be-\ncause these mistaken words were seen in the text-only data\nused in LM training while not in the transcribed speech-text\ndata used in the RNNT training. This example illustrates\nhow fusion cat help a streaming E2E model perform better on\n“tail” phenomena such as proper nouns. Fusion with the LM\nhelped the RNNT model to obtain a richer lattice and surface\nTable 3: Examples of the top hypothesis generated by the\nbaseline and the cold fused RNNT models for Greek.\nExample 1 Example 2\nBaseline\nphilakes ipsous tis\nAfstralias\nandrianoupolis\netsi re sporpou\nanevainis skalopatia\nCold fused\nRNNT\nphilakes ipsistis\nasphalias ( )\nandrianoupolis\nRange Rover sport\n( ) pou anevainis\nskalopatia\nEnglish\nTranslation\nhighest security\n( ) prisons in\nAndrianopolis\nRange Rover sport\n( ) that climbs\nstairs\nTable 4: Early shallow fusion and early cold fusion results.\nModel #params Norweigian Greek Sinhala\nBaseline 120M 24.5 14.5 69.4\nESF 180M 24.4\n(-0.40%)\n14.5\n(0.00%)\n69.3\n(-0.14%)\nECF 200M 22.7\n(-7.34%)\n14.2\n(-2.06%)\n68.1\n(-1.87%)\nbetter hypotheses, and thus resulted in correct inference.\n4.2. Early shallow fusion and early cold fusion\nTable 4 shows the results of early shallow fusion and early\ncold fusion, where the LMs were fused before the joint net-\nwork. It’s worth noting that ESF models needed ﬁne-tuning\nto get improvements. Similar to the results in Table 2, Sinhala\nhad the least WER gain while Norweigian had the most gain\nbecause of the same reasons. For Norweigian and Greek, both\nearly fusion approaches showed signiﬁcant gains in WER, but\nslightly less than SF and CF. This suggests that fusing the LM\nbefore or after the joint network produces similar results. This\nresult is valuable in that it shows us that if we choose to mod-\nify the topology of the RNNT model [22] we can be ﬂexible\nwith the placement of the fusion point while maintaining the\nsuperior quality afforded by fusing a text-trained LM.\n5. DISCUSSION\nIn this work we explored several existing fusion approaches\nas well as new methods that fuse a pre-trained language model\nat an earlier stage to improve RNNT performance for medium\nand low resource languages in a streaming setting. Among\nall the approaches, cold fusion performed best, with WER\nreduction up to 8.5% compared to the baseline. The lan-\nguage model brings additional linguistic features and helps\nthe RNNT to produce richer lattices and obtain better hy-\npotheses. Additionally, we showed that the fusion point can\nbe placed either before or after the joint network while main-\ntaining the quality gains.\n6. REFERENCES\n[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, at-\ntend and spell: A neural network for large vocabulary\nconversational speech recognition,” in ICASSP, 2016,\npp. 4960–4964.\n[2] Alex Graves, “Sequence transduction with recurrent\nneural networks,” ArXiv, vol. abs/1211.3711, 2012.\n[3] R. Prabhavalkar, K. Rao, T. Sainath, B. Li, L. Johnson,\nand N. Jaitly, “A comparison of sequence-to-sequence\nmodels for speech recognition,” in InterSpeech, 2017.\n[4] Chung-Cheng Chiu and Colin Raffel, “Monotonic\nchunkwise attention,” CoRR, vol. abs/1712.05382,\n2017.\n[5] Navdeep Jaitly, Quoc V Le, Oriol Vinyals, Ilya\nSutskever, David Sussillo, and Samy Bengio, “An on-\nline sequence-to-sequence model using partial condi-\ntioning,” in Advances in Neural Information Processing\nSystems 29, D. D. Lee, M. Sugiyama, U. V . Luxburg,\nI. Guyon, and R. Garnett, Eds., pp. 5067–5075. Curran\nAssociates, Inc., 2016.\n[6] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Al-\nvarez, D. Zhao, D. Rybach, A. Kannan, Y . Wu, R. Pang,\nQ. Liang, D. Bhatia, Y . Shangguan, B. Li, G. Pundak,\nK. C. Sim, T. Bagby, S. Chang, K. Rao, and A. Gru-\nenstein, “Streaming end-to-end speech recognition for\nmobile devices,” in ICASSP, 2019, pp. 6381–6385.\n[7] F. Biadsy, M. Ghodsi, , and D. Caseiro, “Effectively\nbuilding tera scale maxent language models incorporat-\ning non-linguistic signals,” in InterSpeech, 2017.\n[8] Tanja Schultz and Alex Waibel, “Multilingual and\ncrosslingual speech recognition,” in Proceedings of the\nDARPA Broadcast News Workshop 1998, 1998.\n[9] Ngoc Thang Vu, Yuanfan Wang, Marten Klose, Zlatka\nMihaylova, and Tanja Schultz, “Improving asr per-\nformance on non-native speech using multilingual and\ncrosslingual information,” in INTERSPEECH, 2014.\n[10] Cal Peyser, Hao Zhang, Tara N. Sainath, and Zelin Wu,\n“Improving performance of end-to-end asr on numeric\nsequences,” in INTERSPEECH, 2019.\n[11] Cal Peyser, Tara N. Sainath, and Golan Pundak, “Im-\nproving proper noun recognition in end-to-end asr by\ncustomization of the mwer loss criterion,” in2020 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing, ICASSP 2020, Barcelona, Spain, May\n4-8, 2020. 2020, pp. 7789–7793, IEEE.\n[12] C ¸ aglar G¨ulc ¸ehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Lo¨ıc Barrault, Huei-Chi Lin, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio, “On using monolin-\ngual corpora in neural machine translation,” ArXiv, vol.\nabs/1503.03535, 2015.\n[13] Kannan A, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen,\nand R. Prabhavalkar, “An analysis of incorporating an\nexternal language model into a sequence-to-sequence\nmodel,” in ICASSP. IEEE, 2018.\n[14] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold\nfusion: Training seq2seq models together with language\nmodels,” ArXiv, vol. abs/1503.03535, 2015.\n[15] C. Shan, C. Weng, G. Wang, D. Su, M. Luo, D. Yu, and\nL. Xie, “Component fusion: Learning replaceable lan-\nguage model component for end-to-end speech recogni-\ntion system,” in ICASSP, 2019, pp. 5361–5635.\n[16] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and\nY . Bengio, “End-to-end attention-based large vocabu-\nlary speech recognition,” in ICASSP. IEEE, 2016.\n[17] J. K. Chorowski and N. Jaitly, “Towards better decoding\nand language model integration in sequence to sequence\nmodels,” in InterSpeech, 2017.\n[18] S. Toshniwal, A. Kannan, C. Chiu, Y . Wu, T. N. Sainath,\nand K. Livescu, “A comparison of techniques for\nlanguage model integration in encoder-decoder speech\nrecognition,” 2018 IEEE Spoken Language Technology\nWorkshop (SLT), pp. 369–375, 2018.\n[19] M. Schuster and K. Nakajima, “Japanese and Korean\nvoice search,” in Proc. ICASSP, 2012.\n[20] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates, “Cold fusion: Training seq2seq models\ntogether with language models,” 2017.\n[21] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan,\nT. N. Sainath, and M. Bacchiani, “Generated of large-\nscale simulated utterances in virtual rooms to train\ndeep-neural networks for far-ﬁeld speech recognition in\nGoogle Home,” in Proc. Interspeech, 2017.\n[22] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. We-\ninstein, “Rnn-transducer with stateless prediction net-\nwork,” in ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), 2020, pp. 7049–7053."
}