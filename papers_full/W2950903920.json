{
  "title": "Towards better decoding and language model integration in sequence to sequence models",
  "url": "https://openalex.org/W2950903920",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2753688165",
      "name": "Chorowski, Jan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173166834",
      "name": "Jaitly, Navdeep",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2361821140",
    "https://openalex.org/W2949640717",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W2950948614",
    "https://openalex.org/W2499293415",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2410539690",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2414484917",
    "https://openalex.org/W2952746495",
    "https://openalex.org/W2953022181",
    "https://openalex.org/W2024539680",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2951093852",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962792802",
    "https://openalex.org/W2260756217",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2311132329",
    "https://openalex.org/W1736701665",
    "https://openalex.org/W1993411524",
    "https://openalex.org/W2952288254"
  ],
  "abstract": "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.",
  "full_text": "Towards better decoding and language model integration in sequence to\nsequence models\nJan Chorowski, Navdeep Jaitly\nGoogle Brain\nGoogle Inc.\nMountain View, CA 94043, USA\njan.chorowski@cs.uni.wroc.pl,ndjaitly@google.com\nAbstract\nThe recently proposed Sequence-to-Sequence (seq2seq) frame-\nwork advocates replacing complex data processing pipelines,\nsuch as an entire automatic speech recognition system, with\na single neural network trained in an end-to-end fashion. In\nthis contribution, we analyse an attention-based seq2seq speech\nrecognition system that directly transcribes recordings into char-\nacters. We observe two shortcomings: overconﬁdence in its\npredictions and a tendency to produce incomplete transcriptions\nwhen language models are used. We propose practical solutions\nto both problems achieving competitive speaker independent\nword error rates on the Wall Street Journal dataset: without sepa-\nrate language models we reach 10.6% WER, while together with\na trigram language model, we reach 6.7% WER.\nIndex Terms: attention mechanism, recurrent neural networks,\nLSTM\n1. Introduction\nDeep learning [ 1] has led to many breakthroughs including\nspeech and image recognition [ 2, 3, 4, 5, 6, 7]. A subfamily\nof deep models, the Sequence-to-Sequence (seq2seq) neural net-\nworks have proved to be very successful on complex transduction\ntasks, such as machine translation [8, 9, 10], speech recognition\n[11, 12, 13], and lip-reading [14]. Seq2seq networks can typi-\ncally be decomposed into modules that implement stages of a\ndata processing pipeline: an encoding module that transforms its\ninputs into a hidden representation, a decoding (spelling) module\nwhich emits target sequences and an attention module that com-\nputes a soft alignment between the hidden representation and the\ntargets. Training directly maximizes the probability of observing\ndesired outputs conditioned on the inputs. This discriminative\ntraining mode is fundamentally different from the generative\n\"noisy channel\" formulation used to build classical state-of-the\nart speech recognition systems. As such, it has beneﬁts and\nlimitations that are different from classical ASR systems.\nUnderstanding and preventing limitations speciﬁc to seq2seq\nmodels is crucial for their successful development. Discrimina-\ntive training allows seq2seq models to focus on the most infor-\nmative features. However, it also increases the risk of overﬁtting\nto those few distinguishing characteristics. We have observed\nthat seq2seq models often yield very sharp predictions, and only\na few hypotheses need to be considered to ﬁnd the most likely\ntranscription of a given utterance. However, high conﬁdence\nreduces the diversity of transcripts obtained using beam search.\nDuring typical training the models are conditioned on ground\ntruth transcripts and are scored on one-step ahead predictions.\nBy itself, this training criterion does not ensure that all relevant\nfragments of the input utterance are transcribed. Subsequently,\nmistakes that are introduced during decoding may cause the\nmodel to skip some words and jump to another place in the\nrecording. The problem of incomplete transcripts is especially\napparent when external language models are used.\n2. Model Description\nOur speech recognition system, builds on the recently proposed\nListen, Attend and Spell network [13]. It is an attention-based\nseq2seq model that is able to directly transcribe an audio record-\ning xinto a space-delimited sequence of characters y. Similarly\nto other seq2seq neural networks, it uses an encoder-decoder\narchitecture composed of three parts: a listener module tasked\nwith acoustic modeling, a speller module tasked with emitting\ncharacters and an attention module serving as the intermediary\nbetween the speller and the listener:\nh= Listen(x) (1)\np(y|x) =AttendAndSpell(y,h) (2)\n2.1. The Listener\nThe listener is a multilayer Bi-LSTM network that transforms\na sequence of N frames of acoustic features x1,x2,..., xN\ninto a possibly shorter sequence of hidden activations\nh1,h2,..., hN/k, where kis a time reduction constant [12, 13].\n2.2. The Speller and the Attention Mechanism\nThe speller computes the probability of a sequence of characters\nconditioned on the activations of the listener. The probability is\ncomputed one character at a time, using the chain rule:\np(y|h) =\n∏\ni\np(yi|y<i,h). (3)\nTo emit a character the speller uses the attention mechanism to\nﬁnd a set of relevant activations of the listener αand summa-\nrize them into a context c. The history of previously emitted\ncharacters is encapsulated in a recurrent state s:\nsi = RecurrentStep(yi−1,si−1,ci−1), (4)\nci,αi = Attend(h,si,αi−1), (5)\np(yi|y<i,h) =CharacterDistribution(si,ci). (6)\nWe implement the recurrent step using a single LSTM layer.\nThe attention mechanism is sensitive to the location of frames\nselected during the previous step and employs the convolutional\nﬁlters over the previous attention weights [11]. The output char-\nacter distribution is computed using a SoftMax function.\narXiv:1612.02695v1  [cs.NE]  8 Dec 2016\n2.3. Training Criterion\nOur speech recognizer computes the probability of a character\nconditioned on the partially emitted transcript and the whole\nutterance. It can thus be trained to minimize the cross-entropy\nbetween the ground-truth characters and model predictions. The\ntraining loss over a single utterance is\nloss(y,x) =−\n∑\ni\n∑\nc\nT(yi,c) logp(yi|y<i,x), (7)\nwhere T(yi,c) denotes the target label function. In the baseline\nmodel T(yi,c) is the indicator [yi = c], i.e. its value is 1 for\nthe correct character, and 0 otherwise. When label smoothing is\nused, T encodes a distribution over characters.\n2.4. Decoding: Beam Search\nDecoding new utterances amounts to ﬁnding the character se-\nquence y∗ that is most probable under the distribution computed\nby the network:\ny∗ = arg max\ny\np(y|x) = arg min\ny\n−log p(y|x). (8)\nDue to the recurrent formulation of the speller function,\nthe most probable transcript cannot be found exactly using the\nViterbi algorithm. Instead, approximate search methods are used.\nTypically, best results are obtained using beam search. The\nsearch begins with the set (beam) of hypotheses containing only\nthe empty transcript. At every step, candidate transcripts are\nformed by extending hypothesis in the beam by one character.\nThe candidates are then scored using the model, and a certain\nnumber of top-scoring candidates forms the new beam. The\nmodel indicates that a transcript is considered to be ﬁnished by\nemitting a special EOS (end-of-sequence) token.\n2.5. Language Model Integration\nThe simplest solution to include a separate language model is\nto extend the beam search cost with a language modeling term\n[12, 4, 15]:\ny∗ = arg min\ny\n−log p(y|x)−λlog pLM(y)−γcoverage, (9)\nwhere coverage refers to a term that promotes longer transcripts\ndescribed it in detail in Section 3.3.\nWe have identiﬁed two challenges in adding the language\nmodel. First, due to model overconﬁdence deviations from\nthe best guess of the network drastically changed the term\n−log p(y|x), which made balancing the terms in eq. (9) dif-\nﬁcult. Second, incomplete transcripts were produced unless a\nrecording coverage term was added.\nEquation (9) is a heuristic involving the multiplication of\na conditional and unconditional probabilities of the transcript\ny. We have tried to justify it by adding an intrinsic language\nmodel suppression term log p(y) that would transform p(y|x)\ninto p(x|y) ∝p(y|x)/p(y). We have estimated the language\nmodeling capability of the speller p(y) by replacing the encoded\nspeech with a constant, separately trained, biasing vector. The\nper character perplexity obtained was about 6.5 and we didn’t\nobserve consistent gains from this extension of the beam search\ncriterion.\n3. Solutions to Seq2Seq Failure Modes\nWe have analysed the impact of model conﬁdence by separating\nits effects on model accuracy and beam search effectiveness.\nWe also propose a practical solution to the partial transcriptions\nproblem, relating to the coverage of the input utterance.\n3.1. Impact of Model Overconﬁdence\nModel conﬁdence is promoted by the the cross-entropy training\ncriterion. For the baseline network the training loss (7) is mini-\nmized when the model concentrates all of its output distribution\non the correct ground-truth character. This leads to very peaked\nprobability distributions, effectively preventing the model from\nindicating sensible alternatives to a given character, such as its\nhomophones. Moreover, overconﬁdence can harm learning the\ndeeper layers of the network. The derivative of the loss backprop-\nagated through the SoftMax function to the logit corresponding\nto character cequals [yi = c]−p(yi|y<i,x), which approaches\n0 as the network’s output becomes concentrated on the correct\ncharacter. Therefore whenever the spelling RNN makes a good\nprediction, very little training signal is propagated through the\nattention mechanism to the listener.\nModel overconﬁdence can have two consequences. First,\nnext-step character predictions may have low accuracy due to\noverﬁtting. Second, overconﬁdence may impact the ability of\nbeam search to ﬁnd good solutions and to recover from errors.\nWe ﬁrst investigate the impact of conﬁdence on beam search\nby varying the temperature of the SoftMax function. Without\nretraining the model, we change the character probability distri-\nbution to depend on a temperature hyperparameter T:\np(yi) = exp(li/T)∑\nj exp(lj/T). (10)\nAt increased temperatures the distribution over characters be-\ncomes more uniform. However, the preferences of the model\nare retained and the ordering of tokens from the most to least\nprobable is preserved. Tuning the temperature therefore allows\nto demonstrate the impact of model conﬁdence on beam search,\nwithout affecting the accuracy of next step predictions.\nDecoding results of a baseline model on the WSJ dev93 data\nset are presented in Figure 1. We haven’t used a language model.\nAt high temperatures deletion errors dominated. We didn’t want\nto change the beam search cost and instead constrained the search\nto emit the EOS token only when its probability was within a\nnarrow range from the most probable token. We compare the\ndefault setting (T = 1), with a sharper distribution (T = 0.5)\nand smoother distributions (T ∈{1.3,..., 4}). All strategies\nlead to the same greedy decoding accuracy, because temperature\nchanges do not affect the selection of the most probable character.\nAs temperature increases beam search ﬁnds better solutions,\nhowever care must be taken to prevent truncated transcripts.\n3.2. Label Smoothing Prevents Overconﬁdence\nA elegant solution to model overconﬁdence was problem pro-\nposed for the Inception image recognition architecture [16]. For\nthe purpose of computing the training cost the ground-truth label\ndistribution is smoothed, with some fraction of the probability\nmass assigned to classes other than the correct one. This in turn\nprevents the model from learning to concentrate all probability\nmass on a single token. Additionally, the model receives more\ntraining signal because the error function cannot easily saturate.\nOriginally uniform label smoothing scheme was proposed\nin which the model is trained to assign βprobability mass to he\nFigure 1: Inﬂuence of beam width and SoftMax temperature on decoding accuracy. In the baseline case (no label smoothing) increasing\nthe temperature reduces the error rate. When label smoothing is used the next-character prediction improves, as witnessed by WER for\nbeam size=1, and tuning the temperature does not bring additional beneﬁts.\nTable 1: Example of model failure on validation ’4k0c030n’\nTranscript LM cost Model cost\nlog p(y) log p(y|x)\n\"chase is nigeria’s registrar and the\nsociety is an independent organi-\nzation hired to count votes\"\n-108.5 -34.5\n\"in the society is an independent\norganization hired to count votes\"\n-64.6 -19.9\n\"chase is nigeria’s registrar\" -40.6 -31.2\n\"chase’s nature is register\" -37.8 -20.3\n\"\" -3.5 -12.5\ncorrect label, and spread the 1 −βprobability mass uniformly\nover all classes [16]. Better results can be obtained with unigram\nsmoothing which distributes the remaining probability mass\nproportionally to the marginal probability of classes [ 17]. In\nthis contribution we propose a neighborhood smoothing scheme\nthat uses the temporal structure of the transcripts: the remaining\n1 −βprobability mass is assigned to tokens neighboring in the\ntranscript. Intuitively, this smoothing scheme helps the model to\nrecover from beam search errors: the network is more likely to\nmake mistakes that simply skip a character of the transcript.\nWe have repeated the analysis of SoftMax temperature on\nbeam search accuracy on a network trained with neighborhood\nsmoothing in Figure 1. We can observe two effects. First, the\nmodel is regularized and greedy decoding leads to nearly 3\npercentage smaller error rate. Second, the entropy of network\npredictions is higher, allowing beam search to discover good\nsolutions without the need for temperature control. Moreover,\nthe since model is trained and evaluated with T = 1we didn’t\nhave to control the emission of EOS token.\n3.3. Solutions to Partial Transcripts Problem\nWhen a language model is used wide beam searches often yield\nincomplete transcripts. With narrow beams, the problem is less\nvisible due to implicit hypothesis pruning. We illustrate a failed\ndecoding in Table 1. The ground truth (ﬁrst row) is the least prob-\nable transcript according both to the network and the language\nmodel. A width 100 beam search with a trigram language model\nﬁnds the second transcript, which misses the beginning of the\nutterance. The last rows demonstrate severely incomplete tran-\nscriptions that may be discovered when decoding is performed\nwith even wider beam sizes.\nWe compare three strategies designed to prevent incomplete\ntranscripts. The ﬁrst strategy doesn’t change the beam search\ncriterion, but forbids emitting the EOS token unless its probabil-\nity is within a set range of that of the most probable token. This\nstrategy prevents truncations, but is inefﬁcient against omissions\nin the middle of the transcript, such as the failure shown in Ta-\nble 1. Alternatively, beam search criterion can be extended to\npromote long transcripts. A term depending on the transcript\nlength was proposed for both CTC [ 4] and seq2seq [ 12] net-\nworks, but its usage was reported to be difﬁcult because beam\nsearch was looping over parts of the recording and additional\nconstraints were needed [12]. To prevent looping we propose to\nuse a coverage term that counts the number of frames that have\nreceived a cumulative attention greater than τ:\ncoverage =\n∑\nj\n[∑\ni\nαij >τ\n]\n. (11)\nThe coverage criterion prevents looping over the utterance\nbecause once the cumulative attention bypasses the threshold τ\na frame is counted as selected and subsequent selections of this\nframe do not reduce the decoding cost. In our implementation,\nthe coverage is recomputed at each beam search iteration using\nall attention weights produced up to this step.\nIn Figure 2 we compare the effects of the three methods\nwhen decoding a network that uses label smoothing and a tri-\ngram language model. Unlike [12] we didn’t experience looping\nwhen beam search promoted transcript length. We hypothe-\nsize that label smoothing increases the cost of correct character\nemissions which helps balancing all terms used by beam search.\nWe observe that at large beam widths constraining EOS emis-\nsions is not sufﬁcient. In contrast, both promoting coverage\nand transcript length yield improvements with increasing beams.\nHowever, simply maximizing transcript length yields more word\ninsertion errors and achieves an overall worse WER.\n4. Experiments\nWe conducted all experiments on the Wall Street Journal dataset,\ntraining on si284, validating on dev93 and evaluating on eval92\nFigure 2: Impact of using techniques that prevent incomplete\ntranscripts when a trigram language models is used on the dev93\nWSJ subset. Results are averaged across two networks\nset. The models were trained on 80-dimensional mel-scale ﬁlter-\nbanks extracted every 10ms form 25ms windows, extended with\ntheir temporal ﬁrst and second order differences and per-speaker\nmean and variance normalization. Our character set consisted\nof lowercase letters, the space, the apostrophe, a noise marker,\nand start- and end- of sequence tokens. For comparison with\npreviously published results, experiments involving language\nmodels used an extended-vocabulary trigram language model\nbuilt by the Kaldi WSJ s5 recipe [ 18]. We have use the FST\nframework to compose the language model with a \"spelling\nlexicon\" [6, 12, 19]. All models were implemented using the\nTensorﬂow framework [20].\nOur base conﬁguration implemented the Listener using 4\nbidirectional LSTM layers of 256 units per direction (512 total),\ninterleaved with 3 time-pooling layers which resulted in an 8-fold\nreduction of the input sequence length, approximately equating\nthe length of hidden activations to the number of characters\nin the transcript. The Speller was a single LSTM layer with\n256 units. Input characters were embedded into 30 dimensions.\nThe attention MLP used 128 hidden units, previous attention\nweights were accessed using 3 convolutional ﬁlters spanning\n100 frames. LSTM weights were initialized uniformly over the\nrange ±0.075. Networks were trained using 8 asynchronous\nreplica workers each employing the ADAM algorithm [21] with\ndefault parameters and the learning rate set initially to 10−3,\nthen reduced to 10−4 and 10−5 after 400k and 500k training\nsteps, respectively. Static Gaussian weight noise with standard\ndeviation 0.075 was applied to all weight matrices after 20000\ntraining steps. We have also used a small weight decay of 10−6.\nWe have compared two label smoothing methods: unigram\nsmoothing [17] with the probability of the correct label set to\n0.95 and neighborhood smoothing with the probability of correct\ntoken set to 0.9 and the remaining probability mass distributed\nsymmetrically over neighbors at distance±1 and ±2 with a 5 : 2\nratio. We have tuned the smoothing parameters with a small grid\nsearch and have found that good results can be obtained for a\nbroad range of settings.\nWe have gathered results obtained without language models\nin Table 2. We have used a beam size of 10 and no mechanism to\npromote longer sequences. We report averages of two runs taken\nat the epoch with the lowest validation WER. Label smoothing\nbrings a large error rate reduction, nearly matching the perfor-\nTable 2: Results without separate language model on WSJ.\nModel Parameters dev93 eval92\nCTC [3] 26.5M - 27.3\nseq2seq [12] 5.7M - 18.6\nseq2seq [24] 5.9M - 12.9\nseq2seq [22] - - 10.5\nBaseline 6.6M 17.9 14.2\nUnigram LS 6.6M 13.7 10.6\nTemporal LS 6.6M 14.1 10.7\nTable 3: Results withextended trigram language model on WSJ.\nModel dev93 eval92\nseq2seq [12] - 9.3\nCTC [3] - 8.2\nCTC [6] - 7.3\nBaseline + Cov 12.6 8.9\nUnigram LS + Cov. 9.9 7.0\nTemporal LS + Cov. 9.7 6.7\nmance achieved with very deep and sophisticated encoders [22].\nTable 3 gathers results that use the extended trigram lan-\nguage model. We report averages of two runs. For each run we\nhave tuned beam search parameters on the validation set and\napplied them on the test set. A typical setup used beam width\n200, language model weight λ= 0.5, coverage weight γ = 1.5\nand coverage threshold τ = 0.5. Our best result surpasses CTC-\nbased networks [6] and matches the results of a DNN-HMM and\nCTC ensemble [23].\n5. Related Work\nLabel smoothing was proposed as an efﬁcient regularizer for\nthe Inception architecture [ 16]. Several improved smoothing\nschemes were proposed, including sampling erroneous labels\ninstead of using a ﬁxed distribution [ 25], using the marginal\nlabel probabilities [17], or using early errors of the model [26].\nSmoothing techniques increase the entropy of a model’s pre-\ndictions, a technique that was used to promote exploration in\nreinforcement learning [27, 28, 29]. Label smoothing prevents\nsaturating the SoftMax nonlinearity and results in better gradient\nﬂow to lower layers of the network [16]. A similar concept, in\nwhich training targets were set slightly below the range of the\noutput nonlinearity was proposed in [30].\nOur seq2seq networks are locally normalized, i.e. the speller\nproduces a probability distribution at every step. Alternatively\nnormalization can be performed globally on whole transcripts.\nIn discriminative training of classical ASR systems normaliza-\ntion is performed over lattices [ 31]. In the case of recurrent\nnetworks lattices are replaced by beam search results. Global\nnormalization has yielded important beneﬁts on many NLP tasks\nincluding parsing and translation [32, 33]. Global normalization\nis expensive, because each training step requires running beam\nsearch inference. It remains to be established whether globally\nnormalized models can be approximated by cheaper to train lo-\ncally normalized models with proper regularization such as label\nsmoothing.\nUsing source coverage vectors has been investigated in neu-\nral machine translation models. Past attentions vectors were used\nas auxiliary inputs in the emitting RNN either directly [34], or\nas cumulative coverage information [35]. Coverage embeddings\nvectors associated with source words end modiﬁed during train-\ning were proposed in [36]. Our solution that employs a coverage\npenalty at decode time only is most similar to the one used by\nthe Google Translation system [10].\n6. Conclusions\nWe have demonstrated that with efﬁcient regularization and care-\nful decoding the sequence-to-sequence approach to speech recog-\nnition can be competitive with other non-HMM techniques, such\nas CTC.\n7. Acknowledgements\n8. References\n[1] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,”Nature, vol.\n521, no. 7553, pp. 436–444, 2015.\n[2] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly,\nA. Senior, V . Vanhoucke, P. Nguyen, T. N. Sainathet al., “Deep\nneural networks for acoustic modeling in speech recognition: The\nshared views of four research groups,” IEEE Signal Processing\nMagazine, vol. 29, no. 6, pp. 82–97, 2012.\n[3] A. Graves and N. Jaitly, “Towards end-to-end speech recognition\nwith recurrent neural networks.” inICML, vol. 14, 2014, pp. 1764–\n1772.\n[4] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen,\nR. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., “Deep\nspeech: Scaling up end-to-end speech recognition,” arXiv preprint\narXiv:1412.5567, 2014.\n[5] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper,\nB. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos\net al., “Deep speech 2: End-to-end speech recognition in english\nand mandarin,”arXiv preprint arXiv:1512.02595, 2015.\n[6] Y . Miao, M. Gowayyed, and F. Metze, “Eesen: End-to-end speech\nrecognition using deep rnn models and wfst-based decoding,” in\n2015 IEEE Workshop on Automatic Speech Recognition and Un-\nderstanding (ASRU). IEEE, 2015, pp. 167–174.\n[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁ-\ncation with deep convolutional neural networks,” inAdvances in\nneural information processing systems, 2012, pp. 1097–1105.\n[8] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence\nlearning with neural networks,” inAdvances in neural information\nprocessing systems, 2014, pp. 3104–3112.\n[9] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[10] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws,\nY . Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil,\nW. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals,\nG. Corrado, M. Hughes, and J. Dean, “Google’s neural machine\ntranslation system: Bridging the gap between human and\nmachine translation,”CoRR, vol. abs/1609.08144, 2016. [Online].\nAvailable: http://arxiv.org/abs/1609.08144\n[11] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Bengio,\n“Attention-based models for speech recognition,” inAdvances in\nNeural Information Processing Systems, 2015, pp. 577–585.\n[12] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Bengio,\n“End-to-end attention-based large vocabulary speech recognition,”\nin 2016 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), March 2016, pp. 4945–4949.\n[13] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, “Listen, attend and\nspell,”arXiv preprint arXiv:1508.01211, 2015.\n[14] J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip reading\nsentences in the wild,”arXiv preprint arXiv:1611.05358, 2016.\n[15] Ç. Gülçehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin,\nF. Bougares, H. Schwenk, and Y . Bengio, “On using monolingual\ncorpora in neural machine translation,”CoRR, vol. abs/1503.03535,\n2015. [Online]. Available: http://arxiv.org/abs/1503.03535\n[16] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-\nthinking the inception architecture for computer vision,” arXiv\npreprint arXiv:1512.00567, 2015.\n[17] G. Pereyra, G. Tucker, J. Chorowski, L. Kaiser, and G. Hin-\nton, “Regularizing neural networks by penalizing conﬁdent\noutput distributions,” in Submitted to ICLR 2017 , 2017,\nhttps://openreview.net/forum?id=HkCjNI5ex.\n[18] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recogni-\ntion toolkit,” inIEEE 2011 Workshop on Automatic Speech Recog-\nnition and Understanding. IEEE Signal Processing Society, Dec.\n2011, iEEE Catalog No.: CFP11SRW-USB.\n[19] C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and M. Mohri,\n“OpenFst: A general and efﬁcient weighted ﬁnite-state transducer\nlibrary,” inProceedings of the Ninth International Conference on\nImplementation and Application of Automata, (CIAA 2007) , ser.\nLecture Notes in Computer Science, vol. 4783. Springer, 2007,\npp. 11–23, http://www.openfst.org.\n[20] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorﬂow:\nLarge-scale machine learning on heterogeneous distributed sys-\ntems,”arXiv preprint arXiv:1603.04467, 2016.\n[21] D. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,”arXiv preprint arXiv:1412.6980, 2014.\n[22] Y . Zhang, W. Chan, and N. Jaitly, “Very deep convolutional\nnetworks for end-to-end speech recognition,” arXiv preprint\narXiv:1610.03022, 2016.\n[23] A. Graves and N. Jaitly, “Towards End-To-End Speech Recognition\nwith Recurrent Neural Networks,” inICML ’14, 2014, pp. 1764–\n1772.\n[24] W. Chan, Y . Zhang, Q. Le, and N. Jaitly, “Latent sequence decom-\npositions,”arXiv preprint arXiv:1610.03035, 2016.\n[25] L. Xie, J. Wang, Z. Wei, M. Wang, and Q. Tian, “Disturblabel: Reg-\nularizing cnn on the loss layer,”arXiv preprint arXiv:1605.00055,\n2016.\n[26] A. Aghajanyan, “Softtarget regularization: An effective\ntechnique to reduce over-ﬁtting in neural networks,” CoRR, vol.\nabs/1609.06693, 2016. [Online]. Available: http://arxiv.org/abs/\n1609.06693\n[27] R. J. Williams and J. Peng, “Function optimization using connec-\ntionist reinforcement learning algorithms,” Connection Science,\nvol. 3, no. 3, pp. 241–268, 1991.\n[28] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep\nreinforcement learning,”arXiv preprint arXiv:1602.01783, 2016.\n[29] Y . Luo, C.-C. Chiu, N. Jaitly, and I. Sutskever, “Learning on-\nline alignments with continuous rewards policy gradient,” arXiv\npreprint arXiv:1608.01281, 2016.\n[30] Y . A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller,\nEfﬁcient BackProp . Berlin, Heidelberg: Springer Berlin\nHeidelberg, 2012, pp. 9–48. [Online]. Available: http:\n//dx.doi.org/10.1007/978-3-642-35289-8_3\n[31] X. He, L. Deng, and W. Chou, “Discriminative learning in se-\nquential pattern recognition,”IEEE Signal Processing Magazine,\nvol. 25, no. 5, pp. 14–36, 2008.\n[32] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev,\nS. Petrov, and M. Collins, “Globally normalized transition-based\nneural networks,” CoRR, vol. abs/1603.06042, 2016. [Online].\nAvailable: http://arxiv.org/abs/1603.06042\n[33] S. Wiseman and A. M. Rush, “Sequence-to-sequence learning\nas beam-search optimization,”CoRR, vol. abs/1606.02960, 2016.\n[Online]. Available: http://arxiv.org/abs/1606.02960\n[34] M. Luong, H. Pham, and C. D. Manning, “Effective\napproaches to attention-based neural machine translation,”\nCoRR, vol. abs/1508.04025, 2015. [Online]. Available: http:\n//arxiv.org/abs/1508.04025\n[35] Z. Tu, Z. Lu, Y . Liu, X. Liu, and H. Li, “Modeling coverage for\nneural machine translation,” CoRR, vol. abs/1601.04811, 2016.\n[Online]. Available: http://arxiv.org/abs/1601.04811\n[36] H. Mi, B. Sankaran, Z. Wang, and A. Ittycheriah, “Coverage\nembedding model for neural machine translation,” CoRR, vol.\nabs/1605.03148, 2016. [Online]. Available: http://arxiv.org/abs/\n1605.03148",
  "topic": "Trigram",
  "concepts": [
    {
      "name": "Trigram",
      "score": 0.8459834456443787
    },
    {
      "name": "Computer science",
      "score": 0.7587257623672485
    },
    {
      "name": "Language model",
      "score": 0.7433470487594604
    },
    {
      "name": "Sequence (biology)",
      "score": 0.7065376043319702
    },
    {
      "name": "Decoding methods",
      "score": 0.6718760132789612
    },
    {
      "name": "Speech recognition",
      "score": 0.635180652141571
    },
    {
      "name": "Word (group theory)",
      "score": 0.56787109375
    },
    {
      "name": "Natural language processing",
      "score": 0.5057981014251709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47503378987312317
    },
    {
      "name": "Word error rate",
      "score": 0.4525607228279114
    },
    {
      "name": "Artificial neural network",
      "score": 0.44168806076049805
    },
    {
      "name": "Linguistics",
      "score": 0.22634348273277283
    },
    {
      "name": "Algorithm",
      "score": 0.16603899002075195
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219388962",
      "name": "University of Wrocław",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ],
  "cited_by": 58
}