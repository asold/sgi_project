{
    "title": "Alternating Language Modeling for Cross-Lingual Pre-Training",
    "url": "https://openalex.org/W2996822578",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A1942304213",
            "name": "Jian Yang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2099570760",
            "name": "Shuming Ma",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2131422403",
            "name": "Dongdong Zhang",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2770339140",
            "name": "Shuangzhi Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2133880114",
            "name": "Zhoujun Li",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2096867225",
            "name": "Ming Zhou",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1942304213",
            "name": "Jian Yang",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2133880114",
            "name": "Zhoujun Li",
            "affiliations": [
                "Beihang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2767989436",
        "https://openalex.org/W2896060389",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W6737778391",
        "https://openalex.org/W2890964657",
        "https://openalex.org/W2798812533",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2595715041",
        "https://openalex.org/W6682383452",
        "https://openalex.org/W6745064204",
        "https://openalex.org/W6759455113",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2587528408",
        "https://openalex.org/W2156985047",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W2772136736",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W2889747665",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2946232455",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W6948213869",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2887557046",
        "https://openalex.org/W6731744468",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2572474373",
        "https://openalex.org/W4241645538",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4301187301",
        "https://openalex.org/W3121694563",
        "https://openalex.org/W2963082277",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4288333985"
    ],
    "abstract": "Language model pre-training has achieved success in many natural language processing tasks. Existing methods for cross-lingual pre-training adopt Translation Language Model to predict masked words with the concatenation of the source sentence and its target equivalent. In this work, we introduce a novel cross-lingual pre-training method, called Alternating Language Modeling (ALM). It code-switches sentences of different languages rather than simple concatenation, hoping to capture the rich cross-lingual context of words and phrases. More specifically, we randomly substitute source phrases with target translations to create code-switched sentences. Then, we use these code-switched data to train ALM model to learn to predict words of different languages. We evaluate our pre-training ALM on the downstream tasks of machine translation and cross-lingual classification. Experiments show that ALM can outperform the previous pre-training methods on three benchmarks.1",
    "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nAlternating Language Modeling for Cross-Lingual Pre-Training\nJian Yang,1∗ Shuming Ma,2 Dongdong Zhang,2 ShuangZhi Wu,3∗ Zhoujun Li,1†\nMing Zhou2\n1State Key Lab of Software Development Environment, Beihang University\n2Microsoft Research Asia\n3SPPD of Tencent Inc.\n{jiaya, lizj}@buaa.edu.cn; {shumma, dozhang, mingzhou}@microsoft.com; frostwu@tencent.com\nAbstract\nLanguage model pre-training has achieved success in many\nnatural language processing tasks. Existing methods for\ncross-lingual pre-training adopt Translation Language Model\nto predict masked words with the concatenation of the source\nsentence and its target equivalent. In this work, we intro-\nduce a novel cross-lingual pre-training method, called Al-\nternating Language Modeling (ALM). It code-switches sen-\ntences of different languages rather than simple concatena-\ntion, hoping to capture the rich cross-lingual context of words\nand phrases. More speciﬁcally, we randomly substitute source\nphrases with target translations to create code-switched sen-\ntences. Then, we use these code-switched data to train ALM\nmodel to learn to predict words of different languages. We\nevaluate our pre-training ALM on the downstream tasks of\nmachine translation and cross-lingual classiﬁcation. Exper-\niments show that ALM can outperform the previous pre-\ntraining methods on three benchmarks.\n1\nIntroduction\nRecently language model pre-training methods, including\nELMo (Peters et al. 2018), GPT (Radford et al. 2018),\nGPT2 (Radford et al. 2019), BERT (Devlin et al. 2019),\nand UniLM (Dong et al. 2019), have achieved impres-\nsive results on various natural language processing tasks\nsuch as question-answering (Min, Seo, and Hajishirzi 2017;\nYang et al. 2019a), machine reading comprehension (Salant\nand Berant 2018; Y u et al. 2018) and natural language infer-\nence (Tay, Luu, and Hui 2018). More recently, XLM (Lam-\nple and Conneau 2019) has extended this approach to cross-\nlingual pre-training, and proven successful in applying lan-\nguage model pre-training in the cross-lingual setting.\nExisting methods for supervised cross-lingual pre-\ntraining adopt a cross-lingual language model objective,\ncalled Translation Language Model (TLM). It makes use of\nparallel data by predicting the masked words with concate-\nnation of the sentence and its translation. In this way, the\n∗Contribution during internship at Microsoft Research Asia.\n†Corresponding author.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Code can be found at https://github.com/zddfunseeker/ALM.\n⍻⌀ 㒯 䖃 ↲ⳤ 坋↧\n澜濕澝澔濌激濁\n濖濴濿濿瀆澳濹瀂瀅澳濹瀅濸瀆濻澳濼瀁濷瀈瀆瀇瀅濼濴濿澳濴濶瀇濼瀂瀁\n濴濶瀇濼瀂瀁濖濴濿濿瀆 濹瀂瀅\n澜濖澝澔澵激濁\n澷濜濝濢濙濧濙澔濨濣濟濙濢\n澹濢濛濠濝濧濜澔濨濣濟濙濢\n㒯䖃 ↲ⳤ\nFigure 1: Example of Translation Language Model and Al-\nternating Language Model.\ncross-lingual pre-training model can learn the relationship\nbetween languages.\nIn this work, we propose a novel cross-lingual language\nmodel, which alternately predicts words of different lan-\nguages. Figure 1 shows an example of the proposed Alter-\nnating Language Model (ALM). Different from XLM, the\ninput sequence of ALM is mixed with different languages,\nso it can capture the rich cross-lingual context of words and\nphrases. Moreover, it forces the language model to predict\none language conditioned on the context of the other lan-\nguage. Therefore, it can minor the gap between the embed-\ndings of the source language and the target languages, which\nis beneﬁcial for the cross-lingual setting.\nBased on Alternating Language Model, we introduce a\nnew cross-lingual pre-training method. More speciﬁcally,\nwe take the Transformer model (V aswani et al. 2017) as\nthe backbone model. Then, we construct the training ex-\namples for pre-training by replacing the phrases with their\ntranslation of the other language. Finally, we pre-train the\nTransformer model with the constructed examples using the\nmasked language model objective. The pre-trained model\ncan be used to further ﬁne-tune the downstream cross-\nlingual tasks.\nTo verify the effectiveness of the proposed method, we\nevaluate our pre-training method on machine translation and\ncross-lingual classiﬁcation. Experiments show that ALM\ncan outperform the previous pre-training methods on three\nbenchmark datasets.\nThe contributions of this work are as follows:\n9386\n⨳ ⴁ⍻઼ ᣕ䆖ছᱏ㌫㔏 global monitor and warning satellite system\nsatellite system⨳ ⴁ⍻ ઼\nSource Target\nᣕ䆖\n⨳ ⴁ⍻ and ᣕ䆖 ছᱏ㌫㔏\nglobal monitor and warning ছᱏ㌫㔏\nglobal ⴁ⍻ and ᣕ䆖 satellite system\n…\nSample 1\nSample 2\nSample n-1\nSample n\nMasked Transformer Encoder\nଵ --ଷସହݕ଻ݕ଼\nଶ ଺\nݔଵݔଷݔସݔହݔ଻\nݕଵݕଷݕସݕହݕ଻ݕ଼\nݔ଺\nݕଵݕ଺\nݔଶSource\nTarget\nFigure 2: Overview of our ALM cross-lingual pre-training method. Given a pair of bilingual sentences, we yield a set of cross-\nlingual sentences. These sentences are used to pre-train the Transformer encoder which predicts an English masked word or a\nChinese one.\n• We propose a novel cross-lingual language model, which\nalternately predicts words of different languages.\n• We introduce a new cross-lingual pre-training method\nbased on the proposed cross-lingual language model,\nwhich can be further ﬁne-tuned on downstream tasks.\n• Experiments show that ALM outperforms the previous\npre-training methods on the benchmark datasets for ma-\nchine translation and cross-lingual text classiﬁcation.\nCross-Lingual Pre-Training\nCross-lingual pre-training trains a model that can be fur-\nther ﬁne-tuned to improve downstream tasks by making use\nof monolingual data and bilingual data. XLM is a recently\nproposed model that achieves success in cross-lingual pre-\ntraining. It consists of two unsupervised models that relies\non monolingual data, and a supervised model that relies on\nbilingual data. These three models of XLM are Causal Lan-\nguage Model (CLM), Masked Language Model (MLM), and\nTranslation Language Model (TLM), respectively.\nUnsupervised Language Modeling\nCLM recurrently predicts the next word given the previous\ncontext, which is the typical objective of language modeling.\nGPT (Radford et al. 2018) is the ﬁrst pre-training model to\nadopt CLM, and GPT-2 (Radford et al. 2019) further proves\nthe success of CLM for pre-training.\nCLM only makes use of the uni-directional context. Dif-\nferent from CLM, MLM uses bidirectional contextual infor-\nmation. It randomly masks some tokens during training and\npredicts the identity of the masked word. BERT (Devlin et\nal. 2019) is the ﬁrst to propose this model and use it for pre-\ntraining. Different from the BERT, XLM (Lample and Con-\nneau 2019) uses an arbitrary number of sentences (truncated\nat 256 tokens) instead of pairs of sentences, and it samples\nthe masked tokens according to a multinomial distribution,\nwhose weights are proportional to the square root of their\ninvert frequencies.\nSupervised Language Modeling\nXLM also proposes an additional objective that can make\nuse of bilingual data called TLM. TLM concatenates paral-\nlel sentences as training samples. Similar to MLM, it ran-\ndomly masks words of concatenated sentences, so that it can\nleverage both words in source language and target language\ntranslation by predicting the masked words. Moreover, TLM\nleverages target sentences to predict source words when the\nsource context is insufﬁcient to predict these words.\nTLM makes use of bilingual data by concatenating sen-\ntences of two languages, so it can learn the relationship be-\ntween languages. In this work, we mainly focus on improv-\ning the supervised pre-training model. We also show that the\nproposed model can be applied to unsupervised settings in\nthe following section.\nAlternating Language Model\nWe propose Alternating Language Model (ALM) to alter-\nnately predict words of different languages. In this section,\nwe present the details of ALM.\nCode-Switched Sequence\nGiven a bilingual sentence pair (X,Y ) with the source\nsentence X = {x1,x2,...,x N } and the target translation\nY = {y1,y2,...,y M }, where N and M are the lengths of\nthe source and target sentences, we create the code-switched\nsequence U by composing the phrases ofX and Y, where\nU={u\n1,u2,..,u L}with the lengthL.\nIn details, for each phrase U[i,j], it comes from either\nsource phrase X[a,b] or target phrase Y[c,d] where the con-\nstraint is that these two phrases are the linguistic translation\ncounterpart in the parallel sentence (X, Y), 1 ≤ a ≤ b ≤ N\nand 1 ≤ c ≤ d ≤ M. We denote the proportion of the source\nwords in the alternating language sequenceU as α.\nSpeciﬁcally, the constituent ofU can be illustrated into\nfour categories:\n• Monolingual source language: that isα =0 .\n• Monolingual target language: that isα =1 .\n9387\nMasked Transformer Encoder\nଵ --ଷସହ଻଼\nଶ ଺\nMasked Transformer Encoder\nଵ --ଷସହ଻଼\nଶ଺\nൌͲ\nൌͳ\nFigure 3: The model architecture of ALM whenα = 0 and\nα =1 .\n• Major source language: that means most ofU is derived\nfrom X where some source phrasesX[a,b] are substituted\nby their target counterpart phrasesY[c,d] (α ≥ 0.5).\n• Major target language: that means most ofU is derived\nfrom Y where some target phrasesY[c,d] are substituted\nby their source counterpart phrasesX[a,b] (α< 0.5).\nConstructing Training Samples\nSince there are few natural code-switched sentences, we\nshould construct them from bilingual sentence pairs. First,\nwe perform word alignment with the GIZA toolkit (Och\nand Ney 2003) between the parallel sentence X and Y,\nand extract a bilingual phrase table using statistical ma-\nchine translation techniques (Koehn, Och, and Marcu 2003).\nThen, for each sentence pair in training corpus, we cre-\nate the major-source-language samples by substituting some\nphrases in source sentence with the corresponding target\nphrases with highest probabilities in phrase table. A similar\nmethod creates major-target-language samples by substitut-\ning some phrases in target sentence with the corresponding\nsource phrases.\nThe details of the construction for a sentence pair are:\n• Each phrase is limited to less than 5 words for both source\nlanguage and target language.\n• The substituted words are less than 30% of the total words\nin the sentence. Therefore, the source words dominate the\nsentence in the major source language, while the target\nwords dominate the sentence in the major target language.\n• Each bilingual sentence pair is used to create multiple al-\nternating language sentences by randomly choosing the\nsubstituted phrases.\nFigure 2 shows an example of constructing code-switched\nsentences. Given the Chinese sentence and its translation,\nmultiple training samples can be derived from one sentence\npair by choosing different phrases to substitute.\nModel Architecture and Pre-Training\nFigure 2 also shows the overall architecture of our proposed\nmodel. Given a parallel sentence pair, we combine two sen-\ntences from different languages into a single code-switched\nsequence as described above. Then we mask out a certain\npercentage of words in the sequences. We feed the masked\nsentences into Transformer model to learn to predict the\nwords being masked out.\nIn details, we sample randomly 15% of the tokens, replace\nthem by a [MASK] token 80% of the time, by a random\ntoken 10% of the time, and keep them unchanged 10% of\nthe time.\nFigure 3 shows two special cases of ALM. Whenα =0 ,\nthe input sequence is purely from source language. It be-\ncomes the masked language model for source language.\nWhen α =1 , the input sequence is purely from target lan-\nguage, so it becomes the masked language model for target\nlanguage. In this way, the model becomes unsupervised be-\ncause it only relies on monolingual data.\nIn practice, we have 10% of training samples withα =0 ,\n10% of samples withα =1 , and the rest with0 <α< 1.\nWe manually choose a proper value ofαwhich ensures some\nphrases are replaced with their counterparts by alignment\ninstead of sweeping all values ofα (0 ≤ α ≤ 1). In order\nto ensure the value ofαis in a reasonable range, we set max\nlength and max number for phrase substitution.\nApplying to Downstream Tasks\nAfter pre-training, we further ﬁne-tune ALM in order to\nadapt the parameters for the downstream tasks, which are\nmachine translation and cross-lingual classiﬁcation.\nMachine Translation After pre-training, we use ALM\nas the encoder of machine translation, and construct a\nTransformer-based decoder conditioned on ALM. We ﬁne-\ntune the parameters of the total encoder-decoder model on\nparallel training dataset of machine translation.\nCross-Lingual Classiﬁcation XNLI (Conneau et al.\n2018) is a signiﬁcant dataset which is similar to the English\nMultiNLI including several languages. Taking the task of\nNLI as an example, we concatenate premise and hypothe-\nsis as input, and feed them into ALM. On top of ALM, we\nadd a linear classiﬁer and a dropout layer after the ﬁrst hid-\nden state for last layer. Then, we ﬁne-tune the parameters of\nALM on training dataset of cross-lingual classiﬁcation.\nExperiments\nWe evaluate our proposed method on machine translation\nand cross-lingual text classiﬁcation. In this section, we pro-\nvide the details, results, and analysis of the experiments.\nDatasets\nFollowing previous work (Lample and Conneau 2019), we\nuse Wikipedia data by using WikiExtractor and WMT data\nas monolingual data. For bilingual data, French, Spanish,\n9388\nRussian, Arabic, and Chinese data are from MultiUN (Ziem-\nski, Junczys-Dowmunt, and Pouliquen 2016). Hindi data is\nfrom the IIT Bombay corpus (Kunchukuttan, Mehta, and\nBhattacharyya 2018). German and Greek are from the EU-\nbookshop corpus. Turkish, Vietnamese and Thai are from\nOpenSubtitles 2018. Urdu and Swahili data are from Tanzil.\nSwahili data is from GlobalV oices. For most languages, we\nuse the tokenizer provided by Moses (Koehn et al. 2007).\nPre-Training Details\nWe use byte pair encoding (BPE) (Sennrich, Haddow, and\nBirch 2016). The vocabulary contains 95K byte pair encod-\ning tokens. We pre-train our model with both 1024 embed-\nding and hidden units, 8 heads, a dropout rate of 0.1 and\nlearned positional embeddings. We use an Adam optimizer\nwith parameters ofβ\n1 =0 .9 and β2 =0 .98. We set the in-\nverse sqrt learning rate schedule with a linear warmup where\nthe number of warmup step is 4000 and a learning rate of\n0.0005.\nFor pre-training data, we use source language monolin-\ngual data (α =0 ) and target language monolingual data\n(α =1 ). Besides, we also split parallel data to expand mono-\nlingual data. For the monolingual data, we regard source\nlanguage mono-lingual data asα =0 and target language\nmono-lingual data as α =1 , which could be classiﬁed\ninto a special situation of ALM. To construct monolingual\ndataset, we use Wikipedia data as monolingual data by us-\ning WikiExtractor. Our pre-training samples includes mono-\nlingual data and parallel data, we use original parallel data\nto generate 20 times code-switched sentences than original\nparallel data. More speciﬁcally, we separately obtain the al-\nternating language sentences of source language and target\nlanguage, which are 40 times than original parallel data in\ntotal. Considering that there exist some bad cases in alter-\nnating language sentences, we ﬁlter some low-quality code-\nswitched sentences of which length is too long or too short,\nand randomly drop some sentences. At last, nearly 1.5 bil-\nlion code-switched sentences are used for pre-training.\nFine-Tuning on Machine Translation\nWe ﬁne-tune the pre-trained ALM on two datasets: WMT14\nEnglish-German machine translation and IWSLT14\nGerman-English machine translation. WMT14 English-\nGerman machine translation dataset has 4.5 million sentence\npairs for training. newsdev2014 is used as the validation\nset, while the newstest2014 is the testing set. IWSLT14\nGerman-English machine translation dataset contains 160\nthousand sentence pairs. They are collected from TED talks.\nWe use iwslt14 devset as the validation set and the iwslt14\ntestset as the testing set.\nWe build a Transformer decoder conditioned on ALM en-\ncoder. We feed the source language into ALM, and generate\nthe target language with decoder. We reload the parameters\nof word embedding and encoder parameters which are also\nused to initialize the decoder for our in-house NMT code\nfrom pre-trained model. We evaluate the performance of the\ntranslated sentences. The evaluation metric is BLEU (Pap-\nineni et al. 2002).\nBaselines We compare our methods with state-of-the-art\nsupervised methods and the pre-training methods, which are\ndescribed as follows:\n• Transformer (V aswani et al. 2017): We implement\nTransformer model with our in-house tensorﬂow code,\nand the experimental settings are the same as Transformer\n(V aswani et al. 2017)\n• ConvS2S (Gehring et al. 2017): We report the results re-\nferring to the paper of convolutional sequence to sequence\nmodel(ConvS2S).\n• Weighted Transformer (Ahmed, Keskar, and Socher\n2017): It uses self-attention branches in place of multi-\nhead attention. The branches replace multiple heads in at-\ntention mechanism of the original Transformer network.\n• Layer-wise Transformer (He et al. 2018): It explicitly\ncoordinates the learning of hidden representations of the\nencoder and decoder, gradually from low level to high\nlevel.\n• RNMT+ (Chen et al. 2018): It combines the advantages\nof both the recurrent structure and Transformer architec-\nture.\n• LightConv and DynamicConv(Wu et al. 2019): Light-\nConv uses a lightweight convolution which can perform\ncompetitively to the best reported self-attention results.\nFurthermore, they introduce dynamic convolutions (Dy-\nnamicConv) which are simpler and more efﬁcient than\nself-attention.\n• Multilingual BERT (Devlin et al. 2019): Multilingual\nBERT (mBERT) extends the BERT model to different\nlanguages. We download the pre-trained model provided\nby the authors, and ﬁne-tune on the machine translation\ndatasets.\n• XLM (Lample and Conneau 2019): We use the released\ncode\n2 and the pre-trained data provided by XLM, and fur-\nther ﬁne-tune the pre-trained model on the corresponding\ndata.\n• MASS (Song et al. 2019): We conduct experiments with\nthe codes provided by the authors. We set the fragment\nlength k as 50% of the total number of masked tokens in\nthe sentence.\nDetails We ﬁne-tune our ALM with the Adam optimizer\n(Kingma and Ba 2015) with a linear warmup (V aswani et al.\n2017). We tune the learning rates based on the performance\non the validation set, and the learning rates are5 ×10\n−4 for\nIWSLT14 German-English and10−3 for WMT14 English-\nGerman. We use the averaged perplexity over all languages\nas a criterion for early stopping. The batch size is set to\n8192 tokens for all experiments. During decoding, we set\nthe beam size to 8.\nResults To prove the effectiveness of ALM, we perform\nexperiments on the English-German and German-English\n2https://github.com/facebookresearch/XLM\n9389\nEn → De BLEU(%)\nTransformer (V aswani et al. 2017) 28.40\nConvS2S (Gehring et al. 2017) 25.16\nWeighted Transformer (Ahmed, Keskar, and Socher 2017) 28.90\nLayer-wise Transformer (He et al. 2018) 29.01\nRNMT+ (Chen et al. 2018) 28.50\nmBERT (Devlin et al. 2019) 28.64\nMASS (Song et al. 2019) 28.92\nXLM (Lample and Conneau 2019) 28.88\nALM (this work) 29.22\nTable 1: Results on WMT14 English-German machine\ntranslation task.\nDe → En BLEU(%)\nTransformer (V aswani et al. 2017) 34.49\nLightConv (Wu et al. 2019) 34.80\nDynamicConv (Wu et al. 2019) 35.20\nAdvsoft (Wang, Gong, and Liu 2019) 35.18\nLayer-wise Transformer (He et al. 2018) 35.07\nmBERT (Devlin et al. 2019) 34.82\nMASS (Song et al. 2019) 35.14\nXLM (Lample and Conneau 2019) 35.22\nALM (this work) 35.53\nTable 2: Results on IWSLT14 German-English machine\ntranslation task.\ntranslation tasks. Table 1 and Table 2 show that our ALM\nhas signiﬁcant improvements over baselines without pre-\ntraining or with pre-training methods.\nIn Table 1, we report the performance of ALM and the\nbaseline models in the WMT14 English-German machine\ntranslation dataset. Transformer is an important baseline,\nand it obtains 28.40 in BLEU score. We also compare ALM\nwith the convolutional baseline ConvS2S, which achieves\n25.16. Weighted Transformer and Layer-wise Transformer\nare two methods to improve the Transformer model, and\nthey get 28.90 and 29.01 in terms of BLEU score. RNMT+\ncombines the recurrent structure and the multi-head atten-\ntion components, which yields an improvement to 28.50\nBLEU score. Our ALM signiﬁcantly outperforms these\nbaseline models. We also compare our model with three\nstate-of-the-art pre-training models. mBERT and MASS\nare unsupervised pre-training models. They achieve 28.64\nBLEU score and 28.92 BLEU score, respectively. XLM is\na mixture of unsupervised and supervised pre-training mod-\nels, achieving 28.88 BLEU score. Our ALM reaches 29.22\nBLEU score, yielding an improvement of +0.58, +0.30, and\n+0.34 BLEU scores.\nIn Table 2, we report the performance of ALM and\nthe baseline models in IWSLT14 German-English machine\ntranslation dataset. We ﬁrst compare our ALM with the su-\npervised models without pre-training. Transformer and its\nvariant Layer-wise Transformer achieves 34.49 and 35.07\nin terms of BLEU score. The convolution-based models,\nLightConv and DynamicConv, achieve 34.80 and 35.20, re-\nspectively. Advsoft gets a BLEU score of 35.18. ALM out-\nperforms these baselines, achieving 35.53 in BLEU score.\nWe also compare ALM with three pre-training baselines.\nIt shows that our ALM obtains the best performance and\nreaches 35.53 BLEU score in this task, outperforming the\nprevious baseline mBERT, MASS, and XLM by +0.71 and\n+0.39, and +0.31 in terms of BLEU score.\nIn general, our ALM could achieve signiﬁcant improve-\nments over all baseline models on two translation tasks. As\nour method pre-trains the encoder on a large scale cross-\nlingual corpus, the word representations and encoder could\nacquire sufﬁcient cross-lingual information. For example,\nthe target phrase can see both its source and target context.\nThis cross-lingual context is helpful for target word genera-\ntion and understanding the source sentence in a cross-lingual\nway.\nFine-Tuning on Cross-Lingual Classiﬁcation\nWe ﬁne-tune the pre-trained ALM model on XNLI dataset\nto evaluate the effectiveness of our model. We build a lin-\near classiﬁer on the top of the pre-trained ALM to project\nthe ﬁrst hidden state of ALM output into the probabili-\nties of each class. We concatenate premise and hypothe-\nsis, and feed them into ALM. We evaluate the performance\nof the ﬁne-tuned model in 15 XNLI languages. Follow-\ning previous work (Lample and Conneau 2019), we eval-\nuate the model in three different settings: “TRANSLA TE-\nTRAIN”, “TRANSLA TE-TEST”, and “CROSS-LINGUAL\nTEST”. The evaluation metric is the accuracy of the pre-\ndicted NLI class.\nBaselines We compare our methods with three strong\nbaselines, including a supervised method without pre-\ntraining, and two pre-training methods:\n• Conneau: Conneau (Conneau et al. 2018) proposes a\nBiLSTM model to set up a baseline for XNLI. We report\nthe scores directly from their paper.\n• Multilingual BERT (Devlin et al. 2019): Multilingual\nBERT (mBERT) extends the BERT model to different\nlanguages, which is also a strong baseline.\n• XLM (Lample and Conneau 2019): XLM is the state-of-\nthe-art model for cross-lingual pre-training. We report the\nresults of XLM directly from their paper.\nDetails We ﬁne-tune our ALM with the Adam optimizer\n(Kingma and Ba 2015) with β\n1 =0 .9 and β2 =0 .997.\nWe tune the learning rates based on the performance on the\nvalidation set, and the learning rates are set to5 ×10\n−6.W e\nset the batch size to 24, and we limit the sentences up to\n256 tokens. We set a rate of dropout 0.15 of last layer. We\nevaluate our model for every 1000 sentences.\nResults Table 3 shows the experimental results of our\nproposed ALM and the baseline models. Following the\nwork of XNLI (Conneau et al. 2018), we evaluate these\nmodels in three different settings: “TRANSLA TE-TRAIN”,\n“TRANSLA TE-TEST”, and “CROSS-LINGUAL TEST”.\nIn the setting “TRANSLA TE-TRAIN”, we translate the\ntraining set of the English MultiNLI dataset into each XNLI\n9390\nen fr es de el bg ru tr ar vi th zh hi sw ur avg.\nMachine translation baselines (TRANSLA TE-TRAIN)\nConneau (Conneau et al. 2018) 73.7 68.3 68.8 66.5 66.4 67.4 66.5 64.5 65.8 66.0 62.8 67.0 62.1 58.2 56.6 65.4\nmBERT (Devlin et al. 2019) 81.9 - 77.8 75.9 ---- 70.7 - - 76.6 - - 61.6 -\nXLM (Lample and Conneau 2019) 85.0 80.2 80.8 80.3 78.1 79.3 78.1 74.7 76.5 76.6 75.5 78.6 72.3 70.9 63.2 76.7\nALM (this work) 85.2 81.1 82.0 82.3 78.3 79.8 78.4 74.9 76.7 76.8 75.6 78.7 72.5 71.5 63.4 77.2\nMachine translation baselines (TRANSLA TE-TEST)\nConneau (Conneau et al. 2018) 73.7 70.4 70.7 68.7 69.1 70.4 67.8 66.3 66.8 66.5 64.4 68.3 64.2 61.8 59.3 67.2\nmBERT (Devlin et al. 2019) 81.4 - 74.9 74.4 ---- 70.4 - - 70.1 - - 62.1 -\nXLM (Lample and Conneau 2019) 85.0 79.0 79.5 78.1 77.8 77.6 75.5 73.7 73.7 70.8 70.4 73.6 69.0 64.7 65.1 74.2\nALM (this work) 85.2 79.1 80.0 78.4 78.0 77.8 77.1 73.9 74.2 71.2 70.5 73.8 69.2 64.8 65.3 74.6\nEvaluation of cross-lingual sentence encoders (CROSS-LINGUAL TEST)\nConneau (Conneau et al. 2018) 73.7 67.7 68.7 67.7 68.9 67.9 65.4 64.2 64.8 66.4 64.1 65.8 64.1 55.7 58.4 65.6\nmBERT (Devlin et al. 2019) 81.4 - 74.3 70.5 ---- 62.1 - - 63.8 - - 58.3 -\nXLM (Lample and Conneau 2019) 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1\nALM (this work) 85.2 79.3 79.2 78.0 76.7 78.1 76.5 73.0 73.2 76.4 73.5 78.6 69.8 69.0 66.8 75.6\nTable 3: Cross-lingual natural language inference (XNLI) test accuracy for the 15 languages.\nlanguages (except English), and ﬁne-tune the models on the\ntranslated training set. In the setting “TRANSLA TE-TEST”,\nwe translate the testing set of each XNLI language to En-\nglish, and evaluate the performance of the models in each\ntranslated testing set. In the setting “CROSS-LINGUAL\nTEST”, we ﬁne-tune the models on the English XNLI train-\ning set, and evaluate the performance directly in each testing\nset. We compare our model with Conneau’s baseline model,\nmBERT, and XLM in these three settings.\nIn the “CROSS-LINGUAL TEST” setting, our ALM sig-\nniﬁcantly outperforms the baseline models. More precisely,\nALM obtains 75.6% accuracy on average, while Conneau’s\nbaseline achieves 65.6% accuracy, and XLM gets 75.1%. On\nthe Russian and Turkish languages, we outperform the base-\nlines by 1.2% and 0.5% respectively. ALM gets 85.2% accu-\nracy in English testing set, outperforming Conneau’s base-\nline model by 11.5%, BERT by 3.8%, and XLM by 0.2% in\nterms of accuracy.\nIn the “TRANSLA TE-TRAIN” setting, our ALM reaches\n77.2% accuracy in average across different languages, which\nindicates that ALM can be ﬁne-tuned for any languages to\nachieve good performance. On the German and French lan-\nguages, we outperform the baselines by 2.0% and 1.9% re-\nspectively. Besides, our ALM achieves higher accuracy than\nXLM in 15 languages.\nIn the “TRANSLA TE-TEST” setting, our ALM obtains\n74.6% average accuracy, while Conneau’s baseline achieves\n67.2% accuracy, and XLM gets 74.2%. In general, our ALM\ncan outperform these three baselines across different exper-\niment settings.\nDiscussions and Analysis\nWe further analyze the advantages of our pre-trained model.\nWe visualize the distribution of our model’s word embed-\nding, and compare it with that of Transformer baseline\nmodel. We evaluate the performance of our ALM given dif-\nferent parallel data, in order to analyze the beneﬁts of pre-\ntraining in the low-resource setting.\nWord Embedding Distribution Figure 4 shows the\nword embedding distributions of Transformer (without pre-\ntraining) and ALM (with pre-training). We project the\nlearned word embeddings from high dimension to 2 dimen-\nsion with the PCA method. We plot both the Chinese word\nembeddings and the English word embeddings in the same\nspace. The hollow cycles denote Chinese words, while the\nsolid cycles denote English words.\nAs for the Transformer baseline, the distribution of the\nChinese word embeddings is very different from that of the\nEnglish word embeddings. We draw a dashed line to illus-\ntrate the separation of the Chinese word embeddings and the\nEnglish word embeddings.\nAs for the pre-trained ALM, the distribution of Chinese\nword embeddings is similar to that of the English word em-\nbeddings. The reason is that we mix Chinese words and\nEnglish words during training, so the embeddings of both\nsource language and target language can distribute in the\nsame space.\nAccording to Figure 4, it also indicates that the source\nwords and its translated target words have closer distance\nthan that of the Transformer baseline model. There are some\ncases which are very close to each other in ALM’s embed-\nding space but far from each other in the Transformer’s em-\nbedding space.\nIt concludes that ALM pre-training method can minor the\ngap between the embeddings of source language and target\nlanguage, which is beneﬁcial for the cross-lingual setting.\nLow Resource Setting We would like to further analyze\nthe performance of our pre-trained ALM given different\nsizes of parallel data. Therefore, we randomly shufﬂe the\nfull parallel training set in the task of IWSLT14 German-\nto-English translation dataset. Then, we extract the random\nK% samples as the ﬁne-tuned parallel data. We setK =\n{10%,20%,30%,40%,50%,60%,70%,80%,90%,\n100%}, and compare our ALM with Transformer baseline\nmodel. We randomly extract speciﬁc data from the whole\nsentence pairs. Figure 5 shows the BLEU scores of our mod-\nels and the baseline. When the parallel data size is small,\n9391\nᷗᷭ\nthey\n壂\nview\n⹔\nwhen\nChinese token\nEnglish token\n(a) Transformer\nwhen\n⹔\nᷗᷭ\ntheyview\n壂\nChinese token\nEnglish token\n(b) ALM\nFigure 4: Visualization of word embedding in Transformer\nand ALM.\nALM can outperform Transformer model by a large margin.\nWith the increase of parallel data, the margin gets narrow\nbecause of the upper bound of the model capacity. It con-\ncludes that ALM pre-training can beneﬁt the performance\nof Transformer model especially when the training samples\nare not sufﬁcient.\nRelated Work\nPre-training and transfer learning are widely used in many\ntasks of natural language processing. ELMo (Peters et al.\n2018) is proposed as a kind of deep contextualized word\nrepresentation that is pre-trained in the large scale corpus\nand can be transferred to other tasks. Universal Language\nModel Fine-tuning (ULMFiT) (Howard and Ruder 2018)\nis an effective transfer learning method that can be ap-\nplied to any task in NLP , and includes techniques that are\nkey for ﬁne-tuning a language model. BERT (Devlin et al.\n2019) achieves state-of-the-art performance among various\npre-training approaches to monolingual NLP tasks. Further-\nmore, XLM and MASS (Song et al. 2019) obtain more\ngreat success in language understanding by pre-training. Un-\nlike BERT that pre-trains only the encoder or the decoder,\nMASS is carefully designed to pre-train the encoder and de-\ncoder jointly by predicting the fragment of the sentence that\nis masked on the encoder side and predict the masked to-\nkens in the decoder side. By masking the input tokens of\n10 20 30 40 50 60 70 80 90 100\nRatio(%)\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0BLEU\n22.6423.05\n23.76\n25.22\n26.12\n27.52\n28.56\n30.11\n33.58\n35.53\n18.78\n19.82\n20.78\n21.62\n23.42\n25.56\n27.12\n29.34\n32.23\n34.49\nALM\nTransformer\nFigure 5: Results of ALM vs Transformer ﬁne-tuning on\nlow-resource data.\nthe decoder that are unmasked in the source side, MASS\ncan force the decoder to rely more on the source repre-\nsentation other than the previous tokens in the target side\nfor the next token prediction by pre-training with monolin-\ngual data. More recently, XLNet (Yang et al. 2019b) pro-\nposes a generalized auto-aggressive pre-training method that\nenables learning bidirectional contexts by maximizing the\nexpected likelihood over all permutations of the factoriza-\ntion order. RoBERTa (Liu et al. 2019) presents a replication\nstudy of BERT pre-training that carefully measures the im-\npact of many key hyperparameters and training data size.\nConclusions\nIn this work, we propose a novel cross-lingual pre-training\nmethod, called Alternating Language Modeling (ALM).\nFirst, we randomly substitute the source phrases with the tar-\nget equivalents to create code-switched sentences. Then, we\nuse these code-switched data to train ALM model to learn to\npredict words of different languages. We evaluate our pre-\ntraining ALM on the downstreams tasks of machine transla-\ntion and cross-lingual classiﬁcation. Experiments show that\nALM can outperform the previous pre-training methods on\nthree benchmark datasets. In the future work, we will ex-\nplore the effect of code-switched sentences being used for\nMASS-like pre-training method.\nAcknowledgments\nThis work is supported by the National Natural Science\nFoundation of China (Grand Nos. U1636211, 61672081,\n61370126), Beijing Advanced Innovation Center for Imag-\ning Technology (No.BAICIT-2016001) and the Fund of the\nState Key Laboratory of Software Development Environ-\nment (No.SKLSDE-2019ZX-17).\nReferences\nAhmed, K.; Keskar, N. S.; and Socher, R. 2017. Weighted\ntransformer network for machine translation. CoRR\nabs/1711.02132.\n9392\nChen, M. X.; Firat, O.; Bapna, A.; Johnson, M.; Macherey,\nW.; Foster, G.; Jones, L.; Schuster, M.; Shazeer, N.; Parmar,\nN.; V aswani, A.; Uszkoreit, J.; Kaiser, L.; Chen, Z.; Wu, Y .;\nand Hughes, M. 2018. The best of both worlds: Combining\nrecent advances in neural machine translation. InACL 2018,\n76–86.\nConneau, A.; Rinott, R.; Lample, G.; Williams, A.; Bow-\nman, S. R.; Schwenk, H.; and Stoyanov, V . 2018.\nXNLI: evaluating cross-lingual sentence representations. In\nEMNLP 2018, 2475–2485.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: pre-training of deep bidirectional transformers for\nlanguage understanding. InNAACL 2019, 4171–4186.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H. 2019. Uniﬁed language\nmodel pre-training for natural language understanding and\ngeneration. CoRR abs/1905.03197.\nGehring, J.; Auli, M.; Grangier, D.; Yarats, D.; and Dauphin,\nY . N. 2017. Convolutional sequence to sequence learning.\nIn ICML 2017, 1243–1252.\nHe, T.; Tan, X.; Xia, Y .; He, D.; Qin, T.; Chen, Z.; and Liu,\nT. 2018. Layer-wise coordination between encoder and\ndecoder for neural machine translation. In NeurIPS 2018,\n7955–7965.\nHoward, J., and Ruder, S. 2018. Universal language model\nﬁne-tuning for text classiﬁcation. InACL 2018, 328–339.\nKingma, D. P ., and Ba, J. 2015. Adam: A method for\nstochastic optimization. InICLR 2015.\nKoehn, P .; Hoang, H.; Birch, A.; Callison-Burch, C.; Fed-\nerico, M.; Bertoldi, N.; Cowan, B.; Shen, W.; Moran, C.;\nZens, R.; Dyer, C.; Bojar, O.; Constantin, A.; and Herbst,\nE. 2007. Moses: Open source toolkit for statistical machine\ntranslation. In ACL 2007, 177–180.\nKoehn, P .; Och, F. J.; and Marcu, D. 2003. Statistical phrase-\nbased translation. InNAACL 2003, 48–54.\nKunchukuttan, A.; Mehta, P .; and Bhattacharyya, P . 2018.\nThe IIT bombay english-hindi parallel corpus. In LREC\n2018, 3473–3476.\nLample, G., and Conneau, A. 2019. Cross-lingual language\nmodel pretraining. CoRR abs/1901.07291.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,\nO.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoberta: A robustly optimized BERT pretraining approach.\nCoRR abs/1907.11692.\nMin, S.; Seo, M. J.; and Hajishirzi, H. 2017. Question an-\nswering through transfer learning from large ﬁne-grained su-\npervision data. InACL 2017, 510–517.\nOch, F. J., and Ney, H. 2003. A systematic comparison\nof various statistical alignment models.Computational Lin-\nguistics 29(1):19–51.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. 2002. Bleu:\na method for automatic evaluation of machine translation. In\nACL 2002, 311–318.\nPeters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark,\nC.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized\nword representations. InNAACL 2018, 2227–2237.\nRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever,\nI. 2018. Improving language understanding by gen-\nerative pre-training. URL https://s3-us-west-2. ama-\nzonaws. com/openai-assets/research-covers/language-\nunsupervised/language\nunderstanding paper . pdf.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI Blog.\nSalant, S., and Berant, J. 2018. Contextualized word rep-\nresentations for reading comprehension. In NAACL 2018,\n554–559.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural ma-\nchine translation of rare words with subword units. InACL\n2016, 1715–1725.\nSong, K.; Tan, X.; Qin, T.; Lu, J.; and Liu, T. 2019. MASS:\nmasked sequence to sequence pre-training for language gen-\neration. In ICML 2019, 5926–5936.\nTay, Y .; Luu, A. T.; and Hui, S. C. 2018. Compare, com-\npress and propagate: Enhancing neural architectures with\nalignment factorization for natural language inference. In\nEMNLP 2018, 1565–1575.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. InNIPS 2017, 5998–6008.\nWang, D.; Gong, C.; and Liu, Q. 2019. Improving neural\nlanguage modeling via adversarial training. InICML 2019,\n6555–6565.\nWu, F.; Fan, A.; Baevski, A.; Dauphin, Y . N.; and Auli, M.\n2019. Pay less attention with lightweight and dynamic con-\nvolutions. In ICLR 2019.\nYang, W.; Xie, Y .; Lin, A.; Li, X.; Tan, L.; Xiong, K.; Li,\nM.; and Lin, J. 2019a. End-to-end open-domain question\nanswering with bertserini. InNAACL 2019, 72–77.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J. G.; Salakhutdi-\nnov, R.; and Le, Q. V . 2019b. Xlnet: Generalized au-\ntoregressive pretraining for language understanding.CoRR\nabs/1906.08237.\nY u, S.; Indurthi, S. R.; Back, S.; and Lee, H. 2018. A multi-\nstage memory augmented neural network for machine read-\ning comprehension. InACL 2018, 21–30.\nZiemski, M.; Junczys-Dowmunt, M.; and Pouliquen, B.\n2016. The united nations parallel corpus v1.0. In LREC\n2016, 3530–3534.\n9393"
}