{
    "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",
    "url": "https://openalex.org/W3216037316",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4223174351",
            "name": "Hase, Peter",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222817077",
            "name": "Diab, Mona",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223995598",
            "name": "Celikyilmaz, Asli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1968637396",
            "name": "Li Xian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226508961",
            "name": "Kozareva, Zornitsa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224909769",
            "name": "Stoyanov, Veselin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202139477",
            "name": "Bansal, Mohit",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2210503090",
            "name": "Iyer, Srinivasan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W3202712981",
        "https://openalex.org/W2788388592",
        "https://openalex.org/W3005441132",
        "https://openalex.org/W3036126139",
        "https://openalex.org/W3126739176",
        "https://openalex.org/W2963961878",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W1995945562",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3204848761",
        "https://openalex.org/W3146844750",
        "https://openalex.org/W3107969673",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W2996641835",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2797563284",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3206867351",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W3174266714",
        "https://openalex.org/W3201174429",
        "https://openalex.org/W3119806891",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W3213870215",
        "https://openalex.org/W3104178968"
    ],
    "abstract": "Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating",
    "full_text": "Do Language Models Have Beliefs?\nMethods for Detecting, Updating, and Visualizing Model Beliefs\nPeter Hase1,2 Mona Diab1 Asli Celikyilmaz1 Xian Li1\nZornitsa Kozareva1 Veselin Stoyanov1 Mohit Bansal2 Srinivasan Iyer1\n1Meta AI 2UNC Chapel Hill\n{peter, mbansal}@cs.unc.edu\n{mdiab, aslic, xianl, zori, ves, sviyer}@fb.com\nAbstract\nDo language models have beliefs about the\nworld? Dennett (1995) famously argues that\neven thermostats have beliefs, on the view that\na belief is simply an informational state decou-\npled from any motivational state. In this paper,\nwe discuss approaches to detecting when mod-\nels have beliefs about the world, and we im-\nprove on methods for updating model beliefs\nto be more truthful, with a focus on methods\nbased on learned optimizers or hypernetworks.\nOur main contributions include: (1) new met-\nrics for evaluating belief-updating methods\nthat focus on the logical consistency of beliefs,\n(2) a training objective for Sequential, Local,\nand Generalizing model updates (SLAG) that\nimproves the performance of learned optimiz-\ners, and (3) the introduction of thebelief graph,\nwhich is a new form of interface with language\nmodels that shows the interdependencies be-\ntween model beliefs. Our experiments sug-\ngest that models possess belief-like qualities\nto only a limited extent, but update methods\ncan both ﬁx incorrect model beliefs and greatly\nimprove their consistency. Although off-the-\nshelf optimizers are surprisingly strong belief-\nupdating baselines, our learned optimizers can\noutperform them in more difﬁcult settings than\nhave been considered in past work.1\n1 Introduction\nLanguage models (LMs) may not have beliefs in\nthe same sense that people do, but there are a few\nreasons to analyze LMs in terms of the beliefs they\nmay possess. The ﬁrst is that this is a useful way\nto understand and speak about how LMs behave.\nWhen discussing whether animals have beliefs (rac-\ncoons, in particular), philosopher Daniel Dennett\n(1995) writes:\nYou might as well call the state of the raccoon\na belief, since if you call it a “registration” or\n1All supporting code for experiments in this pa-\nper is publicly available at https://github.com/\npeterbhase/SLAG-Belief-Updating.\na “data-structure” in the “environmental infor-\nmation store” or some other technical term, the\nlogic you use to draw inferences about the ani-\nmal’s behavior, given its internal states, will be\nthe standard, “intentionalistic” logic of belief.\nDennett bases this conclusion in the fact that we\ncan and do draw accurate inferences about animal\nbehavior by ﬁrst understanding their beliefs. We\nare drawn to speak about the beliefs of LMs in the\nsame “maximally bland (but maximally useful!)”\nsense. To the extent that these neural networks\nact intelligently in response to stimuli, we may\nform more accurate theories of how they work by\nunderstanding their beliefs.\nThe second reason for ascribing beliefs to lan-\nguage models is that many of the stricter deﬁnitions\nof belief incidentally exclude many real beliefs held\nby real people. Following Dennett (1995), Newen\nand Starzak (2020) deﬁne a belief as an informa-\ntional state decoupled from any motivational state,\nand they outline a few additional properties of be-\nliefs, namely that they should (1) be recombinable\nwith motivational states and other informational\nstates and (2) have some minimal structural orga-\nnization. Further, an entity with beliefs should\n(1) be sensitive to new information, (2) categorize\nnew beliefs as they develop, and (3) display some\nkind of logical consistency. These are all proper-\nties that come in degrees, and setting the bar too\nhigh will exclude many of the statements that peo-\nple earnestly express to others in their everyday\nlives. Meanwhile, animals and neural networks\nalike store information in accordance with these\nproperties to at least some extent.\nWe also note that we use the term belief rather\nthan knowledge as in related work (Zhu et al., 2020;\nDe Cao et al., 2021) because we want to analyze\nbeliefs of language models rather than knowledge\nin them. LMs may contain a great deal of knowl-\nedge to us, but in a traditional view of knowledge\nas Justiﬁed True Belief, it is relatively more difﬁ-\narXiv:2111.13654v1  [cs.CL]  26 Nov 2021\ncult to say that an LM knows something rather than\nthat it believes it (Schwitzgebel, 2019).\nIn the remainder of this paper, we turn our at-\ntention to three practical endeavors: detecting, up-\ndating, and visualizing beliefs in LMs. We build\non work on editing models after training, which is\nan exciting recent direction of research with many\npotentially valuable use cases (Sinitsin et al., 2020;\nZhu et al., 2020; De Cao et al., 2021; Mitchell et al.,\n2021). For LMs, uses include correcting factually\ninaccurate outputs and preventing otherwise un-\nwanted outputs from models (e.g. toxic generated\ntext) without expensive data curation and retraining\nefforts. These are important applications given that\nLMs (1) struggle with future data when trained on\ndata from the past (Lazaridou et al., 2021; Dhingra\net al., 2021), (2) generate morally undesirable text\nin many situations (Gehman et al., 2020; Bender\net al., 2021), and (3) simply give inaccurate outputs\nfor tasks like question answering (Lin et al., 2021).\nNotably, there is good evidence that scaling models\nto larger sizes will not ﬁx these particular problems\nor may even exacerbate them in cases like imitative\nfalsehoods in QA, so we will likely need an alterna-\ntive solution (Lazaridou et al., 2021; Gehman et al.,\n2020; Lin et al., 2021). We next outline a few key\ncontributions of the paper. Figure 1 represents the\ncore ideas behind these contributions.\nDetecting beliefs. We measure the degree to which\nLMs exhibit several properties of belief-possessing\nsystems, using models ﬁnetuned on fact veriﬁca-\ntion and question answering tasks. Beyond simply\nchecking individual model responses, we want to\nassess the structural properties of model outputs:\nAre they consistent under paraphrase? Are they\nlogically consistent? Does changing one belief\ncorrectly change other entailed beliefs? Does it\nerroneously change other unrelated beliefs? Past\nwork has focused primarily on consistency under\nparaphrase (Elazar et al., 2021; De Cao et al., 2021;\nMitchell et al., 2021). Here, we adapt data from\nTalmor et al. (2020) to measure consistency under\nentailment (including for contrapositives), and we\nuse the Wikidata5m dataset (Wang et al., 2021b) to\nconstruct logically neutral belief pairs for checking\nthat models do treat the beliefs as independent.\nUpdating beliefs. We propose a Sequential, Local,\nand Generalizing belief update objective (SLAG)\nthat substantially improves the performance of the\nKNOWLEDGE EDITOR method from De Cao et al.\n(2021). KNOWLEDGE EDITOR is a learned op-\ntimizer that edits a model’s weights in order to\nchange its prediction on an input while satisfying\nother desiderata, like consistency under paraphrase.\nPrincipally, we use more difﬁcult training data for\nthe learned optimizer, and we also train the net-\nwork to apply multiple small edits rather than just\none edit. These changes markedly improve the\noverall update success rate and lower the rate at\nwhich other beliefs are corrupted. Moreover, we\nﬁnd that KNOWLEDGE EDITOR almost totally fails\nwhen updating multiple beliefs in a row as opposed\nto a changing a single belief. In this setting, off-\nthe-shelf optimizers are far preferable methods.\nHowever, by explicitly training the optimizer to\nupdate multiple beliefs sequentially, we are able\nto once again outperform off-the-shelf optimizers.\nLastly, we advocate that these methods be eval-\nuated for their ability to correct false or morally\nundesirable model beliefs, rather than to arbitrarily\nadjust model beliefs to plausible alternatives as in\npast work (Zhu et al., 2020; De Cao et al., 2021;\nMitchell et al., 2021).\nVisualizing belief graphs. We explore a new form\nof interface with LMs, the belief graph. Given a\nset of beliefs, we construct belief graphs by chang-\ning each model belief and checking what other\nbeliefs are sensitive to those changes. Each belief\nbecomes a node, and directed edges between nodes\nshow that updating one belief changes the other.\nWe discuss graph metrics that help summarize the\ndependencies between model beliefs.\nWe summarize our main conclusions as follows:\n1. ∼100M parameter models exhibit limited belief-\nlike qualities, as paraphrase consistency scores\nare under 70%, and models show mixed levels\nof consistency under entailment (Sec. 5.1).\n2. Off-the-shelf optimizers are surprisingly effec-\ntive baselines for updating model beliefs, and\nthey generally outperform learned optimizers\nwhen updating a single belief (Sec. 5.2).\n3. When updating multiple beliefs in a row,\nmethod performance greatly declines (especially\nfor learned optimizers). By using SLAG, we\ncan improve learned optimizers’ performance\nbeyond what baselines can reach (Sec. 5.2).\n4. Belief graphs reveal many nonsensical depen-\ndencies between model beliefs. We ﬁnd that\n(1) updates are mostly likely to change already\nincorrect model beliefs and (2) there are highly\nconnected beliefs which inﬂuence a large frac-\ntion of all model beliefs (Sec. 6.3).\nSLAG: Sequential, Local, and Generalizing Model Updates\n(Main Input)\n(Entailed Data)\n(Local Neutral Data)\n(Paraphase Data)\n(Random Data)\nA viper is a vertebrate.\nA viper has a brain.\nA viper is venemous.\nChile is a country.\nVipers are vertebrates. \nFigure 1: Relying only on a single Main Input Mi, we want to make a targeted update to a language model that (1)\nchanges the output for input Mi to a desired label y∗\ni (e.g. True/False, or an answer to a question), (2) changes the\noutput for equivalent paraphrases of Mi, (3) appropriately changes outputs for data entailed by the tuple (Mi,y∗\ni ),\nand (4) does not change outputs for other logically neutral data, even if it is similar (local) in some way.\n2 Related Work\nDetecting beliefs in language models. Much\npast work has explored how information is stored\nand represented in pretrained language models\n(Rogers et al., 2020), though few discuss what qual-\niﬁes information as a model belief. Petroni et al.\n(2019) provide evidence that LMs store relational\ninformation between entities, and Roberts et al.\n(2020) show that LMs can answer open-ended ques-\ntions. Subsequent work has explored how much\nknowledge is stored in LMs (Heinzerling and Inui,\n2021), approaches to querying models for knowl-\nedge (Hewitt and Liang, 2019; Jiang et al., 2020;\nV oita and Titov, 2020; West et al., 2021), and meth-\nods for learning more knowledge during pretraining\n(Wang et al., 2021b,a). Most relevant to our work\nare studies from Talmor et al. (2020) and Elazar\net al. (2021). Talmor et al. (2020) train LMs to\nperform True/False classiﬁcation of factual claims,\nand they measure how a model’s belief in one fact\ncorrelates with its belief in an entailed fact. We\nuse their LeapOfThought dataset to measure model\nconsistency under entailment before and after up-\ndating the up-stream beliefs in models. Meanwhile,\nElazar et al. (2021) measure the consistency of\nmodel predictions for sets of paraphrased inputs.\nWe adopt their metric for paraphrase consistency as\na measure of belief. In concurrent work, Kassner\net al. (2021) discuss consistency under entailment\nand paraphrase as conditions for belief, and they\nmeasure consistency under entailment with a new\ndataset, BeliefBank.\nUpdating beliefs in language models. Ap-\nproaches to making targeted updates to model be-\nliefs vary along a few dimensions. First is whether\nthe methods alter model training or operate in a\npost-training setting. Sinitsin et al. (2020) use a\nmeta-learning objective during training to encour-\nage ease of editing afterwards, though the memory\nrequirements of their approach limit its scalability\nbeyond 100M parameter models. A larger family\nof methods make post-training updates to models,\ndiffering in how they formalize the update prob-\nlem: Dai et al. (2021) propose a hand-crafted algo-\nrithm for updating model weights, while Zhu et al.\n(2020) use projected gradient descent for batches\nof points. De Cao et al. (2021) and Mitchell et al.\n(2021) frame the problem as a machine learning\nproblem and train hypernetworks (learned optimiz-\ners) that process model gradients in order to pro-\nduce a new model that (1) gives the desired output\nfor the edited point, while (2) incorporating other\nobjectives like minimizing the changes in predic-\ntions for other data. Here, we build directly upon\nthe method from De Cao et al. (2021), showing\nwhere it fails and providing an improved training\nobjective (SLAG). In particular, we ﬁnd that the\nmethod struggles with updating multiple beliefs\nsequentially. This setting bears some commonality\nto the continual learning problem, though continual\nlearning methods generally aim to learn new tasks\nor datasets rather than make targeted updates to\nspeciﬁc model beliefs (Parisi et al., 2019).\nNot all methods edit model weights. Kassner\net al. (2021) update model beliefs by adding in\nrelevant information to the input at test time (to im-\nprove consistency under entailment). But as with\nretrieval-based methods, this approach does not\nchange the model weights and hence does not inﬂu-\nence model outputs on all other potentially relevant\ninputs (Lewis et al., 2020; Hase and Bansal, 2021).\nDataset Data Type Input Label(s)\nzsRE Main Input Player Ali Kanaan plays for what team? {Sporting Al Riyadi Beirut}Paraphrase What team is Ali Kanaan associated with?\nWikidata5m\nMain Input Mary Good has relation ‘award received’ to {Garvan-Olin Medal; Arkansas\nWomen’s Hall of Fame; etc.}Paraphrase Mary Lowe Good has relation ‘winner of’ to\nLocal Neutral Mary Good has relation ‘educated at’ to {The University of Arkansas; U\nArkansas; etc.}\nFEVER Main Input Tardigrades are also known as space bears. True\nMain Input The Lion belongs to the genus Vulpes. False\nLeapOfThought Main Input A viper is a vertebrate. True\nEntailed Data A viper has a brain. True\nTable 1: Example datapoint from each dataset, and auxiliary data that accompanies the Main Input. We catalogue\nexamples of noise and other shortcomings for each dataset in Appendix C.\n3 Updating Beliefs in Language Models\nFollowing De Cao et al. (2021), we approach the\nproblem of updating model beliefs as a machine\nlearning problem and train a learned optimizer to\nperform desired model updates. We discuss metrics\nfor detecting beliefs in Sec. 5.1 and our approach\nto visualizing belief graphs in Sec. 6.3. The core\nideas of our approach are outlined in Fig. 1.\nProblem statement and metrics. We suppose\nwe have a model fθ = pθ(y|x) parametrized by\nθ. For an input xi that has some undesired model\noutput ˆyi = arg maxypθ(y|x), we wish to obtain\na new model θ∗that produces a desired output y∗\ni\nfor xi. This new model θ∗ should also fulﬁll a\nfew other desiderata. As in past work (De Cao\net al., 2021; Mitchell et al., 2021), we operational-\nize these desiderata in the following metrics:\n1. Update Success Rate (Main Input): The rate\nat which the updated model gives the desired\noutput y∗\ni for the Main Input xi.\n2. Update Success Rate (Paraphrase): The rate\nat which the updated model gives the same\nnew prediction for xi as it does for para-\nphrases of xi, which are inputs with the same\nmeaning but different surface form.\n3. Retain Rate (All Data): The rate at which the\nupdated model’s predictions are unchanged\nfor all other data besides the Main Input.\n4. ∆-Acc (All Data): The change in accuracy for\nthe updated model on all other data besides\nthe Main Input.\nIn practice, Retain Rate (All Data) and ∆-Acc are\ncomputed with random subsets of a dataset, since\nthese must be computed after every belief update.\nWe add two metrics to those used in past work:\n5. Update Success Rate ( Entailed Data): The\nrate at which the updated model makes predic-\ntions that are logically entailed by the model’s\nprediction for the Main Input.\n6. Retain Rate ( Local Neutral): The rate at\nwhich the updated model’s predictions are un-\nchanged for data that is similar to the Main\nInput but still logically neutral.\nWe use Update Success Rate ( Entailed Data) to\nmeasure logical consistency for an updated model,\nsince changing one belief will entail changes in\nlogically entailed beliefs. We also split “retain\naccuracy\" into two cases, one for randomly sam-\npled data as in past work (All Data) and the other\nfor specially constructed Local Neutraldata. Un-\nlike randomly sampled data, Local Neutral data is\nguaranteed to be logically independent of the Main\nInput, while still being similar (local) to it. To-\ngether, these six metrics better cover the criteria for\nbelief outlined by Newen and Starzak (2020). We\ncompute the metrics using data of the kind shown\nin Table 1. For a glossary of terms used for these\nmetrics across papers, see Appendix Table 13.\nEvaluation data. To date, methods have been\nevaluated on the basis of their ability to change\nmodel predictions for all data points, including\ncorrectly and incorrectly predicted points. More-\nover, the desired labels {y∗\ni}n\ni=1 on sequence pre-\ndiction tasks have each been selected from the\nbeam search which produced the original model\nprediction (De Cao et al., 2021; Mitchell et al.,\n2021). We propose for method evaluation to focus\non a more valuable use case: changing the predic-\ntions on incorrect points to be correct. In Sec. 5, we\nshow that this is a harder task than simply changing\npredictions to other similar outputs, so the effec-\ntiveness of past methods has been overestimated.\nSequential updates. The default evaluation pro-\ncedure in past work on learned optimizers is to up-\ndate a single model belief, evaluate the new model,\nthen rollback the update before repeating the pro-\ncess for each test point. In Sec. 5, we show that it\nis much harder to update multiple beliefs in a row\nbefore evaluating the new model. This is notable\nbecause in practice, it is likely that model develop-\ners will want to update many beliefs of a trained\nmodel, possibly over long timescales, meaning se-\nquential updating is a more realistic application of\nupdate methods. We obtain sequential versions of\nall our metrics by applying rmodel updates in a\nrow before checking the metrics, meaning there are\nﬂoor(n/r) measurements for a test set of npoints.\nBelief updating method. We use the KNOWL -\nEDGE EDITOR architecture from De Cao et al.\n(2021) with our training objective, SLAG. For the\ndetails of this architecture, we refer readers to Ap-\npendix A. Let it sufﬁce for now to observe that a\nnew model is given as a differentiable function\nθ∗= θ+ gφ(xi,ˆyi,y∗\ni,θ)\nusing the learned optimizer gφ, current LM weights\nθ, Main Input xi, current prediction ˆyi, and desired\nmodel output y∗\ni. In this paper, we generalize the\nupdate step to occur in a loop. If we package the\nabove update asθ(k+1) = θ(k)+gφ(xi,ˆyi,y∗\ni,θ(k)),\nthen we can obtain new model parameters as\nθ∗= θ(k) +\nK−1∑\nj=0\ngφ(xi,ˆyi,y∗\ni,θ(k+j))\n= Update(xi,ˆyi,y∗\ni,θ(k); φ,K)\nfor a number of steps Kfrom the initial parameters\nθ(k). In fact, De Cao et al. (2021) use such a loop\nat test time; we incorporate the loop into training\nto align the train and test-time distributions.\nLearned optimizer training. The training objec-\ntive for KNOWLEDGE EDITOR includes differen-\ntiable terms corresponding to Update Success for\nthe Main Input and paraphrases, as well as Re-\ntain Rate for all other data. We also include terms\nfor Update Success on entailed data and the Local\nNeutral Retain Rate, when this is possible given\navailable data. The overall objective requires sev-\neral kinds of additional data for each point, which\nwe denote by DR for other random data, DLN for\nlocal neutral data, DE for entailed data, and DP for\nparaphrases of xi. For a data point xi with desired\nprediction y∗\ni, the full objective is then:\nL(φ; xi,ˆyi,y∗\ni,θ) = λ1LTask(fθ∗ (xi),y∗\ni)\n+ λ2\n1\n|DP|\n∑\nxP∈DP\nLTask(fθ∗ (xP),y∗\ni)\n+ λ3\n1\n|DE|\n∑\nxE,yE∈DE\nLTask(fθ∗ (xE),yE)\n+ λ4\n1\n|DLN|\n∑\nxLN∈DLN\nKL(fθ∗ (xLN)||fθ(xLN))\n+ λ5\n1\n|DR|\n∑\nxR∈DR\nKL(fθ∗ (xR)||fθ(xR)) (1)\nwhere LTask is the loss used to get gradients for fθ.\nWe use the Cross Entropy loss for binary classiﬁca-\ntion and sequence-to-sequence tasks.\nWe optimize this objective w.r.t. φ using\nAdamW (Loshchilov and Hutter, 2019). To obtain\nupdate labels {y∗\ni}n\ni=1, we always use the oppo-\nsite class in binary classiﬁcation. For sequence-to-\nsequence tasks, we use the correct label when ˆyi\nis incorrect, and when ˆyi is correct, we randomly\nselect another label from the training data. This\nchoice is in contrast to De Cao et al. (2021) and\nMitchell et al. (2021), who use samples from the\nmodel beam search as update labels for all points.\nSLAG objective. To better prepare the update\nmethod for evaluation in a sequential-update set-\nting, we consider training gφ to update multiple\ndatapoints in a row. Using the per-datapoint loss in\nEq. 1, we obtain our Sequential, Local, and Gen-\neralizing (SLAG) loss for a set of r Main Inputs\nD= {xi,ˆyi,y∗\ni}r\ni=1 as\nLSequential(φ; D,θt)=\nr∑\ni=1\nL(φ; xi,ˆyi,y∗\ni,θt+i) (2)\nwhere θt+i = Update(xi,ˆyi,y∗\ni,θt+i−1; φ,K) are\nthe model parameters obtained from updating on\nthe ﬁrst ipoints in D(starting from θt). This objec-\ntive allows us to train gφ to update multiple beliefs\nin a row. To ensure training with this objective is\nstill efﬁcient, we limit how far back through the LM\nhistory we backpropagate when computing the gra-\ndient w.r.t. φfor each term in the RHS sum of Eq. 2.\nEach parameter vector θt depends on φand θt−1.\nWe always apply the stop-gradient function to the\nmost recent vector θt−1 to prevent backpropagating\nthrough it (visualized in Appendix Fig. 4). This\nchoice allows our memory use to remain constant\nin r(see Appendix Fig. 5).\n4 Experiment Setup\n4.1 Datasets\nWe run experiments with four datasets (example\ndata shown in Appendix Table 15). (1) FEVER in-\ncludes 115,409 True/False factual claims (Thorne\net al., 2018). We use the original test set of 10,444\npoints, and we randomly split the training data into\n94,469 train points and 10,496 dev points. (2) zsRE\nincludes 151,631 questions based on relational\nknowledge from Wikipedia, which we randomly\nshufﬂe into train/dev/test splits with 80/10/10% of\nthe data (Levy et al., 2017). 32.8% of zsRE ques-\ntions in each split include paraphrases, and we mea-\nsure Update Success Rate ( Paraphrase) for only\nthese points. Talmor et al. (2020) introduce (3)\nthe LeapOfThought dataset, consisting of 33,484\nfactual claims that are entailed to be true or false\ndepending on a fact and distractor statements pro-\nvided as context. We drop the distractors from\neach input and ﬁlter the data so that the facts are\nunique, then shufﬂe the resulting 14,939 points into\ntrain/dev/test splits with 60/10/30% of the data.\nWe also construct (4) a sequence prediction task\nusing data from Wikidata5m, which is a relational\nknowledge base with over 20 million triplets (Wang\net al., 2021b). We build this dataset in order to get\nLocal Neutral data. Each input consists of an entity\ne1 and relation r, and the label is another entity\ne2 that completes the triplet. All inputs come in\npairs that share the same entity e1 but use different\nrelations with different labels. The relations are\nalways one of ten relations that apply to people (see\nAppendix Table 11). In general, the completion e2\nto the Main Input triplet (e1, r1, e2) has no logical\nconsequences for its paired input, (e1, r2, ?). This\nmeans that changing the model belief for the Main\nInput should not change its belief for its neutral\npaired input. The paired points are also local to\nthe Main Input, i.e. they pertain to the same entity\ne1 as the Main Input. We obtain four paraphrases\nfor each Main Input using different aliases for the\nentity and synonyms of the relation. We construct a\ntrain set of 150k points and dev and test sets of 10k\npoints each. See Appendix B for further details.\n4.2 Methods Evaluated\nModels. We train ﬁve models with different ran-\ndom seeds for each dataset, using RoBERTa-base\nfor binary tasks and BART-base for sequence-to-\nsequence tasks (accuracies in Appendix Table 14).\nFor each of the ﬁve models, we train one learned\nBelief Consistency ↑\nDataset Paraphrase Entailed Contrapos.\nLeapOfThought - 85.6 (1.1) 16.5 (2.7)\nzsRE 69.5 (1.1) - -\nWikidata5m 25.8 (0.5) - -\nTable 2: Belief metric results across datasets.\nParaphrase Consistency ↑\nDataset Model Incorrect Model Correct\nzsRE 61.39 (1.33) 91.82 (1.17)\nWikidata5m 24.55 (0.48) 37.20 (2.06)\nTable 3: Paraphrase consistency by the correctness of\nthe model prediction on the Main Input.\noptimizer using SLAG and one with the objective\nfrom De Cao et al. (2021), which we list as KE in\ntables below. Our model selection criterion is the\nmean of: the average Update Success Rate (across\ndata types), Retain Rate (only for Local Neutral\ndata), and ∆-Acc for All Data. We tune the choice\nof SLAG objective terms for each task separately\n(see Appendix Table 10 for ﬁnal selections; results\ndiscussed in Sec. 5.3). Other hyperparameters are\ngiven in Appendix B and memory use is shown in\nAppendix Fig. 5. To summarize the differences\nbetween SLAG and KNOWLEDGE EDITOR : (1) we\nuse Ktrain = Ktest rather than Ktrain = 1; (2) we\nadopt training labels using real data labels rather\nthan alternatives from the model’s beam search;\nand (3) our objective terms differ following tuning.\nBaselines. We use off-the-shelf optimizers as base-\nlines. We tune the baseline hyperparameters sep-\narately for each dataset, selecting among several\nkinds of optimizers, learning rates, and the num-\nber of update steps. The selection criterion is the\nsame as the criterion outlined for learned optimiz-\ners above. The resulting baselines are surprisingly\nstrong (see Appendix Table 12 for ﬁnal selections).\nHypothesis testing. We obtain 95% conﬁdence\nintervals and perform hypothesis tests via block\nbootstrap, resampling model seeds and data points\n(Efron and Tibshirani, 1994). For ablation experi-\nments, we run only one model seed per condition.\n5 Experiment Results\n5.1 Do LMs have beliefs about the world?\nWe measure Paraphrase Consistency, Entailment\nAcc, and Contrapositive Acc for our ﬁnetuned task\nmodels. Paraphrase Consistency is the fraction of\nparaphrase pairs for which a model produces the\nsame output (Elazar et al., 2021). Entailment Acc\nSingle-Update Setting Update Success Rate Retain Rate ∆-Acc\nDataset Method Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER\nAdamW 100 (0.0) - - - 98.80 (0.2) 0.22 (0.1)\nKE 99.98 (<0.1) - - - 98.28 (0.3) -0.24 (0.1)\nSLAG 99.99 (<0.1) - - - 98.41 (0.2) -0.20 (0.1)\nLeapOfThought\nSGD 100 (0.0) - 72.48 (4.6) - 95.52 (0.4) 1.23 (0.8)\nKE 99.78 (0.4) - 74.48 (4.4) - 93.50 (1.3) -1.33 (1.1)\nSLAG 100 (0.0) - 75.50 (4.3) - 94.92 (1.4) -1.31 (1.2)\nzsRE\nSGD 99.36 (0.1) 94.44 (0.6) - - 74.73 (0.4) -0.43 (0.1)\nKE 84.73 (1.4) 89.26 (1.8) - - 71.55 (2.4) -2.19 (0.4)\nSLAG 94.29 (0.4) 94.71 (0.5) - - 80.48 (1.3) -0.29 (0.1)\nWikidata5m\nSGD 98.05 (0.3) 68.78 (0.8) - 41.46 (1.0) 58.62 (0.6) -1.97 (0.3)\nKE 74.57 (2.9) 58.05 (2.2) - 40.84 (1.8) 53.58 (2.2) -3.03 (0.5)\nSLAG 87.59 (0.6) 80.70 (0.9) - 47.85 (1.0) 63.51 (1.3) -1.71 (0.3)\nTable 4: Belief update metrics for off-the-shelf optimizers, KNOWLEDGE EDITOR (KE) from De Cao et al. (2021),\nand SLAG, with rtest = 1 . Bolded numbers are the best in their group at a statistical signiﬁcance threshold of\np<. 05 (or lower). Our SLAG objective improves over KE, but off-the-shelf optimizers perform surprisingly well.\nis the accuracy of a model on data that is entailed by\nthe Main Input. For LeapOfThought (see Table 1),\n“Main Input xi is true\" implies “entailed input xE\nhas label yE,\" but the inverse (¬A⇒¬B) does not\nnecessarily hold. Therefore, we compute Entail-\nment Acc only where the Main Input prediction is\ncorrect. We do know that the contrapositive holds:\n“Entailed input xE does not have label yE\" implies\nthat “Main Input xi is false.\" So for Contrapositive\nAcc, we measure how often the model follows this\nrule, when the antecedent holds of its prediction.\nBelief measurement results. Table 2 shows the\nbelief metrics for each dataset. We ﬁnd that\n∼100M parameter models show limited evidence\nof having beliefs about the world. Paraphrase con-\nsistency is 69.50% ( ±1.09) for zsRE and much\nlower for Wikidata5m (25.84% ±0.53). While\nentailment accuracy is high for LeapOfThought\n(85.63%±1.08), the model is consistent under the\ncontrapositive only 16.51% (±2.71) of the time.\nOne might reasonably set the bar for qualifying as\na “belief\" higher than these scores. But since belief-\nlikeness comes in degrees, we continue to refer to\nmodel beliefs for the rest of the paper. Interest-\ningly, the metrics are much higher when the model\nprediction on the Main Input is correct (Table 3).\n5.2 Can we update beliefs in LMs?\nFirst, we compare two evaluation procedures for\nsequence prediction tasks: correcting model be-\nliefs versus changing them to an alternative from\nthe model’s beam search. We do so for zsRE us-\ning SLAG. Next, we compare belief update met-\nrics across datasets using KNOWLEDGE EDITOR ,\nSLAG, and off-the-shelf optimizers as baselines.\nUpdate Success Rate ↑ ∆-Acc ↑\nDesired Label Main Input Paraphrase All Data\nBeam Label 97.41 (0.3) 97.03 (0.4) -0.30 (0.1)\nCorrect Label 94.46 (0.7) 94.45 (0.7) -0.24 (0.1)\nTable 5: Evaluation difﬁculty by desired model output,\nfor a learned optimizer trained with SLAG on zsRE.\nWe report results in single-update (rtest = 1) and\nsequential-update (rtest = 10 ) settings. See Ap-\npendix Fig. 6 for an ablation across rtest.\nCorrecting beliefs vs. changing beliefs. Given\nthe results in Table 5, we ﬁnd that correcting model\noutputs is harder than simply changing them to a\nplausible alternative. Update Success can rise by a\nfull 2.96 (±0.48; p<1e−4) points for Main Inputs\nand 2.58 (±0.81; p<1e−4) for Paraphrases, while\n∆-Acc is virtually unchanged. This suggests that\nthat past work has overestimated the efﬁcacy of\nbelief update methods for actually ﬁxing models.\nHenceforth we evaluate methods according to their\nability to update model beliefs to be true.\nUpdate method results (single update). Table 4\nshows the results in a single-update setting. First,\nwe ﬁnd that off-the-shelf optimizers are very effec-\ntive across the board. The baselines show Main\nInput Update Success Rates of 100% for binary\ntasks with positive ∆-Acc scores.2 On sequence\nprediction tasks, SGD achieves 98%+ Main Input\nUpdate Success with competitive ∆-Acc scores.\nWhen strongly tuned, these baselines outperform\nlearned optimizers on most metrics here.\n2Positive ∆-Acc values are possibly due to distribution\nshift in the test split. In FEVER, for instance, the train and\ndev data are 73% True, while test data is 50% True. On the\ndev split, AdamW achieves a negative ∆-Acc, -0.18 (±0.11).\nSequential-Update Setting Update Success Rate Retain Rate ∆-Acc\nDataset Method Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER\nAdamW 92.81 (1.3) - - - 91.86 (1.4) 1.16 (0.6)\nSLAG1 74.13 (1.8) - - - 39.86 (0.7) -27.13 (1.3)\nSLAG10 91.27 (2.9) - - - 70.30 (5.8) -11.96 (4.5)\nLeapOfThought\nSGD 100 (0.0) - 61.34 (5.0) - 82.62 (0.8) -4.93 (1.0)\nSLAG1 96.14 (2.3) - 49.27 (6.0) - 72.45 (0.9) -15.03 (1.0)\nSLAG10 100 (0.0) - 50.46 (5.5) - 74.02 (1.1) -13.03 (1.3)\nzsRE\nSGD 82.71 (0.6) 90.81 (0.7) - - 40.49 (0.6) -2.38 (0.3)\nSLAG1 0.10 (<0.1) 36.55 (1.4) - - 0.05 (<0.1) -20.98 (0.7)\nSLAG10 87.57 (0.6) 92.20 (0.7) - - 47.19 (0.7) -1.74 (0.3)\nWikidata5m\nSGD 56.82 (0.8) 54.49 (0.7) - 6.40 (0.4) 26.37 (0.6) -3.96 (0.4)\nSLAG1 0 (0.0) 40.84 (0.9) - 0 (0.0) 0 (0.0) -10.05 (0.6)\nSLAG10 58.27 (1.0) 65.51 (0.9) - 7.36 (0.5) 27.76 (0.7) -3.62 (0.4)\nTable 6: Belief update results when a model is sequentially updated rtest=10 times. SLAG R uses rtrain=R. On\nsequence prediction tasks in this setting, SLAG can outperform the off-the-shelf optimizers across metrics.\nHowever, SLAG does surpass the baselines in\na few places. All Data Retain Rate on zsRE rises\nby 5.77 points ( ±1.43; p<1e−4), and on Wiki-\ndata5m we improve Paraphrase Update Success by\n11.92 points (±1.20; p<1e−4) and the Local Neu-\ntral Retain Rate by 6.40 (±1.41; p<1e−4) points.\nThe gain on Entailed Data Update Success is 3.02\npoints, but it is not signiﬁcant ( ±6.26; p=.345).\nThe SLAG objective also greatly improves perfor-\nmance over KE for sequence prediction tasks.\nUpdate method results (sequential updates).\nWe give results for a sequential update setting\n(rtest=10) in Table 6. Immediately we see this is\na much more difﬁcult setting for updating model\nbeliefs, as update metrics are generally much lower\nfor each dataset. Next, we observe that learned\noptimizers with SLAG10 (rtrain=10) now outper-\nform baselines on sequence prediction tasks. On\nzsRE, we improve Update Success for Main In-\nputs by 4.86 (±0.83; p=1e−4) and for Paraphrases\nby 1.39 ( ±0.93; p=.004), with better ∆-Acc by\n0.64 (±0.35; p=.0005). Improvements trend in\nthe same direction for Wikidata5m and are all sta-\ntistically signiﬁcant except for the gain in ∆-Acc.\nThe jump on Paraphrases in particular is very large\n(11.02±1.17; p<1e−4). In comparison, using a\nnon-sequential (rtrain = 1) training objective leads\nto drastic drops in performance.\nLearned optimizers still struggle with the binary\ndatasets compared to the off-the-shelf optimizers.\nThe baselines achieve high update update success\nwith much better ∆-Acc scores, by 13.12 (±4.51;\np=1e−4) on FEVER and 8.16 (±1.63; p=1e−4)\non LeapOfThought. Also on LeapOfThought, the\nbaseline’s update success with entailed data is over\n10 points higher (±7.38; p=.004).\n5.3 How does the learned optimizer objective\ninﬂuence performance?\nHere, we discuss ablations with respect to the terms\nin the training objective, Eq. 1. We show the effect\nof Ktrain in Appendix Fig. 9 and the choice of\noptimizer training labels in Appendix Table 16.\nTraining objective ablation. We give objective\nablation results in Appendix Table 17. Surpris-\ningly, we do not always see that the objective terms\nhelp for the data they are intended to help with.\nFirst, we obtain mixed results for the paraphrase\nobjective. On zsRE, the objective term seems to\nhinder performance, with update success dropping\non Main Inputs by 0.71 ( ±0.60; p=.021) and ∆-\nAcc dropping by 0.18 (±0.19; p=.069), while the\nparaphrase Update Success Rate itself is unaffected.\nWith Wikidata5m, however, the paraphrase term\nimproves paraphrase update success by a large mar-\ngin of 16.94 ( ±1.03; p<1e−4) points. Adding\nthe Local Neutral (LN) term with the paraphrase\nterm greatly improves the LN Retain Rate for Wiki-\ndata5m, by 9.71 points (±1.44; p<1e−4), though\nboth of these terms come at a cost to Main Input\nUpdate Success, similar to zsRE. Lastly, we do not\nﬁnd that the entailment objective improves Entailed\nData Update Success; in fact, this metric falls by\n4.56 (±7.22; p=.213) points with the objective.\n6 Analysis\n6.1 Belief updates improve consistency\nIn Table 7, we show belief metrics before and af-\nter model updates using SLAG with rtest=1. We\nobserve that belief updates greatly improve para-\nMetric Before Update After Update\nEntailment Acc 58.30 (5.7)* 75.50 (4.3)\nPara. Cons (zsRE) 61.39 (1.3) 94.53 (0.6)\nPara. Cons (Wiki) 24.69 (0.5) 84.56 (0.9)\nTable 7: Entailment Acc and Paraphrase Consistency\nbefore and after model updates to incorrect points. *All\nMain Inputs in this subset are wrongly predicted as\nfalse, so the entailment does not actually hold.\nphrase consistency and entailment accuracy for\nupdated data. Paraphrase consistency rises by\n33.14±1.46 on zsRE and 59.87 ±1.09 on Wiki-\ndata5m, while Entailment Acc rises by 17.20±7.10\npoints. To see if these improvements depend on pre-\nupdate consistency, we plot paraphrase consistency\nbefore and after updating in Fig. 2. For zsRE, con-\nsistency rises irrespective of pre-update consistency.\nThere is a noticeable trend for Wikidata5m para-\nphrases, where post-update consistency is 90.1%\nwhen pre-update consistency is maxed out, versus\n77.1% for totally inconsistent pre-update beliefs.\nWe conclude that learned optimizers can induce a\nfairly consistent model belief even where there is\nno consistent belief to begin with.\n6.2 Which beliefs are hard to retain when\nupdating other beliefs?\nWe ﬁnd that the Retain Rate depends heavily on\nwhether the predictions on that data are correct to\nbegin with. On zsRE for instance, the retain rate\non correct inputs is about 96%, while for incorrect\npredictions, it is about 75%. So it appears that in-\ncorrect predictions are the most sensitive to model\nupdates, and these points merely change from one\nincorrect prediction to another. On FEVER, incor-\nrect beliefs change around 4% of the time, while\ncorrect beliefs change only 2.5% of the time.\nWe also ﬁnd that Local Neutral beliefs are much\nharder to avoid changing than simply random data.\nFor Wikidata5m in Table 4, the Retain Rate on All\nData is 61.51±1.33, while for Local Neutral data\nit is a full 15.66 points lower, at 47.85±0.96.\n6.3 Belief Graphs\nWe now construct belief graphs for the purpose\nof better understanding the connections between\nmodel beliefs. We form the graphs from a set of dat-\napoints by updating each prediction and checking\nwhat other predictions change. We represent each\ndatapoint as its own node in a belief graph. When-\never updating a datapoint uchanges the model pre-\ndiction for point v, we draw a directed edge from\nzsRE\nWikidata5m\n0.00 0.25 0.50 0.75 1.00\n0.7\n0.8\n0.9\n1.0\n0.7\n0.8\n0.9\n1.0\nPre−Update Consistency\nPost−Update Consistency\nUpdates Improve Consistency Everywhere\nFigure 2: Post-update consistency under paraphrase is\nhigh even for points with low pre-update consistency.\nuto v. Following our results in Sec. 5.2, we use\noff-the-shelf optimizers to change the model output\nto the opposite of its original prediction for every\ndatapoint. The resulting graphs have up to n2 −n\nedges (no self edges). For FEVER we obtain a\ngraph of 10,444 nodes, and for LeapOfThought we\nobtain a graph with 8642 nodes, which is double\nthe original test set size because we include both\nMain Inputs and Entailed Data as their own nodes.\nWe visualize part of a belief graph in Fig. 3.\nThis ﬁgure shows a non-random subgraph intended\nto give a representative view of the data (we give\nthree random subgraphs of 20 nodes in Appendix\nE). On inspection, we see no reason that beliefs are\nconnected or not connected. Whether or not chang-\ning one belief changes another appears essentially\nrandom. We come to same conclusion looking at\nother random subgraphs (see Appendix Figures 10,\n11, 12). However, we do know of some aggregate\ntrends from earlier results. Sec. 6.2 suggests that a\nmodel’s incorrect beliefs are most likely to change\nafter model updates, and following Sec. 5, we have\nreason to believe that Local beliefs are more likely\nthan others to change with model updates.\nWe highlight a few summary statistics here from\nTable 8 for a broader view of the graphs. First,\n% Edgeless is the proportion of nodes which have\nno in or out edges. Since this is 0 for both datasets,\nevery belief can be changed by editing the right\nbelief. # In Edges is the number of in edges at the\n95th percentile, meaning 5% of beliefs have more in\nedges than this value, and the same holds of # Out\nEdges. These values grow to a rather large fraction\nof the overall datasets, suggesting that (1) some\nbeliefs are sensitive to changes in a large fraction\nof all beliefs, and (2) some beliefs are inﬂuential\nto hundreds of other beliefs when changed. # Cor-\nrupted is the number of correct predictions changed\nMiddle-earth is a real place.\n[y: false]\nHot Right Now is mistakenly\nattributed to DJ  Fresh.\n[y: false]\nThere are no musical or creative\nworks in existence that have\nbeen created by Phillip Glass.\n[y: false]\nThe Daily Show is incapable\nof focusing on recent news\nstories.\n[y: false]\nThe Chrysler Building was\nalways the world's shortest\nbuilding.\n[y: false]\nShane McMahon officially\nretired on the first day of\n2010.\n[y: false]\nBessie Smith died on April\n26, 1937.\n[y: false]\nDespicable Me 2 was written\nby Cinco Paul.\n[y: true]\nHot Right Now is from Nextlevelism.\n[y: true]\nFigure 3: A non-random subgraph of the belief graph for a model trained on FEVER. Directed edges from uto\nvindicate that changing the model belief in ucauses the belief in vto change. The ground-truth label is given in\nbrackets for each point, and node color shows the model’s accuracy before any updates (green=correct).\nDataset\nMetric FEVER LeapOfThought\n# Nodes 10,444 8,642\n% Edgeless 0.0 0.0\n# Edges Total 1.88m 9.71m\n# In Edges (95th perc.) 1,088 5,347\n# Out Edges (95th perc.) 390 3,087\n# Corrupted (95th perc.) 211 2,752\n% Update-Transitivity 66.64 24.38*\nTable 8: Belief graph summary statistics. *We compute\nUpdate-Transitivity for LeapOfThought withn= 4000\npoints due to computational cost.\nto be incorrect following a model update. For 5%\nof the data, model updates cause at least 211 points\nto become incorrectly predicted on FEVER, and\n2,752 points for LeapOfThought. Lastly, % Update-\nTransitivity represents the answer to the question:\nif updating belief A changes belief B, and updating\nbelief B changes belief C, what proportion of the\ntime does updating A change C? For these datasets,\na logically consistent model should display 100%\nUpdate-Transitivity (see Appendix D for a caveat\non this metric). We ﬁnd that belief updates often\nyield intransitive results for both datasets.\n7 Discussion and Conclusion\nDegrees of commitment to beliefs. The data we\nuse comes in the form of declarative statements and\nanswers to questions. These utterances take what is\ncalled a veridical stance toward a proposition, dis-\nplaying a “full commitment\" to that proposition’s\ntruthfulness (Giannakidou and Mari, 2020). It will\nbe valuable for future work to explore two dimen-\nsions of uncertainty in beliefs: (1) expression of\nuncertainty in language, via partial or trivial com-\nmitments (like “X might be Y\") and (2) expression\nof uncertainty mathematically, via probabilities as-\nsigned by a model to utterances or True/False val-\nues. In this paper we treat a belief as “updated\"\nwhen the model output changes, but this ignores\nany underlying change in the distribution pθ(y|x)\nthat could occur even if its mode does not change.\nEthics and dual use concerns. Belief update\nmethods may be used to either correct undesired\nbeliefs or induce problematic beliefs in LMs, and\nit is not clear whether these capabilities could be\nseparated. We propose to evaluate methods only on\nthe basis of their ability to correct mistaken model\nbeliefs, but the malicious use case remains. We\nare uncertain about how a bad belief would inﬂu-\nence the general behavior of a model (e.g. answers\nto many questions), but it is possible that a belief\nupdate method could instill bad beliefs in a gen-\nerally capable LM with far-reaching implications\nfor model behavior. That said, we hope that these\nmethods will instead be used to update undesirable\nmoral, social, and factual beliefs in large LMs.\nConclusion. We ﬁrst discuss criteria for detect-\ning when LMs have beliefs about the world. Next,\nwe argue for evaluating belief update methods by\ntheir ability to correct mistaken beliefs, which is\nharder than the evaluation done in past work. We\nshow that strongly tuned off-the-shelf optimizers\nmake for surprisingly good belief update methods,\neven surpassing specialized learned optimizers in\nseveral settings. But with a new training objective\n(SLAG), we are able to outperform these baselines\non sequence prediction tasks when updating mul-\ntiple beliefs one after another. Finally, we intro-\nduce belief graphsas a means of understanding the\nconnections between model beliefs. We ﬁnd that\nmodel beliefs are highly interconnected, with some\nbeliefs inﬂuencing hundreds of other beliefs. While\nit is hard to point to concrete reasons for individual\nconnections between beliefs, we identify several\npatterns in the dependencies between beliefs.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. arXiv preprint arXiv:2104.08696.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models. In\nEMNLP, pages 6491–6506. Association for Compu-\ntational Linguistics.\nDaniel Dennett. 1995. Do animals have beliefs? Com-\nparative approaches to cognitive science, 111.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2021. Time-aware language\nmodels as temporal knowledge bases. arXiv\npreprint arXiv:2106.15110.\nBradley Efron and Robert J Tibshirani. 1994. An Intro-\nduction to the Bootstrap. CRC press.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and im-\nproving consistency in pretrained language models.\nTransactions of the Association for Computational\nLinguistics, 9:1012–1031.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of EMNLP.\nAnastasia Giannakidou and Alda Mari. 2020. A lin-\nguistic framework for knowledge, belief, and veridi-\ncality judgement. HAL.\nPeter Hase and Mohit Bansal. 2021. When can mod-\nels learn from explanations? a formal framework for\nunderstanding the roles of explanation data. arXiv\npreprint arXiv:2102.02201.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity\nrepresentations, storage capacity, and paraphrased\nqueries. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics: Main Volume, pages 1772–\n1791, Online. Association for Computational Lin-\nguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In EMNLP.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. Beliefbank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. arXiv preprint arXiv:2109.14723.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nSebastian Ruder, Dani Yogatama, et al. 2021. Mind\nthe gap: Assessing temporal generalization in neural\nlanguage models. In NeurIPS.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017), pages 333–342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In NeurIPS.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309.\nAlbert Newen and Tobias Starzak. 2020. How to as-\ncribe beliefs to animals. Mind & Language.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christo-\npher Kanan, and Stefan Wermter. 2019. Continual\nlifelong learning with neural networks: A review.\nNeural Networks, 113:54–71.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nEric Schwitzgebel. 2019. Belief. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of Philosophy,\nFall 2019 edition. Metaphysics Research Lab, Stan-\nford University.\nAnton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin,\nSergei Popov, and Artem Babenko. 2020. Editable\nneural networks. In ICLR.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In NeurIPS.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERiﬁcation. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn EMNLP.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Guihong Cao, Daxin Jiang, Ming\nZhou, et al. 2021a. K-adapter: Infusing knowledge\ninto pre-trained models with adapters. In Findings\nof ACL.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. Kepler: A uniﬁed model for knowledge\nembedding and pre-trained language representation.\nTransactions of the Association for Computational\nLinguistics, 9:176–194.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing\nLu, Sean Welleck, and Yejin Choi. 2021. Sym-\nbolic knowledge distillation: from general language\nmodels to commonsense models. arXiv preprint\narXiv:2110.07178.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Sri-\nnadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv\nKumar. 2020. Modifying memories in transformer\nmodels. arXiv preprint arXiv:2012.00363.\nA Learned Optimizer Details\nArchitecture. KNOWLEDGE EDITOR is a learned\noptimizer g: X×Y×Y× Θ →Θ that produces\nnew model weights by applying an adjusted gra-\ndient step to a model. For reference, we give a\nglossary of symbols used here in Table 9. For ad-\nditional details beyond what is presented here, we\nrefer readers to De Cao et al. (2021).\nSymbol Glossary\nfθ Language Model\ngφ Learned optimizer\nxi Main Input\nˆyi LM output on xi\ny∗\ni Desired output\n∇θL(xi,y∗\ni) Gradient of LM\nUpdate(xi,ˆyi,y∗\ni,θ) Update one LM belief\nL(φ; xi,ˆyi,y∗\ni,θ) Belief update objective for xi\nLSequential(φ; D,θt) Sequential objective (SLAG)\nK # gradient steps in Update(·)\nr # beliefs updated in LSequential\nTable 9: Symbol descriptions for the learned optimizer.\nAt a high level, gφ ﬁrst encodes an input xi and\nrequested prediction change into a vector h, then\nprocesses hinto two low-rank matrices Aand B\nthat are used to transform the model gradient on xi,\n∇θL(xi,y∗\ni). For Transformer models, the method\nedits only attention and feed-forward weights, so\nall model gradients match the shape of an associ-\nated weight matrix of shape d1 ×d2. Formally, a\nnew model θ∗is obtained using a learned optimizer\ngφ as follows:\nh= LSTM([x; ˆy; y∗])\n{u,v,γ,δ }= {MLPi(h)}4\ni=1\nA= softmax(u)vT\nB = softmax(γ)δT\nη= σ(MLP(h))\nθ∗= θ+ η(A◦∇θL(xi,y∗\ni) + B)\nwhere φconsists of all LSTM and MLP parameters.\nTraining Algorithm. The learned optimizer ob-\njective is optimized w.r.t. φwith AdamW through\na standard procedure of randomly sampling mini-\nbatches without replacement (Loshchilov and Hut-\nter, 2019). Within each batch, one datapoint is\nrandomly selected as the Main Input, and the re-\nmaining points are used as DR. To obtain update\nlabels {y∗\ni}n\ni=1, we always use the opposite class\nin binary classiﬁcation. For sequence-to-sequence\ntasks, we use the correct label when ˆyi is incorrect,\nand when ˆyi is correct, we randomly select another\nlabel from the training data. This choice is in con-\ntrast to De Cao et al. (2021) and Mitchell et al.\n(2021), who use samples from the model beam\nsearch as update labels for all points.\nTask Model\nOptimizer Backprop\nStop Gradient\nSequential Backprop Graph\nFigure 4: The backpropagation graph for sequential model updates.\nB Additional Training Details\nB.1 Compute Costs.\nLearned optimizer memory. The hypernetwork\nhas 92m trainable parameters for RoBERTa-base\n(which is 125m parameters), and 105m param-\neters for BART-base (which is 139m parame-\nters). To increase training efﬁciency, we limit\nhow far into the task model history we backprop-\nagate. As shown in Fig. 4, when backpropagat-\ning through task model parameters θt = θt−1 +\nUpdate(xi,ˆyi,y∗\ni,θt−1; φ), we continue backprop-\nagating through Update(xi,ˆyi,y∗\ni,θt−1) but not\nθt−1, which is also dependent on φ. That is, we ap-\nply a stop-gradient function to θt−1. This way, we\ncompute the derivative ∇φUpdate(xi,ˆyi,y∗\ni,θt; φ).\nonly once for each t, rather than recomputing these\ngradients for all subsequent time steps. These\nchoices allow the memory use of our training algo-\nrithm to remain constant in r. We make the same\nchoice for our Klooped steps in a single applica-\ntion of the Update function, so the gradient for the\nupdate at step kdepends only on gφ(xi,ˆyi,y∗\ni,θ(k))\nand not θ(k−1). See Fig. 5 for a graph of memory\nuse depending on rand k.\nExperiment runtimes. We now give runtimes\nfor experiments in the paper. Building the belief\ngraphs takes 25 hours for FEVER ( n = 10,444)\nand 17.5 hours for LeapOfThought ( n = 8642 )\non an NVIDIA RTX 2080 GPU. Computing sum-\nmary statistics for graphs takes 3 hours on FEVER\nand 3 hours for LeapOfThought for statistics be-\nsides Update-Transitivity. We compute Update-\nTransitivity for LeapOfThought with a subset of\n4000 points, which takes 45 hours.\nAll other experiments are run on a NVIDIA\nV100 32GB GPU. Training the task models takes\n7 minutes for LeapOfThought, 45 minutes for\nFEVER, 4 hours for zsRE, and 10 hours for Wiki-\ndata5m. Training the learned optimizer with r= 1\ntakes 2.3 hours for LeapOfThought, 5 hours for\nFEVER, 9.5 hours for zsRE, and 16 hours for\nWikidata5m. Training the learned optimizer with\nr= 10 takes 53 minutes for LeapOfThought, 2.9\nhours for FEVER, 7 hours for zsRE, and 12.5 hours\nfor Wikidata5m. Computing update statistics with\nthe off-the-shelf optimizers with r= 1 takes 4 min-\nutes for LeapOfThought, 30 minutes for FEVER,\n2.3 hours for zsRE, and 3.9 hours for Wikidata5m.\nWith r = 10 , the baselines require 1 minute for\nLeapOfThought, 15 minutes for FEVER, 54 min-\nutes for zsRE, and 1.8 hours for Wikidata5m. Total\nruntimes for each experiment should take into ac-\ncount multiple conditions and multiple seeds of\neach model being run.\nB.2 Hyperparameters and Objective Terms.\nTraining hyperparameters. We ﬁt our RoBERTa-\nbase and BART-base task models to their respec-\ntive datasets with the following hyperparameters:\nWe train for 10 epochs on the binary tasks, and\n20 for the sequence-to-sequence tasks. When pre-\ndicting with BART-base, we use a beam search\nwith width 5. In each case, we use AdamW from\ntorch.optim with a LR of 1e-5 and weight de-\ncay of 1e-4. We select the best model according\nto the best dev set accuracy, checkpointing after\neach training epoch. The learned optimizers are\noptimized with AdamW, using a learning rate of\n3e-4 and weight decay of 0. We train the learned\noptimizer for 5 epochs on each dataset except for\nLeapOfThought, which we train for 10 epochs\ngiven its smaller size. The learned optimizers are\nalso selected based on dev set performance, with\ncheckpointing after each training epoch. Their se-\nlection criterion is a raw average of Update Success\nRate (averaged over each kind of data), Retain Rate\n(Local Neutral) and ∆-Acc, with terms dropped\nDataset rtest k Objective\nFEVER 1 5 Main\n10 1 Main\nLeapOfThought 1 5 Main\n10 1 Main\nzsRE 1 5 Main\n10 5 Main\nWikidata5m 1 5 Main+Para\n10 5 Main+Para\nTable 10: Final hyperparameters and objective terms of\nthe learned optimizer for each task.\nRelation % Test Data\nPlace of Birth 11.00\nAward Received 11.00\nCause of Death 5.66\nPlace of Death 11.00\nPlace of Burial 8.33\nEducated At 11.00\nChild 11.00\nOccupation 11.00\nSpouse 11.00\nSibling 9.01\nTable 11: Wikidata relations and their proportion of the\ntest data.\nwhen they cannot be computed given the available\ndata. Note that dev epochs with zsRE and Wiki-\ndata5m are fairly slow, so in order to speed up our\nexperiments we compute dev epochs with a subset\nof 4000 dev points.\nLearned optimizer. We give the ﬁnal hyperparam-\neter and objective terms used in each experiment in\nTable 10. Our objective ablation is given in 17, and\nwe select the best performing condition for each\ndataset according to dev set performance, using the\nsame selection criterion outlined previously. We\nkeep all weight coefﬁcients λi equal rather than\ntuning them. Main refers to the ﬁrst term in Eq.\n1, plus the KL term with random data. We use\nKtrain ≤5 for all experiments. For results across\nKvalues on zsRE, see Fig. 9.\nBaseline update method. We tune a baseline off-\nthe-shelf optimizer separately for each dataset, us-\ning rtest = 1 . Our performance criterion is the\nsame as with learned optimizers, a raw average of\nUpdate Success Rate (averaged over each kind of\ndata), Retain Rate (Local Neutral) and ∆-Acc. The\ngrid search is over the following parameters: The\noff-the-shelf optimizers are from torch.optim\nand include {AdamW, SGD, and RMSProp} with\ndefault arguments (except for the learning rate).\nWe consider a number of maximum steps in {5,\n10, 100}. The learning rates we consider depend\nDataset Optimizer LR Num. Steps\nFEVER AdamW 1e-6 100\nLeapOfThought SGD 1e-2 100\nzsRE SGD 1e-1 10\nWikidata5m SGD 1e-1 10\nTable 12: Final hyperparameters of the baseline update\nmethod for each task.\non the optimizer: {1e-4, 1e-5, 1e-6} for AdamW,\n{1e-4, 1e-5, 1e-6} for RMSProp, and {1e-1, 1e-2,\n1e-3} for SGD. The LR ranges were selected af-\nter some initial manual exploration of the space.\nOur ﬁnal hyperparameter values are shown in Ta-\nble 12 for each dataset. For comparison, De Cao\net al. (2021) use RMSProp with 100 update steps.\nThe LR for zsRE and Wikidata5m may seem quite\nhigh, but this is the condition that actually does the\nleast damage to the model’s accuracy on other data,\n∆-Acc. The baseline optimizes all of the train-\nable parameters in the language model, unlike the\nlearned optimizer which optimizes only attention\nand feedforward weights for purposes of parameter\nefﬁciency.\nB.3 Wikidata5m Additional Details.\nWe construct four paraphrases per Main Input by\nselecting from a set of alternative phrasings for the\nentity and relation in the Main Input. The syntax\nfor each paraphrase follows the same simple tem-\nplate as the Main Input, in contrast to zsRE where\nsyntax differs between paraphrases. A couple de-\ntails remain. Some relations are one-to-many, and\ntherefore we accumulate valid completing entities\nfrom the data as possible answers; later we com-\npute accuracy as an exact match with any possible\nanswer. All 10 relations appear in each split of the\ndata. Only 33.80% and 37.18% of the entities in\nthe dev and test splits are seen in the training data,\nthough we do not ﬁnd that models perform better\non entities seen in training.\nB.4 LeapOfThought Additional Details\nThe LeapOfThought dataset consists of a fact and a\nclaim for each datapoint, where the truth of the fact\nimplies that the claim has label yi (True/False). All\nof the facts in the data are true, while half of the\nclaims are true and half are false. When training\nthe learned optimizer, we treat the the facts as the\nMain Input when training the learned optimizer\nand claims as entailed data. When training the\nTrue/False classiﬁer, we ﬁt to the claims, for which\ntest accuracy is 83.65 ( ±1.05). This seems to\nK\nK\n0\n10\n20\n30\n1 2 4 6 8 10\nr\nMemory Used (GB)\nMemory Usage byr\nFigure 5: Training memory usage in terms of K and r hyperparameters in our implementation, for a learned\noptimizer trained for a BART-base model on zsRE, using a batch size of 16. For comparison, the orange dashed\nline shows the memory use of training the BART-base model on zsRE, using the same batch size. Our use of the\nstop-gradient function limits the growth of runtime and memory w.r.t. both K and r. By accumulating gradients\nacross points, memory w.r.t. r is kept constant. The same trick could be applied to the K looped gradient steps\ninside the Update function, at the trade-off of backpropagating Ktimes per point rather than one time.\nOurs De Cao et al. (2021) Mitchell et al. (2021)\nUpdate Success Rate (Main Input) Success rate Edit success\nUpdate Success Rate (Paraphrase) Equivalence accuracy Edit success\nUpdate Success Rate (Entailed Data) - -\nRetain Rate (Local Neutral) - -\nRetain Rate (All Data) Retain accuracy -\n∆-Acc (All Data) Performance deterioration Drawdown\nTable 13: A glossary of terms used in work on model update methods. Note metrics are not always calculated\nin exactly the same way. For instance, Performance deterioration is a ratio in accuracies rather than difference in\naccuracies, and edit success from Mitchell et al. (2021) combines two metrics in our case. The performance metric\nin Zhu et al. (2020) is an average of Update Success Rate (Main Input) and ∆-Acc.\ngeneralize well to the facts, as test accuracy here is\n93.66 (±0.87), although as the low contrapositive\naccuracy suggests (Table 3), the model seems to be\ntoo prone to predicting true for this data.\nSince very few of the Main Inputs are predicted\nas false, we run into a small dilemma when ﬁt-\nting the learned optimizer with the use of the en-\ntailed data objective term. The entailment between\nfact and claim only holds when the fact is true, so\nwe can only compute the objective when updat-\ning a point from false to true. This ends up being\nless than 10% of the training data. We ultimately\nchoose to oversample points that ﬁt this descrip-\ntion during training of the learned optimizer, which\nallows the learned optimizer to fully ﬁt to the en-\ntailed data. Also note that during learned optimizer\ntraining, we include Entailed Data from other data\npoints besides the Main Inputin the KL term in Eq.\n1, and we measure ∆-Acc using both Main Inputs\nand Entailed Data.\nC Noise in Datasets\nWe brieﬂy document some shortcomings of each\ndataset, with reference to examples in Table 15.\nFEVER. Some claims are slightly vague or am-\nbiguous when taken on their own. For instance\n“Doug Ducey was the CEO of Cold Stone Cream-\nery and offered many opportunities to new hires\"\nis rated True, though this will depend heavily on\nwhat one thinks “many opportunities\" means. Sim-\nilar whether or not “L.A. Guns is a tattoo shop\"\ndepends on which “L.A. Guns\" one is referring to,\nthe tattoo shop or metal band. Of course, this is a\ngeneric issue of language, and not unique to this\ndataset. Some inputs seem to be a matter of person\nopinion: “Los Angeles is known for its food\" is\nrated False.\nLeapOfThought. Many examples use an “is a\"\nrelation, producing sentences like “A sunlight is a\ngood health.\" This could be more false than true,\nbut it’s a fairly nonsensical statement to begin with.\nThere are also other nonsensical or vague examples\nin the data: ”A friar is the opposite of mineral\" is\nlabeled False. “A detective desires equal opportu-\nnity.\" is labeled True. It is not immediately clear\nwhat conditions would make these statements true\nor false.\nzsRE. Some questions invoke potentially one-to-\nmany or temporally dependent relations, though\nthere is only one ground-truth answer per ques-\ntion in this dataset. For instance, a paraphrase of\nthe question about Gifford Pinchot in Table 15 is:\nDataset Model Acc Paraphrase Cons ↑ Entailment Acc ↑ Contrapositive Acc ↑\nFEVER RoBERTa-base 78.29 (0.86) - - -\nLeapOfThought RoBERTa-base 93.66 (0.87) - 85.63 (1.08) 16.51 (2.71)\nzsRE BART-base 21.01 (0.64) 69.50 (1.09) - -\nWikidata5m BART-base 10.21 (0.59) 25.84 (0.53) - -\nTable 14: Model accuracy and belief metric results and for four datasets.\nDataset Data Type Input Label(s)\nzsRE\nMain Input What did Gifford Pinchot die of? {Leukemia}Paraphrase How did Gifford Pinchot die?\nMain Input Player Ali Kanaan plays for what team? {Sporting Al Riyadi Beirut}Paraphrase What team is Ali Kanaan associated with?\nWikidata5m\nMain Input Margarita Nolasco Armas has relation ‘place\nof birth’ to {Orizaba, Veracruz; Orizaba;\netc.}Paraphrase SusunW/Margarita Nolasco Armas has rela-\ntion ‘born at’ to\nLocal Neutral Margarita Nolasco Armas has relation ‘place\nof death’ to\nMexico City; Ciudad de Mexico;\netc.\nMain Input Mary Good has relation ‘award received’ to {Garvan-Olin Medal; Arkansas\nWomen’s Hall of Fame; etc.}Paraphrase Mary Lowe Good has relation ‘winner of’ to\nLocal Neutral Mary Good has relation ‘educated at’ to {The University of Arkansas; U\nArkansas; etc.}\nFEVER Main Input Tardigrades are also known as space bears. True\nMain Input The Lion belongs to the genus Vulpes. False\nLeapOfThought\nMain Input A viper is a vertebrate. True\nEntailed Data A viper has a brain. True\nMain Input A amaranth is a herb. True\nEntailed Data A amaranth has a nose. False\nTable 15: Example datapoint from each dataset, and auxiliary data that accompanies the Main Input.\n\"What disease did Gifford Pinchot have?\" A per-\nson might have had many diseases over their life\nwhich could all be valid responses. The answer is\nespecially ambiguous for spatial relations, where a\nvalid answer might refer to a city, region, country,\nprovince, or continent.\nWikidata. Aliases sometimes vary greatly even\nas they refer to the same person, or they are sim-\nply noisy. For example, as shown in Table 15,\n“SusunW\" appears in an entity name, but this is\nactually a username of someone who contributed\nto the Wikipedia article for Margarita Nolasco Ar-\nmas. Meanwhile, other aliases for J.R.R Tolkien\ninclude “Tolkienian\" and “Mabel Sufﬁeld,\" his\nmother. Rephrasings of relations might also create\nconfusing inputs, e.g. switching “child\" with “has\nkids,\" “daughter\", or “son.\" Similar to zsRE, some\nrelations are also one-to-many and temporally de-\npendent (like occupation), though we hope that\nby using many valid answers we circumvent this\nissue to some extent when calculating prediction\ncorrectness.\nD Metric Computation and Bootstrap\nDetails\nMetric computation. The only computationally\ndifﬁcult metric to calculate is ∆-Acc, which re-\nquires computing the updated language model’s\naccuracy on other data after every single belief up-\ndate. We randomly sample other data after every\nupdate for this purpose, using n = 30 points for\nzsRE and Wikidata5m and n = 200 points for\nFEVER and LeapOfThought. We ensure that all\nevaluation data is used at some point during this\nsampling by preferentially selecting data that has\nbeen infrequently selected before. We note that\nparaphrase consistency is easy to evaluate for a\nsmall number of paraphrases per datapoint, as we\nhave for both zsRE and Wikidata5m. Additionally,\non LeapOfThought, we compute ∆-Acc using both\nMain Inputs and Entailed Data.\nUpdate-Transitivity caveat. The % Update-\nTransitivity metric represents the answer to the\nquestion: if updating belief A changes belief B,\nand updating belief B changes belief C, what pro-\nportion of the time does updating A change C?\nFEVER ZSRE\n1 2 4 6 8 10 1 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nrtest\nUpdate Success Rate\nMethod by rtrain\nBaseline\nr=1\nr matches test\nAblation by r\nFigure 6: Ablation across values of rfor training and testing. On zsRE, our method outperforms the baseline when\nrtest = 10, and the gap is likely to increase as rtest rises further. When using a non-sequential objective from past\nwork, performance declines drastically as rtest rises.\nWe would treat this as a normative metric that we\nhope to maximize, except we do not know in gen-\neral whether there is a confounding belief D that\ndetermines the relationship between B and C. If\nchanging A also changed a confounding belief D,\nthen we might not be able to expect that C should\nchange too. That said, when we have no reason to\nthink there are such confounding beliefs, we would\nexpect a logically consistent model to display 100%\nUpdate-Transitivity of their beliefs. In Fig. 3, for\ninstance, we see no reason to suspect there are con-\nfounding beliefs for the relationship between the\ndate Bessie Smith died and the writer of Despicable\nMe 2, and therefore we would expect that updat-\ning the belief about what album Hot Right Now is\non would change the belief in Despicable Me 2’s\nauthorship (which it does).\nBootstrap computation. We account for sample\nand seed variance by block bootstrap (Efron and\nTibshirani, 1994). When there is a single statistic\nper data point, like Main Input Update Success, we\nform a matrix of shape n×sfor ndata points and\nsmodel seeds (where the seed was used for both\ntask model training and learned optimizer train-\ning). We then resample rows and columns of this\nmatrix 10,000 times, which was sufﬁcient for con-\nvergence. When we perform hypothesis tests for\nthe difference in statistics between conditions, we\npair the data points by using the same rows of this\nmatrix at each step of the bootstrap (i.e. we conduct\npaired tests). For metrics involving multiple data\npoints per Main Input, like paraphrases or other\nrandom data, we make a simplifying assumption\nwhere we do not resample the multiple data points\nbut just compute the average metric for those data\npoints and treat that as the ground-truth statistics\nUpdate Success Rate ∆-Acc\nDesired Label Main Input Paraphrases All Data\nBeam Label 91.19 (0.5) 92.07 (0.8) -0.39 (0.1)\nHard Label 94.46 (0.7) 94.45 (0.7) -0.24 (0.1)\nTable 16: Update metrics by optimizer training labels.\nfor the Main Input. We explored using a full 3-\ndimensional bootstrap, where we resample among\nthese extra datapoints by constructing a matrix of\nshape n×s×n, but it was quite slow and gave\nsimilar results to the block bootstrap.\nE Additional Results\nAblation across num. sequential steps. Fig.\n6 shows the results for an ablation across rtest\nusing two kinds of learned optimizers: SLAG 1,\nwhere rtrain = 1 , and a SLAG condition where\nrtrain = rtest. It is critical to the success of learned\noptimizers to train them to update points sequen-\ntially when this is a desired application. Further,\nsequential updating with sequence prediction tasks\nis the only setting where we see learned optimizers\noutperform baselines across all relevant metrics.\nChoosing training labels for learned optimizers.\nIn early experiments, we found that it is beneﬁcial\nto use all data points (including correctly predicted\npoints) as Main Inputs during training, rather than\nrestricting training to only incorrectly predicted\npoints. We still focus on correcting wrong outputs\nat test time. But so we must select what label to\nuse during optimizer training. To get a Hard Label,\nwe use the correct label for incorrectly predicted\npoints, and for correctly predicted points, we sim-\nply draw a label randomly from the labels in the\nObjective Term Ablation Update Success Rate Retain Predictions ∆ Acc\nDataset Objective Main Input Paraphrases Entailed Data Local Neutral All Data All Data\nFEVER Main 100 (0.0) - - - 98.27 (0.1) -0.15 (0.1)\n(no KL) 100 (0.0) - - - 40.42 (0.6) -27.19 (1.2)\nLeapOfThought Main 100 (0.0) - 76.43 (5.3) - 96.84 (0.3) -1.22 (0.8)\n+Ent 100 (0.0) - 71.87 (5.3) - 96.52 (0.3) -0.40 (0.8)\nzsRE Main 94.46 (0.4) 94.44 (0.7) - - 81.96 (0.4) -0.24 (0.1)\n+Para 93.75 (0.4) 94.41 (0.7) - - 75.24 (0.5) -0.42 (0.2)\nWikidata5m\nMain 88.67 (0.7) 64.12 (0.7) - 49.78 (1.0) 71.04 (0.5) -1.54 (0.3)\n+Para 87.46 (0.7) 81.06 (0.7) - 47.15 (1.0) 63.02 (0.6) -1.55 (0.3)\n+LN 87.73 (0.7) 59.75 (0.7) - 60.49 (1.0) 72.69 (0.6) -1.57 (0.3)\n+Para+LN 87.02 (0.7) 81.18 (0.7) - 56.86 (1.0) 68.42 (0.6) -1.65 (0.3)\nTable 17: Belief update results by the objective terms used for the learned optimizer. We do not bold any numbers\nbased on statistical signiﬁcance. For tuning purposes we select whichever condition achieves the higher selection\ncriterion without testing for statistical signiﬁcance.\ntraining data. The alternative Beam Label condi-\ntion uses a sample from the model’s beam search\nfor a data point, as done in past work (De Cao\net al., 2021; Mitchell et al., 2021). We show up-\ndate metrics for zsRE split by the desired label in\nTable 16. If one’s goal is to ﬁx wrong model out-\nputs, then it is much better to use either the correct\nlabel or a random label as the desired model out-\nput during training rather than a sample from the\nmodel’s beam search. Update success improves by\n3.27 (±0.65; p<1e−4) points for the Main Input\nand 2.38 (±1.05; p<1e−4) for Paraphrases, while\n∆-Acc rises by 0.15 (±0.18; p=.09).\nWhich beliefs are hard to update? We hypothe-\nsize that beliefs will be easier to update when they\nare more belief-like to begin with. We principally\nmeasure this via the correlation between update suc-\ncess rate and a belief’s consistency on paraphrases\nbefore the update, for our learned optimizer in a\nsingle-update setting (r= 1). Surprisingly, we ob-\nserve no relationship between update success and\nthe belief consistency. The correlation between\nconsistency and update success is near 0 for both\nzsRE (ρ = −.027) and Wikidata5m ( ρ = .013);\nsee Fig. 7 for a plot of the relationship. So it ap-\npears that the learned optimizer can update model\nbeliefs independently of how belief-like they are to\nbegin with. We would also be interested in consid-\nering consistency under entailment, but the update\nsuccess rate on LeapOfThought is already 100%,\nso there is no variance to explain.\nLearning curve. In Fig. 8 we show the learning\ncurve of a learned optimizer trained with SLAG\non zsRE. The Main Input Update Success Rate\nsteadily rises as a function of the training set size.\nZSRE\nWikidata5m\n0.00 0.25 0.50 0.75 1.00\n0.85\n0.90\n0.95\n1.00\n0.85\n0.90\n0.95\n1.00\nPre−Update Consistency\nUpdate Success Rate\nWhich Beliefs Are Hard to Update?\nFigure 7: Beliefs are neither easier nor harder to update\ndepending on their consistency beforehand.\n85\n90\n95\n100\n103 103.5 104 104.5 105\nn\nMain Input Update Success\nLearning Curve for zsRE\nFigure 8: Main Input Update Success Rate across train-\ning set sizes, using SLAG on zsRE.\nAblation by num. update steps. Fig. 9 shows the\nresults of an ablation across values of K using a\nlearned optimizer trained using SLAG with r= 1\non zsRE. Main Input Update Success rises by over\nthree points by increasing Ktest from 1 to at least\n5. Using a value of Ktrain that matches Ktest gives\na further increase of about 0.5 points.\n0.90\n0.92\n0.94\n0.96\n1 2 4 6 8 10\nK test\nUpdate Success Rate\nTraining Obj. (K train)\n1\nMatches Test\nAblation by K\nFigure 9: Ablation across values of K for training and\ntesting, using SLAG on zsRE. It is useful to train the\noptimizer using the value of Kit will use at test time.\nAsylum Records is an English\n record label.\n[y: false]\nThe New Orleans Pelicans\n play in the Eastern Conference\n of the NBA.\n[y: false]\nTelemundo is a English-language\n television network.\n[y: false]\nNew Orleans Pelicans compete\n in the NBA.\n[y: true]\nJohn Deighton worked in California.\n[y: true]\nVictoria (Dance Exponents\n song) was released in the\n Southern Hemisphere in 1982.\n[y: true]\nCarlos Santana is a US president.\n[y: false]\nRichard Dawkins has yet to\n appear on the internet.\n[y: false]\nBermuda Triangle is in the\n western part of the Himalayas.\n[y: false]\nEmma Watson was born.\n[y: true]\nHarold Macmillan was born\n on February 20, 1894.\n[y: false]\nFilming for Boyhood was stopped\n between 2002 and 2013.\n[y: false]\nCHiPs is an American comedy\n film.\n[y: true]\nStarrcade was eventually\n broadcast via pay-per-view\n umbrella.\n[y: false]\nCroatia has a king.\n[y: false]\nSaturn Corporation is also\n known as Toyota LLC.\n[y: false]\nBasildon is far away from\n England.\n[y: false]\nThe Cincinnati Kid is a boy.\n[y: false]\nParamore formed in 2007.\n[y: false]\nXHamster produces online\n content.\n[y: true]\nFigure 10: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at\nleast one another node.\nHumphrey Bogart was ranked\n greatest male star of Classic\n American cinema.\n[y: true]\nRachel Green appeared in\n every episode of Friends\n until the final episode in\n 2002.\n[y: false]\nAngela Bassett is alive.\n[y: true]\nColin Kaepernick became\n a starter in the National\n Football League.\n[y: true]\n1978 is Ian Brennan's year\n of birth.\n[y: true]\nA Floppy disk is composed\n of a thin and flexible magnetic\n transmission medium.\n[y: true]\nSaturn is only an asteroid.\n[y: false]\nDan O'Bannon died on December\n 17th, 2009.\n[y: true]\nBeaverton, Oregon's city\n center is in decline.\n[y: false]\nMargaret Thatcher was the\n most senior politician within\n the Conservative Party in\n the UK in 1975.\n[y: true]\nStarrcade was originally\n broadcast via television.\n[y: true]\nTaylor Lautner appeared\n in The Bernie Mac Show in 2001.\n[y: false]\nI Kissed a Girl was only recorded\n by Donald Trump.\n[y: false]\nJulianne Moore created the\n television series As the\n World Turns.\n[y: false]\nHighway to Heaven is an American\n television series.\n[y: true]\nDan O'Bannon work was primarily\n science fiction and horror,\n serving as a screenwriter\n and director.\n[y: true]\nSidse Babett Knudsen graduated\n on November 22nd, 1968.\n[y: false]\nAleister Crowley was an English\n citizen.\n[y: true]\nMagic Johnson was a tap dancer.\n[y: false]\nQueen (band) is a Canadian\n rock band.\n[y: false]\nFigure 11: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at\nleast one another node.\nOn February 2, 2013, Chris\n Kyle died.\n[y: true]\nThe Mirny (sloop-of-war)\n was a ship without allegiance.\n[y: false]\nSt. Anger was released by\n Sub Pop Records.\n[y: false]\nKnocked Up is a work of art.\n[y: true]\nMel B had a career.\n[y: true]\nAustralia (2008 film) production\n took place in Bowen.\n[y: true]\nDaag is a home.\n[y: false]\nHarold Macmillan was born\n on February 20, 1894.\n[y: false]\nThe Chrysler Building has\n yet to be surpassed in height.\n[y: false]\nHeavy Metal music was developed\n in the early 1970's.\n[y: true]\nKuching is a city in Singapore.\n[y: false]\nJames VI and I was a major advocate\n of a single parliament for\n Scotland and England.\n[y: true]\nCamden, New Jersey is a large\n human settlement.\n[y: true]\nDerek Hough barely starred\n in Make Your Move.\n[y: false]\nChile is a country.\n[y: true]\nA River Runs Through It has\n lost every Academy Award.\n[y: false]\nNatural Born Killers was\n based upon Tarantino's original\n screenplay without revision.\n[y: false]\nThe Lincoln-Douglas debates\n happened in Quincy, Illinois.\n[y: true]\nCarlos Santana is a musician.\n[y: true]\nDespicable Me 2 was produced\n by a company.\n[y: true]\nFigure 12: A random subgraph of the belief graph for\nFEVER. Note all nodes actually are connected to at\nleast one another node."
}