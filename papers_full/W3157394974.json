{
    "title": "HOTR: End-to-End Human-Object Interaction Detection with Transformers",
    "url": "https://openalex.org/W3157394974",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2296982394",
            "name": "Kim Bumsoo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3174721655",
            "name": "Lee, Junhyun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2258364691",
            "name": "Kang, Jaewoo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3031732907",
            "name": "Kim, Eun-Sol",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223192833",
            "name": "Kim, Hyunwoo J.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1551928752",
        "https://openalex.org/W3101631402",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W3095753865",
        "https://openalex.org/W2990599624",
        "https://openalex.org/W2981045288",
        "https://openalex.org/W3035598501",
        "https://openalex.org/W3104264505",
        "https://openalex.org/W2962844592",
        "https://openalex.org/W3044216181",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W3034895839",
        "https://openalex.org/W3034934229",
        "https://openalex.org/W2982147439",
        "https://openalex.org/W2989506311",
        "https://openalex.org/W2953106684",
        "https://openalex.org/W2997150218",
        "https://openalex.org/W3106988096",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W3035047011",
        "https://openalex.org/W2963097937",
        "https://openalex.org/W2955882737",
        "https://openalex.org/W2964225075",
        "https://openalex.org/W3093781602",
        "https://openalex.org/W3034951775",
        "https://openalex.org/W2888096830",
        "https://openalex.org/W3009811369",
        "https://openalex.org/W3042546157",
        "https://openalex.org/W2982232158",
        "https://openalex.org/W2963480047",
        "https://openalex.org/W3102452576"
    ],
    "abstract": "Human-Object Interaction (HOI) detection is a task of identifying \"a set of interactions\" in an image, which involves the i) localization of the subject (i.e., humans) and target (i.e., objects) of interaction, and ii) the classification of the interaction labels. Most existing methods have indirectly addressed this task by detecting human and object instances and individually inferring every pair of the detected instances. In this paper, we present a novel framework, referred to by HOTR, which directly predicts a set of triplets from an image based on a transformer encoder-decoder architecture. Through the set prediction, our method effectively exploits the inherent semantic relationships in an image and does not require time-consuming post-processing which is the main bottleneck of existing methods. Our proposed algorithm achieves the state-of-the-art performance in two HOI detection benchmarks with an inference time under 1 ms after object detection.",
    "full_text": "HOTR: End-to-End Human-Object Interaction Detection with Transformers\nBumsoo Kim1,2 Junhyun Lee2 Jaewoo Kang2 Eun-Sol Kim1,† Hyunwoo J. Kim2,†\n1Kakao Brain 2Korea University\n{bumsoo.brain, eunsol.kim}@kakaobrain.com\n{meliketoy, ljhyun33, kangj, hyunwoojkim}@korea.ac.kr\nAbstract\nHuman-Object Interaction (HOI) detection is a task of\nidentifying “a set of interactions” in an image, which in-\nvolves the i) localization of the subject ( i.e., humans) and\ntarget (i.e., objects) of interaction, and ii) the classiﬁca-\ntion of the interaction labels. Most existing methods have\nindirectly addressed this task by detecting human and ob-\nject instances and individually inferring every pair of the\ndetected instances. In this paper, we present a novel frame-\nwork, referred by HOTR, which directly predicts a set of\n⟨human, object, interaction ⟩triplets from an image based\non a transformer encoder-decoder architecture. Through\nthe set prediction, our method effectively exploits the in-\nherent semantic relationships in an image and does not\nrequire time-consuming post-processing which is the main\nbottleneck of existing methods. Our proposed algorithm\nachieves the state-of-the-art performance in two HOI de-\ntection benchmarks with an inference time under 1 ms after\nobject detection.\n1. Introduction\nHuman-Object Interaction (HOI) detection has been for-\nmally deﬁned in [8] as the task to predict a set of ⟨human,\nobject, interaction⟩triplets within an image. Previous meth-\nods have addressed this task in an indirect manner by\nperforming object detection ﬁrst and associating ⟨human,\nobject⟩pairs afterward with separate post-processing steps.\nEspecially, early attempts (i.e., sequential HOI detectors [5,\n18, 17, 26]) have performed this association with a subse-\nquent neural network, thus being time-consuming and com-\nputationally expensive.\nTo overcome the redundant inference structure of se-\nquential HOI detectors, recent researches [30, 19, 12] pro-\nposed parallel HOI detectors. These works explicitly lo-\ncalize interactions with either interaction boxes ( i.e., the\ntightest box that covers both the center point of an object\n†corresponding authors\nFigure 1. Time vs. Performance analysis for HOI detectors on\nV-COCO dataset. HOI recognition inference time is measured by\nsubtracting the object detection time from the end-to-end inference\ntime. Blue circle represents sequential HOI detectors, orange cir-\ncle represents parallel HOI detectors and red star represents ours.\nOur method achieves an HOI recognition inference time of 0.9ms,\nbeing signiﬁcantly faster than the parallel HOI detectors such as\nIPNet [30] or UnionDet [12] (the comparison between parallel\nHOI detectors is highlighted in blue).\npair) [30, 19] or union boxes (i.e., the tightest box that cov-\ners both the box regions of an object pair) [12]. The lo-\ncalized interactions are associated with object detection re-\nsults to complete the ⟨human, object, interaction ⟩triplet.\nThe time-consuming neural network inference is replaced\nwith a simple matching based on heuristics such as dis-\ntance [30, 19] or IoU [12].\nHowever, previous works in HOI detection are still\nlimited in two aspects; i) They require additional post-\nprocessing steps like suppressing near-duplicate predictions\nand heuristic thresholding. ii) Although it has been shown\nthat modeling relations between objects helps object detec-\ntion [11, 2], the effectiveness of considering high-level de-\npendency for interactions in HOI detection has not yet been\nfully explored.\nIn this paper, we propose a fast and accurate HOI al-\ngorithm named HOTR (Human-Object interaction TRans-\narXiv:2104.13682v1  [cs.CV]  28 Apr 2021\nformer) that predicts a set of human-object interactions in a\nscene at once with a direct set prediction approach. We de-\nsign an encoder-decoder architecture based on transformers\nto predict a set of HOI triplets, which enables the model\nto overcome both limitations of previous works. First, di-\nrect set-level prediction enables us to eliminate hand-crafted\npost-processing stage. Our model is trained in an end-\nto-end fashion with a set loss function that matches the\npredicted interactions with ground-truth ⟨human, object,\ninteraction⟩triplets. Second, the self-attention mechanisms\nof transformers makes the model exploit the contextual rela-\ntionships between human and object and their interactions,\nencouraging our set-level prediction framework more suit-\nable for high-level scene understanding.\nWe evaluate our model in two HOI detection bench-\nmarks: V-COCO and HICO-DET datasets. Our proposed\narchitecture achieves state-of-the-art performance on two\ndatasets compared to both sequential and parallel HOI de-\ntectors. Also, note that our method is much faster than other\nalgorithms as illustrated in Figure 1, by eliminating time-\nconsuming post-processing through the direct set-level pre-\ndiction. The contribution of this work can be summarized\nas the following:\n• We propose HOTR, the ﬁrst transformer-based set pre-\ndiction approach in HOI detection. HOTR elimi-\nnates the hand-crafted post-processing stage of previ-\nous HOI detectors while being able to model the cor-\nrelations between interactions.\n• We propose various training and inference techniques\nfor HOTR: HO Pointers to associate the outputs of two\nparallel decoders, a recomposition step to predict a set\nof ﬁnal HOI triplets, and a new loss function to enable\nend-to-end training.\n• HOTR achieves state-of-the-art performance on both\nbenchmark datasets in HOI detection with an inference\ntime under 1 ms, being signiﬁcantly faster than previ-\nous parallel HOI detectors (5∼9 ms).\n2. Related Work\n2.1. Human-Object Interaction Detection\nHuman-Object Interaction detection has been initially\nproposed in [8], and has been developed in two main\nstreams: sequential methods and parallel methods. In\nsequential methods, object detection is performed ﬁrst\nand every pair of the detected object is inferred with a\nseparate neural network to predict interactions. Parallel\nHOI detectors perform object detection and interaction\nprediction in parallel and associates them with simple\nheuristics such as distance or IoU.\nSequential HOI Detectors: InteractNet [6] extended an\nexisting object detector by introducing an action-speciﬁc\ndensity map to localize target objects based on the human-\ncentric appearance, and combined features from individual\nboxes to predict the interaction. Note that interaction\ndetection based on visual cues from individual boxes often\nsuffers from the lack of contextual information.\nTo this end, iCAN [5] proposed an instance-centric\nattention module that extracts contextual features comple-\nmentary to the features from the localized objects/humans.\nNo-Frills HOI detection [9] propose a training and infer-\nence HOI detection pipeline only using simple multi-layer\nperceptron. Graph-based approaches have proposed frame-\nworks that can explicitly represent HOI structures with\ngraphs [24, 26, 4, 28, 21]. Deep Contextual Attention [29]\nleverages contextual information by a contextual attention\nframework in HOI. [28] proposes a heterogeneous graph\nnetwork that models humans and objects as different kinds\nof nodes. Various external sources such as linguistic\npriors [23, 31, 17, 4, 1, 32, 20] or human pose infor-\nmation [15, 33, 18, 9, 27, 33] have also been leveraged\nfor further improve performance. Although sequential\nHOI detectors feature a fairly intuitive pipeline and solid\nperformance, they are time-consuming and computation-\nally expensive because of the additional neural network\ninference after the object detection phase.\nParallel HOI Detectors: Attempts for faster HOI detec-\ntion has been also introduced in recent works as parallel\nHOI detectors. These works have directly localized inter-\nactions with interaction points [30, 19] or union boxes [12],\nreplacing the separate neural network for interaction predic-\ntion with a simple heuristic based matching with distance\nor IoUs. Since they can be parallelized with existing ob-\nject detectors, they feature fast inference time. However,\nthese works are limited in that they require a hand-crafted\npostprocessing stage to associate the localized interactions\nwith object detection results. This post-processing step i)\nrequires manual search for the threshold, and ii) generates\nextra time complexity for matching each object pairs with\nthe localized interactions (5∼9 ms).\n2.2. Object Detection with Transformers\nDETR [2] has been recently proposed to eliminate the\nneed for many hand-designed components in object detec-\ntion while demonstrating good performance. DETR infers\na ﬁxed-size set of N predictions, in a single pass through\nthe decoder, where N is set to be signiﬁcantly larger than\nthe typical number of objects in an image. The main loss\nfor DETR produces an optimal bipartite matching between\npredicted and ground-truth objects. Afterward, the object-\nspeciﬁc losses (for class and bounding box) are optimized.\nFigure 2. Overall pipeline of our proposed model. The Instance Decoder and Interaction Decoder run in parallel, and share the Encoder. In\nour recomposition, the interaction representations predicted by the Interaction Decoder are associated with the instance representations to\npredict a ﬁxed set of HOI triplets (see Fig.3). The positional encoding is identical to [2].\n3. Method\nThe goal of this paper is to predict a set of ⟨human, ob-\nject, interaction⟩triplets while considering the inherent se-\nmantic relationships between the triplets in an end-to-end\nmanner. To achieve this goal, we formulate HOI detection\nas set prediction. In this section, we ﬁrst discuss the prob-\nlems of directly extending the set prediction architecture for\nobject detection [2] to HOI detection. Then, we propose our\narchitecture HOTR that parallelly predicts a set of object\ndetection and associates the human and object of the inter-\naction, while the self-attention in transformers models the\nrelationships between the interactions. Finally, we present\nthe details of training for our model including Hungarian\nMatching for HOI detection and our loss function.\n3.1. Detection as Set Prediction\nWe ﬁrst start from object detection as set prediction with\ntransformers, then show how we extend this architecture to\ncapture HOI detection with transformers.\nObject Detection as Set Prediction. Object Detection has\nbeen explored as a set prediction problem by DETR [2].\nSince object detection includes a single classiﬁcation\nand a single localization for each object, the transformer\nencoder-decoder structure in DETR transforms N posi-\ntional embeddings to a set of N predictions for the object\nclass and bounding box.\nHOI Detection as Set Prediction. Similar to object\ndetection, HOI detection can be deﬁned as a set prediction\nproblem where each prediction includes the localization\nof a human region ( i.e., subject of the interaction), an\nobject region (i.e., target of the interaction) and multi-label\nclassiﬁcation of the interaction types. One straightfor-\nward extension is to modify the MLP heads of DETR to\ntransform each positional embedding to predict a human\nbox, object box, and action classiﬁcation. However, this\narchitecture poses a problem where the localization for\nthe same object needs to be redundantly predicted with\nmultiple positional embeddings ( e.g., if the same person\nworks on a computer while sitting on a chair, two different\nqueries have to infer redundant regression for the same\nhuman).\n3.2. HOTR architecture\nThe overall pipeline of HOTR is illustrated in Figure 2.\nOur architecture features a transformer encoder-decoder\nstructure with a shared encoder and two parallel decoders\n(i.e., instance decoder and interaction decoder). The\nresults of the two decoders are associated with using our\nproposed HO Pointers to generate ﬁnal HOI triplets. We\nwill introduce HO Pointers shortly after discussing the\narchitecture of HOTR.\nTransformer Encoder-Decoder architecture. Similar\nto DETR [2], the global context is extracted from the\ninput image by the backbone CNN and a shared encoder.\nAfterward, two sets of positional embeddings ( i.e., the\ninstance queries and the interaction queries) are fed into\nthe two parallel decoders ( i.e., the instance decoder and\ninteraction decoder in Fig. 2). The instance decoder trans-\nforms the instance queries to instance representations for\nobject detection while the interaction decoder transforms\nthe interaction queries to interaction representations for\nFigure 3. Conceptual illustration of how HO Pointers associates\nthe interaction representations with instance representations. As\ninstance representations are pre-trained to perform standard object\ndetection, the interaction representation learns localization by pre-\ndicting the pointer to the index of the instance representations for\neach human and object boxes. Note that the index pointer predic-\ntion is obtained in parallel with instance representations.\ninteraction detection. We apply feed-forward networks\n(FFNs) to the interaction representation and obtain a\nHuman Pointer, an Object Pointer, and interaction type,\nsee Fig. 3. In other words, the interaction representation\nlocalizes human and object regions by pointing the relevant\ninstance representations using the Human Pointer and\nObject Pointer (HO Pointers), instead of directly regressing\nthe bounding box. Our architecture has several advantages\ncompared to the direct regression approach. We found that\ndirectly regressing the bounding box has a problem when\nan object participates in multiple interactions. In the direct\nregression approach, the localization of the identical object\ndiffers across interactions. Our architecture addresses\nthis issue by having separate instance and interaction\nrepresentations and associating them using HO Pointers.\nAlso, our architecture allows learning the localization more\nefﬁciently without the need of learning the localization re-\ndundantly for every interaction. Note that our experiments\nshow that our shared encoder is more effective to learn HO\nPointers than two separate encoders.\nHO Pointers. A conceptual overview of how HO Pointers\nassociate the parallel predictions from the instance decoder\nand the interaction decoder is illustrated in Figure 3. HO\nPointers (i.e., Human Pointer and Object Pointer) contain\nthe indices of the corresponding instance representations of\nthe human and the object in the interaction. After the in-\nteraction decoder transforms Kinteraction queries to Kin-\nteraction representations, an interaction representation zi is\nfed into two feed-forward networks FFN h : Rd → Rd,\nFFNo : Rd → Rd to obtain vectors vh\ni and vo\ni, i.e.,\nvh\ni = FFNh(zi) and vo\ni = FFNo(zi). Then ﬁnally the Hu-\nman/Object Pointers ˆch\ni and ˆco\ni, which are the indices of the\ninstance representations with the highest similarity scores,\nare obtained by\nˆch\ni = argmax\nj\n(\nsim(vh\ni ,µj)\n)\n,\nˆco\ni = argmax\nj\n(\nsim(vo\ni,µj)\n)\n,\n(1)\nwhere µj is the j-th instance representation and\nsim(u,v) = u⊤v/∥u∥∥v∥.\nRecomposition for HOI Set Prediction. From the previ-\nous steps, we now have the following: i) N instance rep-\nresentations µ, and ii) K interaction representations z and\ntheir HO Pointers ˆch and ˆco. Given γ interaction classes,\nour recomposition is to apply the feed-forward networks\nfor bounding box regression and action classiﬁcation as\nFFNbox : Rd →R4, and FFN act : Rd →Rγ, respec-\ntively. Then, the ﬁnal HOI prediction for thei-th interaction\nrepresentation zi is obtained by,\nˆbh\ni = FFNbox(µˆch\ni\n) ∈R4,\nˆbo\ni = FFNbox(µˆco\ni ) ∈R4,\nˆai = FFNact(zi) ∈Rγ.\n(2)\nThe ﬁnal HOI prediction by our HOTR is the set of K\ntriplets, {⟨ˆbh\ni,ˆbo\ni,ˆai⟩}K\ni=1.\nComplexity & Inference time. Previous parallel methods\nhave substituted the costly pair-wise neural network\ninference with a fast matching of triplets (associating\ninteraction regions with corresponding human regions and\nobject regions based on distance [30] or IoU [12]). HOTR\nfurther reduces the inference time after object detection\nby associating K interactions with N instances, resulting\nin a smaller time complexity O(KN). By eliminating\nthe post-processing stages in the previous one-stage HOI\ndetectors including NMS for the interaction region and\ntriplet matching, HOTR diminishes the inference time by\n4 ∼8ms while showing improvement in performance.\n3.3. Training HOTR\nIn this section, we explain the details of HOTR training.\nWe ﬁrst introduce the cost matrix of Hungarian Matching\nfor unique matching between the ground-truth HOI triplets\nand HOI set predictions obtained by recomposition. Then,\nusing the matching result, we deﬁne the loss for HO\nPointers and the ﬁnal training loss.\nHungarian Matching for HOI Detection. HOTR predicts\nK HOI triplets that consist of human box, object box and\nbinary classiﬁcation for the atypes of actions. Each predic-\ntion captures a unique⟨human,object⟩pair with one or more\ninteractions. K is set to be larger than the typical number\nof interacting pairs in an image. We start with the basic cost\nfunction that deﬁnes an optimal bipartite matching between\npredicted and ground truth HOI triplets, and then show how\nwe modify this matching cost for our interaction represen-\ntations.\nLet Ydenote the set of ground truth HOI triplets and\nˆY = {ˆyi}K\ni=1 as the set of K predictions. As K is larger\nthan the number of unique interacting pairs in the image,\nwe consider Yalso as a set of size K padded with ∅ (no\ninteraction). To ﬁnd a bipartite matching between these two\nsets we search for a permutation of K elements σ ∈SK\nwith the lowest cost:\nˆσ= argmin\nσ∈SK\nK∑\ni\nCmatch(yi,ˆyσ(i)), (3)\nwhere Cmatch is a pair-wise matching cost between ground\ntruth yi and a prediction with index σ(i). However, since yi\nis in the form of ⟨hbox,obox,action⟩and ˆyσ(i) is in the form\nof ⟨hidx,oidx,action⟩, we need to modify the cost function\nto compute the matching cost.\nLet Φ : idx →box be a mapping function from ground-\ntruth ⟨hidx,oidx⟩to ground-truth ⟨hbox,obox⟩by optimal\nassignment for object detection. Using the inverse mapping\nΦ−1 : box →idx, we get the ground-truth idx from the\nground-truth box.\nLet M ∈Rd×N be a set of normalized instance repre-\nsentations µ′ = µ/∥µ∥∈ Rd, i.e., M = [µ′\n1 ...µ ′\nN]. We\ncompute ˆPh ∈RK×N that is the set of softmax predictions\nfor the H Pointer in (1) given as\nˆPh = ∥K\ni=1softmax((¯vh\ni )TM), (4)\nwhere ∥K\ni=1 denotes the vertical stack of row vectors and\n¯vh\ni = vh\ni /||vh\ni ||. ˆPo is analogously deﬁned.\nGiven the ground-truth yi = ( bh\ni,bo\ni,ai), ˆPh, and , ˆPo,\nwe convert the ground-truth box to indices by ch\ni =\nΦ−1(bh\ni) and co\ni = Φ−1(bo\ni) and compute our matching cost\nfunction written as\nCmatch(yi,ˆyσ(i)) = −α·1 {ai̸=∅}ˆPh[σ(i),ch\ni]\n−β·1 {ai̸=∅}ˆPo[σ(i),co\ni]\n+1 {ai̸=∅}Lact(ai,ˆaσ(i)),\n(5)\nwhere ˆP[i,j] denotes the element at i-th row and j-th col-\numn, and ˆaσ(i) is the predicted action. The action matching\ncost is calculated as Lact(ai,ˆaσ(i)) = BCELoss(ai,ˆaσ(i)).\nα and β is set as a ﬁxed number to balance the different\nscales of the cost function for index prediction and action\nclassiﬁcation.\nFinal Set Prediction Loss for HOTR.We then compute the\nHungarian loss for all pairs matched above, where the loss\nfor the HOI triplets has the localization loss and the action\nclassiﬁcation loss as\nLH =\nK∑\ni=1\n[\nLloc(ch\ni,co\ni,zσ(i)) + Lact(ai,ˆaσ(i))\n]\n. (6)\nThe localization loss Lloc(ch\ni,co\ni,zσ(i)) is denoted as\nLloc = −log\nexp(sim(FFNh(zσ(i)),µch\ni\n)/τ)\n∑N\nk=1 exp(sim(FFNh(zσ(i)),µk)/τ)\n−log exp(sim(FFNo(zσ(i)),µco\ni /τ)\n∑N\nk=1 exp(sim(FFNo(zσ(i)),µk)/τ)\n,\n(7)\nwhere τ is the temperature that controls the smoothness of\nthe loss function. We empirically found that τ = 0.1 is the\nbest value for our experiments.\nDeﬁning No-Interaction with HOTR. In DETR [2],\nmaximizing the probability of the no-object class for\nthe softmax output naturally suppresses the probability\nof other classes. However, in HOI detection the action\nclassiﬁcation is a multi-label classiﬁcation where each\naction is treated as an individual binary classiﬁcation.\nDue to the absence of an explicit class that can suppress\nthe redundant predictions, HOTR ends up with multiple\npredictions for the same ⟨human,object⟩pair. Therefore,\nHOTR sets an explicit class that learns the interactiveness\n(1 if there is any interaction between the pair, 0 otherwise),\nand suppresses the predictions for redundant pairs that\nhave a low interactiveness score (deﬁned as No-Interaction\nclass). In our experiment in Table. 3, we show that setting\nan explicit class for interactiveness contributes to the ﬁnal\nperformance.\nImplementation Details. We train HOTR with\nAdamW [22]. We set the transformer’s initial learn-\ning rate to 10−4 and weight decay to 10−4. All transformer\nweights are initialized with Xavier init [7]. For a fair\nevaluation with baselines, the Backbone, Encoder, and\nInstance Decoder are pre-trained in MS-COCO and frozen\nduring training. We use the scale augmentation as in\nDETR [2], resizing the input images such that the shortest\nside is at least 480 and at most 800 pixels while the longest\nside at most is 1333.\n4. Experiments\nIn this section, we demonstrate the effectiveness of\nour model in HOI detection. We ﬁrst describe the two\npublic datasets that we use as our benchmark: V-COCO\nand HICO-DET. Next, we show that HOTR successfully\ncaptures HOI triplets, by achieving state-of-the-art perfor-\nmance in both mAP and inference time. Then, we provide\na detailed ablation study of the HOTR architecture.\n4.1. Datasets\nTo validate the performance of our model, we evaluate\nour model on two public benchmark datasets: the V-COCO\n(Verbs in COCO) dataset and HICO-DET dataset. V-COCO\nis a subset of COCO and has 5,400 trainval images\nand 4,946 test images. For V-COCO dataset, we report\nthe AProle over 25 interactions in two scenarios AP #1\nrole and\nAP#2\nrole. The two scenarios represent the different scoring\nways for object occlusion cases. In Scenario1, the model\nshould correctly predict the bounding box of the occluded\nobject as [0,0,0,0] while predicting human bounding box\nand actions correctly. In Scenario2, the model does not\nneed to predict about the occluded object. HICO-DET [3]\nis a subset of HICO dataset and has more than 150K an-\nnotated instances of human-object pairs in 47,051 images\n(37,536 training and 9,515 testing) and is annotated with\n600 ⟨verb,object⟩interaction types. For HICO-DET, we\nreport our performance in the Default setting where we\nevaluate the detection on the full test set. We follow the\nprevious settings and report the mAP over three different\ncategory sets: (1) all 600 HOI categories in HICO (Full),\n(2) 138 HOI categories with less than 10 training instances\n(Rare), and (3) 462 HOI categories with 10 or more training\ninstances (Non-Rare).\n4.2. Quantitative Analysis\nFor quantitative analysis, we use the ofﬁcial evaluation\ncode for computing the performance of both V-COCO and\nHICO-DET. Table 1 and Table 2 show the comparison of\nHOTR with the latest HOI detectors including both se-\nquential and parallel methods. For fair comparison, the\ninstance detectors are ﬁxed by the parameters pre-trained\nin MS-COCO. All results in V-COCO dataset are evalu-\nated with the ﬁxed detector. For the HICO-DET dataset,\nwe provide both results using the ﬁxed detector and the\nﬁne-tuned detector following the common evaluation pro-\ntocol [1, 18, 10, 21, 4, 16, 12, 19].\nOur HOTR achieves a new state-of-the-art performance\non both V-COCO and HICO-DET datasets, while being\nthe fastest parallel detector. Table 1 shows our result\nin the V-COCO dataset with both Scenario1 and Sce-\nnario2. HOTR outperforms the state-of-the-art parallel\nHOI detector [30] in Scenario1 with a margin of 4.2mAP.\nMethod Backbone AP#1\nrole AP#2\nrole\nModels with external features\nTIN (RPDCD) [18] R50 47.8\nVerb Embedding [31] R50 45.9\nRPNN [33] R50 - 47.5\nPMFNet [27] R50-FPN 52.0\nPastaNet [17] R50-FPN 51.0 57.5\nPD-Net [32] R50 52.0 -\nACP [13] R152 53.0\nFCMNet [20] R50 53.1 -\nConsNet [21] R50-FPN 53.2 -\nSequential HOI Detectors\nVSRL [8] R50-FPN 31.8 -\nInteractNet [6] R50-FPN 40.0 48.0\nBAR-CNN [14] R50-FPN 43.6 -\nGPNN [24] R152 44.0 -\niCAN [5] R50 45.3 52.4\nTIN (RCD) [18] R50 43.2 -\nDCA [29] R50 47.3 -\nVSGNet [26] R152 51.8 57.0\nVCL [10] R50-FPN 48.3\nDRG [4] R50-FPN 51.0\nIDN [16] R50 53.3 60.3\nParallel HOI Detectors\nIPNet [30] HG104 51.0 -\nUnionDet [12] R50-FPN 47.5 56.2\nOurs R50 55.2 64.4\nTable 1. Comparison of performance on V-COCO test set.AP#1\nrole ,\nAP#2\nrole denotes the performance under Scenario1 and Scenario2 in\nV-COCO, respectively.\nTable 2 shows the result in HICO-DET in the Default\nsetting for each Full/Rare/Non-Rare class. Due to the noisy\nlabeling for objects in the HICO-DET dataset, ﬁne-tuning\nthe pre-trained object detector on the HICO-DET train set\nprovides a prior that beneﬁts the overall performance [1].\nTherefore, we evaluate our performance in HICO-DET\ndataset under two conditions: i) using pre-trained weights\nfrom MS-COCO which are frozen during training (denoted\nas COCO in the Detector column) and ii) performance\nafter ﬁne-tuning the pre-trained detector on the HICO-DET\ntrain set (denoted as HICO-DET in the Detector column).\nOur model outperforms the state-of-the-art parallel HOI\ndetector under both conditions by a margin of 4.1mAP and\n4mAP, respectively. Below, we provide a more detailed\nanalysis of our performance.\nHOTR vs Sequential Prediction. In comparative analysis\nwith various HOI methods summarized in Table 1 and 2,\nwe also compare the experimental results of HOTR with\nsequential prediction methods. Even though the sequential\nDefault\nMethod Detector Backbone Feature Full Rare Non Rare\nSequential HOI Detectors\nInteractNet [6] COCO R50-FPN A 9.94 7.16 10.77\nGPNN [24] COCO R101 A 13.11 9.41 14.23\niCAN [5] COCO R50 A+S 14.84 10.45 16.15\nDCA [29] COCO R50 A+S 16.24 11.16 17.75\nTIN [18] COCO R50 A+S+P 17.03 13.42 18.11\nRPNN [33] COCO R50 A+P 17.35 12.78 18.71\nPMFNet [27] COCO R50-FPN A+S+P 17.46 15.65 18.00\nNo-Frills HOI [9] COCO R152 A+S+P 17.18 12.17 18.68\nDRG [4] COCO R50-FPN A+S+L 19.26 17.74 19.71\nVCL [10] COCO R50 A+S 19.43 16.55 20.29\nVSGNet [26] COCO R152 A+S 19.80 16.05 20.91\nFCMNet [20] COCO R50 A+S+P 20.41 17.34 21.56\nACP [13] COCO R152 A+S+P 20.59 15.92 21.98\nPD-Net [32] COCO R50 A+S+P+L 20.81 15.90 22.28\nDJ-RN [15] COCO R50 A+S+V 21.34 18.53 22.18\nConsNet [21] COCO R50-FPN A+S+L 22.15 17.12 23.65\nPastaNet [17] COCO R50 A+S+P+L 22.65 21.17 23.09\nIDN [16] COCO R50 A+S 23.36 22.47 23.63\nFunctional Gen. [1] HICO-DET R101 A+S+L 21.96 16.43 23.62\nTIN [18] HICO-DET R50 A+S+P 22.90 14.97 25.26\nVCL [10] HICO-DET R50 A+S 23.63 17.21 25.55\nConsNet [21] HICO-DET R50-FPN A+S+L 24.39 17.10 26.56\nDRG [4] HICO-DET R50-FPN A+S 24.53 19.47 26.04\nIDN [16] HICO-DET R50 A+S 24.58 20.33 25.86\nParallel HOI Detectors\nUnionDet [12] COCO R50-FPN A 14.25 10.23 15.46\nIPNet [30] COCO R50-FPN A 19.56 12.79 21.58\nOurs COCO R50 A 23.46 16.21 25.62\nUnionDet [12] HICO-DET R50-FPN A 17.58 11.72 19.33\nPPDM [19] HICO-DET HG104 A 21.10 14.46 23.09\nOurs HICO-DET R50 A 25.10 17.34 27.42\nTable 2. Performance comparison in HICO-DET. The Detector column is denoted as ‘COCO’ for the models that freeze the object detectors\nwith the weights pre-trained in MS-COCO and ‘HICO-DET’ if the object detector is ﬁne-tuned with the HICO-DET train set. The each\nletter in Feature column stands for A: Appearance (Visual features), S: Interaction Patterns (Spatial Correlations [5]), P: Pose Estimation,\nL: Linguistic Priors, V: V olume [15].\nmethods take advantages from additional information while\nHOTR only utilize visual information, HOTR outperforms\nthe state-of-the-art sequential HOI detector [16] in both\nScenario1 and Scenario2 by 1.9 mAP and 4.1 mAP in\nV-COCO while showing comparable performance (with a\nmargin of 0.1 ∼0.52 mAP) in the Default(Full) evaluation\nof HICO-DET.\nPerformance on HICO-DET Rare Categories. HOTR\nshows state-of-the-art performance across both sequential\nand parallel HOI detectors in the Full evaluation for HICO-\nDET dataset (see Table. 2). However, HOTR underperforms\nthan baseline methods [16] in the Rare setting. Since this\nsetting deals with the action categories that has less than\n10 training instances, it is difﬁcult to achieve accuracy on\nthis setting without the help of external features. There-\nfore, most of the studies that have shown high performance\nin Rare settings make use of additional information, such\nas spatial layouts [5], pose information [18], linguistic pri-\nors [17], and coherence patterns between the humans and\nobjects [16]. In this work, our method is a completely\nvision-based pipeline but if we include the prior knowledge,\nwe expect further improvement in the Rare setting.\nTime analysis. Since the inference time of the object detec-\ntor network (e.g., Faster-RCNN [25]) can vary depending\non benchmark settings (e.g., the library, CUDA, CUDNN\nversion or hyperparameters), the time analysis is based on\nthe pure inference time of the HOI interaction prediction\nmodel excluding the time of the object detection phase\nfor fair comparison with our model. For detailed analysis,\nHOTR takes an average of 36.3ms for the backbone and\nencoder, 23.8ms for the instance decoder and interaction\ndecoder (note that the two decoders run in parallel), and\n0.9ms for the recomposition and ﬁnal HOI triplet inference.\nWe excluded the i/o times in all models including the time\nof previous models loading the RoI align features of Faster-\nRCNN (see Figure.1 for a speed vs time comparison).\nNote that our HOTR runs ×5 ∼ ×9 faster compared to\nthe state-of-the-art parallel HOI detectors, since an explicit\npost-processing stage to assemble the detected objects\nand interaction regions is replaced with a simple O(KN)\nsearch to infer the HO Pointers.\n4.3. Ablation Study\nMethod AP#1\nrole Default(Full)\nHOTR 55.2 23.5\nw/o HO Pointers 39.3 17.2\nw/o Shared Encoders 33.9 14.5\nw/o Interactiveness Suppression 52.2 22.0\nTable 3. Ablation Study on both V-COCO test set (scenario 1,\nAP#1\nrole ) and HICO-DET test set (Default, Full setting without ﬁne-\ntuning the object detector)\nIn this section, we explore how each of the components\nof HOTR contributes to the ﬁnal performance. Table 3\nshows the ﬁnal performance in the V-COCO test set after\nexcluding each components of HOTR. We perform all\nexperiments with the most basic R50-C4 backbone, and\nﬁx the transformer layers to 6 and attention heads 8 and\nthe feed-forward network dimension to d = 1024 unless\notherwise mentioned.\nWith vs Without HO Pointers. In HOTR, the interaction\nrepresentation localizes human and object region by\npointing the relevant instance representations using the\nHuman Pointer and Object Pointer (HO Pointers), instead\nof directly regressing the bounding box. We pose that\nour architecture has advantages compared to the direct\nregression approach, since directly regressing the bounding\nbox for every interaction prediction requires redundant\nbounding box regression for the same object when an object\nparticipates in multiple interactions. Based on the perfor-\nmance gap (55.2 →39.3 in V-COCO and 23.5 →17.2 in\nHICO-DET), it can be concluded that using HO Pointers\nalleviates the issue of direct regression approach.\nShared Encoder vs Separate Encoders. From the Fig. 2,\nthe architecture having separate encoders for each Instance\nand Interaction Decoder can be considered. In this ablation,\nwe verify the role of the shared encoder of the HOTR. In\nTable 3, it is shown that sharing the encoder outperforms\nthe model with separate encoders by a margin of 21.3mAP\nand 9.0mAP in V-COCO and HICO-DET, respectively.\nWe suppose the reason is that the shared encoder helps\nthe decoders learn common visual patterns, thus the HO\nPointers can share the overall context.\nWith vs Without Interactiveness Suppression. Unlike\nsoftmax based classiﬁcation where maximizing the prob-\nability for the no-object class can explicitly diminish\nthe probability of other classes, action classiﬁcation is\na multi-label binary classiﬁcation that treats each class\nindependently. So HOTR sets an explicit class that learns\nthe interactiveness, and suppresses the predictions for\nredundant pairs that have low probability. Table 3 shows\nthat setting an explicit class for interactiveness contributes\n3mAP to the ﬁnal performance.\n5. Conclusion\nIn this paper, we present HOTR, the ﬁrst transformer-\nbased set prediction approach in human-object interaction\nproblem. The set prediction approach of HOTR eliminates\nthe hand-crafted post-processing steps of previous HOI\ndetectors while being able to model the correlations\nbetween interactions. We propose various training and\ninference techniques for HOTR: HOI decomposition with\nparallel decoders for training, recomposition layer based\non similarity for inference, and interactiveness suppression.\nWe develop a novel set-based matching for HOI detection\nthat associates the interaction representations to point at\ninstance representations. Our model achieves state-of-\nthe-art performance in two benchmark datasets in HOI\ndetection: V-COCO and HICO-DET, with a signiﬁcant\nmargin to previous parallel HOI detectors. HOTR achieves\nstate-of-the-art performance on both benchmark datasets\nin HOI detection with an inference time under 1 ms, being\nsigniﬁcantly faster than previous parallel HOI detectors\n(5∼9 ms).\nAcknowledgments. This research was partly supported by\nthe Institute of Information & communications Technology Plan-\nning & Evaluation (IITP) grants funded by the Korea govern-\nment (MSIT) (No.2021-0-00025, Development of Integrated Cog-\nnitive Drone AI for Disaster/Emergency Situations), (IITP-2021-\n0-01819, the ICT Creative Consilience program), and National\nResearch Foundation of Korea (NRF2020R1A2C3010638, NRF-\n2016M3A9A7916996).\nReferences\n[1] Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava,\nand Rama Chellappa. Detecting human-object interactions\nvia functional generalization. In AAAI, pages 10460–10469,\n2020.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020.\n[3] Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia\nDeng. Learning to detect human-object interactions. In 2018\nieee winter conference on applications of computer vision\n(wacv), pages 381–389. IEEE, 2018.\n[4] Chen Gao, Jiarui Xu, Yuliang Zou, and Jia-Bin Huang. Drg:\nDual relation graph for human-object interaction detection.\nIn European Conference on Computer Vision , pages 696–\n712. Springer, 2020.\n[5] Chen Gao, Yuliang Zou, and Jia-Bin Huang. ican: Instance-\ncentric attention network for human-object interaction detec-\ntion. arXiv preprint arXiv:1808.10437, 2018.\n[6] Georgia Gkioxari, Ross Girshick, Piotr Doll ´ar, and Kaiming\nHe. Detecting and recognizing human-object interactions.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8359–8367, 2018.\n[7] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256, 2010.\n[8] Jitendra Gupta, Saurabh Malik. Visual semantic role label-\ning. arXiv preprint arXiv:1505.04474, 2015.\n[9] Tanmay Gupta, Alexander Schwing, and Derek Hoiem. No-\nfrills human-object interaction detection: Factorization, lay-\nout encodings, and training techniques. InProceedings of the\nIEEE International Conference on Computer Vision , pages\n9677–9685, 2019.\n[10] Zhi Hou, Xiaojiang Peng, Yu Qiao, and Dacheng Tao. Vi-\nsual compositional learning for human-object interaction de-\ntection. arXiv preprint arXiv:2007.12407, 2020.\n[11] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588–3597, 2018.\n[12] Bumsoo Kim, Taeho Choi, Jaewoo Kang, and Hyunwoo\nKim. Uniondet: Union-level detection towards real-time\nhuman-object interaction detection. In Proceedings of the\nEuropean conference on computer vision (ECCV), 2020.\n[13] Dong-Jin Kim, Xiao Sun, Jinsoo Choi, Stephen Lin, and\nIn So Kweon. Detecting human-object interactions with ac-\ntion co-occurrence priors. arXiv preprint arXiv:2007.08728,\n2020.\n[14] Alexander Kolesnikov, Alina Kuznetsova, Christoph Lam-\npert, and Vittorio Ferrari. Detecting visual relationships\nusing box attention. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision Workshops, pages 0–\n0, 2019.\n[15] Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi\nLiu, Jiefeng Li, and Cewu Lu. Detailed 2d-3d joint repre-\nsentation for human-object interaction. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10166–10175, 2020.\n[16] Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Yizhuo Li, and\nCewu Lu. Hoi analysis: Integrating and decomposing\nhuman-object interaction. Advances in Neural Information\nProcessing Systems, 33, 2020.\n[17] Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu,\nShiyi Wang, Hao-Shu Fang, Ze Ma, Mingyang Chen, and\nCewu Lu. Pastanet: Toward human activity knowledge en-\ngine. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 382–391, 2020.\n[18] Yong-Lu Li, Siyuan Zhou, Xijie Huang, Liang Xu, Ze Ma,\nHao-Shu Fang, Yanfeng Wang, and Cewu Lu. Transferable\ninteractiveness knowledge for human-object interaction de-\ntection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3585–3594, 2019.\n[19] Yue Liao, Si Liu, Fei Wang, Yanjie Chen, Chen Qian, and Ji-\nashi Feng. Ppdm: Parallel point detection and matching for\nreal-time human-object interaction detection. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 482–490, 2020.\n[20] Y Liu, Q Chen, and A Zisserman. Amplifying key cues for\nhuman-object-interaction detection. Lecture Notes in Com-\nputer Science, 2020.\n[21] Ye Liu, Junsong Yuan, and Chang Wen Chen. Consnet:\nLearning consistency graph for zero-shot human-object in-\nteraction detection. In Proceedings of the 28th ACM Interna-\ntional Conference on Multimedia, pages 4235–4243, 2020.\n[22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[23] Julia Peyre, Ivan Laptev, Cordelia Schmid, and Josef Sivic.\nDetecting unseen visual relations using analogies. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 1981–1990, 2019.\n[24] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen,\nand Song-Chun Zhu. Learning human-object interactions by\ngraph parsing neural networks. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , pages 401–\n417, 2018.\n[25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91–99, 2015.\n[26] Oytun Ulutan, ASM Iftekhar, and Bangalore S Manjunath.\nVsgnet: Spatial attention network for detecting human ob-\nject interactions using graph convolutions. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13617–13626, 2020.\n[27] Bo Wan, Desen Zhou, Yongfei Liu, Rongjie Li, and Xuming\nHe. Pose-aware multi-level feature network for human ob-\nject interaction detection. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 9469–9478,\n2019.\n[28] Hai Wang, Wei-shi Zheng, and Ling Yingbiao. Contextual\nheterogeneous graph network for human-object interaction\ndetection. arXiv preprint arXiv:2010.10001, 2020.\n[29] Tiancai Wang, Rao Muhammad Anwer, Muhammad Haris\nKhan, Fahad Shahbaz Khan, Yanwei Pang, Ling Shao,\nand Jorma Laaksonen. Deep contextual attention for\nhuman-object interaction detection. arXiv preprint\narXiv:1910.07721, 2019.\n[30] Tiancai Wang, Tong Yang, Martin Danelljan, Fahad Shahbaz\nKhan, Xiangyu Zhang, and Jian Sun. Learning human-object\ninteraction detection using interaction points. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4116–4125, 2020.\n[31] Bingjie Xu, Yongkang Wong, Junnan Li, Qi Zhao, and Mo-\nhan S Kankanhalli. Learning to detect human-object interac-\ntions with knowledge. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2019.\n[32] Xubin Zhong, Changxing Ding, Xian Qu, and Dacheng Tao.\nPolysemy deciphering network for human-object interaction\ndetection. In Proc. Eur. Conf. Comput. Vis, 2020.\n[33] Penghao Zhou and Mingmin Chi. Relation parsing neural\nnetwork for human-object interaction detection. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, pages 843–851, 2019."
}