{
  "title": "CellFM: a large-scale foundation model pre-trained on transcriptomics of 100 million human cells",
  "url": "https://openalex.org/W4410521040",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5102001561",
      "name": "Yuansong Zeng",
      "affiliations": [
        null,
        "Chongqing University",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5101373132",
      "name": "Jiancong Xie",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5094184499",
      "name": "Ningyuan Shangguan",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5016333384",
      "name": "Zhuoyi Wei",
      "affiliations": [
        "Huawei Technologies (China)",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5014391363",
      "name": "Wenbing Li",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5109034297",
      "name": "Yun Su",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5054119070",
      "name": "Shuangyu Yang",
      "affiliations": [
        "Sun Yat-sen Memorial Hospital",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5056858312",
      "name": "Chengyang Zhang",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A5062760009",
      "name": "Jinbo Zhang",
      "affiliations": [
        "Singleron Biotechnologies (china)"
      ]
    },
    {
      "id": "https://openalex.org/A5101788721",
      "name": "Nan Fang",
      "affiliations": [
        "Singleron Biotechnologies (china)"
      ]
    },
    {
      "id": "https://openalex.org/A5100412598",
      "name": "Hongyu Zhang",
      "affiliations": [
        "Chongqing University"
      ]
    },
    {
      "id": "https://openalex.org/A5101633465",
      "name": "Yutong Lu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5029051152",
      "name": "Huiying Zhao",
      "affiliations": [
        "Sun Yat-sen Memorial Hospital",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5113325189",
      "name": "Jue Fan",
      "affiliations": [
        "Singleron Biotechnologies (china)"
      ]
    },
    {
      "id": "https://openalex.org/A5055989750",
      "name": "Weijiang Yu",
      "affiliations": [
        "Huawei Technologies (China)",
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5023539493",
      "name": "Yuedong Yang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3013151148",
    "https://openalex.org/W4396780660",
    "https://openalex.org/W4390064086",
    "https://openalex.org/W4290072634",
    "https://openalex.org/W2800392236",
    "https://openalex.org/W4378212544",
    "https://openalex.org/W4205126801",
    "https://openalex.org/W4386553577",
    "https://openalex.org/W4386791134",
    "https://openalex.org/W4387819650",
    "https://openalex.org/W4205206193",
    "https://openalex.org/W4366526178",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W4389132297",
    "https://openalex.org/W4399387478",
    "https://openalex.org/W4403174218",
    "https://openalex.org/W4389498253",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2118258530",
    "https://openalex.org/W2126396457",
    "https://openalex.org/W3194665116",
    "https://openalex.org/W4389142474",
    "https://openalex.org/W2792468812",
    "https://openalex.org/W4320913159",
    "https://openalex.org/W2888113858",
    "https://openalex.org/W4360938460",
    "https://openalex.org/W2562947047",
    "https://openalex.org/W2969093152",
    "https://openalex.org/W4387130479",
    "https://openalex.org/W3198123874",
    "https://openalex.org/W2971653850",
    "https://openalex.org/W1496415998",
    "https://openalex.org/W4307969384",
    "https://openalex.org/W2592374742",
    "https://openalex.org/W2997280954",
    "https://openalex.org/W4385812757",
    "https://openalex.org/W2148472599",
    "https://openalex.org/W4221102522",
    "https://openalex.org/W2782454362",
    "https://openalex.org/W1975361178",
    "https://openalex.org/W1974983696",
    "https://openalex.org/W3138273194",
    "https://openalex.org/W2900239245",
    "https://openalex.org/W3162616609",
    "https://openalex.org/W1992772684",
    "https://openalex.org/W4285046232",
    "https://openalex.org/W2128271831",
    "https://openalex.org/W2555128150",
    "https://openalex.org/W2014130565",
    "https://openalex.org/W3048777112",
    "https://openalex.org/W4384648484",
    "https://openalex.org/W6810443081",
    "https://openalex.org/W4250482878",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4301581299",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6911880720",
    "https://openalex.org/W6911870091",
    "https://openalex.org/W4410521040"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-025-59926-5\nCellFM: a large-scale foundation model pre-\ntrained on transcriptomics of 100 million\nhuman cells\nYuansong Zeng 1,2,3,7 , Jiancong Xie1,7, Ningyuan Shangguan1, Zhuoyi Wei1,4,7,\nWenbing Li1,Y u nS u4, Shuangyu Yang5, Chengyang Zhang2, Jinbo Zhang6,\nNan Fang 6, Hongyu Zhang2, Yutong Lu1, Huiying Zhao5 ,J u eF a n6 ,\nWeijiang Yu 1,4 & Yuedong Yang 1\nSingle-cell sequencing provides transcriptomic proﬁling at single-cell resolu-\ntion, uncovering cellular heterogeneity with unprecedented precision. Yet,\ncurrent single cell data analysis suffers from the inherent data noises, batch\neffects, and sparsity, highlighting the requirement of a uniﬁed model to\nrepresent cellular states. To circumvent this problem, many recent efforts\nfocus on training single-cell foundation models based on large datasets.\nHowever, current human foundation models are still limited by the sizes of\ntraining data and model parameters. Here, we have collected a diverse dataset\nof 100 million human cells, on which we train a single-cell foundation model\n(CellFM) containing 800 million parameters. To balance efﬁciency and per-\nformance, the model is trained through a modiﬁed RetNet framework on the\nMindSpore. Extensive experiments have shown that CellFM outperforms\nexisting models in cell annotation, perturbation prediction, gene function\nprediction, and gene-gene relationship capturing.\nSingle-cell RNA sequencing (scRNA-seq) technologies have revolutio-\nnized molecular biology by enabling the measurement of tran-\nscriptome proﬁles with unparalleled scale and precision\n1,2.A ss i n g l e -\ncell technologies advance, the rapid accumulation of extensive data-\nsets has posed signiﬁcant analytical challenges3,4,p r i m a r i l yd u et ot h e\ndata’s inherent noise, sparsity, and batch effects. Despite the devel-\nopment of numerous single-cell-speciﬁct o o l s5–7 to address these\nchallenges, their performance often falls short when applied to new\ndatasets and struggles to scale with the growing data size. More\nimportantly, these tools fail to fully leverage the rich information\nembedded in large atlas datasets, underscoring the need for novel\ncomputational strategies.\nTo address this challenge, several single-cell foundation models\nhave been developed to analyze single-cell data\n8. Drawing inspiration\nfrom the remarkable success of large language models (LLMs) in nat-\nural language processing (NLP)9, and aiming to reduce training costs,\nresearchers have begun exploring theﬁne-tuning of these LLMs using\nrelatively small amounts of single-cell data. For example,\nCell2Sentence\n10 converts gene expression proﬁles of individual cells\ninto sequences of gene names ordered by expression levels and uses\nthese sequences toﬁne-tune the GPT-2 model. Similarly, GenePT\n11\nutilizes GPT-3.5 to generate gene embeddings based on gene\ndescriptions and metadata. While these approaches have improved by\nﬁne-tuning the GPT models, they still fall short of fully harnessing the\nReceived: 27 July 2024\nAccepted: 2 May 2025\nCheck for updates\n1School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China.2School of Big Data and Software Engineering, Chongqing\nUniversity, Chongqing, China.3Jinfeng Laboratory, Chongqing, China.4Huawei Technologies Co., Ltd, Shenzhen, China.5Department of Medical Research\nCenter, Sun Yat-Sen Memorial Hospital, Sun Yat-Sen University, Guangzhou, China.6Singleron Biotechnologies, Nanjing, Jiangsu, China.7These authors\ncontributed equally: Yuansong Zeng, Jiancong Xie, Zhuoyi Wei.e-mail: zengys@cqu.edu.cn; zhaohy8@mail.sysu.edu.cn; fanjue@singleronbio.com;\nweijiangyu8@gmail.com; yangyd25@mail.sysu.edu.cn\nNature Communications|         (2025) 16:4679 1\n1234567890():,;\n1234567890():,;\nrich gene expression data in large single-cell datasets, highlighting the\nneed for more comprehensive strategies.\nTo create single-cell foundation models from scratch, three\ntypes of single-cell foundation models, including ordering, value\ncategorization, and Value projection were proposed. Weﬁrst intro-\nduce gene-ranking-based models, such as iSEEEK12, which was the\nﬁrst model trained on over 10 million cells by predicting gene rank.\nSimilarly, tGPT13 learns gene embeddings by autoregressively mod-\neling gene ranks relative to their neighbors, processing sequences of\ngenes ordered by expression levels, and predicting the next gene’s\nrank based on prior context. Trained on 22.3 million single-cell\ntranscriptomes from humans and mice, tGPT demonstrated superior\nperformance across multiple datasets. Geneformer\n14 predicts gene\npositions within the cellular context to derive rank-based gene\nembeddings. With training on a dataset of 30 million single-cell\ntranscriptomes spanning diverse human tissues, Geneformer has\nachieved outstanding predictive performance.\nRecently, value categorization strategies were applied to leverage\ngene counts. By representing each gene with an embedding and bin-\nning its RNA counts, these models convert the continuous gene\nexpression data into categorical values, enabling the use of methods\ndesigned for categorical data. For example, scBert\n15 bins gene\nexpression values into discrete “buckets,\" transforming the con-\ntinuous task of predicting gene expression into a classiﬁcation pro-\nblem. Trained on millions of human cells, scBert has shown improved\nperformance across various datasets. Similarly, scGPT\n16 also segments\ngene expression values but enhances the process with an attention\nmask mechanism for autoregressive prediction. Using a self-\nsupervised approach, scGPT optimizes both cell and gene repre-\nsentations. Trained on over 33 million human cells, scGPT excels in\nvarious single-cell tasks. The Universal Cell Embedding (UCE)\n17 cap-\ntures molecular diversity across species by integrating genetic data\nusing protein language models. It uses self-supervised learning to\npredict gene expression by masking a subset of genes, reﬁning pre-\ndictions with binary cross-entropy loss. With over 650 million para-\nmeters, UCE is trained on more than 36 million cells, offering insights\ninto gene expression across diverse cellular contexts.\nTo further predict precise gene values, the value projection-based\nsingle-cell models were proposed. In this strategy, the gene expression\nvector x\ni is expressed as the sum of two components: a projection of\nt h eg e n ee x p r e s s i o nv e c t o ra n dap o s i t i o n a lo rg e n ee m b e d d i n g .U n l i k e\nordering and value categorization methods, the key advantage of value\nprojection is that it preserves the full resolution of the data. For\nexample, scFoundation\n18 directly predicts raw gene expression values\nusing a mask autoencoder (MAE). Trained on a large dataset of around\n50 million human cells with ~0.1 billion parameters, scFoundation\ndemonstrates decent performance in single-cell analysis. Similarly,\nGeneCompass\n19 incorporates four types of biological prior knowledge\nto enhance the understanding of gene regulatory mechanisms during\ngene expression prediction. Trained on about 50 million human and\n50 million mouse cells, GeneCompass has around 100 million para-\nmeters. To integrate metadata information, scELMo\n20 leverages LLMs\nlike GPT-3.5 to generate metadata descriptions and embeddings,\ncombining them with raw data in both zero-shot andﬁne-tuning fra-\nmeworks to address a variety of tasks.\nDespite the endeavor, the potential of foundation models trained\nexclusively on 100 million human cells has yet to be fully explored. The\nlimited availability of sufﬁcient single-species training data, such as for\nhuman cells, hindered the development of large-scale, single-species\nmodels. Existing single-species models are typically trained on around\n50 million cells, resulting in fewer than 100 million parameters. One\nreason for this limitation is the difﬁculty in collecting single-cell data-\nsets, which are often stored in various formats (e.g., FASTQ, h5ad,\nSeurat objects, 10x Genomics) and dispersed across different reposi-\ntories to accommodate diverse data processing and analysis needs.\nHere, we collect a lot of single-cell datasets from public databases\nand then make these data cleansing and standardization of uniﬁed\nformats, resulting in compiling a dataset of approximately 100 million\nhuman cells sequenced through various technologies. These datasets\nare twice as large as those used in the current largest single-species\nmodel, providing a rich foundation for training a larger model\n21,22.W e\nproposed a robust single-cell foundation model CellFM (Fig.1) with an\nimpressive 800 million parameters, marking an eightfold increase in\nmodel parameters over the current largest single-species model. To\nenhance the training of CellFM’s extensive parameters and to handle\nits substantial dataset, we have integrated ERetNet, a Transformer\narchitecture variant with linear complexity. ERetNet’s design ensures a\nbalance between efﬁciency and performance, serving as the backbone\nof our model. CellFM is categorized as a value-projection-based single-\ncell foundation model, as it aims to recover the vector embeddings of\nmasked genes derived from their linear projections based on gene\nexpression values. CellFM is developed using the MindSpore AI fra-\nmework from Huawei and is trained on four Huawei Altas800 servers,\neach equipped with eight Ascend910 NPUs. Our comprehensive\nexperiments have shown that CellFM outperforms existing models\nacross diverse applications, such as cell annotation, perturbation\nprediction, and gene function prediction.\nResults\nOverview of cellFM\nSingle-cell sequencing technology is crucial for revealing the detailed\nlandscape of cellular diversity and function at the single-cell resolu-\ntion. With the development of single-cell sequencing technologies, a\nvast array of datasets has been amassed, laying a robust groundwork\nfor the training of single-cell foundation models. However, these\ndatasets are available across various public repositories, including the\nNational Center for Biotechnology Information (NCBI) Gene Expres-\nsion Omnibus (GEO)\n23, the European Nucleotide Archive (ENA)24,t h e\nGenome Sequence Archive (GSA)25,26,a n dt h eI m m P o r t27. We have\nmeticulously curated single-cell data from these esteemed public\ndatabases (Fig.1a). These datasets are stored in multiple formats, such\nas FASTQ data, expression matrices, or Seurat/Scanpy objects. Weﬁrst\nprocess raw FASTQ data into the gene expression matrix through\nprimary analysis software provided by manufacturers. Subsequently,\nall acquired expression matrices were processed using a standardized\ndata analysis workﬂow facilitated by the SynEcoSys® single-cell data-\nbase (Singleron Biotechnologies)\n28. This process involved three key\nsteps including quality control forﬁltering cells and genes, gene name\nstandardization according to HUGO Gene Nomenclature Committee\n(HGNC) guidelines, and converting the data to a uniﬁed sparse matrix\nformat for subsequent analysis. Our efforts have successfully aggre-\ngated 19,914 samples, totaling 102,304,686 human cells from different\norgans and single-cell sequencing technologies. We have provided a\ndataset summary (Supplementary Fig. S1) and shared a detailed list of\nthe sources for the training datasets used in CellFM ( Supple-\nmentary_Metadata_information.xlsx). Concretely, 46.3 million\ncells were derived from normal donors, the other cells were from\ndiseased donors, such as 7.1 million cells from Viral infections donors\nand 3.5 million cells from lung cancer donors. Most datasets were\nsequenced by 10x genomics 3\n0 containing 66.7 million cells. Among the\ncurated dataset, approximately 70 million cells had annotated cell\ntypes. The training dataset includes a diverse range of cell types, such\nas T cells (19.2 million), mononuclear phagocytes (7.01 million), neu-\nrons (6.29 million), andﬁbroblasts (3 million).\nBuilding on our comprehensive collection of human cell data, we\nintroduce CellFM, an efﬁcient foundation model endowed with 800\nmillion parameters, designed to streamline the analysis of single-cell\ndata (Fig.1b). The model’s core is comprised of an embedding module,\na series of stacked ERetNet Layers, and the low-rank adaptive module\n(LoRA) mechanism. CellFM begins by converting scalar gene\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 2\nFig. 1 | Overview of the CellFM Framework. aCellFM consists of the Embedding\nmodule, the ERetNet module, and the LoRA module. The expression of the cell is\nﬁrst fed into the Embedding module to obtain the initial token values of each gene.\nThe embedded gene tokens of the cell are then input into the ERetNet module to\nlearn the gene-to-gene relationships and gene embeddings. Finally, the Low-Rank\nAdaptation (LoRA) is implemented to minimize the number of training parameters\nof CellFM.b Each ERetNet Layer block integrates Multi-Head Attention (MHA), the\nSimple Gated Linear Unit (SGLU), and Layer Normalization (LN).c The collecting\nworkﬂow and constituents of the training dataset employed within CellFM.d The\npre-trained CellFM model is adaptable for a multitude of single-cell downstream\nanalyzes including cell type annotation, perturbation prediction, gene network\ninference, and gene function prediction.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 3\nexpression data into rich, high-dimensional embedding features\nthrough its embedding module. These gene embeddings are then fed\ninto L ERetNet Layers, which are adept at capturing the nuanced\nrelationships among genes based on their expression proﬁles. Each\nERetNet Layer is composed of several key components: the Gated\nMulti-head Attention (MHA) unit, the Simple Gated Linear Unit (SGLU),\nand Layer Normalization (LN). Collectively, these elements empower\nthe ERetNet Layer to achieve training parallelism, cost-effective infer-\nence, and superior performance (Fig.1c). Furthermore, CellFM inte-\ngrates the LoRA module to reduce the number of trainable parameters\nduring theﬁne-tuning phase when adapting the model to new datasets.\nOnce pre-trained, CellFM can be applied to multiple single-cell\ndownstream applications, such as gene function prediction, cell type\nannotation, perturbation effect prediction, and gene network analy-\nsis (Fig.1d).\nCellFM improves the accuracy of gene function prediction\nGene function prediction is a cornerstone for deciphering the roles\nand properties of genes under diverse conditions14. With the human\ngenome comprising ~20,000 protein-encoding genes29,a n das i g -\nniﬁcant portion lacking functional annotations, the accurate predic-\ntion of gene functions is imperative for a deeper understanding of their\nroles within biological systems. Here, we evaluated the performance of\nCellFM for identifying gene function through three distinct gene\ncategories including Dosage sensitivity (referred to as T1), Bivalent\nmethylation status versus non-methylated (T2), and Bivalent methy-\nlation versus Lys4-only methylation (T3). These categorizations\nrepresent binary classiﬁcation challenges, where model predictions\nare assessed against actual gene function labels. Since the limited\nnumber of genes in these three tasks, typically fewer than 1000,ﬁne-\ntuning existing single-cell foundation models presents a challenge.\nTo make a fair comparison, we adopted a zero-shot learning\nstrategy for each model on the gene function prediction task. As\nshown in Fig. 2a, our model demonstrated remarkable results,\nachieving the best performance on three tasks. CellFM surpassed\nexisting methods, with a 5.68% and 5.86% increase over the top two\ncompeting models UCE and scGPT in terms of average accuracy values,\nrespectively. A similar trend was observed for Macro-F1 scores\n(Fig. 2b). The superior performance of our model was further sub-\nstantiated by visualization results generated by Uniform Manifold\nApproximation and Projection (UMAP) using the gene embeddings\nfrom each pre-trained model. Our model’s ability to distinctly cate-\ngorize dosage-sensitive from non-sensitive genes was evident in Fig.2c\nand Supplementary Fig. S2. However, scGPT and Geneformer exhib-\nited an overlap in the embedding space that could undermine gene\nfunction prediction accuracy. In summary, ourﬁndings underscore the\nmodel’sp r oﬁciency in accurately predicting gene functions utilizing a\nzero-shot approach, showcasing its efﬁcacy without the need for\nextensive modelﬁne-tuning.\nTo evaluate the ability of CellFM on multi-class classiﬁcation, We\nfurther performed the gene function prediction using the data from\nthe Gene Ontology (GO). This dataset includes three major categories:\nbiological process (BP), cellular component (CC), and molecular\nfunction (MF). Detailed information about the GO dataset can be found\nin the referenced study\n30. Given the complexity of predicting all gene\nfunctions (BP: 1578, CC: 253, MF: 299), we focused our evaluation on\nthe top 10 most frequent functions within each category. This\napproach ensures a realistic yet manageable benchmark for model\ncomparison. To guarantee fairness across methods, we intersected the\ngene sets from each foundational single-cell model and maintained\nconsistent training, validation, and test sets. As shown in Fig.2d,\nCellFM demonstrated superior average performance, outperforming\nthe top two models GeneCompass and UCE by 1.6% and 1.94% in\naverage AUPR, respectively. Other models such as scFoundation and\nGeneformer also delivered competitive results with scGPT achieving\nan AUPR of 71.3%. We didn’t compare with scELMo since scELMo\nemployed large language models (LLMs) like GPT-3.5 as generators to\ngenerate embeddings from metadata descriptions in the training\nphase, which has included gene function information.\nCellFM enables predicting perturbation responses\nRecent advancements in sequencing and gene editing have enabled\nlarge-scale experimental perturbation simulations to study changes in\ngene expression and cellular behavior. These simulations are essential\nfor understanding cellular responses to various stimuli and are\nincreasingly applied to investigate drug effects, disease mechanisms,\nand therapeutic interventions. However, the vast combinatorial space\nof potential gene perturbations quickly exceeds the practical limits of\nexperimental feasibility. To overcome this, single-cell foundation\nmodels adopt perturbation modeling, leveraging knowledge from\nknown experimental perturbations to predict responses to unknown\nperturbations. By utilizing self-attention mechanisms over the gene\ndimension, these models can capture intricate gene interactions and\naccurately predict gene expression responses in unseen perturbations.\nThe predictive power of perturbation modeling becomes especially\nvaluable in AI-driven drug discovery, where it is used to forecast how\nexisting drugs or genes will affect cellular processes, identify new drug\ntargets, and explore drug repurposing opportunities. Additionally,\nthese models offer deep insights into cellular heterogeneity, crucial for\ndeveloping personalized medicine strategies.\nTo assess CellFM ’sp r oﬁciency in predicting perturbation\nresponses, we utilized two Perturb-seq datasets: (1) the Adamson\ndataset\n31, encompassing 87 single-gene perturbations with roughly\n100 cells per perturbation and at least 7000 control cells; and (2) the\nNorman dataset\n32, which includes 131 dual-gene and 105 single-gene\nperturbations. As depicted in Fig.3a, we employed the Pearson cor-\nrelation metric on the top 20 differentially expressed genes (De) to\nevaluate each model, whereΔ denotes the degree of gene expression\nalteration post-perturbation relative to the control state. We evaluated\nall single-cell foundation models on the perturbation task by com-\nbining them with the classic perturbation model GEARS, as suggested\nby the study scFoundation. GEARS is a computational tool speciﬁcally\ndesigned for predicting single and multi-gene perturbations based on\nscRNA-seq datasets. GEARS operates by integrating a gene-gene\ninteraction network as prior knowledge, which allows it to leverage\nexisting biological information to improve prediction accuracy. GEARS\nhas demonstrated state-of-the-art performance in gene perturbation\npredictions, making it a leading choice in theﬁeld. Concretely, we\nreplaced GEARS’ gene embeddings with those derived from CellFM\n(Fig. 3a). As shown in Fig.3b-c, our model consistently outperformed\nall competing single-cell foundation models, achieving improvements\nof 1% and 1.45% in average PCC and MSE compared to the second-\nranked model scFoundation, respectively. Additionally, CellFM con-\nsistently surpassed GEARS with 4.75% and 7% improvement regarding\naverage PCC and MSE values, respectively. As shown in Supplementary\nFig. S4, CellFM consistently outperformed all other single-cell foun-\ndation models, as well as the non-foundational model GEARS, when\nmeasured by theR\n2 metric. CellFM achieved anR2value that was 1.3%\nhigher than the second-best scGPT in terms of averageR2.T h ev i s u a l\nresults for two speciﬁc perturbation cases from the Adamson dataset\nin Fig.3d have further shown that CellFM could accurately predict the\ndirection of perturbation.\nThe Norman dataset targeted 105 genes with 236 perturbations\nand represented just 5% of the expansive 5565 possible gene combi-\nnations, highlighting the vast unexplored perturbation space. Conse-\nquently, we harnessed CellFM to extend the scope of perturbations\nvirtually and graphically represent the anticipated average response\nfor each gene combination. Concretely, we trained CellFM on the\nexisting knockouts (KOs) from the Norman dataset and extrapolated\nto other perturbations. CellFM was trained on the original Norman\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 4\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\n0\n1\nCellFM\nscGPT\nGeneformer\nT1 T2 T3\nc\nd\na b\nFig. 2 | Comparison of Gene Function Prediction performance in a zero-shot\nsetting. aaccuracy (ACC) scores and (b) Macro-F1 values for CellFM along with\nother competing single-cell foundation models on the binary classiﬁcation data.\nc The visualization results of CellFM, scGPT, and Geneformer are plotted by the\nUniform Manifold Approximation and Projection (UMAP) using the gene embed-\nding generated through each model.d The Area Under the Precision-Recall Curve\n(AUPR) values for multiple gene functions predicted by CellFM and other com-\npeting single-cell foundation models on Gene Ontology (GO) data, where each\ngene is annotated with numerous functions. MF Molecular Function, CC Cellular\nComponent, and BP Biological Process. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 5\nd e\nf\nUnperturbed gene \nexpressions\nCross-gene\nMLP layer\nGenetic\nperturbation\nPerturbed gene \nexpressions\nFoundation\nModel\nP1\nP4\nP1\nP2 P3\nP4P5\nP6 P7\na\nc\nPerturbation relation graph\nCellFM-\npredicted \nperturbation\nscGPT-\npredicted \nperturbation\nGround-truth \nperturbationX\nTop 1 perturbation\npredictions of\nCNN1+ETS2\nTop 1 perturbation\npredictions of\nETS2+IGDCC3\nAHR+ctrl BPGM+ZBTB1\nb\nFig. 3 | Analysis of Perturbation Response and Reverse Perturbation Predic-\ntions. aA diagram showcasing the perturbation prediction model leveraging cell-\nspeciﬁc gene embeddings derived from CellFM.b The mean square error (MSE)\nbetween predicted and actual post-gene expressions for the top differentially\nexpressed (DE) genes in a zero-shot setting.c Comparison of CellFM with other\nsingle-cell foundation models and the perturbation prediction method GERAS.\nPearson correlation coefﬁcients between predicted and actual gene expression\nchanges are reported for the top differentially expressed (DE) genes in a zero-shot\nsetting.d Analysis of gene expression changes following perturbations ofAHR+ctrl\n(n = 464 cells) andBPGM+ZBTB1(n = 280 cells). The plots compare predicted versus\nactual expression changes for the top 20 differentially expressed genes. Box plot\nelements represent: center line, median (50th percentile); box limits, upper (75th)\nand lower (25th) quartiles; whiskers, 1.5 × interquartile range (IQR) from the box;\npoints beyond whiskers are considered outliers. The horizontal dashed line indi-\ncates the null effect baseline (0 change). Minimum and maximum values are\nrepresented by the whisker endpoints, with all percentiles calculated from the\nempirical distribution of expression changes.e A graphical representation of\npotential perturbation combinations across a 20-gene space, differentiated by\nexperiment type (train, valid, test, unseen). Predicted perturbations are indicated\nby square boxes, with the actual source perturbation marked by a cross. The boxes\nare colored as follows: dark purple for test data, light purple for validation, medium\npurple for training, and gray for unseen.f The accuracy of each model in predicting\nthe correct source of perturbation among the top 10 predictions for test cases in a\nﬁne-tuning setting. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 6\ndataset, which covers 236 perturbations targeting 105 genes. We then\nused the ﬁne-tuned model to predict and expand the responses to\nuntested perturbation combinations in silico. These predictions were\nvisualized as the response for each perturbation combination. We\nexcluded all perturbed genes and plotted the UMAP graph. As shown\nin Supplementary Fig. S5, the clusters exhibited overlap in certain\nregions while remaining distinct in others, consistent with the expec-\ntation that several perturbationseither have no effect or produce\nsimilar effects. The genes depicted in each cluster represent the\n“dominant gene\" within the perturbation combinations. The results\nhave demonstrated a strong association between the clusters and their\nrespective dominant genes. For example, the cluster associated with\nthe SETgene indicates that the data points in this cluster correspond to\ncombined perturbations involving SET and another gene (e.g.,SET\n+CLDN6, SET+MIDN).\nIn addition to the gene perturbation, we further validate the per-\nformance of CellFM on drug-perturbed data. Similarly, we also combined\nCellFM with a non-single foundation model CellOT\n33,w h i c hw a sac l a s s i c\nmodel for drug perturbation prediction. Concretely, we integrated\nCellFM and CellOT by replacing the input cell representation of CellOT\nwith the cell representation from CellFM (Supplementary Fig. S9a), as\nCellOT is speciﬁcally tailored for drug perturbation and does not require\ngene embeddings. In this setting, we compared CellFM with CellOT,\nscGEN, and Identity. As indicated in Figure S9b, CellFM outperformed\nCellOT in perturbation predictionon four drugs, achieving improve-\nments of 66.6% and 2.2% in averagel\n2 and PCC, respectively.\nR e v e r s ep e r t u r b a t i o np r e d i c t i o ni ns i l i c ou s i n gC e l l F M\nBeyond forecasting the outcomes of gene perturbations, the accurate\nprediction of CRISPR target genes that prompt cellular recovery from\ndisease states is equally signiﬁcant. Here, we conducted“in silico\nreverse perturbation prediction” following the study scGPT\n16,u t i l i z i n g\nthe Norman dataset. Concretely, we followed scGPT, selecting 20\nperturbation genes from the Norman dataset to construct perturba-\ntion cases forﬁne-tuning and testing. This combinatorial space con-\nsists of 210 one-gene or two-gene perturbation combinations. The\nsubset was selected to maximize the representation of ground truth\nperturbation data across both training and testing cases, using a ran-\ndom train-test split. Since scGPT did not specify a particular seed for\nsplitting, we used the default seed. The resulting dataset contained 47\n(22%) known perturbations, including 33 training cases, 3 validation\ncases, and 11 test cases, with the remaining perturbation cases left as\nunseen. Both scGPT and CellFM were evaluated on this newly split\ndataset, as we were unable to replicate the exact splits used in the\noriginal scGPT study.\nCellFM demonstrated remarkable success, accurately predicting\nthe perturbations that would yield the observed cellular outcomes. For\nexample, it accurately identiﬁed the combination ofCNN1 and ETS2,a s\nwell as the pairing ofETS2 and IGDCC3, as the top predictions for a\nspeciﬁct e s tc a s e( F i g .3e). A similar accuracy trend was observed for\nthe perturbations involving the combinations ofIGDCC3 and MAPK1,\nand CEBPEand CNN1genes (Supplementary Fig. S6). CellFM and scGPT\nachieved similar performance in relevant perturbations (purple bars in\nSupplementary Fig. S8). However, CellFM achieved an average of\ncorrectly identiﬁed perturbations in 81.8% of the top 10 predictions\n(Fig. 3f), which was 18.1% higher than scGPT. As the number of top\npredictions considered increased, both CellFM and scGPT demon-\nstrated enhanced performance. However, GEARs did not exhibit a\ncomparable level of improvement in accuracy. The performance of\nGEARS was consistent with theﬁndings reported in the scGPT, where\nthe hit rates for top 1 to top 8 predictions also remained constant. This\nbehavior is likely due to the limitations of GEARS as a smaller model,\nwhich struggles to identify perturbation combinations effectively. To\nfurther assess the robustness of CellFM, we performed additional\nevaluations by varying the random seeds during the dataset splitting\nprocess to generate the new training and test cases. As shown in\nSupplementary Fig. S7b, CellFM outperformed scGPT and GEARS in\nterms of average hit rate accuracy. Speciﬁcally, CellFM achieved an\naverage of 36.3% and 54.5% correctly identiﬁed perturbations in the\ntop 3 and top 5 predictions, which were 18.1% and 18.2% higher than\nscGPT, respectively. To further examine whether the performance of\nCellFM was affected by random seeds during modelﬁne-tuning, we re-\nevaluated CellFM, scGPT, and GEARS under different random seeds.\nThe results demonstrated that CellFM consistently maintained com-\nparable top 1 prediction performance, outperforming scGPT by 9.1%\n(Supplementary Fig. S7a). For the reverse perturbation prediction, we\nmaintained the original comparison for CellFM and only evaluated it\nagainst scGPT and GEARS since the newly added single-cell foundation\nmodels neither provided code nor published corresponding results in\ntheir original studies (Supplementary Fig. S3) and reimplementing\nthese models would have required substantial programming effort\nand time.\nThe current study is limited to gene perturbation without con-\nsidering drug molecules. Thus, our method doesn’t perform reverse\nperturbation prediction in silico on the drug perturbation datasets\nsuch as sciPlex, since drug perturbation datasets require additional\ninformation speciﬁc to the drug molecules themselves. In the future,\nwe will expand CellFM’s capabilities to support drug perturbation\ndatasets, which will involve adapting the model architecture to incor-\nporate drug-speciﬁc molecular information.\nCell type annotation with CellFM\nCell type annotation is a cornerstone of single-cell data analysis,\nessential for uncovering the cellular heterogeneity within biological\nsamples. To evaluate CellFM’s competency in cell type annotation, we\nconducted an exhaustive benchmark against several recent single-cell\nfoundation models. We have also included two baseline methods—\nSVM and scmap— which were also suggested simultaneously in the\nbenchmark studies\n34,35. The cell annotation benchmarks used in our\nstudy, including intra-dataset and inter-dataset evaluations, were\nbased on the latest benchmarking framework, scEval\n8,d e s i g n e dt o\nevaluate single-cell foundation models on cell annotation tasks. Fol-\nlowing prior studies such as scGPT and scEval, we utilized randomized\ntrain-test splits for intra-dataset evaluation to assess model perfor-\nmance under consistent experimental conditions. While intra-datasets\nwere a part of our evaluation, we also performed inter-dataset testing,\nwhich better reﬂects real-world scenarios involving large batch effects.\nFor inter-datasets, we partitioned the data by batch or patient ID,\niteratively using each batch as the test set while training on the\nremaining batches. For hPancreas, we directly adopted the train-test\nsettings established in scGPT, ensuring training and testing data came\nfrom different batches. This cross-batch validation ensures a thorough\nand fair assessment of model robustness across batches. Importantly,\nthe cell annotation datasets used in our study were not included dur-\ning the pre-training of CellFM. Forthe zero-shot cell type annotation\ntask, all foundation models, including ours, were fully frozen during\ntraining. Classiﬁers (e.g., MLP or CNN+MLP) were then trained on\nembeddings extracted from these frozen models using labeled train-\ning data and the cross-entropy loss function. These trained classiﬁers\nwere subsequently used to predict cell types on test datasets. We used\nthe default classiﬁers implemented in each single-cell foundation\nmodel. For models like UCE, which lack a predeﬁned classiﬁer, we\nimplemented a multi-layer perceptron (MLP) approach, in line with\nstandard foundation model practices. For the CellFM, we followed it\nwith the MLP classiﬁer for the classiﬁcation task.\nBased on theﬁndings in the“Scaling of data and model size”\nsection (Supplementary Note 4), we used CellFM with 80 million\nmodel parameters (CellFM-80M) for cell annotation and batch effect\ncorrection tasks. Our initial evaluation involved eight intra-datasets.\nFollowing the methodology established by scGPT\n16, we segmented\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 7\nc d\ng h\nCellFM\nCellFM\nscGPT\nscGPT\ne f\na b\nFig. 4 | Zero-shot cell type annotation performance of each model.Heatmaps\nillustrating (a)c l a s s iﬁcation accuracy and (b) Macro-F1 scores of each model across\nintra-datasets. Red indicates lower performance, and blue represents the highest\nperformance.c The river plot of CellFM illustrates the predicted cell types and their\nrelationships to the actual cell types on the Immune dataset.d The river plot of\nscGPT illustrates the predicted cell types and their relationships to the actual cell\ntypes on the Immune dataset. Heatmaps show (e)c l a s s iﬁcation accuracy and (f)\nMacro-F1 scores of each model across inter-datasets, with each value representing\nthe average accuracy calculated fromﬁve independent runs using different random\nseeds. Red indicates lower performance, and blue represents the highest perfor-\nmance. g The river plot of CellFM illustrates the predicted cell types and their\nrelationships to the actual cell types on the hPancreas dataset.h The river of scGPT\nplot illustrates the predicted cell types and their relationships to the actual cell\ntypes on the hPancreas dataset. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 8\neach intra-dataset, allocating 70% for training and the remainder for\ntesting. As illustrated in Fig.4a, CellFM(80M) excelled baselines in\nterms of ACC across intra-datasets (we simply refer to CellFM(80M) as\nCellFM). The average ACC for CellFM was 92.91%, surpassing the\nsecond-ranked single-cell foundation model scFoundation by 2.02%.\nCellFM were also 1.97%, 26.86%, and 10.2% higher than those of SVM,\nscmap, and scBERT, respectively. A similar trend could be found when\nmeasured by the Macro-F1 values (Fig. 4b). To substantiate the\nsuperior outcomes of CellFM, we present a case study of the immune\ndataset. The predictions were visualized in Fig.4c, d and Supplemen-\ntary Fig. S10, 11, CellFM achieved high precision for most cell types. To\nevaluate the efﬁcacy of our model in the context of batch effects, we\nassessed its performance on 7 paired cross-batch datasets. In each\nscenario, a distinct batch of data was designated as the test set,\nwith the remaining data constituting the training set. As depicted in\nFig. 4e, f and Supplementary Fig. S12, our model continued to out-\nperform its competitors and was 2.3% higher than the second-ranked\nsingle-cell foundation model scFoundation. CellFM outperformed\nSVM, scmap, and scBERT on inter-dataset cell annotation tasks, with\naverage accuracy improvements of 2%, 21.1%, and 3.4%, respectively.\nWe further present a visualization case study of the hPancreas dataset\nin Fig. 4g, h and Supplementary Fig. S10-11. CellFM showed a high\nprecision of most cell types. We also evaluated the embedding quality\nof all single-cell foundation models on inter-datasets using the scIB\nmetric scores. As shown in Supplementary Fig. S15, CellFM out-\nperformed competing models in AvgBio scores, aligning with its\nenhanced cell classiﬁcation performance. Additionally, we incorpo-\nrated the scIB metric scores to evaluate the impact of MLP layers in\nCellFM and found minimal performance variation when adjusting lay-\ners from one to three. Supplementary Fig. S16 shows a positive corre-\nlation between classiﬁcation accuracy and AvgBio scores, but both\nmetrics exhibited only minor changes across layer conﬁgurations.\nTo further assess CellFM’s capability to distinguish subtypes such\nas exhausted and activated CD8+ T cells, we evaluated it on the basal\ncell carcinoma (BCC) dataset (GSE123813) and the liver hepatocellular\ncarcinoma (LIHC) dataset (GSE140228), both obtained from a database\narticle\n36 due to their availability in h5ad format and detailed cell type\nannotations. As shown in Supplementary Figs. S13 and S14, CellFM\nconsistently outperformed other models across all cell types, achiev-\ning an average accuracy 2.3% higher than the second-best model, UCE.\nNotably, CellFM demonstrated exceptional performance in distin-\nguishing exhausted and activated CD8+ T cells, surpassing UCE by an\naverage of 6.5%. On the BCC_GSE123813 dataset, CellFM achieved\naccuracy scores of 77% and 74% for exhausted and activated CD8+\nT cells, respectively, outperforming UCE by 6% and 7%. A similar trend\nwas observed on the LIHC_GSE140228 dataset, further conﬁrming\nCellFM’s robustness in identifying these cell states.\nWe made CellFM, scGPT, GeneCompass, and Geneformer toﬁne-\ntune. As shown in Supplementary Fig. S17, the performance ofﬁne-\ntuned scGPT aligns closely with the results reported in its original\narticle and the benchmark study scEval\n8. For instance, ﬁne-tuned\nscGPT achieved 92.2% cell type annotation accuracy on the human\npancreas dataset, comparable to its original study and that reported in\nscEval. Similarly,ﬁne-tuned Geneformer reached 85.3% accuracy on\nthe same dataset, consistent with the performance reported in scEval,\nalthough its original paper did not provide results for cell type classi-\nﬁcation. The ﬁne-tuned GeneCompass achieved comparable results\nwith theﬁne-tuned CellFM. CellFM(800M) obtained low performance\non the intra- and inter-datasets when evaluated through zero-shot.\nHowever, across all inter-datasets, CellFM (800M) demonstrated an\naverage ﬁne-tuning accuracy that was 12.8% and 15.92% higher than\nscGPT and Geneformer, respectively. This performance indicated the\npotential power of the larger model. To evaluate the performance of\nCellFM affected by LoRA during theﬁne-tuning phase, we conducted\nthe ablation experiments on the cell type annotation task using the\ninter-datasets. As shown in Supplementary Fig. S18, the performance\nof CellFM showed minimal changes when LoRA was applied compared\nto when it was not, across the inter-datasets. However, using LoRA\nreduced the time required forﬁne-tuning. Based on theseﬁndings, we\nrecommend applying LoRA during the ﬁne-tuning of CellFM to\nenhance efﬁciency without sacriﬁcing performance.\nSince Long Non-Coding RNAs (lncRNAs) were included in CellFM’s\ntraining data, we evaluated its ability to identify cell-type-speciﬁc\nlncRNAs using attention scores. By analyzing CLS-gene attention, we\nselected the top 100 genes per cell type, identifying critical lncRNAs\nfor classi ﬁcation. Trained on PBMC data, CellFM highlighted\nHOTAIRM1 as a top gene for CD14+ Monocytes. This myeloid-speciﬁc\nlncRNA regulates monocyte differentiation viamiR-3960 and HOXA1,\nwith silencing reducing CD14 and monocyte marker expression.\nIncluding HOTAIRM1 in scRNA-seq annotation improves CD14+ Mono\nidentiﬁcation accuracy, demonstrating lncRNAs’ value in cell type\nclassiﬁcation\n37.\nTo evaluate whether alternative normalization methods might\nfurther improve CellFM’s performance, we tested scTransform38,\nwhich corrects the variance-mean bias. Due to computational con-\nstraints, we performed these experiments using a smaller CellFM\nmodel with 80 million parameters instead of the original 800 million.\nAs shown in Supplementary Fig. S21(a), the performance with\nscTransform was slightly lower than with log1p normalization used in\nCellFM. These results suggested that while scTransform addresses\nvariance-mean bias explicitly, it does not yield substantial improve-\nments in CellFM’s performance for the cell-type annotation tasks\nevaluated.\nTo evaluate the efﬁciency of the modiﬁed ERetNet model used in\nCellFM, we further evaluated two key modiﬁcations to RetNet: repla-\ncing the traditional feedforward network with a gated bilinear net-\nwork, and substituting the pre-layer LayerNorm with the DeepNorm\nlayer normalization technique. We also benchmarked it against the\nclassic Transformer model. All ablation experiments were conducted\nusing a newly trained CellFM model with 80 million parameters, with a\nfocus on the cell type annotation task, due to limitations in time and\ncomputational resources. As shown in Supplementary Fig. S21(b),\nremoving the Simple Gated Linear Unit and DeepNorm resulted in\ndecreases of 0.8% and 0.9%, respectively. Additionally, the removal of\nthe L_cls loss led to a slight drop (0.4%) in performance. When com-\npared to the classic Transformer, CellFM demonstrated a 1.2%\nimprovement. In conclusion, these modiﬁcations collectively con-\ntributed to the robust performance of CellFM, as evidenced by the\nbenchmarking results. Additionally, the implementation of Gated\nMulti-head Attention (MHA) in CellFM improved the computational\ncomplexity fromOðl\n2\nmaxdÞ to Oðlmaxd2=hÞ,w h e r ed was set to 1536 and\nthe number of attention heads (h) was set to 48. Consequently, the\nactual computational complexity of CellFM isO 2048 ×15362\n48\n/C16/C17\n,w h i c h\nis smaller thanO 20482 ×1 5 3 6\n/C16/C17\n: The formula derivation can be found\nin Supplementary Note 1.\nTo further evaluate the performance of CellFM in integrating\ndatasets with batch effects, we conducted a comparison among three\nsingle-cell foundation models: scELMo, scGPT, and UCE. We included\nscGPT, scELMo, and UCE in the comparison, as their original studies\nreported batch correction capabilities. The deep learning framework\nused for CellFM, MindSpore, does not support the Gradient Reversal\nLayer (GRL) technique. Similarly, other single-cell foundation models\nwith batch effect correction functionalities also lack GRL imple-\nmentation. To ensure a fair comparison, we re-evaluated scGPT by\nremoving the GRL loss while retaining its other loss functions. We\nevaluated CellFM across multiple datasets, including PBMC 10k, the\nhuman brain cell atlas, and two versions of Tabula Sapiens. As\ndemonstrated in Supplementary Fig. S19, CellFM achieved the highest\naverage AvgBio scores on these datasets, outperforming the second-\nbest method, UCE, by 2.1%.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 9\nDeciphering gene relationships with CellFM\nThe intricate interplay among target genes within a Gene Regulatory\nNetwork (GRN) is pivotal for orchestrating key biological processes.\nHere, we examined the ability of CellFM to encode these gene rela-\ntionships through its gene embeddings and attention maps. To eval-\nuate the gene relationships efﬁciently captured by the pre-trained\nCellFM, we ﬁne-tuned CellFM using 32484 immune cells from the\nImmune data and about 200000 non-immune cells from the human\nbrain data. As shown in Fig.5a and Supplementary Fig. S22, we present\nt h r e eg e n er e l a t i o n s h i pg r a p h s :F i g .5(a) shows the pre-trained CellFM,\nFigure S22(b) displays CellFM trained on immune cells, and Figure\nS22(c) illustrates CellFM trained on non-immune cells. The results\nshow that the relationships among genesIL-2, IL-3,a n dIL-4, observed\nin the pre-trained CellFM, were preserved when CellFM wasﬁne-tuned\non immune cells. However, these relationships were absent when\nCellFM was trained solely on non-immune cells. Previous studies have\nshown thatIL-2, IL-3,a n dIL-4 are involved in the JAK/STAT pathway.\nIL-2, secreted primarily by Th1 cells, promotes immune activation,\nwhileIL-4, secreted by Th2 cells, facilitates anti-inﬂammatory signaling.\nTogether, they regulate immune responses by mediating cell pro-\nliferation, differentiation, and immune homeostasis. Additionally,IL-3\nstimulates the STAT5 pathway, which regulates cell proliferation, dif-\nferentiation, and anti-apoptotic signaling\n39,40. In summary, CellFM\neffectively preserved biologically relevant immune gene relationships.\nOn the other hand, we observed that the gene relationship structures\nbetween IL1RAP and IL1R1 were only present in the CellFM trained on\nimmune cells. Previous studies have shown thatIL1RAPand IL1R1 form\na functional receptor complex that mediates IL-1β signaling. This\ninteraction plays a critical role in neutrophilic inﬂammation, exacer-\nbating airway inﬂammation and contributing to worsened pulmonary\nobstruction.\nTo further verify the identiﬁed gene programs, we followed scGPT\nto perform Leiden clustering gene programs on the gene similarity\ngraph construed by K-Nearest Neighbors (KNN) and extracted gene\nprograms from gene clusters that consisted ofﬁve or more genes.\nSubsequently, we conducted a comprehensive pathway enrichment\nanalysis based on the gene programs using the Kyoto Encyclopedia of\nGenes and Genomes (KEEG). As illustrated in Fig.5b, we juxtaposed the\nresults yielded by CellFM with those obtained from co-expression\nnetwork analysis. CellFM consistently revealed a signiﬁcantly higher\nnumber of enriched pathways across all clustering resolutions, except\nfor the resolution at 10. To further validate the efﬁciency of the path-\nways identiﬁed by CellFM, we conducted a comparative analysis of the\npathways between CellFM and the co-expression network at a resolu-\ntion of 40. As shown in Supplementary Table S1–S3, both methodol-\nogies identiﬁed 25 common pathways. CellFM uniquely identiﬁed an\nadditional 59 pathways, 7 of which were pertinent to immune system\nprocesses. Conversely, the co-expression network uniquely identiﬁed\n32 pathways, of which only 2 were associated with immune functions.\nThese comprehensiveﬁndings underscore CellFM’s superior capacity\nto capture subtle and intricate gene-gene interactions, thereby\nenabling the elucidation of speciﬁc biological mechanisms within a\nbroader cellular context.\nCellFM efﬁciently identiﬁed genes most affected by\nperturbations\nIn this section, we analyzed the perturbed genes in the perturbation\nexperiments and their most signiﬁcantly affected genes through the\nattention map (Fig.5c). Concretely, we modeled the effects of the\nperturbing gene by providing the model with the control cell expres-\nsion proﬁles (non-perturbed) and explicitly indicating the gene as the\nperturbed gene. The model then predicted how the gene perturbation\nwould affect the expression of other genes. Using the attention\nmechanism, we identiﬁed the 20 genes most inﬂuenced by perturbing\nthe gene.\nWe have provided nine cases in Supplementary Fig. S23. Across\nnine case genes, among the top 20 genes most inﬂuenced by CellFM\nfor each perturbation gene, an average of 18 were found in the ChIP-\nAtlas database\n41. The results showed that CellFM can correctly identify\ninﬂuenced genes through attention scores. Additionally, we further\nconducted the pathway analysis on case perturbation genes JUN and\nSPI1 to show that CellFM captured distinct pathway-activation patterns\nthrough the genes most inﬂuenced by perturbation genes. As illu-\nstrated in Fig.5d, in the Adamson dataset, CellFM identiﬁed the top 20\ngenes most inﬂuenced by gene SPI1. Most genes were conﬁrmed to be\nassociated with SPI1, as validated by the ChIP-Atlas database (Fig.5e).\nWe noted that SPI1 in the column has connectivity, while the SPI1 in the\nrow doesn’t. The reason may be caused by the attention mechanism\nused in CellFM (Supplementary Note 3). Most gene pairs in the heat-\nmap of Fig.5d have values near zero due to our normalization strategy.\nWe ranked all genes in the perturbed dataset from Adamson based on\ntheir attention scores and recalculated the connection scores by\ndividing these ranked scores by the total number of genes. Addition-\nally, the partial relationships betweenCCNB2, CCNA2, TOP2A, MKI67,\nKPNA2,a n dCENPF in Fig.5dw e r e n’t captured likely because CellFM\nfocused on capturing relationships withSPI1. When not designating\nperturbation targets in CellFM (i.e., during regular gene recovery\ntasks), we could observe relations between these genes as Supple-\nmentary Fig. S24. In addition, based on gene embeddings derived from\nCellFM, the cosine similarity scores of these genes were signiﬁcantly\nhigher compared to those of other genes, indicating that CellFM can\nlearn strong intrinsic relationships among them.\nTo further directly demonstrate the relationships betweenSPI1\nand theSPI1 perturbed genes, we obtained theSPI1 ChIP-seq BigWig\nﬁles (SRX2770855) from the public ChIP-Atlas database (https://chip-\natlas.org/peak_browser) and visualized them using the Integrative\nGenomics Viewer (IGV). To select the top 5 genes with the strongest\nSPI1 binding, we identiﬁe dt h o s ew i t ht h eh i g h e s tM A C S 2s c o r e s ,\nindicating the most signiﬁcant SPI1 binding at their genomic loci.\nThese genes were prioritized based on peak intensity and the extent of\noverlap with key genomic regions. The IGV snapshots allowed us to\nvisually conﬁrm the presence and strength ofSPI1binding at these loci,\nproviding direct evidence ofSPI1’s regulatory impact on these genes.\nAs shown in Supplementary Fig. S25, the results clearly show the\nbinding peaks ofSPI1at the genomic loci of the target genes, indicating\npotential regulatory interactions.\nTo construct a comprehensive gene-transcription factor (TF)\ninteraction network for the genes inﬂuenced bySPI1,w eu t i l i z e dt h e\nTRRUST database\n42 (http://www.grnpedia.org/trrust) to identify the\ntop 300 genes most inﬂuenced bySPI1. As shown in Supplementary\nFig. S26, the network diagram illustrates the visualization of these\ninteractions, where nodes represent genes or transcription factors and\nedges denote the interactions between them. Red nodes represent\ntranscription factors, while blue nodes represent genes. Notably,\ngenes such asMYC emerged as hub genes, interacting with multiple\nkey TFs, includingCTNNB1, JUNB,a n dCCNB1\n43–45. This network analysis\nprovides valuable insights into the regulatory mechanisms underlying\nSPI1-mediated gene expression, highlighting potential transcription\nfactor interactions that may drive downstream cellular processes.\nFurthermore, CellFM was able to capture distinct pathway-\nactivation patterns (Fig.5f) through the genes most inﬂuenced by\nSPI1\n46–48. For pathway enrichment analysis, we included all genes from\nthe KEGG pathway database as the background gene set to provide a\nbroader biological context. This approach was intentional to highlight\nthe pathways enriched by the top 20 genes most inﬂuenced bySPI1\nperturbation. The enriched pathways presented in Fig.5fe m p h a s i z e\nthose in whichSPI1 a n di t sp e r t u r b e dg e n e sm a ye x e r tac o o r d i n a t e d\ninﬂuence on important biological processes. For instance, the Human\nT-cell leukemia virus 1 infection (HTLV-1) containedSPI1, CCNB2,a n d\nCCNA2, holding particular signiﬁcance. First, theSPI1 gene plays a\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 10\na b\nc\ne f\nSPI1 most influenced genes (top20)\n00 ...\nEmbedding Module\nERetNet Layer ( )\nGeneA GeneB GeneC\nGeneC GeneB GeneA\nPerturbed Expression\nCCNB2/CCNA2/ORC6\nCCNB2/CCNA2/SPI1\nCCNA2/SPI1\nTOP2A/BIRC5\nCCNB2/CCNA2\nSPI1/JUND\nCCNB2/CCNA2\nCCNA2/BIRC5\nCCNA2/SPI1\nCCNA2/H2BC11\nTarget genes\nd\nFig. 5 | Gene-Gene Relationships Unveiled by CellFM. aA gene cluster comprising\nIL2, IL3,a n dIL4 was identiﬁed through the cosine similarity of gene embeddings\ngenerated by the pre-trained CellFM (zero-shot).b This analysis compares the\nnumber of enriched pathways derived from gene programs extracted by CellFM\n(zero-shot) and a coexpression network within an immune-related human dataset,\nacross various Leiden clustering resolutions.c This workﬂow outlines the process\nof identifying the most inﬂuenced genes through attention maps, where attention\nscores from perturbed cell states are sequentially ranked to select the most\nimpacted genes.d The heatmap displays the connectivity changes in the network of\nthe top 20 genes most inﬂuenced by the geneSPI1in aﬁne-tuning setting. The color\ngradient represents the correlation strength betweenSPI1 a n di t sp e r t u r b e dg e n e s ,\nranging from blue (weakest) to purple (strongest positive).e The network graph\nrepresents the top 20 genes, with ChIP-seq predicted targets validated in the ChIP-\nAtlas database, highlighted in light blue.f The heatmap displays KEGG (Kyoto\nEncyclopedia of Genes and Genomes) pathways enriched for the top 20SPI1-\nimpacted genes, identiﬁed through one-sided hypergeometric tests with\nBenjamini-Hochberg correction for multiple comparisons.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 11\ncritical role in leukemia stem cell (LSC) self-renewal49. In HTLV-1\ninfection,SPI1activation may promote the expansion of leukemia stem\ncells, enhancing their self-renewal capacity and accelerating leukemia\nprogression. Second, theCCNA2 gene primarily regulates cell cycle\ntransitions. HTLV-1-encoded oncoproteinsTax and HBZ disruptCCNA2\nexpression, causing cell cycle dysregulation and increasing cell\nimmortalization potential\n50. In HAM/TSP patients, observed down-\nregulation ofCCNA2 may be due toIRF-1 suppression, which could\nprevent transformation into ATLL. For the other pathway Acute mye-\nloid leukemia (AML) enrichedSPI1 and CCNA2 holding particular sig-\nniﬁcance. First,SPI1 is crucial for normal blood cell differentiation, and\nits dysregulation in AML leads to uncontrolled cell growth and blocked\ndifferentiation, promoting leukemia development\n46.I nA M L ,CCNA2 is\nabnormally overexpressed in certain subtypes, particularly in therapy-\nrelated AML (t-AML)\n51. This overexpression, along with other cell cycle\nregulation genes likeCCNE2and CDC2, is linked to poor prognosis and\nis often associated with chromosomal deletions of 5 and 7, which\ncorrelate with lower survival rates.\nWe also showed the case perturbed geneJUN (Supplementary\nFig. S27) and obtained the ChIP-seq BigWigﬁles (SRX10976151) for the\nJUN transcription factor. The ChIP-seq data forJUN were also obtained\nfrom publicly available datasets and visualized using IGV. Repre-\nsentative snapshots of JUN binding sites across various genomic\nregions are provided in the Supplementary Fig. S28. These visualiza-\ntions highlight signiﬁcant ChIP-seq peaks, offering direct evidence of\nJUN’s binding to speciﬁc genomic loci. Additionally, the pathway Focal\nadhesion in Supplementary Fig. S27c identiﬁed key genes such asJUN,\nCCND3,a n dCTNNB1, all of which play essential roles in cell adhesion\nand proliferation. Concretely,JUN encodes c-Jun, a component of the\nAP-1 complex that regulates gene expression and proliferation. It is\nclosely linked to focal adhesion pathways, particularly throughFAK\n(Focal Adhesion Kinase), which mediates cell attachment to the\nextracellular matrix (ECM) and inﬂuences migration and invasion\n52.\nCCND3, a regulator of the G1/S phase transition, is connected toFAK\nsignaling, linking cell adhesion to cell cycle progression53.A d d i t i o n a l l y ,\nCTNNB1 (β-catenin)through the Wnt/β-catenin pathway promotes cell\nadhesion by interacting with cadherins and modulatingFAK activity.\nThis coordinated regulation ofJUN, CCND3,a n dCTNNB1suggests their\ncollaborative role in driving cancer development and progression\nthrough cell adhesion and proliferation pathways\n54.\nDiscussion\nTo aid efﬁcient analysis of the single-cell data and harness the\nwealth of knowledge contained within single-cell atlas datasets, we\nhave introduced a state-of-the-art foundation model known as\nCellFM. This model was pre-trained on our meticulously curated\ndatasets, encompassing about 100 million human cells. These\ndatasets empower CellFM to generate an expansive set of 800\nmillion model parameters, marking an eightfold increase over the\nparameters present in the current single-cell models trained on a\nsingle species. To augment the training efﬁciency of CellFM, we\nhave adopted the ERetNet architecture as its core. This network\nrepresents an advancement over the traditional RetNet framework,\noffering enhanced parallel training capabilities and cost-effective\ninference. These features collectively contribute to CellFM ’s\nexceptional performance. Moreover, CellFM incorporates a Low-\nRank Adaptive module designed to minimize parameter count\nduring ﬁne-tuning, thereby optimizing the model for speciﬁct a s k s\nwithout compromising its generalizability. Through a series of\ncomprehensive experiments, CellFM has demonstrated its effec-\ntiveness across a range of single-cell tasks including cell type\nannotation, prediction of responses to perturbations, gene network\nanalysis, and gene function prediction.\nTo satisfy the model’s training on large-scale datasets, we have\nchosen a variant of the RetNet architecture as CellFM’s foundation,\ndiverging from the Transformers used in other single-cell founda-\ntion models. The RetNet architecture facilitates parallel, recurrent,\nand chunkwise processing, which we have reﬁned by integrating the\nSGLU module, amplifying training efﬁciency. Additionally, we have\nembedded the Low-Rank Adaptive (LoRA) strategy within CellFM,\noptimizing its training on new datasets with similar characteristics.\nThe combination of this efﬁcient training architecture and the\ncomprehensive datasets forms the basis for developing the current\nlargest CellFM model, equipped with 800 million parameters.\nCellFM is developed using the MindSpore AI framework from Hua-\nwei and is trained on four Huawei Altas800 servers, each equipped\nwith eight Ascend910 NPUs. Our rigorous experiments demon-\nstrated the model’s adaptability and potency in multiple single-cell\ndownstream tasks. In the spirit of research collaboration, we are\ndedicated to sharing our progress by making the CellFM codes and\nthe pre-trained model publicly available. This initiative aims to\nprovide researchers with a uniﬁed framework that streamlines the\nadoption of pre-trained models for their distinct research goals.\nWhile large human datasets have also been used to train multi-\nspecies models like UCE and GeneCompass, the number of human\ncells in these models did not exceed 50 million. In contrast, our\nmodel was trained on ~100 million human cells. Moreover, our\nmodel’s parameter size is eight times larger than GeneCompass\nand 1.23 times larger than UCE. As demonstrated in Figs.2–4,o u r\nmodel consistently outperformed GeneCompass and UCE in tasks\nsuch as cell type annotation, gene function prediction, and cell\nperturbation.\nDespite the advances in CellFM, several limitations remain to be\nexplored. Firstly, the attention map in CellFM was limited in capturing\ngene relationships related to static or global biological knowledge.\nIn the future, we will explore new explainability techniques to over-\ncome this challenge. Furthermore, the current model is limited by\nthe absence of multi-species data, which restricts its potential for\nbroader biological contexts and cross-species comparisons. Finally,\nthe model’s construction did not leverage existing biological prior\nknowledge, which could affect its depth and accuracy in interpreting\nbiological phenomena.\nMethods\nData collection\nAll training data utilized in this study were sourced from reputable\npublic databases. Speciﬁcally, from April 2021 to August 2023, we\nidentiﬁed datasets leveraging keywords like “single-cell RNA\nsequencing,”“ single-cell transcriptome, ” and “single-cell RNA. ”\nThese keywords were used to search through databases such as\nNCBI GEO\n23,E N A24,G S A25,26,I m m P o r t27, and others. In our selection\nprocess, we carefully curated the datasets, retaining only those\nhuman single-cell datasets that were relevant to our study. These\ndatasets were encountered in multiple formats, including FASTQ\ndata, expression matrices, and Seurat/Scanpy objects. Our initial\nstep involved transforming the raw FASTQ data into expression\nmatrices using primary analysis software supplied by the manu-\nfacturers. Following this, all obtained and transformed expression\nmatrices underwent pre-processing through a standardized work-\nﬂow provided by the SynEcoSys® single-cell database from Single-\nron Biotechnologies\n28. This workﬂow included several critical steps:\n(1) Quality control involvedﬁltering cells based on a minimum gene\ncount threshold of 200 genes per cell; (2) Gene name standardiza-\ntion was conducted by the HUGO Gene Nomenclature Committee\n(HGNC) guidelines, ensuring that gene aliases in each dataset were\nconverted to their respective HGNC-approved gene symbols. This\nstep guaranteed the uniqueness and consistency of gene names\nacross all datasets. (3) Finally, the expression matrices for each\nsample were converted into a uniﬁed sparse matrix format, pre-\nparing them for subsequent model training.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 12\nCellFM architecture\nThe CellFM model comprises three core components, including\nthe embedding module, the ERetNet module, and the LoRA\nmodule (Fig. 1). The embedding module in CellFM maps one-\ndimensional scalar values of gene expression to high-dimensional\nembedding features for model training, enabling the representa-\ntion of gene expressions in a high-dimensional space. CellFM then\napplies the ERetNet module to learn the relationships among\ngenes based on the gene expression information. In parallel,\nCellFM uses the LoRA module to help train CellFM more ef ﬁ-\nciently by reducing the number of parameters when adjusting\nmodel weights with new data.\nThe embedding module\nTo efﬁciently train CellFM, we have set an upper limit on the\nnumber of genes it inputs, deﬁned by the threshold lmax =2 0 4 8 .\nFor each cell, if the number of expressed genes exceedslmax,w e\nrandomly select lmax g e n e sw i t hh i g he x p r e s s i o nv a l u e s .C o n -\nversely, if a cell has fewer expressed genes thanlmax,w ep a dt h e\ngene IDs and set the padded values as zero because the model\narchitecture has aﬁxed length l\nmax for parallel computing. These\npadded values won ’t participate in the calculations during\nCellFM’s training and thus don’ti nﬂuence our models. We then\napply a Multilayer Perceptron (MLP) to map the scalar expression\nvalues of genes to embedding vectors necessary for the ERetNet\nmodule as follows:\nX\n1 = LeakyReLU X0W0\n/C0/C1\nX2 = X1 /C1 W1 + α /C1 X1\nX3 = Sof tmaxðX2ÞW2\nð1Þ\nwhere W0 2 R1× b, W1 2 Rb × b,a n dW2 2 Rb × d are learnable para-\nmeter matrices. The coefﬁcientα is a learnable residual coefﬁcient. The\nhyperparametersb and d are set tob = 256 andd = 1536, respectively.\nThe termX0 2 Rlmax ×1 represents the initial input cell pre-processed by\nthe aforementioned workﬂow.\nAs performed in previous LLMs, we randomly mask the\nexpressions of 20% of the genes (denoted asM) and then recover\nthem based on the non-masked genes. During gene masking, the\n20% of genes masked during the pre-training task were exclu-\nsively selected from non-padded genes, ensuring that CellFM\nfocused on reconstructing meaningful gene expression patterns\nusing the remaining relevant gene information. This design allows\nthe model to learn effective compression and meaningful repre-\nsentations without interference from padding values. Speciﬁcally,\nfor the M masked gene expressions, we replace the gene\nexpressions of the cell with a learnable weight vectorX\nM 2 R1× d\ninitialized to zero. Consequently, the featureXtmp can be obtained\nas follows:\nXtmp = M! /C12 X3 + ð1 /C0 M!Þ/C12 XM ð2Þ\nwhere ⊙ is the element-wise product, andM! 2f 0, 1glmax is the mask\nvector indicating the position of masked genes with value 0.\nTo learn the speciﬁc characteristic of each gene, we initialize a\nlearnable embedding matrixEG 2 R24079 ×d. The term 24079 repre-\nsents the number of genesID and thed =1 5 3 6m e a n st h ed i m e n s i o no f\nvector embeddings initialized based on each unique gene ID. We then\nintegrate the gene expression and gene ID embeddings as follows:\nX\nemb = ½EG\ng1, :::, EG\nglmax\n/C138+ Xtmp ð3Þ\nFurthermore, we incorporate an additional learnable weight\nXcls ∈ R1×d, which is appended to the gene expression embeddings. This\nweight facilitates the learning of cell-level features by aggregating gene\ninformation in the following manner:\nXexpr = Xcls k Xemb ð4Þ\nwhere the∥ symbol denotes the concatenation of two vectors.\nThe ERetNet module\nCellFM learns gene embeddings and relationships from gene expres-\nsion through the ERetNet module, a variant of the RetNet55.R e t N e ti s\nan efﬁcient, high-performance variant of the Transformer architecture.\nTo better adapt large-scale single-cell datasets, we have modiﬁed the\nRetNet module in two ways: First, we’ve replaced the traditional\nfeedforward network in RetNet with a gated bilinear network, which\nhas led to improved model performance and a smoother training\nprocess. Second, we’ve reﬁned the model’s training stability and per-\nformance by substituting the pre-layer LayerNorm in RetNet with the\nDeepNorm layer normalization technique\n56. Collectively, these mod-\niﬁcations have resulted in the ERetNet module, which includes a Gated\nMulti-head Attention (MHA), a Gated Linear Unit (SGLU), and layer\nnormalization (LN), all contributing to a more stable and effective gene\nexpression analysis model.\nGated multi-head attention (MHA). The Gated Multi-head Attention\n(MHA) block is used to learn dependencies between genes, which is a\nvariant of the Retention mechanism in the RetNet. To address the\ncomputational inefﬁciency of the exponential attention operations\n(Oðl\n2\nmaxdÞ) implemented in RetNet, we adopt the method proposed by\nShen et al.57.B yﬁrst computing the keys (K) and values (V), followed by\nthe queries (Q), this approach achieves a linear complexity ofO lmaxd2\nh\n/C16/C17\n,\nsigniﬁcantly reducing overhead. Furthermore, we scaled Q, K, and V\nfollowing the study57 to ensure the half-precision training as follows:\nQ =\nReLUðXW QÞﬃﬃ ﬃ\nd\np , K = ReLUðXW K Þﬃﬃ ﬃ\nd\np , V = XW Vﬃﬃﬃﬃﬃ ﬃ\nlcell\np\nAttention ðXÞ = QðKT ðM! /C12 VÞÞ\nð5Þ\nwhere lcell = lmax + 1 denotes the number of expressed genes within\neach cell,M! denotes the mask vector.\nTo enhance the CellFM ’s representational power, we use\nh = d/dhead attention heads in each ERetNet layer, wheredhead =3 2i s\nthe head dimension. Each head consists of three parameter matrices\nW\nQ, WK, WV 2 Rdhead × dhead . In addition, we add a swish gate58 to\nincrease the non-linearity of ERetNet layers. Formally, given inputXexpr,\nwe deﬁne the MHA layer as follows:\nheadi = Attention iðXexpr Þ\nY = GroupNorm h Concatðheadi, :::, headhÞ\n/C0/C1\nMHAðXexpr Þ = ðSwishðXexpr WGÞ/C12 YÞWO\nð6Þ\nwhere WG, WO 2 Rd × d are learnable parameters, and GroupNorm\nnormalizes59 the output of each head, following SubLN proposed in the\nstudy60.\nSimple Gated Linear Unit (SGLU). To improve model performance\nand a smoother training process, we’ve replaced the traditional feed-\nforward network in RetNet with a gated bilinear network. The gated\nunit GLU introduces a multiplicative gating mechanism that explicitly\nindicates the model’s memory of each feature dimension, thereby\nsmoothing the training process and facilitating better integration\nbetween channels. Considering that the gating mechanism inherently\nintroduces nonlinear relationships, to further accelerate the model’s\ncomputation, this work, referring to literature\n61,a d o p t st h eS G L U ,\nwhich is based on the GLU formula62 but omits the Swish activation\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 13\nfunction:\nSGLUðXÞ = ðXW u /C12 XW vÞWo ð7Þ\nwhere ⊙ is the element-wise product.\nLayer normalization (LN). The Transformer architecture in large\nsingle-cell models typically uses post-norm normalization after resi-\ndual connections to enhance model depth and convergence. However,\nthis can cause a gradient explosion as the model size grows. To\ncounteract this, a pre-norm strategy is applied in RetNet for a stabi-\nlized training process, albeit with a potential performance trade-off. To\naddress this gap, CellFM employs the new post-norm normalization\nmethod DeepNorm\n56. DeepNorm reduces the contribution ratio of\neach network block to the output, thereby reducing the amount of\ngradient that needs to be updated and ensuring the stability of train-\ning.\nY\nðlÞ = LNðMHAðXðlÞÞ + λ /C1 XðlÞÞ\nXðl +1 Þ = LNðSGLUðYðlÞÞ + λ /C1 YðlÞÞ\nð8Þ\nwhere LN(⋅ ) is LayerNorm andλ is a hyperparameter.\nLow-rank adaptation (LoRA) module\nLarge models typically comprise hundreds of millions of parameters,\nresulting in considerable time consumption for full model training. To\nalleviate the burden of training on various datasets, we employ the\nLow-Rank Adaptation (LoRA) algorithm\n63. LoRA operates under the\nassumption that updates to pre-trained weights duringﬁne-tuning can\nbe decomposed by low rank. Hence, for a pre-trained weight matrix\nW0 ∈ Rn×k, we utilize low-rank decomposition to constrain the weight\nincrement ∇ W:\nW0 + ΔW = W0 + BA ð9Þ\nwhere B ∈ Rd×r, A ∈ Rr×k, and the rankr<< minðd, kÞ.\nDuring the forward computation, bothW0 and matricesA and B\na r eu s e di nc a l c u l a t i o n sw i t ht h ei n p u tX; however, during the back-\nward propagation, theW0 parameter is frozen and does not undergo\ngradient updates, while onlyA and B are updated. It can be observed\nthat in regular training, the training parameter count forW0 is n × k,\nwhereas, with LoRA-based training, the training parameter count for\nW0 is the sum of the parameters ofA and B, which is (n + k)× r.S i n c et h e\ndimensionr is signiﬁcantly smaller thann and k, the number of training\nparameters for the weights is greatly reduced, leading to a substantial\ndecrease in computational overhead.\nIn the ERetNet architecture, the MHA hasﬁve weight matrices:W\nQ,\nWK, WV, WG,a n dWO, and the SGLU gating unit has three weight\nmatrices: Wu, Wv,a n dWo. We consider the dimensions of these 8\nmatrices to bed × d. In this experiment, we limit the application of\nLoRA to only the ERetNet encoder part and freeze all model para-\nmeters for updates except for the weights of the LayerNorm layer.\nLoss functions\nMean squared error (MSE). In CellFM, we focus on minimizing the\nMean Squared Error (MSE) as the primary metric because it effectively\nmeasures the discrepancy between the predicted and actual gene\nvector embeddings for masked genes. MSE is particularly suitable in\nthis context as it penalizes larger errors more heavily, making it crucial\nfor accurately recovering gene representations. Additionally, MSE has\nbeen widely adopted in similar tasks involving gene expression pre-\ndiction and representation learning. For example, scFoundation\n18 and\nGeneCompass19 employ MSE to optimize gene expression prediction in\nhigh-dimensional spaces, demonstrating its effectiveness in promoting\nprecise modeling. Speciﬁcally, we employ a fully connected MLP\nfollowed by the ERetNet module to estimate the expression value forM\ngenes. The optimization of this objective involves utilizing the MSE loss\nat the masked positions, denoted asM mask. The MSE works as follows:\n^y\ni = MLPðxðLÞ\ni Þ\nLMSE = 1\njMmaskj\nX\n8i, Mðmask, iÞ =1\n~yi /C0 yi\n/C0/C1 2 ð10Þ\n∣Mmask∣ denotes the count of ones in the mask gene vectorMmask for\neach cell.XðLÞ\ni signiﬁes the features derived from the ERetNet for genei\nat theL layer.\nTo further enhance CellFM’s learning capabilities and channel\naggregation, the feature corresponding to tokencls is also leveraged,\nrepresented asXðLÞ\ncls 2 R1× d. This feature passes through an additional\nnetwork designed to predict expression values. Speciﬁcally, a learn-\nable parameter matrixWcls 2 Rd × d and an activation function are\nemployed. After mapping XðLÞ\ncls , the result is multiplied with the\nembeddings of the genes to be predicted in the vocabularyEG to\ncompute inner products, yielding another set of predicted values.\nThese are then compared with the actual values to calculate the mean\nsquared error loss.\nx = σðx\nðLÞ\ncls Wcls Þ\n/C22yi = x@EG\ni\nLcls = 1\njMmask j\nX\n8i, Mðmask, iÞ =1\nð/C22yi /C0 yiÞ2\nð11Þ\nThe @ symbol represents matrix multiplication.σ is an activation\nfunction Sigmoid.EG\ni denotes the embedding of genei in the voca-\nbulary EG. Finally, the total loss functions of CellFM can be obtained as\nfollows:\nLtotal = LMSE + Lcls ð12Þ\nBaseline single-cell foundation models\nWe have incorporated several recent single-cell foundation models\ninto our benchmarking, including scFoundation, GeneCompass, UCE,\nscELMo, scBERT, scGPT, and Geneformer. These models represent\ndifferent approaches to single-cell analysis, and we have categorized\nthem based on their methodological focus. Speciﬁcally, models\nincluding CellFM, scFoundation, scELMo, and GeneCompass fall under\nthe value projection category, while UCE, scGPT, and scBERT belong to\nthe value categorization category, and Geneformer is placed in the\nordering category.\nImplementation details\nCellFM consists of 40 stacked ERetNet blocks, with each block having 48\nattention heads. During pre-training, we used ~100 million cells for\ntraining CellFM. The model was optimized using the Adam optimizer\nwith a starting learning rate of 1e-7 and trained for a total of 2 epochs. The\ntotal batch size was 128, distributed equally across 4 Huawei\nAltas800 servers, each equipped with 8 Ascend910 NPUs. The MindSpore\nAI development framework powered the automatic data parallel training.\nThe decision to train the CellFM model for two epochs was\ninformed by standard practices in large-scale model training\n9,w h e r e\nrapid convergence is typically observed within the initial epochs. To\nvalidate this convergence of CellFM, we conducted the experiment\nusing the 80-million-parameter version of CellFM on all training\ndatasets. The results conﬁrmed the same pattern: the loss dropped\nsharply-from 8 to below 1-during theﬁrst epoch, with only minimal\nchanges in the second epoch (in Supplementary Fig. S29). This beha-\nvior reﬂects the typical convergence dynamics of large-scale models\nand supports our choice to limit training to two epochs to balance\nefﬁciency and performance.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 14\nPre-processing\nThe gene symbols across all raw count gene expression matrices were\nstandardized using the reference mapping provided by the HUGO Gene\nNomenclature Committee. This process included both human protein-\ncoding genes and common mitochondrial genes, resulting in a compre-\nhensive gene setG consisting of 24,078 genes. Ultimately, normalization\nand a log1p transformation were employed across all gene expression\nmatrices to alleviate skewness in the data. For the cell type annotation\ntask, we excluded cell types with fewer than 10 cells in the training data,\nas well as those present in the query data but absent in the reference data.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nT h ed a t a s e t su s e di nt h i ss t u d ya r ep u b l i c l ya c c e s s i b l et h r o u g ht h ec i t e d\npublications, with detailed links provided in Supplementary Table S4.\nDue to the large cell population and computational time and resource\nlimitations, the human brain CellAtlas dataset, originally comprising over\n3 million cells, was downsampled to 126,339 cells using a sampling rate\nof 3.75%, with cells randomly selected by tissue type. Similarly, for the\nTabula Sapiens dataset, both versions (V1 and V2) were downsampled by\ntissue type at a rate of 25%, resulting in 111,013 and 172,999 cells,\nrespectively, to ensure computational efﬁciency while maintaining bio-\nlogical representativeness. Source data are provided with this paper. The\ndata used in this study have been uploaded to Zenodo and are freely\navailable at:https://doi.org/10.5281/zenodo.15138665\n64.\nCode availability\nAll codes used in this study can be available at:https://github.com/\nbiomed-AI/CellFM. The codes used in this study have been uploaded to\nZenodo and are freely available at:https://doi.org/10.5281/zenodo.\n1515590065.\nReferences\n1. Han, X. et al. Construction of a human cell landscape at single-cell\nlevel. Nature 581,3 0 3– 309 (2020).\n2. Lotfollahi, M., Hao, Y., Theis, F. J. & Satija, R. The future of rapid and\nautomated single-cell data analysis using reference mapping.Cell\n187,2 3 4 3– 2358 (2024).\n3. Xu, C. et al. Automatic cell-type harmonization and integration\nacross human cell atlas datasets.Cell 186, 5876– 5891 (2023).\n4. Lu, Y. et al. A single-cell atlas of the multicellular ecosystem of\nprimary and metastatic hepatocellular carcinoma.Nat. Commun.\n13, 4594 (2022).\n5 . W o l f ,F .A . ,A n g e r e r ,P .&T h e i s ,F .J .S c a n p y :l a r g e - s c a l es i n g l e - c e l l\ngene expression data analysis.Genome Biol.19,1 – 5( 2 0 1 8 ) .\n6. Hao, Y. et al. Dictionary learningfor integrative, multimodal and\nscalable single-cell analysis.Nat. Biotechnol.42,2 9 3– 304 (2023).\n7 . Z e n g ,Y . ,W e i ,Z . ,P a n ,Z . ,L u ,Y .&Y a n g ,Y .Ar o b u s ta n ds c a l a b l e\ngraph neural network for accurate single-cell classiﬁcation. Brief.\nBioinforma.23, bbab570 (2022).\n8. Liu, T., Li, K., Wang, Y., Li, H. & Zhao, H. Evaluating the utilities of\nlarge language models in single-cell data analysis. Preprint at\nbioRxiv https://doi.org/10.1101/2023.09.08.555192(2023).\n9. Touvron, H. et al. Llama: open and efﬁcient foundation language\nmodels. Preprint atarXiv https://doi.org/10.48550/arXiv.2302.\n13971.09685(2023).\n10. Levine, D. et al. Cell2sentence: teaching large language models the\nlanguage of biology. Preprint atbioRxiv https://doi.org/10.1101/\n2023.09.11.557287(2023).\n11. Chen, Y. T. & Zou, J. Genept: a simple but hard-to-beat foundation\nmodel for genes and cells built from ChatGPT. Preprint atbioRxiv\nhttps://doi.org/10.1101/2023.10.16.562533(2023).\n12. Shen, H. et al. A universal approach for integrating super large-\nscale single-cell transcriptomes by exploring gene rankings.Brief.\nBioinforma.23, bbab573 (2022).\n13. Shen, H. et al. Generative pretraining from large-scale tran-\nscriptomes for single-cell deciphering.Iscience26,1 0 6 5 3 6( 2 0 2 3 ) .\n14. Theodoris, C. V. et al. Transfer learning enables predictions in net-\nwork biology.Nature 618,6 1 6– 624 (2023).\n15. Yang, F. et al. scbert as a large-scale pretrained deep language\nmodel for cell type annotation of single-cell RNA-seq data.Nat.\nMach. Intell.4,8 5 2– 866 (2022).\n16. Cui, H. et al. scgpt: toward building a foundation model for single-\ncell multi-omics using generative AI.Nat. Methods21,1 4 7 0– 1480\n(2024).\n17. Rosen, Y. et al. Universal cell embeddings: a foundation model for\ncell biology.bioRxiv Preprint athttps://doi.org/10.1101/2023.11.28.\n568918 (2023).\n18. Hao, M. et al. Large-scale foundation model on single-cell tran-\nscriptomics.Nat. methods21,1 4 8 1– 1491 (2024).\n19. Yang, X. et al. Genecompass: deciphering universal gene reg-\nulatory mechanisms with a knowledge-informed cross-species\nfoundation model.Cell Res.34,8 3 0– 845 (2024).\n20. Liu, T., Chen, T., Zheng, W., Luo, X. & Zhao, H. Scelmo: embeddings\nfrom language models are good learners for single-cell data ana-\nlysis. Preprint atbioRxiv https://doi.org/10.1101/2023.12.07.\n569910 (2023).\n21. Hoffmann, J. et al. Training compute-optimal large language\nmodels. arXiv preprinthttps://doi.org/10.48550/arXiv.2203.\n15556 (2022).\n22. Kaplan, J. et al. Scaling laws for neural language models.arXiv\nPreprinthttps://doi.org/10.48550/arXiv.2001.08361(2020).\n23. Barrett, T. et al. Ncbi geo: archive for functional genomics data sets-\n\"update.Nucleic Acids Res.41, D991– D995 (2012).\n24. Cochrane, G. et al. Priorities for nucleotide trace, sequence and\nannotation data capture at the ensembl trace archive and the EMBL\nnucleotide sequence database.Nucleic Acids Res.36,D 5– D12\n(2007).\n25. Chen, T. et al. The genome sequence archive family: toward\nexplosive data growth and diverse data types.Genomics, Proteom.\nBioinforma.19,5 7 8– 583 (2021).\n26. Bai, X. et al. Database resources of the national genomics data\ncenter, china national center for bioinformation in 2024.Nucleic\nAcids Res.52,D 1 8– D32 (2024).\n27. Bhattacharya, S. et al. Immport, toward repurposing of open access\nimmunological assay data for translational and clinical research.\nSci. data5,1 – 9( 2 0 1 8 ) .\n28. Zhang, Y. et al. Synecosys: a multifunctional platform of large-scale\nsingle-cell omics data analysis. Preprint atbioRxiv https://doi.org/\n10.1101/2023.02.14.528566(2023).\n29. Salzberg, S. L. Open questions: how many genes do we have?BMC\nBiol. 16,9 4( 2 0 1 8 ) .\n30. Yuan, Q., Xie, J., Xie, J., Zhao, H. & Yang, Y. Fast and accurate protein\nfunction prediction from sequence through pretrained language\nmodel and homology-based label diffusion.Brief. Bioinforma.24\n,\nbbad117 (2023).\n31. Adamson, B. et al. A multiplexed single-cell crispr screening plat-\nform enables systematic dissection of the unfolded protein\nresponse.Cell 167,1 8 6 7– 1882 (2016).\n32. Norman, T. M. et al. Exploring genetic interaction manifolds con-\nstructed from rich single-cell phenotypes.Science 365,\n786– 793 (2019).\n33. Bunne, C. et al. Learning single-cell perturbation responses using\nneural optimal transport.Nat. methods20,1 7 5 9– 1768 (2023).\n34. Ma, W., Su, K. & Wu, H. Evaluation of some aspects in supervised\ncell type identiﬁcation for single-cell RNA-seq: classiﬁer, feature\nselection, and reference construction.Genome Biol.22,1 – 23 (2021).\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 15\n35. Abdelaal, T. et al. A comparison of automatic cell identiﬁcation\nmethods for single-cell RNA sequencing data.Genome Biol.20,\n1– 19 (2019).\n36. Han, Y. et al. Tisch2: expanded datasets and new tools for single-\ncell transcriptome analyses of the tumor microenvironment.\nNucleic Acids Res.51,D 1 4 2 5– D1431 (2023).\n37. Xin, J. et al. Downregulation of long noncoding RNA hotairm1 pro-\nmotes monocyte/dendritic cell differentiation through competi-\ntively binding to endogenous mir-3960.Onco Targets Ther.10,\n1307– 1315 (2017).\n38. Hafemeister, C. & Satija, R. Normalization and variance stabilization\nof single-cell RNA-seq data using regularized negative binomial\nregression.Genome Biol.20, 296 (2019).\n39. Lu, Q. et al. Effect of penthorum chinense pursh compound on\nAFB1-induced immune imbalance via jak/stat signaling pathway in\nspleen of broiler chicken.Vet. Sci.10, 521 (2023).\n40. Gündogdu, M. S. et al. The haematopoietic gtpase rhoh modulates\nIL3 signalling through regulation of stat activity and IL3 receptor\nexpression.Mol. cancer9,1 – 13 (2010).\n4 1 . Z o u ,Z . ,O h t a ,T . ,M i u r a ,F .&O k i ,S .C h i p - a t l a s2 0 2 1u p d a t e :ad a t a -\nmining suite for exploring epigenomic landscapes by fully inte-\ngrating chip-seq, atac-seq and bisulﬁte-seq data.Nucleic Acids Res.\n50,W 1 7 5– W182 (2022).\n42. Han, H. et al. Trrust v2: an expanded reference database of human\nand mouse transcriptional regulatory interactions.Nucleic acids\nRes. 46,D 3 8 0– D386 (2018).\n4 3 . L i u ,X .e ta l .β-catenin overexpression in malignant glioma and its\nrole in proliferation and apoptosis in glioblastma cells.Med. Oncol.\n28,6 0 8– 614 (2011).\n44. Acosta, J. C. et al. Myc inhibits p27-induced erythroid differentiation\nof leukemia cells by repressing erythroid master genes without\nreversing p27-mediated cell cycle arrest.Mol. Cell Biol.24,\n7286– 95 (2008).\n45. Yin, X.-Y. et al. Inverse regulation of cyclin b1 by c-myc and p53 and\ninduction of tetraploidy by cyclin b1 overexpression.Cancer Res.\n61,6 4 8 7– 6493 (2001).\n46. Wang, X., Jin, P., Zhang, Y. & Wang, K. Circspi1 acts as an oncogene\nin acute myeloid leukemia through antagonizing spi1 and interact-\ning with micrornas.Cell death Dis.12,2 9 7( 2 0 2 1 ) .\n4 7 . Z h u ,H .e ta l .T - a l ll e u k e m i as t e mc e l l’stemness’\nis epigenetically\ncontrolled by the master regulator spi1.Elife 7, e38314 (2018).\n48. Lawrence, A.-L. E. et al. Salmonella enterica serovar typhimurium\nSpi-1 and Spi-2 shape the global transcriptional landscape in a\nhuman intestinal organoid model system.MBio 12,1 0– 1128 (2021).\n49. Hegde, S., Hankey, P. & Paulson, R. F. Self-renewal of leukemia stem\ncells in friend virus-induced erythroleukemia requires proviral\ninsertional activation of Spi1 and hedgehog signaling but not\nmutation of p53.Stem Cells30,1 2 1– 130 (2012).\n50. Saffari, M. et al. Coevolution of htlv-1-hbz, tax, and proviral load with\nhost irf-1 and ccna-2 in ham/tsp patients.Infect., Genet. Evolution\n103, 105337 (2022).\n51. Qian, Z. et al. Cytogenetic and genetic pathways in therapy-related\nacute myeloid leukemia.Chem.-Biol. Interact.184,5 0– 57 (2010).\n52. Wang, Y., Ma, J., Chen, L., Xie, X.-L. & Jiang, H. Inhibition of focal\nadhesion kinase on hepatic stellate-cell adhesion and migration.\nA m .J .M e d .S c i .353,4 1– 48 (2017).\n53. Yamamoto, D. et al. FAK overexpression upregulates cyclin D3 and\nenhances cell proliferation via the pkc and pi3-kinase-akt pathways.\nCell. Signal.15,5 7 5– 583 (2003).\n54. Francalanci, P. et al. Focal adhesion kinase (fak) over-expression\nand prognostic implication in pediatric hepatocellular carcinoma.\nInt. J. Mol. Sci.21, 5795 (2020).\n55. Sun, Y. et al. Retentive network: a successor to transformer for large\nlanguage models. Preprint athttps://doi.org/10.48550/arXiv.2307.\n08621 (2023).\n56. Wang, H. et al. Deepnet: scaling transformers to 1000 layers.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n(IEEE, 2024).\n57. Shen, Z., Zhang, M., Zhao, H., Yi, S. & Li, H.Efﬁcient Attention:\nAttention With Linear Complexities.I nProceedings of the IEEE/CVF\nWinter Conference On Applications Of Computer Vision,\n3531– 3539 (2021).\n58. Ramachandran, P., Zoph, B. & Le, Q. V. Swish: a self-gated activation\nfunction.arXiv Prepr. arXiv:1710. 059417, 5 (2017).\n59. Wu, Y. & He, K. Group normalization. InProceedings of the European\nconference on computer vision (ECCV),3 – 19 (2018).\n60. Shoeybi, M. et al. Megatron-lm: training multi-billion parameter\nlanguage models using model parallelism. Preprint atarXiv https://\ndoi.org/10.48550/arXiv.1909.08053(2019).\n61. Qin, Z. et al. Transnormerllm: a faster and better large language\nmodel with improved transnormer.arXiv 2307.14995 (2023).\n62. Shazeer, N. et al. Glu variants improve transformer. Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.2002.05202(2020).\n63. Hu, E. J. et al. Lora: low-rank adaptation of large language models.\nICLR 1.2 (2022).\n64. Zeng, Y. et al. CellFM: a large-scale foundation model pre-trained\non transcriptomics of 100 million human cells.\nZenodo https://doi.\norg/10.5281/zenodo.15138665(2025).\n65. Zeng, Y. et al. CellFM: a large-scale foundation model pre-trained\non transcriptomics of 100 million human cells.Zenodo https://doi.\norg/10.5281/zenodo.15155900(2025).\nAcknowledgements\nThis study has been supported by the National Key R&D Program of\nChina (2023YFF1204900 to Z.H.), the Fundamental Research Funds for\nthe Central Universities (2024IAIS-QN020 to Z.Y.), the National Natural\nScience Foundation of China (T2394502 to Y.Y. and 62402071 to Z.Y.),\nthe Postdoctoral Fellowship Program of CPSF (GZC20233321 to Z.Y.),\nthe China Postdoctoral Science Foundation (2024M763866 to Z.Y.),\nand the National Science Foundation of Jiangsu Province\n(BK20230278 to J.F.).\nAuthor contributions\nY.Z., W.Y. and Y.Y. conceived and supervised the project. Y.Z., Z.W., J.X.,\nN.S.,Y.S., C.Z. and W.L. contributed to the algorithm implementation.\nY.Z. and Y.Y.wrote the manuscript. Y.Z., S.Y., J.Z., N.F., H.Zhang, Y.L.,\nH.Zhao, J.F. and Y.Y.were involved in the discussion and proofreading.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-59926-5.\nCorrespondenceand requests for materials should be addressed to\nYuansong Zeng, Huiying Zhao, Jue Fan, Weijiang Yu or Yuedong Yang.\nPeer review informationNature Communicationsthanks Yang Xu, who\nco-reviewed with Min Dai, George Gavriilidis and the other, anonymous,\nreviewer(s) for their contribution to the peer review of this work. A peer\nreview ﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 16\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if you modiﬁed the licensed\nmaterial. You do not have permission under this licence to share adapted\nmaterial derived from this article or parts of it. The images or other third\nparty material in this article are included in the article’s Creative\nCommons licence, unless indicatedotherwise in a credit line to the\nmaterial. If material is not included in the article’s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this licence, visithttp://\ncreativecommons.org/licenses/by-nc-nd/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-59926-5\nNature Communications|         (2025) 16:4679 17",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.715035080909729
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5474331378936768
    },
    {
      "name": "Transcriptome",
      "score": 0.47212085127830505
    },
    {
      "name": "Computational biology",
      "score": 0.4547710418701172
    },
    {
      "name": "Computer science",
      "score": 0.42338237166404724
    },
    {
      "name": "Data science",
      "score": 0.35336172580718994
    },
    {
      "name": "Bioinformatics",
      "score": 0.3239951431751251
    },
    {
      "name": "Biology",
      "score": 0.2990264296531677
    },
    {
      "name": "Gene expression",
      "score": 0.1856878101825714
    },
    {
      "name": "Genetics",
      "score": 0.1589706540107727
    },
    {
      "name": "Gene",
      "score": 0.15779924392700195
    },
    {
      "name": "Geography",
      "score": 0.10731852054595947
    },
    {
      "name": "Archaeology",
      "score": 0.07228434085845947
    },
    {
      "name": "Cartography",
      "score": 0.06486126780509949
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158842170",
      "name": "Chongqing University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097354",
      "name": "Sun Yat-sen Memorial Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210109211",
      "name": "Singleron Biotechnologies (china)",
      "country": "CN"
    }
  ]
}