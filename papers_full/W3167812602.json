{
  "title": "GraphiT: Encoding Graph Structure in Transformers",
  "url": "https://openalex.org/W3167812602",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Mialon, Gr\\'egoire",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351111922",
      "name": "Chen De-xiong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288868116",
      "name": "Selosse, Margot",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2695418907",
      "name": "Mairal, Julien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288819942",
      "name": "Mialon, Grégoire",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2026625220",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W2097308346",
    "https://openalex.org/W3035239023",
    "https://openalex.org/W2089703377",
    "https://openalex.org/W1816257748",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W1774304772",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2159156271",
    "https://openalex.org/W2558748708",
    "https://openalex.org/W2115467367",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W2112545207",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2791092480",
    "https://openalex.org/W2147286743",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W2970843311",
    "https://openalex.org/W2970474218",
    "https://openalex.org/W2963925437"
  ],
  "abstract": "We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.",
  "full_text": "GraphiT: Encoding Graph Structure in Transformers\nGrégoire Mialon∗\nInria†‡\ngregoire.mialon@inria.fr\nDexiong Chen∗\nInria†\ndexiong.chen@inria.fr\nMargot Selosse∗\nInria†\nmargot.selosse@inria.fr\nJulien Mairal\nInria†\njulien.mairal@inria.fr\nJune 11, 2021\nAbstract\nWe show that viewing graphs as sets of node features and incorporating structural and positional\ninformation into a transformer architecture is able to outperform representations learned with classical\ngraph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative\npositional encoding strategies in self-attention scores based on positive deﬁnite kernels on graphs, and (ii)\nenumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate\nthese two ideas on many classiﬁcation and regression tasks, demonstrating the eﬀectiveness of each of\nthem independently, as well as their combination. In addition to performing well on standard benchmarks,\nour model also admits natural visualization mechanisms for interpreting graph motifs explaining the\npredictions, making it a potentially strong candidate for scientiﬁc applications where interpretation is\nimportant.1\n1 Introduction\nGraph-structured data are present in numerous scientiﬁc applications and are the subject of growing interest.\nExamples of such data are as varied as proteins in computational biology [29], which may be seen as a sequence\nof amino acids, but also as a graph representing their tertiary structure, molecules in chemoinformatics [11],\nshapes in computer vision and computer graphics [38], electronic health records, or communities in social\nnetworks. Designing graph representations for machine learning is a particularly active area of research, even\nthough not new [4], with a strong eﬀort currently focused on graph neural networks [6, 7, 18, 28, 37, 40]. A\nmajor diﬃculty is to ﬁnd graph representations that are computationally tractable, adaptable to a given task,\nand capable of distinguishing graphs with diﬀerent topological structures and local characteristics.\nIn this paper, we are interested in the transformer, which has become the standard architecture for natural\nlanguage processing [36], and is gaining ground in computer vision [9] and computational biology [27]. The\nability of a transformer to aggregate information across long contexts for sequence data makes it an interesting\nchallenger for GNNs that successively aggregate local information from neighbors in a multilayer fashion.\nIn contrast, a single self-attention layer of the transformer can potentially allow all the nodes of an input\ngraph to communicate. The price to pay is that this core component is invariant to permutations of the input\nnodes, hence does not take the topological structure of the graph into account and looses crucial information.\n∗Equal contribution.\n†Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.\n‡D.I., UMR 8548, École Normale Supérieure, Paris, France.\n1Code available athttps://github.com/inria-thoth/GraphiT.\n1\narXiv:2106.05667v1  [cs.LG]  10 Jun 2021\nFor sequence data, this issue is addressed by encoding positional information of each token and giving it\nto the transformer architecture. For graphs, the problem is more challenging as there is no single way to\nencode node positions. For this reason, there have been few attempts to use vanilla transformers for graph\nrepresentation. To the best of our knowledge, the closest work to ours seems to be the graph transformer\narchitecture of Dwivedi and Bresson [12], who propose an elegant positional encoding strategy based on the\neigenvectors of the graph Laplacian [1]. However, they focus on applying attention to neighboring nodes only,\nas in GNNs, and their results suggest that letting all nodes communicate is not a competitive approach.\nOur paper provides another perspective and reaches a slightly diﬀerent conclusion; we show that even\nthough local communication is indeed often more eﬀective, the transformer with global communication can\nalso achieve good results. For that, we introduce a set of techniques to encode the local graph structure\nwithin our model, GraphiT (encodinggraph structure in transformers). More precisely, GraphiT relies\non two ingredients that may be combined, or used independently. First, we propose relative positional\nencoding strategies for weighting attention scores by using positive deﬁnite kernels, a viewpoint introduced\nfor sequences in [35]. This concept is particularly appealing for graphs since it allows to leverage the rich\nliterature on kernels on graphs [21, 33], which are powerful tools for encoding the similarity between nodes.\nThe second idea consists in computing features encoding the local structure in the graph. To achieve this, we\nleverage the principle of graph convolutional kernel networks (GCKN) of [7], which consists in enumerating\nand encoding small sub-structure (for instance, paths or subtree patterns), which may then be used as an\ninput to the transformer model.\nWe demonstrate the eﬀectiveness of our approach on several classiﬁcation and regression benchmarks,\nshowing that GraphiT with global or local attention layer can outperform GNNs in various tasks, and also\nshow that basic visualization mechanisms allow us to automatically discover discriminative graph motifs,\nwhich is potentially useful for scientiﬁc applications where interpretation is important.\n2 Related work\nGraph kernels. A classical way to represent graphs for machine learning tasks consists in deﬁning a\nhigh-dimensional embedding for graphs, which may then be used to perform prediction with a linear models\n(e.g., support vector machines). Graph kernels typically provide such embeddings by counting the number of\noccurrences of local substructures that are present within a graph [4]. The goal is to choose substructures\nleading to expressive representations suﬃciently discriminative, while allowing fast algorithms to compute\nthe inner-products between these embeddings. For instance, walks have been used for such a purpose [14],\nas well as shortest paths [5], subtrees [15, 24, 31], or graphlets [32]. Our work uses short paths, but other\nsubstructures could be used in principle. Note that graph kernels used in the context of comparing graphs,\nis a line of work diﬀerent from the kernels on graphs that we will introduce in Section 3 for computing\nembeddings of nodes.\nGraph neural networks. Originally introduced in [28], GNNs have been derived as an extension of\nconvolutions for graph-structured data: they use a message passing paradigm in which vectors (messages) are\nexchanged (passed) between neighboring nodes whose representations are updated using neural networks.\nMany strategies have been proposed to aggregate features of neighboring nodes (see,e.g, [6, 11]). The\ngraph attention network (GAT) [37] is the ﬁrst model to use an attention mechanism for aggregating local\ninformation. Recently, hybrid approaches between graph neural networks and graph kernels were proposed\nin [7, 10]. Diﬀusion processes on graphs that are related to the diﬀusion kernel we consider in our model were\nalso used within GNNs in [20].\nTransformers for graph-structured data. Prior to [12], there were some attempts to use transformers\nin the context of graph-structured data. The authors of [22] propose to apply attention to all nodes, yet\nwithout position encoding. In [42], a transformer architecture called Graph-BERT is fed with sampled\nsubgraphs of a single graph in the context of node classiﬁcation and graph clustering. They also propose\nto encode positions by aggregating diﬀerent encoding schemes. However, these encoding schemes are either\n2\nimpossible to use in our settings as they require having sampled subgraphs of regular structures as input, or\nless competitive than Laplacian eigenvectors as observed in [12]. The transformer model introduced in [41]\nneeds to ﬁrst transform an heteregoneous graph into a new graph structure via meta-paths, which does not\ndirectly operate on node features. To the best of our knowledge, our work is the ﬁrst to demonstrate that\nvanilla transformers with appropriate node position encoding can compete with GNNs in graph prediction\ntasks.\n3 Preliminaries about Kernels on Graphs\nSpectral graph analysis. The Laplacian of a graph withn nodes is deﬁned asL= D−A, whereD is a\nn×n diagonal matrix that carries the degrees of each node on the diagonal andA is the adjacency matrix.\nInterestingly,Lis a positive semi-deﬁnite matrix such that for all vectoruin Rn, u⊤Lu= ∑\ni∼j(u[i] −u[j])2,\nwhich can be interpreted as the amount of “oscillation” of the vectoru, hence its “smoothness’, when seen as\na function on the nodes of the graph.\nThe Laplacian is often used via its eigenvalue decompositionL = ∑\niλiuiu⊤\ni , where the eigenvalue\nλi = u⊤\ni Lui characterizes the amount of oscillation of the corresponding eigenvectorui. For this reason, this\ndecomposition is traditionally viewed as the discrete equivalent to the sine/cosine Fourier basis inRn and\nassociated frequencies. Note that very often the normalized LaplacianI−D−1\n2 AD−1\n2 is used instead ofL,\nwhich does not change the above interpretation.\nInterestingly, it is then possible to deﬁne a whole family of positive deﬁnite kernels on the graph [33] by\napplying a regularization functionr to the spectrum ofL. We therefore get a rich class of kernels\nKr =\nm∑\ni=1\nr(λi)uiu⊤\ni , (1)\nassociated with the following norm∥f∥2\nr = ∑m\ni=1 (f⊤\ni ui)2/r(λi) from a reproducing kernel Hilbert space\n(RKHS), wherer: R ↦→R+\n∗ is a non-increasing function such that smoother functions on the graph would\nhave smaller norms in the RKHS. We now introduce two examples.\nDiﬀusion Kernel [21]. It corresponds to the caser(λi) =e−βλi , which gives:\nKD =\nm∑\ni=1\ne−βλi uiu⊤\ni = e−βL = lim\np→+∞\n(\nI−β\npL\n)p\n. (2)\nThe diﬀusion kernel can be seen as a discrete equivalent of the Gaussian kernel, a solution of the heat\nequation in the continuous setting, hence its name. Intuitively, the diﬀusion kernel between two nodes can be\ninterpreted as the quantity of some substance that would accumulate at the ﬁrst node after a given amount\nof time (controlled byβ) if we injected the substance at the other node and let it diﬀuse through the graph.\nIt is related to the random walk kernel that will be presented below.\np-step random walk kernel. By takingr(λi) = (1−γλi)p , we obtain a kernel that admits an interpre-\ntation in terms of p steps of a particular random walk on the graph:\nKpRW = (I−γL)p. (3)\nIt is related to the diﬀusion kernel by choosingγ = β/p and taking the limit withp→+∞, according to (2).\nUnlike the diﬀusion kernel which yields a dense matrix, the random walk kernel is sparse and has limited\nconnectivity, meaning that two nodes are connected inKpRW only if there exists a path of lengthp between\nthem. As these kernels on graphs reﬂect the structural similarity of the nodes independently of their features,\nit is natural to use the resulting Gram matrix to encode such a structure within the transformer model, as\ndetailed next.\n3\n4 Encoding Graph Structure in Transformers\nIn this section, we detail the architecture and structure encoding strategies behind GraphiT. In particular, we\nbuild upon the ﬁndings of [35] for our architectural choices and propose new strategies for encoding structural\ninformation in the transformer architecture.\n4.1 Transformer Architectures for Graphs\nWe process graphs with a vanilla transformer encoder architecture [36] for solving classiﬁcation and regression\ntasks, by seeing graphs as sets of node features. We ﬁrst present the transformer without encoding the graph\nstructure, before introducing mechanisms for encoding nodes positions in Section 4.2, and then topological\nstructures in Section 4.3. Speciﬁcally, a transformer is composed of a sequence of layers, which process an\ninput set ofdin features represented by a matrixX in Rn×din , and compute another set inRn×dout . A critical\ncomponent is the attention mechanism:\nAttention(Q,V ) =softmax\n(QQ⊤\n√dout\n)\nV ∈Rn×dout , (4)\nwith Q⊤= WQX⊤is called the query matrix,V⊤= WVX⊤the value matrix, andWQ,WV in Rdout×din are\nprojection matrices that are learned. Note that following the recommendation of [35], we use the same matrix\nfor keys and queries. This allows us to deﬁne a symmetric and positive deﬁnite kernel on pairs of nodes and\nthus to combine with other kernels on graph. This also reduces the number of parameters in our models\nwithout hurting the performance in practice. During the forward pass, the feature mapX is updated in a\nresidual fashion (with eitherdin = dout or with an additional projection matrix when the dimensions do not\nmatch, omitted here for simplicity) as follows:\nX = X+ Attention(Q,V ).\nNote that transformers without position encoding and GNNs are tightly connected: a GNN can be seen\nas a transformer where aggregation is performed on neighboring nodes only, and a transformer as a GNN\nprocessing a fully-connected graph. However, even in this case, diﬀerences would remain between common\nGNNs and Transformers, as the latter use for example LayerNorm and skip-connections. In our paper, we will\neither adopt the local aggregation strategy, or let all nodes communicate in order to test this inductive bias in\nthe context of graphs, where capturing long range interactions between nodes may be useful for some tasks.\n4.2 Encoding Node Positions\nThe output of the transformer is invariant to permutations in the input data. It is therefore crucial to provide\nthe transformer with information about the data structure. In this section, we revisit previously proposed\nstrategies for sequences and graphs and devise new methods for positional encoding. Note that a natural\nbaseline is simply to adopt a local aggregation strategy similar to GAT [37] or [12].\n4.2.1 Existing Strategies for Sequences and Graphs\nAbsolute and relative positional encoding in transformers for sequences. In NLP, positional\ninformation was initially encoded by adding a vector based on sinusoidal functions to each of the input token\nfeatures [36]. This approach was coined as absolute position encoding and proved to be also useful for other\ndata modalities [9, 12]. In relative position encoding, which was proposed later [26, 30], positional information\nis added to the attention logits and in the values. This information only depends on the relative distance\nbetween the two considered elements in the input sequence.\n4\nAbsolute positional encoding for graphs.Whereas positional encodings can be hand-crafted or learned\nrelatively easily for sequences and images, which respectively admit a chain or grid structure, this task\nbecomes non-trivial for graphs, whose structure may vary a lot inside a data set, besides the fact that the\nconcept of node position is ill-deﬁned. To address these issues, an absolute position encoding scheme was\nrecently proposed in [12], by leveraging the graph Laplacian (LapPE). More precisely, each node of each graph\nis assigned a vector containing the ﬁrstk coordinates of the node in the eigenbasis of the graph normalized\nLaplacian sorted in ascending order of the eigenvalues. Since the ﬁrst eigenvector associated to the eigenvalue\n0 is constant, the ﬁrst coordinate is omitted.\nAs detailed in Section 3, these eigenvectors oscillate more and more and the corresponding coordinates\nare often interpreted as Fourier coeﬃcients representing frequency components in increasing order. Note\nthat eigenvectors of the Laplacian computed on diﬀerent graphs could not be compared to each other in\nprinciple, and are also only deﬁned up to a±1 factor. While this raises a conceptual issue for using them in\nan absolute positional encoding scheme, it is shown in [12]—and conﬁrmed in our experiments—that the issue\nis mitigated by the Fourier interpretation, and that the coordinates used in LapPE are eﬀective in practice\nfor discriminating between nodes in the same way as the position encoding proposed in [36] for sequences.\nYet, because the eigenvectors are deﬁned up to a±1 factor, the sign of the encodings needs to be randomly\nﬂipped during the training of the network. In the next section, we introduce a novel strategy in the spirit of\nrelative positional encoding that does not suﬀer from the previous conceptual issue, and can also be combined\nwith LapPE if needed.\n4.2.2 Relative Position Encoding Strategies by Using Kernels on Graphs\nModulating the transformer attention. To avoid the issue of transferability of the absolute positional\nencoding between graphs, we use information on the nodes structural similarity to bias the attention scores.\nMore precisely, and in the fashion of [25, 35] for sequences, we modulate the attention kernel using the Gram\nmatrix of some kernel on graphs described in Section 3 as follows:\nPosAttention(Q,V,K r) =normalize\n(\nexp\n(QQ⊤\n√dout\n)\n⊙Kr\n)\nV ∈Rn×dout , (5)\nwith the sameQand V matrices, andKr a kernel on the graph. “exp” denotes the elementwise exponential of\nthe matrix entries andnormalize means ℓ1-normalization on rows such that normalization(exp(u))=softmax(u).\nThe reason for multiplying the exponential of the attention logits beforeℓ1-normalizing the rows is that it\ncorresponds to a classical kernel smoothing. Indeed, if we consider the PosAttention output for a nodei:\nPosAttention(Q,V,K r)i =\nn∑\nj=1\nexp\n(\nQiQ⊤\nj /√dout\n)\n×Kr(i,j)∑n\nj′=1 exp\n(\nQiQ⊤\nj′/√dout\n)\n×Kr(i,j′)Vj ∈Rdout ,\nwe obtain a typical smoothing,i.e, a linear combination of features with weights determined by a non-negative\nkernel, herek(i,j) :=exp(QiQ⊤\nj /√dout) ×Kr(i,j), and summing to 1. In fact,k can be considered as a new\nkernel between nodesi and j made by multiplying a kernel based on positions (Kr) and a kernel based on\ncontent (viaQ). As observed in [35] for sequences, modulating the attention logits with a kernel on positions\nis related to relative positional encoding [30], where we bias the attention matrix with a term depending\nonly on the relative diﬀerence in positions between the two elements in the input set. Moreover, during the\nforward pass, the feature mapX is updated as follows:\nX = X+ D−1\n2 PosAttention(Q,V,K r), (6)\nwhere D is the matrix of node degrees. We found such a normalization withD−1/2 to be beneﬁcial in our\nexperiments since it reduces the overwhelming inﬂuence of highly connected graph components. Note that,\nas opposed to absolute position encoding, we do not add positional information to the values and the model\ndoes not rely on the transferability of eigenvectors between diﬀerent Laplacians.\n5\nChoice of kernels and parameters. Interestingly, the choice of the kernel enables to encode a priori\nknowledge directly within the model architecture, while the parameter has an inﬂuence on the attention\nspan. For example, in the diﬀusion kernel,β can be seen as the duration of the diﬀusion process. The\nsmaller it is, the more focused the attention on the close neighbors. Conversely, a largeβ corresponds to\nan homogeneous attention span. As another example, it is clear that the choice ofp in the random walk\nkernel corresponds to visiting at bestp-degree neighbors. In our experiments, the best kernel may vary across\ndatasets, suggesting that long-range global interactions are of diﬀerent importance depending on the task.\nFor example, on the dataset PROTEINS (see Section 5), the vanilla transformer without using any structural\ninformation performs very well.\n4.3 Encoding Topological Structures\nPosition encoding aims at adding positional only information to the feature vector of an input node or to the\nattentions scores. Substructures is a diﬀerent type of information, carrying local positional information and\ncontent, which has been heavily used within graph kernels, see Section 2. In the context of graphs neural\nnetworks, this idea was exploited in the graph convolutional kernel network model (GCKN) of [7], which is a\nhybrid approach between GNNs and graph kernels based on substructure enumeration (e.g., paths). Among\ndiﬀerent strategies we experimented, enriching nodes features with the output of a GCKN layer turned out\nto be a very eﬀective strategy.\nGraph convolutional kernel networks (GCKN).GCKNs [7] is a multi-layer model that produces a\nsequence of graph feature maps akin to a GNN. The main diﬀerence is that each layer enumerates local\nsub-structures at each node (here, paths of lengthk), encodes them using a kernel embedding, and aggregates\nthe resulting sub-structure representations. This results in a feature map that carries more information\nabout the graph structure than traditional neighborhood aggregation based GNNs, which is appealing for\ntransformers since the vanilla version is blind to the graph structure.\nFormally, let us consider a graphGwith nnodes, and let us denote byPk(u) the set of paths shorter than\nor equal tok that start with nodeu. With an abuse of notation,p in Pk(u) will denote the concatenation of\nall node features encountered along the path. Then, a layer of GCKN deﬁnes a feature mapX in Rn×d such\nthat\nX(u) =\n∑\np∈Pk(u)\nψ(p),\nwhere X(u) is the column ofX corresponding to nodeu and ψ is ad-dimensional embedding of the path\nfeatures p. More precisely, the path features in[7] are embedded to a RKHS by using a Gaussian kernel,\nand a ﬁnite-dimensional approximation is obtained by using the Nyström method [39]. The embedding is\nparametrized and can be learned without or with supervision (see [7] for details). Moreover, path features of\nvarying lengths up to a maximal length can be used with GCKN. In this work, we evaluate the strategy of\nencoding a node as the concatenation of its original features and those produced by one GCKN layer. This\nstrategy has proven to be very successful in practice.\n5 Experiments\nIn this section, we evaluate instances of GraphiT as well as popular GNNs and position encoding baselines on\nvarious graph classiﬁcation and regression tasks. We want to answer several questions:\nQ1: Can vanilla transformers, when equipped with appropriate position encoding and/or structural informa-\ntion, outperform GNNs in graph classiﬁcation and regression tasks?\nQ2: Is kernel-based relative positional encoding more eﬀective than the absolute position encoding provided\nby the eigenvectors of the Laplacian (LapPE)?\nQ3: What is the most eﬀective way to encode graph structure information within transformers?\nWe also discuss our results and conduct ablation studies. Finally, we demonstrate the ability of attention\nscores to highlight meaningful graph features when using kernel-based positional encoding.\n6\n5.1 Methodology\nBenchmark and baselines. We benchmark our methods on various graph classiﬁcation datasets with\ndiscrete node labels (MUTAG, PROTEINS, PTC, NCI1) and one regression dataset with discrete node\nlabels (ZINC). These datasets can be obtainede.g via the Pytorch Geometric toolbox [13]. We compare\nour models to the following GNN models: Molecular Fingerprint (MF) [11], Graph Convolutional Networks\n(GCN) [18], Graph Attention Networks (GAT) [37], Graph Isomorphism Networks (GIN) [40] and ﬁnally\nGraph Convolutional Kernel Networks (GCKN) [7]. In particular, GAT is an important baseline as it uses\nattention to aggregate neighboring node information. We compare GraphiT to the transformer architecture\nproposed in [12] and also use their Laplacian absolute position encoding as a baseline for evaluating our graph\nkernel relative position encoding. All models are implemented in Pytorch and our code is available in the\nsupplementary material.\nReporting scores. For all datasets except ZINC, we samples ten times random train/val/test splits, of size\n80/10/10, respectively. For each split, we evaluate all methods by (i) training several models on the train fold\nwith various hyperparameters; (ii) performing model selection on val, by averaging the validation accuracy of\na model on its last 50 epochs; (iii) retraining the selected model on train+val; (iv) estimate the test score by\naveraging the test accuracy over the last 50 epochs. The results reported in our tables are then averaged\nover the ten splits. This procedure is a compromise between a double-nested cross validation procedure that\nwould be too computationally expensive and reporting 10-fold cross validation scores that would overestimate\nthe test accuracy. For ZINC, we use the same train/val/test splits as in [12], train GraphiT with 10 layers, 8\nheads and 64 hidden dimensions as in [12], and report the average test mean absolute error on 4 independent\nruns.\nOptimization procedure and hyperparameter search.Our models are trained with the Adam opti-\nmizer by decreasing the learning rate by a factor of 2 each 50 epochs. For classiﬁcation tasks, we train about\nthe same number (81) of models with diﬀerent hyperparameters for each GNN and transformer method, thus\nspending a similar engineering eﬀort on each method. For GNN models, we select the best type of global\npooling, number of layers and hidden dimensions from three diﬀerent values. Regarding transformers, we\nselect the best number of heads instead of global pooling type for three diﬀerent values. For all considered\nmodels, we also select the best learning rate and weight decay from three diﬀerent values and the number of\nepochs is ﬁxed to 300 to guarantee the convergence. For the ZINC dataset, we found that a standard warmup\nstrategy suggested for transformers in [36] leads to more stable convergence for larger models. The rest of the\nhyperparameters remains the same as used in [12]. More details and precise grids for hyperparameter search\ncan be found in Appendix A.\n5.2 Results and Discussion\nComparison of GraphiT and baselines methods.We show our results in Table 1. For smaller datasets\nsuch as MUTAG, PROTEINS or PTC, our Transformer without positional encoding performs reasonably\nwell compared to GNNs, whereas for NCI1 and ZINC, incorporating structural information into the model is\nkey to good performance. On all datasets, GraphiT is able to perform as well as or better than the baseline\nGNNs. In particular on ZINC, GraphiT outperforms all previous baseline methods by a large margin. For\nthis, it seems that the factorD−1/2 in (6) is important, allowing to capture more information about the\ngraph structure. The answer toQ1 is therefore positive.\nComparison of relative position encoding schemes. Here, we compare our transformer used with\ndiﬀerent relative positional encoding strategies, including adjacency (1-step RW kernel withγ = 1.0 corre-\nsponding to a normalized adjacency matrixD−1/2AD−1/2) which is symmetric but not positive semi-deﬁnite,\n2 and 3-step RW kernel withγ = 0.5 and a diﬀusion kernel withβ = 1. Unlike the vanilla transformer\nthat works poorly on big datasets including NCI1 and ZINC, keeping all nodes communicate through our\ndiﬀusion kernel positional encoding can still achieve performances close to encoding methods relying on local\n7\nTable 1: Average mean classiﬁcation accuracy/mean absolute error.\nMethod / Dataset MUTAG PROTEINS PTC NCI1 ZINC (no edge feat.)\nSize 188 1113 344 4110 12k\nClasses 2 2 2 2 Reg.\nMax. number of nodes 28 620 109 111 37\nMF [11] 81.5±11.0 71.9±5.2 57.3±6.9 80.6±2.5 0.387±0.019\nGCN [18] 78.9±10.1 75.8±5.5 54.0±6.3 75.9±1.6 0.367±0.011\nGAT [37] 80.3±8.5 74.8±4.1 55.0±6.0 76.8±2.1 0.384±0.007\nGIN [40] 82.6±6.2 73.1±4.6 55.0±8.7 81.7±1.7 0.387±0.015\nGCKN-subtree [7] 87.8±9.4 72.0±3.7 62.1±6.4 79.6±1.8 0.474±0.001\n[12] 79.3±11.6 65.8±3.1 58.4±8.2 78.9±1.1 0.359±0.014\n[12] + LapPE 83.9±6.5 70.1±3.2 57.7±3.1 80.0±1.9 0.323±0.013\nTransformers (T) 82.2±6.3 75.6±4.9 58.1±10.5 70.0±4.5 0.696±0.007\nT + LapPE 85.8±5.9 74.6±2.7 55.6±5.0 74.6±1.9 0.507±0.003\nT + Adj PE 87.2±9.8 72.4±4.9 59.9±5.9 79.7±2.0 0.243±0.005\nT + 2-step RW kernel 85.3±6.9 72.8±4.5 62.0±9.4 78.0±1.5 0.243±0.010\nT + 3-step RW kernel 83.3±6.3 76.2±4.4 61.0±6.2 77.6±3.6 0.244±0.011\nT + Diﬀusion kernel 82.7±7.6 74.6±4.2 59.1±7.4 78.9±1.6 0.255±0.010\nT + GCKN 84.4±7.8 69.5±3.8 61.5±5.8 78.1±5.1 0.274±0.011\nT + GCKN + 2-step RW kernel 90.4±5.8 72.5±4.6 58.4±7.6 81.0±1.8 0.213±0.016\nT + GCKN + Adj PE 90.5±7.0 71.1±6.9 57.9±4.2 81.4±2.2 0.211±0.010\ncommunications such as adjacency or p-step kernel encoding. Interestingly, our adjacency encoding, which\ncould be seen as a variant of the neighborhood aggregation of node features used in GAT, is shown to be very\neﬀective on many datasets. In general, sparse local positional encoding seems to be useful for our prediction\ntasks, which tempers the answer toQ1.\nComparisonofstructureencodingschemesinnodefeatures. Inthisparagraph, wecomparediﬀerent\nways of injecting graph structures to the vanilla transformer, including Laplacian PE [12] and unsupervised\nGCKN-path features [7]. We observe that incorporating topological structures directly into the features of\ninput nodes is a very useful strategy for vanilla transformers. This yields signiﬁcant performance improvement\non almost all datasets except PROTEINS, by either using the Laplacian PE or GCKN-path features. Among\nthem, GCKN features are observed to outperform Laplacian PE by a large margin, except on MUTAG and\nPROTEINS (third column of Table 2). A possible reason for this exception is that PROTEINS seems to\nbe very speciﬁc, such that prediction models do not really beneﬁt from encoding positional information. In\nparticular, GCKN brings a pronounced performance boost on ZINC, suggesting the importance of encoding\nsubstructures like paths for prediction tasks on molecules.\nCombining relative position encoding and structure encoding in node features.Table 2 reports\nthe ablation study of the transformer used with or without structure encoding and coupled or not with a\nrelative positional encoding. The results show that relative PE outperforms the topological Laplacian PE,\nsuggesting a positive answer toQ2. However, using both simultaneously improves the results considerably,\nespecially on ZINC. In fact, combining relative position encoding and a structure encoding scheme globally\nimproves the performances. In particular, using the GCKN-path layer features works remarkably well for all\ndatasets except PROTEINS. More precisely, we see that the combination of GCKN with the adjacent matrix\nPE yields the best results among the other combinations for MUTAG and NCI1. In addition, the GCKN\ncoupled with the 3-step RW kernel achieves the second best performance for ZINC. The answer toQ3 is\ntherefore combining a structure encoding scheme in node features, such as GCKN, with relative positional\nencoding.\n8\nTable 2: Ablation: comparison of diﬀerent structural encoding schemes and their combinations.\nDataset Structure encoding\nin node features\nRelative positional encoding in attention scores\nT vanilla T + Adj T + 2-step T + 3-step T + Diﬀusion\nMUTAG\nNone 82.2±6.3 87.2±9.8 85.3±6.9 83.3±6.3 82.7±7.6\nLapPE 85.8±5.9 86.0±4.2 84.7±4.7 83.5±5.2 84.2±7.2\nGCKN 84.4±7.8 90.5±7.0 90.4±5.8 90.0±6.3 90.0±6.8\nPROTEINS\nNone 75.6±4.9 72.4±4.9 72.8±4.5 76.2±4.4 74.6±4.2\nLapPE 74.6±2.7 74.7±5.2 75.0±4.7 74.3±5.3 74.7±5.3\nGCKN 69.5±3.8 71.1±6.9 72.5±4.6 70.0±5.1 72.4±4.9\nPTC\nNone 58.1±10.5 59.9±5.9 62.0±9.4 61.0±6.2 59.1±7.4\nLapPE 55.6±5.0 57.1±3.8 58.8±6.6 57.1±5.3 57.3±7.8\nGCKN 61.5±5.8 57.9±4.2 58.4±7.6 55.2±8.8 55.9±8.1\nNCI1\nNone 70.0±4.5 79.7±2.0 78.0±1.5 77.6±3.6 78.9±1.6\nLapPE 74.6±1.9 78.7±1.9 78.4±1.3 78.7±1.5 77.8±1.0\nGCKN 78.1±5.1 81.4±2.2 81.0±1.8 81.0±1.8 81.0±2.0\nZINC\nNone 0.696±0.007 0.243±0.005 0.243±0.010 0.244±0.011 0.255±0.010\nLapPE 0.507±0.003 0.202±0.011 0.227±0.030 0.210±0.003 0.221±0.038\nGCKN 0.274±0.011 0.211±0.010 0.213±0.016 0.203±0.011 0.218±0.006\nDiscussion. Combining transformer and GCKN features results in substantial improvement over the simple\nsum or mean global poolings used in original GCKN models on ZINC dataset as shown in Table 1, which\nsuggests that transformers can be considered as a very eﬀective method for aggregating local substructure\nfeatures on large datasets at the cost of using much more parameters. This point of view has also been\nexplored in [25], which introduces a diﬀerent form of attention for sequence and text classiﬁcation. A potential\nlimitation of GraphiT is its application to large graphs, as the complexity of the self-attention mechanism\nscales quadratically with the size of the input sequence. However, a recent line of work coined as eﬃcient\ntransformers alleviated these issues both in terms of memory and computational cost [2, 19]. We refer the\nreader to the following survey [34].\n5.3 Visualizing attention scores for the Mutagenicity dataset.\nWe now show that the attention yields visualization mechanisms for detecting important graph motifs.\nMutagenicity. In chemistry, a mutagen is a compound that causes genetic mutations. This important\nproperty is related to the molecular substructures of the compound. The Mutagenicity dataset [17] contains\n4337 molecules to be classiﬁed as mutagen or not, and aims at better understanding which substructures\ncause mutagenicity. We train GraphiT with diﬀusion position encoding on this dataset with the aim to study\nwhether important substructures can be detected. To this end, we feed the model with molecules of the\ndataset and collect the attention scores of each layer averaged by heads, as can be seen in Figure 2 for the\nmolecules of Figure 1.\nPatterns captured in the attention scores.While the scores in the ﬁrst layer are close to the diﬀusion\nkernel, the following layers get sparser. Since the attention matrix is multiplied on the right by the values, the\ncoeﬃcients of the node aggregation step for the n-th node is given by the n-throw of the attention matrix.\nAs a consequence, salientcolumns suggest the presence of important nodes. After veriﬁcation, for many\nsamples of Mutagenicity fed to our model, salient atoms indeed represent important groups in chemistry,\nmany of them being known for causing mutagenicity. In compound 1a, the N atom of the nitro group (NO2)\nis salient. 1a was correctly classiﬁed as a mutagen by our model and the nitro group is indeed known for its\nmutagenetic properties [8]. Note how information ﬂows from O atoms to N in the ﬁrst two layers and then,\nat the last layer, how every element of the carbon skeleton look at N,i.e the NO2 group. We can apply the\n9\nC O H N S\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1011\n12\n13\n14\n15\n16\n17\n18 19\n20\n21\n22\n23\n24\n25 26\n27\n(a) Nitrothiopheneamide-methylbenzene\n0\n12\n3\n4\n5\n6\n7\n89\n10\n11\n12\n13\n14\n15 16\n1718\n19\n20\n21\n22\n23\n24\n25\n26 27 (b) Aminoﬂuoranthene\nFigure 1: Examples of molecules from Mutagenicity correctly classiﬁed as mutagenetic by our model.\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 1\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 2\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\nN\nN\nLayer 3\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 1\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 2\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\nN\nN\nLayer 3\nFigure 2: Attention scores averaged by heads for each layer of our trained model for the compounds in\nFigures 1a (Top) and 1b (Bottom). Top Left: diﬀusion kernel for 1a.Top Right: node 8 (N of NO2) is salient.\nBottom Left: diﬀusion kernel for 1b.Bottom Right: node 14 (N of NH2) is salient.\n10\nsame reasoning for the amino group (NH2) in compound 1b [3]. We were also able to identify long-range\nintramolecular hydrogen bounds such as between H and Cl in other samples. We provide more examples of\nvisualization in Appendix B.\n6 Conclusion\nIn this work, we show that using a transformer to aggregate local substructures with appropriate position\nencoding, GraphiT, is a very eﬀective strategy for graph representation, and that attention scores allow simple\nmodel interpretation. One of the reasons for the success of transformers lies in their scaling properties: in\nlanguage modeling for example, it has been shown that for an equal large amount of data, the transformer’s\nloss follows a faster decreasing power law than Long-Short-Term-Memory networks when the size of the\nmodel increases [16]. We believe that an interesting future direction for this work would be to evaluate if\nGNNs and GraphiT also scale similarly in the context of large self-supervised pre-training, and can achieve a\nsimilar success as in natural language processing.\nAcknowledgments\nGM, DC, MS, and JM were supported by the ERC grant number 714381 (SOLARIS project) and by ANR 3IA\nMIAI@Grenoble Alpes, (ANR-19-P3IA-0003). This work was granted access to the HPC resources of IDRIS\nunder the allocation 2021-AD011012521 made by GENCI. GM thanks Robin Strudel for useful discussions on\ntransformers.\nReferences\n[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.\nNeural computation, 15(6):1373–1396, 2003.\n[2] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer.arXiv:2004.05150,\n2020.\n[3] D. L. Berry, G. M. Schoofs, and W. A. Vance. Mutagenicity of nitroﬂuoranthenes, 3-aminoﬂuoranthene\nand 1-nitropyrene in Chinese hamster V79 cells.Carcinogenesis, 6(10):1403–1407, 1985.\n[4] K. Borgwardt, E. Ghisu, F. Llinares-López, L. O’Bray, and B. Rieck. Graph kernels: State-of-the-art\nand future challenges.arXiv preprint arXiv:2011.03854, 2020.\n[5] K. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. InInternational conference on data\nmining (ICDM), 2005.\n[6] M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning: Going\nbeyond euclidean data.IEEE Signal Processing Magazine, 34(4):18–42, 2017.\n[7] D. Chen, L. Jacob, and J. Mairal. Convolutional kernel networks for graph-structured data. In\nInternational Conference on Machine Learning (ICML), 2020.\n[8] K.-T. Chung, C. A. Murdock, Y. Zhou, S. E. Stevens Jr., Y.-S. Li, C.-I. Wei, S. Y. Fernando, and M.-W.\nChou. Eﬀects of the nitro-group on the mutagenicity and toxicity of some benzamines.Environmental\nand Molecular Mutagenesis, 27(1):67–74, 1996.\n[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. InInternational Conference on Learning Representations\n(ICLR), 2021.\n11\n[10] S. S. Du, K. Hou, R. R. Salakhutdinov, B. Poczos, R. Wang, and K. Xu. Graph neural tangent kernel:\nFusing graph neural networks with graph kernels. InAdvances in Neural Information Processing Systems\n(NeurIPS), 2019.\n[11] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gómez-Bombarelli, T. Hirzel, A. Aspuru-Guzik,\nand R. P. Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. InAdvances in\nNeural Information Processing Systems (NeurIPS), 2015.\n[12] V. P. Dwivedi and X. Bresson. A generalization of transformer networks to graphs.AAAI Workshop on\nDeep Learning on Graphs: Methods and Applications, 2021.\n[13] M. Fey and J. E. Lenssen. Fast graph representation learning with PyTorch Geometric. InICLR\nWorkshop on Representation Learning on Graphs and Manifolds, 2019.\n[14] T. Gärtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and eﬃcient alternatives. In\nLearning theory and kernel machines, pages 129–143. Springer, 2003.\n[15] Z. Harchaoui and F. Bach. Image classiﬁcation with segmentation graph kernels. In 2007 IEEE\nConference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007.\n[16] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. Scaling laws for neural language models, 2020.\n[17] K. Kersting, N. M. Kriege, C. Morris, P. Mutzel, and M. Neumann. Benchmark data sets for graph\nkernels, 2016.\n[18] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks. In\nInternational Conference on Learning Representations (ICLR), 2017.\n[19] N. Kitaev, Łukasz Kaiser, and A. Levskaya. Reformer: The eﬃcient transformer. InInternational\nConference on Learning Representations (ICLR), 2020.\n[20] J. Klicpera, S. Weißenberger, and S. Günnemann. Diﬀusion improves graph learning. InAdvances in\nNeural Information Processing Systems (NeurIPS), 2019.\n[21] R. Kondor and J.-P. Vert. Diﬀusion kernels. InKernel Methods in Computational Biology, pages 171–192.\nMIT Press, 2004.\n[22] Y. Li, X. Liang, Z. Hu, Y. Chen, and E. P. Xing. Graph transformer, 2019.\n[23] M. Låg, J. G. Omichinski, E. Dybing, S. D. Nelson, and E. J. SÃžderlund. Mutagenic activity of\nhalogenated propanes and propenes: eﬀect of bromine and chlorine positioning.Chemico-Biological\nInteractions, 93(1):73–84, 1994.\n[24] P. Mahé and J.-P. Vert. Graph kernels based on tree patterns for molecules.Machine learning, 75(1):3–35,\n2009.\n[25] G. Mialon, D. Chen, A. d’Aspremont, and J. Mairal. A trainable optimal transport embedding for feature\naggregation and its relationship to attention. InInternational Conference on Learning Representations\n(ICLR), 2021.\n[26] A. Parikh, O. Täckström, D. Das, and J. Uszkoreit. A decomposable attention model for natural language\ninference. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n[27] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma, and\nR. Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. bioRxiv, 2019.\n12\n[28] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network\nmodel. IEEE transactions on neural networks, 20(1):61–80, 2008.\n[29] A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, et al. Improved protein structure\nprediction using potentials from deep learning.Nature, 577(7792):706–710, 2020.\n[30] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. InProceedings\nof the North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n[31] N. Shervashidze, P. Schweitzer, E. J. v. Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeiler-Lehman\ngraph kernels.Journal of Machine Learning Research (JMLR), 12:2539–2561, 2011.\n[32] N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Eﬃcient graphlet kernels for\nlarge graph comparison. InInternational Conference on Artiﬁcial Intelligence and Statistics (AISTATS),\n2009.\n[33] A. J. Smola and R. Kondor. Kernels and regularization on graphs. In B. Schölkopf and M. K. Warmuth,\neditors, Learning Theory and Kernel Machines, pages 144–158. Springer Berlin Heidelberg, 2003.\n[34] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler. Eﬃcient transformers: A survey, 2020.\n[35] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov. Transformer dissection: A\nuniﬁed understanding of transformer’s attention via the lens of kernel. InConference on Empirical\nMethods in Natural Language Processing (EMNLP), 2019.\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. InAdvances in Neural Information Processing Systems (NeurIPS), 2017.\n[37] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y. Bengio. Graph attention networks.\nIn International Conference on Learning Representations (ICLR), 2018.\n[38] N. Verma, E. Boyer, and J. Verbeek. Feastnet: Feature-steered graph convolutions for 3d shape analysis.\nIn Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[39] C. K. Williams and M. Seeger. Using the Nyström method to speed up kernel machines. InAdvances in\nNeural Information Processing Systems (NeurIPS), 2001.\n[40] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? InInternational\nConference on Learning Representations (ICLR), 2019.\n[41] S. Yun, M. Jeong, R. Kim, J. Kang, and H. Kim. Graph transformer networks. InAdvances in Neural\nInformation Processing Systems (NeurIPS), 2019.\n[42] J. Zhang, H. Zhang, C. Xia, and L. Sun. Graph-bert: Only attention is needed for learning graph\nrepresentations, 2020.\n13\nAppendix\nA Experimental Details\nIn this section, we provide implementation details and additional experimental results.\nA.1 General Details.\nComputing infrastructure. Computations were done on a GPU cluster equipped with Tesla V100-16G\nand Tesla V100-32G. We have monitored precisely the entire computational cost of this research project\n(including preliminary experiments, designing early and ﬁnal models, evaluating baselines, running ﬁnal\nexperiments), which was approximately 20k GPU hours.\nPosition encoding and structure encoding parameters for all datasets.γ for p-step random walk\nkernels is ﬁxed to 0.5 for both 2- and 3-step random walk kernels.β for the diﬀusion kernel is ﬁxed to 1.0.\nRegarding the structure encoding in node features, the dimension of Laplacian positional encoding is set to 8\nfor ZINC as suggested by [12] and to 2 for graph classiﬁcation datasets. The path size, the ﬁlter number and\nthe bandwidth parameter of the unsupervised GCKN path features (used for structure encoding) are set to 8,\n32 and 0.6 respectively for the ZINC dataset whereas the path size is set to 5 for graph classiﬁcation datasets.\nOther details. For all instances of GraphiT, the hidden dimensions of the feed-forward hidden layer in\neach layer of the encoder are ﬁxed to twice of the dimensions of the attention.\nA.2 Graph Classiﬁcation Datasets\nHere, we provide experimental details for MUTAG, PROTEINS, PTC, NCI1 and Mutagenicity datasets.\nDatasets. These free datasets were collected by [17] for academic purpose. MUTAG consists in classifying\nmolecules into mutagenetic or not. PROTEINS consists in classifying proteins into enzymes and non-enzymes.\nPTC consists in classifying molecules into carcinogenetic or not. NCI1 consists in classifying molecules into\npositive or negative to cell lung cancer. Mutagenicity consists in classifying compounds into mutagenetic or\nnot.\nTraining splits. For MUTAG, PROTEINS, PTC and NCI1, our outer splits correspond to the splits used\nin [40]. Our inner splits that divide their train splits into our train and validation splits are provided in\nour code. The error bars are computed as the standard deviation of the test accuracies across the 10 outer\nfolds. On the other side, as the purpose of using Mutagenicity is model interpretation, we use simple train,\nvalidation and test splits respectively composed of80%, 10%, and10% of the whole dataset.\nHyperparameter choices. For smaller datasets (MUTAG, PTC, PROTEINS), we use the search grids in\nTable 3 for GNNs, the search grids in Table 4 for GCKN and the search grids in Table 5 for both GraphiT and\nthe transformer model of [12]. For bigger datasets (NCI1), we use the search grid in Table 3 for GNNs, the\nsearch grids in Table 4 for GCKN and the search grids in Table 6 for both GraphiT and the transformer model\nof [12]. The best models are selected based on the validation scores that are computed as the average of the\nvalidation scores over the last 50 epochs. Then, the selected model is retrained on train plus validation sets\nand the average of the test scores over the last 50 epochs is reported. For Mutagenicity, we train a GraphiT\n14\nTable 3: Parameter grid for GNNs trained on MUTAG, PROTEINS, PTC, NCI1.\nParameter Grid\nLayers [1, 2, 3]\nHidden dimension [64, 128, 256]\nGlobal pooling [sum, max, mean]\nLearning rate [0.1, 0.01, 0.001]\nWeight decay [0.1, 0.01, 0.001]\nTable 4: Parameter grid for GCKN trained on MUTAG, PROTEINS, PTC, NCI1.\nParameter Grid\nPath size [3, 5, 7]\nHidden dimension [16, 32, 64]\nSigma [0.5]\nGlobal pooling [sum, max, mean]\nLearning rate [0.001]\nWeight decay [0.01, 0.001, 0.0001]\nmodel with 3 layers, 4 heads, 64 hidden dimensions. Initial learning rate was set to 1e-3, weight-decay was\nﬁxed to 1e-4 and structural information was encoded only via relative position encoding with the diﬀusion\nkernel.\nOptimization. We use the cross-entropy loss and Adam optimizer with batch size 128 for GNNs and 32 for\nboth GraphiT and the transformer model of [12]. For transformers, we do not observe signiﬁcant improvement\nusing warm-up strategies on these classiﬁcation tasks. Thus, we simply follow a scheduler that halves the\nlearning rate after every 50 epochs, as for GNNs. All models are trained for 300 epochs.\nA.3 Graph Regression Dataset\nHere, we present experimental details and additional experimental results for ZINC dataset.\nDataset. ZINC is a free dataset consisting of 250k compounds and the task is to predict the constrained\nsolubility of the compounds, formulated as a graph property regression problem. This problem is crucial\nfor designing generative models for molecules. Each molecular graph in ZINC has the type of heavy atoms\nas node features (represented by a binary vector using one-hot encoding) and the type of bonds between\natoms as edge features. In order to focus on the exploitation of the topological structures of the graphs, we\nomitted the edge features in our experiments. They could possibly be incorporated into GraphiT through the\napproach of [12] or considering diﬀerent kernels on graph for each edge bond type, which is left for future\nwork.\nTable 5: Parameter grid for transformers trained on MUTAG, PROTEINS, PTC.\nParameter Grid\nLayers [1, 2, 3]\nHidden dimension [32, 64, 128]\nHeads [1, 4, 8]\nLearning rate [0.001]\nWeight decay [0.01, 0.001, 0.0001]\n15\nTable 6: Parameter grid for transformers trained on NCI1.\nParameter Grid\nLayers [2, 3, 4]\nHidden dimension [64, 128, 256]\nHeads [1, 4, 8]\nLearning rate [0.001]\nWeight decay [0.01, 0.001, 0.0001]\nTraining splits. Following [12], we use a subset of the ZINC dataset composed of respectively 10k, 1k and\n1k graphs for train, validation and test split. The error bars are computed as the standard deviation of test\naccuracies across 4 diﬀerent runs.\nHyperparameter choice. In order to make fair comparisons with the most relevant work [12], we use the\nsame number of layers, number of heads and hidden dimensions, namely 10, 8 and 64. The number of model\nparameters for our transformers is only2/3 of that of [12] as we use a symmetric variant for the attention\nscore function in(5). Regarding the GNNs, we use the values reported in [12] for GIN, GAT and GCN. In\naddition, we use the same grid to train the MF model [11], i.e., a learning rate starting at 0.001, two numbers\nof layers (4 and 16) and the hidden dimension equal to 145. Regarding the GCKN-subtree model, we use the\nsame hidden dimension as GNNs, that is 145. We ﬁx the bandwidth parameter to 0.5 and the path size is\nﬁxed to 10. We select the global pooling form max, mean, sum and weight decay from 0.001, 0.0001 and 1e-5,\nsimilar to the search grid used in [12].\nOptimization. Following [12], we use the L1 loss and the Adam optimization method with batch size of\n128 for training. Regarding the scheduling of the learning rate, we observe that a standard warm-up strategy\nused in [36] leads to more stable convergence for larger models (hidden dimension equal to 128). We therefore\nadopt this strategy throughout the experiments with a warm-up of 2000 iteration steps.\nAdditional results. Besides the relatively small models presented in Table 1, we also show in Table 7\nthe performance of larger models with 128 hidden dimensions. Increasing the number of hidden dimensions\ngenerally results in boosting performance, especially for transformer variants combined with Laplacian\npositional encoding in node features. While sparse local positional encoding is shown to be more useful\ncompared to the positional encoding with a diﬀusion kernel, we show here that a variant of diﬀusion kernel\npositional encoding outperforms all other sparse local positional encoding schemes. Since the skip-connection\nin transformers already assigns some weight to each node itself and the diagonal of our kernels on graphs\nalso have important weight, we considered setting these diagonal to zero in order to remove the eﬀect of the\nattention scores on self-loop. This modiﬁcation leads to considerable performance improvement on longer\nrange relative positional encoding schemes, especially for the transformer with diﬀusion kernel positional\nencoding combined with GCKN in node features, which results in best performance.\nB Additional visualization of Mutagenicity compounds\nIn this section, we use attention scores as a visualization mechanism for detecting substructures that may\npossibly induce mutagenicity. We apply our model to samples from the Mutagenicity data set [17] and analyze\nmolecules that were correctly classiﬁed as mutagenic.\n1,2-Dibromo-3-Chloropropane (DBCP). DBCP in Figure 3 was used as a soil fumigant in various\ncountries. It was banned from use in the United Stated in 1979 after evidences that DBCP causes diseases,\nmale sterility, or blindness, which can be instances of mutagenicity. In Figure 4, attention focuses on the\n16\nTable 7: Mean absolute error for regression problem ZINC. The results are computed from 4 diﬀerent runs\nfollowing [12].\nRelative PE in attention score Structural encoding in node features\nNone LapPE GCKN (p=8,d=32)\n[12] 0.3585±0.0144 0.3233±0.0126 -\nTransformers with d=64\nT 0.6964±0.0067 0.5065±0.0025 0.2741±0.0112\nT + Adj PE 0.2432±0.0046 0.2020±0.0112 0.2106±0.0104\nT + 2-step RW kernel 0.2429±0.0096 0.2272±0.0303 0.2133±0.0161\nT + 3-step RW kernel 0.2435±0.0111 0.2099±0.0027 0.2028±0.0113\nT + diﬀusion 0.2548±0.0102 0.2209±0.0038 0.2180±0.0055\nSetting diagonal to zero, d=64\nT 0.7041±0.0044 0.5123±0.0232 0.2735±0.0046\nT + 2-step 0.2427±0.0053 0.2108±0.0072 0.2176±0.0430\nT + 3-step 0.2451±0.0043 0.2054±0.0072 0.1986±0.0091\nT + diﬀusion 0.2468±0.0061 0.2027±0.0084 0.1967±0.0023\nLarger models with d=128\nT 0.7044±0.0061 0.4965±0.0338 0.2776±0.0084\nT + Adj PE 0.2310±0.0072 0.1911±0.0094 0.2055±0.0062\nT + 2-step RW kernel 0.2759±0.0735 0.2005±0.0064 0.2136±0.0062\nT + 3-step RW kernel 0.2501±0.0328 0.2044±0.0058 0.2128±0.0069\nT + diﬀusion 0.2371±0.0040 0.2116±0.0103 0.2238±0.0068\nSetting diagonal to zero, d=128\nT 0.7044±0.0061 0.4964±0.0340 0.2721±0.0099\nT + 2-step 0.2348±0.0010 0.2012±0.0038 0.2031±0.0083\nT + 3-step 0.2402±0.0056 0.2031±0.0076 0.2019±0.0084\nT + diﬀusion 0.2351±0.0121 0.1985±0.0032 0.2019±0.0018\n17\nC O Cl H N Br S\n0\n1\n2\n3\n4\n56\n7\n8\n9\n10\nFigure 3: 1,2-Dibromo-3-Chloropropane.\n0\n2\n4\n6\n8\n10\n0 2 4 6 8 10\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\nLayer 1\n0\n2\n4\n6\n8\n10\n0 2 4 6 8 10\n0 2 4 6 8 10\n0\n2\n4\n6\n8\n10\nLayer 2\n0\n2\n4\n6\n8\n10\n0 2 4 6 8 10\nBr Br\nBr\nBr\nLayer 3\nFigure 4: Attention scores averaged by heads for each layer of our trained model for the compound in Figure 3.\nLeft: diﬀusion kernel for 3.Right: node 3 and 5 (Br) are salient.\ncarbon skeleton and on the two Bromine (Br) atoms, the latter being indeed known for being associated with\nmutagenicity [23].\nNitrobenzene-nitroimidazothiazole. This compound is shown in Figure 5. As for compound 1a in\nSection 5.3, our model puts emphasis on two nitro groups which are indeed known for inducing mutagenicity.\nTriethylenemelamine. Triethylenemelamine in Figure 7 is a compound exhibiting mutagenic properties,\nand is used to induce cancer in experimental animal models. Our model focuses on the three nitrogen atoms\nof the aziridine groups which are themselves mutagenic compounds.\n18\nC O Cl H N Br S\n0\n1\n2\n3\n4\n5\n6 7\n8\n9\n10\n11\n12 13\n14\n15\n16\n17\n18 19\n20 21\n22\n23 24\n25\nFigure 5: Nitrobenzene-nitroimidazothiazole.\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 1\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 2\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\nN N\nN\nN\nLayer 3\nFigure 6: Attention scores averaged by heads for each layer of our trained model for the compound in Figure 5.\nLeft: diﬀusion kernel for 5.Right: node 5 and 17 (N) are salient.\n19\nC O Cl H N Br S\n0\n1\n2\n3\n45\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 22\n23\n24\n25\n26\nFigure 7: Triethylenemelamine.\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 1\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\n0 5 10 15 20 25\n0\n5\n10\n15\n20\n25\nLayer 2\n0\n5\n10\n15\n20\n25\n0 5 10 15 20 25\nN N N\nN\nN\nN\nLayer 3\nFigure 8: Attention scores averaged by heads for each layer of our trained model for the compound in Figure 7.\nLeft: diﬀusion kernel for 7.Right: node 1, 8, and10 (N) are salient.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7310846447944641
    },
    {
      "name": "Transformer",
      "score": 0.5890306234359741
    },
    {
      "name": "Graph",
      "score": 0.5504008531570435
    },
    {
      "name": "Visualization",
      "score": 0.5270764231681824
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5127835869789124
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5079651474952698
    },
    {
      "name": "Source code",
      "score": 0.44532930850982666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41956645250320435
    },
    {
      "name": "Data mining",
      "score": 0.3563987612724304
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34583136439323425
    },
    {
      "name": "Natural language processing",
      "score": 0.3392758369445801
    },
    {
      "name": "Programming language",
      "score": 0.12066546082496643
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": []
}