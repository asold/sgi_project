{
  "title": "From Bag-of-Words to Pre-trained Neural Language Models: Improving Automatic Classification of App Reviews for Requirements Engineering",
  "url": "https://openalex.org/W4230571585",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4260306882",
      "name": "Adailton Araújo",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    },
    {
      "id": "https://openalex.org/A3011603842",
      "name": "Marcos Gôlo",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    },
    {
      "id": "https://openalex.org/A4260306884",
      "name": "Breno Viana",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    },
    {
      "id": "https://openalex.org/A2707103443",
      "name": "Felipe Sanches",
      "affiliations": [
        "Universidade de São Paulo"
      ]
    },
    {
      "id": "https://openalex.org/A3207240327",
      "name": "Roseli Aparecida Romero",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209254960",
      "name": "Ricardo Marcacini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4260306882",
      "name": "Adailton Araújo",
      "affiliations": [
        "Institute of Mathematics and Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A3011603842",
      "name": "Marcos Gôlo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4260306884",
      "name": "Breno Viana",
      "affiliations": [
        "Institute of Mathematics and Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A2707103443",
      "name": "Felipe Sanches",
      "affiliations": [
        "Institute of Mathematics and Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A3207240327",
      "name": "Roseli Aparecida Romero",
      "affiliations": [
        "Institute of Mathematics and Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A4209254960",
      "name": "Ricardo Marcacini",
      "affiliations": [
        "Institute of Mathematics and Computer Science"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6749267825",
    "https://openalex.org/W2995307967",
    "https://openalex.org/W2746078932",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W3033695544",
    "https://openalex.org/W2165466912",
    "https://openalex.org/W2244750928",
    "https://openalex.org/W6707404553",
    "https://openalex.org/W6687887293",
    "https://openalex.org/W2889366722",
    "https://openalex.org/W2969589564",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2154137718",
    "https://openalex.org/W2884159461",
    "https://openalex.org/W6672191712",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6695661434",
    "https://openalex.org/W2968904996",
    "https://openalex.org/W7001244178",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W7027146396",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2362569215",
    "https://openalex.org/W2203955288",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2085465189",
    "https://openalex.org/W2516809705"
  ],
  "abstract": "Popular mobile applications receive millions of user reviews. Thesereviews contain relevant information, such as problem reports and improvementsuggestions. The reviews information is a valuable knowledge source for soft-ware requirements engineering since the analysis of the reviews feedback helpsto make strategic decisions in order to improve the app quality. However, due tothe large volume of texts, the manual extraction of the relevant information is animpracticable task. In this paper, we investigate and compare textual represen-tation models for app reviews classification. We discuss different aspects andapproaches for the reviews representation, analyzing from the classic Bag-of-Words models to the most recent state-of-the-art Pre-trained Neural Languagemodels. Our findings show that the classic Bag-of-Words model, combined witha careful analysis of text pre-processing techniques, is still a competitive model.However, pre-trained neural language models showed to be more advantageoussince it obtains good classification performance, provides significant dimension-ality reduction, and deals more adequately with semantic proximity between thereviews’ texts, especially the multilingual neural language models.",
  "full_text": "From Bag-of-Words to Pre-trained Neural Language Models:\nImproving Automatic Classiﬁcation of App Reviews for\nRequirements Engineering\nAdailton F. Araujo, Marcos P. S. Gˆolo, Breno M. F. Viana,\nFelipe P. Sanches, Roseli A. F. Romero, Ricardo M. Marcacini\nInstitute of Mathematics and Computer Sciences - University of S˜ao Paulo (USP)\nPO Box 668 – 13.560-970 – S˜ao Carlos – SP – Brazil\n{adailton.araujo,marcosgolo,brenov,fpadula}@usp.br\n{ricardo.marcacini,rafrance}@icmc.usp.br\nAbstract. Popular mobile applications receive millions of user reviews. These\nreviews contain relevant information, such as problem reports and improvement\nsuggestions. The reviews information is a valuable knowledge source for soft-\nware requirements engineering since the analysis of the reviews feedback helps\nto make strategic decisions in order to improve the app quality. However, due to\nthe large volume of texts, the manual extraction of the relevant information is an\nimpracticable task. In this paper, we investigate and compare textual represen-\ntation models for app reviews classiﬁcation. We discuss different aspects and\napproaches for the reviews representation, analyzing from the classic Bag-of-\nWords models to the most recent state-of-the-art Pre-trained Neural Language\nmodels. Our ﬁndings show that the classic Bag-of-Words model, combined with\na careful analysis of text pre-processing techniques, is still a competitive model.\nHowever, pre-trained neural language models showed to be more advantageous\nsince it obtains good classiﬁcation performance, provides signiﬁcant dimension-\nality reduction, and deals more adequately with semantic proximity between the\nreviews’ texts, especially the multilingual neural language models.\n1. Introduction\nMobile applications (app) users can evaluate their usage experience by providing a nu-\nmerical score and textual comments. Such text reviews describe bug reports, new fea-\ntures requests, or just the evaluation of app speciﬁc features. In fact, app reviews are a\nvaluable knowledge source for requirements engineering and software development in-\nnovation. This knowledge has been explored in several tasks, such as feedback-driven\nimprovements to increase the user engagement [Pagano and Maalej 2013], strategic de-\ncisions related to development, validation and product improvement [Maalej et al. 2016]\nand, consequently, obtain a competitive advantage in the market [Shah et al. 2019].\nPopular apps usually receive hundreds of thousands or even millions of\nreviews and the manual analysis of so many reviews is an impracticable task.\nSeveral machine learning-based solutions have been proposed to solve this prob-\nlem [Pagano and Maalej 2013, Guzman et al. 2015, Maalej et al. 2016, Wang et al. 2018,\nAl Kilani et al. 2019, Messaoud et al. 2019, Dabrowski et al. 2020], in which relevant in-\nformation is extracted from the texts to automatically classify a review.\nAn app review classiﬁer can be deﬁned as a function f : X →Y that maps a\nreview x ∈X to a set of c classes Y = {1, ..., c}, where y ∈Y usually indicates use-\nful labels for requirements engineering, such as “bug”, “app feature”, “user experience“,\n“opinion / rating”, among others. In this case, X ∈Rd represents the reviews feature\nspace. Supervised machine learning classiﬁcation aims to learn a function f∗ from a\ntraining set T = {(x1, y1), (x2, y2), ...,(xn, yn)}that approximates the unknown mapping\nfunction f. A well-known issue is that the learning process is directly inﬂuenced by the\nfeature space, i.e., by the technique used to represent the texts (unstructured data) in a vec-\ntor space model. Considering the various techniques for text pre-processing, from tradi-\ntional Bag-of-Words (BoW) models to the most recent Neural Language Models (NLM),\nwe investigate the following research question: what is the impact of different text rep-\nresentation models for app reviews classiﬁcation in the context of software requirements\nengineering?\nMost existing studies use the traditional Bag-of-Words model for textual repre-\nsentation, where word histograms are used as the features in app reviews classiﬁcation\n[Aggarwal 2018]. Although BoW is simple and intuitive, this model has limitations that\nhinder classiﬁcation. First, BoW assumes independence between words, so different sen-\ntences can have the same representation. Even when we use Bag-of-Ngrams to consider\nword order, the classiﬁer usually suffers from high dimensionality and sparse representa-\ntion. Second, there is a lack of semantics, in which similar (or semantically related) words\nor sentences are considered to be distinct features.\nNeural language models are currently state-of-the-art for many tasks in natu-\nral language processing (NLP) [Mulder et al. 2015, Belinkov and Glass 2019]. The ﬁrst\nword embeddings models, such as Word2vec [Mikolov et al. 2013], were promising for\ncomputing proximity between words by using word vectors, but without solving the\nword order drawback. More recently, neural language models based on transformer\narchitectures and attention mechanisms enabled a rich textual representation learning\n[Otter et al. 2020], allowing to identify semantic and syntactic relations, as well as to\nconsider the sequential structure of the texts. Besides, these models are pre-trained in\nlarge textual corpus, thereby enabling their use for new tasks with smaller training corpus\n[Zhou et al. 2020]. Figure 1 presents a simple example of how the similarity between\ntwo sentences can be obtained in pre-trained language models, in which the darker lines\nindicate greater semantic proximity. Note that a traditional BoW model fails to determine\nthe proximity between these sentences.\nFigure 1. Example of the similarity between two sentences in pre-trained NLM.\nThe app freezes frequently\nThe app crashes every time\nIn this paper, we investigate and compare Bag-of-Words models and pre-trained\nneural language models for app reviews classiﬁcation. We analyzed the main term\nweighting strategies for Bag-of-Words models, such as binary weighting, term fre-\nquency (TF) and TFIDF, as well as the use of unigrams and bigrams. Regard-\ning pre-trained neural language models, we analyzed three state-of-the-art deep neu-\nral networks: BERT [Devlin et al. 2018], DistilBERT [Sanh et al. 2019] and RoBERTa\n[Liu et al. 2019]. BERT is a representation model that achieved promising results in\nthe General Language Understanding Evaluation (GLUE) benchmark, both for sentence-\nlevel and token-level tasks. DistilBERT is a smaller (distillate) general-purpose language\nrepresentation model, which obtains competitive results with lower computational costs.\nRoBERTa is an optimized (robust) version of BERT, in which the authors investigated\nnew strategies for training the deep neural network. All textual representation models\nwere evaluated in classiﬁcation tasks, in which app reviews are mapped into require-\nments engineering classes. To mitigate the classiﬁcation method’s bias, we evaluated\nthe representation models considering four different classiﬁers: Support Vector Machines\n(SVM), k-Neighbors Nearest (kNN), Multilayer Perceptron Neural Networks (MPNN),\nand Multinomial Naive Bayes (MNB).\nWe carried out an experimental evaluation on a real-world dataset containing 3691\napp reviews organized into four requirements engineering classes. Experimental results\nshow that app reviews classiﬁcation using pre-trained language models obtains higher\nF1-Measure values than BoW-based classiﬁcation. However, our analysis found no sta-\ntistically signiﬁcant differences between these textual representation models when con-\nsidering the various classiﬁcation methods. We discuss practical advantages in favor of\npre-trained neural language models for app reviews classiﬁcation. While BoW models\nrequire a careful feature selection, stopwords removal and term weighting, the pre-trained\nneural language models can be used as a ready-to-use module that achieves a reduced,\nhigh-quality vector embedding space directly from texts. Moreover, we also show that\nsemantic similarities from neural language models combined with explainable AI tools\nprovide interesting insights involving the extraction of important words from app reviews\nand their relationship with requirements engineering classes.\n2. App Review Classiﬁcation\nApp developers understand that the user’s opinion about their apps’ usage is im-\nportant to the app’s development and maintenance to meet the users’ expectations\n[Pagano and Maalej 2013]. This opinion, called app review, is an important communi-\ncation channel between developers and app users [Messaoud et al. 2019].\nAn app review can be classiﬁed into different classes related to requirements engi-\nneering, such as a fault report describing a found problem in the software which must to\nbe ﬁxed, e.g. “ The order history does not show my last order”; a wish of a new feature,\ne.g. “ It would be wonderful if I could be able to pay with Bitcoin ” or “Too bad it isn’t\navailable of IOS platform”; a description of the usage experience of a speciﬁc feature ,\ne.g. a positive experience “It’s sensational I can see my order location in real time” or a\nnegative experience “Impossible to ﬁnd a good restaurant with this awful search”, as well\nas other types ofratings in general. In this context, it may become strategic for app devel-\nopers to analyze each review aiming to identify the app review class and plan actions for\neach situation. These actions may involve: to ﬁx a critical fault that has affected a wide\nnumber of users; to design a new feature or module for users satisfaction improvement;\nto prioritize opportunities and urgent demands from the perspective of a wide number of\nusers [Al Kilani et al. 2019].\nSeveral works have been developed in the last years aiming to automatize this\napp review classiﬁcation [Maalej et al. 2016, Wang et al. 2018, Aralikatte et al. 2018,\nMessaoud et al. 2019, Al Kilani et al. 2019]. This automation aims to discard the irrele-\nvant reviews and to group truly useful reviews. Supervised Machine Learning algorithms\nhave been used to perform the automatic review classiﬁcation where the important review\ngroups are mapped for the target-classes. First, human experts must manually analyze\neach review to assign one of the target-class (e.g. Rating, User Experience, Bug e Fea-\nture) or discard the review if it does not ﬁt any class. Second, machine learning algorithms\nare trained from the labeled reviews. Thus, the obtained classiﬁcation model are able to\nautomatically assign a target-class to new reviews.\n[Maalej et al. 2016] created a dataset with 3.691 app user reviews that were man-\nually classiﬁed by human experts into four classes: Rating, User Experience, Bug, or\nFeature. In the pre-processing stage, authors used the Bag-of-Words (BoW) model for\ntext representation, as well as stopwords removal, stemming, bigram generation. For\napp review classiﬁcation, they used the Naive Bayes, Decision Tree, and MaxEnt algo-\nrithms. In our work, as well as other works found in the literature [Wang et al. 2018,\nAralikatte et al. 2018, Messaoud et al. 2019, Al Kilani et al. 2019], we observed the use\nof just traditional BoW models for text representation. Thereby, we also used the dataset\ncreated by [Maalej et al. 2016] to compare the performance of BoW models with pre-\ntrained neural language models.\n3. Textual Representation\nMachine learning algorithms for text classiﬁcation require a structured data representa-\ntion. In the natural language context, it is necessary to apply textual data pre-processing\nto build a structured representation of these data. One of the main ways to represent\ntextual data is the traditional frequency-based representation, such as the Bag-of-Words\nmodel. More recently, pre-trained neural language models have become popular due to\nthe competitive performance in text classiﬁcation for different domains. Thereby, this\nsection describes the derivations of frequency-based representations and three neural lan-\nguage models: BERT, RoBERTa, and DistilBERT.\n3.1. Frequency-Based Representations\nIn the vector space model (VSM), each textual document is represented by a vector and\neach vector dimension by a characteristic (attribute) of the textual document. Table 1\nillustrates a matrix representation of the vector space model for a text collection of m\ndocuments and n attributes [Tan et al. 2013]. The set of documents is denoted by D=\n{d1, d2, . . . , dm}and the set of attributes by T = {t1, t2, . . . , tn}. The matrix cells, or\nvector dimensions, quantify the attribute occurrence in a document.\nTable 1. Matrix representation of VSM (m documents and n attributes).\nt1 t2 t3 ... tn\nd1 wd1,t1 wd1,t2 wd1,t3 ... wd1,tn\nd2 wd2,t1 wd2,t2 wd2,t3 ... wd2,tn\n...\n...\n...\n...\n...\n...\ndm wdm,t1 wdm,t2 wdm,t3 ... wdm,tn\nIf the vector dimensions represent simple terms, the representation is called bag-\nof-words [Aggarwal 2018]. The terms weights of BoW may correspond to: (1) Bi-\nnary that indicates just if there is or not occurrence of the term in the document; (2)\nTerm Frequency (TF) that deﬁnes an occurrence frequency of a term in a document;\nTerm Frequency - Inverse Document Frequency (TFIDF) that ponders the TF with the\ninverse of all documents frequency. Equation 1 deﬁnes the TFIDF term weighting,\nTFIDF = TFd,t ·log\n(|D|\nd ft\n)\n(1)\nwhere TFd,t corresponds to the frequency of the term t in the document d, |D|to the\nnumber of documents and d ft to the number of documents that contain the term t.\nThe traditional representation deﬁned by BoW model may be considered as a set\nof unigrams, i.e., a set of unique words. Bigrams, in turn, are all the combinations of\ntwo terms of a set of terms of documents. Bigrams cause a signiﬁcant increase in the\nnumber of attributes and can impair the classiﬁcation process due to the so-called curse of\nhigh dimensionality. In fact, the pre-processing step using BoW models requires careful\nanalysis of the textual dataset and the correct choice of techniques for attribute selection,\nstopword removal and term weighting.\n3.2. Language Model-Based Representations\nLanguage modeling aims to obtain a function to predict a word (or sentence) given pre-\nvious words (or sentences) [Mikolov et al. 2013]. Recent methods use the idea of dis-\ntributed representations, in which texts (e.g. words and/or sentences) are represented by\nlow-dimensional (real-valued) vectors [Mulder et al. 2015]. Thus, texts can be compared\nthrough the correlation or distances between these vectors, which capture syntactic and\nsemantic relationships. Learning good distributed representations is a challenge and neu-\nral networks are currently used for this task, also called Neural Language Models (NLM)\n[Otter et al. 2020].\nNLM may be context-free or context-dependent. Context-free NLMs, such as\nword2vec [Mikolov et al. 2013], transform each text word into a vector of characteristics\nwhich is independent of the context in which the documents are being analyzed (e.g. word\norder). Context-dependent NLMs, on the other hand, group a vector of characteristics for\neach sentence or document, rather than each word, and also consider the context in which\nthe words are written [Devlin et al. 2018]. Context-dependent NLMs perform better than\nthe traditional language models [Devlin et al. 2018, Otter et al. 2020] and were used in\nour study involving app reviews classiﬁcation.\nNLMs may be undirected or bidirected. Undirected models analyze the text in\na single way, like the reading way, for example. Bidirected models, on the other hand,\nanalyze the text in both ways, from left to right and vice-versa. Bidirected NLMs pro-\nvide a better performance than undirected NLMs [Devlin et al. 2018, Liu et al. 2019,\nSanh et al. 2019]. Examples of bidirected models are: BERT [Devlin et al. 2018],\nRoBERTa [Liu et al. 2019] and DistilBERT [Sanh et al. 2019]. BERT-based models use\na masking strategy for training, in which some words in a sentence are replaced by a spe-\ncial token called [MASK]. The model is trained to predict the masked words, using the\ncontext of non-masked words and their word sequence. In this case, the model uses pairs\nof sentences as input, in which it aims to predict whether the second sentence is subse-\nquent to the ﬁrst sentence. During the training stage, BERT-based models use an attention\nmechanism that learns contextual relations between words [Vaswani et al. 2017].\nThe RoBERTa, DistilBERT and Multilingual DistilBERT models are deriva-\ntions of the BERT model. All of these derivations were proposed aiming to\nimprove the results and to decrease the computational cost of the BERT model\n[Devlin et al. 2018]. RoBERTa model uses greater sequences of textual data than orig-\ninal model [Liu et al. 2019]. In addition, this model also removes the next sentence pre-\ndiction goal and changes dynamically the masking pattern [Liu et al. 2019]. The Dis-\ntilBERT model, on the other hand, differs of BERT in the pre-training stage by using\na smaller number of parameters, a knowledge distillation technique (training of a com-\npacted model) and the triple loss technique [Sanh et al. 2019]. This strategy grants the\nDistilBERT model to save 40% of memory and become 60% faster than BERT, besides\npreserving 97% of understanding resources of idioms [Sanh et al. 2019]. The Multilin-\ngual DistilBERT is a DistilBERT model that considers different idioms.\n4. Experimental Evaluation\nWe used the textual collection created by [Maalej et al. 2016] for experimental evaluation.\nThis collection contains the characteristics shown in Table 2.\nTable 2. Textual Collection Characteristics.\nClass Name Number of Reviews Average Review Tokens Total Tokens\nBug 370 19 7220\nUser Experience 607 17 10303\nRating 2462 7 18421\nFeature 252 18 4564\nIn these reviews, the users detailed their experience of mobile applications usage.\nThe dataset contains 3691 app reviews, in English language, organized into four classes.\nThe class Rating presents the user explanation for a given score. The class User Experi-\nence refers to the experience of using a speciﬁc feature. The class Bug describes a found\nproblem in the app which must be ﬁxed. The class Feature describes a wish or request for\na new feature. Besides the free text and the class, each instance of the collection contains\nmetadata information, such as the review date and score. To generate this app review\ndataset, the authors followed a strict protocol of annotation, involving 10 annotators and\ncriteria for selecting a subset of a total of 1.3 thousand of reviews, of several categories,\ngathered from the Apple Store and Google Play Store.\n4.1. Experimental Setup\nOur work applies BoW models and pre-trained NLM models for textual representation.\nIn BoW model, we used three term weighting techniques: TF, TFIDF and Binary. We\nalso considered unigrams and bigrams versions of each one of these term weighting 1.\nIn BoW models, we applied a text cleaning process to decrease the data dimensional-\nity, as well as to increase representation quality. This process, according to Aggarwal\n[Aggarwal 2018], improves the quality of the classiﬁcation algorithms. The cleaning\nsteps were: (1) converting words to lowercase and removal of accents; (2) removal of\npunctuation marks and alphanumeric characters; (3) removal of stopwords; and (4) word\nstemming [Aggarwal 2018].\n1We generated the BoW model with bigrams by using the bigram generator of the scikit-learn library.\nIn the pre-trained models, we do not use any text cleaning techniques to main-\ntain the original text structure, which is important for context-dependent NLMs. We\nused four pre-trained neural language models: BERT, RoBERTa, DistilBERT, and a\nmultilingual version of DistilBERT (M. DistilBERT). Details on training parameters\nand textual corpus used to obtain pre-trained models are available in the work of\n[Reimers and Gurevych 2019]. Table 3 presents an overview of the dimensionality (num-\nber of features) obtained in each textual representation.\nTable 3. Overview of the dimensionality of each textual representation.\nBoW BoW-bigram BERT RoBERTa DistilBERT M. DistilBERT\nNumber of Features 4035 26962 1024 1024 768 512\nWe used four traditional classiﬁcation algorithms: k-Nearest Neighbors (kNN),\nMultinomial Naive Bayes (MNB), Support Vector Machines (SVM) and Multilayer Per-\nceptron (MLP). We also considered two Fine-Tuning algorithms, which are Deep Learn-\ning approaches, for BERT and DistilBERT pre-processing. In the ﬁne-tuning strategy we\nadded new layers to the pre-trained models to consider the task target, i.e., the app reviews\nclassiﬁcation. Thus, the distributed representations of the texts are ﬁne-tuned to consider\nspeciﬁc relations of the classiﬁcation task. The parameters of the ML algorithms we used\nin our experiments were:\n•kNN: k ∈[1, 30] and cosine metric;\n•MNB: None;\n•SVM: kernels = {RBF, sigmoid, polinomial, linear}and C = {1.0};\n•MLP: Layered architecture = {1, 3, 6}, neurons = {50, 100, 150}, moment =\n{0.9}and learning rate = {0.001}.\n•Fine-Tuning: training epochs = {1, 5, 10, 15}, learning rate = {0.00002}, max\ntoken length = {100}and bach size = {32}.\nFor reproducibility purposes, we provide a repository2 with the source code of the\nclassiﬁcation and ﬁne-tuning algorithms, as well as the textual representations obtained.\nWe evaluated the Multi-Class ML algorithms performance by using the 10-Fold\nCross-Validation strategy [Tan et al. 2013]. We use theF1 evaluation measure that corre-\nsponds to the harmonic mean of Precision (Equation 2) and Recall (Equation 3), where\nTP (True Positive) refers to the number of documents of a class in which the algorithm\ncorrectly classiﬁed; FP (False Positive) refers to the number of documents that do not\nbelongs of a class in which the algorithm wrongly classiﬁed as belonging; andFN (False\nNegative) refers to the number of documents of a class in which the algorithm wrongly\nclassiﬁed as another class. Equation 4 deﬁnes the F1 measure.\nP = TP\nTP + FP , (2) R = TP\nTP + FN (3) F1 = 2 ×P ×R\nP + R , (4)\n4.2. Results and Discussion\nWe conducted an experimental evaluation to investigate two aspects involved in the app\nreviews classiﬁcation. In the ﬁrst aspect, our objective is to analyze the impact of each\n2https://github.com/adailtonaraujo/classify_app_review\ntextual representation model considering the four different classiﬁcation algorithms. In\nthe second aspect, we evaluate the impact of ﬁne-tuning a neural language model for a\nspeciﬁc app reviews classiﬁcation task.\nConcerning the ﬁrst aspect, Table 4 presents the classiﬁcation results of the algo-\nrithms kNN, MNB, SVM and MLP. The cell values correspond to the average of macro\nF1 measure of each class of the textual collection. A row of the table represents the F1\nmeasure result for a certain algorithm. For each row, we highlight in blue and green the\ngreatest values of the BoW e NLM models, respectively.\nTable 4. Comparison (macroF1 measure) of bag-of-words models and pre-trained\nneural language models in different classiﬁers.\nBag-of-Words Neural Language ModelsUnigram Bigram\nTF TFIDF Binary TF TFIDF Binary BERT DistilBERT RoBERTa M. DistilBERT\nSVM 0.38 0.35 0.36 0.34 0.28 0.34 0.47 0.47 0.46 0.50\nMLP 0.37 0.34 0.37 0.38 0.31 0.38 0.47 0.46 0.47 0.50\nMNB 0.41 0.42 0.41 0.39 0.38 0.39 0.53 0.51 0.54 0.53\nKNN 0.29 0.28 0.26 0.30 0.26 0.26 0.47 0.46 0.46 0.48\nRegarding the classiﬁcation algorithms, we observe that the MNB obtained the\ngreatest values of the F1 for all text representation models. We also note that the kNN\nalgorithm usually obtains the lowest F1 values for the most text representation models.\nConcerning the BoW-based models, we observe that the bigrams deteriorate the\nclassiﬁcation performance when used in conjunction with Binary and TFIDF term weight-\ning techniques. For TF term weighting, on the other hand, bigrams improved the perfor-\nmance of the MLP and kNN algorithms. We also notice that, for unigrams, the TFIDF\nterm weighting obtained the best F1 value using MNB classiﬁer.\nRegarding the pre-trained NLM models, the Multilingual DistillBERT obtained\nthe greatest values for the most ML algorithms. The only exception was the MNB al-\ngorithm in which the greater F1 value was obtained by the RoBERTa model. We also\nidentiﬁed that the results of the language model-based techniques were always greater\nthan BoW-based for all the classiﬁcation algorithms.\nFigure 2 presents a graphical comparison of the BoW and NLM models based\non Friedman’s non-parametric test with Nemenyi post-test through a critical (statistical)\ndifference diagram [Garc ´ıa et al. 2010]. The diagram presents the average F1 ranking\nof each model, and those which are connected by lines did not present statistically sig-\nniﬁcant differences to each other. We observe that the only statistical difference is the\none presented between three NLMs and one BoW: BERT, RoBERTa, Multilingual Distil-\nBERT and TFIDF-bigram. Although there is no critical difference, we note that the NLMs\npresent better average ranking than the BoW-based approaches. Regarding classiﬁcation\nruntime, the NLMs present better performances than the traditional approaches due to the\nreduced dimensionality of the representation.\nFigure 3 presents the best F1 results of the BoW and NLM models for each clas-\nsiﬁcation algorithm, regardless of BoW’s pre-processing technique and pre-trained lan-\nguage model choice. We observe that the NLM models obtained the best F1 values in all\nclassiﬁcation algorithms.\nFigure 2. Diagram of critical difference of Friedman test with Nemenyi post-test.\n1 2 3 4 5 6 7 8 9 10\nM. DistilBERT\nBERT\nRoBERTa\nDistilBERT\n TF TF-BIGRAM\nTFIDF\nBinary\nBinary-Bigram\nTFIDF-Bigram\nCD\nFigure 3. Comparison of the best F1 values for BoW and NLM.\n0,30\n0,38\n0,42\n0,38\n0,42\n0,48 0,5\n0,54\n0,5\n0,54\n0,0\n0,2\n0,4\n0,6\nKNN MLP MNB SVM Best Overall\nBoW NLM\nBesides of obtaining better F1 values, the NLM models present other advantages.\nThe ﬁrst one is the absence of text tokenization and feature selection. Since the NLM\nmodels were pre-trained on a large textual corpus, such representations are relatively ro-\nbust in relation to the writing errors commonly found in reviews. Furthermore, some\nNLM models are able to deal with several idioms written in a single textual document,\ne.g. Multilingual DistilBERT.\nConcerning the second aspect of our experimental evaluation, we also consider the\nFine-Tuning step in the BERT and DistilBERT models. Figure 4 shows a comparison of\nF1 results. For the classiﬁcation results without Fine-Tuning, we chose the best results of\nboth that were obtained by the MNB algorithm. We observe that the models that use the\nFine-Tuning step obtained betterF1 results. However, considering the small improvement\nin classiﬁer performance and the higher runtime and computational costs of the ﬁne-\ntuning step, we argue that this strategy should only be used for large training sets.\nFigure 4. Comparison of NLM approaches with and without Fine-Tuning.\n0,53 0,51\n0,55\n0,52\n0,0\n0,2\n0,4\n0,6\nBERT DistilBERT\nWithout FineTuning With FineTuning\nAlthough the ﬁne-tuning strategy has not improved signiﬁcantly in F1 measure, it\nis useful to identify patterns (words and expressions) more related to the target classes.\nFor example, we can highlight text excerpts that most inﬂuence the classiﬁer’s decision\nby using the LIME method — a method that infers the relative importance of attributes\nby using an interpretable linear model to generate explainings of classiﬁcation predic-\ntions [Ribeiro et al. 2016]. Figure 5 presents an example of a review of the class Feature\nextracted from our review dataset. The excerpts highlighted in green inﬂuenced more\nthe classiﬁcation, where the words “missing”, “add” and “should” are related to the texts\nthat express the user’s desire for new app features. On the other hand, in red, the irrele-\nvant excerpts for the classiﬁcation, i.e., general words or less related to the context of the\nclass, as “think”, “you” e “app”. Furthermore, the ﬁne-tuning also improved the similar-\nity relation between words. To illustrate this behavior, Figure 6 shows a two-dimensional\nprojection of words that are semantically similar to the word “Error” (a relevant word for\nthe class Bug). We note that the ﬁne-tuned NLM models identiﬁed related words, which\nis potentially useful for exploratory analysis tasks.\nFigure 5. Inference of important words for the class Feature (green and red for\nthe most and less important words, respectively).\nFigure 6. Two-dimensional projection of semantically similar words.\n5. Conclusion\nWe compared BoW-based models and pre-trained NLM models for textual representa-\ntion. In the experimental evaluation, we used a textual collection of mobile app reviews\nand classiﬁcation algorithms of different paradigms. The NLM models obtained greater\nclassiﬁcation F1 values than BoW models. The Multilingual DistilBERT presented the\ngreatest F1 means for most classiﬁcation algorithms. We consider this pre-trained model\nto be a reasonable choice for classiﬁcations tasks of app reviews, because, in addition to\ncompetitive F1 results, it also supports multiple languages and uses less computational\nresources (it is a distilled version of BERT).\nWe also evaluated the ﬁne-tuning usage in the BERT and DistilBERT models.\nThe ﬁne-tuning step presented an average improvement of 0.01 in the F1 measure. This\nimprovement may not be enough to justify ﬁne-tuning due the runtime and high use of\ncomputational resources, especially for small datasets. However, it is important to em-\nphasize that ﬁne-tuning is potentially useful for exploratory analysis and interpretation of\nclassiﬁcation models, as well as the analysis of semantic relations between words.\nThe directions for future work involve investigating the impact of different ﬁne-\ntuning strategies for app reviews classiﬁcation. We intend to explore unsupervised ﬁne-\ntuning methods that allow the use of large collections of unlabeled reviews, as well as\ncross-domain transfer learning learning strategies [Marcacini et al. 2018]. Thus, we in-\ntend to provide pre-trained BERT-based language models for text reviews that will be\nuseful for various tasks related to the opinion mining and sentiment analysis. In addition,\nit is intended to analyze all the described pre-processing techniques with more than one\nnew textual collections in order to make a better and richer evaluation of these techniques.\nAcknowledgements\nThis work was supported by National Council for Scientiﬁc and Technological Develop-\nment (CNPq) [process number 426663/2018-7], CAPES and FAPESP.\nReferences\nAggarwal, C. C. (2018). Machine Learning for Text . Springer Publishing Company,\nIncorporated, 1st edition.\nAl Kilani, N., Tailakh, R., and Hanani, A. (2019). Automatic classiﬁcation of apps re-\nviews for requirement engineering: Exploring the customers need from healthcare\napplications. In 2019 Sixth International Conference on Social Networks Analysis,\nManagement and Security (SNAMS), pages 541–548.\nAralikatte, R., Sridhara, G., Gantayat, N., and Mani, S. (2018). Fault in your stars: an\nanalysis of android app reviews. In Proceedings of the ACM India Joint International\nConference on Data Science and Management of Data, pages 57–66.\nBelinkov, Y . and Glass, J. (2019). Analysis methods in neural language processing: A\nsurvey. Transactions of the Association for Computational Linguistics, 7:49–72.\nDabrowski, J., Letier, E., Perini, A., and Susi, A. (2020). Mining user opinions to sup-\nport requirement engineering: An empirical study. In Advanced Information Systems\nEngineering, pages 401–416, Cham. Springer International Publishing.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805.\nGarc´ıa, S., Fern´andez, A., Luengo, J., and Herrera, F. (2010). Advanced nonparamet-\nric tests for multiple comparisons in the design of experiments in computational in-\ntelligence and data mining: Experimental analysis of power. Information Sciences,\n180(10):2044–2064.\nGuzman, E., El-Haliby, M., and Bruegge, B. (2015). Ensemble methods for app review\nclassiﬁcation: An approach for software evolution (n). In 2015 30th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering (ASE), pages 771–776.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., and Stoyanov, V . (2019). Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nMaalej, W., Kurtanovi´c, Z., Nabil, H., and Stanik, C. (2016). On the automatic classiﬁca-\ntion of app reviews. Requirements Engineering, 21(3):311–331.\nMaalej, W., Nayebi, M., Johann, T., and Ruhe, G. (2016). Toward data-driven require-\nments engineering. IEEE Software, 33(1):48–54.\nMarcacini, R. M., Rossi, R. G., Matsuno, I. P., and Rezende, S. O. (2018). Cross-domain\naspect extraction for sentiment analysis: A transductive learning approach. Decision\nSupport Systems, 114:70–80.\nMessaoud, M. B., Jenhani, I., Jemaa, N. B., and Mkaouer, M. W. (2019). A multi-label\nactive learning approach for mobile app user review classiﬁcation. In International\nConference on Knowledge Science, Engineering and Management, pages 805–816.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed\nrepresentations of words and phrases and their compositionality. InAdvances in neural\ninformation processing systems, pages 3111–3119.\nMulder, W., Bethard, S., and Moens, M.-F. (2015). A survey on the application of recur-\nrent neural networks to statistical language modeling. Computer Speech & Language,\n30(1):61–98.\nOtter, D. W., Medina, J. R., and Kalita, J. K. (2020). A survey of the usages of deep\nlearning for natural language processing. IEEE Transactions on Neural Networks and\nLearning Systems.\nPagano, D. and Maalej, W. (2013). User feedback in the appstore: An empirical study. In\nIEEE International Requirements Engineering Conference (RE), pages 125–134.\nReimers, N. and Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese\nbert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3973–3983.\nRibeiro, M. T., Singh, S., and Guestrin, C. (2016). ” why should i trust you?” explaining\nthe predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD interna-\ntional conference on knowledge discovery and data mining, pages 1135–1144.\nSanh, V ., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\nShah, F. A., Sirts, K., and Pfahl, D. (2019). Using app reviews for competitive analysis:\nTool support. In Proceedings of the 3rd ACM SIGSOFT International Workshop on\nApp Market Analytics, W AMA 2019, pages 40–46, New York, NY , USA. ACM.\nTan, P., Steinbach, M., and Kumar, V . (2013).Introduction to Data Mining: Pearson New\nInternational Edition. Pearson Education Limited.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.,\nand Polosukhin, I. (2017). Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008.\nWang, C., Zhang, F., Liang, P., Daneva, M., and van Sinderen, M. (2018). Can app\nchangelogs improve requirements classiﬁcation from app reviews? an exploratory\nstudy. In Proceedings of the 12th ACM/IEEE International Symposium on Empirical\nSoftware Engineering and Measurement, pages 1–4.\nZhou, X., Zhang, Y ., Cui, L., and Huang, D. (2020). Evaluating commonsense in pre-\ntrained language models. In AAAI, pages 9733–9740.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8172193765640259
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6456122994422913
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.6258701086044312
    },
    {
      "name": "Task (project management)",
      "score": 0.569782018661499
    },
    {
      "name": "Natural language processing",
      "score": 0.55620938539505
    },
    {
      "name": "Feature engineering",
      "score": 0.5395488142967224
    },
    {
      "name": "Language model",
      "score": 0.49425482749938965
    },
    {
      "name": "Representation (politics)",
      "score": 0.4708159565925598
    },
    {
      "name": "Feature extraction",
      "score": 0.4143412411212921
    },
    {
      "name": "Machine learning",
      "score": 0.38090288639068604
    },
    {
      "name": "Information retrieval",
      "score": 0.3236750364303589
    },
    {
      "name": "Deep learning",
      "score": 0.1989736557006836
    },
    {
      "name": "Engineering",
      "score": 0.09459635615348816
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I17974374",
      "name": "Universidade de São Paulo",
      "country": "BR"
    }
  ]
}