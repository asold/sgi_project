{
    "title": "YNU-HPCC at WASSA-2023 Shared Task 1: Large-scale Language Model with LoRA Fine-Tuning for Empathy Detection and Emotion Classification",
    "url": "https://openalex.org/W4385718300",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2106786797",
            "name": "Yukun Wang",
            "affiliations": [
                "Yunnan University"
            ]
        },
        {
            "id": "https://openalex.org/A2105468542",
            "name": "Jin Wang",
            "affiliations": [
                "Yunnan University"
            ]
        },
        {
            "id": "https://openalex.org/A2115527613",
            "name": "XueJie Zhang",
            "affiliations": [
                "Yunnan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4281606473",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2130359236",
        "https://openalex.org/W3038880648",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4200186624",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1496099853",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3161925192"
    ],
    "abstract": "This paper describes the system for the YNU-HPCC team in WASSA-2023 Shared Task 1: Empathy Detection and Emotion Classification. This task needs to predict the empathy, emotion, and personality of the empathic reactions. This system is mainly based on the Decoding-enhanced BERT with disentangled attention (DeBERTa) model with parameter-efficient fine-tuning (PEFT) and the Robustly Optimized BERT Pretraining Approach (RoBERTa). Low-Rank Adaptation (LoRA) fine-tuning in PEFT is used to reduce the training parameters of large language models. Moreover, back translation is introduced to augment the training dataset. This system achieved relatively good results on the competition’s official leaderboard. The code of this system is available here.",
    "full_text": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pages 526–530\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nYNU-HPCC at W ASSA-2023 Shared Task 1: Large-scale Language Model\nwith LoRA Fine-Tuning for Empathy Detection and Emotion Classification\nYukun Wang, Jin Wang and Xuejie Zhang\nSchool of Information Science and Engineering\nYunnan University\nKunming, China\nwangyukun@mail.ynu.edu.cn, {wangjin, xjzhang}@ynu.edu.cn\nAbstract\nThis paper describes the system for the YNU-\nHPCC team in WASSA-2023 Shared Task 1:\nEmpathy Detection and Emotion Classification.\nThis task needs to predict the empathy, emo-\ntion, and personality of the empathic reactions.\nThis system is mainly based on the Decoding-\nenhanced BERT with disentangled attention\n(DeBERTa) model with parameter-efficient\nfine-tuning (PEFT) and the Robustly Optimized\nBERT Pretraining Approach (RoBERTa). Low-\nRank Adaptation (LoRA) fine-tuning in PEFT\nis used to reduce the training parameters of\nlarge language models. Moreover, back trans-\nlation is introduced to augment the training\ndataset. This system achieved relatively good\nresults on the competition’s official leaderboard.\nThe code of this system is available here.\n1 Introduction\nThe purpose of W ASSA-2023 Shared Task 1 (Bar-\nriere et al., 2023) is to use empathic reaction data\nto predict hidden sentiment and personality. This\ntask consisted of five tracks:\n• Track 1: Empathy and Emotion Prediction in\nConversations (CONV), which consists in pre-\ndicting empathy, emotion polarity, and emo-\ntional intensity in a conversation;\n• Track 2: Empathy Prediction (EMP), which\nconsists in predicting empathy, and personal\ndistress in an essay;\n• Track 3: Emotion Classification (EMO),\nwhich consists in predicting the emotion in\nan essay;\n• Track 4: Personality Prediction (PER), which\nconsists in predicting the conscientiousness,\nopenness, extraversion, agreeableness, and\nstability of the essay writer;\n• Track 5: Interpersonal Reactivity Index Pre-\ndiction (IRI), which consists in predicting\nperspective-taking, personal distress, fantasy,\nand empathetic concern of the essay writer;\nAlthough the prediction goals are different, all\nfive tracks can be considered as either a sentiment\nclassification (Peng et al., 2020) or regression task\n(Kong et al., 2022). One of the biggest challenges\nin this task lies in how to learn representation for\nthe given text. The early exploration was based\non text similarity (Jijkoun and Rijke, 2005) or text\nalignment (de Marneffe et al., 2008). With the de-\nvelopment of neural networks, convolutional neural\nnetworks (CNN) (Kim, 2014) and recurrent neural\nnetworks (RNN) (Zaremba et al., 2014) and their\nvariants are adopted to learn text representations.\nBoth CNN and RNN are shallow models, which\nonly incorporate previous knowledge in the first\nlayer of the model. The models are also based on\nword embeddings that are useful in only capturing\nthe semantic meaning of words without understand-\ning higher-level concepts like anaphora, long-term\ndependencies, and many more.\nBeyond word embeddings, recent studies pro-\nposed embedding from language models (ELMo),\nwhich can learn word embeddings by incorporating\nboth word-level characteristics as well as contex-\ntual semantics (Zhang et al., 2021). This also led to\nthe emergence of pre-trained models (PLM) using\nTransformers as basic units. The PLMs, such as\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019), ALBERT (Lan et al., 2019), and DeBERTa\n(He et al., 2020), are first fed a large amount of\nunannotated data, allowing the model to learn the\nusage of various words and how the language is\nwritten in general. Then, they can be finetuned to\nbe transferred to a Natural Language Processing\n(NLP) task where it is fed another smaller task-\nspecific dataset. As the scale of PLMs increases,\nthe model performance in downstream tasks be-\ncomes better and better. Nevertheless, the fine-\n526\ntuning procedure brings about increased require-\nments for model training costs. For example, the\nlarge sequence-to-sequence model GPT-3 has 175B\nparameters (Brown et al., 2020). To reduce train-\ning costs, recent studies suggest using parameter-\nefficient fine-tuning (PEFT) (Houlsby et al., 2019)\nto enable the efficient adaption of PLMs to down-\nstream applications without fine-tuning all the pa-\nrameters of the PLMs.\nTo this end, this paper proposes to use DeBERTa\nfine-tuned with Low-Rank Adaptation (LoRA) (Hu\net al., 2021) in PEFT and RoBERTa for all tracks in\nthis competition. Both the DeBERTa and RoBERTa\nwere initialized from a well-trained checkpoint,\ne.g., deberta-v2-xxlarge with 1.5B parame-\nters and roberta-base with 125M parameters.\nFor finetuning, LoRA only fine-tuned a small num-\nber of (extra) model parameters, thereby greatly de-\ncreasing the computational and storage costs. For\nclassification tasks, a softmax head with the cross-\nentropy loss was applied, while a linear decoder\nhead with the mean squared error was adopted for\nregression tasks.\nThe experimental results on the development\ndataset show that the XXL version of DeBERTa\nwith LoRA and back translation achieves the best\nperformance in tracks 1, 3, and 5. Although\nthe number of trainable parameters decreases, the\nmodel achieves performance comparable to that of\nfull fine-tuning. Additionally, RoBERTa with back\ntranslation achieved the best performance in tracks\n2 and 4. The difference in the performance of the\ntwo models on different tracks may be due to the\nimpact of the size of the training dataset.\nThe rest of this paper is organized as follows.\nSection 2 describes the system model and method.\nSection 3 discusses the specific experimental re-\nsults. Conclusions are finally drawn in Section 4.\n2 System description\nThe architecture of the proposed model is shown\nin Figure 1. The given text of conversations or es-\nsays is input into the tokenizer and then segmented\ninto the corresponding token ID. Subsequently, De-\nBERTa or RoBERTa’s encoder is used to extract\nthe features of the text in a vector format. Mean-\nwhile, LoRA is used to reduce fine-tuning parame-\nters without degrading performance too much. Fi-\nnally, the encoded hidden representation is used for\nboth sentiment classification and regression.\nsoftmax\n…………\nargmax\nClass\nLabel\nOutput Layer\nt[CLS] tl tn tn+1 tm t[SEP]… …\nE[CLS] E1 En En+1 Em E[SEP]… …\nDeBERTa\n[CLS] x1 Xn+1 xm [SEP]… …\nTokenizer\nI feel so sad for these people. It breaks my heart to see \nthem...\nConversations Essays\nClass\nScore\nxn\nor\nFigure 1: The structure for the system.\n2.1 Tokenizer\nSentencePiece and WordPiece were used for De-\nBERTa and RoBERTa to divide the text into sub-\nwords, respectively. The final output X of the tok-\nenizer is denoted as,\nX = [CLS]x1x2 . . . xm[SEP ] (1)\nwhere m is the length of the given text, the [CLS]\nspecial tag is used to indicate the beginning of a\ntext sequence, and the [SEP] special tag is used to\nindicate the separation of a text sequence.\n2.2 RoBERTa\nThe RoBERTa used in this system is a model\nimproved on BERT. BERT’s pre-trained tasks in-\nclude Masked Language Model (MLM) and Next\nSentence Prediction (NSP). RoBERTa removed\nthe NSP task, increased the batch size of the\nmodel, and used more training data. The perfor-\nmance improvement of RoBERTa has been demon-\nstrated through experimental comparison. The\nRoBERTa used in this task was initialized from\nroberta-base, with the main structure of 12\nlayers, 768 hidden size, and 125M total parame-\nters.\n2.3 DeBERTa\nDeBERTa used in this system improves the text\nrepresentation capabilities of BERT and RoBERTa\nmodels using disentangled attention and enhanced\nmask decoder methods. Each word is represented\nusing two vectors that encode its content and po-\nsition, respectively. The attention weights among\n527\nPretrained\nWeights\nWЄ Rd*d   \nA=N(0,∂ 2)\nB=0\nh\nd\nr\nx\nFigure 2: The conceptual diagram of parameter-efficient\nLoRA fine-tuning.\nwords are computed using disentangled matrices\non their contents and relative positions. Then, an\nenhanced mask decoder is used to replace the out-\nput softmax layer to predict the masked tokens\nfor model pretraining. It outperforms BERT and\nRoBERTa on many natural language understanding\n(NLU) tasks. The checkpoint of DeBERTa used in\nthis system is deberta-v2-xxl, with the main\nstructure of 48 layers, 1536 hidden size, and 1.5B\ntotal parameters.\n2.4 LoRA\nTransferring the models to the downstream tasks\nusually depends on the size of the training dataset\nand pre-trained model. However, the hardware cost\nof using large models is very significant. Mean-\nwhile, large models are over-parameterized and\nhave a smaller intrinsic dimension (Houlsby et al.,\n2019). Therefore, this system used LoRA to freeze\nmost parameters and fine-tune the model through\nlow-rank matrices. The LoRA decomposition is\ndefined as,\nW0 + ∆Wx = W0 + BAx (2)\nwhere W0 represents the original parameter matrix.\nIt is very huge and difficult to train. In this sys-\ntem, training updates to W0 can be represented by\n∆W. Therefore, W can be frozen to reduce a large\nnumber of training parameters. A and B represent\nthe low-rank factorization matrix of W0. A is ini-\ntialized with random Gaussian and B is initialized\nwith zero. Therefore, ∆W is initialized with zero.\nLoRA reduces parameters by training a low-rank\niterative decomposition matrix of the original pa-\nrameter matrix. The original parameters of XXL\nDeBERTa used in this system are 1.5B, while the\ntrainable parameters after LoRA processing are\naround 4 million. So, this method makes using a\nlarge language model on consumer-grade GPUs a\nreality.\n2.5 Output Layer\nThe output layer is implemented in two distinct\nways to accomplish classification and regression\ntasks.\nRegression. Regression was performed for tracks\n1, 2, 4, and 5. The training goal is to minimize the\nmean squared error (MSE) loss, denoted as,\nL1 = 1\nn\nn∑\ni=1\n(yi − Pi)2 (3)\nwhere Pi is the predicted value, yi represents the\nground-truth, and n represents the number of train-\ning samples in a batch.\nClassification. The classification was performed\nfor track 3. A softmax function is used to pre-\ndict probability distribution over the candidate la-\nbels. The training objective is to minimize the\ncross-entropy between the predicted labels and the\nground truth, denoted as,\nL2 = − 1\nN\n∑\ni\nC∑\nc=1\nyic log Pic (4)\nwhere C represents the number of categories clas-\nsified, yic is the ground-truth label, and Pic repre-\nsents the prediction probability of the c-th class.\n3 Experimental Results\nThis section evaluates the performance of the pro-\nposed system for both sentiment classification and\nregression tasks.\n3.1 Datasets\nThis task is based on an Empathic Conversations\ndataset. The dataset marks conversations and es-\nsays after people read news stories about individ-\nuals, groups, or others who have been harmed\n(Omitaomu et al., 2022). This dataset for train-\ning contains two levels of sentiment classification:\n(1) Conversations between two users after reading\nthe same news stories. The labels mainly include\nEmotional Polarity, Emotion, and Empathy. (2)\nEssays from each user. The labels mainly include\nEmpathy, Emotion, Personality, and Interpersonal\nReactivity Index. Each sentimental transition in\nuser conversations or essays is interpreted as labels.\nThe size of the training dataset for the conversation\nlevel is around 8700, while the size of the training\n528\n/uni00000014/uni00000048/uni00000010/uni00000018/uni00000016/uni00000048/uni00000010/uni00000018/uni00000018/uni00000048/uni00000010/uni00000018/uni0000001a/uni00000048/uni00000010/uni00000018/uni0000001c/uni00000048/uni00000010/uni00000018\n/uni0000002f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000033/uni00000048/uni00000044/uni00000055/uni00000056/uni00000052/uni00000051/uni00000003/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000037/uni0000004b/uni00000048/uni00000003/uni00000053/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000047/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000057/uni00000003/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\n/uni00000013/uni00000011/uni00000014/uni00000013\n/uni00000013/uni00000011/uni00000014/uni00000018\n/uni00000013/uni00000011/uni00000015/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000013/uni00000011/uni00000016/uni00000018\n/uni00000013/uni00000011/uni00000017/uni00000013\n/uni00000013/uni00000011/uni00000017/uni00000018\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000030/uni00000044/uni00000046/uni00000055/uni00000052/uni00000003F1/uni00000010/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000037/uni00000055/uni00000044/uni00000046/uni0000004e/uni00000003/uni00000014/uni00000003/uni00000010/uni00000003/uni00000026/uni00000032/uni00000031/uni00000039\n/uni00000037/uni00000055/uni00000044/uni00000046/uni0000004e/uni00000003/uni00000015/uni00000003/uni00000010/uni00000003/uni00000028/uni00000030/uni00000033\n/uni00000037/uni00000055/uni00000044/uni00000046/uni0000004e/uni00000003/uni00000016/uni00000003/uni00000010/uni00000003/uni00000028/uni00000030/uni00000032\n/uni00000037/uni00000055/uni00000044/uni00000046/uni0000004e/uni00000003/uni00000017/uni00000003/uni00000010/uni00000003/uni00000033/uni00000028/uni00000035\n/uni00000037/uni00000055/uni00000044/uni00000046/uni0000004e/uni00000003/uni00000018/uni00000003/uni00000010/uni00000003/uni0000002c/uni00000035/uni0000002c\nFigure 3: The performance of different learning rates\non development dataset.\ndataset for the essay level is around 770. Macro\nCorrelation metric is used in tracks 1, 2, 4, and 5,\nMacro F1-score is used in track 3.\n3.2 Implementation Details\nThe conversation-level dataset provided conversa-\ntion text, and the essay-level dataset provided es-\nsay text and person-level demographic informa-\ntion (age, gender, ethnicity, income, and education\nlevel). In track 1, this system used conversation\ntext as training data and used essay text as training\ndata in tracks 2, 3, 4 and 5. All training datasets\nare first translated into Chinese and then translated\nback into English. This method of back translation\ncan double the training datasets. Additionally, this\nsystem has chosen BERT as a baseline model.\nThe learning rate was fine-tuned on the devel-\nopment dataset. The results were shown in Fig.\n3.\n3.3 Comparative Results and Discussion\nTables 1 and 2 show the comparative results of\nBERT, RoBERTa, and DeBERTa with LoRA on\ndifferent classification and regression tasks on the\ndevelopment dataset. It can be found that the aver-\nage performance of the optimized RoBERTa and\nDeBERTa is better than BERT. DeBERTa’s disen-\ntangled attention mechanism helps to improve the\nmodel’s text representation ability because it not\nonly calculates the attention weight of content and\nrelative position for all word pairs but also consid-\ners the absolute positions of words. The results\nshow that DeBERTa + LoRA performs better in\ntracks 1, 3, and 5, while RoBERTa performs better\nin tracks 2 and 4. This may be due to the relatively\nlarger scale of training data for track 1, and the\nTrack BERT RoBERTaDeBERTa+LoRA\nTrack1-CONV0.714 0.721 0.767\nTrack2-EMP0.502 0.624 0.544\nTrack4-PER0.342 0.593 0.508\nTrack5-IRI 0.278 0.353 0.39\nTable 1: Comparative results using Pearson Correlation\nin the development dataset.\nTrack BERT RoBERTaDeBERTa+LoRA\nTrack3-EMO0.271 0.169 0.486\nTable 2: Comparative results using Macro F1 score in\nthe development dataset.\nTrack Score\nTrack1-CONV 0.730 (Pearson Correlation)\nTrack2-EMP 0.288 (Pearson Correlation)\nTrack3-EMO 0.514 (Macro F1)\nTrack4-PER 0.252 (Pearson Correlation)\nTrack5-IRI 0.154 (Pearson Correlation)\nTable 3: Final score in the test dataset.\nfact that track 3 is a complex 31-classification task.\nTherefore, DeBERTa+LoRA improves the perfor-\nmance of sentiment classification and regression\ntasks. We submitted the best results of each track\non the leaderboard. The final results of the test\ndataset are shown in Table 3.\n4 Conclusion\nThis paper proposed a system submitted in shared\ntask 1 of WASSA-2023, which uses RoBERTa\nand XXL version of DeBERTa as the pre-trained\nmodels and fine-tuning the DeBERTa model us-\ning LoRA. The experimental results indicate that\nthis system has achieved good performance. In\naddition, this system has a lot of space for improve-\nment compared to the top-ranked systems. Future\nworks will attempt to try other text augmentation\nand generation methods to achieve better results.\nAcknowledgement\nThis work was supported by the National Natural\nScience Foundation of China (NSFC) under Grants\nNos. 61966038 and 62266051. The authors would\nlike to thank the anonymous reviewers for their\nconstructive comments.\nReferences\nValentin Barriere, Shabnam Tafreshi, João Sedoc, and\nSalvatore Giorgi. 2023. Wassa 2023 shared task:\n529\nPredicting empathy, emotion and personality in inter-\nactions and reaction to news stories. In Proceedings\nof the 13th Workshop on Computational Approaches\nto Subjectivity, Sentiment & Social Media Analysis.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMarie-Catherine de Marneffe, Anna N. Rafferty, and\nChristopher D. Manning. 2008. Finding contradic-\ntions in text. In ACL 2008, Proceedings of the 46th\nAnnual Meeting of the Association for Computational\nLinguistics, June 15-20, 2008, Columbus, Ohio, USA,\npages 1039–1047. The Association for Computer\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-\nenhanced BERT with disentangled attention. CoRR,\nabs/2006.03654.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. CoRR, abs/2106.09685.\nValentin Jijkoun and Maarten Rijke. 2005. Recognizing\ntextual entailment using lexical similarity. Journal of\nColloid and Interface Science - J COLLOID INTER-\nFACE SCI.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classification. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Spe-\ncial Interest Group of the ACL, pages 1746–1751.\nACL.\nJun Kong, Jin Wang, and Xuejie Zhang. 2022. Hi-\nerarchical BERT with an adaptive fine-tuning strat-\negy for document classification. Knowl. Based Syst.,\n238:107872.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. 2019. ALBERT: A lite BERT for self-\nsupervised learning of language representations.\nCoRR, abs/1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nDamilola Omitaomu, Shabnam Tafreshi, Tingting\nLiu, Sven Buechel, Chris Callison-Burch, Johannes\nEichstaedt, Lyle Ungar, and Jo ao Sedoc. 2022.\nEmpathic conversations: A multi-level dataset\nof contextualized conversations. arXiv preprint\narXiv:2205.12698.\nBo Peng, Jin Wang, and Xuejie Zhang. 2020. Adver-\nsarial learning of sentiment word representations for\nsentiment analysis. Information Sciences, 541:426–\n441.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nCoRR, abs/1409.2329.\nYou Zhang, Jin Wang, and Xuejie Zhang. 2021. Person-\nalized sentiment classification of customer reviews\nvia an interactive attributes attention model. Knowl.\nBased Syst., 226:107135.\n530"
}