{
    "title": "Transformer Transforms Salient Object Detection and Camouflaged Object Detection",
    "url": "https://openalex.org/W3153906112",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2132131724",
            "name": "Yuxin Mao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2087085944",
            "name": "Jing Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3086406903",
            "name": "Zhexiong Wan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2474928823",
            "name": "Yuchao Dai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139458400",
            "name": "Aixuan Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2803832878",
            "name": "Yunqiu Lv",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2166086154",
            "name": "Xinyu Tian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2242721764",
            "name": "Deng-Ping Fan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100194327",
            "name": "Nick Barnes",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W20683899",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W3174691471",
        "https://openalex.org/W3168112135",
        "https://openalex.org/W3097725659",
        "https://openalex.org/W3177087374",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3034320133",
        "https://openalex.org/W2038754339",
        "https://openalex.org/W1894057436",
        "https://openalex.org/W2100470808",
        "https://openalex.org/W2970642899",
        "https://openalex.org/W2086791339",
        "https://openalex.org/W2556967412",
        "https://openalex.org/W2962823460",
        "https://openalex.org/W3035422681",
        "https://openalex.org/W3104917119",
        "https://openalex.org/W3108812909",
        "https://openalex.org/W2964352379",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2887522866",
        "https://openalex.org/W2938260698",
        "https://openalex.org/W2765838470",
        "https://openalex.org/W2570916748",
        "https://openalex.org/W3035633116",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W2798857366",
        "https://openalex.org/W3176152216",
        "https://openalex.org/W3202066758",
        "https://openalex.org/W2963529609",
        "https://openalex.org/W3034453930",
        "https://openalex.org/W3108608656",
        "https://openalex.org/W2800623305",
        "https://openalex.org/W3035687312",
        "https://openalex.org/W2907643346",
        "https://openalex.org/W3173782971",
        "https://openalex.org/W3139517252",
        "https://openalex.org/W2957414648",
        "https://openalex.org/W3200675427",
        "https://openalex.org/W2991471181",
        "https://openalex.org/W3145269263",
        "https://openalex.org/W2895251968",
        "https://openalex.org/W3035357085",
        "https://openalex.org/W2963685207",
        "https://openalex.org/W3179443972",
        "https://openalex.org/W3034684132",
        "https://openalex.org/W2963112696",
        "https://openalex.org/W933200379",
        "https://openalex.org/W3104495176",
        "https://openalex.org/W2963342032",
        "https://openalex.org/W2002781701",
        "https://openalex.org/W2740667773",
        "https://openalex.org/W3114152269",
        "https://openalex.org/W2128272608",
        "https://openalex.org/W3096966254",
        "https://openalex.org/W3136635488",
        "https://openalex.org/W3099871687",
        "https://openalex.org/W3127401175",
        "https://openalex.org/W3097053213",
        "https://openalex.org/W3035284915",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2171378720",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2039313011",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963868681",
        "https://openalex.org/W2962864875",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2047670868",
        "https://openalex.org/W1993713494",
        "https://openalex.org/W2950623159",
        "https://openalex.org/W2777511827",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3154371655",
        "https://openalex.org/W3002301267",
        "https://openalex.org/W1966025376",
        "https://openalex.org/W2520707372",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2939217524",
        "https://openalex.org/W3035484352",
        "https://openalex.org/W2037954058",
        "https://openalex.org/W3108822985",
        "https://openalex.org/W3109623941",
        "https://openalex.org/W2799213142",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2998449272",
        "https://openalex.org/W1687112550",
        "https://openalex.org/W2963020481",
        "https://openalex.org/W3106587394",
        "https://openalex.org/W3158818292",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W3147597595",
        "https://openalex.org/W2943545929",
        "https://openalex.org/W2618608106",
        "https://openalex.org/W2990984982",
        "https://openalex.org/W3119780109",
        "https://openalex.org/W2039298799",
        "https://openalex.org/W3101787898",
        "https://openalex.org/W2909381593",
        "https://openalex.org/W2961348656",
        "https://openalex.org/W3120113457",
        "https://openalex.org/W2948300571",
        "https://openalex.org/W3092344722",
        "https://openalex.org/W2996884277",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2987701848",
        "https://openalex.org/W3139180514"
    ],
    "abstract": "The transformer networks are particularly good at modeling long-range dependencies within a long sequence. In this paper, we conduct research on applying the transformer networks for salient object detection (SOD). We adopt the dense transformer backbone for fully supervised RGB image based SOD, RGB-D image pair based SOD, and weakly supervised SOD within a unified framework based on the observation that the transformer backbone can provide accurate structure modeling, which makes it powerful in learning from weak labels with less structure information. Further, we find that the vision transformer architectures do not offer direct spatial supervision, instead encoding position as a feature. Therefore, we investigate the contributions of two strategies to provide stronger spatial supervision through the transformer layers within our unified framework, namely deep supervision and difficulty-aware learning. We find that deep supervision can get gradients back into the higher level features, thus leads to uniform activation within the same semantic object. Difficulty-aware learning on the other hand is capable of identifying the hard pixels for effective hard negative mining. We also visualize features of conventional backbone and transformer backbone before and after fine-tuning them for SOD, and find that transformer backbone encodes more accurate object structure information and more distinct semantic information within the lower and higher level features respectively. We also apply our model to camouflaged object detection (COD) and achieve similar observations as the above three SOD tasks. Extensive experimental results on various SOD and COD tasks illustrate that transformer networks can transform SOD and COD, leading to new benchmarks for each related task. The source code and experimental results are available via our project page: this https URL.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nGenerative Transformer for Accurate and\nReliable Salient Object Detection\nYuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai*,\nAixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan and Nick Barnes\nAbstract‚ÄîTransformer, which originates from machine translation, is particularly powerful at modeling long-range dependencies.\nCurrently, the transformer is making revolutionary progress in various vision tasks, leading to signiÔ¨Åcant performance improvements\ncompared with the convolutional neural network (CNN) based frameworks. In this paper, we conduct extensive research on exploiting\nthe contributions of transformers foraccurate and reliable salient object detection. For the former, we apply transformer to a deterministic\nmodel, and explain that the effective structure modeling and global context modeling abilities lead to its superior performance compared\nwith the CNN based frameworks. For the latter, we observe that both CNN and transformer based frameworks suffer greatly from the\nover-conÔ¨Ådence issue, where the models tend to generate wrong predictions with high conÔ¨Ådence. To estimate the reliability degree\nof both CNN- and transformer-based frameworks, we further present a latent variable model, namely inferential generative adversarial\nnetwork (iGAN), based on the generative adversarial network (GAN). The stochastic attribute of the latent variable makes it convenient\nto estimate the predictive uncertainty, serving as an auxiliary output to evaluate the reliability of model prediction. Different from the\nconventional GAN, which deÔ¨Ånes the distribution of the latent variable as Ô¨Åxed standard normal distribution N(0, I), the proposed\n‚ÄúiGAN‚Äù infers the latent variable by gradient-based Markov Chain Monte Carlo (MCMC), namely Langevin dynamics, leading to an input-\ndependent latent variable model. We apply our proposed iGAN to both fully and weakly supervised salient object detection, and explain\nthat iGAN within the transformer framework leads to both accurate and reliable salient object detection.\nIndex Terms‚ÄîVision Transformer, Salient Object Detection, Inferential Generative Adversarial Network.\n!\n1 I NTRODUCTION\nV\nIsual salient object detection (SOD) [1]‚Äì[14] aims to lo-\ncalize and segment the regions of an image that attract\nhuman attention, which is usually deÔ¨Åned as a binary segmentation\ntask. Depending on whether unimodal data ( i.e. RGB image)\nor multimodal data ( i.e. RGB-D data) is used, the majority of\nsalient object detection models can be roughly divided into RGB\nimage saliency detection [2], [3], [12]‚Äì[14] and RGB-D image\npair saliency detection [4], [5]. The former involves two variables,\nnamely the RGB image x and its corresponding ground truth\nsaliency map y, while the extra depth data d is involved in the\nlatter, making it a multimodal learning task.\nGiven the one-to-one mapping formulation, and the backbone-\ndependent network structures, the main focus of conventional\ndeep RGB image-based saliency detection models is achieving\nstructure-preserving prediction with effective high/low level fea-\nture aggregation. On the other hand, as a multimodal learning\ntask, the basic assumption of RGB-D saliency detection is that\nthe extra depth data can bring informative geometric information,\nwhich can be complementary to the appearance information from\nthe RGB image. In this case, the main focus of existing RGB-D\n‚Ä¢ Yuxin Mao, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv and\nXinyu Tian are with School of Electronics and Information, Northwestern\nPolytechnical University, Xi‚Äôan, China.\n‚Ä¢ Jing Zhang and Nick Barnes are with School of Computing, Australian\nNational University, Canberra, Australia.\n‚Ä¢ Deng-Ping Fan is with the CS, Nankai University, Tianjin, China.\n‚Ä¢ Corresponding author: Yuchao Dai (Email: daiyuchao@nwpu.edu.cn).\n‚Ä¢ The source code and experimental results are publicly available via our\nproject page: https://github.com/fupiao1998/TransformerSOD.\nSOD models [5], [15]‚Äì[17] is to extensively utilize the geometric\ninformation for effective multimodal learning.\nBy thoroughly analyzing the existing saliency detection mod-\nels, we observe three major issues, namely the less effective global\ncontext modeling abilities, the missing structure information issue,\nand the inconsistent depth distribution issue.\nLess effective global context modeling: Conventional CNN\nbased saliency detection models usually consist of two main\nparts: 1) an encoder for feature extraction; and 2) a decoder for\nhigh/low feature aggregation, where the encoder is usually adopted\nfrom an ImageNet pre-trained backbone network, e.g., VGG [29],\nResNet [18]. In this way, the SOD models are mainly designed to\nobtain effective decoders for feature aggregation [2], [3], [30]. We\nvisualize the different levels of CNN and transformer backbone\nfeatures of the SOD models in Fig. 1 and Ô¨Ånd that the former\nencodes less accurate global context than the latter, especially for\nthe large salient foreground (the Ô¨Årst row of Fig. 1).\nMissing structure information: Conventional CNN backbones\nhave gradually larger receptive Ô¨Åelds with the deeper layers by\nusing stride or pooling operation, leading to extensive down-\nsampling as shown in Fig. 1 (‚ÄúCNN‚Äù), where we show the different\nlevels of backbone features of the ResNet50 [18] after Ô¨Åne-tuning\nit for SOD. We observe missing structure information 1 in both\nhigher and lower level features, which makes effective high/low\nfeature aggregation especially necessary for CNN backbone based\nframework to achieve structure-accurate predictions. However,\nonce the information is lost, it will not be fully recovered.\nInconsistent depth distribution: For RGB-D saliency detection,\n1. We deÔ¨Åne ‚Äústructure information‚Äù as the detail-alignment of prediction\nwith the input image.\narXiv:2104.10127v5  [cs.CV]  30 Dec 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nImage GT CNN Transformer\nFig. 1. Visualizing the features of the ResNet50 backbone [18] (‚ÄúCNN‚Äù) and the transformer backbone (‚ÄúTransformer‚Äù) after Ô¨Åne-tuning them for\nSOD.\nTABLE 1\nDetails of the widely used RGB-D saliency datasets to explain the domain gap issue for RGB-D saliency detection.\nDataset Year Size Type Depth Source #Train #Test\nSSB [19] 2012 1,000 Internet Stereo cameras+ optical Ô¨Çow [20] - 1,000\nNLPR [21] 2014 1,000 Indoor/Outdoor Microsoft Kinect [22] 700 300\nDES [23] 2014 135 Indoor Microsoft Kinect [22] - 135\nNJU2K [24] 2014 1,985 Movie/Internet FujiW3 camera + Sum‚Äôs optical Ô¨Çow [25] 1,500 485\nLFSD [26] 2014 80 Indoor/Outdoor Lytro Illum cameras [27] - 80\nSIP [28] 2020 929 Person in outside Huawei Mate10 - 929\nFig. 2. Global contrast of depth from benchmark RGB-D SOD datasets, where the x-axis is the Chi-squared distance between salient foreground\nand non-salient background within the depth data, and the y-axis is the number of images.\nextra depth data is involved, and the mainstream is then to effec-\ntively explore the complementary information of both modalities\nfor effective multimodal learning. We observe that the depth data\nof RGB-D saliency detection can come from different sensors\nas shown in Table 1, thus the training data and the testing data\ncan be treated as from different domains. Further, we Ô¨Ånd that\ndepth from different sensors has different contrast distributions,\nleading to inconsistent input distributions across the training and\ntesting datasets. We compute the global contrast of the depth data\nfrom different testing datasets and show the global depth contrast\nin Fig. 2. The global contrast measures the noticeability of the\nsalient objects. To obtain the global contrast of the depth data, we\nÔ¨Årst compute a 3H dimensional color histogram of both salient\nforeground and background of the depth data. Following [31],\nwe obtain an H = 16 dimensional histogram for the Red,\nGreen, and Blue channel2 of the RGB image respectively, and the\nhistogram of the color image is then the concatenation of the above\nhistograms. Then we adopt the Chi-squared distance to measure\nthe global contrast between salient objects and background. We\ndeÔ¨Åne the mean of the Chi-squared distance as the global depth\ncontrast. Fig. 2 clearly shows that the global contrast of the salient\nforeground with the depth data varies across the testing datasets.\nSimilarly, we obtain the RGB image global contrast and compute\nthe global contrast difference of the RGB image and depth for each\ntesting dataset, which is shown in the title of each Ô¨Ågure in Fig. 2.\nThe various global contrast differences between RGB images and\ndepth data further explain the different contributions of depth.\nAdvantages of Transformer: Researchers have found that the\n2. For gray image, we obtain its color version by concatenating it channel-\nwise to obtain the three-channel color image.\n‚ÄúTransformer‚Äù [32] has great potential to solve the limited recep-\ntive Ô¨Åeld issue in vision tasks. The advantage of the ‚ÄúTransformer‚Äù\nlies in the use of self-attention to capture global contextual\ninformation to establish a long-range dependency. Different from\nconvolutional neural networks that focus on a small patch of\nthe image, the transformer network [32] performs global context\nmodeling with self-attention. Inspired by [33], [34] and the accu-\nrate structure modeling ability of the transformer (see Fig. 1),\nwe conduct extensive research to explore the contributions of\nthe transformer for accurate salient object detection. SpeciÔ¨Åcally,\nwe design transformer based deterministic neural networks for\nSOD, and explain that the accurate structure modeling and the\nglobal context modeling abilities lead to its superior performance\ncompared with the CNN based frameworks (see Table 5).\nOvercoming the over-conÔ¨Ådence issue: Although signiÔ¨Åcant\nperformance has been achieved with the transformer, we still ob-\nserve ‚Äúover-conÔ¨Ådence‚Äù issue within the transformer based SOD\nmodels, where the model tends to generate wrong prediction with\nhigh conÔ¨Ådence, which is also deÔ¨Åned as the model less-calibrated\nissue in [35]. We then present an inferential generative adversarial\nnetwork (iGAN) to analyze the reliability degree [35] of the trans-\nformer based framework. Different from the conventional genera-\ntive adversarial network (GAN) [36] which deÔ¨Ånes the distribution\nof the latent variable as Ô¨Åxed standard normal distributionN(0,I),\nour proposed ‚ÄúiGAN‚Äù infers the latent variable by gradient based\nMarkov Chain Monte Carlo (MCMC), namely Langevin dynamics\n[37]. The latent variable within iGAN is sampled directly from its\ntrue posterior distribution [38], leading to more informative latent\nspace exploration. We apply the proposed iGAN to both fully\nand weakly supervised SOD, and explain that iGAN within the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\ntransformer framework leads to both accurate and reliable salient\nobject detection, where the produced uncertainty maps [39] can\nserve as an auxiliary output to explain the reliability of model\npredictions. Experimental results show that the proposed iGAN\nwithin the transformer backbone can Ô¨Åx the ‚Äúless effective global\ncontext modeling‚Äù and ‚Äúmissing structure information‚Äù issues of\nconventional CNN backbone based framework, and the auxiliary\nuncertainty outputs can be used to explain model reliability.\nFixing the depth domain gap issue: Although consistent per-\nformance improvement is achieved with the transformer backbone\nbased framework, we still observe the depth domain inconsistency\nissue, where the testing datasets with depth different from the\ntraining datasets achieve a marginal performance improvement,\ne.g. LFSD [26] in Fig. 2. To Ô¨Åx it, we present an ‚Äúauxiliary depth‚Äù\nmodule and perform self-supervised depth estimation. The deep\nhybrid model structure [40] via depth reconstruction, i.e. RGB-D\nsaliency detection as conditional generation and depth estimation\nas marginal density estimation, works effectively when a depth\ndomain gap exists.\nOur main contributions can be summarized as: 1) We exten-\nsively explore the contributions of transformer networks [32]‚Äì[34],\n[41] for accurate salient object detection and explain that the\neffective structure and global context modeling abilities lead to\nthe superior performance of the transformer-based saliency detec-\ntion network; 2) We present an inferential generative adversarial\nnetwork (iGAN) to effectively measure the reliability degree of\nthe transformer-based SOD network, leading to reliable saliency\nprediction; 3) We apply iGAN to fully and weakly supervised\nsalient object detection to extensively explore the proposed new\ngenerative model within the transformer framework.\n2 R ELATED WORK\nSalient Object Detection: Driven by visual attention [42], salient\nobjects are deÔ¨Åned as objects that have strong contrast [43]. Early\nworks usually utilized this prior for saliency related feature ex-\ntraction. Deep SOD models usually take the pretrained backbone\nnetworks [18], [29] as an encoder with UNet [44] structure, where\neffective decoders are designed to achieve high-low level feature\naggregation [2], [3], [30], [45]‚Äì[54]. Among them, Wu et al. [2]\nproposed a ‚Äústacked cross reÔ¨Ånement network‚Äù, and used the\ninteraction between the edge module and the detection module to\noptimize the two tasks at the same time. Wei et al. [3] introduced\nan adaptive selection of complementary information when aggre-\ngating multi-scale features with a structure-aware loss function.\nTang et al. [55] modeled the two tasks of discriminating salient\nregions and identifying accurate edges independently and solved\nthe limitations of low-resolution SOD by using low-resolution\nimages to delineate salient regions and using high-resolution to\nreÔ¨Åne salient regions. Meanwhile, edge detection [56], [57] is\nlikewisely used as a piece of auxiliary information [58], [59]\nto improve the performance of SOD. And different attention\nmechanisms such as spatial and channel attention [15], [60] or\npixel-wise contextual attention [46] are also used to learn more\ndiscriminative features. Unlike the mainstream design reÔ¨Ånement\nprediction networks, Zhang et al. [61] proposed an automatic\nconsolidation of multi-level features based on neural architecture\nsearch for Ô¨Çexible integration of information at different scales.\nWith extra depth information, RGB-D pair based SOD models\n[4], [5], [15], [16], [62], [63] mainly focus on exploring the\ncomplementary information between the RGB image and the\ndepth data for effective multi-modal learning. Depending on how\ninformation from these two modalities is fused, existing RGB-\nD SOD models can be roughly divided into three categories:\nearly-fusion models [4], [64], late-fusion models [65]‚Äì[67] and\ncross-level fusion models [5], [15], [17], [68]‚Äì[78]. The early-\nfusion models fuse RGB image and depth data at the input layer,\nforming a four-channel feature map. The late fusion models treat\neach mode (RGB and depth) separately, and then saliency fusion\nis achieved at the output layer. The cross-level fusion models\ngradually fuse features of RGB and depth [5], [17], [72], [73],\n[75], [77]‚Äì[83], which is the main stream for RGB-D SOD.\nVision Transformer and Its Applications: The transformer\nnetwork [32] has sparked great interests in the computer vi-\nsion community to adapt these models for vision tasks such\nas object detection [84]‚Äì[88], object tracking [89], [90], pose\nestimation [91], optical Ô¨Çow [92] etc. Inspired by the success of\nthe Vision Transformer (ViT) [41] in image classiÔ¨Åcation which\nsplits the input image into a sequence of patches and feeds them to\na standard Transformer encoder, some works extend transformers\nfor dense prediction tasks, e.g., semantic segmentation or depth\nestimation. SETR [93] and PVT [87] use several convolutional\nlayers as the decoder to upsample feature maps and get the dense\nprediction with the input image size. DPT [33] uses ViT [41] as an\nencoder to extract features from different spatial resolutions of the\ninitial embedding. Liu et al. [34] presented the Swin Transformer,\na hierarchical transformer with a shifted windowing scheme to\nachieve an efÔ¨Åcient network for vision tasks. Recently, [94],\n[95] introduce the transformer to saliency detection, achieving\nsigniÔ¨Åcant performance improvement.\nGenerative Models and Their Applications: There mainly\nexist two types of generative models, namely latent variable\nmodels [36], [96] and energy-based models [97]. The former\nusually involves an extra latent variable to model the predictive\ndistribution, and the latter directly estimates the compatibility of\nthe input and output variable with a designed energy function.\nThe variational auto-encoder (V AE) [96], [98] and generative\nadversarial network (GAN) [36] are two widely studied latent\nvariable models. V AEs use an extra inference model to con-\nstrain the distribution of the latent variable, and GANs design\na discriminator to distinguish real samples and the generated\nsamples. V AEs have already been successfully applied to image\nsegmentation [99], [100] to produce stochastic predictions during\ntesting. For saliency prediction, [101] adopts a V AE for image\nbackground reconstruction and the residual of the raw image\nand the reconstructed background is then deÔ¨Åned as the salient\nregion(s). Differently, [4] designs conditional variational auto-\nencoder (CV AE) to model the subjective nature of saliency,\nwhere the latent variable is used to model the prediction variants.\nGAN-based methods can be divided into two categories, namely\nfully-supervised and semi-supervised settings. The former [102],\n[103] uses the discriminator to distinguish model predictions from\nground truth, while the latter [104], [105] rely on the GAN\nto explore the contributions of unlabeled data. [106] introduces\nan inferential Wasserstein GAN model, which is a principled\nframework to fuse auto-encoders and Wasserstein GAN and jointly\nlearns an encoder network and a generator network motivated\nby the iterative primal-dual optimization process. Differently, we\ninfer the latent variable via Langevin dynamics [37], which suffers\nno posterior collapse issue [107].\nWeakly Supervised Segmentation Models: Instead of label-\nconsuming pixel-wise annotations, Weakly Supervised Segmenta-\ntion (WSS) models are designed to explore the possibility of using\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nweak labels, e.g., image tags [108]‚Äì[112], bounding box [113]‚Äì\n[117], scribble [6], [118]‚Äì[120], point [121], [122], as supervision.\nThe typical methods [114], [118], [123] usually consider the initial\nsegmentation map produced by traditional unsupervised methods,\nsuch as MCG [124] and GrabCut [125], as the supervision to train\nthe deep neural networks and then repeat the iterative process\nbetween reÔ¨Ånement of the prediction and training of the network.\nHowever, the framework tends to introduce accumulated label\nnoise in each step and the iteration is time-consuming. Zhang et\nal. [9] proposed an end-to-end deep learning framework to predict\nthe latent saliency map from multiple noisy saliency maps created\nby unsupervised handcrafted saliency methods and mitigated the\ninÔ¨Çuence of label noise by a speciÔ¨Åcally-designed noise modeling\nmodule. The idea is further extended [126] to use a generative\nmodel [38] to model the label noise from a single noisy saliency\nmap. In addition to noise modeling strategies, some methods reÔ¨Åne\nthe segmentation map with structure-aware loss functions. Yu et\nal. [120] used partial cross-entropy loss to expand the scribble\nregion to the whole object region and reÔ¨Åne the segmentation with\na local saliency coherency loss. Most WSS methods are based\non CNNs. We explore the potential of transformers for weakly\nsupervised segmentation, especially weakly supervised SOD with\nscribble supervision [6], [120].\n3 A CCURATE AND RELIABLE SALIENT OBJECT\nDETECTION VIA GENERATIVE TRANSFORMER\nAs a context-based task, SOD strongly relies on both local and\nglobal context, where the former is necessary for identifying\nmedian size salient foreground, and the latter is essential for large\nsalient object detection. As discussed in Sec. 1, conventional CNN\nbackbone-based frameworks [2], [30] are effective in modeling the\nlocal context, and their performance deteriorates for salient objects\nthat expand to a larger region (see Fig. 1). The self-attention\nmodel within the transformer framework [32] enables it to achieve\nlong-range dependency modeling with global context exploration,\nwhich is desirable for accurate salient object detection.\nLet‚Äôs deÔ¨Åne our training dataset as D = {xi,yi}N\ni=1 of\nsize N, where xi and yi are the input RGB image (or RGB-D\nimage pair for RGB-D saliency detection) and the corresponding\nground truth saliency map, and i indexes the samples, which\nis omitted. With the transformer backbone (we use Œ∏ to repre-\nsent its parameters), given any testing sample x‚àó with ground\ntruth y‚àó, we deÔ¨Åne its joint distribution as p(x‚àó,y‚àó,Œ∏|D) =\np(y‚àó|x‚àó,Œ∏)p(Œ∏|D)p(x‚àó|D), where p(y‚àó|x‚àó,Œ∏) represents the\npredictive distribution or the inherent randomness given Œ∏ as\nthe oracle [39]. p(Œ∏|D) explains the ambiguity of the model Œ∏\ngiven the provided training dataset D, and p(x‚àó|D) measures the\ndiscrepancy between x‚àóand the training dataset D.\nWith the global context modeling ability of transformer,\nthe p(Œ∏|D) term can be modeled more effectively compared\nwith the CNN frameworks. However, there exists no solution\nin the transformer framework to model the predictive distribu-\ntion p(y‚àó|x‚àó,Œ∏). Further, the training/testing discrepancy is not\nmentioned either, thus it is inconvenient to evaluate the domain\ngap caused by p(x‚àó|D). To Ô¨Åx the above-mentioned issues, we\nintroduce a latent variable model with an extra latent variable\nz involved to model the inherent data noise. Further, we Ô¨Ånd\none of the main domain gap for RGB-D saliency detection lies\nin the inconsistent depth data as shown in Table 1 and Fig. 2.\nWe then introduce an auxiliary depth module, achieving self-\nsupervised learning, with which it‚Äôs more convenient to evaluate\nthe training/testing discrepancy. SpeciÔ¨Åcally, with extra latent\nvariable z, the joint distribution of the testing sample x‚àócan be\nrewritten as:\np(x‚àó,y‚àó,Œ∏,z |D)\n= p(x‚àó,y‚àó,Œ∏,z,D )\np(D)\n= p(y‚àó|x‚àó,Œ∏,z )p(x‚àó|Œ∏,z,D )p(Œ∏,z,D )\np(D)\n= p(y‚àó|x‚àó,Œ∏,z )p(Œ∏|D)p(z|x‚àó,D)p(x‚àó|D).\n(1)\nThe extra latent variable in Eq. (1) makes it convenient to\nestimate p(y‚àó|x‚àó,Œ∏,z ), where the latent variable z can be sam-\npled from p(z|x‚àó,D) during testing, modeling the discrepancy\nbetween training and testing sample. SpeciÔ¨Åcally, for RGB-D\nsaliency detection, the p(x‚àó|D) term is modeled directly fol-\nlowing self-supervised learning with an auxiliary depth module,\nwhich can also be explained as an auto-encoder framework. In\nthe following, we will Ô¨Årst introduce a transformer for accurate\nsaliency detection (Sec. 3.1) for effective model parameter estima-\ntion (p(Œ∏|D)). We present the auxiliary depth module in Sec. 3.2,\nachieving an auto-encoder framework to evaluate the domain gap\nbetween training and testing samples, or the p(x‚àó|D) term. We\nwill introduce the latent variable model in Sec. 3.3 to achieve\nmodeling of p(z|x‚àó,D), with which it‚Äôs convenient to model\nthe inherent randomness of model prediction p(y‚àó|x‚àó,Œ∏,z ). We\npresent the objective function in Sec. 3.4.\n3.1 Transformer for Accurate Saliency Detection\nThe straightforward solution of using a transformer is to replace\nthe CNN backbone with a transformer backbone, leading to the\n‚Äútransformer encoder‚Äù. We take the Swin transformer [34] as our\ntransformer encoder, which takes the embedded image features as\ninput and produces a list of feature maps fŒ∏1 (x) = {tl}4\nl=1 of\nchannel size 128, 256, 512 and 1024 respectively, representing\ndifferent levels of features. Different from [33], [41] that use\nÔ¨Åxed tokenization, the Swin transformer [34] is a hierarchical\ntransformer structure whose representation is computed with self-\nattention in shifted non-overlapped windows, thus it enables even\nlarger receptive Ô¨Åeld modeling. Given the ‚Äútransformer encoder‚Äù,\nwe design a simple ‚Äúconvolution decoder‚Äù to achieve high/low\nlevel feature aggregation. SpeciÔ¨Åcally, we Ô¨Årst feed each backbone\nfeature tl to a simple convolutional block and obtain the new\nbackbone feature {t‚Ä≤\nl}4\nl=1 of the same channel size C = 32.\nSuch channel reduction operation aims to further enhance context\nmodeling and reduce the huge memory requirement. Our Ô¨Ånal\nsaliency map s = fŒ∏2 ({t‚Ä≤\nl}4\nl=1) is then obtained via a decoder\nparameterized by Œ∏2.\nThe detailed structure of the decoder can be formulated\nas s = fŒ∏msd (fŒ∏rcab [({t‚Ä≤\nl}4\nl=1)]), where [¬∑] denotes the channel-\nwise concatenation operation, fŒ∏rcab is the residual channel atten-\ntion block [127], fŒ∏msd is the multi-scale dilated convolutional\nblock [128] to obtain a one-channel saliency map. Note that,\nŒ∏ = {Œ∏1,Œ∏2}indicates the entire parameters of our salient object\ndetection network. fŒ∏(x) can directly produce the saliency map\nfor RGB image x. For RGB-D saliency detection, we perform\nearly fusion by simply concatenating the RGB image and depth\ndata at the input layer, and feeding it to a 3√ó3 convolutional layer\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nto generate a new input tensor x‚Ä≤with a channel size of 3, which\nis then fed to the saliency generator Œ∏. With the global context\nmodeling ability, the regression ability of the transformer is proven\nbetter than CNN frameworks, leading to better Œ∏estimation given\nthe same training dataset Dwith less ambiguity/uncertainty.\n3.2 Auxiliary Depth Module\nAs we discussed before, the inconsistent depth data (see both\nTable 1 and Fig. 2) may hinder the performance of existing RGB-\nD saliency detection models. We then propose an auxiliary depth\nmodule to solve the ‚Äúdistribution gap‚Äù issue within existing RGB-\nD SOD datasets. The auxiliary depth module with parameters Œ∏3\nhas the same structure as our saliency decoder Œ∏2, which takes\nthe backbone feature fŒ∏1 (x‚Ä≤) as input, and outputs a one-channel\ndepth map d‚Ä≤ = fŒ∏3 (fŒ∏1 (x‚Ä≤)). Within this framework, the Ô¨Ånal\nloss function has extra depth related loss:\nLdepth = Œ±(Œ≤‚àóLssim + (1‚àíŒ≤) ‚àóL1), (2)\nwhere Œ±= 0.1 is used to control the contribution of the auxiliary\ndepth module, and following the conventional setting, we set Œ≤ =\n0.85 in this paper. Lssim is the SSIM loss function [129] and L1\ndenotes the L1 loss.\nThe Auto-encoder for joint distribution modeling: Given a test\nsample x‚àó3, for a segmentation model with parameters Œ∏, the\noutput distribution is deÔ¨Åned as p(y‚àó|x‚àó,Œ∏), and there is no way\nto model the marginal data distribution p(x‚àó). For RGB saliency\ndetection, the ‚Äúdomain gap‚Äù issue is not that signiÔ¨Åcant. However,\nfor RGB-D saliency detection, we observe differences between\ntraining depth data and testing depth data (see Table 1 and Fig. 2),\nmaking reliable p(x‚àó) estimation necessary for RGB-D saliency\ndetection to take into account the training/testing discrepancy. In\nour setting, with the auxiliary depth module, we obtain the model-\ning of p(x‚àó) or p(x‚àó|D) via conditional self-supervised learning,\nwhere the RGB image is the conditional variable. Combining\nwith the main task, i.e. salient object detection, we achieve the\njoint distribution modeling instead of the conditional distribution\nmodeling. The hybrid model [40] is proven more robust to the\ndepth distribution gap issue as discussed in Sec. 1.\n3.3 Generative Model for Reliable Saliency Detection\nAs deep neural networks can Ô¨Åt any random noise [35], the deter-\nministic CNN and transformer backbone based models have se-\nrious over-conÔ¨Ådence issues, where the model could inaccurately\nassign a high probability to the wrong prediction. To overcome\nthis, a model which is aware of its prediction with reasonable\npredictive distribution modeling is desired. As discussed in Eq. (1),\na latent variable model makes it convenient to estimate predictive\ndistribution p(y‚àó|x‚àó,Œ∏,z ), which can be deÔ¨Åned as being Gaus-\nsian distribution via:\np(y‚àó|x‚àó,Œ∏,z ) =N(¬µ(x‚àó,Œ∏,z ),œÉ2(x‚àó,Œ∏,z )), (3)\nwhere the mean is ¬µ(x‚àó,Œ∏,z ) = Ez‚àºp(z|x‚àó,D)p(y‚àó|x‚àó,Œ∏,z )\nand the variance (uncertainty) is œÉ2(x‚àó,Œ∏,z ) =\nEz‚àºp(z|x‚àó,D)(p(y‚àó|x‚àó,Œ∏,z ) ‚àí¬µ(x‚àó,Œ∏,z ))2. Eq. (3) presents\na convenient solution for evaluating uncertainty from a latent\nvariable model, where the randomness or uncertainty of model\nprediction is controlled by the latent variable z, making\n3. we deÔ¨Åne x‚àó as RGB image for RGB saliency detection and RGB-D\nimage pair for RGB-D saliency detection.\nmeaningful z quite desirable for reliable uncertainty estimation.\nWe will Ô¨Årst introduce the existing latent variable models and\nadapt them to our task. We then analyze their advantages and\nlimitations. Based on this, we present the proposed inferential\ngenerative adversarial net.\nConventional latent variable models\n1) Generative Adversarial Nets (GAN) [36]: Within the GAN-\nbased framework, we design an extra fully convolutional discrim-\ninator gŒ≤ following [105], where Œ≤ is the parameter set of the\ndiscriminator. Two different modules (the saliency generator fŒ∏\nand the discriminator gŒ≤ in our case) play the minimax game in\nGAN based framework:\nmin\nfŒ∏\nmax\ngŒ≤\nV(gŒ≤,fŒ∏) =E(x,y)‚àºpdata(x,y)[log gŒ≤(y|x)]\n+ Ez‚àºp(z)[log(1 ‚àígŒ≤(fŒ∏(x,z)))],\n(4)\nwhere pdata(x,y) is the joint distribution of training data, p(z)\nis the prior distribution of the latent variable z, which is usually\ndeÔ¨Åned as p(z) =N(0,I).\nIn practice, we deÔ¨Åne the loss function for the generator as\nthe sum of a reconstruction loss Lrec, and an adversarial loss Ladv,\nwhich is Lgen = Lrec + ŒªLadv, where the hyper-parameter Œª is\ntuned, and empirically we set Œª = 0.1 for stable training. For\nSOD, we deÔ¨Åne the reconstruction loss Lrec as the structure-aware\nloss as in Eq. (10), and the adversarial loss as cross-entropy loss:\nLadv = Lce(gŒ≤(fŒ∏(x,z)),1), which fools the discriminator that\nthe prediction is real, where Lce is the binary cross-entropy loss,\nand 1 is an all-one matrix. The discriminator gŒ≤ is trained via\nthe loss function: Ldis = Lce(gŒ≤(fŒ∏(x,z)),0) +Lce(gŒ≤(y),1),\nwhich aims to correctly distinguish prediction and ground truth.\nSimilarly, 0 is an all-zero matrix. In this way, the generator loss\nand the discriminator loss can be summarized as:\nLgen = Lrec + ŒªLadv,\nLdis = Lce(gŒ≤(fŒ∏(x,z)),0) +Lce(gŒ≤(y),1). (5)\n2) Variational Auto-encoder (VAE) [96]: For dense prediction\ntask with input variable x and output variable y, we refer to\nconditional variational auto-encoder (CV AE) [98] instead, where\nthe input image x is the conditional variable. As a conditional\ndirected graph model, a conventional CV AE mainly contains two\nmodules: a generator model fŒ∏(x), which is a saliency generator\nin this paper, to produce the task related predictions, and an\ninference model qŒ∏(z|x,y), which infers the latent variable zwith\nimage xand annotation y as input. Learning a CV AE framework\ninvolves approximation of the true posterior distribution of zwith\nan inference model qŒ∏(z|x,y), with the loss function as:\nLcvae = Eh‚àºqŒ∏(z|x,y)[‚àílogpŒ∏(y|x,z)]\nÓ¥ô Ó¥òÓ¥ó Ó¥ö\nLrec\n+DKL(qŒ∏(z|x,y) ‚à•pŒ∏(z|x)).\n(6)\nThe Ô¨Årst term is the reconstruction loss and the second is the\nKullback-Leibler divergence of prior distribution pŒ∏(z|x) and\nposterior distribution qŒ∏(z|x,y), where both of them are usually\nparameterized by multi-layer perceptron (MLP).\n3) Alternating back-propagation (ABP): Alternating back-\npropagation [38] updates the latent variable and network param-\neters in an EM manner. Given the network prediction with the\ncurrent parameter set, it infers the latent variable by the Langevin\ndynamics based Markov Chain Monte Carlo (MCMC) [37], which\nis called ‚ÄúInferential back-propagation‚Äù [38]. Given the updated\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nùë°ùëô ùëô=0\n4 ùë°ùëô‚Ä≤ ùëô=0\n4\nùí©(0,I)\nùëß0ùëß\nTransformer\nBackbone Conv\nNeck\nDecoder\nùë• ùë† ùë¶\nDiscriminator\nGenerator\nC\nùëë ùëë‚Ä≤\nLangevin\nDynamics\nReal\nFakeC\nauxiliary depth module\nFig. 3. Flowchart of our proposed inferential GAN (iGAN) (note that the\n‚Äúauxiliary depth‚Äù module is removed for RGB SOD). The ‚ÄúGenerator‚Äù\ntakes image x and latent variable z as input, and generates saliency\nmap s, where the latent variablez is updated via the Langevin dynamics\nbased MCMC [37]. The fully convolutional ‚ÄúDiscriminator‚Äù is designed to\ndistinguish prediction (fake) and ground truth (real). d‚Ä≤is the predicted\ndepth for RGB-D SOD.\nlatent variable z, the network parameter set is updated with gra-\ndient descent, which is called ‚ÄúLearning back-propagation‚Äù [38].\nSimilar to the V AE [96] or CV AE [98] frameworks, ABP intends\nto infer z and learn the network parameter Œ∏ to minimize the\nreconstruction loss. SpeciÔ¨Åcally, ABP [38] samples z directly\nfrom its posterior distribution with a gradient-based Monte Carlo\nmethod, namely Langevin Dynamics [37]:\nzt+1 = zt + s2\nt\n2\n[‚àÇ\n‚àÇz log pŒ∏(y,zt|x)\n]\n+ stN(0,I), (7)\nwhere z0 ‚àºN(0,I), and the gradient term is deÔ¨Åned as:\n‚àÇ\n‚àÇz log pŒ∏(y,z|x) = 1\nœÉ2 (y‚àífŒ∏(x,z)) ‚àÇ\n‚àÇzfŒ∏(x,z) ‚àíz. (8)\nt is the time step for Langevin sampling, st is the step size, œÉ2\nis variance of the inherent labeling noise. As no extra network\nis involved in the ABP based framework, the Ô¨Ånal loss function\ncontains only the task-related loss.\nAnalysis of the conventional latent variable models: As no in-\nference model is included in GAN [36], the latent variable z\nwithin GAN is always sampled from the standard normal dis-\ntribution N(0,I), which is less informative. For the CV AE based\nmodel [96], [98], it samples the latent variablezfrom the designed\nposterior distribution during training, and the distribution gap\nbetween the designed posterior and the true distribution leads\nto the posterior collapse issue [107], where the latent variable\nis independent of the input image, leading to less representative\nlatent space. For the ABP [38] based framework, although it\nsamples from the true posterior distribution via Eq. (7), the task\nrelated training is not changed, and our experimental results show\nthat the deterministic performance is usually heavily inÔ¨Çuenced,\nespecially for the conventional CNN backbone based frameworks.\nThe proposed inferential GAN\nFor the V AE [96], [98] based framework, as parameters of the\nreconstruction model and the posterior net, are updated together,\nthe convergence of the DKL term in Eq. (6) may inÔ¨Çuence the\nconvergence of the reconstruction part, which is also discussed\nin [130]. Although beta-V AE [130] can slightly balance the con-\nvergence of the two parts, carefully picked hyper-parameters are\nneeded. For the ABP [38] based framework, the basic assumption\nto achieve sampling from the true posterior distribution is that the\ntime step should be large enough, and the step size should be\ninÔ¨Ånitely small (see Eq. (8)). For the GAN [36] based framework,\nthe less informative latent space makes it not ideal to be directly\nused to model the predictive distribution. However, its adversarial\ntraining strategy can usually lead to better model performance\nthan the other two latent variable models. In this paper, we\nintroduce inferential generative adversarial network (iGAN), a\nnew generative model for SOD, where we infer the latent variable\nwithin the proposed framework instead of deÔ¨Åning it as Ô¨Åxed\nN(0,I). SpeciÔ¨Åcally, the proposed iGAN infers the latent variable\nby gradient-based Markov Chain Monte Carlo (MCMC), namely\nLangevin dynamics [37] (see Fig. 3) following ABP [38], leading\nto an image conditioned latent variable. Further, as adversarial\ntraining is applied, our new generative model can achieve reliable\nlatent space exploration with fewer time steps.\nAlgorithm 1 iGAN for fully supervised salient object detection\nInput: (1) Training images {xi}N\n1 with associated saliency maps\n{yi}N\n1 , where i indexes images, and N is the size of the training\ndataset (We perform early fusion for RGB-D saliency detection\nmodel). (2) Maximal number of learning epochs Ep; (3) Numbers of\nLangevin steps for posterior T; (4) Langevin step sizes for posterior\nst and variance of inherent labeling noise œÉ2.\nOutput: Parameters Œ∏for the generator and Œ≤for the discriminator.\n1: Initialize Œ∏and Œ≤\n2: for ep‚Üê1 to Ep do\n3: Sample image-saliency pairs {(xi,yi)}N\ni\n4: For each (xi,yi), sample the prior zi\n0 ‚àºN(0,I), and sample\nthe posterior zi\nt using T Langevin steps in Eq. (7) with a step size\nst and inherent noise œÉ2.\n5: Update the transformer generator with model prediction\nfŒ∏(x,zi\nT) using the generator loss function in Eq. (5).\n6: Update the discriminator with loss function in Eq. (9).\n7: end for\nFollowing the previous variable deÔ¨Ånitions, given the training\nexample (x,y), we intend to infer z and learn the network\nparameters Œ∏ to minimize the reconstruction loss as well as a\nregularization term that corresponds to the prior on z. Our iGAN\nbased framework includes three main parts: a generator for task re-\nlated predictions, a discriminator to distinguish the prediction and\nground truth, and an inference model via Langevin dynamics [37]\nto infer the latent variable with gradient based MCMC. Different\nfrom the isotropic Gaussian distribution assumption for the latent\nvariable in GAN [36], or the possible posterior issue [107] within\nV AE [96], our latent variable z is sampled directly from its real\nposterior distribution via gradient based MCMC following [38].\nFurther, we introduce extra adversarial loss and the fully convo-\nlutional discriminator, serving as a higher-order loss function for\naccurate deterministic predictions. Empirically, we set st = 0.1\nand œÉ2 = 0.3 in Eq. (7) and Eq. (8). During training, we sample\nz0 from N(0,I), and update z via Eq. (7) by running T = 5\nsteps of Langevin sampling [37], and the Ô¨Ånal zT is then used\nto generate saliency prediction in our case. For testing, we can\nsample directly from the prior distribution N(0,I).\nNetwork Details: The proposed iGAN can be applied to any\ndeterministic saliency detection model, and we show the Ô¨Çowchart\nof the proposed iGAN for saliency detection in Fig. 3. SpeciÔ¨Åcally,\nwe Ô¨Årst extend the latent variable z to the same spatial size\nas the highest level backbone feature ( t4 in this paper). Then\nwe concatenate z with t4 channel-wise and feed it to a 3 √ó3\nconvolutional layer, which will serve as the new t4 for saliency\nprediction. The discriminator contains four 3 √ó3 convolutional\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nlayers following batch normalization and leakyReLU activation\nfunction with 64 channels, which takes the concatenation of image\nand model prediction (or ground truth) as input to estimate its\npixel-wise realness. In this way, the discriminator loss in Eq. (5)\ncan be rewritten as:\nLdis = Lce(gŒ≤([fŒ∏(x,z),x]),0) +Lce(gŒ≤([y,x]),1), (9)\nwhere [¬∑,¬∑] is the channel-wise concatenation operation. The train-\ning of the proposed iGAN is the same as the conventional GAN\nbased models in Eq. (5), except that we have an extra inference\nmodel via MCMC [37]. We show the learning pipeline of iGAN\nin Algorithm 1.\nInferential GAN analysis: Same as other generative models, iGAN\naims to produce reliable uncertainty maps while keeping the deter-\nministic performance unchanged. As the conventional GAN [36]\nhas no inference step, the latent variable is independent of the\ninput image x, leading to less informative uncertainty maps while\nsampling from the latent space at test time. Although V AE [96]\nand ABP [38] can produce input-dependent latent space modeling,\nthe possible posterior collapse [107] issue within the former and\nthe less accurate deterministic prediction of the latter limit their\napplications for SOD. With the proposed iGAN, we can achieve\ntwo main beneÔ¨Åts: 1) extra inference step is included without\nincreasing model parameters, leading to an input-dependent latent\nvariable; 2) with the adversarial loss function serving as a high-\norder similarity measure, iGAN can lead to more effective model\nlearning compared with ABP [38]. For the former, our iGAN is\nbuilt upon GAN [36] and ABP [38], and the fully convolutional\ndiscriminator introduces less than 1M extra parameters, which\nis comparable to both the alternative latent variable models and\nthe deterministic models. For the latter, the adversarial training\nis proven effective in maintaining the deterministic performance\ncompared with the alternative stochastic models.\n3.4 Objective Function\nFor RGB saliency detection, we remove the ‚Äúauxiliary depth‚Äù\nmodule from Fig. 3. The objective is shown in Eq. (5), where\nthe reconstruction loss Lrec is chosen as the structure-aware loss\nfrom [3], which is the sum of the weighted binary cross-entropy\nloss and the weighted IOU loss:\nLRGB\nrec = œâ(Lce(s,y) +Liou(s,y)), (10)\nwhere y is the ground truth saliency map, œâ is the edge-aware\nweight, and is deÔ¨Åned as œâ = 1 + 5‚àó|(ap(y) ‚àíy)|, with ap(.)\nrepresenting the average pooling operation.Lce is the binary cross-\nentropy loss. Liou is the weighted IOU loss [3].\nFor RGB-D saliency detection, Fig. 3 represents the entire\ntraining pipeline. As shown, we introduce an auxiliary depth\nmodule to model the joint distribution in Eq. (1). SpeciÔ¨Åcally, as\nan early fusion model, we concatenate the RGB image and depth\ndata at the input level, which is then fed to a 3 √ó3 convolutional\nlayer to produce a tensor with channel size 3, which is deÔ¨Åned\nas the fused input x‚Ä≤. The proposed iGAN framework for RGB-\nD SOD takes x‚Ä≤as input and produces both saliency map s and\ndepth prediction d‚Ä≤. The reconstruction loss for RGB-D saliency\ndetection is deÔ¨Åned as:\nLRGBD\nrec = œâ(Lce(s,y) +Liou(s,y)) +ŒªLdepth, (11)\nwhere Œªis used to balance the contribution of the auxiliary depth\nmodule, and empirically we set Œª = 1. The generator loss and\ndiscriminator loss are obtained following Eq. (5). Note that for\nboth RGB and RGB-D SOD models, the latent variable z is\nupdated via Langevin dynamics as shown in Eq. (7).\n4 E XPERIMENTS\nDataset: In this paper, we conduct research on salient object de-\ntection (SOD), including both RGB image-based SOD and RGB-\nD image pair-based SOD. For the former, we perform fully and\nweakly supervised saliency detection. Within the fully supervised\nlearning frameworks, we train the models by using the DUTS\ntraining dataset [7] D1 ={xi,yi}N\ni=1 of size N = 10,553, and\ntest on six other widely used datasets: the DUTS testing dataset,\nECSSD [131], DUT [132], HKU-IS [133], PASCAL-S [134] and\nthe SOD testing dataset [135]. For the weakly supervised models,\nwe use the DUTS-S training dataset [6], D2 = {xi,yi}N\ni=1 of\nsize N = 10,553, where yi is the scribble annotation. The testing\ndataset is the same as the fully supervised RGB SOD models.\nFor RGB-D SOD, we follow the conventional setting, where\nthe training set D3 = {xi,yi}N\ni=1 is a combination of 1,485\nimages from the NJU2K dataset [24] and 700 images from the\nNLPR dataset [21]. We then test the performance of our model\nand competing models on the NJU2K testing set, NLPR testing\nset, LFSD [26], DES [23], SSB [19] and SIP [28] dataset.\nEvaluation Metrics: For all three tasks, we use four evaluation\nmetrics to measure the performance, including Mean Absolute\nError M, Mean F-measure ( FŒ≤), Mean E-measure ( EŒæ) [139]\nand S-measure (SŒ±) [140].\nMAE Mis deÔ¨Åned as the pixel-wise difference between the\nprediction sand the ground truth y: M= 1\nH√óW|c‚àíy|, where\nH and W are the height and width of ccorrespondingly.\nF-measure FŒ≤ is a region-based similarity metric, and we pro-\nvide the mean F-measure using varying Ô¨Åxed (0-255) thresholds.\nE-measure EŒæ is the recently proposed Enhanced alignment\nmeasure [139] in the binary map evaluation Ô¨Åeld to jointly capture\nimage-level statistics and local pixel matching information.\nS-measure SŒ± is a structure based measure [140], which\ncombines the region-aware ( Sr) and object-aware ( So) structural\nsimilarity as their Ô¨Ånal structure metric: SŒ± = Œ±So+ (1‚àíŒ±)Sr,\nwhere Œ±‚àà[0,1] is set to 0.5 by default.\nCalibration measures: Due to the close correlation between un-\ncertainty estimation and model calibration [35], uncertainty is usu-\nally evaluated with modal calibration measures [141], i.e.expected\ncalibration error (ECE) [142]. The basic assumption is that a\nreliable uncertainty output should lead to a well-calibrated model,\nwhere model conÔ¨Ådence is consistent with model accuracy.\n4.1 Accurate and Reliable Fully-supervised Salient Ob-\nject Detection\n4.1.1 Performance Comparison with Benchmark Models\nWe compare the proposed framework with benchmark saliency\nmodels and show model performance in Tables 2, 3 and 4. Note\nthat, VST [94] and GTSOD [95] are two existing transformer\nbased saliency detection models.\nWe observe the competitive performance of our CNN based\ngenerative model (CIGAN) with existing techniques in Tables\n2 and 3. To focus on explaining the superior performance of\nthe transformer backbone for SOD, our decoder has only 1M\nparameters, which is around 5% of model parameters of existing\ntechniques. Further, we Ô¨Ånd a better performance of our generative\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nTABLE 2\nPerformance comparison with benchmark fully-supervised RGB SOD models.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì\nCIGAN .876 .820 .906 .042 .923 .913 .945 .037 .823 .733 .848 .061 .911 .892 .943 .034 .856 .836 .893 .068 .833 .816 .862 .075\nTIGAN .909 .873 .941 .028 .941 .936 .964 .025 .861 .796 .890 .047 .929 .918 .962 .025 .879 .869 .916 .054 .861 .854 .894 .060\nSCRN [2] .885 .833 .900 .040 .920 .910 .933 .041 .837 .749 .847 .056 .916 .894 .935 .034 .869 .833 .892 .063 .817 .790 .829 .087\nF3Net [3] .888 .852 .920 .035 .919 .921 .943 .036 .839 .766 .864 .053 .917 .910 .952 .028 .861 .835 .898 .062 .824 .814 .850 .077\nITSD [136] .886 .841 .917 .039 .920 .916 .943 .037 .842 .767 .867 .056 .921 .906 .950 .030 .860 .830 .894 .066 .836 .829 .867 .076\nPAKRN [12] .900 .876 .935 .033 .928 .930 .951 .032 .853 .796 .888 .050 .923 .919 .955 .028 .859 .856 .898 .068 .833 .836 .866 .074\nMSFNet [61] .877 .855 .927 .034 .915 .927 .951 .033 .832 .772 .873 .050 .909 .913 .957 .027 .849 .855 .900 .064 .813 .822 .852 .077\nCTDNet [137] .893 .862 .928 .034 .925 .928 .950 .032 .844 .779 .874 .052 .919 .915 .954 .028 .861 .856 .901 .064 .829 .832 .858 .074\nVST [94] .896 .842 .918 .037 .932 .911 .943 .034 .850 .771 .869 .058 .928 .903 .950 .030 .873 .832 .900 .067 .854 .833 .879 .065\nGTSOD [95] .908 .875 .942 .029 .935 .935 .962 .026 .858 .797 .892 .051 .930 .922 .964 .023 .877 .855 .915 .054 .860 .860 .898 .061\nTABLE 3\nPerformance comparison with benchmark fully-supervised RGB-D SOD models.\nNJU2K [24] SSB [19] DES [23] NLPR [21] LFSD [26] SIP [28]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì\nCIGAN .914 .900 .939 .036 .903 .876 .934 .040 .937 .921 .970 .018 .922 .890 .952 .025 .851 .832 .889 .075 .884 .870 .917 .049\nTIGAN .928 .919 .956 .028 .915 .893 .947 .034 .940 .929 .970 .016 .932 .911 .961 .020 .884 .868 .911 .057 .905 .901 .941 .037\nBBSNet [5] .921 .902 .938 .035 .908 .883 .928 .041 .933 .910 .949 .021 .930 .896 .950 .023 .864 .843 .883 .072 .879 .868 .906 .055\nBiaNet [76] .915 .903 .934 .039 .904 .879 .926 .043 .931 .910 .948 .021 .925 .894 .948 .024 .845 .834 .871 .085 .883 .873 .913 .052\nCoNet [74] .911 .903 .944 .036 .896 .877 .939 .040 .906 .880 .939 .026 .900 .859 .937 .030 .842 .834 .886 .077 .868 .855 .915 .054\nUCNet [4] .897 .886 .930 .043 .903 .884 .938 .039 .934 .919 .967 .019 .920 .891 .951 .025 .864 .855 .901 .066 .875 .867 .914 .051\nJLDCF [16] .902 .885 .935 .041 .903 .873 .936 .040 .931 .907 .959 .021 .925 .894 .955 .022 .862 .848 .894 .070 .880 .873 .918 .049\nVST [94] .922 .898 .939 .035 .913 .879 .937 .038 .943 .920 .965 .017 .932 .897 .951 .024 .882 .871 .917 .061 .904 .894 .933 .040\nGTSOD [95] .929 .924 .956 .028 .916 .898 .950 .032 .945 .928 .971 .016 .938 .921 .966 .018 .872 .862 .901 .066 .906 .908 .940 .037\nTABLE 4\nPerformance comparison with benchmark weakly-supervised RGB SOD models.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì\nCIGAN .834 .779 .887 .056 .896 .890 .938 .044 .799 .713 .838 .070 .886 .873 .938 .039 .827 .810 .880 .079 .800 .793 .855 .083\nTIGAN .855 .814 .918 .043 .905 .905 .950 .037 .826 .760 .874 .058 .893 .887 .949 .035 .844 .839 .902 .066 .811 .810 .872 .082\nSSAL [6] .803 .747 .865 .062 .863 .865 .908 .061 .785 .702 .835 .068 .865 .858 .923 .047 .798 .773 .854 .093 .750 .743 .801 .108\nWSS [7] .748 .633 .806 .100 .808 .774 .801 .106 .730 .590 .729 .110 .822 .773 .819 .079 .701 .691 .687 .187 .698 .635 .687 .152\nC2S [138] .805 .718 .845 .071 - - - - .773 .665 .810 .082 .869 .837 .910 .053 .784 .806 .813 .130 .770 .741 .799 .117\nSCWS [120] .841 .818 .901 .049 .879 .894 .924 .051 .813 .751 .856 .060 .883 .892 .938 .038 .821 .815 .877 .078 .782 .791 .833 .090\nmodel (TIGAN) compared with VST [94], indicating the supe-\nriority of the proposed model. Different from the deterministic\nVST [94], as a generative model, we aim to produce stochastic\npredictions leading to reliable saliency prediction. In this way,\nwe compare with GTSOD [95], another generative transformer\nSOD model, in the way of both accurate and reliable saliency\nprediction. Tables 2 and 3 show that the proposed iGAN achieves\ncomparable performance compared with GTSOD [95], leading\nto an alternative generative saliency transformer. In Fig. 4, we\nfurther visualize the produced uncertainty maps of GTSOD [95]\nand ours for RGB SOD. The more reliable uncertainty maps,\nhighlighting the less conÔ¨Ådent or hard regions, further explain\nour superiority. Besides the visual comparison, we also compute\ncalibration measures of GTSOD [95] and ours and show the results\nin Table 6, which clearly shows the advantages of our model in\nachieving better calibrated models compared with GTSOD [95].\n4.1.2 Accurate Saliency Model\nIn Sec. 1, we discuss that the CNN backbone is not effective in\ndetecting salient objects that rely on global context and the stride\nand pooling operation lead to less accurate structure information\nof CNN backbone features. We then compare the performance\nof CNN backbone (B cnn with ResNet50 [18] backbone) and\ntransformer backbone model (B tr with Swin transformer back-\nbone [34]) for RGB image based SOD, and show performance in\nTable 5, where the models share the same decoder 4. Note that,\nfor both B cnn and B tr, we use binary cross-entropy loss for\nthe saliency generator. To further explain how the two types of\nbackbone based models perform with structure-aware loss [3] in\nEq. (10), we train B cnn and B tr with a structure-aware loss\ninstead and obtain model B‚Äô cnn and B‚Äô tr respectively. We also\nvisualize predictions from the two different backbone networks\nin Fig. 5. The signiÔ¨Åcant performance gap between the two\ndifferent backbones (B cnn and B tr) indicates the superiority of\nthe transformer backbone for SOD. Further, we observe, although\nthe transformer has encoded accurate structure information, the\nstructure-aware loss function (B‚Äô tr) that penalizes the wrong\nprediction along object boundaries can lead to more accurate\npredictions (see Table 5 and Fig. 5).\nTransformer backbone for large salient object detection:\nAs discussed, the larger receptive Ô¨Åeld of transformer backbone\nmakes it ideal for the context based task, i.e. salient object detec-\ntion. We then visualize the performance (we use mean absolute\nerror (MAE) here as it‚Äôs easy to implement and share the similar\nperformance comparison trend with other measures) of CNN\n4. We adjust the decoder accordingly to the backbone features.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 5\nBaseline model performance, where B cnn and B tr are models with CNN and transformer backbones respectively using a binary cross-entropy\nloss function, B‚Äô cnn and B‚Äô tr are the corresponding models with the structure-aware loss function in Eq. (10).\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚Üì\nB cnn .878 .818 .895 .042 .922 .907 .937 .039 .822 .724 .834 .061 .912 .886 .933 .035 .862 .838 .891 .067 .831 .808 .846 .079\nB‚Äô cnn .882 .840 .916 .037 .922 .919 .947 .035 .823 .742 .851 .057 .912 .901 .947 .030 .855 .841 .896 .065 .832 .825 .863 .073\nB tr .907 .863 .930 .031 .939 .929 .957 .028 .858 .786 .878 .051 .929 .912 .954 .027 .881 .866 .911 .056 .854 .841 .882 .065\nB‚Äô tr .911 .882 .947 .026 .939 .940 .965 .024 .860 .801 .894 .045 .927 .921 .964 .023 .876 .872 .917 .053 .858 .853 .897 .059\nImage GT GTSOD [95] TIGAN\nFig. 4. Performance comparison with existing generative SOD model\nGTSOD [95], where prediction in each block is the model prediction and\nthe corresponding uncertainty map.\nImage GT B cnn B‚Äô cnn B tr B‚Äô tr\nFig. 5. Predictions of CNN and transformer backbone models without\n(B cnn and B tr) and with (B‚Äô cnn and B‚Äô tr) structure-aware loss.\nTABLE 6\nCalibration degree of conventional uncertainty estimation techniques\nand the proposed inferential GAN with transformer backbone.\nDUTS ECSSD DUT HKU-IS PASCAL-S SOD\n[7] [131] [132] [133] [134] [135]\nGTSOD [95] .0388 .0214 .0461 .0231 .0476 .0552\nMCD .0391 .0230 .0468 .0234 .0498 .0581\nTGAN .0382 .0227 .0464 .0227 .0488 .0570\nTCV AE .0377 .0220 .0441 .0229 .0497 .0560\nTABP .0365 .0204 .0430 .0211 .0487 .0545\nTIGAN .0353 .0198 .0400 .0198 .0467 .0519\nbackbone (B‚Äô cnn) and transformer backbone (B‚Äô tr) w.r.t. size\nof the salient foreground in Fig. 6. SpeciÔ¨Åcally, we uniformly\ngroup the scale of salient foreground to 10 bins and compute\nthe mean performance of each backbone based model. Fig. 6\nshows that the transformer backbone based model outperforms the\nCNN backbone based model almost consistently across all scales.\nSpeciÔ¨Åcally, the performance gap for the largest foreground scale\n(on the DUT [132] and SOD datasets [135]) is the most signiÔ¨Åcant\ncompared with other scales, which explains the superiority of\nthe transformer for large salient object detection. There also\nexist scales for the HKU-IS [133] and PASCAL-S [134] datasets\nwhen the transformer backbone based model fails to outperform\nthe CNN backbone based model, which are mainly due to the\n‚Äúdouble-edged sword‚Äù effect of the transformer‚Äôs larger receptive\nÔ¨Åeld, which will be explained in Sec. 4.3. Images in ECSSD\ndataset [131] are relatively simpler compared with other datasets,\n2 4 6 8\nScale\n0.1\n0.2MAE\nDUTS\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n1000\n2000Number\n2 4 6 8\nScale\n0.00\n0.25\n0.50MAE\nECSSD\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n200Number\n2 4 6 8\nScale\n0.1\n0.2MAE\nDUT\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n1000\n2000Number\n2 4 6\nScale\n0.02\n0.04\n0.06MAE\nHKU-IS\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n1000Number\n2 4 6 8\nScale\n0.1\n0.2MAE\nPASCAL-S\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n100Number\n2 4 6 8\nScale\n0.1\n0.2MAE\nSOD\nTransformer\nCNN\n0.0 2.5 5.0 7.5 10.0\nScale\n0\n50Number\nFig. 6. Model (B‚Äô cnn and B‚Äô tr in Table 5) performance (the top curve\nof each block) w.r.t. salient foreground size distribution (the bottom bar\nof each block) on six testing datasets.\nand the foreground objects are distributed compactly around the\nimage center, leading to less signiÔ¨Åcant performance gain with the\ntransformer backbone.\nAuxiliary depth module for depth domain gap modeling: We\ndiscuss this in Sec. 1 that the inconsistent depth distribution (see\nTable 1 and Fig. 2) leads to a domain gap between the training\ndatasets and testing datasets for RGB-D salient object detection.\nTo solve the issue, we present an auxiliary depth module to fully\nexplore the depth contribution and Ô¨Åx the depth domain gap\nissue via self-supervised learning. To evaluate the effectiveness\nof the proposed auxiliary depth module, we design three SOD\nmodels, namely the pure RGB image based model (‚ÄúCB RGB‚Äù\nand ‚ÄúTB RGB‚Äù), early fusion model (‚ÄúCEarly‚Äù and ‚ÄúTEarly‚Äù)\nand early fusion model with auxiliary depth module (‚ÄúCADE‚Äù\nand ‚ÄúTADE‚Äù), where ‚ÄúC*‚Äù represents CNN backbone (ResNet50\n[18]), and ‚ÄúT*‚Äù is the transformer backbone. The performance of\neach model is shown in Table 7.\nFirstly, we observe the improved performance of early fusion\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nTABLE 7\nDepth contribution analysis for RGB-D SOD. Given the RGB image based model (‚Äú* RGB‚Äù), we Ô¨Årst adapt it for RGB-D saliency detection with\nearly-fusion (‚Äú*Early‚Äù). Then, the auxiliary depth module is attached to ‚Äú*Early‚Äù to analyze the depth contribution.\nNJU2K [24] SSB [19] DES [23] NLPR [21] LFSD [26] SIP [28]\nMethod SŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚Üì\nCB RGB .906 .891 .936 .038 .903 .882 .933 .039 .908 .890 .936 .025 .916 .891 .948 .025 .802 .777 .833 .105 .874 .862 .913 .052\nCEarly .912 .901 .941 .036 .903 .881 .934 .038 .932 .921 .964 .018 .914 .886 .945 .026 .830 .804 .858 .082 .878 .864 .914 .050\nCADE .911 .902 .939 .036 .902 .877 .936 .039 .935 .922 .968 .018 .922 .896 .951 .025 .860 .848 .899 .069 .888 .880 .925 .045\nTB RGB .924 .919 .955 .029 .922 .906 .952 .030 .918 .908 .943 .022 .931 .914 .962 .021 .869 .856 .899 .067 .895 .898 .935 .042\nTEarly .925 .917 .955 .028 .911 .890 .948 .033 .938 .926 .974 .016 .935 .916 .865 .019 .875 .862 .903 .060 .897 .895 .938 .039\nTADE .925 .917 .953 .029 .911 .890 .946 .034 .944 .930 .977 .015 .934 .913 .965 .018 .879 .869 .910 .056 .902 .895 .939 .038\nTABLE 8\nReliable fully-supervised RGB SOD models, where we present performance of stochastic saliency prediction models via GAN, CVAE, ABP as well\nas the proposed iGAN. Performance of the baseline deterministic models (B‚Äô cnn and B‚Äô tr in Table 5) are listed for easier reference.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚Üì\nCGAN .881 .839 .917 .036 .919 .916 .945 .036 .818 .734 .845 .056 .909 .898 .945 .031 .857 .845 .899 .064 .818 .807 .846 .078\nCCV AE .877 .833 .911 .040 .922 .920 .949 .034 .817 .735 .845 .063 .910 .900 .947 .031 .855 .842 .897 .066 .830 .822 .866 .073\nCABP .828 .757 .859 .058 .887 .877 .913 .055 .778 .670 .801 .078 .878 .855 .913 .047 .810 .782 .845 .094 .773 .744 .799 .102\nCIGAN .876 .820 .906 .042 .923 .913 .945 .037 .823 .733 .848 .061 .911 .892 .943 .034 .856 .836 .893 .068 .833 .816 .862 .075\nB‚Äô cnn .882 .840 .916 .037 .922 .919 .947 .035 .823 .742 .851 .057 .912 .901 .947 .030 .855 .841 .896 .065 .832 .825 .863 .073\nTGAN .907 .877 .944 .029 .939 .938 .964 .025 .852 .789 .882 .051 .927 .920 .963 .024 .878 .872 .918 .053 .855 .849 .894 .061\nTCV AE .908 .879 .945 .028 .940 .940 .966 .024 .857 .796 .890 .048 .927 .922 .964 .024 .876 .871 .918 .054 .858 .854 .898 .060\nTABP .910 .878 .944 .028 .942 .940 .966 .024 .860 .799 .891 .048 .929 .922 .964 .024 .879 .870 .918 .054 .860 .858 .897 .061\nTIGAN .909 .873 .941 .028 .941 .936 .964 .025 .861 .796 .890 .047 .929 .918 .962 .025 .879 .869 .916 .054 .861 .854 .894 .060\nB‚Äô tr .911 .882 .947 .026 .939 .940 .965 .024 .860 .801 .894 .045 .927 .921 .964 .023 .876 .872 .917 .053 .858 .853 .897 .059\nTABLE 9\nReliable fully-supervised RGB-D SOD models, where we present performance of stochastic saliency prediction models via GAN, CVAE, ABP as\nwell as the proposed iGAN. Performance of the baseline models (CADE and TADE in Table 7) are listed for easier reference.\nNJU2K [24] SSB [19] DES [23] NLPR [21] LFSD [26] SIP [28]\nMethod SŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚Üì\nCGAN .914 .905 .943 .035 .904 .881 .937 .039 .929 .917 .957 .019 .924 .899 .954 .023 .849 .826 .884 .074 .885 .875 .921 .047\nCCV AE .906 .894 .937 .039 .896 .871 .934 .041 .940 .923 .975 .017 .916 .891 .951 .026 .841 .825 .881 .075 .887 .878 .927 .045\nCABP .916 .903 .941 .034 .905 .878 .935 .039 .941 .928 .972 .017 .921 .891 .949 .025 .845 .828 .876 .077 .888 .876 .922 .046\nCIGAN .914 .900 .939 .036 .903 .876 .934 .040 .937 .921 .970 .018 .922 .890 .952 .025 .851 .832 .889 .075 .884 .870 .917 .049\nCADE .911 .902 .939 .036 .902 .877 .936 .039 .935 .922 .968 .018 .922 .896 .951 .025 .860 .848 .899 .069 .888 .880 .925 .045\nTGAN .928 .921 .956 .027 .911 .890 .946 .034 .941 .931 .975 .015 .934 .915 .964 .019 .875 .860 .903 .060 .900 .901 .939 .038\nTCV AE .928 .922 .956 .028 .911 .889 .944 .035 .941 .929 .973 .016 .935 .915 .964 .020 .879 .863 .905 .060 .903 .904 .941 .038\nTABP .927 .917 .954 .029 .913 .891 .947 .034 .943 .929 .974 .015 .933 .911 .962 .020 .870 .852 .900 .065 .904 .900 .942 .037\nTIGAN .928 .919 .956 .028 .915 .893 .947 .034 .940 .929 .970 .016 .932 .911 .961 .020 .884 .868 .911 .057 .905 .901 .941 .037\nTADE .925 .917 .953 .029 .911 .890 .946 .034 .944 .930 .977 .015 .934 .913 .965 .018 .879 .869 .910 .056 .902 .895 .939 .038\nmodels compared with training only with RGB images, indicating\nthe beneÔ¨Åts of depth for SOD. Further, as shown in Fig. 2,\nsalient foreground depth contrast of the LFSD dataset [26] is most\ndifferent from the corresponding RGB image salient foreground\ncontrast. In this way, we claim that the auxiliary depth module\nshould contribute the most, which is consistent with our exper-\niments. Note that, the proposed auxiliary depth module aims to\nfurther explore the depth contribution, especially for depth data\nthat distributes differently from the training dataset.\n4.1.3 Reliable Saliency Model\nModel reliability is an important factor for measuring account-\nability for decisions before deployment, and a reliable model\nshould be aware of its predictions. In this paper, we introduce\nthe iGAN for reliable saliency detection with an image condi-\ntioned latent prior. In addition to the proposed iGAN, we also\ndesign GAN-based [36], CV AE-based [96], [98] and ABP-based\n[38] generative models for RGB SOD and RGB-D SOD. The\nperformance is shown in Table 8 and Table 9 respectively, where\n‚ÄúCGAN‚Äù, ‚ÄúCCV AE‚Äù, ‚ÄúCABP‚Äù and ‚ÄúCIGAN‚Äù are the stochastic\nmodels based on GAN, CV AE, ABP, and the proposed inferential\nGAN respectively with CNN backbone, and ‚ÄúTGAN‚Äù, ‚ÄúTCV AE‚Äù,\n‚ÄúTABP‚Äù and ‚ÄúTIGAN‚Äù are the transformer counterparts.\nFor easier reference, we also include the baseline models\nB‚Äô cnn and B‚Äô tr from Table 5 in Table 8, and the CNN and trans-\nformer backbone based stochastic models are built upon the two\nbaseline models respectively. The stochastic RGB-D SOD models\nin Table 9 are based on the corresponding RGB-D SOD models\nwith auxiliary depth module as shown in Table 7. We show the\nperformance of ‚ÄúCADE‚Äù and ‚ÄúTADE‚Äù for easier reference. Table\n8 and Table 9 show that the four types of generative models can\nachieve comparable deterministic performance (compared with the\ncorresponding deterministic baseline models) for both RGB image\nbased SOD and RGB-D image pair based SOD. As the goal\nof a generative model is to obtain stochastic predictions for the\nmodel explanation, the deterministic performance may be slightly\nThe proposed inferential GAN, e.g. ‚ÄúCABP‚Äù for RGB SOD. The\nmain reason lies in two parts. First, the hyper-parameters within\nthe inference model in Eq. (7) need to be tuned to effectively\nexplore the latent space. Second, the Ô¨Ånal performances of those\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nImage GT TGAN TCV AE TABP TIGAN\nFig. 7. Predictions from different generative models in Table 8, where we randomly sampleT = 10times and obtain the entropy of mean prediction\nas predictive uncertainty [39]. Note that the predictions within each method block are saliency prediction and uncertainty respectively.\nTABLE 10\nWeakly-supervised SOD models analysis with both the CNN and transformer backbone, where models in the top block are the CNN based models\nwith different weakly supervised loss functions, and models in the bottom block are the corresponding transformer based counterparts.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì\nBCNN .754 .641 .806 .088 .822 .771 .868 .087 .723 .588 .758 .107 .807 .739 .864 .080 .760 .703 .812 .117 .745 .692 .801 .117\nBCNN S .805 .724 .858 .069 .869 .846 .913 .060 .771 .664 .806 .088 .859 .822 .913 .055 .802 .768 .855 .095 .780 .750 .841 .098\nBCNN G .835 .781 .884 .053 .895 .889 .934 .046 .796 .708 .831 .068 .883 .871 .932 .041 .825 .807 .874 .080 .797 .787 .843 .870\nBCNN self .787 .682 .834 .077 .851 .803 .888 .075 .761 .634 .793 .089 .833 .767 .880 .072 .796 .743 .839 .100 .766 .716 .811 .111\nBCNN weak .839 .792 .892 .050 .895 .892 .934 .044 .800 .719 .835 .063 .886 .877 .935 .039 .829 .813 .880 .078 .788 .778 .834 .088\nBT .748 .636 .798 .081 .834 .786 .885 .071 .728 .600 .765 .099 .802 .734 .861 .077 .777 .727 .830 .100 .756 .710 .820 .108\nBT S .829 .767 .893 .053 .886 .878 .938 .045 .807 .726 .854 .069 .869 .848 .932 .046 .825 .810 .884 .078 .802 .791 .869 .085\nBT G .857 .808 .916 .043 .908 .903 .951 .036 .825 .751 .870 .059 .896 .883 .949 .035 .843 .831 .898 .069 .819 .813 .877 .077\nBT self .791 .683 .840 .070 .854 .799 .893 .069 .773 .649 .809 .083 .832 .763 .882 .070 .802 .748 .846 .094 .781 .732 .836 .101\nBT weak .858 .815 .917 .042 .908 .907 .952 .035 .835 .770 .879 .054 .898 .891 .952 .034 .848 .843 .904 .065 .821 .819 .880 .075\ngenerative models are obtained by performing multiple iterations\n(10 iterations in this paper) of forward passes during testing, and\nthe performance of the mean prediction is then reported, which\nvaries with different iterations of sampling. In this paper, we\nfocus on the new generative model for accurate and reliable SOD,\nwe leave the generative model hyper-parameter tuning for future\nwork.\nBesides the deterministic performance, the main advantage\nof the generative model is its ability for stochastic predictions,\nmaking it possible to estimate predictive uncertainty [39] for\nmodel reliability estimation. In Fig. 7, we visualize the uncertainty\nmaps of each generative model. In this paper, the ‚Äúuncertainty‚Äù\nrefers to predictive uncertainty [39], [143], which is the total\nuncertainty, including both data uncertainty and model uncertainty.\nGiven the mean predictions after multiple forward passes during\ntesting, the predictive uncertainty is deÔ¨Åned as the entropy of\nthe mean prediction. A reliable model should be aware of its\nprediction, leading to a reasonable uncertainty model to explain\nmodel prediction. Fig. 7 shows that the uncertainty map from the\nproposed inferential GAN explains better model prediction, high-\nlighting the hard samples caused by training/testing discrepancy.\nEspecially, for the 1st sample the bottom region is relatively low-\ncontrast, which is different from the high-contrast foreground in\nthe training dataset. The ground truth of the 2nd image is biased,\nfocusing only on a compact foreground region, where the less\ncompact region is discarded. All four latent variable models can\ndiscover the discarded less compact region, where the uncertainty\nmap of ‚ÄúTIGAN‚Äù is more informative in explaining the less\naccurate predictions (see Table 6 for extensive comparison).\n4.2 Accurate and Reliable Weakly-supervised Salient\nObject Detection\nDifferent from pixel-wise annotation based fully supervised SOD,\nweakly supervised SOD models learn saliency from cheap annota-\ntions, e.g., scribble annotations [6], image-level labels [7]. In this\npaper, we investigate the superiority of the transformer backbone\nfor weakly supervised SOD with scribble supervision [6].\n4.2.1 Weakly-supervised Transformer\nThe main difÔ¨Åculty of learning from weak annotations is the\nmissing structure information, which cannot be recovered without\nextra structure-aware regularizers. In this way, the main focus of\ndesigning models to learn from weak annotations is to recover the\nmissing structure information. SpeciÔ¨Åcally, we investigate three\nwidely used strategies, namely smoothness loss [144], gated CRF\nloss [123], and data-augmentation based consistency loss. The Ô¨Årst\nand second ones aim to recover the structure of prediction, and\nthe third one aims to achieve transformation robust prediction,\nserving as an internal data augmentation trick. To test how the\nmodel performs with different loss functions within both the CNN\nframework and the transformer framework, we design models with\neach type of loss function within both the CNN backbone and\ntransformer backbone and report the performance in Table 10.\nImplementation details: To train the weakly-supervised trans-\nformer for SOD, similar to [6], we adopt extra smoothness\nloss [144] Lsm, the gated CRF loss [123] Lgcrf and the data-\naugmentation based self-supervised learning strategy [120] Lss\nto recover the missing structure information in scribble anno-\ntation. The smoothness loss aims to produce a saliency map\nwith edges well-aligned with the input image. The gated CRF\nloss introduces pairwise constraints to produce a saliency map\nwith spatial consistency. The self-supervised learning strategy\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nImage GT CNN Transformer\nFig. 8. Features of CNN (ResNet50 [18]) and transformer backbone (Swin [34]) for weakly-supervised SOD using only partial cross-entropy loss.\nImage Full Weak BCNN BT BT S BT G BT self BT weak\nFig. 9. Weakly supervised saliency model predictions with different loss functions. Detailed performance of each model is illustrated in Table 10.\naims to achieve transformation robust predictions. For the Swin\ntransformer backbone [34], as it can only take Ô¨Åxed-size input, we\nperform image rotation instead of image scaling to achieve data\naugmentation. SpeciÔ¨Åcally, we deÔ¨Åne the self-supervised loss as a\nweighted sum of the structural similarity index measure [129] and\nL1 loss, which is deÔ¨Åned as:\nLss = Œ±‚àóSSIM(s,st) + (1‚àíŒ±) ‚àóL1(s,st), (12)\nwhere s = fŒ∏(T(x)) is the output of the generator with the\nrotated image5 T(x) as input, and st = T(fŒ∏(x)) is the rotated\nprediction with original image x as input. We randomly pick\nthe rotation T(.) from {œÄ,1/2œÄ,‚àí1/2œÄ}in our experiments. Œ±\nis used to balance the two types of loss functions and we set\nŒ± = 0.85 in our experiments following [120]. Further, given the\nscribble annotation, we adopt the partial cross-entropy loss Lpce to\nconstrain predictions on the scribble region. In this way, we deÔ¨Åne\nthe loss function for the weakly-supervised model as:\nLweak = Lpce + Œª1 ‚àóLsm + Œª2 ‚àóLgcrf + Œª3 ‚àóLss. (13)\nWith grid search, we set Œª1 = 0.3, Œª2 = 1.0 and Œª3 = 1.2.\nCNN backbone vs Transformer backbone: We design two\ndifferent models with ResNet50 [18] and Swin transformer [34] as\nthe backbone. The decoder part is the same as B‚Äô tr in Table 5. We\ntrain the two models with only partial cross-entropy loss, and we\nshow their corresponding results in Table 10 ‚ÄúBCNN‚Äù and ‚ÄúBT‚Äù\nrespectively. The signiÔ¨Åcantly improved performance of ‚ÄúBT‚Äù\ncompared with ‚ÄúBCNN‚Äù shows the effectiveness of the trans-\nformer backbone for weakly-supervised SOD. We also visualize\nthe features of the two trained models in Fig. 8, where heat maps\nare the features and gray maps are the predictions. We observe\nclear structure information in ‚ÄúTransformer‚Äù, which explains the\nsuperior performance of the transformer for weakly-supervised\nlearning via supervisions with less structure information [145].\nWeakly-supervised loss analysis: Besides the partial cross-\nentropy loss, we use three extra loss functions for weakly-\nsupervised SOD, namely smoothness loss to constrain the pre-\ndictions to be well aligned with the image edges, gated CRF\nloss to regularize the pairwise term predictions which aim to\nproduce similar predictions for spatially similar pixels, and a self-\nsupervised loss to effectively learn from less supervision data with\n5. In this paper, we perform image rotation instead of image scaling due to\nthe Ô¨Åxed input size of Swin transformer [34].\nconsistency loss, e.g., rotation-invariant predictions. We then carry\nout extra experiments to verify the effectiveness of each loss func-\ntion and show the results in Table 10. ‚ÄúBCNN S‚Äù, ‚ÄúBCNN G‚Äù\nand ‚ÄúBCNN self‚Äù indicate baseline model (‚ÄúBCNN‚Äù) training\nwith extra smoothness loss ‚ÄúLsm‚Äù, gated CRF loss ‚ÄúLgcrf‚Äù and self-\nsupervised loss ‚Äú Lss‚Äù. ‚ÄúBT S‚Äù, ‚ÄúBT G‚Äù and ‚ÄúBT self‚Äù are the\ncorresponding transformer backbone counterparts. We observe an\nimproved performance of each extra loss function, which explains\ntheir effectiveness. Further, we Ô¨Ånd that the smoothness and gated\nCRF achieve more performance gain than the self-supervised\nloss, which mainly comes from their effective structure modeling\nability. The improved performance of BCNN weak and BT weak\nwith the weighted loss function in Eq. (13) compared with the\ncorresponding models with individual loss functions veriÔ¨Åes the\neffectiveness of the proposed weighted weakly supervised loss\nfunction. We also show predictions of the weakly supervised\nmodels in Fig. 9. It is clear that both the base model with only\npartial cross-entropy loss (‚ÄúBCNN‚Äù and ‚ÄúBT‚Äù) and the model with\nextra self-supervised loss (‚ÄúBT self‚Äù) fail to accurately localize\nobject boundaries, leading to blurred predictions. The main reason\nis the absence of structure constraints. The smoothness loss and the\ngated CRF loss work better in modeling the structure information,\nleading to more accurate predictions, especially along object\nboundaries, and models (‚ÄúBCNN weak‚Äù and ‚ÄúBT weak‚Äù) with\nour Ô¨Ånal loss function (Eq. (13)) achieve the best performance.\n4.2.2 Reliable Weakly-supervised Transformer\nThe generative models for the fully-supervised setting are straight-\nforward, as we can simply take the inferred latent variable as\npart of the input, and the discriminator can directly estimate\nthe real/fake of its input (the ground truth/model prediction). In\nthis section, we apply our proposed inferential GAN (iGAN) to\nweakly-supervised RGB SOD. We also design the GAN [36],\nCV AE [96], [98], and ABP [38] based generative models for\ncomparison. Similarly, we design the generative model within\nboth the CNN backbone and the transformer backbone. The\nperformance is shown in Table 11, where ‚ÄúCGAN‚Äù, ‚ÄúCCV AE‚Äù,\n‚ÄúCABP‚Äù and ‚ÄúCIGAN‚Äù are the stochastic models based on GAN,\nCV AE, ABP, and the proposed inferential GAN respectively with\nCNN backbone, and ‚ÄúTGAN‚Äù, ‚ÄúTCV AE‚Äù, ‚ÄúTABP‚Äù and ‚ÄúTIGAN‚Äù\nare the corresponding transformer counterparts. Same as the fully-\nsupervised reliable models in Table 8 and Table 9, the prior\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nTABLE 11\nReliable weakly-supervised RGB SOD models, where the deterministic models (‚Äú* weak‚Äù) is trained loss function in Eq.(13).\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚ÜëM‚Üì\nCGAN .834 .785 .887 .053 .891 .888 .932 .046 .792 .706 .826 .069 .881 .873 .933 .041 .828 .814 .880 .079 .798 .797 .852 .082\nCCV AE .832 .781 .882 .055 .894 .889 .934 .045 .793 .708 .827 .070 .883 .872 .933 .041 .822 .806 .876 .081 .797 .792 .851 .085\nCABP .838 .790 .893 .051 .894 .890 .935 .044 .801 .718 .838 .064 .887 .878 .937 .039 .828 .813 .882 .078 .794 .791 .848 .084\nCIGAN .834 .779 .887 .056 .896 .890 .938 .044 .799 .713 .838 .070 .886 .873 .938 .039 .827 .810 .880 .079 .800 .793 .855 .083\nBCNN weak .839 .792 .892 .050 .895 .892 .934 .044 .800 .719 .835 .063 .886 .877 .935 .039 .829 .813 .880 .078 .788 .778 .834 .088\nTGAN .856 .813 .918 .043 .906 .905 .950 .037 .824 .753 .868 .060 .895 .886 .949 .035 .848 .840 .905 .065 .819 .816 .878 .076\nTCV AE .855 .813 .916 .043 .907 .906 .950 .036 .825 .757 .872 .059 .894 .887 .949 .035 .843 .837 .900 .067 .814 .813 .873 .079\nTABP .854 .812 .917 .043 .905 .905 .951 .036 .827 .759 .875 .058 .893 .887 .950 .035 .847 .844 .906 .064 .810 .810 .868 .082\nTIGAN .855 .814 .918 .043 .905 .905 .950 .037 .826 .760 .874 .058 .893 .887 .949 .035 .844 .839 .902 .066 .811 .810 .872 .082\nBT weak .858 .815 .917 .042 .908 .907 .952 .035 .835 .770 .879 .054 .898 .891 .952 .034 .848 .843 .904 .065 .821 .819 .880 .075\nTABLE 12\nModel analysis related experiments, where we discuss model performance with respect to model optimizer (‚ÄúB‚ÄôSGD‚Äù and ‚ÄúB‚ÄôtrSGD‚Äù),\ninitialization weights (‚ÄúB‚ÄôR‚Äù, B‚ÄôtrR and B‚Äô tr22K) and different transformer backbones (‚ÄúB‚ÄôViT‚Äù).\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚Üì\nB‚Äô cnn .882 .840 .916 .037 .922 .919 .947 .035 .823 .742 .851 .057 .912 .901 .947 .030 .855 .841 .896 .065 .832 .825 .863 .073\nB‚Äô SGD .876 .826 .910 .041 .918 .910 .944 .038 .820 .733 .846 .062 .909 .894 .945 .033 .856 .840 .895 .067 .827 .813 .863 .077\nB‚Äô R .745 .623 .773 .110 .832 .789 .853 .093 .738 .605 .762 .114 .820 .760 .858 .084 .752 .697 .777 .140 .725 .673 .758 .145\nB‚Äô tr .911 .882 .947 .026 .939 .940 .965 .024 .860 .801 .894 .045 .927 .921 .964 .023 .876 .872 .917 .053 .858 .853 .897 .059\nB‚Äô trSGD .899 .861 .936 .031 .928 .923 .954 .032 .854 .786 .886 .046 .921 .909 .956 .029 .867 .856 .905 .061 .833 .818 .862 .075\nB‚Äô trR .768 .667 .804 .097 .848 .819 .874 .082 .754 .637 .784 .105 .843 .803 .884 .070 .760 .715 .794 .134 .730 .689 .763 .142\nB‚Äô tr22K .918 .891 .952 .025 .944 .943 .967 .022 .869 .814 .902 .044 .933 .928 .968 .022 .885 .881 .925 .050 .863 .863 .900 .059\nB‚Äô ViT .922 .899 .955 .023 .943 .945 .967 .022 .874 .824 .906 .043 .934 .931 .969 .021 .884 .884 .926 .050 .858 .859 .895 .065\nTABLE 13\nReplacing the CNN backbone of existing RGB SOD models with a transformer backbone.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚ÜìSŒ± ‚ÜëFŒ≤ ‚ÜëEŒæ ‚Üë M‚Üì\nSCRN [2] .885 .833 .900 .040 .920 .910 .933 .041 .837 .749 .847 .056 .916 .894 .935 .034 .869 .833 .892 .063 .817 .790 .829 .087\nF3Net [3] .888 .852 .920 .035 .919 .921 .943 .036 .839 .766 .864 .053 .917 .910 .952 .028 .861 .835 .898 .062 .824 .814 .850 .077\nSCRN* [2] .908 .873 .937 .029 .937 .935 .960 .027 .864 .800 .890 .044 .930 .919 .958 .026 .875 .870 .911 .057 .843 .836 .865 .069\nF3Net* [3] .913 .891 .950 .024 .939 .941 .965 .023 .860 .802 .891 .042 .928 .925 .964 .023 .872 .872 .916 .055 .848 .849 .881 .065\nB‚Äô tr .911 .882 .947 .026 .939 .940 .965 .024 .860 .801 .894 .045 .927 .921 .964 .023 .876 .872 .917 .053 .858 .853 .897 .059\nand posterior distribution models of the CV AE based models are\ndesigned following [4].\nImplementation details: For the learning process of the reliable\nweakly-supervised transformer, we apply the same pipeline of\nthe fully supervised method. Considering there is only scribble\nground-truth as supervision in training the discriminator, we use\npartial cross-entropy loss for training the discriminator within the\nGAN and our proposed iGAN based models, and the adversarial\nloss Ladv is also partial cross-entropy loss.\nPerformance analysis: For easier reference, we also list the base-\nline models ‚ÄúBCNN weak‚Äù and ‚ÄúBT weak‚Äù from Table 10 and\nTable 11, and the CNN and transformer backbone based stochastic\nmodels are built upon the two baseline models respectively. Same\nas the results of fully supervised SOD models in Table 8, the three\ntypes of generative models for weakly-supervised SOD can also\nachieve comparable deterministic performance.\n4.3 Discussions\nWe further analyze our transformer backbone-based models in\ndetails. Unless otherwise stated, the experiments are based on the\nfully supervised deterministic RGB SOD (‚ÄúB‚Äô tr‚Äù in Table 5).\nModel performance w.r.t. optimizer: We observe that the\nAdamW optimizer is more suitable to train the transformer back-\nbone (Swin [34] in particular) based framework compared with\nSGD. To explain this, we train B‚Äô cnn and B‚Äô tr with SGD as\noptimizer, leading to B‚Äô SGD and B‚Äô trSGD respectively in Table\n12. We observe that for both the CNN and transformer backbone\nbased networks, the SGD optimizer usually achieves worse per-\nformance compared with the AdamW optimizer. Note that models\nwith the two types of optimizers share the same initial learning\nrate, and ‚ÄúSGD‚Äù in this paper is SGD with a momentum of 0.9.\nWe Ô¨Ånd that after the Ô¨Årst epoch, the AdamW optimizer based\nmodel jumps directly to a minimum of smaller loss compared\nwith SGD, and later, the loss decrease behaviors of both models\nare similar. We also tried different learning rate conÔ¨Ågurations for\nmodels with the two types of the optimizers, and the performance\nof SGD based model is still bad. We further explore whether the\nAdamW converges faster than SGD. However, even when we train\nmore epochs for SGD based model, the conclusion is still similar.\nWe will investigate it further to extensively explain the different\nmodel behaviors with various types of optimizers.\nThe importance of initialization weights: For both CNN\nand transformer backbone, we initialize them with the image\nclassiÔ¨Åcation model trained on the ImageNet-1K [146] dataset.\nTo test how the initialization weights contribute to the model\nperformance, we randomly initialize the two models (B‚Äô cnn\nand B‚Äô tr in Table 5) and obtain model performance as B‚Äô R\nand B‚Äô trR in Table 12. We observe the worse performance of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\nboth B‚Äô R and B‚Äô trR, which further illustrates the necessity\nof Ô¨Åne-tuning the backbone models for SOD. We also initialize\nour transformer backbone with parameters pre-trained on the\nImageNet-22K dataset and show the result as B‚Äô tr22K in Table\n12. The better performance of B‚Äô tr22K compared with B‚Äô tr\nagain explains the importance of the initialization weights.\nDifferent transformer backbones analysis: Following our\npipeline, we change the Swin transformer backbone [34] to the\nViT backbone [33], [41], and achieve B‚Äô ViT in Table 12. Note\nthat, the ViT backbone we used in Table 12 is initialized with\nweights trained on ImageNet-22K. The comparable performance\nof B‚Äô ViT compared with B‚Äô tr22K explains that the two types of\nbackbones both work for SOD.\n10 30 50 70 90\nTraining data ratio /%\n0.03\n0.04\n0.05MAE\nDUTS\nTransformer\nCNN\n10 30 50 70 90\nTraining data ratio /%\n0.05\n0.06MAE\nDUT\nTransformer\nCNN\n10 30 50 70 90\nTraining data ratio /%\n0.03\n0.04MAE\nECSSD\nTransformer\nCNN\n10 30 50 70 90\nTraining data ratio /%\n0.025\n0.030\n0.035MAE\nHKU-IS\nTransformer\nCNN\n10 30 50 70 90\nTraining data ratio /%\n0.06\n0.07MAE\nPASCAL-S\nTransformer\nCNN\n10 30 50 70 90\nTraining data ratio /%\n0.06\n0.08MAE\nSOD\nTransformer\nCNN\nFig. 10. Model performance of the CNN backbone and the transformer\nbackbone w.r.t.different training dataset sizes on six testing datasets.\nModel performance w.r.t. training datasets scales: As the two\ntypes of backbones have signiÔ¨Åcantly different numbers of model\nparameters, leading to different model capacities. We aim to\nanalyze how model capacity is sensitive to scales of the training\ndataset. We then train our transformer backbone networks (B‚Äô tr)\nand CNN backbone based model (B‚Äô cnn) in Table 5) with\ndifferent sizes of training datasets, which are 10%, 30%, 50%,\n70%, 90% of the entire training dataset respectively, and report\nthe model performance in Fig. 10. The consistently better perfor-\nmance of the transformer backbone-based model with regard to\ndifferent numbers of training examples explains its effectiveness.\nMeanwhile, we observe that the model performance is not always\nincreasing with a larger training dataset, which inspires us to work\non an active learning-based transformer network to actively select\nrepresentative samples for model training.\nModel performance with different decoders: To test how the\ntransformer encoder performs with different decoders, we change\nthe backbone of existing SOD models (SCRN [2] and F3Net [3])\nto transformer backbone [34], and show their performance in\nTable 13, where ‚Äú*‚Äù is the transformer backbone based counter-\npart. Table 13 shows that the transformer backbone can indeed\nimprove the performance of existing SOD models. However, we\nobserve similar performance of model with our decoder (B‚Äô tr)\n(around 1M parameters) compared with other complicated de-\ncoders (more than 20M for both SCRN [2] and F3Net [3]). The\nSwin backbone model [34] has around 85M parameters, and its\nhigh capacity poses challenges to the decoder design. We argue\nthat the transformer-compatible decoder should be investigated to\nfurther explore the contribution of transformer backbones.\nModel performance with conventional uncertainty estimation\ntechniques: A systematic way to deal with model uncertainty is\nvia Bayesian statistics [39], [143], [149]‚Äì[151]. Bayesian Neural\nNetworks aim to learn a distribution over each of the network\nparameters by placing a prior probability distribution over network\nweights, i.e. p(Œ∏|D). According to the Bayesian rule, the posterior\nover model parameters p(Œ∏|x,y) (or p(Œ∏|D)) can be achieved as:\np(Œ∏|x,y) =p(x,y|Œ∏)p(Œ∏)\np(x,y) = p(x,y|Œ∏)p(Œ∏)‚à´\np(x,y|Œ∏)p(Œ∏)dŒ∏. (14)\nThe marginalization over Œ∏to calculate p(x,y) in the denominator\nis intractable. p(Œ∏|x,y) is then not available in closed-form, mak-\ning it computationally intractable to calculate the exact Bayesian\nposterior. More efforts have been put into developing approxi-\nmations of Bayesian Neural Network that can work in practice,\nincluding Variational Inference (VI) [152]‚Äì[154] and Markov\nChain Monte Carlo (MCMC) [155]. [156] shows that a neural\nnetwork of arbitrary depth and non-linearity, with dropout applied\nbefore every weighted layer, is mathematically equivalent to an\napproximation to the probabilistic deep Gaussian process [157]\n(GP). Based on it, [156] further shows that the dropout objective\nminimizes the KL divergence between an approximate distribution\nand the posterior of a deep Gaussian process. In this paper,\nwe apply MC-dropout [156] as a free-lunch model uncertainty\nestimation technique to our baseline model B‚Äô tr in Table 8 with\ndropout rate 0.3, and show its performance in Table 6, which again\nverify the effectiveness of our model in achieving better model\ncalibration.\nRobustness to adversarial attack: Deep neural network based\nmodels are known to suffer from adversarial examples. With small\nperturbations, model predictions can be changed drastically [158].\nCommon defense methods for adversarial attacks include adver-\nsarial training [148], certiÔ¨Åed robustness [159], etc.In this paper,\nwe investigate model robustness with respect to adversarial attack.\nSpeciÔ¨Åcally, we discuss FGSM [147], a gradient based attack, and\nperform adversarial training [148] to achieve model defense.\nFGSM [147] attack only needs to do backprop once to get the\ngradient of classiÔ¨Åcation loss with respect to the input x, and the\nadversarial sample xadv can be generated via:\nxadv = x+ Œµsign(‚àáxL(Œ∏,x,y )), (15)\nwhere L(Œ∏,x,y )) is the classiÔ¨Åcation loss, and the sign function\nis used to achieve faster convergence. Correspondingly, the adver-\nsarial training based defense is achieved via training the model\nwith adversarial sample pair (xadv,y), leading to a new objective:\nmin\nŒ∏\nE(x,y)‚àºD\n[\nmax\nŒ¥‚ààS\nL(Œ∏,x + Œ¥,y)\n]\n, (16)\nwhere Sis the candidate adversarial attacks. In practice, a more\nefÔ¨Åcient way to achieve adversarial training based defense is\nthorough joint training with both clean sample xand adversarial\nsample xadv with weighted loss:\nmin\nŒ∏\n(Œ±L(Œ∏,x,y ) + (1‚àíŒ±)L(Œ∏,xadv,y)) , (17)\nwhere Œ± = 0 .5 is used to control the balance of accurate\nprediction (L(Œ∏,x,y )) and model robustness (L(Œ∏,xadv,y)).\nIn this paper, we perform adversarial attack with FGSM [147]\nand defense with adversarial training in Eq. (17) to both the base-\nline models (B‚Äô cnn and B‚Äô tr in Table 8) and the proposed iGAN\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\nTABLE 14\nModel robustness to adversarial attack and defense.\nDUTS [7] ECSSD [131] DUT [132] HKU-IS [133] PASCAL-S [134] SOD [135]\nMethod SŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚ÜìSŒ± ‚Üë FŒ≤ ‚Üë EŒæ ‚Üë M‚Üì\nB‚Äô cnn .882 .840 .916 .037 .922 .919 .947 .035 .823 .742 .851 .057 .912 .901 .947 .030 .855 .841 .896 .065 .832 .825 .863 .073\nCIGAN .876 .820 .906 .042 .923 .913 .945 .037 .823 .733 .848 .061 .911 .892 .943 .034 .856 .836 .893 .068 .833 .816 .862 .075\nB‚Äô tr .911 .882 .947 .026 .939 .940 .965 .024 .860 .801 .894 .045 .927 .921 .964 .023 .876 .872 .917 .053 .858 .853 .897 .059\nTIGAN .909 .873 .941 .028 .941 .936 .964 .025 .861 .796 .890 .047 .929 .918 .962 .025 .879 .869 .916 .054 .861 .854 .894 .060\nPerforming FGSM [147] Attack\nAB‚Äô cnn .782 .709 .824 .082 .837 .829 .874 .078 .714 .594 .744 .113 .838 .818 .889 .061 .771 .747 .819 .113 .713 .680 .747 .134\nACIGAN .786 .692 .813 .092 .858 .833 .883 .076 .730 .601 .753 .119 .853 .814 .887 .065 .789 .758 .824 .113 .737 .690 .774 .136\nAB‚Äô tr .802 .744 .872 .065 .844 .840 .898 .068 .746 .652 .808 .093 .840 .823 .910 .056 .779 .759 .842 .100 .746 .734 .807 .113\nATIGAN .881 .837 .921 .040 .916 .912 .946 .037 .841 .773 .879 .056 .909 .897 .950 .033 .856 .851 .899 .067 .848 .847 .891 .067\nPerforming Adversarial Training [148] based Defense\nDB‚Äô cnn .822 .761 .862 .063 .872 .868 .906 .059 .756 .649 .786 .088 .870 .855 .916 .047 .806 .787 .850 .093 .765 .740 .798 .105\nDCIGAN .838 .775 .872 .059 .891 .881 .918 .053 .781 .676 .808 .084 .886 .866 .922 .044 .826 .806 .867 .088 .783 .759 .819 .104\nDB‚Äô tr .861 .822 .909 .045 .902 .905 .939 .041 .802 .726 .845 .070 .893 .887 .941 .036 .827 .819 .877 .078 .797 .793 .842 .090\nDTIGAN .906 .874 .942 .030 .939 .938 .964 .025 .864 .806 .898 .048 .929 .921 .964 .024 .873 .869 .915 .056 .866 .866 .903 .058\nx y CIGAN TIGAN\nFig. 11. Model robustness to adversarial attack, i.e. FGSM [147] attack, where the samples (from left to right) within each method (CIGAN and\nTIGAN) are the prediction of x, the adversarial sample xadv, its prediction sadv, and the prediction of xadv after adversarial training.\nImage GT B‚Äô cnn B‚Äô tr CIGAN CGAN\nFig. 12. Failure cases of the transformer backbone compared with the\nCNN backbone (B‚Äô cnn and B‚Äô tr), and iGAN compared with CGAN\nwithin the CNN backbone (CIGAN and CGAN) for RGB SOD.\nbased frameworks (CIGAN and TIGAN in Table 8). SpeciÔ¨Åcally,\nbased on the above models, we set Œµ = 8/255 in Eq. (15)\nfollowing [147] to generate adversarial samples xadv of each\ntraining image, which will be used to train the above models\nagain together with the clean sample x to achieve the defense\nprocess. We report model performance in Table 14. Note that\nthe performance of attacked models is obtained by performing\nFGSM [147] attack on the testing samples, where the adversarial\ntesting samples are fed to the speciÔ¨Åc model to generate the\npredictions. We show in Fig. 11 the clean sample x and its\nprediction s, the adversarial sample xadv and its prediction sadv,\nand the prediction of x and xadv after the defense. Fig. 11 and\nTable 14 show that invisible adversarial attack [147] can cause\nsigniÔ¨Åcant performance degradation, which can be partially solved\nwith the adversarial training strategies [148]. We also observe the\nslightly robust performance of the proposed iGAN frameworks\n(CIGAN and TIGAN) compared with the baseline models, which\nfurther explain the robustness of the proposed generative model.\nFailure Case Analysis: To further investigate the limitations of\nboth the transformer backbone and the iGAN framework, we look\ndeeper in Table 8 and Table 9, and the predictions of each related\nmodel. We Ô¨Ånd out two main issues: 1) the transformer backbone\ndoes not always perform superior to the CNN backbone; 2) our\niGAN model can lead to over-smoothed predictions compared\nwith CGAN due to the diverse generation process.\nFor the former, to exclude the inÔ¨Çuence of the iGAN frame-\nwork, we compare the predictions of the CNN backbone (B‚Äô cnn)\nand transformer backbone (B‚Äô tr) for RGB SOD, and show sam-\nples in Fig. 12. We observe more false positives within B‚Äô tr,\nwhich can be explained as the ‚Äúdouble-edged sword‚Äù of the\ntransformer backbone. On the one hand, the larger receptive Ô¨Åeld\nof the transformer makes it superior in localizing the larger salient\nforeground. On the other hand, the less salient objects that expand\nfor a larger region can be falsely detected as positive foreground.\nWe argue that salient object ranking [160] can be beneÔ¨Åcial in\nidentifying the less salient regions by providing extra saliency\ndegree evaluation. For the latter, as the latent variable z within\niGAN is conditioned on input x, leading to informative latent\nspace, where the predictive distributionp(y‚àó|x‚àó,Œ∏,z ) for input x‚àó\nhas larger variance compared with the CGAN based framework.\nIn this case, the averaged prediction is over-smoothed. Similarly,\nthis issue can also be Ô¨Åxed with saliency ranking [160].\n5 C ONCLUSION AND FUTURE WORK\nIn this paper, we proposed an inferential GAN within the trans-\nformer framework for both fully and weakly supervised SOD.\nDifferent from typical GANs that deÔ¨Åne the prior distribution\nas a standard normal distribution, we inferred the latent variable\nvia Langevin Dynamics [37], a gradient based MCMC, leading\nto the image-conditioned prior distribution. Through extensive\nexperiments, we observed that a larger receptive Ô¨Åeld of the\ntransformer leads to its better performance on images with larger\nsalient objects (see Fig. 6). However, we also found the double-\nedged sword effect of the larger receptive Ô¨Åeld that leads to serious\nfalse positives (see Fig. 12). Further, for RGB-D SOD,we found\nthat the various depth sensors lead to a domain gap between the\ntraining dataset and the testing dataset (see Table 1 and Fig. 2).\nWe then presented auxiliary depth module, leading to consistent\ndepth contribution (see Table 7). For weakly supervised SOD,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16\nwe observed that the accurate structure information encoded in\nthe transformer backbone as shown in Fig. 8 makes it power-\nful in generating structure-preserving predictions (see Table 10).\nExtensive experimental results demonstrate the superiority of our\ntransformer backbone-based generative network, achieving new\nbenchmarks with reliable uncertainty maps.\nOur proposed generative model aims to estimate the reliability\nof saliency prediction with uncertainty maps, which also show\nsuperiority in achieving robust models (see Table 14 and Fig. 11)\nand well calibrated models (see Table 6). Our future work includes\ntwo main parts. Firstly, we will apply the produced uncertainty\nmap (see Fig. 7 ) to the saliency generator for effective hard\nnegative mining. In this way, the uncertainty map can not only\nexplain model predictions but also serve as an important prior\nfor effective model learning. Secondly, we have several hyper-\nparameters within the inference model, and we observe that our\nmodel performance can be inÔ¨Çuenced by them. We plan to further\ninvestigate model performance w.r.t.those hyper-parameters.\nACKNOWLEDGMENT\nThis work was partly supported by the National Natural Science\nFoundation of China (62271410, 61871325).\nREFERENCES\n[1] M. Zhuge, D.-P. Fan, N. Liu, D. Zhang, D. Xu, and L. Shao, ‚ÄúSalient\nobject detection via integrity learning,‚Äù IEEE TPAMI, 2023.\n[2] Z. Wu, L. Su, and Q. Huang, ‚ÄúStacked cross reÔ¨Ånement network for\nedge-aware salient object detection,‚Äù in IEEE ICCV, 2019.\n[3] J. Wei, S. Wang, and Q. Huang, ‚ÄúF 3net: Fusion, feedback and focus for\nsalient object detection,‚Äù in AAAI, 2020.\n[4] J. Zhang, D.-P. Fan, Y . Dai, S. Anwar, F. Saleh, S. Aliakbarian,\nand N. Barnes, ‚ÄúUncertainty inspired rgb-d saliency detection,‚Äù IEEE\nTPAMI, vol. 44, no. 9, pp. 5761‚Äì5779, 2022.\n[5] D.-P. Fan, Y . Zhai, A. Borji, J. Yang, and L. Shao, ‚ÄúBBS-Net: RGB-D\nsalient object detection with a bifurcated backbone strategy network,‚Äù\nin ECCV, 2020.\n[6] J. Zhang, X. Yu, A. Li, P. Song, B. Liu, and Y . Dai, ‚ÄúWeakly-supervised\nsalient object detection via scribble annotations,‚Äù in IEEE CVPR, 2020.\n[7] L. Wang, H. Lu, Y . Wang, M. Feng, D. Wang, B. Yin, and X. Ruan,\n‚ÄúLearning to detect salient objects with image-level supervision,‚Äù in\nIEEE CVPR, 2017.\n[8] W. Wang, Q. Lai, H. Fu, J. Shen, H. Ling, and R. Yang, ‚ÄúSalient object\ndetection in the deep learning era: An in-depth survey,‚Äù IEEE TPAMI,\nvol. 44, no. 6, pp. 3239‚Äì3259, 2022.\n[9] J. Zhang, T. Zhang, Y . Dai, M. Harandi, and R. Hartley, ‚ÄúDeep\nunsupervised saliency detection: A multiple noisy labeling perspective,‚Äù\nin IEEE CVPR, 2018.\n[10] D. T. Nguyen, M. Dax, C. K. Mummadi, T. P. N. Ngo, T. H. P. Nguyen,\nZ. Lou, and T. Brox, ‚ÄúDeepUSPS: Deep robust unsupervised saliency\nprediction with self-supervision,‚Äù in NeurIPS, 2019.\n[11] M. Zhang, T. Liu, Y . Piao, S. Yao, and H. Lu, ‚ÄúAuto-msfnet: Search\nmulti-scale fusion network for salient object detection,‚Äù in ACM MM,\n2021.\n[12] B. Xu, H. Liang, R. Liang, and P. Chen, ‚ÄúLocate globally, segment\nlocally: A progressive architecture with knowledge review network for\nsalient object detection,‚Äù in AAAI, 2021.\n[13] Y .-H. Wu, Y . Liu, L. Zhang, M.-M. Cheng, and B. Ren, ‚ÄúEdn:\nSalient object detection via extremely-downsampled network,‚Äù IEEE\nTIP, vol. 31, pp. 3125‚Äì3136, 2022.\n[14] Z. Yang, S. Soltanian-Zadeh, and S. Farsiu, ‚ÄúBiconnet: an edge-\npreserved connectivity-based approach for salient object detection,‚Äù\nPattern Recognition, vol. 121, p. 108231, 2022.\n[15] Y . Piao, W. Ji, J. Li, M. Zhang, and H. Lu, ‚ÄúDepth-induced multi-scale\nrecurrent attention network for saliency detection,‚Äù inIEEE ICCV, 2019.\n[16] K. Fu, D.-P. Fan, G.-P. Ji, and Q. Zhao, ‚ÄúJL-DCF: Joint learning\nand densely-cooperative fusion framework for RGB-D salient object\ndetection,‚Äù in IEEE CVPR, 2020.\n[17] J. Zhang, D.-P. Fan, Y . Dai, X. Yu, Y . Zhong, N. Barnes, and L. Shao,\n‚ÄúRgb-d saliency detection via cascaded mutual information minimiza-\ntion,‚Äù in IEEE ICCV, 2021.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in IEEE CVPR, 2016.\n[19] Y . Niu, Y . Geng, X. Li, and F. Liu, ‚ÄúLeveraging stereopsis for saliency\nanalysis,‚Äù in IEEE CVPR, 2012.\n[20] C. Liu, J. Yuen, and A. Torralba, ‚ÄúSift Ô¨Çow: Dense correspondence\nacross scenes and its applications,‚Äù IEEE TPAMI , vol. 33, no. 5,\npp. 978‚Äì994, 2010.\n[21] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, ‚ÄúRGBD salient object\ndetection: A benchmark and algorithms,‚Äù in ECCV, 2014.\n[22] Z. Zhang, ‚ÄúMicrosoft kinect sensor and its effect,‚Äù IEEE multimedia ,\nvol. 19, no. 2, pp. 4‚Äì10, 2012.\n[23] Y . Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, ‚ÄúDepth enhanced saliency\ndetection method,‚Äù in ICIMCS, 2014.\n[24] R. Ju, Y . Liu, T. Ren, L. Ge, and G. Wu, ‚ÄúDepth-aware salient\nobject detection using anisotropic center-surround difference,‚Äù Signal\nProcessing: Image Communication, vol. 38, pp. 115‚Äì126, 2015.\n[25] D. Sun, S. Roth, and M. J. Black, ‚ÄúSecrets of optical Ô¨Çow estimation\nand their principles,‚Äù in IEEE CVPR, 2010.\n[26] N. Li, J. Ye, Y . Ji, H. Ling, and J. Yu, ‚ÄúSaliency detection on light Ô¨Åeld,‚Äù\nin IEEE CVPR, 2014.\n[27] R. Ng, M. Levoy, M. Br ¬¥edif, G. Duval, M. Horowitz, and P. Hanrahan,\nLight Ô¨Åeld photography with a hand-held plenoptic camera. PhD thesis,\nStanford University, 2005.\n[28] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, ‚ÄúRethinking\nRGB-D Salient Object Detection: Models, Datasets, and Large-Scale\nBenchmarks,‚Äù IEEE TNNLS, vol. 32, no. 5, pp. 2075‚Äì2089, 2020.\n[29] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for\nlarge-scale image recognition,‚Äù in ICLR, 2015.\n[30] Z. Wu, L. Su, and Q. Huang, ‚ÄúCascaded partial decoder for fast and\naccurate salient object detection,‚Äù in IEEE CVPR, 2019.\n[31] J. Kim, D. Han, Y .-W. Tai, and J. Kim, ‚ÄúSalient region detection via\nhigh-dimensional color transform and local spatial support,‚Äù IEEE TIP,\nvol. 25, no. 1, pp. 9‚Äì23, 2015.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in NeurIPS,\n2017.\n[33] R. Ranftl, A. Bochkovskiy, and V . Koltun, ‚ÄúVision transformers for\ndense prediction,‚Äù in IEEE ICCV, 2021.\n[34] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n‚ÄúSwin transformer: Hierarchical vision transformer using shifted win-\ndows,‚Äù in IEEE ICCV, 2021.\n[35] C. Guo, G. Pleiss, Y . Sun, and K. Q. Weinberger, ‚ÄúOn calibration of\nmodern neural networks,‚Äù in ICML, 2017.\n[36] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y . Bengio, ‚ÄúGenerative adversarial nets,‚Äù in\nNeurIPS, 2014.\n[37] R. Neal, ‚ÄúMcmc using hamiltonian dynamics,‚Äù Handbook of Markov\nChain Monte Carlo, 2011.\n[38] T. Han, Y . Lu, S.-C. Zhu, and Y . N. Wu, ‚ÄúAlternating back-propagation\nfor generator network,‚Äù in AAAI, 2017.\n[39] A. Kendall and Y . Gal, ‚ÄúWhat uncertainties do we need in bayesian\ndeep learning for computer vision?,‚Äù in NeurIPS, 2017.\n[40] S. Cao and Z. Zhang, ‚ÄúDeep hybrid models for out-of-distribution\ndetection,‚Äù in IEEE CVPR, 2022.\n[41] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:\nTransformers for image recognition at scale,‚Äù in ICLR, 2021.\n[42] L. Itti, C. Koch, and E. Niebur, ‚ÄúA model of saliency-based visual\nattention for rapid scene analysis,‚Äù IEEE TPAMI , vol. 20, no. 11,\npp. 1254‚Äì1259, 1998.\n[43] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S.-M. Hu, ‚ÄúGlobal\ncontrast based salient region detection,‚Äù IEEE TPAMI, vol. 37, no. 3,\npp. 569‚Äì582, 2014.\n[44] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional net-\nworks for biomedical image segmentation,‚Äù in MICCAI, 2015.\n[45] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin, ‚ÄúNon-\nlocal deep features for salient object detection,‚Äù in IEEE CVPR, 2017.\n[46] N. Liu, J. Han, and M.-H. Yang, ‚ÄúPicanet: Learning pixel-wise contex-\ntual attention for saliency detection,‚Äù in IEEE CVPR, 2018.\n[47] B. Wang, Q. Chen, M. Zhou, Z. Zhang, X. Jin, and K. Gai, ‚ÄúProgressive\nfeature polishing network for salient object detection.,‚Äù in AAAI, 2020.\n[48] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. H. Torr, ‚ÄúDeeply\nsupervised salient object detection with short connections,‚Äù in IEEE\nCVPR, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17\n[49] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu, ‚ÄúA stagewise\nreÔ¨Ånement model for detecting salient objects in images,‚Äù in IEEE\nICCV, 2017.\n[50] X. Zhao, Y . Pang, L. Zhang, H. Lu, and L. Zhang, ‚ÄúSuppress and\nbalance: A simple gated network for salient object detection,‚Äù inECCV,\n2020.\n[51] Z. Chen, Q. Xu, R. Cong, and Q. Huang, ‚ÄúGlobal context-aware\nprogressive aggregation network for salient object detection,‚Äù in AAAI,\n2020.\n[52] W. Zhang, L. Zheng, H. Wang, X. Wu, and X. Li, ‚ÄúSaliency hierarchy\nmodeling via generative kernels for salient object detection,‚Äù in ECCV,\n2022.\n[53] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand,\n‚ÄúBasnet: Boundary-aware salient object detection,‚Äù in IEEE CVPR ,\n2019.\n[54] Y . Pang, X. Zhao, L. Zhang, and H. Lu, ‚ÄúMulti-scale interactive network\nfor salient object detection,‚Äù in IEEE CVPR, 2020.\n[55] L. Tang, B. Li, Y . Zhong, S. Ding, and M. Song, ‚ÄúDisentangled high\nquality salient object detection,‚Äù in IEEE ICCV, 2021.\n[56] S. Xie and Z. Tu, ‚ÄúHolistically-nested edge detection,‚Äù in IEEE ICCV,\n2015.\n[57] Y . Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai, ‚ÄúRicher convolu-\ntional features for edge detection,‚Äù in IEEE CVPR, 2017.\n[58] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand,\n‚ÄúBASNet: Boundary-aware salient object detection,‚Äù in IEEE CVPR ,\n2019.\n[59] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y . Cao, J. Yang, and M.-M. Cheng,\n‚ÄúEGNet:edge guidance network for salient object detection,‚Äù in IEEE\nICCV, 2019.\n[60] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, ‚ÄúProgressive attention\nguided recurrent network for salient object detection,‚Äù in IEEE CVPR,\n2018.\n[61] M. Zhang, T. Liu, Y . Piao, S. Yao, and H. Lu, ‚ÄúAuto-msfnet: Search\nmulti-scale fusion network for salient object detection,‚Äù in ACM MM,\n2021.\n[62] J. Han, H. Chen, N. Liu, C. Yan, and X. Li, ‚ÄúCnns-based rgb-d\nsaliency detection via cross-view transfer and multiview fusion,‚Äù IEEE\nTransactions on Cybernetics, vol. 48, no. 11, pp. 3171‚Äì3183, 2017.\n[63] M. Lee, C. Park, S. Cho, and S. Lee, ‚ÄúSpsn: Superpixel prototype\nsampling network for rgb-d salient object detection,‚Äù in ECCV, 2022.\n[64] L. Qu, S. He, J. Zhang, J. Tian, Y . Tang, and Q. Yang, ‚ÄúRGBD salient\nobject detection via deep fusion,‚Äù IEEE TIP, vol. 26, no. 5, pp. 2274‚Äì\n2285, 2017.\n[65] N. Wang and X. Gong, ‚ÄúAdaptive fusion for RGB-D salient object\ndetection,‚Äù IEEE Access, vol. 7, pp. 55277‚Äì55284, 2019.\n[66] J. Han, H. Chen, N. Liu, C. Yan, and X. Li, ‚ÄúCNNs-based RGB-D\nsaliency detection via cross-view transfer and multiview fusion,‚Äù IEEE\nTOC, vol. 48, no. 11, pp. 3171‚Äì3183, 2017.\n[67] Y . Piao, Z. Rong, M. Zhang, W. Ren, and H. Lu, ‚ÄúA2dele: Adaptive and\nattentive depth distiller for efÔ¨Åcient RGB-D salient object detection,‚Äù in\nIEEE CVPR, 2020.\n[68] H. Chen and Y . Li, ‚ÄúProgressively complementarity-aware fusion net-\nwork for RGB-D salient object detection,‚Äù in IEEE CVPR, 2018.\n[69] H. Chen, Y . Li, and D. Su, ‚ÄúMulti-modal fusion network with multi-\nscale multi-path and cross-modal interactions for RGB-D salient object\ndetection,‚Äù Pattern Recognition, vol. 86, pp. 376‚Äì385, 2019.\n[70] H. Chen and Y . Li, ‚ÄúThree-stream attention-aware network for RGB-\nD salient object detection,‚Äù IEEE TIP, vol. 28, no. 6, pp. 2825‚Äì2835,\n2019.\n[71] J.-X. Zhao, Y . Cao, D.-P. Fan, M.-M. Cheng, X.-Y . Li, and L. Zhang,\n‚ÄúContrast prior and Ô¨Çuid pyramid integration for RGBD salient object\ndetection,‚Äù in IEEE CVPR, 2019.\n[72] M. Zhang, W. Ren, Y . Piao, Z. Rong, and H. Lu, ‚ÄúSelect, supplement\nand focus for RGB-D saliency detection,‚Äù in IEEE CVPR, 2020.\n[73] N. Liu, N. Zhang, L. Shao, and J. Han, ‚ÄúLearning selective mutual\nattention and contrast for rgb-d saliency detection,‚Äù IEEE TPAMI ,\nvol. 44, no. 12, pp. 9026‚Äì9042, 2022.\n[74] W. Ji, J. Li, M. Zhang, Y . Piao, and H. Lu, ‚ÄúAccurate RGB-D salient\nobject detection via collaborative learning,‚Äù in ECCV, 2020.\n[75] Y . Pang, L. Zhang, X. Zhao, and H. Lu, ‚ÄúHierarchical dynamic Ô¨Åltering\nnetwork for RGB-D salient object detection,‚Äù in ECCV, 2020.\n[76] Z. Zhang, Z. Lin, J. Xu, W.-D. Jin, S.-P. Lu, and D.-P. Fan, ‚ÄúBilateral\nattention network for RGB-D salient object detection,‚Äù IEEE TIP ,\nvol. 30, pp. 1949‚Äì1961, 2021.\n[77] C. Li, R. Cong, Y . Piao, Q. Xu, and C. C. Loy, ‚ÄúRGB-D salient object\ndetection with cross-modality modulation and selection,‚Äù in ECCV,\n2020.\n[78] G. Li, Z. Liu, L. Ye, Y . Wang, and H. Ling, ‚ÄúCross-modal weighting\nnetwork for RGB-D salient object detection,‚Äù in ECCV, 2020.\n[79] A. Luo, X. Li, F. Yang, Z. Jiao, H. Cheng, and S. Lyu, ‚ÄúCascade graph\nneural networks for RGB-D salient object detection,‚Äù in ECCV, 2020.\n[80] S. Chen and Y . Fu, ‚ÄúProgressively guided alternate reÔ¨Ånement network\nfor RGB-D salient object detection,‚Äù in ECCV, 2020.\n[81] M. Zhang, S. X. Fei, J. Liu, S. Xu, Y . Piao, and H. Lu, ‚ÄúAsymmetric two-\nstream architecture for accurate RGB-D saliency detection,‚Äù in ECCV,\n2020.\n[82] G. Li, Z. Liu, M. Chen, Z. Bai, W. Lin, and H. Ling, ‚ÄúHierarchical\nalternate interaction network for rgb-d salient object detection,‚Äù IEEE\nTIP, vol. 30, pp. 3528‚Äì3542, 2021.\n[83] K. Fu, D.-P. Fan, G.-P. Ji, Q. Zhao, J. Shen, and C. Zhu, ‚ÄúSiamese\nnetwork for rgb-d salient object detection and beyond,‚Äù IEEE TPAMI,\nvol. 44, no. 9, pp. 5541‚Äì5559, 2022.\n[84] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in\nECCV, 2020.\n[85] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‚ÄúDeformable DETR:\nDeformable transformers for end-to-end object detection,‚Äù in ICLR,\n2021.\n[86] Z. Dai, B. Cai, Y . Lin, and J. Chen, ‚ÄúUP-DETR: Unsupervised pre-\ntraining for object detection with transformers,‚Äù in IEEE CVPR, 2021.\n[87] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‚ÄúPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,‚Äù in IEEE ICCV, 2021.\n[88] G. Zhang, Z. Luo, K. Cui, S. Lu, and E. P. Xing, ‚ÄúMeta-detr: Image-\nlevel few-shot detection with inter-class correlation exploitation,‚Äù IEEE\nTPAMI, 2022.\n[89] Y . Xu, Y . Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda,\n‚ÄúTranscenter: Transformers with dense representations for multiple-\nobject tracking,‚Äù IEEE TPAMI, 2022.\n[90] B. Yan, H. Peng, J. Fu, D. Wang, and H. Lu, ‚ÄúLearning spatio-temporal\ntransformer for visual tracking,‚Äù in IEEE ICCV, 2021.\n[91] W. Mao, Y . Ge, C. Shen, Z. Tian, X. Wang, Z. Wang, and A. v. den\nHengel, ‚ÄúPoseur: Direct human pose estimation with transformers,‚Äù in\nECCV, 2022.\n[92] S. Jiang, D. Campbell, Y . Lu, H. Li, and R. Hartley, ‚ÄúLearning to\nestimate hidden motions with global motion aggregation,‚Äù in IEEE\nICCV, 2021.\n[93] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, and L. Zhang, ‚ÄúRethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers,‚Äù in IEEE\nCVPR, 2021.\n[94] N. Liu, N. Zhang, K. Wan, L. Shao, and J. Han, ‚ÄúVisual saliency\ntransformer,‚Äù in IEEE ICCV, 2021.\n[95] J. Zhang, J. Xie, N. Barnes, and P. Li, ‚ÄúLearning generative vision\ntransformer with energy-based latent space for saliency prediction,‚Äù in\nNeurIPS, 2021.\n[96] D. Kingma and M. Welling, ‚ÄúAuto-encoding variational bayes,‚Äù in\nICLR, 2014.\n[97] Y . LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang, ‚ÄúA tutorial\non energy-based learning,‚Äù Predicting structured data , vol. 1, no. 0,\n2006.\n[98] K. Sohn, H. Lee, and X. Yan, ‚ÄúLearning structured output representation\nusing deep conditional generative models,‚Äù in NeurIPS, 2015.\n[99] C. F. Baumgartner, K. C. Tezcan, K. Chaitanya, A. M. H ¬®otker,\nU. J. Muehlematter, K. Schawkat, A. S. Becker, O. Donati, and\nE. Konukoglu, ‚ÄúPhiseg: Capturing uncertainty in medical image seg-\nmentation,‚Äù in MICCAI, 2019.\n[100] S. Kohl, B. Romera-Paredes, C. Meyer, J. De Fauw, J. R. Ledsam,\nK. Maier-Hein, S. M. A. Eslami, D. Jimenez Rezende, and O. Ron-\nneberger, ‚ÄúA probabilistic u-net for segmentation of ambiguous images,‚Äù\nin NeurIPS, 2018.\n[101] B. Li, Z. Sun, and Y . Guo, ‚ÄúSupervae: Superpixelwise variational\nautoencoder for salient object detection,‚Äù in AAAI, 2019.\n[102] R. Groenendijk, S. Karaoglu, T. Gevers, and T. Mensink, ‚ÄúOn the beneÔ¨Åt\nof adversarial training for monocular depth estimation,‚ÄùCVIU, vol. 190,\np. 102848, 2020.\n[103] Q. H. Le, K. Youcef-Toumi, D. Tsetserukou, and A. Jahanian, ‚ÄúGan\nmask r-cnn:instance semantic segmentation beneÔ¨Åts from generative\nadversarial networks,‚Äù in NeurIPS Workshop, 2020.\n[104] N. Souly, C. Spampinato, and M. Shah, ‚ÄúSemi supervised semantic\nsegmentation using generative adversarial network,‚Äù in IEEE ICCV ,\n2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18\n[105] W.-C. Hung, Y .-H. Tsai, Y .-T. Liou, Y .-Y . Lin, and M.-H. Yang,\n‚ÄúAdversarial learning for semi-supervised semantic segmentation,‚Äù in\nBMVC, 2018.\n[106] Y . Chen, Q. Gao, X. Wang, et al. , ‚ÄúInferential wasserstein generative\nadversarial networks,‚Äù Journal of the Royal Statistical Society Series B ,\nvol. 84, no. 1, pp. 83‚Äì113, 2022.\n[107] J. He, D. Spokoyny, G. Neubig, and T. Berg-Kirkpatrick, ‚ÄúLagging\ninference networks and posterior collapse in variational autoencoders,‚Äù\nin ICLR, 2019.\n[108] K.-J. Hsu12, Y .-Y . Lin, and Y .-Y . Chuang, ‚ÄúWeakly supervised saliency\ndetection with a category-driven map generator,‚Äù BMVC, 2017.\n[109] G. Li, Y . Xie, and L. Lin, ‚ÄúWeakly supervised salient object detection\nusing image labels,‚Äù in AAAI, 2018.\n[110] J. Ahn and S. Kwak, ‚ÄúLearning pixel-level semantic afÔ¨Ånity with image-\nlevel supervision for weakly supervised semantic segmentation,‚Äù in\nIEEE CVPR, 2018.\n[111] Z. Huang, X. Wang, J. Wang, W. Liu, and J. Wang, ‚ÄúWeakly-supervised\nsemantic segmentation network with deep seeded region growing,‚Äù in\nIEEE CVPR, 2018.\n[112] H. Zhang, Y . Zeng, H. Lu, L. Zhang, J. Li, and J. Qi, ‚ÄúLearning to\ndetect salient object with multi-source weak supervision,‚ÄùIEEE TPAMI,\nvol. 44, no. 7, pp. 3577‚Äì3589, 2022.\n[113] C. Song, Y . Huang, W. Ouyang, and L. Wang, ‚ÄúBox-driven class-\nwise region masking and Ô¨Ålling rate guided loss for weakly supervised\nsemantic segmentation,‚Äù in IEEE CVPR, 2019.\n[114] J. Dai, K. He, and J. Sun, ‚ÄúBoxsup: Exploiting bounding boxes to\nsupervise convolutional networks for semantic segmentation,‚Äù in IEEE\nICCV, 2015.\n[115] V . Kulharia, S. Chandra, A. Agrawal, P. Torr, and A. Tyagi, ‚ÄúBox2seg:\nAttention weighted loss and discriminative feature learning for weakly\nsupervised segmentation,‚Äù in ECCV, 2020.\n[116] J. Lee, J. Yi, C. Shin, and S. Yoon, ‚ÄúBbam: Bounding box attribution\nmap for weakly supervised semantic and instance segmentation,‚Äù in\nIEEE CVPR, 2021.\n[117] Z. Tian, C. Shen, X. Wang, and H. Chen, ‚ÄúBoxinst: High-performance\ninstance segmentation with box annotations,‚Äù in IEEE CVPR, 2021.\n[118] D. Lin, J. Dai, J. Jia, K. He, and J. Sun, ‚ÄúScribblesup: Scribble-\nsupervised convolutional networks for semantic segmentation,‚Äù inIEEE\nCVPR, 2016.\n[119] P. Vernaza and M. Chandraker, ‚ÄúLearning random-walk label propa-\ngation for weakly-supervised semantic segmentation,‚Äù in IEEE CVPR,\n2017.\n[120] S. Yu, B. Zhang, J. Xiao, and E. G. Lim, ‚ÄúStructure-consistent weakly\nsupervised salient object detection with local saliency coherence,‚Äù in\nAAAI, 2021.\n[121] A. Bearman, O. Russakovsky, V . Ferrari, and L. Fei-Fei, ‚ÄúWhat‚Äôs the\npoint: Semantic segmentation with point supervision,‚Äù in ECCV, 2016.\n[122] H. Chen, J. Wang, H. C. Chen, X. Zhen, F. Zheng, R. Ji, and L. Shao,\n‚ÄúSeminar learning for click-level weakly supervised semantic segmen-\ntation,‚Äù in IEEE ICCV, 2021.\n[123] A. Obukhov, S. Georgoulis, D. Dai, and L. Van Gool, ‚ÄúGated CRF\nloss for weakly supervised semantic image segmentation,‚Äù in NeurIPS,\n2019.\n[124] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik,\n‚ÄúMultiscale combinatorial grouping,‚Äù in IEEE CVPR, 2014.\n[125] C. Rother, V . Kolmogorov, and A. Blake, ‚Äú‚Äùgrabcut‚Äù interactive fore-\nground extraction using iterated graph cuts,‚Äù ACM TOG, vol. 23, no. 3,\npp. 309‚Äì314, 2004.\n[126] J. Zhang, J. Xie, and N. Barnes, ‚ÄúLearning noise-aware encoder-\ndecoder from noisy labels by alternating back-propagation for saliency\ndetection,‚Äù in ECCV, 2020.\n[127] Y . Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y . Fu, ‚ÄúImage super-\nresolution using very deep residual channel attention networks,‚Äù in\nECCV, 2018.\n[128] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, ‚ÄúDenseaspp for semantic\nsegmentation in street scenes,‚Äù in IEEE CVPR, 2018.\n[129] C. Godard, O. Mac Aodha, and G. J. Brostow, ‚ÄúUnsupervised monocu-\nlar depth estimation with left-right consistency,‚Äù in IEEE CVPR, 2017.\n[130] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,\nS. Mohamed, and A. Lerchner, ‚Äúbeta-V AE: Learning basic visual\nconcepts with a constrained variational framework,‚Äù in ICLR, 2017.\n[131] Q. Yan, L. Xu, J. Shi, and J. Jia, ‚ÄúHierarchical saliency detection,‚Äù in\nIEEE CVPR, 2013.\n[132] C. Yang, L. Zhang, H. Lu, X. Ruan, and M.-H. Yang, ‚ÄúSaliency\ndetection via graph-based manifold ranking,‚Äù in IEEE CVPR, 2013.\n[133] G. Li and Y . Yu, ‚ÄúVisual saliency based on multiscale deep features,‚Äù in\nIEEE CVPR, 2015.\n[134] Y . Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, ‚ÄúThe secrets of\nsalient object segmentation,‚Äù in IEEE CVPR, 2014.\n[135] V . Movahedi and J. H. Elder, ‚ÄúDesign and perceptual validation of\nperformance measures for salient object segmentation,‚Äù in IEEE CVPR\nWorkshop, 2010.\n[136] H. Zhou, X. Xie, J.-H. Lai, Z. Chen, and L. Yang, ‚ÄúInteractive two-\nstream decoder for accurate and fast saliency detection,‚Äù inIEEE CVPR,\n2020.\n[137] Z. Zhao, C. Xia, C. Xie, and J. Li, ‚ÄúComplementary trilateral decoder\nfor fast and accurate salient object detection,‚Äù in ACM MM, 2021.\n[138] X. Li, F. Yang, H. Cheng, W. Liu, and D. Shen, ‚ÄúContour knowledge\ntransfer for salient object detection,‚Äù in ECCV, 2018.\n[139] D.-P. Fan, C. Gong, Y . Cao, B. Ren, M.-M. Cheng, and A. Borji,\n‚ÄúEnhanced-alignment measure for binary foreground map evaluation,‚Äù\nin IJCAI, 2018.\n[140] D.-P. Fan, M.-M. Cheng, Y . Liu, T. Li, and A. Borji, ‚ÄúStructure-measure:\nA new way to evaluate foreground maps,‚Äù in IEEE ICCV, 2017.\n[141] G. Franchi, X. Yu, A. Bursuc, E. Aldea, S. Dubuisson, and D. Filliat,\n‚ÄúLatent discriminant deterministic uncertainty,‚Äù in ECCV, 2022.\n[142] M. H. DeGroot and S. E. Fienberg, ‚ÄúThe comparison and evaluation\nof forecasters,‚Äù Journal of the Royal Statistical Society: Series D (The\nStatistician), vol. 32, no. 1-2, pp. 12‚Äì22, 1983.\n[143] S. Depeweg, J.-M. Hernandez-Lobato, F. Doshi-Velez, and S. Udluft,\n‚ÄúDecomposition of uncertainty in Bayesian deep learning for efÔ¨Åcient\nand risk-sensitive learning,‚Äù in ICML, 2018.\n[144] Y . Wang, Y . Yang, Z. Yang, L. Zhao, P. Wang, and W. Xu, ‚ÄúOcclusion\naware unsupervised learning of optical Ô¨Çow,‚Äù in IEEE CVPR, 2018.\n[145] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan, and M.-H.\nYang, ‚ÄúIntriguing properties of vision transformers,‚Äù in NeurIPS, 2021.\n[146] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‚ÄúImagenet:\nA large-scale hierarchical image database,‚Äù in IEEE CVPR, 2009.\n[147] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‚ÄúExplaining and harnessing\nadversarial examples,‚Äù in ICLR, 2014.\n[148] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, ‚ÄúTowards\ndeep learning models resistant to adversarial attacks,‚Äù in ICLR, 2018.\n[149] W. Wright, ‚ÄúBayesian approach to neural-network modeling with input\nuncertainty,‚ÄùIEEE TNN, vol. 10, no. 6, pp. 1261‚Äì1270, 1999.\n[150] Y . Gal and Z. Ghahramani, ‚ÄúBayesian convolutional neural net-\nworks with bernoulli approximate variational inference,‚Äù arXiv preprint\narXiv:1506.02158, 2015.\n[151] M. Vadera, B. Jalaian, and B. Marlin, ‚ÄúGeneralized bayesian posterior\nexpectation distillation for deep neural networks,‚Äù in CUAI, 2020.\n[152] M. I. Jordan, Z. Ghahramani, and et al., ‚ÄúAn introduction to variational\nmethods for graphical models,‚Äù inMachine Learning, pp. 183‚Äì233, MIT\nPress, 1999.\n[153] M. J. Wainwright and M. I. Jordan, ‚ÄúGraphical models, exponential fam-\nilies, and variational inference,‚Äù Foundations and Trends¬Æ in Machine\nLearning, vol. 1, no. 1‚Äì2, pp. 1‚Äì305, 2008.\n[154] S. Y . Lee, ‚ÄúGibbs sampler and coordinate ascent variational inference:\nA set-theoretical review,‚Äù Communications in Statistics-Theory and\nMethods, vol. 51, no. 6, pp. 1549‚Äì1568, 2022.\n[155] W. K. Hastings, ‚ÄúMonte Carlo sampling methods using Markov chains\nand their applications,‚Äù Biometrika, vol. 57, pp. 97‚Äì109, 04 1970.\n[156] Y . Gal and Z. Ghahramani, ‚ÄúDropout as a bayesian approximation:\nRepresenting model uncertainty in deep learning,‚Äù in ICML, 2016.\n[157] A. Damianou and N. D. Lawrence, ‚ÄúDeep Gaussian processes,‚Äù in\nAISTATS, 2013.\n[158] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-\nlow, and R. Fergus, ‚ÄúIntriguing properties of neural networks,‚Äù inICLR,\n2014.\n[159] E. Wong and Z. Kolter, ‚ÄúProvable defenses against adversarial examples\nvia the convex outer adversarial polytope,‚Äù in ICLR, 2018.\n[160] M. A. Islam, M. Kalash, and N. D. Bruce, ‚ÄúRevisiting salient object\ndetection: Simultaneous detection, ranking, and subitizing of multiple\nsalient objects,‚Äù in IEEE CVPR, 2018."
}