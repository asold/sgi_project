{
  "title": "An improved transformer-based concrete crack classification method",
  "url": "https://openalex.org/W4392798797",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2627976352",
      "name": "Guanting Ye",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A1994056853",
      "name": "Wei Dai",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A4367409297",
      "name": "Jintai Tao",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A4367409296",
      "name": "Jinsheng Qu",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2086176540",
      "name": "Lin Zhu",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2121576242",
      "name": "Qiang Jin",
      "affiliations": [
        "Xinjiang Agricultural University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2588180165",
    "https://openalex.org/W4324359016",
    "https://openalex.org/W1607537728",
    "https://openalex.org/W2019496031",
    "https://openalex.org/W2037325199",
    "https://openalex.org/W2172941934",
    "https://openalex.org/W2526166515",
    "https://openalex.org/W2896496331",
    "https://openalex.org/W2069500249",
    "https://openalex.org/W2762239123",
    "https://openalex.org/W3201430624",
    "https://openalex.org/W2765982206",
    "https://openalex.org/W2511065100",
    "https://openalex.org/W2598457882",
    "https://openalex.org/W2768955070",
    "https://openalex.org/W2887597701",
    "https://openalex.org/W2889494142",
    "https://openalex.org/W2792249564",
    "https://openalex.org/W2757455114",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2520200494",
    "https://openalex.org/W3011200270",
    "https://openalex.org/W3186248524",
    "https://openalex.org/W4298619157",
    "https://openalex.org/W4309710444",
    "https://openalex.org/W3020736816",
    "https://openalex.org/W4307959636",
    "https://openalex.org/W4298143102",
    "https://openalex.org/W4284969145",
    "https://openalex.org/W4367186595",
    "https://openalex.org/W2979396152",
    "https://openalex.org/W4200478585",
    "https://openalex.org/W3044580098",
    "https://openalex.org/W3203911595",
    "https://openalex.org/W3181547280",
    "https://openalex.org/W4313653990",
    "https://openalex.org/W2737163017",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3173197792",
    "https://openalex.org/W3183859557",
    "https://openalex.org/W2490270993",
    "https://openalex.org/W2790508300",
    "https://openalex.org/W2899242765",
    "https://openalex.org/W2810123099",
    "https://openalex.org/W3177525997",
    "https://openalex.org/W2746826351",
    "https://openalex.org/W4247926870"
  ],
  "abstract": "Abstract In concrete structures, surface cracks are an important indicator for assessing the durability and serviceability of the structure. Existing convolutional neural networks for concrete crack identification are inefficient and computationally costly. Therefore, a new Cross Swin transformer-skip (CSW-S) is proposed to classify concrete cracks. The method is optimized by adding residual links to the existing Cross Swin transformer network and then trained and tested using a dataset with 17,000 images. The experimental results show that the improved CSW-S network has an extended range of extracted image features, which improves the accuracy of crack recognition. A detection accuracy of 96.92% is obtained using the trained CSW-S without pretraining. The improved transformer model has higher recognition efficiency and accuracy than the traditional transformer model and the classical CNN model.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports\nAn improved transformer‑based \nconcrete crack classification \nmethod\nGuanting Ye 1,2, Wei Dai 2, Jintai Tao 2, Jinsheng Qu 2, Lin Zhu 1 & Qiang Jin 1*\nIn concrete structures, surface cracks are an important indicator for assessing the durability and \nserviceability of the structure. Existing convolutional neural networks for concrete crack identification \nare inefficient and computationally costly. Therefore, a new Cross Swin transformer‑skip (CSW‑S) is \nproposed to classify concrete cracks. The method is optimized by adding residual links to the existing \nCross Swin transformer network and then trained and tested using a dataset with 17,000 images. The \nexperimental results show that the improved CSW‑S network has an extended range of extracted \nimage features, which improves the accuracy of crack recognition. A detection accuracy of 96.92% is \nobtained using the trained CSW‑S without pretraining. The improved transformer model has higher \nrecognition efficiency and accuracy than the traditional transformer model and the classical CNN \nmodel.\nKeywords Image feature extraction, Transformer, Deep learning, Structural health monitoring, Crack \ndetection\nConcrete is one of the most commonly used materials in civil engineering, and concrete structures are subjected \nto external loads, Such as live loads (overloading, vehicle impact, etc.) 1, natural environments (temperature \nchanges, humidity, acid rain, wind loads and ice loads, etc.)2, easily lead to fatigue effects and material degrada-\ntion. Cracks in concrete structures can affect the local or overall structural safety.\nCurrent crack detection of concrete structures is usually performed by manually capturing crack patterns and \nanalyzing them. This method has disadvantages such as low efficiency, high cost, high risk and unguaranteed \ndetection  accuracy3. Image processing techniques such as linear array scanning  camera4, RGB-D  sensor5, black \nbox  camera6,  accelerometer7, laser  scanner8,9, fuzzy clustering  method10, X-ray11, and fast  tomography12 have been \nwidely used to detect cracks in concrete structures. However, with the development of vision-based inspection \nequipment and image processing technology, computer vision has become the main method to detect cracks \nin concrete structures. In recent years, deep  learning13 has greatly contributed to the development of computer \nvision, and methods based on deep learning of convolutional neural networks have been widely used for concrete \ncrack detection. Deep learning based methods include: image recognition, target detection, image segmentation.\nThe classification-based approach lies in determining whether the image/image patch contains cracks.The \nmethod based on convolutional neural network (CNN) was first applied to crack  detection14. The application of \ncomputer vision in crack detection of concrete structures has attracted more and more experts’  attention15–18. \nLi et al. 19 proposed a unified, purely vision-based approach to detect and classify various typical defects. \nGopalakrishnan et al. 20 proposed different classifiers based on VGG-16 neural network model for automatic \nclassification of hot mix asphalt (HMA) and silicate cement concrete (PCC) images. The single layer trained \nclassifier based on pre-trained VGG-16 showed the best performance.\nBecause detection-based methods aim to generate bounding boxes containing cracks, existing object detection \nmethods, such as fast-RCNN,  YOLO21 and  SSD22, are often used for crack detection. Cha and Choi’s16 First Crack \nDetection Study Using Faster R-CNN. The faster RCNN is more accurate than the CNN and K-value models and \ncan accurately locate cracks. Huyan et al.23 proposed a novel architecture called cju-NET that outperforms tradi-\ntional image processing methods on test datasets. Ali et al.24 proposed an autonomous unmanned aerial vehicle \n(UAV) system integrated with an improved region-based faster convolutional neural network (Faster R-CNN) \nfor identifying various types of structural damage, and map detected damage in GPS-denied environments. \nMeng et al.25 proposed an automatic real-time crack detection method based on UAV . The results show that this \nmethod can significantly improve the MIoU value of crack edge detection and the accuracy of maximum crack \nwidth measurement by automatically approaching the vicinity of the crack under non-ideal shooting conditions. \nOPEN\n1College of Hydraulic and Civil Engineering, Xinjiang Agricultural University, Urumqi 830052, China. 2College of \nInternational Education, Xinjiang Agricultural University, Urumqi 830052, China. *email: tm-jinq@xjau.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nZeng et al.26 proposed a UAV damage detection task planning method based on Bayesian risk, which not only \nminimizes the UAV path length but also reduces the related SHM cost. Zhang et al.27 built a database containing \nmany road damage images (e.g., strip cracks, web cracks, potholes, and ruts) and trained an improved SSD-\nMobile Net deep neural network. They indicated that the improved method obtained a high average precision \n(AP).  Zhang28 used an unmanned aerial vehicle (UAV) road damage database to enhance the utilization of basic \nfeatures in Y ou Only Look Once Version 3 (YOLO v3). Adding MLAB between the backbone network and the \nfeature fusion part effectively increases the mAP value of the proposed network to 68.75%, while the accuracy \nof the original network is only 61.09%.  Zheng29 proposed an improved crack detection method of YOLO v4 \nlightweight visual model, which meets the real-time operation requirements without sacrificing accuracy. The \ndetection accuracy, recall rate and F1 score of this method are 93.96% respectively., 90.12% and 92%.  Qu30 pro-\nposed an improved first-level target detector based on the YOLOv5 method, which improved the problem of \nmissed detection and false detection of large targets, and had better detection results. Compared with the original \nmodel size YOLOv5_S, in PASCAL VOC and The mAP@ [0.5:0.95] on the MS COCO dataset is improved by \n4.4% and 1.4% respectively. Y e et al.31 proposed an improved YOLOv7 network, and the experimental results \nshow that the network not only can effectively detect crack images of different sizes, but also achieves satisfac -\ntory results in validating the robustness of noise contaminated images of different types and different intensities.\nSegmentation-based methods are pixel-level crack detection methods in which each pixel is categorized as \ncracked or uncracked. It obtains pixel-level positional information, which allows for more important characteri-\nzation information of the cracks to be obtained from the detection results. Lightweight real-time crack segmen-\ntation methods for cracks in complex background scenes have recently emerged. For example, Choi and  Cha32 \nproposed an original convolutional neural network. Compared with the latest model, this model has 88 times \nfewer parameters than the latest model, but it can still obtain better evaluation indicators. Additionally, the model \nprocesses 1025 × 512 pixel real-time (36 FPS) images 46 times faster than recently developed images. Kang and \n Cha33 proposed a new semantic transformer representation network (STRNet) to solve the problem of real-time \nsegmentation of pixel-level cracks in complex scenes. 1203 images were used for training, and further extensive \ncomprehensive enhancement was performed using 545 Test images (1280 × 720, 1024 × 512) are studied; its \nperformance is compared with recently developed advanced networks (Attention U-net, CrackSegNet, Deeplab \nV3+ , FPHBN, and Unet+ +), among which STRNet outperforms on the evaluation metrics Top performer—It \nachieved the fastest processing at 49.2 frames per second. Das and Leung et al. 34 proposed a new deep learning \nmodel called strain hardening Segmentation Network (SHSnet). The calculation accuracy of this model is 85%, \nthe required parameters are 1 order of magnitude less than the traditional model, and the calculation time is \nsaved > 95%. Zhang et al.35 proposed a crack identification method based on improved U-Net. To detect cracks \nmore accurately, they used the generalized die loss function. Zhu et al. 36 proposed a multi-objective detection \nalgorithm for pavement damage detection, which proved reasonable robustness in the classification and location \nof pavement damage. Das et al. 37. employed a deep convolutional neural network to detect and pinpoint thin \ncracks in strain-hardened cementitious composites (SHCC) under real-world conditions. The model was trained \non an NVIDIA 1060 6 GB GPU with a 227 × 227 × 3 image as the input size, and the training results showcased \nthe network’s robustness and superiority in handling variations in image quality, adaptation to new datasets, and \nhuman-like level reasoning capabilities. Furthermore, they proposed an image processing technique to extract \nthe crack density parameter from the outcomes of a custom deep convolutional neural network. Their findings \ndemonstrated the network’s ability to accurately classify thin cracks in SHCC despite the presence of interfering \nfactors such as sensors, markers, and inhomogeneous sample edges. Regardless of test configuration, structure \nsize or geometry, cracks can be controlled below 100 μm, up to a few percent of strain. Li and  Zhao38 introduced \na high-resolution concrete damage image synthesis method based on Conditional Generative Adversarial Net -\nwork (CGAN). Initially, they trained a low-resolution generator and GPU. They then utilized the low-resolution \ngenerator to synthesize a 512 × 1024 resolution low-resolution damage image, which was combined with the \ncorresponding 1024 × 2048 resolution semantic high-resolution map to generate a high-resolution damage image \nat 1024 × 2048 resolution. The results show that the synthesized images have good realism and can be used for \ntraining and testing of concrete damage detection networks based on deep learning. Omar and  Nehdi39 utilized \nUnmanned Aerial Vehicle (UAV) infrared thermography to detect concrete bridge decks. They achieved this \nby capturing thermal images using a high-resolution thermal imager during low altitude flights, analyzing the \nimages using a k-means clustering tech-nique, and subsequently segmenting the spliced images and identifying \nthe target thresholds. Their findings demonstrated that UAVs equipped with high-resolution thermal infrared \nimagery can effectively detect sub-surface anomalies in bridge decks without disrupting traffic flow, providing a \nvaluable tool for rapidly assessing bridge conditions and conserving maintenance funds.\nFrom the above research, it can be seen that classification-based, object detection and segmentation methods \nhave been widely used in crack detection. However, due to different conditions, cracks on the concrete surface \nhave unique characteristics, such as stains, impurities, etc. Different damages have different pixel ratios across \nthe entire image, in order for the network to have better generalization capabilities when dealing with different \nsizes, shapes and types of cracks. Therefore, researchers proposed the Transformer  architecture40. The trans-\nformer model employs a multi-head self-attention mechanism in order to obtain global connectivity between \neach pixel point and capture semantic associations over longer intervals. In order to improve efficiency while \nmaintaining high accuracy, Krichene et al.41 proposed a new architecture, DoT, a dual-transformer model that \njointly trains two transformers by optimizing task-specific losses. The results show that for accuracy For a small \ndecrease, DoT improves training and inference times by at least 50%.The Vision Transformer proposed by \nAlexey Dosovitskiy et al.42 tested the Vision Transformer on the dataset ImageNet-21 k with 21 k categories and \na total of 14 M images, and achieved better test results than ResNet. Therefore, it performs well in analyzing and \npredicting larger image data.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nVision Transformer is comparable to state-of-the-art convolutional networks and can be used to obtain similar \nfeatures at both shallow and deep layers. In the image classification task, the image is classified by dividing it into \nsmall chunks, then joining these chunks into a sequence, and finally feeding this sequence into the Transformer. \nThe advantage of Vision Transformer is that it can parallelise the training and has the ability to have global \ninformation. Nah et al.43 proposed the Swin transformer (Swin T), which reduces the complexity of operations \nthrough a technique that employs a local sliding window. Although global attention-based Transformers excel in \nperformance, they are typically high in complexity and computationally intensive. However, local attention-based \nTransformers limit the interactions of each token’s receptive field, thus slowing down the growth of the receptive \nfield. Therefore, when the resolution of the image is high, the data processing of ViT and SwinT will be low. Other \n researchers44 proposed a cross-window self-attention mechanism and devised the CSWin transformer (CSWT). \nIn the self-attention module CSwin, a cross-shaped window self-attention mechanism is proposed, which can \ncompute horizontal and vertical self-attention in parallel, thus obtaining better results with less computation. In \naddition, Locally Enhanced Position Encoding (LePE) can better handle local position information and support \narbitrary shape inputs.The following differences exist between CSwin and Swin:\n(1) Facet delineation method: Swin uses non-overlapping convolutions of size 4 × 4 (step = 4) to delineate \nfacets, while CSwin uses overlapping convolutions of size 7 × 7 (step = 4) to delineate facets.\n(2) Window division method: Swin uses square windows for division, while CSwin uses horizontal and \nvertical rectangular windows for division, which improves the sensory field to some extent.\n(3) Facet merging method: Swin uses interval extraction of elements and then concat for facet merging, \nwhile CSwin uses convolution of size 3 × 3 (stride = 2) instead of this method.\nDue to its unique structural design, it is effective in terms of detection speed, but CSW is not accurate \nenough in identifying cracks in concrete structures in complex situations. In this paper, to overcome the limita-\ntions of the above networks, a new network, CSWin transformer-skip (CSW-S), is proposed based on CSWin \ntransformer. CSW-S compensates for the slow detection and low accuracy of other network models. Cracks in \nconcrete structures can be detected using this image classification technique, and it is expected to achieve better \nperformance. The core structure of this paper is as follows. First, the model structure of CSW-S is described. \nNext, the details of the CSW-S dataset are provided, and the hyperparameter settings are listed. Then, the test \nresults of concrete crack images are analyzed using the trained CSW-S. Finally, the performance and future work \nof the method are discussed.\nThe architecture of the proposed CSW-S.\nBased on the research of He K. et al.45, the CSW-S architecture is redesigned, as shown in Fig. 1. This archi-\ntecture consists of three parts: a convolutional token embedding layer, a CSWin transformer block, and a con-\nvolutional layer. In the convolutional token embedding layer, a convolutional layer with overlap (step size 4, \nconvolutional kernel size 7 × 7) is used to transform the input into a patch token of size H/4*W/4.\nThe result is then fed into four identical CSWin transformer blocks, and the autonomy mechanism in the \nblocks is used to extract the information in the image. Then, the features are extracted. In the convolutional \nlayer, the image size is modified, the extracted feature maps is vectorized, and the values are mapped to (0,1) \nprobabilities to classify the image information. In addition, we modify the connectivity of the network structure \nby adding residual links to enable information from different layers to flow, effectively improving the network \nperformance. The details of the specific network structure are shown in Table 1.\nCross‑shaped window self‑attention\nTo expand the perceptual field of the transformer without computing global self-attention, the current main-\nstream practice is to compute self-attention for the transformer of local attention and then expand the perceptual \nfield by a shift window. However, the token in each self-attention block is still a limited self-attention region. \nMany blocks must be stacked to fuse more feature information and thus achieve a global perceptual field. This \nbrings the problem of a more significant number of parameters and more complex computational difficulty. In \nthe latest research in computer vision, cross-shaped window self-attention is used to solve this problem. Most \nnetworks, such as the Swin transformer, only use the self-attention mechanism, and only one small square of \ninformation can be extracted at a time in the face of image feature extraction. The cross-shaped window self-\nattention used in this model can be used to extract the image features within the horizontal or vertical stripes to \nexpand the attention range of tokens within a transformer’s block, as shown in Fig. 2. Each stripe is obtained by \nsplitting the input features into equal-width stripes. The effect of stripe width is mathematically analyzed, and \nthe stripe width is varied for different layers of the transformer network, enabling powerful modeling capabilities \nwhile limiting the computational cost.\nLocally‑enhanced positional encoding\nIn this paper, to compensate for the shortcomings of self-attention (SA), a new locally-enhanced positional \nencoding (LePE) is used. Unlike APE and CPE in vision transformer, in which location information is directly \nadded to the input token in self-attention and then sent into the vision transformer, in Swin transformer, RPE \nis used to embed location information directly into the correlation calculation of the transformer block. LePE \nis used to learn the location information of a value directly using deep convolution, which is then summed with \nresiduals and embedded it into the transformer block very conveniently. The position of the value is encoded \ndirectly, and the position information is fused into the SA after a matrix operation. Moreover, the depthwise \nconvolution of the values is used to adjust the size of the parameters and make the calculation easier.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nResidual connections\nIn engineering inspection, especially in some larger projects, the amount of data is often quite large. CSWin \ntransformer is a highly accurate target detection network due to its relatively highly complex structure. As the \ndepth of the network increases, gradient dissipation and gradient explosion may occur. Therefore, this study \naims to improve the performance and convergence of the neural network while using the network’s powerful \nability to extract the target features. Local low-level features in the shallow layer are retained to enhance the \nhigh-level features in the deep layer. The modified network in this paper can better fuse the information between \nthe shallow and deep layers and thus avoid spurious gradient explosion and gradient disappearance problems.\nTherefore, a skip connection is added to the original skeleton, as shown in Fig. 3. First, the features between \nstage 1 and stage 2, the features between stage 2 and stage 3, and the features of the layering stage 4 are fused. \nThe result is fused again with the three features in the first stage, the first two features in the second stage, and \ndirectly with the features in the first step of the third stage and then output. The use of several stages can better \ncapture long-range dependencies and global information. It also allows the convolutional layers to be stacked \nmore abstractly and the information to be more condensed.\nFigure 1.  Framework of CSwin Transformer-skip.\nTable 1.  CSW-S network structure details. Dim refers to the dimension of the fully connected network spread \ninto and number head the number of multi-headed attention heads. Each head is responsible for different \ncorrelations. The resolution is the size of the picture before it is spread into a vector, and depth is the number of \nrepetitions.\nStructure Input Convolution kernel Passage Step length\nConvolutional token enbedding 24 × 24 × 3 7 × 7 64 54\nStructure Input depth number head dim reso split-size\nStage1 3136 × 64 1 2 64 56 1\nStage2 3136 × 64 2 4 128 28 2\nStage3 3136 × 64 2 8 256 14 7\nStage4 3136 × 64 1 16 512 7 7\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nFigure 2.  Cross-window self-attention mechanism.\nFigure 3.  Residual linking process.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nClassification\nTo classify the input data, a softmax layer is essential. The softmax layer uses the softmax function given in \nEq. (1) to estimate the likelihood p(y(i) = j(i) | x(i);w) for each of the k classes j(i), where w is the weight, i is the \ni-th example of the n input examples, and W T\nK x(i) is the input to the softmax layer. The right-hand side of the \nequation is a k-dimensional vector that represents the k estimated likelihoods that 1/∑k\nj=1eW T\nJ X (i)\n to normalize \nthe probability distribution.\nTraining and performance evaluation\nData set\nIn addition, in the process of crack recognition, the training of neural network requires a sufficient number of \nimages, if the number of trained images is not enough, it may limit the application in practice. Y ang et al.46 used \n278 spalling and 954 crack images and adopted VGG-16 CNN architecture for training, and the classification \naccuracy of this method on the SCCS database reached 93.36%. Zou et al.47 built a DeepCrack network on Seg-\nNet’s encoder-decoder architecture and trained 3500 images with size of 512 × 512. achieves F-measure over 0.87 \non the three challenging datasets in average. Achieves f-measure over 0.87 on the three challenging datasets in \naverage. Silva and Lucena et al. 48 developed a deep learning crack detection model based on CNN, which was \ntrained using a dataset containing 3500 concrete surface images and obtained an accuracy of 92.27%. Therefore, \nwhile designing better network models, increasing the number of trained crack images will help to improve the \napplication of network models in practical engineering.\nIn this paper, a dataset was prepared using a multipurpose pavement inspection vehicle to detect cracks and \nacquire images. The database contains 8000 images of concrete pavement cracks of size 4000 4000. These images \nwere cropped to 224 pixels 224 pixels, and a dataset with 17,000 cropped cracked images was obtained. For faster \ntraining and testing of CSW-S, the cracks were numbered and manually labeled with crack contours to construct \nthe pavement crack detection dataset. The dataset was randomly divided into a training set of 11,900 images and \na test set of 5100 images at a ratio of 7:3.\nThe transformer model was used to complete the binary classification problem. The photos must be manually \nclassified into two categories (cracked and uncracked). In order to make the classification of the transformer \nmodel more accurate, the training set and the test set contain photos of various forms of cracks with impuri-\nties, dark spots, and graffiti as shown in Table 2. All these photos were used in the transformer model to extract \nand learn the characteristic disturbance information of cracks. The Transformer model uses the dataset shown \n(1)\np(y(i) = j(i) |x(i);W ) =\n\n\np(y(i) = 1 |x(i);W )\np(y(i) = 2 |x(i);W )\n...\np(y(i) = k |x(i);W )\n\n\n= 1\n�k\nj= 1 e\nW T\nj x(i)\n\n\neW T\n1 x(ϕ)\neW T\n2 x(i)\n...\neW T\nk x(i)\n\n\n,\ni= 1, ... ,N\nTable 2.  Types of images in the dataset.\nDataset Image size Training set Validation set Total number\nCrack data set 2242 11,900 5100 17,000\nLongitudina 2242 3566 1558 5124\nTransverse 2242 5962 2591 8553\nFatigue 2242 2372 951 3323\nFigure 4.  Visualized (a) blurred images, (b) clear images, (c) overexposed images.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nin Fig. 4, which we visualized. The CSW-S model is used to extract the crack features hidden in the photos to \nachieve crack classification.\nHyper‑parameter\nThe goal of training the CSWin transformer-skip was to increase the training data’s variability and avoid over -\nfitting. The dropout method was used between two fully connected layers to reduce overfitting by preventing \ncomplex adaptation to the training data. The output of each neuron was set to zero with a probability of 0.5. The \ngraphics processing unit (GPU) was used to accelerate the training of the CSWin transformer. Further accelera-\ntion was achieved by using a rectified linear unit (ReLU) as the activation function, which is more effective than \nthe hyperbolic tangent (tanh) and sigmoid functions used in traditional neuronal models for training and evalua-\ntion. The neural network was trained using the stochastic gradient descent (SGD) method, and the PyTorch deep \nlearning framework was used to train the CSWin transformer-skip. In training, the initial global learning rate was \nset to 1e–5, and the weight decay was set to 0.0005. The stochastic gradient descent (SGD) method was used to \nupdate the network parameters with a minimum batch size of 2 for each iteration. A total of 230 iterations were \nused to train the network. All experiments in this paper were performed using a single QuadroRTXA5000 GPU.\nTesting and discussions.\nTo evaluate the performance of the proposed model, we conducted tests on the dataset provided in Section. \n“Training and performance evaluation” and validated the network performance through three experiments: \ninvestigating the network hyperparameters, the dataset size, and comparing it with different network models, \nrespectively. This section briefly outlines the general flow of each method.\nEvaluation\nTo quantitatively assess the performance of the model, several evaluation factors commonly used in classification \ntasks were used. These include the true positive (TP), true negative (TN), false-positive (FP), and false-negative \n(FN). True positives are samples that were correctly classified as cracked, and true negatives are samples that \nwere correctly classified as uncracked. Similarly, false-positives are samples that were uncracked but incorrectly \nclassified as cracked by the network. False-negatives are cracked samples incorrectly classified as uncracked by \nthe network. Table 2 shows the network’s accuracy, precision, and completeness. Recall can be interpreted as the \npercentage of crack samples identified by the network to the total number of cracks in the dataset. Accuracy, on \nthe other hand, is the percentage of predicted cracks that are cracked. The accuracy, accuracy, and completeness \nare calculated as follows:\nTraining and validation\nDuring the training of CSW-S, the learning rate affects the verification accuracy and convergence speed. To select \nthe best base learning rate, the base learning rates used in this paper were set to 0.1, 0.05, 0.01, 0.005, 0.001, \n0.0005 and 0.0001. The CSW-S was trained for 124 epochs at different base learning rates, and validation was \nperformed once after each epoch. The recorded confirmation accuracies are shown in Fig.  5.\nAccording to Fig. 5, the training accuracy and convergence speed as a whole decreases as the base learning \nrate varies from 0.0001 to 0.1, and peaks at 0.0001. When the base learning rate is 0.0005, this will lead to an \nincrease in accuracy first and then remain at 45%, indicating that the training of CSW-S is non-converging. The \nkey finding from the training results is that choosing a small base learning rate within a certain range allows the \nCNN to converge faster during training and achieve higher verification accuracy.\nAs the highest accuracy of 95.9% was achieved after the 109th epoch training, the training result at a base \nlearning rate of 0.0001 was finally chosen as the image classifier.\n(2)Accuracy= TP + TN\nTP + TN + FP + FN\n(3)Presion= TP\nTP + FP\n(4)Recall= TP\nTP + TN\n(5)Speciﬁcity= TN\nTN + TP\n(6)F 1score= 2 × Presion ·Recall\nPresion + Recall\n(7)MIOU = 1\n2\n( TP\nTP + FN + FN + TN\nTN + FP+ FN\n)\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nEffect of dataset size on model performance\nTo investigate the effect of dataset size on model performance. There are 3 datasets A, B, C. 1000, 2000, 3000 \nimages were randomly selected from all the images respectively. To ensure comparability of the model results, \nthe ratio of the number of cracks to the background was kept consistent across the three datasets. As presented \nin Section. “The architecture of the proposed CSW-S” , the proposed model was trained and tested with three \ndifferent datasets by validating the proposed model. The results show that the Accuracy values of the model \nincrease continuously as the size of the dataset increases. This is shown in Fig. 6. The model using dataset C has \nthe highest curve for each defect detection and represents the best performance.\nFive network models were chosen to compare with the CSW-S network model: two classical convolutional \nneural networks, GoogLeNet and AlexNet, and three artificial neural networks using self-attention, vision trans-\nformer (ViT), Swin transformer (SwinT), and CSWin transformer (CSW). Table 3 show the results of the quan-\ntitative comparison between the different methods.\nViT networks have three parts: an embedding layer, a transformer encoder layer, and an MLP head layer. \nViT differs significantly from a convolutional neural network (CNN) in the way it processes features. The input \nimage is first divided into 16 × 16 patches and fed into the vision transformer network. Apart from the first layer \nembedding, there are no convolutional operations in ViT, and interactions between different locations are only \nFigure 5.  Training results for different learning rates.\nFigure 6.  Network training graphs for different dataset Model Comparison.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nachieved through the self-attention layer. CNNs have limitations in modeling global information due to limita-\ntions in the size of the convolutional kernel. ViT allows all locations in the image to interact globally through \nself-attention. ViT demonstrates that a transformer pretrained on a large dataset, without modifications for the \nvisual task, can perform similarly or even better than the best CNNs. The resulting experiments yielded perfor-\nmance metrics of 78.15%, 62%, and 73.94% for accuracy, recall, and F1 score, respectively.\nIn GoogLeNet, a multiscale fusion network structure, in which an inception structure is introduced to fuse \nfeature information at different scales, is proposed. A 1*1 convolutional kernel is also used for dimensionality \nreduction and mapping. The fully connected layer is discarded in favor of an average pooling layer, significantly \nreducing the model parameters. As a result, not only is the accuracy rate very high but the number of parameters \nis lower than in other convolutional neural networks, effectively preventing overfitting. The resulting experiments \nyielded performance metrics of 90.47%, 93.60%, and 90.75% for accuracy, recall, and F1 score, respectively.\nA crack detection method based on the convolutional neural network AlexNet has been carried out in many \nexperiments. AlexNet has an eight-layer structure. The first five layers are convolutional layers, and the last three \nlayers are fully connected layers. Compared to previous models, ResNet has deeper layers, pioneers the use of \nReLU as a nonlinear mapping function, and can be trained using multiple CPUs. This effectively solves the prob-\nlem of overfitting that other previous networks were prone to and achieves higher recognition accuracy. However, \nas with other earlier networks, when the values of the input model are very large or very small, saturation occurs, \ni.e., the gradient of neurons approaches 0, and therefore, the gradient disappearance problem occurs. Therefore, \nit is often difficult to process more complex image data. The resulting performance metrics for accuracy, recall, \nand F1 score were 94.92%, 97.17%, and 95.02%, respectively.\nThe Swin transformer draws on many design concepts and prior knowledge of convolutional neural networks. \nThe sequence length and computational complexity are by computing self-attention within a small window. In \naddition, a pooling-like operation called patch merging, where four adjacent small patches are combined into \none large patch to obtain features of different sizes, is proposed. The resulting experiments yielded performance \nmetrics of 96.15%, 96.94%, and 96.18% for accuracy, recall, and F1 score, respectively.\nTable 3 shows that compared with other network models, an accuracy of 96.92%, a precision of 98.65%, a \nspecificity of 98.70%, an F1 score of 96.85%, and an MIoU of 94.03% were achieved using the proposed network \nmodel, which was better than the other network models. However, the recall, which was 95.13%, was lower than \nthat of AlexNet (97.17%) and CSW (96.94%). In particular, the accuracy, precision, recall, specificity, F1 score, \nand MIoU improved by 1.26%, 1.3%, 1.25%, 1.25%, 1.27%, and 2.34%, respectively, using the proposed CSW-S \ncompared to CSW . The experimental results demonstrated the superior performance of the proposed CSW-S \nfor concrete crack classification and proved the effectiveness of the CSW-based improvements. However, in \nterms of detection speed, although there was a decrease in the FPS of the improved model, it had little impact \non practical inspection, as the detection accuracy is more important than the detection speed for the detection \nof cracks in concrete.\nConclusion\nThis paper presents a crack detection framework based on CSWin transformer for crack detection of concrete \npavement. The image dataset consists of 1000 images with 4000 × 4000 pixel resolution, cropped to 224 × 224 \npixels, resulting in 17,000 cracked cropped image datasets. In order to ensure the performance of the network, \nwe adjusted the network by designing different parameters and obtained the highest accuracy of 95.9%. In order \nto investigate the effect of data set size on model performance, we randomly selected 1000,2000,3000 images with \ncracks from the data set we made, and tested the proposed network model. It was proved that the accuracy value \nof our network increased with the increase of data set size. Because the methodology of this paper is intended to \nprovide quality assessment and monitoring of newly constructed concrete pavements, the photographs of cracks \nin the selected dataset were taken under conditions free of complex environmental factors (such as obstacles like \nleaves, rubbish, etc.). The method is only suitable for testing concrete pavements with surfaces free of obstruc -\ntions. It may not be able to provide accurate results in the case of complex pavement environments.\nIn the proposed CSW-S model, multiple fusion residual links are added, and compared with other networks, \nthe model has extremely high feature extraction capability on the target, while capturing remote dependen -\ncies and global information better. Using this method, the shallow and deep image information is better fused \nto produce more concentrated fracture characteristics and reduce model computational complexity, gradient \nexplosion, and gradient disappearance. Compared with the existing commonly used network framework, the \naccuracy of the model can reach 96.92%, the accuracy can reach 98.65%, and the F1 score can reach 96.85% \nwithout pre-training. Therefore, the model can be used as a new technology and method for crack identification \nTable 3.  Evaluation on crack detection. Significant values are in bold.\nMethods Accuracy Precsion Recall Specificity F1score MIOU\nViT 78.15% 91.59% 62% 94.31% 73.94% 63.50%\nGoogleNet 90.47% 88.08% 93.60% 87.33% 90.75% 82.58%\nAlexNet 94.92% 92.98% 97.17% 92.66% 95.02% 90.32%\nCSW 95.66% 97.35% 93.88% 97.45% 95.58% 91.69%\nSwinT 96.15% 95.44% 96.94% 95.37% 96.18% 92.59%\nCSW-S 96.92% 98.65% 95.13% 98.70% 96.85% 94.03%\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\nin the health monitoring of concrete pavement engineering. In future studies, more images of concrete damage \ntypes under different conditions will be added to the existing database to improve the adaptability and robustness \nof the proposed method, and comparative studies will be conducted.\nData availability\nThe datasets used and/or analysed during the current study available from the corresponding author on reason-\nable request.\nReceived: 16 April 2023; Accepted: 16 February 2024\nReferences\n 1. Chen, F ., Jahanshahi, M. R., Wu, R. & Joffe, C. A texture-Based video processing methodology using bayesian data fusion for \nautonomous crack detection on metallic surfaces. Comput. Aided Civ. Eng. 32, 271–287 (2017).\n 2. Golewski, G. L. The Phenomenon of cracking in cement concretes and reinforced concrete structures: The mechanism of cracks \nformation, causes of their initiation, types and places of occurrence, and methods of detection—A review. Buildings 13, 765 (2023).\n 3. Huang, J., Liu, W . & Sun, X. A pavement crack detection method combining 2D with 3D information based on dempster-shafer \ntheory. Comput. Aided Civ. Eng. 29, 299–313 (2014).\n 4. Gavilán, M. et al. Adaptive road crack detection system by pavement classification. Sensors 11, 9628–9657 (2011).\n 5. Jahanshahi, M. R., Jazizadeh, F ., Masri, S. F . & Becerik-Gerber, B. Unsupervised approach for autonomous pavement-defect detec-\ntion and quantification using an inexpensive depth sensor. J. Comput. Civ. Eng. 27, 743–754 (2013).\n 6. Jo, Y . & Ryu, S. Pothole detection system using a black-box camera. Sensors 15, 29316–29331 (2015).\n 7. Radopoulou, S. C. & Brilakis, I. Automated detection of multiple pavement defects. J. Comput. Civ. Eng. 31, 04016057 (2017).\n 8. Zhang, D. et al. Automatic pavement defect detection using 3D laser profiling technology. Autom. Constr. 96, 350–365 (2018).\n 9. Bursanescu, L., Bursanescu, M., Hamdi, M., Lardigue, A. & Paiement, D. Three-dimensional infrared laser vision system for road \nsurface features analysis. In Presented at the ROMOPTO 2000: Sixth Conference on Optics (ed. Vlad, V . I.) 801 https:// doi. org/ 10. \n1117/ 12. 432808 (Bucharest, Romania, 2001).\n 10. Abu-Mahfouz, I. & Banerjee, A. Crack detection and identification using vibration signals and fuzzy clustering. Proced. Comput. \nSci. 114, 266–274 (2017).\n 11. Suzuki, T. & Aoki, M. Damage Identification Of Cracked Concrete by X-Ray Computed Tomography Method (Korea Concrete Institute \nSeoul, 2010).\n 12. Das, A. K. & Leung, C. K. Fast tomography: A greedy, heuristic, mesh size–independent methodology for local velocity reconstruc-\ntion for AE waves in distance decaying environment in semi real-time. Struct. Health Monit. 21, 1555–1573 (2022).\n 13. Heaton, J. Ian Goodfellow, Y oshua Bengio, and Aaron Courville: Deep learning: The MIT Press, 2016, 800 pp, ISBN: 0262035618.. \nGenet. Progr. Evolv. Mach. 19, 305–307 (2018).\n 14. Zhang, L., Y ang, F ., Daniel Zhang, Y . & Zhu, Y . J. Road crack detection using deep convolutional neural network. In 2016 IEEE \nInternational Conference on Image Processing (ICIP) (eds Zhang, L. et al.) 3708–3712 (IEEE, 2016).\n 15. Cha, Y ., Choi, W . & Büyüköztürk, O. Deep learning-based crack damage detection using convolutional neural networks. Comput. \nAided Civ. Eng. 32, 361–378 (2017).\n 16. Cha, Y ., Choi, W ., Suh, G., Mahmoudkhani, S. & Büyüköztürk, O. Autonomous structural visual inspection using region-based \ndeep learning for detecting multiple damage types. Comput. Aided Civ. Eng. 33, 731–747 (2018).\n 17. Dorafshan, S., Thomas, R. J. & Maguire, M. Comparison of deep convolutional neural networks and edge detectors for image-based \ncrack detection in concrete. Constr. Build. Mater. 186, 1031–1045 (2018).\n 18. Y ang, X. et al. Automatic pixel-level crack detection and measurement using fully convolutional network. Comput. Aided Civ. Eng. \n33, 1090–1109 (2018).\n 19. Li, R., Yuan, Y ., Zhang, W . & Yuan, Y . Unified vision-based methodology for simultaneous concrete defect detection and geolo-\ncalization. Comput. Aided Civ. Eng. 33, 527–544 (2018).\n 20. Gopalakrishnan, K., Khaitan, S. K., Choudhary, A. & Agrawal, A. Deep convolutional neural networks with transfer learning for \ncomputer vision-based data-driven pavement distress detection. Constr. Build. Mater. 157, 322–330 (2017).\n 21. Redmon, J., Divvala, S., Girshick, R. & Farhadi, A. Y ou only look once: Unified, real-time object detection. In Proceedings of the \nIEEE conference on computer vision and pattern recognition 779–788 (2016).\n 22. Leibe, B., Matas, J., Sebe, N., Welling, M. (Eds.) 2016. In Proc. Computer Vision – ECCV 2016: 14th European Conference, Amster-\ndam, The Netherlands, October 11–14, 2016, Part I, Lecture Notes in Computer Science. https:// doi. org/ 10. 1007/ 978-3- 319- 46448-0 \n(Springer International Publishing, Cham, 2016)\n 23. Huyan, J., Li, W ., Tighe, S., Xu, Z. & Zhai, J. CrackU-net: A novel deep convolutional neural network for pixelwise pavement crack \ndetection. Struct. Control Health Monit. https:// doi. org/ 10. 1002/ stc. 2551 (2020).\n 24. Ali, R., Kang, D., Suh, G. & Cha, Y .-J. Real-time multiple damage mapping using autonomous UAV and deep faster region-based \nneural networks for GPS-denied structures. Autom. Constr. 130, 103831 (2021).\n 25. Meng, S., Gao, Z., Zhou, Y ., He, B. & Djerrad, A. Real-time automatic crack detection method based on drone. Comput. Aided Civ. \nEng. 38, 849–872 (2023).\n 26. Zeng, J., Wu, Z., Todd, M. D. & Hu, Z. Bayes risk-based mission planning of unmanned aerial vehicles for autonomous damage \ninspection. Mech. Syst. Signal Process. 187, 109958 (2023).\n 27. Zhang, K., Li, H., Wang, Z. & Zhao, X. Feature recognition and detection for road damage based on intelligent inspection terminal. \nIn Smart Structures and NDE for Industry 4.0, Smart Cities, and Energy Systems (eds Farhangdoust, S. & Meyendorf, N. G.) (SPIE, \n2020). https:// doi. org/ 10. 1117/ 12. 25583 95.\n 28. Zhang, Y . et al. Road damage detection using UAV images based on multi-level attention mechanism. Autom. Constr. 144, 104613 \n(2022).\n 29. Zhang, J., Qian, S. & Tan, C. Automated bridge crack detection method based on lightweight vision models. Complex Intell. Syst. \n9, 1639–1652 (2023).\n 30. Qu, Z., Gao, L., Wang, S., Yin, H. & Yi, T. An improved YOLOv5 method for large objects detection with multi-scale feature cross-\nlayer fusion network. Image Vis. Comput. 125, 104518 (2022).\n 31. Y e, G. et al. Autonomous surface crack identification of concrete structures based on the YOLOv7 algorithm. J. Build. Eng.  73, \n106688 (2023).\n 32. Choi, W . & Cha, Y .-J. SDDNet: Real-time crack segmentation. IEEE Trans. Ind. Electron. 67, 8016–8025 (2020).\n 33. Kang, D. H. & Cha, Y .-J. Efficient attention-based deep encoder and decoder for automatic crack segmentation. Struct. Health \nMonit. 21, 2190–2205 (2022).\n 34. Das, A. K. & Leung, C. K. Y . A novel deep learning model for end-to-end characterization of thin cracking in SHCCs. In Strain \nHardening Cementitious Composites (eds Kunieda, M. et al.) (Springer International Publishing, 2023).\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:6226  | https://doi.org/10.1038/s41598-024-54835-x\nwww.nature.com/scientificreports/\n 35. Zhang, L., Shen, J. & Zhu, B. A research on an improved Unet-based concrete crack detection algorithm. Struct. Health Monit. 20, \n1864–1879 (2021).\n 36. Zhu, J. et al. Pavement distress detection using convolutional neural networks with images captured via UAV . Autom. Constr. 133, \n103991 (2022).\n 37. Das, A. K., Leung, C. & K. Y . & Wan, K. T.,. Application of deep convolutional neural networks for automated and rapid identifica-\ntion and computation of crack statistics of thin cracks in strain hardening cementitious composites (SHCCs). Cem. Concr. Compos. \n122, 104159 (2021).\n 38. Li, S. & Zhao, X. High-resolution concrete damage image synthesis using conditional generative adversarial network. Autom. \nConstr. 147, 104739 (2023).\n 39. Omar, T. & Nehdi, M. L. Remote sensing of concrete bridge decks using unmanned aerial vehicle infrared thermography. Autom. \nConstr. 83, 360–371 (2017).\n 40. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Proc. Syst. 30 (2017).\n 41. Krichene, S., Mueller, T. & Eisenschlos, J. DoT: An efficient Double Transformer for NLP tasks with tables. In Findings of the \nAssociation for Computational Linguistics: ACL-IJCNLP 2021. 3273–3283 (2021).\n 42. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010. \n11929, 2020.\n 43. Nah, S., Kim, T. H. & Lee, K. M. Deep multi-scale convolutional neural network for dynamic scene deblurring. In 2017 IEEE \nConference on Computer Vision and Pattern Recognition (CVPR) (eds Nah, S. et al.) (IEEE, 2017).\n 44. Cai, Z. et al. A unified multi-scale deep convolutional neural network for fast object detection. In Proc. Computer Vision–ECCV \n2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Part IV 14. 354–370 (Springer International \nPublishing, 2016).\n 45. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision \nand Pattern Recognition (CVPR) (eds He, K. et al.) (IEEE, 2016).\n 46. Y ang, L. et al. A robotic system towards concrete structure spalling and crack database. In 2017 IEEE International Conference on \nRobotics and Biomimetics (ROBIO) (eds Y ang, L. et al.) (IEEE, 2017).\n 47. Zou, Q. et al. DeepCrack: Learning hierarchical convolutional features for crack detection. IEEE Trans. Image Process. 28, 1498–\n1512 (2019).\n 48. Silva, W . R. L. D. & Lucena, D. S. D. Concrete cracks detection based on deep learning image classification. In The 18th Interna-\ntional Conference on Experimental Mechanics (eds Silva, W . R. L. D. & Lucena, D. S. D.) 489 (MDPI, 2018). https:// doi. org/ 10. 3390/ \nICEM18- 05387.\nAuthor contributions\nSoftware, G. Y .; Writing—original draft, G. Y ..; Methodology, W .D.; Data curation, J.T, J.Q. and W .D.; Super-\nvision—review and editing, Q.J. and L.Z. All authors have read and agreed to the published version of the \nmanuscript.\nFunding\nNatural Science Foundation of Xinjiang Uygur Autonomous Region (grant no. 2023D01A33) and the 2022 Xin-\njiang Autonomous Region-level Grand Innovation Project for University Students (grant no. S2022107580107).\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 54835-x.\nCorrespondence and requests for materials should be addressed to Q.J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5705804228782654
    },
    {
      "name": "Transformer",
      "score": 0.5080673694610596
    },
    {
      "name": "Structural engineering",
      "score": 0.32757818698883057
    },
    {
      "name": "Engineering",
      "score": 0.1888751983642578
    },
    {
      "name": "Electrical engineering",
      "score": 0.08193349838256836
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166427055",
      "name": "Xinjiang Agricultural University",
      "country": "CN"
    }
  ]
}