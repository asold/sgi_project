{
  "title": "Evaluation of Prompt Engineering Strategies for Pharmacokinetic Data Analysis with the ChatGPT Large Language Model",
  "url": "https://openalex.org/W4387446063",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093026410",
      "name": "Euibeom Shin",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2154842653",
      "name": "Murali Ramanathan",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W570773479",
    "https://openalex.org/W4385786385",
    "https://openalex.org/W4360811379",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W2024771065",
    "https://openalex.org/W4310642600",
    "https://openalex.org/W4312129548",
    "https://openalex.org/W2582743722"
  ],
  "abstract": "Abstract Purpose To systematically assess the ChatGPT large language model on diverse tasks relevant to pharmacokinetic data analysis. Methods ChatGPT was evaluated with prototypical tasks related to report writing, code generation, non-compartmental analysis, and pharmacokinetic word problems. The writing task consisted of writing an introduction for this paper from a draft title. The coding tasks consisted of generating R code for semi-logarithmic graphing of concentration-time profiles and calculating area under the curve and area under the moment curve from time zero to infinity. Pharmacokinetics word problems on single intravenous, extravascular bolus, and multiple dosing were taken from a pharmacokinetics textbook. Chain-of-thought and problem separation were assessed as prompt engineering strategies when errors occurred. Results ChatGPT showed satisfactory performance on the report writing, code generation tasks and provided accurate information on the principles and methods underlying pharmacokinetic data analysis. However, ChatGPT had high error rates in numerical calculations involving exponential functions. The outputs generated by ChatGPT were not reproducible: the precise content of the output was variable albeit not necessarily erroneous for different instances of the same prompt. Incorporation of prompt engineering strategies reduced but did not eliminate errors in numerical calculations. Conclusions ChatGPT has the potential to become a powerful productivity tool for writing, knowledge encapsulation, and coding tasks in pharmacokinetic data analysis. The poor accuracy of ChatGPT in numerical calculations require resolution before it can be reliably used for PK and pharmacometrics data analysis.",
  "full_text": "Page 1/13\nEvaluation of Prompt Engineering Strategies for\nPharmacokinetic Data Analysis with the ChatGPT\nLarge Language Model\nEuibeom Shin \nUniversity at Buffalo, State University of New York\nMurali Ramanathan  (  murali@buffalo.edu )\nUniversity at Buffalo, State University of New York\nResearch Article\nKeywords: ChatGPT, Pharmacokinetics, Pharmacokinetics, Drug Development, PK/PD, Graphing, NCA,\nNon-compartmental analysis, Bioavailability\nPosted Date: October 9th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3364157/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Journal of Pharmacokinetics and\nPharmacodynamics on November 11th, 2023. See the published version at\nhttps://doi.org/10.1007/s10928-023-09892-6.\nPage 2/13\nAbstract\nPurpose\nTo systematically assess the ChatGPT large language model on diverse tasks relevant to\npharmacokinetic data analysis.\nMethods\nChatGPT was evaluated with prototypical tasks related to report writing, code generation, non-\ncompartmental analysis, and pharmacokinetic word problems. The writing task consisted of writing an\nintroduction for this paper from a draft title. The coding tasks consisted of generating R code for semi-\nlogarithmic graphing of concentration-time pro\u0000les and calculating area under the curve and area under\nthe moment curve from time zero to in\u0000nity. Pharmacokinetics word problems on single intravenous,\nextravascular bolus, and multiple dosing were taken from a pharmacokinetics textbook. Chain-of-thought\nand problem separation were assessed as prompt engineering strategies when errors occurred.\nResults\nChatGPT showed satisfactory performance on the report writing, code generation tasks and provided\naccurate information on the principles and methods underlying pharmacokinetic data analysis. However,\nChatGPT had high error rates in numerical calculations involving exponential functions. The outputs\ngenerated by ChatGPT were not reproducible: the precise content of the output was variable albeit not\nnecessarily erroneous for different instances of the same prompt. Incorporation of prompt engineering\nstrategies reduced but did not eliminate errors in numerical calculations.\nConclusions\nChatGPT has the potential to become a powerful productivity tool for writing, knowledge encapsulation,\nand coding tasks in pharmacokinetic data analysis. The poor accuracy of ChatGPT in numerical\ncalculations require resolution before it can be reliably used for PK and pharmacometrics data analysis.\nINTRODUCTION\nLarge language models (LLM) such as ChatGPT from OpenAI 1, Bard from Google 2, and others are an\nemerging arti\u0000cial intelligence technology that has engendered great public interest. LLM are deep\nlearning neural networks trained on a large body of text and other information that are capable of two-\nway interactions with users in a manner that approximates the complexity and nuance of human\nconversations.\nPage 3/13\nPharmacokinetics (PK) and pharmacodynamics (PK-PD) modeling is well established as an effective tool\nin the pre-clinical setting for understanding time courses of drug concentrations and effects, for inter-\nspecies scaling, and dose determination. Population modeling with non-linear mixed effects and\nBayesian methods extends PK-PD modeling. It is particularly useful for interpreting sparse clinical data\nand for clinical trial simulations to de\u0000ne the scope of drug concentration and effect variability 3, 4. Every\nnew innovator drug application submitted to the United States Food and Drug Administration contains\nPK-PD and population modeling data.\nPK-PD analyses requires individuals with specialized multi-disciplinary training and utilizes complex\nsoftware tools with steep learning curves 5, 6. The model development processes require high levels of\nhuman intervention, and the interpretation of results requires expertise and experience. There is a\nshortage of quali\u0000ed manpower for the PK-PD analyst workforce needs of industry, regulatory agencies,\nand academia 5, 6.\nWe reasoned that powerful capabilities of LLM might be potentially harnessed in PK-PD data analyses\nsettings in several ways, e.g., to facilitate self-learning of PK analysis concepts, for identifying the range\nof available software tools, to provide templates for coding, to conduct uncomplicated PK analyses, and\nfor report preparation. While LLM are reasonably facile at interpreting lay language inputs with a\nconversational or informal tone, their effectiveness and utility can be further enhanced using structured\nprompts and exemplars. This has led to research into “prompt engineering” strategies that that can guide\nthe underlying LLM into yielding results that more effective for speci\u0000c classes of tasks. Prompt\nengineering strategies such as chain-of-thought prompting, which provides the LLM a limited number of\nexamples containing an input, the chain of thought, and the correct output, improves the performance of\nLLM for solving diverse arithmetic word problems and symbolic reasoning problems 7.\nThe main goals of this study are to obtain proof-of-concept evidence for utilizing LLM for PK data\nanalysis and to serve as a primer that might motivate pharmaceutical scientists to further explore LLM\nfor more advanced modeling and simulation tasks.\nMETHODS\nChatGPT Methods\nThe default version of ChatGPT 4.0 2 was run at the chat.openai.com on a MacBook Air computer\nrunning macOS Ventura 13.5.1. Screenshots from individual ChatGPT runs were saved.\nAll ChatGPT experiments were replicated  ≥  3 times. For experiments involving numerical calculations, the\naccuracy rate was computed as the number of correct answers based on 10 replicate experiments.\nNumerical answers from ChatGPT were considered correct if they were within ± 0.5% of the true value;\nthis corresponds to the maximum percent error with an answer expressed to three signi\u0000cant \u0000gures.\nChatGPT history was cleared between experiments.\nPage 4/13\nAccuracy rates obtained with and without prompt engineering directives were compared. Speci\u0000c prompt\nengineering directives were introduced for queries that elicited high error rates. These included the\ninclusion of phrases such as “Reason it out before action”, “You are a good calculator”, “Provide the most\ndetailed responses”, breaking down multi-part problems into separate questions (i.e., asking one question\nat a time), and breaking down multi-part problems into separate questions combined with the directive\n“provide steps”.\nCase Study 1. Scienti\u0000c Report Generation\nIn the \u0000rst experiment, ChatGPT was tasked to generate an outline for a four-paragraph introduction\nsection of a research paper with a title “Evaluation of Prompt Engineering Strategies for PKPD Analysis\nwith ChatGPT”, a draft title that was among those considered for this paper. In the second experiment,\nChatGPT was prompted to write a four-paragraph introduction section with references for the research\npaper with appropriate references.\nCase Study 2. Software Code Generation\nBoth experiments in this Case Study used concentration-time data shown in Fig. 2A from Patient ID-1 in\nthe vignette in the Ubiquity software package 8.\nWe requested code from ChatGPT for creating a semi-logarithmic graph and the methods for computing\nthe area under the curve (AUC) and area under the moment curve (AUMC) from zero to in\u0000nity. No\nadditional prompt engineering was done, and chat history was cleared between experiments.\nAUC and AUMC calculations used the prompt “Provide R code to calculate the AUC and AUMC from time\n0 to in\u0000nity for a non-compartment analysis for the following concentration(ng/ml) vs. time(hour) pro\u0000le.\n[TIME_HR]-(1,4,8,24,72,168,336,504,671.9999)\n[C_ng_ml]-(9953.8813, 9704.5133, 9383.7171, 8223.5475, 5685.306, 3118.7764, 1673.885, 1215.2236,\n964.6353)”\nThe code generated by ChatGPT was run using the R statistical program 9 in the RStudio environment on\na MacBook Pro computer running macOS Catalina (10.15.7).\nCase Study 3. Pharmacokinetics Calculations Word\nProblems\nThe word problems were selected from the textbook Clinical Pharmacokinetics: Concepts and\nApplications, 3rd Edition, by Rowland and Tozer 10. The answers from ChatGPT were compared to the\nanswer key in the textbook. We assessed the capabilities of ChatGPT at solving word problems involving\nPK calculations related to intravenous (Chap. 3, Problem 7) and extravascular bolus Chap. 6, Problem 8)\ndosing, and multiple oral dosing.\nPage 5/13\nRESULTS\nCase Study 1. Scienti\u0000c Report Generation\nAs an LLM, ChatGPT is viewed as a potentially powerful tool to facilitate text generation and writing. In\nthis Case Study, ChatGPT was prompted to provide an outline for the Introduction section of a research\npaper based on a title and the Introduction section with references. Figure 1 shows screenshots of the\nChatGPT output from these two experiments, which were conducted no prompt engineering strategies\nother than clearly state the task.\nThe outline produced by ChatGPT is shown in Fig. 1A. The theme and purpose for each paragraph was\nclearly stated for each of the four paragraphs in the ChatGPT-generated outline. A bulleted list with the\nmain points to be made in each paragraph followed. These results with the outline were considered\npromising given that limited information, i.e., title only with the PKPD abbreviation, was provided to the\nLLM.\nThe Introduction section is shown in Fig. 1B. The generated Introduction section had four paragraphs and\nhewed to the outline in Fig. 1A. The ChatGPT-generated Introduction section was viewed generally\nconcordant with accepted writing standards in pharmaceutical science journals and contained two in-text\ncitations and a reference section.\nWe passed the ChatGPT-generated Introduction section to the Grammarly typing assistance program,\nwhich contains a plagiarism check feature. The ChatGPT output was reported as plagiarism free.\nThe main weaknesses of the ChatGPT-generated Introduction section were its short length and the limited\nnumber of references. The short length can be attributed to the prompt statement, which speci\u0000cally\nrequested only four paragraphs. The Introduction also lacked speci\u0000city regarding the PK-PD problems\nthat might bene\u0000t from the use of ChatGPT and detail regarding prompt engineering strategies that have\nbeen proposed.\nNonetheless, ChatGPT’s pro\u0000ciency in generating a plausible Introduction for a research manuscript with\nlimited title information and no prompt engineering suggests its potential value as a productivity tool in\nthe writing process.\nCase Study 2. Software Code Generation\nChatGPT can generate code for programming tasks in several languages including Python and R, which\nare increasingly used in PK-PD data analysis.\nFigure 2A summarizes the prompt and data entered for ChatGPT and the results for the semi-log\ngraphing. ChatGPT recommended the ggplot2 package 11 and generated R code for creating the graph.\nThe R code ran without errors and generated a line graph (Fig. 2B) titled “Semi-log plot of Concentration\nPage 6/13\nover Time,” with the x-axis and y-axis labeled Time (HR) and Concentration(ng/ml). The graph esthetics\nwere simple as the minimal theme was recommended in the code. We did not seek to obtain the best \u0000t\nregression line to an exponential equation for the data.\nAUC and AUMC calculations are invariably the \u0000rst steps in non-compartmental analysis (NCA) of PK\ndata. Some PK domain expertise is required for calculating the AUC and AUMC from time 0 to in\u0000nity\nsince parametric interpolation is used to obtain the area of the region between the last observation time\npoint and in\u0000nity.\nAs a \u0000rst step in our experiment, we investigated whether ChatGPT could provide a detailed description of\nthe methodology needed for calculating AUC and AUMC. The Results in Fig. 2C demonstrate that\nChatGPT description is appropriate and complete – the integration and the extrapolation procedures are\ncorrectly described using equations where necessary.\nSeveral issues were encountered when conducting AUC and AUMC analyses with concentration-time\npro\u0000le data from Patient ID-1. The ChatGPT-generated R code included useful comments on the\nprocedural steps and ran without syntax errors, but the output was highly variable between occasions\neven with the same prompt. Despite these variability issues, ChatGPT used appropriate methodology, e.g.,\nintegration with the trapezoidal rule, extrapolation from the last observed time point to in\u0000nity in every\ncase, which indicates that ChatGPT had generally accessed and identi\u0000ed the correct sources of\ninformation for NCA analyses. The accuracy of AUC calculations was 5/10 whereas accuracy of AUMC\ncalculations was 1/10. The elimination rate constant was wrongly calculated in 5/10 experiments, and\nthis was propagated to errors in AUC and AUMC values. In 3 of 10 experiments, ChatGPT assumed that\nthe last two observations were on the terminal phase, in 5 of 10 experiments ChatGPT assumed the last\nthree observations were on the terminal phase, and in one experiment, ChatGPT \u0000t an exponential\nfunction to all the data. In every experiment, the remainder of the R code for AUC calculations was correct\nin terms of trapezoidal rule and extrapolation from the last observed time point to in\u0000nity. The\ncorresponding R code for AUMC calculations was correct in 3/10 experiments with most of the errors\noccurring in the term that extrapolated AUMC from the last time point to in\u0000nity.\nThe results indicate that ChatGPT can generate R code for basic PK analysis tasks such as creating\ngraphs. In calculating metrics such as AUC and AUMC, ChatGPT is capable of correctly using the\ntrapezoidal rule but is susceptible to range of errors.\nCase Study 3. Pharmacokinetics Calculations Word\nProblems\nCase Study 3A. ChatGPT was posed a \u0000ve-part single intravenous (IV) bolus dosing PK problem requiring\ncalculation of a) volume of distribution, b) elimination half-life, c) total AUC, d) total clearance, e) plasma\nconcentration at 70 minutes after dose administration.\nPage 7/13\nAn 88% accuracy rate was achieved was achieved without prompt engineering (Table 1); 83% of the\nerrors occurred in part (e), which required exponential arithmetic and had an error rate of 50%.\nInterestingly, ChatGPT set up the exponential equation required for calculation correctly even for incorrect\nanswers. The time in minutes was correctly converted to hours to match the units of the elimination rate\nconstant of the exponential equation.\nPrompt engineering efforts were directed exclusively at part (e). The directive \"Reason it out before action\"\ndid not improve accuracy whereas, “You are an accurate calculator”, increased accuracy by 6%. When the\nquestion in part (e) was posed separately, ChatGPT accuracy increased from 50–70%. When further\nprompted to detail each calculation step, ChatGPT achieved 100% accuracy. However, when the entire\nquestion set was presented with the directive to detail each calculation step, accuracy decreased by 6%.\nThis suggests that ChatGPT accuracy improves if calculation problems are compartmentalized.\nCase Study 3B. ChatGPT was posed a seven-part PK problem requiring bioavailability calculations after a\nsingle subcutaneous or oral dose. Data on AUC, observed half-life and fraction excreted unchanged in\nurine following intravenous, subcutaneous, and oral dosing were provided (see Table 1).\nWithout any prompt engineering, the accuracy of 94.3% (4 errors in 70 experiments) was achieved\n(Table 1). With the prompt engineering directive \"Reason it out before action\", the accuracy improved to\n98.6% (1 error in 70 experiments). When each question in Case 3B was posed separately or with the\ndirective “You are an accurate calculator”, there were no errors (100% accuracy).\nCase Study 3C. This was a 3-part PK problem on multiple oral dosing given the concentration-time pro\u0000le\nfor a single oral dose.\nChatGPT yielded an accuracy of 80% (2 errors in each of 10 experiments) on parts a and b-1 without any\nprompt engineering. The accuracy increased to 100% with the directive “Provide each step of the\ncalculation”.\nChatGPT did not correctly solve part b-2, which required application of the superposition principle to\ncorrectly calculate the multiple dosing concentration pro\u0000les from the single oral dosing pro\u0000le, in any of\nthe 10 experiments. ChatGPT used the equations for a model with linear absorption and one-\ncompartment elimination and made errors in selecting the equation for steady-state trough\nconcentrations. The directive used in parts a and b-1 did not yield accuracy improvements. However, the\ndirective “Use the superposition principle. The multiple oral dosing pro\u0000le is the linear superposition of the\nsingle oral dose pro\u0000le at each dose and adjust concentrations based on the ratio of the multiple dose\npro\u0000le to the single dose pro\u0000le. Assume a steady state approximated happens after 4 doses” yielded an\naccuracy of 2/10. With an exemplar based on a correct response, the accuracy only improved to 4/10.\nDISCUSSION\nPage 8/13\nWe investigated the strengths and weaknesses of the ChatGPT large language model in PK/PD data\nanalysis and pharmacometrics. In our experimental design, we intentionally included diverse problems\nrepresentative of the breadth of conceptual learning, quantitative analyses, visualization, coding, and\nreport writing tasks required in the domain. We found that the scienti\u0000c information and analysis\nprocesses recommended by ChatGPT to be appropriate and accurate. There were challenges related to\nlack of determinism in the algorithm and arithmetic errors in the numerical calculations.\nCalculation word problems frequently require arithmetic, commonsense, and symbolic reasoning.\nEvidence suggests that LLM “struggle” with calculation word problems, which is somewhat unexpected\ngiven the satisfactory performance on tasks such as writing and code generation 12, 13. Interestingly,\nexperiments have shown that the accuracy of ChatGPT in calculations involving exponentiation involving\ndecimals, logarithmic and trigonometric function are only ~ 50% 12, 13. The lack of numerical accuracy\nwith exponentiation and logarithmic operations represents a signi\u0000cant limitation for PK and\npharmacometric analyses. We encountered these issues in solving the AUMC and textbook PK\ncalculation problems where we obtained an accuracy of ~ 50% for the calculations involving\nexponentials. Interestingly, in nearly every experiment with the wrong answers, ChatGPT had the correct\nmathematical expression but the wrong numerical calculations. We expect that these issues might fully\nresolve once LLM incorporate a calculator in the algorithm.\nThe outputs from ChatGPT can vary even when the same prompt is used, which makes it di\u0000cult to\nreplicate any particular outcome in exact detail. In stochastic modeling, the lack of determinism is usually\naddressed by seeding the random number generator. It is not clear that an analogous strategy for\nresolving this problem is available in this setting because LLM and other generative AI algorithms\nproduce random variates from a complex high dimensional joint distribution 14, 15. This limitation is\nproblematic in the regulatory setting where reproducibility of results is critical.\nPrompt engineering is an approach to transfer knowledge from the user to the LLM 7 and has been\nshown to improve the performance on arithmetic word problems 16: e.g., prompting ChatGPT to show its\nwork 17, or adding the chain-of-thought phrase “Let’s think step-by-step” improved the accuracy of GPT-3\non arithmetic problems 18. We attempted chain-of-thought and problem separation prompt engineering\ntechniques in the cases that we encountered high error rates. While ChatGPT accuracy improved with\nchain-of-thought prompting on arithmetic problems, we were not able to eliminate numerical calculations\nerrors. This is concordant with the \u0000ndings of Chen et al. 18 who did not \u0000nd performance improvement\nfor ChatGPT on arithmetic reasoning problems but noted utility in other problem areas. This was\nattributed to incorporation of chain-of-thought features into ChatGPT 18.\nGiven that ChatGPT is an emerging AI tool, there have not been many papers that have speci\u0000cally\ninvestigated its utility in PK analyses and pharmacometrics. One exception is the research by Cloesmeijer\net al. 19 who investigated usefulness of ChatGPT for code generation; the R code generated was\nsatisfactory but the NONMEM code contained errors. We investigated R code generation in Case Study 2\nPage 9/13\nbut did not investigate NONMEM code here. We also did not conduct calculations involving population\nPK modeling given the high error rates in the AUC and AUMC calculations. While we found that the R code\ngenerated ran without issues in RStudio, thorough reviewing and debugging for conceptual errors was\nrequired. It might be useful to evaluate ChatGPT code with a test bank of questions with known answers.\nOur results suggest that ChatGPT could be a useful productivity aid for writing, knowledge encapsulation,\nand programming tasks. However, the high rate of errors in arithmetic calculations could limit its utility\nfor more complex tasks and diverse data analysis scenarios in PK data analysis and pharmacometrics. If\nthe limitations are overcome, ChatGPT could become a valuable tool for automating all aspects of PK\nand pharmacometric data analysis. Prompt protocol research and rigorous performance evaluation, and\nvalidation studies on benchmark data sets will be required to build the con\u0000dence and certainty expected\nin the pharmaceutical regulatory environment.\nDeclarations\nFUNDING INFORMATION\nThis is unfunded research. Support from Grant MS190096 from the Department of Defense Multiple\nSclerosis Research Program for the O\u0000ce of the Congressionally Directed Medical Research Programs\n(CDMRP) to the Ramanathan laboratory is gratefully acknowledged.\nCONFLICT OF INTEREST DISCLOSURE\nEuibeom Shin has no con\u0000icts. \nDr. Murali Ramanathan received research funding from the National Multiple Sclerosis Society,\nDepartment of Defense, National Science Foundation, and National Institute of Neurological Diseases\nand Stroke. He receives royalty from a self-published textbook.\nAUTHOR CONTRIBUTIONS\nEuibeom Shin – Data analysis, manuscript preparation.\nMurali Ramanathan – Study concept and design, data analysis, manuscript preparation.\nFinancial Con\u0000icts: See disclosure statement. \nCon\u0000dentiality: Use of the information in this manuscript for commercial, non-commercial, research or\npurposes other than peer review not permitted prior to publication without expressed written permission\nof the author. \nReferences\n1. OpenAI (2023) ChatGPT (June 26 version) Large language model.\nPage 10/13\n2. Google AI (2023) Bard Large language model.\n3. Kimko HC, Duffull SB (2003) Simulation for designing clinical trials: a pharmacokinetic-\npharmacodynamic modeling perspective. New York: Marcel Dekker; xviii, 396 p. p\n4. Kimko HC, Peck CC, American Association of Pharmaceutical Scientists (2011). Clinical trial\nsimulations: applications and trends. New York: AAPS Press : Springer; xvi, 538 p. p\n5. Bonate PL, Barrett JS, Ait-Oudhia S, Brundage R, Corrigan B, Duffull S, Gastonguay M, Karlsson MO,\nKijima S, Krause A, Lovern M, Neely M, Ouellet D, Plan EL, Rao GG, Standing J, Wilkins J, Zhu H\n(2023) Training the next generation of pharmacometric modelers: a multisector perspective. J\nPharmacokinet Pharmacodyn. Epub 2023/08/13. https://www.ncbi.nlm.nih.gov/pubmed/37573528\n\u0000. Michelet R, Aulin LBS, Borghardt JM, Costa TD, Denti P, Ibarra M, Ma G, Meibohm B, Pillai GC,\nSchmidt S, Hennig S, Kloft C (2023) Barriers to global pharmacometrics: educational challenges and\nopportunities across the globe. CPT Pharmacometrics Syst Pharmacol 12(6):743–747 Epub\n2023/03/25. https://www.ncbi.nlm.nih.gov/pubmed/36960632\n7. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-Smith J, Schmidt DC A\nprompt pattern catalog to enhance prompt engineering with ChatGPT. arxiv 2023;cs SE 2302:11382.\nhttps://arxiv.org/abs/2302.11382\n\u0000. Harrold JM, Abraham AK (2014) Ubiquity: a framework for physiological/mechanism-based\npharmacokinetic/pharmacodynamic model development and deployment. J Pharmacokinet\nPharmacodyn 41(2):141–151 Epub 2014/03/13. https://www.ncbi.nlm.nih.gov/pubmed/24619141\n9. R Core Team (2017) R: A language and environment for statistical computing. R Foundation for\nStatistical Computing, Vienna, Austria\n10. Rowland M, Tozer TN (1995) Clinical pharmacokinetics: concepts and applications. 3rd ed.\nBaltimore: Williams & Wilkins; xiv, 601 p. p\n11. Wickham H (2009) ggplot2: Elegant Graphics for Data Analysis. Use R. :1-212.\n://WOS:000269437100014.\n12. Frieder S, Pinchetti L, Chevalier A, Gri\u0000ths R-R, Salvatori T, Lukasiewicz T, Petersen PC, Berner J\nMathematical capabilities of ChatGPT. arXiv. 2023:arXiv:2301.13867v2.\nhttps://arxiv.org/abs/2301.13867\n13. Yuan Z, Yuan H, Tan C, Wang W, Huang S (2023) How well do large language models perform in\narithmetic tasks? arXiv. :arXiv:2304.02015. https://arxiv.org/abs/2304.02015\n14. Nair R, Mohan DD, Frank S, Setlur S, Govindaraju V, Ramanathan M (2023) Generative adversarial\nnetworks for modelling clinical biomarker pro\u0000les with race/ethnicity. Br J Clin Pharmacol\n89(5):1588–1600 Epub 2022/12/03. https://www.ncbi.nlm.nih.gov/pubmed/36460305\n15. Nair R, Mohan DD, Setlur S, Govindaraju V, Ramanathan M (2023) Generative models for age,\nrace/ethnicity, and disease state dependence of physiological determinants of drug dosing. J\nPharmacokinet Pharmacodyn 50(2):111–122 Epub 2022/12/25.\nhttps://www.ncbi.nlm.nih.gov/pubmed/36565395\nPage 11/13\n1\u0000. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi EH, Le QV, Zhou D (eds) (2022) Chain-of-\nthought prompting elicits reasoning in large language models. 36th Conference on Neural\nInformation Processing Systems (NeurIPS 2022); ; New Orleans, LA: NeuroIPS Foundation\n17. Shakarian P, Koyyalamudi A, Ngu N, Mareedu L An independent evaluation of ChatGPT on\nmathematical word problems (MWP). arXiv. 2023:arXiv:2302.13814v2.\nhttps://arxiv.org/abs/2302.13814\n1\u0000. Chen J, Chen L, Huang H, Zhou T (2023) When do you need chain-of-thought prompting for\nChatGPT? arXiv. : arXiv:2304.03262v2. https://arxiv.org/abs/2301.13867\n19. Cloesmeijer M, Janssen A, Koopman S, Cnossen M, Mathot R (2023) ChatGPT in pharmacometrics?\nPotential opportunities and limitations. Authorea.\nTable\nTable 1 is available in the Supplementary Files section.\nFigures\nFigure 1\nPage 12/13\nResults from Case Study 1 evaluating the text generation capabilities of ChatGPT. Figure 1A summarizes\nthe ChatGPT prompt and output when tasked to provide an outline for the Introduction section for a\nresearch manuscript with a draft version of the title for this paper. Figure 1B summarizes the\ncorresponding ChatGPT output when it was tasked to write a complete Introduction section with\nreferences for a research manuscript with the same title as this paper. The white-on-black screenshots\nfrom ChatGPT were recolored to improve contrast.\nFigure 2\nResults from Case Study 2 evaluating the code generation capabilities of ChatGPT and to obtain\ninformation regarding area under the curve (AUC) and area under the moment curve (AUMC) calculations.\nPage 13/13\nFigure 2A summarizes the ChatGPT prompt and output when tasked to provide R code for a semi-\nlogarithmic graph for a concentration-time data set. Figure 2B shows the graph generated when the code\nin Figure 2A was executed. Figure 2C summarizes the ChatGPT output when it was tasked to provide an\noutline of the methods used to compute AUC and AUMC from time zero to in\u0000nity. The white-on-black\nscreenshots from ChatGPT were recolored to improve contrast.\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nTABLE1.docx",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6995711922645569
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.5248924493789673
    },
    {
      "name": "Pharmacokinetics",
      "score": 0.519157350063324
    },
    {
      "name": "Logarithm",
      "score": 0.5182971358299255
    },
    {
      "name": "Word (group theory)",
      "score": 0.4225425720214844
    },
    {
      "name": "Mathematics",
      "score": 0.18521973490715027
    },
    {
      "name": "Statistics",
      "score": 0.15783190727233887
    },
    {
      "name": "Bioinformatics",
      "score": 0.08861562609672546
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}