{
  "title": "Can Large Language Models Predict Antimicrobial Peptide Activity and Toxicity?",
  "url": "https://openalex.org/W4392709001",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4288158428",
      "name": "Markus Orsi",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A4223443570",
      "name": "Jean-Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896127999",
    "https://openalex.org/W3040996852",
    "https://openalex.org/W3007285729",
    "https://openalex.org/W2906960763",
    "https://openalex.org/W3125778162",
    "https://openalex.org/W2784920021",
    "https://openalex.org/W2791848964",
    "https://openalex.org/W2998197489",
    "https://openalex.org/W3091899249",
    "https://openalex.org/W3025501158",
    "https://openalex.org/W3137064251",
    "https://openalex.org/W4288036349",
    "https://openalex.org/W4378212018",
    "https://openalex.org/W4366823167",
    "https://openalex.org/W4392162030",
    "https://openalex.org/W3039894784",
    "https://openalex.org/W3009548091",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4391561379",
    "https://openalex.org/W4387560715",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W4390012432",
    "https://openalex.org/W2076860935",
    "https://openalex.org/W2898320067",
    "https://openalex.org/W1971024387",
    "https://openalex.org/W3035302862",
    "https://openalex.org/W3118695441",
    "https://openalex.org/W3089139172",
    "https://openalex.org/W3206426065",
    "https://openalex.org/W3008588639",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2884267293",
    "https://openalex.org/W4392709001",
    "https://openalex.org/W4280563407",
    "https://openalex.org/W4396723768",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4391972484",
    "https://openalex.org/W4389991792",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W2948261638"
  ],
  "abstract": "Antimicrobial peptides (AMPs) are naturally occurring or designed peptides up to a few tens of amino acids which may help address the antimicrobial resistance crisis. However, their clinical development is limited by toxicity to human cells, a parameter which is very difficult to control. Given the similarity between peptide sequences and words, large language models (LLMs) might be able to predict AMP activity and toxicity. To test this hypothesis, we fine-tuned LLMs using data from the Database of Antimicrobial Activity and Structure of Peptides (DBAASP). GPT-3 performed well but not reproducibly for activity prediction and hemolysis, taken as a proxy for toxicity. The later GPT-3.5 performed more poorly and was surpassed by recurrent neural networks (RNN) trained on sequence-activity data or support vector machines (SVM) trained on MAP4C molecular fingerprint-activity data. These simpler models are therefore recommended, although the rapid evolution of LLMs warrants future re-evaluation of their prediction abilities.",
  "full_text": "1 \n \nCan Large Language Models Predict Antimicrobial \nPeptide Activity and Toxicity? \n \nMarkus Orsi,a and Jean-Louis Reymonda* \na) Department of Chemistry, Biochemistry and Pharmaceutical Sciences, University of Bern, \nFreiestrasse 3, 3012 Bern, Switzerland \ne-mail: jean-louis.reymond@unibe.ch \n \nAbstract \nAntimicrobial peptides (AMPs) are naturally occurring or designed peptides up to a few tens of \namino acids which may help address the antimicrobial resistance crisis. However, their clinical \ndevelopment is limited by toxicity to human cells, a parameter which is very difficult to control. \nGiven the similarity between peptide sequences and words, large language models (LLMs) might be \nable to predict AMP activity and toxicity. To test this hypothesis, we fine-tuned LLMs using data \nfrom the Database of Antimicrobial Activity and Structure of Peptides (DBAASP). GPT-3 \nperformed well but not reproducibly for activity prediction and hemolysis, taken as a proxy for \ntoxicity. The later GPT-3.5 performed more poorly and was surpassed by recurrent neural networks \n(RNN) trained on sequence-activity data or support vector machines (SVM) trained on MAP4C \nmolecular fingerprint-activity data. These simpler models are therefore recommended, although the \nrapid evolution of LLMs warrants future re-evaluation of their prediction abilities.  \nKeywords: large language models, LLM, GPT, hemolysis, activity prediction, antimicrobial \npeptides \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n2 \n \nIntroduction \n \nAntimicrobial peptides (AMPs) have gained significant attention in the field of drug discovery due \nto their potential therapeutic applications in the fight against antimicrobial resistance.1â€“3 However, \nthe vast number of possible peptide sequences and their complex structure-activity relationship \nlandscape mean that it is difficult to rationally design peptides with the desired biological activity, \nin particular tuning their activity versus toxicity to human cells, which is often measured as \nhemolysis of human red blood cells.4,5  \nTo address this issue, several machine-learning models have been developed for the de novo \ndesign of antimicrobial peptides.6â€“21 Because property prediction from a peptide sequence can be \nframed as a natural language processing problem, many of these models use architectures \nspecifically designed for language processing tasks.22â€“24 Furthermore, the emergence of large \nlanguage models (LLMs), such as OpenAIâ€™s GPT models,25 has opened new possibilities for \nleveraging powerful language processing capabilities in drug discovery applications. Recent \nattempts by Jablonka et al. to explore the capabilities of GPT-3 for predicting properties of small \nmolecules in various applications have shown that GPT-3 was able to perform comparably or even \noutperform conventional statistical models, particularly in the low data regime.26 There also have \nbeen successful efforts into augmenting LLM capabilities to tackle tasks related to small molecule \nchemistry in the areas of organic synthesis, drug discovery, and materials design.27â€“30 Hereby, the \nmodels mainly orchestrate a set of tools to solve chemistry tasks starting from a natural language \nprompt.31â€“33 However, to the best of our knowledge LLMs have not been implemented to predict \nthe bioactivity of peptides yet.   \nIn this study, we aimed to compare GPT models fine-tuned on antimicrobial peptide \nsequence data with models that have been previously used to predict antimicrobial activity and \nhemolysis of peptide sequences.13,14 Alongside evaluating the performance of the fine-tuned GPT \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n3 \n \nmodels, we also seek to explore the advantages and disadvantages they offer in terms of time and \ncost effectiveness. Furthermore, we compare the performance of models trained on amino acid \nsequences to a support-vector machine (SVM) trained on the MAP4C fingerprint.34  \nMethods \nDatasets \nThe datasets used in this study were peptide sequences with annotated antimicrobial and hemolytic \nactivity collected from the Database of Antimicrobial Activity and Structure of Peptides \n(DBAASP).13,35 The dataset used for the classification tasks contained 9,548 (7,160 training / 2,388 \nvalidation) sequences with annotated antimicrobial activity, of which 2,262 (1,723 training / 539 \nvalidation) sequences had additional hemolytic activity annotations. To test models in low data \nregimes, we randomly selected subsets from the original training sets, representing approximately \n20% and 2% of the original activity set, and approximately 10% of the original hemolysis set. All \ndatasets are further described in Table 1.  To ensure consistency, we maintained the same training \nand test split for all initial evaluations. For the detailed study, we used the same 5-fold cross-\nvalidation sets.  \nTable 1 : Sizes and composition of the datasets used in the present study. Datasets are available at \nhttps://github.com/reymond-group/LLM_classifier. \nName Size # Actives / Not Hemolytic # Inactives / Hemolytic \nActivity Training 7,160 3,580 3,580 \nActivity Training 20% 1,400 701 699 \nActivity Training 2% 140 74 66 \nActivity Validation 2,388 1,194 1,194 \nHemolysis Training 1,723 717 1,006 \nHemolysis Training 10% 170 65 105 \nHemolysis Validation 539 226 313 \n \n \n \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n4 \n \nModels \nAs reference models, we used our previously reported NaÃ¯ve Bayes (NB), Support Vector Machine \n(SVM), Random Forest (RF), and Recurrent Neural Network (RNN) classifiers trained on the same \ndata.13 We furthermore trained two additional SVM models on alternative representations of peptide \nsequences: one utilizing the MAP4C fingerprint34 with a custom Jaccard kernel, and another using \npredicted fraction of helical residues and hydrophobic moment with a linear kernel. Fraction of \nhelical residues were predicted using SPIDER3.36 Hydrophobic moment was computed using the \nmethod of Eisenberg et al.37 \nTo explore the potential of GPT-3 models for antimicrobial and hemolytic activity \nclassification, we performed fine-tuning of the Ada, Babbage, and Curie models which were \naccessible through the OpenAI API (v0.28.0, accessed between 25.05.2023 and 01.06.2023). The \nfine-tuning process involved training each model using the full, 20% and 2% sets for activity \nclassification and the full and 10% set for the hemolysis classification. In the later evaluation with \nthe more advanced LLM GPT-3.5 Turbo, fine-tuning was also performed via OpenAI's Python API \n(v1.11.1), following the provided guidelines, but we restricted ourselves to the full model. The \nutilized fine-tuning datasets contained a system role (\"predicting antimicrobial activity/hemolysis \nfrom an amino acid sequence\"), a user message (peptide sequence formatted as \"SEQUENCE ->\"), \nand a system message (\"0\" for negative labels and \"1\" for positive labels).  \nMetrics \nAll models were evaluated using five commonly accepted performance metrics: ROC AUC, \nAccuracy, Precision, Recall and F1. Metrics were either calculated using the scikit-learn (v1.4.0) \nPython (v3.12.1) package (reference models and GPT-3.5) or directly obtained from the OpenAI \nplatform after fine-tuning was completed (for all GPT-3 models). \nROC AUC (Receiver Operating Characteristic Area Under the Curve:  The ROC AUC \nmeasures the area under the Receiver Operating Characteristic curve, which plots the True Positive \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n5 \n \nRate (Sensitivity) against the False Positive Rate. A higher ROC AUC value (ranging from 0 to 1) \nindicates better discrimination and predictive performance of the model. \nAccuracy: Accuracy measures the overall correctness of the model's predictions, calculating \nthe ratio of correctly classified instances to the total number of instances. It provides a general \nunderstanding of the model's performance but can be misleading in imbalanced datasets. \nğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = ğ‘‡ğ‘ƒ +ğ‘‡ğ‘\nğ‘‡ğ‘ƒ +ğ¹ğ‘+ğ‘‡ğ‘ +ğ¹ğ‘ƒ \nPrecision: Precision measures the proportion of true positives out of all predicted positives. \nIt focuses on the model's ability to avoid false positives. \nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ \nRecall: Recall measures the proportion of true positives out of all actual positives. It \nrepresents the model's ability to identify positive instances accurately.  \nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = ğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ \nF1 score: F1 is the harmonic mean of precision and recall. It provides a balanced measure \nthat considers both precision and recall. \nğ¹1 = 2âˆ—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™  \n \n \n  \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n6 \n \nResults and Discussion \n \nModel screening \nStarting from the DBAASP dataset of 9,548 peptide sequences annotated with antibacterial activity \nand 2,262 peptide sequences annotated with hemolysis effect, we had previously evaluated NB, RF, \nSVM and RNN models, and found the latter to perform best for predicting both activity and \nhemolysis from sequence data.13,14 For additional reference, we trained an SVM on the fraction of \nhelical residues and the hydrophobic moment, two properties commonly known to correlate with \nantimicrobial activity, as well as another SVM on MAP4C, a molecular fingerprint that can reliably \nencode large molecules such as natural products and peptides including their chirality,34 a parameter \nwhich we considered important since our data listed sequences containing both L- and D-amino \nacids. \n Aiming to test how LLMs perform in predicting antimicrobial activity and hemolysis, we \nfirst fine-tuned and evaluated GPT-3 Ada, Babbage, and Curie models. As discussed in our preprint, \nthese models performed slightly better than the reference models, and even provided good \nperformances when trained in low data regime (20% and 2% of full data). However, these models \nwere later deprecated by OpenAI and their performance cannot be reproduced. We therefore discuss \nherein only the results obtained with the more recent GPT-3.5 model, in comparison with the \nreference models.  \nFor both, prediction of antimicrobial activity and prediction of hemolysis, the top-\nperforming models were the MAP4C SVM and the RNN model trained on sequence data, the latter \nbeing the best performer in our original work (Table 2).13 The performances for both models were \nin a similar range, although the RNN displayed a notably higher ROC-AUC in both tasks. GPT-3.5 \ndisplayed the highest recall performance among the activity models, indicative of the model's \ntendency to overly favor positive predictions, potentially leading to increased false positive \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n7 \n \npredictions. On the other hand, the features SVM trained only on helicity and hydrophobic moment \ndid not perform significantly above background, and was later used as a negative control model.  \nTable 2. Performance metrics of all models tested on antimicrobial activity and hemolysis classification. The best value \nfor each metric is highlighted in bold.  \nModel ROC AUC Accuracy Precision Recall F1 \nNB act. 0.55 0.55 0.59 0.32 0.42 \nRF act. 0.81 0.71 0.7 0.75 0.73 \nSVM act. 0.75 0.68 0.68 0.68 0.68 \nRNN act. 0.84 0.76 0.74 0.8 0.77 \nFeatures SVM act. 0.65 0.65 0.66 0.62 0.64 \nMAP4C SVM act. 0.8 0.8 0.79 0.83 0.8 \nGPT-3.5 Turbo act. 0.68 0.68 0.62 0.93 0.75 \nNB hem. 0.58 0.56 0.48 0.76 0.59 \nRF hem. 0.8 0.77 0.81 0.6 0.69 \nSVM hem. 0.69 0.73 0.72 0.58 0.65 \nRNN hem. 0.87 0.76 0.7 0.76 0.73 \nFeatures SVM hem. 0.62 0.63 0.57 0.5 0.54 \nMAP4C SVM hem. 0.83 0.83 0.76 0.85 0.8 \nGPT-3.5 Turbo hem. 0.65 0.69 0.72 0.43 0.54 \n \n \nModel comparison \nFollowing the initial model screening, we aimed to validate our findings through a more robust \napproach: a 5-fold cross-validation involving GPT-3.5, the MAP4C SVM, the RNN, and finally the \nfeatures SVM as negative control. For this purpose, we generated five data splits and conducted \npredictions anew.  \nThe results, depicted in Figure 1a for antimicrobial activity prediction and Figure 1b for \nhemolysis prediction, confirmed our earlier observations (performances in Table S2). Notably, the \nRNN performances were higher than those observed in the screening experiment, and were clearly \nabove those of GTP-3.5. Furthermore, both the RNN and MAP4C SVM demonstrated comparable \nperformances, indicating the validity of both approaches in predicting antimicrobial activity and \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n8 \n \nhemolysis. The finding that simpler machine learning architectures, like SVM, can rival the \nperformance of more complex RNNs in predicting antimicrobial activity and hemolysis is \nparticularly interesting. A comparison with models trained on similar datasets, which achieve \nsimilar performances as reported in this study, further reinforces the consistency of our findings.19â€“\n21  \nThis raises questions about the importance of model architecture versus foundational \nelements such as data quality and feature engineering. It suggests that a balanced approach, \nprioritizing optimization of these foundational components, could prove more beneficial than \nfocusing solely on model complexity. \n \nFigure 1: Results of the 5-fold cross-validation study aimed at validating MAP4C SVM, Features SVM, RNN, and \nGPT-3.5 turbo performance for a) antimicrobial activity and b) hemolysis predictions. The mean performance across \nthe 5 cross-validations for each metric is shown as a bar, the standard deviation is displayed with an error bar. The \nresults confirmed earlier observations but showed notably higher performances for the RNN compared to the one-shot \nscreening experiment. Both the RNN and MAP4C SVM demonstrated comparable performances . \nData visualization \nThe high performance achieved by the SVM trained on the MAP4C fingerprint suggested that the \nnearest neighbor relationships in the MAP4C feature space could be sufficient to distinguish active \nfrom inactive and hemolytic from non-hemolytic peptide sequences. In our previous work, we \nobserved that the MAP4 fingerprint38 correctly clustered natural products, taken from the \nCOCONUT database,39 according to their organism of origin.40,41 In analogy to our previous work, \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n9 \n \nwe were curious to see whether a spatial separation of actives/inactives and hemolytic/non-\nhemolytic sequences can be obtained from encoding with MAP4C, the chiral version of MAP4, \npossibly explaining the good performance of the MAP4C SVM model. For this, we reduced the \n2048-dimensional feature space of MAP4C to 2D using the dimensionality reduction method \nTMAP,42 and used the obtained visualization to display a set of molecular properties.  \nFirst, we wanted to confirm that the TMAP visualization aligns with intuitive distributions \nof structural features relevant for peptides. For that, we colored  the data points based on their heavy \natom count (HAC), an indicator of molecular size, and fraction of carbon atoms (fraction C), a \nsimple proxy for the hydrophobicity of a peptide sequence. The TMAP revealed visible clusters for \nboth, HAC (Figure 2a)  and fraction C (Figure 2b), indicating that the reduced MAP4C features \ncan reliably represent simple molecular descriptors in the underlying chemical space.  \nFollowing this first observation, we wanted to test if we can detect clusters within TMAP \nvisualizations of more complex physicochemical properties, such as the predicted fraction of helical \nresidues (Figure 2c) and the hydrophobic moment (Figure 2d). In both cases, we could not detect \nlarge homogenous clusters as was the case for HAC and fraction C. However, the data formed a \nlarge number of small local clusters, indicating that the nearest neighbor relationships in the \nMAP4C feature space can possibly be used to distinguish sequences with high \nhelicity/hydrophobicity opposed to sequences with low helicity/hydrophobicity.  \nFinally, we analysed the distribution of active versus inactive (Figure 2e) and hemolytic \nversus non-hemolytic (Figure 2f) sequences in the MAP4C chemical space. Similarly to the \nvisualizations of predicted fraction of helical residues and hydrophobic moment, active and inactive \nor hemolytic and non-hemolytic sequences are spatially separated in a large number of small, local \nclusters. This finding is particularly interesting as it suggests that nearest neighbor relationships in \nthe MAP4C feature space are sufficient to separate peptide sequences based on their antimicrobial \nactivity and hemolysis. It further provides an explanation to the good performance obtained with the \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n10 \n \nMAP4C SVM, which can leverage the nearest neighbor relationships stored in the MAP4C \nfingerprint feature space when provided with a custom Jaccard kernel function. \n \nFigure 2: Chemical space covered by the 9,548 peptide sequences with annotated antimicrobial activity extracted from \nthe Database of Antimicrobial Activity and Structure of Peptides (DBAASP). The sequences are encoded using the \nMAP4C fingerprint and the resulting 2048-dimensional space reduced to 2D using TMAP. The sequences in the 2D \nTMAP were colored based on a) heavy atom count, b) fraction of carbon atoms, c) predicted fraction of helical residues, \nd) hydrophobic moment, e) annotated antimicrobial activity and f) annotated hemolysis.  \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n11 \n \nConclusion \nIn the present study we investigated the potential of LLMs as predictive tools for antimicrobial \nactivity and hemolysis of peptide sequences. We assessed that fine-tuning GPT models in cloud is a \nrelatively easy and fast process as access through the API eliminates the need to buy expensive \nhardware and requires little technical expertise. Duration of fine-tuning was short, and the \nassociated costs were low (Table S3). In contrast to cloud-based fine-tuning, local model training \ninvolves setting up and maintaining hardware, which can be costly and require technical expertise. \nWhile less complex models like RNNs and SVMs have lower hardware requirements, training \nlarger models such as LLMs locally can pose challenges in terms of scalability, as one can rapidly \nface limitations in terms of hardware capacity and maintenance costs.  \nHowever, the lack of control over the training environment in cloud-based approaches raises \nconcerns regarding reproducibility of scientific results. In the course of this study, we had originally \nfine-tuned GPT-3 models Ada, Babbage and Curie. These models performed slightly better than the \nreference models, even achieving good performances in low data regimes. Unfortunately, these \nmodels were later deprecated by OpenAI and their performance cannot be reproduced. When fine-\ntuning a newer iteration of GPT-3 (GPT-3.5 Turbo), we observed a significant decrease in \nperformance for the same task. We attribute the drop in performance to the increasing optimization \nof LLMs for conversational interactions, which may negatively impact their effectiveness in out-of-\nscope predictive tasks. These findings highlight the potential risk of how not controlling one's own \nmodels can compromise the reproducibility and reliability of scientific results. \nThe aforementioned findings suggest a diminishing suitability of chat oriented LLMs for \nclassification tasks over time, a function beyond their intended design. This observation specifically \napplies to LLMs tailored for conversational or human interaction purposes, rather than specialized \nLLMs trained on domain-specific data. Unfortunately, the latter do not provide the ease of access \nand usability that GPT models do. Consequently, we expect that LLMs will increasingly be \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n12 \n \nemployed in human interaction settings, facilitating the integration of various chemical tools \nthrough natural language interfaces as is being pioneered by Bran31 and Boiko et al.32  \nFinally, we could demonstrate in the present study that classical machine learning \ntechniques, such as SVMs trained on MAP4C fingerprint encodings, can achieve state-of-the-art \nperformance in the prediction of antimicrobial activity and hemolysis. This finding is especially \ninteresting, as it showcases that good performance can be achieved by less complex models, putting \nthe emphasis on data quality rather than model complexity.  \n \nCode availability \nThe source codes and datasets used for this study are available at https://github.com/reymond-\ngroup/LLM_classifier. \n \nAuthor Contribution Statement \nMO designed and realized the project and wrote the paper. JLR designed and supervised the project \nand wrote the paper. Both authors read and approved the final manuscript. \n \nAcknowledgements \nThis work was supported by the Swiss National Science Foundation (200020_178998) and the \nEuropean Research Council (885076). MO thanks Sacha Javor for the helpful discussion and \ncomments. \n \n \n  \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n13 \n \nReferences \n(1) Lakemeyer, M.; Zhao, W.; Mandl, F. A.; Hammann, P.; Sieber, S. A. Thinking Outside the \nBox-Novel Antibacterials To Tackle the Resistance Crisis. Angew. Chem. Int. Ed. 2018, 57 \n(44), 14440â€“14475. https://doi.org/10.1002/anie.201804971. \n(2) Magana, M.; Pushpanathan, M.; Santos, A. L.; Leanse, L.; Fernandez, M.; Ioannidis, A.; \nGiulianotti, M. A.; Apidianakis, Y.; Bradfute, S.; Ferguson, A. L.; Cherkasov, A.; Seleem, M. \nN.; Pinilla, C.; De La Fuente-Nunez, C.; Lazaridis, T.; Dai, T.; Houghten, R. A.; Hancock, R. \nE. W.; Tegos, G. P. The Value of Antimicrobial Peptides in the Age of Resistance. Lancet \nInfect. Dis. 2020, 20 (9), e216â€“e230. https://doi.org/10.1016/S1473-3099(20)30327-3. \n(3) Mookherjee, N.; Anderson, M. A.; Haagsman, H. P.; Davidson, D. J. Antimicrobial Host \nDefence Peptides: Functions and Clinical Potential. Nat. Rev. Drug Discov. 2020, 19 (5), 311â€“\n332. https://doi.org/10.1038/s41573-019-0058-8. \n(4) Torres, M. D. T.; Sothiselvam, S.; Lu, T. K.; De La Fuente-Nunez, C. Peptide Design \nPrinciples for Antimicrobial Applications. J. Mol. Biol. 2019, 431 (18), 3547â€“3567. \nhttps://doi.org/10.1016/j.jmb.2018.12.015. \n(5) Capecchi, A.; Reymond, J.-L. Peptides in Chemical Space. Med. Drug Discov. 2021, 9, \n100081. https://doi.org/10.1016/j.medidd.2021.100081. \n(6) MÃ¼ller, A. T.; Hiss, J. A.; Schneider, G. Recurrent Neural Network Model for Constructive \nPeptide Design. J. Chem. Inf. Model. 2018, 58 (2), 472â€“479. \nhttps://doi.org/10.1021/acs.jcim.7b00414. \n(7) Veltri, D.; Kamath, U.; Shehu, A. Deep Learning Improves Antimicrobial Peptide \nRecognition. Bioinformatics 2018, 34 (16), 2740â€“2747. \nhttps://doi.org/10.1093/bioinformatics/bty179. \n(8) Liu, S. Novel 3D Structure Based Model for Activity Prediction and Design of Antimicrobial \nPeptides. Sci. Rep. 2018. \n(9) Su, X.; Xu, J.; Yin, Y.; Quan, X.; Zhang, H. Antimicrobial Peptide Identification Using Multi-\nScale Convolutional Network. BMC Bioinformatics 2019, 20 (1), 730. \nhttps://doi.org/10.1186/s12859-019-3327-y. \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n14 \n \n(10) Vishnepolsky, B.; Zaalishvili, G.; Karapetian, M.; Nasrashvili, T.; Kuljanishvili, N.; \nGabrielian, A.; Rosenthal, A.; Hurt, D. E.; Tartakovsky, M.; Grigolava, M.; Pirtskhalava, M. \nDe Novo Design and In Vitro Testing of Antimicrobial Peptides against Gram-Negative \nBacteria. 2019. \n(11) Plisson, F.; RamÃ­rez-SÃ¡nchez, O.; MartÃ­nez-HernÃ¡ndez, C. Machine Learning-Guided \nDiscovery and Design of Non-Hemolytic Peptides. Sci. Rep. 2020, 10 (1), 16581. \nhttps://doi.org/10.1038/s41598-020-73644-6. \n(12) Yan, J.; Bhadra, P.; Li, A.; Sethiya, P.; Qin, L.; Tai, H. K.; Wong, K. H.; Siu, S. W. I. Deep-\nAmPEP30: Improve Short Antimicrobial Peptides Prediction with Deep Learning. Mol. Ther. - \nNucleic Acids 2020, 20, 882â€“894. https://doi.org/10.1016/j.omtn.2020.05.006. \n(13) Capecchi, A.; Cai, X.; Personne, H.; KÃ¶hler, T.; van Delden, C.; Reymond, J.-L. Machine \nLearning Designs Non-Hemolytic Antimicrobial Peptides. Chem. Sci. 2021, 12 (26), 9221â€“\n9232. https://doi.org/10.1039/D1SC01713F. \n(14) Zakharova, E.; Orsi, M.; Capecchi, A.; Reymond, J. Machine Learning Guided Discovery of \nNonâ€Hemolytic Membrane Disruptive Anticancer Peptides. ChemMedChem 2022. \nhttps://doi.org/10.1002/cmdc.202200291. \n(15) Liu, G.; Catacutan, D. B.; Rathod, K.; Swanson, K.; Jin, W.; Mohammed, J. C.; Chiappino-\nPepe, A.; Syed, S. A.; Fragis, M.; Rachwalski, K.; Magolan, J.; Surette, M. G.; Coombes, B. \nK.; Jaakkola, T.; Barzilay, R.; Collins, J. J.; Stokes, J. M. Deep Learning-Guided Discovery of \nan Antibiotic Targeting Acinetobacter Baumannii. Nat. Chem. Biol. 2023. \nhttps://doi.org/10.1038/s41589-023-01349-8. \n(16) Wan, F.; De La Fuente-Nunez, C. Mining for Antimicrobial Peptides in Sequence Space. Nat. \nBiomed. Eng. 2023. https://doi.org/10.1038/s41551-023-01027-z. \n(17) Aguilera-Puga, M. D. C.; Plisson, F. Structure-Aware Machine Learning Strategies for \nAntimicrobial Peptide Discovery; preprint; In Review, 2024. https://doi.org/10.21203/rs.3.rs-\n3938402/v1. \n(18) Wan, F.; Wong, F.; Collins, J. J.; De La Fuente-Nunez, C. Machine Learning for \nAntimicrobial Peptide Identification and Design. Nat. Rev. Bioeng. 2024. \nhttps://doi.org/10.1038/s44222-024-00152-x. \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n15 \n \n(19) Timmons, P. B.; Hewage, C. M. HAPPENN Is a Novel Tool for Hemolytic Activity Prediction \nfor Therapeutic Peptides Which Employs Neural Networks. Sci. Rep. 2020, 10 (1), 10869. \nhttps://doi.org/10.1038/s41598-020-67701-3. \n(20) Hasan, M. M.; Schaduangrat, N.; Basith, S.; Lee, G.; Shoombuatong, W.; Manavalan, B. \nHLPpred-Fuse: Improved and Robust Prediction of Hemolytic Peptide and Its Activity by \nFusing Multiple Feature Representation. Bioinformatics 2020, 36 (11), 3350â€“3356. \nhttps://doi.org/10.1093/bioinformatics/btaa160. \n(21) Ansari, M.; White, A. D. Serverless Prediction of Peptide Properties with Recurrent Neural \nNetworks. J Chem Inf Model 2023. \n(22) Hochreiter, S.; Schmidhuber, J. Long Short-Term Memory. Neural Comput. 1997, 9 (8), \n1735â€“1780. https://doi.org/10.1162/neco.1997.9.8.1735. \n(23) Cho, K.; van Merrienboer, B.; Bahdanau, D.; Bengio, Y. On the Properties of Neural Machine \nTranslation: Encoder-Decoder Approaches. arXiv October 7, 2014. \nhttp://arxiv.org/abs/1409.1259 (accessed 2023-05-31). \n(24) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; \nPolosukhin, I. Attention Is All You Need. arXiv December 5, 2017. \nhttp://arxiv.org/abs/1706.03762 (accessed 2023-05-31). \n(25) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; \nShyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; \nChild, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; \nLitwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, \nI.; Amodei, D. Language Models Are Few-Shot Learners. arXiv July 22, 2020. \nhttp://arxiv.org/abs/2005.14165 (accessed 2023-05-31). \n(26) Jablonka, K. M.; Schwaller, P.; Ortega-Guerrero, A.; Smit, B. Leveraging Large Language \nModels for Predictive Chemistry. Nat. Mach. Intell. 2024, 6 (2), 161â€“169. \nhttps://doi.org/10.1038/s42256-023-00788-1. \n(27) Bran, A. M.; Schwaller, P. Transformers and Large Language Models for Chemistry and Drug \nDiscovery. 2023. https://doi.org/10.48550/ARXIV.2310.06083. \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n16 \n \n(28) Guo, T.; Guo, K.; Nan, B.; Liang, Z.; Guo, Z.; Chawla, N. V.; Wiest, O.; Zhang, X. What Can \nLarge Language Models Do in Chemistry? A Comprehensive Benchmark on Eight Tasks. \n(29) Castro Nascimento, C. M.; Pimentel, A. S. Do Large Language Models Understand \nChemistry? A Conversation with ChatGPT. J. Chem. Inf. Model. 2023, 63 (6), 1649â€“1655. \nhttps://doi.org/10.1021/acs.jcim.3c00285. \n(30) White, A. D.; Hocky, G. M.; Gandhi, H. A.; Ansari, M.; Cox, S.; Wellawatte, G. P.; Sasmal, \nS.; Yang, Z.; Liu, K.; Singh, Y.; PeÃ±a Ccoa, W. J. Assessment of Chemistry Knowledge in \nLarge Language Models That Generate Code. Digit. Discov. 2023, 2 (2), 368â€“376. \nhttps://doi.org/10.1039/D2DD00087C. \n(31) Bran, A. M.; Cox, S.; White, A. D.; Schwaller, P. ChemCrow: Augmenting Large-Language \nModels with Chemistry Tools. arXiv April 12, 2023. http://arxiv.org/abs/2304.05376 \n(accessed 2023-05-31). \n(32) Boiko, D. A.; MacKnight, R.; Kline, B.; Gomes, G. Autonomous Chemical Research with \nLarge Language Models. Nature 2023, 624 (7992), 570â€“578. https://doi.org/10.1038/s41586-\n023-06792-0. \n(33) Jablonka, K. M.; Ai, Q.; Al-Feghali, A.; Badhwar, S.; Bocarsly, J. D.; Bran, A. M.; Bringuier, \nS.; Brinson, L. C.; Choudhary, K.; Circi, D.; Cox, S.; De Jong, W. A.; Evans, M. L.; Gastellu, \nN.; Genzling, J.; Gil, M. V.; Gupta, A. K.; Hong, Z.; Imran, A.; Kruschwitz, S.; Labarre, A.; \nLÃ¡la, J.; Liu, T.; Ma, S.; Majumdar, S.; Merz, G. W.; Moitessier, N.; Moubarak, E.; MouriÃ±o, \nB.; Pelkie, B.; Pieler, M.; Ramos, M. C.; RankoviÄ‡, B.; Rodriques, S. G.; Sanders, J. N.; \nSchwaller, P.; Schwarting, M.; Shi, J.; Smit, B.; Smith, B. E.; Van Herck, J.; VÃ¶lker, C.; Ward, \nL.; Warren, S.; Weiser, B.; Zhang, S.; Zhang, X.; Zia, G. A.; Scourtas, A.; Schmidt, K. J.; \nFoster, I.; White, A. D.; Blaiszik, B. 14 Examples of How LLMs Can Transform Materials \nScience and Chemistry: A Reflection on a Large Language Model Hackathon. Digit. Discov. \n2023, 2 (5), 1233â€“1250. https://doi.org/10.1039/D3DD00113J. \n(34) Orsi, M.; Reymond, J.-L. One Chiral Fingerprint to Find Them All; preprint; Chemistry, 2023. \nhttps://doi.org/10.26434/chemrxiv-2023-33j02. \n(35) Gogoladze, G.; Grigolava, M.; Vishnepolsky, B.; Chubinidze, M.; Duroux, P.; Lefranc, M.-P.; \nPirtskhalava, M. DBAASPâ€¯: Database of Antimicrobial Activity and Structure of Peptides. \nFEMS Microbiol. Lett. 2014, 357 (1), 63â€“68. https://doi.org/10.1111/1574-6968.12489. \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n17 \n \n(36) Heffernan, R.; Paliwal, K.; Lyons, J.; Singh, J.; Yang, Y.; Zhou, Y. Singleâ€sequenceâ€based \nPrediction of Protein Secondary Structures and Solvent Accessibility by Deep Wholeâ€\nsequence Learning. J. Comput. Chem. 2018, 39 (26), 2210â€“2216. \nhttps://doi.org/10.1002/jcc.25534. \n(37) Eisenberg, D.; Weiss, R. M.; Terwilliger, T. C. The Helical Hydrophobic Moment: A Measure \nof the Amphiphilicity of a Helix. Nature 1982, 299 (5881), 371â€“374. \nhttps://doi.org/10.1038/299371a0. \n(38) Capecchi, A.; Probst, D.; Reymond, J.-L. One Molecular Fingerprint to Rule Them All: Drugs, \nBiomolecules, and the Metabolome. J. Cheminformatics 2020, 12 (1), 43. \nhttps://doi.org/10.1186/s13321-020-00445-4. \n(39) Sorokina, M.; Merseburger, P.; Rajan, K.; Yirik, M. A.; Steinbeck, C. COCONUT Online: \nCollection of Open Natural Products Database. J. Cheminformatics 2021, 13 (1), 2. \nhttps://doi.org/10.1186/s13321-020-00478-9. \n(40) Capecchi, A.; Reymond, J.-L. Assigning the Origin of Microbial Natural Products by \nChemical Space Map and Machine Learning. Biomolecules 2020, 10 (10), 1385. \nhttps://doi.org/10.3390/biom10101385. \n(41) Capecchi, A.; Reymond, J.-L. Classifying Natural Products from Plants, Fungi or Bacteria \nUsing the COCONUT Database and Machine Learning. J. Cheminformatics 2021, 13 (1), 82. \nhttps://doi.org/10.1186/s13321-021-00559-3. \n(42) Probst, D.; Reymond, J.-L. Visualization of Very Large High-Dimensional Data Sets as \nMinimum Spanning Trees. J. Cheminformatics 2020, 12 (1), 12. \nhttps://doi.org/10.1186/s13321-020-0416-x. \n \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n1 \n \nSupplementary Information for: \n \nCan Large Language Models Predict Antimicrobial \nPeptide Activity and Toxicity? \nMarkus Orsi,a and Jean-Louis Reymonda* \na) Department of Chemistry, Biochemistry and Pharmaceutical Sciences, University of Bern, \nFreiestrasse 3, 3012 Bern, Switzerland \ne-mail: jean-louis.reymond@unibe.ch \n \nTable of Contents  \n \n \nTable S1 â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦2  \n  \nTable S2 â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦2 \n \nTable S3 â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦3 \n \n \n \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n2 \n \nTable S1. Performance metrics of all models tested on antimicrobial activity and hemolysis classification. The best \nvalue for each metric is highlighted in bold for activity and hemolysis separately. Results for reduced training sets are \nreported for 20% and 2% size of the original activity dataset and 10% of the original hemolysis set.  \nModel ROC AUC Accuracy Precision Recall F1 \nGPT-3 Ada act. 0.84 0.78 0.78 0.78 0.78 \nGPT-3 Babbage act. 0.85 0.79 0.79 0.78 0.79 \nGPT-3 Curie act. 0.86 0.79 0.78 0.81 0.79 \nGPT-3 Ada 20% act. 0.75 0.69 0.7 0.67 0.68 \nGPT-3 Babbage 20% act. 0.76 0.69 0.7 0.69 0.68 \nGPT-3 Curie 20% act. 0.76 0.7 0.71 0.71 0.71 \nGPT-3 Ada 2% act. 0.66 0.6 0.6 0.63 0.61 \nGPT-3 Babbage 2% act. 0.66 0.62 0.6 0.73 0.66 \nGPT-3 Curie 2% act. 0.65 0.6 0.6 0.63 0.61 \nGPT-3 Ada hem. 0.9 0.82 0.8 0.79 0.79 \nGPT-3 Babbage hem. 0.87 0.8 0.76 0.76 0.76 \nGPT-3 Curie hem. 0.89 0.84 0.82 0.79 0.8 \nGPT-3 Ada 10% hem. 0.72 0.68 0.63 0.58 0.6 \nGPT-3 Babbage 10% hem. 0.72 0.7 0.65 0.6 0.62 \nGPT-3 Curie 10% hem. 0.73 0.68 0.63 0.59 0.61 \n \n \nTable S2. Mean and standard deviation of performance metrics of selected models tested on antimicrobial activity and \nhemolysis classification. The best value for each metric is highlighted in bold.  \nModel ROC AUC Accuracy Precision Recall F1 \nFeatures SVM act. 0.65 Â± 0.01 0.65 Â± 0.01 0.65 Â± 0.01 0.63 Â± 0.01 0.64 Â± 0.01 \nMAP4C SVM act. 0.8 Â± 0.01 0.8 Â± 0.01 0.78 Â± 0.01 0.83 Â± 0.01 0.80 Â± 0.01 \nRNN act. 0.85 Â± 0.01 0.78 Â± 0.01 0.76 Â± 0.02 0.81 Â± 0.01 0.78 Â± 0.01 \nGPT-3.5 Turbo act. 0.69 Â± 0.01 0.69 Â± 0.01 0.62 Â± 0.01 0.95 Â± 0.01 0.75 Â± 0.01 \nFeatures SVM hem. 0.62 Â± 0.01 0.64 Â± 0.01 0.59 Â± 0.02 0.48 Â± 0.02 0.53 Â± 0.01 \nMAP4C SVM hem. 0.82 Â± 0.02 0.82 Â± 0.01 0.78 Â± 0.02 0.82 Â± 0.04 0.79 Â± 0.01 \nRNN hem. 0.87 Â± 0.01 0.81 Â± 0.01 0.77 Â± 0.03 0.79 Â± 0.03 0.78 Â± 0.01 \nGPT-3.5 Turbo hem. 0.47 Â± 0.01 0.48 Â± 0.01 0.38 Â± 0.02 0.36 Â± 0.02 0.37 Â± 0.02 \n \n  \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n3 \n \nTable S3. Training times and costs of GPT models on the full training sets. \nModel Time (h) Costs ($) \nGPT-3 Ada Activity 01:05:04 $0.39 \nGPT-3 Babbage Activity 01:09:38 $0.59 \nGPT-3 Curie Activity 01:15:05 $2.93 \nGPT-3.5 Turbo Activity 00:53:24 $7.00 \nGPT-3 Ada Hemolysis 00:55:37 $0.09 \nGPT-3 Babbage Hemolysis  00:57:19 $0.13 \nGPT-3 Curie Hemolysis 01:08:09 $0.67 \nGPT-3.5 Turbo Hemolysis 00:55:58 $1.66 \n \nhttps://doi.org/10.26434/chemrxiv-2023-74041-v2 ORCID: https://orcid.org/0000-0003-2724-2942 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0",
  "topic": "Antimicrobial",
  "concepts": [
    {
      "name": "Antimicrobial",
      "score": 0.6960374116897583
    },
    {
      "name": "Toxicity",
      "score": 0.5520272850990295
    },
    {
      "name": "Peptide",
      "score": 0.5126994848251343
    },
    {
      "name": "Computational biology",
      "score": 0.38915517926216125
    },
    {
      "name": "Chemistry",
      "score": 0.3372465968132019
    },
    {
      "name": "Biology",
      "score": 0.3049226403236389
    },
    {
      "name": "Medicine",
      "score": 0.27713143825531006
    },
    {
      "name": "Internal medicine",
      "score": 0.2456953525543213
    },
    {
      "name": "Microbiology",
      "score": 0.21630626916885376
    },
    {
      "name": "Biochemistry",
      "score": 0.16556450724601746
    }
  ]
}