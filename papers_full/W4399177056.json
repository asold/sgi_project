{
  "title": "Large Language Models as Planning Domain Generators",
  "url": "https://openalex.org/W4399177056",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1964321420",
      "name": "James Oswald",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2238314490",
      "name": "Kavitha Srinivas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2998106543",
      "name": "Harsha Kokel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118800098",
      "name": "Junkyu Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098402621",
      "name": "Michael Katz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132080712",
      "name": "Shirin Sohrabi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964321420",
      "name": "James Oswald",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2238314490",
      "name": "Kavitha Srinivas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2998106543",
      "name": "Harsha Kokel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118800098",
      "name": "Junkyu Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098402621",
      "name": "Michael Katz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132080712",
      "name": "Shirin Sohrabi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4303478243",
    "https://openalex.org/W3111885891",
    "https://openalex.org/W4393161058",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4312898723",
    "https://openalex.org/W2549482841",
    "https://openalex.org/W3101355526",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4381104068",
    "https://openalex.org/W4378499172",
    "https://openalex.org/W4378510404",
    "https://openalex.org/W3100276112",
    "https://openalex.org/W4393160795",
    "https://openalex.org/W2990122567",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4391816278",
    "https://openalex.org/W2613410561",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4366999541"
  ],
  "abstract": "Developing domain models is one of the few remaining places that require manual human labor in AI planning. Thus, in order to make planning more accessible, it is desirable to automate the process of domain model generation. To this end, we investigate if large language models (LLMs) can be used to generate planning domain models from simple textual descriptions. Specifically, we introduce a framework for automated evaluation of LLM-generated domains by comparing the sets of plans for domain instances. Finally, we perform an empirical analysis of 7 large language models, including coding and chat models across 9 different planning domains, and under three classes of natural language domain descriptions. Our results indicate that LLMs, particularly those with high parameter counts, exhibit a moderate level of proficiency in generating correct planning domains from natural language descriptions. Our code is available at https://github.com/IBM/NL2PDDL.",
  "full_text": "Large Language Models as Planning Domain Generators\nJames Oswald1, Kavitha Srinivas2, Harsha Kokel2, Junkyu Lee2, Michael Katz2, Shirin Sohrabi2\n1Rensselaer Polytechnic Institute\n2IBM Research\noswalj@rpi.edu, {kavitha.srinivas, harsha.kokel, junkyu.lee, michael.katz1}@ibm.com, ssohrab@us.ibm.com\nAbstract\nDeveloping domain models is one of the few remaining\nplaces that require manual human labor in AI planning. Thus,\nin order to make planning more accessible, it is desirable to\nautomate the process of domain model generation. To this\nend, we investigate if large language models (LLMs) can be\nused to generate planning domain models from simple tex-\ntual descriptions. Specifically, we introduce a framework for\nautomated evaluation of LLM-generated domains by com-\nparing the sets of plans for domain instances. Finally, we\nperform an empirical analysis of 7 large language models,\nincluding coding and chat models across 9 different plan-\nning domains, and under three classes of natural language\ndomain descriptions. Our results indicate that LLMs, partic-\nularly those with high parameter counts, exhibit a moderate\nlevel of proficiency in generating correct planning domains\nfrom natural language descriptions. Our code is available at\nhttps://github.com/IBM/NL2PDDL.\nIntroduction\nLarge language models (LLMs) have demonstrated robust\nemergent abilities for open-ended tasks like story gener-\nation, poetry, and dialogue (Zhao et al. 2023b; Hayawi,\nShahriar, and Mathew 2024). Their potential is no longer\nlimited to natural language. Rather, they have shown the\nability to generate highly structured output that resembles\ncode from natural language descriptions of programs (Li\net al. 2023; Touvron, Lavril, and Izacard 2023). It is natural\nto wonder how these abilities generalize to knowledge engi-\nneering tasks such as those used for problem representation\nin symbolic methods. Despite the efficacy of symbolic meth-\nods such as boolean satisfiability (SAT) solvers (Biere et al.\n2021), automated planners (Helmert 2006), and automated\ntheorem provers (Harrison, Urban, and Wiedijk 2014) in\ntheir respective domains, the issue of representing a problem\naccurately and efficiently still hinders the wider adoption\nand accessibility of these powerful methods. If LLMs can\nbridge the gap between natural language description of the\nproblem and symbolic representation, it would enable large-\nscale adoption of symbolic methods and reduce the depen-\ndency on technical experts. It is natural to look to the emer-\ngent abilities of LLMs as a potential bridge that could tie nat-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nural language descriptions of problems to accurate symbolic\nrepresentations for automated planning (Ghallab, Nau, and\nTraverso 2004). If LLMs are sufficiently equipped for this\ntask, this would reduce the dependency on technical experts\nas well as lead to a wider adoption of symbolic methods.\nTo this end, our work investigates using LLMs for generat-\ning problem representations for automated planning. Specif-\nically, we evaluate the capabilities of LLMs to automatically\ntranslate natural language descriptions of planning domains\nto Planning Domain Description Language (PDDL) (Ghal-\nlab et al. 1998).\nThe problem of domain generation from natural language\nhas been studied earlier (Lindsay et al. 2017; Hayton et al.\n2020) and recently Guan et al. (2023) also attempted this\nproblem using LLMs. Despite these studies, the task of eval-\nuating the usefulness of the generated domain description is\nextremely difficult. Previous works leveraged human experts\nfor evaluation. We argue that for rigorous, automated evalu-\nation we need a ground truth; a vetted domain specification.\nHence, in this work we focus on the task of creating high-\nquality reconstructions of the PDDL domain from natural\nlanguage; where the generated domain is ideally equivalent\nto the ground truth. Restricting the generation of the PDDL\ndomain to an approximated equivalence class would make\nthe generated domains more amenable to existing planners\nand further the goal of using the generated descriptions for\nproducing executable plans. To further clarify, while Guan\net al. (2023) uses LLMs to learn a PDDL model from a tex-\ntual description, this is not our main purpose in this work.\nWe aim in this work to understand how such methods can be\nevaluated, and due to this, need to depend on the additional\nassumption that a reference domain is available. While this\nis a stronger assumption than what is made in earlier work,\nthis allows for fully automated evaluation.\nThe core contributions of this work are fourfold. First, we\ndefine a task of PDDL domain reconstruction from natural\nlanguage; based on a ground truth. Second, we define two\nmetrics for evaluating domain quality that do not depend on\nany form of manual human evaluation. Third, we examine\nclasses of natural language descriptions of PDDL actions to\ninvestigate if the inclusion and exclusion of particular infor-\nmation impacts the ability to generate domains or the qual-\nity of generated domains. Finally, we evaluate 7 different\nLLMs, including coding and chat models, and provide a de-\nProceedings of the Thirty-Fourth International Conference on Automated Planning and Scheduling (ICAPS2024)\n423\ntailed analysis of the results from each on 9 domains.\nBackground\nPlanning In this work, we use the Planning Domain Def-\ninition Language (PDDL) for the declarative plan represen-\ntation, but when necessary to discuss the underlying for-\nmalisms we refer to parts of planning problems and domains\nusing the following lifted STRIPS formalism, largely in line\nwith Corr ˆea and Seipp (2022). A lifted STRIPS planning\nproblem is defined as a 5-tuple Π = ⟨F, C, A, s0, S∗⟩. F\nis a finite set of predicates that describe the world. C is a\nfinite set of constants representing objects in the world, op-\ntionally including type information. We define Fg as the set\nof all grounded predicates, that is, predicates in which all\nvariables are replaced by legal constants from C. A state\ns ⊆ Fg is a set of grounded predicates that describe the\nstate of the world, such that f ∈ s if and only if f is a\ntrue fact about the world. The set of all possible states is the\npower set of Fg, denoted by S. A is a set of action schema\nwhere each a ∈ Ais a 3-tuple ⟨pre(a), add(a), del(a)⟩\nwhere pre(a) ⊆ Fis the set of predicates that must hold\nto apply the action, add(a) ⊆ Fis the set of predicates that\nbecome true after the action is applied, and del(a) ⊆ Fis\nthe set of predicates that become false. An action schema\na ∈ A can be grounded by substituting all variables in a\nwith allowed constants from C. The grounded action ag =\n⟨preg(ag), addg(ag), delg(ag)⟩ is defined as a 3-tuple of its\ngrounded pre, add, and del predicates, and we define Ag as\nthe set of all grounded actions. Finally,s0 ⊆ Fg is the initial\nstate of the world for the planning task andS∗ ⊆ S is the set\nof possible goal states.\nFor a grounded action ag ∈ Ag and a state s ∈ S, we\nsay that ag is applicable in s if preg(ag) ⊆ s. Applying an\napplicable action ag in the state s results in a state s[ag] :=\n(s/delg(ag)) ∪ addg(ag). A plan for a problem Π is, there-\nfore a sequence of grounded actions π = ( a1, ··· , an)\nwhich when applied transforms the initial states0 into a goal\nstate in S∗. The action sequence defines a state sequence\nS = (s0, ··· , sn) such that si = si−1[ai] for 1 ≤ i ≤ n and\nsn ∈ S∗. The set of all plans for Π is denoted by PΠ.\nA planning domain for a lifted STRIPS planning prob-\nlem Π is the problem’s predicate and action schema sets\nD = ⟨F, A⟩, while we say Π is a problem for D and write\nΠD if Π uses D as its underlying domain, regardless of the\nspecific objects, initial state, and goal states (C , s0, S∗) for\nthe problem.\nLarge Language Models (LLMs) Language Models are\nprobabilistic predictors for language tokens that when given\na sequence of tokens T = ( t0, t1, ··· , tn) in a corpus C\nwill output a set of predictions and associated probabilities\nP ⊆ C × I Rfor tn+1 based on the data the model has been\ntrained on. Different decoding strategies can be used to se-\nlect a token in P based on the probabilities, one such strat-\negy is the greedy strategy which sets tn+1 equal to the high-\nest probability token in P. The new tn+1 can be appended\nto T and the process can be repeated for the next token. The\nmaximum allowed size ofT is known as thecontext window,\nwhich limits the amount of tokens able to use for prediction.\nLarge language models are a class of language models\ncharacterized by their large size and emergent abilities on\ntasks that smaller language models are unable to perform\non. LLMs are almost always implemented on top of a Trans-\nformer architecture (Vaswani et al. 2017). There are many\ndifferent types of large language models trained on various\ntypes of data, and models may be tuned to perform differ-\nent types of tasks such as code generation (Li et al. 2023) or\nacting as chat agents (Touvron, Lavril, and Izacard 2023); a\nsurvey can be found at Zhao et al. (2023a).\nIn-Context Learning for LLM inference is a technique\nclassified as an emergent ability of LLMs to perform at a\nhigher level of performance on tasks using examples of\ndesired inputs and outputs (Dong et al. 2022). For example,\nrather than the prompt: “Solve the following\naddition problem: 1 + 2”, an in-context learn-\ning prompt would read: “Solve the following\naddition problems: In: 2 + 3, Out: 5;\nIn: 4 + 2, Out: 6; ..., In: 1 + 2,”, where\nthe prompt is composed of 3 parts (1) An instruction (2) a\nset of context examples and a (3) a query which is expected\nto be answered inline with the context examples. In-context\nlearning is used in our work and much of the related work\nsuch as Liu et al. (2023) and Guan et al. (2023).\nApproach\nThe goal of this work is the evaluation of LLM’s abilities\nto generate PDDL domains. In particular, we are interested\nin generating and evaluating these domains on an action-\nby-action basis where each prompt to the LLM is a request\nto generate one action in a domain using context examples\nfrom other domains. This action-by-action prompting was\ninspired by Guan et al. (2023) and is primarily a concern\ndue to the size of the LLM’s context window.\nWe now turn to characterizing the concrete task we are\ntrying to solve, an overview can be seen in Figure 1. In order\nto evaluate generated domains automatically, a ground truth\ndomain is needed to compare the generated domains against.\nFor this, we use existing PDDL domains as a starting point\nin our approach. Given a starting domain D = ⟨F, A⟩, we\nbegin by converting all action schema in A to natural lan-\nguage descriptions of action schema,N(A). We assume that\na list of the predicates in the domainF and natural language\ndescriptions of these predicatesN(F) are given to us as con-\ntext for the domain. This assumption, while slightly limiting\naccessibility, is the cornerstone that allows this task a much\nmore robust set of automatic evaluation options than when\nthe context for the domain is just a natural language descrip-\ntion, as in, e.g., Guan et al. (2023). The natural language\naction N(a) ∈ N(A), along with a specification of domain\npredicates ⟨F, N(F)⟩, is used as the query for the in-context\nlearning prompt. For the prompt’s context examples, other\nactions are randomly sampled from the action schema out-\nside of the domain D of the current action. A model then\ntakes these prompts and transforms them into a sequence of\ntokens T(a) representing a as a PDDL action. An attempt\nis made to parse T(a) as a PDDL action a′. This is the first\nlocation at which automated evaluation is possible, as there\nare numerous reasons whyT(a) may fail to be a valid PDDL\n424\nListing 1: A context example from a prompt N(a) for the\nfly-airplane action from the logistics domain, including the\n”Allowed Predicates” which function as the domain specifi-\ncation ⟨F, N(F)⟩.\nAllowed Predicates:\n(in-city ?loc - place ?city - city) : a\nplace loc is in a city.\n(at ?obj - physobj ?loc - place) : a\nphysical object obj is at a place loc.\n(in ?pkg - package ?veh - vehicle) : a\npackage pkg is in a vehicle veh.\nInput:\nThe action, \"FLY-AIRPLANE\" will fly an\nairplane from one airport to another.\nAfter the action, the airplane will be\nin the new location.\nPDDL Action:\n(:action FLY-AIRPLANE\n:parameters (?airplane - airplane ?loc-\nfrom - airport ?loc-to - airport)\n:precondition (at ?airplane ?loc-from)\n:effect (and (not (at ?airplane ?loc-\nfrom)) (at ?airplane ?loc-to))\n)\naction, many of which can be extracted by just attempting to\nparse T(a). For all T(a) that were successfully parsed into a\nreconstructed PDDL actiona′, we add them to the set of suc-\ncessfully reconstructed actions A′. Next, for each a′ ∈ A′\nwe create a reconstructed domain D′ from D by replacing\nA with (A/a) ∪ a′ where a is the original action that gen-\nerated a′. Note that for our formulation A′ is not the set of\nactions for a D′, rather we look at |A′| new domains D′s for\neach action, inline with our action by action-based evalua-\ntion strategy. This is also due to practicality reasons, in order\nto use A′ for D′, all actions in the domain would need to get\nthrough the parsing phase in which T(a) is converted to a′,\nthis is simply not a reasonable assumption to make. Our task\nthen, is to evaluate the quality of each D′ with respect to D.\nDescription Classes\nWe investigate several strategies for converting PDDL ac-\ntion schema a ∈ Ato their natural language descriptions,\nN(a) ∈ N(A). Each strategy produces a distinct class of\nnatural language representations of the action model.\n1. Base Nb(A): Base descriptions include only information\nincluding the action name, parameters, and the parameter\ntypes of the action, as well as a one-line description of\nwhat the action does without explicitly mentioning any\npredicates. For example: “The action ’unstack’ will have\na hand unstack a block x from a block y. ”\n2. Flipped Nf(A): Flipped descriptions include the base\ndescriptions with an additional description of all predi-\ncates that are deleted preconditions in that action schema,\nthat is, for an action schema a ∈ A, Nf(a) is Nb(a) ex-\ntended with a description of predicates inpre(a)∩del(a)\nFigure 1: A high-level overview of our proposed task.\nas preconditions. The motivation behind this class is to\nevaluate if predicates that are explicitly changed are the\nmost important things to include in a natural language de-\nscription for the LLM, as they might be for a person when\ndescribing a domain. For example:“The action ’unstack’\nwill have a hand unstack a block x from a block y, if the\nblock x is clear, on top of y, and the hand is empty. ”\n3. Random Nr(A): Random descriptions act as a random\nbaseline to compare against flipped descriptions, as well\nas another higher information content baseline to com-\npare against base descriptions. For each action schema\na, the description includes the base description Nb(a),\nand descriptions of |pre(a) ∩ del(a)| random predicates\nsampled from pre(a), add(a) and del(a), where is the\ndescription is dependent on if the predicate was sampled\nfrom the precondition or effect. For example: “The ac-\ntion ’unstack’ will have a hand unstack a block x from\na block y, if the hand is empty and x is on y. After the\naction, y should be clear. ”.\nEvaluating\nWhen considering how to evaluate the performance of LLMs\non this task, note that LLMs will frequently output se-\nquences of tokens for our evaluation that cannot be inter-\npreted as a valid planning domain. Some of these errors are\nsyntax based while others are based on the semantics of the\nunderlying PDDL tokens. If a model does output a valid do-\nmain, it must be evaluated in terms of its quality.\nDomain Reconstruction Quality Metrics\nEvaluating the quality of a correctly generated planning do-\nmain is a difficult task. Current metrics such as human ex-\npert evaluation (Guan et al. 2023; Li et al. 2024; Hayton\net al. 2020) provide a rough but subjective measure that is\nimpossible to automate. Like Guan et al. (2023), we have de-\nsigned our task such that all generated domains are based on\n425\nan existing domain which we can evaluate with respect to a\nbaseline. We look at and evaluate two automated metrics for\nmeasuring the quality of generated domains. The first met-\nric, action reconstruction error, is a more traditional auto-\nmated metric that measures the distance between generated\nactions in domains, but we note it is a poor metric. We pro-\npose a second metric, heuristic domain equivalence, which\nprovides a more robust and tolerant approximation of true\ndomain equivalence.\nAction Reconstruction Error (ARE) The Action Recon-\nstruction Error (ARE) is a measure of how different two\naction schema a, a′ ∈ Aare. We define the action recon-\nstruction error as the size of the difference of predicates in\nthe precondition and effect between a and a′:\nARE(a, a′) =|pre(a) △ pre(a′)|+\n|add(a) △ add(a′)|+\n|del(a) △ del(a′)|\nwhere A△B is the symmetric difference(A/B)∪(B/A).\nThis metric is useful for understanding the distribution of\nhow close is the output domain (from the model) to the\noriginal domains. However, we claim that this metric is not\na good measurement of actual domain quality. It does not\ntake into account the fact that preconditions and effects can\nbe added or removed from an action without changing the\nmeaning of the action at all, for example, adding a static\npredicate from a precondition as an effect. To remedy this,\nwe propose an alternative metric based on how usable the\ndomain is for planning.\nPlan Applicability for Heuristic Domain Equivalence\nThe primary reason a planning domain is created is so that\nit can be used as the underlying representation for a set of\nproblems in the domain. The problems implicitly define a\nset of plans, and when reconstructing domains, we can mea-\nsure domain equivalence in terms of the equivalence of the\nsets of plans for a collection of problems. While it is not\npractical to check if the full set of plans is equivalent, it is\npossible to check for a number of plans on some problems\nwe care about in the domain.\nThe domain equivalence heuristic is computed as follows:\ngiven an original planning domain D, a reconstructed plan-\nning domain D′, and a set of solvable planning problems for\nD, PD, each problem Π ∈ PD can be transformed into a\nproblem Π′ ∈ PD′ that uses D′ as its underlying domain.\nFor each such pair of problems Π and Π′ and some corre-\nsponding subsets of their plans P ⊆ PΠ and P′ ⊆ PΠ′ ,\nwe can cross check whether P ⊆ PΠ′ and P′ ⊆ PΠ. For\neach individual plan, the test can be efficiently performed\nusing a plan validator 1. This heuristic, plan equivalence on\nP for a subset of plans, is a necessary condition for true do-\nmain equivalence, and its negation is a sufficient condition\nto show true domain inequality.\nResult Classes\nWe propose four result classes for classifying the action\nfrom an LLMs output. Each class other than the heuristically\n1https://github.com/KCL-Planning/V AL\nequivalent domain class has multiple sub-classes to give a\nbetter idea of the types of problems encountered.\n1. Syntax Error: The model produced syntactically invalid\nPDDL. This PDDL cannot be parsed to evaluate an ac-\ntion reconstruction error. Subclasses (in precedence or-\nder): (1) No PDDL (NoPDDL): Model did not output\nany PDDL, (2) Parenthesis Mismatch (PError): issues re-\ngarding the matching parenthesis in the PDDL (3) Unex-\npected Token (UToken): The PDDL parser failed after\nfinding an unexpected token.\n2. Semantic Error: The model produced syntactically\nvalid PDDL, but the PDDL doesn’t integrate with the\nintended problems. Subclasses (1) Type Error (TError):\nThe model produced an unexpected type (2) Predicate\nArgument Error (PAError): the wrong number of vari-\nables were passed to a predicate (3) Wrong Action Name\n(NError), The name of the action is wrong (4) Bad\nPrecondition (BPError): PDDL STRIPS does not allow\nnegated preconditions, but one is present.\n3. Different Domain: The model produced syntactically\nvalid PDDL that integrates with the original domain, but\nthe underlying domains are different by way of the do-\nmain equivalence heuristic. The behavior of the actions\nis not as intended, plans from the original domain cannot\nbe applied in the new domain and vice versa. Subclasses\n(1) No Plans Found (NoPlan): No plans were able to be\nfound on problems in the new domain (2) New Plan Ap-\nplication Error (NPApp): Could not apply a new plan to\nthe original domain (3) Original Plan Application Error\n(OPApp): The original plan could not be applied to the\nnew domain.\n4. (Heuristically) Equivalent Domain: The model pro-\nduced syntactically valid PDDL that integrates with the\ndesired domain under the domain equivalence heuristic,\nplans from the original domain can be applied in the new\ndomain and vice versa.\nThe classes form a hierarchy in which syntax errors super-\nsede semantic errors which supersede both the different and\nequivalent domain classes which are mutually exclusive.i.e.\nAn output with both syntax and semantic errors will only be\nmarked with the error caught first, the syntax error.\nExperiments and Results\nSetup\nWe evaluate the LLaMA family of LLMs (Touvron, Lavril,\nand Izacard 2023), as well as StarCoder (SC) (Li et al. 2023).\nFor LLaMA we evaluate both the base pre-trained models\nat 7b, 13b, 70b parameters. We also evaluate the 7b, 13b,\n70b LLaMA models that have been finetuned for chat us-\ning reinforcement learning with human feedback (RLHF)\n(Ouyang et al. 2022). For token selection for all models, we\nuse greedy sampling in which the token with the highest out-\nput probability is selected as the next token.\nFor our domains, we select 9 PDDL domains with vary-\ning action and predicate complexities. We include 2 recent\ndomains, “Forest” and “Delivery” from Yang et al. (2022),\n426\na domain “Heavy” from Silver et al. (2024) and a novel do-\nmain, “Trackbuilding”. The latter two domains are guaran-\nteed not to be in the training set, as they were created af-\nter LLaMA and StarCoder were trained; these domains are\nmarked with a dagger (†). The remainder of our domains\nare famous classical planning domains from various Inter-\nnational Planning Competitions.\n1. Blocksworld – 5 predicates 4 actions: A robot hand tries\nto stack blocks on a table in a particular configuration.\n2. Gripper – 4 predicates 3 actions: A robot moves balls\nfrom one room to another using grippers.\n3. Heavy†– 5 predicates 2 actions: Specified items must be\npacked into a box depending on item weight.\n4. Forest – 5 predicates 2 actions: Hikers must navigate to a\nlocation over varying terrain.\n5. Logistics – 3 predicates 6 actions: Items must be trans-\nported to locations using planes and trucks.\n6. Depot – 6 predicates 5 actions: A combination of blocks\nand logistics domains.\n7. Miconic – 6 predicates 4 actions: A lift delivers multi-\nple passengers to their desired floors from their starting\nfloors.\n8. Trackbuilding†– 4 predicates 3 actions: An agent must\nbuild a path for a train to take to a given location.\n9. Delivery - 7 predicates 3 actions: A delivery person must\ndeliver newspapers to a number of safe locations from a\nhome base.\nFor the domain equivalence heuristic, our problem set con-\nsists of 2 simple randomly selected problems from each do-\nmain. We select the top100 plans using the K∗ planner (Lee,\nKatz, and Sohrabi 2023). The top-k plans for a problem Π\nare the set of k different plans with the lowest costs, which\nin our case is the same as the length of the plan. While anyk\nplans could be used for computing the domain equivalence\nheuristic, using the top-k plans we ensure that minimally the\noptimal plans for the evaluated problems are equivalent. To\ntest for plan validity we use V AL.\nEvaluating Heuristic Domain Equivalence Over\nDifferent LLMs\nFor this experiment, we exclusively use base descriptions in\nwhich only a description of the action’s parameters and types\nwithout reference to predicates is provided. For prompt\ngeneration, each base action description is turned into 60\nprompts, each with 3 randomly sampled context examples\nfrom outside of its domain. We note that this sampling is\ndone uniformly across all types of actions, the only restric-\ntion being that the action used for context cannot be in the\nsame domain as the action we are generating for. We chose\nto use 60 prompts as a trade-off between experiment runtime\nand statistical significance. We chose to use 3 context exam-\nples after a manual parameter search; increasing the number\nof context examples further did not improve results and de-\ncreasing past 3 led to worse results.\nFigure 2 (Top) displays the breakdown of outputs over\nthe primary result classes. Two results are immediately ap-\nparent from this. First, LLMs particularly larger ones, are\nResult class Star LLaMA\n& subclass Coder 7b 7b-C 13b 13b-C 70b 70b-C\nSyntax\nNoPDDL 0.00 0.16 0.00 0.00 0.00 0.00 0.16\nPError 0.00 0.00 0.78 0.00 0.00 0.00 0.00\nUTok\nen 2.34 16.09 20.31 0.78 27.03 0.31 10.00\nTotal 2.34 16.25 21.09 0.78 27.03 0.31 10.16\nSemantics\nPAError 16.56 16.56 21.56 11.41 15.16 4.22 10.78\nNError 0.00 0.47 0.62 0.00 0.62 0.00 0.00\nTError 1.72 3.59 12.03 5.62 6.09 1.41 1.25\nBPError 0.00 0.00 0.16 0.16 0.00 0.00 0.00\nTotal 18.28 20.62 34.38 17.19 21.88 5.62 12.03\nDiff\nNoPlan 51.41 47.34 23.59 58.59 26.25 43.59 39.22\nNPApp 6.41 7.34 10.47 7.66 10.78 12.03 11.09\nOPApp 9.22 3.28 5.00 8.12 6.09 13.59 6.25\nTotal 67.03 57.97 39.06 74.38 43.12 69.22 56.56\nEquiv 12.34 5.16 5.47 7.66 7.97 24.84 21.25\nTable\n1: Distribution of result classes and subclasses. Lower\nis better for all classes and subclasses except equivalent do-\nmain (Equiv), for which higher is better. Best results in bold.\nquite good at generating syntactically and semantically valid\nPDDL, the best model LLaMA-2-70b, is able to construct\nvalid PDDL in 94% of domains. When looking at valid\nPDDL generated, we see that the ratio of heuristically equiv-\nalent domains to non-equivalent domains and the number\nof heuristically equivalent domains is largely dependent on\nmodel size (see Figure 3). The best result was on LLaMA-2-\n70b. It reconstructed 25% domains to be heuristically equiv-\nalent to the natural language descriptions. This is a very\npromising result in terms of the applicability of LLMs for\nthe task of PDDL domain generation. Second, in terms of\ndifferent types of models, it is surprising that the LLaMA\nchat models perform worse on this task than base LLaMA\nmodels across the board. Typically these models that have\nbeen trained with RLHF are seen to do better than base mod-\nels across the board (Ouyang et al. 2022).\nWe next turn to discuss result subclasses. Table 1 dis-\nplays the lopsided breakdown of syntax and semantic errors.\nThere were almost no instances of the No PDDL subclass,\nall models evaluated output something minimally interpre-\ntative as PDDL; except LLaMA 7b. Parenthesis mismatch\nerrors (PError) were also negligible. The overwhelming ma-\njority of syntax errors were unexpected token errors (UTo-\nken). This encompassed a whole range of issues from du-\nplicate “:precondition” tags to attempting to add type\nannotation to variables mentioned in predicates. For seman-\ntic errors, the primary breakdown was dominated by issues\nrelated to predicate-argument (PAError) counts where the\nmodel added or removed arguments to predicates in the ac-\ntion schema. Type errors (TError) were rare, we note that\nLLAMA 70b Chat performed best in this regard. Incorrect\naction name errors (NError) were exceedingly rare and Bad\nPrecondition (BPError) was rarer still. Of the semantically\nand syntactically valid domains, the majority were differ-\n427\nFigure 2: (Top) Characterizing LMM outputs in terms of core result classes. (Bottom) Breakdown of Diff domain subclasses.\nFigure 3: Overview of LLaMA result class percentages with\nrespect to model size. Contains both chat and base models.\nent domains. Different-domain subclasses displayed in Fig-\nure 2 (Bottom) and Table 1 reveal an interesting insight\ninto the quality of generated domains. The results show that\nacross the board, the different domains could not be used for\nplanning; the planner failed to produce any valid plan us-\ning the reconstructed problems in the domainPD′ (NoPlan).\nThe remaining different-domains failures are split relatively\nequally due to failures in cross-validating the new plans on\nthe original domains (NPApp) and vice versa (OPApp).\nEvaluating Heuristic Domain Equivalence Over\nDescription Classes and LLMs\nFor this experiment, we evaluate result classes over the three\nproposed description classes. To generate our prompts, we\nmap each action to 20 prompts in each of the 3 description\nclasses. The context for the prompts is taken from the same\ndescription class and is always taken from domains outside\nthe domain of the action to evaluate. For evaluation, we use\nthe same setup as our first experiment and evaluate over our\nresult classes. Figure 4 displays a breakdown of the perfor-\nmance of each model on each description class. The results\nshow that while on some models the flipped class performs\nwell, it is not consistent and not as statistically significant\nas we had predicted. We are surprised to see that the base\nclass performs on par with the random and flipped classes\non the LLaMA models, leading us to conclude that at least\nfor the classes we looked at where the number of predicates\nin flipped is small, the extra information provided by the\nrandom and flipped descriptions is not significant enough to\nsway the results for these models. The anomaly here is Star-\nCoder in which providing the extra context in the random\nand flipped classes boots its performance by around 10%.\nAction Reconstruction Error and Result Class\nFor this experiment, we evaluate the models in terms of\ntheir action reconstruction error to see how close from a\npredicate-by-predicate point of view the model gets to re-\nconstructing the original actions. Additionally, we investi-\ngate the the relationship between the action reconstruction\nerror and the result classes as well as how the action recon-\nstruction error may be used to augment our use of heuristic\ndomain equivalence. This experiment uses the same setup\nas the experiment over description classes, each a ∈ Ais\nmapped to Nb(a) and is used for 60 prompts. All prompts\nare evaluated on each LLM and result classes and ARE is\nevaluated for classes for all classes except syntax errors as\nARE cannot be automatically computed without a parsed ac-\ntion.\nFigure 5 displays the distributions of action reconstruc-\ntion errors (ARE) for each model, and splits each bucket by\nreconstruction class. This gives a good picture of how much\neach model deviates from the original action. We note that\nthe better-performing models tend to have their distributions\ncluster around lower AREs, that is, they construct actions\nthat are similar in terms of the exact predicates used in the\noriginal action. This additionally exposes the flaws of ARE\nas a metric for domain equivalence as we can see that just\n428\nFigure 4: Breakdown of LLMs over top level result classes vs different description classes.\nbeing close to the original action in terms of predicate simi-\nlarity is not good enough and that plenty of domains outside\nthis range are heuristically equivalent. This understanding\nof ARE can also help us find false positives in heuristically\nequivalent domains that are not truly equal since only a fi-\nnite number of problems and plans for each problem can be\nevaluated. Hence when searching for false positives it can be\nuseful to start with domains with the highest ARE since it is\nmore likely something with many predicates changed from\nthe original action represents a different domain.\nRelated Work\nLarge Language Models and Planning\nThere are been a number of papers that investigate the use of\nLLMs for planning. Some recent work (cf. Valmeekam et al.\n(2023); Raman et al. (2022)) use LLMs as planners, while\nothers (cf Guan et al. (2023); Liu et al. (2023)) use LLMs\nas auxiliary components of a hybrid planning system while\nleveraging automated planners for solving the planning task.\nThe general consensus seems to be that LLMs are not very\ngood as planners. This finding was one of the motivations\nfor this work in this work, as we focus on using LLMs to aid\nautomated planning rather than as planners themselves.\nLLM+P The LLM+P framework (Liu et al. 2023) was one\nof the first to recognize the potential of combining LLMs\nand planners as hybrid systems, and utilizing LLMs to east\nthe use of automated planners . The LLM+P architecture\ntakes in (1) natural language descriptions of problem in a\nplanning domain, (2) a context example of a natural lan-\nguage problem in the given domain being converted to a\nPDDL problem, and (3) a PDDL domain file. Using these\ninputs the model uses an underlying LLM to convert the nat-\nural language problem description and context into a PDDL\nproblem. This is then combine with the PDDL domain in-\nput to an automated planner producing a PDDL plan, the\nresulting plan is then fed into an LLM which describes the\nplan in natural language. LLM+P’s applicability is some-\nwhat hindered by their assumptions that a PDDL domain\nexists, and context examples converting natural language de-\nscriptions of problems to PDDL problems for these domains\nexist. Such assumptions are impossible to meet in the case\nof things like narrative action model acquisition, and indeed\nstill requires an expert in the system somewhere to write the\ndomains and the context examples. Our work does not focus\non using LLMs to generate PDDL tasks, but it is tangential\nto all of LLM+P’s assumptions. We (1) investigate the con-\nstruction the PDDL domain rather than have it provided and\n(2) do this using context examples from arbitrary domains\nrather than from the same domain.\nLLM-DM The most closely related work to ours is the\nend-to-end domain construction and planning framework\nfrom Guan et al. (2023) which we will call LLM-DM. LLM-\nDM is composed of a three-part process, automated domain\nconstruction, human refinement of domain, and planning\nwith the domain. We are interested primarily in their au-\ntomated domain construction as it is a very similar task to\nours. For this, LLM-DM generates a domain on an action-\nby-action basis, each prompt containing five parts: (1) an in-\nstruction describing the PDDL creation task, (2) one or two\ncontext examples from the blocksworld domain on what a\ntranslation of an action description to PDDL looks like, (3)\na natural language description of the domain, (4) a natural\nlanguage description of the action and (5) a dynamically up-\ndated list of predicates used by the domain including nat-\nural language action descriptions. As the domain is gener-\nated action-by-action, the instruction and context examples\ninclude requests for the model to generate a list of new pred-\n429\nFigure 5: Action Reconstruction Error (ARE) distribution with respect to reconstruction class over LLMs.\nicates based on the description of the action. LLM-DM eval-\nuates constructing PDDL on three domains (Logistics, Tyre-\nworld, and a custom domain, ”Household”) using the LLMs\nGPT-4 (OpenAI 2023) and GPT-3.5 Turbo (ChatGPT). To\nmeasure the quality of the constructed domain, manual hu-\nman evaluation is used, experts annotate the PDDL domain\noutput, marking the PDDL with mistakes and corrections,\nwhich the authors claim provides and approximate distance\nbetween the generated PDDL and correct PDDL.\nLLM-DM provided inspiration in our work to generate\ndomains using LLMs on an action-by-action basis rather\nthan trying to have the LLM output the full domain. The au-\nthors cite well-founded concerns about the context window\nsize and the potential for corrective feedback on an action-\nby-action basis, making this more useful for the end user.\nFor our work, instead of providing the model with a descrip-\ntion of the domain and having the model extract the pred-\nicates at each stage on-top of the action translation, we ex-\nplicitly provided the allowed predicates and their description\nas the description of the domain. This change is key for be-\ning able to automatically evaluate the constructed domains,\nand is responsible for our automated evaluation approaches\nrather than a manual evaluation approaches.\nTextual and Narrative Action Model Acquisition\nThe task we propose is similar to the action-model extraction\nfrom text task (Lindsay et al. 2017) and narrative action-\nmodel acquisition task from text task (Hayton et al. 2020;\nLi et al. 2024) in which the goal is from natural language\nto generate the entire domain model from Fg and Ag if\ngrounded and F, A, and potentially C if lifted. A downside\nof these tasks is that it very difficult to automatically evalu-\nate performance on, as it requires a full understanding of the\nnatural language text and expert knowledge of PDDL do-\nmains. Evaluation for these tasks is frequently done either\nvia expert analysis of the generated PDDL domain such as\nin (Hayton et al. 2020; Huang, Chen, and Zhang 2014) or\nautomated metrics such as that can’t fully capture the per-\nformance of the model. These shortcomings in evaluation\nwere a driver of both our problem formulation and proposed\ndomain quality metrics.\nConclusion and Future Work\nThere are many avenues that could be explored using this\nwork as a springboard. In particular, we are interested in\nthree main directions: (1) deeper investigations of the ca-\npabilities of large language models in terms of selection and\ntuning, (2) using re-prompting for fixing mistakes in PDDL\nfor chat-based LLMs, (3) investigating more robust tasks\nand metrics.\nFirst, in terms of LLMs there is a lot that could be done\nto extend this work. The results showing improved perfor-\nmance on larger models are a good starting point for future\nwork and are in line with Guan et al. (2023) which evalu-\nates with respect to GPT-4 and GPT 3.5. coming to similar\nconclusions that larger pre-trained models are better when it\ncomes to handling PDDL construction. Future work and ap-\nplications not interested in tuning should take this into con-\nsideration using larger models such as GPT-4 and LLaMA-\n70b as baselines, other large models such as Bloom (Big-\nScience Workshop 2022) would be promising to evaluate\nover. Our experiment over description classes revealed the\ncoding model StarCoder performs quite well in certain cases\nwhen additional predicate information is included in natural\nlanguage descriptions, we believe this warrants a further in-\nvestigation of coding models and their capabilities. Beyond\njust the selection of LLMs, there are two more properties of\nLLMs we could investigate. First, LLM tuning approaches,\nsuch as finetuning and prompt tuning have been shown to\nallow small LLMs to perform well on tasks they are tuned\non. Second, chat-based LLMs with large context windows\ncan be re-prompt and provide corrective feedback (Raman\net al. 2022). Guan et al. (2023) successfully demonstrate\ncorrective reprompting from tools like V AL and other re-\nprompting to provide corrective feedback to LLMs. Using\nour result classification system, adding support for correc-\ntive reprompting where the re-prompt is based on informa-\ntion regarding the result class is a clear next step.\nFinally, we discuss potential alternatives that could be\nmade to our evaluation. As discussed in our approach, we\ndo not use A′ as the set of action schema for a D′ for a\nnumber of practical reasons. However, evaluating the per-\nformance of domains in which all actions are generated is a\ndesirable target for evaluation. Towards this end, it would be\ninteresting to evaluate with respect to a form of iterative do-\nmain completion task after an initial action has been gener-\nated. Previously generated actions in A′ could then be used\nas part of the prompt until a full reconstructed action schema\nfor the reconstructed domain D′ has been constructed.\nAcknowledgments\nThis work is supported by IBM Research through the Rens-\nselaer IBM AI Research Collaboration (https://airc.rpi.edu/)\n430\nReferences\nBiere, A.; Heule, M.; van Maaren, H.; and Walsh, T., eds.\n2021. Handbook of Satisfiability - Second Edition, volume\n336 of Frontiers in Artificial Intelligence and Applications.\nIOS Press.\nBigScience Workshop. 2022. BLOOM (Revision 4ab0472).\nCorrˆea, A. B.; and Seipp, J. 2022. Best-First Width Search\nfor Lifted Classical Planning. In ICAPS, 11–15. AAAI\nPress.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.;\nSun, X.; Xu, J.; and Sui, Z. 2022. A Survey on In-context\nLearning.\nGhallab, M.; Howe, A.; Knoblock, C.; McDermott, D.; Ram,\nA.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL —\nThe Planning Domain Definition Language. Technical Re-\nport.\nGhallab, M.; Nau, D. S.; and Traverso, P. 2004. Automated\nplanning - theory and practice. Elsevier.\nGuan, L.; Valmeekam, K.; Sreedharan, S.; and Kambham-\npati, S. 2023. Leveraging Pre-trained Large Language Mod-\nels to Construct and Utilize World Models for Model-based\nTask Planning. In NeurIPS.\nHarrison, J.; Urban, J.; and Wiedijk, F. 2014. History of\nInteractive Theorem Proving. In Computational Logic, vol-\nume 9 of Handbook of the History of Logic, 135–214. Else-\nvier.\nHayawi, K.; Shahriar, S.; and Mathew, S. S. 2024. The imi-\ntation game: Detecting human and AI-generated texts in the\nera of ChatGPT and BARD.Journal of Information Science.\nHayton, T.; Porteous, J.; Ferreira, J. F.; and Lindsay, A.\n2020. Narrative Planning Model Acquisition from Text\nSummaries and Descriptions. In AAAI, 1709–1716. AAAI\nPress.\nHelmert, M. 2006. The Fast Downward Planning System. J.\nArtif. Intell. Res., 26: 191–246.\nHuang, R.; Chen, Y .; and Zhang, W. 2014. SAS+ Planning\nas Satisfiability. J. Artif. Intell. Res., 43: 293–328.\nLee, J.; Katz, M.; and Sohrabi, S. 2023. On K* Search for\nTop-k Planning. In Proceedings of the 16th Annual Sympo-\nsium on Combinatorial Search (SoCS 2023). AAAI Press.\nLi, R.; Ben allal, L.; Zi, Y .; Muennighoff, N.; Kocetkov, D.;\net al. 2023. StarCoder: may the source be with you!Transac-\ntions on Machine Learning Research. Reproducibility Cer-\ntification.\nLi, R.; Cui, L.; Lin, S.; and Haslum, P. 2024. NaRuto: Auto-\nmatically Acquiring Planning Models from Narrative Texts.\nIn AAAI, volume 38, 20194–20202. AAAI Press.\nLindsay, A.; Read, J.; Ferreira, J. F.; Hayton, T.; Porteous,\nJ.; and Gregory, P. 2017. Framer: Planning Models from\nNatural Language Action Descriptions. In ICAPS, 434–442.\nAAAI Press.\nLiu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas,\nJ.; and Stone, P. 2023. LLM+P: Empowering Large\nLanguage Models with Optimal Planning Proficiency.\narXiv:2304.11477.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; et al. 2022.\nTraining language models to follow instructions with human\nfeedback. In NeurIPS.\nRaman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius, D.;\nand Tellex, S. 2022. Planning With Large Language Models\nVia Corrective Re-Prompting. In NeurIPS 2022 Foundation\nModels for Decision Making Workshop.\nSilver, T.; Dan, S.; Srinivas, K.; Tenenbaum, J. B.; Kael-\nbling, L. P.; and Katz, M. 2024. Generalized Planning in\nPDDL Domains with Pretrained Large Language Models.\nIn AAAI Conference on Artificial Intelligence (AAAI). AAAI\nPress.\nTouvron, H.; Lavril, T.; and Izacard, G. 2023. LLaMA:\nOpen and Efficient Foundation Language Models.\narXiv:2302.13971.\nValmeekam, K.; Marquez, M.; Sreedharan, S.; and Kamb-\nhampati, S. 2023. On the Planning Abilities of Large Lan-\nguage Models - A Critical Investigation. In Oh, A.; Neu-\nmann, T.; Globerson, A.; Saenko, K.; Hardt, M.; and Levine,\nS., eds., Advances in Neural Information Processing Sys-\ntems, volume 36, 75993–76005. Curran Associates, Inc.\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\nAttention is All you Need. In NIPS.\nYang, R.; Silver, T.; Curtis, A.; Lozano-P´erez, T.; and Kael-\nbling, L. P. 2022. PG3: Policy-Guided Planning for Gener-\nalized Policy Generation. In IJCAI, 4686–4692.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.; and rong Wen, J. 2023a. A Survey of\nLarge Language Models. arXiv:2303.18223.\nZhao, Z.; Song, S.; Duah, B.; Macbeth, J.; Carter, S. A.; Van,\nM. P.; Bravo, N. S.; Klenk, M.; Sick, K.; and Filipowicz, A.\nL. S. 2023b. More human than human: LLM-generated nar-\nratives outperform human-LLM interleaved narratives. In\nCreativity & Cognition, 368–370. ACM.\n431",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7718547582626343
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6850865483283997
    },
    {
      "name": "Domain model",
      "score": 0.6025710105895996
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.5732608437538147
    },
    {
      "name": "Natural language",
      "score": 0.5300459861755371
    },
    {
      "name": "Domain-specific language",
      "score": 0.5043493509292603
    },
    {
      "name": "IBM",
      "score": 0.48208242654800415
    },
    {
      "name": "Natural language processing",
      "score": 0.4651744067668915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44799065589904785
    },
    {
      "name": "Domain analysis",
      "score": 0.44234007596969604
    },
    {
      "name": "Language model",
      "score": 0.43888595700263977
    },
    {
      "name": "Software engineering",
      "score": 0.3337762951850891
    },
    {
      "name": "Programming language",
      "score": 0.2999696135520935
    },
    {
      "name": "Domain knowledge",
      "score": 0.2697541117668152
    },
    {
      "name": "Software",
      "score": 0.11095026135444641
    },
    {
      "name": "Software development",
      "score": 0.08418133854866028
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Software construction",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Nanotechnology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165799507",
      "name": "Rensselaer Polytechnic Institute",
      "country": "US"
    }
  ]
}