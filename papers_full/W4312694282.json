{
    "title": "Affine Medical Image Registration with Coarse-to-Fine Vision Transformer",
    "url": "https://openalex.org/W4312694282",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Mok, Chi Wing",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": null,
            "name": "Chung, Chi Shing",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2083099567",
        "https://openalex.org/W3204283261",
        "https://openalex.org/W2344328023",
        "https://openalex.org/W2063815712",
        "https://openalex.org/W2507987841",
        "https://openalex.org/W2150534249",
        "https://openalex.org/W2156875677",
        "https://openalex.org/W3204825373",
        "https://openalex.org/W3092446792",
        "https://openalex.org/W3035201239",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W2963720324",
        "https://openalex.org/W2891590469",
        "https://openalex.org/W1992265439",
        "https://openalex.org/W4241074797",
        "https://openalex.org/W6798046796",
        "https://openalex.org/W2979786163",
        "https://openalex.org/W6697168704",
        "https://openalex.org/W3173512372",
        "https://openalex.org/W6753441378",
        "https://openalex.org/W6794262402",
        "https://openalex.org/W6638444622",
        "https://openalex.org/W2787740020",
        "https://openalex.org/W4214669216",
        "https://openalex.org/W2133584444",
        "https://openalex.org/W3201337522",
        "https://openalex.org/W6796931752",
        "https://openalex.org/W6794345597",
        "https://openalex.org/W2136145485",
        "https://openalex.org/W2799913653",
        "https://openalex.org/W6803356969",
        "https://openalex.org/W2983976417",
        "https://openalex.org/W6788564147",
        "https://openalex.org/W3199376581",
        "https://openalex.org/W3047142913",
        "https://openalex.org/W1993495699",
        "https://openalex.org/W6966651814",
        "https://openalex.org/W6797533734",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6618372016",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W6794906783",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2133287637",
        "https://openalex.org/W3211573487",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W3156621598",
        "https://openalex.org/W603908379",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3098269293",
        "https://openalex.org/W3103052966",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W2963623257",
        "https://openalex.org/W3176521625",
        "https://openalex.org/W2767463438",
        "https://openalex.org/W2296294727",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W3100018800"
    ],
    "abstract": "Affine registration is indispensable in a comprehensive medical image registration pipeline. However, only a few studies focus on fast and robust affine registration algorithms. Most of these studies utilize convolutional neural networks (CNNs) to learn joint affine and non-parametric registration, while the standalone performance of the affine subnetwork is less explored. Moreover, existing CNN-based affine registration approaches focus either on the local misalignment or the global orientation and position of the input to predict the affine transformation matrix, which are sensitive to spatial initialization and exhibit limited generalizability apart from the training dataset. In this paper, we present a fast and robust learning-based algorithm, Coarseto-Fine Vision Transformer (C2FViT), for 3D affine medical image registration. Our method naturally leverages the global connectivity and locality of the convolutional vision transformer and the multi-resolution strategy to learn the global affine registration. We evaluate our method on 3D brain atlas registration and template-matching normalization. Comprehensive results demonstrate that our method is superior to the existing CNNs-based affine registration methods in terms of registration accuracy, robustness and generalizability while preserving the runtime advantage of the learning-based methods. The source code is available at https://github.com/cwmok/C2FViT.",
    "full_text": ",/978\u000474(\u0004\u0011\u0004-278-898-32%0\u00046)437-836=\n4YFPMWLIV\n'ST]VMKLX\u0004\n8LMW\u0004ZIVWMSR\u0004MW\u0004EZEMPEFPI\u0004EX\u0004,/978\u000474(\u0004\u0011\u0004-RWXMXYXMSREP\u00046ITSWMXSV]\u0004\fŚƚƚƉƐ͗ͬͬƌĞƉŽƐŝƚŽƌǇ͘ŚŬƵƐƚ͘ĞĚƵ͘ŚŬͬŝƌ\r\n-J\u0004MX\u0004MW\u0004XLI\u0004EYXLSV\u000bW\u0004TVI\u0011TYFPMWLIH\u0004ZIVWMSR\u0010\u0004GLERKIW\u0004MRXVSHYGIH\u0004EW\u0004E\u0004VIWYPX\u0004SJ\u0004TYFPMWLMRK\u0004TVSGIWWIW\u0004\nWYGL\u0004EW\u0004GST]\u0011IHMXMRK\u0004ERH\u0004JSVQEXXMRK\u0004QE]\u0004RSX\u0004FI\u0004VIJPIGXIH\u0004MR\u0004XLMW\u0004HSGYQIRX\u0012\u0004*SV\u0004E\u0004HIJMRMXMZI\u0004\nZIVWMSR\u0004SJ\u0004XLMW\u0004[SVO\u0010\u0004TPIEWI\u0004VIJIV\u0004XS\u0004XLI\u0004TYFPMWLIH\u0004ZIVWMSR\u0012\n:IVWMSR\n(3-\n8MXPI\n%YXLSVW\n7SYVGI\n'SRJIVIRGI\nAffine Medical Image Registration with Coarse-To-Fine Vision Transformer \nMok, Chi Wing; Chung, Chi Shing\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR), v. 2022, June 2022, p. 20803-20812\nAccepted Version\n10.1109/CVPR52688.2022.02017\nIEEE\n© 2022 IEEE.  Personal use of this material is permitted.  Permission from IEEE \nmust be obtained for all other uses, in any current or future media, including \nreprinting/republishing this material for advertising or promotional purposes, \ncreating new collective works, for resale or redistribution to servers or lists, or reuse \nof any copyrighted component of this work in other works.\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New \nOrleans, USA, 18 - 24 June 2022 \nAfﬁne Medical Image Registration with Coarse-to-Fine Vision Transformer\nTony C. W. Mok, Albert C. S. Chung\nDepartment of Computer Science and Engineering,\nThe Hong Kong University of Science and Technology\ncwmokab@connect.ust.hk, achung@cse.ust.hk\nAbstract\nAfﬁne registration is indispensable in a comprehensive\nmedical image registration pipeline. However , only a few\nstudies focus on fast and robust afﬁne registration algo-\nrithms. Most of these studies utilize convolutional neural\nnetworks (CNNs) to learn joint afﬁne and non-parametric\nregistration, while the standalone performance of the afﬁne\nsubnetwork is less explored. Moreover , existing CNN-based\nafﬁne registration approaches focus either on the local mis-\nalignment or the global orientation and position of the in-\nput to predict the afﬁne transformation matrix, which are\nsensitive to spatial initialization and exhibit limited gener-\nalizability apart from the training dataset. In this paper , we\npresent a fast and robust learning-based algorithm, Coarse-\nto-Fine Vision Transformer (C2FViT), for 3D afﬁne medi-\ncal image registration. Our method naturally leverages the\nglobal connectivity and locality of the convolutional vision\ntransformer and the multi-resolution strategy to learn the\nglobal afﬁne registration. We evaluate our method on 3D\nbrain atlas registration and template-matching normaliza-\ntion. Comprehensive results demonstrate that our method\nis superior to the existing CNNs-based afﬁne registration\nmethods in terms of registration accuracy, robustness and\ngeneralizability while preserving the runtime advantage of\nthe learning-based methods. The source code is available\nat https://github.com/cwmok/C2FViT.\n1. Introduction\nRigid and afﬁne registration is crucial in a variety of\nmedical imaging studies and has been a topic of active re-\nsearch for decades. In a comprehensive image registration\nframework, the target image pair is often pre-aligned based\non a rigid or afﬁne transformation before using deformable\n(non-rigid) registration, eliminating the possible linear and\nlarge spatial misalignment between the target image pair.\nSolid structures such as bones can be aligned well with\nrigid and afﬁne registration [ 29, 37]. In conventional im-\nage registration approaches, inaccurate pre-alignment of the\nConvolutional Patch \nEmbedding\nTransformer Encoder \nMLP\nMLP HeadMLP\nGlobal average pooling\nࢂࡷࡽ \n×ࡺ\nConvolutional \nFeed-Forward\nMulti-Head \nAttention\nࣛࣛ\nࣛ\nb) Siamese Network(a) Concatenation (c) C2FViT\nConvolutional Layers\nTransformer Encoder\nFigure 1. Comparisons of different architectures for afﬁne regis-\ntration. The concatenation-based (VTN-Afﬁne [ 46]) and Siamese\nnetwork (ConvNet-Afﬁne [11]) approaches are based on convolu-\ntional neural networks, while our proposed C2FViT is based on vi-\nsion transformers. For brevity, we illustrate 1-level C2FViT only.\nLocal and global operations are in green and purple, respectively.\nimage pair may impair the registration accuracy or impede\nthe convergence of the optimization algorithm, resulting in\nsub-optimal solutions [47]. The success of recent learning-\nbased deformable image registration approaches has largely\nbeen fueled [3,9,11,17,19,20,34–36] by accurate afﬁne ini-\ntialization using conventional image registration methods.\nWhile the conventional approaches excel in registration per-\nformance, the registration time is dependent on the degree\nof misalignment between the input images and can be time-\nconsuming with high-resolution 3D image volumes. To fa-\ncilitate real-time automated image registration, a few stud-\nies [21, 22, 40, 46] have been proposed to learn joint afﬁne\nand non-parametric registration with convolutional neural\nnetworks (CNNs). However, the standalone performance of\n20835\nthe afﬁne subnetwork compared to the conventional afﬁne\nregistration algorithm is less explored. Moreover, consider-\ning that afﬁne transformation is global and generally targets\nthe possible large displacement, we argue that CNNs are not\nthe ideal architecture to encode the orientation and absolu-\ntion position of the image scans in Cartesian space or afﬁne\nparameters due to the inductive biases embedded into the\narchitectural structure of CNNs.\nIn this paper, we analyze and expose the generic inabil-\nity and limited generlizability of CNN-based afﬁne regis-\ntration methods in cases with large initial misalignment and\nunseen image pairs apart from the training dataset. Moti-\nvated by the recent success of vision transformer models\n[10, 12, 41, 43, 44], we depart from the existing CNN-based\napproaches and propose a coarse-to-ﬁne vision transformer\n(C2FViT) dedicated to 3D medical afﬁne registration. To\nthe best of our knowledge, this is the ﬁrst learning-based\nafﬁne registration approach that considers the non-local de-\npendencies between input images when learning the global\nafﬁne registration for 3D medical image registration.\nThe main contributions of this work are as follows:\n• we quantitatively investigate and analyze the registra-\ntion performance, robustness and generalizability of\nexisting learning-based afﬁne registration methods and\nconventional afﬁne registration methods in 3D brain\nregistration;\n• we present a novel learning-based afﬁne registration\nalgorithm, namely C2FViT, which leverages convo-\nlutional vision transformers with the multi-resolution\nstrategy. C2FViT outperforms the recent CNN-based\nafﬁne registration approaches while demonstrating su-\nperior robustness and generalizability across datasets;\n• the proposed learning paradigm and objective func-\ntions can be adapted to a variety of parametric regis-\ntration approaches with minimum effort.\nWe evaluate our method on two tasks: template-\nmatching normalization to MNI152 space [ 13–15] and 3D\nbrain atlas registration in native space. Results demonstrate\nthat our method not only achieves superior registration per-\nformance over existing CNN-based methods, but the trained\nmodel also generalizes well to an unseen dataset beyond\nthe training dataset, reaching the registration performance\nof conventional afﬁne registration methods.\n2. Related Work\n2.1. Learning-based Afﬁne Registration Methods\nConventional approaches often formulate the afﬁne reg-\nistration problem to an iterative optimization problem,\nwhich optimizes the afﬁne parameters directly using adap-\ntive gradient descent [ 1, 25] or convex optimization [ 18].\nWhile conventional approaches excel in registration ac-\ncuracy, the registration time is subject to the complex-\nity and resolution of the input image pairs. Recently,\nmany learning-based approaches have been proposed for\nfast afﬁne registration. These approaches signiﬁcantly ac-\ncelerate the registration time by formulating the afﬁne reg-\nistration problem as a learning problem using CNNs and cir-\ncumventing the costly iterative optimization in conventional\napproaches. Existing CNN-based afﬁne registration ap-\nproaches can be divided into two categories: concatenation-\nbased [ 21, 22, 33, 46] and Siamese network approaches\n[5, 11, 38] as shown in ﬁgure 1.\nZhao et al. [ 46] propose a concatenation-based afﬁne\nsubnetwork that concatenates the ﬁxed and moving images\nas input, and exploits single-stream CNNs to extract the fea-\ntures based on the local misalignment of the input. Consid-\nering afﬁne registration is global, their method is not ca-\npable of input with large initial misalignment as the afﬁne\nsubnetwork lacks global connectivity and only focuses on\nthe overlapping region between two image spaces. In con-\ntrast to the concatenation-based method, de V os et al. [ 11]\npropose an unsupervised afﬁne registration method using\nthe Siamese CNN architecture for ﬁxed and moving im-\nages. A global average pooling [ 27] is applied to the end\nof each pipeline in order to extract one feature per feature\nmap, forcing the networks to encode orientations and afﬁne\ntransformations globally. Although their network focuses\non the global high-level geometrical features of separated\ninput, their method completely ignores the local features\nof the initial misalignment between the input image pair.\nMoreover, a recent study [28] demonstrates that a pure CNN\nencoder fails spectacularly in a seemingly trivial coordinate\ntransform problem, implying that a pure CNN encoder may\nnot be an ideal architecture to encode the orientations and\nabsolution positions of the image scans in Cartesian space\nor to afﬁne parameters. Shen et al. [ 40] also report that\nCNN-based afﬁne registration methods do not perform well\nin practice, even for deep CNNs with large receptive ﬁelds.\nIt is worth noting that most of the existing CNN-based\nafﬁne registration methods [5,11,21,22,38,46] jointly eval-\nuate the afﬁne and deformable registration performance or\ncompletely ignore the standalone performance of the afﬁne\nsubnetwork compared to the conventional afﬁne registration\nalgorithms. As inaccurate afﬁne pre-alignment of the image\npair may impair the registration accuracy or impede the con-\nvergence of the deformable registration algorithm [ 40, 47],\na comprehensive evaluation of the CNN-based afﬁne regis-\ntration methods should by no means be ignored.\n2.2. Vision Transformer\nCNNs architecture generally has limitations in mod-\nelling explicit long-range dependencies due to the intrinsic\ninductive biases, i.e., weight sharing and locality, embedded\n20836\ninto the architectural structure of CNNs. Recently, Dosovit-\nskiy et al. [ 12] proposed a pioneering work, Vision Trans-\nformer (ViT), for image classiﬁcation and proved that a pure\ntransformer [ 41] architecture can attain a state-of-the-art\nperformance. Compared to CNN-based approaches, ViT\noffers less image-speciﬁc inductive bias and has tremen-\ndous potential when training in large scale datasets. Wang\net al. [ 43] develop a pyramid architectural design for a\npure transformer model to imitate the multi-scale strategy\nin CNNs, achieving promising results in various computer\nvision tasks. Subsequent studies [6–8,10,16,26,42,44] fur-\nther extend ViT to pyramid architectural design and intro-\nduce convolutions to ViT. These studies demonstrate that\nintroducing moderate convolutional inductive bias to ViT\nimproves the overall performance, especially for training\nwith small datasets. Apart from pure ViT methods, Zhang et\nal. [45] and Chen et al. [4] combine CNN encoder-decoder\nwith transformer for deformable registration.\nWhile CNNs have achieved remarkable success in de-\nformable medical image registration, we argue that CNNs\nare not an ideal architecture for modelling and learning\nafﬁne registration. In contrast to deformable image reg-\nistration, afﬁne registration is often used to mitigate and\nremove large linear misalignment, which is considered to\nbe a global operation and contradicts the inductive bias\nembedded in the architectural structure of CNNs. Build-\ning on the insights of ViT and its variants [ 10, 12, 43, 44],\nwe depart from the CNNs architecture and propose a pure\ntransformer-based method dedicated to 3D medical afﬁne\nregistration.\n3. Method\nLet F, M be ﬁxed and moving volumes deﬁned over a\nn-D mutual spatial domain Ω ⊆ Rn. In this paper, we fo-\ncus on 3D afﬁne medical image registration, i.e., n =3\nand Ω ⊆ R3. For simplicity, we further assume that F\nand M are single-channel, grayscale images. Our goal is to\nlearn the optimal afﬁne matrix that align F and M. Specif-\nically, we parametrized the afﬁne registration problem as a\nfunction f\nθ(F,M )= A using a coarse-to-ﬁne vision trans-\nformer (C2FViT), where θ is a set of learning parameters\nand Arepresents the predicted afﬁne transformation matrix.\n3.1. Coarse-to-ﬁne Vision Transformer (C2FViT)\nThe overall pipeline of our method is depicted in ﬁgure\n2. Our method has been divided into L stages that solves\nthe afﬁne registration in a coarse-to-ﬁne manner with an\nimage pyramid. All stages share an identical architecture\nconsisting of a convolutional patch embeddinglayer and N\ni\ntransformer encoder blocks, where Ni denotes the number\nof transformer blocks in stage i. Each transformer encoder\nblock consists of an alternating multi-head self-attention\nmodule and a convolutional feed-forward layer, as depicted\nin ﬁgure 1. We use L =3 and N\ni =4 for each stage i\nthroughout this paper. Speciﬁcally, we ﬁrst create the in-\nput pyramid by downsampling the input F and M with tri-\nlinear interpolation to obtain F\ni ∈{ F1,F2,...,F L} (and\nMi ∈{ M1,M2,...,M L}), where Fi represents the down-\nsampled F with a scale factor of 0.5L−i and FL = F.\nWe then concatenate Fi and Mi, and the concatenated in-\nput is subjected to the convolutional patch embedding layer.\nDifferent from the prior Transformer-based architectures\n[10, 12, 43, 44], we prune all the layer normalization opera-\ntions as we did not observe noticeable effects on the image\nregistration performance in our experiments. Next, a stack\nof N\ni transformer encoder blocks take as input the image\npatch embedding map and output the feature embedding of\nthe input. C2FViT solves the afﬁne registration problem in\na coarse-to-ﬁne manner, and the intermediate input moving\nimage M\ni is transformed via progressive spatial transfor-\nmation. Additionally, for stage i> 1, a residual connection\nfrom the output embeddings (tokens) of the previous stage\ni−1 is added to the patch embeddings of the current stage\ni. Finally, the estimated afﬁne matrix A\nL of the ﬁnal stage\nis adopted as the output of our model fθ.\n3.1.1 Locality of C2FViT\nWhile the ViT model [ 12] excels in modelling long-range\ndependencies within a sequence of non-overlapping im-\nage patches due to the self-attention mechanism, the vi-\nsion transformer model lacks locality mechanisms to model\nthe relationship between the input patch and its neighbours.\nTherefore, we follow [ 26, 42, 44] to add locality to our\ntransformers in C2FViT. Speciﬁcally, we mainly improve\nthe transformer in two aspects: patch embedding and feed-\nforward layer.\nAs shown in ﬁgure 2, we depart from the linear patch\nembedding approach [ 12] and adopt convolutional patch\nembedding [42, 44] instead. The goal of the convolutional\npatch embedding layer is to convert the input images into\na sequence of overlapping patch embeddings. Formally,\ngiven a concatenated input I ∈ R\nH×W×D×C, where H, W\nand D denote the spatial dimension of I, and C is the num-\nber of channels, the convolutional patch embedding layer\nutilizes a 3D convolution layer to compute the patch em-\nbedding map Z ∈ R\nHi×Wi×Di×d of I. Speciﬁcally, the\nkernel size, stride, number of zero-paddings and number of\nfeature maps of the 3D convolution layer are denoted ask\n3,\ns, p and d, respectively. Next, the patch embedding map Z\nis then ﬂattened into a sequence of patch embeddings (to-\nkens) {ˆZ\ni ∈ Rd|i =1 ,...,N }, where N = HiWiDi and\nd is the embedding dimension. The patch embeddings can\nbe aggregated into a matrix ˆZ ∈ RN×d. We restrict the\nnumber of patches N to 4096 and the embedding dimen-\nsion d to 256 for all convolutional patch embedding layers\nin C2FViT by varying the stride s of the convolution layer,\n20837\nConvolutional Patch \nEmbedding\nTransformer \nEncoder \nMLP \nHead\nConvolutional Patch \nEmbedding\nTransformer \nEncoder \nMLP \nHead\nConvolutional Patch \nEmbedding\nTransformer \nEncoder \nMLP \nHead\nStage 1 Stage 2 Stage 3\nFixed image \nMoving image \nPosition Embedding\nElement-wise Addition\nS Spatial Transform\nS S\nଵ\nଶ\nଷ\nଶ\nଵ\nଷ\n×ଵ ×ଶ ×ଷ\nଷ\nFigure 2. Overview of the proposed Coarse-to-Fine Vision Transformer (C2FViT). The entire model is divided into three stages, solving\nthe afﬁne registration in a coarse-to-ﬁne manner.\ni.e., s =( H\n16 , W\n16 , D\n16 ). Moreover, we enforce the window\noverlapping to the sliding window of the convolution oper-\nation by setting k to 2s−1, and pad the feature with zeros\n(p = ⌊\nk\n2 ⌋). In contrast to the linear patch embedding in ViT,\nthe convolutional patch embedding in C2FViT helps model\nlocal spatial context and features across the ﬁxed and mov-\ning images. It also provides ﬂexibility to adjust the number\nand feature dimensions of patch embeddings. On the other\nhand, the feed-forward layer in ViT consists of a MLP block\nwith two hidden layers. In the transformer encoder, the\nfeed-forward layer is the only local and translation equiv-\nariance. Since the feed-forward layer in ViT is applied to\nthe patch embeddings map in a patch-wise manner, it lacks\na local mechanism to model the relationship between adja-\ncent patch embeddings. As such, we add a 3×3×3depth-\nwise convolution layer in between two hidden layers of a\nMLP block in the feed-forward layer of C2FViT [ 26, 42].\nThe depth-wise convolution further introduces locality into\nthe transformer encoder of C2FViT.\n3.1.2 Global Connectivity of C2FViT\nTransformers excel in modelling long-range dependen-\ncies within a sequence of embedding owing to their self-\nattention mechanism. In contrast to existing CNN-based\nafﬁne registration approaches, the misalignment and the\nglobal relationship between the ﬁxed and moving images\ncan be captured and modelled by the similarity between\nthe projected query-key pairs in transformer encoders of\nC2FViT, yielding the attention score for each patch embed-\nding. Speciﬁcally, the query Q,k e y K, and value V are\na linearly projection of the patch embeddings (tokens), i.e.,\nQ = ˆZW\nQ, K = ˆZWK and V = ˆZWV . We further ex-\ntend the self-attention module to a multi-head self-attention\n(MHA) module [41]. Given the number of attention heads\nis h, the linear projection matrices W\nQ\nj , WK\nj and WV\nj for\neach attention head j are the same size, i.e., WQ\nj , WK\nj ,\nWV\nj ∈ Rd×dh and dh = d\nh. Following the self-attention\nmechanism [12, 41] in the original transformer, our atten-\ntion operation for attention head j is computed as:\nAttention(Qj,Kj,Vj)=S o f t m a x (QjKT\nj√dh\n)Vj (1)\nwhere dh is the embedding dimension for the attention\nhead. At the end, the attended embeddings of all atten-\ntion heads are concatenated and linear projected by a ma-\ntrix W\nO ∈ Rd×d. In this study, we employ h =2 attention\nheads and d = 256embedding dimension for all the trans-\nformer encoders.\n3.1.3 Progressive Spatial Transformation\nWe adopt the multiresolution strategy into our architectural\ndesign. Speciﬁcally, a classiﬁcation head, which is imple-\nmented by two successive multilayer perceptrons (MLP)\nlayers with the hyperbolic tangent (Tanh) activation func-\ntion, is appended at the end of each stage in C2FViT. The\nclassiﬁcation head takes as input the averaged patch-wise\npatch embedding and outputs a set of afﬁne transforma-\ntion parameters. In the intermediate stage i, the derived\nafﬁne matrix is used to progressively transform the moving\nimage M\ni+1 with a spatial transformer [ 23]. The warped\nmoving image Mi+1 is then concatenated with ﬁxed im-\nage Fi+1 and taken as input for stage i +1 . With the\nproposed progressive spatial transformation, the linear mis-\nalignment of the input images can easily be eliminated with\nlow-resolution input, and the transformers from the higher\nlevel can focus on the complex misalignment between the\ninput image pair, reducing the complexity of the problem at\nthe higher stages.\n3.2. Decoupled Afﬁne Transformation\nWhile directly estimating the afﬁne matrix is feasible\n[21, 38, 46], this transformation model cannot generalize\n20838\nto other parametric registration methods as the afﬁne ma-\ntrix cannot decompose into a set of linear geometric trans-\nformation matrices, i.e., translation, rotation, scaling and\nshearing. In the transformation model of C2FViT, we take\na step further and utilize C2FViT to predict a set of geomet-\nric transformation parameters instead of directly estimating\nthe afﬁne matrix. Formally, the afﬁne registration problem\nis reduced to f\nθ(F,M )=[ t,r,s,h], where t,r,s,h ∈ R3\nrepresent the translation, rotation, scaling and shearing pa-\nrameters. Given T , R, S and H, the resulting afﬁne matrix\nA can be derived by a set of geometric transformation ma-\ntrices via matrix multiplication as A = T· R · S· H, where\nT , R, S and H denote the translation, rotation, scaling\nand shearing transformation matrices derived by the corre-\nsponding geometric transformation parameters ( t, r, s and\nh), respectively. Our proposed transformation model can\neasily be transferred to other parametric registration settings\nby pruning or modifying undesired geometric transforma-\ntion matrices. For instance, our C2FViT can be applied\nto rigid registration by removing the scaling and shearing\nmatrices. Furthermore, our transformation model is capa-\nble of geometrical constraints, reducing the searching space\nof the model during optimization. In this work, the out-\nput geometric transformation parameters are constrained as\nfollows: rotation and shearing parameters are constrained\nbetween −π and +π, the translation parameters are con-\nstrained between -50% and +50% of the maximum spatial\nresolution, and the scaling parameters are constrained be-\ntween 0.5 and 1.5. In this paper, we use the center of mass\nof the input instead of the geometric center for rotation and\nshearing. The center of mass c\nI of the image I is deﬁned as\ncI =\n∑\np∈Ω pI(p)∑\np∈Ω I(p) . If the background intensity of the image\nscan is non-zero, the origin of the rotation can be set to the\ngeometric center of the image.\n3.3. Unsupervised and Semi-supervised Learning\nIn contrast to the conventional afﬁne registration meth-\nods, we parametrize the afﬁne registration problem as a\nlearning problem. Speciﬁcally, we formulate the function\nf\nθ(F,M )= Af, where fθ and Af represent the C2FViT\nmodel and the output afﬁne transformation matrix, respec-\ntively. Mathematically, our goal is to minimize the follow-\ning equation:\nθ\n∗ =a r gm i n\nθ\n[\nE(F,M)∈D L\n(\nF,M (φ(Af)\n)]\n, (2)\nwhere the θ is the learning parameters in C2FViT, ﬁxed\nand moving images are randomly sampled from the training\ndataset Dand the loss functionLmeasures the dissimilarity\nbetween the ﬁxed image and the afﬁne transformed moving\nimage M(φ(A\nf)). In our unsupervised learning setting, we\nuse the negative NCC similarity measure with the similarity\nFigure 3. Example coronal MR slices from the atlases (ﬁxed\nimages), moving images, resulting warped images for ConvNet-\nAfﬁne, VTN-Afﬁe and our method without center of mass initial-\nization.\npyramid [35] Lsim to quantify the distance between F and\nM(φ(Af)) such that L = Lsim and Lsim is deﬁned as:\nLsim(F,M (φ)) =\n∑\ni∈[1..L]\n− 1\n2(L−i) NCCw(Fi,Mi(φ)),\n(3)\nwhere L denotes the number of image pyramid levels,\nNCCw represents the local normalized cross-correlation\nwith windows size w3, and (Fi,Mi) denotes the images in\nthe image pyramid, i.e., F1 is the image with the lowest res-\nolution. In addition, our method is also capable of semi-\nsupervised learning if the anatomical segmentation maps\nof the ﬁxed and moving images are available in the train-\ning dataset. Given anatomical segmentation maps of ﬁxed\nimage S\nF and warped moving image SM(φ), the semi-\nsupervised C2FViT can be formulated by changing the sim-\nilarity measure L in eq. 2 to L\nsim +λLseg, where Lseg is\ndeﬁned as follows:\nLseg(SF,SM(φ)) = 1\nK\n∑\ni∈[1..K]\n(\n1− 2(Si\nF ∩Si\nM(φ))\n|Si\nF|+|Si\nM(φ)|\n)\n(4)\nwhere K denotes the number of anatomical structures.\nFor the semi-supervised C2FViT, we utilize all available\nanatomical segmentations in our experiments. In this paper,\nwe employ L =3 image pyramid levels and λ =0 .5.\n4. Experiments\n4.1. Data and Pre-processing\nWe evaluated our method on brain template-matching\nnormalization and atlas-based registration using 414 T1-\nweighted brain MRI scans from the OASIS dataset [30] and\n20839\n40 brain MRI scans from the LPBA dataset [ 39]. For the\nOASIS dataset, we resampled and padded all MRI scans to\n256×256×256with the same resolution (1mm×1mm×\n1mm) followed by standard preprocessing steps, including\nmotion correction, skull stripping and subcortical structure\nsegmentation, for each MRI scan using FreeSurfer [14]. For\nthe LPBA dataset, the MRI scans are skull-stripped, and\nthe manual delineation of the subcortical structures are pro-\nvided. All brain MRI scans in our experiments are in na-\ntive space, except the MNI152 brain template. We split the\nOASIS dataset into 255, 10 and 149 volumes for training,\nvalidation, and test sets, respectively. For the LPBA dataset,\nwe included all 40 scans as the test set.\nWe evaluated our method on two applications of brain\nregistration: brain template-matching normalization to\nMNI152 space and atlas-based registration in native space.\nBrain template-matching normalization is a standard appli-\ncation in analyzing inter-subject images and a necessary\npre-processing step in most deformable image registration\nmethods. For the task of brain template-matching nor-\nmalization, we afﬁnely register all test scans in the OA-\nSIS dataset to an MNI152 (6\nth generation) brain template\n[13–15], which is derived from 152 structural images and\naveraged together after non-linear registration into the com-\nmon MNI152 co-ordinate system. We train the learning-\nbased methods with the training dataset of OASIS and the\nMNI152 template, which employ the MNI152 template as\nthe ﬁxed image and MRI scans from the training dataset as\nmoving images. For the atlas-based registration task, we\nrandomly select 3 and 2 scans from the test set of OASIS\nand LPBA datasets respectively as atlases. Then, we align\nthe remaining MRI scans in the test set to the selected at-\nlases within the same dataset. Note that in the atlas-based\nregistration task, we train the learning-based methods with\npairwise brain registration, which randomly samples two\nimage scans as ﬁxed and moving images, using only the\ntraining set of the OASIS dataset, i.e., the selected atlases\nand the MRI scans from the LPBA dataset were not in-\nvolved in the training.\nConventionally, afﬁne registration methods often initial-\nize the input images with center of mass (CoM) initializa-\ntion by default [ 32], which initializes the translation pa-\nrameters using the CoM of the input images. Equivalently,\nthe CoM initialization for learning-based methods can be\nachieved by translating the CoM of the moving image to\nthe CoM of the ﬁxed image. We evaluated our method with\nand without the CoM initialization, and the results are listed\nin table 1 and table 2, respectively.\n4.2. Measurement\nTo quantify the registration performance of an afﬁne\nregistration algorithm, we register each subject to an at-\nlas or MNI152 template, propagate the subcortical struc-\nture segmentation map using the resulting afﬁne transfor-\nmation matrix, and measure the volume overlap using the\nDice similarity coefﬁcient (DSC) and 30% lowest DSC of\nall cases (DSC30). We also measure the 95% percentile\nof the Hausdorff distance (HD95) of the segmentation map\nto represent the reliability of the registration algorithm. In\nthe brain template-matching normalization task, 4 subcorti-\ncal structures, i.e., caudate, cerebellum, putamen and thala-\nmus, are included in the evaluation. In the atlas-based reg-\nistration with the OASIS dataset, 23 subcortical structures\nare included, as shown in the boxplot in ﬁgure 4. For the\natlas-based registration with the LPBA dataset, we utilize\nall manual segmentation of the brain scan, including cere-\nbrospinal ﬂuid (CSF), gray matter (GM) and white matter\n(WM), for evaluation.\n4.3. Baseline Methods\nWe compare our method with two state-of-the-art con-\nventional afﬁne registration methods (ANTs [1] and Elastix\n[25]) and two learning-based afﬁne registration approaches\n(ConvNet-Afﬁne [11] and VTN-Afﬁne [ 46]). Speciﬁcally,\nwe use the ANTs afﬁne registration implementation in the\npublicly available ANTs software package [ 2], and we use\nthe Elastix afﬁne registration algorithm in the SimpleElastix\ntoolbox [31]. Both methods use a 3-level multi-resolution\noptimization strategy with adaptive gradient descent opti-\nmization and the mutual information as the similarity mea-\nsure. For ConvNet-Afﬁne and VTN-Afﬁne, we follow their\npapers to implement their afﬁne subnetworks. The initial\nnumber of feature channels for both methods is set to 16,\nand we follow the rules in their papers to deﬁne the growth\nof network depth and the hidden dimension of each con-\nvolution layer. By default, all learning-based methods are\ntrained in an unsupervised manner with the similarity pyra-\nmid as described in eq. 3. We also extend the unsupervised\nlearning-based methods to semi-supervised variants using\nthe same semi-supervised object function as our method,\ndenoted as C2FViT-semi, ConvNet-Afﬁne-semi and VTN-\nAfﬁne-semi.\n4.4. Implementation\nThe learning-based methods, i.e., C2FViT, ConvNet-\nAfﬁne and VTN-Afﬁne, are developed and trained using\nPytorch. All the methods are trained or executed on a stan-\ndalone workstation equipped with an Nvidia TITAN RTX\nGPU and an Intel Core i7-7700 CPU. The learning-based\napproaches are trained with half-resolution image scans by\ndownsampling the image scans with trilinear interpolation.\nThen, we apply the resulting afﬁne transformation to the\nfull-resolution image scans for evaluation. We adopt the\nAdam optimizer [24] with a ﬁxed learning rate of 1e\n−4 and\nbatch size sets to 1 for all learning-based approaches.\n20840\nMethod #Param Template-Matching Normalization (MNI152) Atlas-Based Registration (OASIS) Atlas-Based Registration (OASIS train ⇒ LPBAtest)\nDSC4 ↑ DSC304 ↑ HD954 ↓ Ttest ↓ DSC23 ↑ DSC3023 ↑ HD9523 ↓ Ttest ↓ DSC3 ↑ DSC303 ↑ HD953 ↓ Ttest ↓\nInitial - 0.14 ± 0.12 0.02 ± 0.02 29.26 ± 11.33 - 0.18 ± 0.14 0.06 ± 0.02 15.53 ± 6.77 - 0.33 ± 0.06 0.26 ± 0.03 12.43 ± 4.65 -\nConvNet-Afﬁne [11] 14.7 M 0.65 ± 0.08 0.56 ± 0.06 6.14 ± 1.33 0.12 ± 0.09 s 0.57 ± 0.07 0.48 ± 0.05 4.10 ± 1.01 0.09 ± 0.06 s 0.36 ± 0.07 0.28 ± 0.03 11.58 ± 4.99 0.11 ± 0.08 s\nVTN-Afﬁne [46] 14.0 M 0.67 ± 0.06 0.60 ± 0.05 5.80 ± 1.01 2e-3 ± 4e-4 s 0.57 ± 0.08 0.48 ± 0.06 4.18 ± 1.08 3e-3 ± 8e-4 s 0.31 ± 0.06 0.24 ± 0.03 14.99 ± 5.34 2e-3 ± 6e-4 s\nC2FViT (ours) 15.2 M 0.71 ± 0.06 0.64 ± 0.04 5.17 ± 0.81 0.09 ± 0.03 s 0.64 ± 0.06 0.57 ± 0.05 3.33 ± 0.77 0.08 ± 0.01 s 0.47 ± 0.04 0.42 ± 0.02 6.55 ± 1.60 0.14 ± 0.06 s\nTable 1. Quantitative results of template-matching normalization and atlas-based registration without center of mass initialization. The\nsubscript of each metric indicates the number of anatomical structures involved. ↑: higher is better, and ↓: lower is better. Initial: initial\nresults in native space without registration.\nMethod #Param Template-Matching Normalization (MNI152) Atlas-Based Registration (OASIS) Atlas-Based Registration (OASIS train ⇒ LPBAtest)\nDSC4 ↑ DSC304 ↑ HD954 ↓ Ttest ↓ DSC23 ↑ DSC3023 ↑ HD9523 ↓ Ttest ↓ DSC3 ↑ DSC303 ↑ HD953 ↓ Ttest ↓\nInitial (CoM) - 0.49 ± 0.11 0.35 ± 0.06 11.03 ± 3.48 - 0.45 ± 0.12 0.29 ± 0.06 6.97 ± 2.89 - 0.45 ± 0.04 0.41 ± 0.01 6.87 ± 1.69 -\nElastix [25] - 0.73 ± 0.07 0.64 ± 0.06 5.01 ± 1.44 6.6 ± 0.2 s 0.63 ± 0.09 0.52 ± 0.08 3.89 ± 1.72 6.3 ± 0.2 s 0.55 ± 0.02 0.53 ± 0.02 4.11 ± 1.01 6.4 ± 0.2 s\nANTs [1] - 0.74 ± 0.06 0.67 ± 0.05 4.65 ± 0.57 38.2 ± 3.2 s 0.67 ± 0.08 0.58 ± 0.08 3.27 ± 1.56 37.7 ± 2.5 s 0.54 ± 0.03 0.50 ± 0.02 4.53 ± 1.38 46.6 ± 15.3 s\nConvNet-Afﬁne [11] 14.7 M 0.70 ± 0.06 0.63 ± 0.05 5.28 ± 0.68 0.12 ± 0.08 s 0.62 ± 0.06 0.55 ± 0.05 3.43 ± 0.91 0.10 ± 0.07 s 0.45 ± 0.04 0.41 ± 0.01 7.46 ± 1.87 0.11 ± 0.08 s\nVTN-Afﬁne [46] 14.0 M 0.71 ± 0.06 0.64 ± 0.05 5.11 ± 0.74 3e-3 ± 9e-4 s 0.66 ± 0.06 0.59 ± 0.06 3.02 ± 0.81 2e-3 ± 7e-4 s 0.43 ± 0.04 0.39 ± 0.02 8.02 ± 2.23 2e-3 ± 6e-4 s\nC2FViT (ours) 15.2 M 0.72 ± 0.06 0.65 ± 0.05 4.99 ± 0.75 0.12 ± 0.04 s 0.66 ± 0.05 0.61 ± 0.04 2.96 ± 0.54 0.09 ± 0.02 s 0.54 ± 0.03 0.51 ± 0.04 4.06 ± 1.12 0.12 ± 0.04 s\nConvNet-Afﬁne-semi [11] 14.7 M 0.73 ± 0.06 0.66 ± 0.04 4.94 ± 0.76 0.12 ± 0.09 s 0.63 ± 0.06 0.56 ± 0.06 3.46 ± 0.96 0.10 ± 0.07s 0.43 ± 0.03 0.40 ± 0.02 6.90 ± 1.52 0.12 ± 0.08 s\nVTN-Afﬁne-semi [46] 14.0 M 0.75 ± 0.05 0.70 ± 0.04 4.65 ± 0.66 2e-3 ± 6e-4 s 0.68 ± 0.05 0.62 ± 0.04 2.94 ± 0.64 2e-3 ± 8e-4 s 0.44 ± 0.04 0.40 ± 0.02 7.27 ± 1.96 2e-3 ± 1e-3 s\nC2FViT-semi (ours) 15.2 M 0.76 ± 0.05 0.70 ± 0.04 4.60 ± 0.69 0.13 ± 0.05 s 0.69 ± 0.04 0.64 ± 0.04 2.81 ± 0.55 0.08 ± 0.02 s 0.51 ± 0.03 0.47 ± 0.04 4.58 ± 1.71 0.13 ± 0.05 s\nTable 2. Quantitative results on template-matching normalization, OASIS and LPBA dataset with center of mass initialization. The\nsubscript of each metric indicates the number of anatomical structures involved. ↑: higher is better, and ↓: lower is better. Initial (CoM):\ninitial results with the center of mass initialization. To our knowledge, ANTs and Elastix do not have a GPU implementation.\n4.5. Results\n4.5.1 Registration accuracy and Robustness\nTable 1 shows the results of template-matching normaliza-\ntion and atlas-based registration of the learning-based meth-\nods without spatial initialization. Figure 3 illustrates the\nqualitative results of all tasks without spatial initialization.\nThe low initial Dice scores over all subjects, suggesting that\nthere is a large misalignment within each test case. Our pro-\nposed method is signiﬁcantly better than ConvNet-Afﬁne\nand VTN-Afﬁne in terms of DSC, DSC30 and HD95 over\nall three tasks, suggesting our method is robust and accurate\nin afﬁne registration with large initial misalignment. We vi-\nsualize the distribution of Dice scores for each subcortical\nstructure as in the boxplot in ﬁgure 4. Compared to VTN-\nAfﬁne, the C2FViT model achieves consistently better per-\nformance across all structures.\nTable 2 shows the results of tasks with CoM initial-\nization. This simple but effective initialization boosts\nthe initial Dice scores from 0.14, 0.18 and 0.33 to 0.49,\n0.45 and 0.45, respectively, implying that the initializa-\ntion eliminates most of the misalignment due to translation.\nAll three learning-based methods improve signiﬁcantly on\nafﬁne alignment with CoM initialization. For an unsuper-\nvised manner, our method achieves comparable Dice mea-\nsures to the conventional methods (ANTs and Elastix), and\nslightly better than ConvNet-Afﬁne and VTN-Afﬁne. It is\nworth noting that VTN-Afﬁne gains signiﬁcant improve-\nment in registration performance of template-matching and\natlas-based registration (OASIS) under CoM initialization.\nNevertheless, the validity of the initial registration should\nbe questioned when the two images are acquired in differ-\nent imaging modalities and hence, the registration perfor-\nmance without spatial initialization should be considered\nwhen evaluating the learning-based afﬁne registration al-\ngorithm. With our proposed semi-supervised settings, our\nmethod C2FViT-semi achieves the best overall registration\nperformance in the template-matching normalization and\nthe atlas-based registration task on the OASIS dataset.\n4.5.2 Generalizability Analysis\nAs shown in the results of the LPBA dataset in tables 1 and\n2, ConvNet-Afﬁne and VTN-Afﬁne, using models trained\non the OASIS dataset, fail spectacularly in the test set of\nLPBA, which obtain -5% and -2% loss in DSC with VTN-\nAfﬁne, and +3% and +0% gain in DSC with ConvNet-\nAfﬁne compared to initial results without registration and\nwith spatial initialization, respectively. The results imply\nthat their models cannot generalize well to an unseen dataset\nin practice regardless of spatial initialization. By contrast,\nour C2FViT model achieves a comparable registration per-\nformance to the conventional afﬁne registration approaches\nANTs and Elastix in the task with the LPBA dataset, reach-\ning an average Dice score of 0.54 and HD95 of 4.06 in the\ntask with the LPBA dataset, as shown in table 2. While the\nsemi-supervised settings improve the dataset-speciﬁc per-\nformance of learning-based models in template-matching\nnormalization and atlas-based registration with the OASIS\ndataset, the semi-supervised models are inferior to their un-\nsupervised models in the LPBA dataset, indicating anatom-\nical knowledge injected to the model with semi-supervision\nmay not generalize well to unseen data beyond the training\ndataset.\n20841\nFigure 4. Boxplots illustrating Dice scores of each anatomical structure for C2FViT, VTN and ANTs in the atlas-based registration with\nthe OASIS dataset. The left and right hemispheres of the brain are combined into one structure for visualization. The brain stem (BS),\nthalamus (Th), cerebellum cortex (CblmC), lateral ventricle (LV), cerebellum white matter (WM), putamen (Pu), caudate (Ca), pallidum\n(Pa), hippocampus (Hi), 3rd ventricle (3V), 4th ventricle (4V), amygdala (Am), and cerebral cortex (CeblC) are included. Methods with\n(CoM) postﬁx are trained and tested on MRI scans with the center of mass initialization.\nMethods DSC23 HD9523 Ttest #Param\nVanilla C2FViT-s1 0.61 3.53 0.05 ± 0.04 s 5.0 M\nVanilla C2FViT-s2 0.62 3.57 0.06 ± 0.05 s 10.0 M\nVanilla C2FViT-s3 0.62 3.46 0.07 ± 0.02 s 15.2 M\n+Progressive Spatial Transformation 0.64 (+0.02) 3.33 (-0.13) 0.08 ± 0.02 s 15.2 M\n+Center of Mass Initialization 0.66 (+0.02) 2.96 (-0.37) 0.09 ± 0.02 s 15.2 M\n+Semi-supervision 0.69 (+0.03) 2.81 (-0.15) 0.08 ± 0.02 s 15.2 M\nTable 3. Inﬂuence of the number of stages, progressive spa-\ntial transformation, center of mass initialization and the semi-\nsupervised learning to the C2FViT model. The C2FViT with post-\nﬁx -s{n} represents the C2FViT model with an n-stage.\n4.5.3 Runtime Analysis\nThe average runtimes (denoted as T test) of all methods in\nthe inference phase are reported in tables 1 and 2. We re-\nport the average registration time for each task. C2FViT,\nConvNet-Afﬁne and VTN-Afﬁne are faster than the ANTs\nand Elastix by order of magnitude, thanks to the GPU ac-\nceleration and the effective learning formulation. Moreover,\nANTs runtimes vary widely, as its convergence depends on\nthe degree of initial misalignment of the task. On the other\nhand, Elastix runtimes are stable at around 6.6 seconds per\nalignment task because of the early stopping strategy used\nduring the afﬁne alignment.\nDSC23 ↑ DSC3023 ↑ HD9523 ↓ Ttest ↓\nC2FViT-direct 0.63 ± 0.06 0.55 ± 0.04 3.43 ± 0.73 0.02 ± 4e-3 s\nC2FViT-decouple 0.64 ± 0.06 0.57 ± 0.05 3.33 ± 0.77 0.08 ± 0.01 s\nTable 4. Inﬂuence of the proposed decoupled afﬁne transmation\nmodel compared to the direct afﬁne matrix estimation model.\n4.5.4 Ablation study\nTable 3 shows the ablation study results of C2FViT in the\nOASIS atlas-based registration task. The results suggest\nthat the proposed progressive spatial transformation, CoM\ninitialization and semi-supervised learning consistently im-\nprove the registration performance of C2FViT without\nadding extra learning parameters or signiﬁcant computa-\ntional burden to the model. Table 4 presents the results of\nC2FViT using two different transformation models in the\nOASIS atlas-based registration task. The proposed decou-\npled afﬁne transformation model is slightly better than di-\nrectly learning the afﬁne matrix, in terms of registration per-\nformance, at the cost of registration runtime. Moreover,\nthe decoupled afﬁne transformation model can be easily\nadapted to other parametric registration methods by prun-\ning or modifying the geometrical transformation matrices.\n5. Conclusion\nWe have proposed a Coarse-to-Fine Vision Transformer\ndedicated to 3D afﬁne medical image registration. Unlike\nprior works using CNN-based afﬁne registration methods,\nour method leverages the global connectivity of the self-\nattention operator and moderates the locality of the con-\nvolutional feed-forward layer to encode the global orien-\ntations, spatial positions and long-term dependencies of the\nimage pair to a set of geometric transformation parameters.\nComprehensive experiments demonstrate that our method\nnot only achieves superior registration performance over\nthe existing CNN-based methods under data with large ini-\ntial misalignment and is robust to an unseen dataset, but\nalso our method with semi-supervision outperforms con-\nventional methods in terms of dataset-speciﬁc and preserves\nthe runtime advantage of learning-based methods. Never-\ntheless, there is still a gap between unsupervised learning-\nbased approaches and conventional approaches. We believe\nthat expanding the training dataset and introducing task-\nspeciﬁc data augmentation techniques would likely lead to\nperformance improvement.\n20842\nReferences\n[1] Brian B Avants, Nick Tustison, and Gang Song. Advanced\nnormalization tools (ANTS). Insight j, 2(365):1–35, 2009.\n2, 6, 7\n[2] Brian B Avants, Nicholas J Tustison, Gang Song, and Others.\nA reproducible evaluation of ANTs similarity metric perfor-\nmance in brain image registration. Neuroimage, 54(3):2033–\n2044, 2011. 6\n[3] Guha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Gut-\ntag, and Adrian V Dalca. An unsupervised learning model\nfor deformable medical image registration. InProceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 9252–9260, 2018. 1\n[4] Junyu Chen, Yufan He, Eric C Frey, Ye Li, and Yong Du. Vit-\nv-net: Vision transformer for unsupervised volumetric medi-\ncal image registration.Medical Imaging with Deep Learning,\n2021. 3\n[5] Xu Chen, Yanda Meng, Yitian Zhao, Rachel Williams, Srini-\nvasa R Vallabhaneni, and Yalin Zheng. Learning unsuper-\nvised parameter-speciﬁc afﬁne transformation for medical\nimages registration. In International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention,\npages 24–34. Springer, 2021. 2\n[6] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,\nLonghui Wei, and Qi Tian. Visformer: The vision-friendly\ntransformer. arXiv preprint arXiv:2104.12533, 2021. 3\n[7] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vi-\nsion transformers. arXiv preprint arXiv:2104.13840, 1(2):3,\n2021. 3\n[8] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data\nsizes. arXiv preprint arXiv:2106.04803, 2021. 3\n[9] Adrian V Dalca, Guha Balakrishnan, John Guttag, and\nMert R Sabuncu. Unsupervised learning for fast probabilis-\ntic diffeomorphic registration. In International Conference\non Medical Image Computing and Computer-Assisted Inter-\nvention, pages 729–738. Springer, 2018. 1\n[10] St ´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases.\narXiv preprint arXiv:2103.10697, 2021. 2, 3\n[11] Bob D de V os, Floris F Berendsen, Max A Viergever, Hes-\nsam Sokooti, Marius Staring, and Ivana Iˇsgum. A deep learn-\ning framework for unsupervised afﬁne and deformable image\nregistration. Medical image analysis, 52:128–143, 2019. 1,\n2, 6, 7\n[12] Al exey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3, 4\n[13] Alan C Evans, Andrew L Janke, D Louis Collins, and Syl-\nvain Baillet. Brain templates and atlases. Neuroimage,\n62(2):911–922, 2012. 2, 6\n[14] Bruce Fischl. FreeSurfer. Neuroimage, 62(2):774–781,\n2012. 2, 6\n[15] G ˇRnther Grabner, Andrew L Janke, Marc M Budge, David\nSmith, Jens Pruessner, and D Louis Collins. Symmetric at-\nlasing and model based segmentation: an application to the\nhippocampus in older adults. In International Conference on\nMedical Image Computing and Computer-Assisted Interven-\ntion, pages 58–66. Springer, 2006. 2, 6\n[16] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang,\nChunjing Xu, and Yunhe Wang. Cmt: Convolutional\nneural networks meet vision transformers. arXiv preprint\narXiv:2107.06263, 2021. 3\n[17] Mattias P Heinrich. Closing the gap between deep and con-\nventional image registration using probabilistic dense dis-\nplacement networks. In International Conference on Med-\nical Image Computing and Computer-Assisted Intervention,\npages 50–58. Springer, 2019. 1\n[18] Mattias P Heinrich, Oskar Maier, and Heinz Handels. Multi-\nmodal Multi-Atlas Segmentation using Discrete Optimisa-\ntion and Self-Similarities. VISCERAL Challenge@ ISBI,\n1390:27, 2015. 2\n[19] Alessa Hering, Stephanie H ¨ager, Jan Moltz, Nikolas Less-\nmann, Stefan Heldmann, and Bram van Ginneken. Cnn-\nbased lung ct registration with multiple anatomical con-\nstraints. Medical Image Analysis, page 102139, 2021. 1\n[20] Andrew Hoopes, Malte Hoffmann, Bruce Fischl, John Gut-\ntag, and Adrian V Dalca. Hypermorph: Amortized hyper-\nparameter learning for image registration. In International\nConference on Information Processing in Medical Imaging,\n2021. 1\n[21] Yipeng Hu, Marc Modat, Eli Gibson, Nooshin Ghavami, Es-\nter Bonmati, Caroline M Moore, Mark Emberton, J Alison\nNoble, Dean C Barratt, and Tom Vercauteren. Label-driven\nweakly-supervised learning for multimodal deformable im-\nage registration. In 2018 IEEE 15th International Sympo-\nsium on Biomedical Imaging (ISBI 2018), pages 1070–1074.\nIEEE, 2018. 1, 2, 4\n[22] Weijian Huang, Hao Yang, Xinfeng Liu, Cheng Li, Ian\nZhang, Rongpin Wang, Hairong Zheng, and Shanshan Wang.\nA coarse-to-ﬁne deformable transformation framework for\nunsupervised multi-contrast mr image registration with dual\nconsistency constraint. IEEE Transactions on Medical Imag-\ning, 2021. 1, 2\n[23] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and\nOthers. Spatial transformer networks. In Advances in neural\ninformation processing systems, pages 2017–2025, 2015. 4\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 6\n[25] Stefan Klein, Marius Staring, Keelin Murphy, Max A\nViergever, and Josien PW Pluim. Elastix: a toolbox for\nintensity-based medical image registration. IEEE transac-\ntions on medical imaging, 29(1):196–205, 2009. 2, 6, 7\n[26] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021. 3, 4\n[27] Min Lin, Qiang Chen, and Shuicheng Yan. Network in net-\nwork. arXiv preprint arXiv:1312.4400, 2013.\n2\n20843\n[28] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski\nSuch, Eric Frank, Alex Sergeev, and Jason Yosinski. An\nintriguing failing of convolutional neural networks and the\ncoordconv solution. arXiv preprint arXiv:1807.03247, 2018.\n2\n[29] JB Antoine Maintz and Max A Viergever. A survey of med-\nical image registration. Medical image analysis, 2(1):1–36,\n1998. 1\n[30] Daniel S Marcus, Tracy H Wang, Jamie Parker, John G Cser-\nnansky, John C Morris, and Randy L Buckner. Open Access\nSeries of Imaging Studies (OASIS): cross-sectional MRI\ndata in young, middle aged, nondemented, and demented\nolder adults. Journal of cognitive neuroscience, 19(9):1498–\n1507, 2007. 5\n[31] Kasper Marstal, Floris Berendsen, Marius Staring, and Ste-\nfan Klein. Simpleelastix: A user-friendly, multi-lingual li-\nbrary for medical image registration. In Proceedings of the\nIEEE conference on computer vision and pattern recognition\nworkshops, pages 134–142, 2016. 6\n[32] Matthew Michael McCormick, Xiaoxiao Liu, Luis Ibanez,\nJulien Jomier, and Charles Marion. ITK: enabling repro-\nducible research and open science. Frontiers in neuroinfor-\nmatics, 8:13, 2014. 6\n[33] Shun Miao, Z Jane Wang, and Rui Liao. A cnn regression\napproach for real-time 2d/3d registration. IEEE transactions\non medical imaging, 35(5):1352–1363, 2016. 2\n[34] Tony C W Mok and Albert Chung. Fast Symmetric Dif-\nfeomorphic Image Registration with Convolutional Neural\nNetworks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4644–\n4653, 2020. 1\n[35] Tony C W Mok and Alber t C S Chung. Large Deformation\nDiffeomorphic Image Registration with Laplacian Pyramid\nNetworks. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 211–\n221. Springer, 2020. 1, 5\n[36] Tony C W Mok and Albert C S Chung. Conditional De-\nformable Image Registration with Convolutional Neural Net-\nwork. In International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention, 2021. 1\n[37] Josien PW Pluim, JB Antoine Maintz, and Max A Viergever.\nMutual-information-based registration of medical images: a\nsurvey. IEEE transactions on medical imaging, 22(8):986–\n1004, 2003. 1\n[38] Wei Shao, Indrani Bhattacharya, Simon JC Soerensen,\nChristian A Kunder, Jeffrey B Wang, Richard E Fan, Pejman\nGhanouni, James D Brooks, Geoffrey A Sonn, and Mirabela\nRusu. Weakly supervised registration of prostate mri and\nhistopathology images. In International Conference on Med-\nical Image Computing and Computer-Assisted Intervention,\npages 98–107. Springer, 2021. 2, 4\n[39] David W Shattuck, Mubeena Mirza, Vitria Adisetiyo, Cor-\nnelius Hojatkashani, Georges Salamon, Katherine L Narr,\nRussell A Poldrack, Robert M Bilder, and Arthur W Toga.\nConstruction of a 3D probabilistic atlas of human cortical\nstructures. Neuroimage, 39(3):1064–1080, 2008. 6\n[40] Zhengyang Shen, Xu Han, Zhenlin Xu, and Marc Nietham-\nmer. Networks for joint afﬁne and non-parametric image\nregistration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4224–\n4233, 2019. 1, 2\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2\n,\n3, 4\n[42] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPvtv2: Improved baselines with pyramid vision transformer.\narXiv preprint arXiv:2106.13797, 2021. 3, 4\n[43] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In IEEE ICCV, 2021. 2, 3\n[44] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021. 2, 3\n[45] Yungeng Zhang, Yuru Pei, and Hongbin Zha. Learning dual\ntransformer network for diffeomorphic registration. In In-\nternational Conference on Medical Image Computing and\nComputer-Assisted Intervention, pages 129–138. Springer,\n2021. 3\n[46] Shengyu Zhao, Tingfung Lau, Ji Luo, I Eric, Chao Chang,\nand Yan Xu. Unsupervised 3d end-to-end medical image\nregistration with volume tweening network. IEEE journal of\nbiomedical and health informatics, 24(5):1394–1404, 2019.\n1, 2, 4, 6, 7\n[47] Wu Zhou, Lijuan Zhang, Yaoqin Xie, and Changhong Liang.\nA novel technique for prealignment in multimodality medi-\ncal image registration. BioMed research international, 2014,\n2014. 1, 2\n20844"
}