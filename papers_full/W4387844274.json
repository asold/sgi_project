{
    "title": "Pose2Trajectory: Using transformers on body pose to predict tennis player’s trajectory",
    "url": "https://openalex.org/W4387844274",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5084878664",
            "name": "Ali K. AlShami",
            "affiliations": [
                "University of Colorado Colorado Springs"
            ]
        },
        {
            "id": "https://openalex.org/A5049661026",
            "name": "Terrance E. Boult",
            "affiliations": [
                "University of Colorado Colorado Springs"
            ]
        },
        {
            "id": "https://openalex.org/A5049180880",
            "name": "Jugal Kalita",
            "affiliations": [
                "University of Colorado Colorado Springs"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6679196293",
        "https://openalex.org/W2109940535",
        "https://openalex.org/W1979529260",
        "https://openalex.org/W3112052929",
        "https://openalex.org/W3119857975",
        "https://openalex.org/W6661950643",
        "https://openalex.org/W2982511588",
        "https://openalex.org/W6791573647",
        "https://openalex.org/W1990213678",
        "https://openalex.org/W6855334548",
        "https://openalex.org/W2505976722",
        "https://openalex.org/W6758124449",
        "https://openalex.org/W3176157278",
        "https://openalex.org/W6766076520",
        "https://openalex.org/W6765246382",
        "https://openalex.org/W4292179114",
        "https://openalex.org/W6691241951",
        "https://openalex.org/W6735331427",
        "https://openalex.org/W3084173793",
        "https://openalex.org/W6802459829",
        "https://openalex.org/W6684373561",
        "https://openalex.org/W2134944993",
        "https://openalex.org/W6765361892",
        "https://openalex.org/W6761121490",
        "https://openalex.org/W6752001723",
        "https://openalex.org/W6774458761",
        "https://openalex.org/W4312893480",
        "https://openalex.org/W6810078117",
        "https://openalex.org/W4287734800",
        "https://openalex.org/W6839119880",
        "https://openalex.org/W6778059963",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6855194969",
        "https://openalex.org/W3136792391",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W6628973269",
        "https://openalex.org/W6785652829",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W6789107872",
        "https://openalex.org/W4225510413",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W1973965874",
        "https://openalex.org/W4301409532",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4224992933",
        "https://openalex.org/W4288287716",
        "https://openalex.org/W4283748233",
        "https://openalex.org/W3002709689",
        "https://openalex.org/W4288283362",
        "https://openalex.org/W2492746552",
        "https://openalex.org/W4242743978",
        "https://openalex.org/W2618176478",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2105934661",
        "https://openalex.org/W2962972423",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2741951152",
        "https://openalex.org/W4316843048",
        "https://openalex.org/W4254367177",
        "https://openalex.org/W4234552385",
        "https://openalex.org/W4254057358",
        "https://openalex.org/W4235990554",
        "https://openalex.org/W4296405214",
        "https://openalex.org/W4289254601",
        "https://openalex.org/W2963966386",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385271020",
        "https://openalex.org/W4249142012",
        "https://openalex.org/W3094502228"
    ],
    "abstract": null,
    "full_text": "Pose2Trajectory: Using Transformers on Body Pose to\nPredict Tennis Player’s Trajectory\nAli K. AlShami, Terrance Boult, Jugal Kalita\nComputer Science Department, University of Colorado, Colorado Springs\naalshami@uccs.edu,tboult@uccs.edu, jkalita@uccs.edu\nhttps://github.com/alshami52/Pose2Trajectory.git\nAbstract\nTracking the trajectory of tennis players can help camera operators in production.\nPredicting future movement enables cameras to automatically track and predict\na player’s future trajectory without human intervention. Predicting future human\nmovement in the context of complex physical tasks is also intellectually satisfying.\nSwift advancements in sports analytics and the wide availability of videos for\ntennis have inspired us to propose a novel method called Pose2Trajectory, which\npredicts a tennis player’s future trajectory as a sequence derived from their body\njoints’ data and ball position. Demonstrating impressive accuracy, our approach\ncapitalizes on body joint information to provide a comprehensive understanding of\nthe human body’s geometry and motion, thereby enhancing the prediction of the\nplayer’s trajectory. We use encoder-decoder Transformer architecture trained on the\njoints and trajectory information of the players with ball positions. The predicted\nsequence can provide information to help close-up cameras to keep tracking the\ntennis player, following centroid coordinates. We generate a high-quality dataset\nfrom multiple videos to assist tennis player movement prediction using object\ndetection and human pose estimation methods. It contains bounding boxes and\njoint information for tennis players and ball positions in singles tennis games.\nOur method shows promising results in predicting the tennis player’s movement\ntrajectory with different sequence prediction lengths using the joints and trajectory\ninformation with the ball position.\n1 Introduction\nArtificial Intelligence (AI) has become indispensable in many domains, including autonomous\nsystems, surveillance systems, and sports analysis [1]. The power of AI technology has profoundly\nimpacted and transformed sectors such as transportation, surveillance, logistics, manufacturing,\nand healthcare [2]. In surveillance systems, developers leverage AI to enhance performance with\nfacial recognition, action recognition, and prediction technologies. Sports analysis involves using\ndata points to evaluate and improve athletic performance by generating insights that can be used\nto optimize player training and game strategies, including recommending beneficial and optimal\nball trajectory and speed. Sports analysis has become increasingly popular due to the availability of\nwearable devices, videos, and other advanced technologies that make data collection and analysis\nmore accessible.\nIn sports, AI can potentially revolutionize how athletes and coaches approach performance analysis.\nA trained AI can efficiently process vast amounts of data in real time. This data-driven approach can\nhelp with injury prevention and enable coaches and trainers to make informed decisions, optimize\ntraining regimes, identify areas for improvement, and provide invaluable insights into player and\nteam performance.\narXiv:2411.04501v1  [cs.CV]  7 Nov 2024\nFigure 1: The figure shows the prediction of the future movement trajectory of a tennis player fifteen\n(250 ms), thirty (500 ms), and sixty (1s) frames ahead (until reaching the ball). The trajectory is\npredicted as the second player’s centroid around which we show a 224 × 224 bounding box in future\nframes. The player appears lighter in future frames. The original image has been taken from videos\non the TennisTV YouTube channel.\nTennis is a popular sport that first originated in England in the 19th century. In recent years, tennis\nhas seen numerous innovations that help improve referees’ decision-making and enhance player\nperformance, including the Hawk-Eye line calling system [3] and telemetry sensors [4]. Processing\nand analyzing large amounts of data in tennis utilizing AI algorithms can help recognize and predict\nplayer actions and movements, forecast future location, speed, and spin of shots, and recognize\nracquet movements (swing). All this can help trainers evaluate player performance, focus on strengths\nand weaknesses, and potentially help coaches make more informed decisions during tennis matches.\nMachine Learning algorithms can also help enhance the production of professional tennis game\nvideos by predicting the players’ future movement trajectories. The process of recording a tennis\nmatch is complex, requiring multiple cameras, skilled camera operators, and advanced broadcasting\nequipment. Typically, six to eight cameras are strategically positioned around the court, capturing the\naction from different angles [5]. The equipment may include long-range cameras, which provide a\ncomprehensive view of the court and player movements, and close-up cameras, which offer more\ndetailed shots of the players following the ball. All cameras are controlled by skilled operators\nwho follow the action and adjust the angles and zoom levels to capture the best shots. By using\nthe predicted future movement trajectories of the tennis players, these cameras have the potential\nto automatically track the movement of the players, reducing the need for human intervention and\nenhancing the accuracy and speed of the recording process.\nPredicting the future trajectory of the player in sports is a challenging task. Researchers have\nattempted to solve this problem in football and basketball [6][7]. In tennis, people use the players’\ntrajectory information to evaluate performance, style, strategy, and the nature of the movement of the\nplayers [8] [9].\nEnhancing the accuracy of future trajectory prediction for tennis players can enable cameras to\nautonomously estimate a player’s upcoming movements without human intervention. To achieve\nthis, it is crucial to supply predicted data for the player’s trajectory at least half a second in advance,\ntaking into consideration the physical constraints of the close-up camera. Typically, cameras used in\ntennis matches can process multiple requests per second, with a maximum capacity of ten requests\nper second. However, factors such as processing speed, image resolution, compression settings, and\nnetwork conditions can impact the camera’s response time. Consequently, it is essential to provide\n2\npredictions at least half a second ahead, or ideally, a full second, to accommodate the camera’s\nresponse time limitations.\nWhile many research studies on predicting future trajectories for single or multiple individuals\nrely on the centroid of the bounding box to estimate their movement path, this approach may not\nbe adequate for accurately predicting a tennis player’s trajectory. Tennis players exhibit a wide\nrange of movements to reach the ball, which can introduce noise to the centroid-based analysis. To\nimprove prediction accuracy, it is essential to consider specific body parts, such as the arms’ joints\n(determining which one holds the racket), legs’ joints, and head, as well as their orientation. This\nprompts the question: how crucial are the various body parts of a tennis player in making predictions?\nExamining these elements can offer a more holistic understanding of a player’s geometry and motion,\nultimately leading to more precise trajectory predictions.\nKalman filter and machine learning methods can be potentially used in sports to track players.\nHowever, these algorithms are not explicitly built or reliable for predicting future trajectories over\nlong periods. In our novel approach, we predict the future movement trajectory as a sequence based\non the players’ body joints, trajectory, and ball position information. In Figure 1, we show the\nsuitability of our method by predicting 15, 30, and 60 frames ahead. We use the encoder-decoder\nTransformer model in this approach. The encoder part is fed all the joint positions and the centroid\npoints of the players and ball positions in several frames as a sequence, representing the player’s past\nmovement. The decoder part predicts the future movement trajectory of the players as a sequence of\ncentroid points. We use a high-quality dataset we created for this work.\nIn Section 2, we present relevant previous work, including the prediction of future movements in\nsports. Section 3 describes our prediction system, including tennis player and ball detection, body\njoint detection, and the use of the Transformer model to predict future movement trajectory. In\nSection 4, we demonstrate that the body joint information is essential to predict the future trajectory\nof the players by evaluating alternate models trained on the trajectory information. Finally, Sections\n5 and 6 provide information about our dataset and our conclusions.\n2 Related Work\nLimited research has been conducted on tennis using computer vision and machine learning to\nsatisfy different requirements. Most of these requirements derive from the needs of referees and\ntennis players, from beginner to professional, to make better decisions and improve performance,\nincluding recognition of the movement of a player’s racquet when hitting the ball (swing) [10][11],\nphysical simulation of tennis skills from large-scale demonstrations captured in broadcast videos\n[12], forecasting future shot locations [13][14], and recognizing tennis players’ actions or predicting\nfuture actions based on videos or sensor information [15] [16][17]. Additionally, recent studies have\nutilized TrackNet-based neural networks [18] to pinpoint the ball’s position, identify court lines, and\ntrack player movements for comprehensive analysis of tennis games [19].\nAnalyzing tennis players’ trajectories offers valuable insights that can improve predicting their future\nmovements, performance, and strategies. LucentVision, a visualization system, employs real-time\nvideo analysis to deliver in-depth information on performance, style, and tactics, all derived from\ntrajectory data [8]. Another study [ 9] leverages trajectory information to identify player direction\nshifts. By harnessing these innovative approaches, we can gain a deeper understanding of tennis\nplayers’ techniques and tactics, ultimately improving predictions and analysis in the sport.\nThe Kalman filter [20], a widely used estimation algorithm, excels at predicting an object’s subsequent\nmovements by incorporating both current measurements and the predicted state from the previous\nstep. Its performance hinges on how closely the model assumptions align with the actual system\ndynamics and noise properties. When the dynamics are accurately modeled, and the noise is Gaussian,\nthe Kalman filter can precisely estimate the current state. However, its efficacy may decline when\npredicting multiple steps ahead (e.g., 10, 25, 60 frames) due to various factors, such as model\nmismatch, accumulated uncertainty, process noise, and nonlinear dynamics. Despite these limitations,\nthe Kalman filter remains a powerful tool for short-term predictions, especially when the underlying\nassumptions hold true.\nIn a study by McEwen et al. [21], the use of the Kalman filter demonstrated suboptimal performance\nin predicting the future location of animals based on their centroid positions in images with a\n3\nresolution of 160x120 pixels. When forecasting five frames ahead, the average error amounted to 80.5\npixels in the x-direction and 19.7 pixels in the y-direction, which is much inferior compared to the\nprediction of the next frame. Due to the substantial decrease in prediction accuracy as the prediction\nrange increases, we decided against employing the Kalman filter as a method for predicting object\npositions in this context.\nMachine learning methods such as SORT (Simple Online and Real-time Tracking) [ 22], Deep-\nSORT (Deep Simple Online and Real-time Tracking) [23], FairMOT (Fair Multi-Object Tracking)\n[24], ByteTrack (Multi-Object Tracking by Associating Every Detection Box) [25], and BoT-SPRT\n(Bayesian Online Tracking with Spatial Prior-Regularized Trees) [26] primarily focus on tracking\nobjects and maintaining their identities in real-time or near-real-time within video sequences. While\nthese methods are not explicitly designed for predicting long-term future trajectories, such as half a\nsecond or a full second ahead, they can offer some degree of short-term prediction. By estimating an\nobject’s current state—including its position and velocity—these algorithms can roughly extrapolate\nthe object’s trajectory for a few frames into the future. However, their accuracy is likely to diminish\ndrastically for longer-term predictions, as the underlying motion models and assumptions may not\nremain valid over extended time periods.\nPredicting human trajectories as a set of 2D coordinates has been extensively studied in the literature.\nEarly research primarily relied on handcrafted features [27][28]. In contrast, recent studies have used\ndata-driven models such as recurrent neural networks with spatial and temporal features [ 29] and\nGenerative Adversarial Networks (GAN) [30][31] with attention mechanisms to predict the future\nhuman trajectory of individuals.\nRecent studies have used the Transformer to predict future human motion trajectory in various tasks\n[32]. In Li et al. [ 33], the authors used graph-based representations of the pedestrian environment\nwith Transformer and memory replay to improve prediction accuracy. Another approach used non-\nautoregressive Transformers to simultaneously predict both the trajectory and poses of humans,\navoiding the limitations of traditional autoregressive models [34] [35]. Group-aware spatial-temporal\nTransformers, such as [36], can predict the trajectories of multiple humans by considering group\ninteractions\nFinally, Chen et al. [37] and Yu et al. [38] used spatial-temporal Transformer networks to predict\nfuture trajectories of humans in complex environments. These Transformer-based models have shown\npromising results in predicting future human movement trajectories, which have potential applications\nin autonomous driving, robotics, and other fields.\n3 Dataset\nOur dataset provides information for tennis players extracted from videos. The information presents\n2D keypoints corresponding to the joints of the tennis players and the ball’s coordinates in each frame.\nThe videos for the datasets were collected from the TennisTV channel on YouTube for Vienna Open,\na professional tennis tournament played on indoor hard courts. Each video presents a point in a tennis\nmatch, beginning when one player starts serving until one of the players misses the ball. Tennis\nplayers are fast in performing some movements, including serve, front-hand, and back-hand. We\nconvert the videos to images with an average rate of 60 frames per second to cover all the movement\ninformation. We modify the shape of our dataset based on the sequence length used for training the\nmodel. The multivariate time series dataset has more than one series of observations at each time\nstep. The dataset is available for research.\nWe have faced some issues creating our dataset. However, we found solutions for the issues. The\nplayer detector model aims to detect tennis players, not other people. However, other people are\nusually involved in tennis videos, including spectators, the referee, and the ball boy/girl. Therefore,\nsearching for instances of class person is not a solution. We have tried alternate solutions to avoid\ncapturing other people. The first solution is estimating the background in a sequence of images with\nmedian filtering. Median filtering is a non-linear filtering technique that replaces the value of each\npixel with the median value of its neighboring pixels within a defined kernel size [39]. However, we\nfound that median filtering needs to be more accurate when other people move. The second solution\nis to detect the tennis court lines using the Harris corner detector to detect corners and then identify\nthe point nearest each player [40]. Finally, we used Euclidean distance in each frame to calculate the\ndistance between the person object to the closest points.\n4\nFigure 2: Some of the tennis players’ movements that Faster-RCNN could not detect. This is likely\nbecause Faster RCNN was pre-trained on non-sports images.\nWe encountered issues with the Faster RCNN model’s inability to detect certain tennis player\nmovements, as illustrated in Figure 2. This limitation arises from the model’s pre-training on the\nCOCO dataset [41], which unfortunately lacks sports or tennis-specific images that would have been\nmore suitable for our needs. To mitigate this, we implemented a GUI script to facilitate a review\nand clean-up process for the image data. This script automatically sorted images with incorrect\nor poor bounding boxes into a separate file, where they were subsequently manually labeled using\nAWS SageMaker. As a future direction, we intend to leverage novel image classification and image\ncapturing approaches to better identify unique tennis player movements [42] [43].\nAnother issue that we faced was detecting the tennis ball in videos. Sometimes, the tennis ball\ndisappeared for a few seconds in a part of the video when one of the players hit the ball upward,\ncausing the ball to leave the frame before it came down to the second player, taking a curved trajectory.\nFor example, the ball was invisible for over one hundred frames in one video. We solved the problem\nusing polynomial regression using ten points before and ten after the missing data. After drawing the\nplot using polynomial regression, we generated points that present the tennis ball positions, taking\nadvantage of the plot.\n4 Approach\nIn this section, we present our system for predicting the future trajectory of a tennis player based on\npast motion. As shown in Figure 4, our system design incorporates several neural network modules.\nIt consists of four main modules, namely: (1) tennis player detection using Faster RCNN, (2) ball\ndetection using TrackNet, (3) detection of body joints using ViTPose, and (4) future trajectory\nprediction using Transformer.\n4.1 Detection of the Tennis Players\nSignificant works have been published in the last few years to detect objects from videos, including\nFaster RCNN [44], YOLO [45], YOLOv6 [46], SSD [47], and RetinaNet [48]. In this work, we use\nthe Faster RCNN object detection model to detect the tennis players [44].\nFaster R-CNN is a cutting-edge deep learning model for object detection and segmentation, enhancing\ntraditional CNNs with innovative features. It incorporates a Region Proposal Network (RPN), which\nactively scans an image to produce object-boundary proposals. These proposals are subsequently\nfunneled to a CNN that extracts vital features. A classifier and a bounding box regressor then work\nin concert to predict the object classes and obtain their precise locations within the image. The\nsynergistic working of RPN and CNN equips Faster R-CNN with accuracy in object detection that\nmarkedly surpasses its forerunners. In our methodology, videos from the dataset are transformed\ninto sequences of images, which are then channeled into the Faster RCNN model for latent feature\nextraction. This model further applies a CNN-based RPN to the unearthed feature maps, generating\nbox proposals. Utilizing the concept of an anchor box for feature maps, RPN emerges as an adept\ntool for predicting tennis player movements. Each output point materializes a variety of boxes, with\neach box encapsulating class information and coordinate positions. Specifically, we targeted the class\nperson to recognize the tennis players, a process illustrated in Figure 3.\n5\nFigure 3: Detecting the tennis player bounding boxes, ball position, and joints using Faster RCNN,\nViTPose, and TrackNet. The image has been taken from videos on the TennisTV YouTube channel.\n4.2 Detection of the Ball Position\nMany works have attempted to detect tiny objects such as balls in different sports from video,\nincluding TrackNet [18], TrackNetV2 [49], and MonoTrack [ 50]. We used the TrackNet model\n[18] to detect the tennis ball. Detecting a fast-moving ball in sports videos presents a significant\nchallenge, particularly when the balls appear tiny and can travel at extraordinary speeds. Based\non the tennicreative website, the velocity of professional tennis for the first serve often reaches\napproximately 120 mph for men and around 105 mph for women. To accurately detect a tennis ball\nhurtling at such formidable speeds, a high-performance model is essential. For our analysis, we\nemployed the TrackNet model to obtain the coordinates of the tennis ball’s position within each video\nframe, a process vividly illustrated in Figure 3. We use the TrackNet model, which is a heatmap-based\ndeep learning architecture meticulously designed to track the elusive movements of tiny tennis balls\nwithin video sequences. Distinct from traditional models, TrackNet’s training enables it to identify\nthe ball in individual frames while deciphering its flight patterns across successive frames. With the\ncapacity to process images at a resolution of 640 × 360, TrackNet generates a detection heatmap from\na single frame or a sequence of frames, ensuring the precise localization of the ball. This capability\nallows TrackNet to exhibit high precision, even in the realm of public-domain videos.\n4.3 Detection of Body Joints\nTo detect the tennis players’ body joints, we use a state-of-the-art human pose estimation method\ncalled ViTPose [51], designed to estimate the 2D and 3D poses of human bodies from 2D RGB\nimages. The model uses a Vision Transformer (ViT)-based [52] architecture for image classification\ntasks. ViTPose takes an input image and encodes it using a ViT backbone. The ViT backbone\nis trained to divide the input image into a set of patches, which are then processed by a series of\nTransformer layers to extract features representative of the entire image. It is trained on large datasets\nof labeled images, making it a powerful and efficient model for estimating human poses from the 2D\nimage. After detecting the tennis players, we crop the image for each player to size 224 × 224 and\nfeed it to the ViTPose model and get the coordinates of seventeen 2D keypoints,X and Y , for each\nkeypoint, as output for joint locations such as the elbows, wrists, and knees, as shown in Figure 3.\n4.4 Transformer to Predict Future Trajectory\nThe Transformer model has an encoder-decoder deep learning neural network architecture that was\nfirst introduced for machine translation [53]. It has since become the foundational model in natural\nlanguage processing and computer vision, including the Bidirectional Encoder Representations from\nTransformers (BERT) [54] and Generative Pre-Training (GPT) [55] and the Vision Transformer (ViT)\n[52]. The model is based on the concept of attention mechanism, introduced by Bahdanau et al.\n[56] to address the bottleneck that arises using a fixed-length encoding vector, where the decoder\nhas limited access to the information provided by the input. The encoder takes an input sequence\nand generates a hidden representation that captures and learns dynamic contextual interactions in\n6\nFigure 4: Our encoder-decoder Transformer model to predict a tennis player’s trajectory. The encoder\nencodes players’ centroids, joint positions, and ball positions with time information. The decoder\ntakes part of the encoder information along with time information and predicts future centroids of the\nplayer at several future time points. Si represents a feature set of 74 values at time i, including player\ncentroids, player joint positions, and ball positions. S\n′\ni represents a set of two values indicating X\nand Y positions of player centroid that we want to predict at time i.\nthe sequence. The decoder takes the hidden representation of the encoder and generates the output\nsequence. Since Transformer-based architectures have performed well in sequence-to-sequence tasks,\nincluding trajectory prediction, we believe that it is an excellent choice for predicting the future\ntrajectory of tennis players as a sequence from videos.\nIn this subsection, we present our Pose2Trajectory approach, a novel method that predicts the future\ntrajectory of a tennis player as a sequence of 2D coordinates. Our method performs a sequence-\nto-sequence transformation using an encoder-decoder Transformer architecture. Specifically, the\nencoder is fed several sets of features in a time sequence, with each set representing information\nabout a past frame that our model considers when making the prediction. These features include the\ncoordinates of joint positions and the trajectory of the tennis players in terms of centroid locations as\nwell as ball positions.\nThe decoder also takes several sets of features in time sequence as input. The decoder input consists of\ntwo parts: i) selected information (the subset to be predicted) from several frames from the immediate\npast that were input to the encoder also, and ii) selected information (the subset to be predicted) from\na few frames that follow the frames in i). Note that there are two types of information in Figure 4. Si\nrepresents 74 features of information about frames i containing body joints, ball, and player centroid,\nwhereas S\n′\ncontains only two features corresponding to body centroid. Thus, the sets of features in\nthe decoder input and target represent the centroid locations of the tennis player we want to predict.\nThe approach we use is called the technique of teacher forcing to aid the learning process, where\nthe decoder is fed with previous values of the actual output sequence instead of its prediction [57].\nThis is a proven method to help a sequence-to-sequence model learn from the correct sequence of\nground-truth outputs during training, improving accuracy and generalization to new data. By utilizing\nthese input features to describe the players’ motions, our model can capture complex relationships\nbetween them, resulting in better forecasting results.\nAs previously mentioned, the Transformer encoder plays a crucial role in processing the input\nsequence by generating a set of encoded representations that capture the temporal dependencies\nand patterns in the sequence. In our method, the decoder input incorporates some small subsets of\nfeatures that the decoder needs to predict from the encoder input, while the decoder target sequence\nalso contains features from the decoder input, resulting in improved encoder decoder processing.\nThis technique has been successfully applied in time series forecasting studies [58].\nFor example, in Figure 4, we predict a tennis player’s movement six frames into the future, given\nan encoder sequence of 30 frames. In this example, the input to the encoder is a sequence of sets\nof features, given as [S1, S2, S3, ..., S30], while the decoder input is [S\n′\n27, ..., S\n′\n33]. The goal of the\ndecoder is to output a sequence of sets of features that corresponds to the predicted future trajectory,\n7\nFigure 5: The prediction results for X and Y coordinates with and without using the LSTM in our\narchitecture.\nnamely [ ˆS\n′\n30, ...,ˆS\n′\n36]. When using Model Family 1 for this example, as mentioned in Table 1, S\nrepresents the centroid coordinates of both players, denoted as S = C1, C2. On the other hand, with\nModel Families 2 and 3, S represents the centroid coordinates of the players and their respective\njoint positions, denoted as S = C1, J1, C2, J2. In Model Family 4, S includes all the information\nin model families 2 and 3 plus the position of the ball: S = C1, J1, C2, J2, B. Note that S\n′\nand ˆS\n′\nrepresent the centroid coordinates of the player we want to predict. For example, if we predict player\n1, then S\n′\n= C1 and ˆS\n′\n= C1, respectively.\nTemporal information is necessary to predict future human movement because it allows the system to\nunderstand how movement patterns change over time. By analyzing tennis players’ historical body\njoint positions, we can identify patterns and trends that can be used to forecast future movement\ntrajectories. In our work, we use the Time2Vector model [ 59] to convert the timestamp inputs\ninto a vector representation by mapping the time steps to a high-dimensional space using Fourier\ntransforms[60]. The Time2vector model can be presented mathematically as follows:\nt2v(τ)[i] =\n\u001aωiτ + φi if i = 0\nF(ωiτ + φi), if 1 ≤ i ≤ k (1)\nwhere t2v(τ)[i] is the ith element of t2v(τ), F is a periodic activation function and ωi and φi are\nlearnable parameters.\nUsing the time vector only in the encoder Transformer is common since the encoder is responsible\nfor processing the input sequence and capturing the relevant information in time. By encoding\nthe time dimension of the input sequence using Time2Vector, the model can capture the temporal\npatterns in the input data and use this information to generate accurate predictions. Adding the time\npatterns to the decoder of the sequence-to-sequence model can help the model generate more accurate\npredictions by providing information about the time at which each prediction should be made. In our\ncase, using the time vector in the decoder part of our model is also helpful. We concatenate the time\nvector representation with the input sequences before we feed it to the encoder and decoder. We also\nused an LSTM model with the encoder and decoder to make the prediction smoother. As shown in\nFigure 5, the prediction results for X and Y are smoother using the LSTM in our architecture,making\nit better for camera control. LSTMs can make predictions smoother due to their ability to learn and\nmaintain internal states that represent long-term dependencies in the data. In the context of time series\nprediction or sequence-to-sequence problems, smoother predictions are desirable to reduce noise\nand capture the overall trend of the data. LSTM networks can achieve this by retaining important\ninformation from previous time steps and incorporating it into the current prediction. As shown in\nFigure 4, this work uses two Transformer encoder layers. Each layer contains two sub-layers: a\nself-attention and a fully connected feed-forward sub-layer, where each sub-layer is followed by a\nnormalization layer. The output of the Transformer encoder feeds to the Transformer decoder to\nprovide the decoder with the necessary context and information to generate accurate predictions for\nthe output sequence. We use a Transformer decoder that contains three sub-layers. In addition to\n8\nTable 1: Evaluating the prediction results of the first player with different Model Families that train\non sequences of different lengths and numbers of features using the Mean Euclidean Distance Error\n(MEDE). We calculate the MEDE between the ground truth of the data with the prediction results in\npixels.\nPredicting Seq-len\nModel Name Training Seq-len 50 ms 100 ms 150 ms 200 ms 250 ms 500 ms 1 s\n500 ms 79 95 110 90 91 102 93\nModel Family 1 750 ms 89 154 149 177 131 134 142\n1 ms 105 108 129 132 158 189 124\n500 ms 36 50 47 71 84 91 109\nModel Family 2 750 ms 64 71 86 82 99 148 119\n1 ms 85 93 89 103 119 126 147\n500 ms 46 52 57 53 64 83 86\nModel Family 3 750 ms 42 63 82 88 123 93 98\n1 ms 54 59 69 101 99 118 128\n500 ms 58 46 60 66 77 48 67\nModel Family 4 750 ms 55 69 71 80 88 71 78\n1 ms 61 73 115 114 85 77 70\nthe two sub-layers in each encoder layer, the decoder inserts a third sub-layer to apply self-attention\nmechanisms over the encoder output. The decoder mask has also been used in this work to introduce\na sequence mask that ensures that the model only attends to the past time steps during inference and\nnot to the future time steps that need to be predicted. The mask is necessary to prevent the model\nfrom using future information when making predictions, which violates the causality principle in\ntime series forecasting. The Mean Square Error (MSE) loss function has been used with the Adam\noptimizer [61] with hyperparameters β1 = 0.9, β2 = 0.98 and ϵ = 10−9. Dropout and weight decay\nhave been used for this work to prevent overfitting with 30 epochs.\n5 Experiments and Results\nThis section presents the performed experiments and results for different encoder-decoder models. Our\nmodels have been trained on sequences of different input lengths to predict a set of 2D coordinates of\nthe centroids representing the future trajectory as a sequence. The input features include the trajectory\ninformation of the players, as well as all joints and ball positions.\nTable 1 displays the use of four distinct model Families 1 through 4. Model Family 1 was trained\nexclusively on tennis players’ trajectory information to predict a single player’s movement trajectory.\nModel Family 2 used trajectory information and body joint data for all players to predict the player’s\ntrajectory. Model Family 3 employed the same features as Model Family 2 but with the additional use\nof a mask to account for occlusions. Even though Model Family 2 shows good results when feeding\nall the body joint features to the encoder, the mask has improved Model Family 3 performance. In\nModel Family 4, we used trajectory information and body joint data for both players plus the ball\nposition with the mask to predict one player’s trajectory.\nTo evaluate the performance of the Model Families, we calculate the Mean Euclidean Distance Error\n(MEDE) [62] between the ground truth of the data with the prediction results in pixels using this\nequation:\nMEDE =\nPT\nt=1\nq\n(xt − ˆxt)2 + (yt − ˆyt)2\nT . (2)\nThe variables xt and ˆxt represent the ground truth and prediction, respectively, for the X coordinate.\nSimilarly, the yt and ˆyt represent ground truth and prediction for the Y coordinate. The summation is\ntaken over T time points. We can think of our approach as utilizing close-up cameras to continuously\ntrack the tennis player by predicting future movement trajectory with a 224 x 224 boundary box,\nas shown in Figure 1. We assume that the predicted trajectory remains inside the bounding box up\n9\nFigure 6: Compare the predicted results of 250 ms, 500 ms, and 1 s (X and Y ) of model Family 4\nwith the ground truth of the centroid coordinates of the tennis player.\nto the error of 48 pixels when predicting 30 frames ahead (500 ms) in Model Family 4. Thus, it\nkeeps cameras capturing the player’s body movements. In addition, we evaluate the performance of\nModel Family 4, which has been trained on 500 ms sequences, in predicting sequences of different\nlengths (250 ms, 500 ms, 1 s), using the ground truth centroid coordinates X and Y as the reference\nfor comparison, as shown in Figure 6.\n5.1 Analysis\nOur experiments evaluated four model families to predict the future trajectory of a tennis player.\nModel Family 1 performed the worst, while Model Family 2 showed improvement in short trajectory\nprediction, predicting 50 ms, 100 ms, and 150 ms ahead, as shown in Table 1 with bold font.\nModel Family 2 showed bad predictions with longer sequences. However, using a mask, Model\nFamily 3 significantly improved the prediction of long sequences. Applying the mask in our work\nhelped obtain the best results when predicting 200 ms and 250 ms into the future by preventing\ninformation leakage during training. By masking future time steps, the model was forced to learn\nhow to generate accurate predictions based on the information available at each time step, resulting\nin more robust and generalizable representations.\nIn the final part of our experiments, we incorporated the position of the tennis ball into the input\nsequence for Model Family 4. This allowed the model to capture the relationship between the players’\nmovements and the ball’s position, leading to more accurate predictions, especially when the ball’s\nposition influenced the player’s movement. As a result, Model Family 4 showed the best results\nwith less error in pixels using the MEDE for predictions 500 ms and 1 s ahead, with 48 and 67,\nrespectively.\nOur findings suggest incorporating contextual information like ball positions and masking techniques\ncan significantly improve models’ performance for predicting long sequences of a tennis player’s\nmovement trajectory.\n6 Conclusion\nIn this work, we have proposed a novel approach for predicting the future movement trajectory\nof tennis players by leveraging various sources of information, such as the player’s joints, past\ntrajectory, and ball positions. Our approach has the potential to enable cameras to track the tennis\nplayers by automatically predicting their centroid location without any human intervention, using\nan encoder-decoder Transformer model. Our model incorporates a variety of available information,\nincluding body joints, trajectory, and ball position, to most accurately predict the player’s future\ntrajectory. In particular, The model Family 4 shows the least error for predicting long sequences and\nuses all the features with a decoder mask.\n10\nReferences\n[1] Ali Kareem Alhami. Generating tennis player by the predicting movement using 2d pose estimation. 2022.\n[2] Bruno Siciliano, Oussama Khatib, and Torsten Kröger. Springer handbook of robotics , volume 200.\nSpringer, 2008.\n[3] NEIL Owens, C Harris, and C Stennett. Hawk-eye tennis system. In 2003 international conference on\nvisual information engineering VIE 2003, pages 182–185. IET, 2003.\n[4] MF Bergeron, Jennifer L Waller, and EL Marinik. V oluntary fluid intake and core temperature responses in\nadolescent tennis players: sports beverage versus water. British journal of sports medicine, 40(5):406–410,\n2006.\n[5] Alison L Sheets, Geoffrey D Abrams, Stefano Corazza, Marc R Safran, and Thomas P Andriacchi.\nKinematics differences between the flat, kick, and slice serves measured using a markerless motion capture\nmethod. Annals of biomedical engineering, 39:3011–3020, 2011.\n[6] Per Lindström, Ludwig Jacobsson, Niklas Carlsson, and Patrick Lambrix. Predicting player trajectories in\nshot situations in soccer. In Machine Learning and Data Mining for Sports Analytics: 7th International\nWorkshop, MLSA 2020, Co-located with ECML/PKDD 2020, Ghent, Belgium, September 14–18, 2020,\nProceedings 7, pages 62–75. Springer, 2020.\n[7] Sandro Hauri, Nemanja Djuric, Vladan Radosavljevic, and Slobodan Vucetic. Multi-modal trajectory\nprediction of nba players. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 1640–1649, 2021.\n[8] Gopal Pingali, Agata Opalach, Yves Jean, and Ingrid Carlbom. Visualization of sports using motion\ntrajectories: providing insights into performance, style, and strategy. pages 75–544, 2001.\n[9] Brandon Giles, Stephanie Kovalchik, and Machar Reid. A machine learning approach for automatic\ndetection and classification of changes of direction from player tracking data in professional tennis.Journal\nof sports sciences, 38(1):106–113, 2020.\n[10] Kevin Ma. A real time artificial intelligent system for tennis swing classification. pages 000021–000026,\n2021.\n[11] Guangyu Zhu, Qingming Huang, Changsheng Xu, Liyuan Xing, Wen Gao, and Hongxun Yao. Human\nbehavior analysis for highlight ranking in broadcast racket sports video. IEEE Transactions on Multimedia,\n9(6):1167–1182, 2007.\n[12] Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon\nFatahalian. Learning physically simulated tennis skills from broadcast videos. ACM Transactions on\nGraphics (TOG), 42(4):1–14, 2023.\n[13] Xinyu Wei, Patrick Lucey, Stuart Morgan, and Sridha Sridharan. Forecasting the next shot location in tennis\nusing fine-grained spatiotemporal tracking data. IEEE Transactions on Knowledge and Data Engineering,\n28(11):2988–2997, 2016.\n[14] Tharindu Fernando, Simon Denman, Sridha Sridharan, and Clinton Fookes. Memory augmented deep\ngenerative models for forecasting the next shot location in tennis. IEEE Transactions on Knowledge and\nData Engineering, 32(9):1785–1797, 2019.\n[15] Bai Ning and Liu Na. Deep spatial/temporal-level feature engineering for tennis-based action recognition.\nFuture Generation Computer Systems, 125:188–193, 2021.\n[16] Silvia Vinyes Mora and William J Knottenbelt. Deep learning for domain-specific action recognition in\ntennis. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,\npages 114–122, 2017.\n[17] Tom Polk, Dominik Jäckle, Johannes Häußler, and Jing Yang. Courttime: Generating actionable insights\ninto tennis matches using visual analytics. IEEE Transactions on Visualization and Computer Graphics,\n26(1):397–406, 2019.\n[18] Yu-Chuan Huang, I-No Liao, Ching-Hsuan Chen, Tsì-Uí ˙Ik, and Wen-Chih Peng. Tracknet: A deep\nlearning network for tracking high-speed and tiny objects in sports applications. In 2019 16th IEEE\nInternational Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–8. IEEE,\n2019.\n[19] Nayara MS Rocha, Milena F Pinto, Iago Z Biundini, Aurelio G Melo, and André LM Marcato. Analysis of\ntennis games using tracknet-based neural network and applying morphological operations to the match\nvideos. Signal, Image and Video Processing, 17(4):1133–1141, 2023.\n[20] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\n[21] Ben McEwen, Richard Green, Menno Finlay-Smit, and Clare McLennan. Predictive animal tracking for\ninvasive species identification and elimination.\n11\n[22] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime\ntracking. In 2016 IEEE international conference on image processing (ICIP), pages 3464–3468. IEEE,\n2016.\n[23] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep\nassociation metric. In 2017 IEEE international conference on image processing (ICIP), pages 3645–3649.\nIEEE, 2017.\n[24] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness\nof detection and re-identification in multiple object tracking. International Journal of Computer Vision,\n129:3069–3087, 2021.\n[25] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu,\nand Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In Computer\nVision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part\nXXII, pages 1–21. Springer, 2022.\n[26] Nir Aharon, Roy Orfaig, and Ben-Zion Bobrovsky. Bot-sort: Robust associations multi-pedestrian tracking.\narXiv preprint arXiv:2206.14651, 2022.\n[27] Ramin Mehran, Alexis Oyama, and Mubarak Shah. Abnormal crowd behavior detection using social force\nmodel. In 2009 IEEE conference on computer vision and pattern recognition, pages 935–942. IEEE, 2009.\n[28] Alexandre Alahi, Vignesh Ramanathan, and Li Fei-Fei. Socially-aware large-scale crowd forecasting. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2203–2210,\n2014.\n[29] Vineet Kosaraju, Amir Sadeghian, Roberto Martín-Martín, Ian Reid, Hamid Rezatofighi, and Silvio\nSavarese. Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[30] Javad Amirian, Jean-Bernard Hayet, and Julien Pettré. Social ways: Learning multi-modal distributions of\npedestrian trajectories with gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, 2019.\n[31] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio Savarese.\nSophie: An attentive gan for predicting paths compliant to social and physical constraints. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pages 1349–1358, 2019.\n[32] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for trajectory\nforecasting. In 2020 25th international conference on pattern recognition (ICPR), pages 10335–10342.\nIEEE, 2021.\n[33] Lihuan Li, Maurice Pagnucco, and Yang Song. Graph-based spatial transformer with memory replay for\nmulti-future pedestrian trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 2231–2241, 2022.\n[34] Mohammad Mahdavian, Payam Nikdel, Mahdi TaherAhmadi, and Mo Chen. Stpotr: Simultaneous human\ntrajectory and pose prediction using a non-autoregressive transformer for robot following ahead. arXiv\npreprint arXiv:2209.07600, 2022.\n[35] Lina Achaji, Thierno Barry, Thibault Fouqueray, Julien Moreau, Francois Aioun, and Francois Charpil-\nlet. Pretr: spatio-temporal non-autoregressive trajectory prediction transformer. In 2022 IEEE 25th\nInternational Conference on Intelligent Transportation Systems (ITSC), pages 2457–2464. IEEE, 2022.\n[36] Lei Zhou, Dingye Yang, Xiaolin Zhai, Shichao Wu, ZhengXi Hu, and Jingtai Liu. Ga-stt: Human\ntrajectory prediction with group aware spatial-temporal transformer. IEEE Robotics and Automation\nLetters, 7(3):7660–7667, 2022.\n[37] Weihuang Chen, Fangfang Wang, and Hongbin Sun. S2tnet: Spatio-temporal transformer networks for\ntrajectory prediction in autonomous driving. In Asian Conference on Machine Learning, pages 454–469.\nPMLR, 2021.\n[38] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer networks\nfor pedestrian trajectory prediction. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XII 16, pages 507–523. Springer, 2020.\n[39] ˙Imren Dinç, Semih Dinç, Madhav Sigdel, Madhu S Sigdel, Ramazan S Aygün, and Marc L Pusey. Dt-\nbinarize: A decision tree based binarization for protein crystal images. In Emerging trends in image\nprocessing, computer vision and pattern recognition, pages 183–199. Elsevier, 2015.\n[40] Konstantinos G Derpanis. The harris corner detector. York University, 2:1–2, 2004.\n[41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740–755. Springer, 2014.\n12\n[42] A Shrivastava, P Kumar, Anubhav, C V ondrick, W Scheirer, DS Prijatelj, M Jafarzadeh, T Ahmad, S Cruz,\nR Rabinowitz, et al. Novelty in image classification. In A Unifying Framework for Formal Theories of\nNovelty: Discussions, Guidelines, and Examples for Artificial Intelligence, pages 37–48. Springer, 2023.\n[43] Chenggang Yan, Yiming Hao, Liang Li, Jian Yin, Anan Liu, Zhendong Mao, Zhenyu Chen, and Xingyu\nGao. Task-adaptive attention for image captioning. IEEE Transactions on Circuits and Systems for Video\ntechnology, 32(1):43–51, 2021.\n[44] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages\n1440–1448, 2015.\n[45] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time\nobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n779–788, 2016.\n[46] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and\nXiangxiang Chu. Yolov6 v3. 0: A full-scale reloading. arXiv preprint arXiv:2301.05586, 2023.\n[47] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and\nAlexander C Berg. Ssd: Single shot multibox detector. In Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14, pages 21–37.\nSpringer, 2016.\n[48] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object\ndetection. In Proceedings of the IEEE international conference on computer vision, pages 2980–2988,\n2017.\n[49] Nien-En Sun, Yu-Ching Lin, Shao-Ping Chuang, Tzu-Han Hsu, Dung-Ru Yu, Ho-Yi Chung, and Tsì-Uí\n˙Ik. Tracknetv2: Efficient shuttlecock tracking network. In 2020 International Conference on Pervasive\nArtificial Intelligence (ICPAI), pages 86–91. IEEE, 2020.\n[50] Paul Liu and Jui-Hsien Wang. Monotrack: Shuttle trajectory reconstruction from monocular badminton\nvideo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n3513–3522, 2022.\n[51] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for\nhuman pose estimation. arXiv preprint arXiv:2204.12484, 2022.\n[52] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[54] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. 2018.\n[56] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[57] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural\nnetworks. Neural computation, 1(2):270–280, 1989.\n[58] Neo Wu, Bradley Green, Xue Ben, and Shawn O’Banion. Deep transformer models for time series\nforecasting: The influenza prevalence case. arXiv preprint arXiv:2001.08317, 2020.\n[59] Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur,\nStella Wu, Cathal Smyth, Pascal Poupart, and Marcus Brubaker. Time2vec: Learning a vector representation\nof time. arXiv preprint arXiv:1907.05321, 2019.\n[60] Ronald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications, volume\n31999. McGraw-Hill New York, 1986.\n[61] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[62] Per-Erik Danielsson. Euclidean distance mapping. Computer Graphics and image processing, 14(3):227–\n248, 1980.\n13"
}