{
  "title": "Character and Subword-Based Word Representation for Neural Language Modeling Prediction",
  "url": "https://openalex.org/W2756999401",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2251561025",
      "name": "Matthieu Labeau",
      "affiliations": [
        "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A208304746",
      "name": "Alexandre Allauzen",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2056250865",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963022149",
    "https://openalex.org/W2950621961",
    "https://openalex.org/W1917350224",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2962732637",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W2963099225",
    "https://openalex.org/W1483912251",
    "https://openalex.org/W2101609803",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2463895987",
    "https://openalex.org/W2250741237",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2740646860",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2963208801",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W2175585630",
    "https://openalex.org/W2040711288",
    "https://openalex.org/W2142377809",
    "https://openalex.org/W2251648400",
    "https://openalex.org/W1860935423",
    "https://openalex.org/W1566256432",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W1505680913",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2949810612",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2611767671",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2143719855",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2963965928",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W1589170661",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W2585253991",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2100068632",
    "https://openalex.org/W2964090065",
    "https://openalex.org/W2507731975",
    "https://openalex.org/W2962819663"
  ],
  "abstract": "International audience",
  "full_text": "Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 1–13,\nCopenhagen, Denmark, September 7, 2017.c⃝2017 Association for Computational Linguistics.\nCharacter and Subword-Based word Representation for Neural Language\nModeling prediction\nMatthieu Labeau\nLIMSI-CNRS / Orsay, France\nlabeau@limsi.fr\nAlexandre Allauzen\nLIMSI-CNRS / Orsay, France\nallauzen@limsi.fr\nAbstract\nMost of neural language models use dif-\nferent kinds of embeddings for word pre-\ndiction. While word embeddings can be\nassociated to each word in the vocabulary\nor derived from characters as well as fac-\ntored morphological decomposition, these\nword representations are mainly used to\nparametrize the input, i.e. the context of\nprediction. This work investigates the ef-\nfect of using subword units (character and\nfactored morphological decomposition) to\nbuild output representations for neural lan-\nguage modeling. We present a case study\non Czech, a morphologically-rich lan-\nguage, experimenting with different input\nand output representations. When work-\ning with the full training vocabulary, de-\nspite unstable training, our experiments\nshow that augmenting the output word rep-\nresentations with character-based embed-\ndings can signiﬁcantly improve the perfor-\nmance of the model. Moreover, reducing\nthe size of the output look-up table, to let\nthe character-based embeddings represent\nrare words, brings further improvement.\n1 Introduction\nMost of neural language models, such as n-gram\nmodels (Bengio et al., 2003) are word based and\nrely on the deﬁnition of a ﬁnite vocabulary V.\nTherefore, a look-up table maps each wordw ∈V\nto a vector of real features, and is stored in a ma-\ntrix. While this approach yields signiﬁcant im-\nprovement for a variety of tasks and languages,\nsee for instance (Schwenk, 2007) in speech recog-\nnition and (Le et al., 2012; Devlin et al., 2014;\nBahdanau et al., 2014) in machine translation, it\ninduces several limitations.\nFor morphologically-rich languages, like Czech\nor German, the lexical coverage is still an impor-\ntant issue, since there is a combinatorial explosion\nof word forms, most of which are hardly observed\non training data. On the one hand, growing the\nlook-up table is not a solution, since it would in-\ncrease the number of parameters without having\nenough training examples for a proper estimation.\nOn the other hand, rare words can be replaced by\na special token. This acts as a word class merg-\ning very different words without any distinction,\nwhile using different word classes to handle out-\nof-vocabulary words (OOVs) (Allauzen and Gau-\nvain, 2005) does not really solve this issue, since\nrare words are difﬁcult to classify. Moreover, for\nmost inﬂected or agglutinative forms, as well as\nfor compound words, the word structure is over-\nlooked, wasting parameters for modeling forms\nthat could be more efﬁciently handled by word de-\ncomposition into subwords units.\nUsing subword units, whether they are built via\na different supervised method with embedded lan-\nguage knowledge, or from the training data, has\nbeen attempted many times, especially for speech\nrecognition. The main goal is to reduce the OOV\nrate. While most of them were focused on a spe-\nciﬁc language, (Creutz et al., 2007) is a represen-\ntative example of such a model applied to several\nmorphologically-rich languages.\nOne of the ﬁrst occurrences of general lan-\nguage models integrating morphological features\nto represent words are the factored language\nmodel (Bilmes and Kirchhoff, 2003) and its neu-\nral version (Alexandrescu and Kirchhoff, 2006).\nInput words are represented by their embedding,\nplus several other features, some of which include\nmorphemes. To alleviate the impact of OOVs,\n(Mueller and Schuetze, 2011) used morphologi-\ncal features for class-based predictions when in-\nput words are unknown, obtaining state-of-the-art\n1\nresults on English. More recently, several types\nof language models represent words as function of\nsubwords units: using a recursive structure (Lu-\nong et al., 2013), or an additive one (Botha and\nBlunsom, 2014). Quite a lot of work has been\nmade on language models that extract features di-\nrectly from the character sequence, whether they\nuse character n-grams (Sperr et al., 2013), or char-\nacters composed by a convolutional layer (Santos\nand Zadrozny, 2014; Kim et al., 2015) or a Bi-\nLSTM layer (Ling et al., 2015). This avoids us-\ning an external morphological analyser. We can\nnote that these types of models have also been ap-\nplied with success to several other task, including\nlearning word representations (Qiu et al., 2014;\nCotterell et al., 2016; Bojanowski et al., 2016;\nWieting et al., 2016), POS tagging (Plank et al.,\n2016; Ma and Hovy, 2016; Heigold et al., 2017),\nNamed entity recognition (Gillick et al., 2016),\nParsing (Ballesteros et al., 2015) and Machine\ntranslation (Costa-juss`a and Fonollosa, 2016). Re-\ncently, an exhaustive summary of previous work\non word representation by composing subword\nunits was presented in (Vania and Lopez, 2017).\nThis work also compares the types of subword\nunit, how they are composed, and their impact on\nvarious morphological typologies.\nWhile recurrent neural networks have shown\nexcellent performances for character-level lan-\nguage modeling (Sutskever et al., 2011; Hermans\nand Schrauwen, 2013), the results of such models\nare usually worse than those that use word-level\nprediction, since they have to consider a far longer\nhistory of tokens to be able to predict the next one\ncorrectly. However, more recent work (Hwang and\nSung, 2017) seems to obtain very satisfactory re-\nsults with a supplementary word-level layer that\nallows a better processing of the longer history.\nOur work focuses on replacing output word\nembeddings by representations built from sub-\nwords. To the best of our knowledge, such a model\nhas only been proposed in (J ´ozefowicz et al.,\n2016), which evaluates the use of convolutional\nand LSTM layers to build word representations\nfor outputs words. They allow the model to trade\nsize against perplexity, since their model performs\nworse than the classic softmax approach, but with\nfar less parameters. We ﬁrst propose to study the\ntraining of a language model which augments or\ncompletely replaces output words representations\nwith character-based representations. We compare\nthe effect of different architectures, as well as the\neffect of different input representations. Our re-\nsults show that:\n•When evaluating perplexity on the full training\nvocabulary, using an augmented output repre-\nsentation improves the model performance.\n•Not using the look-up table for rare words also\nimproves the model performance.\nFinally, we describe a short experiment with fac-\ntoring the output predictions using a morphologi-\ncal analysis, which we believe could lead to a fa-\ncilitated word generation when combined with re-\ninﬂexion models.\nOur paper is organized as follows: Section 2\ndescribes the general architecture of the language\nmodel, and of the representations used, as well as\nits training, Section 3 presents the experiments and\nSection 4 gives our results and discussion.\n2 Language model\nWe use a recurrent neural language\nmodel (Mikolov et al., 2010). The input of the net-\nwork is a sequence of words S = (w1,...,w |S|).\nGiven a ﬁxed sized vocabulary V, the lan-\nguage model outputs a multinomial distribution\nP(wi = j|wi−1\n1 ),∀j ∈ Vfor each position i in\nthe sequence, and with the prediction contexte\nwi−1\n1 = w1,...,w i−1. This allows us to compute\nthe following probability :\nP(w1,...,w |S|) =\n|S|∏\ni=1\nP(wi|wi−1\n1 )\nOur model uses the LSTM variant (Hochreiter\nand Schmidhuber, 1997). The hidden state hi will\nbe computed using the previous hidden state and a\ncomputed representation rwi of the word in posi-\ntion iin the sequence:\nhi = LSTM(rwi,hi−1)\nThe conditional probability distribution of the\nnext word is computed with a softmax function:\nP(wi = j|wi−1\n1 ) =\nexp\n(\nhirout\nj + bj\n)\n∑\nk∈V\nexp\n(\nhirout\nk + bk\n) (1)\n2\nWe propose to improve output word embed-\ndings by using representations built from sub-\nwords, as it is often done for input words.\nUsually, input and output word embeddings are\nparameters, stored in look-up matrices W and\nWout. The word embedding rword\nw of a word w\nis simply the column of W corresponding to its\nindex in the vocabulary V:\nrword\nw = [W]w\n2.1 Representing words\nWe consider two other types of representations:\ndecomposition of the words into characters (or n-\ngrams of characters), and decomposing them into\na Lemma and positional tags using a morpholog-\nical analysis. An example of these different de-\ncompositions is shown in table 1.\nRepresentation Decomposition\nWord po ˘c´atku\nCharacters p+o+ ˘c+´a+t+k+u\nCharacter 3-grams po ˘c+o˘c´a+˘c´at+´atk+tku\nLemma + Tags po ˘c´atek+N+MascIn+Sg+Loc+Act\nTable 1: Example of subword decompositions\nused for Czech word\n2.1.1 Character-based representations\nA word w is a character sequence {c1,..,c |w|}\nrepresented by their embeddings{rchar\nc1 ,.., rchar\nc|w| },\nwhere rchar\nci = [C]ci denotes the vector associated\nto the character ci. To infer a word embedding\nfrom its character embeddings, we use two differ-\nent architectures:\nFirst, a convolution layer (Waibel et al., 1990;\nCollobert et al., 2011), similar to layers used\nin (Santos and Zadrozny, 2014; Kim et al., 2015),\napplies a convolution ﬁlter WCNN\nnc over a sliding\nwindow of nccharacters, producing local features:\nxn\nnc = WCNN\nnc (rchar\ncn−nc+1 : ..: rchar\ncn )T + bCNN\nnc\nwhere xn\nnc is a vector obtained for each posi-\ntion nin the word. The embeddings of w is then\nobtained by applying a max-pooling and the acti-\nvation function φ:\n[rnc\nw ]i = φ\n(\n|w|−nc+1\nmax\nn=1\n[xn\nnc]i\n)\n(2)\nWe can use multiple ﬁlters ofncf different sizes\nand concatenate their results:\nrCharCNN\nw = (r\nnc1\nw : ... : r\nncf\nw ) (3)\nOur second method uses abi-LSTM (Hochreiter\nand Schmidhuber, 1997; Graves et al., 2005), on\ncharacters, similarly to (Ling et al., 2015). It com-\nbines the ﬁnal states −−→h|w|and ← −h1 of two LSTMs,\nrespectively over the character sequence and the\nreverse character sequence, which are computed\nas such:\n− →hi = LSTM(rchar\nci ,−−→hi−1)\n← −hj = LSTM(rchar\ncj ,←−−hj+1)\nrCharBiLSTM\nw = −−→h|w|: ← −h1 (4)\n2.1.2 Lemma+Tags decomposition\nFor morphologically-rich languages, the different\nmorphological properties of a word (gender, case,\n... ) are usually encoded using mutliple tags as\nshown in table 1. Therefore a word w is decom-\nposed into a lemma l along with a set of associ-\nated sub-tags T = {t1,..,t |T|}of ﬁxed size |T|.\nFor a given word, a single tag can be simply cre-\nated by the concatenation of the subtags. How-\never, this implies a large tagset and mitigates the\ngeneralization power since some sub-tags combi-\nnations can remain unobserved on training data.\nIn this work we prefer a factored representation\nwhere each sub-tags is considered independently.\nLemmas, similarly to surface forms, are repre-\nsented by |VL|vectors stored in a look-up matrix\nL, and rlemma\nl = [L]l. For every words, each sub-\ntag has its own vocabulary and its own look-up\nmatrix. However, the additional cost is negligi-\nble given their small size (see table 3). To infer a\nword embedding from a sub-tags set, we also use\ntwo methods. First, we simply concatenate their\nembeddings:\nrTagConcat\nT = rtag1\nt1 : ... : rtagi\nti : ... : r\ntag|T|\nt|T| (5)\nThe second method uses a bidirectionnal LSTM\non the sequence of tags T, using exactly the same\nstructure as in section 2.1.1:\nrTagBiLSTM\nT = −−→h|T|: ← −h1 (6)\n3\nHighwayLayer\nLSTM\nSoftmax\n’to’ ’re´aln´e’ ’?’\nLook-up tableC\nMax-Pooling\nWCNN3Look-up tableW\newordw eCharCNNw\nWords + CharCNNfor all words inV\n’je’ ’to’ ’re´aln´e’\nw c1 c2 c3 c4 c5\nWords +CharCNN\nFigure 1: Example architecture of our language model, when using word embeddings and a character\nCNN to build both input and output word representations.\n2.2 Training\nOur ﬁnal model, as illustrated in ﬁgure 1, uses\nconcatenation of word, character-based or lemma\nand tags embeddings, to obtain input and out-\nput word representations. Following (Kim et al.,\n2015), we used a Highway layer (Srivastava et al.,\n2015) to model interactions between concatenated\nembeddings of various sources.\nUsually, such a model is trained by maximizing\nthe log-likelihood. For a given word wi given its\npreceding sequence w1,...,w i−1, the model pa-\nrameters θare estimated in order to maximize the\nfollowing function for all the sequences observed\nin the training data:\nLL(θ) =\n|S|∑\ni=1\nlog Pθ(wi|wi−1\n1 ) (7)\nThis objective function implies a very costly\nsummation imposed by the softmax activation of\nthe output layer: large output vocabularies cause\na computational bottleneck due to the output nor-\nmalization.\nDifferent solutions have been proposed, as\nshortlists (Schwenk, 2007), hierarchical soft-\nmax (Morin and Bengio, 2005; Mnih and Hinton,\n2009; Le et al., 2011), or self-normalisation tech-\nniques (Devlin et al., 2014; Andreas et al., 2015;\nChen et al., 2016). Sampling-based techniques ex-\nplore a different solution, where a limited number\nof negative examples are sampled to reduce the\nnormalization cost. Working with a large vocab-\nulary, and with output representations potentially\nmore costly to compute, we choose to use the fol-\nlowing sampling-based training algorithms:\n•Target sampling, which is based on importance\nsampling (Bengio and S´en´ecal, 2008; Jean et al.,\n2015), directly approximates the normalization\nover Vby normalizing over a sampled subset.\nIndeed, the gradient of the objective described\nin equation 7 is written as:\n∂\n∂θ log Pθ(wi|wi−1\n1 ) = ∂\n∂θ(hirout\nwi + bwi)\n−Ew∼Pθ(.|wi−1\n1 )\n[∂\n∂θ(hirout\nwi + bwi)\n]\n(8)\nThe idea is to approximate the expectation of\nthe second term by importance sampling a sub-\nset of Vfrom a proposal distribution Q. Tar-\nget sampling implies associating with a part\n4\nDi of the training data a subset Vi of V that\ncorresponds to the target words of Di plus a\nsmall subset of the remaining words. The re-\nsulting objective is equivalent to approximating\nthe probability computed in equation 1 by nor-\nmalizing it only over Vi.\n•Noise contrastive estimation (NCE), introduced\nin (Gutmann and Hyv ¨arinen, 2012; Mnih and\nTeh, 2012), aims to discriminate between one\nexample sampled from the real data Dand k\nfrom a noise distribution Pn, and results in the\nmodel being theoretically unnormalized. The\nidea is to sample examples according to a mix-\nture:\nP(w|wi−1\n1 ) = 1\nk+ 1PD(w|wi−1\n1 )\n+ k\nk+ 1Pn(w|wi−1\n1 )\n(9)\nand train the model to recover whether the sam-\nple came from the data or the noise distribution.\nThis is done by minimizing the binary cross-\nentropy of recognizing the current sample’s ori-\ngin, using the posterior probabilities:\nP(w∼PD|w,wi−1\n1 ) =\nPθ(w|wi−1\n1 )\nPθ(w|wi−1\n1 ) +kPn(w|wi−1\n1 )\n(10)\nP(w∼Pn|w,wi−1\n1 ) = 1\n−P(w∼PD|w,wi−1\n1 )\n(11)\nBesides, the probabilities intervening in equa-\ntion 10 can be replaced by unnormalized scores\nat training time, since we can consider normal-\nizing quantities as parameters to be learned.\n•BlackOut (Ji et al., 2015), also approximating\nthe normalization computation, with a weighted\nsampling scheme and a discriminative objec-\ntive.It can be considered as a variant from NCE\nwhere we sample a set of kexamples Sk from a\nproposal distribution Q. We then proceed to ap-\nply NCE with a re-weighted noise distribution\nPn(w|wi−1\n1 ) = 1\nk\n∑\nwj∈Sk\nQ(wj)\nQ(w) Pθ(ww|wi−1\n1 )\n(12)\nwhich empirically behaves far better than NCE,\nproviding an improved stability. BlackOut can\nalso be linked to Importance sampling.\nUltimately, these three algorithms approximate\nthe negative log-likelihood computed on a number\nk of negative samples from V, using an easy to\nsample distribution.\n3 Experiments\nExperiments are carried out on Czech, a morpho-\nlogically rich language using the different criteria\ndescribed in section 2.2.\n3.1 Data\nWe used data from the parallel corpus News-\ncommentary 2015, from the WMT News MT\nTask. The data consists in 210K word sequences,\namounting in about 4,7M tokens. We divided the\ndata into a training, development and testing sets,\nthese last two amounting to 150K tokens each. In\nour experiments, we use different vocabulary sizes\nby varying the frequency threshold: words are se-\nlected when their frequency in the training data are\nstricly higher than the threshold. Table 2 shows\nthe correspondences between vocabulary sizes and\nthese thresholds.\nfTh |VTh|\n0 (All words) 159142\n1 66743\n5 37010\n10 25295\nTable 2: V ocabulary sizes for different frequency\nthresholds\nThe lemma and tags decomposition pre-\nsented in section 2.1.2 were obtained with Mor-\nphodita (Strakov ´a et al., 2014). There is 12 tag\ncategories for Czech. V ocabulary sizes for charac-\nters, lemma and tags are detailed in table 3.\n|VC| |VL| |V tagi|i=1..|T|\n155 61364 [12, 65, 11, 6, 9, 6, 3, 5, 5, 4, 3, 3]\nTable 3: V ocabulary sizes for subword units\n3.2 Setup\nThe different versions of our model used in ex-\nperiments are shown in table 4. We used a High-\nway layer when there is a concatenation of em-\nbeddings of different sources, which is for almost\nall architectures. We tried applying a Highway\nlayer to the output representation, but it seemed\n5\nalmost always counter-productive, rendering train-\ning more unstable. In all experiments presented\nhere, weights are not tied between input and output\nrepresentations, since our preliminary experiments\nwith tied weights always gave worst results. Be-\nsides, we didn’t mix structures for character-level\nrepresentations (for example, using an input Char-\nCNN and output CharLSTM) since our ﬁrst ex-\nperiments gave systematically worse results than\nusing the same structures). When using different\ntypes of representations, we kept consistency be-\ntween vocabularies: if both lemmas and words are\nused in a model, any lemma considered unknown\nwill have its corresponding word unknown, and in-\nversely. The same (or corresponding) vocabularies\nare used for inputs, outputs, and evaluation. The\nonly exception is presented in section 4.4. When\nusing a character-based output representation, dur-\ning evaluation, the unknown token is built from a\nspeciﬁc character token, a speciﬁc lemma token,\nand 12 speciﬁc tag tokens that are parameters of\nthe model.\nInput representation rw Eq\nWords rwordw\nCharCNN Hw(rCharCNNw ) 3\nCharBiLSTM Hw(rCharBiLSTMw ) 4\nWords + CharCNN Hw(rwordw :rCharCNNw )\nWords + CharBiLSTMHw(rwordw :rCharBiLSTMw )\nLemma + Tags Concat.Hw(rlemmal :rTagConcatT ) 5\nLemma + TagsBiLSTMHw(rlemmal :rTagBiLSTMT ) 6\nOutput representationroutw\nWords rwordw\nWords + CharCNN rwordw :rCharCNNw\nWords + CharBiLSTMrwordw :rCharBiLSTMw\nLemmas rlemmal\nLemmas + CharCNNrlemmal :rCharCNNl\nLemmas + CharLSTMrlemmal :rBiLSTMl\nTable 4: Detail of input and output representations\nused in our experiments. Hw designate the use of\na Highway layer\nOur experiments aim at comparing potential use\nof subword-based word representation, and thus\nare not directed towards performance. For this rea-\nson, we used the same implementation for all ex-\nperiments and did not speciﬁcally try to optimize\nthe general model structure or the dimensional hy-\nperparameters, neither compared our results with\nbenchmarks on Czech corpora.\n3.3 Training and evaluation\nLanguage models are evaluated with perplexity:\nPPL = exp\n\n\n|S|∑\ni=1\n−log Pθ(wi|wi−1\n1 )\n|S|\n\n\nover all sequences in the testing data. Perplex-\nity is computed for a ﬁxed output vocabulary V,\nwhich allows to compare models using the same\noutput vocabulary. However, we can’t evaluate\nmodel performance on out-of-vocabulary words,\nsince those are to be classiﬁed as the unknown to-\nken in V.\nOur models are implemented with Tensor-\nﬂow (Abadi et al., 2015). We use the Adam algo-\nrithm (Kingma and Ba, 2014) with an initial learn-\ning rate of 5 ∗10−4 for training, over a maximum\nof 10 epochs, with a batch size of 128 sequences.\nHowever, since the training is often unstable, the\nmodel backtracks to the last checkpoint if it does\nnot improve its performance on validation data af-\nter 1/10 of an epoch, and stop training after10 un-\nsuccessful loadings in a row. To avoid overﬁtting,\nwe use dropout with probability 0.5 on recurrent\nlayers, and L2 regularization on feedforward lay-\ners.\nWe use two hidden layers, and choose our em-\nbeddings dimensions in order to obtain, for each\ntype of representation, an embedding dimension\nof 150. In the case of the CNN, we used ﬁlters\nof 3, 5 and 7 characters, of dimension 30, 50, and\n70. Whether we use NCE, blackOut, or impor-\ntance sampling, we draw k = 500 noise samples\nby batch. For all experiments, we report the per-\nplexity on test data at the end of training. Results\npresented in tables 5, 6, 7 are the average of the\nresults obtained on 5 models, and the standard de-\nviation.\n4 Results\n4.1 Inﬂuence of the vocabulary size\nWe ﬁrst train our model with different vocabulary\nsizes. As shown in ﬁgure 2, our model fails to im-\nprove upon the conventional word model when the\noutput vocabulary size is relatively small (shown\non the two leftmost graphs). More precisely, mod-\nels that use word and character-based representa-\ntions at the output seem unable to learn after a\ncouple of iterations. We ﬁrst link this behaviour\nto the difﬁculty met by the authors in (J ´ozefowicz\net al., 2016): since most logits are tied when we\nuse an output character-based representation - as\n6\n0 1 2 3 4\nfTh = 10\n200\n400\n600\n800\n1000Test perplexity\n0 1 2 3 4\nfTh = 5\n0 1 2 3 4\nfTh = 1\nWords\nWords+CharCNN\n0 1 2 3 4\nfTh = 0\nFigure 2: Test perplexities obtained when training models using Words as input representation and\nWords+CharCNN as output representations, for various vocabulary sizes. Corresponding vocabulary\nsizes are given in table 2. The models are trained with target sampling.\nopposed to independently learned word embed-\ndings, the function mapping from word to word\nrepresentation is smoother and training becomes\nmore difﬁcult. They used a smaller learning rate\nand a low dimensional correction factor, learned\nfor each word, as a work-around.\nHowever, increasing the vocabulary size re-\nduces this effect . This is especially clear with the\nwhole training vocabulary (on the rightmost graph\nof ﬁgure 2): in this setup, using a character-based\nrepresentation improves the performance of the\nmodel. We can assume that, for rare words, learn-\ning independent embeddings fails since scarce up-\ndates of these embeddings are insufﬁcient. For the\nrare words, combining word and character-based\nembeddings allows the model to better counteract\nthe sparsity issue.\n4.2 Choice of the training criterion\nGiven the previous results, we use the full train-\ning vocabulary to assess the impact of the train-\ning criterion. However, using this full training\nvocabulary renders training very unstable, espe-\ncially with sampling-based algorithms. Stability\nissues, especially for the Noise-contrastive estima-\ntion, have previously been discussed (Chen et al.,\n2016; J ´ozefowicz et al., 2016). We shortly ex-\nperimented to choose the most practical criterion\nto use. Figure 3 shows the shape of the training\ncurves. While target sampling and blackOut both\nseem to work properly, NCE needs far more noise\nsamples to converge. We believe this is related\nto the tensorﬂow implementation, which re-use\nthe same noise samples for every example in the\nbatch, which leads to a lack of diversity in negative\n0 1 2 3 4\nTest perplexity\n12K\n20K\n28KNCE\n0 1 2 3 4\n400\n800\n1200\n1600\n2000BlackOut\n0 1 2 3 4\n400\n800\n1200\n1600\n2000Importance Sampling\nWords\nWords+CharCNN\nFigure 3: Test perplexities obtained when train-\ning models using Words as input representation\nand Words+CharCNN as output representations,\nfor various training methods: Noise contrastive es-\ntimation, blackOut and Target sampling.\n7\nexamples. Augmenting the number of samples or\nreducing the size of the batch are possible solu-\ntions, but they increase the training time. Black-\nOut obtains better results because, while very sim-\nilar to NCE, the scores used as coming from the\nnoise distribution are context-dependent, which\nbrings diversity to negative examples.\nOverall, across several experiments, target sam-\npling performs better than blackOut, and we\nchoose to use it for the rest of our experiments.\nSince training is still quite unstable, depending on\nthe architecture, we report results across 5 train-\nings for the next sections.\nOutput RepresentationWords Words + Char\nInput Representation CNN BiLSTM\nWords 563 ±53 432 ±18 480 ±31\nChar CNN 698 ±41 543 ±16 -\nBiLSTM 971±24 - 938 ±48\nWords +\nChar\nCNN 495±34 411 ±40 -\nBiLSTM 537±24 - 480±21\nLemmas +\nTags\nConcat. 521 ±47 424 ±22 502 ±54\nBiLSTM 541±8 445 ±24 496 ±39\nTable 5: Average test perplexities obtained when\ntraining 5 models with target sampling, for various\ninput/output representations. Results in bold are\nthe best models for a given output representation.\n4.3 Effects of the representation choice\nTable 5 gathers the main experimental results to\nassess which combination of input and output rep-\nresentations gives the best performance. For any\ninput representation, augmenting the output rep-\nresentation with a character-based embedding im-\nproves the performance of the model. It is espe-\ncially true for convolutional layers. We also can\nnotice that the improvement is better for models\nthat performed badly with basic output word em-\nbeddings.\nOverall, biLSTMs perform worse than their\nconvolution/concatenation counterparts. Finally,\nthe best average perplexity of 495 for word only\noutput representations is improved to an average\nof 411 for augmented output representations.\nOther output representations : First, our ex-\nperiments with only character-based embeddings\nas output representations give results far worse\nthan those reported in 5, with our best model ob-\ntaining an average perplexity of ≈2500. Train-\ning is also far more unstable. We believe these\nresults are linked to the difﬁculties mentioned\nin (J´ozefowicz et al., 2016) and in section 4.1.\nWe also tried to use the lemma+tags decompo-\nsition presented in section 2.1.2, but without suc-\ncess. When tags were ambiguous across several\noccurrences of the same words, we tried using spe-\nciﬁc tokens, or choosing the most frequent tags,\nbut in both cases the model severely overﬁts.\nFinally, we tried to use word embeddings pre-\ntrained with word2vec (Mikolov et al., 2013) as\noutput representations. We obtained results very\nsimilar to those of classical word embeddings,\nwith a small but noticeable improvement when the\ninput representation used LSTM. However, these\nimprovements are still well under those obtained\nby augmenting the output representation with a\ncharacter-based embedding.\n4.4 Inﬂuence of the size of the word\nembeddings vocabulary\nInput\nRepresentationfWTh\nWords + CharCNN\nAll words Frequent words Rare words\nWords\n0 432 ±18 286 ±13 5170±1310\n1 415 ±26 258 ±13 6510 ±780\n5 390±20 250 ±14 7210±1290\n10 416±20 265 ±11 8100 ±580\nCharCNN\n0 543 ±16 348 ±10 6400 ±1480\n1 523 ±17 328 ±22 5070±1080\n5 478±16 316 ±27 7000 ±1800\n10 488±31 338 ±25 8210 ±2160\nWords +\nCharCNN\n0 411 ±40 271 ±30 4470±190\n1 374 ±10 242±7 5020±870\n5 367±14 241 ±8 5560±1220\n10 393±19 254 ±13 6600 ±1910\nLemma +\nTagsConcat.\nAll 449±26 293 ±16 5830 ±760\n1 439 ±34 287 ±11 6220 ±1080\n5 408±32 269 ±20 4600 ±1280\n10 430±32 269±20 8410±1140\nLemma +\nTagsBiLSTM\nAll 445±24 288 ±13 7150 ±1560\n1 424 ±34 281 ±21 5380 ±1000\n5 387±9 258 ±5 4300 ±1390\n10 442±17 287 ±13 7390 ±1690\nTable 6: Test perplexity averaged on 5 models\ntrained with target sampling, for various input rep-\nresentations and output word look-up table sizes.\nCorresponding vocabulary sizes are given in ta-\nble 2. Test perplexities are given for all words,\nfrequent words (frequency > 10) and rare words\n(frequency <10). In bold are the best models for\na given input representation.\n8\nFollowing our observations in section 4.1, we\nthen assess the effect of reducing the word vo-\ncabulary size for Words+CharCNN output repre-\nsentation. We don’t change the size of the event\nspace: when constructing output representations\nfor words under a chosen frequency, we simply\ndon’t use the word representation. For example,\nusing a threshold of fW\nTh = 10 means that words\nthat appear less thant ten times won’t have their\nown word embedding, and will be represented\nby the unknown word token combined with their\ncharacter-based representation. Results are shown\nin table 6. We can see that for all input representa-\ntions, using a speciﬁc unknown token in place of a\nspeciﬁc word embedding for words appearing less\nthan 5 times in training data gives the best perfor-\nmance. Reducing the look-up table to words only\nappearing more than 10 times gives worse results,\nwhile they are still better than if we keep the full\ntable. However, there is no clear trend when look-\ning at the rare words perplexities, which are very\nhard to interpret, given their very high standard de-\nviation. With a smaller output word look-up table,\nour best average perplexity of 411 is reduced to\n376, which is a very sizeable overall improvement.\nOutput RepresentationLemmas Lemmas + Char\nInput Representation CNN BiLSTM\nWords 240 ±12 220 ±9 222 ±12\nChar CNNN 308 ±15 270 ±11 -\nBiLSTM 477±17 - 429 ±9\nWords +\nChar\nCNN 234 ±9 203±7 -\nBiLSTM 238±6 - 225 ±11\nLemmas +\nTags\nConcat. 239 ±3 211 ±5 217 ±9\nBiLSTM 232±5 203 ±6 212 ±6\nTable 7: Test perplexities averaged on5 models on\nlemmas with a multiple objectives cost function.\nResults are given for various input/output repre-\nsentations. In bold are the best models for a given\noutput representation.\n4.5 Predicting root and tags jointly\nWhile using the lemma+tags decomposition to\nbuild output representation was not, in our ex-\nperiments, successful, we investigated a factorised\nprediction of lemma and tags. We used different\ncosts for predicting lemmas and each tag, which\nare summed into a ﬁnal objective function. As re-\ncently seen in (Martinez et al., 2016; Burlot and\nYvon, 2017), these objectives are individually eas-\nier when working with morphologically-rich lan-\nguages, and fully inﬂected words can be obtained\nby using morphological inﬂection models, which\nhave been shown to be quite successful (Faruqui\net al., 2016; Kann et al., 2017).\nTable 7 shows the test perplexities on lemmas\nfor various input and output representations. We\ncan observe that in all cases training is far more\nstable, with generally lower standard deviations.\nIn this case, using a lemma+tags with a BiLSTM\nor a Words+CharCNN input representation both\ngive the best results, while augmenting the output\nrepresentation of the lemma with a character-build\nembedding also improves results. This makes the\njoint learning of a factored prediction and reinﬂec-\ntion language model a very interesting direction\nfor future work.\n5 Conclusion\nWe described a neural language model allowing\nthe use of subword units for both input and output\nword representations. While in our experiments\ntraining with a full vocabulary is unstable, we can\nidentify important trends: augmenting output rep-\nresentations with character-based embeddings im-\nproves the model performance, and in this setup,\nreplacing independent word embeddings by the\nunknown token for rare words yields further im-\nprovement. It is worth noticing that this also opens\nthe vocabulary, since our model can be used to\nrescore unknown words. Additional experiments\nsuggest that factoring the output of the model with\na lemma+tags decomposition, then re-inﬂecting\nthese into words, could make generation easier:\nthis is a direction we plan to investigate.\nAcknowledgements\nWe wish to thank the anonymous reviewers for\ntheir helpful comments. This work has been\nfunded by the European Union’s Horizon 2020\nresearch and innovation programme under grant\nagreement No. 645452 (QT21).\nReferences\nMart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\n9\nLevenberg, Dan Man´e, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-\nwar, Paul Tucker, Vincent Vanhoucke, Vijay Va-\nsudevan, Fernanda Vi ´egas, Oriol Vinyals, Pete\nWarden, Martin Wattenberg, Martin Wicke, Yuan\nYu, and Xiaoqiang Zheng. 2015. TensorFlow:\nLarge-scale machine learning on heterogeneous sys-\ntems. Software available from tensorﬂow.org.\nhttp://tensorﬂow.org/.\nAndrei Alexandrescu and Katrin Kirchhoff. 2006.\nFactored neural language models. In Pro-\nceedings of the Human Language Technology\nConference of the NAACL, Companion Volume:\nShort Papers . Association for Computational\nLinguistics, New York City, USA, pages 1–4.\nhttp://www.aclweb.org/anthology/N/N06/N06-\n2001.\nA. Allauzen and J.L Gauvain. 2005. Open vocabulary\nasr for audiovisual document indexation. In IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nJacob Andreas, Maxim Rabinovich, Michael I. Jor-\ndan, and Dan Klein. 2015. On the accu-\nracy of self-normalized log-linear models. In\nAdvances in Neural Information Processing Sys-\ntems 28: Annual Conference on Neural Infor-\nmation Processing Systems 2015, December 7-\n12, 2015, Montreal, Quebec, Canada . pages\n1783–1791. http://papers.nips.cc/paper/5806-on-\nthe-accuracy-of-self-normalized-log-linear-models.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR\nabs/1409.0473.\nMiguel Ballesteros, Chris Dyer, and Noah A. Smith.\n2015. Improved transition-based parsing by model-\ning characters instead of words with lstms. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\nods in Natural Language Processing . Association\nfor Computational Linguistics, Lisbon, Portugal,\npages 349–359. http://aclweb.org/anthology/D15-\n1041.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch 3:1137–1155.\nYoshua Bengio and Jean-S ´ebastien S ´en´ecal. 2008.\nAdaptive importance sampling to accelerate train-\ning of a neural probabilistic language model. IEEE\nTrans. Neural Networks 19(4):713–722.\nJeff A. Bilmes and Katrin Kirchhoff. 2003. Factored\nlanguage models and generalized parallel backoff.\nIn Proceedings of the 2003 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics on Human Language Technol-\nogy: Companion Volume of the Proceedings of\nHLT-NAACL 2003–short Papers - Volume 2 . As-\nsociation for Computational Linguistics, Strouds-\nburg, PA, USA, NAACL-Short ’03, pages 4–6.\nhttps://doi.org/10.3115/1073483.1073485.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2016. Enriching word vectors\nwith subword information. CoRR abs/1607.04606.\nhttp://arxiv.org/abs/1607.04606.\nJan A. Botha and Phil Blunsom. 2014. Composi-\ntional Morphology for Word Representations and\nLanguage Modelling. In Proceedings of the Inter-\nnational Conference of Machine Learning (ICML) .\nBeijing, China.\nFranck Burlot and Franc ¸ois Yvon. 2017. Learning\nmorphological normalization for translation from\nand into morphologically rich language. The Prague\nBulletin of Mathematical Linguistics (Proc. EAMT)\n(108):49–60.\nWenlin Chen, David Grangier, and Michael Auli. 2016.\nStrategies for training large vocabulary neural lan-\nguage models. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics, Berlin, Germany, pages\n1975–1985. http://www.aclweb.org/anthology/P16-\n1186.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost)\nfrom scratch. J. Mach. Learn. Res. 12:2493–2537.\nhttp://dl.acm.org/citation.cfm?id=1953048.2078186.\nMarta R. Costa-juss `a and Jos ´e A. R. Fonollosa. 2016.\nCharacter-based neural machine translation. In\nProceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 2: Short Papers) . Association for Computa-\ntional Linguistics, Berlin, Germany, pages 357–361.\nhttp://anthology.aclweb.org/P16-2058.\nRyan Cotterell, Hinrich Sch ¨utze, and Jason Eis-\nner. 2016. Morphological smoothing and ex-\ntrapolation of word embeddings. In Proceed-\nings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume\n1: Long Papers) . Association for Computational\nLinguistics, Berlin, Germany, pages 1651–1660.\nhttp://www.aclweb.org/anthology/P16-1156.\nMathias Creutz, Teemu Hirsim ¨aki, Mikko Kurimo,\nAntti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti\nVarjokallio, Ebru Arisoy, Murat Sarac ¸lar, and An-\ndreas Stolcke. 2007. Morph-based speech recog-\nnition and modeling of out-of-vocabulary words\nacross languages. ACM Trans. Speech Lang. Pro-\ncess. 5(1):3:1–3:29.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard Schwartz, and John Makhoul.\n10\n2014. Fast and robust neural network joint mod-\nels for statistical machine translation. In Pro-\nceedings of the 52nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics, Baltimore, Maryland, pages 1370–1380.\nhttp://www.aclweb.org/anthology/P14-1129.\nManaal Faruqui, Yulia Tsvetkov, Graham Neubig, and\nChris Dyer. 2016. Morphological inﬂection gener-\nation using character sequence to sequence learn-\ning. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies. Association for Computational Lin-\nguistics, San Diego, California, pages 634–643.\nhttp://www.aclweb.org/anthology/N16-1077.\nDan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag\nSubramanya. 2016. Multilingual language process-\ning from bytes. In Proceedings of the 2016 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies. Association for Computational\nLinguistics, San Diego, California, pages 1296–\n1306. http://www.aclweb.org/anthology/N16-1155.\nAlex Graves, Santiago Fern´andez, and J¨urgen Schmid-\nhuber. 2005. Bidirectional LSTM networks for im-\nproved phoneme classiﬁcation and recognition. In\nArtiﬁcial Neural Networks: Formal Models and\nTheir Applications - ICANN 2005, 15th Interna-\ntional Conference, Warsaw, Poland, September 11-\n15, 2005, Proceedings, Part II. pages 799–804.\nMichael U. Gutmann and Aapo Hyv ¨arinen. 2012.\nNoise-contrastive estimation of unnormalized sta-\ntistical models, with applications to natural image\nstatistics. J. Mach. Learn. Res. 13(1):307–361.\nGeorg Heigold, Guenter Neumann, and Josef van\nGenabith. 2017. An extensive empirical eval-\nuation of character-based morphological tagging\nfor 14 languages. In Proceedings of the 15th\nConference of the European Chapter of the As-\nsociation for Computational Linguistics: Vol-\nume 1, Long Papers . Association for Computa-\ntional Linguistics, Valencia, Spain, pages 505–513.\nhttp://www.aclweb.org/anthology/E17-1048.\nMichiel Hermans and Benjamin Schrauwen. 2013.\nTraining and analysing deep recurrent neural net-\nworks. In C. J. C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K. Q. Weinberger, editors,\nAdvances in Neural Information Processing Sys-\ntems 26 , Curran Associates, Inc., pages 190–\n198. http://papers.nips.cc/paper/5166-training-and-\nanalysing-deep-recurrent-neural-networks.pdf.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput. 9(8):1735–\n1780. https://doi.org/10.1162/neco.1997.9.8.1735.\nKyuyeon Hwang and Wonyong Sung. 2017. Character-\nlevel language modeling with hierarchical recur-\nrent neural networks. In 2017 IEEE Inter-\nnational Conference on Acoustics, Speech and\nSignal Processing, ICASSP 2017, New Orleans,\nLA, USA, March 5-9, 2017 . pages 5720–5724.\nhttps://doi.org/10.1109/ICASSP.2017.7953252.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large\ntarget vocabulary for neural machine translation.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers). Associ-\nation for Computational Linguistics, Beijing, China,\npages 1–10. http://www.aclweb.org/anthology/P15-\n1001.\nShihao Ji, S. V . N. Vishwanathan, Nadathur Satish,\nMichael J. Anderson, and Pradeep Dubey. 2015.\nBlackout: Speeding up recurrent neural network lan-\nguage models with very large vocabularies. CoRR\nabs/1511.06909. http://arxiv.org/abs/1511.06909.\nRafal J´ozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. CoRR abs/1602.02410.\nhttp://arxiv.org/abs/1602.02410.\nKatharina Kann, Ryan Cotterell, and Hinrich Sch ¨utze.\n2017. Neural multi-source morphological re-\ninﬂection. In Proceedings of the 15th Con-\nference of the European Chapter of the As-\nsociation for Computational Linguistics: Vol-\nume 1, Long Papers . Association for Computa-\ntional Linguistics, Valencia, Spain, pages 514–524.\nhttp://www.aclweb.org/anthology/E17-1049.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2015. Character-aware neural language\nmodels. arXiv preprint arXiv:1508.06615 .\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR\nabs/1412.6980. http://arxiv.org/abs/1412.6980.\nHai-Son Le, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012. Continuous space translation models with\nneural networks. In Proceedings of the 2012 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics, Montr ´eal, Canada, pages 39–48.\nhttp://www.aclweb.org/anthology/N12-1005.\nHai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-\nLuc Gauvain, and Francois Yvon. 2011. Struc-\ntured output layer neural network language model.\nIn IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . Prague,\nCzech Republic, pages 5524–5527.\nWang Ling, Chris Dyer, Alan W Black, Isabel Tran-\ncoso, Ramon Fermandez, Silvio Amir, Luis Marujo,\nand Tiago Luis. 2015. Finding function in form:\nCompositional character models for open vocabu-\nlary word representation. In Proceedings of the\n11\n2015 Conference on Empirical Methods in Natu-\nral Language Processing . Association for Compu-\ntational Linguistics, Lisbon, Portugal, pages 1520–\n1530. http://aclweb.org/anthology/D15-1176.\nThang Luong, Richard Socher, and Christopher Man-\nning. 2013. Better word representations with recur-\nsive neural networks for morphology. In Proceed-\nings of the Seventeenth Conference on Computa-\ntional Natural Language Learning . Association for\nComputational Linguistics, Soﬁa, Bulgaria, pages\n104–113. http://www.aclweb.org/anthology/W13-\n3512.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional lstm-cnns-crf.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) . Association for Computational\nLinguistics, Berlin, Germany, pages 1064–1074.\nhttp://www.aclweb.org/anthology/P16-1101.\nMercedes Garcia Martinez, Loc Barrault, and Fethi\nBougares. 2016. Factored neural machine transla-\ntion architectures. In International Workshop on\nSpoken Language Translation (IWSLT’16) . Seattle\n(USA).\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH 2010, 11th Annual Conference of the\nInternational Speech Communication Association,\nMakuhari, Chiba, Japan, September 26-30, 2010 .\npages 1045–1048.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. 2013. Distributed\nrepresentations of words and phrases and their\ncompositionality. In C. J. C. Burges, L. Bottou,\nM. Welling, Z. Ghahramani, and K. Q. Wein-\nberger, editors, Advances in Neural Information\nProcessing Systems 26 . Curran Associates, Inc.,\npages 3111–3119. http://papers.nips.cc/paper/5021-\ndistributed-representations-of-words-and-phrases-\nand-their-compositionality.pdf.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model. In\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bottou,\neditors, Advances in Neural Information Processing\nSystems 21 , Curran Associates, Inc., pages 1081–\n1088. http://papers.nips.cc/paper/3583-a-scalable-\nhierarchical-distributed-language-model.pdf.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and sim-\nple algorithm for training neural probabilistic lan-\nguage models. In ICML. icml.cc / Omnipress.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nRobert G. Cowell and Zoubin Ghahramani, editors,\nProceedings of the Tenth International Workshop\non Artiﬁcial Intelligence and Statistics . Society for\nArtiﬁcial Intelligence and Statistics, pages 246–252.\nhttp://www.iro.umontreal.ca/ lisa/pointeurs/hierarchical-\nnnlm-aistats05.pdf.\nThomas Mueller and Hinrich Schuetze. 2011. Im-\nproved modeling of out-of-vocabulary words us-\ning morphological classes. In Proceedings of the\n49th Annual Meeting of the Association for Com-\nputational Linguistics: Human Language Tech-\nnologies. Association for Computational Linguis-\ntics, Portland, Oregon, USA, pages 524–528.\nhttp://www.aclweb.org/anthology/P11-2092.\nBarbara Plank, Anders Søgaard, and Yoav Goldberg.\n2016. Multilingual part-of-speech tagging with\nbidirectional long short-term memory models and\nauxiliary loss. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) . Association for\nComputational Linguistics, Berlin, Germany, pages\n412–418. http://anthology.aclweb.org/P16-2067.\nSiyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-\nYan Liu. 2014. Co-learning of word representations\nand morpheme representations. In Proceedings of\nCOLING 2014, the 25th International Conference\non Computational Linguistics: Technical Papers .\nDublin City University and Association for Com-\nputational Linguistics, Dublin, Ireland, pages 141–\n150. http://www.aclweb.org/anthology/C14-1015.\nCicero D. Santos and Bianca Zadrozny. 2014. Learning\ncharacter-level representations for part-of-speech\ntagging. In Tony Jebara and Eric P. Xing, editors,\nProceedings of the 31st International Conference\non Machine Learning (ICML-14). JMLR Workshop\nand Conference Proceedings, pages 1818–1826.\nhttp://jmlr.org/proceedings/papers/v32/santos14.pdf.\nHolger Schwenk. 2007. Continuous space language\nmodels. Comput. Speech Lang. 21(3):492–518.\nHenning Sperr, Jan Niehues, and Alex Waibel. 2013.\nLetter n-gram-based input encoding for continu-\nous space language models. In Proceedings of\nthe Workshop on Continuous Vector Space Models\nand their Compositionality. Association for Compu-\ntational Linguistics, Soﬁa, Bulgaria, pages 30–39.\nhttp://www.aclweb.org/anthology/W13-3204.\nRupesh Kumar Srivastava, Klaus Greff, and\nJ¨urgen Schmidhuber. 2015. Training very\ndeep networks. CoRR abs/1507.06228.\nhttp://arxiv.org/abs/1507.06228.\nJana Strakov ´a, Milan Straka, and Jan Haji ˇc. 2014.\nOpen-source tools for morphology, lemmatiza-\ntion, pos tagging and named entity recognition.\nIn Proceedings of 52nd Annual Meeting of the\nAssociation for Computational Linguistics: Sys-\ntem Demonstrations. Association for Computational\nLinguistics, Baltimore, Maryland, pages 13–18.\nhttp://www.aclweb.org/anthology/P14-5003.\nIlya Sutskever, James Martens, and Geoffrey Hinton.\n2011. Generating text with recurrent neural net-\n12\nworks. In Lise Getoor and Tobias Scheffer, edi-\ntors, Proceedings of the 28th International Confer-\nence on Machine Learning (ICML-11) . ACM, New\nYork, NY , USA, ICML ’11, pages 1017–1024.\nClara Vania and Adam Lopez. 2017. From char-\nacters to words to in between: Do we cap-\nture morphology? CoRR abs/1704.08352.\nhttp://arxiv.org/abs/1704.08352.\nAlexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-\nton, Kiyohiro Shikano, and Kevin J. Lang. 1990.\nReadings in Speech Recognition, Morgan Kaufmann\nPublishers Inc., San Francisco, CA, USA, chap-\nter Phoneme Recognition Using Time-delay Neural\nNetworks, pages 393–404.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2016. Charagram: Embedding words and\nsentences via character n-grams. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics, Austin, Texas, pages 1504–1515.\nhttps://aclweb.org/anthology/D16-1157.\n13",
  "topic": "Word (group theory)",
  "concepts": [
    {
      "name": "Word (group theory)",
      "score": 0.7979800701141357
    },
    {
      "name": "Character (mathematics)",
      "score": 0.7843077182769775
    },
    {
      "name": "Computer science",
      "score": 0.7787362337112427
    },
    {
      "name": "Vocabulary",
      "score": 0.7094777822494507
    },
    {
      "name": "Language model",
      "score": 0.6757831573486328
    },
    {
      "name": "Natural language processing",
      "score": 0.6609222888946533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6070734262466431
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5926594734191895
    },
    {
      "name": "Representation (politics)",
      "score": 0.5189526081085205
    },
    {
      "name": "Decomposition",
      "score": 0.4530770480632782
    },
    {
      "name": "Czech",
      "score": 0.4301333427429199
    },
    {
      "name": "Speech recognition",
      "score": 0.4251164197921753
    },
    {
      "name": "Linguistics",
      "score": 0.2520383298397064
    },
    {
      "name": "Mathematics",
      "score": 0.11692684888839722
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165912",
      "name": "Laboratoire Traitement et Communication de l’Information",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I12356871",
      "name": "Télécom Paris",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210115485",
      "name": "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I102197404",
      "name": "Université Paris-Sud",
      "country": "FR"
    }
  ]
}