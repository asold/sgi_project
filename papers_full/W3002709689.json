{
  "title": "Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case",
  "url": "https://openalex.org/W3002709689",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Wu, Neo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287917222",
      "name": "Green, Bradley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2915586907",
      "name": "Ben Xue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288889730",
      "name": "O'Banion, Shawn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1979941595",
    "https://openalex.org/W2910597988",
    "https://openalex.org/W2090079165",
    "https://openalex.org/W2117239687",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2068181924",
    "https://openalex.org/W2080321187",
    "https://openalex.org/W2130094219",
    "https://openalex.org/W2989766499",
    "https://openalex.org/W1998367480",
    "https://openalex.org/W2070272165",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2833607381",
    "https://openalex.org/W2995809239",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W154374270",
    "https://openalex.org/W1549386224",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2775569101",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2953101261"
  ],
  "abstract": "In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.",
  "full_text": "Deep Transformer Models for Time Series Forecasting:\nThe Inﬂuenza Prevalence Case\nNeo Wu1 Bradley Green 1 Xue Ben 1 Shawn O’Banion1\nAbstract\nIn this paper, we present a new approach to time\nseries forecasting. Time series data are preva-\nlent in many scientiﬁc and engineering disciplines.\nTime series forecasting is a crucial task in mod-\neling time series data, and is an important area\nof machine learning. In this work we developed\na novel method that employs Transformer-based\nmachine learning models to forecast time series\ndata. This approach works by leveraging self-\nattention mechanisms to learn complex patterns\nand dynamics from time series data. Moreover,\nit is a generic framework and can be applied to\nunivariate and multivariate time series data, as\nwell as time series embeddings. Using inﬂuenza-\nlike illness (ILI) forecasting as a case study, we\nshow that the forecasting results produced by our\napproach are favorably comparable to the state-\nof-the-art.\n1. Introduction\nSeasonal inﬂuenza epidemics create huge health and eco-\nnomic burdens, causing 291,000 - 646,000 deaths world-\nwide (Iuliano et al., 2018). In the United States, the Centers\nfor Disease Control and Prevention (CDC) publishes weekly\nILI reports based on its surveillance network. Despite its sig-\nniﬁcance in monitoring disease prevalence, typically there\nis at least a one-week delay of ILI reports due to data col-\nlection and aggregation. Therefore, forecasting ILI activity\nis critical for real-time disease monitoring, and for public\nhealth agencies to allocate resources to plan and prepare for\npotential pandemics.\nA variety of methods have been developed to forecast these\nILI time series data. These approaches range from mecha-\nnistic approaches to statistical and machine learning meth-\n1Google, LLC, 651 N 34th St., Seattle, W A 98103 USA. Corre-\nspondence to: Neo Wu <neowu@google.com>, Bradley Green\n<brg@google.com>, Xue Ben <sherryben@google.com>,\nShawn O’Banion<obanion@google.com>.\nTo be submitted to the37 th International Conference on Machine\nLearning, Vienna, Austria, 2020. Copyright 2020 by the author(s).\nods. Mechanistic modeling is based on the understanding of\nunderlying disease infection dynamics. For example, com-\npartmental methods such as SIR are popular approaches to\nsimulating disease spreading dynamics.\nStatistical and machine learning methods leverage the\nground truth data to learn the trends and patterns. One\npopular family of methods includes auto-regression (AR),\nautoregressive moving average (ARMA), and autoregressive\nintegrated moving average (ARIMA). Additionally, deep\nlearning approaches based on convolutional and recurrent\nneural networks have been developed to model ILI data.\nThese sequence-aligned models are natural choices for mod-\neling time series data. However, due to “gradient vanishing\nand exploding” problems in RNNs and the limits of convo-\nlutional ﬁlters, these methods have limitations in modeling\nlong-term and complex relations in the sequence data.\nIn this work, we developed a novel time series forecasting\napproach based on Transformer architecture (Vaswani et al.,\n2017). Unlike sequence-aligned models, Transformer does\nnot process data in an ordered sequence manner. Instead,\nit processes entire sequence of data and uses self-attention\nmechanisms to learn dependencies in the sequence. There-\nfore, Transformer-based models have the potential to model\ncomplex dynamics of time series data that are challenging\nfor sequence models. In this work we use ILI forecasting as\na case study to show that a Transformer-based model can be\nsuccessfully applied to the task of times series forecasting\nand that it outperforms many existing forecasting techniques.\nSpeciﬁcally, our contributions are the following:\n• We developed a general Transformer-based model for\ntime series forecasting.\n• We showed that our approach is complementary to\nstate space models. It can model observed data. Using\nembeddings as a proxy, our approach can also model\nstate variables and phase space of the systems.\n• Using ILI forecasting as a case study, we demonstrated\nthat our Transformer-based model is able to accurately\nforecast ILI prevalence using a variety of features.\n• We showed that in the ILI case our Transformer-based\nmodel achieves state-of-the-art forecasting results.\narXiv:2001.08317v1  [cs.LG]  23 Jan 2020\nDeep Transformer Models for Time Series Forecasting\nEncoder output\nDecoder output\nFigure 1.Architecture of Transformer-based forecasting model.\n2. Related Work\nSeveral studies have used Internet data such as Google\nTrends(Ginsberg et al., 2009), Twitter (Paul et al., 2014) and\nWikipedia (McIver & Brownstein, 2014) to forecast ILI ra-\ntios. Google Flu Trends (GFT) employs a linear model that\nuses Google search volumes of predeﬁned terms to estimate\ncurrent ILI ratios (“nowcasting”). While initially regarded\nas a great success, GFT suffered from over-estimation of\npeak ILI magnitude in subsequent years (Olson et al., 2013;\nLazer et al., 2014).\nOther studies combined GFT with new modeling techniques\nand additional signals. Lazer et al (2014) suggested an\nautoregression-based (AR) approach to extend GFT. Santil-\nlana et al(2014) improved GFT by developing a model that\nautomatically selects queries and updates models for ILI\nforecasting. Araz et al(2014) built linear regression models\nusing GFT data with extra signals. Yang et al(2015) de-\nveloped an autoregression-based model with Google search\ndata (“ARGO”) to estimate inﬂuenza epidemics. ARGO\noutperforms previous Google-search-based models. More\nrecently, a new ensemble model (“ARGONet”) built on top\nof ARGO was developed (Lu et al., 2019). This new ap-\nproach leverages spatial information to improve the model\nand achieved state-of-the-art results for ILI forecasting.\nDeep learning techniques have also been used for ILI fore-\ncasting. Liu et al(2018) trained an LSTM-based model to\npredict inﬂuenza prevalence using Google Trends, climate,\nair pollution and virological surviellence data. Venna et al\n(2019) developed an LSTM-based multi-stage model to in-\ncorporate climate and spatio-temporal adjustment factors\nfor inﬂuenza forecasting. Attention-based technqiues are\nalso applied for ILI forecasting. Zhu et al (2019) devel-\noped multi-channel LSTM neural networks to learn from\ndifferent types of inputs. Their model uses an attention\nlayer to associate model output with the input sequence\nto further improve forecast accuracy. Kondo et al(2019)\nadapted a sequence-to-sequence (“Seq2Seq”) model with a\nsimilar attention mechanism to predict inﬂuenza prevalence\nand showed that their approach outperformed ARIMA and\nLSTM-based models.\n3. Background\n3.1. Inﬂuenza and ILI\nInﬂuenza is a common infectious disease caused by infec-\ntion of inﬂuenza virus. Inﬂuenza affects up to 35 million\npeople each year and creates huge health and economic\nburdens (Iuliano et al., 2018; Putri et al., 2018). In the\nUnited States, the Centers for Disease Control and Preven-\ntion (CDC) coordinates a reporting network over a large\ngeographic area. Participating health providers within the\nDeep Transformer Models for Time Series Forecasting\nnetwork report statistics of patients showing inﬂuenza-like\nillness (ILI) symptoms. ILI symptoms are usually deﬁned\nas fever and cough and/or sore throat. The ILI ratio is com-\nputed as the ratio of the number of patients seen with ILI\nand the total number of patient visits that calendar week.\nThe CDC published ILI ratios for the USA and all but one\nindividual state (Florida). Additionally, state-level ILI ratios\nare normalized by state populations.\n3.2. State Space Models\nState space modeling (SSM) is widely applied to dynamical\nsystems. The evolution of dynamical systems is controlled\nby non-observable state variables. The system exhibits ob-\nservable variables which are determined by state variables.\nSSM has been applied to study complex systems in biology\nand ﬁnance.\nState space models model both state and observable vari-\nables. For example, a generalized linear state space model\ncan be expressed in the following equations:\nxt = Ztαt + ϵt (1)\nαt+1 = Ttαt + Rtηt,t = 1,...,n, (2)\nwhere xt and αt are time-indexed observation vectors and\nstate vectors, respectively. Equation 1, called observation\nequation, is a regression-like equation. It models the rela-\ntionship of observable xt and the underlying state variable\nαt. Equation 2 is the state equation, and has autoregres-\nsive nature. It governs how the state variables evolve over\ntime. ϵt and ηt are innovation components and are usually\nmodeled as Gaussian processes.\nIn this section, we brieﬂy mention a few commonly used\nSSM models in ILI forecasting.\nCompartmental Models Compartmentmental models\nare a speciﬁc form of SSMs and have been widely used\nto study infectious diseases. In a compartmental model,\na population is divided into different groups (“compart-\nments”). Each group is modeled by a time-dependent state\nvariable. An prominent example of compartmental model\nis “Suscepted-Infected-Recovered” (SIR) model, where the\nsystem is governed by three state variables (S(t), I(t), R(t))\nthrough the following ordinary differential equations:\ndS\ndt = −βIS\nN\ndI\ndt = βIS\nN − γI\ndR\ndt = −γI\nIn this treatment, ILI time series is an observable variable\nof the system: ILI(t) =I(t)/(I(t) +S(t) +R(t)).\nAlthough originally developed to model infectious diseases,\ncompartmental models have been applied to other disci-\nplines such as ecology and economics. While compart-\nmental models are useful, they require prior knowledge on\nthe parameters of the differential equations and they lack\nﬂexibility of updating parameters upon new observations.\nARIMA Box-Jenkins ARIMA (Auto-Regressive Inte-\ngrated Moving Average) is another popular approach to\nmodeling dynamical systems. ARIMA models the observed\nvariable xt and assumes xt can be decomposed into trend,\nseasonal and irregular components. Instead of modeling\nthese components separately, Box and Jenkins had the idea\nof differencing the time series xt in order to eliminate trend\nand seasonality. The resulting series is treated as station-\nary time series data and is modeled using combination of\nits lagged time series values (“AR”) and moving average\nof lagged forecast errors (“MA”). An ARIMA model is\ntypically speciﬁed by a tuple (p,d,q ), where pand q de-\nﬁne the orders of AR and MA, and dspeciﬁes the order of\ndifferencing operation.\nARIMA can be written in SSM form, and common SSM\ntechniques such as ﬁltering and smoothing can be applied\nto ARIMA as well. Nevertheless, ARIMA is a kind of\n“blackbox” approach where the model purely depends on\nthe observed data and has no analysis of the states of the\nunderlying systems (Durbin & Koopman, 2012).\nTime Delay Embedding For a scalar time series data xt,\nits time delay embedding (TDE) is formed by embedding\neach scalar value xt into a d-dimensional time-delay space:\nTDEd,τ(xt) = (xt,xt−τ,...,x t−(d−1)τ)\nFor any non-linear dynamical systems, the delay-embedding\ntheorem (Takens’ theorem) (Takens, 1981) states that there\nexists a certain (d,τ)-time delay embedding such that the\nevolution of the original state variables (“phase space”)\ncan be recovered in the delay coordinates of the observed\nvariables. In the case of ILI forecasting, Takens’ theorem\nsuggests that TDEd,τ of ILI ratios (“observed variable”) can\napproximate the underlying dynamical systems governed by\nbiological and physical mechanisms.\nTDEs were ﬁrst explored for time series forecasting in the\nseminal work by Sugihara and May (1990). They showed\nthat TDEs can be used to make short-range predictions based\non the qualitative assessment of a system’s dynamics with-\nout any knowledge on the underlying mechanisms. They\ndeveloped two TDE-based models to predict chickenpox\nand measles prevalence, and compared them with AR-based\napproaches. Their analysis suggests that TDE-based model\nperforms equally well for chickenpox case prediction and\noutperforms AR for measles case prediction.\nDeep Transformer Models for Time Series Forecasting\nx\nh\ny\nWx\nWy\nWh\nUnfold\nxt-1\nht-1\nyt-1\nWx\nWy\nxt\nht\nyt\nWx\nWy\nxt+1\nht+1\nyt+1\nWx\nWy\nWhWh Wh Wh\n... ...\nFigure 2.Folded and unfolded representations of recurrent neural\nnetwork.\nIn the SSM framework, the time delay embedding is a pow-\nerful tool to bridge state variables and observed data by\nmaking it possible to learn geometrical and topological in-\nformation of underlying dynamical systems without an un-\nderstanding of the state variables and phase space of the\nsystems. Depite the amazing property of TDEs, to the best\nof our knowledge, TDEs haven’t been studied extensively\nfor machine learning models.\n3.3. Sequence Models\nMany real-world machine learning tasks deal with different\ntypes of sequential data ranging from natural language text,\naudio, and video, to DNA sequences and time series data.\nSequence models are speciﬁcally designed to model such\ndata. In this section, we brieﬂy review a few different types\nof common sequence models.\nRecurrent Neural Networks Unlike traditional feed-\nforward networks, RNN is recurrent in nature - it performs\nthe same function to each input xt, and the output yt de-\npends on both the input xt and the previous state ht−1.\nThe simple RNN illustrated in Figure 3.3 can be expressed\nas follows:\nht = σ(Wxxt + Whht−1 + bh)\nyt = σ(Wyht + by)\nWhere xt is the input vector, ht is the hidden state vec-\ntor, and yt is the output vector. W’s and b’s are learned\nparameters, and σis the activation function.\nLSTM While RNN has internal memory to process se-\nquence data, it suffers from gradient vanishing and explod-\ning problems when processing long sequences. Long Short-\nTerm Memory (LSTM) networks were speciﬁcally devel-\noped to address this limitation (Hochreiter & Schmidhuber,\n1997). LSTM employs three gates, including an input gate,\nforget gate and output gate, to modulate the information\nﬂow across the cells and prevent gradient vanishing and\nexplosion.\nxt-1\nCt-1,ht-1\nyt-1\nxt\nyt\nCt+1,ht+1\nxt+1\nyt+1\nLSTM cell\nσ σ tanh σ\ntanh\nCt-1\nht-1\nxt\nht\nCt\nft it\not\nht\n... ...\nFigure 3.Long Short-Term Memory network and LSTM unit.\nft = σ(Wf[ht−1,xt] +bf)\nit = σ(Wi[ht−1,xt] +bi)\n˜Ct = tanh(WC[ht−1,xt] +bC)\nCt = ft ∗ Ct−1 + it ∗ ˜Ct\nyt = σ(Wy[ht−1,xt] +by)\nht = yt ∗ tanh(Ct)\nSeq2Seq Sequence-to-sequence (Seq2Seq) architecture\nis developed for machine learning tasks where both input\nand output are sequences. A Seq2Seq model is comprised\nof three components including an encoder, an intermediate\nvector, and a decoder. Encoder is a stack of LSTM or other\nrecurrent units. Each unit accepts a single element from\nthe input sequence. The ﬁnal hidden state of the encoder is\ncalled the encoder vector or context vector, which encodes\nall of the information from the input data. The decoder is\nalso made of a stack of recurrent units and takes the encoder\nvector as its ﬁrst hidden state. Each recurrent unit computes\nits own hidden state and produces an output element. Figure\n3.3 illustrates the Seq2Seq architecture.\nSeq2Seq has been widely applied in language translation\ntasks. However, its performance degrades with long sen-\ntences because it cannot adequately encode a long sequence\ninto the intermediate vector (even with LSTM cells). There-\nfore, long-term dependencies tend to be dropped in the\nencoder vector.\n4. Model\n4.1. Problem Description\nWe formuate ILI forecasting as a supervised ma-\nchine learning task. Given a time series contain-\ning N weekly data points xt−N+1,...,x t−1,xt, for M-\nstep ahead prediction, the input X of the supervised\nML model is xt−N+1,...,x t−M, and the output Y is\nxt−M+1,xt−M+2,...,x t. Each data point xt can be a scalar\nor a vector containing multiple features.\nDeep Transformer Models for Time Series Forecasting\nx1\nLSTM\nx2\nLSTM\nx3\nLSTMh2 h3h1\nEncoder Vector\ny1 y2\nLSTM LSTM\nFigure 4.Sequence-to-sequence (Seq2Seq) architecture to model\nsequence input and output.\nTime\nFix-length time window\nFigure 5.Using sliding window to construct supervised learning\nexamples from time series data.\n4.2. Data\nWe utilized country- and state-level historical ILI data from\n2010 to 2018 from the CDC (CDC).\nTo produce a labeled dataset, we used a ﬁxed-length sliding\ntime window approach (Figure 5) to construct X,Y pairs\nfor model training and evaluation. Before applying the slid-\ning window to get features and labels, we perform min-max\nscaling on all the data with the maximum and minimum val-\nues of training dataset. We then run a sliding window on the\nscaled training set to get training samples with features and\nlabels, which are the previous N and next M observations\nrespectively. Test samples are also constructed in the same\nmanner for model evaluation. The train and test split ratio\nis 2:1. Training data from different states are concatenated\nto form the training set for the global model.\n4.3. Transformer Model\n4.3.1. M ODEL ARCHITECTURE\nOur Transformer-based ILI forecasting model follows the\noriginal Transformer architecture (Vaswani et al., 2017)\nconsisting of encoder and decoder layers.\nEncoder The encoder is composed of an input layer, a po-\nsitional encoding layer, and a stack of four identical encoder\nlayers. The input layer maps the input time series data to a\nvector of dimension dmodel through a fully-connected net-\nwork. This step is essential for the model to employ a multi-\nhead attention mechanism. Positional encoding with sine\nand cosine functions is used to encode sequential informa-\ntion in the time series data by element-wise addition of the\ninput vector with a positional encoding vector. The resulting\nvector is fed into four encoder layers. Each encoder layer\nconsists of two sub-layers: a self-attention sub-layer and a\nfully-connected feed-forward sub-layer. Each sub-layer is\nfollowed by a normalization layer. The encoder produces a\ndmodel-dimensional vector to feed to the decoder.\nDecoder We employ a decoder design that is similar to\nthe original Transformer architecture (Vaswani et al., 2017).\nThe decoder is also composed of the input layer, four identi-\ncal decoder layers, and an output layer. The decoder input\nbegins with the last data point of the encoder input. The\ninput layer maps the decoder input to a dmodel-dimensional\nvector. In addition to the two sub-layers in each encoder\nlayer, the decoder inserts a third sub-layer to apply self-\nattention mechanisms over the encoder output. Finally, there\nis an output layer that maps the output of last decoder layer\nto the target time sequence. We employ look-ahead masking\nand one-position offset between the decoder input and tar-\nget output in the decoder to ensure that prediction of a time\nseries data point will only depend on previous data points.\n4.3.2. T RAINING\nTraining Data and Batching In a typical training setup,\nwe train the model to predict 4 future weekly ILI ratios from\n10 trailing weekly datapoints. That is, given the encoder\ninput (x1,x2,...,x 10) and the decoder input (x10,...,x 13),\nthe decoder aims to output (x11,...,x 14). A look-ahead\nmask is applied to ensure that attention will only be applied\nto datapoints prior to target data by the model. That is,\nwhen predicting target (x11,x12), the mask ensures atten-\ntion weights are only on (x10,x11) so the decoder doesn’t\nleak information about x12 and x13 from the decoder input.\nA minibatch of size 64 is used for training.\nOptimizer We used the Adam optimizer (Kingma & Ba,\n2015) with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. A custom\nlearning rate with following schedule is used:\nlrate=d0.5\nmodel ∗ min(step num0.5,\nstep num∗ warmup steps−1.5)\nWhere warmup steps= 5000.\nRegularization We apply dropout techniques for each of\nthe three types of sub-layers in the encoder and decoder: the\nDeep Transformer Models for Time Series Forecasting\nself-attention sub-layer, the feed-forward sub-layer, and the\nnormalization sub-layer. A dropout rate of 0.2 is used for\neach sub-layer.\n4.3.3. E VALUATION\nIn evaluation, labeled test data are constructed using a ﬁx-\nlength sliding window as well. One-step ahead prediction is\nperformed by the trained Transformer model. We computed\nPearson correlation coefﬁcient and root-mean-square errors\n(RMSE) between the actual data yi and the predicted value\nˆyi.\n4.4. ARIMA, LSTM and Seq2Seq Models\nThis section describes other models we developed to bench-\nmark Transformer-based model.\nARIMA A univariate ARIMA model is used as a baseline.\nIt treats the time dependent ILI ratios as a univariate time\nseries that follows a ﬁxed dynamic. Each week’s ILI ratio is\ndependent on previouspweeks’ observations and previousq\nweeks’ estimation errors. We selected the order of ARIMA\nmodel using AIC and BIC to balance model complexity and\ngeneralization. We used ARIMA(3,0,3) and a constant\ntrend to keep the model parsimonious. The model is formu-\nlated in the State Space modeling framework and trained\nwith the ﬁrst two-thirds of the dataset. The ﬁtted parameters\nare then used on the full time series to ﬁlter hidden states\nand make four-step ahead predictions.\nLSTM The LSTM model has a stack of two LSTM layers\nand a ﬁnal dense layer to predict the multiple step ILI ratios\ndirectly. The LSTM layers encode sequential information\nfrom input through the recurrent network. The dense con-\nnected layer takes ﬁnal output from the second LSTM layer\nand outputs a vector of size 4, which is equal to the number\nof steps ahead predictions. The two LSTM layers are 32 and\n16 units respectively. A dropout rate 0.2 is applied to LSTM\nlayers for regularization. Huber loss, Adam optimizer, and\na learning rate of 0.02 are used for training.\nSeq2Seq The tested Seq2Seq model has an encoder-\ndecoder architecture, where the encoder is composed of\na fully connected dense layer and a GRU layer to learn from\nthe sequential input and to return a sequence of encoded\noutputs and a ﬁnal hidden state. The decoder is of the same\nstructure as input. The dense layer is of 16 units and the\nGRU layer is of 32 units. An attention mechanism is also\nadopted in this Seq2Seq model. Speciﬁcally, Bahdanau at-\ntention (Bahdanau et al., 2015) is applied on the sequence\nof encoder outputs at each decoding step to make next step\nprediction. Teacher forcing (Williams & Zipser, 1989) is\nutilized in decoder for faster convergence and to address in-\nstability. During training, the true ILI rate is used at current\ntime step as input for the next time step, instead of using\noutput computed from the decoder unit. A dropout rate 0.2\nis applied in all recurrent layers. We used Huber loss, Adam\noptimizer, and a learning rate of 0.02 for training.\n5. Experiment\n5.1. One-step-ahead Forecasting Using ILI Data Alone\nIn our ﬁrst experiment, we tested whether our Transformer-\nbased model could predict the ILI ratio one-week ahead\nfrom 10 weeks of historical datapoints. For evaluation, the\ntrained global model performs one-step ahead prediction for\neach state using the testing data set. Pearson correlation and\nroot-mean-square error (RMSE) values were calculated for\neach state.\nWe compared the Transformer’s performance with ARIMA,\nLSTM, and Seq2Seq with attention models. Table 1 sum-\nmarizes the correlation coefﬁcients and RMSEs for each\nmethod, as well as relative performance gain with respect\nto ARIMA method. The comparison suggests that deep\nlearning models overall outperform ARIMA for both cor-\nrelation and RMSE. Within the three deep learning ap-\nproaches, the correlation coefﬁcients are very similar with\nthe Transformer-based model being slightly higher than\nLSTM and Seq2Seq with attention models. In terms of\nRMSE, the Transformer model outperforms both LSTM\nand Seq2Seq with attention models, with relative RMSE\ndecrease of 27 % and 8.4 %, respectively. This analysis\nsuggests that attention mechanisms contribute to forecasting\nperformance, as Seq2Seq with attention and Transformer\nmodels outperform the plain LSTM model. Additionally,\nthe Transformer shows better forecasting performance com-\npared to Seq2Seq with attention model, suggesting that\nTransformer’s self-attention mechanism can better capture\ncomplex dynamical patterns in the data compared to the\nlinear attention mechanism used in Seq2Seq. Interestingly,\nit’s worth noting that Transformer exhibits the best metrics\nfor US-level ILI forecasting (Pearson correlation = 0.984\nand RMSE = 0.3318). Since a single model is trained using\ndata from all the states, it suggests that the model indeed\ncan generalize various state-level patterns for country-level\nprediction.\n5.2. One-step-ahead Forecasting Using Feature Vectors\nWe next tested whether our Transformer-based model can\nlearn from multiple features (i.e., multivariate time series\ndata) for ILI forecasting. In the United States, the ﬂu season\nusually starts in early October and peaks between January\nand Februray. We hypothesized that week number is an\ninformative signal for the model. Therefore we introduced\n”week number” as a time-indexed feature to the model. Ad-\nditionally, we included the ﬁrst and second order differences\nDeep Transformer Models for Time Series Forecasting\nFigure 6.Pearson Correlation of ARGONet and transformer models.\nFigure 7.RMSE of ARGONet and transformer models.\nDeep Transformer Models for Time Series Forecasting\nTable 1.Summary of model performances with relative change\nwith respect to baseline model.\nModel Pearson Correlation RMSE\nARIMA 0.769\n(+0 %)\n1.020\n(-0 %)\nLSTM 0.924\n(+19.9 %)\n0.807\n(-20.9 %)\nSeq2Seq+attn 0.920\n(+19.5 %)\n0.642\n(-37.1 %)\nTransformer 0.928\n(+20.7 %)\n0.588\n(-42.4 %)\nof the time series as two explicit numerical features in the\nmodel.\nOur results suggest that including these features improves\nmodel performance (mean Pearson correlation: 0.931, mean\nRMSE = 0.585). However, the improvement is not signif-\nicant compared to the Transformer model using ILI data\nalone. This suggests that the additional features are likely\nto encode little new information to the model. That is, the\nintroduced ﬁrst and second order difference features are\nlikely to be redundant if the Transformer-based model is\nable to rely on the self-attention mechanism to learn short\nand long-range dependencies from the ILI time series.\nWe compared our results with the ILI forecasting data by\nARGONet (Lu et al., 2019), a state-of-the-art ILI forecast-\ning model in the literature. Figure 6 and ﬁgure 7 show\nthe correlation and RMSE values of ARGONet and our\ntransformer results. Overall, the Transformer-based model\nperforms equally with ARGONet, with the mean correlation\nslightly improved (ARGONet: 0.912, Transformer: 0.931),\nand mean RMSE value slightly degraded (ARGONet: 0.550,\nTransformer: 0.593).\n5.3. Forecasting Using Time Delay Embedding\nIn this section, we tested whether the Transformer-based\nmodel can directly model phase space of a dynamical sys-\ntem. To that end, we constructed time delay embeddings\n(TDEs) from historical ILI data, since TDEs (with sufﬁcient\ndimensionality) are topologically equivalent to the unknown\nphase space of dynamical systems. In other words, com-\npared to ILI data, which are observed scalar variables, TDEs\nencode additional geometrical and topological information\nof the systems that governs the process of inﬂuenza infec-\ntion and spreading. Therefore, using TDEs should provide\nricher information compared to scalar time series input.\nTo verify this hypothesis, we constructed time delay em-\nbeddings of dimension 2 to 32 from ILI data and applied\ntransformer-based ILI forecasting using TDEs as features.\nTable 2 summarizes the forecasting metrics with different\nTDE dimensions d. In all the experiments, we use τ = 1to\nconstruct TDEs. Varying TDE dimensionality does not sig-\nniﬁcantly alter Pearson correlation coefﬁcients. The RMSE\nvalue reached minimum with dimensionality of 8. This\nvalue is close to the optimal TDE dimensionality of 5 and\n5-7 for forecasting chickenpox and measles (Sugihara &\nMay, 1990).\nTable 2.Performance of Time Delay Embeddings\nDimension Pearson Correlation RMSE\n2 0.926 0.745\n4 0.929 0.778\n6 0.927 0.618\n8 0.926 0.605\n16 0.925 0.623\n32 0.925 0.804\n6. Conclusions\nIn this work, we presented a Transformer-based approach to\nforecasting time series data. Compared to other sequence-\naligned deep learning methods, our approach leverages self-\nattention mechanisms to model sequence data, and therefore\nit can learn complex dependencies of various lengths from\ntime series data.\nMoreover, this Transformer-based approach is a generic\nframework for modeling various non-linear dynamical sys-\ntems. As manifested in the ILI case, this approach can\nmodel observed time series data as well as phase space of\nstate variables through time delay embeddings. It is also\nextensible and can be adpated to model both univariate and\nmultivariate time series data with minimum modiﬁcations\nto model implementations.\nFinally, although the current case study focuses on time\nseries data, we hypothesize that our approach can be further\nextended to model spatio-temporal data indexed by both\ntime and location coordinates. Self-attention mechanisms\ncan be generalized to learn relations between two arbitrary\npoints in spatio-temporal space. This is a direction we plan\nto explore in the future.\nReferences\nCdc ﬂuview dashboard. https://gis.cdc.gov/\ngrasp/fluview/fluportaldashboard.html.\nAraz, O. M., Bentley, D., and Muelleman, R. L. Using\ngoogle ﬂu trends data in forecasting inﬂuenza-like-illness\nrelated ed visits in omaha, nebraska. The American Jour-\nDeep Transformer Models for Time Series Forecasting\nnal of Emergency Medicine, 32(9):1016–1023, Sep 2014.\nISSN 0735-6757.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. In\n3rd International Conference on Learning Representa-\ntions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings, 2015.\nDurbin, J. and Koopman, S. J. Time Series Analysis by State\nSpace Methods: Second Edition. Oxford University Press,\n2nd edition, 2012.\nGinsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L.,\nSmolinski, M. S., and Brilliant, L. Detecting inﬂuenza\nepidemics using search engine query data. Nature, 457\n(7232):1012–1014, 2009. ISSN 1476-4687.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nIuliano, A. D., Roguski, K. M., Chang, H. H., Muscatello,\nD. J., Palekar, R., Tempia, S., Cohen, C., Gran, J. M.,\nSchanzer, D., Cowling, B. J., Wu, P., Kyncl, J., Ang,\nL. W., Park, M., Redlberger-Fritz, M., Yu, H., Espenhain,\nL., Krishnan, A., Emukule, G., van Asten, L., Pereira da\nSilva, S., Aungkulanon, S., Buchholz, U., Widdowson,\nM.-A., Bresee, J. S., and Network, G. S. I.-a. M. C. Esti-\nmates of global seasonal inﬂuenza-associated respiratory\nmortality: a modelling study. Lancet (London, England),\n391(10127):1285–1300, Mar 2018. ISSN 1474-547X.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In 3rd International Conference on Learn-\ning Representations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings, 2015.\nKondo, K., Ishikawa, A., and Kimura, M. Sequence to\nsequence with attention for inﬂuenza prevalence predic-\ntion using google trends. In Proceedings of the 2019\n3rd International Conference on Computational Biology\nand Bioinformatics, ICCBB 19, pp. 17, New York, NY ,\nUSA, 2019. Association for Computing Machinery. ISBN\n9781450376815.\nLazer, D., Kennedy, R., King, G., and Vespignani, A. The\nparable of google ﬂu: Traps in big data analysis. Science,\n343(6176):1203–1205, 2014. ISSN 0036-8075.\nLiu, L., Han, M., Zhou, Y ., and Wang, Y . Lstm recurrent\nneural networks for inﬂuenza trends prediction. In Zhang,\nF., Cai, Z., Skums, P., and Zhang, S. (eds.), Bioinfor-\nmatics Research and Applications, pp. 259–264, Cham,\n2018. Springer International Publishing. ISBN 978-3-\n319-94968-0.\nLu, F. S., Hattab, M. W., Clemente, C. L., Biggerstaff, M.,\nand Santillana, M. Improved state-level inﬂuenza now-\ncasting in the united states leveraging internet-based data\nand network approaches. Nature Communications, 10(1):\n147, 2019. ISSN 2041-1723.\nMcIver, D. J. and Brownstein, J. S. Wikipedia usage esti-\nmates prevalence of inﬂuenza-like illness in the united\nstates in near real-time. PLOS Computational Biology,\n10(4):1–8, 04 2014.\nOlson, D. R., Konty, K. J., Paladini, M., Viboud, C., and\nSimonsen, L. Reassessing google ﬂu trends data for detec-\ntion of seasonal and pandemic inﬂuenza: a comparative\nepidemiological study at three geographic scales. PLoS\ncomputational biology, 9(10):e1003256–e1003256, 2013.\nISSN 1553-7358.\nPaul, M. J., Dredze, M., and Broniatowski, D. Twit-\nter improves inﬂuenza forecasting. PLoS currents, 6:\necurrents.outbreaks.90b9ed0f59bae4ccaa683a39865d9117,\nOct 2014. ISSN 2157-3999.\nPutri, W. C., Muscatello, D. J., Stockwell, M. S., and Newall,\nA. T. Economic burden of seasonal inﬂuenza in the united\nstates. Vaccine, 36(27):3960 – 3966, 2018.\nSantillana, M., Zhang, D. W., Althouse, B. M., and Ayers,\nJ. W. What can digital disease detection learn from (an\nexternal revision to) google ﬂu trends? American Journal\nof Preventive Medicine, 47(3):341–347, Sep 2014. ISSN\n0749-3797.\nSugihara, G. and May, R. M. Nonlinear forecasting as a\nway of distinguishing chaos from measurement error in\ntime series. Nature, 344(6268):734–741, 1990. ISSN\n1476-4687.\nTakens, F. Detecting strange attractors in turbulence. In\nRand, D. and Young, L.-S. (eds.), Dynamical Systems\nand Turbulence, Warwick 1980, pp. 366–381, Berlin, Hei-\ndelberg, 1981. Springer Berlin Heidelberg.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\ntion is all you need. In Guyon, I., Luxburg, U. V ., Ben-\ngio, S., Wallach, H., Fergus, R., Vishwanathan, S., and\nGarnett, R. (eds.), Advances in Neural Information Pro-\ncessing Systems 30, pp. 5998–6008. Curran Associates,\nInc., 2017.\nVenna, S. R., Tavanaei, A., Gottumukkala, R. N., Raghavan,\nV . V ., Maida, A. S., and Nichols, S. A novel data-driven\nmodel for real-time inﬂuenza forecasting. IEEE Access,\n7:7691–7701, 2019. ISSN 2169-3536.\nWilliams, R. J. and Zipser, D. A learning algorithm for con-\ntinually running fully recurrent neural networks. Neural\nComputation, 1(2):270–280, 1989.\nDeep Transformer Models for Time Series Forecasting\nYang, S., Santillana, M., and Kou, S. C. Accurate estimation\nof inﬂuenza epidemics using google search data via argo.\nProceedings of the National Academy of Sciences, 112\n(47):14473–14478, 2015. ISSN 0027-8424.\nZhu, X., Fu, B., Yang, Y ., Ma, Y ., Hao, J., Chen, S., Liu,\nS., Li, T., Liu, S., Guo, W., and Liao, Z. Attention-\nbased recurrent neural network for inﬂuenza epidemic\nprediction. BMC Bioinformatics, 20(18):575, 2019. ISSN\n1471-2105.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5533426403999329
    },
    {
      "name": "Time series",
      "score": 0.47055983543395996
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.4546722173690796
    },
    {
      "name": "Econometrics",
      "score": 0.4426339268684387
    },
    {
      "name": "Computer science",
      "score": 0.3686673641204834
    },
    {
      "name": "Economics",
      "score": 0.24143752455711365
    },
    {
      "name": "Engineering",
      "score": 0.2344670593738556
    },
    {
      "name": "Machine learning",
      "score": 0.19924971461296082
    },
    {
      "name": "Electrical engineering",
      "score": 0.10692450404167175
    },
    {
      "name": "Geology",
      "score": 0.10024383664131165
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}