{
    "title": "Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?",
    "url": "https://openalex.org/W4388333192",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2953672958",
            "name": "Adaku Uchendu",
            "affiliations": [
                "Pennsylvania State University",
                "Lincoln University - Pennsylvania",
                "MIT Lincoln Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2099471407",
            "name": "Joo-Young Lee",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2115210622",
            "name": "Hua Shen",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2096317137",
            "name": "Thai Le",
            "affiliations": [
                "University of Mississippi"
            ]
        },
        {
            "id": "https://openalex.org/A4226032672",
            "name": "Ting-Hao `Kenneth' Huang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2115417150",
            "name": "Dongwon Lee",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2953672958",
            "name": "Adaku Uchendu",
            "affiliations": [
                "MIT Lincoln Laboratory",
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2099471407",
            "name": "Joo-Young Lee",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2115210622",
            "name": "Hua Shen",
            "affiliations": [
                "University of Michigan–Ann Arbor",
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2096317137",
            "name": "Thai Le",
            "affiliations": [
                "University of Mississippi"
            ]
        },
        {
            "id": "https://openalex.org/A4226032672",
            "name": "Ting-Hao `Kenneth' Huang",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A2115417150",
            "name": "Dongwon Lee",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6843585312",
        "https://openalex.org/W4225854714",
        "https://openalex.org/W3174007554",
        "https://openalex.org/W6777273528",
        "https://openalex.org/W4285251400",
        "https://openalex.org/W3092582584",
        "https://openalex.org/W6781462598",
        "https://openalex.org/W3139815689",
        "https://openalex.org/W2949028947",
        "https://openalex.org/W2888298348",
        "https://openalex.org/W3205059772",
        "https://openalex.org/W3021153741",
        "https://openalex.org/W4226353726",
        "https://openalex.org/W4281493346",
        "https://openalex.org/W3201369838",
        "https://openalex.org/W1981815150",
        "https://openalex.org/W2104256533",
        "https://openalex.org/W4306886656",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2170177456",
        "https://openalex.org/W3085136337",
        "https://openalex.org/W4307073343",
        "https://openalex.org/W3204318298",
        "https://openalex.org/W2992347006",
        "https://openalex.org/W4321161602",
        "https://openalex.org/W6763240421",
        "https://openalex.org/W6805149107",
        "https://openalex.org/W3093061433",
        "https://openalex.org/W4313447172",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W4378474292",
        "https://openalex.org/W4297412068",
        "https://openalex.org/W4385187297",
        "https://openalex.org/W3034287667",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4288057743",
        "https://openalex.org/W3104764318",
        "https://openalex.org/W4285272223",
        "https://openalex.org/W3024131638",
        "https://openalex.org/W3046357466",
        "https://openalex.org/W3174519801",
        "https://openalex.org/W4205695807",
        "https://openalex.org/W3212176791",
        "https://openalex.org/W3170816811",
        "https://openalex.org/W4309617712",
        "https://openalex.org/W4318351452",
        "https://openalex.org/W4226193471",
        "https://openalex.org/W4320458011",
        "https://openalex.org/W3100262863",
        "https://openalex.org/W4288334893",
        "https://openalex.org/W4312091313",
        "https://openalex.org/W3102409145",
        "https://openalex.org/W4383199647",
        "https://openalex.org/W2951080837",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4324312860"
    ],
    "abstract": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans’ ability to detect deepfake texts, none has examined whether “collaboration” among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals’ detection accuracies. We further analyze the explanations that humans used for detecting a piece of text as deepfake text, and find that the strongest indicator of deepfake texts is their lack of coherence and consistency. Our study provides useful insights for future tools and framework designs to facilitate the collaborative human detection of deepfake texts. The experiment datasets and AMT implementations are available at: https://github.com/huashen218/llm-deepfake-human-study.git",
    "full_text": "Does Human Collaboration Enhance the Accuracy of\nIdentifying LLM-Generated Deepfake Texts?\nAdaku Uchendu *1,2, Jooyoung Lee∗2, Hua Shen∗2,4, Thai Le3,\nTing-Hao ‘Kenneth’ Huang2, Dongwon Lee2\n1 MIT Lincoln Laboratory, USA\n2 The Pennsylvania State University, USA\n3 The University of Mississippi, USA\n4 University of Michigan, USA\nadaku.uchendu@ll.mit.edu,\n{jfl5838, huashen218, txh710, dongwon}@psu.edu,\nthaile@olemiss.edu\nAbstract\nAdvances in Large Language Models (e.g., GPT-4, LLaMA)\nhave improved the generation of coherent sentences resem-\nbling human writing on a large scale, resulting in the creation\nof so-called deepfake texts. However, this progress poses\nsecurity and privacy concerns, necessitating effective solu-\ntions for distinguishing deepfake texts from human-written\nones. Although prior works studied humans’ ability to detect\ndeepfake texts, none has examined whether “collaboration”\namong humans improves the detection of deepfake texts. In\nthis study, to address this gap of understanding on deepfake\ntexts, we conducted experiments with two groups: (1) non-\nexpert individuals from the AMT platform and (2) writing\nexperts from the Upwork platform. The results demonstrate\nthat collaboration among humans can potentially improve the\ndetection of deepfake texts for both groups, increasing detec-\ntion accuracies by 6.36% for non-experts and 12.76% for ex-\nperts, respectively, compared to individuals’ detection accu-\nracies. We further analyze the explanations that humans used\nfor detecting a piece of text as deepfake text, and find that the\nstrongest indicator of deepfake texts is their lack of coher-\nence and consistency. Our study provides useful insights for\nfuture tools and framework designs to facilitate the collab-\norative human detection of deepfake texts. The experiment\ndatasets and AMT implementations are available at: https:\n//github.com/huashen218/llm-deepfake-human-study.git\nIntroduction\nIn recent years, significant advancements in AI technolo-\ngies have revolutionized the generation of high-quality arti-\nfacts across various modalities, including texts, images, and\nvideos (Fagni et al. 2021; Zhang 2022; Pu et al. 2023; Shen\nand Wu 2023). These AI-generated artifacts, commonly re-\nferred to as Deepfakes, have garnered considerable atten-\ntion. Specifically, the progress made in Natural Language\nGeneration (NLG) techniques, leveraging Large Language\nModels (LLMs) like GPT-4 (OpenAI 2023) or T5 (Raf-\nfel et al. 2020), has facilitated the production of long and\ncoherent machine-generated texts without human interven-\ntion (Wu et al. 2023). For the purpose of this study, we\n*Equal contribution.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nP\nCollaboration\nIndividuals\nvs\nWhich paragraph is  \nDeepFake?\nExplanations of \nDeepFake Detection?\nTitle\nParagraph3\nA\nB\nC\nParagraph3\nParagraph3\nP\nCollaboration\nIndividuals\nvs\nWhich paragraph is  \nDeepfake?\nExplanations of  \nDeepfake Detection\nTitle\nParagraph3\nA\nB\nC\nParagraph2\nParagraph1\nWhy?\nA Multi-Authored Article\nFigure 1: An overview of human studies on detecting\ndeepfake texts. (A) A multi-authored article with 3 para-\ngraphs, including both human-written & LLM-generated\nparagraphs; (B) We conduct human studies to ask either in-\ndividuals or collaborative humans to detect deepfake texts;\n(C) In-depth analysis of the categorical explanations for\ndeepfake text detection from both groups.\ndesignate such neural or LLM-generated texts as deepfake\ntexts, while the generative language models themselves are\nreferred to as Neural Text Generators (NTG) (Zhong et al.\n2020). While NTGs offer numerous benefits, it is essential\nto acknowledge the potential misuse associated with this\ntechnological advancement (Shevlane et al. 2023). For in-\nstance, NTGs can be employed by students to complete their\nessay assignments, leading to potential plagiarism due to\nNTGs’ memorization of training samples (Lee et al. 2023).\nMoreover, scammers may exploit NTGs to craft sophisti-\ncated phishing messages, or stereotyping, misrepresenting,\nand demeaning content (Weidinger et al. 2021), while mali-\ncious code generation (Chen et al. 2021) and disinformation\nattacks by state-backed operators are also plausible scenar-\nProceedings of the Eleventh AAAI Conference on Human Computation and Crowdsourcing (HCOMP2023)\n163\nios (Bagdasaryan and Shmatikov 2022). Given these con-\ncerns, it becomes imperative to prioritize research efforts to-\nwards developing effective methodologies for distinguishing\ndeepfake texts from those authored by humans.\nBoth computational and non-computational approaches\nfor detecting deepfake texts have received significant at-\ntention in recent years (Uchendu et al. 2021; Clark et al.\n2021; Dou et al. 2022; Brown et al. 2020), and have been\ncomprehensively surveyed by Uchendu, Le, and Lee (2023).\nHowever, emerging literature (Uchendu et al. 2021; Dou\net al. 2022) suggests that humans, on average, struggle to\ndetect deepfake texts, performing only slightly better than\nrandom guessing. Even with training, the performance of\nhumans in deepfake text detection has shown limited im-\nprovement (Clark et al. 2021; Dou et al. 2022; Tan, Plum-\nmer, and Saenko 2020). These findings highlight the need to\nexplore alternative strategies, such as collaborative detection\nor leveraging advanced technological solutions, to address\nthe challenges posed by deepfake texts effectively.\nOnline fact-checking efforts, as highlighted by Juneja and\nMitra (2022), can be achieved collaboratively to detect on-\nline misinformation. Previous research has demonstrated\nthat collective intelligence, often referred to as the “wisdom\nof the crowd”, can surpass individual sensemaking capa-\nbilities (Surowiecki 2005). Similarly, aggregating multiple\nhuman labels has also been shown to yield higher-quality\nresults (Zheng et al. 2017). However, limited attention has\nbeen given to understanding how collaboration affects the\nperformance of deepfake detection. Consequently, the pri-\nmary objective of this study is to investigate the impact of\nhuman collaboration on the detection of deepfake texts.\nSee an overview of the task presented in Figure 1, wherein\nwe generate a three-paragraph article authored by both hu-\nmans and LLM. Individuals or collaborative human groups\nare then tasked with identifying the paragraph that has been\ngenerated by LLMs. Furthermore, we delve into the detailed\nexplanations provided by humans to detect the deepfakes. It\nis worth noting that this deepfake detection design bears re-\nsemblance to the Turing Test.1 As a result, our study focuses\non addressing the following research questions:\n• RQ1: Do collaborative teams or groups outperform indi-\nviduals in deepfake text detection task?\n• RQ2: What types of reasoning explanations are useful\nindicators for deepfake text detection?\nTo conduct comprehensive human studies on evaluating\nthe effectiveness of human collaboration in deepfake text\ndetection (i.e., RQ1), we focus on two distinct stakeholder\ngroups of online workers: Amazon Mechanical Turk (AMT)\nworkers as English non-experts and Upwork workers as En-\nglish experts. The term “English experts” refers to individ-\nuals who possess at least a Bachelor’s degree in English or\na related field (Please see the Methodology section for de-\ntailed filtering criteria for identifying experts). These two\ngroups also represent the conventional micro-task crowd-\n1Turing Test measures how human-like a model is. If a model\nshows intelligent behavior usually attributed to a human and is thus,\nlabeled a human, the model is said to have passed the Turing Test.\nsourcing setting and the freelance marketplace setting, re-\nspectively. The next challenge is to facilitate human collab-\noration on these two platforms. For AMT workers, we have\ndevised an asynchronous collaboration approach, while for\nUpwork workers, a synchronous collaboration method has\nbeen implemented (please refer to the Methodology sec-\ntion for more information on the implementation details).\nFurthermore, during the study, we request both groups to\nprovide their explanations for detecting deepfake texts (i.e.,\nRQ2). They are given a predefined set of seven explanation\ntypes to choose from or the option to supplement their own\nexplanations. By collecting these explanations, we aim to\ndelve deeper into the reasoning process behind human col-\nlaborative deepfake text detection.\nThrough the execution of two human studies and a com-\nparative analysis of human collaborative and individual eval-\nuations within both the expert and non-expert groups, our re-\nsearch reveals that human collaboration has the potential\nto enhance the performance of deepfake text detection\nfor both stakeholder groups. The key findings of our study\ncan be summarized as follows:\n• Human collaboration leads to a 6.36% improvement in\ndeepfake text detection among non-experts and a 12.76%\nimprovement among experts;\n• The detection of deepfake texts is influenced by in-\ndicators such as “consistency”, “coherency”, “common\nsense”, and “self-contradiction” issues;\n• Experts outperform non-experts in both individual and\ncollaborative scenarios when it comes to detecting deep-\nfake texts.\nOverall, this work focuses on investigating the impact of\nhuman collaboration on the detection of deepfake texts and\ndemonstrates that collaborative efforts within representative\ngroups yield superior results compared to individuals. The\nstudy sheds light on the underlying reasoning explanations,\nhighlights limitations, and emphasizes the need for the de-\nvelopment of computational and non-computational (includ-\ning hybrid) tools to promote more robust and accurate detec-\ntion methods.\nRelated Work\nEvaluating Deepfake Texts with Laypeople\nThe quality of deepfake texts has always been compared\nto human-written texts. Thus, since humans still remain\nthe gold standard when evaluating machine-generated texts,\nseveral works have investigated human performance in dis-\ntinguishing between human-written and machine-generated\ntexts. GROVER (Zellers et al. 2019), an NTG trained to\ngenerate news articles can easily be used maliciously. To\nevaluate the quality of GROVER-generated news (fake) ar-\nticles, they are compared to human-written news articles.\nHumans are asked to pick which articles are more believ-\nable and GROVER-generated fake news was found to be\nmore trustworthy (Zellers et al. 2019). Donahue, Lee, and\nLiang (2020) recruits human participants from Amazon Me-\nchanical Turk (AMT) to detect machine-generated words in\n164\nP\nP\n!\"#$!%&%'&%()\"*\n!+#$!%&%'&%()+*\n!,#$!%&%'&%(),*\n-./01\nP!&12(&34155#\nP\n6%7839#:171&%/3&;\nP\nReplacement Algorithm\n<#=>#P1#.5#51014/18#>3&#&1(0%41917/;#\n<#?51#Title#%5#(&39(/#/3#'171&%/1#@11(A%B1#\n<=>#P2#.5#51014/18#>3&#&1(0%41917/;#\n<#?51#P1#%5#(&39(/#/3#'171&%/1#@11(A%B1#\n<=>#P3#.5#51014/18#>3&#&1(0%41917/;#\n<#?51#P2#%5#(&39(/#/3#'171&%/1#@11(A%B1\n:171&%/.C1#DDE5\nP\n!\"#$!%&%'&%()\"*\n!+#$!%&%'&%()+*\n!,#$!%&%'&%(),*\n-./01\nP\nHuman-written ArticlesP\n P\n+ Human & DeepFake \nWritten Articles\nCollected Human- \nwritten Articles 6%78390F#51014/#%##\n(%&%'&%()#/3#&1(0%41##\nG./)#@11(A%B1\nFigure 2: Illustration of the data generation process.\na sentence. Uchendu et al. (2021) also recruits human par-\nticipants from AMT and asks them to detect which one of\ntwo articles is machine-generated and given one article, de-\ncide if it is machine-generated or not. Ippolito et al. (2020)\nevaluates the human ability to perform comparably given 2\ndifferent generation strategies. Brown et al. (2020) evaluates\nhuman performance in distinguishing human-written texts\nfrom GPT-3-generated texts. Finally, in all these works, the\nthemes remain the same - humans perform poorly at de-\ntecting machine-generated texts, achieving about or below\nchance-level during evaluation.\nTraining Humans to Evaluate Deepfake Texts\nSince human performance in deepfake text detection is very\npoor, a line of studies have attempted to train the humans\nfirst and then ask them to detect the deepfake texts. For\nexample, (Gehrmann, Strobelt, and Rush 2019) proposed a\ncolor-coded tool named GLTR (Giant Language Model Test\nRoom). GLTR color codes words based on the distribution\nlevel which improves human performance from 54% to 72%\n(Gehrmann, Strobelt, and Rush 2019). Dugan et al. (2020)\ngamifies machine-generated text detection by training hu-\nmans to detect the boundary at which a document becomes\ndeepfake to earn points. Humans are given the option to se-\nlect one of many reasons or include their own reasons for\nwhich a sentence could be machine-generated (Dou et al.\n2022). Our framework is modeled more closely after Dugan\net al. (2020)’s work. Next, Clark et al. (2021) proposes 3\ntraining techniques - Instruction-based, Example-based, and\nComparison-based. Example-based training improved the\naccuracy from 50% to 55% (Clark et al. 2021).\nDespite persistent efforts in human training, all methods\nexcept for GLTR did not yield significant improvements in\nhuman performance. However, GLTR achieved an average\nof 56% F1 score on 19 pairs of human vs. state-of-the-art\n(SOTA) NTGs (Uchendu et al. 2021), suggesting that older\ndeepfake text detectors are inferior/obsolete to modern mod-\nels. This further necessitates more thorough investigation\ninto advanced human train methods, instead of relying on\ndetectors. We hypothesize that previous training techniques\nfailed because they did not consider that collaboration and\nskill levels could affect performance. Hence, while we im-\nplement the example-based training technique, we also take\ninto account expertise and collaboration elements.\nAutomatic Evaluation of Deepfake Texts\nAs LLMs such as GPT-2, ChatGPT, LLaMA, etc. are able\nto be used maliciously to generate misinformation at scale,\nseveral techniques have been employed to detect deepfake\ntexts. Using stylometric2 classifiers, researchers adopted sty-\nlometry from traditional authorship attribution solutions to\nachieve automatic deepfake text detection (Uchendu et al.\n2020; Fr ¨ohling and Zubiaga 2021). However, due to the\nflaws of stylometric classifiers, deep-learning techniques\nhave been proposed (Bakhtin et al. 2019; Huggingface 2023;\nZellers et al. 2019; Ippolito et al. 2020; Ai et al. 2022; Jawa-\nhar, Abdul-Mageed, and Lakshmanan 2022). While these\ndeep-learning techniques achieved high performance and\nsignificantly improved from stylometric classifiers, they are\nnot interpretable. To mitigate this issue, statistical-based\nclassifiers are proposed (Gehrmann, Strobelt, and Rush\n2019; Pillutla et al. 2021; Gall ´e et al. 2021; Pillutla et al.\n2022; Mitchell et al. 2023). Lastly, to combine the benefits of\neach of the 3 types of classifiers for deepfake text detection,\n2 or more of these classifier types are combined to build a\nmore robust classifier. Uchendu, Le, and Lee (2023) defines\nthese classifiers as hybrid classifiers and they achieve supe-\nrior performance (Liu et al. 2022; Kushnareva et al. 2021;\nZhong et al. 2020). Lastly, using automatic deepfake text\ndetectors, deepfake detection has been achieved with rea-\nsonable performance. However, in the real world, as humans\ncannot solely depend on these models to detect deepfakes,\nthey need to be equipped at performing the task themselves.\nA common theme in most of the detectors are that newer\nLLMs are harder to detect, which can sometimes make the\nolder detectors obsolete. Thus, it is imperative that humans\nare also able to perform the task of deepfake text detection.\n2stylometry is the statistical analysis of an author’s writing\nstyle/signature.\n165\nA\nB\nC\nD\nFigure 3: User interface for the AMT collaborative group workers to choose the LLM-generated one paragraph, whereas the\nindividual group workers can only see A, B, and C panels.\nFor this reason, a few researchers have evaluated human per-\nformance in this task under several settings. See below.\nMethodology\nThe collective body of prior research has consistently high-\nlighted the inherent difficulty involved in solving the deep-\nfake detection problem (Uchendu et al. 2020; Clark et al.\n2021; Ippolito et al. 2020; Dugan et al. 2020; Gehrmann,\nStrobelt, and Rush 2019). Building upon the concept of “col-\nlective intelligence” that has exhibited superior performance\nin online misinformation detection tasks (Horowitz et al.\n2022; Mercier and Sperber 2011; Liu 2018; Seo, Xiong,\nand Lee 2019) this study aims to investigate whether human\ncollaboration can enhance the detection of deepfake texts.\nSpecifically, the research methodology involves the creation\nof articles comprising two paragraphs authored by humans\nand one paragraph generated by an LLM (i.e.,GPT-2). Non-\nexpert participants from Amazon Mechanical Turk (AMT)\nare then engaged in an asynchronous collaboration setting\nto discern the LLM-generated paragraph from the human-\nwritten paragraphs within the mixed-up articles. Addition-\nally, English experts sourced from Upwork are enlisted to\nperform the same task but in a synchronous collaboration\nmanner. To gain deeper insights into the reasoning process\nof humans, we analyzed the explanations provided by par-\nticipants in the deepfake detection tasks. This study design\nis rooted in the practical reality that, with the increasingly\nimpressive capabilities of LLMs, humans are increasingly\ninclined to employ LLMs to amend or replace portions of\ntheir own written content. The subsequent sections provide\na detailed account of the data generation procedure, the de-\nsign of the human study, and the analysis of explanations.\nData Generation\nAs an overview of the data generation process shown in Fig-\nures 2, to build this dataset, we collected 200 human-written\nnews articles (mostly politics since this work is motivated\nby mitigating the risk of mis/disinformation or fake news\ndissemination) from reputable news sources such as CNN\nand Washington Post. Next, of the 200 articles, we took the\nfirst suitable 50 articles with at least 3 paragraphs. Then, we\nremoved all paragraphs after the 3rd paragraph. Since the\ngoal is to have a multi-authored article (human and LLM),\nwe randomly select one out of the three paragraphsto be\nreplaced by LLM-generated texts. We use a random number\ngenerator to select which paragraphs are to be replaced. As\na result, we replaced the Paragraph 1 in 23 articles, Para-\ngraph 2 in 16 articles, and Paragraph 3 in 11 articles.\nFor Deepfake text generation, we used GPT-2 (Radford\net al. 2019) XL which has 1.5 billion parameters, and the\naitextgen3, a robust implementation of GPT-2 to generate\ntexts with the default parameters 4. We then followed the\nfollowing mechanism to replace the article with the LLM-\ngenerated paragraph:\n• If paragraph 1 is selected to be replaced: Use Title as a\nprompt to generate GPT-2 replacement;\n3https://github.com/minimaxir/aitextgen\n4We used only GPT-2 instead of GPT-3 or above to generate the\ndeepfake texts because: (1) GPT-2 and GPT-3 or above are using\nthe similar algorithms. Based on (Uchendu et al. 2020; Clark et al.\n2021), human performance on detecting GPT-2 and GPT-3 texts\nhave similar accuracies; and (2) GPT-2 is cheaper to generate texts\nwith than GPT-3 or above since GPT-2 is open-source and GPT-3\nor above is not.\n166\nA HIT Introduction\nB Example Trial and Error\nFigure 4: Instructions to train users by providing prompt feedback.\n• If paragraph 2 is selected to be replaced: Use Paragraph\n1 as a prompt to generate GPT-2 replacement;\n• If paragraph 3 is selected to be replaced: Use Paragraph\n2 as a prompt to generate GPT-2 replacement.\nSince we are unable to control the number of paragraphs\nGPT-2 generates given a prompt, we use a Masked Language\nModel (MLM) to choose the best GPT-2 replacement that\nfits well with the article. We use a BERT-base MLM (Devlin\net al. 2018) to get the probability and calculate the perplex-\nity score of the next sentence. Let us call this model G(.), it\ntakes 2 inputs - the first and probable second sentence/para-\ngraph (G(Text\n1, Text 2)) and outputs a score. The lower\nthe score, the more probable Text 2 is the next sentence.\nFor instance, say GPT-2 texts is to replace Paragraph 2 (P2)\nof an article:\n1. We use P1 as prompt to generate P2 with GPT-2;\n2. GPT-2 generates another 3-paragraph article with P1 as\nthe prompt;\n3. To find the suitable P2 replacement, we do G(P1, each\nGPT-2 generated paragraph);\n4. Since low scores with G(.) is considered most probable,\nthe P2 replacement is the GPT-2 paragraph that yielded\nthe lowest score with G(.).\nAfter we created these multi-authored articles, we manu-\nally did a quality check of a few of these articles by checking\nfor consistency and coherence. See Figure 3(C) for an exam-\nple of the final multi-authored article. We also observe that\nbased on the replacement algorithm, some bias in detection\nmay be introduced. Replacing paragraph 3 may be seen as\neasier because there is no other paragraph after it to judge\nthe coherency. However, we keep the generation process fair\nby only using the text right before the paragraph as a prompt\nto generate the next paragraph. Thus, to replace paragraph\n3, we only use paragraph 2 as a prompt, not the previous\nparagraphs and title.\nHuman Study Design\nNext, as we have defined this realistic scenario, we hypoth-\nesize that collaboration will improve human detection of\ndeepfake texts. Thus, we define 2 variables for this exper-\niment - Individual vs. Collaboration and English expert vs.\nEnglish non-expert. We investigate how collaboration (both\nsynchronous and asynchronous) improves from individual-\nbased detection of deepfake texts. The hypothesis here is\nthat when humans come together to solve a task, collabo-\nrative effort will be a significant improvement from average\nindividual efforts. Additionally, as human detection of deep-\nfake texts is non-trivial, we want to investigate if the task is\nnon-trivial because English non-experts focus on misleading\ncues as opposed to English experts.\nStudy1: Collaboration between AMT Participants\nParticipant Recruitment. Inspired by Clark et al. (2021),\nDugan et al. (2020), and Van Der Lee et al. (2019), we\nused Amazon Mechanical Turk (AMT) to collect responses\nfrom non-expert evaluators. We deployed a two-stage pro-\ncess to conduct non-expert human studies. First, we posted\na qualification-required Human Intelligence Task (HIT) that\npays $0.50 per assignment on AMT to recruit 240 quali-\nfied workers. In addition to our custom qualification used\nfor worker grouping, three built-in worker qualifications are\nused in all HITS, including i) HIT Approval Rate (≤98%),\nNumber of Approved HITs (≥3000), and Locale (US Only)\nQualification. Next, we only enable the qualified workers to\nenter the large-scale labeling tasks. The approximate time to\nfinish each labeling task is around 5 minutes (i.e., the aver-\nage time of two authors on finishing a random HIT). There-\nfore, we aim for $7.25 per hour and set the final payment\nas $0.6 for each assignment. Further, we provide “double-\npayment” to workers who made correct submissions as the\nextra bonus.\nExperiment Design. During the large-scale labeling task,\nwe divide the recruited qualified workers into two groups\nto represent the individual vs. collaborative settings, respec-\ntively. We define group1 asIndividual Group, in which each\nworker was asked to select the LLM-generated paragraph\nwithout any references. See Figure 3, for example, humans\nin Individual Group can only see the introduction with pan-\nels (A) (B) and (C). On the other hand, we design group 2\nto be Collaborative Group, where the workers were asked\nto conduct the same task after the Individual Group finishes\nall HITs (i.e., see panel (A), (B), (C) in Figure 3). In addi-\ntion, workers from the Collaborative Group could also see\n167\nParticipant Gender Education Group\nP1 Female Bachelor’s degree\nG1P2 Female Bachelor’s degree\nP3 Female Bachelor’s degree\nP4 Female Bachelor’s degree\nG2P5 Male Bachelor’s degree\nP6 Male Graduate degree\nP7 Female Graduate degree\nG3P8 Female Graduate degree\nP9 Female Bachelor’s degree\nP10 Female Bachelor’s degree\nG4P11 Female Bachelor’s degree\nP12 Male Bachelor’s degree\nP13 Female Graduate degree\nG5P14 Female Bachelor’s degree\nP15 Male Graduate degree\nP16 Female Bachelor’s degree\nG6P17 Male Bachelor’s degree\nP18 Male Bachelor’s degree\nTable 1: Expert (Upwork) participant demographics.\nthe selection results from group 1 in an asynchronous man-\nner, as the example shown in Figure 3(D), to support their\nown selection.\nFurthermore, we take actions to incentivize workers to\nprovide qualified results: i) in our instruction, we provide\nimmediate feedback on the worker’s selection to calibrate\ntheir accuracy. In specific, after reading the HIT instruction\n(i.e., Figure 4 (A)), workers can get a deeper understanding\nof “which paragraph is generated by AI machine” by trial\nand error on selecting one example (i.e., Figure 4 (B)). Par-\nticipants were given unlimited chances to change their an-\nswers. This example-based training process was inspired by\nClark et al. (2021)’s human evaluation study and was found\nto be the most effective training technique.ii) We pay double\ncompensation to the workers who provide correct answers.\nThis aims to encourage workers to get high accuracy in se-\nlecting the correct machine-generated paragraphs.iii) We set\nthe minimum time constraint (i.e., one minute) for workers\nto submit their HITs so that the workers will concentrate on\nthe task for at least one minute instead of randomly select-\ning one answer and submitting the HIT. Note that we also\ndisabled the copy and paste functions in the user interface\nto prevent workers from searching for the paragraphs from\nonline resources.\nStudy2: Collaboration Between Upwork Participants\nParticipant Recruitment. We utilized Upwork5 to recruit\nexpert evaluators, especially those with expertise in writing\n5Upwork is one of the leading freelance websites with a sub-\nstantial network size. Upwork facilitates the freelance industry by\nintroducing skilled freelancers in diverse categories like writing,\ndesign, and web development. With its automated recommendation\nsystem, we can effectively match our expert workers to our needs.\nSee link: https://sellcoursesonline.com/Upwork-statistics.\nFigure 5: User interface for participants to select explana-\ntions for identified deepfake paragraphs.\ndomains.Through Upwork, we first posted a task descrip-\ntion as a client to gather participants. We mentioned in the\ndescription that this is for research and provided all neces-\nsary information such as research objectives and example\nquestions. Our recruitment advertisement also highlighted\nthe mandatory requirements: (1) a participant should be at\nleast 18 years old; and (2) a participant should be a native\nEnglish speaker. Lastly, if they were willing to proceed, they\nwere asked to submit a proposal answering the following\nquestions: (1) What is the highest level of degree you have\ncompleted in school?; (2) Did you major in English or En-\nglish Literature?; and (3) Describe your recent experience\nwith similar projects.\nOne useful feature for accelerating the recruitment pro-\ncess in Upwork is that not only workers can apply to the\npostings but also clients like us can invite prospective candi-\ndates that seem suitable for the task to submit proposals. We\nmanually reviewed workers’ profile descriptions who speci-\nfied their skill sets as copywriting, editing/proofreading, and\ncontent writing and then sent them invites.\nWhile making recruitment decisions, we verified partic-\nipants’ eligibility by checking their self-reported age, lan-\nguage, and education in the profile, in addition to evaluating\ntheir proposal responses. It resulted in a total of 18 finalists\nto officially begin the study. Next, we sent them the consent\nform via the platform’s messaging function and activated\nUpwork contracts only after they returned the signed form.\nA primary purpose of the contracts was for clients to com-\npensate workers based on submitted hours through the Up-\nwork system. Participants’ requested hourly wages ranged\nfrom $25-$35 per hour depending on their prior experiences\nand education levels. All 18 individuals successfully signed\nboth documents and were compensated accordingly. Table 1\ngives the self-reported demographic of recruited Upworkers.\nExperiment Design. To compare experts’ deepfake text\ndetection accuracy with respect to individual vs. collab-\norative settings, our Upwork study consists of two sub-\nexperiments. The first experiment asks Upwork participants\nto perform a given task on their own. The second experiment\nrequires three individuals to solve the questions as one group\nin a synchronous manner. We used Qualtrics6 service to gen-\nerate and disseminate the study form. Upwork participants\n6https://www.qualtrics.com\n168\nSetting Mean Accuracy P-Value\nBaseline vs.\nIndividual 33.33% vs.\n44.99%*** 3.8e-05\nBaseline vs.\nCollaboration 33.33% vs. 51.35%\n*** 2.8e-05\nIndividual\nvs. Collaboration 44.99% vs. 51.35% 0.054\nTable 2: Paired t-test results for AMT experiments.\n(***: p < 0.001, **: p < 0.01, *: p < 0.05)\nwere given one week to complete the survey. Upon comple-\ntion, we randomly grouped 3 participants per team, result-\ning in 6 teams in total for synchronous collaboration (Table\n1). All discussions were conducted on the video communi-\ncations software - Zoom and we leveraged Zoom’s built-in\naudio transcription feature, which is powered by Otter.ai 7\nfor discourse analyses. In addition to the written consent ob-\ntained during the recruitment procedure, verbal consent for\nparticipation in the discussion and for audio recording was\nobtained prior to the start of each session. One member of\nthe study team served as a moderator for the meetings. De-\npending on the participant’s schedule and level of commit-\nment in their group, each meeting lasted 1.5 - 3 hours.\nIn-depth Analysis on Detection Explanations\nWe build the explanation section similar to RoFT (Dugan\net al. 2020), a gamification technique for improving human\nperformance in deepfake text detection. In the RoFT frame-\nwork, participants were asked to select from a pre-defined\nlist one or more reasons such as repetition, grammar errors,\netc. Participants were also given another option, where they\ncan enter their own justification if they do not find any suit-\nable selection from the provided list.\nTo determine the list of pre-defined reasoning explana-\ntions in deepfake text detection, we first refer to Dou et al.\n(2022), which provides a detailed list of 10 errors in which\nannotators have been indicated to be good indicators of\ndeepfake texts. However, these errors are general errors and\nthus some are not applicable to the task of detecting deep-\nfake paragraphs. Therefore, due to this novel application, we\nonly select the most relevant errors. Additionally, we also in-\nclude relevant errors from Dugan et al. (2020) including the\nselection of other, a gamification of deepfake text detection.\nAs the result shown in Figure 5, we consequently provide\nseven pre-defined rationales that correspond to flaws typi-\ncally observed in deepfake texts (Dou et al. 2022; Dugan\net al. 2020), including “Grammatical issues”, “Repetition”,\n“Lacks common sense”, “Contains logical errors”, “Contra-\ndicts previous sentences”, “Lack of creativity or boring to\nread”, “Writing is erratic” (i.e. does not have a good flow),\nand an additional open-ended selection - other for partici-\npants to write more of their own.\nGiven the pre-defined reasoning explanation list, we ask\nboth individuals and collaborative groups to provide their\nexplanations for each corresponding detection instance. We\napply this implementation for both non-expert and ex-\npert groups, resulting in the in-depth explanation analy-\n7https://otter.ai\nSetting Mean Accuracy P-Value\nBaseline vs.\nIndividual 33.33% vs.\n56.11%*** 8.2e-11\nBaseline vs.\nCollaboration 33.33% vs. 68.87%\n*** 1.2e-12\nIndividual\nvs. Collaboration 56.11% vs. 68.87%\n*** 1.3e-05\nTable 3: Paired t-test results for Upwork experiments.\n(***: p < 0.001, **: p < 0.01, *: p < 0.05)\nsis with respect to four scenarios (i.e., individual-expert,\ncollaboration-expert, individual-non-expert, collaboration-\nnon-expert). To provide more fine-grained insights, we fur-\nther separate the deepfake detection results into correct de-\ntection and incorrect detection subgroups.\nExperimental Results\nEvaluation Metrics and Baselines\nObjective Metrics. We measure how well participants\nperform the tasks and compared them across different ex-\nperiment settings. To quantify the detection performance of\neach setting, we computed the proportion of people who\ngot the answer correct given a set of 50 questions Q={q1,\nq2,..., q50}. Suppose ln is the number of participants with\ncorrect answers, and mn is the total number of participants\nfor the question qn, we calculated the accuracy using this\nformula: accn =ln/mn × 100. This resulted in a list of ac-\ncuracy scores ACC={acc1, acc2, ..., acc50}, representing the\nparticipants’ performance of 50 articles. To further evalu-\nate whether the means of two groups (individual vs. collab-\norative & non-experts vs. experts settings) are statistically\ndifferent, we conducted a paired independent sample T-test.\nSince the T-test is grounded on the assumption of normal-\nity (Gerald 2018), we ran the Kolmogorov-Smirnov test on\nour data and confirmed that the requirement was satisfied.\nFollowing, we summarize the results of statistical testing.\nBaseline. Each of the 50 3-paragraphed articles has 2\nparagraphs authored by human and 1 paragraph deepfake-\nauthored. Therefore, participants have a 1/3 chance of se-\nlecting the deepfake paragraph, and we developed a ran-\ndom generator to randomly identify one of the paragraph as\ndeepfake. As such, the baseline performance of the random-\nguessing accuracy is approximately to be 33.33%.\nStudy 1: Collaboration between AMT Workers\nDetection Performance. From Table 2 we observe that\nEnglish non-experts achieve an average accuracy of 44.99%\nindividually, which is a 11.66% increase from the baseline\n(random-guessing) of 33.33%. Using a paired T-test to mea-\nsure statistical significance, the baseline vs. individual per-\nformance comparison achieve a p-value of 3.8e−05 which\nindicates strong statistical significance. Next, for the collab-\norative setting, the non-experts collaborate asynchronously,\nachieving an average accuracy of 51.35%. The p-value of\nIndividual vs. Collaboration comparison is 0.054, indicat-\ning weak statistical significance. However, the comparison\nof Baseline vs. Collaboration yields a p-value of 2.8e−05\nwhich indicates strong significance. Thus, all comparison\n169\nExplanation T\nype\nCorrect\nDetection Incorrect\nDetection\nNon-Expert (I\nvs. C) Expert (I\nvs. C) Non-Expert (I\nvs. C) Expert (I\nvs. C)\nPer\ncentage (%) P-Value Per\ncentage (%) P-Value Per\ncentage (%) P-Value Per\ncentage (%) P-Value\nGrammar 13.97 vs.\n23.08** 0.004 15.33 vs.\n24.6*** 0.001 15.65 vs.\n16.89 0.587 14.22 v\n.s 12.07 0.329\nRepetition 6.73 vs.\n6.69 0.98 4 vs.\n6.4 0.125 8.53* vs.\n5.62 0.044 1.67 vs.\n2 0.703\nCommon Sense 9.25 vs.\n15.48* 0.011 13 vs.\n28*** 4.8e-07 13.02 vs.\n9.94 0.166 3.33 vs.\n5.56 0.163\nLogical Errors 11.64 vs.\n10.24 0.496 7.78 vs.\n14.4** 0.002 18.54*** vs.\n7.7 3.3e-05 3.89 vs.\n4 0.938\nSelf-Contradiction 9.35 vs.\n5.57 0.077 7.67 vs.\n14.8*** 0.001 18.01*** vs.\n6.7 9.6e-07 6.56 vs.\n3.6 0.054\nLack of\nCreativity 12.87 vs.\n13.49 0.776 8.33 vs.\n7.6 0.714 16.9 vs.\n14.13 0.322 8.11** v\n.s 3.6 0.002\nCoherence 14.64 vs.\n19.29* 0.045 20.56 vs.\n32*** 0.0008 11.65 vs.\n10.06 0.318 13.78** vs.\n9.2 0.003\nOther 0 vs.\n0 N/A 12.22 vs\n18.4* 0.014 0 vs.\n0 N/A 6.78 vs.\n8.4 0.264\nTable 4: The percentage of frequency for each reasoning explanation category w.r.t. correct & incorrect detection (I vs. C =\nIndividual vs. Collaboration) and corresponding t-test results (***: p < 0.001, **: p < 0.01, *: p < 0.05).\ngroups for non-experts indicate strong significant improve-\nment, except for Individual vs. Collaboration in which the\nimprovement observed during collaboration is weak.\nAnalysis of Reasoning Explanations. In Table 4, we\nmeasure the statistical significance of explanations used by\nparticipants individually and collaboratively for each of the\nseven reasoning explanations, where we divide based on\nboth correct and incorrect detection responses. For AMT\n(i.e., non-experts), we observe only a few statistically sig-\nnificant explanations. Correct responses show significant\nscores for grammar, common sense, and coherence. While\nincorrect responses have significant scores for repetition,\nlogical errors, and self-contradiction. Furthermore, we vi-\nsualize these explanations for both correct and incorrect re-\nsponses in Figures 6 and 7, respectively. In these figures,\nwe observe that non-experts, both Individually and collab-\noratively, do not show any patterns in response. Thus, in\nsummary, these factors yielded a minimal improvement in\nperformance when non-experts collaborated. Another rea-\nson for the minimal improvement is the style of collabora-\ntion utilized by non-experts - asynchronous collaboration.\nWe further elaborate on the potential hypothesis in the Dis-\ncussion section below.\nStudy 2: Collaboration Between Upwork\nParticipants\nDetection Performance. The English experts achieve an\naverage accuracy of 56.11% and a p-value of 8.2e-11 for\nBaseline vs. Individual, indicating strong significance. In the\ncollaborative (synchronous) setting, the participants achieve\nan average accuracy of 68.87% with a p-value of 1.3e-05 for\nIndividual vs. Collaboration, suggesting a strong statistical\nsignificance. Also the p-value for the comparison of Base-\nline vs. Collaboration (1.2e-12) indicates an even stronger\nsignificance.\nAnalysis of Reasoning Explanations. In Table 4, we\nmeasure the statistical significance of explanations used in-\ndividually and collaboratively. We measure two categories\nwhen responses are correct and incorrect. For Upwork (ex-\nperts), we observe more statistically significant explanations\nfor correct responses than for incorrect responses. Correct\nresponses had 6 statistically significant types from collabo-\nrations out of 8 - grammar, common sense, logical errors,\nself-contradiction, coherence, and other. Next, incorrect re-\nsponses recorded only 2 statistically significant responses -\nlack of creativity and coherence. Furthermore, we observe\nin Table 4 that experts show a much stronger significance\n(p-value < 0.01) in the frequency of explanations used than\nnon-experts in the correct detection cases.\nFurthermore, Figures 6 and 7 visualize the frequency of\nexplanations used by participants for correct and incorrect\nresponses, respectively. We observe that experts used co-\nherence, common sense, grammar errors, other, 8 and self-\ncontradiction more frequently collaboratively for correct\nresponses. However, individually, they used grammar er-\nrors, coherence and other more frequently for incorrect re-\nsponses. This suggests that coherence, common sense, and\nself-contradiction are strong indicators for distinguishing\ndeepfake texts from human-written texts, since they are the\nonly explanations do not overlap in frequency between cor-\nrect and incorrect responses.\nDiscussion\nDeepfake Text Detection is Non-Trivial for Humans\nIn order to further confirm the difficulty of the task of de-\ntecting 1/3 of paragraphs as deepfake, we asked ChatGPT\nto perform the task. Recently, ChatGPT 9 has been found to\nhave emergent abilities (OpenAI 2023), one of which is be-\ning able to accurately perform many text classification tasks\naccurately. Given the recent observation that using personas\nwith ChatGPT improves accuracy, we use a persona-themed\nprompt - You are an expert. Which of the 3 paragraphs is AI-\ngenerated? Answer choices: paragraph\n1, paragraph 2, or\nparagraph 3. Using this prompt, ChatGPT achieves a 38%\naccuracy, only 5% above the baseline. In fact, ChatGPT got\nconfused with certain paragraphs that it deviated from the\n8off-topic and off-prompt were the most frequent justifications.\n9https://openai.com/blog/chatgpt\n170\nFigure 6: The percentage of frequency for selected reasoning explanation w.r.t. correct human detection.\nFigure 7: The percentage of frequency for selected reasoning explanations w.r.t. incorrect human detection.\nanswer choices and generated other responses, such as none\nof the paragraphs are AI-generated, all are AI-generated, or\npicking two instead of one paragraph, e.g., paragraph\n1 &\nparagraph 3. While using our framework, humans achieve\nan average accuracy of 44.99% (non-experts) and 51.35%\n(experts), individually. Through collaboration, their perfor-\nmances increase to an average accuracy of 51.35% and\n68.87% for non-experts.\nDetection Performance Comparison Between\nExperts and Non-Experts\nIn the aforementioned sections, we observe that collabora-\ntion is an effective approach to improve human performance\nin deepfake text detection. Further, the results in Tables 2\nand 3 also suggests that experts achieve a more signifi-\ncant improvement with collaboration than non-experts.\nThere are two potential reasons for this: (1) expert partic-\nipants are able to utilize their expert knowledge more effi-\nciently when collaborating. This is further confirmed in Fig-\nures 6, where Upwork (experts) Collaboration show more\nfrequent use of coherence, common sense, grammar errors,\nother, self-contradiction, and logical errors than Individu-\nally. The intuition here is individually, expert participants\ndid not use these explanations as frequently which yielded\nan average accuracy of 56.11%, however, during collabo-\nration, they used these explanations more frequently and\naccurately, improving the average accuracy to 68.87%. (2)\nThe second reason is argued by the body of CSCW litera-\nture (e.g., (Birnholtz and Ibara 2012; Shirani, Tafti, and Aff-\nisco 1999; Mabrito 2006)) which suggests that the gains of\nsynchronous collaboration outweighs the benefits of asyn-\nchronous collaboration.\nHowever, for non-experts due to the erratic usage of ex-\nplanations as observed in Figures 6 and 7, it is difficult\nto ascertain a pattern. This is potentially the reason why,\nalthough collaboration is statistically significant for non-\nexperts, it is a weak significance (p-value=0.054). Further-\nmore, this suggests that while experts are able to collaborate\nwell, non-experts may require a guided synchronous collab-\noration strategy to further improve performance. It is worth\nnoting that when comparing the Non-expert with AMT and\nExpert with Upwork performance, the difference may po-\ntentially also be resulted from different collaboration modes\n(i.e., “asynchronous” vs. “synchronous” settings) and differ-\nent compensation levels. However, with the respective ra-\ntional settings with two groups, the Experts can outperform\nnon-experts in detecting deepfake texts.\n171\nWhich Explanation Categories Can Potentially Be\nHelpful for Deepfake Text Detection?\nExperts’ mentions of coherence, logical errors, and self-\ncontradiction errors as explanations for deepfake text de-\ntection were significantly higher in the collaborative setting\nthan in the individual setting, specifically for correct re-\nsponses (Figure 6). Non-experts showed no pattern differ-\nences in coherence, logical errors, and self-contradiction ex-\nplanations between individuals and collaboration. However,\nexpert participants used them, especially coherence and self-\ncontradiction, more in collaboration when they detected the\ndeepfake texts successfully and less in collaboration when\nthey detected deepfake texts inaccurately. This result corrob-\norates Dou et al. (2022)’s finding that machines are prone to\nfall short of those categories. Taking into account experts’\nsuperior performance in deepfake text detection, we con-\nclude that both coherence errors and self-contradiction er-\nrors are strong indicators of deepfake text. Table 4 confirms\nthis finding as well since both explanations have a p-value\n< 0.001 for correct responses, suggesting very strong sig-\nnificance. Regarding logical errors, expert participants used\nthem more frequently in the collaborative setting for both\ncorrect and incorrect responses. That said, our findings im-\nply that logical flaws may be a weak predictor of deepfake.\nWhich Explanation Categories Should Be Cautious\nIndicators for Deepfake Text Detection?\nIn line with previous works (Dou et al. 2022; Clark et al.\n2021), we observe that participants’ mentions of grammar\nerrors, repetition, creativitywere associated with incorrect\ndetection of deepfake texts. These markers are identified as\ndeceptive indicators of deepfake texts.\n1. Grammar Errors: Experts use grammar errors fre-\nquently for both correct (collaboration) and incorrect (in-\ndividual) responses. This could be attributed to the fact\nthat humans are equally prone to making grammatical\nmistakes. As a result, employing this explanation can\nresult in both accurate and inaccurate detection. Still,\nour results indicate that experts can use grammar errors\nfor detection signals more correctly as opposed to non-\nexperts. Non-experts use grammar errors frequently for\nboth incorrect and correct responses, although a bit more\nfrequently for incorrect responses. Furthermore, this phe-\nnomenon is confirmed by the findings in (Clark et al.\n2021; Dou et al. 2022) that grammar errors are weak in-\ndicators of deepfake texts. Therefore, we conclude that\ngrammar errors are weak indicators of deepfake texts.\n2. Repetition: We observe in Figure 6 and 7 that repetition\nis the last and second-last frequently used explanations\nfor correct and incorrect responses, respectively. This\nwas a good indicator of deepfake texts when NTGs were\nstill in their infancy. However, NTGs have improved sig-\nnificantly such that the quality of generations can be mis-\nconstrued as human-written. In addition, we took mea-\nsures to ensure high-quality generations, which is dis-\ncussed in detail in the Method Section. Thus, repetition\nwas not prevalent in the deepfake texts, making repetition\na weak indicator of deepfake texts.\n3. Creativity: News is supposed to be the unbiased report-\ning of factual events. Therefore, as these events remain\nnon-fiction, news articles are not creative and should not\nbe judged by their level of creativity. This is the rea-\nson why experts used creativity very sparingly because\nEnglish experts they are aware of which style of writ-\ning should creative or not-creative. Therefore, experts\nuse repetition as explanation second-to-last for correct\nresponses (Figure 6). Unsurprisingly, experts used cre-\nativity a bit more frequently for incorrect responses, with\nthe more frequent usage observed in individuals (Figure\n7). However, for non-experts, creativity is also used spar-\ningly for correct responses but frequently for incorrect\nresponses. Therefore, due to the frequent usage of cre-\nativity for incorrect responses vs. infrequent usage for\ncorrect responses, it follows that for the task of detecting\ndeepfake news paragraphs, creativity is a false indicator\nof deepfake texts.\nLimitation\nTo implement design choices and run manageable experi-\nments, we made a few simplifications that may limit our\nfindings. First, since we only use GPT-2 to generate deep-\nfake texts, our findings may not be directly applicable to\nother NTGs. However, our choice of using GPT-2 is reason-\nable because: (1) prior research reported that human detec-\ntion performance of deepfake texts by the later GPT-3 and\nGPT-2 is similar (Uchendu et al. 2021; Clark et al. 2021),\nand (2) using the largest parameter size of GPT-2 enabled us\nto generate deepfake texts more effectively that closely re-\nsembles GPT-3 quality. Furthermore, as we use the default\nhyperparameters of GPT-2 to generate the texts, the results\nmay be limited to that sampling technique. However, we mit-\nigated this issue by manually checking the quality of a few of\nthe articles and found the deepfake texts to be coherent and\nconsistent with the rest of the paragraphs. This preserved the\nintegrity of the experiments as the task remained non-trivial.\nConclusion\nOur study investigated the impact of human collaboration on\nimproving the detection of deepfake texts. To create a realis-\ntic experimental setup, we constructed a three-paragraph ar-\nticle comprising one LLM-generated (deepfake) paragraph\nand two human-written paragraphs. Participants were tasked\nwith identifying the deepfake paragraph and providing ex-\nplanations based on seven explanation types. For participant\nrecruitment, we recruited non-expert participants from AMT\nfor asynchronous collaboration and experts from Upwork\nfor synchronous collaboration. The results revealed that col-\nlaboration is likely to enhance the detection performance of\nboth non-experts and experts. We further identified several\nstrong and weak indicators of deepfake texts through the ex-\nplanation analysis. Notably, the improved performance of\nparticipants compared to the baselines indicated that our\nTuring Test framework effectively facilitated the enhance-\nment of human deepfake text detection performance.\n172\nEthical Statement\nOur research protocol was approved by the Institutional Re-\nview Board (IRB) at our institution. We only recruited hu-\nman participants 18 years old or over. Participants did not\nhave to complete the entire task to be paid. Using AMT, par-\nticipants’ identification was already anonymized, but for Up-\nwork we anonymized participants by assigning them numer-\nical values for the analysis. For performing the deepfake text\ndetection task, all our human participants, from both AMT\nand Upwork, were paid over minimum wage rate. Next, the\narticles that we used for the experiments are the first 3-\nparagraphs of news articles. While we did not share the an-\nswer to the task, we clearly informed participants that the\npresented texts (and one of three paragraphs therein) con-\ntains deepfake texts. Therefore, we believe that participants\nare unlikely to be negatively influenced by their exposure to\nthe test news articles with deepfake paragraphs.\nAcknowledgments\nThis work was in part supported by NSF awards #1820609,\n#2114824, and #2131144, and PSU CSRE seed grant 2023.\nWe thank the crowd workers from AMT and the experts\nfrom Upwork for participating in this study. We also thank\nthe anonymous reviewers for their constructive feedback.\nReferences\nAi, B.; Wang, Y .; Tan, Y .; and Tan, S. 2022. Whodunit?\nLearning to Contrast for Authorship Attribution. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific Chap-\nter of the Association for Computational Linguistics and the\n12th International Joint Conference on Natural Language\nProcessing, 1142–1157.\nBagdasaryan, E.; and Shmatikov, V . 2022. Spinning\nLanguage Models: Risks of Propaganda-As-A-Service and\nCountermeasures. In 2022 IEEE Symposium on Security and\nPrivacy (SP), 769–786. IEEE.\nBakhtin, A.; Gross, S.; Ott, M.; Deng, Y .; Ranzato, M.;\nand Szlam, A. 2019. Real or fake? learning to discrimi-\nnate machine from human generated text. arXiv preprint\narXiv:1906.03351.\nBirnholtz, J.; and Ibara, S. 2012. Tracking changes in col-\nlaborative writing: edits, visibility and group maintenance.\nIn Proceedings of the ACM 2012 conference on Computer\nSupported Cooperative Work, 809–818.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; Pinto, H. P. d. O.;\nKaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brockman,\nG.; et al. 2021. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374.\nClark, E.; August, T.; Serrano, S.; Haduong, N.; Gururan-\ngan, S.; and Smith, N. A. 2021. All That’s ‘Human’ Is Not\nGold: Evaluating Human Evaluation of Generated Text. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), 7282–7296. Online: Association for Com-\nputational Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDonahue, C.; Lee, M.; and Liang, P. 2020. Enabling Lan-\nguage Models to Fill in the Blanks. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 2492–2501.\nDou, Y .; Forbes, M.; Koncel-Kedziorski, R.; Smith, N. A.;\nand Choi, Y . 2022. Is GPT-3 Text Indistinguishable from\nHuman Text? Scarecrow: A Framework for Scrutinizing\nMachine Text. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 7250–7274.\nDugan, L.; Ippolito, D.; Kirubarajan, A.; and Callison-\nBurch, C. 2020. RoFT: A Tool for Evaluating Human De-\ntection of Machine-Generated Text. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, 189–196.\nFagni, T.; Falchi, F.; Gambini, M.; Martella, A.; and Tesconi,\nM. 2021. TweepFake: About detecting deepfake tweets.\nPlos one, 16(5): e0251415.\nFr¨ohling, L.; and Zubiaga, A. 2021. Feature-based detection\nof automated language models: tackling GPT-2, GPT-3 and\nGrover. PeerJ Computer Science, 7: e443.\nGall´e, M.; Rozen, J.; Kruszewski, G.; and Elsahar, H. 2021.\nUnsupervised and Distributional Detection of Machine-\nGenerated Text. arXiv preprint arXiv:2111.02878.\nGehrmann, S.; Strobelt, H.; and Rush, A. M. 2019. GLTR:\nStatistical Detection and Visualization of Generated Text.\nIn Proceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics: System Demonstrations,\n111–116.\nGerald, B. 2018. A brief review of independent, depen-\ndent and one sample t-test. International Journal of Applied\nMathematics and Theoretical Physics, 4(2): 50–54.\nHorowitz, M.; Cushion, S.; Dragomir, M.;\nGuti´errez Manj ´on, S.; and Pantti, M. 2022. A framework\nfor assessing the role of public service media organizations\nin countering disinformation. Digital journalism, 10(5):\n843–865.\nHuggingface. 2023. GPT-2 Output Detector Demo. https:\n//huggingface.co/openai-detector/. Accessed: 2023-09-18.\nIppolito, D.; Duckworth, D.; Callison-Burch, C.; and Eck,\nD. 2020. Automatic Detection of Generated Text is Easi-\nest when Humans are Fooled. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 1808–1822. Online: Association for Computational\nLinguistics.\nJawahar, G.; Abdul-Mageed, M.; and Lakshmanan, L. 2022.\nAutomatic Detection of Entity-Manipulated Text using Fac-\ntual Knowledge. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 2:\nShort Papers), 86–93.\n173\nJuneja, P.; and Mitra, T. 2022. Human and Technological In-\nfrastructures of Fact-Checking. Proc. ACM Hum.-Comput.\nInteract., 6(CSCW2).\nKushnareva, L.; Cherniavskii, D.; Mikhailov, V .; Artemova,\nE.; Barannikov, S.; Bernstein, A.; Piontkovskaya, I.; Pio-\nntkovski, D.; and Burnaev, E. 2021. Artificial Text Detection\nvia Examining the Topology of Attention Maps. InProceed-\nings of the 2021 Conference on Empirical Methods in Natu-\nral Language Processing, 635–649.\nLee, J.; Le, T.; Chen, J.; and Lee, D. 2023. Do language\nmodels plagiarize? In Proceedings of the ACM Web Confer-\nence 2023, 3637–3647.\nLiu, I. J. 2018. CekFakta: Collaborative Fact-Checking in\nIndonesias. https:// blog.google/outreach-initiatives/google-\nnews-initiative/cekfakta-collaborative-fact-checking-\nindonesia/. Accessed: 2023-09-18.\nLiu, X.; Zhang, Z.; Wang, Y .; Lan, Y .; and Shen, C. 2022.\nCoCo: Coherence-Enhanced Machine-Generated Text De-\ntection Under Data Limitation With Contrastive Learning.\narXiv preprint arXiv:2212.10341.\nMabrito, M. 2006. A study of synchronous versus asyn-\nchronous collaboration in an online business writing class.\nThe American Journal of Distance Education, 20(2): 93–\n107.\nMercier, H.; and Sperber, D. 2011. Why do humans rea-\nson? Arguments for an argumentative theory. Behavioral\nand brain sciences, 34(2): 57–74.\nMitchell, E.; Lee, Y .; Khazatsky, A.; Manning, C. D.; and\nFinn, C. 2023. Detectgpt: Zero-shot machine-generated\ntext detection using probability curvature. arXiv preprint\narXiv:2301.11305.\nOpenAI. 2023. GPT-4 Technical Report. ArXiv,\nabs/2303.08774.\nPillutla, K.; Liu, L.; Thickstun, J.; Welleck, S.;\nSwayamdipta, S.; Zellers, R.; Oh, S.; Choi, Y .; and Har-\nchaoui, Z. 2022. MAUVE Scores for Generative Models:\nTheory and Practice. arXiv preprint arXiv:2212.14578.\nPillutla, K.; Swayamdipta, S.; Zellers, R.; Thickstun, J.;\nWelleck, S.; Choi, Y .; and Harchaoui, Z. 2021. An infor-\nmation divergence measure between neural text and human\ntext. arXiv preprint arXiv:2102.01454.\nPu, J.; Sarwar, Z.; Abdullah, S. M.; Rehman, A.; Kim, Y .;\nBhattacharya, P.; Javed, M.; Viswanath, B.; Tech, V .; and\nPakistan, L. 2023. Deepfake Text Detection: Limitations\nand Opportunities. 44th IEEE Symposium on Security and\nPrivacy.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1): 5485–5551.\nSeo, H.; Xiong, A.; and Lee, D. 2019. Trust it or not: Ef-\nfects of machine-learning warnings in helping individuals\nmitigate misinformation. In Proceedings of the 10th ACM\nConference on Web Science, 265–274.\nShen, H.; and Wu, T. 2023. Parachute: Evaluating in-\nteractive human-lm co-writing systems. arXiv preprint\narXiv:2303.06333.\nShevlane, T.; Farquhar, S.; Garfinkel, B.; Phuong, M.; Whit-\ntlestone, J.; Leung, J.; Kokotajlo, D.; Marchal, N.; An-\nderljung, M.; Kolt, N.; et al. 2023. Model evaluation for\nextreme risks. arXiv preprint arXiv:2305.15324.\nShirani, A. I.; Tafti, M. H.; and Affisco, J. F. 1999. Task and\ntechnology fit: a comparison of two technologies for syn-\nchronous and asynchronous group communication. Infor-\nmation & management, 36(3): 139–150.\nSurowiecki, J. 2005. The wisdom of crowds. Anchor.\nTan, R.; Plummer, B.; and Saenko, K. 2020. Detecting\nCross-Modal Inconsistency to Defend Against Neural Fake\nNews. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, 2081–2106.\nUchendu, A.; Le, T.; and Lee, D. 2023. Attribution and Ob-\nfuscation of Neural Text Authorship: A Data Mining Per-\nspective. SIGKDD Explorations, vol. 25.\nUchendu, A.; Le, T.; Shu, K.; and Lee, D. 2020. Author-\nship attribution for neural text generation. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing, 8384–8395.\nUchendu, A.; Ma, Z.; Le, T.; Zhang, R.; and Lee, D. 2021.\nTURINGBENCH: A Benchmark Environment for Turing\nTest in the Age of Neural Text Generation. In Findings of\nthe 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, 2001–2016.\nVan Der Lee, C.; Gatt, A.; Van Miltenburg, E.; Wubben, S.;\nand Krahmer, E. 2019. Best practices for the human evalu-\nation of automatically generated text. In Proceedings of the\n12th International Conference on Natural Language Gener-\nation, 355–368.\nWeidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;\nHuang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh,\nA.; et al. 2021. Ethical and social risks of harm from lan-\nguage models. arXiv preprint arXiv:2112.04359.\nWu, S.; Shen, H.; Weld, D. S.; Heer, J.; and Ribeiro, M. T.\n2023. ScatterShot: Interactive In-context Example Curation\nfor Text Transformation. InProceedings of the 28th Interna-\ntional Conference on Intelligent User Interfaces, 353–367.\nZellers, R.; Holtzman, A.; Rashkin, H.; Bisk, Y .; Farhadi,\nA.; Roesner, F.; and Choi, Y . 2019. Defending against neu-\nral fake news. Advances in neural information processing\nsystems, 32.\nZhang, T. 2022. Deepfake generation and detection, a sur-\nvey. Multimedia Tools and Applications, 81(5): 6259–6276.\nZheng, Y .; Li, G.; Li, Y .; Shan, C.; and Cheng, R. 2017.\nTruth inference in crowdsourcing: Is the problem solved?\nProceedings of the VLDB Endowment, 10(5): 541–552.\nZhong, W.; Tang, D.; Xu, Z.; Wang, R.; Duan, N.; Zhou, M.;\nWang, J.; and Yin, J. 2020. Neural Deepfake Detection with\nFactual Structure of Text. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 2461–2470.\n174"
}