{
  "title": "P2T: Pyramid Pooling Transformer for Scene Understanding",
  "url": "https://openalex.org/W3177220026",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5001598042",
      "name": "Yu-Huan Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5056961910",
      "name": "Yun Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028120929",
      "name": "Xin Zhan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5037131575",
      "name": "Ming‚ÄìMing Cheng",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3159833358",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3127842933",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2039313011",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2928165649",
    "https://openalex.org/W2780708736",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3112885960",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2945472816",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2740667773",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3113755791",
    "https://openalex.org/W2939217524",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3129436779",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W3119586106",
    "https://openalex.org/W2086791339",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2963306157",
    "https://openalex.org/W2565516711",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3100321043",
    "https://openalex.org/W2888036988",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3137142667",
    "https://openalex.org/W2963032190",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W2963017889",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3136838953",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2806070179",
    "https://openalex.org/W2963619659"
  ],
  "abstract": "Recently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. One of the most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational cost (quadratic complexity). A popular solution to this problem is to use a single pooling operation to reduce the sequence length. This paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation seems less powerful. To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction. However, pyramid pooling has not been explored in backbone network design. To bridge this gap, we propose to adapt pyramid pooling to Multi-Head Self-Attention (MHSA) in the vision transformer, simultaneously reducing the sequence length and capturing powerful contextual features. Plugged with our pooling-based MHSA, we build a universal vision transformer backbone, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various vision tasks such as image classification, semantic segmentation, object detection, and instance segmentation, compared to previous CNN- and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1\nP2T: Pyramid Pooling Transformer\nfor Scene Understanding\nYu-Huan Wu, Yun Liu, Xin Zhan, and Ming-Ming Cheng\nAbstract‚ÄîRecently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. One of\nthe most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational\ncost (quadratic complexity). A popular solution to this problem is to use a single pooling operation to reduce the sequence length.\nThis paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation\nseems less powerful. To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing\nto its powerful ability in context abstraction. However, pyramid pooling has not been explored in backbone network design. To bridge\nthis gap, we propose to adapt pyramid pooling to Multi-Head Self-Attention (MHSA) in the vision transformer, simultaneously reducing\nthe sequence length and capturing powerful contextual features. Plugged with our pooling-based MHSA, we build a universal vision\ntransformer backbone, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as\nthe backbone network, it shows substantial superiority in various vision tasks such as image classiÔ¨Åcation, semantic segmentation,\nobject detection, and instance segmentation, compared to previous CNN- and transformer-based networks. The code will be released\nat https://github.com/yuhuan-wu/P2T.\nIndex Terms‚ÄîTransformer, backbone network, efÔ¨Åcient self-attention, pyramid pooling, scene understanding\n!\n1 I NTRODUCTION\nI\nN the past decade, convolutional neural networks (CNNs)\nhave dominated computer vision and achieved many\ngreat stories [1]‚Äì[9]. The state-of-the-art of various vision\ntasks on many large-scale datasets has been signiÔ¨Åcantly\npushed forward [10]‚Äì[14]. In an orthogonal Ô¨Åeld, i.e., nat-\nural language processing (NLP), the dominating technique\nis transformer [15]. Transformer entirely relies on self-\nattention to capture the long-range global relationships and\nhas achieved brilliant successes. Considering that global in-\nformation is also essential for vision tasks, a proper adaption\nof the transformer [15] should be useful to overcome the\nlimitation of CNNs, i.e., CNNs usually enlarge the receptive\nÔ¨Åeld by stacking more layers.\nLots of efforts are dedicated to exploring such a proper\nadaption of the transformer [15]. Some early attempts use\nCNNs [2], [4] to extract deep features that are fed into trans-\nformers for further processing and regressing the targets\n[16]‚Äì[18]. Dosovitskiy et al. [19] made a thorough success\nby applying a pure transformer network for image classiÔ¨Å-\ncation. They split an image into patches and took each patch\nas a word/token in an NLP application so that transformer\ncan then be directly adopted. This simple method attains\ncompetitive performance on ImageNet [10]. Therefore, a\n‚Ä¢ Y.-H. Wu and M.-M. Cheng are with TMCC, College of\nComputer Science, Nankai University, Tianjin, China. (E-mail:\nwuyuhuan@mail.nankai.edu.cn, cmm@nankai.edu.cn)\n‚Ä¢ Y. Liu is with Institute for Infocomm Research (I2R), Agency\nfor Science, Technology and Research (A*STAR), Singapore. (E-mail:\nvagrantlyun@gmail.com)\n‚Ä¢ X. Zhan is with Alibaba DAMO Academy, Hangzhou, China.\n‚Ä¢ The Ô¨Årst two authors contributed equally to this work.\n‚Ä¢ Corresponding author: M.-M. Cheng. (E-mail: cmm@nankai.edu.cn)\n‚Ä¢ This work is done while Y.-H. Wu is a research intern at Alibaba DAMO\nAcademy.\n20 40 60 80 100\nParameters (M)\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0ADE20K mIoU (%)\nResNet\nResNeXt\nPVT\nTwins\nSwin\nPVTv2\nP2T (Ours)\nAccuracy VS Parameters \nResNet\nResNeXt\nPVT\nTwins\nSwin\nPVTv2\nP2T (Ours)\nFig. 1. Experimental results for semantic segmentation on the\nADE20K dataset [14].Following PVT [21], Semantic FPN [26] is chosen\nas the basic method, equipped with different backbone networks, includ-\ning ResNet [4], ResNeXt [27], PVT [21], Twins [28], Swin Transformer\n[22], PVTv2 [29], and our P2T.\nnew concept of the vision transformer appears. In a very\nshort period, a large amount of literature has emerged\nto improve the vision transformer [19], and much better\nperformance than CNNs has been achieved [20]‚Äì[25].\nNevertheless, there is still one challenging problem in\nvision transformers, i.e., the length of the data sequence.\nWhen viewing image patches as tokens, the sequence length\nis much longer than in NLP applications. For example, in\narXiv:2106.12011v6  [cs.CV]  31 Aug 2022\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2\nNLP , the well-known WMT2014 English-German dataset\n[30] has 50M English words with 2M sentences, with an\naverage sequence length of 25. In contrast, in computer\nvision, we usually use the image resolution of 224 √ó224 for\nimage classiÔ¨Åcation on the ImageNet dataset [10], resulting\nin the sequence length of 3136 if we use the common patch\nsize of 4 √ó4. Since the computational and space complexity\nof Multi-Head Self-Attention (MHSA) in the transformer is\nquadratic (rather than linear in CNNs) to the image size,\ndirectly applying the transformer to vision tasks has a\nhigh requirement for computational resources. To make a\npure transformer network possible for image classiÔ¨Åcation,\nViT [19] uses large patch sizes, e.g., 16 and 32, to reduce\nthe sequence length, and achieves great success for image\nclassiÔ¨Åcation. Later, many transformer works signiÔ¨Åcantly\nimprove the performance of ViT [19] by introducing the\npyramid structure [20]‚Äì[24], [28], [31], [32], where the input\nlayer Ô¨Årst uses the small patch size of 4√ó4 and the sequence\nlength is gradually reduced by merging adjacent image\npatches.\nTo further reduce the computational cost of MHSA, PVT\n[21] and MViT [23] use a single pooling operation to down-\nsample the feature map in the computation of MHSA. With\nthe pooled features, they model token-to-region relationship\nrather than the expected token-to-token relationship. Swin\nTransformer [22] proposes to compute MHSA within small\nwindows rather than across the whole input, modeling local\nrelationships. It uses a window shift strategy to gradually\nenlarge the receptive Ô¨Åeld, like CNNs, enlarging the recep-\ntive Ô¨Åeld through stacking more layers [33]. However, an\nessential characteristic of the vision transformer is its direct\nglobal relationship modeling , which is also why we transfer\nfrom CNNs to transformers.\nHere, we consider how to improve PVT [21] and MViT\n[23] where the pooled feature extracted by a single pooling\noperation seems less powerful. If we can squeeze the input\nfeature into a powerful representation with a short sequence\nlength, we may achieve better performance. To this end,\nwe note that pyramid pooling [34]‚Äì[37] is a long-history\ntechnique for computer vision, which extracts contextual\ninformation and utilizes multiple pooling operations with\ndifferent receptive Ô¨Åelds and strides onto the input fea-\nture map. This simple technique has been demonstrated\nto be effective in various downstream vision tasks such\nas semantic segmentation [37] and object detection [36].\nNevertheless, recent pyramid pooling approaches highly\nrely on a pretrained CNN backbone, and thus they are\nlimited to a speciÔ¨Åc task. In another word, the pyramid\npooling technique has not been explored in the backbone\nnetwork design that has broad applications. Motivated by\nthis, we bridge this gap by adapting pyramid pooling to\nthe vision transformer block for simultaneously reducing\nthe sequence length and learning powerful contextual rep-\nresentations. Pyramid pooling is also very efÔ¨Åcient and thus\nwill only induce negligible computational cost for the vision\ntransformer.\nWe achieve this goal by proposing a new transformer\nbackbone network, i.e., Pyramid Pooling Transformer\n(P2T). We adapt the idea of pyramid pooling to the com-\nputation of multi-head self-attention (MHSA) in the vision\ntransformer, reducing the computational cost of MHSA\nand capturing rich contextual information simultaneously.\nBy applying the new pooling-based MHSA, P2T exhibits\nstronger ability in feature representation learning and visual\nrecognition than PVT [21] and MViT [23] which are based\non a single pooling operation. We evaluate P2T for vari-\nous typical vision tasks, like image classiÔ¨Åcation, semantic\nsegmentation, object detection, and instance segmentation.\nExtensive experiments demonstrate that P2T performs bet-\nter than all previous CNN- and transformer-based backbone\nnetworks for these fundamental vision tasks (see Fig. 1 for\ncomparisons on semantic segmentation).\nIn summary, our main contributions include:\n‚Ä¢ We encapsulate pyramid pooling to MHSA, simul-\ntaneously reducing the sequence length of image\ntokens and extracting powerful contextual features.\n‚Ä¢ We plug our pooling-based MHSA into the vision\ntransformer to build a new backbone network, i.e.,\nP2T, making it Ô¨Çexible and powerful for visual recog-\nnition.\n‚Ä¢ We conduct extensive experiments to demonstrate\nthat, when applied as a backbone network for var-\nious scene understanding tasks, P2T achieves sub-\nstantially better performance than previous CNN-\nand transformer-based networks.\n2 R ELATED WORK\n2.1 Convolutional Neural Networks\nSince AlexNet [1] won the champion in the ILSVRC-2012\ncompetition [10], numerous advanced techniques have been\ninvented for improving CNNs, achieving many successful\nstories in computer vision. VGG [2] and GoogleNet [3]\nÔ¨Årst try to deepen CNNs for better image recognition.\nThen, ResNets [4] succeed in building very deep CNNs\nwith the help of residual connections. ResNeXts [27] and\nRes2Nets [9] improve ResNets [4] by exploring its cardinal\noperation. DenseNets [5] introduce dense connections that\nconnect each layer to all its subsequent layers for easing\noptimization. MobileNets [38], [39] decompose a vanilla\nconvolution into a 1 √ó1 convolution and a depthwise sepa-\nrable convolution to build lightweight CNNs for mobile and\nembedded vision applications. ShufÔ¨ÇeNets [40], [41] further\nreduce the latency of MobileNets [38], [39] by replacing the\n1 √ó1 convolution with the grouped 1 √ó1 convolution and\nthe channel shufÔ¨Çe operation. EfÔ¨ÅcientNet [7] and MnasNet\n[42] adopt neural architecture search (NAS) to search for\noptimal network architectures. Since our work focuses on\nthe transformer [15], a comprehensive survey of CNNs is\nbeyond the scope of this paper. Please refer to [43] and [44]\nfor a more extensive survey.\n2.2 Vision Transformer\nTransformer is initially proposed for machine translation in\nNLP [15]. Through MHSA, transformer entirely relies on\nself-attention to model global token-to-token dependencies.\nConsidering that global relationship is also highly required\nby computer vision tasks, it is a natural idea to adopt trans-\nformer for improving vision tasks. However, transformer\nis designed to process sequence data and thus cannot pro-\ncess images directly. Hence, some researchers use CNNs to\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3\nPatch Embedding\nPatch Embedding\nPyramid Pooling Transformer\nPatch Embedding\nPyramid Pooling Transformer\nPyramid Pooling Transformer\nClassification Task\nPatch EmbeddingPyramid Pooling Transformer\n3√óùêª√óùëä ùê∂!√óùêª4√óùëä4 ùê∂\"√óùêª8√óùëä8 ùê∂#√óùêª16√óùëä16 ùê∂$√óùêª32√óùëä32\nDownstream Scene Understanding Tasks\nùêµ! ùêµ\" ùêµ# ùêµ$\nImage\nFig. 2. Architecture of the proposed P2T network. We replace the traditional MHSA with our pooling-based MHSA. The feature maps\n{B1, B2, B3, B4}can be used for downstream scene understanding tasks.\nextract 2D representation that is then Ô¨Çattened and fed into\nthe transformer [16], [17], [45]‚Äì[47]. DETR [16] is a milestone\nin this direction.\nInstead of relying on the CNN backbone for feature\nextraction, Dosovitskiy et al. [19] proposed the Ô¨Årst vision\ntransformer (ViT). They split an image into small patches,\nand each patch is viewed as a word/token in an NLP ap-\nplication. Thus, a pure transformer network can be directly\nadopted with the class token used for image classiÔ¨Åcation.\nTheir method achieved competitive performance on Ima-\ngeNet [10]. Then, DeiT [48] alleviates the required resources\nfor training ViT [19] via knowledge distillation. T2T-ViT\n[49] proposes to split an image with overlapping to better\npreserve local structures. CvT [31] introduces depthwise\nconvolution for generating the query, key, and value in\nthe computation of MHSA. CPVT [50] proposes to replace\nthe absolute positional encoding with the conditional po-\nsitional encoding via a depthwise convolution. Some ef-\nforts are contributed to building the pyramid structure for\nthe vision transformer using pooling operations [20]‚Äì[23].\nAmong them, PVT [21] and MViT [23] Ô¨Årst adopt the single\npooling operation to reduce the number of tokens when\ncomputing MHSA. In this way, they actually conduct token-\nto-region relationship modeling, not the expected token-\nto-token modeling. Since this paper also resolves the long\nsequence length problem through pooling, we adopt PVT\n[21] and MViT [23] as strong baselines in this paper. Swin\nTransformer [22] reduces the computational load of MHSA\nby computing it within small windows. However, Swin\nTransformer gradually achieves global relationship model-\ning by window shift, somewhat like CNNs that enlarge the\nreceptive Ô¨Åeld by stacking more layers [33]. Hence, we think\nthat Swin Transformer [22] sacriÔ¨Åces an essential character-\nistic of the vision transformer, i.e., direct global relationship\nmodeling.\nDifferent from PVT [21] and MViT [23] where the pooled\nfeature extracted by a single pooling operation seems less\npowerful, we adapt the idea of pyramid pooling to the\nvision transformer, simultaneously reducing the sequence\nlength and learning powerful contextual representations.\nWith more powerful representations, it is intuitive that\npyramid pooling may work better than single pooling for\ncomputing self-attention in MHSA. Pyramid pooling is very\nefÔ¨Åcient and thus will only induce negligible computational\ncost. Experiments show that the proposed P2T performs\nsubstantially better performance than previous CNN- and\ntransformer-based networks. Besides, our design is also\ncompatible with other transformer techniques such as patch\nembedding [51], positional encoding [50], and feed-forward\nnetwork [24], [52], [53].\n2.3 Pyramid Pooling\nIn computer vision, pyramid pooling is a long-history and\nwidely-acknowledged technique for extracting feature pre-\nsentations. Before the renaissance of deep CNNs [1], there\nemerged several well-known works that applied pyramid\npooling for recognizing natural scenes [34], [35]. Inspired\nby [34], [35], He et al. [36] introduced pyramid pooling to\ndeep CNNs for image classiÔ¨Åcation and object detection.\nThey adopted several pooling operations to pool the Ô¨Ånal\nconvolutional feature map of a CNN backbone into several\nÔ¨Åxed-size maps. These resulting maps are then Ô¨Çattened and\nconcatenated into a Ô¨Åxed-length representation for robust\nvisual recognition. Then, Zhao et al. [37] applied pyramid\npooling for semantic segmentation. Instead of Ô¨Çattening\nin [36], they upsampled the pooled Ô¨Åxed-size maps into\nthe original size and concatenated the upsampled maps\nfor prediction. Their success suggests the effectiveness of\npyramid pooling in dense prediction. After that, pyramid\npooling has been widely applied to various vision tasks such\nas semantic segmentation [37], [54]‚Äì[57] and object detection\n[36], [58]‚Äì[60].\nUnlike existing literature that explores pyramid pooling\nin CNNs for speciÔ¨Åc tasks, we propose to adapt the concept\nof pyramid pooling to the vision transformer backbone\nnetwork. With this idea, we Ô¨Årst embed the pyramid pooling\ninto the basic pooling-based attention block of our P2T back-\nbone, which can simultaneously reduce the sequence length\nand learn powerful contextual feature representations. P2T\ncan be easily used by various vision tasks for feature rep-\nresentation learning, while previous works about pyramid\npooling are limited to a speciÔ¨Åc vision task. Extensive ex-\nperiments on image classiÔ¨Åcation, semantic segmentation,\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4\nùë∏ Pùüè\nPool1\nMHSA\nPyramid  Pooling\nP-MHSA\nFeed-Forward\nAdd, LN\nAdd, LN\nInput Features\nOutput Features\nOutput Features\nInput Features\n+Concatenation\nK,ùëΩ\n(a) (b)\nFig. 3. Illustration of Pyramid Pooling Transformer. (a) The brief\nstructure of Pyramid Pooling Transformer. (b) The detailed structure of\nthe pooling-based MHSA.\nobject detection, and instance segmentation demonstrate\nthe superiority of P2T compared with existing CNN- and\ntransformer-based networks. Therefore, this work is distinc-\ntive and would beneÔ¨Åt the research on various vision tasks.\n3 M ETHODOLOGY\nIn this section, we Ô¨Årst provide an overview of our P2T\nnetworks in ¬ß3.1. Then, we present the architecture of P2T\nwith pooling-based MHSA in ¬ß3.2. Finally, we introduce\nsome implementation details of our networks in ¬ß3.3.\n3.1 Overview\nThe overall architecture of P2T is illustrated in Fig. 2.\nWith a natural color image as input, P2T Ô¨Årst splits it into\nH\n4 √óW\n4 patches, each Ô¨Çattened to 48 ( 4 √ó4 √ó3) elements.\nFollowing [21], we feed these Ô¨Çattened patches to a patch\nembedding module, which consists of a linear projection\nlayer followed by the addition with a learnable positional\nencoding. The patch embedding module will expand the\nfeature dimension from 48 to C1. Then, we stack the pro-\nposed pyramid pooling transformer blocks that will be\nintroduced in ¬ß3.2. The whole network can be divided into\nfour stages with feature dimensions of Ci (i = {1,2,3,4}),\nrespectively. Between every two stages, each 2 √ó2 patch\ngroup is concatenated and linearly projected from 4 √óCi\nto Ci+1 dimension ( i = {1,2,3}). In this way, the scales\nof four stages become H\n4 √ó W\n4 , H\n8 √ó W\n8 , H\n16 √ó W\n16 , and\nH\n32 √óW\n32 , respectively. From four stages, we can derive four\nfeature representations {B1,B2,B3,B4}, respectively. Only\nB4 will be used for Ô¨Ånal prediction for image classiÔ¨Åcation,\nwhile all pyramid features can be utilized for downstream\nscene understanding tasks.\n3.2 Pyramid Pooling Transformer\nPyramid pooling has been widely used in many scene un-\nderstanding tasks collaborating with CNNs [36], [37], [61]‚Äì\n[68]. However, existing literature usually applies pyramid\npooling on top of CNN backbones for extracting global\nand contextual information for a speciÔ¨Åc task. In contrast,\nthis paper is the Ô¨Årst to explore pyramid pooling in trans-\nformers and backbone networks, targeting for improving\nvarious scene understanding tasks generically. To this end,\nwe adapt the idea of pyramid pooling to the transformer,\nsimultaneously reducing the computational load of MHSA\nand capturing rich contextual information.\nLet us continue by introducing the proposed P2T, the\nstructure of which is illustrated in Fig. 3 (a). The input Ô¨Årst\npasses through the pooling-based MHSA, whose output is\nadded with the residual identity, followed by LayerNorm\n[69]. Like the traditional transformer block [19], [21], [48], a\nfeed-forward network (FFN) follows for feature projection.\nA residual connection and LayerNorm [69] are applied\nagain. The above process can be formulated as\nXatt = LayerNorm(X + P-MHSA(X)),\nXout = LayerNorm(Xatt + FFN(Xatt)), (1)\nwhere X, Xatt, and Xout are the input, the output of\nMHSA, and the output of the transformer block, respec-\ntively.P-MHSA is the abbreviation of pooling-based MHSA.\n3.2.1 Pooling-based MHSA\nHere, we present the design of our pooling-based MHSA. Its\nstructure is shown in Fig. 3 (b). First, the inputX is reshaped\ninto the 2D space. Then, we apply multiple average pooling\nlayers with various ratios onto the reshaped X to generate\npyramid feature maps, like\nP1 = AvgPool1(X),\nP2 = AvgPool2(X),\n¬∑¬∑¬∑ ,\nPn = AvgPooln(X),\n(2)\nwhere {P1,P2,..., Pn}denote the generated pyramid fea-\nture maps and nis the number of pooling layers. Next, we\nfeed pyramid feature maps to the depthwise convolution for\nrelative positional encoding:\nPenc\ni = DWConv(Pi) +Pi, i = 1,2,¬∑¬∑¬∑ ,n, (3)\nwhere DWConv (¬∑) indicates the depthwise convolution\nwith the kernel size 3 √ó3, and Penc\ni is Pi with the relative\npositional encoding. Since Pi is the pooled feature, the\noperation in Equ. 3 only has a little computational cost.\nAfter that, we Ô¨Çatten and concatenate these pyramid feature\nmaps:\nP = LayerNorm(Concat(Penc\n1 ,Penc\n2 ,..., Penc\nn )), (4)\nwhere the Ô¨Çattening operation is omitted for simplicity. In\nthis way, P can be a shorter sequence than the input X\nif pooling ratios are large enough. Besides, P contains the\ncontextual abstraction of the input X and can thus serve as\na strong substitute for the input X when computing MHSA.\nSuppose the query, key, and value tensors in MHSA [19]\nare Q, K, and V, respectively. Instead of using traditional\n(Q,K,V) = (XWq,XWk,XWv), (5)\nwe propose to use\n(Q,K,V) = (XWq,PWk,PWv), (6)\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5\nTABLE 1\nDetailed settings of the proposed P2T. The parameters of building blocks are shown in brackets, with the numbers of blocks stacked. For the\nÔ¨Årst stage, we apply a 7 √ó7 convolution with C output channels and a stride of S for patch embedding. Each IRB uses an expansion ratio ofE. For\nsimplicity, we omit the patch embedding operation, i.e., a 3 √ó3 convolution with a stride of S = 2, after the t-th stage (t = {2, 3, 4}). ‚Äú#Params‚Äù\nrefers to the number of parameters. ‚Äú#Flops‚Äù denotes the computational cost with the input size of224 √ó224.\nStage Input Size Operator P2T-Tiny P2T-Small P2T-Base P2T-Large\n1 224 √ó224 7 √ó7 conv. C = 48, S = 4 C = 64, S = 4\n2 56 √ó56 P-MHSA\nIRB\n[\nC = 48\nE = 8\n]\n√ó2\n[\nC = 64\nE = 8\n]\n√ó2\n[\nC = 64\nE = 8\n]\n√ó3\n[\nC = 64\nE = 8\n]\n√ó3\n3 28 √ó28 P-MHSA\nIRB\n[\nC = 96\nE = 8\n]\n√ó2\n[\nC = 128\nE = 8\n]\n√ó2\n[\nC = 128\nE = 8\n]\n√ó4\n[\nC = 128\nE = 8\n]\n√ó8\n4 14 √ó14 P-MHSA\nIRB\n[\nC = 240\nE = 4\n]\n√ó6\n[\nC = 320\nE = 4\n]\n√ó9\n[\nC = 320\nE = 4\n]\n√ó18\n[\nC = 320\nE = 4\n]\n√ó27\n5 7 √ó7 P-MHSA\nIRB\n[\nC = 384\nE = 4\n]\n√ó3\n[\nC = 512\nE = 4\n]\n√ó3\n[\nC = 512\nE = 4\n]\n√ó3\n[\nC = 640\nE = 4\n]\n√ó3\n1 √ó1 - Global Average Pooling, 1000-d FC, Softmax\n#Params 11.6M 24.1M 36.1M 54.5M\n#Flops 1.8G 3.7G 6.5G 9.8G\nin which Wq, Wk, and Wv denote the weight matrices of\nlinear transformations for generating the query, key, and\nvalue tensors, respectively. Then, Q,K,V are fed into the\nattention module to compute the attention A, which can be\nformulated as below:\nA = Softmax( Q √óKT\n‚àödK\n) √óV, (7)\nwhere dK is the channel dimension of K, and\n‚àödK can\nserve as an approximate normalization. The Softmax func-\ntion is applied along the rows of the matrix. Equ. 7 omits\nthe concept of multiple heads [15], [19] for simplicity.\nSince K and V have a smaller length than X, the\nproposed P-MHSA is more efÔ¨Åcient than traditional MHSA\n[15], [19]. Besides, since K and V contains highly-abstracted\nmulti-scale information, the proposed P-MHSA has a\nstronger capability in global contextual dependency model-\ning, which is helpful for scene understanding [36], [37], [62],\n[65]‚Äì[68]. From a different perspective, pyramid pooling is\nusually used as an effective technique connected upon back-\nbone networks; in contrast, this paper Ô¨Årst exploits pyramid\npooling within backbone networks through transformers,\nthus providing powerful feature representation learning for\nscene understanding. With the above analyses, P-MHSA\nis expected to be more efÔ¨Åcient and more effective than\ntraditional MHSA [15], [19].\nAnalysis of computational complexity. As described in\nEqu. 2, the proposed pooling-based attention leverages sev-\neral pooling operations to generate pyramid feature maps.\nThe pyramid pooling operation only has negligible O(NC)\ncomputational complexity, where N and C represent the\nsequence length and the feature dimension, respectively.\nHence, the computational complexity for computing self-\nattention can be formulated as\nO(P-MHSA) = (N + 2M)C2 + 2NMC, (8)\nwhere M is the concatenated sequence length of all pooled\nfeatures. For the default pooling ratios of {12, 16, 20, 24 },\nwe have M ‚âà N\n66.3 ‚âà N\n82 , which is comparable with the\ncomputational cost of MHSA in PVT [21].\n3.2.2 Feed-Forward Network\nFeed-Forward Network (FFN) is an essential component of\ntransformers for feature enhancement [15], [70]. Previous\ntransformers usually apply an MLP as the FFN [15], [19],\n[21] and entirely rely on attention to capture inter-pixel\ndependencies. Though effective, this architecture is not good\nat learning 2D locality, which plays a critical role in scene\nunderstanding. To this end, we follow [52], [53] to insert the\ndepthwise convolution into FFN so that the resulting trans-\nformer can inherit the merits of both transformer ( i.e., long-\nrange dependency modeling) and CNN ( i.e., 2D locality).\nSpeciÔ¨Åcally, we adopt the Inverted Bottleneck Block (IRB),\nproposed in MobileNetV2 [39], as the FFN.\nTo adapt IRB for the vision transformer, we Ô¨Årst trans-\nform the input sequence Xatt to a 2D feature map XI\natt:\nXI\natt = Seq2Image(Xatt), (9)\nwhere Seq2Image (¬∑) is to reshape the 1D sequence to a\n2D feature map. Given the input XI\natt, IRB can be directly\napplied, like\nX1\nIRB = Act(XI\nattW1\nIRB),\nXout\nIRB = Act(DWConv(X1\nIRB))W2\nIRB,\n(10)\nwhere W1\nIRB, W2\nIRB indicate the weight matrices of 1√ó1 con-\nvolutions, ‚ÄúAct‚Äù indicates the nonlinear activation function,\nXout\nIRB is the output of IRB. Since Xout\nIRB is a 2D feature map,\nwe Ô¨Ånally transform it to a 1D sequence:\nXS\nIRB = Image2Seq(Xout\nIRB), (11)\nwhere Image2Seq (¬∑) is the operation that reshapes the 2D\nfeature map to a 1D sequence. XS\nIRB is the output of FFN,\nwith the same shape as Xatt.\n3.3 Implementation Details\nP2T with different depths. Following previous backbone\narchitectures [4], [21], [22], [24], [27], we build P2T with\ndifferent depths via stacking the different number of pyra-\nmid pooling transformers at each stage. In this manner, we\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6\npropose four versions of P2T, i.e., P2T-Tiny, P2T-Small, P2T-\nBase, and P2T-Large with similar numbers of parameters to\nResNet-18 [4], ResNet-50 [4], ResNet-101 [4], and PVT-Large\n[21], respectively. Each head of P-MHSA has 64 feature\nchannels except that each head has 48 feature channels in\nP2T-Tiny. Other conÔ¨Ågurations for different versions of P2T\nare shown in Table 1.\nPyramid pooling settings. We empirically set the number\nof parallel pooling operations in P-MHSA as 4. At different\nstages, the pooling ratios of pyramid pooling transform-\ners are different. The pooling ratios for the Ô¨Årst stage are\nempirically set as {12,16,20,24}. Pooling ratios in each\nnext stage are divided by 2 except that, in the last stage,\nthey are set as {1,2,3,4}. In each transformer block, all\ndepthwise convolutions (Equ. 3) in P-MHSA share the same\nparameters.\nOther settings. Although a larger kernel size ( e.g., 5 √ó5)\nof depthwise convolution (in Equ. 3) can bring better perfor-\nmance, the kernel size of all depthwise convolutions is set\nto 3 √ó3 for efÔ¨Åciency. We choose Hardswish [71] as the non-\nlinear activation function because it saves much memory\ncompared with GELU [72]. Hardswish [71] also empirically\nworks well. Same with PVTv2 [29], we apply overlapped\npatch embedding. That is, we use 3 √ó3 convolution with a\nstride of 2 for patch embedding from the second to the last\nstage, while we apply a 7 √ó7 convolution with a stride of 4\nfor patch embedding in the Ô¨Årst stage.\n4 E XPERIMENTS\nWe Ô¨Årst introduce the experiments on image classiÔ¨Åcation in\n¬ß4.1. Then we validate P2T‚Äôs effectiveness on several scene\nunderstanding tasks, i.e., semantic segmentation, object de-\ntection, and instance segmentation in ¬ß4.2, ¬ß4.3, and ¬ß4.4,\nrespectively. At last, we conduct ablation studies for better\nunderstanding our method in ¬ß4.5.\n4.1 Image ClassiÔ¨Åcation\nImage classiÔ¨Åcation is the most common task for evaluating\nthe capability of backbone networks. It aims to assign a class\nlabel to each natural image input. Many other tasks build\non top of image classiÔ¨Åcation via applying classiÔ¨Åcation\nnetworks as the backbones for feature extraction.\nExperimental setup. As described in ¬ß3.1, only the output\nfeature B4 of the last stage is utilized here. Following regu-\nlar CNN networks [4], [5], [9], we append a global average\npooling layer and a fully-connected layer on top of B4 to\nobtain the Ô¨Ånal classiÔ¨Åcation scores. We train our network\non the ImageNet-1K dataset [10], which has 1.28M training\nimages and 50k validation images. For a fair comparison, we\nfollow PVT [21] to adopt the same training protocols as DeiT\n[48] (without knowledge distillation), which is a standard\nchoice for training vision transformers. SpeciÔ¨Åcally, we use\nAdamW [76] as the optimizer, with the initial learning\nrate of 1e-3, weight decay of 0.05, and a mini-batch of\n1024 images. We train P2T for 300 epochs with the cosine\nlearning rate decay strategy. Images are resized to the size\nof 224√ó224 for training and testing. Models are warmed up\nTABLE 2\nImage classiÔ¨Åcation results on the ImageNet-1K dataset [10].\n‚ÄúTop-1‚Äù indicates the top-1 accuracy rate. ‚Äú*‚Äù indicates the results with\nknowledge distillation [48]. ‚Äú#P (M)‚Äù denotes the number of parameters\n(M). Both the number of computational cost (GFlops) and running\nspeed (frames per second, FPS) are reported with the default\n224 √ó224 input size for each network except that the speed of ViT -B\n[19] is tested with the input size of 384 √ó384. FPS is tested on a single\nRTX 2070 GPU. The results of the proposed P2T are marked in bold.\nMethod #P (M) ‚Üì GFlops ‚Üì Top-1 (%) ‚Üë FPS ‚Üë\nResNet-18 [4] 11.7 1.8 68.5 1410\nDeiT-Tiny/16* [48] 5.7 1.3 72.2 1212\nViL-Tiny [73] 6.7 1.3 76.7 441\nPVT-Tiny [21] 13.2 1.9 75.1 608\nPVTv2-B1 [29] 13.1 2.1 78.7 502\nP2T-Tiny (Ours) 11.6 1.8 79.8 473\nResNet-50 [4] 25.6 4.1 78.5 483\nResNeXt-50-32x4d [27] 25.0 4.3 79.5 407\nRes2Net-50 [9] 25.7 4.5 80.3 430\nDeiT-Small/16* [48] 22.1 4.6 79.9 489\nPVT-Small [21] 24.5 3.8 79.8 336\nT2T-ViTt-14 [49] 21.5 5.2 80.7 305\nSwin-T [22] 29.0 4.5 81.3 349\nTwins-SVT-S [28] 24.0 2.9 81.7 439\nViL-Small [73] 25.0 4.9 82.4 187\nPVTv2-B2 [29] 25.4 4.0 82.0 284\nP2T-Small (Ours) 24.1 3.7 82.4 284\nResNet-101 [4] 44.7 7.9 79.8 288\nResNeXt-101-32x4d [27] 44.2 8.0 80.6 228\nRes2Net-101 [9] 45.2 8.3 81.2 265\nPVT-Medium [21] 44.2 6.7 81.2 216\nT2T-ViTt-19 [49] 39.2 8.4 81.4 202\nSwin-S [22] 50.0 8.7 83.0 207\nViL-Medium [73] 40.4 8.7 83.5 114\nMViT-B-16 [23] 37.0 7.8 83.1 222\nPVTv2-B3 [29] 45.2 6.9 83.2 189\nP2T-Base (Ours) 36.1 6.5 83.5 182\nResNeXt-101-64x4d [27] 83.5 15.6 81.5 147\nMViT-B-24 [23] 53.5 10.9 83.0 151\nViL-Base [73] 57.0 13.4 83.7 67\nPVT-Large [21] 61.4 9.8 81.7 152\nDeiT-Base/16* [48] 86.6 17.6 81.8 161\nViT-Base/16 [19] 86.6 17.6 77.9 49\nSwin-B [22] 88.0 15.4 83.3 140\nTwins-SVT-L [28] 99.2 14.8 83.3 143\nPVTv2-B4 [29] 62.6 10.1 83.6 133\nPVTv2-B5 [29] 82.0 11.8 83.8 120\nP2T-Large (Ours) 54.5 9.8 83.9 128\nfor the Ô¨Årst Ô¨Åve epochs. The data augmentation is also the\nsame as [21], [48].\nExperimental results. The quantitative comparisons are\nsummarized in Table 2. All models are trained and eval-\nuated with the input size of 224 √ó224 except that\nwe follow the ofÔ¨Åcial ViT [19] to train and evaluate\nit with the input size of 384 √ó384. P2T largely out-\nperforms regular CNN models like ResNets [4] and\nResNeXts [27]. For example, although the running time\nof P2T-Tiny/Small/Base/Large is 2.98/1.70/1.58/1.15 times\nthat of ResNet-18/50/101 [4] and ResNeXt-101-64x4d\n[27], the top-1 accuracy of P2T-Tiny/Small/Base/Large\nis 11.3%/3.9%/3.7%/2.4% better than that of ResNet-\n18/50/101 [4] and ResNeXt-101-64x4d [27], respectively.\nAs can be seen, our P2T also achieves superior results\ncompared with recent state-of-the-art transformer models.\nFor example, P2T-Small/Base/Large is 1.1%/0.5%/0.6%\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7\nTABLE 3\nExperimental results on the validation set of the ADE20K dataset\n[14] for semantic segmentation. We replace the backbone of\nSemantic FPN [26] with various network architectures. The number of\nGFlops is calculated with the input size of 512 √ó512. FPS is tested on\na single RTX 2070 GPU. The results of P2T backbones are marked in\nbold.\nBackbone Semantic FPN [26]\n#Param (M) ‚Üì GFlops ‚Üì mIoU (%) ‚Üë FPS ‚Üë\nResNet-18 [4] 15.5 31.9 32.9 68\nPVT-Tiny [21] 17.0 32.1 35.7 36\nPVTv2-B1 [29] 17.8 33.1 41.5 30\nP2T-Tiny (Ours) 15.4 31.6 43.4 31\nResNet-50 [4] 28.5 45.4 36.7 35\nPVT-Small [21] 28.2 42.9 39.8 26\nSwin-T [22] 31.9 46 41.5 26\nTwins-SVT-S [28] 28.3 37 43.2 27\nPVTv2-B2 [29] 29.1 44.1 46.1 21\nP2T-Small (Ours) 27.8 42.7 46.7 24\nResNet-101 [4] 47.5 64.8 38.8 26\nResNeXt-101-32x4d [27] 47.1 64.6 39.7 20\nPVT-Medium [21] 48.0 59.4 41.6 19\nSwin-S [22] 53.2 70 45.2 18\nTwins-SVT-B [28] 60.4 67 45.3 17\nPVTv2-B3 [29] 49.0 60.7 47.3 15\nP2T-Base (Ours) 39.8 58.5 48.7 16\nResNeXt-101-64x4d [27] 86.4 104.2 40.2 15\nPVT-Large [21] 65.1 78.0 42.1 15\nSwin-B [22] 91.2 107 46.0 13\nTwins-SVT-L [28] 102 103.7 46.7 13\nPVTv2-B4 [29] 66.3 79.6 48.6 11\nPVTv2-B5 [29] 85.7 89.4 48.9 10\nP2T-Large (Ours) 58.1 77.7 49.4 12\nbetter than Swin Transformer [22] with fewer network\nparameters and less computational cost. Although PVTv2\n[29] has large improvement over PVT [21], our P2T-\nTiny/Small/Base/Large still have 1.1%/0.4%/0.3%/0.3%\nimprovement over PVTv2-B1/B2/B3/B4 [29] with fewer\nparameters and less computational cost. P2T applies four\nparallel pooling operations when computing self-attention,\nstill achieving competitive speed with PVTv2 [29]. ViL [73]\nachieves competitive performance with P2T. Nevertheless,\nthe speed of ViL [73] is much slower than our P2T, and the\ncomputational cost of ViL [73] is also much larger than P2T.\nP2T also largely outperforms ViT [19] and DeiT [48] with\nmuch fewer parameters, implying that P2T achieves better\nperformance without a large amount of training data and\nknowledge distillation. Therefore, P2T is very capable for\nimage classiÔ¨Åcation.\n4.2 Semantic Segmentation\nGiven a natural image input, semantic segmentation aims at\nassigning a semantic label to each pixel. It is one of the most\nfundamental dense prediction tasks in computer vision.\nExperimental setup. We evaluate P2T and its competitors\non the ADE20K [14] dataset. The ADE20K dataset is a chal-\nlenging scene understanding dataset with 150 Ô¨Åne-grained\nsemantic classes. This dataset has 20000, 2000, and 3302\nimages for training, validation, and testing, respectively.\nFollowing [21], [28], Semantic FPN [26] is chosen as the basic\nmethod for a fair comparison. We replace the backbone of\nSemantic FPN [26] with various network architectures. All\nbackbones of semantic FPN have been pretrained on the\nImageNet-1K [10] dataset, and other layers are initialized\nusing the Xavier method [77]. All networks are trained\nfor 80k iterations. We apply AdamW [76] as the network\noptimizer, with the initial learning rate of 1e-4 and weight\ndecay of 1e-4. The poly learning rate schedule with Œ≥ = 0.9\nis adopted. Each mini-batch has 16 images. Images are\nresized and randomly cropped to 512 √ó512 for training.\nSynchronized batch normalization across GPUs is also en-\nabled. During testing, images are resized to the shorter side\nof 512 pixels. Multi-scale testing and Ô¨Çipping are disabled.\nFollowing [21], we use the MMSegmentation toolbox [78] to\nimplement the above experiments.\nExperimental results. Quantitative comparison results\nare shown in Table 3. We compare our proposed P2T with\nResNets [4], ResNeXts [27], PVT [21], Swin Transform-\ners [22], Twins [28], and PVTv2 [29]. The results of each\nnetwork are from the ofÔ¨Åcial papers or re-implemented\nusing the ofÔ¨Åcial training conÔ¨Ågurations. BeneÔ¨Åting from\nthe pyramid pooling technique, the results of Semantic\nFPN [26] with our P2T backbone are much better than\nother CNN and transformer competitors. Typically, P2T-\nTiny/Small/Base/Large are 10.5%/10.0%/9.9%/9.2% bet-\nter than ResNet-18/50/101 [4] and ResNeXt-10-64x4d [27]\nwith fewer parameters and GFlops, respectively. Compared\nwith Swin Transformer [22] which introduces local self-\nattention with transformers, P2T-Small/Base/Large achieve\n5.2%/3.5%/3.4% improvement over Swin-T/S/B [22], re-\nspectively, showing that global relationship modeling is\nsigniÔ¨Åcant for visual recognition. Twins [28] combine the\nlocal self-attention from Swin Transformers [22] and the\nglobal self-attention from PVT [21]. As can be observed,\nTwins [28] perform better than Swin Transformers [22], sug-\ngesting that global self-attention is signiÔ¨Åcant again. Unlike\nTwins [28], we apply pure global self-attention via pyramid\npooling, learning richer contexts. P2T-Small/Base/Large\nare 3.5%/3.4%/2.7% better than Twins-SVT-S/B/L [28],\nrespectively. PVTv2 [29] is the improved version of PVT\n[21], serving as the strongest competitor for our P2T. P2T-\nTiny/Small/Base/Large are 1.9%/0.6%/1.4%/0.8% better\nthan PVTv2-B1/B2/B3/B4 [29], respectively. Besides, P2T-\nTiny/Small/Base/Large always have fewer parameters, less\ncomputational cost, and faster speed than the corresponding\nPVTv2-B1/B2/B3/B4 [29]. At last, we found that P2T-Tiny\nis 3.2% better than ResNeXt-101-64x4d [27] with twice the\nspeed. Based on the above observations, we can conclude\nthat P2T is very capable for semantic segmentation.\n4.3 Object Detection\nObject detection is also one of the most fundamental and\nchallenging tasks for decades in computer vision. It aims to\ndetect and recognize instances of semantic objects of certain\nclasses in natural images. Here, we evaluate P2T and its\ncompetitors on the MS-COCO [11] dataset.\nExperimental setup. MS-COCO [11] is a large-scale chal-\nlenging dataset for object detection, instance segmentation,\nand keypoint detection. MS-COCO train2017 (118k im-\nages) and val2017 (5k images) sets are used for training\nand validation in our experiments, respectively. RetinaNet\n[74] is applied as the basic framework because it has been\nwidely acknowledged by this community [21], [22]. Each\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8\nTABLE 4\nObject detection results with RetinaNet [74] and instance segmentation results with Mask R-CNN [75] on the MS-COCO val2017 set\n[11]. ‚ÄúR‚Äù and ‚ÄúX‚Äù represent the ResNet [4] and ResNeXt [27], respectively. The number of Flops is computed with the input size of800 √ó1280.\nFPS is tested on a single RTX 2070 GPU. The results of P2T backbones are marked in bold.\nBackbone\nObject Detection Instance Segmentation\n#Param\n(M) ‚Üì\n#Flops\n(G)‚Üì\n#FPS\n‚Üë\nRetinaNet [74] #Param\n(M) ‚Üì\n#Flops\n(G) ‚Üì\n#FPS\n‚Üë\nMask R-CNN [75]\nAP ‚ÜëAP50 AP75 APS ‚ÜëAPM APL APb ‚ÜëAPb\n50 APb\n75 APm ‚ÜëAPm\n50 APm\n75\nR-18 [4] 21.3 190 19.3 31.8 49.6 33.6 16.3 34.3 43.2 31.2 209 17.3 34.0 54.0 36.7 31.2 51.0 32.7\nViL-Tiny [73] 16.6 204 4.2 40.8 61.3 43.6 26.7 44.9 53.6 26.9 223 3.9 41.4 63.5 45.0 38.1 60.3 40.8\nPVT-Tiny [21] 23.0 205 10.7 36.7 56.9 38.9 22.6 38.8 50.0 32.9 223 10.0 36.7 59.2 39.3 35.1 56.7 37.3\nPVTv2-B1 [29] 23.8 209 8.5 40.2 60.7 42.4 22.8 43.3 54.0 33.7 227 8.0 41.8 64.3 45.9 38.8 61.2 41.6\nP2T-Tiny (Ours) 21.1 206 9.3 41.3 62.0 44.1 24.6 44.8 56.0 31.3 225 8.8 43.3 65.7 47.3 39.6 62.5 42.3\nR-50 [4] 37.7 239 13.0 36.3 55.3 38.6 19.3 40.0 48.8 44.2 260 11.5 38.0 58.6 41.4 34.4 55.1 36.7\nPVT-Small [21] 34.2 261 7.7 40.4 61.3 43.0 25.0 42.9 55.7 44.1 280 7.0 40.4 62.9 43.8 37.8 60.1 40.3\nSwin-T [22] 38.5 248 9.7 41.5 62.1 44.2 25.1 44.9 55.5 47.8 264 8.8 42.2 64.6 46.2 39.1 61.6 42.0\nViL-Small [73] 35.7 292 3.4 44.2 65.2 47.6 28.8 48.0 57.8 45.0 310 3.2 44.9 67.1 49.3 41.0 64,2 44.1\nTwins-SVT-S [28] 34.3 236 8.5 43.0 64.2 46.3 28.0 46.4 57.5 44.0 254 7.7 43.4 66.0 47.3 40.3 63.2 43.4\nPVTv2-B2 [29] 35.1 266 5.8 43.8 64.8 46.8 26.0 47.6 59.2 45.0 285 5.4 45.3 67.1 49.6 41.2 64.2 44.4\nP2T-Small (Ours) 33.8 260 7.4 44.4 65.3 47.6 27.0 48.3 59.4 43.7 279 6.7 45.5 67.7 49.8 41.4 64.6 44.5\nR-101 [4] 56.7 315 9.8 38.5 57.8 41.2 21.4 42.6 51.1 63.2 336 9.1 40.4 61.1 44.2 36.4 57.7 38.8\nX-101-32x4d [27] 56.4 319 8.5 39.9 59.6 42.7 22.3 44.2 52.5 62.8 340 7.9 41.9 62.5 45.9 37.5 59.4 40.2\nPVT-Medium [21] 53.9 349 5.7 41.9 63.1 44.3 25.0 44.9 57.6 63.9 367 5.3 42.0 64.4 45.6 39.0 61.6 42.1\nSwin-S [22] 59.8 336 7.1 44.5 65.7 47.5 27.4 48.0 59.9 69.1 354 6.6 44.8 66.6 48.9 40.9 63.4 44.2\nPVTv2-B3 [29] 55.0 354 4.5 45.9 66.8 49.3 28.6 49.8 61.4 64.9 372 4.2 47.0 68.1 51.7 42.5 65.7 45.7\nP2T-Base (Ours) 45.8 344 5.0 46.1 67.5 49.6 30.2 50.6 60.9 55.7 363 4.7 47.2 69.3 51.6 42.7 66.1 45.9\nX-101-64x4d [27] 95.5 473 6.2 41.0 60.9 44.0 23.9 45.2 54.0 101.9 493 5.7 42.8 63.8 47.3 38.4 60.6 41.3\nPVT-Large [21] 71.1 450 4.4 42.6 63.7 45.4 25.8 46.0 58.4 81.0 469 4.1 42.9 65.0 46.6 39.5 61.9 42.5\nTwins-SVT-B [28] 67.0 376 5.1 45.3 66.7 48.1 28.5 48.9 60.6 76.3 395 4.6 45.2 67.6 49.3 41.5 64.5 44.8\nPVTv2-B4 [29] 72.3 457 3.4 46.1 66.9 49.2 28.4 50.0 62.2 82.2 475 3.2 47.5 68.7 52.0 42.7 66.1 46.1\nPVTv2-B5 [29] 91.7 514 3.2 46.2 67.1 49.5 28.5 50.0 62.5 101.6 532 3.0 47.4 68.6 51.9 42.5 65.7 46.0\nP2T-Large (Ours) 64.4 449 3.8 47.2 68.4 50.9 32.4 51.6 62.2 74.0 467 3.5 48.3 70.2 53.3 43.5 67.3 46.9\nmini-batch has 16 images with an initial learning rate of\n1e-4. Following the popular MMDetection toolbox [79], we\ntrain each network for 12 epochs, and the learning rate is di-\nvided by 10 after 8 and 11 epochs. The network optimizer is\nAdamW [76], a popular optimizer for training transformers.\nThe weight decay is set as 1e-4. During training and testing,\nthe shorter side of input images is resized to 800 pixels.\nThe longer side will keep the ratio of the images within\n1333 pixels. In the training stage, only random horizontal\nÔ¨Çipping is used for data augmentation. Standard COCO API\nis utilized for evaluation, and we report results in terms\nof AP , AP 50, AP 75, AP S, AP M , and AP L metrics. AP S,\nAPM , and AP L mean AP scores for small, medium, and\nlarge objects deÔ¨Åned in [11], respectively. AP is usually\nviewed as the primary metric. For each metric, larger scores\nindicate better performance. We also report the number of\nparameters and the computational cost for reference.\nExperimental results. The evaluation results on the MS-\nCOCO dataset are summarized in the left part of Table 4.\nThe results of other networks are from the ofÔ¨Åcial papers or\nre-implemented using the ofÔ¨Åcial conÔ¨Ågurations. The below\ndiscussion refers to the metric of AP if not stated. We can\nobserve that our P2T achieves the best performance under\nall tiny/small/large complexity settings. For example, P2T-\nSmall achieves 2.9%, 1.4%, and 0.6% higher AP over Swin-\nT [22], Twins-SVT-S [28], and PVTv2-B2 [29], respectively.\nP2T-Tiny is 1.1% better than PVTv2 [29]. Compared with\nViL [73], P2T-Tiny/Small are 0.5% and 0.2% better than ViL-\nTiny/Small [73], respectively. Note that ViL [73] runs at a\nmuch slower speed than P2T, as shown in Table 4. With\nthe base complexity setting, P2T-Base outperforms Swin-S\n[29] by 1.0% and is 0.2% better than the best competitor\nPVTv2-B3. With the large complexity setting, P2T-Large\nachieves 1.1% and 1.9% better AP than PVTv2-B4 [29] and\nTwins-SVT-B [28], respectively. At all complexity levels,\nP2T always outperforms PVTv2 [29] with fewer network\nparameters, less computational cost, and faster speed. P2T-\nTiny/Small/Base/Large are 9.5%/8.1%/7.0%/6.2% better\nthan ResNet-18/50/101 [4] and ResNeXt-101-64x4d [27],\nrespectively. Therefore, P2T is very capable for object de-\ntection.\n4.4 Instance Segmentation\nInstance segmentation is another fundamental vision task. It\ncan be regarded as an advanced case of object detection by\noutputting Ô¨Åne-grained object masks instead of bounding\nboxes in object detection.\nExperimental setup. We evaluate the performance of in-\nstance segmentation on the well-known MS-COCO dataset\n[11]. MS-COCO train2017 and val2017 sets are used\nfor training and validation in our experiments. Mask R-\nCNN [75] is applied as the basic framework using different\nbackbone networks. The training settings are the same as\nwhat we use for object detection in ¬ß4.3. We report evalua-\ntion results for object detection and instance segmentation\nin terms of AP b, AP b\n50, AP b\n75, AP m, AP m\n50, and AP m\n75 met-\nrics, where ‚Äúb‚Äù and ‚Äúm‚Äù indicate bounding box and mask\nmetrics, respectively. AP b and AP m are set as the primary\nevaluation metrics.\nExperimental results. The comparisons between P2T\nand its competitors are displayed in the right part of\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9\nTABLE 5\nAblation studies on multiple pyramid pooling ratios. The ‚ÄúD ratio‚Äù\nindicates the downsampling ratio between the initial sequence length\nand the downsampled sequence length. ‚ÄúTop-1‚Äù denotes the top-1\nclassiÔ¨Åcation accuracy rate on the ImageNet-1K validation set [10], and\n‚ÄúmIoU‚Äù indicates the results for semantic segmentation on the ADE20K\ndataset [14].\nNo. Pooling Ratio(s) D Ratio ‚Üë Top-1 (%) ‚Üë mIoU (%) ‚Üë\n1 24 576 70.6 27.5\n2 16 256 72.5 33.0\n3 12 144 73.9 34.3\n4 8 64 73.9 34.4\n5 12, 24 115 74.4 34.8\n6 12, 16, 20, 24 66 74.7 35.7\nTABLE 6\nAblation studies for replacing single pooling operation with\nmultiple pooling operations at different stages. Since both stage 2\nand 3 of our network only have two basic blocks, we merge them into\none choice. ‚ÄúTop-1‚Äù denotes the top-1 classiÔ¨Åcation accuracy rate on\nthe ImageNet-1K validation set [10], and ‚ÄúmIoU‚Äù is the results for\nsemantic segmentation on the ADE20K dataset [14].\nNo. Stage # of Network Top-1 (%) ‚Üë mIoU ‚Üë[2, 3] 4 5\n1 73.9 34.4\n2 \u0014 74.1 34.9\n3 \u0014 \u0014 74.5 35.5\n4 \u0014 \u0014 \u0014 74.7 35.7\nTABLE 7\nAblation studies on the choices of pooling operations. We can see\nthat other choices work worse than average pooling. ‚ÄúTop-1‚Äù indicates\nthe top-1 accuracy rate on the ImageNet-1K dataset [10] for image\nclassiÔ¨Åcation. ‚ÄúmIoU‚Äù is the mean IoU rate on the ADE20K dataset [14]\nfor semantic segmentation.\nPooling Type Top-1 (%) ‚Üë mIoU (%) ‚Üë\nAverage Pooling 74.7 35.7\nMax Pooling 73.0 33.2\nConvolution 73.8 35.5\nTable 4. P2T achieves the best performance consistently\ncompared to existing CNN and transformer backbone net-\nworks. Compared with transformer-based backbones, P2T\nachieves the best performance at all complexity levels. Re-\ngarding the bounding box metric AP b, P2T-Small/Base are\n3.3%/2.4% better than Swin-T/S [22], and P2T-Small/Large\nare 2.1%/3.1% better than Twins-SVT-S/B [28]. P2T-\nTiny/Small/Base/Large are 1.5%/0.2%/0.2%/0.8% better\nthan PVTv2-B1/B2/B3/B4 [29] with fewer parameters, less\ncomputational cost, and faster speed, respectively. In terms\nof the mask metric AP m, we also observe similar improve-\nments as observed using bounding box metrics. Compared\nwith ResNet-based backbones, P2T signiÔ¨Åcantly outper-\nforms ResNets [4] and ResNeXts [27] at all complexity\nlevels. It is also surprising that our lightest P2T-Tiny is 0.5%\nand 1.2% better than ResNeXt-101-64x4d [27] in terms of\nbounding box and mask metrics, respectively. Therefore,\nP2T is very capable for instance segmentation.\nTABLE 8\nAblation study on the Ô¨Åxed pooled size. GFlops is computed with an\ninput size of 512 √ó512 for the semantic segmentation model, i.e.,\nSemantic FPN [26]. Memory (Mem) denotes the training GPU memory\nusage for Semantic FPN [26] with a batch size of 2. ‚ÄúTop-1‚Äù and ‚ÄúmIoU‚Äù\nindicate the top-1 classiÔ¨Åcation accuracy on ImageNet-1K [10] and\nsegmentation mIoU on ADE20K [14], respectively.\nPooling Operation GFlops ‚ÜìMem (GB) ‚ÜìTop-1 (%) ‚ÜëmIoU (%) ‚Üë\nFixed Pooled ratios 41.6 3.3 74.7 35.7\nFixed Pooled Sizes 38.9 2.9 74.4 33.3\n4.5 Ablation Studies\nExperimental setup. In this section, we perform ablation\nstudies to analyze the efÔ¨Åcacy of each design choice in P2T.\nWe evaluate the performance of model variants on semantic\nsegmentation and image classiÔ¨Åcation. Due to the limited\ncomputational resources, we only train each model variant\non the ImageNet dataset [10] for 100 epochs, while other\ntraining settings keep the same as in¬ß4.1. Then, we Ô¨Åne-tune\nthe ImageNet-pretrained model on the ADE20K dataset [14]\nwith the same training settings as in ¬ß4.2.\nMultiple pyramid pooling ratios. To validate the sig-\nniÔ¨Åcance of using multiple pooling ratios, we conduct\nexperiments to evaluate the performance of P2T with\none/two/four parallel pooling operations. The baseline is\nP2T-Small without relative positional encoding, IRB, and\noverlapping patch embedding. The results are shown in\nTable 5. As can be seen, the single pooling operation with\na large pooling ratio ( e.g., 16, 24) has a large squeezed\nratio for the sequence length. Still, it results in very poor\nperformance for both image classiÔ¨Åcation and semantic seg-\nmentation. However, when the single pooling operation is\nwith a pooling ratio ‚â§12, the performance will be saturated\nif we further decrease the pooling ratio. When we adopt\ntwo parallel pooling operations, even with a high squeezed\nratio, the performance still becomes better for both image\nclassiÔ¨Åcation and semantic segmentation. When we have\nfour parallel pooling operations, we can derive the best\nperformance with the comparable squeezed ratio for the\npooling ratio of 8 (the setting in PVT [21]).\nSigniÔ¨Åcance of pyramid pooling for different stages. We\nperform ablation studies of the pyramid pooling design of\nP2T. for different stages. Since stage 1 only contains convo-\nlutions for downsampling, we do not perform such ablation\nstudy at stage 1. The baseline is same with the last ablation\nstudies. The pooling ratio of single pooling operation is set\nto 8 for ensuring comparable downsampling ratios. Results\nare shown in Table 6. We can observe pyramid pooling can\nimprove the performance at all stages. The performance\nbecomes higher when more stages are applied with multiple\npooling operations. From the results, the improvement on\napplying multiple pooling operations on stage 4 (No. 3 of\nTable 6) is larger than that on other stages (No. 2, 4 of\nTable 6), because stage 4 has more basic blocks than the\nsummation of stage [2,3] and stage 5.\nPooling operation choices. We conduct experiments for\ndifferent pooling operations, as shown in Table 7. There are\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10\nTABLE 9\nAblation study on the relative positional encoding (RPE), IRB, and\noverlapping patch embedding (OPE). ‚ÄúTop-1‚Äù and ‚ÄúmIoU‚Äù indicate\nthe top-1 classiÔ¨Åcation accuracy on ImageNet-1K [10] and\nsegmentation mIoU on ADE20K [14], respectively.\nRPE IRB OPE Top-1 (%) ‚Üë mIoU (%) ‚Üë\n74.7 35.7\n\u0014 76.4 37.4\n\u0014 \u0014 79.5 42.7\n\u0014 \u0014 \u0014 79.7 44.1\nthree typical choices, i.e., max pooling, depthwise convo-\nlution, and the default average pooling. The kernel size of\ndepthwise convolution is the same as max/average pooling\nto keep the same downsampling rate. It is obvious that\ndifferent pooling types do not affect the computational\ncomplexity and they only affect the number of the param-\neters of the downsampling kernel. Regarding the results of\nImageNet classiÔ¨Åcation accuracy [10] and ADE20K segmen-\ntation mIoU [14], average pooling is much better than the\nother two choices. Thus, we apply average pooling as the\ndefault pooling choice.\nFixed pooled sizes. When using Ô¨Åxed pooling ratios, the\nsize of the pooled feature map will vary with the input\nfeature map. Here, we try to Ô¨Åx the pooled sizes as {1, 2, 3,\n6}for all stages, using adaptive average pooling. The results\nare shown in Table 8. Compared with our default setting,\nabout 10% of memory usage and 12% of computational\ncost are saved. However, the top-1 classiÔ¨Åcation accuracy\ndrops by 0.3%, and the semantic segmentation performance\nis 2.4% lower. Hence, we choose to use Ô¨Åxed pooling ratios\nrather than Ô¨Åxed pooled sizes.\nSelection of activation functions. We use the Hardswish\nfunction [71] for nonlinear activation to reduce GPU mem-\nory usage in the training phase. Typically, when we train\nP2T-Small on ImageNet [10] with a batch size of 64, the\nGPU memory usage of GELU [72] is 10.5GB, which is 3.6GB\n(+52%) more than that of Hardswish [71]. We also Ô¨Ånd\nthat there is no signiÔ¨Åcant accuracy decrease if we employ\nHardswish [71].\nOther design choices. To validate the effectiveness of\nother design choices like relative positional encoding, IRB,\nand overlapping patch embedding, we add these compo-\nnents one by one to the baseline. Experimental results are\nshown in Table 9. As can be seen, relative positional encod-\ning has signiÔ¨Åcant improvement for both image classiÔ¨Åca-\ntion and semantic segmentation. With large pooling ratios,\nthe pooled features would have small scales, so relative\npositional encoding only needs negligible computational\ncost (5M Flops for the input size of 224 √ó224). An extra\ndepthwise convolution in the feed-forward network, i.e.,\nIRB, also shows signiÔ¨Åcant improvement, demonstrating the\nnecessity of the 2D locality enhancement. We further follow\n[29] to add overlapping patch embedding, and 0.2%/1.4%\nimprovement is observed for image classiÔ¨Åcation and se-\nmantic segmentation, respectively.\n5 C ONCLUSION\nThis paper introduces pyramid pooling into MHSA for\nalleviating the high computational cost of MHSA in the\nvision transformer. Compared with the strategy of applying\na single pooling operation in MHSA [21], [23], our pooling-\nbased MHSA not only reduces the sequence length but\nalso learns powerful contextual representations simultane-\nously via pyramid pooling. Equipped with the pooling-\nbased MHSA, we construct a new backbone network, called\nPyramid Pooling Transformer (P2T). To demonstrate the\neffectiveness of P2T, we conduct extensive experiments on\nseveral fundamental vision tasks, including image classiÔ¨Åca-\ntion, semantic segmentation, object detection, and instance\nsegmentation. Experimental results suggest that P2T signif-\nicantly outperforms previous CNN- and transformer-based\nbackbone networks.\nACKNOWLEDGEMENT\nThis work is supported in part by Major Project for New\nGeneration of AI under Grant No. 2018AAA0100400, in\npart by NSFC under Grant No. 61922046, in part by\nAlibaba Innovative Research (AIR) Program, in part by\nAlibaba Research Intern Program, and in part by the Agency\nfor Science, Technology and Research (A*STAR) under\nits AME Programmatic Funds (No. A1892b0026 and No.\nA19E3b0099).\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Å-\ncation with deep convolutional neural networks,‚Äù in Adv. Neural\nInform. Process. Syst., 2012, pp. 1097‚Äì1105.\n[2] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional net-\nworks for large-scale image recognition,‚Äù in Int. Conf. Learn. Rep-\nresent., 2015.\n[3] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, ‚ÄúGoing deeper with\nconvolutions,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog. , 2015, pp.\n1‚Äì9.\n[4] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for\nimage recognition,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog., 2016,\npp. 770‚Äì778.\n[5] G. Huang, Z. Liu, G. Pleiss, L. Van Der Maaten, and K. Weinberger,\n‚ÄúConvolutional networks with dense connectivity,‚Äù IEEE Trans.\nPattern Anal. Mach. Intell., 2019.\n[6] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, ‚ÄúSqueeze-and-\nexcitation networks,‚Äù IEEE Trans. Pattern Anal. Mach. Intell., vol. 42,\nno. 8, pp. 2011‚Äì2023, 2020.\n[7] M. Tan and Q. Le, ‚ÄúEfÔ¨ÅcientNet: Rethinking model scaling for\nconvolutional neural networks,‚Äù in Int. Conf. Mach. Learn. , 2019,\npp. 6105‚Äì6114.\n[8] Y.-H. Wu, Y. Liu, J. Xu, J.-W. Bian, Y. Gu, and M.-M. Cheng,\n‚ÄúMobileSal: Extremely efÔ¨Åcient RGB-D salient object detection,‚Äù\nIEEE Trans. Pattern Anal. Mach. Intell., 2021.\n[9] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and\nP . H. Torr, ‚ÄúRes2Net: A new multi-scale backbone architecture,‚Äù\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 43, no. 2, pp. 652‚Äì662,\n2021.\n[10] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., ‚ÄúImageNet\nlarge scale visual recognition challenge,‚Äù Int. J. Comput. Vis. , vol.\n115, no. 3, pp. 211‚Äì252, 2015.\n[11] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft COCO: Common objects in\ncontext,‚Äù in Eur. Conf. Comput. Vis., 2014, pp. 740‚Äì755.\n[12] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman, ‚ÄúThe pascal visual object classes (voc) challenge,‚Äù Int. J.\nComput. Vis., vol. 88, no. 2, pp. 303‚Äì338, 2010.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11\n[13] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, ‚ÄúThe cityscapes dataset\nfor semantic urban scene understanding,‚Äù in IEEE Conf. Comput.\nVis. Pattern Recog., 2016, pp. 3213‚Äì3223.\n[14] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n‚ÄúScene parsing through ADE20K dataset,‚Äù in IEEE Conf. Comput.\nVis. Pattern Recog., 2017, pp. 633‚Äì641.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Adv. Neural Inform. Process. Syst., 2017, pp. 6000‚Äì6010.\n[16] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‚ÄúEnd-to-end object detection with transformers,‚Äù in\nEur. Conf. Comput. Vis., 2020, pp. 213‚Äì229.\n[17] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‚ÄúDeformable\nDETR: Deformable transformers for end-to-end object detection,‚Äù\narXiv preprint arXiv:2010.04159, 2020.\n[18] R. Hu and A. Singh, ‚ÄúTransformer is all you need: Multimodal\nmultitask learning with a uniÔ¨Åed transformer,‚Äù arXiv preprint\narXiv:2102.10772, 2021.\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‚ÄúAn image is worth 16x16 words:\nTransformers for image recognition at scale,‚Äù in Int. Conf. Learn.\nRepresent., 2021.\n[20] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, ‚ÄúRethinking\nspatial dimensions of vision transformers,‚Äù in Int. Conf. Comput.\nVis., 2021, pp. 11 936‚Äì11 945.\n[21] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, ‚ÄúPyramid Vision Transformer: A versatile backbone\nfor dense prediction without convolutions,‚Äù in Int. Conf. Comput.\nVis., 2021.\n[22] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n‚ÄúSwin Transformer: Hierarchical vision transformer using shifted\nwindows,‚Äù in Int. Conf. Comput. Vis., 2021, pp. 10 012‚Äì10 022.\n[23] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and\nC. Feichtenhofer, ‚ÄúMultiscale vision transformers,‚Äù in Int. Conf.\nComput. Vis., 2021, pp. 6824‚Äì6835.\n[24] Y. Liu, Y.-H. Wu, G. Sun, L. Zhang, A. Chhatkuli, and L. Van Gool,\n‚ÄúVision transformers with hierarchical attention,‚Äù arXiv preprint\narXiv:2106.03180, 2021.\n[25] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\n‚ÄúSwinIR: Image restoration using Swin Transformer,‚Äù in Int. Conf.\nComput. Vis., 2021, pp. 1833‚Äì1844.\n[26] A. Kirillov, R. Girshick, K. He, and P . Doll ¬¥ar, ‚ÄúPanoptic feature\npyramid networks,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog., 2019,\npp. 6399‚Äì6408.\n[27] S. Xie, R. Girshick, P . Doll ¬¥ar, Z. Tu, and K. He, ‚ÄúAggregated\nresidual transformations for deep neural networks,‚Äù in IEEE Conf.\nComput. Vis. Pattern Recog., 2017, pp. 1492‚Äì1500.\n[28] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and\nC. Shen, ‚ÄúTwins: Revisiting the design of spatial attention in vision\ntransformers,‚Äù in Adv. Neural Inform. Process. Syst., 2021.\n[29] W. Wang, E. Xie, X. Li, D.-P . Fan, K. Song, D. Liang, T. Lu, P . Luo,\nand L. Shao, ‚ÄúPVTv2: Improved baselines with pyramid vision\ntransformer,‚Äù arXiv preprint arXiv:2106.13797, 2021.\n[30] O. Bojar, C. Buck, C. Federmann, B. Haddow, P . Koehn, J. Leveling,\nC. Monz, P . Pecina, M. Post, H. Saint-Amandet al., ‚ÄúFindings of the\n2014 workshop on statistical machine translation,‚Äù inThe Workshop\non Statistical Machine Translation, 2014, pp. 12‚Äì58.\n[31] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n‚ÄúCvT: Introducing convolutions to vision transformers,‚Äù in Int.\nConf. Comput. Vis., 2021, pp. 22‚Äì31.\n[32] B. Graham, A. El-Nouby, H. Touvron, P . Stock, A. Joulin, H. J¬¥egou,\nand M. Douze, ‚ÄúLeViT: a vision transformer in ConvNet‚Äôs clothing\nfor faster inference,‚Äù in Int. Conf. Comput. Vis. , 2021, pp. 12 259‚Äì\n12 269.\n[33] Q. Han, Z. Fan, Q. Dai, L. Sun, M.-M. Cheng, J. Liu, and\nJ. Wang, ‚ÄúDemystifying local vision transformer: Sparse con-\nnectivity, weight sharing, and dynamic weight,‚Äù arXiv preprint\narXiv:2106.04263, 2021.\n[34] K. Grauman and T. Darrell, ‚ÄúThe pyramid match kernel: Discrim-\ninative classiÔ¨Åcation with sets of image features,‚Äù in Int. Conf.\nComput. Vis., vol. 2. IEEE, 2005, pp. 1458‚Äì1465.\n[35] S. Lazebnik, C. Schmid, and J. Ponce, ‚ÄúBeyond bags of features:\nSpatial pyramid matching for recognizing natural scene cate-\ngories,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog. , vol. 2. IEEE,\n2006, pp. 2169‚Äì2178.\n[36] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúSpatial pyramid pooling in\ndeep convolutional networks for visual recognition,‚Äù IEEE Trans.\nPattern Anal. Mach. Intell., vol. 37, no. 9, pp. 1904‚Äì1916, 2015.\n[37] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, ‚ÄúPyramid scene parsing\nnetwork,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog., 2017, pp. 2881‚Äì\n2890.\n[38] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, ‚ÄúMobileNets: EfÔ¨Åcient\nconvolutional neural networks for mobile vision applications,‚Äù\narXiv preprint arXiv:1704.04861, 2017.\n[39] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n‚ÄúMobileNetV2: Inverted residuals and linear bottlenecks,‚Äù in IEEE\nConf. Comput. Vis. Pattern Recog., 2018, pp. 4510‚Äì4520.\n[40] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, ‚ÄúShufÔ¨ÇeNet v2: Practical\nguidelines for efÔ¨Åcient CNN architecture design,‚Äù in Eur. Conf.\nComput. Vis., 2018, pp. 116‚Äì131.\n[41] X. Zhang, X. Zhou, M. Lin, and J. Sun, ‚ÄúShufÔ¨ÇeNet: An extremely\nefÔ¨Åcient convolutional neural network for mobile devices,‚Äù in\nIEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 6848‚Äì6856.\n[42] M. Tan, B. Chen, R. Pang, V . Vasudevan, M. Sandler, A. Howard,\nand Q. V . Le, ‚ÄúMnasNet: Platform-aware neural architecture search\nfor mobile,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog. , 2019, pp.\n2820‚Äì2828.\n[43] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, ‚ÄúA survey of\nthe recent architectures of deep convolutional neural networks,‚Äù\nArtif. Intell. Review, vol. 53, no. 8, pp. 5455‚Äì5516, 2020.\n[44] W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, ‚ÄúA sur-\nvey of deep neural network architectures and their applications,‚Äù\nNeurocomputing, vol. 234, pp. 11‚Äì26, 2017.\n[45] H. Wang, Y. Zhu, H. Adam, A. Yuille, and L.-C. Chen, ‚ÄúMaX-\nDeepLab: End-to-end panoptic segmentation with mask trans-\nformers,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog., 2021, pp. 5463‚Äì\n5474.\n[46] R. Liu, Z. Yuan, T. Liu, and Z. Xiong, ‚ÄúEnd-to-end lane shape\nprediction with transformers,‚Äù in IEEE Winter Conf. Appl. Comput.\nVis., 2021, pp. 3694‚Äì3702.\n[47] J. Hu, L. Cao, L. Yao, S. Zhang, Y. Wang, K. Li, F. Huang,\nR. Ji, and L. Shao, ‚ÄúISTR: End-to-end instance segmentation with\ntransformers,‚Äù arXiv preprint arXiv:2105.00637, 2021.\n[48] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ¬¥egou, ‚ÄúTraining data-efÔ¨Åcient image transformers & distilla-\ntion through attention,‚Äù in Int. Conf. Mach. Learn., 2021, pp. 10 347‚Äì\n10 357.\n[49] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and\nS. Yan, ‚ÄúTokens-to-token ViT: Training vision transformers from\nscratch on ImageNet,‚Äù in Int. Conf. Comput. Vis., 2021, pp. 558‚Äì567.\n[50] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen,\n‚ÄúConditional positional encodings for vision transformers,‚Äù arXiv\npreprint arXiv:2102.10882, 2021.\n[51] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, X. Jin, A. Wang, and\nJ. Feng, ‚ÄúToken labeling: Training a 85.5% top-1 accuracy vision\ntransformer with 56M parameters on ImageNet,‚Äù arXiv preprint\narXiv:2104.10858, 2021.\n[52] Y. Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool, ‚ÄúLo-\ncalViT: Bringing locality to vision transformers,‚Äù arXiv preprint\narXiv:2104.05707, 2021.\n[53] K. Yuan, S. Guo, Z. Liu, A. Zhou, F. Yu, and W. Wu, ‚ÄúIncorpo-\nrating convolution designs into visual transformers,‚Äù in Int. Conf.\nComput. Vis., 2021, pp. 579‚Äì588.\n[54] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L.\nYuille, ‚ÄúDeeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs,‚Äù IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 40, no. 4, pp. 834‚Äì848, 2017.\n[55] M. M. K. Sarker, H. A. Rashwan, F. Akram, S. F. Banu, A. Saleh,\nV . K. Singh, F. U. Chowdhury, S. Abdulwahab, S. Romani,\nP . Radeva et al. , ‚ÄúSlsdeep: Skin lesion segmentation based on\ndilated residual and pyramid pooling networks,‚Äù in Med. Image.\nComput. Comput. Assist. Interv. Springer, 2018, pp. 21‚Äì29.\n[56] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, ‚ÄúOcnet:\nObject context for semantic segmentation,‚Äù Int. J. Comput. Vis., pp.\n1‚Äì24, 2021.\n[57] X. Lian, Y. Pang, J. Han, and J. Pan, ‚ÄúCascaded hierarchical atrous\nspatial pyramid pooling module for semantic segmentation,‚Äù Pat-\ntern Recognition, vol. 110, p. 107622, 2021.\n[58] D. Yoo, S. Park, J.-Y. Lee, and I. So Kweon, ‚ÄúMulti-scale pyramid\npooling for deep convolutional representation,‚Äù in Int. Conf. Com-\nput. Vis. Worksh., 2015, pp. 71‚Äì80.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12\n[59] S.-W. Kim, H.-K. Kook, J.-Y. Sun, M.-C. Kang, and S.-J. Ko, ‚ÄúPar-\nallel feature pyramid network for object detection,‚Äù in Eur. Conf.\nComput. Vis., 2018, pp. 234‚Äì250.\n[60] Z. Huang, J. Wang, X. Fu, T. Yu, Y. Guo, and R. Wang, ‚ÄúDc-spp-\nyolo: Dense connection and spatial pyramid pooling based yolo\nfor object detection,‚Äù Information Sciences , vol. 522, pp. 241‚Äì258,\n2020.\n[61] P . Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, ‚ÄúAmulet:\nAggregating multi-level convolutional features for salient object\ndetection,‚Äù in Int. Conf. Comput. Vis., 2017, pp. 202‚Äì211.\n[62] J.-R. Chang and Y.-S. Chen, ‚ÄúPyramid stereo matching network,‚Äù\nin IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 5410‚Äì5418.\n[63] J.-J. Liu, Q. Hou, Z.-A. Liu, and M.-M. Cheng, ‚ÄúPoolnet+: Ex-\nploring the potential of pooling for salient object detection,‚Äù IEEE\nTP AMI, 2022.\n[64] Y.-H. Wu, Y. Liu, L. Zhang, W. Gao, and M.-M. Cheng, ‚ÄúRegu-\nlarized densely-connected pyramid network for salient instance\nsegmentation,‚Äù IEEE Trans. Image Process. , vol. 30, pp. 3897‚Äì3907,\n2021.\n[65] D. Park, K. Kim, and S. Young Chun, ‚ÄúEfÔ¨Åcient module based\nsingle image super resolution for multiple problems,‚Äù in IEEE\nConf. Comput. Vis. Pattern Recog. Worksh., 2018, pp. 882‚Äì890.\n[66] H. Zhang, V . Sindagi, and V . M. Patel, ‚ÄúImage de-raining using\na conditional generative adversarial network,‚Äù IEEE Trans. Circ.\nSyst. Video Technol., vol. 30, no. 11, pp. 3943‚Äì3956, 2019.\n[67] H. Zhang and V . M. Patel, ‚ÄúDensely connected pyramid dehazing\nnetwork,‚Äù in IEEE Conf. Comput. Vis. Pattern Recog., 2018, pp. 3194‚Äì\n3203.\n[68] T. Wang, A. Borji, L. Zhang, P . Zhang, and H. Lu, ‚ÄúA stagewise\nreÔ¨Ånement model for detecting salient objects in images,‚Äù in Int.\nConf. Comput. Vis., 2017, pp. 4019‚Äì4028.\n[69] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù arXiv\npreprint arXiv:1607.06450, 2016.\n[70] Y. Dong, J.-B. Cordonnier, and A. Loukas, ‚ÄúAttention is not all you\nneed: Pure attention loses rank doubly exponentially with depth,‚Äù\narXiv preprint arXiv:2103.03404, 2021.\n[71] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan,\nW. Wang, Y. Zhu, R. Pang, V . Vasudevan et al. , ‚ÄúSearching for\nMobileNetV3,‚Äù in Int. Conf. Comput. Vis., 2019, pp. 1314‚Äì1324.\n[72] D. Hendrycks and K. Gimpel, ‚ÄúGaussian error linear units\n(GELUs),‚Äù arXiv preprint arXiv:1606.08415, 2016.\n[73] P . Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,\n‚ÄúMulti-scale vision Longformer: A new vision transformer for\nhigh-resolution image encoding,‚Äù in Int. Conf. Comput. Vis. , 2021,\npp. 2998‚Äì3008.\n[74] T.-Y. Lin, P . Goyal, R. Girshick, K. He, and P . Doll ¬¥ar, ‚ÄúFocal loss\nfor dense object detection,‚Äù in Int. Conf. Comput. Vis. , 2017, pp.\n2980‚Äì2988.\n[75] K. He, G. Gkioxari, P . Doll ¬¥ar, and R. Girshick, ‚ÄúMask R-CNN,‚Äù\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 42, no. 2, pp. 386‚Äì397,\n2020.\n[76] I. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regulariza-\ntion,‚Äù in Int. Conf. Learn. Represent., 2017.\n[77] X. Glorot and Y. Bengio, ‚ÄúUnderstanding the difÔ¨Åculty of training\ndeep feedforward neural networks,‚Äù in Int. Conf. Artif. Intell. Stat.,\n2010, pp. 249‚Äì256.\n[78] M. Contributors, ‚ÄúMMSegmentation: OpenMMLab semantic\nsegmentation toolbox and benchmark,‚Äù https://github.com/\nopen-mmlab/mmsegmentation, 2020.\n[79] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,\nZ. Liu, J. Xu et al., ‚ÄúMMDetection: Open MMLab detection toolbox\nand benchmark,‚Äù arXiv preprint arXiv:1906.07155, 2019.\nYu-Huan Wu is currently a Ph.D. candidate\nwith the TMCC, College of Computer Science\nat Nankai University, supervised by Prof. Ming-\nMing Cheng. He received his bachelor‚Äôs degree\nfrom Xidian University in 2018. His research\ninterests include computer vision and machine\nlearning.\nYun Liu received his bachelor‚Äôs degree and his\ndoctoral degree from Nankai University in 2016\nand 2020, respectively. His Ph.D. supervisor was\nProf. Ming-Ming Cheng. Then, he worked with\nProf. Luc Van Gool for one and a half years\nas a postdoctoral scholar at Computer Vision\nLab, ETH Zurich. Currently, he is a scientist at\nInstitute for Infocomm Research (I2R), A*STAR.\nHis research interests include computer vision\nand machine learning.\nXin Zhan received his bachelor‚Äôs and doctoral\ndegrees from USTC in 2010 and 2015, respec-\ntively. Currently, he works as a researcher of\nAlibaba DAMO Academy. His research interests\ninclude perception for autonomous driving.\nMing-Ming Cheng received his PhD degree\nfrom Tsinghua University in 2012. Then he did\ntwo years research fellow with Prof. Philip Torr\nin Oxford. He is now a professor at Nankai Uni-\nversity, leading the Media Computing Lab. His\nresearch interests include computer graphics,\ncomputer vision, and image processing. He re-\nceived research awards, including ACM China\nRising Star Award, IBM Global SUR Award, and\nCCF-Intel Y oung Faculty Researcher Program.\nHe is on the editorial boards of IEEE TPAMI/TIP .",
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.9124017953872681
    },
    {
      "name": "Transformer",
      "score": 0.6930669546127319
    },
    {
      "name": "Computer science",
      "score": 0.6461737751960754
    },
    {
      "name": "Segmentation",
      "score": 0.6378724575042725
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.563129723072052
    },
    {
      "name": "Artificial intelligence",
      "score": 0.519515872001648
    },
    {
      "name": "Motif (music)",
      "score": 0.45491623878479004
    },
    {
      "name": "Computer vision",
      "score": 0.3765837550163269
    },
    {
      "name": "Engineering",
      "score": 0.14572390913963318
    },
    {
      "name": "Mathematics",
      "score": 0.10091057419776917
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}