{
  "title": "Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model",
  "url": "https://openalex.org/W3103939752",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5077260647",
      "name": "Bugeun Kim",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5038247010",
      "name": "Kyung Seo Ki",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5022465625",
      "name": "Donggeon Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5041905573",
      "name": "Gahgene Gweon",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W2905122540",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2758343314",
    "https://openalex.org/W2962927633",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W1539746312",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W2963705839",
    "https://openalex.org/W3020084238",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2561975193",
    "https://openalex.org/W2252202991",
    "https://openalex.org/W2828541159",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2930786691",
    "https://openalex.org/W2417863416",
    "https://openalex.org/W2963199195",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W2962970011",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2251349042",
    "https://openalex.org/W2975059944"
  ],
  "abstract": "Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output. However, such a neural model suffered two issues: expression fragmentation and operand-context separation. To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations. The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS. Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS. The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation. (2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3768–3779,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n3768\nPoint to the Expression: Solving Algebraic Word Problems using the\nExpression-Pointer Transformer Model\nBugeun Kim Kyung Seo Ki Donggeon Lee Gahgene Gweon\nDepartment of Transdisciplinary Studies,\nSeoul National University,\nSeoul, Republic of Korea\n{cd4209,kskee88,lsw5835,ggweon}@snu.ac.kr\nAbstract\nSolving algebraic word problems has recently\nemerged as an important natural language pro-\ncessing task. To solve algebraic word prob-\nlems, recent studies suggested neural mod-\nels that generate solution equations by using\n‘Op (operator/operand)’ tokens as a unit of\ninput/output. However, such a neural model\nsuffered two issues: expression fragmentation\nand operand-context separation. To address\neach of these two issues, we propose a pure\nneural model, Expression-Pointer Transformer\n(EPT), which uses (1) ‘Expression’ token and\n(2) operand-context pointers when generat-\ning solution equations. The performance of\nthe EPT model is tested on three datasets:\nALG514, DRAW-1K, and MAWPS. Com-\npared to the state-of-the-art (SoTA) models,\nthe EPT model achieved a comparable perfor-\nmance accuracy in each of the three datasets;\n81.3% on ALG514, 59.5% on DRAW-1K,\nand 84.5% on MAWPS. The contribution of\nthis paper is two-fold; (1) We propose a\npure neural model, EPT, which can address\nthe expression fragmentation and the operand-\ncontext separation. (2) The fully automatic\nEPT model, which does not use hand-crafted\nfeatures, yields comparable performance to\nexisting models using hand-crafted features,\nand achieves better performance than existing\npure neural models by at most 40%.\n1 Introduction\nSolving algebraic word problems has recently\nbecome an important research task in that auto-\nmatically generating solution equations requires\nunderstanding natural language. Table 1 shows\na sample algebraic word problem, along with\ncorresponding solution equations that are used\nto generate answers for the problem. To solve\nsuch problems with deep learning technology,\nresearchers recently suggested neural models that\ngenerate solution equations automatically (Huang\nProblem One number is eight more than\ntwice another and their sum is 20.\nWhat are their numbers?\nNumbers 1(‘one’), 8(‘eight’), 2(‘twice’), 20.\nEquations x0 −2x1 = 8, x0 + x1 = 20\nAnswers (16,4)\nTable 1: A sample algebraic word problem\net al., 2018; Amini et al., 2019; Chiang and Chen,\n2019; Wang et al., 2019). However, suggested\nneural models showed a fairly large performance\ngap compared to existing state-of-the-art models\nbased on hand-crafted features in popular algebraic\nword problem datasets, such as ALG514 (44.5% for\npure neural model vs. 83.0% for using hand-crafted\nfeatures) (Huang et al., 2018; Upadhyay and Chang,\n2016). To address the large performance gap in\nthis study, we propose a larger unit of input/output\n(I/O) token called “Expressions” for a pure neural\nmodel. Figure 1 illustrates conventionally used “Op\n(operator/operands)” versus our newly proposed\n“Expression” token.\nTo improve the performance of pure neural\nmodels that can solve algebraic word problems, we\nidentiﬁed two issues that can be addressed using\nExpression tokens, which are shown in Figure\n1: (1) expression fragmentation and (2) operand-\ncontext separation. First, the expression fragmen-\ntation issue is a segmentation of an expression\ntree, which represents a computational structure\nof equations that are used to generate a solution.\nThis issue arises when Op, rather than the whole\nexpression tree, is used as an input/output unit\nof a problem-solving model. For example, as\nshown in Figure 1 (a), using Op tokens as an\ninput to a problem-solving model disassembles a\ntree structure into operators (“ ×”) and operands\n(“x1” and “2”). Meanwhile, we propose using the\n3769\nFigure 1: Illustration using the word problem in Table 1 for the (a) expression fragmentation issue, (b) operand-\ncontext separation issue, and (c) our solution for these two issues.\n“Expression” (×(x1,2)) token, which can explicitly\ncapture a tree structure as a whole, as shown in\nFigure 1 (c).\nThe second issue of operand-context separation\nis the disconnection between an operand and\na number that is associated with the operand.\nThis issue arises when a problem-solving model\nsubstitutes a number stated in an algebraic word\nproblem into an abstract symbol for generalization.\nAs shown in Figure 1 (b), when using an Op token,\nthe number 8 is changed into an abstract symbol\n‘N1’. Meanwhile, when using an Expression token,\nthe number 8 is not transformed into a symbol.\nRather a pointer is made to the location where the\nnumber 8 occurred in an algebraic word problem.\nTherefore, using such an “operand-context pointer”\nenables a model to access contextual information\nabout the number directly, as shown in Figure 1 (c);\nthus, the operand-context separation issue can be\naddressed.\nIn this paper, we propose a pure neural model\ncalled Expression-Pointer Transformer (EPT) to\naddress the two issues above. The contribution of\nthis paper is two-fold;\n1. We propose a pure neural model, Expression-\nPointer Transformer (EPT), which can address\nthe expression fragmentation and operand-\ncontext separation issues.\n2. The EPT model is the ﬁrst pure neural model\nthat showed comparable accuracy to the exist-\ning state-of-the-art models, which used hand-\ncrafted features. Compared to the state-of-\nthe-art pure neural models, the EPT achieves\nbetter performance by about 40%.\nIn the rest of the paper, we introduce existing\napproaches to solve algebraic word problems\nin Section 2. Next, Section 3 introduces our\nproposed model, EPT, and Section 4 reports the\nexperimental settings. Then in Section 5, results of\ntwo studies are presented. Section 5.1 presents a\nperformance comparison between EPT and existing\nSoTA models. Section 5.2 presents an ablation\nstudy examining the effects of Expression tokens\nand applying operand-context pointers. Finally, in\nSection 6, a conclusion is presented with possible\nfuture directions for our work.\n2 Related work\nOur goal is to design a pure neural model that\ngenerates equations using ‘Expression’ tokens to\nsolve algebraic word problems. Early attempts\nfor solving algebraic word problems noted the\nimportance of Expressions in building models with\nhand-crafted features (Kushman et al., 2014; Roy\net al., 2015; Roy and Roth, 2015; Zhou et al., 2015;\nUpadhyay et al., 2016). However, recent neural\nmodels have only utilized ‘Op (operator/operand)’\ntokens (Wang et al., 2017; Amini et al., 2019;\nChiang and Chen, 2019; Huang et al., 2018; Wang\net al., 2019), resulting in two issues: (1) the\nexpression fragmentation issue and (2) the operand-\ncontext separation issue. In the remaining section,\nwe present existing methods for tackling each of\nthese two issues.\nTo address the expression fragmentation issue,\nresearchers tried to reﬂect relational information\nbetween operators and operands either by using a\ntwo-step procedure or a single step with sequence-\nto-sequence models. Earlier attempts predicted\noperators and their operands by using a two-step\nprocedure. Such early models selected operators\nﬁrst by classifying a predeﬁned template (Kushman\net al., 2014; Zhou et al., 2015; Upadhyay et al.,\n2016), then in the second step, operands were ap-\nplied to the template selected in the ﬁrst step. Other\nmodels selected operands ﬁrst before constructing\nexpression trees with operators in the second step\n(Roy et al., 2015; Roy and Roth, 2015). However,\nsuch two-step procedures in these early attempts\n3770\nInput Output Expression token Meaning\nposition index Operator ( fi) Operand 0 ( ai0) Operand 1 ( ai1)\n0 - BEGIN (Start an equation)\n1 R0 VAR (Generate variable x0)\n2 R1 VAR (Generate variable x1)\n3 R2 × 2 R1 2x1\n4 R3 − R0 R2 x0 −2x1\n5 R4 = R3 8 x0 −2x1 = 8\n6 R5 + R0 R1 x0 + x1\n7 R6 = R5 20 x0 + x1 = 20\n- R7 END (Gather all equations)\nTable 2: The Expression token sequence for x0 −2x1 = 8and x0 + x1 = 20\ncan be performed via a single-step procedure with\nneural models. Speciﬁcally, recent attempts have\nutilized sequence-to-sequence (seq2seq) models as\na single-step procedure to learn the implicit rela-\ntionship between operators and operands (Amini\net al., 2019; Chiang and Chen, 2019; Wang et al.,\n2019). For example, to capture the operator-\noperand relationship, Chiang and Chen (2019)\nconstructed a seq2seq model that used push/pop\nactions on a stack for generating operator/operand\ntokens. Similarly, Amini et al. (2019) built a\nseq2seq model to generate an operator token right\nafter producing required operand tokens. However,\nalthough these seq2seq approaches consider rela-\ntional information of operands when generating\noperators, the approach still does not address\nthe problem of lacking relation information of\noperators when generating operands. On the other\nhand, by using Expression token, our model can\nconsider relational information when generating\nboth operator and operands.\nSecondly, there were efforts to address the\noperand-context separation issue. To utilize contex-\ntual information of an operand token, researchers\nbuilt hand-crafted features that capture the semantic\ncontent of a word, such as the unit of a given num-\nber (Roy and Roth, 2015; Koncel-Kedziorski et al.,\n2015; Zhou et al., 2015; Upadhyay et al., 2016;\nRoy and Roth, 2017) or dependency relationship\nbetween numbers (Kushman et al., 2014; Zhou\net al., 2015; Upadhyay et al., 2016). However,\ndevising hand-crafted input features was time-\nconsuming and required domain expertise. There-\nfore, recent approaches have employed distributed\nrepresentations and neural models to learn numeric\ncontext of operands automatically (Wang et al.,\n2017; Huang et al., 2018; Chiang and Chen, 2019;\nAmini et al., 2019). For example, Huang et al.\n(2018) used a pointer-generator network that can\npoint to the context of a number in a given math\nproblem. Although Huang’s model can address the\noperand-context separation issue using pointers,\ntheir pure neural model did not yield a comparable\nperformance to the state-of-the-art model using\nhand-crafted features (44.5% vs. 83.0%). In this\npaper, we propose that by including additional\npointers that utilize the contextual information\nof operands and neighboring Expression tokens,\nperformance of pure neural models can improve.\n3 EPT: Expression-Pointer Transformer\nFigure 2 shows the proposed Expression-Pointer\nTransformer (EPT) 1 model, which adopts the\nencoder-decoder architecture of a Transformer\nmodel (Vaswani et al., 2017). The EPT utilizes\nthe ALBERT model (Lan et al., 2019), a pretrained\nlanguage model, as the encoder. The encoder input\nis tokenized words of the given word problem, and\nencoder output is the encoder’s hidden-state vectors\nthat denote numeric contexts of the given problem.\nAfter obtaining the encoder’s hidden-state vec-\ntors from the ALBERT encoder, the transformer\ndecoder generates ‘Expression’ tokens. The two\ndecoder inputs are Expression tokens and the\nALBERT encoder’s hidden-state vectors, which\nare used as memories. For the given example\nproblem, the input is a list of 8 Expression tokens\nshown in Table 2. We included three special\ncommands in the list: VAR (generate a variable),\nBEGIN (start an equation), and END (gather all\nequations). Following the order speciﬁed in the list\nof Table 2, the EPT receives one input Expression\n1The code is available on https://github.com/\nsnucclab/ept.\n3771\nFigure 2: The architecture of Expression-Pointer Transformer (EPT) where two ideas applied: (1) Expression\ntoken and (2) operand-context pointer.\nat a time. For the ith Expression input, the model\ncomputes an input vector vi. The EPT’s decoder\nthen transforms this input vector to a decoder’s\nhidden-state vector di. Finally, the EPT predicts\nthe next Expression token by generating the next\noperator and operands simultaneously.\nTo produce ‘Expression’ tokens, two compo-\nnents are modiﬁed from the vanilla Transformer:\ninput vector and output layer. In the following\nsubsections, we explain the two components.\n3.1 Input vector of EPT’s decoder\nThe input vector vi of ith Expression token is\nobtained by combining operator embedding fi and\noperand embedding aij as follows:\nvi = FFin (Concat (fi,ai1,ai2,···,aip)) , (1)\nwhere FF∗indicates a feed-forward linear layer,\nand Concat(·) means concatenation of all vectors\ninside the parentheses. All the vectors, including\nvi, fi, and aij, have the same dimension D. For-\nmulae for computing the two types of embedding\nvectors, fi and aij are stated in the next paragraph.\nFor the operator token fi of ith Expression, the\nEPT computes the operator embedding vector fi as\nin Vaswani et al. (2017)’s setting:\nfi = LNf (cfEf(fi) + PE(i)) , (2)\nwhere E∗(·) indicates a look-up table for embed-\nding vectors, c∗denotes a scalar parameter, and\nLN∗(·) and PE(·) represent layer normalization\n(Ba et al., 2016) and positional encoding (Vaswani\net al., 2017), respectively.\nThe embedding vector aij, which represents\nthe jth operand of ith Expression, is calculated\ndifferently according to the operand aij’s source.\nTo reﬂect contextual information of operands, three\npossible sources are utilized: problem-dependent\nnumbers, problem-independent constants, and the\nresult of prior Expression tokens. First, problem-\ndependent numbers are numbers provided in an\nalgebraic problem (e.g., ‘20’ in Table 1). To\ncompute aij of a number, we reuse the encoder’s\nhidden-state vectors corresponding to such number\ntokens as follows:\naij = LNa\n(\ncaunum + eaij\n)\n, (3)\nwhere u∗denotes a vector representing the source,\nand eaij is the encoder’s hidden-state vector cor-\nresponding to the number aij.2 Second, problem-\nindependent constants are predeﬁned numbers that\nare not stated in the problem (e.g., 100 is often used\nfor percentiles). To compute aij of a constant, we\nuse a look-up table Ec as follows:\naij = LNa (cauconst + Ec(aij)) . (4)\nNote that LNa, ca are shared across different\nsources. Third, the result of the prior Expression\ntoken is an Expression generated before the ith\nExpression (e.g., R0). To compute aij of a result,\nwe utilize the positional encoding as follows3:\naij = LNa (cauexpr + PE(k)) , (5)\n2When two or more tokens form a number in the problem,\nwe averaged all related hidden-state vectors.\n3Since we want to sustain simultaneous decoding, which\nis one of the strengths in the Transformer, we use PE(k) for\nthe kth prior Expression, although it is possible to use decoder\nhidden state dk.\n3772\nwhere kis the index where the prior Expressionaij\ngenerated.\n3.2 Output layer of EPT’s decoder\nThe output layer of the EPT’s decoder predicts\nthe next operator fi+1 and operands ai+1,j si-\nmultaneously when the ith Expression token is\nprovided. First, the next operator,fi+1, is predicted\nas follows:\nfi+1 = arg max\nf\nσ(f|FFout(di)), (6)\nwhere σ(k|x) is the probability of selecting an\nitem k under a distribution following the output\nof softmax function, σ(x).\nSecond, to utilize the context of operands when\npredicting an operand, the output layer applies\n‘operand-context pointers,’ inspired by the pointer\nnetworks (Vinyals et al., 2015). In the pointer\nnetworks, the output layer predicts the next token\nusing attention over candidate vectors. The EPT\ncollects candidate vectors for the next (i+ 1)th\nExpression in three different ways depending on\nthe source of operands:\nek for the kth number in the problem,\ndk for the kth Expression output,\nEc(x) for a constant x\n(7)\nThen the EPT predicts the next jth operand ai+1,j,\nas follows. Let Aij be a matrix whose row vectors\nare such candidates. Then, the EPT predicts ai+1,j\nby computing attention of a query vector Qij on a\nkey matrix Kij, as follows.\nQij = FF query,j(di), (8)\nKij = FF key,j(Aij), (9)\nai+1,j = arg max\na\nσ\n(\na\n⏐⏐⏐⏐⏐\nQijK⊤\nij√\nD\n)\n.(10)\nAs the output layer is modiﬁed to predict an\noperator and its operands simultaneously, we also\nmodiﬁed the loss function. We compute the loss\nof an Expression by summing up the loss of an\noperator and the loss of required arguments. All\nloss functions are computed using cross-entropy\nwith the label smoothing approach (Szegedy et al.,\n2016).\n4 Experimental Setup\n4.1 Metric and Datasets\nThe metric for measuring the EPT model’s perfor-\nmance is answer accuracy, which is the proportion\nALG514 DRAW-1K MAWPS\nDataset size\nProblems 514 1,000 2,373\nSplits 5-fold Train 600 5-fold\nDev., Test 200\nComplexity of generating equations (per problem)\nUnknown 1.82 1.75 1.00\nOp tokens 13.08 14.16 6.20\nComplexity of selecting an operand (per problem)\nNumbers 4.26 3.88 2.72\nExpressions 7.45 7.95 3.60\nTable 3: Characteristics of datasets used in the experi-\nment\nof correctly answered problems over the entire set\nof problems. We regard a problem is correctly\nanswered if a solution to the generated equations\nmatches the correct answer without considering the\norder of answer-tuple, as in Kushman et al. (2014).\nTo obtain a solution to the generated equations, we\nuse SymPy (Meurer et al., 2017) at the end of the\ntraining phase.\nFor the datasets, we use three publicly avail-\nable English algebraic word problem datasets 4:\nALG514 (Kushman et al., 2014) 5, DRAW-1K\n(Upadhyay and Chang, 2016) 6, and MAWPS\n(Koncel-Kedziorski et al., 2016) 7. The three\ndatasets differ in terms of size and complexity, as\nshown in Table 3. The high-complexity datasets,\nALG514 and DRAW-1K, require more expressions\nand unknowns when solving the algebraic problems\nthan the low-complexity dataset, MAWPS. For\nDRAW-1K, we report the accuracy of a model on\nthe development and test set since training and\ndevelopment sets are provided. For the other two\ndatasets — MAWPS and ALG514, — we report the\naverage accuracy and standard error using 5-fold\ncross-validation.\n4.2 Baseline and ablated models\nWe examine the performance of EPT against ﬁve\nexisting state-of-the-art (SoTA) models. The ﬁve\nmodels are categorized into three types; model\nusing hand-crafted features, pure neural models,\n4We provide a preprocessed version of these datasets\non https://github.com/snucclab/ept/tree/\nmaster/dataset.\n5http://groups.csail.mit.edu/rbg/code/\nwordprobs/\n6https://www.microsoft.com/en-us/\ndownload/details.aspx?id=52628\n7http://lang.ee.washington.edu/MAWPS\n3773\nand a hybrid of these two types.\n•Models using hand-crafted features use expert-\ndeﬁned input features without using a neural\nmodel: MixedSP (Upadhyay et al., 2016).\nUpadhyay et al. (2016) designed a model\nusing a set of hand-crafted features similar\nto those used by Zhou et al. (2015). Using a\ndata augmentation technique, they achieved\nthe SoTA on ALG514 (83.0%) and DRAW-1K\n(59.5%).\n•Pure neural models take algebraic word prob-\nlems as the raw input to a neural model\nand do not require the use of a rule-based\nmodel: CASS-RL (Huang et al., 2018) and\nT-MTDNN (Lee and Gweon, 2020). The\nCASS-RL, which applied pointer-generator\nnetworks to generate Op tokens, achieved the\nbest-performing neural model on ALG514\n(44.5%). The T-MTDNN is the SoTA model\non MAWPS (78.88%) dataset. T-MTDNN\nutilized multi-task learning for training a\ntemplate classiﬁcation model and a number\naligning model.\n•Hybrid models are models that are neither\npurely hand-crafted nor pure neural models:\nCASS-hybrid (Huang et al., 2018) and DNS\n(Wang et al., 2019). The CASS-hybrid is the\nbest-performing hybrid model of the CASS-\nRL and Huang et al. (2017)’s model using\nhand-crafted features. The DNS is a hybrid\nmodel of a sequence-to-sequence model and a\nmodel using hand-crafted features. We copied\nthe accuracy of DNS on DRAW-1K from\nZhang et al. (2019).\nAfter examining the EPT model performance,\nwe conducted an ablation study to analyze the\neffect of using two main components of EPT;\nExpression tokens and operand-context pointers.\nWe compared three types of models to test each of\nthe components: (1) the vanilla Transformer model,\n(2) the Transformer with Expression token model,\nwhich investigates the effect of using Expression\ntokens, and (3) the EPT, which investigates the\neffect of using pointers in addition to Expression\ntokens. Additional details on the input/output of\nthe vanilla Transformer and the Transformer with\nExpression token models are provided in Appendix\nA.\n4.3 Implementation details\nThe implementation details of EPT and its ablated\nmodels are as follows. To build encoder-decoder\nmodels, we used PyTorch 1.5 (Paszke et al.,\n2019). For the encoder, three different sizes of\nALBERT models in the transformers\nlibrary (Wolf et al., 2019) are used:\nalbert-base-v2, albert-large-v2,\nand albert-xlarge-v2. We ﬁxed the\nencoder’s embedding matrix during the training\nsince such ﬁxation preserves the world knowledge\nembedded in the matrix and stabilizes the entire\nlearning process. For the decoder, we stacked\nsix decoder layers and shared the parameters\nacross different layers to reduce memory usage.\nWe set the dimension of input vector D as the\nsame dimension of encoder hidden-state vectors.\nTo train and evaluate the entire model, we used\nteacher forcing in the training phase and beam\nsearch with 3 beams in the evaluation phase.\nFor the hyperparameters of the EPT, parameters\nfollow the ALBERT model’s parameters except\nfor training epoch, batch size, warm-up epoch,\nand learning rate. First, for the training epoch\nT, a model is trained in 500, 500, and 100\nepochs on ALG514, DRAW-1K, and MAWPS,\nrespectively. For batch sizes, we used 2,048\n(albert-base-v2 and albert-large-v2)\nand 1,024 ( albert-xlarge-v2) in terms of\nOp or Expression tokens. To acquire a similar\neffect of using 4,096 tokens as a batch, we also\nemployed gradient accumulation technique on two\ntypes of consecutive mini-batches; two ( base\nand large) and four ( xlarge). Then, for the\nwarm-up epoch and learning rate, we conduct the\ngrid-search algorithm for each pair of a dataset\nand the size of the ALBERT model. For the grid\nsearch, we set the sampling space as follows:\n{0.00125,0.00176,0.0025}for the learning rates\nand {0,0.005T,0.01T,0.015T,0.02T,0.025T}\nfor the warm-up. The resulting parameters are\nlisted in Appendix B. During each grid search,\nwe only use the following training/validation\nsets and keep other sets unseen: the fold-0\ntraining/test split for ALG514 and MAWPS and\nthe training/development set for DRAW-1K. For\nthe unstated hyperparameters, the parameters\nfollow those of the ALBERT. These parameters\ninclude the optimizer and warm-up scheduler; we\nused LAMB (You et al., 2019) optimizer with\nβ1 = 0.9, β2 = 0.999, and ϵ = 10−12; and we\n3774\nModel ALG DRAW-1K MAWPS\n514 (Dev.) (Test)\nState of the art (SOTA)\nHand-crafted 83.0[M] 59.5[M] —\nPure neural 44.5 [C] — 78.9[T]\nEnsembles 82.5 [H] 31.0[D] —\nExpression-Pointer Transformer\nEPT (B) 75.46 55.5 51.5 83.41\n(Std. Err) (2.23) (0.32)\nEPT (L) 81.31 63.5 59.0 84.51\n(Std. Err) (1.88) (1.37)\nEPT (XL) — * 60.5 59.5 — *\nNote: [M]MixedSP,[C]CASS-RL, [T]T-MTDNN,\n[H]CASS-hybrid, [D]DNS. *Overﬁtted on some folds.\nTable 4: Accuracy(%) of the EPT and existing models.\n(B), (L), and (XL) indicate albert-base-v2,\nalbert-large-v2, and albert-xlarge-v2.\nemployed linear decay with warm-up scheduling.\nAll the experiment, including hyperparameter\nsearch, was conducted on a local computer with\n64GB RAM and two GTX1080 Ti GPUs.\n5 Result and Discussion\nIn section 5.1, we ﬁrst present a comparison\nstudy, which examines the EPT’s performance.\nNext, in section 5.2, we present an ablation study,\nwhich analyzes the two main components of EPT;\nExpression tokens and operand-context pointers.\n5.1 Comparison study\nAs shown in Table 4, the performance of EPT\nis comparable or better in terms of performance\naccuracy compared to existing state-of-the-art\n(SoTA) models when tested on the three datasets\nof ALG514, DRAW-1K, and MAWPS. The fully\nautomatic EPT model, which does not use hand-\ncrafted features, yields comparable performance\nto existing models using hand-crafted features.\nSpeciﬁcally, on the ALG514 dataset, the EPT\noutperforms the best-performing pure neural model\nby about 40% and shows comparable performance\naccuracy to the SoTA model that uses hand-crafted\nfeatures. On the DRAW-1K dataset, which is\nharder than ALG514 dataset, a similar performance\ntrend to ALG514 is found. The EPT model outper-\nforms the hybrid model by about 30% and achieved\ncomparable accuracy to the SoTA model that uses\nhand-crafted features. On the MAWPS dataset,\nwhich is only tested on pure neural models in\nModel ALG DRAW-1K MAWPS\n514 (Dev.) (Test)\nVanilla Transfo. 27.52 14.5 24.0 79.83\n(Std. Err) (4.39) (1.03)\n+ Expression 42.03 32.0 32.5 80.46\n(Std. Err) (1.97) (1.09)\n+ Pointer (EPT) 75.46 55.5 51.5 83.41\n(Std. Err) (2.23) (0.32)\nTable 5: Accuracy(%) of the EPT and its ablated\nmodels (albert-base-v2).\nexisting studies, the EPT achieves SoTA accuracy.\nOne possible explanation for EPT’s outstanding\nperformance over the existing pure neural model is\nthe use of operand’s contextual information. Exist-\ning neural models solve algebraic word problems\nby using symbols to provide an abstraction of\nproblem-dependent numbers or unknowns. For\nexample, Figure 1 shows that existing methods\nused Op tokens, such as x0 and N1. However,\ntreating operands as symbols only reﬂects 2 out\nof 4 means in which symbols are used in hu-\nmans’ mathematical problem-solving procedures\n(Usiskin, 1999). The 4 means of symbol usage\nare; (1) generalizing common patterns, (2) repre-\nsenting unknowns in an equation, (3) indicating an\nargument of a function, and (4) replacing arbitrary\nmarks. By applying template classiﬁcation or\nmachine learning techniques, (1) and (2) were\nsuccessfully utilized in existing neural models.\nHowever, the existing neural models could not\nconsider (3) and (4). Therefore, in our suggested\nEPT model, we dealt with (3) by using Expression\ntokens and (4) by using operand-context pointers.\nWe suspect that the EPT’s performance, which is\ncomparable to existing models using hand-crafted\nfeatures, comes from dealing with (3) and (4)\nexplicitly when solving algebraic word problems.\n5.2 Ablation study\nFrom the ablation study, our data showed that\nthe two components of generating ‘Expression’\ntoken and applying operand-context pointer, each\nimproved the accuracy of the EPT model in\ndifferent ways. Speciﬁcally, as seen in Table 5,\nadding Expression token to the vanilla Transformer\nimproved the performance accuracy by about 15%\nin ALG514 and DRAW-1K and about 1% in\nMAWPS. In addition, applying operand-context\npointer to the Transformer with Expression token\n3775\nCase 1.\nEffect of\nusing\nExpression\ntokens\nProblem The sum of two numbers is 90. Three times the smaller is 10 more\nthan the larger. Find the larger number.\nExpected 3x0 −x1= 10 , x0 + x1= 90\nVanilla Transformer x0 + x1= 3 , x0 −x1= 10 (Incorrect)\n+ Expression 3x0 −x1= 10 , x0 + x1= 90 (Correct)\n+ Pointer (EPT) 3x0 −x1= 10 , x0 + x1= 90 (Correct)\nCase 2.\nEffect of\nusing\npointers\nProblem A minor league baseball team plays 130 games in a season. If the\nteam won 14 more than three times as many games as they lost,\nhow many wins and losses did the team have?\nExpected x0 −3x1= 14 , x0 + x1=130\nVanilla Transformer 14x0 −3x1= 0 , x0 + x1=130 (Incorrect)\n+ Expression x0 −3x1= 14 , 130x0 −x1= 0 (Incorrect)\n+ Pointer (EPT) x0 −3x1= 14 , x0 + x1=130 (Correct)\nCase 3.\nCompara-\ntive\nerror\nProblem One number is 6 more than another. If the sum of the smaller\nnumber and 3 times the larger number is 34, ﬁnd the two numbers.\nExpected x0 + 3x1= 34 , x1 −x0= 6\nVanilla Transformer x0 + 3x1= 34 , x1 −x0= 6 (Correct)\n+ Expression 3x0 + 34x1= 2 , x1 −x0= 6 (Incorrect)\n+ Pointer (EPT) 3x0 + x1= 34 , x1 −x0= 6 (Incorrect)\nCase 4.\nTemporal\norder error\nProblem The denominator of a fraction exceeds the numerator by 7. if the\nnumerator is increased by three and the denominator increased by\n5, the resulting fraction is equal to half. Find the original fraction.\nExpected x0 −1\n2 x1=1\n2 ·5 −3, x0 −x1= 7\nVanilla Transformer 3x0 + 5x1= 1\n2 N4, ( Incorrect)\n+ Expression 3x0 −5x1= 0 , x0 + x1= 7 (Incorrect)\n+ Pointer (EPT) 5x0 −3x1= 1\n2 , x1 −x0= 7 (Incorrect)\nTable 6: Sample incorrect problems (albert-base-v2) from the DRAW-1K development dataset.\nmodel enhanced the performance by about 30% in\nALG514 and DRAW-1K and about 3% in MAWPS.\nTable 6 shows the result of an error analysis.\nThe cases 1 and 2 show how the EPT model’s\ntwo components contributed to performance im-\nprovement. In case 1, the vanilla Transformer\nyields an incorrect solution equation by incorrectly\nassociating x0 + x1 and 3. However, using an\nExpression token, the explicit relationship between\noperator and operands is maintained, enabling the\ndistinction between x0 +x1 and 3x0 −x1. The case\n2 example shows how adding an operand-context\npointer can help distinguish between different\nexpressions, in our example, x0, 130x0, and 14x0.\nAs the operand-context pointer directly points to\nthe contextual information of an operand, the EPT\ncould utilize the relationship between unknown\n(x0) and its multiples ( 130x0 or 14x0) without\nconfusion.\nWe observed that the existing pure neural\nmodel’s performance on low-complexity dataset of\nMAWPS was relatively high at 78.9%, compared\nto that of high-complexity dataset of ALG514\n(44.5%). Therefore, using Expression tokens and\noperand-context pointers contributed to higher\nperformance when applied to high-complexity\ndatasets of ALG514 and DRAW-1K, as shown in\nTable 5. We suspect two possible explanations for\nsuch a performance enhancement.\nFirst, using Expression tokens in high-\ncomplexity datasets address the expression\nfragmentation issue when generating solution\nequations, which is more complex in ALG514 and\nDRAW-1K than MAWPS. Speciﬁcally, Table 3\nshows that on average the number of unknowns\nin ALG514 and DRAW-1K is almost twice\n(1.82 and 1.75, respectively) than MAWPS (1.0).\nSimilarly, the number of Op tokens is also twice\nin ALG514 and DRAW-1K (13.08 and 14.16,\nrespectively) than that of MAWPS (6.20). As\nthe expression fragmentation issue can arise for\neach token, probability of fragmentation issues’\noccurrence increases exponentially as the number\nof unknowns/Op tokens in a problem increases.\n3776\nTherefore, the vanilla Transformer model, which\ncould not handle the fragmentation issue, yields\nlow accuracy on high-complexity datasets.\nSecond, using operand-context pointers in high-\ncomplexity datasets addresses the operand-context\nseparation issue when selecting an operand, which\nis more complex in ALG514 and DRAW-1K than\nMAWPS. Speciﬁcally, Table 3 shows that on\naverage the amount of Expression tokens is also\ntwice in ALG514 and DRAW-1K (7.45 and 7.95,\nrespectively) than that of MAWPS (3.60). As\nnumbers and Expression tokens are candidates\nfor selecting an operand, probability of separation\nissues’ occurrence increases linearly as the amount\nof numbers/Expressions in an equation increases.\nSince a Transformer with Expression token could\nnot handle the separation issue, the model showed\nlower accuracy on high-complexity datasets.\nIn addition to the correctly solved problem\nexamples, Table 6 also shows cases 3 and 4, which\nwere incorrectly answered by the EPT model. The\nerroneous examples can be categorized into two\ngroups; ‘Comparative’ error and ‘Temporal order’\nerror. ‘Comparative’ occurs when an algebraic\nproblem contains comparative phrases, such as ‘6\nmore than,’ as in case 3. 49.3% of incorrectly\nsolved problems contained comparatives. When\ngenerating solution equations for the comparative\nphrases, the order of arguments is a matter for an\nequation that contains non-commutative operators,\nsuch as subtractions or divisions. Therefore,\nerrors occurred when the order of arguments\nfor comparative phrases with non-commutative\noperators was mixed up. Another group of error is\n‘Temporal order’ error that occurs when a problem\ncontains phrases with temporal orders, such as\n‘the numerator is increased by three,’ as in case\n4. 44.5% of incorrectly solved problems contained\ntemporal orders. We suspect that these problems\noccur when co-referencing is not handled correctly.\nIn a word problem with temporal ordering, a same\nentity may have two or more numeric values that\nchange over time. For example, in case 4, the\ndenominator has two different values of x1 and\nx1 + 7. The EPT model failed to assign a same\nvariable for the denominators. The model assigned\nx0 in the former expression and x1 in the latter.\n6 Conclusion\nIn this study, we proposed a neural algebraic word\nproblem solver, Expression-Pointer Transformer\n(EPT), and examined its characteristics. We\ndesigned EPT to address two issues: expression\nfragmentation and operand-context separation. The\nEPT resolves the expression fragmentation issue by\ngenerating ‘Expression’ tokens, which simultane-\nously generate an operator and required operands.\nIn addition, the EPT resolves the operand-context\nseparation issue by applying operand-context point-\ners. Our work is meaningful in that we demon-\nstrated a possibility for alleviating the costly pro-\ncedure of devising hand-crafted features in the\ndomain of solving algebraic word problems. As\nfuture work, we plan to generalize the EPT to other\ndatasets, including non-English word problems\nor non-algebraic domains in math, to extend our\nmodel.\nAcknowledgments\nThis work was supported by the National\nResearch Foundation of Korea (NRF) grant\nfunded by the Korea government (MSIT) (No.\n2020R1C1C1010162).\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable\nmath word problem solving with operation-based\nformalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2357–2367, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nTing-Rui Chiang and Yun-Nung Chen. 2019.\nSemantically-aligned equation generation for\nsolving and reasoning math word problems.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2656–2668, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nDanqing Huang, Jing Liu, Chin-Yew Lin, and Jian\nYin. 2018. Neural math word problem solver with\nreinforcement learning. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 213–223, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nDanqing Huang, Shuming Shi, Chin-Yew Lin, and Jian\nYin. 2017. Learning ﬁne-grained expressions to\n3777\nsolve math word problems. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 805–814, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1152–1157, San Diego, California. Association for\nComputational Linguistics.\nNate Kushman, Yoav Artzi, Luke Zettlemoyer, and\nRegina Barzilay. 2014. Learning to automatically\nsolve algebra word problems. In Proceedings of\nthe 52nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 271–281, Baltimore, Maryland. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations.\nD. Lee and G. Gweon. 2020. Solving arithmetic word\nproblems with a templatebased multi-task deep neu-\nral network (t-mtdnn). In 2020 IEEE International\nConference on Big Data and Smart Computing (Big-\nComp), pages 271–274.\nAaron Meurer, Christopher P. Smith, Mateusz Pa-\nprocki, Ond ˇrej ˇCert´ık, Sergey B. Kirpichev,\nMatthew Rocklin, AMiT Kumar, Sergiu Ivanov,\nJason K. Moore, Sartaj Singh, Thilina Rathnayake,\nSean Vig, Brian E. Granger, Richard P. Muller,\nFrancesco Bonazzi, Harsh Gupta, Shivam Vats,\nFredrik Johansson, Fabian Pedregosa, Matthew J.\nCurry, Andy R. Terrel, ˇStˇep´an Rou ˇcka, Ashutosh\nSaboo, Isuru Fernando, Sumith Kulal, Robert Cimr-\nman, and Anthony Scopatz. 2017. Sympy: symbolic\ncomputing in python. PeerJ Computer Science ,\n3:e103.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alch ´e-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 32 , pages 8024–8035. Curran\nAssociates, Inc.\nSubhro Roy and Dan Roth. 2015. Solving general\narithmetic word problems. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing , pages 1743–1752, Lisbon,\nPortugal. Association for Computational Linguis-\ntics.\nSubhro Roy and Dan Roth. 2017. Unit dependency\ngraph and its application to arithmetic word problem\nsolving. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence , AAAI’17,\npage 3082–3088. AAAI Press.\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-\ning about quantities in natural language. Transac-\ntions of the Association for Computational Linguis-\ntics, 3:1–13.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nThe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nShyam Upadhyay and Ming-Wei Chang. 2016. An-\nnotating derivations: A new evaluation strategy\nand dataset for algebra word problems. CoRR,\nabs/1609.07197.\nShyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang,\nand Wen-tau Yih. 2016. Learning from explicit\nand implicit supervision jointly for algebra word\nproblems. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 297–306, Austin, Texas. Association for\nComputational Linguistics.\nZalman Usiskin. 1999. Algebraic Thinking, Grades\nK-12: Readings from NCTM’s School-Based Jour-\nnals and Other Publications , chapter Conceptions\nof School Algebra and Uses of Variables. National\nCouncil of Teachers of Mathematics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural\nInformation Processing Systems 28 , pages 2692–\n2700. Curran Associates, Inc.\nLei Wang, Dongxiang Zhang, Jipeng Zhang, Xing Xu,\nLianli Gao, Bing Tian Dai, and Heng Tao Shen.\n2019. Template-based math word problem solvers\nwith recursive neural networks. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, pages 7144–7151.\nYan Wang, Xiaojiang Liu, and Shuming Shi. 2017.\nDeep neural solver for math word problems. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n845–854, Copenhagen, Denmark. Association for\nComputational Linguistics.\n3778\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Fun-\ntowicz, and Jamie Brew. 2019. Huggingface’s\ntransformers: State-of-the-art natural language pro-\ncessing. ArXiv, abs/1910.03771.\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reducing\nBERT pre-training time from 3 days to 76 minutes.\nCoRR, abs/1904.00962.\nD. Zhang, L. Wang, L. Zhang, B. T. Dai, and H. T.\nShen. 2019. The gap of semantic parsing: A\nsurvey on automatic math word problem solvers.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, pages 1–1.\nLipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015.\nLearn to solve algebra word problems using\nquadratic programming. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 817–822, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nA Input/output of ablation models\nIn this section, we describe how we compute the\ninput and output of the two ablation models: (1) a\nvanilla Transformer and (2) a vanilla Transformer\nwith ‘Expression’ tokens. Figure 3 shows the two\nmodels.\nThe ﬁrst ablation model is a vanilla Transformer.\nThe model generates an ‘Op’ token sequence and\ndoes not use operand-context pointers. The model\nmanages an ‘Op’ token vocabulary that contains\noperators, constants, variables, and number place-\nholders (e.g., N0). So the input of this model’s\ndecoder only utilizes a look-up table for embedding\nvectors. For the decoder’s output, the vanilla\nTransformer uses a feed-forward softmax layer to\noutput the probability of selecting an Op token. In\nsummary, the input vector vi of a token ti and the\noutput ti+1 can be computed as follows.\nvi = LN in (cinEin(ti) + PE(i)) , (11)\nti+1 = arg max\nt\nσ(FFout(di))t . (12)\nThe second ablation model is a vanilla Trans-\nformer model that uses ‘Expression’ tokens as\na unit of input/output. This model generates an\n‘Expression’ token sequence but does not apply\noperand-context pointers. Instead of using operand-\ncontext pointers, this model uses an operand\nvocabulary that contains constants, placeholders for\nnumbers, and placeholders of previous Expression\ntoken results (e.g., R0). The input of this model’s\ndecoder is similar to that of EPT’s decoder, but we\nreplaced the equations 3 and 5 with the following\nformulae.\naij = LN a (caunum + Ec(aij)) , (13)\naij = LN a (cauexpr + Ec(aij)) . (14)\nFor the output of this model’s decoder, we used\na feed-forward softmax layer to output the proba-\nbility of selecting an operand. Since the softmax\noutput can select the unavailable operand, we set\nthe probability of such unavailable tokens as zeros\nto mask them. So, we replace equation 10 with the\nfollowing formula.\nai+1,j = arg max\na\nσ(a|M (FFj(di))) , (15)\nwhere M is a masking function to set zero probabil-\nity on unavailable tokens when generating ith Op\ntoken. The other unstated equations 1, 2, 4, and 6\nremain the same.\nB Hyperparameters used for this study\nTable 7 shows the best parameters and perfor-\nmances on the development set, which are found\nusing grid search.\n3779\nFigure 3: The architecture of two ablated model of EPT: a vanilla Transformer and a Transformer using Expression\ntokens\nModel # of Hyper-parameters Performance\nParams Learning Rate Warm-up on Dev.\nALG514 dataset\nEPT (B) 25.29M .00176 2.0% (10.0 epochs) 78.43\n(L) 41.85M .0025 2.5% (12.5 epochs) 81.37\n(XL) 155.30M .00176 1.0% ( 5.0 epochs) 83.33\nDRAW-1K dataset\nEPT (B) 25.29M .00176 2.5% (12.5 epochs) 58.5\n(L) 41.86M .00176 0.5% ( 2.5 epochs) 63.5\n(XL) 155.31M .00176 1.0% ( 5.0 epochs) 60.5\nMAWPS dataset\nEPT (B) 25.30M .0025 1.0% ( 1.0 epoch ) 83.33\n(L) 41.87M .0025 0.0% ( 0.0 epoch ) 83.97\n(XL) 155.33M .00176 2.5% ( 2.5 epochs) 83.97\nTable 7: Best performing hyperparameters for each pair of a model and a dataset. (B), (L), and (XL) indicate\nalbert-base-v2, albert-large-v2, and albert-xlarge-v2.",
  "topic": "Operand",
  "concepts": [
    {
      "name": "Operand",
      "score": 0.8334860801696777
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.7434031367301941
    },
    {
      "name": "Computer science",
      "score": 0.7237665057182312
    },
    {
      "name": "Transformer",
      "score": 0.5825761556625366
    },
    {
      "name": "Artificial neural network",
      "score": 0.5793402194976807
    },
    {
      "name": "Algebraic expression",
      "score": 0.5372393727302551
    },
    {
      "name": "Expression (computer science)",
      "score": 0.5177878141403198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4809693396091461
    },
    {
      "name": "Security token",
      "score": 0.44766709208488464
    },
    {
      "name": "Arithmetic",
      "score": 0.40746232867240906
    },
    {
      "name": "Theoretical computer science",
      "score": 0.378667950630188
    },
    {
      "name": "Algebraic number",
      "score": 0.36208564043045044
    },
    {
      "name": "Natural language processing",
      "score": 0.3243144154548645
    },
    {
      "name": "Algorithm",
      "score": 0.3226723074913025
    },
    {
      "name": "Programming language",
      "score": 0.30005112290382385
    },
    {
      "name": "Mathematics",
      "score": 0.14850980043411255
    },
    {
      "name": "Engineering",
      "score": 0.07102036476135254
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ],
  "cited_by": 28
}