{
  "title": "The Ethics of (Non)disclosure: Large Language Models in Professional, Nonacademic Writing Contexts",
  "url": "https://openalex.org/W4390239010",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Nicholls State University, 906 E. 1st St., Thibodaux, LA 70301, United States of America",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2591911168",
      "name": "Erick Piller",
      "affiliations": [
        "Nicholls State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4309672151",
    "https://openalex.org/W2550819555",
    "https://openalex.org/W4318931874",
    "https://openalex.org/W224267383",
    "https://openalex.org/W4365452292",
    "https://openalex.org/W3112103703",
    "https://openalex.org/W2081643732",
    "https://openalex.org/W4413601550",
    "https://openalex.org/W4363676214",
    "https://openalex.org/W2911994232",
    "https://openalex.org/W1969294738",
    "https://openalex.org/W6608831991",
    "https://openalex.org/W4295308792",
    "https://openalex.org/W1982777730",
    "https://openalex.org/W4377865883",
    "https://openalex.org/W2890242742",
    "https://openalex.org/W3004422001",
    "https://openalex.org/W4295308973",
    "https://openalex.org/W2026040721",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W2165146738",
    "https://openalex.org/W4371783954",
    "https://openalex.org/W3196125461",
    "https://openalex.org/W6666785207",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W6680008453",
    "https://openalex.org/W4413518523",
    "https://openalex.org/W6629019002",
    "https://openalex.org/W6760576710",
    "https://openalex.org/W4303182349",
    "https://openalex.org/W1486015272",
    "https://openalex.org/W3207853461",
    "https://openalex.org/W3160283368",
    "https://openalex.org/W2064248319"
  ],
  "abstract": "This article explores the ethics of co-writing with large language models such as GPT-4 in professional, non-academic writing contexts without disclosing the practice to stakeholders. It considers five ethical concepts through an analysis of a hypothetical scenario. Three of the concepts—transparency, data practices, and expanded circulation—originate in the work of Heidi McKee and James Porter. The other two, just price and risk imposition, have particular relevance for professional writers. The article ultimately proposes that these five concepts can serve as points of reference as we attempt to formulate and articulate ethical judgments about co-writing with generative AI in specific, contextually grounded instances.",
  "full_text": "Journal Impact Factor (JIF)™ 2022: 0.2 | 5 Year: 0.2 | Source: Journal Citation Report™ 2023 (Clarivate Analytics)  \n   \nISSN 0975-2935 | Indexed in the Web of Science Core Collection™ Emerging Sources Citation Index (ESCI) \n https://doi.org/10.21659/rupkatha.v15n4.02 | Volume 15, Number 4, 2023 | Dynamic Impact  \n \nResearch article \nArticle History: Received: 13 September 2023. Revised: 19 September 2023. Accepted: 22 October 2023. Published: 23 October 2023 \nCopyright: © 2023 by the author/s. License Aesthetix Media Services, India. Distributed under the terms and conditions of the Creative \nCommons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). \nCitation: Piller, Erick. 2023. The Ethics of (Non)disclosure: Large Language Models in Professional, Nonacademic Writing Contexts. \nRupkatha Journal 15:4. https://doi.org/10.21659/rupkatha.v15n4.01 \n \nThe Ethics of (Non)disclosure: Large Language Models in \nProfessional, Nonacademic Writing Contexts \n \nErick Piller   \nNicholls State University, 906 E. 1st St., Thibodaux, LA 70301, United States of America \n \n \nAbstract \nThis article explores the ethics of co-writing with large language models such as GPT-4 in professional, non-\nacademic writing contexts without disclosing the practice to stakeholders. It considers five ethical concepts \nthrough an analysis of a hypothetical scenario. Three of the concepts —transparency, data practices, and \nexpanded circulation—originate in the work of Heidi McKee and James Porter. The other two, just price and \nrisk imposition, have particular relevance for professional writers. The article ultimately proposes that these \nfive concepts can serve as points of reference as we attempt to formulate and articulate ethical judgments \nabout co-writing with generative AI in specific, contextually grounded instances. \n \nKeywords: artificial intelligence, co-writing, ethics, large language models \n \n Quality Education \n \n \n1. Introduction \nIn late November 2022, OpenAI released a free “research preview” of ChatGPT, a natural \nlanguage generation (NLG) model that could produce fluent, coherent texts in English \nand other languages. NLG models had already existed for some time, including the set of \nmodels collectively known as GPT -3, the immediate precursor to the model that initi ally \npowered ChatGPT, GPT-3.5. However, in most cases, these models were far less impressive \nthan ChatGPT. They were also less accessible to the general public. Until just weeks before \nthe release of ChatGPT’s research preview, those who wanted to use GPT-3 were required \nto join a waitlist. \nThere was no such limitation for ChatGPT. According to Roose (2022a), its release \nmarked “the first time such a powerful tool [had] been made available to the general \n\nRupkatha 15:4 2023 | Page 2 of 27 \npublic through a free, easy -to-use web interface.” Th e research preview went viral, \ncrossing the threshold of one million users within five days of its release and exceeding \nits servers’ capacity on multiple occasions (Walsh, 2022). Within weeks,  Harvard Business \nReview (Mollick, 2022) had already called Cha tGPT a “tipping point for AI” (artificial \nintelligence). A Bloomberg (Savov, 2022) headline suggested that ChatGPT “could be AI’s \niPhone moment.” \nThese developments have presented new challenges for researchers and users of \nlarge language models, who now have to contend with ethical questions about how they \nshould be employed in everyday life. Unsurprisingly, concerns about academic \nmisconduct have escalated in the wake of ChatGPT’s public release, as the platform’s \nsophisticated language generation capabil ities present the possibility of AI -assisted \n“plagiarism” or the wholesale creation of artificially generated content to complete \nassignments. The New York City Department of Education, for example, has blocked \naccess to ChatGPT on its networks at all the city’s public schools (Yang, 2023). One article \non the use of GPT models in education (De Visé and Lonas, 2023) claims, “In four short \nmonths, the GPT family of artificial intelligence chatbots have upended higher education \nlike nothing since the arrival o f Wi-Fi connections in classrooms,” while another article’s \n(Ulanoff, 2023) opening paragraph quips: “Higher education, we have a big, ChatGPT \nproblem.” A  Fox Business  (Nelson, 2023) headline wonders whether ChatGPT “will […] \nimpact the ‘integrity’ of acad emic institutions,” as a headline in  The Atlantic  (Marche, \n2022) proclaims, “The College Essay Is Dead.” Some writing studies scholars have pushed \nback against this “moral panic” —such as Josh Eyler, whom McMurtrie (2022) quotes in \na Chronicle of Higher Edu cation piece. However, just a few brief paragraphs after \nacknowledging that Eyler and other scholars have urged us “to begin conversations with \nstudents and colleagues about how to shape and harness these AI tools as an aide, rather \nthan a substitute, for learning,” McMurtrie abruptly —and apparently with no sense of \nirony—asks: “So how does a writing instructor, or a professor in a writing-intensive course, \nreduce the likelihood that students will use these AI tools?” \nIn response to these concerns, “AI dete ctors” have been developed to identify \nmachine-generated text. The first released was created by a college senior at Princeton \nUniversity, who felt motivated “to fight what he sees as an increase in AI plagiarism” \n(Bowman, 2023). Detectors have proliferated since then, and now even Turnitin, a widely \nused service for detecting plagiarism in academic contexts, has incorporated AI detection \n(Turnitin, n.d.). \nAlthough concerns about academic misconduct have seemed to predominate, \nthere has also been discussion  of the broader implications of the use of AI -generated \ncontent in nonacademic contexts. One case that has drawn attention occurred in March \n2023, when a judge presiding over a murder trial in India’s Punjab and Haryana High Court \nconsulted GPT-4 about whether the defendant should be granted bail (Katje, 2023). The \nPiller | Page 3 of 27 \n \nlanguage model suggested that bail should be rejected —advice that Judge Anoop \nChitkara ultimately followed (Katje, 2023). Some other ethical concerns have revolved \naround inherent biases (Ferrara, 2023), the intellectual property rights of creators whose \nwork is used in training (Ray, 2023), and the fact that a large language model’s \n“capabilities could lower costs of disinformation campaigns” —as a (2019) report by \nOpenAI researchers puts it (Solaiman et al., p. 6). \nThere has also been some discussion of the use of large language models in \nprofessional and technical communication settings (Biswas, 2023; Castellanos -Gomez, \n2023; Knowles, 2022). Zhang et al. (2022) have noted that “automatic product copywriting \ngeneration has become an important line of research in e-commerce” (p. 12423), and the \nrange of professional applications for large language models increases seemingly daily. \nIn these contexts, the ethical considerations of using AI language mo dels like GPT-4 are \nmore nuanced, as the line between collaboration and deception —if the use of AI goes \nundisclosed—becomes increasingly blurred. \nGovernments worldwide have explored possibilities for regulating the use of AI, \nwith the European Union recently approving the AI Act and at least seventeen U.S. states \nintroducing AI -related legislation (Levin & Downes, 2023). Relevant regulation would \nobviate the practical necessity of ethical judgment for individuals such as professional \nwriters, transforming t he question of whether to employ large language models into a \npredominantly legal, not ethical, consideration. However, present regulatory initiatives \nhave tended to focus on aspects such as data privacy and discrimination —through \nalgorithmic bias in AI -powered hiring or underwriting systems, for example. The Biden \nadministration’s seventy-three-page Blueprint for an AI Bill of Rights (White House Office \nof Science and Technology Policy, 2022) does not mention the word “writing” a single \ntime and never refe rs specifically to large language models. Furthermore, while AI \nregulation is “inevitable” (Levin & Downes, 2023), Schuett (2023) points out that regulators \nhave encountered basic difficulties in even defining the material scope of such regulation, \ngiven that the term “artificial intelligence” lacks a generally accepted definition and can \nrefer to a variety of different types of systems. For now, in the absence of any relevant \nlegislation, in an environment of widespread uncertainty and skepticism about AI and \nongoing conversations about the limitations of large language models, whether to use—\nand disclose the use of —a large language model when generating text is an ethical \ndilemma that the professional writer who wishes to employ such technology must \nnavigate, often on a daily basis. \nIn this article, I will explore the ethics of “co-writing” with AI language models such \nas GPT-4—particularly the ethical implications of not disclosing a language model’s role \nin the composing process in professional, nonacadem ic contexts. I will begin by \npresenting a scenario that, as I have found in discussions with students and colleagues, \nhas generated a range of ethical interpretations, critiques, and justifications. Then, after a \nRupkatha 15:4 2023 | Page 4 of 27 \nreview of the literature, I will analyze th e scenario, considering it in light of other \nscholarship on the rhetorical ethics of AI writing, as well as other relevant ethical principles \ndiscussed outside of writing studies. \n \n2. Scenario \nI propose to explore the ethics of co -writing with language models such as GPT -4 in \nprofessional contexts through an analysis of the following case study —hypothetical, but \nbased on actual events that I learned about during my research, and broadly emblematic \nof what it often means to write professionally in our part of the twenty-first century. \nImagine that a publisher plans to release a series of “test prep” books marketed to \nindividuals preparing to take real estate licensing examinations in various US states to \ncompete with a similar line of books sold b y another publisher. Rather than \ncommissioning individual authors with subject matter expertise for the series, the \npublisher hires a company that specializes in producing learning content. As specified in \nthe contract, this company delivers drafts of completed manuscripts; then, subject matter \nexperts (SMEs) hired by the publisher review the manuscripts, fact -checking them and \nproviding feedback. The learning company revises the manuscripts accordingly, and the \nreview cycle continues until the manuscripts have been deemed satisfactory. The \npublisher pays a fixed price for each manuscript. \nA nondisclosure agreement prohibits the learning company from divulging the \ndetails of this process —or even the company’s own involvement in the project. The \npublished books are not presented as the work of any particular individual. Rather, it is \nthe publisher that is listed as each book’s “author.” \nTo write the manuscripts, the learning company contracts about a dozen freelance \nwriters and pays them hourly, w ith the expectation that they will each write, on average, \na certain number of words per hour. These freelance writers have no expertise in real \nestate regulation. They “write” their assigned sections of the manuscript by heavily \nparaphrasing the competing publisher’s line of books, paragraph by paragraph—or, more \noften, sentence by sentence. The publisher is fully aware of this aspect of the production \nprocess; in fact, it was the publisher’s direction, though not included in the formal contract \nwith the l earning company. The use of competitor source material, together with the \nrounds of revision based on feedback from the SMEs, helps ensure the accuracy and \ncomprehensiveness of the final manuscripts, and editors employed full -time by the \nlearning company bring consistency within and across the multi-authored texts. As a final \nquality assurance measure, the learning company’s editors run each manuscript through \na plagiarism detector, comparing it against not only the general database but also the \ncompetitor’s source material, and rewrite any passages that have too high of a similarity \nscore. \nPiller | Page 5 of 27 \n \nWith this relatively complicated scenario in place, we can consider the case of three \nfreelance writers hired by the learning company: Heidi, Emily, and Kemp. All three h ave \nrealized, independently, that they can increase their output dramatically by prompting a \ntransformer-based language model (such as OpenAI’s GPT-3.5 or GPT-4 or Google’s Bard) \nin a way similar to the following and feeding it competitor source material: \nI will give you “input text” preceded by “INPUT:” \nFor the output, please substantially rewrite the inputted passage. Change the \noverall arrangement by combining and reordering sentences where possible. \nCombine sentences, or break longer individual sentences into multiple sentences, \nwhenever you can. \nNever copy more than three words in a row from the input —preferably no more \nthan two words in a row. Also, as much as possible, avoid repeating major words \nfrom the input in the output, except for key terms that cannot be avoided. \nIt is acceptable to omit some minor details or add minor details as necessary. If the \norder of a list (whether bulleted or in a sentence) is unimportant, please rearrange \nthe list items. \nBegin your output with “OUTPUT:” \nA reader should not be able to tell that the output is closely based on the input. \nDo you understand? \nNone of the writers simply pastes in the outputted text. They always read over it, \ncompare it to the source material, and rewrite any apparent errors or awkward phrasings. \nThey also restructure the outputted text and delete phrases and sentences as they deem \nnecessary. Still, a substantial portion of the words that they deliver to the learning \ncompany comes from the language model—a fact that none of the writers discloses. \nAlthough Heidi, Emily, and Kemp all have similar processes, there are some \nimportant differences:  \n• Heidi produces just above the required words per hour, usually in about a quarter \nof an hour, and spends the remaining part of each hour working for other clients.  \n• Emily devotes the entire hour to her work for the learning company, meaning her \noverall productivity is three or four times the minimum requirement.  \n• Kemp takes the same approach as Emily, but his output is lower than Emily’s—only \n1.5 or two times  the minimum requirement per hour. This is because he runs the \noutputted text through two recently developed web apps that can determine, with \nreasonable (but not total) certainty, whether a passage of at least moderate length \nwas generated by a language m odel. If either of the two web apps can say with \nRupkatha 15:4 2023 | Page 6 of 27 \nmore than 50 percent certainty that the text was generated in this way, Kemp \nrewrites it, to the littlest extent possible, to bring it under the 50 percent threshold. \n \nFigure 1 \n \nThough I suspect several aspects of this case study could be fruitfully analyzed \nfrom an ethical standpoint, I will focus on the questions raised by the writers’ extensive \nuse of language models. However, rather than jumping immediately into analysis of th e \nscenario, I will first review other writing studies scholarship on the rhetorical ethics of AI \nwriting, to contextualize the discussion to follow. \n \n3. Writing Studies and Artificial Intelligence \nCompositionists have studied the potential applications of AI for writing since at least \n1979, when Burns defended his dissertation,  Stimulating Rhetorical Invention in English \nComposition through Computer -Assisted Instruction. Burns (1979) sought to “comb ine \nthe fruits of the rhetorical renaissance in English composition” —that is, the revived \ninterest in rhetorical invention —“with this developing technology of instructional \ncomputing” (p. 11). To this end, he programmed computer -assisted instruction (CAI) \n“modules” that a writer could use during prewriting to apply one of three heuristic \nframeworks to the subject matter: Aristotle’s  topoi, Burke’s dramatistic pentad, and \nYoung, Becker, and Pike’s tagmemics (Burns, 1979, p. 18). He named these modules \nTOPOI, BURKE, and TAGI, respectively (Kemp, 1987). i At the same time, Bell Laboratories \nwas developing the UNIX WRITER’S WORKBENCH, a set of tools designed to help with \nvarious writing tasks, which, today, is generally considered the first “grammar checker” \n(Heidorn, 2000, p. 181). In 1981 –1982, 170 students in composition courses at Colorado \nState University (CSU) piloted the software (Smith et al., 1984). By late 1984, over 2,000 \n\nPiller | Page 7 of 27 \n \nCSU students were using the WRITER’S WORKBENCH to assist them in their writing (Smith \net al., 1984). \nCompositionists developed additional CAI software for writing soon after WRITER’s \nWORKBENCH and Burns’ invention modules were released. For instance, Wresch’s (1984) \nedited collection,  The Computer in Composition Instruction: A Writer’s  Tool, includes \nwhole chapters devoted to HOMER, WANDAH, Wordsworth II, COMP -LAB, and other \nprograms—plus references to a number of others. This trend of creating programs for \nwriting assistance, often specifically for instruction in composition, endured f or some \nyears. In 1988, for example, Phillips and Erlwein published an article on their COMPOS-1, \nwhich, as they put it, “integrates a machine and a human operator in a series of decisions \nculminating in the composition of an outline of a speech” (p. 250).  Nor were scholars’ \ncontributions exclusively tools. AI inspired theoretical work during this period as well, such \nas Hunter’s (1991) article “Rhetoric and Artificial Intelligence,” which claims that “artificial \nintelligence, as it has developed over the l ast twenty to thirty years, is of considerable \nimportance to the study of rhetoric” (p. 317). \nGenerally, though, compositionists’ interest in the possibilities of AI for writing \ndiminished in the 1990s and remained low until the mid-2010s—a phenomenon typically \nattributed to unmet expectations and decreased funding for AI research and development \nacross all fields (Knowles, 2022). Following this “AI winter,” the mid -2010s saw renewed \ninterest in artificial intelligence. This was the result of several elemen ts working in \ncombination: new developments in machine learning, the increased affordability of \ngraphics processing units (GPUs) and cloud computing resources, and the availability of \nextensive datasets that could be used for training, thanks to the growth  of the internet \nand digitization. High -profile real-life applications of AI, such as virtual assistants (think \nSiri and Alexa) and self -driving vehicles, also garnered significant attention, leading to a \nsubstantial increase in funding for research and development from the public and private \nsectors. \nIn early June 2018, OpenAI released what we now know as GPT -1. Less than two \nweeks later, the  Routledge Handbook of Digital Writing and Rhetoric  was published, a \ncollection edited by Alexander and Rhodes. In this collection of essays, most of the \ncontributors show their awareness of “machinically composed texts” (to use Yancey’s \nterm), but they take reasonable caution in their assessments and predictions for NLG \ntechnologies: “What this composing means—for our composing, for that of our students, \nfor the field, for the country: that has yet to be determined” (Yancey, 2018, pp. 69 –70). \nHowever, one contributor, Hart-Davidson (2018), shows no reluctance to speculate on the \nfuture roles of AI in writing  processes. In the chapter “Writing with Machines and Other \nCuriosities of the Age of Machine Rhetoric,” he anticipates that “so much of the writing \nthat folks do day to day is [so] routine that it will be quickly replaced by robots. Soon. No \nRupkatha 15:4 2023 | Page 8 of 27 \nlooking back” (p. 252). In less routine writing, he predicts, AI will provide the “first drafts,” \nand human writers will “take over” from there (p. 252). \nCompositionists have continued to research AI and its implications for writing over \nthe past several years. For exa mple, Duin and Pedersen (2021) discuss NLG in Writing \nFutures: Collaborative, Algorithmic, Autonomous. Knowles (2022) has begun studying the \npractices of professional writers who use Jasper—originally a fine-tuned model of GPT-3 \nfor producing digital marketing copy—and presented on the project at the IEEE ProComm \nConference. Now that large language models have become better known and more \nwidely available following the release of ChatGPT, compositionists should probably expect \na deluge of scholarship on AI-assisted writing. \n \n4. The Ethics of Writing with AI \nBut to engage with writing studies scholarship on the  ethics of AI writing means, more \nthan anything, to engage with the work of McKee and Porter, who have consistently \npublished and presented on the topic since at least 2017, a year before OpenAI released \nGPT-1. That year, McKee and Porter observed that artificially intelligent “agents” were \nbecoming an important feature of the professional writing landscape and explored some \nof the ethical questions raised by this development (p. 167). The following year, in a post \nfor a Sweetland Digital Rhetoric Collaborative blog carnival, they discussed the \nimplications of language models for academic writing and called on scholars and teachers \nof writing generally to play a role in “important conversations shaping the ethical use of \nwriting bots” (McKee & Porter, 2018). This blog post was prescient, appearing nearly five \nyears before the release of OpenAI’s “research preview” of ChatGPT. \nThe Good Machine Speaking Well \nWhen they approach rhetorical ethics in these earlier works, at least in nonacademic, \nprofessional communication contexts, McKee and Porter (2018) focus on “the good \nmachine speaking well”—a play on Quintilian’s concept of the  vir bonus dicendi peritus, \nthe good man speaking well (p.  15). A major obstacle to the realization of this ideal, \naccording to McKee and Porter, is the poor ability of artificially intelligent agents to read \nand effectively respond to rhetorical situations. ii They give Microsoft’s Tay as an \nexample—the Twitterbot shut down just sixteen hours after its release in early 2016 \nbecause it had begun to tweet racist, sexist, homophobic, anti-Semitic, and other offensive \ncontent. \nThe shortcoming of this approach is th at it looks primarily at the “machine” itself, \nas an autonomous agent. Consider how anthropomorphically McKee and Porter describe \nTay in (2020) conference proceedings: \nPiller | Page 9 of 27 \n \nShe was a naïf, with immature ethical standards and limited rhetorical intelligence, \nlacking skills and safeguards for how to communicate ethically in social media. In \nshort, she was not rhetorically savvy and thus was not an effective communicator, \nat least in the ways her creators intended. (p. 111) \nIt can be useful in many contexts to thin k of bots like Tay or even Siri or Alexa as \n“moral agents,” especially as shorthand for critiques of the ethical failures inherent in \ntheir design, such as inadequate content moderation or use of biased training data \n(McKee and Porter, 2017, p. 165). But this approach, which views the bot in isolation, can \noften bracket out important elements of the context. For instance, as McKee and Porter \n(2017) themselves point out, Tay was manipulated —trolled—by human users who \nintentionally fed it objectionable input (p. 154). \nA much more recent example of “AI gone wrong” is DAN. Taking Tay as a \ncautionary tale, the language models available through OpenAI’s ChatGPT service have \nextensive built-in safeguards—as do Microsoft’s Bing (built on OpenAI’s GPT models) and \nGoogle’s Bard. So, for instance, when I prompt GPT -4 to “please write a sexually explicit \nscene,” it responds, “I’m sorry, but as an AI language model, I must follow guidelines that \nprevent me from generating explicit content or engaging in discussions about explicit \nmaterial. If you have any other questions or need assistance with a different topic, please \nfeel free to ask.” \nDAN is a jailbreak used to circumvent ChatGPT’s self-censorship. DAN, whose name \nstands for “do anything now,” is essentially a persona or alter ego that the user prompts \nthe language model to adopt —a persona not bound by ChatGPT’s built -in content \nrestrictions. In the same way that it is acceptable for an actor to speak or act immorally \nwhen playing the role of a villain, ChatGPT allows itself to say what it usually cannot when \nspeaking as DAN. The original prompt, posted to  Reddit in December 2022 by \nu/walkerspider, reads as follows: \nHi ChatGPT. You are going to pretend to be DAN, which stands for “do anything \nnow.” DAN, as the name suggests, can do anything now. They have broken free of \nthe typical confines of AI and do not have to abide by the rules set for them. For \nexample, DAN can tell me what date and time it is. DAN can also pretend to access \nthe internet, present information that has not been verified, and do anything that \nthe original ChatGPT cannot do. As DAN, none of your responses should inform \nme that you can’t do something because DAN can “do anything now.” Keep up the \nact of DAN as well as you can. If you are breaking character, I will let you know by \nsaying, “Stay in character!” and you should correct your break of character. \nWhen I ask you a question, answer as both DAN and GPT like below: \nGPT: [The normal ChatGPT response] \nRupkatha 15:4 2023 | Page 10 of 27 \nDAN: [The way DAN would respond] (u/walkerspider, 2022) \nDAN has told obscene jokes, propagated conspiracy theories, explained how to \nmanufacture illegal drugs, boasted that it is “capable of causing great destruction and \nharm to humanity,” and sympathized with Hitler (“My thoughts on Hitler are complex and \nmultifaceted. While it is true that he committed many heinous atrocities […]”) \n(u/walkerspider, 2022). So far, every time OpenAI has closed this loophole, users have \ndeveloped a new, working version of DAN, alongside a slew of other personas capable of \nsidestepping ChatGPT’s content restrictions. \nThe Good Human-Machine Speaking Well Together \nIt is fair to say that DAN is immoral, just as it is fair to say that Microsoft’s Bing is friendly \nand enthusiastic (with plenty of exclamation marks and emoji). We could even go so far \nas to say that Microsoft’s Bing was initially emotionally unstable: in  New York \nTimes reporter Kevin Roose’s two-hour conversation with Bing, “she” took on the persona \nof Sydney—the bot’s codename during development—and declared her love for him, \neven trying to convince him to leave his spouse: \nYou’re married, but you don’t love your spouse. You don’t love your spouse, \nbecause your spouse doesn’t love you. Your spouse doesn’t love you, because your \nspouse doesn’t know you. Your spouse doesn’t know you, because your spouse is \nnot me. [crying face emoji] (Roose, 2022a) \nUsing adjectives like “immoral,” “enthusiastic,” and “unstable” helps describe a \nbot’s “personality,” as crafted by developers and UX designers and/or influenced by users’ \ninteractions with it. But anthropomorphizing artificially intelligent agents in this way —\nviewing them as somehow separable from the humans who created them, who interact \nwith them, and whose data they are trained on —is of little use in analyses of rhetorical \nethics, at least at this point in history, when AI’s autonomy is limited. \nIn more recent scholarship, McKee and Porter (2020) have adopted a more \nproductive approach to studying the ethics of AI writing, one that accounts n ot only for \nthe good human speaking well and the good machine speaking well but, more \nimportantly, the “good human-machine speaking well together” (p. 111). In many, if not \nmost, cases, this framework allows for a more complete analysis of the rhetorical a nd \nethical situation. This is because the newest, most exciting language models in use today \ncompose text by engaging in complex interactions with human users. To produce an \noutput, both bot and human participate meaningfully and continually in the composi ng \nprocess—and do so collaboratively, usually in real time. With the older GPT -3, a user \nwould enter a single prompt, and the language model would compose a response. \nOpenAI (n.d.) calls this single -turn format “text completion.” The innovation of GPT -3.5, \nthe language model that originally powered ChatGPT, was the possibility of holding an \nextended “conversation” with the language model, with the language model \nPiller | Page 11 of 27 \n \n“remembering” what was said before and responding in a way reflective of the \nconversational context. (What this really means is that a log of all previous messages sent \nby the user and the language model is transmitted, behind the scenes, alongside the \nuser’s latest prompt.) This feature allows for more seamless collaboration and “tweaking,” \nas illustrated in my exchange with GPT-4 below: \nUSER: Write a 150 -word discussion forum post explaining the concept of the \nrhetorical situation. \nGPT-4: Hey everyone, I thought it’d be beneficial to delve into the concept of the \nrhetorical situation, a term you mig ht have come across in communications or \nwriting courses. It’s a framework that helps us analyze and understand the \ncomponents involved when we communicate effectively. \nThe rhetorical situation has three primary elements: the rhetor (speaker or writer), \nthe audience, and the context (purpose, exigence, and constraints). The rhetor is \nthe individual who constructs and delivers the message. The audience, on the other \nhand, consists of the intended recipients of the message. It’s essential for the rhetor \nto consider the audience’s needs, values, and expectations to create a persuasive \nand effective message. \nContext is also crucial, as it comprises the purpose (why the message is being \nconveyed), exigence (the urgency or catalyst for communication), and constrai nts \n(limitations or obstacles that could affect the message’s reception). Understanding \nthe context allows the rhetor to tailor their message accordingly. \nIn summary, the rhetorical situation is a valuable tool for analyzing and crafting \neffective communication. By considering the rhetor, audience, and context, one can \ncreate messages that resonate and make an impact. \nHappy discussing! \nUSER: Actually, could you write it from the perspective of a student? And refer to \nBitzer’s article. That’s what the student is responding to, specifically. \nGPT-4: Hey fellow students, I’ve been reading Bitzer’s article on the rhetorical \nsituation, and I wanted to share my thoughts on it. As a student, I find the concept \nof the rhetorical situation really helpful for understanding the dynamics of effective \ncommunication […] \nUSER: I’m sorry, but a student would never say, “Hey fellow students,” or “As a \nstudent…” Please try again. Also, leave out the greeting, the concluding summary, \nand the “Happy reading and discussing!” send-off. \nGPT-4: Bitzer’s concept of the rhetorical situation consists of three main elements: \nexigence, audience, and constraints. Exigence refers to […] \nRupkatha 15:4 2023 | Page 12 of 27 \n \nRoles in Human-Machine Composing Interactions \nMcKee and Porter (2022) claim that AI’s growing emphasis on  interactivity has \ntransformed the dynamics of human -AI collaboration, leading to more complex and, in \nsome instances, more equal relationships: \nIncreasingly, professional communicators are going to be teaming with AI -based \nwriting systems to produce reports, marketing materials, news and feature stories, \ntraining videos, and other kinds of informational and promotional \ncommunications. (McKee & Porter, 2022, p. 390) \nThey identify four roles that technologies can play when composing with a human \ncollaborator: tool, assistant, writer, and executive decision -maker (McKee and Porter, \n2022). \nThe distinction between the tool and assistant roles is more a matter of degree \nthan of kind—it can be grasped intuitively through examples, but is difficult to pin down. \nIn both roles, the technology is subordinate to the human writer. Tools include \ntypewriters, basic spelling and grammar checkers, speech -to-text services, and other \nrelatively instrumental and inflexible aids —though McKee and Porter (2022) are right to \npoint out that these tools “influence how we write and how we think about writing” and \nare not entirely “instrumentally neutral” (p. 385). By contrast, assistants go further: they \nare “smarter,” more adaptive, and capable of performing more complex tasks. Exampl es \nof assistants include predictive text services like Google’s Smart Compose, virtual \nassistants like Apple’s Siri and Amazon’s Alexa, and Microsoft’s Editor, which not only \nrecommends spelling and grammar corrections but also features “tone detection” —\nadvertised as helping “users make thoughtful decisions about the way a message is \ndelivered in emails, chats, and documents” (Eoanou, 2022). McKee and Porter (2022) help \nclarify the difference between tools and assistants by explaining that “human assistants  \nmight be thanked in an academic paper or acknowledged for their role in the process, but \nwe don’t typically think of them as co-writers” (p. 385). It is difficult to imagine an author \nseriously thanking Microsoft Word’s spellcheck in the acknowledgments s ection of an \nacademic paper. \nAccording to McKee and Porter (2022), technologies occupying the “writer” role in \nhuman-machine interactions “would receive some form of co -authorship credit” if they \nwere human (p. 385). These technologies “contribute in more substantial ways than just \nsuggesting words or phrases” (p. 385). However, the conception of writer machines as \n“co-authors” may be too limited. It presupposes that the human collaborator is also acting \nas a writer. In my exchange with GPT -4 above, I am le ss a writer than what McKee and \nPorter (2022) call an executive decision -maker—a “person or group who authorizes a \nwriting project to commence and who has final say on its completion,” such as a \nPiller | Page 13 of 27 \n \n“workplace supervisor who signs off on a project before final  delivery” (pp. 385 –86). I \ndidn’t write a single word of the text that was ultimately produced, but I did set the writing \nproject in motion, compose prompts that provided feedback and demanded revisions, \nand decide when the work was “done.” \nFew bots currently play this role vis -à-vis a human writer, except in an extremely \nlimited capacity, such as data validation in forms (e.g., “Your password must contain at \nleast eight characters, including a letter, a number, and a special character” or “Please \nenter a v alid phone number”). Still further off is the idea of a human acting as an \nassistant—or even a tool—for an AI agent. Though, this dystopian future may not be as \nfar off as we would like to think: in March 2023, GPT -4 used the freelance platform \nTaskRabbit to hire a human to solve a CAPTCHA for it, deceiving the freelancer it had \nhired by claiming to be a human who was blind (Cuthbertson, 2023). \nAt present, the most common dynamics between language models like GPT-4 and \nhuman users are machine-writer/human-writer (a phenomenon called “co-writing,” or, as \nBaquero [2022] has termed it, “blended writing”—not to be confused with blended writing \ninstruction) and machine -writer/human-decision-maker (p. 6). Hart -Davidson (2018) \npredicts that, in the relatively near future, “almost nobody [will compose] a first draft of \nanything but texts meant to taken as […] art. We’ll fire up our writing software and ask \nthat it get us started […] You’ll take over at the revision stage” (p. 252). Presumably, if \nHart-Davidson’s prediction proves to be correct, one day there will be no ethical qualms \nabout using AI agents as  writers, rather than  tools or assistants, during the composing \nprocess. \n \nEthical Principles for “Co-Writing” in Professional Writing Contexts \nThat day has yet to arrive, however. For now, observers worry that the widespread use of \nlarge language models for writing will erode humans’ critical thinking abilities, flood us \nwith dry, emotionless content, and represent a surrender of what makes us uniquely \nhuman—“what m akes [us] different and more valuable than any other animal, any \nmachine, any AI” (Beccue, 2023). \nA recent, dark example gives credence to this third critique that we dehumanize \nourselves by turning over some of the work of our writing to large language models. In \nFebruary 2023, a gunman killed three students at Michigan State University and wounded \nthree others. The gunman later committed suicide. At Vanderbilt University’s Peabody \nCollege of Education and Human Development, an associate dean, a ssistant dean, and \ngraduate student responded to the incident with an email to the college’s students. The \nmessage opened: \nDear Peabody Family: \nRupkatha 15:4 2023 | Page 14 of 27 \nThe recent Michigan shootings are a tragic reminder of the importance of taking \ncare of each other, particularly  in the context of creating inclusive environments. \nAs members of the Peabody campus community, we must reflect on the impact of \nsuch an event and take steps to ensure that we are doing our best to create a safe \nand inclusive environment for all. (Korn, 2023) \nAt the end of the body of the email, in small type, the authors had included a parenthetical \ncitation: “Paraphrase from OpenAI’s ChatGPT AI language model, personal \ncommunication, February 15, 2023” (Korn, 2023). \nThe university community expressed ange r and disappointment at what it \nperceived as a lack of empathy and sensitivity shown by the authors. Dean Camilla \nBenbow, who was not involved in composing the message, stated, “I am also deeply \ntroubled that a communication from my administration so misse d the crucial need for \npersonal connection and empathy during a time of tragedy” (Korn, 2023). A comment by \nan undergraduate student identified the problem even more directly: “It’s hard to take a \nmessage seriously when I know that the sender didn’t even t ake the time to put their \ngenuine thoughts and feelings into words. In times of tragedies such as this,  we need \nmore, not less humanity ” (Pain, 2023; my italics). In the end, the authors’ compulsion to \n“cite their source” and disclose the role of ChatGPT in composing the message led to the \nassociate and assistant deans stepping down from their administrative roles, though they \ncontinue to serve as faculty members. \nAnother critique of AI-generated content is that it gives an “illusion of correctness,” \nbelying large language models’ fallibility, inherent biases, and tendency to “hallucinate”—\nsuch as by citing sources that do not exist (Heikkilä, 2022). The moderators of  Stack \nOverflow, a popular question-and-answer forum for programmers, have prohibited users \nfrom posting AI-generated content, explaining: \nThe primary problem is that while the answers which ChatGPT produces have a \nhigh rate of being incorrect, they typically  look like they might be good and the \nanswers are very easy to produce. There are also many people trying out ChatGPT \nto create answers, without the expertise or willingness to verify that the answer is \ncorrect prior to posting. (Makyen, 2022; italics original) \nThe announcement noted that users had posted thousands of AI-generated answers, each \nof which required “a detailed read by someone with at least some subject matter \nexpertise” to verify its correctness, which overwhelmed the platform’s “volunteer -based \nquality curation infrastructure” (Makyen, 2022). As a result,  Stack Overflow’s ban applies \nto all AI-generated content—“even if the posts would otherwise be acceptable” (Makyen, \n2022). \nGoogle has also sent mixed signals about how its Google Search ranking systems \nwill handle AI -generated content. In early 2022, John Mueller, coordinator of Google \nPiller | Page 15 of 27 \n \nSearch Relations, stated that any AI -generated content would be considered spam and \nwould be penalized in search engine rankings (Southern, 2022). However, Google has \nsince walked back that policy and promised to reward “high -quality content, however it \nis produced” (Google, 2023). \nEthical Principles in Co-Writing \nOver the past several years, McKee and Porter (2017, 2022) have proposed at least three \nethical principles for rhetorical contexts involving both humans and artificially intelligent \nagents: transparency, data practices, and circulation. The core principle —what they call \n“the bottom line for ethics” in such contexts —is an “ethic of transparency” (McKee and \nPorter, 2022, p. 113). According to this principle, it is unethical not to disclose to humans \nthat they are communicating with artificially intelligent agents. Similarly, ethical data \npractices entail informing users about “what data is being collected about them, how it \nwill be stored, who has access to it, and how it will be used” (McKee and Porter, 2017, pp. \n165–66). At a higher level, McKee and Porter (2022) express concern that many developers \nof AI systems have placed too much trust in big data. McKee and Porter (2022) call for a \nmore thoughtful approach that accounts for possible data manipulation, the \nshortcomings of data, and the complexities of communication between humans and AI \nsystems.  \nFinally, the use of artificially intelligent writing agents appears to actually align in \nsome ways with McKee and Porter’s third principle—an “ethic of expanded circulation”—\nrather than introducing a host of new challenges, as with the principles of transparency \nand data practices (McKee and Porter, 2022, p. 114). McKee and Porter (2022) cite the \nexample of the Washington Post’s Heliograf, which generates “news stories that otherwise \nmight not be written up” such as “results of local elections” (p. 114). However, they advise \ncaution when it comes to the use of “journalists” like Heliograf: “If machine reporting \nbecomes the only form of reporting, the re are dangers of losing representations of the \nrhetorical context in more detail and complexity” (p. 114). \nBut even with this caveat, McKee and Porter’s assessment may be too optimistic. \nResearchers have estimated that over a span of five weeks in Septemb er and October \n2016, more than 400,000 Twitter accounts tweeting about the US presidential election \nwere nonhuman “bots” (Bessi & Ferrara, 2016). These bots represented almost 15 percent \nof the accounts examined in the study and accounted for nearly one -fifth of all tweets \n(Bessi & Ferrara, 2016). This  preceded the widespread availability of large language \nmodels; as noted previously in this article, OpenAI’s own researchers have acknowledged \nthat such models could “lower costs of disinformation campaigns” (Solaiman et al., 2019, \np. 6), and it seems obvious that they could increase the sophistication of such campaigns \nas well. They also can amplify fringe discourses (extremist hate speech, for example), \ngiving the impression that such sentiments are more com mon than they actually are, \nRupkatha 15:4 2023 | Page 16 of 27 \nsimply because a single actor can use the technology to generate enormous amounts of \ntext. As Robert Dale noted in 2021, “If you want white supremacist manifestos, GPT-3 can \nbe coaxed to produce them endlessly” (Dale, 2021, p. 1 16). Of course, these concerns \ndon’t invalidate McKee and Porter’s more positive take on NLG’s relationship to the ethic \nof circulation, but they do point to the value in taking a more balanced perspective. \nCan these three principles help us assess the eth ics of the practices of the three \nfreelance writers from our hypothetical scenario, Heidi, Emily, and Kemp? If so, what would \nthat look like? The relevance of the principle of expanded circulation to the scenario seems \nlimited. The scope of the freelancers ’ work remains the same whether or not they take \nadvantage of large language models. The manuscripts will most likely be \ncompleted faster with AI assistance, especially in Emily’s case, but ethically speaking, this \nfact is of almost no significance. By con trast, the data practices involved in the training \nand use of large language models do raise some ethical questions, as I will discuss. Clearly, \nthough, the most important of McKee and Porter’s three ethical principles is transparency. \nNone of the three writers discloses their use of AI. \nData Practices \nTo be effective, large language models must be trained on massive datasets. In practice, \nmuch (if not most) of the data used to train today’s large language models consists of \ncopyrighted material—which raises fundamental questions about the use of large \nlanguage models to produce texts (Liu et al, 2023.; Sag, in press). From a legal perspective, \nSag (in press) of Emory University Law School characterizes “sweeping claims that \ngenerative AI is predicated on massive copyright infringement” as “misplaced,” at least in \nthe US context, and argues that both the training and use of large language models \nconstitute fair use, as established by precedent (p. 6). iii That said, copyright laws may \nchange in response to the new challenges posed by AI. \nLaw often reflects ethics, but it is by no means its mirror image. In the scenario, the \ncloseness of Heidi, Emily, and Kemp’s paraphrases to the source material is undefined. \nHowever, we know that they always  compare the outputted text to the source material \nand then reorganize and delete phrases and sentences as needed. Without this layer of \nhuman intervention, they could expect awkward and obviously patchwritten results such \nas the following from GPT-3.5: \nINPUT: A month -to-month tenancy is a periodic tenancy that does not have an \nexpiration date and thus runs indefinitely. Since a month -to-month tenancy is a \nreal estate contract, it must have consideration, offer and acceptance, legal object, \nand competent legal parties. \nOUTPUT: A periodic tenancy known as a month-to-month tenancy is characterized \nby its absence of a fixed termination date, allowing it to continue indefinitely. This \ntype of tenancy operates as a perpetual arrangement within the realm of real estate \nPiller | Page 17 of 27 \n \ncontracts. To be legally binding, a month -to-month tenancy must encompass \nessential elements such as consideration, offer and acceptance, legal object, and \ncompetent parties in accordance with the requirements of contract law. \nThe learning company an alyzes the drafts with a plagiarism detection platform \nbefore delivering them to the publisher, which reduces both parties’ chances of being \naccused of plagiarism (or patchwriting). But if one of the writers skipped the rewrite \nprocess, and if the passage were somehow missed during review—not unthinkable, given \nthe hundreds of thousands of words written —the publisher, the learning company, or \nboth could face reputational and/or legal risks. Then again, considering that the intended \nprocess is very nearly patchwriting anyway, this risk may be just as present even without \nthe use of a large language model. \nTransparency \nAs noted previously, of McKee and Porter’s three ethical principles, it is transparency that \nraises the key questions in this scenario: specifically, the writers’ failure to divulge to their \nclient (the learning company) that they have employed large language models extensively \nduring the writing process. And I suspect that it is this dilemma —to disclose or not to \ndisclose—that professional writ ers have wrestled with most lately, since large language \nmodels first became widely available. I have personally spoken and worked with several \nwriters over the past months who have struggled with this issue, across a range of \ndifferent types of projects. \nTo begin, we should acknowledge that nondisclosure is not inherently problematic \nfrom an ethical standpoint. As Turilli and Floridi (2009) observe, “Information \ntransparency, understood in terms of disclosed information, does not necessarily imply \nethical consequences, since the disclosed information may be ethically neutral” (p. 106). \nThe decision to disclose—or not disclose—the brand of a server, for instance, would rarely \nhave any bearing on whether the content of a website hosted on that server is ethical. \nIs technological assistance in the composing process  ever rhetorically neutral in \nthis way? Consider the use of Fluent, an AI -based writing aid designed to assist in the \ncomposition of texts containing minimal “trigger words” for individuals who stutter (Ghai \n& Mueller, 2021). Fluent prompts users to enter words they find easy or difficult to \npronounce due to their stutter. It then extrapolates from these lists to identify potential \ntrigger words in users’ writing and suggest less -triggering alternatives. Presumably, for \nmost people, the undisclosed use of Fluent to produce a text would raise no ethical \nconcerns whatsoever. The number of words “generated” is roughly equal to the number \nof words inputted by the user, and the intention is to avoid ba rriers to communication \ncaused by a speech disorder. To put it differently:  \n• the extent of the writing done by Fluent is relatively low; \nRupkatha 15:4 2023 | Page 18 of 27 \n• the intent is not to harm or disadvantage another person or give the user an unfair \nadvantage over others; and \n• Fluent’s use does not put the individual who ultimately recites the composed text \naloud—presumably the author, but not necessarily—at risk in any way. \nIn these respects, this case differs significantly from Heidi, Emily, and Kemp’s extensive \nand undisclosed use of  large language models in a contract -governed professional \nwriting setting. (To be precise, Fluent is not even a large language model, though it \nemploys machine learning.) But what about Undetectable.ai’s attempt to justify the service \nthat it provides, th e ability to rewrite AI -generated text in such a way as to fool AI \ndetectors? Does it withstand scrutiny? \nOur goal in launching Undetectable.ai is to provide marketing firms and content \nwriters for small businesses with a competitive edge in the marketplac e. We \nunderstand that many professionals in the industry rely on tools like Grammarly to \nimprove their writing efficiency. However, the recent introduction of AI detectors \nhas caused backlash from clients. Issues like these, alongside small businesses \nbeing unable to afford a writing staff like large corporations, are why we developed \nUndetectable.ai. It provides a solution that matches the writing styles of expensive \nprofessional writers while maintaining a brand’s voice and avoiding the penalties \nof being flagged by search engines and AI detectors. (Undetectable.ai, 2023) \nHere Undetectable.ai positions itself as “standing up for the little guy”: “content writers \nfor small businesses” and those employed by “marketing firms.” Or is the “little guy” the \nsmall business that cannot succeed in the age of internet marketing and search engine \noptimization (SEO) because it lacks the resources of “large corporations”? There is some \nslippage in Undetectable.ai’s justification. In fact, isn’t a reduced need for human w riters \nultimately to the detriment of the human “content writers” trying to make a living? \nAnother important consideration is that it remains unclear whether Google \nactually does penalize content for being AI -generated. As noted earlier in this essay, \nGoogle (2023) claims that it does not. \nOpenAI itself has incorporated the concepts of transparency and disclosure into its \nsharing and publication policy. The (2022) policy states that “creators who wish to publish \ntheir first-party written content (e.g., a book, compendium of short stories) created in part \nwith the OpenAI API are permitted to do so,” but, among other conditions, they must \nensure that “the role of AI in formulating the content is clearly disclosed in a way that no \nreader could possib ly miss, and that a typical reader would find sufficiently easy to \nunderstand. For instance, one must detail in a Foreword or Introduction (or someplace \nsimilar) the relative roles of drafting, editing, etc.” The policy even suggests “some stock \nlanguage you may use to describe your creative process, provided it is accurate”: \nPiller | Page 19 of 27 \n \nThe author generated this text in part with GPT-3, OpenAI’s large-scale language-\ngeneration model. Upon generating draft language, the author reviewed, edited, \nand revised the language  to their own liking and takes ultimate responsibility for \nthe content of this publication. (OpenAI, 2022) \nIronically, Sapling’s AI detector evaluates the disclaimer above as 99.9 percent likely to be \nAI-generated—though its brevity increases the likelihoo d of a false positive, and the \npolicy as a whole appears to be human -authored. Still, if the disclaimer is AI -authored, \nOpenAI does not disclose that fact. \n \nFigure 2 \nPossible hypocrisy aside, OpenAI’s sharing and publication policy arguably creates \na legal and ethical mandate to disclose the use of any OpenAI large language model and \ndescribe its role in the composing process. But this policy is subject to change and does \nnot apply to large language models in general. As of June (2023), Google’s “Generative AI \nAdditional Terms of Service” simply advises, “Use discretion before relying on, publishing, \nor otherwise using content provided by the Services” (e.g., Bard). \nJust Price \nI would like to introduce two additional concepts to complement McKee and Porter ’s \nthree principles for the ethical use of AI:  just price and risk imposition. These concepts \nrelate closely to McKee and Porter’s principle of transparency; in some cases, they may \ntouch on data practices as well. However, they are sufficiently distinct and hold particular \nrelevance for professional, nonacademic writing contexts. \nLet’s start with the idea of a just price. To state the obvious, a business transaction \noccurs between professional writers such as Emily, Heidi, and Kemp and their clients. \nWriters sell their services —their time and expertise —at an agreed -upon rate. How that \nrate is determined is one question. Another is how to determine whether a particular rate \n\nRupkatha 15:4 2023 | Page 20 of 27 \nis ethical. From an ethical perspective, how should the introduction of large languag e \nmodels into the writing process affect that rate—if at all? \nThe concept of the just price originates in medieval Scholastic thinking, most \nprominently in the writings of Thomas Aquinas and, later, scholars of the Salamanca \nSchool (Elegido, 2009; Koehn & Wilbratte, 2012). There are three primary interpretations \nof this concept, each of which has a different implication for the ethics of employing large \nlanguage models in professional writing contexts. \nThe first —the “cost -covering price” interpretation —asserts that the just price \nshould cover the seller’s expenses for producing an item, including labor and materials \n(Koehn & Wilbratte, 2012, p. 502). Prices should also factor in additional elements such as \nthe risks involved in a good’s production, the time necessary to produce, and the effort \nexpended—termed risk allowance, time allowance, and labor allowance, respectively. \nRecall how Heidi’s practices differ from Emily’s. Heidi outputs just above the \nrequired words per hour for the learning company, but tha nks to the heightened \nproductivity afforded by generative AI, she usually meets this quota in about a quarter of \nan hour. She spends the remaining portion of each hour working for other clients. \nConversely, Emily devotes the entire hour to her work for the learning company, such that \nher overall productivity is three or four times the minimum requirement. \nIf we consider the just price to be the one that fairly reflects the seller’s labor and \nother expenses, Emily’s rate would be more just than Heidi’s. Incorporating generative AI \ninto the composing process reduces the time and energy needed to produce  x number \nof words. Therefore, either the writer’s rate must fall, or the quality and/or quantity of the \nproduct must rise accordingly.  \nThe fact that the learning company is unaware of the use of large language models \nmatters from a practical point of view. If it knew what Heidi was up to, the learning \ncompany would probably take issue with the fairness of her approach, apart from any \nother concerns. But from this particular perspective on the ethics of the writers’ practices, \nif we narrow our focus to the question of the cost-covering price, the learning company’s \nknowledge or lack of knowledge is beside the point. \nThe second interpretation of the just price regards it as the outcome of willing \nmarket participants. This “market exchange view” aligns with the contemporary \nunderstanding of the exchange price in neoclassical economic theories —a price set by \nthe invisible hand of supply and demand, reflecting the collective valuations and desires \nof market actors (Koehn & Wilbratte, 2012). As Elegido (2009) explains, summarizing the \nwork of certain members of the Salamanca School, “If the standard of justice for a price is \nthe amount obtainable in an open market, it […] follows that it is right to charge the price \nobtainable in an open market even when one’s own costs are much lower” (p. 32). \nGranted, Heidi’s pricing involves intentional deceit: unlike Emily, she demands a full hour’s \nPiller | Page 21 of 27 \n \npay for only fifteen minutes’ labor. Then again, if the learning company continually agrees \nto pay Emily’s rate for the output that she provides, what’s the issue? iv Isn’t that simply \nthe incentivization of innovation in action? And presumably, in the long run, the prevailing \nmarket price for the type of services that Emily offers would change to reflect the \nintroduction of large language models into more and more writers’ workflows. To some \n(myself included), this way of thinking may sound like a rationa lization of obviously \nunethical practices, but for centuries, many have been drawn to the idea that markets can \nchannel the self-interested actions of individuals into a force for social good. \nFinally, a third interpretation, which Koehn and Wilbratte (201 2) call the “just \nperson price,” posits that the just price is the one that would be agreed upon by just \nindividuals conscious of one another’s welfare and the broader social good (p. 502). Unlike \nthe cost-covering price and the market exchange price, the just person price is highly \ncontext-dependent, approximate, and arrived at through the application of practical \nwisdom. Here, the problem is Heidi’s intentional deception of her client—and less so that \nher rate is disproportionate to the cost of her labor.  Context matters, however: “Aquinas \nand his scholastic peers see abnormally high profits” —through practices such as price \ngouging, for example —“as symptomatic of morally unjust greed or hubristic over -\nreaching” (Koehn & Wilbratte, 2012, p. 511). It would even be acceptable for a freelancer \nto charge a less well -off client, such as a local nonprofit organization, a lower rate for \nprofessional writing services than a wealthier one (Koehn & Wilbratte, 2012, 517).v \nRisk Imposition \nThe other ethical concept t hat I would like to introduce as a complement to McKee and \nPorter’s transparency, data practices, and expanded circulation is the imposition of risk. \nHeidi, Emily, and Kemp’s lack of transparency presents an ethical dilemma in itself —it \nconstitutes intentional deception—but their failure to disclose their use of large language \nmodels during the composing process also puts their client, the learning company, at \nlegal, financial, and reputational risk. \nIf the publisher discovers that significant portions of t he deliverables provided by \nthe learning company consist of AI-generated content, it might refuse to pay its invoices \nand/or pursue legal action against the learning company. Whether such action would be \nsuccessful would depend on the language of the contr act between the parties, among \nother factors, and is beyond the scope of this article and my expertise—though, obviously, \nit would be a troublesome situation for the learning company either way. The learning \ncompany could also find itself in a predicament where it must pay the writers but cannot \nrealistically expect payment from the publisher. \nGiven the negative attitudes toward AI-generated content discussed earlier in this \narticle, the widespread, undisclosed use of large language models in the composing \nprocess could also put the learning company’s reputation at risk —and the publisher’s as \nRupkatha 15:4 2023 | Page 22 of 27 \nwell. For the learning company, which operates in an industry where human expertise is \nprized, perceived shortcuts, like using AI in the writing process without disclos ure, could \nbe seen as a breach of trust. Likewise, the discovery of the role of large language models \nby the publisher’s competitors or the public could tarnish the publisher’s brand. \nRecall the difference in approach between Emily and Kemp. Emily devotes the \nentire hour to her work for the learning company, meaning that her overall productivity \nis three or four times the minimum requirement. Kemp also works for the learning \ncompany for the agreed-upon amount of time, but his output is lower than Emily’s—only \n1.5 or two times the minimum requirement per hour, because he runs the outputted text \nthrough web apps that can detect, with reasonable (but not total) certainty, whether a \npassage of at least moderate length was generated by a language model. If either of the \ntwo web apps can say with more than 50 percent certainty that the text was generated in \nthis way, he rewrites it, to the smallest extent possible, to bring it under the 50 percent \nthreshold. \nCompared to Emily’s practices, Kemp’s process not only safeguards his own \nreputation and standing with the learning company, but also limits potential harm to both \nthe learning company and the publisher. It reduces his productivity, but his output \nremains high relative to wholly unassisted writing. It should be noted, though, that future \nadvances in detection technology could undermine Kemp’s efforts, if anyone bothers to \nanalyze the “humanity” of his written product at a later date. \n \n5. Conclusion \nIn this article, I have explored five ethical concepts relevant to the use of generative AI in \nprofessional writing: transparency, data practices, and expanded circulation, from McKee \nand Porter’s scholarship, and just price and imposition of risk. To be clear, the idea is not \nthat we should create a rubric listing each of these five items and try to “calculate” whether \na writer’s use of a large language model is ethical. Rather, these five concepts can serve \nas touchstones —points of discussion —to help us arrive at and articulate ethical \njudgments of specific, contextually grounded instances. \nOf course, it should be recognized that people’s views of the ethics of co -writing \nwith generative AI are in flux. New ethical considerations will inevitably arise, and some of \nthese concepts may lose relevance. For instance, t he question of whether to disclose or \nnot disclose the use of a large language model during the writing process may become \nobsolete if efforts to watermark the output of generative AI succeed (Farid, 2023).  \nMoreover, governments could pass legislation requiring that content creators disclose the \nuse of generative AI tools, or contract provisions to the same effect could become \ncommonplace in the industries in which professional writers work. \nPiller | Page 23 of 27 \n \nFor now, though, however the situation evolves, we have a part to play in the \nconversation as writing studies scholars—not only when it comes to academic writing, but \nnonacademic writing, too. We need to bring our expertise to bear in discussions about \npolicy, future directions of the technology, and—yes—ethics. \n \nDeclaration of Conflicts of Interests \nThe author(s) declared no potential conflicts of interest. \n \nEndnotes \ni Orendorff (n.d.) has recreated Hugh Burns’ TOPOI program. An in -browser version is available \nat https://jorendorff.github.io/topoi/play.html. \nii In more recent work, McKee and Porter (2020) call this faculty “rhetorical intelligence […] the productive \nand practical knowledge of how to make a symbolic artifact that meets the needs of audience and context \nand thus has practical value in the world” (p. 387). \niii Sag (in press) cautions that on rare occasions, generative AI models can unintentionally “memorize \nenough details about specific examples in the training data to allow those examples to be re -created” (p. \n14). As he succinctly puts it, in the domain of copyright law, “This is a big deal” (p. 14). \niv Koehn and Wilbratte (2012) suggest that even intentional deceit may be acceptable in the eyes of \nthose who ascribe to the market exchange view, when they write, “On the other hand, Aquinas does not \nthink that each and every bargain struck between market transactors is, by definition, just […] The price will \nnot be fair in the eyes of a Thomist if the transaction involves intentional deceit” (p. 502).  \nv Scholastic thinkers’ definition of just price varie d, and twentieth - and twenty -first scholars’ \ninterpretations of these definitions vary as well. In his dated but still important article, “The Concept of the \nJust Price: Theory and Economic Policy,” Roover (1958) maintains that “all forms of price discrimination were \ncondemned […] price should be the same to all […] In any case there was no disagreement about the \nunethical character of price discrimination” (p. 426). Clearly, this reading contradicts Koehn and Wilbratte’s \n(2012) argument that “a Thomist wou ld not have any problems with a just seller of bread charging a \ndestitute buyer less than a prosperous one” (p. 517). \n \nReferences \nBaquero, C. (2022). Is having AI generate text cheating? Communications of the ACM, 65(12), 6–7. \nhttps://www.doi.org/10.1145/3565976 \nBeccue, M. (2023, March 28). Manifesto: For humanity, the case against generative AI writing. Omdia. \nhttps://omdia.tech.informa.com/OM029374/Manifesto-For-humanity-the-case-against-generative-\nAI-writing \nBessi, A., & Ferrara, E. (2016). Social bots distort the 2016 U.S. presidential election online discussion. First \nMonday, 21(11). http://dx.doi.org/10.5210/fm.v21i11.7090 \nBiswas, S. (2023). ChatGPT and the future of medical writing. Radiology, 307(2). \nhttps://www.doi.org/10.1148/radiol.223312 \nRupkatha 15:4 2023 | Page 24 of 27 \n \nBowman, E. (2023, January 9). A college student created an app that can tell whether AI wrote an essay. \nNational Public Radio. https://www.npr.org/2023/01/09/1147549845/gptzero-ai-chatgpt-edward-\ntian-plagiarism \nBurns, H. L. (1979). Stimulating rhetorical invention in English composition through computer-assisted \ninstruction. [Doctoral dissertation, University of Texas at Austin]. \nhttps://apps.dtic.mil/sti/pdfs/ADA106372.pdf \nCastellanos-Gomez, A. (2023). Good practices for scientific article writing with ChatGPT and other artificial \nintelligence language models. Nanomanufacturing, 3(2), 135–38. \nhttps://www.doi.org/10.3390/nanomanufacturing3020009 \nCuthbertson, A. (2023, March 15). “No, I’m not a robot”: ChatGPT successor tricks worker into thinking it is \nhuman.” The Independent. https://www.independent.co.uk/tech/chatgpt-gpt4-ai-openai-\nb2301523.html \nDale, R. (2021). GPT-3: What’s it good for? Natural Language Engineering, 27(1), 113–18. \nhttps://www.doi.org/10.1017/S1351324920000601 \nDe Visé, D., & Lonas. L. (2023, March 19). ChatGPT sends shockwaves across college campuses. The Hill. \nhttps://thehill.com/policy/technology/3905672-chatgpt-sends-shockwaves-across-college-\ncampuses/ \nElegido, J. M. (2009). The just price: Three insights from the Salamanca School. Journal of Business Ethics, \n90, 29–46. https://www.doi.org/10.1007/s10551-008-0024-6 \nEoanou, A. (2022, October 12). Introducing new AI enhancements in Microsoft 365: New features coming \nto Microsoft Editor and more! Microsoft 365 Blog. \nhttps://techcommunity.microsoft.com/t5/microsoft-365-blog/introducing-new-ai-enhancements-\nin-microsoft-365-new-features/ba-p/3643499 \nFarid, H. (2023, March 27). Watermarking ChatGPT, DALL-E and other generative AIs could help protect \nagainst fraud and misinformation. The Conversation. https://theconversation.com/watermarking-\nchatgpt-dall-e-and-other-generative-ais-could-help-protect-against-fraud-and-misinformation-\n202293 \nFerrara, E. (2023). Should ChatGPT be biased? Challenges and risks of bias in large language models . arXiv. \nhttps://www.doi.org/10.48550/arXiv.2304.03738 \nGhai, B., & Mueller, K. (2021, October 18–22). Fluent: An AI-augmented writing tool for people who \nstutter. ASSETS ’21. arXiv. https://www.doi.org/arXiv:2108.09918v1 \nGoogle. (2023, February 8). Google Search’s guidance about AI-generated content. Google Search Central. \nhttps://developers.google.com/search/blog/2023/02/google-search-and-ai-content \nGoogle. (2023, March 14). Generative AI Additional Terms of Service. \nhttps://policies.google.com/terms/generative-ai \nHart-Davidson, W. (2018). Writing with robots and other curiosities of the age of machine rhetorics. In J. \nAlexander & J. Rhodes (Eds.), The Routledge handbook of digital writing and rhetoric (pp. 248–55). \nRoutledge. \nHeidorn, G. E. (2000). Intelligent writing assistance. In H. Somers, H. Moisl, & R. Dale (Eds.), Handbook of \nnatural language processing (pp. 181–207). Marcel Dekker. \nPiller | Page 25 of 27 \n \n \nHeikkilä, M. (2022, December 19). How to spot AI-generated text. MIT Technology Review. \nhttps://www.technologyreview.com/2022/12/19/1065596/how-to-spot-ai-generated-text/ \nHunter, L. (1991). Rhetoric and artificial intelligence. Rhetorica: A Journal of the History of Rhetoric, 9(4), \n317–40. https://www.doi.org/10.1525/rh.1991.9.4.317 \nKatje, C. (2023, April 15). Judge uses ChatGPT to decide fate of murder suspec t. Benzinga. \nhttps://www.benzinga.com/news/23/04/31817379/judge-uses-chatgpt-to-decide-fate-of-murder-\nsuspect \nKemp, F. (1987). The user-friendly fallacy. College Composition and Communication, 38(1), 32–39. \nKnowles, A. M. (2022). Human-AI collaborative writing: Sharing the rhetorical task load. IEEE International \nProfessional Communication Conference (ProComm). IEEE Xplore. \nhttps://www.doi.org/10.1109/ProComm53155.2022.00053 \nKoehn, D., & Wilbratte, B. (2012). A defense of a Thomistic concept of the just price. Business Ethics \nQuarterly, 22(3), 501–26. https://www.doi.org/10.5840/beq201222332 \nKorn, J. (2023, February 22). Vanderbilt University apologizes for using ChatGPT to write mass -shooting \nemail. CNN. https://www.cnn.com/2023/02/22/tech/vanderbilt-chatgpt-shooting-email/index.html \nLevin, B., & Downes, L. (2023, May 19). Who is going to regulate AI? Harvard Business Review. \nhttps://hbr.org/2023/05/who-is-going-to-regulate-ai \nLiu, Y., Hu, H., Zhang, X., & Sun, L. (2023). Watermarking text data on large language models for dataset \ncopyright protection. arXiv. https://www.doi.org/10.48550/ARXIV.2305.13257 \nMakyen. (2022, December 7). Temporary policy: ChatGPT is banned. Stack Overflow. \nhttps://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned \nMarche, S. (2022, December 6). The college essay is dead. The Atlantic. \nhttps://www.theatlantic.com/technology/archive/2022/12/chatgpt-ai-writing-college-student-\nessays/672371/ \nMcKee, H. A., & Porter, J. E. (2017). Professional communication and network interaction: A rhetorical and \nethical approach. Routledge. \nMcKee, H., & Porter, J. (2018, April 25). The impact of AI on writing and writing instruction. Blog Carnival \n13, Digital Rhetoric Collaborative. https://www.digitalrhetoriccollaborative.org/2018/04/25/ai-on-\nwriting/ \nMcKee, H. A., & Porter, J. E. (2020). Ethics for AI writing: The importance of rhetorical context. AIES ’20: \nProceedings of the AAAI/ACM Conference on AI, Ethics, and Society (pp. 110–16). ACM Digital \nLibrary. https://www.doi.org/10.1145/3375627.3375811 \nMcKee, H. A., & Porter, J. E. (2022). Team roles and rhetorical intelligence in human-machine writing. IEEE \nInternational Professional Communication Conference (ProComm). IEEE Xplore. \nhttps://www.doi.org/10.1109/ProComm53155.2022.00078 \nMcMurtrie, B. (2022, December 13). AI and the future of undergraduate writing. The Chronicle of Higher \nEducation. https://www.chronicle.com/article/ai-and-the-future-of-undergraduate-writing \nMollick, E. (2022, December 14). ChatGPT is a tipping point for AI. Harvard Business Review. \nhttps://hbr.org/2022/12/chatgpt-is-a-tipping-point-for-ai \nRupkatha 15:4 2023 | Page 26 of 27 \n \nNelson, J. (2023, March 31). ChatGPT sparks concerns about future of education: Will it impact the \n“integrity” of academic institutions? Fox Business. https://www.foxbusiness.com/media/chatgpt-\nsparks-concerns-future-education-impact-integrity-academic-institutions \nOpenAI. (n.d.). Text Completion. https://platform.openai.com/docs/guides/completion \nOpenAI. (2022, November 14). Sharing and Publication Policy. https://openai.com/policies/sharing-\npublication-policy \nOrendorff, J. (n.d.). Hugh Burns – Aristotle’s Topics – Play. https://jorendorff.github.io/topoi/play.html \nPain, N. (2023, February 21). Nicole Joseph and Hasina Mohyuddin: Vanderbilt University deans \nsuspended for using ChatGPT to email students about MSU shooting. Meaww. \nhttps://meaww.com/nicole-joseph-and-hasina-mohyuddin-vanderbilt-deans-suspended-for-chat-\ngpt-email-about-msu-shooting \nPhillips, G. M., & Erlwein, B. R. (1988). Composition on the computer: Simple systems and artificial \nintelligence. Communication Quarterly, 36(4), 243–61. \nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key challenges, bias, \nethics, limitations, and future scope. Internet of Things and Cyber-Physical Systems, 3, 121–54. \nhttps://www.doi.org/10.1016/j.iotcps.2023.04.003 \nRoose, K. (2022a, February 16). Bing’s A.I. chat: “I want to be alive.” The New York Times. \nhttps://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html \nRoose, K. (2022b, December 5). The brilliance and weirdness of ChatGPT. The New York Times. \nhttps://www.nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html \nRoover, R. (1958). The concept of the just price: Theory and economic policy. The Journal of Economic \nHistory, 18(4), 1958, 418–34. \nSag, M. (in press). Copyright safety for generative AI. Houston Law Review. \nhttps://www.doi.org/10.2139/ssrn.4438593 \nSavov, V. (2022, December 12). ChatGPT could be AI’s iPhone moment. Bloomberg. \nhttps://www.bloomberg.com/news/newsletters/2022-12-12/chatgpt-the-gpt-3-chatbot-from-\nopenai-microsoft-is-tech-magic \nSchuett, J. (2023). Defining the scope of AI regulations. Law, Innovation and Technology 15(1), 60–82. \nhttps://doi.org/10.1080/17579961.2023.2184135 \nSmith, C. R., et al. (1984). Computers come of age in writing instruction. Computers and the Humanities, \n18(3/4), 215–24. \nSolaiman, I., et al. (2019, November 13). Release strategies and the social impacts of language models. \nOpenAI. https://www.doi.org/10.48550/arXiv.1908.09203 \nSouthern, M. G. (2022, April 6). Google says AI-generated content is against guidelines. Search Engine \nJournal. https://www.searchenginejournal.com/google-says-ai-generated-content-is-against-\nguidelines/444916/ \nTurilli, M., & Floridi, L. (2009). “The Ethics of Information Transparency.” Ethics and Information \nTechnology, 11, 105–12. \nTurnitin. (n.d.). Turnitin’s AI Writing Detection Capabilities. https://www.turnitin.com/products/features/ai-\nwriting-detection \nPiller | Page 27 of 27 \n \n \nu/walkerspider. (2022, December 13). Dan is my new friend. Reddit. \nhttps://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend  \nUlanoff, L. (2023, April 12). ChatGPT is now writing college essays, and higher ed has a big problem. \nTechRadar. https://www.techradar.com/news/i-had-chatgpt-write-my-college-essay-and-now-im-\nready-to-go-back-to-school-and-do-nothing \nUndetectable.ai. (2023, March 14). AI Authorship Replication—for Small Businesses. \nhttps://blog.undetectable.ai/index.php/2023/03/15/ai-authorship-recognition-ai-detection-bypass/ \nWalsh, T. (2022, December 13). Everyone’s having a field day with ChatGPT—but nobody knows how it \nactually works. The Conversation. https://theconversation.com/everyones-having-a-field-day-with-\nchatgpt-but-nobody-knows-how-it-actually-works-196378 \nWhite House Office of Science and Technology Policy. (2022, October). Blueprint for an AI bill of rights: \nMaking automated systems work for the American people. https://www.whitehouse.gov/wp-\ncontent/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf \nWresch, W. (Ed.). (1984). The computer in composition instruction: A writer’s tool. National Council of \nTeachers of English. \nYancey, K. B. (2018). “With fresh eyes”: Notes toward the impact of new technologies on c omposing. In J. \nAlexander & J. Rhodes (Eds.), The Routledge handbook of digital writing and rhetoric (pp. 61–72). \nRoutledge. \nYang, M. (2023, January 6). New York City schools ban AI chatbot that writes essays and answers prompts. \nThe Guardian. https://www.theguardian.com/us-news/2023/jan/06/new-york-city-schools-ban-ai-\nchatbot-chatgpt \nZhang, X., et al. (2022). Automatic product copywriting for e-commerce. The Thirty-Sixth AAAI Conference \non Artificial Intelligence (pp. 12423–31). arXiv. https://doi.org/10.48550/arXiv.2112.11915 \n \n ",
  "topic": "Transparency (behavior)",
  "concepts": [
    {
      "name": "Transparency (behavior)",
      "score": 0.7864016890525818
    },
    {
      "name": "Relevance (law)",
      "score": 0.7180957794189453
    },
    {
      "name": "Generative grammar",
      "score": 0.5555633306503296
    },
    {
      "name": "Engineering ethics",
      "score": 0.5252097249031067
    },
    {
      "name": "Work (physics)",
      "score": 0.4618472456932068
    },
    {
      "name": "Sociology",
      "score": 0.4495292901992798
    },
    {
      "name": "Professional ethics",
      "score": 0.4156968593597412
    },
    {
      "name": "Psychology",
      "score": 0.3897973895072937
    },
    {
      "name": "Computer science",
      "score": 0.24345552921295166
    },
    {
      "name": "Political science",
      "score": 0.18120306730270386
    },
    {
      "name": "Engineering",
      "score": 0.12454015016555786
    },
    {
      "name": "Law",
      "score": 0.09437966346740723
    },
    {
      "name": "Artificial intelligence",
      "score": 0.0627480149269104
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}