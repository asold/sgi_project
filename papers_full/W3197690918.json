{
    "title": "AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models",
    "url": "https://openalex.org/W3197690918",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2966945397",
            "name": "Harish Tayyar Madabushi",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A4287855517",
            "name": "Edward Gow-Smith",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A2169929857",
            "name": "Carolina Scarton",
            "affiliations": [
                "University of Sheffield"
            ]
        },
        {
            "id": "https://openalex.org/A2157257934",
            "name": "Aline Villavicencio",
            "affiliations": [
                "University of Sheffield"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1984052055",
        "https://openalex.org/W2966176804",
        "https://openalex.org/W1976632482",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2074228526",
        "https://openalex.org/W2898914189",
        "https://openalex.org/W630883834",
        "https://openalex.org/W2664496537",
        "https://openalex.org/W3100475986",
        "https://openalex.org/W2962688231",
        "https://openalex.org/W3008931407",
        "https://openalex.org/W2576501720",
        "https://openalex.org/W2096733369",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2176363684",
        "https://openalex.org/W2439228446",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W1847856012",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2508116074",
        "https://openalex.org/W25517462",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2962694015",
        "https://openalex.org/W2572185161",
        "https://openalex.org/W2963340990",
        "https://openalex.org/W638675188",
        "https://openalex.org/W2126168798",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2115723735",
        "https://openalex.org/W3119135819",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W1781420840",
        "https://openalex.org/W1255655817",
        "https://openalex.org/W2619927796",
        "https://openalex.org/W3155825510",
        "https://openalex.org/W1498763386"
    ],
    "abstract": "Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are urgently needed. Existing datasets are limited to providing the degree of idiomaticity of expressions along with the literal and, where applicable, (a single) non-literal interpretation of MWEs. This work presents a novel dataset of naturally occurring sentences containing MWEs manually classified into a fine-grained set of meanings, spanning both English and Portuguese. We use this dataset in two tasks designed to test i) a language model’s ability to detect idiom usage, and ii) the effectiveness of a language model in generating representations of sentences containing idioms. Our experiments demonstrate that, on the task of detecting idiomatic usage, these models perform reasonably well in the one-shot and few-shot scenarios, but that there is significant scope for improvement in the zero-shot scenario. On the task of representing idiomaticity, we find that pre-training is not always effective, while fine-tuning could provide a sample efficient method of learning representations of sentences containing MWEs.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3464–3477\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3464\nAStitchInLanguageModels: Dataset and Methods for the Exploration of\nIdiomaticity in Pre-Trained Language Models\nHarish Tayyar Madabushi, Edward Gow-Smith,\nCarolina Scarton and Aline Villavicencio\nDepartment of Computer Science, University of Shefﬁeld\nUnited Kingdom\n{h.tayyarmadabushi, egow-smith1, c.scarton, a.villavicencio}\n@sheffield.ac.uk\nAbstract\nDespite their success in a variety of NLP\ntasks, pre-trained language models, due to\ntheir heavy reliance on compositionality, fail\nin effectively capturing the meanings of mul-\ntiword expressions (MWEs), especially id-\nioms. Therefore, datasets and methods to im-\nprove the representation of MWEs are urgently\nneeded. Existing datasets are limited to provid-\ning the degree of idiomaticity of expressions\nalong with the literal and, where applicable,\n(a single) non-literal interpretation of MWEs.\nThis work presents a novel dataset of naturally\noccurring sentences containing MWEs manu-\nally classiﬁed into a ﬁne-grained set of mean-\nings, spanning both English and Portuguese.\nWe use this dataset in two tasks designed to\ntest i) a language model’s ability to detect id-\niom usage, and ii) the effectiveness of a lan-\nguage model in generating representations of\nsentences containing idioms. Our experiments\ndemonstrate that, on the task of detecting id-\niomatic usage, these models perform reason-\nably well in the one-shot and few-shot scenar-\nios, but that there is signiﬁcant scope for im-\nprovement in the zero-shot scenario. On the\ntask of representing idiomaticity, we ﬁnd that\npre-training is not always effective, while ﬁne-\ntuning could provide a sample efﬁcient method\nof learning representations of sentences con-\ntaining MWEs.\n1 Introduction and Motivation\nPre-trained language models such as BERT (De-\nvlin et al., 2019) and XLNet (Yang et al., 2019)\nhave been widely used in a variety of Natural Lan-\nguage Processing tasks. Despite their success in\nmultiple downstream applications, such as sentence\nclassiﬁcation (Zhang et al., 2019) and reading com-\nprehension (Raffel et al., 2019), they are unable\nto effectively represent idiomatic multiword ex-\npressions (MWEs) (Yu and Ettinger, 2020; Garcia\net al., 2021). Capturing idiomaticity is particularly\nchallenging as the representations of words and\nphrases are explicitly designed to be compositional\nboth in non-contextual (Mitchell and Lapata, 2010;\nMikolov et al., 2013b) and contextual embedding\nmodels. Pre-trained language models in particu-\nlar exploit compositionality at both the word and\nsub-word levels (Devlin et al., 2019) to reduce the\nsize of their vocabulary, which makes represent-\ning idiomatic phrases particularly challenging. The\neffective representation of idiomatic MWEs is crit-\nical for them to be correctly interpreted in down-\nstream tasks. Such an improvement will beneﬁt\nboth classiﬁcation-based problems (e.g. sentiment\nanalysis) and sequence-to-sequence tasks (e.g. ma-\nchine translation).\nTo this end, we present a dataset consisting of\nnaturally occurring sentences containing poten-\ntially idiomatic MWEs and two tasks aimed at\nevaluating language models’ ability to effectively\ndetect and represent idiomaticity. The primary con-\ntributions of this work are:\n1. A novel dataset consisting of:\n(a) naturally occurring sentences (and two sur-\nrounding sentences) containing potentially\nidiomatic MWEs annotated with a ﬁne-\ngrained set of meanings: compositional\nmeaning, idiomatic meaning(s), proper\nnoun and “meta usage”;\n(b) paraphrases for each meaning of each\nMWE;\n2. Two tasks aiming at evaluating i) a model’s abil-\nity to detect idiomatic usage, and ii) the effec-\ntiveness of sentence embeddings in representing\nidiomaticity. Table 1 provides details of these\ntasks and associated subtasks, each designed to\ntest different aspects of models.\n(a) These tasks are presented in multilingual,\nzero-shot, one-shot and few-shot settings.\n(b) We provide strong baselines using state-of-\nthe-art models, including experiments with\none-shot and few-shot setups for idiomatic-\n3465\nity detection and the use of the idiom prin-\nciple for detecting and representing MWEs\nin contextual embeddings. Our results high-\nlight the signiﬁcant scope for improvement.\nTask 1\nSubtask A Coarse-grained classiﬁcation of\nexamples containing idioms.\nSubtask B Fine-grained classiﬁcation of\nexamples into meanings.\nTask 2\nSubtask A\nEffective representation of sentences\ncontaining idiomatic phrases using\nonly pre-training.\nSubtask B\nEffective representation of sentences\nusing both pre-training and\nﬁne-tuning.\nTable 1: AStitchInLanguageModels Tasks: The two\ntasks and associated subtasks.\nThis dataset and associated tasks have the po-\ntential to catalyse research into representing more\ncomplex elements of language beginning with id-\niomaticity, thus ensuring a timelystitch in language\nmodels. We call this dataset and associated tasks\nAStitchInLanguageModels, and make the dataset,\nthe associated splits for each task, pre-training data,\npre-trained and ﬁne-tuned models, program code\nand associated processing scripts, including hyper-\nparameters, publicly available in the interest of\nreproducibility and for subsequent reuse1.\nThis paper is organised as follows: Section 2\npresents a discussion of related work. We then\npresent AStitchInLanguageModels consisting of\nthe novel MWE dataset and the two associated\ntasks in Section 3. We discuss our experiments\nand results for these two tasks in Section 4, before\npresenting a discussion of the more interesting ele-\nments of our ﬁndings in Section 5. We present our\nconclusions and possible avenues of future work in\nSection 6.\n2 Related work\nThe problems posed by MWEs to NLP models have\nbeen known for some time (Sag et al., 2002; Con-\nstant et al., 2017; Shwartz and Dagan, 2019). For\ninstance, Sag et al. (2002) refer to the idiomatic-\nity problem and place the need for effective pro-\ncessing of MWEs on par with that for word sense\ndisambiguation to be able to effectively process\ntext. While their analysis focused on symbolic\nmethods, this problem still persists: Shwartz and\nDagan (2019) showed, using six tasks, that con-\n1https://github.com/H-TayyarMadabushi\n/AStitchInLanguageModels\ntextual pre-trained language models, capable of\nhandling polysemy, continued to be unable to ef-\nfectively handle idiomatic MWEs, although they\ntend to do better than their non-contextual predeces-\nsors. Further experiments with probing pre-trained\nlanguage models across multiple languages have\nalso conﬁrmed this result (Yu and Ettinger, 2020;\nGarcia et al., 2021).\n2.1 Existing Datasets\nDatasets of MWE annotated corpora include that\nassociated with the PARSEME shared task (Savary\net al., 2017) which focuses on verbal MWEs and\nthe STREUSLE dataset (Schneider et al., 2014;\nSchneider and Smith, 2015; Schneider et al., 2016)\nwhich includes noun, verb, prepositional and pos-\nsessive expressions including “semantic super-\nsenses”. However, most existing datasets associ-\nated with compositionality of MWEs consist of\nisolated phrases, labelled with overall composi-\ntionality scores (Venkatapathy and Joshi, 2005;\nBiemann and Giesbrecht, 2011; Farahmand et al.,\n2015), scores of how individual words contribute\nto the meaning of the MWE (Venkatapathy and\nJoshi, 2005), or both (Reddy et al., 2011; Cordeiro\net al., 2019; Schulte im Walde et al., 2016).\nWhile most of these target only English, some in-\nclude scores for other languages such as German\n(Schulte im Walde et al., 2016), and French and\nPortuguese (Cordeiro et al., 2019).\nExisting datasets of compositionality that in-\nclude context often add context automatically by\nﬁrst selecting MWEs that are either only com-\npositional or only idiomatic. For instance, the\nVNC-Tokens Dataset (Cook et al., 2008) consists\nof 53 English MWEs each with a maximum of\n100 sentences extracted from the BNC, while Tu\nand Roth (2012) collected 1,348 sentences associ-\nated with 23 verb phrases annotated as composi-\ntional and idiomatic. Shwartz and Dagan (2019)\nfocused on a subset of noun compounds that are\nonly compositional or idiomatic from the dataset\nprovided by Reddy et al. (2011) and automatically\nadded sentences from Wikipedia. Finally, the NCS\nDataset (Garcia et al., 2021) consists of 280 En-\nglish and 180 Portuguese MWEs, annotated with\ndegrees of compositionality and three sentences\ncontaining each of the MWEs.\nDespite the importance of the context surround-\ning an MWE, where available, context, in the form\nof sentences containing MWEs, is available only\n3466\nfor those MWEs that are either idiomatic or com-\npositional. This signiﬁcant shortcoming makes it\nimpossible to train models to learn to differentiate\nbetween the compositional and idiomatic usage of\nthe same MWE.\nFinally, while existing datasets also provide para-\nphrases for the compositional and idiomatic mean-\nings of MWEs (Hendrickx et al., 2013; Garcia et al.,\n2021), they are limited to having exactly one com-\npositional and one idiomatic meaning, which is not\nalways the case as is exempliﬁed by the phrase\n“head hunter” which, while not having a literal us-\nage, has multiple idiomatic meanings (i.e recruiter,\nbaseball pitcher who aims for the head, and hunter).\nAStitchInLanguageModels is designed to alle-\nviate these shortcomings, speciﬁcally: a) the lack\nof context sentences, b) the need for ﬁne grained\nclassiﬁcation of MWEs, and a more complete set\nof paraphrases for all possible meanings of MWEs\n(Section 3).\n2.2 Methods\nThe task of identifying idiomaticity in sentences\nwas initially addressed by use of symbolic meth-\nods (Baldwin and Villavicencio, 2002; Sag et al.,\n2002), statistical properties of text such as mutual\ninformation (Lin, 1999), and latent semantic analy-\nsis (Baldwin et al., 2003).\nThe subsequent adoption of distributional seman-\ntics led to the use of constituent word embeddings\nto determine the compositionality of phrases, such\nas in the work by Katz and Giesbrecht (2006) who\nmade use of the semantic similarity between the\ndistributional vectors associated with an MWE as\na whole and those associated with its parts to de-\ntermine compositionality. This is achieved by use\nof a single token to represent an MWE. This trend\ncontinued with the introduction of neural distribu-\ntional semantic models such as word2vec (Mikolov\net al., 2013a) wherein MWEs were taken as sin-\ngle units in learning embeddings (Mikolov et al.,\n2013b). This method was improved upon by use\nof an explicit disambiguation step prior to com-\nposition (Kartsaklis et al., 2014), and by the joint\nlearning of compositional and idiomatic embed-\ndings using a “compositionality scoring” func-\ntion (Hashimoto and Tsuruoka, 2016). This “single\ntoken” method has the advantage of being rooted in\nthe linguistic idiom principle (Sinclair et al., 1991),\nwhich postulates that humans process idioms by\ntreating them as a “single independent token”.\nDespite being the only method of handling\nMWEs and having had relative success, it is not\nwithout its shortcomings. The ﬁrst is that the fre-\nquency of MWEs tends to be low (a problem that\nworsens with the increase in length of MWEs) and\nsince the quality of distributional representations\ntends be proportional to the number of instances of\na token, representations of MWEs are often lacking.\nThe second is that non-contextual type level repre-\nsentations are inherently limited as MWEs often\nhave multiple meanings, as detailed in Section 2.1.\nWhile contextual embeddings can handle pol-\nysemy, they fail to fully capture the meaning of\nMWEs as discussed earlier. How contextual em-\nbeddings fair in comparison to their non-contextual\npredecessors is not entirely clear as Nandakumar\net al. (2019) found that they do worse on some tasks\nwhile Shwartz and Dagan (2019) found that they\ndo better. Hashempour and Villavicencio (2020)\nadopted the idiom principle (MWE as a single to-\nken) with contextual language models (speciﬁcally\nBERT), and found that this method does not beneﬁt\ntransformer-based pre-trained models. However,\nthey did not introduce a new token to represent\neach MWE as is required during the training of\nnon-contextual models built on the idiom principle,\nbut instead replaced MWEs with a single token in\nthe input and rely on BERT’s word-piece tokenizer.\nTo the best of our knowledge this work is the ﬁrst to\nintroduce new tokens for MWEs into a contextual\npre-trained language model (see Section 4.2).\n3 AStitchInLanguageModels: Dataset\nand Tasks\nTo create a dataset and tasks aimed at improving\nlanguage models’ ability to identify and capture\nidiomaticity, we ﬁrst collected examples of MWE\nusage in naturally occurring sentences along with\nthe two surrounding sentences. We then annotated\nthese examples with a ﬁne-grained set of mean-\nings associated with each usage. We restrict our\nattention to noun compounds, a subset of idiomatic\nMWEs, sourced from the Noun Compound Senses\n(NCS) dataset (Cordeiro et al., 2019), which ex-\ntends the dataset by Reddy et al. (2011).\n3.1 Data Collection and Annotation\nA total of 12 judges were asked to collect exam-\nples containing a list of MWEs occurring natu-\nrally in context, in both English and Portuguese.\nFor each MWE, judges were instructed to obtain\n3467\n7 to 10 examples of each meaning ( “Idiomatic”,\n“Non-Idiomatic”, “Proper Noun” and “Meta Us-\nage”) where possible, with between 20 and 30 total\nexamples for each MWE. We deﬁne “Meta Usage”\nto be the literal use of an MWE in a metaphor (e.g.\nlife vest in “Let the Word of God be our life vest to\nkeep us aﬂoat, so as not to drown.”). Judges were\nadditionally instructed to add to the list of possible\nmeanings associated with each MWE based on the\nusage they observed when collecting examples, or\nto ﬂag examples with novel usage for review by\nlanguage experts. Emphasis was put on extract-\ning high-quality examples with three contiguous\nsentences and correct formatting, containing no\nunusual characters. The data consists of excerpts\nof text from the web, each a maximum of three\nsentences, thus adhering to fair use.\nThe meanings of each MWE were then para-\nphrased by language experts. The idiomatic para-\nphrases aim to concisely convey the meaning of the\nidiom. For example, cutting edge is paraphrased\nto most advanced and night owl is paraphrased to\nnocturnal person in the idiomatic case. The aim\nof the literal paraphrase is to apply a minimal lex-\nical alteration that shifts the MWE away from its\nidiomatic meaning(s). For example, cutting edge is\nparaphrased to slicing edge and night owl is para-\nphrased to night hooter in the literal case. This ad-\nversarial paraphrasing is designed to test a model’s\nability to discern a compositional meaning from an\nidiomatic one, and aims to ensure that models must\nhave a nuanced understanding of idiomaticity for\nthem to be successful. Examples of the annotated\ndata are shown in Table 2.\nFinally, each example was annotated with a label\nand corresponding paraphrase by two judges. The\nCohen’s kappa coefﬁcient of inter-rater reliability\nwas 0.887 for English and 0.807 for Portuguese.\nWe note that a signiﬁcant proportion of disagree-\nments arose from a difference in interpretation of\nthe “Proper Noun” and “Meta Usage” labels, and\nfrom what constituted “low quality” for discarding\nexamples. For resolution of disagreements a ﬁnal\nlabel was decided based on a discussion between\nthe judges.\n3.2 The Final Dataset\nThe ﬁnal dataset consists of 4,558 English exam-\nples containing 223 MWEs, and 1,872 Portuguese\nexamples containing 113 MWEs.\nWe divide this data into training, development\nand test splits as follows: the test and development\nsplits consist of sentences containing 30 and 20 id-\nioms each in English and Portuguese respectively.\nTo enable the testing of models under different\nscenarios of data availability, we create three dif-\nferent setups of the test split for each language.\nThe ﬁrst, the zero-shot setup, consists of sentences\ncontaining 163 idioms in English and 60 idioms\nin Portuguese, which do not occur in the devel-\nopment and test sets. The second, the one-shot\nsetup, consists of exactly one non-idiomatic and\n(where available) one idiomatic example associated\nwith each MWE in the development and test sets.\nThe third and ﬁnal, the few-shot setup, consists of\nbetween 1 and 4 examples associated with each\nmeaning of each MWE in the development and test\nsets. The exact number of examples available is\nproportional to the original number of examples\nassociated with that speciﬁc meaning of that idiom.\nWe make it clear that there are no overlapping target\nsentences between the three splits - the only over-\nlap is in terms of the idioms contained in examples.\nDetailed statistics for the English and Portuguese\ndatasets are provided in Appendix A.\n3.3 Tasks\nIn addition to the dataset of labelled contextualised\nMWEs, we present two tasks.\n3.3.1 Task 1: Idiomaticity Detection\nThe ﬁrst task we propose is designed to evaluate the\nextent to which models can identify idiomaticity\nin text and consists of two Subtasks: a coarse-\ngrained classiﬁcation task (Subtask A) and a ﬁne-\ngrained classiﬁcation task (Subtask B). For the\ncoarse-grained subtask, the problem is simpliﬁed\nto classifying the examples as either “Idiomatic” or\n“Non-Idiomatic”. For the purposes of this subtask,\nanything labelled as “Literal” or “Proper Noun”\nwas classed as “Non-Idiomatic” and given a label\nof 1, whilst all “Idiomatic” labels as well as “Meta\nUsage” were given a label of 0. (See also Table 2).\nFor the ﬁne-grained task, the possible meanings\nare equivalent to the paraphrases in the dataset, de-\nscribed previously. Since this problem does not\nhave a ﬁxed number of labels (given that each\nMWE has a different set of meanings), we con-\nvert this to a binary classiﬁcation problem: the ﬁrst\ninput is the example containing the MWE and the\nsecond the paraphrase of each possible meaning\nof the MWE (or one of the phrases “Proper Noun”\nor “Meta Usage”). An input pair is labelled 1 if\n3468\nMWE Target Sentence Previous Sentence Next Sentence Label Idiomatic? Paraphrase\ngold mineThis means that search data is a\ngold minefor marketing strategy.\n(marketingweek.com)\nThe data that those\nsearches generate\nbuilds...\nIt reveals which\ntypes of product...\nIdiomatic 1 Yes source of fortune\ngold mineThe hashtag “Qixiagold minein-\ncident” has been viewed many\nmillion of times on the social me-\ndia site Weibo. (wsws.org)\nThe rescue opera-\ntion took place...\nA week after the ex-\nplosion...\nLiteral No mine\ngold mineTheGold Mine’s plain frontage\n& sparse, white-walled dining\nroom suggest that it’s a quick-ﬁx\nrefuelling stop rather than a place\nto linger. (squaremeal.co.uk)\nSquareMeal Re-\nview of Gold\nMine\nThe menu touts a\nbewildering array\nof dishes...\nProper Noun No Proper Noun\nTable 2: A sample of the dataset for one MWE ( gold mine). Context sentences are truncated for brevity. The\n\"Idiomatic?\" column is used for the coarse-grained classiﬁcation task (Subtask 1 A) and the \"Paraphrase\" column\nis used for the ﬁne-grained classiﬁcation task (Subtask 1 B) and the representation task (Task 2).\nthe paraphrase represents the correct meaning of\nthe MWE in the example and 0 otherwise. In ad-\ndition, we report scores for both subtasks in the\nzero-shot, one-shot and few-shot setups to better\nevaluate a model’s ability to generalise and learn\nin a sample efﬁcient fashion. We note that this was\nimpossible prior to the introduction of AStitchIn-\nLanguageModels as all previous datasets which\nincluded context considered only one meaning per\nMWE (see Section 2.1). Due to the imbalanced\nnature of these subtasks, we use Macro F1-score as\nthe measure of evaluation.\nWe note that due to the different ways in which\nthe two settings in this Task are setup the results\nfor the two settings in this task are not directly\ncomparable.\n3.3.2 Task 2: Idiomaticity Representation\nWhile the identiﬁcation of idiomaticity is impor-\ntant, downstream tasks require embeddings that\neffectively capture idiomaticity, which is the pur-\npose of the second task. For this task, we design\na metric to measure how consistent a model is in\ncapturing similarity between sentences containing\nidiomatic elements and sentences that are purely\ncompositional.\nAs each possible meaning of an MWE contained\nin each example is associated with a paraphrase,\nthis task requires a model to generate similarity\nscores for each example Esuch that:\n∀i∈I\n(\nsim(E,E→c) = 1;\nsim(E,E→i) =sim(E→c,E→i)\n) (1)\nwhere E→c represents the example with the MWE\nin E replaced by the paraphrase of the correct\nmeaning associated with the MWE, and E→i the\nexample with the MWE replaced by a paraphrase\nof one of the incorrect meanings of the MWE in\nE(see Table 3 for examples).\nWithout additional checks, models can trivially\nsucceed in this task by simply assigning a simi-\nlarity score of 1 to every sentence pair. To pre-\nvent this, we splice in development and test data\nfrom the Semantic Text Similarity (STS) bench-\nmark dataset (Cer et al., 2017) in English and from\nthe ASSIN2 STS dataset (Real et al., 2020) for\nPortuguese.\nWe note that the expected similarity scores are\napproximates as the paraphrases need not have ex-\nactly the same meanings as that of the MWE they\nare paraphrasing. However, we consider this dif-\nference to be acceptable given the typical nature\nof annotation of semantic similarity data wherein\nannotators use labels between 1 and 5.\nFinally, we divide this task into two subtasks:\nSubtask A which requires the solving of this task\nusing only pre-training and Subtask B which al-\nlows the ﬁne-tuning of models. For clarity, we\ndeﬁne pre-training to be the training of a model\non any task other than idiomatic STS (and can in-\nclude “ﬁne-tuning” on a different task), and ﬁne-\ntuning to include the inclusion of training on any\nSTS dataset which includes potentially idiomatic\nMWEs. We use Spearman correlation coefﬁcient\nas the measure of evaluation for both subtasks in\nTask 2 as it has been shown that Pearson correlation\nis poorly suited for comparing performance on the\nSTS task (Reimers et al., 2016).\n4 Experiments and Results\nOur aim was to investigate the performance of state-\nof-the-art transformer-based pre-trained language\nmodels on these tasks, and how their performance\nvaried with different input features (i.e. inclusion\n3469\nSentence (E) Correct Replacement (EMWE→c) Wrong Replacement (EMWE→i) Expected\nWhen removing a big ﬁsh\nfrom a net, it should be held in\na manner that supports the\ngirth. (newsdakota.com)\nWhen removing a ﬁsh from a\nnet, it should be held in a\nmanner that supports the girth.\nWhen removing a important\nperson from a net, it should be\nheld in a manner that supports\nthe girth.\nsim(E,E→c) = 1\nsim(E,E→i) =sim(E→c,E→i)\nTo pay attention only to new\nhousing and houses I think\nskews the big picture.\n(streets.mn)\nTo pay attention only to new\nhousing and houses I think\nskews the whole situation.\nTo pay attention only to new\nhousing and houses I think\nskews the large image.\nsim(E,E→c) = 1\nsim(E,E→i) =sim(E→c,E→i)\nTable 3: Task 2 - Models are required to be consistent in assigning semantic similarity scores as measured by use\nof the paraphrases of different meanings.\nof MWE, context sentences), problem setups (i.e.\nzero-shot, one-shot and few-shot), and training\nregimes (i.e. pre-training, ﬁne-tuning) so as to\nprovide a baseline for the AStitchInLanguageMod-\nels dataset. Here we provide an overview of the\nexperiments ran and our results. More detailed de-\nscription of the experimental procedure, including\nruntimes are given in Appendix B.\n4.1 Task 1: Idiomaticity Detection\nFor Subtask A, which requires the coarse-grained\nclassiﬁcation of examples, we start by exploring the\nimpact of three variables in the zero-shot setup: the\npre-trained language model, the inclusion of con-\ntext (the two surrounding sentences), and adding\nthe relevant MWE as a feature. The context is\nincluded by simply concatenating the three con-\ntiguous sentences, and the MWE is included by\nseparating it from the rest of the input by use of\nthe “[SEP]” token. For the purposes of brevity,\nwe report a subset of variations highlighting the\nmost interesting results, with more details of the\nexperimental procedure in Appendix B. Among\nthe results for Task 1, Subtask A (Table 4), the best-\nperforming experimental settings from the zero-\nshot setting (by development F1 score) were trans-\nferred over to the one-shot and few-shot problem\nsetups. While the inclusion of context (surround-\ning sentences) did not change the performance of\nthe models signiﬁcantly, and will not be used in\nthe other experiments, the inclusion of the relevant\nMWE was found to be beneﬁcial to performance.\nFor Subtask B, ﬁne-grained classiﬁcation, the\nbest-performing experimental settings found for\nthe ﬁrst subtask were used for the multiclass data,\nalthough the MWE was not included as a feature,\nsince our previous method for inclusion is incom-\npatible with the passing of the paraphrase; the input\nconsists of the target sentence without the previ-\nous or next sentences followed by a single possible\nmeaning of the MWE separated by the “[SEP]”\ntoken. The task is thus reduced to a binary classiﬁ-\ncation task wherein the model is required to predict\n1 when the target sentence is followed by the cor-\nrect paraphrase and 0 otherwise. The results are in\nTable 5.\n4.2 Task 2: Idiomaticity Representation\nTask 2 requires models to output the semantic sim-\nilarity between sentences in a consistent manner.\nGiven that sentence embeddings generated by pre-\ntrained language models cannot directly be used to\ncalculate semantic similarity (Devlin et al., 2019),\nwe used Sentence BERT (Reimers and Gurevych,\n2019) which consists of a siamese network struc-\nture with a regression objective function consist-\ning of the mean-squared error loss calculated over\nthe cosine similarity of two input sentences during\ntraining. This results in sentences whose semantic\nsimilarity can be compared using cosine similar-\nity (Schroff et al., 2015). We note that while this\nis not strictly required for our purpose, we use this\nmethod as the siamese network structure is likely\nto be beneﬁcial in ﬁne-tuning on the idiomatic STS\ndata where the similarity scores are all relatively\nclose to each other.\nTo test the effectiveness of the idiom principle to\nrepresent MWEs (Section 2.2) for Task 2, we anal-\nyse three different settings, involving the expansion\nof the vocabulary of pre-trained models by the ad-\ndition of a single token to represent each MWE: In\nthe ﬁrst setting (“all replace”) all instances of an\nMWE are replaced with the corresponding token\nbefore input to the model; in the second (“select re-\nplace”) each input sentence is ﬁrst classiﬁed using\nthe one-shot model for course grained classiﬁca-\ntion (Section 4.1) and a given instance of an MWE\nis replaced only when the one-shot model predicts\nthat the MWE in a given sentence has an idiomatic\nmeaning; and in the third (“no replace”) there is no\nchange to either the model (no special token added)\nor their input.\nFor Subtask A, which requires the use of\nonly pre-training, we collect sentences (including,\n3470\nProblem Setup Model Context? MWE? Dev F1 Test F1\nEnglish\nzero-shot\nBERT base (cased) No No 0.724 0.688\nBERT base (cased) Yes No 0.717 0.797\nBERT base (cased) Yes Yes 0.779 0.774\nBERT base (cased) No Yes 0.785 0.821\nXLNet base (cased) No Yes 0.823 0.832\none-shot XLNet base (cased) No Yes 0.897 0.874\none-shot XLNet base (cased) Yes No 0.689 0.701\none-shot XLNet base (cased) No No 0.755 0.754\nfew-shot XLNet base (cased) No Yes 0.959 0.971\nfew-shot XLNet base (cased) Yes No 0.782 0.806\nfew-shot XLNet base (cased) No No 0.792 0.853\nPortuguese\nzero-shot\nXLM-RoBERTa base (cased) No No 0.593 0.528\nXLM-RoBERTa base (cased) Yes No 0.542 0.562\nXLM-RoBERTa base (cased) Yes Yes 0.696 0.604\nXLM-RoBERTa base (cased) No Yes 0.703 0.579\nBERT base multilingual (cased) No Yes 0.686 0.560\none-shot XLM-RoBERTa base (cased) No Yes 0.877 0.778\none-shot XLM-RoBERTa base (cased) Yes No 0.605 0.563\none-shot XLM-RoBERTa base (cased) No No 0.638 0.534\nfew-shot XLM-RoBERTa base (cased) No Yes 0.926 0.944\nfew-shot XLM-RoBERTa base (cased) Yes No 0.655 0.684\nfew-shot XLM-RoBERTa base (cased) No No 0.796 0.696\nTable 4: Evaluation results for Task 1 Subtask A (with best results for each setting in bold).\nProblem Setup Model Dev F1 Test F1\nEn\nzero-shot XLNet base (cased) 0.852 0.875\none-shot XLNet base (cased) 0.923 0.927\nfew-shot XLNet base (cased) 0.933 0.948\nPt\nzero-shot XLM-RoBERTa base (cased) 0.843 0.778\none-shot XLM-RoBERTa base (cased) 0.852 0.858\nfew-shot XLM-RoBERTa base (cased) 0.909 0.878\nTable 5: Evaluation results for Task 1 Subtask B.\nwhere available, the paragraph they occur in) from\nthe Common Crawl News Dataset 2 spanning the\nﬁrst 6 months of 2020 (over half a terabyte of text).\nThis results in about 220,000 sentences in English\nand about 16,000 in Portuguese containing relevant\nMWEs. We use this data to continue pre-training\nBERT base in both the “all replace” and “select\nreplace” variations described above. Unlike our\nother experiments, we do not pre-train multiple\ntimes due to time and resource constraints. We also\nlimit pre-training to 5 epochs for English and 10\nepochs for Portuguese based on results from our\nexploratory experiments.\nIn addition to these two models, we also test\nBERT base with no modiﬁcations, and a version of\nBERT base with the addition of tokens associated\nwith each MWE but no pre-training (the embed-\ndings associated with these tokens are randomly\ninitialised). The “all replace” and “select replace”\nmodels have their pre-training and input sentences\ntokenized according to the same strategy. Each of\nthese models are subsequently trained using the\nSentence BERT architecture so as to ensure that\nthe resultant embeddings can be compared using\n2https://commoncrawl.org\ncosine similarity. We train using the training data\nfrom the STS benchmark dataset (Cer et al., 2017)\nfor English and the ASSIN2 STS dataset (Real\net al., 2020) for Portuguese. This training does\nnot violate the “pre-train only” requirement of this\ntask as we do not train on idiomatic STS data. The\nresults are presented in Table 6.\nTokenization Dev ρ Test ρ\nEnglish\nDefault 0.767 0.744\nAll Tokenized (No\nPre-Training) 0.826 0.801\nAll Tokenized 0.835 0.811\nSelect Tokenized 0.848 0.805\nPortuguese\nDefault 0.726 0.785\nAll Tokenized (No\nPre-Training) 0.749 0.798\nAll Tokenized 0.742 0.805\nSelect Tokenized 0.750 0.814\nTable 6: Results for Task 2 Subtask A.\nFor Subtask B, we ﬁne-tune the “no replace”,\n“all replace” and “select replace” versions of BERT\nbase on both the standard STS data as in Subtask\nA and training data constructed from the zero-shot\nand few-shot version of the training data using\nEquation 1. Therefore, during ﬁne-tuning, the\ngold similarity score forsim(E,E→c) is 1 and that\n3471\nfor sim(E,E→i) is sim(E→c,E→i). Both the “re-\nplace” versions of the model are ﬁne-tuned from\nscratch (i.e. the tokens associated with MWE are\nrandom and not pre-trained as in Subtask A). Al-\nthough it is possible to start with the pre-trained\nversion of the “replace” models, we make the con-\nscious decision not to, so we might test if this sam-\nple efﬁcient method of learning is feasible. The\nresults for these models are presented in Table 7.\nTokenization Dev ρ Test ρ\nEn\nDefault 0.818 0.823\nAll Tokenized 0.821 0.817\nSelect Tokenized 0.851 0.825\nPt\nDefault 0.752 0.811\nAll Tokenized 0.803 0.835\nSelect Tokenized 0.806 0.818\nTable 7: Results for Task 2 Subtask B.\n5 Discussion\nThis section discusses some highlights of our re-\nsults.\n5.1 Detection of Idiomaticity\nIn the task of detecting idiomaticity (Task 1), we\nﬁnd that in the zero-shot setting, the models per-\nform poorly in both the coarse-grained and ﬁne-\ngrained subtasks. This shows there is still signiﬁ-\ncant room for improvement in this task. The most\ninteresting result was that models perform surpris-\ningly well in the one-shot and few-shot setups. This\nis a novel observation, made possible by the unique\nnature of this dataset and is likely to be very help-\nful in developing methods of identifying idiomatic\nlanguage.\nWe found that including context sentences did\nnot always lead to signiﬁcantly improved model\nperformance. Intuitively, one would expect an in-\ncrease in performance due to the availability of\nmore relevant data. A possible reason we did\nnot observe this in our experiments is that we in-\ncluded context by simply concatenating the three\nsentences, which means the model has no aware-\nness of which sentence is relevant and could be\ndeceived by surrounding sentences containing id-\niomatic expressions, for example. However, in the\nzero shot setting, including the context while ex-\ncluding the target MWE led to a signiﬁcant increase\nin generalisability as measured by the increased per-\nformance on the test set. This combination led to\nan increase of almost 8 points over the development\nset in English and 2 points in Portuguese where all\nother combinations led to a drop in performance\non the test set as compared to the development set.\nThe inclusion of the relevant MWE, was gen-\nerally found to be greatly beneﬁcial to model per-\nformance. The intuition behind this is that models\nare able to “focus” on the relevant MWE when\ndetermining idiomaticity. In the one and few shot\nsettings in particular, this inclusion signiﬁcantly\nboosted performance. When models had previ-\nously not encountered examples associated with a\nparticular MWE in the training data (as in the zero\nshot setting), including the MWE did less to boost\nperformance, although it still did improve results.\nThe only advantage of excluding the MWE was in\nhelping with generalisation as detailed above. In\nthe case of both MWE and context inclusion, we ex-\npect more sophisticated methods of incorporating\nthis information to further boost performance.\nWe note that results in English outperform those\nin Portuguese. We believe that this difference could\nbe a result of three factors: a) the fact that there\nis less training data available in Portuguese, b) be-\ncause models are pre-trained on signiﬁcantly less\nPortuguese data, and c) due to the higher degree of\ninﬂection in Portuguese.\n5.2 Representation of Idiomaticity\nRecall that the evaluation data for Task 2 included\ndata from standard STS datasets to ensure that the\ntask is not trivially solvable (Section 3.3.2). We\nreport results on only the MWE subset of the eval-\nuation data in Tables 8 and 9 for Subtasks A and B\nrespectively.\nTokenization EN\nNon-STS ρ\nPT\nNon-STS ρ\nDefault 0.219 0.203\nAll Tokenized (No\nPre-Training) 0.395 0.274\nAll Tokenized 0.459 0.369\nSelect Tokenized 0.437 0.332\nTable 8: Results on only the MWE subset of the Test\nsplit for Task 2 Subtask A.\nTokenization EN\nNon-STS ρ\nPT\nNon-STS ρ\nDefault 0.627 0.312\nAll Tokenized 0.611 0.379\nSelect Tokenized 0.618 0.416\nTable 9: Results on only the MWE subset of the Test\nsplit for Task 2 Subtask B.\nThese results show the signiﬁcant scope for\nimprovement in representing idiomaticity (given\n3472\nmodel performance on the standard STS bench-\nmark datasets is close 0.9ρ). Additionally, we note\nthat in Subtask A, which requires the use of only\npre-training, it is better to tokenize all pre-training\ndata, thus maximising the amount of training data,\nrather than selectively tokenizing training data. In\nSubtask B (ﬁne-tuning), however, selective tok-\nenization seems to have a slight advantage although\nthe default tokenization seems to be more suitable\nin English.\nThus, our experiments exploring the use of the id-\niomatic principle to capture idiomaticity in contex-\ntual pre-trained models (Task 2 Subtask A), show\nthat while replacing potential MWEs with a sin-\ngle token does improve performance, further pre-\ntraining with text tokenized either using “all re-\nplace” or “select replace” improves performance\nonly on the MWE subset of the evaluation split. On\nthe full test set, which includes standard STS data\n(Table 6), however, additional pre-training with\nMWE data does not always improve over a random\nrepresentation of MWE tokens and when it does,\nit does so only slightly. This is an interesting re-\nsult, and could be because gains made by the use\nof the idiom principle are offset by the continuing\nto pre-train on a relatively small set of sentences\nthat include a randomly initialised token added to\nthe vocabulary, or because the gains made on the\nMWE subset are diluted across the entire test split.\nExperiments using ﬁne-tuning (Task 2 Subtask\nB, see Section 4.2, Table 6) show, unsurprisingly,\nthat pre-trained language models are extremely ef-\nfective in transfer learning. What is particularly\ninteresting, though, is that starting with random em-\nbeddings for tokens representing MWEs can lead\nto comparative (and in some cases slightly better)\nscores. This suggests that these tokens have at least\na reasonable representation level, thus providing\na sample efﬁcient method of learning embeddings\nfor them. However, further experiments on differ-\nent tasks are required to test the extent to which\nthese tokens have been trained.\n6 Conclusions and Future Work\nIn this work we presented a novel dataset of natu-\nrally occurring idiomatic MWE usage in English\nand Portuguese, with associated tasks aimed at test-\ning the ability of language models to deal with\nidiomaticity. In addition, we ran a number of ex-\nperiments on these tasks.\nIn terms of idiomaticity detection, the results of\nour experiments show these models achieve rea-\nsonable performance in the one-shot and few-shot\nsettings, but particularly struggle with the zero-shot\nsetting, where the models encounter unseen MWEs\nat inference time.\nWhen it comes to the representation of idiomatic-\nity, our experiments show that while the use of the\nidiom principle does help in representing MWEs,\nthese gains do not transfer to a signiﬁcant over-\nall increase in performance on the entire test split.\nThe large number of MWEs makes including all\nof them in the vocabulary impractical, likewise\nselectively training models with MWEs of inter-\nest is impractical due to the cost of pre-training.\nThis underscores the need for a more nuanced ap-\nproach to incorporating the idiom principle with\npre-trained language models. Additionally, in cre-\nating representations for MWEs that are partially\ncompositional, methods that make use of the rep-\nresentations of constituent words such as attentive\nmimicking (Schick and Schütze, 2019) might be\nbeneﬁcial and we intend to experiment with these\nmethods in future. We also ﬁnd that pre-training is\npotentially an effective way of learning these repre-\nsentations, although more experiments are required\nto test these representations.\nThere are many avenues for future work using\nthe data presented here, including running cross-\nlingual experiments across different scenarios of\ndata availability. Although our experiments have\nbeen limited to the use of transformer based pre-\ntrained language models, the dataset and tasks\nwe present can be used with any language model.\nWhile this work provides a useful dataset for the\ninvestigation of idiomaticity, we intend to expand\nthis dataset in order to cover a broader set of lan-\nguages, and include a wider range of idiomatic\nMWE types, including more syntactically ﬂexible\nexpressions. One limitation of the dataset is that\nthe paraphrases generated are syntactically rigid,\nand for Task 2 the replacement sentences may not\nalways be grammatically correct (see Table 3). Al-\nthough this is sufﬁcient for current purposes, future\ndatasets could generate paraphrases per sentence\nrather than per MWE.\nAcknowledgements\nThis work was partially supported by the UK EP-\nSRC grant EP/T02450X/1 and the CDT in Speech\nand Language Technologies and their Applications\nfunded by UKRI (grant number EP/S023062/1).\n3473\nReferences\nTimothy Baldwin, Colin Bannard, Takaaki Tanaka,\nand Dominic Widdows. 2003. An empirical model\nof multiword expression decomposability. In Pro-\nceedings of the ACL 2003 Workshop on Multiword\nExpressions: Analysis, Acquisition and Treatment ,\npages 89–96, Sapporo, Japan. Association for Com-\nputational Linguistics.\nTimothy Baldwin and Aline Villavicencio. 2002. Ex-\ntracting the unextractable: A case study on verb-\nparticles. In COLING-02: The 6th Conference on\nNatural Language Learning 2002 (CoNLL-2002).\nChris Biemann and Eugenie Giesbrecht. 2011. Dis-\ntributional semantics and compositionality 2011:\nShared task description and results. In Proceedings\nof the Workshop on Distributional Semantics and\nCompositionality, pages 21–28, Portland, Oregon,\nUSA. Association for Computational Linguistics.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nMathieu Constant, Gül¸ sen Eryi ˇgit, Johanna Monti,\nLonneke van der Plas, Carlos Ramisch, Michael\nRosner, and Amalia Todirascu. 2017. Survey: Mul-\ntiword expression processing: A Survey. Computa-\ntional Linguistics, 43(4):837–892.\nPaul Cook, Afsaneh Fazly, and Suzanne Stevenson.\n2008. The VNCTokens Dataset. In In proceedings\nof the MWE workshop. ACL.\nSilvio Cordeiro, Aline Villavicencio, Marco Idiart, and\nCarlos Ramisch. 2019. Unsupervised composition-\nality prediction of nominal compounds. Computa-\ntional Linguistics, 45(1):1–57.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMeghdad Farahmand, Aaron Smith, and Joakim Nivre.\n2015. A multiword expression data set: Anno-\ntating non-compositionality and conventionalization\nfor English noun compounds. In Proceedings of\nthe 11th Workshop on Multiword Expressions, pages\n29–33, Denver, Colorado. Association for Computa-\ntional Linguistics.\nMarcos Garcia, Tiago Kramer Vieira, Carolina Scarton,\nMarco Idiart, and Aline Villavicencio. 2021. Prob-\ning for idiomaticity in vector space models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3551–3564, Online.\nAssociation for Computational Linguistics.\nReyhaneh Hashempour and Aline Villavicencio. 2020.\nLeveraging contextual embeddings and idiom prin-\nciple for detecting idiomaticity in potentially id-\niomatic expressions. In Proceedings of the Work-\nshop on the Cognitive Aspects of the Lexicon, pages\n72–80, Online. Association for Computational Lin-\nguistics.\nKazuma Hashimoto and Yoshimasa Tsuruoka. 2016.\nAdaptive joint learning of compositional and non-\ncompositional phrase embeddings. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 205–215, Berlin, Germany. Association for\nComputational Linguistics.\nIris Hendrickx, Zornitsa Kozareva, Preslav Nakov, Di-\narmuid Ó Séaghdha, Stan Szpakowicz, and Tony\nVeale. 2013. SemEval-2013 task 4: Free para-\nphrases of noun compounds. In Second Joint Con-\nference on Lexical and Computational Semantics\n(*SEM), Volume 2: Proceedings of the Seventh In-\nternational Workshop on Semantic Evaluation (Se-\nmEval 2013) , pages 138–143, Atlanta, Georgia,\nUSA. Association for Computational Linguistics.\nDimitri Kartsaklis, Nal Kalchbrenner, and Mehrnoosh\nSadrzadeh. 2014. Resolving lexical ambiguity in\ntensor regression models of meaning. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 212–217, Baltimore, Maryland. Associ-\nation for Computational Linguistics.\nGraham Katz and Eugenie Giesbrecht. 2006. Au-\ntomatic identiﬁcation of non-compositional multi-\nword expressions using latent semantic analysis. In\nProceedings of the Workshop on Multiword Expres-\nsions: Identifying and Exploiting Underlying Prop-\nerties, pages 12–19, Sydney, Australia. Association\nfor Computational Linguistics.\nDekang Lin. 1999. Automatic identiﬁcation of non-\ncompositional phrases. In Proceedings of the 37th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , pages 317–324, College Park,\nMaryland, USA. Association for Computational Lin-\nguistics.\nTomas Mikolov, Kai Chen, Greg S. Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word rep-\nresentations in vector space.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013b. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of the 26th International Con-\nference on Neural Information Processing Systems\n- Volume 2, NIPS’13, page 3111–3119, Red Hook,\nNY , USA. Curran Associates Inc.\n3474\nJeff Mitchell and Mirella Lapata. 2010. Composition\nin distributional models of semantics. Cognitive sci-\nence, 34(8):1388–1429.\nNavnita Nandakumar, Timothy Baldwin, and Bahar\nSalehi. 2019. How well do embedding models cap-\nture non-compositionality? a view from multiword\nexpressions. In Proceedings of the 3rd Workshop on\nEvaluating Vector Space Representations for NLP ,\npages 27–34, Minneapolis, USA. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nLivy Real, Erick Fonseca, and Hugo Goncalo Oliveira.\n2020. The assin 2 shared task: a quick overview.\nIn International Conference on Computational Pro-\ncessing of the Portuguese Language, pages 406–412.\nSpringer.\nSiva Reddy, Diana McCarthy, and Suresh Manand-\nhar. 2011. An empirical study on compositional-\nity in compound nouns. In Proceedings of 5th In-\nternational Joint Conference on Natural Language\nProcessing, pages 210–218, Chiang Mai, Thailand.\nAsian Federation of Natural Language Processing.\nNils Reimers, Philip Beyer, and Iryna Gurevych. 2016.\nTask-oriented intrinsic evaluation of semantic tex-\ntual similarity. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 87–96, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nIvan A. Sag, Timothy Baldwin, Francis Bond, Ann A.\nCopestake, and Dan Flickinger. 2002. Multiword\nexpressions: A pain in the neck for nlp. In Proceed-\nings of the Third International Conference on Com-\nputational Linguistics and Intelligent Text Process-\ning, CICLing ’02, page 1–15, Berlin, Heidelberg.\nSpringer-Verlag.\nAgata Savary, Carlos Ramisch, Silvio Cordeiro, Fed-\nerico Sangati, Veronika Vincze, Behrang Qasem-\niZadeh, Marie Candito, Fabienne Cap, V oula Giouli,\nIvelina Stoyanova, and Antoine Doucet. 2017. The\nPARSEME shared task on automatic identiﬁcation\nof verbal multiword expressions. In Proceedings of\nthe 13th Workshop on Multiword Expressions (MWE\n2017), pages 31–47, Valencia, Spain. Association\nfor Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2019. Attentive\nmimicking: Better word embeddings by attending\nto informative contexts. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 489–494, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nNathan Schneider, Jena D. Hwang, Vivek Srikumar,\nMeredith Green, Abhijit Suresh, Kathryn Conger,\nTim O’Gorman, and Martha Palmer. 2016. A cor-\npus of preposition supersenses. In Proceedings of\nthe 10th Linguistic Annotation Workshop held in\nconjunction with ACL 2016 (LAW-X 2016) , pages\n99–109, Berlin, Germany. Association for Compu-\ntational Linguistics.\nNathan Schneider, Spencer Onuffer, Nora Kazour,\nEmily Danchik, Michael T. Mordowanec, Henrietta\nConrad, and Noah A. Smith. 2014. Comprehensive\nannotation of multiword expressions in a social web\ncorpus. In Proceedings of the Ninth International\nConference on Language Resources and Evaluation\n(LREC’14), pages 455–461, Reykjavik, Iceland. Eu-\nropean Language Resources Association (ELRA).\nNathan Schneider and Noah A. Smith. 2015. A corpus\nand model integrating multiword expressions and su-\npersenses. In Proceedings of the 2015 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1537–1547, Denver, Colorado. As-\nsociation for Computational Linguistics.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. Facenet: A uniﬁed embedding for\nface recognition and clustering. In 2015 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 815–823.\nSabine Schulte im Walde, Anna Hätty, Stefan Bott,\nand Nana Khvtisavrishvili. 2016. GhoSt-NN: A\nrepresentative gold standard of German noun-noun\ncompounds. In Proceedings of the Tenth Inter-\nnational Conference on Language Resources and\nEvaluation (LREC’16), pages 2285–2292, Portorož,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nVered Shwartz and Ido Dagan. 2019. Still a pain in the\nneck: Evaluating text representations on lexical com-\nposition. Transactions of the Association for Com-\nputational Linguistics, 7:403–419.\nJ. Sinclair, L. Sinclair, and R. Carter. 1991. Corpus,\nConcordance, Collocation. Describing English lan-\nguage. Oxford University Press.\nYuancheng Tu and Dan Roth. 2012. Sorting out the\nmost confusing English phrasal verbs. In *SEM\n2012: The First Joint Conference on Lexical and\nComputational Semantics – Volume 1: Proceedings\nof the main conference and the shared task, and Vol-\nume 2: Proceedings of the Sixth International Work-\nshop on Semantic Evaluation (SemEval 2012), pages\n65–69, Montréal, Canada. Association for Computa-\ntional Linguistics.\n3475\nSriram Venkatapathy and Aravind Joshi. 2005. Mea-\nsuring the relative compositionality of verb-noun (V-\nn) collocations by integrating features. In Proceed-\nings of Human Language Technology Conference\nand Conference on Empirical Methods in Natural\nLanguage Processing, pages 899–906, Vancouver,\nBritish Columbia, Canada. Association for Compu-\ntational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems , volume 32. Curran\nAssociates, Inc.\nLang Yu and Allyson Ettinger. 2020. Assessing phrasal\nrepresentation and composition in transformers. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4896–4907, Online. Association for Computa-\ntional Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\n3476\nA Dataset Statistics\nDetailed statistics for the English and Portuguese\ndatasets are shown in Table 10 and Table 11, respec-\ntively. The train, dev and set breakdowns are shown\nin the leftmost column, with the further breakdown\nof the train set into zero-shot, one-shot and few-\nshot setups. Note here that the one-shot data is\ncontained within the few-shot data. The MWEs\ncolumn is the number of MWEs that the examples\nspan - the one-shot and few-shot setups contain\nall the MWEs from the dev and test sets, but the\nexamples are different. The next columns show\nthe ﬁne-grained and coarse-grained breakdown of\nthe dataset, used in Task 1 Subtask B and Task 1\nSubtask A, respectively.\nB Experimental Procedure\nB.1 Task 1 Subtask A\nThe Task 1 experiments were run on NVIDIA Tesla\nK80s. For the ﬁrst subtask, we ran a range of exper-\niments, varying the model used, whether context\nwas used, and whether the MWE was used. We ran\neach experiment for 9 epochs with ﬁve seeds (0 - 5).\nFor all experiments, we used a max sequence length\nof 128 and a learning rate of 2e-5. The standard\ntokenizers for each model were used for tokenizing\nthe input. The results for the best performing seed\nand epoch (by F1 score) for each experiment are\nshown in Table 12, with approximate training run\ntimes (for one seed for nine epochs). We started\nwith the zero-shot experiments, then took the best\nperforming models, and continued training them\nfrom the best epoch for another 9 epochs in the\none-shot and few-shot setups.\nB.2 Task 1 Subtask B\nFor the second subtask, we took the best-\nperforming experimental settings from Subtask\nA: XLNET base (cased) for English and BERT\nbase multilingual (cased) for Portuguese, exclud-\ning context but not including the MWE since we\ninstead pass the relevant paraphrase of the MWE\n(either correct or incorrect). These models were\nthen trained on the multiclass data, again for 9\nepochs and with ﬁve seeds (0-5). Again, the best-\nperforming models in the zero-shot setup were con-\ntinued training from the best epoch for another 9\nepochs in the few-shot and one-shot setups. These\nexperiments are shown in Table 13. Training times\nare increased due to the larger dataset from genera-\ntion of negative samples.\nB.3 Task 2\nPre-training models was done using NVIDIA Tesla\nV100s and took approximately 15 hours for each\nof the two models in English (BERT base on “all\nreplaced” and “select replaced”) and 5 hours for\neach model in Portuguese (BERT base multilin-\ngual). Due to time and resource limitations, we\npre-train models only once. All models were pre-\ntrained for 5 epochs based on the evaluation on a\ndevelopment set and our initial experiments which\nshowed that further pre-training did not improve\nresults.\nFor Subtask A, ﬁne-tuning these models using\nthe Sentence BERT architecture (so as to be able\nto compare the resultant embeddings using cosine\nsimilarity) was done using NVIDIA K80 GPUs\nfor English and took approximately 6 minutes per\nseed. Since we tested four variations (original\nBERT, BERT tokenized but not pre-trained, BERT\nall tokenized and select tokenized) each with ﬁve\nseeds, these experiments took a total of about two\nhours. The multilingual models required the use of\nNVIDIA Tesla V100s due to their larger size and\ntook about 3 minutes to train each model (per seed)\nand consequently took a total of about an hour to\ntrain. The best model was picked based on the per-\nformance on the STS dataset they were trained on\n(i.e. the STS benchmark dataset for English and\nASSIN2 for Portuguese).\nSubtask B similarly required the use of NVIDIA\nK80 GPUs for English and NVIDIA Tesla V100s\nfor Portuguese. We select the best models ﬁne-\ntuned using the Sentence BERT architecture (with\nno pre-training) from Subtask A and continue pre-\ntraining with MWE speciﬁc data. This process took\napproximately 6 minutes per model in English and\n3 minutes in Portuguese leading to a total of about\n30 minutes and 15 minutes respectively.\nAll ﬁne-tuning was done for as many epochs as\nwas required to see a drop in performance on the\ncorresponding development set.\nB.4 Larger Models\nExploratory experiments on Task 1 showed that the\nlarger language models performed worse than the\nbase ones, and thus these were the ones we used in\nour experiments.\nFor task 2, we use the smaller base models due\nto the limited amount of pre-train data, which we\nbelieve would make the use of larger models im-\npractical.\n3477\nSet Non-Idiomatic (1) Idiomatic (0)\nMWEs Lit PN Tot 1 2 3 Meta Tot Tot\ntrain\nzero-shot 163 1110 455 1565 1614 92 8 48 1762 3327\n(one-shot) 60 29 26 55 25 5 0 2 32 87\nfew-shot 60 135 50 185 81 11 0 5 97 282\ntotal 223 1245 505 1750 1695 103 8 53 1859 3609\ndev 30 174 110 284 157 14 0 11 182 466\ntest 30 271 63 334 118 24 0 7 149 483\ntotal 223 1690 678 2368 1970 141 8 71 2190 4558\nTable 10: Breakdown of the English dataset.\nSet Non-Idiomatic (1) Idiomatic (0)\nMWEs Lit PN Tot 1 2 3 Meta Tot Tot\ntrain\nzero-shot 73 284 107 391 697 55 2 19 773 1164\n(one-shot) 40 17 8 25 26 2 0 0 28 53\nfew-shot 40 55 14 69 80 6 0 1 87 156\ntotal 113 339 121 460 777 61 2 20 860 1320\ndev 20 96 23 119 137 16 0 1 154 273\ntest 20 94 20 114 151 9 0 5 165 279\ntotal 113 529 164 693 1065 86 2 26 1179 1872\nTable 11: Breakdown of the Portuguese dataset.\nProblem Setup Model Context? MWE? Train Time Dev Accuracy Dev F1\nEnglish\nzero-shot\nBERT base (cased) No No ~1 hour 0.732 0.724\nBERT base (cased) Yes No ~1 hour 0.732 0.717\nBERT base (cased) Yes Yes ~1 hour 0.785 0.779\nBERT base (cased) No Yes ~1 hour 0.796 0.785\nBERT base (uncased) No Yes ~1 hour 0.777 0.77\nXLNet base (cased) No Yes ~1 hour 0.828 0.823\nDistilBERT base (cased) No Yes ~1 hour 0.768 0.757\nRoBERTa base (cased) No Yes ~1 hour 0.807 0.801\none-shot XLNet base (cased) No Yes +~5 mins 0.903 0.897\none-shot XLNet base (cased) Yes No +~5 mins 0.719 0.689\none-shot XLNet base (cased) No No +~5 mins 0.775 0.755\nfew-shot XLNet base (cased) No Yes +~1min 0.961 0.959\nfew-shot XLNet base (cased) Yes No +~1min 0.807 0.782\nfew-shot XLNet base (cased) No No +~1min 0.813 0.792\nPortuguese\nzero-shot\nXLM-RoBERTa base (cased) No No ~1 hour 0.604 0.593\nXLM-RoBERTa base (cased) Yes No ~1 hour 0.56 0.542\nXLM-RoBERTa base (cased) Yes Yes ~1 hour 0.714 0.696\nXLM-RoBERTa base (cased) No Yes ~1 hour 0.729 0.703\nBERT base multilingual (cased) No Yes ~1 hour 0.707 0.686\none-shot XLM-RoBERTa base (cased) No Yes +~5 mins 0.879 0.877\none-shot XLM-RoBERTa base (cased) Yes No +~5 mins 0.615 0.605\none-shot XLM-RoBERTa base (cased) No No +~5 mins 0.641 0.638\nfew-shot XLM-RoBERTa base (cased) No Yes +~1min 0.927 0.926\nfew-shot XLM-RoBERTa base (cased) Yes No +~1min 0.656 0.655\nfew-shot XLM-RoBERTa base (cased) No No +~1min 0.799 0.796\nTable 12: Dev set results for Task 1 Subtask A\nProblem Setup Model Train Time Dev Accuracy Dev F1\nEn\nzero-shot XLNet base (cased) ~2.5 hours 0.883 0.852\none-shot XLNet base (cased) +~20 mins 0.938 0.923\nfew-shot XLNet base (cased) +~1 hour 0.947 0.933\nPt\nzero-shot XLM-RoBERTa base (cased) ~1 hour 0.886 0.843\none-shot XLM-RoBERTa base (cased) +~5 mins 0.888 0.852\nfew-shot XLM-RoBERTa base (cased) +~20 mins 0.931 0.909\nTable 13: Dev set results for Task 1 Subtask B"
}