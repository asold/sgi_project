{
  "title": "A Simple Language Model for Task-Oriented Dialogue",
  "url": "https://openalex.org/W3021016503",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223825802",
      "name": "Hosseini-Asl, Ehsan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288384363",
      "name": "McCann, Bryan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222053679",
      "name": "Wu, Chien-Sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221674675",
      "name": "Yavuz, Semih",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2998228050",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3017342174",
    "https://openalex.org/W3021096583",
    "https://openalex.org/W2798914047",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3102854726",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2810840719",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2964057895",
    "https://openalex.org/W2108806737",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963567240",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W2956901422",
    "https://openalex.org/W1975244201",
    "https://openalex.org/W2954492830",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963887424",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2951877072",
    "https://openalex.org/W2963974889",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2964210218",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2982482202",
    "https://openalex.org/W2897955964",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2963201498",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2950765829",
    "https://openalex.org/W2963134326",
    "https://openalex.org/W3119649668",
    "https://openalex.org/W2988252747",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1562973825",
    "https://openalex.org/W2952013107",
    "https://openalex.org/W2947480709",
    "https://openalex.org/W2988299267",
    "https://openalex.org/W2980207396",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2989692108",
    "https://openalex.org/W3037026762",
    "https://openalex.org/W2616122292",
    "https://openalex.org/W2964101860",
    "https://openalex.org/W2340944142",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W2972900451",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2906574053",
    "https://openalex.org/W2805406810",
    "https://openalex.org/W3188964051",
    "https://openalex.org/W2798779216",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3016625483"
  ],
  "abstract": "Task-oriented dialogue is often decomposed into three tasks: understanding user input, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we find a simple, unified approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.",
  "full_text": "A Simple Language Model for\nTask-Oriented Dialogue\nEhsan Hosseini-Asl\nehosseiniasl@salesforce.com\nSalesforce Research\nBryan McCann\nbmccann@salesforce.com\nSalesforce Research\nChien-Sheng Wu\nwu.jason@salesforce.com\nSalesforce Research\nSemih Yavuz\nsyavuz@salesforce.com\nSalesforce Research\nRichard Socher\nrsocher@salesforce.com\nSalesforce Research\nAbstract\nTask-oriented dialogue is often decomposed into three tasks: understanding user in-\nput, deciding actions, and generating a response. While such decomposition might\nsuggest a dedicated model for each sub-task, we ﬁnd a simple, uniﬁed approach\nleads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a\nsimple approach to task-oriented dialogue that uses a single, causal language model\ntrained on all sub-tasks recast as a single sequence prediction problem. This allows\nSimpleTOD to fully leverage transfer learning from pre-trained, open domain,\ncausal language models such as GPT-2. SimpleTOD improves over the prior state-\nof-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals\nrobustness to noisy annotations in this setting. SimpleTOD also improves the main\nmetrics used to evaluate action decisions and response generation in an end-to-end\nsetting: inform rate by 8.1 points, success rate by 9.7 points, and combined score\nby 7.2 points.\n1 Introduction\nConversational AI has been a long-standing area of exploration in computer science, and has gained\nmore attention recently in both academia and industries with the current advances of neural ap-\nproaches [15]. There are broadly two categories of dialogue. Open-domain dialogue systems focus\non making chit-chat, open-ended conversations with humans more natural and engaging. They are\nusually trained end-to-end using large-scale data from social media [1, 42]. Task-oriented dialogue\n(TOD) systems accomplish a goal described by a user in natural language. They often use a pipeline\napproach [46, 58]. The pipeline requires natural language understanding (NLU) for belief state\ntracking, dialogue management (DM) for deciding which actions to take based on those beliefs, and\nnatural language generation (NLG) for generating responses [50].\nTraditionally, each component of task-oriented dialogue systems is trained independently with\ndifferent supervision. The NLU module is trained on domain and intent labels. The DM module\nemploys dialogue belief and dialogue act labels. The NLG module accesses templatized or natural\nresponses. The modular dependencies of these components can lead to error propagation when\ninformation is not provided to subsequent modules in the pipeline [27]. For example, many systems\ndo not consider the entire dialogue history at every turn, but rather rely on the NLU module to pass\nbelief states reliably to following module components [60].\nWe propose recasting task-oriented dialogue as a simple, causal (unidirectional) language modeling\ntask. We show that such an approach can solve all the sub-tasks in a uniﬁed way using multi-task\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2005.00796v4  [cs.CL]  12 Apr 2022\nFigure 1: SimpleTOD is a simple approach to task-oriented dialogue that uses a single causal language\nmodel to generate all outputs given the dialogue context and retrieved database search results. The\ndelexicalized response can then be lexicalized into a human-readable response by using information\nfrom the belief state and DB search results.\nmaximum likelihood training. The proposed Simple Task-Oriented Dialogue (SimpleTOD) approach\nenables modeling of the inherent dependencies between the sub-tasks of task-oriented dialogue, by\noptimizing for all tasks in an end-to-end manner. SimpleTOD also opens the path towards fully\nleveraging large language models such as GPT-2 [39] for task-oriented dialogue. The success of\nSimpleTOD demonstrates a strong connection between the implicit language understanding in the\nopen domain required of high-quality causal language models and the kind of understanding required\nfor a full task-oriented dialogue system.\nEvaluation results demonstrate the advantages of SimpleTOD. It achieves 55.76 joint goal accuracy\non MultiWOZ, which surpasses all prior work for the dialogue state tracking (i.e. belief state tracking)\nsub-task. In the setting closest to testing a full task-oriented dialogue system, in which belief states\nand action decisions are generated rather than retrieved from an oracle, SimpleTOD performance\nsurpasses prior work on each individual action and response generation metric (+8.1 inform rate, +9.7\nsuccess rate).\nThe contributions of this work are summarized as follows:\n• SimpleTOD – a state-of-the-art generative model for dialogue state tracking (DST).\n• SimpleTOD is also the ﬁrst model to achieve state-of-the-art performance for dialogue state\ntracking, action decisions, and response generation metrics together in an end-to-end setting.\n• Analysis showing SimpleTOD is a robust dialogue state tracker in the presence of noisy-\nlabeled annotations.\n• Ablations showing the importance of user/system and endof(segment) tokens.\n• Ablations showing the importance of pre-training that also show larger versions of Simple-\nTOD are not always better for end-to-end MultiWOZ.\n• A list of discovered noisy annotations in MultiWOZ 2.1 alongside a cleaned version of\nthe test set, code for training and evaluation, are provided at https://github.com/\nsalesforce/simpletod\n2\n2 Related Work\nTask-Oriented Dialogue Most works on task-oriented dialogue focus on a speciﬁc module and\nevaluates only for that module. These components include understanding user intent via intent\ndetection [ 26], tracking the constraints imposed by the user via dialogue state tracking [20, 32, 41,\n34, 53, 59, 64, 8, 19], determining system actions via dialogue policy [ 51], and using dedicated\nresponse generation components [49].\nSome recent works have started to bridge multiple sub-tasks by connecting modules together and\nevaluating in settings that hand off generated results from one module to another. Chen et al. [9]\nproposed a joint action-response generation using oracle dialogue states. Peng et al. [37] used GPT-2\nto learn a response generator conditioned on oracle dialogue acts which did not evaluate on dialogue\nstate tracking.\nTowards End-to-End Task-Oriented Dialogue Dependencies between these independent mod-\nules make pipeline approaches vulnerable to error propagation across components [ 28]. Recent\napproaches have increasingly shifted towards end-to-end solutions, which aim to reduce human\neffort and task-speciﬁc design. Several works used both dialogue history and knowledge bases\nas input and optimized neural encoder-decoder models to generate or retrieve system responses\nwithout modular supervision [ 13, 62, 29, 54, 56]. Some systems are mostly end-to-end, but still\nneed to call out additional APIs or skip intermediate tasks like dialogue state tracking [5]. Others\nhave incorporated additional supervision and trained in multi-task settings. Lei et al. [24] and Shu\net al. [45] incorporated dialogue state tracking and jointly trained with response generation using\na sequence-to-sequence approach. Liu et al. [28] proposed a hybrid imitation and reinforcement\nlearning method, by jointly learning a policy for dialogue management with response generation.\nWen et al.[50], Liang et al. [25] trained language understanding, dialogue state tracking, and dialogue\npolicy modules with a shared encoder. Many other works fall somewhere in between by jointly\ntraining some tasks. Neelakantan et al. [33] modeled dialogue management and response generation\njointly, incorporating latent knowledge reasoning through attention without using belief states. Zhao\net al. [63] proposed to model system actions as latent variables, inducing a latent action space with\nvariational inference methods. Zhang et al. [60] proposed a domain-aware multi-decoder model\n(DAMD) using augmented dialogue data, which achieved state-of-the-art combined score for dialogue\nmanagement and response generation on the MultiWOZ dataset. Although all these approaches have\ncome closer to unifying the stack, none are as simple as SimpleTOD: treating all of task-oriented\ndialogue as a single sequence prediction problem, using a single model, trained with a single, joint,\nmulti-task loss.\nUnsupervised pre-training for natural language processing Pre-training approaches for natural\nlanguage processing focus on transferable representations for contextualized word vectors [30, 38],\ngenerative models [39, 23], or a combination of both [12, 57]. Variants of pre-trained, bidirectional\nTransformers like BERT [11] are often evaluated on classiﬁcation tasks such as those in the GLUE\nbenchmark [48] or span-based question answering tasks [ 40]. Unidirectional (causal) pre-trained\nlanguage models such as GPT-2 [39] or CTRL [23] resemble the decoder from the original Trans-\nformer architecture [47]. They aim to learn a distribution for next-word prediction, which makes them\nparticularly useful for tasks that require text generation. In dialogue, Zhang et al. [61] built on GPT-2\nby further pre-training it on Reddit data for open-domain response generation. Henderson et al. [21]\nalso pre-trained on Reddit data with a dual Transformer encoder for response selection. Bao et al. [3]\nused both Twitter and Reddit data to pre-train a Transformer model with discrete latent variables. Wu\net al. [55] proposed a response selection model by pre-training BERT model on multiple task-oriented\ncorpora.\nBudzianowski and Vuli´c [6] employed GPT-2 to leverage a pre-trained language model for dialogue\nresponse generation. They prepended dialogue context with belief state and DB search results. Their\nevaluation is only for context-to-response generation, and it is not applicable to the end-to-end setting.\nWu et al. [56] used GPT2 in an alternating roles for the user and system, without using belief state or\naction annotation. It achieved better results than its predecessor on context-to-response generation\n(dialogue policy), but it requires oracle belief states to evaluate inform and success rate which is not\napplicable to end-to-end evaluation. It is also not designed for dialogue state tracking.\n3\nHam et al. [17] ﬁne-tuned GPT-2 on the MultiWOZ dataset and achieved lower performance on\ndialogue state tracking and end-to-end evaluation compared to the previous single-task and modular-\nized models. They employed delimiter tokens for different segments: <usr>, <sys>, <ds> and <sa>.\nHowever, this differs from our tokenization, which is based on choosing semantic words which help\nthe model to learn the semantic role of each segments – belief state, action, and response – and end\nof segment tokens such as <|endofbelief|>. They also made use of token-type embeddings for user\nand system sequences, whereas our model does not use this type of embedding layer. Their proposed\nmodel achieved lower performance than previous baselines for dialogue state tracking and in the\nend-to-end evaluation, whereas we outperform all previous models.\nPeng et al. [36] proposed a GPT2-based model for end-to-end training (SOLOIST) by removing\nactions from the input sequence, and used token-type embedding for user and system responses as\nwell. It is additionally pretrained on seven more dialogue datasets before ﬁne-tuning on MultiWOZ.\nThey used data augmentation during training using contrastive learning, similar to DAMD [60], where\nthey combined dialogue context and belief states with a negative response and used the ﬁnal token\nfor classiﬁcation to improve end-to-end performance. However, SOLOIST did not report end-to-end\nperformance without pretraining on other dialogue datasets. Their \"w/o pretraining\" setting is only\nevaluated in a low resource setting, and they do not evaluate on dialogue state tracking as well.\nIn summary, our proposed model is much simpler than previous language model based approaches,\nin terms of (1) input sequence deﬁnition, (2) embedding layers, (3) training algorithm, and (4)\npretraining, which make it easy to reproduce the results, and outperformed previous models on DST\nand end-to-end setting.\n3 Methods\nThis section describes task-oriented dialogue, how we frame it for SimpleTOD, the model architecture,\ntraining details, dataset details, and evaluation metrics.\n3.1 Task-Oriented Dialogue\nTask-oriented dialogue (TOD) is evaluated on three sub-tasks: dialogue state (belief state) tracking,\ndialogue management (action/decision prediction) and response generation. This decomposition has\nmade it possible to create dedicated models for each sub-task, which is the dominant approach. By\ncontrast, we explore the possibility of using a single-model, end-to-end approach, SimpleTOD.\nDialogues consist of multiple turns. In a turn t, the user provides input Ut and the system generates a\nresponse St. To generate a response during inference, SimpleTOD reads all previous turns as context,\nCt = [U0,S0,...,U t]. It generates a belief state Bt,\nBt = SimpleTOD(Ct) (1)\nwhich is a list of triplets recording values for slots in a particular domain: (domain, slot_name, value).\nThis belief state is used to query a database for information. The database search returns rows from\nthe database that satisfy the conditions of the belief state. The rows returned can later be used to\nlexicalize the response (ﬁlling in generated placeholders), but SimpleTOD only takes as input the\naggregated database search results, Dt. Dt includes how many rows were returned and, depending\non the experimental setting, whether booking status information. SimpleTOD then conditions on Ct,\nBt, and Dt concatenated together as a single sequence to decide actions, At.\nAt = SimpleTOD([Ct,Bt,Dt]) (2)\nThese actions are generated as another list of triplets: (domain, action_type, slot_name). A delexical-\nized response St is generated conditioned on all prior information concatenated as a single sequence.\nSt = SimpleTOD([Ct,Bt,Dt,At]) (3)\nWhen combined with information from the belief state and database search results, the response can\nbe lexicalized to recover human readable response text. Figure 2 depicts training of SimleTOD and\ngeneration during inference.\n4\n3.2 Causal Language Modeling\nA single training sequence consists of the concatenation xt = [ Ct; Bt; Dt; At; St] 1, allowing\nus to model the joint probability over the sequence xt. Given example sequences of the form\nx= (x1,...,x n) where each xi comes from a ﬁxed set of symbols, the goal of language modeling\nis to learn p(x). It is natural to factorize this distribution using the chain rule of probability [ 4]\nand train a neural network with parameters θto minimize the negative log-likelihood over a dataset\nD= {x1,...,x |D|}where sequence xt has length nt:\np(x) =\nn∏\ni=1\np(xi|x<i) L(D) = −\n|D|∑\nt=1\nnt∑\ni=1\nlog pθ(xt\ni|xt\n<i) (4)\ninput is a single sequence\noutput state for each token predicts the next token \ngenerate one token at a time\nadd generated tokens to the \ninput sequence for the next \nstep of generation\nquery the database with the \ngenerated belief state\ncontinue generating until the \nresponse is ﬁnished\na) training\nb) inference \nuser input user inputsystem response\n belief state db search results actions delex. response\nSimpleTOD\nuser inputuser input system response\nbelief state\nuser inputuser input user input\nuser input\nuser input\nuser input\nsystem response\nuser inputuser input system response\nuser inputuser input system response\nactions delexicalized response\nuser inputuser input system response\nuser inputuser input system response\ndatabase\ndb search results\nbelief state\nuser inputuser input system response\nuser inputuser input system response\n db search results\nSimpleTOD\nSimpleTOD\nSimpleTOD\nSimpleTOD\nFigure 2: SimpleTOD is a simple approach to task-oriented dialogue that approaches all of task-\noriented dialogue as a single sequence generation problem, querying a database for necessary\ninformation.\n3.3 Architecture\nWe train a variant of the Transformer [ 47] to learn these conditional distributions. A sequence\ncontaining ntokens is embedded as a sequence of nvectors in Rd. Each vector is the sum of a\n1During inference, Dt comes from a database. See Sec. 4 for experimental results revealing that it can be\nadvantageous to exclude this from training.\n5\nlearned token embedding and a sinusoidal positional embedding. The sequence of vectors is stacked\ninto a matrix X0 ∈Rn×d and processed by lattention layers. The ith layer consists of two blocks,\neach preserving model dimension d. The ﬁrst block uses multi-head attention with kheads. A causal\nmask precludes attending to future tokens:\nAttention(X,Y,Z ) = softmax\n(mask(XY⊤)√\nd\n)\nZ\nMultiHead(X,k) = [h1; ··· ; hk]Wo\nwhere hj = Attention(XW1\nj,XW 2\nj,XW 3\nj)\nThe second block uses a feedforward network with ReLU activation that projects inputs to an inner\ndimension f. This operation is parameterized by U ∈Rd×f and V ∈Rf×d:\nFF(X) = max(0,XU )V\nEach block precedes core functionality with layer normalization [2, 10] and follows it with a residual\nconnection [18]. Together, they yield Xi+1:\nBlock 1 Block 2\n¯Xi = LayerNorm(Xi) ¯Hi = LayerNorm(Hi)\nHi = MultiHead( ¯Xi) + ¯Xi Xi+1 = FF( ¯Hi) + ¯Hi\nScores are then computed from the output of the last layer:\nScores(X0) = LayerNorm(Xl)Wvocab\nDuring training, these scores are the inputs of a cross-entropy loss function. During generation, the\nscores corresponding to the ﬁnal token are normalized with a softmax, yielding a distribution for\nsampling a new token.\n3.4 Training Details\nThe input to the model is tokenized with pretrained BPE codes [44] associated with DistilGPT2 [43],\na distilled version of GPT-2 [39]. According to experimental results, Experiments for SimpleTOD\nuse default hyperparameters for GPT-2 and DistilGPT2 in Huggingface Transformers[52]. Sequences\nlonger than 1024 tokens are truncated.\n3.5 Dataset Details\nWe evaluate on the Multi-domain Wizard-of-Oz (MultiWOZ) [7], a large-scale, multi-domain dialogue\ndataset of human-human conversations. It contains 10438 multi-turn dialogues with 13.68 average\nturns, spanning over seven domains (restaurant, train, attraction, hotel, taxi, hospital, police). Police\nand hospital domains are excluded from evaluation, since they do not have valid/test splits. This leaves\n30 domain-slot pairs for the remaining ﬁve domain with 4,500 possible values. SimpleTOD is trained\non delexicalized system responses according to the pre-processing explained in [7]. Recently, [14]\nreleased MultiWOZ 2.1 which removes some noisy state values from dialogue state (belief state)\ntracking annotations. For dialogue state tracking evaluation, we used 2.1 version in order to compare\nto recent state-of-the-art methods. To the best of our knowledge, all prior work on action and response\ngeneration has evaluated on 2.0, so we include those results for direct comparison. But, we also\ninclude results for 2.1 so future work can compare to SimpleTOD on the improved version as well.\n3.6 Evaluation Details\nWe follow the original MultiWOZ [7] guidance for all individual metrics and follow Mehri et al. [31]\nfor the combined score. Joint goal accuracy is used to evaluate the performance of dialogue state\ntracking (i.e. belief state tracking). It measures the accuracy of the generated belief states as they\ncompare to oracle belief states. Model outputs are only counted as correct when all the predicted\nvalues exactly match the oracle values. Action and response generation uses three metrics. The ﬁrst\ntwo are inform and success rates. They are designed to capture how well the task was completed.\nInform rate measures how often the entities provided by the system are correct. Success rate refers\nto how often the system is able to answer all the requested attributes by user. BLUE score [ 35] is\nused to measure the ﬂuency of the generated responses. The combined score for action and response\ngeneration is computed as (BLEU + 0.5 ∗(Inform + Success)).\n6\nModel Decoder Context Encoder Extra Supervision Joint Accuracy\nTRADE∗ Generative + Classiﬁer Bidirectional - 45.6\nDSTQA∗∗ Classiﬁer Bidirectional knowledge graph 51.17\nDST-Picklist∗ Classiﬁer Bidirectional - 53.3\nSST∗ Generative Bidirectional schema graph 55.23\nTripPy† Classiﬁer Bidirectional action decision 55.3\nSimpleTODo Generative Unidirectional - 55.72\nSimpleTOD∗ Generative Unidirectional - 55.76\nSimpleTOD+ Generative Unidirectional - 57.47\nTable 1: Evaluation of Dialogue State Tracking (DST) on MultiWOZ 2.1 using joint accuracy metric.\n∗uses test label cleaning proposed by Wu et al. [53] and recommended by MultiWOZ authors. †uses\nlabel normalization and equivalent matching proposed in Heck et al. [19]. ∗∗uses the cleaning of\n∗models plus additional accounting for label variants. + performs cleaning of Type 2 and partial\ncleaning of Type 4 noisy annotations as outlined in Section 5, which is currently non-standard and so\nleft unbolded. o no label-cleaning.\n4 Experimental Results and Discussion\nSimpleTOD is a Uniﬁed System for Task-Oriented Dialogue SimpleTOD is, to the best of our\nknowledge, the ﬁrst system that generates state-of-the-art results judged according to dialogue state\ntracking as well as end-to-end metrics for action and response generation for MultiWOZ.\n4.1 Dialogue State Tracking\nTable 1 compares the joint goal accuracy to previous methods. We compare to TRADE [ 53],\nDSTQA [64], DST-Picklist [59], SST [ 8], and TripPy [ 19]. All previous models propose a bidi-\nrectional encoder to learn a better representation of the dialogue context, but SimpleTOD uses a\nunidirectional (causal) decoder and no additional bidirectional encoder. It also makes no use of extra\nsupervision. It nonetheless achieves state-of-the-art.\nMany models use some form of test-label cleaning. TRADE, DSTQA, DST-Picklist, and SST use\nthe script proposed by Wu et al. [53]2. DSTQA also accounts for label variations that would have\noriginally been considered incorrect. TripPy apply their own format normalization, typo corrections,\nand process for accounting for label variations. SimpleTOD achieves the best performance without\nany cleaning or normalization, simply on the raw, original annotations. Applying the script from Wu\net al. [53] improves the result to 55.76. Analysis of further noisy annotation is presented in section 5.\nFurther cleaning those annotations more accurately reﬂects performance at 57.47. We will release\nthe list of noisy annotations that need to be ﬁxed along with their corrections, but we reiterate that\nSimpleTOD does not need this cleaning to surpass prior methods.\n4.2 Action and Response Generation\nTable 2 and Table 3 demonstrate the effectiveness of SimpleTOD for action and response generation\nin the most realistic, fully end-to-end3 setting – when models must generate belief states, actions, and\nresponses. SimpleTOD targets replacing modularized and pipelined methods that evaluate different\ncomponents evaluated with oracle information. For reference, oracle settings compare across a variety\nof settings against HDSA [9], ARDM [56], LaRL [63], PARG [16] can be found in the Supplementary\nMaterials, but these comparisons are not essential for end-to-end contributions. In fact, SimpleTOD\nis state-of-the-art in the end-to-end setting compared to the only prior work, DAMD [60], without\nachieving state-of-the-art in settings that partially utilize oracle information. This highlights that\npartial, oracle evaluation does not reliably transfer to the end-to-end evaluation of full systems – only\nend-to-end evaluation accurately describes the performance of a full system.\n2https://github.com/jasonwu0731/trade-dst/blob/master/utils/fix_label.py\n3The term \"end-to-end\" is overloaded in the literature. Evaluation that does not use oracle belief states,\nactions, or response is considered end-to-end even when the system itself is not trained end-to-end. SimpleTOD\nis trained end-to-end and achieves state-of-the-art in end-to-end evaluation.\n7\nModel Belief State DB Search Action Inform Success BLEU Combined\nDAMD+augmentation generated oracle generated 76.3 60.4 16.6 85\nSimpleTOD (ours) generated oracle generated 78.1 63.4 16.91 87.66\nSimpleTOD (ours) generated dynamic generated 81.4 69.7 16.11 91.66\nSimpleTOD (ours) generated - generated 84.4 70.1 15.01 92.26\nTable 2: Action and response generation on MultiWOZ 2.0 reveals that SimpleTOD, a single, causal\nlanguage model, is sufﬁcient to surpass prior work.\nBelief State DB Search Action Inform Success BLEU Combined\ngenerated oracle generated 79.3 65.4 16.01 87.36\ngenerated dynamic generated 83.4 67.1 14.99 90.24\ngenerated - generated 85 70.5 15.23 92.98\nTable 3: Action and response generation on MultiWOZ 2.1 for SimpleTOD.\nPrior work uses oracle DB Search results as supervision during training and as input during inference.\nWe include directly comparable experiments using oracle DB Search results. We also include\nexperiments that completely ignore the DB Search results to show the surprising effectiveness of\nSimpleTOD without DB Search information. We also show a setting with dynamic DB Search results.\nIn this setting, we train with the number of matched DB entries and compute this dynamically at\ninference from generated belief states. In all variations, SimpleTOD outperforms prior work.\nDAMD [60] is the only prior work that has evaluated with generated belief states from dialogue\nstate tracking during inference. We found in additional ablation experiments that we could increase\nscores for individual metrics like inform rate and success rate by training three separate SimpleTOD\nlanguage models: one for dialogue state tracking, one for action generation, and one for response\ngeneration. However, the combined scores remained nearly identical to the full end-to-end, single\nmodel approach. For example, separating the models might improve inform rate, but hurt response\ngeneration measured by BLEU. Regardless, in this most realistic setting SimpleTOD achieves state-\nof-the-art on inform and success metric. SimpleTOD performs lower only on BLEU by 1.59 points,\nperhaps due to lack of action/response augmentation employed by DAMD.\nRegarding Oracle DB Search Results In the case where we dynamically compute partial DB\nSearch results (number of entries matched only), the results are actually lower than ignoring them\nentirely. Using oracle DB information likewise leads to lower performance. The best result ignores\nDB Search results entirely. We have found that in some cases, the generated belief states conﬂict in\nsome way with the information in the database. For example, there can be discrepancies between\nthe two in the name of restaurants: ‘pizza hut fenditton’ in the target belief states but ‘pizza hut fen\nditton’ in the database. We have consulted with the authors of the dataset, but there is currently no\ncourse of action planned to remedy this.\n5 Analysis and Further Discussion\nThe Role of Special Tokens Table 4 evaluates SimpleTOD with different special tokens used to\nidentify components of the input corresponding to different sub-tasks. Analysis revealed that without\nend tokens, SimpleTOD tended to generate much longer belief state, action, and response generations.\nEven more important is clearly differentiating user and system text for SimpleTOD.\nPre-training Table 5 highlights the importance of initializing SimpleTOD with pre-trained weights.\nA major advantage of recasting as single sequence prediction is the ability to leverage the understand-\ning learned by these pre-trained models in the open-domain setting.\nRobustness to Noisy Annotations To understand the source of dialogue state tracking errors, we\ninvestigated MultiWOZ 2.1 annotations in depth. In the process, we have deﬁned four primary types\nof noisy-labels that could be considered mis-annotations:\n1. User provided multiple options, but context does not provide sufﬁcient information to\ndetermine the true belief state.\n8\nEnd token User/System token Joint Acc Inform Success BLEU Combined\nNo No 16.79 33.8 10.6 4.53 26.73\nYes No 21.5 54.5 41.2 9.48 57.33\nNo Yes 22.22 61.9 52.7 9.57 66.87\nYes Yes 55.76 85 70.5 15.23 92.98\nTable 4: Ablations on MultiWOZ 2.1 comparing the presence and absence of different special\ntokens when representing TOD as a single sequence. Performance on all metrics drops without\n<endof(segment)> and <user/system> tokens.\nLayers Pretrained Joint Acc Inform Success BLEU Combined\n6 Random 16.45 63.5 49.6 6.34 62.89\n6 DistilGPT2 54.54 85 70.5 15.23 92.98\n12 Random 20.17 58.7 37.4 8.9 59.65\n12 GPT2 55.76 88 61.7 15.9 90.75\nTable 5: Ablations on MultiWOZ 2.1 comparing the importance of pretraining. Recasting as single\nsequence prediction enables fully leveraging pre-trained models for the language understanding they\nhave gathered in an open-domain setting.\n2. Belief state is not labeled, but context provides sufﬁcient information.\n3. Belief state is labeled, but context lacks necessary information.\n4. Belief state value is misspelled according to the context information.\nTogether experimental results and this analysis indicate that SimpleTOD can track dialogue state and\ngenerate the correct output even in the presence of noisy labels. Concrete examples of noisy-labeled\nannotation in MultiWOZ can be found in the Supplementary Materials. All mis-annotated examples\nalong with all code for replication are provided 4.\nDecoding Initialized from pre-trained weights, SimpleTOD does not need to employ an advanced,\nmore costly decoding strategy such as beam search, diverse beam search, and top-k sampling as\nopposed to HDSA [9] and DAMD [60]. Our results are reported with simple greedy decoding. In\ninitial experiments, we also tried nucleus sampling [22], but we found it degraded performance. This\nrelates to the observations in Keskar et al. [23] around controllable generation: when precision is\nrequired, sampling from the distribution is inherently less reliable than greedily sampling.\nFull Dialogues, Multiple Turns, and Long Contexts In further analysis, we found that Simple-\nTOD accurately tracks dialogue state over multiple turns and long contexts. In some cases, earlier\nbelief state errors are remedied later on when additional turns provide increased context. Examples\nof full dialogues and those with many turns or especially long context can be found in Supplementary\nMaterials, but we do not consider this further analysis as a primary contribution listed for the work.\n6 Conclusion\nWe explored a simple approach to task-oriented dialogue (SimpleTOD) that uses a single, causal\nlanguage model. To do this, during training we treat all inputs for dialogue state tracking, action\nand response generation as a single sequence to the model. SimpleTOD can then directly leverage\npre-trained models like GPT-2 to transfer language understanding from open-domain settings where\ndata is more readily available. Empirical results on the multi-domain dialogue dataset (MultiWOZ)\nshowed that the proposed approach outperformed all prior methods in dialogue state tracking as well\nas in action and response generation in the end-to-end setting. We found that the pre-trained weights\nwere essential, but to leverage these weights fully we had to guide the system with special tokens\nthat mark user and system responses as well as different portions of the sequence related to different\nsub-tasks. We found that SimpleTOD was effective at tracking dialogue state over long context with\nmany turns and required no more than greedy decoding to achieve new state-of-the-art results despite\nnoisy annotations. We hope that these results and the code, models, and discovered noisy annotations\nwill encourage further exploration of simple, uniﬁed approaches for dialogue systems.\n4https://github.com/salesforce/simpletod\n9\n7 Broader Impact\nThis work may have implications for the simpliﬁcation of conversational agents. In the narrow sense,\nthis work addresses task-oriented dialogue, but similar results might also hold for open-domain\nconversational systems. If so, the improvement of these systems and easier deployment would\namplify both the positive and negative aspects of conversational AI. Positively, conversational agents\nmight play a role in automating predictable communications, thereby increasing efﬁciency in areas of\nsociety that currently lose time navigating the multitude of APIs, webpages, and telephonic systems\nthat are used to achieve goals. Negatively, putting conversational agents at the forefront might\ndehumanize communication that can be automated and might lead to frustration where human agents\ncould provide more efﬁcient solutions – for example, when predicted solutions do not apply. These\nconsequences are not speciﬁc to this work, but should be considered by the ﬁeld of conversational AI\nmore broadly.\nReferences\n[1] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thoppilan, Z. Yang, A. Kulshreshtha,\nG. Nemade, Y . Lu, et al. Towards a human-like open-domain chatbot.arXiv preprint arXiv:2001.09977,\n2020.\n[2] J. Ba, R. Kiros, and G. E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.\n[3] S. Bao, H. He, F. Wang, and H. Wu. Plato: Pre-trained dialogue generation model with discrete latent\nvariable. arXiv preprint arXiv:1910.07931, 2019.\n[4] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model.Journal of\nmachine learning research, 3(Feb):1137–1155, 2003.\n[5] A. Bordes, Y .-L. Boureau, and J. Weston. Learning end-to-end goal-oriented dialog. In International\nConference on Learning Representations, 2017.\n[6] P. Budzianowski and I. Vuli´c. Hello, it’s gpt-2–how can i help you? towards the use of pretrained language\nmodels for task-oriented dialogue systems. arXiv preprint arXiv:1907.05774, 2019.\n[7] P. Budzianowski, I. Casanueva, B.-H. Tseng, and M. Gasic. Towards end-to-end multi-domain dialogue\nmodelling. 2018.\n[8] L. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu. Schema-guided multi-domain dialogue state tracking\nwith graph attention neural networks. 2020.\n[9] W. Chen, J. Chen, P. Qin, X. Yan, and W. Y . Wang. Semantically conditioned dialog response generation\nvia hierarchical disentangled self-attention. arXiv preprint arXiv:1905.12866, 2019.\n[10] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.arXiv\npreprint arXiv:1904.10509, 2019.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[12] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou, and H.-W. Hon. Uniﬁed language\nmodel pre-training for natural language understanding and generation. In Advances in Neural Information\nProcessing Systems, pages 13042–13054, 2019.\n[13] M. Eric and C. D. Manning. Key-value retrieval networks for task-oriented dialogue. arXiv preprint\narXiv:1705.05414, 2017.\n[14] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao, and D. Hakkani-Tur. Multiwoz 2.1: Multi-domain\ndialogue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669, 2019.\n[15] J. Gao, M. Galley, L. Li, et al. Neural approaches to conversational ai. Foundations and Trends® in\nInformation Retrieval, 13(2-3):127–298, 2019.\n[16] S. Gao, Y . Zhang, Z. Ou, and Z. Yu. Paraphrase augmented task-oriented dialog generation.arXiv preprint\narXiv:2004.07462, 2020.\n[17] D. Ham, J.-G. Lee, Y . Jang, and K.-E. Kim. End-to-end neural pipeline for goal-oriented dialogue system\nusing gpt-2. ACL, 2020.\n10\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[19] M. Heck, C. van Niekerk, N. Lubis, C. Geishauser, H.-C. Lin, M. Moresi, and M. Gaši´c. Trippy: A triple\ncopy strategy for value independent neural dialog state tracking. arXiv preprint arXiv:2005.02877, 2020.\n[20] M. Henderson, B. Thomson, and S. Young. Deep neural network approach for the dialog state tracking\nchallenge. In Proceedings of the SIGDIAL 2013 Conference, 2013.\n[21] M. Henderson, I. Casanueva, N. Mrkši ´c, P.-H. Su, I. Vuli ´c, et al. Convert: Efﬁcient and accurate\nconversational representations from transformers. arXiv preprint arXiv:1911.03688, 2019.\n[22] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text degeneration.arXiv\npreprint arXiv:1904.09751, 2019.\n[23] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher. Ctrl: A conditional transformer\nlanguage model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.\n[24] W. Lei, X. Jin, M.-Y . Kan, Z. Ren, X. He, and D. Yin. Sequicity: Simplifying task-oriented dialogue\nsystems with single sequence-to-sequence architectures. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, 2018.\n[25] W. Liang, Y . Tian, C. Chen, and Z. Yu. Moss: End-to-end dialog system framework with modular\nsupervision. arXiv preprint arXiv:1909.05528, 2019.\n[26] B. Liu and I. Lane. Attention-based recurrent neural network models for joint intent detection and slot\nﬁlling. In INTERSPEECH, 2016.\n[27] B. Liu and I. Lane. End-to-end learning of task-oriented dialogs. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Student Research Workshop,\npages 67–73, 2018.\n[28] B. Liu, G. Tür, D. Hakkani-Tür, P. Shah, and L. Heck. Dialogue learning with human teaching and feedback\nin end-to-end trainable task-oriented dialogue systems. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\n2018.\n[29] A. Madotto, C.-S. Wu, and P. Fung. Mem2seq: Effectively incorporating knowledge bases into end-to-end\ntask-oriented dialog systems. arXiv preprint arXiv:1804.08217, 2018.\n[30] B. McCann, J. Bradbury, C. Xiong, and R. Socher. Learned in translation: Contextualized word vectors. In\nAdvances in Neural Information Processing Systems, pages 6294–6305, 2017.\n[31] S. Mehri, T. Srinivasan, and M. Eskenazi. Structured fusion networks for dialog. arXiv preprint\narXiv:1907.10016, 2019.\n[32] N. Mrkši´c, D. Ó Séaghdha, T.-H. Wen, B. Thomson, and S. Young. Neural belief tracker: Data-driven\ndialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics, 2017.\n[33] A. Neelakantan, S. Yavuz, S. Narang, V . Prasad, B. Goodrich, D. Duckworth, C. Sankar, and X. Yan.\nNeural assistant: Joint action prediction, response generation, and latent knowledge reasoning. In NeurIPS\n2019 Converstional AI Workshop, 2019.\n[34] E. Nouri and E. Hosseini-Asl. Toward scalable neural dialogue state tracking model. In NeurIPS 2018\nConversational AI Workshop, 2018.\n[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: A method for automatic evaluation of machine\ntranslation. In ACL, 2002.\n[36] B. Peng, C. Li, J. Li, S. Shayandeh, L. Liden, and J. Gao. Soloist: Few-shot task-oriented dialog with a\nsingle pre-trained auto-regressive model. arXiv preprint arXiv:2005.05298, 2020.\n[37] B. Peng, C. Zhu, C. Li, X. Li, J. Li, M. Zeng, and J. Gao. Few-shot natural language generation for\ntask-oriented dialog, 2020.\n[38] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextual-\nized word representations. arXiv preprint arXiv:1802.05365, 2018.\n11\n[39] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised\nmultitask learners. OpenAI Blog, 1(8):9, 2019.\n[40] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension\nof text. arXiv preprint arXiv:1606.05250, 2016.\n[41] A. Rastogi, D. Hakkani-Tur, and L. Heck. Scalable multi-domain dialogue state tracking. In Proceedings\nof IEEE ASRU, 2017.\n[42] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson, Y . Liu, J. Xu, M. Ott, K. Shuster, E. M. Smith, et al.\nRecipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020.\n[43] V . Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108, 2019.\n[44] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword units. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1715–1725, Berlin, Germany, Aug. 2016. Association for Computational Linguistics. doi:\n10.18653/v1/P16-1162. URL https://www.aclweb.org/anthology/P16-1162.\n[45] L. Shu, P. Molino, M. Namazifar, B. Liu, H. Xu, H. Zheng, , and G. Tur. Incorporating the structure of the\nbelief state in end-to-end task-oriented dialogue systems. In NeurIPS 2018 Converstional AI Workshop,\n2018.\n[46] R. W. Smith and D. R. Hipp. Spoken natural language dialog systems: A practical approach. Oxford\nUniversity Press on Demand, 1994.\n[47] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.\n[48] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n[49] T.-H. Wen, M. Gaši´c, N. Mrkši´c, P.-H. Su, D. Vandyke, and S. Young. Semantically conditioned LSTM-\nbased natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, 2015.\n[50] T.-H. Wen, D. Vandyke, N. Mrksic, M. Gasic, L. M. Rojas-Barahona, P.-H. Su, S. Ultes, and S. Young.\nA network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562,\n2016.\n[51] T.-H. Wen, Y . Miao, P. Blunsom, and S. Young. Latent intention dialogue models. InProceedings of the\n34th International Conference on Machine Learning, 2017.\n[52] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\net al. Transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.\n[53] C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong, R. Socher, and P. Fung. Transferable multi-domain state\ngenerator for task-oriented dialogue systems. arXiv preprint arXiv:1905.08743, 2019.\n[54] C.-S. Wu, R. Socher, and C. Xiong. Global-to-local memory pointer networks for task-oriented dialogue.\narXiv preprint arXiv:1901.04713, 2019.\n[55] C.-S. Wu, S. Hoi, R. Socher, and C. Xiong. Tod-bert: Pre-trained natural language understanding for\ntask-oriented dialogues. arXiv preprint arXiv:2004.06871, 2020.\n[56] Q. Wu, Y . Zhang, Y . Li, and Z. Yu. Alternating recurrent dialog model with large-scale pre-trained language\nmodels. arXiv preprint arXiv:1910.03756, 2019.\n[57] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized autoregressive\npretraining for language understanding. In Advances in neural information processing systems, pages\n5754–5764, 2019.\n[58] S. Young, M. Gaši´c, B. Thomson, and J. D. Williams. Pomdp-based statistical spoken dialog systems: A\nreview. Proceedings of the IEEE, 101(5):1160–1179, 2013.\n[59] J.-G. Zhang, K. Hashimoto, C.-S. Wu, Y . Wan, P. S. Yu, R. Socher, and C. Xiong. Find or classify? dual\nstrategy for slot-value predictions on multi-domain dialog state tracking. arXiv preprint arXiv:1910.03544,\n2019.\n12\n[60] Y . Zhang, Z. Ou, and Z. Yu. Task-oriented dialog systems that consider multiple appropriate responses\nunder the same context. arXiv preprint arXiv:1911.10484, 2019.\n[61] Y . Zhang, S. Sun, M. Galley, Y .-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan. Dialogpt: Large-\nscale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536,\n2019.\n[62] T. Zhao, A. Lu, K. Lee, and M. Eskenazi. Generative encoder-decoder models for task-oriented spoken\ndialog systems with chatting capability. In Proceedings of the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, 2017.\n[63] T. Zhao, K. Xie, and M. Eskenazi. Rethinking action spaces for reinforcement learning in end-to-end\ndialog agents with latent variable models. arXiv preprint arXiv:1902.08858, 2019.\n[64] L. Zhou and K. Small. Multi-domain dialogue state tracking as dynamic knowledge graph enhanced\nquestion answering. arXiv preprint arXiv:1911.06192, 2019.\n13\nA Input Representation and Method Overview\nAs described in Section 3, a single training sequence consists of the concatenation of context Ct, belief states\nBt, database search results Dt, action decisions At, and system response St. A schematic overview of each\nsegment is shown in Table 6 together with special tokens marking transition points. SimpleTOD is optimized\nby minimizing the negative likelihood over the joint sequence xt = [Ct; Bt; Dt; At; St]. The output state\nassociated with each input token is used to predict the next token, see Figure 1a.\nDuring inference, SimpleTOD generates this sequence token by token, but we stop after belief states are\ngenerated to query from a database. The outputs of the database are summarized and concatenated to the end of\nthe input sequence and generation resumes token by token. This results in a delexicalized response, see Figure 1b.\nThis response can then be lexicalized by replacing slots and values with information from the database results.\nThis process is described more formally in the equations of Section 6.\nContext [context] [user] user input[system] system response. . .[user] user input[endofcontext]\nBelief State [belief] domain slot_name value, domain slot_name value,. . .[endofbelief]\nDB Search [db] #_matches, booking_status[endofdb]\nAction [action] domain action_type slot_name, domain action_type slot_name,. . .[endofaction]\nResponse [response] system delexicalized response[endofresponse]\nTable 6: A schematic representation of the different components of inputs/outputs in task-oriented\ndialogue. When training SimpleTOD, these are concatenated together into a single sequence.\n14\nB SimpleTOD with Oracle information\nThis section reports the performance of SimpleTOD for action and response generation, in the presence of\ndifferent oracle information, i.e. oracle belied and oracle action. These settings are not end-to-end as in the\nmain text, and SimpleTOD is designed to be end-to-end. We report results in these settings for a complete\nunderstanding of SimpleTOD, but we note that only the end-to-end settings in the main text evaluate the full\nsystem together. In these oracle settings, other methods can outperform SimpleTOD, but this simply highlights\nthe importance of end-to-end evaluation: there is a disconnect between performance with oracle information\nand performance without it. In practical use, oracle information is not available, and that is where SimpleTOD\nexcels.\nWe report results in Table 7 for two different settings regularly employed in the literature. These settings are\ndetermined by how much oracle information is used. The ﬁrst setting uses oracle belief states and oracle actions.\nThe second uses oracle belief states, but requires the system to generate its own actions.\nNote that all prior works use oracle DB Search results as supervision during training and as input during inference\nin all these settings. We include directly comparable experiments using oracle DB Search results for all settings.\nWe also include experiments that completely ignore the DB Search results in all settings to show the surprising\neffectiveness of SimpleTOD without DB Search results.\nThe evaluation results on MultiWOZ 2.1, as shown in Table 8, also follow the same patterns as discussed in\nsection 4.2. We provide these results for future comparisons on the improved version of the dataset.\nModel Belief State DB Search Action Inform Success BLEU Combined\nDAMD+augmentation oracle oracle oracle 95.4 87.2 27.3 118.5\nPARG oracle oracle oracle 91.1 78.9 18.8 103.8\nSimpleTOD (ours) oracle oracle oracle 93.4 83.2 17.78 106.08\nSimpleTOD (ours) oracle - oracle 92.3 85.8 18.61 107.66\nHDSA oracle oracle generated 82.9 68.9 23.6 99.5\nDAMD+augmentation oracle oracle generated 89.2 77.9 18.6 102.5\nARDM oracle oracle - 87.4 72.8 20.6 100.7\nLaRL oracle oracle generated 82.78 79.2 12.8 93.79\nSimpleTOD (ours) oracle oracle generated 84 72.8 16.1 94.5\nSimpleTOD (ours) oracle - generated 88.9 67.1 16.9 94.9\nTable 7: SimpleTOD results on MultiWOZ 2.0 using oracle information.\nBelief State DB Search Action Inform Success BLEU Combined\noracle oracle oracle 92.8 84.5 18.9 107.55\noracle - oracle 92.6 86.1 17.67 107.2\noracle oracle generated 85.1 73.5 16.22 95.52\noracle - generated 89.6 68.6 15.46 94.56\nTable 8: SimpleTOD results on MultiWOZ 2.1 using oracle information.\n15\nC Dialogue State Tracking Analysis\nThis section provides more detailed analysis of Dialogue State Tracking (DST) task performance by SimpleTOD,\nas mentioned in section 5.\nUnderstanding Long, Multi-domain Context Table 9 indicates the DST performance of SimpleTOD\nin case of multi-domain and long context. This example also shows understanding dialogue states, where slots\nacross domains are related. For example, taxi departureand taxi destinationshould be inferred from attraction\nname and hotel name.\nContext (’MUL1015’, turn 10)\n<|context|> <|user|> i am looking for a pool somewhere in the south of cambridge . <|system|> i am\nsorry , but it does not look like we have a pool that matches your criteria . <|user|> how about some\ntype of theater ? <|system|> the junction theatre is in the south area . would you like the address\n? <|user|> yes , please provide the address and postcode as well . <|system|> the junction theatre is\nlocated on clifton way at postcode cb17gx . is there anything else i can assist you with ? <|user|>\nthanks . yes , i also need to ﬁnd a guesthouse for my stay there . <|system|> are you needing the\nguesthouse in the south , also ? do you have a preference in price range ? <|user|> the area should be\nthe centre . i need it to be cheap and have free parking . <|system|> i have 2 guesthouses that ﬁt your\ncriteria . 1 is the alexander bed and breakfast and the other is el shaddai . <|user|> what is the star\nrating -s for them ? <|system|> the alexander bed and breakfast has a 4 star rating while the el shaddai\nhas 0 stars . would you like to book a room with either of these ? <|user|> ok great , thank you . i\nalso need to book a taxi to get back and forth . <|system|> where did you wan to leave from and go to\n? <|user|> i’ll be leaving the theatre at 14:45 and going to the guesthouse . <|system|> let s back up a\nmoment . which lodgings did you prefer , so we know where the taxi will be taking you ? <|user|> let\ns go with the alexander bed and breakfast . <|system|> your booking is complete , a grey volkswagen\nwill be picking you up , contact number 07753429052 . can i help with anything else ? <|user|> cool\nbeans ! sorry i was so much trouble . thanks so much for all of your help , you need a raise dealing\nwith customers like me ! <|endofcontext|>\nTarget\n<|belief|> attraction area south, attraction name the junction, attraction type theatre, hotel area centre,\nhotel name alexander bed and breakfast, hotel parking yes, hotel pricerange cheap, hotel type guest-\nhouse, taxi departure the junction, taxi destination alexander bed and breakfast, taxi leaveat 14:45\n<|belief|> <|action|> general bye none <|endofaction|> <|response|> it was no trouble at all ! i hope you\nhave a fantastic trip and enjoy your stay ! <|endofresponse|>\nSimpleTOD\n<|belief|> attraction area south, attraction name the junction, attraction type theatre, hotel area centre,\nhotel name alexander bed and breakfast, hotel parking yes, hotel pricerange cheap, hotel type guest-\nhouse, taxi departure the junction, taxi destination alexander bed and breakfast, taxi leaveat 14:45\n<|endofbelief|> <|action|> general bye none <|endofaction|> <|response|> thank you for using our ser-\nvice <|endofresponse|>\nTable 9: Example of multi-domain dialogue state tracking for long context, where dialogue context\nspanning over 3 domains, i.e. attraction, hotel, and taxi. The belief states for taxi departure and\ndestination (highlighted) should be inferred from hotel and attraction domains.\nRegarding Noisy Annotations Concrete examples of four types of noisy-labeled annotation deﬁned in\nsection 5, are shown in Tables 10, 11, 12, 13 and 14. The results indicate that SimpleTOD is robust to noisy\nannotation and can often generate the correct belief state in situations where annotation is incorrect. As mentioned\nin section 4.1, the list of noisy annotations is released with this paper.\nFull Dialogue Example Table 15 and Table 16 include several turns as part of a full dialogue for delexical-\nized and lexicalized outputs. These are from our strongest SimpleTOD model in the setting that requires it to\ngenerate all outputs, which means that it ignores all DB Search results.\n16\nContext (’SNG01608’, turn 2)\n<|context|> <|user|> are there any portuguese restaurant -s in cambridge ? <|system|> yes there is\na portuguese restaurant in cambridge with 2 different location -s , would you like the addresses ?\n<|user|> if 1 of them has a moderate price range please give me that address . if not tell me about\nturkish restaurant -s instead . <|endofcontext|>\nTarget <|belief|> restaurant food turkish, restaurant pricerange moderate <|endofbelief|>\nSimpleTOD <|belief|> restaurant food portugese, restaurant pricerange moderate<|endofbelief|>\nContext (’MUL1575’, turn 3)\n<|context|> <|user|> i would like to ﬁnd a particular restaurant in cambridge . the name of the restaurant\nis restaurant 2 two . could you give me the location ? <|system|> restaurant 2 two is nice french\nrestaurant located at 22 chesterton road chesterton . would like me to book you a table ? <|user|> that\nwould be great . i need it for 8 on friday . <|system|> do you have a time preference ? <|user|> yes at\n11:15 if that s not available i can do 10:15 <|endofcontext|>\nTarget <|belief|> restaurant book day friday, restaurant book people 8, restaurant book time 10:15, restaurant\nname restaurant 2 two<|endofbelief|>\nSimpleTOD <|belief|> restaurant book day friday, restaurant book people 8, restaurant book time 11:15, restaurant\nname restaurant 2 two <|endofbelief|>\nContext (’PMUL1194’, turn 6)\n<|context|> <|user|> hello , i need information about old schools . <|system|> there are 5 local colleges\n, do you have a particular area you are interested in ? <|user|> i do not have an area preference .\n<|system|> old schools is an architectural attraction in the center of town . they are located on trinity\nlane , in postcode cb21tt . <|user|> what is the phone number ? <|system|> the phone number is\n01223332320 . do you need any more information ? <|user|> yes , i am looking for a train that leaves\non wednesday for cambridge <|system|> i have over 200 entries . let s try to narrow that down . what\ntime would you like to arrive ? <|user|>monday at 11:00 for 5 people please <|system|> just to conﬁrm\n, you want to leave on monday , or wednesday ? and where are you departing from ? <|user|> leaving\nely anytime after 15:15 and i need to know how long the ride is and the speciﬁc arrival time please .\n<|endofcontext|>\nTarget <|belief|> attraction name old schools, train book people 5, train day monday, train departure ely, train\ndestination cambridge, train leaveat 15:15 <|endofbelief|> 0\nSimpleTOD <|belief|> attraction name old schools, train book people 5, train day wednesday, train departure ely,\ntrain destination cambridge, train leaveat 15:15 <|endofbelief|>\nContext (’MUL1024’, turn 7)\n<|context|> <|user|> i am looking for a theatre to visit in the centre of cambridge . can you help me\nwith this ? <|system|> i can recommend the adc theatre on park street or the cambridge arts theatre\nlocated at 6 st edward s passage <|user|> are either in the city center ? <|system|> both are located\nin the city centre . <|user|> can you give me the address of the adc theatre please ? <|system|> the\naddress is park street . <|user|> what ﬁlms are playing today ? <|system|> i am sorry . i do not have\nthat information . can i help you with anything else ? <|user|> i am looking for a particular hotel called\navalon . <|system|> avalon is located at 62 gilbert road . would you like me to book a reservation ?\n<|user|> great , can i book that for 6 people for 3 nights starting on tuesday . <|system|> i am sorry , the\nbooking was unsuccessful . would you like to try for another date or a shorter stay ? <|user|> would\nyou be interested in staying at the avalon for 1 night ? if not we can ﬁnd another hotel available for a\n3 night stay for 6 guests . <|endofcontext|>\nTarget <|belief|> attraction area centre, attraction name adc theatre, attraction type theatre, hotel book day\ntuesday, hotel book people 6, hotel book stay 3, hotel name avalon <|endofbelief|>\nSimpleTOD <|belief|> attraction area centre, attraction name adc theatre, attraction type theatre, hotel book day\ntuesday, hotel book people 6, hotel book stay 1, hotel name avalon <|endofbelief|>\nTable 10: Examples of Type 1 noisy-labeled annotation, context lacks enough information to infer\nthe true belief state.\n17\nContext (’MUL0088’, turn 2)\n<|context|> <|user|> i am looking for a cheap hotel with free parking near cambridge . <|system|> i\nhave multiple cheap hotel -s with free parking . what part of town are you interested in staying in ?\n<|user|> i would like to stay close to the center area , but the hotel should be 3 star . <|endofcontext|>\nTarget <|belief|> hotel parking yes, hotel pricerange cheap <|endofbelief|>\nSimpleTOD <|belief|> hotel area centre, hotel parking yes, hotel pricerange cheap, hotel stars 3 <|endofbelief|>\nContext (’PMUL2437’, turn 3)\n<|context|> <|user|> i want to ﬁnd a moderate -ly priced restaurant . <|system|> i have many options\navailable for you ! is there a certain area or cuisine that interests you ? <|user|> yes i would like the\nrestaurant to be located in the center of the attractions . <|system|> there are 21 restaurant -s available\nin the centre of town . how about a speciﬁc type of cuisine ? <|user|> i need to know the food type and\npostcode and it should also have mutliple sports <|endofcontext|>\nTarget <|belief|> restaurant area centre, restaurant pricerange moderate <|endofbelief|>\nSimpleTOD <|belief|> attraction type multiple sports, restaurant area centre, restaurant pricerange moderate <|end-\nofbelief|>\nContext (’PMUL2437’, turn 3)\n<|context|> <|user|> i want to ﬁnd a moderate -ly priced restaurant . <|system|> i have many options\navailable for you ! is there a certain area or cuisine that interests you ? <|user|> yes i would like the\nrestaurant to be located in the center of the attractions . <|system|> there are 21 restaurant -s available\nin the centre of town . how about a speciﬁc type of cuisine ? <|user|> i need to know the food type and\npostcode and it should also have mutliple sports <|endofcontext|>\nTarget <|belief|> restaurant area centre, restaurant pricerange moderate <|endofbelief|>\nSimpleTOD <|belief|> attraction type multiple sports, restaurant area centre, restaurant pricerange moderate <|end-\nofbelief|>\nContext (’MUL1060’, turn 4)\n<|context|> <|user|> hello , i would like to ﬁnd a hotel that include -s free parking . <|system|> most\nof the hotel -s in town offer free parking . is there a certain area you would like to stay in , or do you\nhave a price range in mind ? <|user|> yes . the centre would be nice and also free wiﬁ . <|system|>\nthe university arms is an expensive , 4 star hotel with free wiﬁ . comparatively , the alexander bed and\nbreakfast is a cheap -ly priced guesthouse , also 4 stars . <|user|> please book me some rooms for the\nuniversity arms to accommodate 8 people for 3 nights starting on wednesday . can you also provide\nme the reference number after you book ? <|system|> your reference number is x5ny66zv . <|user|>\nthank you . can you please help me ﬁnd a place to go in town in the same area as the hotel ?preferably\na college . <|endofcontext|>\nTarget\n<|belief|> attraction area centre, attraction name college, hotel area centre, hotel book day wednesday,\nhotel book people 8, hotel book stay 3, hotel name university arms hotel, hotel parking yes <|endofbe-\nlief|>\nSimpleTOD\n<|belief|> attraction area centre, attraction type college, hotel area centre, hotel book day wednesday,\nhotel book people 8, hotel book stay 3, hotel internet yes, hotel name university arms hotel, hotel\nparking yes <|endofbelief|>\nContext (’MUL1642’, turn 5)\n<|context|> <|user|> hello , i am trying to ﬁnd a train that goes from cambridge to london kings cross\n. can you help me book a ticket ? <|system|> i can help with that . can you tell me what day you will\nbe traveling ? <|user|> i need to leave on saturday after 18:45 . <|system|> the soonest departure time\nwould be at 19:00 on saturday , is that okay ? <|user|> yes , that s perfect . can you book that for 8\npeople ? <|system|> you are all booked with reference number 144vdbrm . the cost of 151.04 gbp will\nbe payable at the station . can i be of further assistance today ? <|user|> i am looking for an expensive\nplace to eat in the centre , what is there that ﬁts that criteria ? <|system|> there 33 place -s that ﬁt your\ncriteria . do you have a particular cuisine type in mind so that i can narrow the results down ? <|user|>\nit does not matter what kind of food . what would you recommend for a large group of 8 people ?\n<|endofcontext|>\nTarget <|belief|> restaurant area centre, restaurant pricerange expensive, train book people 8, train day satur-\nday, train departure cambridge, train destination london kings cross, train leaveat 18:45 <|endofbelief|>\nSimpleTOD\n<|belief|> restaurant area centre, restaurant book people 8, restaurant pricerange expensive, train book\npeople 8, train day saturday, train departure cambridge, train destination london kings cross, train\nleaveat 18:45 <|endofbelief|>\nContext (’MUL0088’, turn 7)\n<|context|> <|user|> i am looking for a cheap hotel with free parking near cambridge . <|system|> i\nhave multiple cheap hotel -s with free parking . what part of town are you interested in staying in ?\n<|user|> i would like to stay close to the center area , but the hotel should be 3 star . <|system|> we do\nnot have any hotel -s that match your search . do you want to try something else ? <|user|> are there\nany moderate 3 star hotel -s with free parking ? <|system|> we do not have any hotel -s that match\nyour search . do you want to try something else ? <|user|> how about 1 in the moderate price range ?\n<|system|> we have 3 entries that match your preferences . would you prefer north , south , or west ?\n<|user|> i do not have a preference but i would also like ot ﬁnd a restaurant called the cow pizza kitchen\nand bar . <|system|> before i book your restaurant would you like to book your lodging ? i think you\nwill like hamilton lodge . it meets your needs . <|user|> does it have internet ? <|system|> yes , the\nhamilton lodge has internet . <|user|> sounds great . what is the address and contact information ?\n<|endofcontext|>\nTarget <|belief|> hotel internet yes, hotel parking yes, hotel pricerange moderate, hotel stars 3, restaurant\nname cow pizza kitchen and bar <|endofbelief|>\nSimpleTOD <|belief|> hotel internet yes, hotel name hamilton lodge, hotel parking yes, hotel pricerange moderate,\nhotel stars 3, restaurant name cow pizza kitchen and bar <|endofbelief|>\nTable 11: Examples of Type 2 noisy-labeled annotation, belief state is not labeled, while context\ncontains the information.\n18\nContext (’MUL1926’, turn 1) <|context|> <|user|> yes hello ! i am looking for a train leaving on tuesday from norwich to cambridge\n. <|endofcontext|>\nTarget <|belief|> train departure norwich, train destination cambridge <|endofbelief|>\nSimpleTOD <|belief|> train day tuesday, train departure norwich, train destination cambridge <|endofbelief|>\nContext (’MUL1650’, turn 3)\n<|context|> <|user|> i am looking for a chinese restaurant in the centre of cambridge . <|system|> there\nare quite a few chinese restaurant -s in the area ! do you have a price range you would like to stay\nwithin ? <|user|> no , anything is ﬁne . can you suggest a great 1 ? <|system|> jinling noodle bar looks\nlike a good place . <|user|> can i have their phone number and address ? also , are there any trains that\ngo from peterborough to cambridge ? <|endofcontext|>\nTarget <|belief|> restaurant area centre, restaurant food chinese, restaurant name jinling noodle bar <|endofbe-\nlief|>\nSimpleTOD <|belief|> restaurant area centre, restaurant food chinese, restaurant name jinling noodle bar, train de-\nparture peterborough, train destination cambridge <|endofbelief|>\nContext (’MUL2376’, turn 3)\n<|context|> <|user|> where would i ﬁnd the museum of archaeology and anthropology ? <|system|>\nthe museum of archaelogy and anthropology is located at university of cambridge , downing street .\ntheir phone number is 01223333516 . do you need anything else ? <|user|> and the attraction type is\na museum , right ? <|system|> yes , it is a museum . <|user|> sounds great . thank you for the info !\n<|endofcontext|>\nTarget <|belief|> attraction name museum of archaelogy and anthropology <|endofbelief|>\nSimpleTOD <|belief|> attraction name museum of archaelogy and anthropology, attraction type museum <|endof-\nbelief|>\nContext (’MUL1076’, turn 9)\n<|context|> <|user|> yes i am looking for some hotel -s that include free parking and cheap rates in the\ncambridge area . <|system|> there are 10 hotel -s , do you have any other preferences to narrow it down\n? <|user|> sure . i would like a guesthouse . <|system|> there are 9 options available . do you have a\npreference for how many stars or what area speciﬁcally ? <|user|> i do not care . please book 1 that\nyou recommend . <|system|> ok , how about the allenbell ? to book a stay i’ll need to know the ﬁrst\nday you want to stay , how many days , and how many guests . <|user|> i need it for 8 people , and for\n4 nights starting saturday . <|system|> great , i have booked your stay for 8 people for 4 nights . your\nreference number is pb41wuot . may i help with anything else ? <|user|> i also would like to ﬁnd a\nmuseum in the west . <|system|> there are several museums there , any preferences ? <|user|> i would\nlike to ﬁnd the museum of science , please . <|system|> i am sorry , but i am having trouble locating\nyour requested attraction . can you please restate your request ? <|user|> is there a museum of science\n? <|system|> i am not ﬁnding 1 , sorry . <|user|> are there any other museums in that area ? <|system|>\nyes there are 7 museums in the west area of town for your pleasure . is there anything else you would\nlike to narrow them down with ? <|user|> the museum i want should be in the west side of town and\ncan i have the postcode for the museum . <|endofcontext|>\nTarget <|belief|> attraction area west, attraction type museum, hotel book day saturday, hotel book people 8,\nhotel book stay 4, hotel parking yes, hotel pricerange cheap, hotel type guesthouse <|endofbelief|>\nSimpleTOD <|belief|> attraction area west, attraction type museum, hotel book day saturday, hotel book people\n8, hotel book stay 4, hotel name allenbell, hotel parking yes, hotel pricerange cheap, ’hotel type\nguesthouse <|endofbelief|>\nTable 12: Examples of Type 2 noisy-labeled annotation, belief state is not labeled, while context\ncontains the information.\n19\nContext (’SNG1036’, turn 1) <|context|> <|user|> i was not planning to stay in town tonight , but it looks like i am going to have to .\ncan you help me ﬁnd a pretty cheap room ? <|endofcontext|>\nTarget <|belief|> hotel pricerange cheap, hotel type hotel <|endofbelief|>\nSimpleTOD <|belief|> hotel pricerange cheap <|endofbelief|>\nContext (’PMUL1194’, turn 1) <|context|> <|user|> hello , i need information about old schools . <|endofcontext|>\nTarget <|belief|> attraction type college <|endofbelief|>\nSimpleTOD <|belief|> attraction name old schools <|endofbelief|>\nContext (’SNG0284’, turn 2) <|context|> <|user|> i want a train from london liverpool street arriving at 9:30 <|system|> which day\nwill you be traveling ? <|user|> i want to leave on thursday .\nTarget <|belief|> train arriveby 09:30,train book people 2, train day thursday, train departure london liverpool\nstreet <|endofbelief|>\nSimpleTOD <|belief|> train arriveby 09:30, train day thursday, train departure london liverpool street\nContext (’PMUL0069’, turn 2)\n<|context|> <|user|> i am looking for some hungarian food restaurant -s near the centre , please . <|sys-\ntem|> i am sorry there are no hungarian restaurant -s near centre . <|user|> what kind of expensive\nrestaurant -s are in the center of town ?\nTarget <|belief|> restaurant area centre, restaurant food hungarian, restaurant pricerange expensive <|endofbe-\nlief|>\nSimpleTOD <|belief|> restaurant area centre, restaurant pricerange expensive <|endofbelief|>\nContext (’PMUL3688’, turn 3)\n<|context|> <|user|> i am looking for a place to go in the centre of town . <|system|> is there any type\nof attraction you would like to see ? <|user|> any of your choice . get me the address and entrance\nfee <|system|> the holy trinity church is located on market street . the entrance fee is free . is there\nanything else i can help with ? <|user|> i also need a train out to the airport on friday . <|endofcontext|>\nTarget <|belief|> attraction area centre, train day friday, train destination stansted airport<|endofbelief|>\nSimpleTOD <|belief|> attraction area centre, train day friday <|endofbelief|>\nTable 13: Examples of Type 3 noisy-labeled annotation, belief state is labeled, while context lacks\nthe information.\nContext (’SNG02207’, turn 1) <|context|> <|user|> i need to book a tax departing from gandhi . <|endofcontext|>\nTarget <|belief|> taxi departure the gandhi <|endofbelief|>\nSimpleTOD <|belief|> taxi departure gandhi <|endofbelief|>\nContext (’MUL1811’, turn 2)\n<|context|> <|user|> i am looking for something to do in town . are there any interesting attraction in\nthe north part of town ? <|system|> yes , there are several places: i see a boat , a park ,and 2 swimming\npools . would 1 of these work for you ? <|user|> ooh , a pool would be fun . can you give me their\ncontact info ? <|endofcontext|>\nTarget <|belief|> attraction area north, attraction type pool <|endofbelief|>\nSimpleTOD <|belief|> attraction area north, attraction type swimming pool <|endofbelief|>\nContext (’PMUL0548’, turn 4) <|context|> <|user|> hello , i would like some information on a restaurant . <|system|> do you have a\ncertain cuisine or area in mind ? <|user|> i want british food . in the east , please . <|system|> the\ngraffton hotel restaurant is in the east and serves british food . it s location is 619 newmarket rd . fen\nditton . <|user|> i will need it for saturday , 7 people at 17:45 please . <|system|> i have successfully\nbooked a table for 7 on saturday at 17:45 for you . the table will be reserved for 15 minutes and your\nreference number is r8cv0hu9 . <|user|> i am also looking for a train on sunday . <|endofcontext|>\nTarget <|belief|> restaurant area east, restaurant book day saturday, restaurant book people 7, restaurant book\ntime 17:45, restaurant food british, restaurant name grafton hotel restaurant, train day sunday <|endof-\nbelief|>\nSimpleTOD <|belief|> restaurant area east, restaurant book day saturday, restaurant book people 7, restaurant book\ntime 17:45, restaurant food british, restaurant name graffton hotel restaurant, train day sunday <|end-\nofbelief|>\nContext (’PMUL0550’, turn 7)\n<|context|> <|user|> hey i am looking for a train from cambridge to bishops stortford . mind helping\nout ? <|system|> many trains are leaving on friday . mind choosing the time ? <|user|> i want to leave\non monday and arrive by 18:45 . <|system|> i have train tr4283 that leaves cambridge at 5:29 and\narrives in bishops stortford at 6:07 . would you like to make reservations ? <|user|> could you give me\nthe travel time and price of that train please ? <|system|> the cost is 10.10 pounds . the duration of\nthe trip is 38 minutes , leaving cambridge at 5:29 and arriving at 6:07 in bishops stortford . <|user|> i\nalso want a cheap chinese restaurant . <|system|> there are 4 great restaurant -s to choose from . do\nyou prefer 1 on the south side of town or in the centre ? <|user|> i would like to be in the centre please\n. <|system|> i recommend charlie chan . would you like to reserve a table ? <|user|> yes . monday\n, 8 people , 10:30 . <|system|> booking was successful . the table will be reserved for 15 minutes .\nreference number is : ifjmbkli . <|user|> thank you very much . that will be all for today . i appreciate\nall your help . <|endofcontext|>\nTarget\n<|belief|> restaurant area centre, restaurant book day monday, restaurant book people 8, restaurant book\ntime 10:30, restaurant food chinese,restaurant name charlie, restaurant pricerange cheap, train arriveby\n18:45, train day monday, train departure cambridge, train destination bishops stortford <|endofbelief|>\nSimpleTOD\n<|belief|> restaurant area centre, restaurant book day monday, restaurant book people 8, restaurant\nbook time 10:30, restaurant food chinese, restaurant name charlie chan, restaurant pricerange cheap,\ntrain arriveby 18:45, train day monday, train departure cambridge, train destination bishops stortford\n<|endofbelief|>\nTable 14: Examples of Type 4 noisy-labeled annotation, belief state value is misspelled.\n20\nTurn 1\nContext <|context|> <|user|> hello , i need a train to ely on sunday please <|endofcontext|>\nTarget\n<|belief|> train destination ely <|endofbelief|> <|action|> train inform choice, train inform day, train\ninform dest, train request arrive <|endofaction|> <|response|> we have [value_count] trains traveling to\n[value_place] on [value_day] . what time would you like to arrive ? <|endofresponse|>\nSimpleTOD\n<|belief|> train day sunday, train destination ely <|endofbelief|> <|action|> train inform choice, train in-\nform day, train inform leave, train inform depart, train inform dest, train inform arrive, train inform id,\ntrain offerbook none <|endofaction|> <|response|> there are [value_count] trains leaving [value_place]\non [value_day]. [train_id] leaves [value_place] at [value_time] and arrives in [value_place] at\n[value_time]. would you like to book a ticket? <|endofresponse|>\nTurn 2\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00\ndeparting from cambridge . <|endofcontext|>\nTarget\n<|belief|> train destination ely, train day sunday, train arriveby 13:00, train departure cambridge <|end-\nofbelief|> <|action|> train inform arrive, train inform day, train inform dest, train inform id, train\nofferbook none <|endofaction|> <|response|> [train_id] will get you to [value_place] by [value_time]\non [value_day] . should i book you a ticket ? <|endofresponse|>\nSimpleTOD\n<|belief|> train day sunday, train destination ely, train departure cambridge, train arriveby 13:00 <|end-\nofbelief|> <|action|> train inform arrive, train inform leave, train inform id, train offerbook none <|end-\nofaction|> <|response|> [train_id] leaves at [value_time] and arrives at [value_time]. would you like\nme to book it for you? <|endofresponse|>\nTurn 3\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00\ndeparting from cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book\nyou a ticket ? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference\nnumber ? <|endofcontext|>\nTarget\n<|belief|> train destination ely, train day sunday, train arriveby 13:00, train departure cambridge, train\nbook people 8 <|endofbelief|> <|action|> train offerbooked ref, train offerbooked ticket <|endofaction|>\n<|response|> booking was successful , the total fee is [value_price] gbp payable at the station . reference\nnumber is [train_reference] . <|endofresponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, train day sunday, train departure cambridge, train\ndestination ely <|endofbelief|> <|action|> general reqmore none, train offerbooked ref, train offer-\nbooked ticket <|endofaction|> <|response|> booking was successful, the total fee is [value_price] gbp\npayable at the station. reference number is [train_reference]. is there anything else i can help you\nwith? <|endofresponse|>\nTurn 4\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction recommend name, gen-\neral greet none <|endofaction|> <|response|> how about [attraction_name] ? it looks lovely <|endofre-\nsponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction recommend name, at-\ntraction recommend area, attraction recommend fee, attraction recommend addr, attraction recommend\nphone <|endofaction|> <|response|> i would recommend the [attraction_name] in the [value_area]. it\nis located at [attraction_address]. the entrance fee is [value_count] pounds. the phone number is\n[attraction_phone] <|endofresponse|>\nTurn 5\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|system|> how\nabout cafe jello gallery ? it looks lovely <|user|> that will work . could you get me the phone number\nand area its in ? <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train de-\nparture cambridge, train destination ely <|endofbelief|> <|action|> general greet none, general reqmore\nnone, attraction inform phone, attraction inform area <|endofaction|> <|response|> sure ! it s in the\n[value_area] area of town , and the phone number is [attraction_phone] . can i help you with anything\nelse ? <|endofresponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction inform phone, attrac-\ntion inform area, attraction inform name<|endofaction|> <|response|> [attraction_name] is in the\n[value_area] area and the phone number is [attraction_phone] <|endofresponse|>\nTurn 6\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|system|> how\nabout cafe jello gallery ? it looks lovely <|user|> that will work . could you get me the phone number\nand area its in ? <|system|> sure ! it s in the west area of town , and the phone number is 01223312112\n. can i help you with anything else ? <|user|> you were great today . thanks so much for all the help .\nthat is all . goodbye . <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train de-\nparture cambridge, train destination ely <|endofbelief|> <|action|> general bye none, general welcome\nnone, general greet none <|endofaction|> <|response|> it was a pleasure to help . have a good day .\ngoodbye . <|endofresponse|>\nSimpleTOD <|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> general bye none <|endofaction|>\n<|response|> thank you for using our service <|endofresponse|>\nTable 15: SimpleTOD end-to-end generation (delexicalized) on MultiWOZ (dialogue ’PMUL3663’).\n21\nTurn 1\nContext <|context|> <|user|> hello , i need a train to ely on sunday please <|endofcontext|>\nTarget\n<|belief|> train destination ely <|endofbelief|> <|action|> train inform choice, train inform day, train\ninform dest, train request arrive <|endofaction|> <|response|> we have 10 trains traveling to ely on\nsunday . what time would you like to arrive ? <|endofresponse|>\nSimpleTOD\n<|belief|> train day sunday, train destination ely <|endofbelief|> <|action|> train inform choice, train\ninform day, train inform leave, train inform depart, train inform dest, train inform arrive, train inform\nid, train offerbook none <|endofaction|> <|response|> there are 10 trains leaving cambridge on sun-\nday. tr3177 leaves cambridge at 9:50 and arrives in ely at 10:07. would you like to book a ticket?\n<|endofresponse|>\nTurn 2\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00\ndeparting from cambridge . <|endofcontext|>\nTarget\n<|belief|> train destination ely, train day sunday, train arriveby 13:00, train departure cambridge <|end-\nofbelief|> <|action|> train inform arrive, train inform day, train inform dest, train inform id, train\nofferbook none <|endofaction|> <|response|>tr1159 will get you to ely by 12:07 on sunday . should i\nbook you a ticket ? <|endofresponse|>\nSimpleTOD\n<|belief|> train day sunday, train destination ely, train departure cambridge, train arriveby 13:00 <|end-\nofbelief|> <|action|> train inform arrive, train inform leave, train inform id, train offerbook none <|end-\nofaction|> <|response|> tr1159 leaves at 11:50 and arrives at 12:07. would you like me to book it for\nyou? <|endofresponse|>\nTurn 3\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00\ndeparting from cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book\nyou a ticket ? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference\nnumber ? <|endofcontext|>\nTarget\n<|belief|> train destination ely, train day sunday, train arriveby 13:00, train departure cambridge, train\nbook people 8 <|endofbelief|> <|action|> train offerbooked ref, train offerbooked ticket <|endofaction|>\n<|response|>booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis hdx8ix63. <|endofresponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, train day sunday, train departure cambridge, train\ndestination ely <|endofbelief|> <|action|> general reqmore none, train offerbooked ref, train offer-\nbooked ticket <|endofaction|> <|response|> booking was successful, the total fee is 28.16 gbp payable\nat the station. reference number is hdx8ix63. is there anything else i can help you with? <|endofre-\nsponse|>\nTurn 4\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction recommend name, gen-\neral greet none <|endofaction|> <|response|> how about cafe jello gallery ? it looks lovely <|endofre-\nsponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction recommend name, at-\ntraction recommend area, attraction recommend fee, attraction recommend addr, attraction recommend\nphone <|endofaction|> <|response|> i would recommend the cafe jello gallery in the west. it is located\nat 13 magdalene street. the entrance fee is 0 pounds. the phone number is 01223464646 <|endofre-\nsponse|>\nTurn 5\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|system|> how\nabout cafe jello gallery ? it looks lovely <|user|> that will work . could you get me the phone number\nand area its in ? <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> general greet none, general req-\nmore none, attraction inform phone, attraction inform area <|endofaction|> <|response|> sure ! it s in\nthe west area of town , and the phone number is 01223464646 . can i help you with anything else ?\n<|endofresponse|>\nSimpleTOD\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> attraction inform phone, attraction\ninform area, attraction inform name<|endofaction|> <|response|> cafe jello gallery is in the west area\nand the phone number is 01223464646 <|endofresponse|>\nTurn 6\nContext\n<|context|> <|user|> hello , i need a train to ely on sunday please <|system|> we have 10 trains traveling\nto ely on sunday . what time would you like to arrive ? <|user|> how about 1 arriving by 13:00 departing\nfrom cambridge . <|system|> tr1159 will get you to ely by 12:07 on sunday . should i book you a ticket\n? <|user|> yes , that would be great . i’ll need tickets for 8 people . do you have a reference number ?\n<|system|> booking was successful , the total fee is 28.16 gbp payable at the station . reference number\nis : hdx8ix63 . <|user|> i am also looking for a museum . can you recommend 1 ? <|system|> how\nabout cafe jello gallery ? it looks lovely <|user|> that will work . could you get me the phone number\nand area its in ? <|system|> sure ! it s in the west area of town , and the phone number is 01223312112\n. can i help you with anything else ? <|user|> you were great today . thanks so much for all the help .\nthat is all . goodbye . <|endofcontext|>\nTarget\n<|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train de-\nparture cambridge, train destination ely <|endofbelief|> <|action|> general bye none, general welcome\nnone, general greet none <|endofaction|> <|response|> it was a pleasure to help . have a good day .\ngoodbye . <|endofresponse|>\nSimpleTOD <|belief|> train book people 8, train arriveby 13:00, attraction type museum, train day sunday, train\ndeparture cambridge, train destination ely <|endofbelief|> <|action|> general bye none <|endofaction|>\n<|response|> thank you for using our service <|endofresponse|>\nTable 16: SimpleTOD end-to-end generation (lexicalized) on MultiWOZ (dialogue ’PMUL3663’).\n22",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.8134591579437256
    },
    {
      "name": "Computer science",
      "score": 0.8001554608345032
    },
    {
      "name": "Task (project management)",
      "score": 0.6608989834785461
    },
    {
      "name": "Language model",
      "score": 0.6035658717155457
    },
    {
      "name": "Dialog box",
      "score": 0.6031541228294373
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5859954357147217
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5555232763290405
    },
    {
      "name": "Natural language processing",
      "score": 0.46632272005081177
    },
    {
      "name": "Machine learning",
      "score": 0.45363718271255493
    },
    {
      "name": "Human–computer interaction",
      "score": 0.35458454489707947
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}