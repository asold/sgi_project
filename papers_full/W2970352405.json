{
  "title": "Multi-Source Transformer for Kazakh-Russian-English Neural Machine Translation",
  "url": "https://openalex.org/W2970352405",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2252210186",
      "name": "Patrick Littell",
      "affiliations": [
        "National Research Council Canada"
      ]
    },
    {
      "id": "https://openalex.org/A3148429403",
      "name": "Chi-Kiu Lo",
      "affiliations": [
        "National Research Council Canada"
      ]
    },
    {
      "id": "https://openalex.org/A2110352811",
      "name": "Samuel Larkin",
      "affiliations": [
        "National Research Council Canada"
      ]
    },
    {
      "id": "https://openalex.org/A5103359343",
      "name": "Darlene Stewart",
      "affiliations": [
        "National Research Council Canada"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W193080678",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2250907725",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2778814079",
    "https://openalex.org/W2159755860",
    "https://openalex.org/W2161227214",
    "https://openalex.org/W2903297715",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2962929176",
    "https://openalex.org/W2963407669",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W932413789",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2897058237",
    "https://openalex.org/W3082674894",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2180952760",
    "https://openalex.org/W2970986500",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963216553"
  ],
  "abstract": "We describe the neural machine translation (NMT) system developed at the National Research Council of Canada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT taking both the original Kazakh sentence and its Russian translation as input for translating into English.",
  "full_text": "Proceedings of the Fourth Conference on Machine Translation (WMT), V olume 2: Shared Task Papers (Day 1) pages 267–274\nFlorence, Italy, August 1-2, 2019.c⃝2019 Association for Computational Linguistics\n267\nMulti-Source Transformer for\nKazakh-Russian-English Neural Machine Translation\nPatrick Littell Chi-kiu Lo Samuel Larkin Darlene Stewart\nNRC-CNRC\nNational Research Council of Canada\n1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada\n{Patrick.Littell|Chikiu.Lo|Samuel.Larkin|Darlene.Stewart}@nrc-cnrc.gc.ca\nAbstract\nWe describe the neural machine translation\n(NMT) system developed at the National Re-\nsearch Council of Canada (NRC) for the\nKazakh-English news translation task of the\nFourth Conference on Machine Translation\n(WMT19). Our submission is a multi-source\nNMT system taking both the original Kazakh\nsentence and its Russian translation as input\nfor translating into English.\n1 Introduction\nThe WMT19 (Bojar et al., 2019) Kazakh-English\nNews Translation task presented a machine trans-\nlation scenario in which parallel resources be-\ntween the two languages (˜200k sentences) were\nconsiderably fewer than parallel resources be-\ntween these languages and a third language, Rus-\nsian (˜14M English-Russian sentence pairs and\n˜5M Kazakh-Russian pairs).\nThe NRC team therefore explored machine\ntranslation pipelines that utilized the Russian re-\nsources, including:\n1. “Pivoting” through Russian: training an MT\nsystem from Kazakh to Russian, and another\nsystem from Russian to English (Fig. 1a).\n2. Creating a synthetic Kazakh-English paral-\nlel corpus by training a Russian-Kazakh MT\nsystem and using it to “cross-translate” 1 the\nRussian-English corpus (Fig. 1b).\n3. Training a multi-encoder (Libovick ´y and\nHelcl, 2017; Libovick ´y et al., 2018) Trans-\nformer system (Vaswani et al., 2017) from\n1We term synthetic data creation by translation between\nsource languages “cross-translation” to distinguish it from\n“back-translation” in the sense of Sennrich et al. (2016).\nNishimura et al. (2018), which also uses source 1-to-source2\ntranslation, calls both kinds of synthetic data creation “back-\ntranslation”, but because our pipeline uses both kinds we dis-\ntinguish them with separate terms.\nKazakh/Russian to English that subsumes\nboth of these approaches (Fig. 1c).\nTechniques (1) and (2) both involve the trans-\nlation of genuine data into a synthetic translation\n(into Russian in the ﬁrst case, and into Kazakh in\nthe second case). It is, however, possible to attend\nto both the original sentence and its translation\nusing multi-source techniques (Zoph and Knight,\n2016; Libovick´y and Helcl, 2017; Nishimura et al.,\n2018); we hypothesized that giving the system\nboth the originals and “cross-translations”, in\nboth directions (Kazakh-to-Russian and Russian-\nto-Kazakh), would allow the system to make use\nof the additional information available by seeing\nthe sources before translation.\nOur multi-encoder Transformer approach per-\nformed best among our submitted systems by a\nconsiderable margin, outperforming pivoting by\n4.2 BLEU and augmentation by one-way cross-\ntranslation by 10.2 BLEU.2\n2 Multilingual data\n2.1 Kazakh-English\nThe raw bilingual Kazakh-English data provided\nfor the constrained news translation task consists\nof web-crawled data, news commentary data and\nWikipedia article titles. In total, they account for\n˜200k sentence pairs. All these data were used to\ntrain the foundation systems for back-translation.\nSince the web-crawled data is very noisy, we re-\nmoved all the web-crawled portion from the train-\ning data before training our ﬁnal submitted sys-\ntem.\nFor tuning and evaluating, we used the\nnewsdev2019-kken data set; for SMT, we\n2However, these systems, as submitted, are not directly\ncomparable due to some additional data ﬁltering in our ﬁnal\nsubmitted system; we will be releasing more direct compar-\nisons and a more thorough description of the architecture in a\ncompanion article.\n268\n(a) “Pivoting”: two systems (source-\nto-L3 and L3-to-target) executed in a\npipeline\n(b) Augmentation of source/target\ncorpus with “cross-translated” syn-\nthetic data\n(c) Multi-source system with augmen-\ntation by cross-translation in both di-\nrections\nFigure 1: Approaches to utilizing a third language (“L3”) in machine translation.\nsplit it into two sets as our internal dev and dev-\ntest; dev contains 1266 sentence pairs and devtest\ncontains the remaining 800 sentence pairs.\n2.2 Kazakh-Russian\nThe raw bilingual Kazakh-Russian data provided\nto assist in the news translation task is web-\ncrawled data. In total, they account for ˜5M sen-\ntence pairs. All these data were used to train the\nfoundation systems for cross-translation.\nFor tuning and evaluating, we randomly se-\nlected 1000 sentence pairs each for the dev and\ndevtest sets from the provided bilingual data. The\nremaining bilingual data is de-duplicated against\nthe bag of 6-grams collected from the dev and de-\nvtest sets. The de-duplicated bilingual data has\n˜4.2M sentence pairs.\n2.3 Russian-English\nThe raw bilingual Russian-English data we used\nin our systems consists of web-crawled data, news\ncommentary data and Wikipedia article titles. In\ntotal they account for ˜14M sentence pairs. All\nthese data were used to train the foundation sys-\ntems for back-translation. Since the Paracrawl\nportion of the bilingual data is very noisy, be-\nfore training our ﬁnal submitted system we ran our\nparallel corpus ﬁltering pipeline (Lo et al., 2018)\nwith YiSi-2 as the scoring function (instead of MT\n+ YiSi-1) and trimmed the size of the Paracrawl\nportion from 12M sentence pairs to 4M sentence\npairs.\nFor tuning and evaluating, we used the\nnewstest2017-enru data set as the dev set\nand the newstest2018-enru data set as the\ndevtest set.\n3 Data preparation\n3.1 Cleaning and tokenization\nOur preprocessing pipeline begins by cleaning the\nUTF-8 with both Moses’ cleaning script 3 and an\nin-house script that performs additional white-\nspace, hyphen, and control character normaliza-\ntion. We then proceed to normalize and tokenize\nthe sentences with Moses’ punctuation normaliza-\ntion4 and tokenization scripts5.\n3.2 Transliteration\nTo mitigate some of the overall complexity, and\nallow greater sharing in joint BPE models and\nweight tying, we ﬁrst converted the Kazakh and\nRussian text from Cyrillic to Roman, using ofﬁ-\ncial Romanization standards using spm normalize\n(Kudo, 2018) and transliteration tables from Wik-\ntionary for Kazakh6 and Russian7.\n3.3 Byte-pair encoding\nOur BPE model is a joint one across transliter-\nated Kazakh, transliterated Russian, and English.\nUsing fastBPE8, we created a 90k-operation BPE\nmodel, balancing the three languages with ˜8.2M\nsentences of each, using:\n•all available Kazakh from bilinugual kk-en;\n•all available Kazakh from bilinugual kk-ru;\n3github.com/moses-smt/\nmosesdecoder/scripts/tokenizer/\nremove-non-printing-char.perl\n4github.com/moses-smt/mosesdecoder/\nscripts/tokenizer/normalize-punctuation.\nperl\n5github.com/moses-smt/mosesdecoder/\nscripts/tokenizer/tokenizer.perl\n6en.wiktionary.org/wiki/Module:\nkk-translit\n7en.wiktionary.org/wiki/Module:\nru-translit\n8github.com/glample/fastBPE\n269\n•all monolingual Kazakh news and wiki data;\n•all available English from bilingual kk-en;\n•a sample of ˜8M English sentences from\nbilingual ru-en and monolingual en;\n•all available Russian from bilinugual kk-ru;\n•a sample of ˜3.2M Russian sentences from\nbilingual ru-en and monolingual ru.\nA separate vocabulary was extracted for each lan-\nguage using the corpora used to create the BPE\nmodel. The BPE model was then applied to all\ntraining, dev and devtest data.\n4 Multi-encoder transformer\nWe implemented a multi-source Transformer\n(Vaswani et al., 2017) architecture, in the Sock-\neye (Hieber et al., 2017) framework, that combines\nthe output of two encoders (one for Kazakh, one\nfor Russian); this architecture will be described in\ngreater detail in a companion paper.\nOur encoder combination takes place during at-\ntention (that is, the attention step in which infor-\nmation from the decoder and encoders are com-\nbined, rather than the self-attention steps inside\neach encoder and decoder); Figure 2 illustrates the\nposition in which the multiple sources are com-\nbined into a single representation.\nFirst, we perform multi-head scaled dot-product\nattention between the the decoder and each en-\ncoder separately.\nC(s) = MultiHead(s)\n(\nD,H(s),H(s)\n)\n(1)\nMultiHead(s) (Q,K,V ) =\nh∑\ni\nHead(s)\ni WO\ni\n(s)\n(2)\nHead(s)\ni (Q,K,V ,dk) =\nA(QWQ\ni\n(s)\n,KW K\ni\n(s)\n,V WV\ni\n(s)\n,dk) (3)\nA(Q,K,V ,dk) =softmax\n(QK⊤\n√dk\n)\nV (4)\nwhere D = (d1,d2,··· ,dn), di ∈Rdmodel repre-\nsents the decoder states, H = (h1,h2,··· ,hm),\nhi ∈ Rdmodel represents the outputs of the\nencoder’s ﬁnal self-attention layer, WQ\ni\n(s)\n∈\nRdmodel×dk , WK\ni\n(s) ∈ Rdmodel×dk , WV\ni\n(s) ∈\nFigure 2: Multi-source attention on S sources. Each\noutput from the S encoders is attended to by a sepa-\nrate multi-head attention layer (Eqs. 1-4), and then the\noutputs of these attention layers are combined (Eq. 5).\nRdmodel×dk and WO\ni\n(s) ∈Rdk×dmodel are trainable\nparameter matrices which project the key, query\nand value into a smaller dimensionality. Together\nwith dk = dmodel/h, we have C(s) ∈Rn×dmodel .\nNext, we combine the outputs from the different\nencoders with a simple projection and sum, similar\nto what Libovick ´y et al. (2018) refer to as “paral-\nlel”:\n˜C =\nS∑\ni\nC(i)WC(i)\n(5)\nAs this is essentially the same operation as the\nmulti-head combination in Equation (2), and no\nnonlinearities intervene, we can also conceptual-\nize Equations (1)-(5) as if they were a single multi-\nhead attention layer with S∗hheads (in this case\n2 ∗8 heads), in which each group of h heads is\nconstrained to attend to the output of one encoder.\nWe also experimented with a hierarchical atten-\ntion mechanism along the lines of Libovick ´y and\nHelcl (2017) and Libovick ´y et al. (2018), but as\nthis did not outperform the simpler combination\nmechanism in (5) in internal testing, our submit-\nted systems utilized the latter.\n270\nFigure 3: The relations of all the MT systems involved in building the NRC ﬁnal submitted system.\n5 Experiments and results\n5.1 NMT Setup\nOur code extends sockeye-1.18.72 from Hieber\net al. (2017). Each source encoder has 6 lay-\ners and our decoder also has 6 layers, with a\nmodel dimension of dmodel = 512 and 2048 hid-\nden units sub-layer feed-forward networks. We\nuse weight tying, where the source embeddings,\nthe target embeddings and the target softmax\nweights are tied, which implies a shared vocab.\nWe trained employing a cross-entropy loss with\nAdam (Kingma and Ba, 2014), β1 = 0.9, β2 =\n0.999, ϵ = 1e−8 and an initial learning rate of\n0.0001, decreasing the learning by 0.7 each time\nthe development-set BLEU did not improve for 8\ncheckpoints. We optimized against BLEU using\nnewsdev2019-kken as the development set,\nstopping early if BLEU did not improve for 32\ncheckpoints of 1000 updates each. The inputs and\noutput lengths were restricted to a maximum of 60\ntokens, and mini-batches were of variable size de-\npending on sentence length, with each mini-batch\ncontaining up to 4096 words.\n5.2 SMT Setup\nWe trained en2kk, ru2kk and en2ru SMT sys-\ntems using Portage (Larkin et al., 2010), a conven-\ntional log-linear phrase-based SMT system, us-\ning the corresponding BPEed parallel corpora pre-\npared as described in Section 3. The translation\nmodel of each SMT system uses IBM4 word align-\nments (Brown et al., 1993) with grow-diag-ﬁnal-\nand phrase extraction heuristics (Koehn et al.,\n2003). The systems each have two n-gram lan-\nguage models: a 5-gram language model (LM)\n(a mixture LM in the kk2en case) trained on the\ntarget-side of the corresponding parallel corpora\nusing SRILM (Stolcke, 2002), and a pruned 6-\ngram LM trained on the monolingual training cor-\npora (for en2ru, trained just on news using KenLM\n(Heaﬁeld, 2011); for ru2kk and en2kk, a static\nmixture LM trained on all monolingual Kazakh\ndata using SRILM). Each SMT system also in-\ncludes a hierachical distortion model, a sparse fea-\nture model consisting of the standard sparse fea-\ntures proposed in Hopkins and May (2011) and\nsparse hierarchical distortion model features pro-\nposed in Cherry (2013), and a neural network joint\nmodel, or NNJM, with 3 words of target con-\ntext and 11 words of source context, effectively a\n15-gram LM (Vaswani et al., 2013; Devlin et al.,\n2014). The parameters of the log-linear model\nwere tuned by optimizing BLEU on the develop-\nment set using the batch variant of the margin in-\nfused relaxed algorithm (MIRA) by Cherry and\nFoster (2012). Decoding uses the cube-pruning\nalgorithm of Huang and Chiang (2007) with a 7-\nword distortion limit.\nWe then used these SMT systems to back-\ntranslate a ˜2M sentence subselection of monolin-\ngual English news into Kazakh and Russian, and\na ˜5M sentence subselection of monolingual Rus-\nsian news into Kazakh, as well as cross-translating\nthe Russian of the ru-en parallel corpora into\nKazakh.\n5.3 Building the NRC Submission System\nOur ﬁnal submission involved several SMT com-\nponents and several NMT components to produce\nback-translations and cross-translations needed\nfor our multi-source submission system, as shown\nin Figure 3.\n271\nAvailable Training Dev./Test BLEU\nResources Source 1 Source 2 Att. Comb. Source 1 Source 2 Dev. Test\nkk-en kk+en2kk – – kk – 12.8 9.9\nkk-en, ru-en kk+ru+en2kk – – kk – 15.4 12.6\nkk-en, kk-ru, ru-en kk+ru2kk+en2kk – – kk – 17.9 14.8\nkk-ru, ru-en pivoting 19.3 20.8\nkk-en, kk-ru, ru-en kk+ru2kk+en2kk kk2ru+ru+en2ru Parallel kk kk2ru 19.6 24.2 /25.0*\nTable 1: BLEU scores on WMT19 Kazakh-English news translation. en2kk denotes synthetic Kazakh back-\ntranslated from English. ru2kk denotes synthetic Kazakh cross-translated from Russian. en2ru denotes synthetic\nRussian back-translated from English. kk2ru denotes synthetic Russian cross-translated from Kazakh. * denotes an\nunofﬁcial post-competition result, a fully-trained version of our top system, which had only been partially trained\ndue to time constraints.\n5.3.1 Synthetic cross-translations\nTo synthesize cross-translations, we trained 3 sys-\ntems using our ﬁltered ˜4.2M sentences of bilin-\ngual Russian-Kazakh data. First, we trained a\nRussian-to-Kazakh (ru2kk) SMT system and then\nused it to generate ˜5M sentences of synthetic\nKazakh. Augmenting the bilingual data with the\nKazakh back-translations, we trained a Kazakh-to-\nRussian NMT system to back translate ˜800k sen-\ntences of monolingual Kazakh news for a ru2kk\nNMT system and to cross translate ˜125k kk-en\nsentences for one component of our ﬁnal system.\nFinally, we trained a Russian-to-Kazkah NMT\nsystem using the bilingual data and the synthetic\nRussian to cross translate ˜6M for our second com-\nponent of the ﬁnal system.\n5.3.2 Synthetic back-translation\nA stack of another three MT systems was used\nto synthesize Kazakh from English using ˜200k\nof available English-Kazakh bilingual data for\ntraining. Starting with an English-to-Kazakh\nSMT system, ˜2M English sentences were back-\ntranslated to Kazakh. Augmenting the bilingual\ndata with the newly generated Kazakh, we trained\na NMT Kazakh-to-English system and back trans-\nlated ˜800k sentences of Kazakh news. The last\nEnglish-to-Kazakh NMT system in that stack was\ntrained using the bilingual data enlarged with the\n˜800k previously generated back-translations. It\ngenerated our en2kk back-translation of ˜2M sen-\ntences of English news.\nOur ﬁnal component was accomplished by\ntraining an English-to-Russian SMT system us-\ning ˜14.3M bilingual sentences and back translat-\ning the ˜2M sentence subselection of English news\ninto Russian.\n5.3.3 Putting it all together\nThe box labelled “NRC’s Submission” in Figure\n3 depicts how each sub-corpus was assembled\ninto the ﬁnal bilingual corpora used to train our\nmulti-source NMT submission system. Each set\nof curly braces surrounds a pair of corresponding\nKazakh and Russian sources. The ﬁrst pair repre-\nsents Kazakh and its cross-translation to Russian,\nthe second is the cross-translation of Russian-to-\nKazakh with the original Russian, and lastly we\nhave our sub-selected corpus back-translated into\nboth Kazakh and Russian.\n5.4 Results\nWe can see in Table 1 that the full multi-\nsource, multi-encoder system with two-way cross-\ntranslation (both Kazakh-to-Russian and Russian-\nto-Kazakh) is signiﬁcantly better than our other\nsystems, outperforming the pivoting system (on\nthe fourth line) by 4.2 BLEU and augmentation\nby one-way cross-translation (on the third line) by\n10.2 BLEU.\nWe believe this improvement over the other two\nmethods is due to the model being able to attend\nto additional original data, to which the other sys-\ntems do not have direct access. Both pivoting and\none-way synthetic augmentation involve “discard-\ning” genuine data, in that some of the original sen-\ntences – Kazakh sentences in the former, and Rus-\nsian sentences in the later – are never seen by the\ndownstream system, since they are only encoun-\ntered in translation. Multi-source methods allow a\nsystem to attend to the original data in both direc-\ntions, thus capturing information that would oth-\nerwise be lost in translation.\nNotable in this table is the comparative im-\nprovement of the test scores over the dev scores,\nbetween the pivoting (line 4) and multi-source\n(line 5) systems. This can be explained, we\n272\nSystem BLEU YiSi-1 YiSi-1 srl\nNEU 30.5 79.19 76.97\nrug-morfessor 27.9 77.70 75.47\ntalp-upc-2019 24.9 75.07 72.74\nNRC-CNRC 24.9 75.76 73.41\nFrank-s-MT 19.8 76.17 73.87\nTable 2: Automatic evaluation results for the top 5 con-\nstrained systems in WMT19\nSystem Ave Ave. B\nNEU 70.1 0.218\nrug-morfessor 69.7 0.189\ntalp-upc-2019 67.1 0.113\nNRC-CNRC 67.0 0.092\nFrank-s-MT 65.8 0.066\nTable 3: Human evaluation results for the top 5 con-\nstrained systems in WMT19\nthink, by a domain difference between the dev\nand test sets, where the dev set was sampled from\nthe same news commentary dataset as the train-\ning data, whereas the test set comes from actual\nnewswire text. The scores appear to show that\nthe multi-source system has managed to general-\nize better to newswire text, possibly because it has\nseen synthetic newswire text (synthesized from the\nEnglish-Russian dataset) and can respond more\nappropriately to it.9\nTables 2 and 3 compare our multi-source sys-\ntem to the other ofﬁcial submissions in the top 5 of\nthe WMT19 competition. In automatic evaluation\nby BLEU, we were tied for third place, although\nwith a slight edge when measured by YiSi-1 (Lo,\n2019); in human evaluation, we were in a statisti-\ncal tie for second place. Notably, our multi-source\nsystem was the top non-ensemble pure NMT sys-\ntem, with other higher-scoring systems either be-\ning ensembles or SMT/NMT hybrids.\n6 Conclusion and future work\nWe present the NRC submission to the WMT19\nKazakh-English news translation shared task. Our\nsubmitted system is a multi-source, multi-encoder\nneural machine translation system that takes Rus-\nsian as the second source in the system. The ad-\n9Note that, although we did perform additional ﬁltering\non the training data of the multi-source system, we do not\nbelieve this is the cause of the better performance on the test\ncompared to the pivoting system. In later tests, we found the\npivoting system to be relatively insensitive to this ﬁltering\nprocess, giving similar BLEU on both dev and test.\nvantages of using the multi-source NMT archi-\ntecture are that it incorporates additional informa-\ntion obtained from 1) the Russian-English training\ndata cross translated into Kazakh, and 2) the Rus-\nsian cross translated from Kazakh in the Kazakh-\nRussian training data.\nThe drawback of this approach is the compar-\native complexity of the pipeline, with separate\nsystems being trained to create back-translations\nand cross-translations (including back-translations\nto train those systems themselves). This com-\nplexity was difﬁcult for a human team to manage\nwhen considered for three languages; it would be\nprohibitive (without additional automation) when\nmaking systems that involve four or more lan-\nguages. Making use of the multi-source architec-\nture itself for creating back- and cross-translations\ntogether, and sharing encoders and decoders be-\ntween systems that share languages, would con-\nsiderably lessen the the complexity of the pipeline\nand the number of distinct systems that need to be\ntrained.\nIn other future work, we want to consider addi-\ntional methods of multi-source attention, as well\nas other means of creating cross-linguistic syn-\nthetic data beyond machine translation, for lower-\nresource language pairs that do not have substan-\ntial parallel data but may be, for example, closely\nrelated.\nReferences\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Matthias Huck,\nPhilipp Koehn, Christof Monz, Mathias M¨uller, and\nMatt Post. 2019. Findings of the 2019 conference on\nmachine translation (wmt19). In Proceedings of the\nFourth Conference on Machine Translation, Volume\n2: Shared Task Papers, Florence, Italy. Association\nfor Computational Linguistics.\nPeter F. Brown, Stephen Della Pietra, Vincent J. Della\nPietra, and Robert L. Mercer. 1993. The mathemat-\nics of statistical machine translation: Parameter esti-\nmation. Computational Linguistics, 19(2):263–311.\nColin Cherry. 2013. Improved reordering for phrase-\nbased translation using sparse features. In Human\nLanguage Technologies: Conference of the North\nAmerican Chapter of the Association of Computa-\ntional Linguistics, Proceedings, June 9-14, 2013,\nWestin Peachtree Plaza Hotel, Atlanta, Georgia,\nUSA, pages 22–31. The Association for Computa-\ntional Linguistics.\nColin Cherry and George F. Foster. 2012. Batch tun-\ning strategies for statistical machine translation. In\n273\nHuman Language Technologies: Conference of the\nNorth American Chapter of the Association of Com-\nputational Linguistics, Proceedings, June 3-8, 2012,\nMontr´eal, Canada, pages 427–436. The Association\nfor Computational Linguistics.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard M. Schwartz, and John Makhoul.\n2014. Fast and robust neural network joint models\nfor statistical machine translation. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics, ACL 2014, June 22-27,\n2014, Baltimore, MD, USA, Volume 1: Long Pa-\npers, pages 1370–1380. The Association for Com-\nputer Linguistics.\nKenneth Heaﬁeld. 2011. KenLM: faster and smaller\nlanguage model queries. In Proceedings of the\nEMNLP 2011 Sixth Workshop on Statistical Ma-\nchine Translation, pages 187–197, Edinburgh, Scot-\nland, United Kingdom.\nFelix Hieber, Tobias Domhan, Michael Denkowski,\nDavid Vilar, Artem Sokolov, Ann Clifton, and Matt\nPost. 2017. Sockeye: A toolkit for neural machine\ntranslation. CoRR, abs/1712.05690.\nMark Hopkins and Jonathan May. 2011. Tuning as\nranking. In Proceedings of the 2011 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2011, 27-31 July 2011, John McIntyre\nConference Centre, Edinburgh, UK, A meeting of\nSIGDAT, a Special Interest Group of the ACL, pages\n1352–1362. ACL.\nLiang Huang and David Chiang. 2007. Forest rescor-\ning: Faster decoding with integrated language mod-\nels. In ACL 2007, Proceedings of the 45th Annual\nMeeting of the Association for Computational Lin-\nguistics, June 23-30, 2007, Prague, Czech Republic.\nThe Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. Cite\narxiv:1412.6980Comment: Published as a confer-\nence paper at the 3rd International Conference for\nLearning Representations, San Diego, 2015.\nPhilipp Koehn, Franz Josef Och, and Daniel Marcu.\n2003. Statistical phrase-based translation. In Hu-\nman Language Technology Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics, HLT-NAACL 2003, Edmonton,\nCanada, May 27 - June 1, 2003. The Association for\nComputational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. CoRR, abs/1804.10959.\nSamuel Larkin, Boxing Chen, George Foster, Ulrich\nGermann, Eric Joanis, Howard Johnson, and Roland\nKuhn. 2010. Lessons from nrc’s portage system at\nwmt 2010. In Proceedings of the Joint Fifth Work-\nshop on Statistical Machine Translation and Met-\nricsMATR, WMT ’10, pages 127–132, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nJindˇrich Libovick´y and Jindˇrich Helcl. 2017. Attention\nstrategies for multi-source sequence-to-sequence\nlearning. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), pages 196–202, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nJindˇrich Libovick ´y, Jind ˇrich Helcl, and David\nMareˇcek. 2018. Input combination strategies for\nmulti-source transformer decoder. In Proceedings\nof the Third Conference on Machine Translation:\nResearch Papers, pages 253–260, Belgium, Brus-\nsels. Association for Computational Linguistics.\nChi-kiu Lo. 2019. YiSi - A uniﬁed semantic MT\nquality evaluation and estimation metric for lan-\nguages with different levels of available resources.\nIn Proceedings of the Fourth Conference on Ma-\nchine Translation, Volume 2: Shared Task Papers,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nChi-kiu Lo, Michel Simard, Darlene Stewart, Samuel\nLarkin, Cyril Goutte, and Patrick Littell. 2018. Ac-\ncurate semantic textual similarity for cleaning noisy\nparallel corpora using semantic machine translation\nevaluation metric: The NRC supervised submissions\nto the parallel corpus ﬁltering task. In Proceedings\nof the Third Conference on Machine Translation:\nShared Task Papers, pages 908–916, Belgium, Brus-\nsels. Association for Computational Linguistics.\nYuta Nishimura, Katsuhito Sudoh, Graham Neubig,\nand Satoshi Nakamura. 2018. Multi-source neural\nmachine translation with data augmentation. CoRR,\nabs/1810.06826.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAndreas Stolcke. 2002. SRILM - an extensible\nlanguage modeling toolkit. In 7th International\nConference on Spoken Language Processing, IC-\nSLP2002 - INTERSPEECH 2002, Denver, Col-\norado, USA, September 16-20, 2002. ISCA.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale\nneural language models improves translation. In\nProceedings of the 2013 Conference on Empirical\n274\nMethods in Natural Language Processing, EMNLP\n2013, 18-21 October 2013, Grand Hyatt Seattle,\nSeattle, Washington, USA, A meeting of SIGDAT,\na Special Interest Group of the ACL, pages 1387–\n1392. ACL.\nBarret Zoph and Kevin Knight. 2016. Multi-source\nneural translation. In Proceedings of NAACL-HLT,\npages 30–34.",
  "topic": "Kazakh",
  "concepts": [
    {
      "name": "Kazakh",
      "score": 0.9600532054901123
    },
    {
      "name": "Machine translation",
      "score": 0.8916951417922974
    },
    {
      "name": "Computer science",
      "score": 0.7247053384780884
    },
    {
      "name": "Transformer",
      "score": 0.7046492099761963
    },
    {
      "name": "Sentence",
      "score": 0.6233648061752319
    },
    {
      "name": "Natural language processing",
      "score": 0.5943475365638733
    },
    {
      "name": "Example-based machine translation",
      "score": 0.5699073672294617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5567606091499329
    },
    {
      "name": "Linguistics",
      "score": 0.30109867453575134
    },
    {
      "name": "Engineering",
      "score": 0.15046077966690063
    },
    {
      "name": "Voltage",
      "score": 0.06050330400466919
    },
    {
      "name": "Electrical engineering",
      "score": 0.04725813865661621
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210159778",
      "name": "National Research Council Canada",
      "country": "CA"
    }
  ]
}