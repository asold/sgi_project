{
  "title": "Is it Time to Replace CNNs with Transformers for Medical Images?",
  "url": "https://openalex.org/W3193873731",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222997821",
      "name": "Matsoukas, Christos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222997822",
      "name": "Haslum, Johan Fredin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2625519285",
      "name": "Söderberg, Magnus",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743841407",
      "name": "Smith, Kevin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3102785203",
    "https://openalex.org/W3028443516",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2964416181",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2963946669",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2203224402",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3181327235",
    "https://openalex.org/W3120430728",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2970854840",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3136635488",
    "https://openalex.org/W3156621598"
  ],
  "abstract": "Convolutional Neural Networks (CNNs) have reigned for a decade as the de facto approach to automated medical image diagnosis. Recently, vision transformers (ViTs) have appeared as a competitive alternative to CNNs, yielding similar levels of performance while possessing several interesting properties that could prove beneficial for medical imaging tasks. In this work, we explore whether it is time to move to transformer-based models or if we should keep working with CNNs - can we trivially switch to transformers? If so, what are the advantages and drawbacks of switching to ViTs for medical image diagnosis? We consider these questions in a series of experiments on three mainstream medical image datasets. Our findings show that, while CNNs perform better when trained from scratch, off-the-shelf vision transformers using default hyperparameters are on par with CNNs when pretrained on ImageNet, and outperform their CNN counterparts when pretrained using self-supervision.",
  "full_text": "Is it Time to Replace CNNs with Transformers for Medical Images?\nChristos Matsoukas 1,2,3 *\nJohan Fredin Haslum 1,2,3 Magnus S¨oderberg 3 Kevin Smith 1,2\n1 KTH Royal Institute of Technology, Stockholm, Sweden\n2 Science for Life Laboratory, Stockholm, Sweden\n3 AstraZeneca, Gothenburg, Sweden\nAbstract\nConvolutional Neural Networks (CNNs) have reigned for\na decade as the de facto approach to automated medical im-\nage diagnosis. Recently, vision transformers (ViTs) have\nappeared as a competitive alternative to CNNs, yielding\nsimilar levels of performance while possessing several in-\nteresting properties that could prove beneﬁcial for medi-\ncal imaging tasks. In this work, we explore whether it is\ntime to move to transformer-based models or if we should\nkeep working with CNNs – can we trivially switch to trans-\nformers? If so, what are the advantages and drawbacks of\nswitching to ViTs for medical image diagnosis? We consider\nthese questions in a series of experiments on three main-\nstream medical image datasets. Our ﬁndings show that,\nwhile CNNs perform better when trained from scratch, off-\nthe-shelf vision transformers using default hyperparameters\nare on par with CNNs when pretrained on ImageNet, and\noutperform their CNN counterparts when pretrained using\nself-supervision.\n1. Introduction\nVision transformers have gained increased popularity\nfor image recognition tasks recently, signalling a transi-\ntion from convolution-based feature extractors (CNNs) to\nattention-based models (ViTs). Following the success of\nDosovitskiy et al. [11], numerous approaches for adapting\ntransformers to vision tasks have been suggested [17]. In\nthe natural image domain, transformers have been shown\nto outperform CNNs on standard vision tasks such as IMA-\nGENET classiﬁcation, [11] as well as in object detection [3]\nand semantic segmentation [21]. The attention mechanism\ncentral to transformers offers several key advantages over\n*Corresponding author: Christos Matsoukas <matsou@kth.se>\nOriginally published at the ICCV 2021 Workshop on Computer Vision for\nAutomated Medical Diagnosis (CVAMD).\nconvolutions: (1) it captures long-range relationships, (2)\nit has the capacity for adaptive modeling via dynamically\ncomputed self-attention weights that capture relationships\nbetween tokens, (3) it provides a type of built-in saliency\nwhich gives insight as to what the model focused on [4].\nYet, evidence suggests that vision transformers require\nvery large datasets to outperform CNNs – in [11], the ben-\neﬁts of ViT only became evident when Google’s private\n300 million image dataset, JFT-300M, was used for pre-\ntraining. Their reliance on data of this scale is a barrier to\nthe widespread application of transformers. This problem\nis particularly acute in the medical imaging domain, where\ndatasets are smaller and are often accompanied by less reli-\nable labels.\nCNNs, like ViTs, suffer worse performance when data is\nscarce. The standard solution is to employ transfer learn-\ning: typically, a model is pretrained on a larger dataset such\nas I MAGE NET [10] and then ﬁne-tuned for speciﬁc tasks\nusing smaller, specialized, datasets. CNNs pre-trained on\nIMAGE NET typically outperform those trained from scratch\nin the medical domain, both in terms of ﬁnal performance\nand reduced training time [20].\nSelf-supervision is a learning approach to deal with un-\nlabeled data that has recently gained much attention. It has\nbeen shown that self-supervised pretraining of CNNs in the\ntarget domain before ﬁne-tuning can increase performance\n[1]. Initialization from I MAGE NET helps self-supervised\nCNNs converge faster, and usually with better predictive\nperformance [1].\nThese techniques to deal with the lack of data in the med-\nical image domain have proven effective for CNNs, but it\nremains unclear whether vision transformers beneﬁt sim-\nilarly. Some studies suggest that pre-training CNNs for\nmedical image analysis using I MAGE NET does not rely on\nfeature reuse –following conventional wisdom– but, rather\ndue to better initialization and weight scaling [20]. This\ncalls into question whether transformers beneﬁt from these\ntechniques. If they do, there is little to prevent ViTs from\nbecoming the dominant architecture for medical images.\narXiv:2108.09038v1  [cs.CV]  20 Aug 2021\nIn this work, we explore whether ViTs can easily replace\nCNNs for medical imaging tasks, and if there is an advan-\ntage of doing so. We consider the use-case of a typical\npractitioner, equipped with a limited computational budget\nand access to conventional medical datasets, with an eye\ntowards “plug-and-play” solutions. To this end, we con-\nduct experiments on three mainstream publicly available\ndatasets. Through these experiments we show that:\n• ViTs pretrained on IMAGE NET perform comparably to\nCNNs when data is limited.\n• Transfer learning favours ViTs when applying standard\ntraining protocols and settings.\n• ViTs outperform their CNN counterparts when self-\nsupervised pre-training is followed by supervised ﬁne-\ntuning.\nThese ﬁndings suggest that medical image analysis can\nseamlessly transition from CNNs to ViTs, while at\nthe same time gaining improved explainability prop-\nerties. To promote transparency and reproducibil-\nity, we share our open-source code, available at\nhttps://github.com/ChrisMats/medical transformers.\n2. Related Work\nThe use of vision transformers in the natural imaging do-\nmain has exploded recently, with applications ranging from\nclassiﬁcation [11], to object detection [3] and segmenta-\ntion [21]. In medical imaging, however, the use of ViTs\nhas been limited – primarily focused on focused on seg-\nmentation [6, 19]. Only a handful of studies have tack-\nled other tasks such as 3-D image registration [5] and de-\ntection [12]. Notably, none of these works consider pure,\noff-the-shelf, vision transformers – all propose custom ar-\nchitectures combining transformer/attention modules with\nconvolutional feature extractors.\nAlthough it is well-known that CNNs usually beneﬁt\nfrom transfer learning to medical imaging domains, the\nsource of these beneﬁts is disputed. The conventional wis-\ndom that feature re-use contributes to better performance\nwas questioned by Raghu et al. [20], who rather attribute\nthe improved performance to good initialization and weight\nstatistics. Regardless of the reason, the question of whether\nViTs beneﬁt from transfer learning to medical domains is\nyet to be explored.\nRecent advances in self-supervised learning have dra-\nmatically improved performance of label-free learning.\nState-of-the-art methods such as DINO [4] and BYOL [13]\nhave reached performance on par with supervised learning\non IMAGE NET and other standard benchmarks. While these\ntop-performing methods have not yet been proven for med-\nical imaging, Azizi et al. [1] adopted SimCLR [7], an ear-\nlier self-supervised contrastive learning method, to pretrain\nCNNs. This yielded state-of-the-art results for predictions\non chest X-rays and skin lesions. However, it has yet to\nbe shown how self-supervised learning combined with ViTs\nperforms in medical imaging, and whether this combination\noutperforms its CNN counterparts.\n3. Methods\nThe main question we investigate is whether ViTs can\nbe used as a plug-and-play alternative to CNNs for medi-\ncal diagnostic tasks. To that end, we conducted a series of\nexperiments to compare vanilla ViTs and CNNs under sim-\nilar conditions, keeping hyperparameter tuning to a mini-\nmum. To ensure a fair and interpretable comparison, we\nselected RESNET50 [15] as the representative CNN model,\nand DEIT-S with 16ˆ16 tokens [23] as the ViT. These mod-\nels were chosen because they are comparable in the number\nof parameters, memory requirements, and compute.\nAs mentioned above, CNNs rely on initialization strate-\ngies to improve performance when data is less abundant, as\nis the case for medical images. The standard approach is\nto use transfer learning – initialize the model with weights\npretrained on I MAGE NET and ﬁne-tune on the target do-\nmain. More recently, self-supervised pretraining has be-\ncome a popular way to initialize neural networks [13, 4, 1].\nAccordingly, we consider three initialization strategies:\n(1) randomly initialized weights, (2) transfer learning us-\ning supervised I MAGE NET pretrained weights, (3) self-\nsupervised pretraining on the target dataset, after initializa-\ntion as in (2). We apply these strategies on three standard\nmedical imaging datasets chosen to cover a diverse set of\ntarget domains:\n• APTOS 2019 – In this dataset, the task is classiﬁca-\ntion of diabetic retinopathy images into 5 categories\nof disease severity [16]. APTOS 2019 contains 3,662\nhigh-resolution retinal images.\n• ISIC 2019 – Here, the task is to classify 25,333 der-\nmoscopic images among nine different diagnostic cat-\negories of skin lesions [24, 8, 9].\n• CBIS-DDSM – This dataset contains 10,239 mam-\nmography images and the task is to detect the presence\nof masses in the mammograms.\nDatasets were divided into train/test/validation splits\n(80/10/10), with the exception of APTOS, which was di-\nvided 70/15/15 due to its small size. All supervised training\nuses an A DAM optimizer [18] with a base learning rate of\n10´4 with a warm-up period of 1,000 iterations. When the\nvalidation metrics saturate, the learning rate is dropped by a\nfactor of 10 until it reaches its ﬁnal value of 10´6. All im-\nages are resized to 256 ˆ 256 and standard augmentations\nInitialization Model APTOS2019 , κÒ ISIC2019, Recall Ò DDSM, ROC-AUC Ò\nRandom ResNet50 0.849 ˘0.022 0.662 ˘0.018 0.917 ˘0.005\nDeiT-S 0.687 ˘0.017 0.579 ˘0.028 0.908 ˘0.015\nImageNet (supervised) ResNet50 0.893 ˘0.004 0.810 ˘0.008 0.953 ˘0.008\nDeiT-S 0.896 ˘0.005 0.844 ˘0.021 0.947 ˘0.011\nImageNet (supervised) +\nSelf-supervised with DINO [4]\nResNet50 0.894 ˘0.008 0.833 ˘0.007 0.955 ˘0.002\nDeiT-S 0.896 ˘0.010 0.853 ˘0.009 0.956 ˘0.002\nTable 1: Comparison of vanilla CNNs vs. ViTs with different initialization strategies on medical imaging tasks.For each task\nwe report the median ( ˘ standard deviation) over 5 repetitions using the metrics that are commonly used in the literature.\nFor APTOS2019 we report quadratic Cohen Kappa, for ISIC2019 we report recall (which is semantically equivalent to the\nbalanced multi-class accuracy) , and for CBIS-DDSM we report ROC-AUC.\nwere applied 1 We repeat each experiment ﬁve times, and\nselect the checkpoint with highest validation score of each\nrun. We use the above settings unless otherwise speciﬁed.\n4. Experiments\nAre randomly initialized transformers useful? We be-\ngin by comparing D EIT-S against R ESNET50 with ran-\ndomly initialized weights (Kaiming initialization [14]). For\nthese experiments, the base learning rate was set to 0.0003\nfollowing a grid search. The results in Table 1 indicate that\nin this setting, CNNs outperform ViTs by a large margin\nacross the board. These results are in line with previous ob-\nservations in the natural image domain, where ViTs trained\non limited data are outperformed by similarly-sized CNNs,\na trend that was attributed to ViT’s lack of inductive bias\n[11]. Since most medical imaging datasets are of modest\nsize, the usefulness of randomly initialized ViTs appears to\nbe limited.\nDoes pretraining transformers on I MAGE NET work in\nthe medical image domain? On medical image datasets,\nrandom initialization is rarely used in practice. The standard\nprocedure is to train CNNs by initializing the network with\nIMAGE NET pretrained weights, followed by ﬁne-tuning on\ndata from the target domain. Here, we investigate if this\napproach can be effectively applied to vanilla ViTs. To test\nthis, we initialize all models with weights that have been\npre-trained on I MAGE NET in a fully-supervised fashion.\nWe then ﬁne-tune using the procedure described above. The\nresults in Table 1 show that both CNNs and ViTs beneﬁt sig-\nniﬁcantly from I MAGE NET initialization. In fact, ViTs ap-\npear to beneﬁt more, as they perform on par with their CNN\ncounterparts. This indicates that, when initialized with I M-\nAGE NET, CNNs can be replaced with vanilla ViTs without\ncompromising performance on medical imaging tasks with\nmodest-sized training data.\n1Training augmentations include: normalization; color jitter including\nbrightness, contrast, saturation, hue; horizontal ﬂip; vertical ﬂip; and ran-\ndom resized crops.\nDo transformers beneﬁt from self-supervision in the\nmedical image domain? Recent self-supervised learning\nschemes such as DINO [4] and BYOL [13] approach su-\npervised learning performance. Moreover, if they are used\nfor pretraining in combination with supervised ﬁne-tuning,\nthey can achieve a new state-of-the-art [4, 13]. While this\nphenomenon has been demonstrated for CNNs and ViTs in\nlarger data regimes, it is not clear whether self-supervised\npretraining of ViTs helps for medical imaging tasks, espe-\ncially on modest- and low-sized data. To test this, we adopt\nthe self-supervised learning scheme of DINO [4], which\ncan be readily applied to both CNNs and ViTs. DINO\nuses self-distillation to encourage a student and teacher net-\nwork to produce similar representations given differently-\naugmented inputs. Our self-supervised pretraining starts\nwith IMAGE NET initialization, then applies self-supervised\nlearning on the target medical domain data following the de-\nfault settings suggested by the authors of the original paper\n[4] – except for three small changes: (1) the base learning\nrate was set to 0.0001, (2) the initial weight decay is set at\n10´5 and increased to10´4 using a cosine schedule, and(3)\nwe used an EMA of 0.99. The same settings were used for\nboth CNNs and ViTs; both were pre-trained for 300 epochs\nusing a batch size of 256, followed by ﬁne-tuning as de-\nscribed in Section 3.\nThe results reported in Table 1 show that both ViTs and\nCNNs perform better with self-supervised pretraining. ViTs\nappear to outperform CNNs in this setting, albeit by a small\nmargin. Studies on natural images suggest the gap between\nViTs and CNNs will grow with more data [4].\n5. Discussion\nOur investigation compares the performance of vanilla\nCNNs and ViTs on medical image tasks under three differ-\nent initialization strategies. The results of the experiments\ncorroborate previous ﬁndings and provide new insights.\nIn medical images, as was previously reported in the nat-\nural image domain, we found that CNNs outperform ViTs\nwhen trained from scratch in a low data regime [11]. This\ntrend appears consistently across all the datasets and ﬁts\nwell with the “transformers lack inductive bias” argument.\nSurprisingly, when initialized with supervised I MA-\nGENET pretrained weights, the gap between CNN and ViT\nperformance disappears on medical tasks. The beneﬁts\nof supervised I MAGE NET pretraining on CNNs is well-\nknown, but it was unexpected that ViTs would beneﬁt so\nstrongly. To the best of our knowledge, we are the ﬁrst to\nconﬁrm that supervised I MAGE NET pretraining is as effec-\ntive for ViTs as it is for CNNs on medical imaging tasks.\nThis suggests that further improvements could be gained via\ntransfer learning from other domains more closely related to\nthe task, as is the case for CNNs [2].\nWe investigated the effect of self-supervised pre-training\non the medical image domain. Our results indicate small\nbut consistent improvements for ViTs and CNNs. While the\nbest overall performance is obtained using self-supervised\nViTs, interestingly in this low-data regime we do not yet\nsee the strong advantage for self-supervision favoring ViTs\nthat was previously reported in the natural image domain\nwith more data, e.g. in [4]\nLarge labeled medical image datasets are rare due to the\ncost of expert annotation, but it may be possible to gather\nlarge amounts of unlabeled images. This suggests a tanta-\nlizing opportunity to apply self-supervision on large medi-\ncal image datasets, where only a small fraction are labeled.\nTo summarize our ﬁndings, for the medical image do-\nmain:\n• As expected, ViTs are worse than CNNs in the low\ndata regime if one simply trains from scratch.\n• Transfer learning bridges the performance gap be-\ntween CNNs and ViTs; performance is similar.\n• The best performance is obtained with self-supervised\npre-training + ﬁne-tuning, where ViTs enjoy a small\nadvantage over comparable CNNs.\nInterpretability. It appears that ViTs can replace CNNs\nfor medical image tasks – are there any other reasons for\nchoosing ViTs over CNNs? One should consider the added\nbeneﬁts of visualizing transformer attention maps. Built-in\nto the self-attention mechanism of transformers is an atten-\ntion map that provides, for free, new insight into how the\nmodel makes decisions. CNNs do not naturally lend them-\nselves well to visualizing saliency. Popular CNN explain-\nability methods such as class activation maps (CAM) [25]\nand Grad-CAM [22] provide coarse visualizations because\nof pooled layers. Transformer tokens give a ﬁner picture of\nattention, and the self-attention maps explicitly model in-\nteractions between every region in the image, in contrast to\nthe limited receptive ﬁeld of the CNN. While the difference\nin quality of explainability has yet to be quantiﬁed, many\nhave noted the qualitative improvements in interpretability\nafforded by transformer attention [4].\nISIC 2019 APTOS 2019 CBIS-DDSM\nFigure 1: Comparison of saliency maps for a R ESNET50\n(second row) and D EIT-S (third row) in three of the\ndatasets. Each column contains the original, a visualisation\nof the ResNet50’s Grad-CAM [22] saliency and a visualisa-\ntion of the DEIT-S’s attention map.\nIn Figure 1 we show examples from each of the datasets,\nalong with Grad-CAM visualizations of the R ESNET-50\nand the top-50% self-attention of 16 ˆ 16 DEIT-S CLS to-\nken heads. Notice how the self-attention of ViTs provide a\nclear, localized picture of the attention, e.g. attention at the\nboundary of the skin lesion in ISIC, on hemorrhages and\nexudates in APTOS, and at the dense region of the breast\nin CBIS-DDSM. This granularity of attention is difﬁcult to\nachieve with CNNs.\n6. Conclusion\nFinally, to answer the question posed in the title: vanilla\ntransformers can reliably replace CNNs on medical im-\nage tasks with little effort . More precisely, ViTs can reach\nthe same level of performance as CNNs in small medical\ndatasets, but require transfer learning in order to do so.\nHowever, using ImageNet pretrained weights is the stan-\ndard approach for CNNs as well, so the switch to ViTs is\ntrivial. Furthermore, ViTs can outperform CNNs using SSL\npre-training when working with limited number of samples,\nbut only marginally. As the number of samples grows, the\nmargin between ViT and CNN is expected to grow as well.\nThis equal or better performance comes with the additional\nbeneﬁt of built in high-resolution saliency maps that can be\nused to better understand the model’s decisions.\nAcknowledgements. This work was supported by the\nWallenberg Autonomous Systems Program (W ASP), and\nthe Swedish Research Council (VR) 2017-04609. We thank\nMoein Sorkhei and Emir Konuk for the thoughtful discus-\nsions.\nReferences\n[1] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary\nBeaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan\nKarthikesalingam, Simon Kornblith, Ting Chen, et al. Big\nself-supervised models advance medical image classiﬁca-\ntion. arXiv preprint arXiv:2101.05224, 2021. 1, 2\n[2] Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan,\nAtsuto Maki, and Stefan Carlsson. Factors of transferability\nfor a generic convnet representation. IEEE transactions on\npattern analysis and machine intelligence, 38(9):1790–1802,\n2016. 4\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020. 1,\n2\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. arXiv\npreprint arXiv:2104.14294, 2021. 1, 2, 3, 4\n[5] Junyu Chen, Yufan He, Eric C Frey, Ye Li, and Yong Du. Vit-\nv-net: Vision transformer for unsupervised volumetric med-\nical image registration. arXiv preprint arXiv:2104.06468 ,\n2021. 2\n[6] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan\nAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.\nTransunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306,\n2021. 2\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 2\n[8] Noel CF Codella, David Gutman, M Emre Celebi, Brian\nHelba, Michael A Marchetti, Stephen W Dusza, Aadi\nKalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kit-\ntler, et al. Skin lesion analysis toward melanoma detection:\nA challenge at the 2017 international symposium on biomed-\nical imaging (isbi), hosted by the international skin imaging\ncollaboration (isic). In 2018 IEEE 15th international sym-\nposium on biomedical imaging (ISBI 2018), pages 168–172.\nIEEE, 2018. 2\n[9] Marc Combalia, Noel CF Codella, Veronica Rotemberg,\nBrian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Car-\nrera, Alicia Barreiro, Allan C Halpern, Susana Puig, et al.\nBcn20000: Dermoscopic lesions in the wild. arXiv preprint\narXiv:1908.02288, 2019. 2\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 1\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. InInternational Con-\nference on Learning Representations, 2020. 1, 2, 3\n[12] Linh T Duong, Nhi H Le, Toan B Tran, Vuong M Ngo, and\nPhuong T Nguyen. Detection of tuberculosis from chest\nx-ray images: Boosting the performance with vision trans-\nformer and transfer learning. Expert Systems with Applica-\ntions, page 115519, 2021. 2\n[13] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020. 2, 3\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectiﬁers: Surpassing human-level perfor-\nmance on imagenet classiﬁcation. In Proceedings of the\nIEEE international conference on computer vision , pages\n1026–1034, 2015. 3\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2016. 2\n[16] Kaggle. Aptos 2019 blindness detection, 2019. 2\n[17] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint\narXiv:2101.01169, 2021. 1\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 2\n[19] Daniel H Pak, Andr ´es Caballero, Wei Sun, and James S\nDuncan. Efﬁcient aortic valve multilabel segmentation us-\ning a spatial transformer network. In 2020 IEEE 17th Inter-\nnational Symposium on Biomedical Imaging (ISBI) , pages\n1738–1742. IEEE, 2020. 2\n[20] Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy\nBengio. Transfusion: Understanding transfer learning for\nmedical imaging. In Advances in Neural Information Pro-\ncessing Systems, pages 3342–3352, 2019. 1, 2\n[21] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.\nVision transformers for dense prediction. arXiv preprint\narXiv:2103.13413, 2021. 1, 2\n[22] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 618–626,\n2017. 4\n[23] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347–10357. PMLR, 2021. 2\n[24] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The\nham10000 dataset, a large collection of multi-source der-\nmatoscopic images of common pigmented skin lesions. Sci-\nentiﬁc data, 5(1):1–9, 2018. 2\n[25] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 4",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.7592411041259766
    },
    {
      "name": "Computer science",
      "score": 0.744165301322937
    },
    {
      "name": "Transformer",
      "score": 0.7296146154403687
    },
    {
      "name": "Hyperparameter",
      "score": 0.6796460747718811
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6083998084068298
    },
    {
      "name": "De facto",
      "score": 0.5828831791877747
    },
    {
      "name": "Medical imaging",
      "score": 0.4479120969772339
    },
    {
      "name": "Machine learning",
      "score": 0.41454166173934937
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37245994806289673
    },
    {
      "name": "Computer vision",
      "score": 0.3343130648136139
    },
    {
      "name": "Engineering",
      "score": 0.10479038953781128
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86987016",
      "name": "KTH Royal Institute of Technology",
      "country": "SE"
    }
  ]
}