{
  "title": "CATs: Cost Aggregation Transformers for Visual Correspondence",
  "url": "https://openalex.org/W3210771406",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5031106030",
      "name": "Seokju Cho",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061636336",
      "name": "Sunghwan Hong",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5014123447",
      "name": "Sangryul Jeon",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A5074098034",
      "name": "Yunsung Lee",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5073320959",
      "name": "Kwanghoon Sohn",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A5085363061",
      "name": "Seungryong Kim",
      "affiliations": [
        "√âcole Polytechnique F√©d√©rale de Lausanne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2566812041",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2970350341",
    "https://openalex.org/W2798976292",
    "https://openalex.org/W2953142863",
    "https://openalex.org/W2593948489",
    "https://openalex.org/W3106982106",
    "https://openalex.org/W3035477606",
    "https://openalex.org/W3092135995",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1996140089",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964141676",
    "https://openalex.org/W2885650013",
    "https://openalex.org/W3149878926",
    "https://openalex.org/W2604881841",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2964073646",
    "https://openalex.org/W2611605760",
    "https://openalex.org/W2165949425",
    "https://openalex.org/W2982121679",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2985822417",
    "https://openalex.org/W3103187163",
    "https://openalex.org/W2464606141",
    "https://openalex.org/W3181461461",
    "https://openalex.org/W2963020784",
    "https://openalex.org/W3104100999",
    "https://openalex.org/W3099319035",
    "https://openalex.org/W2886782161",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3167674261",
    "https://openalex.org/W2930360627",
    "https://openalex.org/W3035242260",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2560474170",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2090518410",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2612584387",
    "https://openalex.org/W1968799614",
    "https://openalex.org/W3140551255",
    "https://openalex.org/W2935594133",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3095145867",
    "https://openalex.org/W2141362318",
    "https://openalex.org/W2104974755",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W3035578028",
    "https://openalex.org/W2897093986"
  ],
  "abstract": "We propose a novel cost aggregation network, called Cost Aggregation Transformers (CATs), to find dense correspondences between semantically similar images with additional challenges posed by large intra-class appearance and geometric variations. Cost aggregation is a highly important process in matching tasks, which the matching accuracy depends on the quality of its output. Compared to hand-crafted or CNN-based methods addressing the cost aggregation, in that either lacks robustness to severe deformations or inherit the limitation of CNNs that fail to discriminate incorrect matches due to limited receptive fields, CATs explore global consensus among initial correlation map with the help of some architectural designs that allow us to fully leverage self-attention mechanism. Specifically, we include appearance affinity modeling to aid the cost aggregation process in order to disambiguate the noisy initial correlation maps and propose multi-level aggregation to efficiently capture different semantics from hierarchical feature representations. We then combine with swapping self-attention technique and residual connections not only to enforce consistent matching but also to ease the learning process, which we find that these result in an apparent performance boost. We conduct experiments to demonstrate the effectiveness of the proposed model over the latest methods and provide extensive ablation studies. Project page is available at : https://sunghwanhong.github.io/CATs/.",
  "full_text": "CATs: Cost Aggregation Transformers for Visual\nCorrespondence\nSeokju Cho‚àó\nYonsei University\nSunghwan Hong‚àó\nKorea University\nSangryul Jeon\nYonsei University\nYunsung Lee\nKorea University\nKwanghoon Sohn\nYonsei University\nSeungryong Kim‚Ä†\nKorea University\nAbstract\nWe propose a novel cost aggregation network, called Cost Aggregation Transform-\ners (CATs), to Ô¨Ånd dense correspondences between semantically similar images\nwith additional challenges posed by large intra-class appearance and geometric\nvariations. Cost aggregation is a highly important process in matching tasks, which\nthe matching accuracy depends on the quality of its output. Compared to hand-\ncrafted or CNN-based methods addressing the cost aggregation, in that either lacks\nrobustness to severe deformations or inherit the limitation of CNNs that fail to\ndiscriminate incorrect matches due to limited receptive Ô¨Åelds, CATs explore global\nconsensus among initial correlation map with the help of some architectural de-\nsigns that allow us to fully leverage self-attention mechanism. SpeciÔ¨Åcally, we\ninclude appearance afÔ¨Ånity modeling to aid the cost aggregation process in order to\ndisambiguate the noisy initial correlation maps and propose multi-level aggregation\nto efÔ¨Åciently capture different semantics from hierarchical feature representations.\nWe then combine with swapping self-attention technique and residual connections\nnot only to enforce consistent matching, but also to ease the learning process, which\nwe Ô¨Ånd that these result in an apparent performance boost. We conduct experiments\nto demonstrate the effectiveness of the proposed model over the latest methods\nand provide extensive ablation studies. Code and trained models are available\nat https://sunghwanhong.github.io/CATs/.\n1 Introduction\nEstablishing dense correspondences across semantically similar images can facilitate many Computer\nVision applications, including semantic segmentation [46, 54, 36], object detection [29], and image\nediting [53, 30, 28, 25]. Unlike classical dense correspondence problems that consider visually similar\nimages taken under the geometrically constrained settings [16, 19, 50, 18], semantic correspondence\nposes additional challenges from large intra-class appearance and geometric variations caused by the\nunconstrained settings of given image pair.\nRecent approaches [ 42, 43, 45, 34, 37, 39, 31, 58, 47, 57, 51, 35] addressed these challenges by\ncarefully designing deep convolutional neural networks (CNNs)-based models analogously to the\nclassical matching pipeline [48, 41], feature extraction, cost aggregation, and Ô¨Çow estimation. Several\nworks [24, 9, 37, 39, 47, 51] focused on the feature extraction stage, as it has been proven that the more\npowerful feature representation the model learns, the more robust matching is obtained [24, 9, 51].\nHowever, solely relying on the matching similarity between features without any prior often suffers\n‚àóEqual contribution\n‚Ä†Corresponding author\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.02520v4  [cs.CV]  15 Dec 2021\nfrom the challenges due to ambiguities generated by repetitive patterns or background clutters [42,\n24, 26]. On the other hand, some methods [42, 49, 43, 23, 26, 58] focused on Ô¨Çow estimation stage\neither by designing additional CNN as an ad-hoc regressor that predicts the parameters of a single\nglobal transformation [42, 43], Ô¨Ånding conÔ¨Ådent matches from correlation maps [20, 26], or directly\nfeeding the correlation maps into the decoder to infer dense correspondences [58]. However, these\nmethods highly rely on the quality of the initial correlation maps.\nThe latest methods [45, 37, 44, 21, 31, 27, 35] have focused on the second stage, highlighting the\nimportance of cost aggregation. Since the quality of correlation maps is of prime importance, they\nproposed to reÔ¨Åne the matching scores by formulating the task as optimal transport problem [47, 31],\nre-weighting matching scores by Hough space voting for geometric consistency [37, 39], or utilizing\nhigh-dimensional 4D or 6D convolutions to Ô¨Ånd locally consistent matches [45, 44, 27, 35]. Although\nformulated variously, these methods either use hand-crafted techniques that are neither learnable nor\nrobust to severe deformations, or inherit the limitation of CNNs, e.g., limited receptive Ô¨Åelds, failing\nto discriminate incorrect matches that are locally consistent.\nIn this work, we focus on the cost aggregation stage, and propose a novel cost aggregation network to\ntackle aforementioned issues. Our network, called Cost Aggregation with Transformers (CATs), is\nbased on Transformer [61, 10], which is renowned for its global receptive Ô¨Åeld. By considering all\nthe matching scores computed between features of input images globally, our aggregation networks\nexplore global consensus and thus reÔ¨Åne the ambiguous or noisy matching scores effectively.\nSpeciÔ¨Åcally, based on the observation that desired correspondence should be aligned at discontinuities\nwith appearance of images, we concatenate an appearance embedding with the correlation map, which\nhelps to disambiguate the correlation map within the Transformer. To beneÔ¨Åt from hierarchical feature\nrepresentations, following [26, 39, 58], we use a stack of correlation maps constructed from multi-\nlevel features, and propose to effectively aggregate the scores across the multi-level correlation maps.\nFurthermore, we consider bidirectional nature of correlation map, and leverage the correlation map\nfrom both directions, obtaining reciprocal scores by swapping the pair of dimensions of correlation\nmap in order to allow global consensus in both perspective. In addition to all these combined, we\nprovide residual connections around aggregation networks in order to ease the learning process.\nWe demonstrate our method on several benchmarks [38, 11, 12]. Experimental results on various\nbenchmarks prove the effectiveness of the proposed model over the latest methods for semantic\ncorrespondence. We also provide an extensive ablation study to validate and analyze components in\nCATs.\n2 Related Work\nSemantic Correspondence. Methods for semantic correspondence generally follow the classical\nmatching pipeline [48, 41], including feature extraction, cost aggregation, and Ô¨Çow estimation. Most\nearly efforts [7, 30, 11] leveraged the hand-crafted features which are inherently limited in capturing\nhigh-level semantics. Though using deep CNN-based features [5, 24, 42, 43, 23, 49, 26] has become\nincreasingly popular thanks to their invariance to deformations, without a means to reÔ¨Åne the matching\nscores independently computed between the features, the performance would be rather limited.\nTo alleviate this, several methods focused on Ô¨Çow estimation stage. Rocco et al. [42, 43] proposed an\nend-to-end network to predict global transformation parameters from the matching scores, and their\nsuccess inspired many variants [49, 23, 25]. RTNs [23] obtain semantic correspondences through\nan iterative process of estimating spatial transformations. DGC-Net [34], Semantic-GLU-Net [58]\nand DMP [15] utilize a CNN-based decoder to directly Ô¨Ånd correspondence Ô¨Åelds. PDC-Net [ 59]\nproposed a Ô¨Çexible probabilistic model that jointly learns the Ô¨Çow estimation and its uncertainty.\nArguably, directly regressing correspondences from the initial matching scores highly relies on the\nquality of them.\nRecent numerous methods [45, 37, 39, 31, 47, 51, 35] thus have focused on cost aggregation stage to\nreÔ¨Åne the initial matching scores. Among hand-crafted methods, SCOT [ 31] formulates semantic\ncorrespondence as an optimal transport problem and attempts to solve two issues, namely many to\none matching and background matching. HPF [37] Ô¨Årst computes appearance matching conÔ¨Ådence\nusing hyperpixel features and then uses Regularized Hough Matching (RHM) algorithm for cost\naggregation to enforce geometric consistency. DHPF [39], that replaces feature selection algorithm\n2\nProjection\nProjection\nùêº!FeatureBackbone\nùêº\"FeatureBackbone\nTransformer Aggregator\nTransformer Aggregator\nAvg. & Soft-argmax\nSource Appearance Affinity\nTarget Appearance Affinity\nResidual Connection\nShared\nTranspose\nFeature ExtractionCost AggregationFlow Estimation\n‚Ñéùë§‚Ñéùë§\nùêøùëù ùêπ#$%&\n‚Ñéùë§‚Ñéùë§ùêø\nùëù\nTranspose\nùê∑!\nùê∑\"\nCorrelation\nFigure 1: Overall network architecture. Our networks consist of feature extraction, cost aggrega-\ntion, and Ô¨Çow estimation modules. We Ô¨Årst extract multi-level dense features and construct a stack of\ncorrelation maps. We then concatenate with embedded features and feed into the Transformer-based\ncost aggregator to obtain a reÔ¨Åned correlation map. The Ô¨Çow is then inferred from the reÔ¨Åned map.\nof HPF [37] with trainable networks, also uses RHM. However, these hand-crafted techniques for\nreÔ¨Åning the matching scores are neither learnable nor robust to severe deformations. As learning-\nbased approaches, NC-Net [45] utilizes 4D convolution to achieve local neighborhood consensus\nby Ô¨Ånding locally consistent matches, and its variants [ 44, 27] proposed more efÔ¨Åcient methods.\nGOCor [57] proposed aggregation module that directly improves the correlation maps. GSF [ 21]\nformulated pruning module to suppress false positives of correspondences in order to reÔ¨Åne the\ninitial correlation maps. CHM [35] goes one step further, proposing a learnable geometric matching\nalgorithm which utilizes 6D convolution. However, they are all limited in the sense that they inherit\nlimitation of CNN-based architectures, which is local receptive Ô¨Åelds.\nTransformers in Vision. Transformer [61], the de factostandard for Natural Language Processing\n(NLP) tasks, has recently imposed signiÔ¨Åcant impact on various tasks in Computer Vision Ô¨Åelds such\nas image classiÔ¨Åcation [10, 55], object detection [3, 62], tracking and matching [52, 51]. ViT [10], the\nÔ¨Årst work to propose an end-to-end Transformer-based architecture for the image classiÔ¨Åcation task,\nsuccessfully extended the receptive Ô¨Åeld, owing to its self-attention nature that can capture global\nrelationship between features. For visual correspondence, LoFTR [51] uses cross and self-attention\nmodule to reÔ¨Åne the feature maps conditioned on both input images, and formulate the hand-crafted\naggregation layer with dual-softmax [45, 60] and optimal transport [47] to infer correspondences.\nCOTR [22] takes coordinates as an input and addresses dense correspondence task without the use of\ncorrelation map. Unlike these, for the Ô¨Årst time, we propose a Transformer-based cost aggregation\nmodule.\n3 Methodology\n3.1 Motivation and Overview\nLet us denote a pair of images, i.e., source and target, as Is and It, which represent semantically\nsimilar images, and features extracted from Is and It as Ds and Dt, respectively. Here, our goal is\nto establish a dense correspondence Ô¨Åeld F(i) between two images that is deÔ¨Åned for each pixel i,\nwhich warps It towards Is.\nEstimating the correspondence with sole reliance on matching similarities betweenDs and Dt is often\nchallenged by the ambiguous matches due to the repetitive patterns or background clutters [42, 24, 26].\nTo address this, numerous methods proposed cost aggregation techniques that focus on reÔ¨Åning the\ninitial matching similarities either by formulating the task as optimal transport problem [47, 31], using\nregularized Hough matching to re-weight the costs [37, 39], or 4D or 6D convolutions [45, 27, 44, 35].\nHowever, these methods either use hand-crafted techniques that are weak to severe deformations, or\nfail to discriminate incorrect matches due to limited receptive Ô¨Åelds.\n3\n(a) Source\n (b) Target\n (c) Raw Corr.\n (d) Self-Attn.\n (e) ReÔ¨Åned Corr.\n (f) Ground-truth\nFigure 2: Visualization of correlation map and self-attention: (a) source image, (b) target image,\n(c) raw correlation map, (d) self-attention, (e) reÔ¨Åned correlation map, and (f) ground-truth, which\nare bilinearly upsampled. The visualization proves that CATs successfully aggregates the costs by\nintegrating the surrounding information of the query, represented as green circle in the source.\nTo overcome these, we present Transformer-based cost aggregation networks that effectively integrate\ninformation present in all pairwise matching costs, dubbed CATs, as illustrated in Fig. 1. As done\nwidely in other works [42, 45, 50, 34, 37], we follow the common practice for feature extraction and\ncost computation. In the following, we Ô¨Årst explain feature extraction and cost computation, and then\ndescribe several critical design choices we made for effective aggregation of the matching costs.\n3.2 Feature Extraction and Cost Computation\nTo extract dense feature maps from images, we follow [26, 37, 39] that use multi-level features for\nconstruction of correlation maps. We use CNNs that produce a sequence of Lfeature maps, and Dl\nrepresents a feature map at l-th level. As done in [37], we use different combination of multi-level\nfeatures depending on the dataset trained on, e.g., PF-PASCAL [ 12] or SPair-71k [ 38]. Given a\nsequence of feature maps, we resize all the selected feature maps to Rh√ów√óc, with height h, width w,\nand cchannels. The resized features then undergo l-2 normalization.\nGiven resized dense features Ds and Dt, we compute a correlation map C‚àà Rhw√óhw using the\ninner product between features: C(i,j) =Dt(i) ¬∑Ds(j) with points iand jin the target and source\nfeatures, respectively. In this way, all pairwise feature matches are computed and stored. However,\nraw matching scores contain numerous ambiguous matching points as exempliÔ¨Åed in Fig. 2, which\nresults inaccurate correspondences. To remedy this, we propose cost aggregation networks in the\nfollowing that aim to reÔ¨Åne the ambiguous or noisy matching scores.\n3.3 Transformer Aggregator\nRenowned for its global receptive Ô¨Åelds, one of the key elements of Transformer [ 61] is the self-\nattention mechanism, which enables Ô¨Ånding the correlated input tokens by Ô¨Årst feeding into scaled\ndot product attention function, normalizing with Layer Normalization (LN) [ 1], and passing the\nnormalized values to a MLP. Several works [10, 3, 62, 51] have shown that given images or features\nas input, Transformers [61] integrate the global information in a Ô¨Çexible manner by learning to Ô¨Ånd\nthe attention scores for all pairs of tokens.\nIn this paper, we leverage the Transformers to integrate the matching scores to discover global\nconsensus by considering global context information. SpeciÔ¨Åcally, we obtain a reÔ¨Åned cost C‚Ä≤by\nfeeding the raw cost Cto the Transformer T, consisting of self-attention, LN, and MLP modules:\nC‚Ä≤= T(C+ Epos), (1)\nwhere Epos denotes positional embedding. The standard Transformer receives as input a 1D sequence\nof token embeddings. In our context, we reshape the correlation map Cinto a sequence of vectors\nC(k) ‚ààR1√óhw for k ‚àà{1,...,hw }. We visualize the reÔ¨Åned correlation map with self-attention\nin Fig. 2, where the ambiguities are signiÔ¨Åcantly resolved.\nAppearance AfÔ¨Ånity Modeling. When only matching costs are considered for aggregation, self-\nattention layer processes the correlation map itself disregarding the noise involved in the correlation\nmap, which may lead to inaccurate correspondences. Rather than solely relying on raw correlation\nmap, we additionally provide an appearance embedding from input features to disambiguate the\ncorrelation map aided by appearance afÔ¨Ånity within the Transformer. Intuition behind is that visually\nsimilar points in an image, e.g., color or feature, have similar correspondences, as proven in stereo\nmatching literature, e.g., Cost V olume Filtering (CVF) [16, 50].\n4\nIntra-CorrelationSelf-Attention\nLN\nLN & MLP \nPositionalEmbedding\n‚Ñéùë§‚Ñéùë§\nùêø\n‚Ñéùë§‚Ñéùë§\nùêøùëù\n ‚Ñéùë§‚Ñéùë§\nùêøùëù\nInter-CorrelationSelf-Attention\nLN\nLN & MLP \n‚Ñéùë§‚Ñéùë§\nùêøùëù\nProjection\nFigure 3: Illustration of Transformer aggregator.Given correlation mapsCwith projected features,\nTransformer aggregation consisting of intra- and inter-correlation self-attention with LN and MLP\nreÔ¨Ånes the inputs not only across spatial domains but across levels.\nTo provide appearance afÔ¨Ånity, we propose to concatenate embedded features projected from input\nfeatures with the correlation map. We Ô¨Årst feed the features Dinto linear projection networks, and\nthen concatenate the output along corresponding dimension, so that the correlation map is augmented\nsuch that [C,P(D)] ‚ààRhw√ó(hw+p), where [ ¬∑] denotes concatenation, Pdenotes linear projection\nnetworks, and pis channel dimension of embedded feature. Within the Transformer, self-attention\nlayer aggregates the correlation map and passes the output to the linear projection networks to retain\nthe size of original correlation C.\nMulti-Level Aggregation. As shown in [37, 34, 39, 58, 31], leveraging multi-level features allows\ncapturing hierarchical semantic feature representations. Thus we also use multi-level features from\ndifferent levels of convolutional layers to construct a stack of correlation maps. Each correlation map\nCl computed between Dl\ns and Dl\nt is concatenated with corresponding embedded features and fed into\nthe aggregation networks. The aggregation networks now consider multiple correlations, aiming to\neffectively aggregates the matches by the hierarchical semantic representations.\nAs shown in Fig. 3, a stack of Laugmented correlation maps, [Cl,P(Dl)]L\nl=1 ‚ààRhw√ó(hw+p)√óL,\nundergo the Transformer aggregator. For each l-th augmented correlation map, we aggregate with\nself-attention layer across all the points in the augmented correlation map, and we refer this as\nintra-correlation self-attention. In addition, subsequent to this, the correlation map undergoes inter-\ncorrelation self-attention across multi-level dimensions. Contrary to HPF [37] that concatenates all\nthe multi-level features and compute a correlation map, which disregards the level-wise similarities,\nwithin the inter-correlation layer of the proposed model, the similar matching scores are explored\nacross multi-level dimensions. In this way, we can embrace richer semantics in different levels of\nfeature maps, as shown in Fig. 4.\n3.4 Cost Aggregation with Transformers\nBy leveraging the Transformer aggregator, we present cost aggregation framework with following\nadditional techniques to improve the performance.\nSwapping Self-Attention. To obtain a reÔ¨Åned correlation map invariant to order of the input images\nand impose consistent matching scores, we argue that reciprocal scores should be used as aids to infer\nconÔ¨Ådent correspondences. As correlation map contains bidirectional matching scores, from both\ntarget and source perspective, we can leverage matching similarities from both directions in order to\nobtain more reciprocal scores as done similarly in other works [45, 26].\nAs shown in Fig. 1, we Ô¨Årst feed the augmented correlation map to the aforementioned Transformer\naggregator. Then we transpose the output, swapping the pair of dimensions in order to concatenate\nwith the embedded feature from the other image, and feed into the subsequent another aggregator.\nNote that we share the parameters of the Transformer aggregators to obtain reciprocal scores. Formally,\nwe deÔ¨Åne the whole process as following:\nS= T([Cl,P(Dl\nt)]L\nl=1 + Epos),\nC‚Ä≤= T([(Sl)T,P(Dl\ns)]L\nl=1 + Epos),\n(2)\nwhere CT(i,j) =C(j,i) denotes swapping the pair of dimensions corresponding to the source and\ntarget images; Sdenotes the intermediate correlation map before swapping the axis. Note that\nNC-Net [45] proposed a similar procedure, but instead of processing serially, they separately process\nthe correlation map and its transposed version and add the outputs, which is designed to produce\n5\n(a) Source\n (b) Target\n (c) Corr. #1\n (d) Corr. #3\n (e) HPF [37]\n (f) CATs\nFigure 4: Visualization of multi-level aggregation: (a) source, (b) target images, (c), (d) multi-level\ncorrelation maps (e.g., l = 1and l = 3), respectively, and Ô¨Ånal correlation maps by (e) HPF [ 37]\nand (f) CATs. Note that HPF and CATs utilize the same feature maps. Compared to HPF, CATs\nsuccessfully embrace richer semantics in different levels of feature map.\na correlation map invariant to the particular order of the input images. Unlike this, we process the\ncorrelation map serially, Ô¨Årst aggregating one pair of dimensions and then further aggregating with\nrespect to the other pair. In this way, the subsequent attention layer is given more consistent matching\nscores as an input, allowing further reduction of inconsistent matching scores. We include an ablation\nstudy to justify our choice in Section 4.4\nResidual Connection. At the initial phase when the correlation map is fed into the Transformers,\nnoisy score maps are inferred due to randomly-initialized parameters, which could complicate the\nlearning process. To stabilize the learning process and provide a better initialization for the matching,\nwe employ the residual connection. SpeciÔ¨Åcally, we enforce the cost aggregation networks to estimate\nthe residual correlation by adding residual connection around aggregation networks.\n3.5 Training\nData Augmentation. Transformer is well known for lacking some of inductive bias and its data-\nhungry nature thus necessitates a large quantity of training data to be fed [61, 10]. Recent methods [55,\n56, 32] that employ the Transformer to address Computer Vision tasks have empirically shown that\ndata augmentation techniques have positive impact on performance. However, in correspondence task,\nthe question of to what extent can data augmentation affect the performance has not yet been properly\naddressed. From the experiments, we empirically Ô¨Ånd that data augmentation has positive impacts on\nperformance in semantic correspondence with Transformers as reported in Section 4.4. To apply data\naugmentation [6, 2] with predetermined probabilities to input images at random. SpeciÔ¨Åcally, 50% of\nthe time, we randomly crop the input image, and independently for each augmentation function used\nin [6], we set the probability for applying the augmentation as 20%. More details can be found in\nsupplementary material.\nTraining Objective. As in [37, 39, 35], we assume that the ground-truth keypoints are given for\neach pair of images. We Ô¨Årst average the stack of reÔ¨Åned correlation maps C‚Ä≤‚ààRhw√óhw√óL to obtain\nC‚Ä≤‚Ä≤‚ààRhw√óhw and then transform it into a dense Ô¨Çow Ô¨Åeld Fpred using soft-argmax operator [26].\nSubsequently, we compare the predicted dense Ô¨Çow Ô¨Åeld with the ground-truth Ô¨Çow Ô¨Åeld FGT\nobtained by following the protocol of [37] using input keypoints. For the training objective, we utilize\nAverage End-Point Error (AEPE) [34], computed by averaging the Euclidean distance between the\nground-truth and estimated Ô¨Çow. We thus formulate the objective function as L= ‚à•FGT ‚àíFpred‚à•2.\n4 Experiments\n4.1 Implementation Details\nFor backbone feature extractor, we use ResNet-101 [14] pre-trained on ImageNet [8], and follow-\ning [37], extract the features from the best subset layers. Other backbone features can also be used,\nwhich we analyze the effect of various backbone features in the following ablation study. For the\nhyper-parameters for Transformer encoder, we set the depth as 1 and the number of heads as 6.\nWe resize the spatial size of the input image pairs to 256√ó256 and a sequence of selected features\nare resized to 16 √ó16. We use a learnable positional embedding [ 10], instead of Ô¨Åxed [ 61]. We\nimplemented our network using PyTorch [40], and AdamW [33] optimizer with an initial learning\n6\nTable 1: Quantitative evaluation on standard benchmarks [38, 11, 12]. Higher PCK is better.\nThe best results are in bold, and the second best results are underlined. CATs‚Ä†means CATs without\nÔ¨Åne-tuning feature backbone. Feat.-level: Feature-level, FT. feat.: Fine-tune feature.\nMethods Feat.-levelFT. feat. Aggregation\nSPair-71k [38]PF-PASCAL [12]PF-WILLOW [11]\nPCK @Œ±bbox PCK @Œ±img PCK @Œ±bbox\n0.1 0.05 0.1 0.15 0.05 0.1 0.15\nWTA Single \u0017 - 25.7 35.2 53.3 62.8 24.7 46.9 59.0\nCNNGeo [42] Single \u0017 - 20.6 41.0 69.5 80.4 36.9 69.2 77.8\nA2Net [49] Single \u0017 - 22.3 42.8 70.8 83.3 36.3 68.8 84.4\nWeakAlign [43] Single \u0017 - 20.9 49.0 74.8 84.0 37.0 70.2 79.9\nRTNs [23] Single \u0017 - 25.7 55.2 75.9 85.2 41.3 71.9 86.2\nSFNet [26] Multi \u0017 - - 53.6 81.9 90.6 46.3 74.0 84.2\nNC-Net [45] Single \u0013 4D Conv. 20.1 54.3 78.9 86.0 33.8 67.0 83.7\nDCC-Net [17] Single \u0017 4D Conv. - 55.6 82.3 90.5 43.6 73.8 86.5\nHPF [37] Multi - RHM 28.2 60.1 84.8 92.7 45.9 74.4 85.6\nGSF [21] Multi \u0017 2D Conv. 36.1 65.6 87.8 95.9 49.1 78.7 90.2\nANC-Net [27] Single \u0017 4D Conv. - - 86.1 - - - -\nDHPF [39] Multi \u0017 RHM 37.3 75.7 90.7 95.0 49.5 77.6 89.1\nSCOT [31] Multi - OT-RHM 35.6 63.1 85.4 92.7 47.8 76.0 87.1\nCHM [35] Single \u0017 6D Conv. 46.3 80.1 91.6 94.9 52.7 79.4 87.5\nCATs‚Ä† Multi \u0017 Transformer 42.4 67.5 89.1 94.9 46.6 75.6 87.5\nCATs Multi \u0013 Transformer 49.9 75.4 92.6 96.4 50.3 79.2 90.3\nTable 2: Per-class quantitative evaluation on SPair-71k [38] benchmark.\nMethods aero. bike bird boat bott. bus car cat chai. cow dog hors. mbik. pers. plan. shee. trai. tvall\nCNNGeo [42]23.4 16.7 40.2 14.3 36.4 27.7 26.0 32.7 12.7 27.4 22.8 13.7 20.9 21.0 17.5 10.2 30.8 34.120.6A2Net [49] 22.6 18.5 42.0 16.4 37.9 30.8 26.5 35.6 13.3 29.6 24.3 16.0 21.6 22.8 20.5 13.5 31.4 36.522.3WeakAlign [43]22.2 17.6 41.9 15.1 38.1 27.4 27.2 31.8 12.8 26.8 22.6 14.2 20.0 22.2 17.9 10.4 32.2 35.120.9NC-Net [45]17.9 12.2 32.1 11.7 29.0 19.9 16.1 39.2 9.9 23.9 18.8 15.7 17.4 15.9 14.8 9.6 24.2 31.120.1HPF [37] 25.2 18.9 52.1 15.7 38.0 22.8 19.1 52.9 17.9 33.0 32.8 20.6 24.4 27.9 21.1 15.9 31.5 35.628.2SCOT [31] 34.9 20.7 63.8 21.1 43.5 27.3 21.3 63.1 20.0 42.9 42.5 31.1 29.8 35.0 27.7 24.4 48.4 40.835.6DHPF [39] 38.4 23.8 68.3 18.9 42.6 27.9 20.1 61.6 22.0 46.9 46.1 33.5 27.6 40.1 27.6 28.1 49.5 46.537.3CHM [35] 49.629.368.7 29.745.348.439.564.9 20.360.556.146.0 33.8 44.3 38.9 31.4 72.255.546.3\nCATs‚Ä† 46.5 26.9 69.124.3 44.3 38.5 30.2 65.715.9 53.7 52.2 46.732.7 35.2 32.2 31.2 68.0 49.142.4CATs 52.0 34.7 72.2 34.3 49.9 57.5 43.6 66.5 24.4 63.2 56.5 52.0 42.6 41.7 43.0 33.6 72.6 58.049.9\nrate of 3e‚àí5 for the CATs layers and 3e‚àí6 for the backbone features are used, which we gradually\ndecrease during training.\n4.2 Experimental Settings\nIn this section, we conduct comprehensive experiments for semantic corrspondence, by evaluating\nour approach through comparisons to state-of-the-art methods including CNNGeo [42], A2Net [49],\nWeakAlign [43], NC-Net [45], RTNs [23], SFNet [26], HPF [37], DCC-Net [17], ANC-Net [27],\nDHPF [39], SCOT [31], GSF [21], and CHMNet [35]. In Section 4.3, we Ô¨Årst evaluate matching\nresults on several benchmarks with quantitative measures, and then provide an analysis of each\ncomponent in our framework in Section 4.4. For more implementation details, please refer to our\nimplementation available at https://github.com/SunghwanHong/CATs.\nDatasets. SPair-71k [38] provides total 70,958 image pairs with extreme and diverse viewpoint,\nscale variations, and rich annotations for each image pair, e.g., keypoints, scale difference, truncation\nand occlusion difference, and clear data split. Previously, for semantic matching, most of the datasets\nare limited to a small quantity with similar viewpoints and scales [ 11, 12]. As our network relies\non Transformer which requires a large number of data for training, SPair-71k [38] makes the use of\nTransformer in our model feasible. we also consider PF-PASCAL [12] containing 1,351 image pairs\nfrom 20 categories and PF-WILLOW [11] containing 900 image pairs from 4 categories, each dataset\nproviding corresponding ground-truth annotations.\nEvaluation Metric. For evaluation on SPair-71k [38], PF-WILLOW [11], and PF-PASCAL [12],\nwe employ a percentage of correct keypoints (PCK), computed as the ratio of estimated keypoints\nwithin the threshold from ground-truths to the total number of keypoints. Given predicted keypoint\nkpred and ground-truth keypoint kGT, we count the number of predicted keypoints that satisfy\nfollowing condition: d(kpred,kGT) ‚â§Œ±¬∑max(H,W ), where d( ¬∑) denotes Euclidean distance; Œ±\n7\nFigure 5: Qualitative results on SPair-71k [38]: (from top to bottom) keypoints transfer results by\nSCOT [31], DHPF [39], and CATs. Note that green and red line denotes correct and wrong prediction,\nrespectively, with respect to the ground-truth.\ndenotes a threshold which we evaluate on PF-PASCAL with Œ±img, SPair-71k and PF-WILLOW with\nŒ±bbox; H and W denote height and width of the object bounding box or entire image, respectively.\n4.3 Matching Results\nFor a fair comparison, we follow the evaluation protocol of [37] for SPair-71k, which our network\nis trained on the training split and evaluated on the test split. Similarly, for PF-PASCAL and PF-\nWILLOW, following the common evaluation protocol of [13, 23, 17, 37, 39], we train our network\non the training split of PF-PASCAL [12] and then evaluate on the test split of PF-PASCAL [12] and\nPF-WILLOW [11]. All the results of other methods are reported under identical setting.\nTable 1 summarizes quantitative results on SPair-71k [38], PF-PASCAL [12] and PF-WILLOW [11].\nWe note whether each method leverages multi-level features and Ô¨Åne-tunes the backbone features in\norder to ensure a fair comparison. We additionally denote the types of cost aggregation. Generally,\nour CATs outperform other methods over all the benchmarks. This is also conÔ¨Årmed by the results\non SPair-71k, as shown in Table 2, where the proposed method outperforms other methods by large\nmargin. Note that CATs‚Ä†reports lower PCK than that of CHM, and this is because CHM Ô¨Åne-tunes\nits backbone networks while CATs‚Ä†does not. Fig. 1 visualizes qualitative results for extremely\nchallenging image pairs. We observe that compared to current state-of-the-art methods [31, 39], our\nmethod is capable of suppressing noisy scores and Ô¨Ånd accurate correspondences in cases with large\nscale and geometric variations.\nIt is notable that CATs generally report lower PCK on PF-WILLOW [11] compared to other state-\nof-the-art methods. This is because the Transformer is well known for lacking some of inductive\nbias. When we evaluate on PF-WILLOW, we infer with the model trained on the training split of PF-\nPASCAL, which only contains 1,351 image pairs, and as only relatively small quantity of image pairs\nis available within the PF-PASCAL training split, the Transformer shows low generalization power.\nThis demonstrates that the Transformer-based architecture indeed requires a means to compensate for\nthe lack of inductive bias, e.g., data augmentation.\n4.4 Ablation Study\nIn this section we show an ablation analysis to validate critical components we made to design our\narchitecture, and provide an analysis on use of different backbone features, and data augmentation.\nWe train all the variants on the training split of SPair-71k [38] when evaluating on SPair-71k, and\ntrain on PF-PASCAL [12] for evaluating on PF-PASCAL. We measure the PCK, and each ablation\nexperiment is conducted under same experimental setting for a fair comparison.\nNetwork Architecture. Table 3 shows the analysis on key components in our architecture. There\nare four key components we analyze for the ablation study, including appearance modelling, multi-\nlevel aggregation, swapping self-attention, and residual connection.\n8\nTable 3: Ablation study of CATs.\nComponents SPair-71k\nŒ±bbox= 0.1\n(I) Baseline 26.8\n(II) + Appearance Modelling 33.5\n(III) + Multi-level Aggregation35.9\n(IV) + Swapping Self-Attention38.8\n(V) + Residual Connection 42.4\nWe Ô¨Årst deÔ¨Åne the model without any of these as baseline,\nwhich simply feeds the correlation map into the self-\nattention layer. We evaluate on SPair-71k benchmark by\nprogressively adding the each key component. From I to\nV, we observe consistent increase in performance when\neach component is added. II shows a large improvement\nin performance, which demonstrates that the appearance\nmodelling enabled the model to reÔ¨Åne the ambiguous\nor noisy matching scores. Although relatively small\nincrease in PCK forIII, it proves that the proposed model\nsuccessfully aggregates the multi-level correlation maps. Furthermore, IV and V show apparent\nincrease, proving the signiÔ¨Åcance of both components.\nTable 4: Ablation study of feature backbone.\nFeature Backbone SPair-71k PF-PASCAL\nŒ±bbox= 0.1 Œ±img= 0.1\nDeiT-Bsingle[55] 32.1 76.5\nDeiT-Ball[55] 38.2 87.5\nDINO w/ ViT-B/16single[4] 39.5 88.9\nDINO w/ ViT-B/16all[4] 42.0 88.9\nResNet-101single[14] 37.4 87.3\nResNet-101multi[14] 42.4 89.1\nFeature Backbone. As shown in Table 4, we\nexplore the impact of different feature back-\nbones on the performance on SPair-71k [ 38]\nand PF-PASCAL [12]. We report the results of\nmodels with backbone networks frozen. The top\ntwo rows are models with DeiT-B [55], next two\nrows use DINO [ 4], and the rest use ResNet-\n101 [14] as backbone. SpeciÔ¨Åcally, subscript\nsingle for DeiT-B and DINO, we use the fea-\nture map extracted at the last layer for the single-\nlevel, while for subscriptall, every feature map\nfrom 12 layers is used for cost construction. For ResNet-101 subscript single, we use a single-level\nfeature cropped at conv4 ‚àí23, while for multi, we use the best layer subset provided by [37]. Sum-\nmarizing the results, we observed that leveraging multi-level features showed apparent improvements\nin performance, proving effectiveness of multi-level aggregation introduced by our method. It is worth\nnoting that DINO, which is more excel at dense tasks than DeiT-B, outperforms DeiT-B when applied\nto semantic matching. This indicates that Ô¨Åne-tuning the feature could enhance the performance. To\nbest of our knowledge, we are the Ô¨Årst to employ Transformer-based features for semantic matching.\nIt would be an interesting setup to train an end-to-end Transformer-based networks, and we hope this\nwork draws attention from community and made useful for future works.\nTable 5: Effects of augmentation.\nAugment. SPair-71k\nŒ±bbox= 0.1\nDHPF [39] \u0017 37.3\nDHPF [39] \u0013 39.4\nCATs \u0017 43.5\nCATs \u0013 49.9\nData Augmentation. In Table 5, we compared the PCK per-\nformance between our variants and DHPF [39]. We note if the\nmodel is trained with augmentation. For a fair comparison, we\nevaluate both DHPF [39] and CATs trained on SPair-71k [38]\nusing strong supervision, which assumes that the ground-truth\nkeypoints are given. The results show that compared to DHPF,\na CNN-based method, data augmentation has a larger inÔ¨Çuence\non CATs in terms of performance. This demonstrates that not\nonly we eased the data-hunger problem inherent in Transform-\ners, but also found that applying augmentations for matching has positive effects. Augmentation\ntechnique would bring a highly likely improvements in performance, and we hope that the future\nworks beneÔ¨Åt from this.\nSerial swapping. It is apparent that Equation 2 is not designed for an order-invariant output.\nDifferent from NC-Net [45], we let the correlation map undergo the self-attention module in a serial\nmanner. We conducted a simple experiment to compare the difference between each approach. From\nexperiments, we obtained the results of parallel and serial processing on SPair-71k with Œ±bbox = 0.1,\nwhich are PCK of 40.8 and 42.4, respectively. In light of this, although CATs may not support order\ninvariance, adopting serial processing can obtain higher PCK as it has a better capability to reduce\ninconsistent matching scores by additionally processing the already processed cost map, which we\nÔ¨Ånalize the architecture to include serial processing.\n4.5 Analysis\nVisualizing Self-Attention. We visualize the multi-level attention maps obtained from the Trans-\nformer aggregator. As shown in Fig. 6, the learned self-attention map at each level exhibits different\n9\nFigure 6: Visualization of self-attention: (from left to right) source and target images, and multi-\nlevel self-attentions. Note that each attention map attends different aspects, and CATs aggregates the\ncost leveraging hierarchical semantic representations.\naspect. With these self-attentions, our networks can leverage multi-level correlations to capture\nhierarchical semantic feature representations effectively.\nTable 6: Memory and run-time comparison.\nInference time for aggregator is denoted by (¬∑).\nAggregationMemory [GB]Run-time [ms]\nNC-Net [45]4D Conv. 1.2 193.3 (166.1)SCOT [31] OT-RHM 4.6 146.5 (81.6)DHPF [39] RHM 1.6 57.7 (29.5)CHM [35] 6D Conv 1.6 47.2(38.3)\nCATs Transformer 1.9 34.5(7.4)\nMemory and run-time. In Table 6, we show\nthe memory and run-time comparison to NC-\nNet [45], SCOT [31], DHPF [39] and CHM [35]\nwith CATs. For a fair comparison, the results\nare obtained using a single NVIDIA GeForce\nRTX 2080 Ti GPU and Intel Core i7-10700 CPU.\nWe measure the inference time for both the pro-\ncess without counting feature extraction, and the\nwhole process. Thanks to Transformers‚Äô fast com-\nputation nature, compared to other methods, our method is beyond compare. We also Ô¨Ånd that\ncompared to other cost aggregation methods including 4D, 6D convolutons, OT-RHM and RHM,\nours show comparable efÔ¨Åciency in terms of computational cost. Note that NC-Net utilizes a single\nfeature map while other methods utilize multi-level feature maps. We used the standard self-attention\nmodule for implementation, but more advanced and efÔ¨Åcient transformer [32] architectures could\nreduce the overall memory consumption.\n4.6 Limitations\nOne obvious limitation that CATs possess is that when applying the method to non-corresponding\nimages, the proposed method would still deliver correspondences as it lacks power to ignore pixels that\ndo not have correspondence at all. A straightforward solution would be to consider including a module\nto account for pixel-wise matching conÔ¨Ådence. Another limitation of CATs would be its inability to\naddress a task of Ô¨Ånding accurate correspondences given multi-objects or non-corresponding objects.\nAddressing such challenges would be a promising direction for future work.\n5 Conclusion\nIn this paper, we have proposed, for the Ô¨Årst time, Transformer-based cost aggregation networks\nfor semantic correspondence which enables aggregating the matching scores computed between\ninput features, dubbed CATs. We have made several architectural designs in the network architec-\nture, including appearance afÔ¨Ånity modelling, multi-level aggregation, swapping self-attention, and\nresidual correlation. We have shown that our method surpasses the current state-of-the-art in several\nbenchmarks. Moreover, we have conducted extensive ablation studies to validate our choices and\nexplore its capacity. A natural next step, which we leave for future work, is to examine how CATs\ncould extend its domain to tasks including 3-D reconstruction, semantic segmentation and stitching,\nand to explore self-supervised learning.\nAcknowledgements\nThis research was supported by the MSIT, Korea, under the ICT Creative Consilience program\n(IITP-2021-2020-0-01819) and (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge\nAcquisition and Inference Techniques) supervised by the IITP and National Research Foundation of\nKorea (NRF-2021R1C1C1006897).\n10\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Alexander Buslaev, Vladimir I Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and\nAlexandr A Kalinin. Albumentations: fast and Ô¨Çexible image augmentations. Information, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV. Springer, 2020.\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294,\n2021.\n[5] Christopher B Choy, JunYoung Gwak, Silvio Savarese, and Manmohan Chandraker. Universal correspon-\ndence network. NeurIPS, 29:2414‚Äì2422, 2016.\n[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, 2020.\n[7] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. InCVPR Workshops),\n2005.\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\n[9] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point\ndetection and description. In CVPR, 2018.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[11] Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal Ô¨Çow. In CVPR, 2016.\n[12] Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal Ô¨Çow: Semantic correspondences\nfrom object proposals. IEEE transactions on pattern analysis and machine intelligence, 2017.\n[13] Kai Han, Rafael S Rezende, Bumsub Ham, Kwan-Yee K Wong, Minsu Cho, Cordelia Schmid, and Jean\nPonce. Scnet: Learning semantic correspondence. In ICCV, 2017.\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[15] Sunghwan Hong and Seungryong Kim. Deep matching prior: Test-time optimization for dense correspon-\ndence. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages\n9907‚Äì9917, October 2021.\n[16] Asmaa Hosni, Christoph Rhemann, Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast cost-volume\nÔ¨Åltering for visual correspondence and beyond. PAMI, 2012.\n[17] Shuaiyi Huang, Qiuyue Wang, Songyang Zhang, Shipeng Yan, and Xuming He. Dynamic context\ncorrespondence network for semantic alignment. In ICCV, 2019.\n[18] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. LiteÔ¨Çownet: A lightweight convolutional neural\nnetwork for optical Ô¨Çow estimation. In CVPR, 2018.\n[19] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.\nFlownet 2.0: Evolution of optical Ô¨Çow estimation with deep networks. In CVPR, 2017.\n[20] Sangryul Jeon, Seungryong Kim, Dongbo Min, and Kwanghoon Sohn. Parn: Pyramidal afÔ¨Åne regression\nnetworks for dense semantic correspondence. In ECCV, 2018.\n[21] Sangryul Jeon, Dongbo Min, Seungryong Kim, Jihwan Choe, and Kwanghoon Sohn. Guided semantic\nÔ¨Çow. In ECCV. Springer, 2020.\n[22] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. COTR: Correspondence\nTransformer for Matching Across Images. In Int. Conf. Comput. Vis., 2021.\n11\n[23] Seungryong Kim, Stephen Lin, Sang Ryul Jeon, Dongbo Min, and Kwanghoon Sohn. Recurrent transformer\nnetworks for semantic correspondence. In NeurIPS, 2018.\n[24] Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, and Kwanghoon Sohn. Fcss:\nFully convolutional self-similarity for dense semantic correspondence. In CVPR, 2017.\n[25] Seungryong Kim, Dongbo Min, Somi Jeong, Sunok Kim, Sangryul Jeon, and Kwanghoon Sohn. Semantic\nattribute matching networks. In CVPR, 2019.\n[26] Junghyup Lee, Dohyung Kim, Jean Ponce, and Bumsub Ham. Sfnet: Learning object-aware semantic\ncorrespondence. In CVPR, 2019.\n[27] Shuda Li, Kai Han, Theo W Costain, Henry Howard-Jenkins, and Victor Prisacariu. Correspondence\nnetworks with adaptive neighbourhood consensus. In CVPR, 2020.\n[28] Jing Liao, Yuan Yao, Lu Yuan, Gang Hua, and Sing Bing Kang. Visual attribute transfer through deep\nimage analogy. arXiv:1705.01088, 2017.\n[29] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[30] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift Ô¨Çow: Dense correspondence across scenes and its\napplications. IEEE transactions on pattern analysis and machine intelligence, 33(5):978‚Äì994, 2010.\n[31] Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang. Semantic correspondence as an optimal transport\nproblem. In CVPR, 2020.\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv:1711.05101, 2017.\n[34] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, and Juho Kannala. Dgc-net:\nDense geometric correspondence network. In WACV, 2019.\n[35] Juhong Min and Minsu Cho. Convolutional hough matching networks. arXiv preprint arXiv:2103.16831,\n2021.\n[36] Juhong Min, Dahyun Kang, and Minsu Cho. Hypercorrelation squeeze for few-shot segmentation. arXiv\npreprint arXiv:2104.01538, 2021.\n[37] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Hyperpixel Ô¨Çow: Semantic correspondence with\nmulti-layer neural features. In ICCV, 2019.\n[38] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Spair-71k: A large-scale benchmark for semantic\ncorrespondence. arXiv preprint arXiv:1908.10543, 2019.\n[39] Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Learning to compose hypercolumns for visual\ncorrespondence. In ECCV, 2020.\n[40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n[41] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with\nlarge vocabularies and fast spatial matching. In CVPR. IEEE, 2007.\n[42] Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural network architecture for\ngeometric matching. In CVPR, 2017.\n[43] Ignacio Rocco, Relja Arandjelovi¬¥c, and Josef Sivic. End-to-end weakly-supervised semantic alignment. In\nCVPR, 2018.\n[44] Ignacio Rocco, Relja Arandjelovi ¬¥c, and Josef Sivic. EfÔ¨Åcient neighbourhood consensus networks via\nsubmanifold sparse convolutions. In ECCV, 2020.\n[45] Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovi¬¥c, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Neigh-\nbourhood consensus networks. In NeurIPS, 2018.\n12\n[46] Michael Rubinstein, Armand Joulin, Johannes Kopf, and Ce Liu. Unsupervised joint object discovery and\nsegmentation in internet images. In CVPR, 2013.\n[47] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning\nfeature matching with graph neural networks. In CVPR, 2020.\n[48] Daniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo correspon-\ndence algorithms. International journal of computer vision, 2002.\n[49] Paul Hongsuck Seo, Jongmin Lee, Deunsol Jung, Bohyung Han, and Minsu Cho. Attentive semantic\nalignment with offset-aware correlation kernels. In ECCV, 2018.\n[50] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical Ô¨Çow using pyramid,\nwarping, and cost volume. In CVPR, 2018.\n[51] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature\nmatching with transformers. arXiv preprint arXiv:2104.00680, 2021.\n[52] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan,\nChanghu Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv preprint\narXiv:2012.15460, 2020.\n[53] Richard Szeliski. Image alignment and stitching: A tutorial. Foundations and Trends¬Æ in Computer\nGraphics and Vision, 2006.\n[54] Tatsunori Taniai, Sudipta N Sinha, and Yoichi Sato. Joint recovery of dense correspondence and cosegmen-\ntation in two images. In CVPR, 2016.\n[55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers and distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[56] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv√© J√©gou. Going deeper\nwith image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[57] Prune Truong, Martin Danelljan, Luc V Gool, and Radu Timofte. Gocor: Bringing globally optimized\ncorrespondence volumes into your neural network. In NeurIPS, 2020.\n[58] Prune Truong, Martin Danelljan, and Radu Timofte. Glu-net: Global-local universal network for dense\nÔ¨Çow and correspondences. In CVPR, 2020.\n[59] Prune Truong, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning accurate dense correspon-\ndences and when to trust them. arXiv preprint arXiv:2101.01710, 2021.\n[60] Micha≈Ç J Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk: Learning local features with policy gradient.\narXiv preprint arXiv:2006.13566, 2020.\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[62] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n13\nAppendix\nIn this document, we provide more implementation details of CATs and more results on SPair-\n71k [38], PF-PASCAL [12], and PF-WILLOW [11].\nAppendix A. More Implementation Details\nNetwork Architecture Details. Given resized input images Is,It ‚ààR256√ó256√ó3, we conducted\nexperiments using different feature backbone networks, including DeiT-B [ 55], DINO [ 4] and\nResNet-101 [ 14]. For the ResNet-101 multi in the paper, we use the best layer subset [ 37] of\n(0,8,20,21,26,28,29,30) for SPair-71k, and (2,17,21,22,25,26,28) for PF-PASCAL and PF-WILLOW.\nWe resized the spatial resolution of extracted feature maps to16 √ó16. The extracted features undergo\nl-2 normalization and the correlation maps are constructed using dot products. Contrary to original\nTransformer [61] with encoder-decoder architecture, CATs is an encoder-only architecture. Within\nour Transformer aggregator, as explained in the paper, we concatenate the embedded features with\ncorrelation maps. We feed the resized features into the projection networks to reduce the dimension\nfrom cto 128, where cis the channel dimension of the feature. We then feed the augmented corre-\nlation map into the transformer encoder, which we use 1 encoder layer and 6 heads in multi-head\nattention layers. We then use soft-argmax function [26] with temperature œÑ = 0.02 to infer a dense\ncorrespondence Ô¨Åeld.\nTable 1: Data Augmentation.\nAugmentation type Probability\n(I) ToGray 0.2\n(II) Posterize 0.2\n(III) Equalize 0.2\n(IV) Sharpen 0.2\n(V) RandomBrightnessContrast0.2\n(VI) Solarize 0.2\n(VII) ColorJitter 0.2\nTraining Details. For training on both SPair-\n71k [38] and PF-PASCAL [12], we set the initial learn-\ning rate for CATs as 3e-5 and backbone networks as\n3e-6. We then decrease the learning rate using multi-\nstep learning rate decay [ 40]. We use a batch size of\n32. We trained our networks using AdamW [33] with\nweight decay of 0.05. For data augmentation implemen-\ntation, we implemented random cropping of image with\nprobability set to 0.5, and used functions implemented\nby [2] as shown in Table 1.\nAppendix B. Reasoning of Architectural Choices\nTable 2: Ablation study\nof correlation map.\nMethod SPair-71k\nŒ±bbox= 0.1\nCATs‚Ä† 42.4\nw/o corr. 37.3\nCorrelation Map. Given the results from ablation study on architecture\ndesigns in the paper, we Ô¨Ånd that use of appearance and the self-attention\nmechanism are critical to the performance. However, since transformers\nhave the ability to perform dot products and use of appearance is critical\nfor matching task, one may raise a question: Why correlation map?As\na concurrent work, COTR [22] attempts to omit correlation map and lets\ntransformers to make correlation among features. They show that this is\na highly effective strategy in forming correspondences.\nAs shown in the Table 2, we conducted an ablation study to Ô¨Ånd out if the use of cost volume is\nbeneÔ¨Åcial for our setting. We conduct the experiment with the simplest setup by setting the values of\ncorrelation map to zeros. In Table 3, for the experimental setting for COTR, we excluded zoom in\ntechnique, set the number of layers in transformer to 1 and changed the architecture to output a Ô¨Çow\nmap instead of pixel coordinates. We used single pair of feature maps for computing correlation map\nand left all other components in the pipeline the same. More details of setting of both experiments\ncan be found in supplementary materials.\nTable 3: Comparison to COTR.\nMA.: Multi-level Aggregation\nModel SPair-71kRun-time [ms]Œ±bbox= 0.1\nCOTR [22] 22.1 56.1\nCATs‚Ä†w/o MA. 37.4 39.1\nGiven Table 3 in main paper and the results for experiments\nvalidating the use of correlation map, we could say that the sole\nuse of transformer (with its ability to perform dot products) or\nsole use of appearance is not sufÔ¨Åcient, but rather use of both\ncost volume and appearance allow the transformer to relate\nthe pairwise relationships and appearance, which helps to Ô¨Ånd\nmore accurate correspondences. However, this is an ongoing\n14\nresearch topic whether explicitly using the correlation map for forming correspondences is better or\nnot, which we leave to community for further study.\nAppendix C. Additional Results\nMore Qualitative Results. We provide more comparison of CATs and other state-of-the-art meth-\nods on SPair-71k [38], PF-PASCAL [12], and PF-WILLOW [12]. We also present multi-head and\nmulti-level attention visualization on SPair-71k in Fig 4, and multi-level aggregation in Fig 5.\nBroader Impact\nOur cost aggregation networks can be beneÔ¨Åcial in a wide range of applications including semantic\nsegmentation [46, 54, 36], object detection [29], and image editing [53, 30, 28, 25], as well as dense\ncorrespondence. For example, some methods for semantic segmentation tasks require cost volume\naggregation. Such adoption would enhance the performance, which could affect various applications,\ne.g., autonomous driving. On the other hand, our module risks being used for malicious works, which\nincludes image surveillance system, but on its own, we doubt that it can be used for such works.\n15\n(a) DHPF [39]\n (b) SCOT [31]\n (c) CATs\n (d) Ground-truth\nFigure 1: Qualitative results on SPair-71k [38]: keypoints transfer results by (a) DHPF [39], (b)\nSCOT [31], and (c) CATs, and (d) ground-truth. Note that green and red line denotes correct and\nwrong prediction, respectively, with respect to the ground-truth.\n16\n(a) DHPF [39]\n (b) SCOT [31]\n (c) CATs\n (d) Ground-truth\nFigure 2: Qualitative results on PF-PASCAL [12]\n(a) DHPF [39]\n (b) SCOT [31]\n (c) CATs\n (d) Ground-truth\nFigure 3: Qualitative results on PF-WILLOW [11].\n17\nAvgHead 1 Head 2 Head 3 Head 4 Head 5\nLayer 7 Layer 6 Layer 5 Layer 4 Layer 3 Layer 2 Layer 1 Layer 0\nSource Image Target Image\nHead 0\nFigure 4: Visualization of multi-head and multi-level self-attention. Each head at l-th level layer,\nspeciÔ¨Åcally among (0,8,20,21,26,28,29,30) layers of ResNet-101 [14] as in [37], attends different\nregions, which CATs successfully aggregates the multi-level correlation maps to infer reliable\ncorrespondences.\n18\nSource Target Corr #1 Corr #2 Corr #3 Corr #4 Corr #5 Corr #6 Corr #7 Corr #8 CATs\nFigure 5: Visualization of multi-level aggregation. Each correlation refers to one of the\n(0,8,20,21,26,28,29,30) layers of ResNet-101, and our proposed method successfully aggregates the\nmulti-level correlation maps.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.763784646987915
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6472758054733276
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5748101472854614
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5206148624420166
    },
    {
      "name": "Transformer",
      "score": 0.4699896574020386
    },
    {
      "name": "Residual",
      "score": 0.41247814893722534
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4065570831298828
    },
    {
      "name": "Machine learning",
      "score": 0.3352190852165222
    },
    {
      "name": "Data mining",
      "score": 0.3336558938026428
    },
    {
      "name": "Algorithm",
      "score": 0.20122334361076355
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197347611",
      "name": "Korea University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I5124864",
      "name": "√âcole Polytechnique F√©d√©rale de Lausanne",
      "country": "CH"
    }
  ],
  "cited_by": 6
}