{
  "title": "An enhanced Swin Transformer for soccer player reidentification",
  "url": "https://openalex.org/W4390754447",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5080130223",
      "name": "Sara Akan",
      "affiliations": [
        "Yıldız Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5036755640",
      "name": "Songül Albayrak",
      "affiliations": [
        "Yıldız Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5033947768",
      "name": "Mohammad Alfrad Nobel Bhuiyan",
      "affiliations": [
        "Louisiana State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2889854113",
    "https://openalex.org/W3132882981",
    "https://openalex.org/W2965575500",
    "https://openalex.org/W2964130064",
    "https://openalex.org/W6600710366",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2795758732",
    "https://openalex.org/W3034372982",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W3204665409",
    "https://openalex.org/W3178886938",
    "https://openalex.org/W2091225325",
    "https://openalex.org/W2144958063",
    "https://openalex.org/W2155236699",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W3006714671",
    "https://openalex.org/W4285228203",
    "https://openalex.org/W4312951616",
    "https://openalex.org/W4312802153",
    "https://openalex.org/W2344924411",
    "https://openalex.org/W2342611082",
    "https://openalex.org/W2963574614",
    "https://openalex.org/W1982925187",
    "https://openalex.org/W3098711604",
    "https://openalex.org/W1971955426",
    "https://openalex.org/W2467139031",
    "https://openalex.org/W2724213014",
    "https://openalex.org/W2890159224",
    "https://openalex.org/W2963438548",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W4308347502",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W4214736485",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4283262388",
    "https://openalex.org/W2963901085",
    "https://openalex.org/W2963000559",
    "https://openalex.org/W3035724178",
    "https://openalex.org/W2789546350",
    "https://openalex.org/W4205831880",
    "https://openalex.org/W4311791534",
    "https://openalex.org/W3134728312",
    "https://openalex.org/W2962767316",
    "https://openalex.org/W2584637367",
    "https://openalex.org/W3186413649",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2990827756",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W3034580371",
    "https://openalex.org/W3217250086",
    "https://openalex.org/W3100927979",
    "https://openalex.org/W3100506510"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports\nAn enhanced Swin Transformer \nfor soccer player reidentification\nSara Akan 1, Songül Varlı 1 & Mohammad Alfrad Nobel Bhuiyan 2*\nThe re-identification (ReID) of objects in images is a widely studied topic in computer vision, with \nsignificant relevance to various applications. The ReID of players in broadcast videos of team sports \nis the focus of this study. We specifically focus on identifying the same player in images taken at any \ngiven moment during a game from various camera angles. This work varies from other person ReID \napps since the same team wears very similar clothes, there are few samples for each identification, \nand image resolutions are low. One of the hardest parts of object ReID is robust feature representation \nextraction. Despite the great success of current convolutional neural network-based (CNN) methods, \nmost studies only consider learning representations from images, neglecting long-range dependency. \nTransformer-based model studies are increasing and yielding encouraging results. Transformers \nstill have trouble extracting features from small objects and visual cues. To address these issues, we \nenhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction \nSwin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature \nextraction. We also use three loss functions to handle imbalanced data and highlight challenging \nsituations. Re-ranking with k-reciprocal encoding was used in this study’s retrieval phase, and its \nassessment findings were provided. Finally, we conducted experiments on the Market-1501 and \nSoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches \nrank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the \nMarket-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.\nRecently, automated sports video analysis has attracted considerable interest, especially in team sports like soccer, \nbasketball, and volleyball, due to the increasing demand for semantic information extraction from sports experts \nand fans. The outcomes of sports analysis can be implemented in a variety of scenarios, such as TV storytelling, \naltering the training schedule, creating game statistics, and identifying the strengths and weaknesses of a team \nor a player. player re-identification is a cornerstone of modern sports analysis, enriching our understanding of \nthe game, enhancing coaching decisions, and delivering a more comprehensive and engaging experience for \nfans and stakeholders alike. As a result, player ReID is a crucial research topic for maximizing the advantages \nof automatic sports analysis. It is necessary to connect the correct player to each track and link his/her actions \nand statistics to it to re-identify a player.\nReidentifying players in broadcast sports videos can be challenging due to factors like low video resolution, \ncamera movement, player pose, lighting conditions, and uniform similarities among players. Players can be \nreidentified on the field using their faces and jersey  numbers1,2. When the player’s face is clearly visible in close-\nup shots, using face recognition as a ReID technique is effective, but it becomes impractical in overview shots. \nOn the other hand, strategies based on jersey numbers have promise because they make up a sizable portion \nof a player’s back uniform and because HD sports videos are growing in popularity. However, several external \nfactors, like clocks, advertising logos, banners, player tilting, motion blur, and viewing angles, might potentially \npose difficulties in accurately discerning jersey  numbers3.\nSoccer player ReID research is crucial for sports as a sub-branch of person ReID, but there is a lack of research \non it thus far. In general, prior person ReID techniques gather attributes from whole images and then match \ngallery candidates based on how those attributes are represented visually. To create an effective representation, \nprior methods directly used global person  features4,5. Object ReID tasks have been dominated by CNN-based \ntechniques that are robust and discriminative in feature  extraction6–13. The analysis of CNN-based techniques in \nthe field of object ReID primarily concentrated on visual features. This paper focuses on the application of ReID \ntechnology in the context of soccer players. To be more specific, we want to develop a system that can accurately \nre-identify players from different camera angles at any moment throughout a game. This system has numer -\nous potential applications, including player tracking across multiple cameras, automated highlight videos for a \nOPEN\n1Department of Computer Engineering, Yildiz Technical University, Istanbul, Turkey. 2Department of Medicine, \nLouisiana State University Health, Shreveport, LA, USA. *email: nobel.bhuiyan@lsuhs.edu\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nsingle player, and improving referee assistance  tools14–17. The following significant differences exist between sport \nplayer ReID task and person ReID applications: (1) Players from the same team often look alike, as they wear the \nsame team jerseys; (2) The quality of images in sport player ReID is often low, with occlusions and fast player \nmovement making it harder to re-identify players. These problems are illustrated in Fig. 1. (3) Training machine \nlearning models for sport player ReID is more challenging due to the limited number of samples for each identity.\nThis study proposes a technique known as RFES-ReID to address the challenging task of soccer player ReID. \nSwin Transformer serves as the backbone network in our study, and we take use of its local attention mechanism \nand superior large-range dependency modeling capability to enhance ReID’s effectiveness. Furthermore, we \ndesigned a regional feature extraction block to take advantage of capturing fine-grained details from specific \nareas of interest within an image. The player ReID training process is then enhanced using a fusion loss function. \nRe-ranking is additionally used with the baseline model presented here to further improve performance and \nhandle false matches. The main contributions of this paper are briefly summarized as follows.\nUsing Swin Transformer as a backbone network to extract image features to address the issue of CNN’s large-\nrange dependency modeling and the high computational cost of traditional Transformers.\nThe proposed RFES enhances feature extraction accuracy from small-scale objects and improves the model’s \nlocal perception abilities by incorporating the benefits of CNNs.\nThe soccer player ReID network is enhanced through the use of cross-entropy loss, triplet loss, and focal loss \nfor accurate classification, handling unbalanced data, and considering inter-sample similarities and difficult-\nto-separate samples.\nThe RFES-ReID framework provides competitive results on person and soccer player ReID benchmarks, \nnamely Market-150118 and SoccerNet-v319.\nIn summary, the Pros and Cons of the proposed method are as follows.\nThe incorporation of the Swin Transformer as a backbone network offers effective large-range dependency \nmodeling, addressing a key limitation of CNNs in soccer player re-identification. The Regional Feature Enhance-\nment Strategy (RFES) improves small-scale object perception, enhancing local perception abilities. Versatile loss \nfunctions, including cross-entropy, triplet, and focal loss, contribute to accurate classification and robust handling \nof diverse scenarios. Although Market-1501 and SoccerNet-v3 benchmark results are competitive, there may \nbe disadvantages such as higher computational costs, more implementation complexity, reliance on benchmark \ndatasets, and hyperparameter sensitivity.\nFigure 1.  Some examples of various challenges: (a) Similar uniforms; (b) Occlusion; (c, d) low resolution and \ndifferent body movements.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nRelated works\nSince soccer player ReID is a branch of person ReID and there do not exist many studies for sports player ReID, \nmore studies for person ReID are taken into consideration here. Person ReID is a process that involves identifying \nand matching the same person across multiple images captured by different cameras. The goal is to find images \nof the same person from a gallery of images taken by various cameras that do not overlap. The task has a wide \nvariety of possible uses for public safety, particularly in smart monitoring systems. Person ReID is a challeng -\ning process since a person’s look differs across various cameras. This is due to a multitude of issues, including \nillumination variances, occlusion, position changes, and backdrop clutter.\nBefore deep learning algorithms came along, early studies on human re-recognition mostly concentrated on \nenhancing similarity metrics and manually improving visual features. Deep learning techniques have revolution-\nized person ReID tasks by automatically extracting superior features from person images and learning better \nsimilarity metrics, making them increasingly prevalent in modern applications. Deep learning techniques, in \ncontrast to conventional approaches, have the ability to automatically extract better features of person images and \nlearn better similarity metrics at the same time. CNN-based approaches have consistently been at the forefront of \nthe extraction of distinguishable and robust features, which play a vital role in the process of  ReID6,20,21. Recent \nyears have seen a significant improvement in the task of person ReID thanks to high-performance deep learning \n algorithms22–25. CNNs are used in the current methods to solve the person ReID problem using a wide range of \ntechniques, including multi-class  classification26–28,  verification29–31, distance-based deep  approaches5,32,33 and \npart-based deep  approaches6,34–37. Although CNN approaches have achieved significant  success38, they analyze \none local area at a time and encounter a reduction in detailed information due to the use of convolution and \ndownsampling operators such as pooling and stepwise convolution. CNN focuses on detecting edges, shapes, \nand distinctive features of a person, but it does not consider the interdependencies and interactions among all \nof these features. Consequently, when images of people are subjected to rotation or taken from various perspec-\ntives, the performance of the CNN model typically falls short of expectations. However, the development of the \nattention mechanism has effectively addressed the issue of information loss in convolutional neural  networks39.\nTransformer based person ReID\nTransformer40 is a popular model in natural language processing (NLP) and outperforms RNN-based and CNN-\nbased models in machine translation tasks. Vision Transformer (ViT) 41 inspired by Transformers’ scaling in \nNLP , a standard Transformer was directly applied to images with minimal modifications. ViT was introduced \nin 2020 for image classification, and its application later expanded to various computer vision tasks beyond clas-\nsification. This model outperforms CNNs in image classification tasks. The utilization of transformer models in \ncomputer vision, particularly in the domain of person ReID, is increasingly prevalent among researchers. CNN \nprimarily emphasizes the extraction of edge, shape, and person features while neglecting to account for their \ninterrelationships. The effectiveness of feature extraction for the recognition of images has been established by \nViT and Data efficient image Transformers (DeiT), indicating the practicality of a CNN-based technique. Per -\nson re-identification using CNNs captures person features without considering their relationships, whereas the \nemergence of Vision Transformers effectively addresses this issue by using a multi-head attention mechanism \nand excelling in diverse scenarios like different body movements and occlusions. ViT and Data efficient image \nTransformers (DeiT)42 demonstrate that Transformers can serve as practical alternatives for feature extraction \nin computer vision tasks.\nTransReID43 is a method for person ReID based on ViT by adding the jigsaw patch module (JPM) and the \nside information embeddings (SIE) but it requires a larger pre-training dataset due to the domain gap between \nImageNet and ReID datasets. Luo et al. 44 proposed TransReID-SSL aims to bridge this gap by examining self-\nsupervised learning methods with ViT pretrained on unlabeled person images. The results show that ViT sig -\nnificantly outperforms ImageNet supervised pre-training models on ReID tasks.\nThe ViT model’s patch size is fixed and scaled uniformly. The scale is uniform for the domain of NLP , while \nthe patch size of the image in computer vision is variable and may be either large or small. In computer vision, \nthe patch size often must be modified for downstream tasks like target recognition, pixel-level segmentation, etc. \nThe presence of potential computing problems for ViT due to modifying patch sizes and its limited viability for \ndownstream tasks if patches remain constant has been addressed by the emergence of the Swin  Transformer45.\nSwin Transformer is a sliding-window variant of ViT can effectively tackle this problem and improve perfor-\nmance in tasks like classification, detection, and segmentation. Many of the hyperparameters typically present in \nCNNs can be manually adjusted in Swin Transformer. These include the number of network blocks, the number \nof layers within each block, and the dimensions of the input image, among others. Several studies have employed \na Swin Transformer for object  ReID39,46. However, they used an additional segmentation step that is a crucial \nprocess that involves categorizing entire regions, requiring high computational requirements and processing \ntimes, and can lead to inaccuracies that impact subsequent classification tasks. It uses a hierarchical network \nstructure like CNNs. It uses a shifted window mechanism to share pixel points in different windows by dividing \nViT sample blocks into varying sizes based on hierarchy. Swin Transformer improves the network’s \"perceptual \nfield\" and information utilization compared to the TransReID and TransReID-SSL methods used for person ReID.\nLoss metrics for person re-identifying\nIn the design of deep metric learning for person ReID, many well-known loss functions are routinely utilized. \nThese include identity loss, verification loss, and triplet loss. The person ReID is formulated as a classification \nproblem by the identity loss. When given a query image, the ReID system returns the ID of the person who is the \nfocus of the search. To determine identity loss, the cross  entropy47 function is frequently used. The verification \nloss looks for the best pair-wise arrangement of two subjects. Contrastive  loss48 and binary verification  loss29 \n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nare two other frequently used functions. The contrastive loss can be represented with a linear combination of \na pairwise distance in embedding feature space and a binary label, while binary verification loss distinguishes \nbetween positive and negative image pair sets. The approach known as triplet  loss49, considers ReID as a cluster-\ning task, according to the principle of regulating the feature distance between positive and negative pairings. To \nbe more specific, there should be a defined margin within which the positive pair is separated from the negative \npair. We notice that the majority of methods combine the aforementioned three types of  loss50,51. However, the \nissue of data imbalance has only been considered in a few  methods52,53. Focal loss has been shown to be efficient \nfor networks that prioritize learning hard-to-separate examples during training in order to tackle the issue of \ndata  imbalance54. The overall performance of a person ReID can be improved by including the focal loss in deep \nmetric learning. This prompted us to use a fusion loss function that incorporates not only cross-entropy and \nTriplet loss but also focal loss.\nProposed method\nThe proposed framework pre-processes and feeds the input query image into the RFES-ReID module. A fusion \nloss module is then added to the training procedure to get ID loss. On the other hand, the procedure in the infer-\nence mode is precisely the same, with the exception that a re-ranking optimization is used after the generation \nof the initial ID list. Figure 2 shows the framework of the proposed method.\nRegional feature extraction Swin Transformer module (RFES)\nThere are four different types of the Swin Transformer: Swin-T, Swin-S, Swin-B, and Swin-L 45. This study use \nSwin-T, which takes into consideration the uniqueness and computational difficulty of person ReID images. \nRespectively, there are 2, 2, 6, and 2 blocks on each stage. The flowchart of the network’s regional feature extrac-\ntion Swin Transformer (RFES) is shown in Fig. 3.\nA quick review of Swin Transformer\nAs mentioned before, the Swin Transformer is a ViT-based module that uses sliding windows. The architecture of \nthe Swin Transformer is shown in Fig. 4. It replaces the Multi-Head Attention mechanism (MSA) with W-MSA \nFigure 2.  General RFES-ReID method block diagram.\nFigure 3.  Regional feature extraction Swim transformer architecture.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nand SW-MAS, enhancing computational efficiency and classification accuracy through the use of restricted and \nsliding windows.\nSwin Transformer, as seen in Fig. 5, first reshapes an input of size H × W × C into a HW\nM2 × M2 × C feature \nby dividing the input into non-overlapping M × M local windows, where HW\nM2  is the total number of windows. \nThe standard self-attention (also known as local attention) is then calculated independently for each window. \nThe query, key, and value matrices (Q, K and V) are calculated for a local window feature X ∈ RM 2×C as:\nwhere PQ,PK and PV are shared projection matrices between several windows. We typically have \nQ,K,V ∈ RM 2 ×d . Thus, the self-attention mechanism computes the attention matrix in a local window as\n(1)Q = XPQ, K = XPK , V = XPV ,\nFigure 4.  The Architecture of the Swin Transformer Model.\nFigure 5.  Blocks of Swin Transformers.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nwhere B is the relevant positional encoding that can be learned. In practice, we concatenate the results for MSA \nby performing the attention function for h times in parallel, as described in Vaswani et al.40.\nFor more feature transformations, a multi-layer perceptron (MLP) with two fully connected layers and GELU \nnon-linearity between them is utilized. The LayerNorm (LN) layer is introduced prior to both the MSA module \nand the MLP module. Additionally, a residual connection is used for both modules. Nevertheless, in the case \nwhen the partition is fixed for distinct layers, there exists a lack of interconnectivity among local windows. Hence, \nto facilitate cross-window connections, a combination of regular and shifted window partitioning techniques is \n used45. Specifically, shifted window partitioning involves moving the feature by \n(⌊ M\n2\n⌋\n,\n⌊ M\n2\n⌋)\n pixels prior to the \npartitioning process. The whole procedure is calculated per:\nRegional Feature Extraction Block (RFEB)\nThe detection of local correlation and structural information may be compromised by position encoding in a \ntransformer. The Swin Transformer incorporates a shift window scheme; however, it fails to adequately encode \na significant amount of spatial context information. To tackle this issue, a proposed solution is the implementa-\ntion of a regional feature extraction block (RFEB), which is positioned prior to the Swin Transformer block, as \ndepicted in Fig. 6.\nThe RFEB first performs a conversion process wherein a set of vector features is transformed into a spatial \nfeature map. This conversion is necessary since the Swin Transformer model replaces the typical CNNs’ feature \nmaps with vectors. Consider the conversion of a token with the dimensions (B, H × W , C) into a feature map with \nthe dimensions (B, C, H, W ) as an example. This is followed by the addition of a 3 × 3 layer dilated  convolutions55 \n(dilation = 2) and a GELU activation function, and the inclusion of a residual connection to boost the spatial \nlocal feature extraction while maintaining a sizeable receptive field. The feature map is then given to the Swin \nTransformer block after being reshaped to (B, H × W , C) . Dilated convolution’s properties expand the spatial \nimage’s receptive field, allowing for the effective coding of a wide variety of contextual information at various \nscales. Dilated convolution provides the receptive field’s expansion. Unlike traditional 3 × 3 convolutions, dilated \n(2)Attention(Q,K,V ) = SoftMax\n(QK T\n√\nd\n+B\n)\nV ,\n(3)\nˆXl= W − MSA\n(\nLN\n(\nXl−1\n))\n+ Xl−1\nXl= MLP\n(\nLN\n(\nˆXl\n))\n+ ˆXl\nˆXl+1 = SW − MSA\n(\nLN\n(\nXl\n))\n+ Xl\nXl+1 = MLP\n(\nLN\n(\nˆXl+1\n))\n+ ˆXl+1\nFigure 6.  Structure of regional feature extraction block.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nconvolutions with the same kernel size have a 7 × 7 receptive field, allowing for feature resolution enhancement \nwithout sacrificing field size.\nLoss computation\nFollowing the feature generation phase, the resultant features are sent to the fusion loss stage, where three distinct \nloss functions, namely cross-entropy, triplet loss, and focal loss, are calculated. The results are then sent to a fully \nconnected (FC) layer for ID prediction. Figure 7 illustrates the presented loss computation.\nEach loss function focuses on different aspects of the learning task. The cross-entropy loss encourages correct \nclassification, the triplet loss focuses on inter-sample similarities, and the focal loss addresses class imbalance \nissues. The proposed model learns not only to accurately classify but also to capture fine-grained similarities and \neffectively handle hard and imbalanced data, which increases the model’s discriminative strength and facilitates \nthe model’s ability to learn effectively.\nCross-entropy loss Following is the definition of the cross-entropy loss function for many classes:\np i indicates the probability obtained by the i-th sample’s predicted person classification score, and yi represents \nthe true label of the i-th sample.\nTriplet loss In terms of metric learning, triplet loss is the most popular. Numerous metric learning techniques \nhave been developed to enhance the performance of the triplet loss. One of the benefits of using the triple loss \napproach is its ability to facilitate the acquisition of intricate image details throughout the learning process. Each \niteration involves the input of three paired images: an anchor picture a , a positive sample p with the same ID as a , \nand a negative sample n with a different ID. The mathematical expression for the triplet loss function is given by:\nThe Euclidean distance, da,p , is determined by the feature vectors of a and p , and da,n , in a similar manner.\nFocal loss The expression for the Focal loss function in the context of many categories is given by:\nn is the number of categories, whereas γ is a hyperparameter that has a value larger than zero. The phrase \n(\n1 − p i\n)γ \nis used to increase the weight of the loss of the hard-to-separate samples in the overall loss and decrease the \ninfluence of the easy-to-separate samples. Due to the increased loss of hard-to-separate samples during training, \nthe model is more attentive to these samples. It addresses the issue of a high number of easy-to-separate samples, \nreducing the total loss and enhancing the model’s capacity for judgment regarding hard-to-separate samples.\nThe fusion loss uses a combination of cross-entropy loss, triplet loss, and focal loss. The use of cross-entropy \nloss promotes accurate classification. The triplet loss function is used to group data together in the feature space \nand get knowledge about the similarity between these samples. Moreover, Focal loss classifies the samples in \nthe feature space by learning the interface of different feature space samples. The goal of using fusion loss is to \nimprove the network by letting different loss functions limit each other. This aids network learning of representa-\ntive characteristics. The fusion loss is expressed as:\nRe-ranking optimization\nIn the inference stage of the suggested method, re-ranking optimization is used to improve the accuracy of the \nfinal prediction of person ReID. Figure  8 shows that re-ranking with k-reciprocal  encoding56, which is a post-\nprocessing method, is done after the first list of IDs has been obtained. The proposed method uses re-ranking to \nimprove prediction accuracy while re-identifying soccer players. In our implementation the same parameters \n(4)LCE\n(\npi,yi\n)\n=− yilog pi\n(5)LT = max\n(\nda,p − da,n + marg in,0\n)\n(6)LF =\nn∑\ni=1\n(\n1 − pi\n)γ LCE\n(\npi,yi\n)\n(7)LFusion= LCE + LT + LF\nFigure 7.  Fusion loss computation.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nare used as in the original  paper56. Once the first ranked list is obtained, the top-k samples from this list are \nencoded as reciprocal neighbor features. These features are then leveraged to get k-reciprocal features. The Jac-\ncard distance is assessed after the k-reciprocal features of both images have been identified. The final distance \nis then calculated by averaging the Jaccard distance with the Manhalanobis distance of feature appearance. The \ninitial ranking list is then updated based on the final distance.\nExperimental results and performance evaluation\nDatasets and settings\nMarket-1501 and SoccerNet-V3 Re-identification are two benchmark ReID datasets that we used in our experi-\nments. The following provides brief explanations of these datasets:\nMarket-150118 contains 32,668 pedestrian images that were gathered by six campus cameras. It is separated \ninto two groups. There are 12,963 images of 751 different IDs in the training set. The testing set also includes \n19,281 images of 750 different IDs.\nSoccerNet-v3  ReID19,57 were used for additional evaluation of our experiments. This dataset consists of \n340,993 player thumbnails and images from their replays that were taken from SoccerNet videos of various events. \nThe data is split into the train, validation, test, and challenge, respectively. There are a total of 248,234 samples in \nthe training data. There are 34,989 gallery images and 11,777 query images in the test split. On the other hand, \naccording to the challenge  website19, player identity labels are created from linkages between bounding boxes \ninside an action and are thus only valid within the specified action. Since player ID tags do not stay the same \nfrom one action to another, a assigned player has a different ID for every action they are in. Because of this, only \nFigure 8.  Re-ranking procedure for Player Re-ID.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nsamples from the same activity are compared to one another throughout the assessment process. Therefore, we \njust need to compare each query sample to the gallery examples that have the same action. We exclusively train \nour networks on the train split to evaluate them on the test split.\nEvaluation metrics To evaluate the performance of the ReID technique, two widely used evaluation metrics, \nnamely Cumulative Matching Characteristic (CMC) and Mean Average Precision (mAP), are utilized. The CMC \nmetric considers ReID as a ranking issue. Therefore, we focus on reporting the cumulative matching accuracy at \nRank-1. Rank-1 is the standard accuracy at which the model generates the input identity with the greatest prob-\nability. According to Zheng et al.18, the mean average precision (mAP) considers ReID as an object retrieval issue.\nExperimental settings\nThe main challenges noted in our analysis are sample imbalance and lack of robustness. This lack of robustness \nrefers to the system’s vulnerability to variations or changes in input data, particularly when dealing with multiple-\ninput resolutions, and it can impact the system’s ability to maintain consistent performance across different \nsituations. To overcome this issue, a pre-processing phase is implemented in which IDs in the SoccerNet-v3 \ndataset’s training set with less than four images are removed. During the training process, the images used for \ntraining are subjected to various augmentation techniques, such as random horizontal flipping, random cropping, \nand random  erasing58. These techniques are used with the aim of enhancing the robustness of the model. The \ntraining parameters have been taken from the Swin Transformer paper’s settings. The batch size is configured as \n32, comprising 8 unique IDs, with each ID encapsulating 4 images. The number of windows is determined by \ndividing the original article into a grid of 4 × 4 . The input images have a size of 224 × 224 .  AdamW59 optimizer \nis employed for 120 epochs with a cosine decay learning rate scheduler and 10 epochs of linear warm-up. The \nlearning rate is initialized as 0.001, and a weight decay of 0.05 is used. The margin of the triple loss is set at 0.3. \nThe experimental running environment is the Windows 11 Home operating system. The processor is an Intel \n13th Gen Core i9-13900KF , the memory is 64GB, the graphics processing card is an Nvidia GeForce RTX 4080 \n(16 GB). Also, the Cuda, Python, and Pytorch versions are 11.3, 3.6, and 1.10.4, respectively.\nAblation studies\nTable 1 lists the results of ablation research on individual components of the proposed method. The components \ninclude the backbone (Swin-T) and the RFEB, evaluated on both the Market-1501 and SoccerNet-v3 datasets. \nStarting with the backbone, incorporating the RFEB without fusion loss and re-ranking leads to improved per-\nformance in terms of Rank-1 accuracy and mAP . Introducing the fusion loss without re-ranking further enhances \nthe results, demonstrating the significance of incorporating this component. However, the highest performance \nis achieved when both the fusion loss and re-ranking are combined, resulting in the highest Rank-1 accuracy \nand mAP values across both datasets. These findings emphasize the importance of the RFEB, fusion loss, and \nre-ranking in optimizing the proposed method, showcasing their collective impact on the accuracy and precision \nof person ReID in the Market-1501 and SoccerNet-v3 datasets.\nThe impact on fusion loss function\nTable 2 examines the impact of different loss functions on the performance of RFES-ReID, with a particular focus \non the fusion loss function. Results show that the fusion loss consistently outperforms other loss functions across \nboth datasets, Market-1501 and SoccerNet-v3. RFES-ReID utilizing the fusion loss achieves impressive Rank-1 \naccuracies of 95.10% on Market-1501 and 81.82% on SoccerNet-v3, along with the mAP values of 86.97% and \n85.02%, respectively. These findings highlight the significant impact of the Fusion loss function in enhancing the \naccuracy and precision of RFES-ReID. By effectively combining multiple loss components, the fusion loss enables \nthe model to better capture and discriminate person features, leading to superior performance compared to other \nloss functions such as Cross-entropy, Triplet, and Focal. The results underscore the importance of incorporating \nthe fusion loss function in RFES-ReID for achieving improved person ReID outcomes.\nThe proposed model uses fusion loss, including a combination of cross-entropy loss, triplet loss, and focal \nloss. The cross-entropy loss promotes correct classification. On the other hand, the triplet loss focuses on inter-\nsample similarities. The focal loss increases the weighting of the loss associated with hard-to-separate samples \nwithin the overall loss function by using the term \n(\n1 − p i\n)γ , while simultaneously decreasing the weighting of \nthe loss associated with easily-to-separate data. During the training process, the amplification of loss for hard-\nto-separate data leads to increased attention from the model toward these samples. The SoccerNet-v3 dataset is \nused to evaluate a significant hyperparameter, and afterwards, the optimal hyperparameter is chosen. The results \nof the experiment conducted on SoccerNet-v3 are shown in Fig. 9. It has been shown that the relation between \nTable 1.  Ablation testing of individual components: impact on the proposed method’s performance.\nMethods Market-1501 SoccorNet-v3\nFusion loss Re-ranking Rank-1 (%) mAP (%) Rank-1(%) mAP (%)\nBackbone (Swin-T) – – 94.15 83.00 75.34 80.16\n+ RFEB\n– – 94.73 83.54 79.63 81.39\n✓ – 95.7 87.3 81.8 85.0\n✓ ✓ 96.2 89.1 84.1 86.7\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nmAP , Rank-1, and γ is not linear and that it does not become better as γ increases. The best result is achieved \nwhen γ = 2 , hence the focal loss parameter γ = 2 is taken into consideration.\nComparison with the state-of-the-art method\nThe proposed method is compared with other methods on Market-1501 and SoccerNet-v3 datasets. The meth-\nods utilize various backbones and input sizes, with evaluation metrics including Rank-1 accuracy and mAP . \nTable 3 presents a comparison of different methods on the Market-1501 dataset, which is commonly used for \nperson ReID research. Among them,  PCB6,  ABDNet60,  SAN61,  PGFA62,  MGN7, and RGA-SC63 are based on CNN \nmethods and rest is based on transformer methods. On the Market-1501, the proposed method outperforms \nCNN-based methods, indicating that the transformer-based method outperformes the CNN-based method, \nand the use of transformers to solve the problem of person ReID is becoming more common and unavoidable. \nWe also surpass transformer-based method except TransReID-SSL64. But as it is clear from Table 3 they provide \ncompetitive results. The proposed method stands out with the Rank-1 accuracy of 96.2% and the mAP of 89.1%. \nThese results demonstrate the effectiveness of the RFES-ReID method in accurately identifying and matching \npersons in the Market-1501 dataset, positioning it among the state-of-the-art techniques for person ReID tasks.\nTable 4 provides a comparison of various methods applied to the SoccerNet-v3. The methods utilize different \nbackbones, such as CNN, ViT, DeiT, and Swin-T, and have varying input sizes. Notable observations include the \nCNN-based methods with the lowest performance, while transformer-based methods show higher accuracies. \nThe TransReID-SSL method stands out with an impressive Rank-1 accuracy of 83.8% and the mAP of 80.1%. \nHowever, the proposed method (RFES-ReID), utilizing the Swin-T backbone, performs exceptionally well with \nan 84.1% Rank-1 accuracy and the mAP of 86.7%. Overall, these results showcase the advancements made in \nvideo-based person ReID techniques for soccer-related applications.\nTables 3 and 4 describe the effect of re-ranking on the RFES-ReID method for Market-1501 and SoccerNet-\nv3. Without applying re-ranking, RFES-ReID achieves the Rank-1 accuracy of 95.7% and the mAP of 87.3% on \nMarket-1501, while on SoccerNet-v3, it attains the Rank-1 accuracy of 81.8% and the mAP of 85.0%. Meanwhile, \nthe application of re-ranking leads to notable improvements in performance. By applying re-ranking, RFES-ReID \nachieves an increased Rank-1 accuracy of 96.2% on Market-1501 (an increase of 0.5%) and 84.1% on SoccerNet-\nv3 (an increase of 2.3%). Additionally, the mAP also shows improvement, reaching 89.1% on Market-1501 (an \nincrease of 1.8%) and 86.7% on SoccerNet-v3 (an increase of 1.7%). These results highlight the positive impact \nTable 2.  The effectiveness of loss selections on RFES-ReID.\nMethods\nMarket-1501 SoccorNet-v3\nRank-1 (%) mAP (%) Rank-1 (%) mAP (%)\nCross-entropy 92.0 79.31 74.63 78.96\nTriplet 94.73 83.54 79.63 81.39\nFocal 94.89 86.78 80.06 84.25\nCross-entropy + Triplet 94.86 84.23 79.87 82.54\nCross-entropy + Focal 94.96 86.81 80.45 84.58\nTriplet + Focal 95.23 87.04 80.92 84.71\nFusion 95.7 87.3 81.8 85.0\nFigure 9.  The impact of different γ on Rank-1 (%) and mAP on SoccerNet-v3 was used to choose γ.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nof re-ranking on the RFES-ReID method, resulting in enhanced accuracy and precision in identifying and \nmatching persons in both datasets.\nThis paper introduces an innovative approach to person and soccer player ReID by utilizing the Swin Trans-\nformer as the backbone network, addressing the shortcomings associated with traditional CNNs and their \ncomputational demands. The proposed RFES method improves feature extraction precision for smaller objects \nand enhances the model’s local perception capabilities by incorporating the strengths of CNNs. Additionally, \nenhancements in the soccer player ReID network are achieved through the integration of cross-entropy loss, \ntriplet loss, and focal loss, facilitating precise classification, handling of imbalanced data, and consideration of \ninter-sample similarities and challenging-to-distinguish samples.\nThe RFES-ReID framework demonstrates competitive performance across person and soccer player ReID \nbenchmarks, specifically Market-1501 and SoccerNet-v3. The proposed method consistently outperforms \nCNN-based approaches on the Market-1501 dataset. Moreover, in comparison to various methods applied to \nSoccerNet-v3, the RFES-ReID method exhibits superior accuracy. Notably, the TransReID-SSL method shows \npromising results, but the RFES-ReID method, leveraging the Swin-T backbone, performs exceptionally well \nin terms of both Rank-1 accuracy and mAP , solidifying its position among the state-of-the-art techniques for \nperson ReID tasks and surpassing CNN-based methods.\nDiscussion on comparative performance\nThis paper introduces an innovative approach to soccer player and person ReID by utilizing the Swin Transformer \nas the backbone network, addressing the shortcomings associated with traditional CNNs and their computational \ndemands. The proposed RFES method improves feature extraction precision for smaller objects and enhances \nthe model’s local perception capabilities by incorporating the strengths of CNNs. Additionally, enhancements in \nTable 3.  Comparison with the state-of-the-art methods on Market-1501 dataset. Significant values are in bold.\nMethods Backbone Size Rank-1 (%) mAP (%)\nResNet50\nCNN\n256 × 128 88.8 71.5\nOSNet 256 × 128 94.8 84.9\nPCB6 384 × 128 93.8 81.6\nABDNet60 384 × 128 95.6 88.3\nSAN61 256 × 128 96.1 88.0\nPGFA62 256 × 128 91.2 76.8\nMGN7 384 × 128 95.7 86.9\nRGA-SC63 256 × 128 96.1 88.4\nBaseline\nViT-B/16\n256 × 128 94.7 86.8\nTransReID43 256 × 128 95.0 88.2\nTransReID-SSL64 256 × 128 96.2 91.3\nBaseline\nDeiT-B/16\n256 × 128 94.4 86.6\nTransReID43 256 × 128 94.7 88.0\nBaseline\nSwin-T\n224 × 224 94.1 86.0\nTL-TransNet +  BAR46 224 × 224 95.34 92.60\nOurs (w/o re-ranking) 224 × 224 95.7 87.3\nOurs (with re-ranking) 224 × 224 96.2 89.1\nTable 4.  Comparsion with the state-of-the-art methods on SoccerNet-v3 dataset. Significant values are in \nbold.\nMethods Backbone Size Rank-1 (%) mAP (%)\nResNet50\nCNN\n256 × 128 48.4 59.1\nOSNet 256 × 128 69.2 76.4\nBaseline ViT-B/16 256 × 128 68.2 75.7\nTransReID43 ViT-B/16 256 × 128 68.6 77.2\nTransReID-SSL44 ViT/S-16 256 × 128 83.8 80.1\nSports Re-ID65 ViT/S-16 256 × 128 81.5 86.0\nBaseline\nDeiT-B/16\n256 × 128 65.0 73.2\nTransReID43 256 × 128 65.4 74.6\nBaseline\nSwin-T\n224 × 224 75.3 80.2\nOurs (w/o re-ranking) 224 × 224 81.8 85.0\nOurs (with re-ranking) 224 × 224 84.1 86.7\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\nthe soccer player ReID network are achieved through the integration of cross-entropy loss, triplet loss, and focal \nloss, facilitating precise classification, handling of imbalanced data, and consideration of inter-sample similarities \nand challenging-to-distinguish samples.\nThe RFES-ReID framework demonstrates competitive performance across person and soccer player ReID \nbenchmarks, specifically Market-1501 and SoccerNet-v3. The proposed method consistently outperforms \nCNN-based approaches on the Market-1501 dataset. Moreover, in comparison to various methods applied to \nSoccerNet-v3, the RFES-ReID method exhibits superior accuracy. Notably, the TransReID-SSL method shows \npromising results, but the RFES-ReID method, leveraging the Swin-T backbone, performs exceptionally well \nin terms of both Rank-1 accuracy and mAP , solidifying its position among the state-of-the-art techniques for \nperson ReID tasks and surpassing CNN-based methods.\nConclusions\nThere are some important distinctions between surveillance ReID applications and player ReID in broadcast \nvideo. Strong features in the images are required because of these distinctions. Due to data availability, we chose \nto concentrate on soccer in this study, although the concepts covered here are relevant to numerous team sports. \nThis paper proposed a Regional Feature Extraction Swin Transformer (RFES) to address the soccer player ReID \nproblem. Firstly, the Swin Transformer is used as a feature extraction network to get around both the long-range \ndependencies issue of conventional CNNs and the high computational complexity of transformers. Secondly, \na regional feature extraction module is applied to extract low-dimensional feature representations. Finally, we \nintegrate three different loss functions to manage unbalanced data, highlight hard situations, and pay more atten-\ntion to hard-to-separate samples. The rank list’s quality was then raised by using re-ranking with k-reciprocal \nencoding. The results of the experiment on the Market-1501 and SoccerNet-v3 datasets show that the suggested \nmodel outperforms state-of-the-art approaches while being straightforward and efficient. For future study, we \nwill use the provided model to extract more effective features and improve additional team sports player ReID \ntasks by considering perspective difference.\nData availability\nThe datasets analyzed during the current study are available in the following public domain resources: https://  \nwww. soccer- net. org/ data, https:// zheng- lab. cecs. anu. edu. au/ Proje ct/ proje ct_ reid. html.\nCode availability\nThe utilized codes and data are available on request to enable the method proposed in the manuscript to be \nreplicated by readers.\nReceived: 30 August 2023; Accepted: 9 January 2024\nReferences\n 1. Li, G., Xu, S., Liu, X., Li, L. & Wang, C. Jersey number recognition with semi-supervised spatial transformer network. in Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops 1783–1790 (2018).\n 2. Nady, A. & Hemayed, E. E. Player identification in different sports. in VISIGRAPP (5: VISAPP) 653–660 (2021).\n 3. Liu, H. & Bhanu, B. Pose-guided R-CNN for jersey number recognition in sports. in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition Workshops 0 (2019).\n 4. Sun, Y ., Zheng, L., Deng, W . & Wang, S. Svdnet for pedestrian retrieval. in Proceedings of the IEEE International Conference on \nComputer Vision 3800–3808 (2017).\n 5. Hermans, A., Beyer, L. & Leibe, B. In defense of the triplet loss for person re-identification. arXiv preprint arXiv: 1703. 07737 (2017).\n 6. Sun, Y ., Zheng, L., Y ang, Y ., Tian, Q. & Wang, S. Beyond part models: Person retrieval with refined part pooling (and a strong \nconvolutional baseline). in Proceedings of the European Conference on Computer Vision (ECCV) 480–496 (2018).\n 7. Wang, G., Yuan, Y ., Chen, X., Li, J. & Zhou, X. Learning discriminative features with multiple granularities for person re-identi-\nfication. in MM 2018—Proceedings of the 2018 ACM Multimedia Conference 274–282 (2018). https:// doi. org/ 10. 1145/ 32405 08. \n32405 52.\n 8. Wang, G., Lai, J.-H., Liang, W . & Wang, G. Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain \nperson re-identification. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 10568–10577 \n(2020).\n 9. Chen, W ., Chen, X., Zhang, J. & Huang, K. A multi-task deep network for person re-identification. Proc. AAAI Conf. Artif. Intell. \n31, 3988–3994 (2017).\n 10. He, L., Liang, J., Li, H. & Sun, Z. Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach. \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 7073–7082 (2018).\n 11. He, L. et al. Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification. in Proceedings of the \nIEEE/CVF International Conference on Computer Vision 8450–8459 (2019).\n 12. Isobe, T. et al. Towards discriminative representation learning for unsupervised person re-identification. in Proceedings of the \nIEEE/CVF International Conference on Computer Vision 8526–8536 (2021).\n 13. Zhang, Y . et al. Graph convolution for re-ranking in person re-identification. in ICASSP , IEEE International Conference on Acoustics, \nSpeech and Signal Processing—Proceedings 2022-May, 2704–2708 (2022).\n 14. Akan, S. & Varlı, S. Use of deep learning in soccer videos analysis: Survey. Multimed. Syst. https:// doi. org/ 10. 1007/ S00530- 022- \n01027-0/ METRI CS (2022).\n 15. Leo, M. et al. Real-time multiview analysis of soccer matches for understanding interactions between ball and players. in CIVR \n2008—Proceedings of the International Conference on Content-based Image and Video Retrieval 525–534 (2008). https:// doi. org/ \n10. 1145/ 13863 52. 13864 19.\n 16. D’Orazio, T., Leo, M., Mosca, N., Spagnolo, P . & Mazzeo, P . L. A semi-automatic system for ground truth generation of soccer video \nsequences. in 6th IEEE International Conference on Advanced Video and Signal Based Surveillance, A VSS 2009 559–564 (2009). \nhttps:// doi. org/ 10. 1109/ AVSS. 2009. 69.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\n 17. D’Orazio, T. et al. An investigation into the feasibility of real-time soccer offside detection from a multiple camera system. IEEE \nTrans. Circ. Syst. Video Technol. 19, 1804–1818 (2009).\n 18. Zheng, L. et al. Scalable person re-identification: A benchmark. in Proceedings of the IEEE International Conference on Computer \nVision 1116–1124 (2015).\n 19. SoccerNet—Re-identification. https:// www. soccer- net. org/ tasks/ re- ident ifica tion.\n 20. O’Shea, K. & Nash, R. An introduction to convolutional neural networks. Int. J. Res. Appl. Sci. Eng. Technol. 10, 943–947 (2015).\n 21. Sun, Y . et al. Circle loss: A unified perspective of pair similarity optimization. in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition 6398–6407 (2020).\n 22. Chen, Y . et al. Deep attention aware feature learning for person re-identification. Pattern Recognit. 126, 108567 (2022).\n 23. Si, T., He, F ., Zhang, Z. & Duan, Y .  Hybrid contrastive learning for unsupervised person re-identification, In IEEE Trans. Multimed. \n25, 4323–4334. https:// doi. org/ 10. 1109/ TMM. 2022. 31744 14 (2023).\n 24. Wang, Z. et al. Feature erasing and diffusion network for occluded person re-identification. in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition 4754–4763 (2022).\n 25. Y ang, M. et al. Learning with twin noisy labels for visible-infrared person re-identification. in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition 14308–14317 (2022).\n 26. Wu, S. et al. An enhanced deep feature representation for person re-identification. 2016 IEEE Winter Conference on Applications \nof Computer Vision, WACV 2016 (2016). https:// doi. org/ 10. 1109/ W ACV . 2016. 74776 81.\n 27. Xiao, T., Li, H., Ouyang, W . & Wang, X. Learning deep feature representations with domain guided dropout for person re-\nidentification. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 1249–1258 (2016).\n 28. Xiao, T., Li, S., Wang, B., Lin, L. & Wang, X. Joint detection and identification feature learning for person search. in Proceedings of \nthe IEEE Conference on Computer Vision and Pattern Recognition 3415–3424 (2017).\n 29. Li, W ., Zhao, R., Xiao, T. & Wang, X. Deepreid: Deep filter pairing neural network for person re-identification. in Proceedings of \nthe IEEE Conference on Computer Vision and Pattern Recognition 152–159 (2014).\n 30. Zheng, Z., Zheng, L. & Y ang, Y . A discriminatively learned CNN embedding for person reidentification. ACM Trans. Multimed. \nComput. Commun. Appl. (TOMM) 14, 1–20 (2017).\n 31. Wu, L., Shen, C. & Hengel, A. van den. Personnet: Person re-identification with deep convolutional neural networks. arXiv preprint \narXiv: 1601. 07255 (2016).\n 32. Ding, S., Lin, L., Wang, G. & Chao, H. Deep feature learning with relative distance comparison for person re-identification. Pattern \nRecognit. 48, 2993–3003 (2015).\n 33. Cheng, D., Gong, Y ., Zhou, S., Wang, J. & Zheng, N. Person re-identification by multi-channel parts-based CNN with improved \ntriplet loss function. in Proceedings of the iEEE Conference on Computer Vision and Pattern Recognition 1335–1344 (2016).\n 34. Y ao, H. et al. Deep representation learning with part loss for person re-identification. IEEE Trans. Image Process. 28, 2860–2871 \n(2019).\n 35. Ge, Y. et al. Fd-gan: Pose-guided feature distilling gan for robust person re-identification. Adv. Neural Inf. Process. Syst. 31, \n1230–1241 (2018).\n 36. Su, C. et al. Pose-driven deep convolutional model for person re-identification. in Proceedings of the IEEE International Conference \non Computer Vision 3960–3969 (2017).\n 37. Suh, Y ., Wang, J., Tang, S., Mei, T. & Lee, K. M. Part-aligned bilinear representations for person re-identification. in Proceedings of \nthe European Conference on Computer Vision (ECCV) 402–419 (2018).\n 38. Zaremba, W ., Sutskever, I. & Vinyals, O. Recurrent neural network regularization. arXiv preprint arXiv: 1409. 2329 (2014).\n 39. Li, J., Yu, C., Shi, J., Zhang, C. & Ke, T. Vehicle re-identification method based on Swin-Transformer network. Array 16, 100255 \n(2022).\n 40. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 6000–6010 (2017).\n 41. Dosovitskiy, A. et al. An image is worth 16 × 16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.  \n11929 (2020).\n 42. Touvron, H. et al. Training data-efficient image transformers & distillation through attention. in International Conference on \nMachine Learning 10347–10357 (PMLR, 2021).\n 43. He, S. et al. Transreid: Transformer-based object re-identification. in Proceedings of the IEEE/CVF International Conference on \nComputer Vision 15013–15022 (2021).\n 44. Luo, H. et al. Self-supervised pre-training for transformer-based person re-identification. arXiv preprint arXiv: 2111. 12084 (2021).\n 45. Liu, Z. et al. Swin Transformer: Hierarchical vision transformer using shifted windows. in Proceedings of the IEEE/CVF International \nConference on Computer Vision 10012–10022 (2021).\n 46. Wang, Q. et al. Swin Transformer based on two-fold loss and background adaptation re-ranking for person re-identification. \nElectronics. 11, 1941 (2022).\n 47. Zheng, L. et al. Person re-identification in the wild. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-\ntion 1367–1376 (2017).\n 48. Deng, W . et al. Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. \nin Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 994–1003 (2018).\n 49. Yuan, Y ., Chen, W ., Y ang, Y . & Wang, Z. In defense of the triplet loss again: Learning robust person re-identification with fast \napproximated triplet loss and label distillation. in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Rec -\nognition Workshops 354–355 (2020).\n 50. Bai, Y . et al. Group-sensitive triplet embedding for vehicle reidentification. IEEE Trans. Multimed. 20, 2385–2399 (2018).\n 51. Lin, Y ., Y an, K., Du, X., Lin, Y . & Peng, Y . Unsupervised Learning Boost Person Re-identification and Real World Application. in \nProceedings—2021 IEEE International Conference on Big Data, Big Data 2021 3191–3196 (2021). https:// doi. org/ 10. 1109/ BIGDA \nTA525 89. 2021. 96719 79.\n 52. Huang, S.-K., Hsu, C.-C. & Wang, W .-Y . Person re-identification with improved performance by incorporating focal Tversky Loss \nin AGW baseline. Sensors 22, 9852 (2022).\n 53. Zhang, X., Chen, X., Sun, W . & He, X. Vehicle re-identification model based on optimized DenseNet121 with joint loss. Comput. \nMater. Continua. 67. https:// www. techs cience. com/ cmc/ v67n3/ 41646/ html (2021).\n 54. Abraham, N. & Khan, N. M. A novel focal tversky loss function with improved attention u-net for lesion segmentation. in Proceed-\nings—International Symposium on Biomedical Imaging 2019-April, 683–687 (2019).\n 55. Yu, F . & Koltun, V . Multi-scale context aggregation by dilated convolutions. in 4th International Conference on Learning Representa-\ntions, ICLR 2016—Conference Track Proceedings (2015).\n 56. Zhong, Z., Zheng, L., Cao, D. & Li, S. Re-ranking person re-identification with k-reciprocal encoding. in Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition 1318–1327 (2017).\n 57. Deliege, A. et al. Soccernet-v2: A dataset and benchmarks for holistic understanding of broadcast soccer videos. in Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition 4508–4519 (2021).\n 58. Zhong, Z., Zheng, L., Kang, G., Li, S. & Y ang, Y . Random erasing data augmentation. Proc. AAAI Conf. Artif. Intell. 34, 13001–13008 \n(2020).\n 59. Kingma, D. P . & Ba, J. L. Adam: A method for stochastic optimization. in 3rd International Conference on Learning Representations, \nICLR 2015—Conference Track Proceedings (2014) doi:https:// doi. org/ 10. 48550/ arxiv. 1412. 6980.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:1139  | https://doi.org/10.1038/s41598-024-51767-4\nwww.nature.com/scientificreports/\n 60. Chen, T. et al. Abd-net: Attentive but diverse person re-identification. in Proceedings of the IEEE/CVF International Conference on \nComputer Vision 8351–8361 (2019).\n 61. Jin, X., Lan, C., Zeng, W ., Wei, G. & Chen, Z. Semantics-aligned representation learning for person re-identification. Proc. AAAI \nConf. Artif. Intell. 34, 11173–11180 (2020).\n 62. Miao, J., Wu, Y ., Liu, P ., Ding, Y . & Y ang, Y . Pose-guided feature alignment for occluded person re-identification. in Proceedings of \nthe IEEE/CVF International Conference on Computer Vision 542–551 (2019).\n 63. Zhang, Z., Lan, C., Zeng, W ., Jin, X. & Chen, Z. Relation-aware global attention for person re-identification. in Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition 3186–3195 (2020).\n 64. Luo, H. et al. Self-Supervised pre-training for transformer-based person re-identification. (2021). https:// doi. org/ 10. 48550/ arxiv. \n2111. 12084.\n 65. Comandur, B. Sports Re-ID: Improving re-identification of players in broadcast videos of team sports. arXiv preprint arXiv: 2206. \n02373 (2022).\nAuthor contributions\nS.A. drafted the manuscript and carried out implementations and simulations for this manuscript. S.A. and S.V . \nprovided core concepts, and S.V . and M.A.N.B. proofread the manuscript and approved the final manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.A.N.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8145036697387695
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6638690233230591
    },
    {
      "name": "Transformer",
      "score": 0.626258134841919
    },
    {
      "name": "Feature extraction",
      "score": 0.6114728450775146
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5790136456489563
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.472525417804718
    },
    {
      "name": "Deep learning",
      "score": 0.45763304829597473
    },
    {
      "name": "Feature learning",
      "score": 0.4515884220600128
    },
    {
      "name": "Image retrieval",
      "score": 0.4312005341053009
    },
    {
      "name": "Machine learning",
      "score": 0.377395898103714
    },
    {
      "name": "Image (mathematics)",
      "score": 0.17443323135375977
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4101805",
      "name": "Yıldız Technical University",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I121820613",
      "name": "Louisiana State University",
      "country": "US"
    }
  ],
  "cited_by": 7
}