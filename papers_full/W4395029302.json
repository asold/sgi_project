{
  "title": "GeoFormer: An Effective Transformer-Based Siamese Network for UAV Geolocalization",
  "url": "https://openalex.org/W4395029302",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2536185421",
      "name": "Qing-Ge Li",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2108089435",
      "name": "Xiaogang Yang",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2122770842",
      "name": "Jiwei Fan",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2145052468",
      "name": "Ruitao Lü",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A1981468358",
      "name": "Bin Tang",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2108172520",
      "name": "Siyu Wang",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2230780729",
      "name": "Shuang Su",
      "affiliations": [
        "PLA Rocket Force University of Engineering"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3183574614",
    "https://openalex.org/W3092535672",
    "https://openalex.org/W3111766820",
    "https://openalex.org/W4200476543",
    "https://openalex.org/W4365516966",
    "https://openalex.org/W4388157208",
    "https://openalex.org/W6857839881",
    "https://openalex.org/W4387623802",
    "https://openalex.org/W4280490589",
    "https://openalex.org/W4384521492",
    "https://openalex.org/W4313524854",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4285246495",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4319300245",
    "https://openalex.org/W6803850277",
    "https://openalex.org/W4382467090",
    "https://openalex.org/W4312703192",
    "https://openalex.org/W4312478462",
    "https://openalex.org/W4206760693",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W4390873434",
    "https://openalex.org/W4213436832",
    "https://openalex.org/W3206077556",
    "https://openalex.org/W3205797353",
    "https://openalex.org/W4205635926",
    "https://openalex.org/W3120013642",
    "https://openalex.org/W4312854990",
    "https://openalex.org/W2997129483",
    "https://openalex.org/W3179267034",
    "https://openalex.org/W6932051444",
    "https://openalex.org/W4312597490",
    "https://openalex.org/W2799087793",
    "https://openalex.org/W4392940374",
    "https://openalex.org/W4285124116",
    "https://openalex.org/W2737260104",
    "https://openalex.org/W3106462076",
    "https://openalex.org/W2963210849",
    "https://openalex.org/W3035158519",
    "https://openalex.org/W4213144930",
    "https://openalex.org/W4221138518",
    "https://openalex.org/W4297883949",
    "https://openalex.org/W3214744507",
    "https://openalex.org/W3174800995",
    "https://openalex.org/W3115854999",
    "https://openalex.org/W3092933908",
    "https://openalex.org/W2572697301",
    "https://openalex.org/W2963474852",
    "https://openalex.org/W3081227581",
    "https://openalex.org/W3207216715",
    "https://openalex.org/W4205945937",
    "https://openalex.org/W4389065321",
    "https://openalex.org/W3120991189",
    "https://openalex.org/W4386076421",
    "https://openalex.org/W4386996906",
    "https://openalex.org/W2951019013",
    "https://openalex.org/W3196819200",
    "https://openalex.org/W4226255358",
    "https://openalex.org/W2144123711",
    "https://openalex.org/W3188333065",
    "https://openalex.org/W3203518786",
    "https://openalex.org/W3166285241",
    "https://openalex.org/W4312548398",
    "https://openalex.org/W2111308925",
    "https://openalex.org/W1532362218",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W1677409904",
    "https://openalex.org/W2117228865",
    "https://openalex.org/W2320444803",
    "https://openalex.org/W2979458572",
    "https://openalex.org/W6764177528",
    "https://openalex.org/W6767766851",
    "https://openalex.org/W6780053115",
    "https://openalex.org/W3034411221",
    "https://openalex.org/W3043075211",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W4390872507",
    "https://openalex.org/W2992077842",
    "https://openalex.org/W2432402544",
    "https://openalex.org/W2479919622",
    "https://openalex.org/W1946093182",
    "https://openalex.org/W2598199894",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W2883311563",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W2755992512",
    "https://openalex.org/W2561600829",
    "https://openalex.org/W3115267626",
    "https://openalex.org/W3100555577",
    "https://openalex.org/W4226337479",
    "https://openalex.org/W4388685310"
  ],
  "abstract": "Cross-view geolocalization of unmanned aerial vehicles (UAVs) is a challenging task due to the positional discrepancies and uncertainties in scale and distance between UAVs and satellite views. Existing transformer-based geolocalization methods mainly use encoders to mine image contextual information. However, these methods have some limitations when dealing with scale changes in cross-view images. Therefore, we present an effective transformer-based Siamese network tailored for UAV geolocalization, called GeoFormer. First, an efficient transformer feature extraction network was designed, which utilizes linear attention to reduce the computational complexity and improve the computational efficiency of the network. Among them, we designed an efficient separable perceptron module based on depthwise separable convolution, which can effectively reduce the computational cost while improving the feature representation of the network. Second, we proposed a multiscale feature aggregation module, which deeply fuses salient features at different scales through a feedforward neural network to generate global feature representations with rich semantics, which improves the model&#x0027;s ability to capture image details and represent robust features. Additionally, we designed a semantic-guided region segmentation module, which utilizes a <italic>k</italic>-modes clustering algorithm to divide the feature map into multiple regions with semantic consistency and performs feature recognition within each semantic region to improve the accuracy of image matching. Finally, we designed a hierarchical reinforcement rotation matching strategy to achieve accurate UAV geolocalization based on the retrieval results of UAV view query satellite images using SuperPoint keypoints extraction and LightGlue rotation matching. According to the experimental results, our method effectively achieves UAV geolocalization.",
  "full_text": "1 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nGeoFormer: An Effective Transformer-based Siamese \nNetwork for UA V Geo-localization \n \nQingge Li, Xiaogang Yang, Jiwei Fan, Ruitao Lu, Bin Tang, Siyu Wang, and Shuang Su \n \n  \nAbstract—Cross-view geo -localization of unmanned aerial \nvehicles (UAVs) is a challenging task due to the positional \ndiscrepancies and uncertainties in scale and distance between \nUAVs and satellite views. Existing transformer -based geo -\nlocalization methods mainly use encoders to mine image \ncontextual information. However, these methods have some \nlimitations when dealing with scale changes in cross -view images. \nTherefore, we present an effective transformer -based Siamese \nnetwork tailored for UAV geo -localization, called GeoFormer. \nFirstly, an efficient transformer feature extraction network was \ndesigned, which utilizes linear attention to reduce the \ncomputational complexity and improve the computational \nefficiency of the network.  Among them, we designed an efficient \nseparable perceptron module based on depth-wise separable \nconvolution, which can effectively reduce  the computational cost \nwhile improving the feature representation of the network . \nSecondly, we proposed a multi-scale feature aggregation module \n(MFAM), which deeply fuses salient features at different scales \nthrough a feed -forward neural network to generate global \nfeature representations with rich semantics, which improves the \nmodel's ability to capture image details and represent robust \nfeatures. Additionally, we design ed a semantic -guided region \nsegmentation module (SRSM), which utilizes a k -modes \nclustering algorithm to divide the feature map into multiple \nregions with semantic consistency and performs feature \nrecognition within each semantic region to improve the accuracy \nof image matching.  Finally, we design ed a hierarchical \nreinforcement rotation matching strategy  to achieve accurate \nUAV geo-localization based on the retrieval results of UAV view \nquery satellite images using Super Point keypoints extraction and \nLightGlue rotation matching.  According to the experimental \nresults, our method effectively achieves UAV geo-localization. \n \nIndex Terms —UAV geo -localization, cross-view image retrieval, \nheterologous scene matching, transformer, linear attention, \nSiamese network. \nI. INTRODUCTION \nNMANNED aerial vehicles (UAVs) have emerged as \nversatile and efficient tools in various application \ndomains, such as aerial surveillance [1], target \ntracking [2], [3], and disaster response [4], [5]. A pivotal task \nwithin UAV systems is geo -localization, which estimates the \ngeographic coordinates of drones in real time. Geo-\n \nManuscript received 08 January 2024; revised 02 March 2024; accepted 21 \nApril 2024. Date of publication xx April 2024; date of current version 21 \nApril 2024. (Corresponding author: Xiaogang Yang.) \nThe authors are with the College of Missile Engineering, Rocket Force  \nUniversity of Engineering, Xi’an 710038, China  (e-mail: \nlqg19950105@163.com; doctoryxg@163.com ; fjw19900619@163.com; \nlrt19880220@163.com; springgoneautumn@163.com; wsy960328@163.com; \n18843226755@163.com).  \nDigital Object Identifier \nlocalization is achieved by matching UAV and satellite \nimagery. Its applications can be broadly divided into two \ncategories: UAV target localization (UAV→Satellite) and \nUAV navigation (Satellite→UAV). Accurate geo -localization \nis paramount for ensuring the effectiveness of UAV missions. \nHowever, since UAV and satellite views are acquired under \ndifferent conditions of light, weather , and seasonal variations, \nthere are large differences in the visual detail features of the \nsame scene in the images.  In addition, t here are positional \ndifferences as well as scale and distance uncertainties between \nobjects in UAV images and satellite views due to variations in \nUAV flight altitude and shooting angles. These increase the \ndifficulty of accurate geo-localization between UAV and \nsatellite views. To address this challenge, it is necessary to \nefficiently extract salient features in the images, and identify \nand correct the deviations through invariant features obtained \nby multi-scale feature fusion to ensure that UAVs can achieve \naccurate geo -localization under various environmental \nconditions. \nThe development of deep learning has provided important \ndata, model, and algorithmic support for remote sensing image \nanalysis and applications [6], [7], [8], [9], [10], [11], and \nsignificant advancements have been achieved regarding cross -\nview geo -localization methods. Most deep -learning geo -\nlocalization methods [12], [13] utilize convolutional neural \nnetworks (CNNs) to extract image features and subsequently \nestimate the position of the UAV by matching and comparing \nthe visual features between drone and satellite images. \nHowever, there are some shortcomings to CNN -based \nmethods. The relatively weak capacity of CNNs in capturing \ncontextual information may lead to inadequate modeling of \nglobal relationships in cross -view geo -localization tasks. \nSimultaneously, operations such as pooling and convolution in \nthe CNN may diminish the resolution of images and destroy \nthe recognizable fine-grained information within the images. \nOver the past few years, the transformer [14] has been \nsuccessfully used for various computer version (CV) tasks. The \nremarkable contextual modeling capability of the transformer \ncompensates for the limitations of CNNs. At present, \ntransformer-based cross -view geo -localization technology \nmainly utilizes transformer encoders as the backbone of feature \nextraction, improving the ability of contextual feature extraction \n[15], [16], [17], [18]. Some methods use ViT [19] as the \nbackbone for extracting context-sensitive information [20], [21] \nto better adapt to image data. Although these methods have \nstrong geo-localization performance, ViT divides images into \nfixed-size blocks and then treats the relationships of all image \nblocks equally on a global scale, without distinguishing whether \nU \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nthese image blocks are from adjacent regions. Swin \nTransformer [22], [23] effectively reduces both information loss \nand optimizes computational complexity by introducing \nvariable windows and cross-window connection mechanisms. \nInspired by the Swin transformer, we proposed an effective \ntransformer-based Siamese network for UAV geo -localization, \nnamed GeoFormer. We proposed an efficient transformer \nfeature extraction network, where our designed efficient \nseparable perceptron (ESP) module reduces network \ncomputational complexity while ensuring effective extraction of \nimage features. In addition, we designed a multi -scale feature \naggregation module (MFAM), which improves the network's \nrepresentation of global features by deeply fusing salient \nfeatures from different scales and different receptive fields.  In \naddition, we design a semantic -guided region segmentation \nmodule (SRSM), which clusters the feature map by k -modes \nalgorithm to obtain multiple non -overlapping semantic \nconsistent regions, and then performs feature recognition within \nthe regions separately.  Finally, we design the hierarchical \nstructure enhanced  heterogeneous image rotation matching \nstrategy. Based on the results of UAV image query satellite \nimages, SuperPoint is used to extract keypoints and then \ncombined with LightGlue rotation matching to achieve accurate \nUAV geo-localization. \nTo summarize, our work makes the following primary \ncontributions: \n \n● We construct an effective transformer -based Siamese \nnetwork for UAV geo-localization called GeoFormer. \nWe design the ESP module to replace the MLP in the \noriginal feature extraction network to capture the \nspatial relevance and contextual information of the \nimage with lower computational complexity. In \naddition, we utilize linear attention to replace the \noriginal dot-product attention to improve the context  \nawareness and computational efficiency of the \nfeature representation. \n● We propose an MFAM module for the deep fusion of \nsalient features at different scales and different \nreceptive fields to generate a global feature \nrepresentation with rich semantic information. The \nmodule is simple and effective and improves the \nmodel's ability to capture image details and robust \nfeature representation. \n● We construct an SRSM module that utilizes the k -\nmodes clustering algorithm to segment the feature \nmap into multiple non -overlapping semantic \nconsistent regions and then recognize them separately \nwithin each sub -region, making full use of the \nsemantic features of the image to improve the \naccuracy of feature matching. \n● We designed a hierarchical reinforcement \nheterogeneous image rotation matching strategy, \nwhich utilizes the SuperPoint keypoints extraction \nalgorithm combined with LightGlue secondary \nrotation matching to improve the rotation matching \nlocalization accuracy between heterogen eous images. \nWe also construct a Cognition dataset.  The \nexperimental results on University -1652 and \nCognition datasets showed that GeoFormer \neffectively achieved UAV geo-localization. \n \nThe remainder of this article is structured as follows: We \nintroduce some related work in Section 2. We provide a \ndetailed introduction to the proposed GeoFormer approach in \nSection 3. Sections 4 and 5 give the experimental results and \nconclusions, respectively. \nII. RELATED WORK \nThis section provides a brief overview of relevant prior \nresearch, including cross-view geo-localization, transformer in \ncomputer version (CV), and heterologous scene matching. \nA. CNNs in Geo-localization \nIn recent years, with the development of deep learning, \nCNN-based geo -localization techniques have achieved \nremarkable results [24], [25], [26], [27], [28], [29], [30], [31], \n[32], [33], [34], [35], [36]. These approaches can be classified \ninto two types: Feature matching -based and image retrieval -\nbased methods. \nInitially, some feature matching -based cross -view geo -\nlocalization algorithms were developed. The design of the \nfeature extraction and matching technique is the emphasis of \nthese methods. The SeLF [37] method integrates semantic \ninformation into the L2 -Net [38] feature extraction network, \nencoding pixel semantics into their feature mappings to obtain \nbetter key points and descriptors, thereby improving the \nrobustness of local feature matching. This approach effectively \nenhanced the localization accuracy on some popular \nbenchmarks [39], [40]. DSM -Net [41] utilizes dynamic \nsimilarity to align image directions within a limited field of \nview. In another study [42], the robustness was further \nenhanced by considering the local and global properties of \naerial images based on DSM -Net. Similarly, the coarse \npositioning performance was significantly improved by \nconsidering the geometric correspondence of feature points \nbased on DSM -Net [43]. However, this approach can only \nprocess one image in each view at a time and is unable to \nsimultaneously learn features of multiple images at the same \nposition. \nAnother type of research is based on the concept of image \nretrieval [44], [45]. First, views from the same geographical \nlocation are considered as a class and, then, based on the \nimage features, the category of images with unknown \ngeographical locations is retrieved within the class set.  The \ngenerative adversarial networks (GANs) were utilized to \nperform cross -view image style conversion to a similar style, \nfollowed by image retrieval  [46]. LCM [47] simplifies the \nretrieval problem into a classification problem, achieving bi -\ndirectional matching between UAV and satellite images, and \nattaining satisfactory accuracy on University -1652 [48]. RK -\nNet [13] utilizes a unit subtraction attention module (USAM) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nto detect representative key points, and achieved good geo -\nlocalization results on typical benchmarks [48], [49], [50] by \ncomparing salient regions. LPN [51] adopts a square -ring \nfeature partition approach to learn contextual information by \nutilizing data from the environment around target buildings. \nBased on the LPN module, multi -scale block attention \n(MSBA) [52] effectively achieves geo -localization by \ncapturing the relationships between regions, enabling each \nregion to attend to different features. \nFor cross-view geo-localization tasks, due to the significant \nscale differences between images of different views, a part of \nthe research enhances the image feature representation through \nmulti-scale feature fusion. Li et al. [53] proposed a new multi-\nscale attention encoder aiming at overcoming the challenge of \nperspective and appearance differences by transforming from \nstreet-view images to aerial-view images. PaSS-KD [54] self-\nenhances the extraction and representation of cross -view \nimage features by using local and multi -scale knowledge as \nfine-grained location-dependent supervision, which effectively \nhandles the large differences in scene context and object scale \nand significantly improves image retrieval performance. For \nthe problem of significant differences in visual detail features \nbetween cross -view images, part of the research improves \nfeature discrimination through semantic information.  \nRodrigues et al. [55] solved the problem of scene changes due \nto temporal differences in cross -view geo-localization through \na semantics -driven data augmentation technique and a multi -\nscale attention network. Xue et al. [56] proposed the extraction \nof global reliable features by embedding high -level semantics \nto extract global reliable features to improve the visual \nlocalization task in large -scale environments, which improves \nthe accuracy of matching by detecting keypoints from reliable \nregions and reduces the number of unreliable features. \nB. Transformer in Geo-localization \nTransformer [14] was first applied to machine translation \ntasks in the field of natural language processing (NLP), which \nis a sequence-to-sequence autoregressive model. The ViT [19] \nmodel utilizes the classic Transformer encoder structure to \nachieve image classification tasks, marking the beginning of \nthe Transformer's application in the field of vision and \ngradually playing a role in cross -view geo -localization [57]. \nFollowing the architecture of NetVLAD [58], TransVLAD \n[15] utilizes a sparse transformer encoder to obtain global \ndescriptors. It was  further combined with DFM [59] to obtain \nmore dense and accurate matching results. L2LTR [16] \nemploys a transformer encoder as a backbone, utilizing self \nand cross attention mechanisms to emulate global dependency \nrelationships between adjacent layers, thereby enhancing the \nquality of the learned representations. GeoDTR [17] utilizes a \ntransformer encoder to separate geometric information from \nthe original features and, through a novel geometric contextual \nextraction module, it can learn the spatial correlations between \nvisual features in satellite and ground images. TransGeo [18] \nfully leverages the advantages of transformer encoder global \ninformation modeling and explicit positional encoding, \nreducing computational costs and enhancing performance. \nTransLocator [20] can simultaneously complete tasks of \ngeographic graphic localization and scene recognition using a \nSiamese network with a ViT backbone. The FSRA [21] \nemploys ViT to extract features from input images. \nSubsequently, the feature maps undergo spatial segmentation \nand alignment, demonstrating strong performance in UAV \ntarget localization and UAV navigation tasks. \nHowever, since ViT  mainly divides the input image into \nfixed-size blocks and then applies the self -attention \nmechanism for feature extraction by considering these blocks \nas elements in a sequence. This approach uniformly considers \nthe relationship between all blocks in each attention layer, but \nit does not distinguish whether these blocks come from \nneighboring regions of the image. As a result, ViT's \nprocessing is globally homogeneous in space. In contrast, \nSwin Transformer [22] introduces a hierarchical structure and \na shift window mechanism, which enhances the model's \nability to capture spatial relationships between neighboring \nregions through window sliding and offsetting. As a result, \nSwin Transformer is able to capture local features in an image \nmore effectively, while ensuring the integration of global \ninformation through cross -window connections  [60]. In \naddition, Swin Transformer improves computational \nefficiency by performing self -attention computation \nindependently within each window, enabling the model to \nprocess windows in parallel. \nC. Heterologous Scene Matching \nFinding the matching relationship between two \nheterogeneous images is the foundation of UAV geo -\nlocalization. The matching methods in UAV localization  are \nmainly divided into three categories: region -based, feature \npoint-based, and dense matching methods. The region -based \nmatching method first divides the image into different region \nblocks, and matches and locates them by comparing the \nsimilarity [61], [62], [63]. Although this method has a simple \nprinciple, it requires pre-construction of reference images with \nknown geographic information locations. The dense matching \nmethod without detectors matches by directly comparing the \nfeatures of pixels or image blocks. The COTR [64], LoFTR \n[65], and ASpanFormer [66] algorithms achieve dense \nmatching of local features based on the self -attention and \ncross-attention mechanisms. The DFM [59] algorithm adopts a \nmatching strategy from coarse to fine, utilizing geometric \ntransformations and twisted secondary matching to optimize \nthe initial matching results, thereby achieving higher matching \naccuracy. The dense matching method can better adapt to \nimages without obvious features or targets. However, such \nmethods are sensitive to image noise and difficult to match \nunder complex texture and rotation conditions. Moreover, it \nrequires a large amount of computation and has a slow \nmatching speed, making it unsuitable for UAV localization. \nThe feature-point-based matching and localization method \nextracts keypoints in image s and compares the similarity for \nmatching and localization. It usually includes steps such as \nkeypoints detection, feature description, and matching. The \nmatching methods based on feature points are mainly divided \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \ninto traditional methods and deep -learning-based methods. \nTraditional methods rely on manually designed local invariant \nfeatures and descriptors [67], [68], and typical algorithms \ninclude SIFT [69], SURF [70], ORB [71], etc. The manually \ndesigned feature points and descriptors are affected by factors \nsuch as image quality, scale changes, and perspective changes, \nand are not robust enough for complex scenes and large -scale \ndata. In recent years, with the use of convolution instead of \nSIFT feature extraction in LIFT [72], deep -learning-based \nfeature point matching methods have become mainstream \n[73], [74], [75], [76], [77]. The most successful one among \nthem is SuperPoint [78], which uses CNNs to detect keypoints \nin images with good accuracy and robustness and generates \ncorresponding descriptors. On this basis, SuperGlue [79] takes \nthe detected features and descriptors as inputs, uses graph \nneural networks to identify the cross-attention and self -\nattention between features, and then uses the Sinkhorn \nalgorithm for optimal matching. SuperGlue can achieve \neffective and robust matching by learning prior knowledge of \nscene geometry, but it also inherits the limitations of \ntransformer training, and its computational complexity \nincreases twice with the number of feature points. To address \nthis issue, LightGlue [80] designed a confidence classifier to \nadaptively adjust the depth and width of the network, thereby \nreducing the computational complexity of the model and \nimproving its matching speed. The combination of SuperPoint \nand LightGlue can achieve good matching results, but there is \nstill significant room for improvement in the rotation matching \ntask of heterogeneous scenes.  \nIII. METHOD \nThis section provides a detailed description of the proposed \ntransformer-based Siamese network for UAV geo-localization. \nFig. 1 depicts the architecture of GeoFormer, which is divided \ninto four sections: The efficient transformer feature extraction \nnetwork, the multi-scale feature aggregation module (MFAM), \nthe semantic -guided region segmentation module (SRSM) , \nand the rotation matching positioning  module. GeoFormer is \ncomposed of two branches: a UAV -view branch and a \nsatellite-view branch. These two branches concurrently \nprocess two input data streams while sharing network weights. \nThe input of GeoFormer is 224× 224-pixel paired images. Each \nimage is divided into 4 patches and input into a feature \nextraction network. The feature extraction network consists of \n4 effective transformer layers, and through the designed ESP \nmodule, contextual information can be extracted with low \ncomputational complexity. We subsequently constructed the \nMFAM module to  integrate multi -scale details and global \ninformation from different levels of feature maps, to improve the \nrobust feature representation capability. Then, we designed the \nSRSM module to divide the feature map into multiple semantic \nconsistency regions and then match them separately, improving \nthe accuracy of feature matching. In addition, based on the \nretrieval results of satellite images by drones, a two -stage \nheterogeneous image rotation matching module was constructed, \nand precise UAV geo -localization was achieved using \nhomography transformation. Finally, we established loss \nfunctions that can effectively train GeoFormer. \n \n \nFig. 1. The architecture of the GeoFormer framework. \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \nFig. 2. The structure of the efficient transformer feature extraction network. \n \nA. Efficient Transformer Feature Extraction \nWe designed an efficient transformer feature extraction \nnetwork as backbone in order to better extract the spatial \ncorrelation and contextual information of images, as shown in \nFig. 2. The feature extraction network has 4 layers, each with \n[2, 2, 6, 2] E -Swin transformer blocks. The hierarchical \nstructure of the network can better capture information at \ndifferent granularity levels; that is, lower -level layers focus on \nlocal details and fine -grained features, while higher -level \nlayers capture more global and abstract representations. The \nvisualization feature maps output by each layer of the feature \nextraction network are shown in Fig . 3. It can be seen that the \nlower levels primarily attend to the fine -textured features, \nwhile the upper layers place greater emphasis on the deeper \nsemantic features of the image, ultimately leading to the \nsegmentation of buildings, roadways, and vegetation within \nthe heatmap. Due to the distinct semantic information present \nin the feature maps output by each level, utilizing MFAM \n(detailed in Section 3.2) and SRSM (detailed in Section 3.3) to \nestablish connections and integrate information between \ndifferent levels can successfully enhance the feature \nexpression ability and matching performance. \n \n \nFig. 3.  Visualization of the feature maps: (a) shows input \nimages; (b), (c), (d), and (e) show the feature maps from layers \n1, 2, 3, and 4, respectively. \n \nWe created the E -Swin transformer block to capture the \ncontextual data and the dependencies between elements with \nlow computational burden , as shown in Fig. 2. The \nfundamental architecture of the E -Swin transformer block \nconsists of feed -forward networks and multi -head self -\nattention (MHSA) mechanisms. The crucial components are \nwindow-based MHSA (W -MHSA) and shifted window -based \nMHSA (SW -MHSA). W -MHSA segments the feature map \ninto a series of windows and then independently carries out \nMHSA computations within each window. This window \ndesign enables the model to process the windows in parallel, \nsignificantly enhancing the computational efficiency. Building \nupon W -MHSA, SW -MHSA introduces a mechanism for \nshifted windows, enhancing the ability to capture spatial \nrelationships between adjacent regions. This enables the \nmodel to effectively capture the global context and \ndependencies between distant regions. Specifically, W -MHSA \nand SW -MHSA not only possess the inductive bias \ncharacteristic of CNNs, but can also capture long -range \ndependencies and spatial relationships. Hence, they exhibit \nsignificant advantages when processing image data. In \naddition, we replaced the original dot -product attention with \nlinear attention, as shown in Fig. 4.  \n \n \nFig. 4. The attention layer in the feature extraction network. \n \nIn dot -product attention, using dot -product operations to \ncalculate attention weights can lead to weight decay or \nexplosion issues, especially when dealing with long -distance \ndependencies. Linear attention reduces this problem by using \nlinear transformations to calculate attention weights, making it \nmore suitable for handling tasks with long -distance \ndependencies. Moreover, compared to dot -product operations, \nlinear transformations have lower computational complexity \nand are more efficient in computation. The following details \nthe computation of the E-Swin transformer blocks: \n \n11ˆ W-MHSA(Norm( ))l l lx x x −−=+ , (1) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \nˆ ˆESP(Norm( ))l l lx x x=+ , (2) \n \n1ˆ SW-MHSA(Norm( ))l l lx x x+ =+ , (3) \n \n1 1 1 ˆ ˆESP(Norm( ))l l lx x x+ + +=+ , (4) \nwhere \nˆlx  and \nlx  denote the output features of layer \nl , \nNorm( )\n denotes layer normalization . \nW-MHSA( )  and \nSW-MHSA( )\n represent the window -based MHSA and \nshifted window-based MHSA, respectively. \nWe designed the efficient separate perceptron (ESP) as an \nimportant component of the E-Swin transformer block to \nextract contextual information with lower computational \ncomplexity. The ESP structure is shown in Fig . 2. Due to the \nhigh computational cost of W -MSA and SW-MSA, as well as \nthe need for self -attention calculation of the entire input \nfeature map in the E -Swin Transformer Block, it may occupy \na large amount of memory when processing large -sized \nimages. In order to mitigate the memory consumption and \ncomputational complexity of the E -Swin transformer block, \nwe devised ESP. It replaces the standard convolution \noperation with depth -wise separable convolution, including \ndepth-wise convolution and pointwise convolutions. This \nseparation significantly decreases the computational \ncomplexity while still capturing essential feature interactions. \nBy introducing depth-wise separable convolutions, the E -Swin \ntransformer block achieves higher efficiency without \nsacrificing performance. The ESP layer effectively captures \nspatial and inter -channel dependencies in a more efficient \nmanner, enabling the network to process large -scale data with \nfewer computational resources. The ESP is formulated as  \nfollows: \n \nFC(GELU(DWConv( )))out inxx = , (5) \nwhere \ninx  is the feature from the layer normalization, \noutx  is \nthe output feature of ESP, FC means fully connected layer, \nDWConv means depth-wise separable convolution, and GELU \nis the Gaussian error linear unit activation function. \nB. Multi-scale Feature Aggregation \nWe proposed the MFAM , which aims to fully utilize the \nsalient features and semantic information of the feature maps \nextracted by backbone at different scales and different \nreceptive fields to enhance the global feature representation of \nthe network.  MFAM is a simple and effective feedforward \nneural network, which is a lightweight decoder mainly \ncomposed of multilayer perceptron (MLP), as shown in Fig . 5. \nInitially, MFAM extracts the output feature maps of each layer \nof the Geoformer backbone, and unifies the channel \ndimensions of multi-level features \niF  through the MLP  layer \nto ensure that the features have the same dimensions before \nfusion, so as to improve the efficiency and effectiveness of \nfeature fusion. Different scale features represent different \nreceptive fields and semantic information. Subsequently, after \nthe ReLU non -linear activation and up -sampling operations, \nthe features become a quarter of the original and concatenated \ntogether. This process increases the model's full utilization of \nsalient features at different scales and in different receptive \nfields, thus enriching the semantic information captured.  Then, \nutilizing another MLP layer for feature deep fusion, the global \nfeature \nF  is obtained. This design allows the model to better \nintegrate information from different levels to generate global \nfeature representations  with rich semantics , which is critical \nfor improving model performance in cross -view geo -\nlocalization tasks. Finally, the model is able to utilize more \ncomprehensive and integrated information for the final \nprediction of global features \nF , which improves the accuracy \nof the prediction. The computational process of MFAM is as \nfollows: \n \nReLU[ Linear( , )( )],  i iiF C C F i= , (6) \n \nUpsample( )( ),  44\nii\nWHF F i=   , (7) \n \nLinear(4 , )(Concat( )),  iF C C F i= , (8) \n \nLinear( , )( )clsM C N F= , (9) \nwhere\nLinear( , )( )in outCC   represents a linear transformation \nlayer, \ninC  and \noutC  represent the input and output vector \ndimensions, respectively, \nReLU( )  represents the ReLU non -\nlinear activation function, \nclsN  indicates the total amount of \ncategories, and \nM  denotes the final predicted feature vector. \n \nFig. 5. The structure of the MFAM. \n \nMFAM enhances the feature representation capability by \nfusing the details and global features of feature maps extracted \nby the backbone at different scales. By fusing multi -level \nfeatures into a single vector before classification, as opposed \nto separately classifying the outputs at each level, the \ncomputational complexity of the model is reduced. Moreover, \nMFAM exhibits a certain degree of flexibility, allowing for \nthe independent adjustment of feature extraction levels and \nscales according to the requirements of a given task. MFAM \ncan fully harness semantic information from different scales \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \n \nFig. 6. The structure of SRSM. \n \nand levels, thus enhancing its perceptual and expressive \ncapabilities for targets of various scales. Compared with Swin \nTransformer, although Swin Transformer is able to generate \nfeature maps at different scales through its hierarchical design, \nwhich represent different image details and high -level \nsemantic information, the interaction and fusion between \nfeatures at different scales are not sufficient. Our MFAM \nensures that salient features with different semantics can be \ncombined more effectively by fusing these multi-scale features \nmore explicitly, thus improving the model's ability to capture \nimage details and represent robust features. \n \nC. Semantic-guided Region Segmentation \nInspired by the heatmap segmentation module  (HSM) in \nFSRA [21], we designed SRSM to segment feature maps into \nmultiple regions based on semantic information, and then \nperform feature matching within each semantic region to \nimprove the accuracy of feature matching. The main steps of \nSRSM are mainly divided into feature sorting, region \nsegmentation, region feature averaging, and feature \nreordering, as shown in Fig. 6. The input of SRSM is the \noutput of the last layer of the feature extraction network. First, \nthe average value of each slice in the feature vector is \ncomputed and then arranged in decreasing order to obtain an \nordered sequence of features. We believe that slices with \nsimilar feature values often reflect similar semantic \ninformation. Therefore, we further cluster the ordered feature \nsequence using the k-modes algorithm, and the ordered feature \nsequence can speed up the clustering. In this way, we classify \nthe feature sequences into several classes, and the number of \nslices in each class may be the same or different. Then, based \non the classification results, the feature sequence is partitioned \ninto several different region feature sequences, and the slices \nwithin each region feature sequence represent a set of visually \nand semantically similar and related feature information. \nTherefore, we consider that each region feature sequence \ncorresponds to a class of semantic information. Then, the \naverage value of each region feature sequence is computed \nand used to replace the feature value of the corresponding \nregion feature sequence.  Finally, each feature value of three \nsemantic regions is mapped back to the original feature map in \nthe initial order to obtain the segmentation result , as shown in \nFigure 7. \n \nFig. 7. Semantic region segmentation results of SRSM. \n \nSRSM improves the perception of local targets or details \nand enhances matching performance by partitioning feature \nmaps into multiple semantic regions. SRSM performs local \nfeature matching within the semantic regions corresponding to \ntwo images, which can obtain more accurate matching results. \nCompare with the HSM in FSRA [21], which arranges the \nfeature sequences in decreasing order and then divide them \ndirectly into multiple sub -sequences of the same length, and \nthen calculate the average of each sub -sequence as a \nrepresentative. This method directly divides the sequences into \nequal lengths based on the order of the sequences, which is a \nuniform division based on the position, and the lengths of the \nobtained subsequences are fixed. Our proposed SRSM \nperforms clustering using k -modes algorithm after arranging \nthe feature sequences in decreasing order and divides the \nsequences into several classes based on the similarity of \nfeature values. This method classifies the features adaptively \nbased on the similarity between feature values instead of \nenforcing equal division, and the number of slices of each \nregion feature sequence is unknown. The SRSM clusters the \nfeatures by the k -modes algorithm, which makes each cluster \nbetter reflect the similar feature information, thus improving \nthe semantic sensitivity and accuracy of feature matching. \nCompared with the uniform segmentation of HSM, SRSM \nallows different region feature sequences to have different \nlengths, and this flexibility allows the model to fit the actual \nsemantic distribution better. Ordering the feature sequences \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nfirst in SRSM can speed up the clustering of the k -modes \nalgorithm because ordered feature sequences reduce the \nnumber of iterations of the algorithm in the initial stage, \nmaking the clustering process more efficient. \nSRSM independently classifies the feature vectors of each \nregion through a classifier, obtaining classification results for \neach semantic region. The classifier first performs a linear \ntransformation on the input data. Next, normalization is \nintroduced to accelerate the convergence speed of model \ntraining while improving the robustness and generalization \nability of the model. Finally, Dropout is utilized to randomly \ndeactivate some neurons during the training process to solve \nthe overfitting problem. In addition, SRSM can flexibly adjust \nthe number of region segmentation according to the image \ntype and task requirements, endowing SRSM with flexibility \nand adaptability to different scenarios and objectives. \n \nD. Rotation Matching Positioning \nAfter obtaining satellite image s retrieved according to  the \nUAV query image, keypoint matching is utilized to obtain \naccurate UAV geo -localization results.  The UAV images are \nviewed directly downwards, the center point of the UAV \nimage represents the position  of the UAV , and the position of \nthe UAV center  point in the satellite view needs to be found \nbased on the keypoint matching relationship. A hierarchical \nreinforcement matching and localization method was designed \nto address the issue of low matching accuracy caused by the \nprevalent angular differences between UAVs and satellite \nimages. The specific process is shown in Fig. 8. \n \n \nFig. 8. The structure of keypoint matching and localization. \n \nFirstly, SuperPoint is utilized to detect the keypoints in the \nsatellite and UAV images. Then LightGlue is utilized for \nprimary matching to obtain the homography transformation \nmatrix \n1H . After that, the UAV view image is rotated to \neliminate the angular difference with the satellite view  \naccording to \n1H . Then LightGlue secondary matching is \nperformed and the keypoint matching result is rotated to the \noriginal image by \n1\n1\nH , and the corresponding homography \ntransformation matrix \n2H  is calculated. Finally, the position of \nthe UAV is obtained in the satellite view based on  the \nkeypoint transform matrix \n2H . \n \nE. Image Sequence Consistency Voting Strategy \nWhen the top retrieved target is a false image, the keypoint \nmatching localization fails to achieve UAV geo -localization \ndirectly. To address this challenge, we adopt an image \nsequence consistency voting strategy to optimize the algorithm \nand improve its robustness. We use a sequence of UAV aerial \nimages rather than a single image to predict the location of the \nUAV. To reduce the computational burden, we select at least 3 \nUAV aerial image sequences for retrieval and keypoint \nmatching. When the keypoint matching result reaches the \nthreshold criterion it is considered as a successful match. \nWhen the number of successful matches is not less than 2, we \npredict the location of the UAV based on the consistent \nmatching results. Otherwise, it is recognized that the UAV \nlocalization fails. At this time, another set of UAV aerial \nimage sequences is taken to re -locate the UAV based on the \nimage sequence consistency voting strategy to ensure that \naccurate UAV geo -localization is achieved.  The specific \nprocess is shown in Fig. 9. \n \nF. Loss Function \nFor the classification loss, the cross -entropy (CE) loss \nfunction is used. The optimization process aims to ensure that \nfeature vectors from the same geographical location are closer \ntogether. The following is the classification loss formula: \n \ncls 1 log( )\nC\niiiL y p ==−  , (10) \nwhere \nC  is the total number of categories, \np  represents the \nprediction result, \niy  represents the ground -truth label using \none-hot encoding. If the i-th category is the correct category, \nthen \niy =1, otherwise \niy =0. Therefore, only items of the \ncorrect category will be calculated in the total loss. \nIn addition, we compute the KL loss based on the \nKullback–Leibler (KL) divergence to evaluate the disparity \nbetween the ground -truth and the predicted results, using the \nfollowing formula: \n \n1\n1 2 1\n1 2\n( ) log\niN\ni\ni\ni\npKLDiv p p p p=\n= \n , (11) \n \nKL 1 2 2 1 ( ) ( )L KLDiv p p KLDiv p p=+\n , (12) \nwhere \n1p  and \n2p  represent the predicted results for the drone \nand satellite views, respectively. \nAdditionally, we utilize the triplet loss [81], [82] to \nminimize the distance between samples from different \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \ngeographical locations. The triplet loss algorithm leverages the \ndistance relation -ships among anchor, positive, and negative \nsamples for training purposes. Here, the anchor and the \npositive samples belong to the same category, while the \nnegative sample belongs to a different category. The formula \nfor the triplet loss is as follows: \n \n Triplet 1 2 1 3max ( ( , ) ( , ) ),0L d s s d s s m= − + , (13) \n \n11 2( , )d s x s x=− , (14) \nwhere \n2  denotes the 2 -norm; \n1s , \n2s , and \n3s  represent the \nanchor sample, the positive sample, and the negative sample, \nrespectively; and \nm  is a predefined hyperparameter. \nThe total loss is the sum of the classification loss, KL loss, \nand triplet loss: \n \ntotal cls KL TripletL L L L= + + . (15) \n \n \n \nFig. 9. Process of image sequence consistency voting strategy. \n \nIV. EXPERIMENTAL RESULTS AND DISCUSSION \nA. Datasets and Evaluation Metrics \nWe conducted experiment s based on University -1652 and \nCognition datasets . In this paper, we train GeoFormer using \nUniversity-1652 and perform ablation experiments. The \nGeoFormer performance is evaluated on the Cognition test \ndataset. Firstly, the satellite image is retrieved using the real -\ntime image of the UAV view to get the approximate location \nof the UAV . Then, the precise geographic location of the \nUAV is obtained using homography transformation based on \nthe matching results between the UAV and the satellite \nimages. \nUniversity-1652 includes 1652 buildings from 72 global \nuniversities. Each building is associated with an average of 1 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nsatellite view, 54 drone views, and 3.38 ground views. Here, \nwe focused on satellite and drone views. The data structure of \nUniversity-1652 is detailed in TABLE I . In the training \ndataset, there were 701 buildings from 33 universities, and a \ntotal of 38,555 images, comprising 37,854 drone views and  \n701 satellite views, were available for training purposes. The \ntest dataset comprised 951 buildings from 39 universities. The \ntraining and test datasets did not overlap. This dataset was \nused to evaluate two tasks: UAV target localization \n(UAV→Satellite) and UAV navigation (Satellite→UAV). For \nthe UAV→Satellite task, only one satellite image \nauthentically matched the drone query image. \n \nTABLE I \nDATA STRUCTURE OF UNIVERSITY-1652 DATASET. \nViews \nTraining Test \nimages classes Drone→Satellite Satellite→Drone \nimages classes images classes \nDrone 37,854 701 37,854 701 51,355 951 \nSatellite 701 701 951 951 701 701 \n \nIn addition, we constructed a dataset named Cognition, \nwhose training and test datasets are independent of each other. \nIn this paper, only the Cognition test dataset is utilized to test \nthe effectiveness of the proposed method , and an example of \nsample images is shown in Fig. 10. Cognition is a m ulti-view \nmulti-source dataset, and the data structure is shown in \nTABLE Ⅱ. Cognition dataset includes 8 UAV flight scenarios \nwith no overlapping areas in Xi'an, Shaanxi Province, and \nFengyang, Anhui Province, suburban areas, and 5 navigation \nlandmarks are selected in each scenario. The Cognition \ntraining dataset has a total of 25 navigation landmarks, each \nwith an average of 1 satellite view and 163.8 UAV view \nimages. Among them, t he UAV view images for each \nnavigation landmarks contain 108.2 visible and 55.6 infrared \nimages on average. \nThe Cognition test dataset used in this paper includes 15 \nnavigation landmarks  under 3 scenarios, each navigation \nlandmarks has 1 satellite view and 165.6 UAV view images on \naverage, where the geographic information location of the \nsatellite view images is known, and the UAV view images are \nthe real -time image sequences captured by the UAVs during \ntheir flights according to the predetermined routes. \n \nTABLE Ⅱ \nDATA STRUCTURE OF COGNITION DATASET. \nViews Training Test \nimages classes scene images classes scene \nDrone 4095 25 5 2484 15 3 \nSatellite 25 25 5 15 15 3 \n \nWe employed recall@K (R@K) [49], [50], [83] and \naverage precision (AP) [84], [85] as evaluation metrics. R@K \nfocuses on the ability of the model to find the correct location \nin the top K retrieval results. The value of K in R@K depends \non the requirements in practical applications, with smaller \nvalues of K corresponding to more stringent evaluation \ncriteria. Given a query image, if the correctly matched image \nappears in t  the top  K images in the sorted list of retrieval \nresults, this query is considered successful and the value of \nR@K is set to 1. Otherwise, R@K is set to 0. As shown in the \nfollowing equation. \n \n1 if orderRecall@K 0 otherwise\ntrue K= \n , (16) \nwhere \nordertrue  is the sequence number of the first correctly \nmatched image in the ranked list. \nAP evaluates the performance of the model over the entire \nretrieval list, in particular for the sorting accuracy of the \ncorrect matches. AP calculates the area under the Precision -\nRecall (PR) curve. Specifically, whenever a correct match is \nretrieved, the current precision is computed and the average of \nthese precision values is subsequently computed as the AP. \nThus, the AP is able to synthesize the accuracy and \ncompleteness of the model during the retrieval process. The \ncomputation of the AP is as follows. \n \n1\n( ) ( )\nn\nk\nAP P k r k\n=\n=   , (17) \n \n( ) ( ) ( 1)r k R k R k = − − , (18) \nwhere \n()Pk  and \n()Rk  represent the accuracy and recall of the \ntop-K matching results, and \n(0) 0R = . \nIn order to quantitatively analyze the results of the matching \nexperiments, the matching algorithm is evaluated using correct \nmatching points (CMP), matching accuracy (MA) and \nmatching error (ME).  CMP are pairs of matching points that \nsatisfy the following equation. \n \n22( , ) : ( ) ( ) − + − i i i iCMP x y x x y y , (19) \nwhere \n( , )iixy  is the position of the matched feature point and \n( , )iixy\n is the position of the true corresponding feature \npoint. If the distance between them is less than the accuracy \nthreshold \n , the feature point is the correct matching point.  \nMA is the percentage of feature points correctly matched to \nall feature points, calculated as follows. \n \nMA = CMP\nall\nN\nN , (20) \nwhere \nallN  is the number of all feature point pairs matched, \nand \nCMPN  is the number of feature point pairs correctly \nmatched. \nME refers to the accuracy of feature point matching , \ncalculated as follows. \n \n( )\n2\n2\n1ME ( , ) ( , ) =−  H i i i ii x y x yCMP , (21) \nwhere \n2H represents the true transformation model between \nthe two images obtained after rotation matching. ME reflects \nthe positional offset error at the pixel level of the matching \npoint. Average matching error (AME) is the average of the \nmatching error obtained from each matching when performing \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nmultiple image matching operations.  AME can be regarded as \na quantitative measure of the average performance of an image \nmatching algorithm, which effectively reflects the \ncomprehensive performance and stability of the algorithms in \ndifferent situations. \n \nB. Implementation Details \nWe utilized image augmentation techniques to alleviate the \nissue of imbalanced samples. As there was only one satellite \nimage for each category, k images were generated through \nrandom drifting, filling, cropping, coloring, and other \nenhancement techniques, where k represents the sampling rate. \nSimultaneously, k images were randomly selected from \ndifferent perspectives, corresponding to the respective satellite \nview category. The ablation experiment pro -vides a detailed \nstudy of the sampling rate k, and the experimental results \nindicated that GeoFormer performs best when k=2. \nIn the training period, the input image was adjusted from \n512× 512 pixels to 224× 224 pixels. The backbone was \nKaiming-initialized [86] based on the pretrained weights for \nImageNet1K. We employed the SGD optimizer with Nesterov \nmomentum, with a weight decay of 5×10−4 and a momentum \nvalue of 0.9. The model was trained for 200 epochs, and the \nbatch size was set to 8. The learning rate was reduced to one -\ntenth of its initial value when the training steps reached 70 or \n110 epochs. Fig. 1 1 shows the loss function used during \ntraining. The training and testing processes are conducted \nusing the PyTorch [87] platform and an Nvidia 3060 GPU. \n \n \nFig. 10. Sample images of Cognition test dataset. \n \n \n \n  \n(a) (b) \nFig. 11. Loss function variation curve. (a) shows the triplet loss, and (b) shows the total loss. \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nTABLE Ⅲ \nCOMPARISON WITH SOTA METHODS ON UNIVERSITY-1652. \nMethod Backbone Resolution Params FLOPs Drone→UAV Satellite→UAV \nR@1 (%) AP (%) R@1 (%) AP (%) \nSoft-Margin Triplet \nLoss[50] VGG-16 256× 256 138M 16G 53.21 58.03 65.62 54.47 \nUniversity-1652[48] ResNet50 256× 256 26M 3.5G 58.49 63.13 71.18 58.74 \nInstance Loss[88] ResNet50 256× 256 26M 3.5G 58.23 62.91 74.47 59.45 \nInstance Loss + \nGeM Pooling[89] ResNet50 256× 256 26M 3.5G 65.32 69.61 79.03 65.35 \nInstance Loss \n+ USAM[13] ResNet50 256× 256 48M 24G 65.63 69.68 78.32 64.87 \nLCM[47] ResNet50 256× 256 26M 3.5G 66.65 70.82 79.89 65.38 \nLPN[51] ResNet50 256× 256 26M 3.5G 74.16 77.39 85.16 73.68 \nSGM[60] Swin-T 256× 256 28M 5.9G 82.14 84.72 88.16 81.81 \nFSRA[21] Vit-S 256× 256 48.23M 18.84G 85.50 87.53 89.73 84.94 \nOurs \n(k = 2, n = 3) \nE-Swin-T 224× 224 21.68M 6.69G 87.21 89.10 90.58 87.04 \nE-Swin-S 224× 224 34.90M 11.72G 88.09 89.88 91.44 87.71 \nE-Swin-B 224× 224 61.01M 19.46G 88.16 90.03 91.87 87.92 \nE-Swin-L 224× 224 157M 37.7G 89.08 90.83 92.30 88.54 \n \nTABLE Ⅳ \nBACKBONE COMPARISON ON UNIVERSITY-1652. \nMethod Backbone Drone→Satellite Satellite→Drone \nR@1 (%) R@5 (%) R@10 (%) AP (%) R@1 (%) R@5 (%) R@10 (%) AP (%) \nGeoFormer-T E-Swin-T 87.21 95.48 96.91 89.10 90.58 93.72 95.15 87.04 \nGeoFormer-S E-Swin-S 88.09 95.90 97.14 89.88 91.44 94.72 95.58 87.71 \nGeoFormer-B E-Swin-B 88.16 96.46 97.90 90.03 91.87 95.15 96.01 87.92 \nGeoFormer-L E-Swin-L 89.08 96.83 98.09 90.83 92.30 95.29 96.29 88.54 \n \nC. Comparison to SOTA Methods \nWe compared GeoFormer with SOTA methods, including \nthose employing different loss functions (soft -margin triplet \nloss [50], University-1652 [48], instance loss [13], [88], [89], \nLCM [47], LPN [51], SGM [60] and FSRA [21]). The \ncomparative results are reported in TABLE Ⅲ. SGM [60] and \nFSRA [21] are Transformer -based approaches.  Our proposed \nmethod achieved 89.08% R@1 and 90.83% AP for \nUAV→Satellite, and 92.30% R@1 and 88.54% AP for \nSatellite→UAV. Furthermore, as the number of GeoFormer \nparameters increased, the performance continuously improved.  \nGeoFormer outperforms existing state-of-the-art methods in \nR@1 and AP metrics while reducing computational costs.  For \ninstance, compared to the FSRA, GeoFormer improved by \n1.71% R@1 and 1.57% AP in the UAV→Satellite mission, \nwhile reducing parameter count by 26.55M. In the Satellite→\nUAV mission, an increase of 0.85% R@1 and 2.1% AP were \nachieved, and the model accuracy was improved while \nsignificantly reducing the number of parameters.  Compared \nwith SGM, GeoFormer improved by 5.07% R@1 and 4.38% \nAP in the UAV→Satellite mission, while reducing parameter \ncount by 6.32M. In the Satellite→UAV mission, an increase \nof 2.42% R@1 and 5.23% AP were achieved, significantly \nimproving th e accuracy of the model while reducing the \nnumber of parameters. \n \nD. Ablation Study \n(1) Model Structure Ablation \nThe experimental results for GeoFormer -T, GeoFormer -S, \nGeoFormer-B, and GeoFormer -L on University -1652 are \nshown in TABLE Ⅳ. The test results of the trained \nGeoFormer-T, GeoFormer-S, GeoFormer-B and GeoFormer-L \non the Cognition test dataset are shown in TABLE Ⅴ. The \nbackbone of GeoFormer -T, GeoFormer-S, GeoFormer-B, and \nGeoFormer-L are E -Swin-T, E -Swin-S, E -Swin-B, and E -\nSwin-L, respectively. During the experiment, the triplet loss \n(M=0.3) and KL loss were added, with a sampling rate of k=2 \nand a region number of n=3. The input image sizes are all \n224× 224. The experimental results demonstrated that, as the \nmodel parameter quantity increased, the values of the metrics \nalso increased. \nOn University -1652, GeoFormer -L compared to the \nsmallest GeoFormer -T, GeoFormer-L demonstrated \nimprovements in the R@1 value (from 87.21% to 89.08%), \nthe R@5 value (from 95.48% to 96.83%), the R@10 value \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n(from 96.91% to 98.96%), and the AP value (from 89.10% to \n90.83%) in the Drone→Satellite task. In the Satellite→Drone \ntask, the R@1 value was improved from 90.58% to 92.30%, \nthe R@5 value improved from 93.72% to 95.29%, the R@10 \nvalue improved from 95.15% to 96.29%, and the AP value \nimproved from 87.04% to 88.54%, demonstrating overall \nimproved performance with this model. \n \nTABLE Ⅴ \nBACKBONE COMPARISON ON COGNITION DATASET. \nMethod Drone→Satellite Satellite→Drone \nR@1 (%) AP (%) R@1 (%) AP (%) \nGeoFormer-T 65.45 71.49 73.33 60.71 \nGeoFormer-S 68.11 72.09 86.67 67.23 \nGeoFormer-B 68.77 73.31 86.67 65.19 \nGeoFormer-L 76.08 79.76 93.33 73.83 \n \n(2) Key Components Ablation \nTo explore the effectiveness of ESP, MHSA, MFAM, and \nSRSM, we performed ablation experiments of key \ncomponents on GeoFormer -T. The input image size in the \nexperiment was 224 ×  224. Triplet loss (M=0.3) and KL loss \nare added during the experiments, with the sampling rate k set \nto 2 and the number of regions n=3.  We kept the original \ndesign of the GeoFormer -T unchanged as a baseline, and the \nresults of the experiments are shown in Group 5 in TABLE \nⅥ. \nFor the ESP module, by comparing the experiments in  \nGroup 1 and Group 5 , we find that after utilizing the ESP \nmodule instead of the original MLP in the backbone, the \nnumber of parameters is reduced from 29.07M to 21.68M, the \ncomputation amount is decreased from 9.36G to 6.69G, and \nthe FPS is improved from 48.89 to 53.00.  This indicates that \nthe ESP module can effectively reduce the computational \ncomplexity of the model and the computational cost, thus \nimproving computational efficiency. Meanwhile, the retrieval \nperformance of the model is significantly improved on the \nSatellite→UAV and UAV →Satellite tasks. This shows that \nthe ESP module can effectively improve the computational \nefficiency and retrieval accuracy of the model. \nFor the linear attention-based MHSA module, by comparing \nthe experiments in Group 2 and Group 5, it can be found that \ncompared with the dot -product attention -based MHSA, the \nlinear attention we adopt does not change the number of \nmodel parameters, but the computation amount of the model is \nreduced from 7.61G to 6.69G, and the FPS of the model is \nimproved from 47.94 to 53.00 . The computational amount of \nthe model is reduced and computational efficiency is \nimproved. Meanwhile, the R@1 on the UAV →satellite task is \nimproved with the adoption of linear attention in MHSA , \nwhich is crucial for the practical task of retrieving satellite \nimages based on UAV aerial images. \nAfter adding MFAM, the R@1 and AP increased by 1.04% \nand 0.89% respectively on the UAV→satellite task, and by \n1.65% and 0.14% respectively on the Satellite→UAV task. \nThe MFAM enhances the efficiency of the model in utilizing \nmulti-scale features, thereby improving the accuracy and \nrobustness of geo-localization.  \nSince the SRSM does not contain a neural network , the \nnumber of parameters and computational complexity \ncalculated in the TABLE Ⅵ are for the neural network, so the \nnumber of parameters and computational complexity  after \nremoving the SRSM are the same as in the baseline,  but the \nFPS is increased due to the reduction in computation of the \nentire code. In addition, the geo -localization performance can \nbe effectively improved by adding the SRSM. \n(3) Semantic Region Quantity Ablation \nThe number of semantic region segmentations is an \nimportant metric for GeoFormer. Experiments were performed \nto determine whether the number of semantic regions affected \nR@1 and AP, and the results are shown in Fig. 1 2. We \nperform experiments on GeoFormer-T based on the triplet loss \n(M=0.3), with a sampling rate of k=1 and input images size of \n224 ×  224. The green line represents UAV target localization \ntasks (Drone→Satellite), while the red line represents UAV \nnavigation tasks (Satellite→Drone). The optimal performance \nfor R@1 and AP was observed when the number of regions \nwas three. Furthermore, it was found that the performance of \nR@1 and AP was optimal when n is a multiple of 3. To \ndecrease the parameters of the network and computational \ncomplexity, we thus used n = 3 as the default setting in \nsubsequent experiments. \n(4) Sampling Rate Ablation \nFor every satellite image in University -1652, there are 54 \nUAV-view images. This significant disparity in sample \nquantities may lead to the model assigning a higher weight to \nthe more numerous classes during prediction, thereby \naffecting its performance. We employed a synthetic sampling \nmethod to address the problem of unbalanced dataset samples.  \n \nTABLE Ⅶ \nCOMPARISON OF SAMPLING RATES FOR DIFFERENT MODEL \nSTRUCTURES. \nMethod K Drone→Satellite Satellite→Drone \nR@1 (%) AP (%) R@1 (%) AP (%) \nGeoFormer-T 1 87.05 88.97 91.30 86.79 \n2 87.21 89.10 90.58 87.04 \nGeoFormer-S 1 87.19 89.08 90.87 86.47 \n2 88.09 89.88 91.44 87.71 \nGeoFormer-B 1 87.21 89.20 91.73 86.58 \n2 88.16 90.03 91.87 87.92 \nGeoFormer-L 1 88.39 90.34 91.87 87.03 \n2 89.08 90.83 92.30 88.54 \n \nThe sampling rate, k, can be considered as a \nhyperparameter. Under the conditions of the triplet loss \n(M=0.3) and n=3, we conducted sampling rate ablation \nexperiments using GeoFormer-T, as depicted in Fig. 13. It was \nobserved that the AP and R@1 indicators exhibited a trend of \nincreasing followed by decreasing, reaching an overall \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \noptimum at k=2. Furthermore, we trained the model by adding \nthe KL divergence loss. The AP and R@1 indicators similarly \ndisplayed a pattern of increase followed by decrease, reaching \ntheir best levels at k=2. The value of k influenced the training \ntime but had no effect on inference. Additionally, under the \nconditions of adding the KL loss, triplet loss (M=0.3), and \nn=3, we compared k across different model structures, as \npresented in TABLE Ⅶ. We observed that the performance \nin terms of R@1 and AP was optimal when k=2. For instance, \nin the Drone→Satellite task, GeoFormer -L achieved 89.08% \nR@1 and 90.83% AP at k=2, representing an improvement of \n0.69% in R@1 and 0.49% in AP compared to k=1. In the \nSatellite→Drone task, GeoFormer -L attained 92.30% R@1 \nand 88.54% AP at k=2, marking a 0.43% increase in R@1 and \n1.51% increase in AP compared to k=1, thus yielding higher \nprecision. \n \nTABLE Ⅵ \nKEY COMPONENTS ABLATION STUDY ON UNIVERSITY-1652. \nGroup ESP MHSA \n(linear) MFAM SRSM Params FLOPs FPS Drone→Satellite Satellite→Drone \nR@1 (%) AP (%) R@1 (%) AP (%) \n1 - √ √ √ 29.07M 9.36G 48.89 82.27 84.82 88.87 82.34 \n2 √ - √ √ 21.68M 7.61G 47.94 86.62 89.75 90.79 88.43 \n3 √ √ - √ 20.61M 6.07G 53.48 86.17 88.21 90.44 85.39 \n4 √ √ √ - 21.68M 6.69G 58.52 85.45 87.49 89.02 84.06 \n5 √ √ √ √ 21.68M 6.69G 53.00 87.21 89.10 90.58 87.04 \n \n \n  \nFig. 12. Variation curve of the number of semantic regions n in the Drone→Satellite and Satellite→Drone tasks.  \n \n \n  \nFig. 13. The variation curve of the sampling rate k in the Drone→Satellite and Satellite→Drone tasks.  \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n15 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n(5) Loss Function Ablation \nTo examine how the loss function affects the various model \narchitectures, we first compared the presence and absence of \nthe KL loss under the conditions of k=1, triplet loss (M=0.3), \nand n=3, as shown in TABLE Ⅷ.  \n \nTABLE Ⅷ \nCOMPARISON OF KL LOSS UNDER DIFFERENT MODEL \nSTRUCTURES. \nMethod Kl_loss UAV→Satellite Satellite→UAV \nR@1 (%) AP (%) R@1 (%) AP (%) \nGeoFormer-T - 85.99 87.99 90.87 85.46 \n√ 87.05 88.97 91.30 86.79 \nGeoFormer-S - 86.09 88.19 91.44 86.00 \n√ 87.19 89.08 90.87 86.47 \nGeoFormer-B - 87.30 89.26 91.30 86.89 \n√ 87.21 89.20 91.73 86.58 \nGeoFormer-L - 87.63 89.54 91.30 86.69 \n√ 88.39 90.34 91.87 87.03 \n \nIt was observed that the performance in terms of R@1 and \nAP was optimal when the KL loss was added. For instance, in \nthe UAV→Satellite task, GeoFormer -L achieved 88.39% \nR@1 and 90.34% AP with the addition of the KL loss, leading \nto an improvement of 0.79% in R@1 and 0.80% in AP \ncompared to when the KL loss was not used. Similarly, in the \nSatellite→UAV task, GeoFormer-L attained 91.87% R@1 and \n87.03% AP with the addition of the KL loss, an improvement \nof 0.57% in R@1 and 0.34% in AP compared to when the KL \nloss was not added, thus achieving higher accuracy. We \nemployed three strategies to improve the UAV and satellite -\nview picture matching task performance: The KL loss, the \ntriplet loss (M=0.3), and multi -sampling when n=3. TABLE \nⅨ displays the results of the ablation experiment.  \n \nTABLE Ⅸ \nEXPERIMENTAL RESULTS OF LOSS FUNCTION ABLATION \nUNDER DIFFERENT SAMPLING RATES. \nk Loss Function UAV→Satellite Satellite→UAV \nCE KL Triplet R@1 (%) AP (%) R@1 (%) AP (%) \n1 \n√ - - 81.92 84.53 86.45 80.84 \n√ √ - 85.85 87.97 91.73 85.32 \n√ - √ 85.99 87.99 90.87 85.46 \n√ √ √ 87.05 88.97 91.30 86.79 \n2 \n√ - - 79.97 82.79 86.02 79.87 \n√ √ - 83.99 86.33 87.73 82.71 \n√ - √ 86.69 88.64 90.44 86.36 \n√ √ √ 87.21 89.10 90.58 87.04 \n \nIn the UAV→Satellite  task, when k=1, using only the KL \nloss increased R@1 by 3.93% and AP by 3.44%, while using \nonly the triplet loss increased R@1 by 4.07% and AP by \n3.46%. When both the KL loss and triplet loss were used \nsimultaneously, there was a 5.13% increase in R@1 and \n4.44% increase in AP. In the Satellite→UAV task, when k=2, \nusing only the KL loss increased R@1 by 1.71% and AP by \n2.84, while using only the triplet loss led to a 4.42% increase \nin R@1 and 6.49% increase in AP. When both the KL loss and \ntriplet loss were used simultaneously, there was an 4.56% \nincrease in R@1 and 7.17% increase in AP. Therefore, the KL \nloss and triplet loss were concurrently incorporated into \nGeoFormer. \n \nE. Matching Methods Comparison \nIn order to evaluate the matching performance of the \nproposed matching algorithm, it is compared with several \npopular feature point matching methods and Transformer-\nbased matching algorithms on the Cognition dataset, including \nSIFT [69], SURF  [70], ORB  [71], LoFTR  [65], COTR  [64], \nSuperGlue [79], and LightGlue  [80]. In particular, the \nSuperGlue, LightGlue and the proposed method are further \nmatched on the basis of SuperPoint feature point extraction. \nThe correct matching point CMP, matching accuracy MA, \naverage matching error AME and matching time of each \nmatching algorithm are counted, where the accuracy threshold  \n\n of CMP is taken as 30 pixels, and the AME is the average \nof the ME obtained from 10 experiments conducted in each \nscenario (5 scenarios in total) , and the results are shown in \nTABLE Ⅹ.  \nComparing and analyzing the data in the TABLE Ⅹ, it can \nbe observed that under 5 different scenarios, our proposed \nmethod matches the highest number of feature points among \nall the compared algorithms, with a matching accuracy of \n81.67%. This reflects that it is not only able to recognize a \nlarge number of feature points, but also able to match these \npoints efficiently and accurately. Although the matching time \nis not the fastest among all the algorithms, it is a \ncomprehensive performance algorithm that ensures the \naccuracy of the matching while maintaining a fast-matching \nspeed. Therefore, it is very suitable for practical applications \nthat need to consider both speed and accuracy. In addition, the \nproposed method has a high matching accuracy with an \naverage matching error of 4.02 pixel, which is especially \nimportant for the subsequent UAV precise localization tasks. \n \nTABLE Ⅹ \nPERFORMANCE COMPARISON OF MATCHING METHODS ON \nCOGNITION DATASET. \nMethods \nallN  \nCMPN  MA/% AME/pixel Time/s \nSIFT [69] 201.4 15.8 7.67 12.45 0.14 \nSURF [70] 191.8 13 7.19 9.36 0.17 \nORB [71] 347 14.2 3.95 13.22 0.14 \nLoFTR [65] 572 320 37.79 11.22 0.21 \nCOTR [64] 93.4 63.8 66.26 13.26 47.15 \nSuperGlue [79] 181 162.2 88.01 6.35 0.18 \nLightGlue [80] 258 211 55.68 8.83 0.07 \nOurs 382 315.8 81.67 4.02 0.14 \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n16 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nV. DISCUSSION \nTo further substantiate the reliability of the proposed \nmethod, we visualized the heat maps of both GeoFormer and \nthe baseline method, as depicted in Fig. 1 4. By comparing the \nheat maps generated by the baseline and GeoFormer models \nfor drone and satellite views, it was observed that our method \nexhibited a higher focus on critical areas, particularly those \npertaining to geo -graphically referenced target structures. \nSimultaneously, GeoFormer activates the regions where \ngeographic targets are situated, as well as their adjacent areas, \nthus emphasizing global information. GeoFormer aligns more \nclosely with the perceptual processes of the human visual \nsystem; for the image to be recognized, preliminary \ndiscrimination is made by paying attention to the salient \nfeatures, following which contextual information is used for \nfurther perception. \nThe visualization of the experimental result of GeoFormer \non the University-1652 dataset is depicted in Fig. 1 5. A proper \nmatching image is indicated by a green box, while a wrong \none is shown by a red box. For the Drone→Satellite task, three \ndrone images were chosen at random from the test dataset. \nAfter that, comparable satellite photos were selected from the \nsatellite gallery dataset and ordered according to their \nsimilarity. We took the five most similar drone images out of \nthe retrieval results for each one. As each drone image \ncategory corresponds to only one satellite image, the \nlocalization result was entirely accurate, as demonstrated in \nFig. 1 5(a). For the Satellite→Drone task, we selected three \nsatellite-view images at random from the test dataset. We \ncollected similar drone -view images from the drone gallery \ndataset for each satellite -view image and ranked them based \non their similarity. We then selected the top -5 retrieval results \nfor each satellite -view image, as illustrated in Fig. 1 5(b). The \nproposed GeoFormer still yielded entirely accurate results. \nThe experimental results demonstrate that this method exhibits \nhigh top -5 accuracy in UAV target localization and UAV \nnavigation tasks, thus confirming the reliability of the \nproposed approach.  \n \nFig. 14. Comparison of baseline and proposed method for visualization of heat maps. (a) Scenario 1, (b) Scenario 2. \n \n \n(a) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n17 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \n(b) \nFig. 1 5. Visualization of top-5 results of image retrieval on University -1652 dataset. (a) Drone→Satellite task, (b) \nSatellite→Drone task. : Correctly retrieved images, : Mis-retrieved images. \n \nWe perform UAV →Satellite and Satellite→UAV image \nretrieval on the Cognition dataset, and the results are shown in \nFig. 1 6. For the UAV →Satellite task, the center point of the \nUAV view image represents the position of the UAV, since \nthe UAV view image is obtained from a front down view.  For \neach UAV view image, there is only one corresponding \nsatellite image. We retrieve the top -5 satellite images in terms \nof similarity and get exactly the right localization results.  We \nobserve that the localization ability of the model is affected \nwhen the navigation landmarks are shown in only a small part \nof the image or when large areas are occluded. The failure \ncases are shown in Fig. 1 7. In real -world scenarios, target \nbuildings may often be occluded by foreground objects, such \nas trees or other buildings, resulting in the loss of key feature \ninformation. Differences in capturing angles and distances \nmay also cause key features of the target to be only partially \ndisplayed in the image. When the key features are incomplete \ndue to occlusion or image capture angle, it does not provide \nenough information for feature extraction and recognition. In \naddition, there are a large number of similar buildings and \nstructures in urban environments, and these similar navigation \nlandmarks may have similar visual features to the target \nbuilding, increasing the difficulty of matching. Therefore, the \nmethods may have limitations in distinguishing highly similar \nobjects and may not be able to effectively distinguish subtle \ndifferences, especially when the feature extraction algorithms \nare unable to capture sufficiently rich feature information. \n \n(a) \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n18 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \n(b) \nFig. 1 6. Visualization of top -5 results of image retrieval  on Cognition dataset.  (a) Drone→Satellite task, (b) Satellite→Drone \ntask. : Correctly retrieved images, : Mis-retrieved images. \n \n \nFig. 17. The difficult and incorrect examples on Cognition dataset. : Correctly retrieved images, : Mis-retrieved images. \n \n \n \nFig. 18. Comparison of matching results. (a) Scenario 1, (b) Scenario 2. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n19 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \nFig. 19. Visualization of positioning results.  \n \nBased on the retrieval results, the approximate location of \nthe UAV can be obtained, and then the feature point matching \nalgorithm can be used to precisely locate the UAV. The \ncomparison results of the matching algorithms are shown in \nFig. 1 8, which shows that the proposed matching algorithm \nhas the least number of mismatches and the densest number of \ncorrect matching points. According to the matching results , \nthe homography transformation matrix can be obtained to \ndetermine the corresponding position of the center point of the \nUAV view in the satellite view, and obtain the precise \npositioning result of the UAV, as shown in Fig. 1 9. It can be \nseen that the localization results of the matching method \nproposed are closest to the ground truth, proving the \neffectiveness of our method. \nDuring the imaging process of UAV aerial and satellite \nimagery, the data are inevitably affected by various \nvariabilities such as illumination changes, imaging angles, \natmospheric perturbations, sensor noise, and spatial and \ntemporal variations in the features themselves  [90]. These \nvariabilities will undoubtedly have a negative impact on the \naccuracy of UAV cross -view geo -localization tasks. To \naddress these variabilities, GeoFormer effectively fuses \nfeatures at different scales through a multi-scale feature \naggregation module (MFAM), which helps the model capture \nsemantic feature information from detail to global, enhancing \nthe ability to capture image details and represent robust \nfeatures. The designed semantic-guided region segmentation \nmodule (SRSM) helps to improve the accuracy of feature \nmatching when the imaging conditions and observation angles \nchange, and reduces the possibility of mis -matching by \nmatching features within non-overlapping semantic regions. In \naddition, the designed hierarchical reinforcement rotation \nmatching method can effectively improve the matching \nlocalization accuracy in the case of large differences in \nimaging angles. Future work can further explore adaptive \nstrategies for various variabilities to enhance the performance \nof the model in more widely used scenarios. \nⅥ. CONCLUSION \nWe proposed an effective transformer -based Siamese \nnetwork, called GeoFormer, specifically designed for UAV \ncross-view geo -localization. We designed the ESP module in \nthe efficient transformer feature extraction network to reduce \nthe computational complexity while effectively extracting \nglobal features and contextual information  using linear \nattention. Furthermore, our proposed MFAM can effectively \nfuse multi -scale features and improve the robust feature \nrepresentation ability. Additionally, our designed SRSM \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n20 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \nimproves the accuracy of feature matching by dividing the \nfeature map into non -overlapping semantic regions and \nperforming feature matching within each semantic region. By \ndesigning loss functions and multiple sampling strategies, \nGeoFormer was adjusted to a better state. Experiments on the \nUniversity-1652 dataset indicated that GeoFormer exhibits \nstate-of-the-art performance. Experiments on the Cognition \ndataset validate the effectiveness of the proposed UAV \ngeolocalization method. While the proposed GeoFormer \ndemonstrated a high retrieval accuracy and strong robustness, \nthere is still room for further improvement; for example, the \nfeature extraction network of GeoFormer could be further \nsimplified to reduce computation time. Although the proposed \nGeoFormer has high retrieval accuracy and strong robustness, \nthere is still room for further improvement. In the future, more \nefficient and lightweight model structures will be the focus of \nresearch in order to better adapt to the needs of edge \ncomputing and mobile devices. Future research needs to focus \nmore on model inference speed and energy efficiency while \nmaintaining model accuracy. This includes exploring new \nhardware acceleration techniques, optimizing algorithms to \nreduce unnecessary computations, and investigating energy \nefficiency optimization strategies. Algorithm optimization \nincludes further simplifying the feature extraction network, \nexploring new model compression techniques and knowledge \ndistillation methods to maintain high accuracy while \nsignificantly reducing model parameters and inference time. \nREFERENCES \n[1] J. W. Fan, X. G. Yang, R. T. Lu, X. L. Xie, and W. P. \nLi, \"Design and Implementation of Intelligent \nInspection and Alarm Flight System for Epidemic \nPrevention,\" Drones, vol. 5, no. 3, SEP 2021. \n[2] R. Lu, X. Yang, W. Li, J. Fan, D. Li, and X. Jing, \n\"Robust Infrared Small Target Detection via \nMultidirectional Derivative-Based Weighted Contrast \nMeasure,\" IEEE Geoscience and Remote Sensing \nLetters, vol. 19, pp. 1-5, 2022. \n[3] R. Lu et al., \"Infrared Small Target Detection Based \non Local Hypergraph Dissimilarity Measure,\" IEEE \nGeoscience and Remote Sensing Letters, vol. 19, pp. \n1-5, 2022. \n[4] J. Fan, R. Lu, X. Yang, F. Gao, Q. Li, and J. Zeng, \n\"Design and Implementation of Intelligent EOD \nSystem Based on Six -Rotor UAV,\" Drones, vol. 5, \nno. 4, pp. 146, 2021. \n[5] Q. Li, X. Yang, R. Lu, J. Fan, S. Wang, and Z. Qin, \n\"VisionICE: Air -Ground Integrated Intelligent \nCognition Visual Enhancement System Based on a \nUAV,\" Drones, Article vol. 7, no. 4, APR 2023. \n[6] D. Hong  et al. , \"Cross -city matters: A multimodal \nremote sensing benchmark dataset for cross -city \nsemantic segmentation using high -resolution domain \nadaptation networks,\" Remote Sensing of \nEnvironment, vol. 299, pp. 113856, Dec 2023. \n[7] D. Hong  et al. , \"SpectralGPT: Spectral foundation \nmodel,\" arXiv preprint arXiv:2311.07113, 2023. \n[8] D. Hong, J. Yao, C. Li, D. Meng, N. Yokoya, and J. \nChanussot, \"Decoupled -and-coupled networks: Self -\nsupervised hyperspectral image super -resolution with \nsubpixel fusion,\" IEEE Transactions on Geoscience \nand Remote Sensing, vol. 61, pp. 5527812, 2023. \n[9] J. Wang, W. Li, Y. Gao, M. Zhang, R. Tao, and Q. \nDu, \"Hyperspectral and SAR image classification via \nmultiscale interactive fusion network,\" IEEE \nTransactions on Neural Networks and Learning \nSystems, vol. 34, no. 12, pp. 10823-10837, 2023. \n[10] J. Wang, W. Li, M. Zhang, R. Tao, and J. Chanussot, \n\"Remote-sensing scene classification via multistage \nself-guided separation network,\" IEEE Transactions \non Geoscience and Remote Sensing, vol. 61, pp. \n5615312, 2023. \n[11] M. Zhang, W. Li, X. Zhao, H. Liu, R. Tao, and Q. Du, \n\"Morphological transformation and spatial -logical \naggregation for tree species classification using \nhyperspectral imagery,\" IEEE Transactions on \nGeoscience and Remote Sensing, vol. 61, pp. \n5501212, 2023. \n[12] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual \nlearning for image recognition,\" in Proceedings of \nthe IEEE conference on computer vision and pattern \nrecognition, 2016, pp. 770-778. \n[13] J. L. Lin  et al. , \"Joint Representation Learning and \nKeypoint Detection for Cross -View Geo -\nLocalization,\" Ieee Transactions on Image \nProcessing, vol. 31, pp. 3780-3792, 2022. \n[14] A. Vaswani  et al. , \"Attention is all you need,\" \npresented at the Advances in neural information \nprocessing systems, 2017.  \n[15] Y. Xu  et al. , \"TransVLAD: Multi -Scale Attention -\nBased Global Descriptors for Visual Geo -\nLocalization,\" presented at the 2023 IEEE/CVF \nWINTER CONFERENCE ON APPLICATIONS OF \nCOMPUTER VISION (WACV), 2023, 2023. \nProceedings Paper.  \n[16] H. Yang, X. Lu, and Y. Zhu, \"Cross -view Geo -\nlocalization with Layer -to-Layer Transformer,\" \npresented at the ADVANCES IN NEURAL \nINFORMATION PROCESSING SYSTEMS 34 \n(NEURIPS 2021), 2021, 2021. Proceedings Paper.  \n[17] X. Zhang, X. Li, W. Sultani, Y. Zhou, and S. Wshah, \n\"Cross-view geo -localization via learning \ndisentangled geometric layout correspondence,\" in \nProceedings of the AAAI Conference on Artificial \nIntelligence, 2023, vol. 37, no. 3, pp. 3480-3488. \n[18] S. Zhu, M. Shah, C. Chen, and S. O. C. Ieee Comp, \n\"TransGeo: Transformer Is All You Need for Cross -\nview Image Geo -localization,\" presented at the 2022 \nIEEE/CVF CONFERENCE ON COMPUTER \nVISION AND PATTERN RECOGNITION (CVPR \n2022), 2022, 2022. Proceedings Paper.  \n[19] A. Dosovitskiy  et al. , \"An image is worth 16x16 \nwords: Transformers for image recognition at scale,\" \npresented at the International Conference on \nLearning Representations, 2021.  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n21 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n[20] S. Pramanick, E. M. Nowara, J. Gleason, C. D. \nCastillo, and R. Chellappa, \"Where in the World Is \nThis Image? Transformer -Based Geo -localization in \nthe Wild,\" presented at the COMPUTER VISION, \nECCV 2022, PT XXXVIII, 2022, 2022. Proceedings \nPaper.  \n[21] M. Dai, J. H. Hu, J. D. Zhuang, and E. H. Zheng, \"A \nTransformer-Based Feature Segmentation and \nRegion Alignment Method for UAV -View Geo -\nLocalization,\" Ieee Transactions on Circuits and \nSystems for Video Technology, vol. 32, no. 7, pp. \n4376-4389, Jul 2022. \n[22] Z. Liu  et al. , \"Swin transformer: Hierarchical vision \ntransformer using shifted windows,\" in Proceedings \nof the IEEE/CVF International Conference on \nComputer Vision, 2021, pp. 10012-10022. \n[23] Z. Liu  et al. , \"Swin Transformer V2: Scaling Up \nCapacity and Resolution,\" arXiv preprint \narXiv:2111.09883, 2021. \n[24] F. Deuser, K. Habel, and N. Oswald, \"Sample4Geo: \nHard Negative Sampling For Cross -View Geo -\nLocalisation,\" in Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision \n(ICCV), 2023, pp. 16847-16856. \n[25] Y. L. Guo, M. Choi, K. Li, F. Boussaid, and M. \nBennamoun, \"Soft Exemplar Highlighting for Cross -\nView Image -Based Geo -Localization,\" Ieee \nTransactions on Image Processing, vol. 31, pp. 2094-\n2105, 2022. \n[26] X. W. Zhang  et al. , \"SSA -Net: Spatial Scale \nAttention Network for Image -Based Geo -\nLocalization,\" Ieee Geoscience and Remote Sensing \nLetters, vol. 19, 2022. \n[27] Y. Zhu, B. Sun, X. Lu, and S. Jia, \"Geographic \nSemantic Network for Cross -View Image Geo -\nLocalization,\" IEEE TRANSACTIONS ON \nGEOSCIENCE AND REMOTE SENSING, Article \nvol. 60, 2022 2022. \n[28] Z. Zeng, Z. Wang, F. Yang, and S. i. Satoh, \"Geo -\nLocalization via Ground -to-Satellite Cross -View \nImage Retrieval,\" IEEE TRANSACTIONS ON \nMULTIMEDIA, Article vol. 25, pp. 2176 -2188, 2023 \n2023. \n[29] S. Zhu, T. Yang, and C. Chen, \"Revisiting street -to-\naerial view image geo -localization and orientation \nestimation,\" in Proceedings of the IEEE/CVF Winter \nConference on Applications of Computer Vision , \n2021, pp. 756-765. \n[30] G. Berton, C. Masone, B. Caputo, and S. O. C. Ieee \nComp, \"Rethinking Visual Geo -localization for \nLarge-Scale Applications,\" presented at the 2022 \nIEEE/CVF CONFERENCE ON COMPUTER \nVISION AND PATTERN RECOGNITION (CVPR \n2022), 2022, 2022. Proceedings Paper.  \n[31] Y. Shi, X. Yu, L. Liu, T. Zhang, and H. Li, \"Optimal \nfeature transport for cross -view image geo -\nlocalization,\" in Proceedings of the AAAI Conference \non Artificial Intelligence , 2020, vol. 34, no. 07, pp. \n11990-11997. \n[32] S. Zhu, T. Yang, C. Chen, and S. O. C. Ieee Comp, \n\"VIGOR: Cross -View Image Geo -localization \nbeyond One-to-one Retrieval,\" presented at the 2021 \nIEEE/CVF CONFERENCE ON COMPUTER \nVISION AND PATTERN RECOGNITION, CVPR \n2021, 2021, 2021. Proceedings Paper.  \n[33] Y. Shi, L. Liu, X. Yu, and H. Li, \"Spatial -Aware \nFeature Aggregation for Cross -View Image based \nGeo-Localization,\" presented at the ADVANCES IN \nNEURAL INFORMATION PROCESSING \nSYSTEMS 32 (NIPS 2019), 2019, 2019. Proceedings \nPaper.  \n[34] G. Berton  et al. , \"Deep Visual Geo -localization \nBenchmark,\" presented at the 2022 IEEE/CVF \nCONFERENCE ON COMPUTER VISION AND \nPATTERN RECOGNITION (CVPR 2022), 2022, \n2022. Proceedings Paper.  \n[35] S. Hu, M. Feng, R. M. H. Nguyen, G. H. Lee, and \nIeee, \"CVM-Net: Cross-View Matching Network for \nImage-Based Ground -to-Aerial Geo -Localization,\" \npresented at the 2018 IEEE/CVF CONFERENCE \nON COMPUTER VISION AND PATTERN \nRECOGNITION (CVPR), 2018, 2018. Proceedings \nPaper.  \n[36] J. Li, C. Yang, B. Qi, M. Zhu, and N. Wu, \"4SCIG: A \nFour-branch Framework to Reduce the Interference \nof Sky Area in Cross -view Image Geo -localization,\" \nIEEE Transactions on Geoscience and Remote \nSensing, pp. 1-1, 2024. \n[37] B. Fan  et al. , \"Learning Semantic -Aware Local \nFeatures for Long Term Visual Localization,\" Ieee \nTransactions on Image Processing, vol. 31, pp. 4842-\n4855, Jul 2022. \n[38] Y. Tian, B. Fan, and F. Wu, \"L2 -net: Deep learning \nof discriminative patch descriptor in euclidean \nspace,\" in Proceedings of the IEEE conference on \ncomputer vision and pattern recognition , 2017, pp. \n661-669. \n[39] Z. Zhang, T. Sattler, and D. Scaramuzza, \"Reference \npose generation for long -term visual localization via \nlearned features and view synthesis,\" International \nJournal of Computer Vision, vol. 129, pp. 821 -844, \n2021. \n[40] H. Taira  et al. , \"InLoc: Indoor visual localization \nwith dense matching and view synthesis,\" in \nProceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 2018, pp. 7199-7209. \n[41] Y. Shi, X. Yu, D. Campbell, and H. Li, \"Where am i \nlooking at? joint location and orientation estimation \nby cross -view matching,\" in Proceedings of the \nIEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2020, pp. 4064-4072. \n[42] R. Rodrigues, M. Tani, and I. C. Soc, \"Global Assists \nLocal: Effective Aerial Representations for Field of \nView Constrained Image Geo -Localization,\" in 22nd \nIEEE/CVF Winter Conference on Applications of \nComputer Vision (WACV) , Waikoloa, HI, 2022, pp. \n2694-2702, 2022. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n22 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n[43] Y. Shi, X. Yu, L. Liu, D. Campbell, P. Koniusz, and \nH. Li, \"Accurate 3-DoF Camera Geo-Localization via \nGround-to-Satellite Image Matching,\" IEEE \nTRANSACTIONS ON PATTERN ANALYSIS AND \nMACHINE INTELLIGENCE, Article vol. 45, no. 3, \npp. 2682-2697, 2023 MAR 1 2023. \n[44] T. Y. Chu, Y. M. Chen, H. Su, Z. Z. Xu, G. D. Chen, \nand A. N. Zhou, \"A news picture geo -localization \npipeline based on deep learning and street view \nimages,\" International Journal of Digital Earth, vol. \n15, no. 1, pp. 1485-1505, Dec 2022. \n[45] X. Y. Tian, J. Shao, D. Q. Ouyang, and H. T. Shen, \n\"UAV-Satellite View Synthesis for Cross -View Geo-\nLocalization,\" Ieee Transactions on Circuits and \nSystems for Video Technology, vol. 32, no. 7, pp. \n4804-4815, Jul 2022. \n[46] A. Toker, Q. Zhou, M. Maximov, and L. Leal -Taixé , \n\"Coming down to earth: Satellite -to-street view \nsynthesis for geo -localization,\" in Proceedings of the \nIEEE/CVF Conference on Computer Vision and \nPattern Recognition, 2021, pp. 6488-6497. \n[47] L. R. Ding, J. Zhou, L. X. Meng, and Z. Y. Long, \"A \nPractical Cross -View Image Matching Method \nbetween UAV and Satellite for UAV -Based Geo -\nLocalization,\" Remote Sensing, vol. 13, no. 1, Jan \n2021. \n[48] Z. Zheng, Y. Wei, and Y. Yang, \"University -1652: A \nmulti-view multi -source benchmark for drone -based \ngeo-localization,\" in Proceedings of the 28th ACM \ninternational conference on Multimedia , 2020, pp. \n1395-1403. \n[49] M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, \n\"Predicting ground -level scene layout from aerial \nimagery,\" in Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition , 2017, pp. \n867-875. \n[50] L. Liu and H. Li, \"Lending orientation to neural \nnetworks for cross -view geo -localization,\" in \nProceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition , 2019, pp. \n5624-5633. \n[51] T. Y. Wang et al., \"Each Part Matters: Local Patterns \nFacilitate Cross -View Geo -Localization,\" Ieee \nTransactions on Circuits and Systems for Video \nTechnology, vol. 32, no. 2, pp. 867-879, Feb 2022. \n[52] J. D. Zhuang, M. Dai, X. R. Y. Chen, and E. H. \nZheng, \"A Faster and More Effective Cross -View \nMatching Method of UAV and Satellite Images for \nUAV Geolocalization,\" Remote Sensing, vol. 13, no. \n19, Oct 2021. \n[53] S. Li, Z. Tu, Y. Chen, and T. Yu, \"Multi -scale \nattention encoder for street -to-aerial image geo -\nlocalization,\" CAAI Transactions on Intelligence \nTechnology, vol. 8, pp. 166-176, 2023. \n[54] S. Li, M. Hu, X. Xiao, and Z. Tu, \"Patch Similarity \nSelf-Knowledge Distillation for Cross -view Geo -\nlocalization,\" IEEE Transactions on Circuits and \nSystems for Video Technology, pp. 1-13, 2023. \n[55] R. Rodrigues and M. Tani, \"Are these from the same \nplace? seeing the unseen in cross -view image geo -\nlocalization,\" in Proceedings of the IEEE/CVF \nWinter Conference on Applications of Computer \nVision, pp. 3753-3761, 2021. \n[56] F. Xue, I. Budvytis, and R. Cipolla, \"SFD2: \nSemantic-guided Feature Detection and Description,\" \nin Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition , pp. 5206 -\n5216, 2023. \n[57] Z. Cui et al., \"A Novel Geo -Localization Method for \nUAV and Satellite Images Using Cross -View \nConsistent Attention,\" Remote Sensing, vol. 15, no. \n19, pp. 4667, 2023. \n[58] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, J. \nSivic, and Ieee, \"NetVLAD: CNN architecture for \nweakly supervised place recognition,\" presented at \nthe 2016 IEEE CONFERENCE ON COMPUTER \nVISION AND PATTERN RECOGNITION (CVPR), \n2016, 2016. Proceedings Paper.  \n[59] U. Efe, K. G. Ince, and A. Alatan, \"DFM: A \nperformance baseline for deep feature matching,\" in \nProceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition , 2021, pp. \n4284-4293. \n[60] J. D. Zhuang, X. R. Y. Chen, M. Dai, W. B. Lan, Y. \nH. Cai, and E. H. Zheng, \"A Semantic Guidance and \nTransformer-Based Matching Method for UAVs and \nSatellite Images for UAV Geo -Localization,\" Ieee \nAccess, vol. 10, pp. 34277-34287, 2022. \n[61] S. Suri and P. Reinartz, \"Mutual -Information-Based \nRegistration of TerraSAR -X and Ikonos Imagery in \nUrban Areas,\" IEEE Transactions on Geoscience and \nRemote Sensing, vol. 48, no. 2, pp. 939-949, 2010. \n[62] Y. Fang, J. Hu, C. Du, Z. Liu, and L. Zhang, \"SAR -\noptical image matching by integrating Siamese U -Net \nwith FFT correlation,\" IEEE Geoscience and Remote \nSensing Letters, vol. 19, pp. 1-5, 2021. \n[63] O. Kwon, \"Similarity measures for object matching \nin computer vision,\" University of Bolton, England, \n2016. \n[64] W. Jiang, E. Trulls, J. Hosang, A. Tagliasacchi, and \nK. M. Yi, \"Cotr: Correspondence transformer for \nmatching across images,\" in Proceedings of the \nIEEE/CVF International Conference on Computer \nVision, 2021, pp. 6207-6217. \n[65] J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, \n\"LoFTR: Detector -free local feature matching with \ntransformers,\" in Proceedings of the IEEE/CVF \nconference on computer vision and pattern \nrecognition, 2021, pp. 8922-8931. \n[66] H. Chen  et al. , \"Aspanformer: Detector -free image \nmatching with adaptive span transformer,\" in \nEuropean Conference on Computer Vision , 2022, pp. \n20-36: Springer. \n[67] C. Harris and M. Stephens, \"A Combined Corner and \nEdge Detector,\" presented at the Alvey vision \nconference, 1988.  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n23 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n[68] E. Rosten and T. Drummond, \"Machine learning for \nhigh-speed corner detection,\" in European \nConference on Computer Vision, Graz, Austria, 2006, \nvol. 3951 LNCS, pp. 430-443: Springer Verlag. \n[69] D. G. Lowe, \"Distinctive image features from scale -\ninvariant keypoints,\" International journal of \ncomputer vision, vol. 60, no. 2, pp. 91-110, 2004. \n[70] H. Bay, T. Tuytelaars, and L. V. Gool, \"SURF: \nSpeeded up robust features,\" in European Conference \non Computer Vision, 2006, vol. 3951, pp. 407-417. \n[71] E. Rublee, V. Rabaud, K. Konolige, and G. R. \nBradski, \"ORB: an efficient alternative to SIFT or \nSURF,\" in IEEE International Conference on \nComputer Vision, 2011, pp. 2564-2571. \n[72] K. M. Yi, E. Trulls, V. Lepetit, and P. Fua, \"Lift: \nLearned invariant feature transform,\" in Computer \nVision–ECCV 2016: 14th European Conference, \nAmsterdam, The Netherlands, October 11 -14, 2016, \nProceedings, Part VI 14, 2016, pp. 467-483: Springer. \n[73] M. Dusmanu et al., \"D2-net: A trainable cnn for joint \ndescription and detection of local features,\" in \nProceedings of the IEEE/CVF conference on \ncomputer vision and pattern recognition , 2019, pp. \n8092-8101: IEEE. \n[74] J. Revaud, P. Weinzaepfel, C. De Souza, and M. \nHumenberger, \"R2D2: Repeatable and Reliable \nDetector and Descriptor,\" in 33rd Conference on \nNeural Information Processing Systems (NeurIPS) , \nVancouver, CANADA, 2019, vol. 32, pp. 1 -12, LA \nJOLLA: Neural Information Processing Systems \n(Nips), 2019. \n[75] Y. Liu, Z. Shen, Z. Lin, S. Peng, H. Bao, and X. \nZhou, \"Gift: Learning transformation -invariant dense \nvisual descriptors via group cnns,\" Advances in \nNeural Information Processing Systems, vol. 32, \n2019. \n[76] M. Tyszkiewicz, P. Fua, and E. Trulls, \"DISK: \nLearning local features with policy gradient,\" \nAdvances in Neural Information Processing Systems, \nvol. 33, pp. 14254-14265, 2020. \n[77] Z. Luo  et al. , \"Aslfeat: Learning local features of \naccurate shape and localization,\" in Proceedings of \nthe IEEE/CVF conference on computer vision and \npattern recognition, 2020, pp. 6589-6598. \n[78] D. DeTone, T. Malisiewicz, and A. Rabinovich, \n\"Superpoint: Self -supervised interest point detection \nand description,\" in Proceedings of the IEEE \nconference on computer vision and pattern \nrecognition workshops, 2018, pp. 224-236. \n[79] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. \nRabinovich, \"Superglue: Learning feature matching \nwith graph neural networks,\" in Proceedings of the \nIEEE/CVF conference on computer vision and \npattern recognition, 2020, pp. 4938-4947. \n[80] P. Lindenberger, P. -E. Sarlin, and M. Pollefeys, \n\"LightGlue: Local Feature Matching at Light Speed,\" \narXiv preprint arXiv:2306.13643, 2023. \n[81] S. Cai, Y. Guo, S. Khan, J. Hu, and G. Wen, \n\"Ground-to-aerial image geo -localization with a hard \nexemplar reweighting triplet loss,\" in Proceedings of \nthe IEEE/CVF International Conference on \nComputer Vision, 2019, pp. 8391-8400. \n[82] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, \"End -to-\nend comparative attention networks for person re -\nidentification,\" IEEE transactions on image \nprocessing, vol. 26, no. 7, pp. 3492-3506, 2017. \n[83] N. N. Vo and J. Hays, \"Localizing and orienting \nstreet views using overhead imagery,\" in European \nconference on computer vision , 2016, pp. 494 -509: \nSpringer. \n[84] T.-Y. Lin, Y. Cui, S. Belongie, and J. Hays, \n\"Learning deep representations for ground -to-aerial \ngeolocalization,\" in Proceedings of the IEEE \nconference on computer vision and pattern \nrecognition, 2015, pp. 5007-5015. \n[85] Y. Tian, C. Chen, and M. Shah, \"Cross -view image \nmatching for geo -localization in urban \nenvironments,\" in Proceedings of the IEEE \nConference on Computer Vision and Pattern \nRecognition, 2017, pp. 3608-3616. \n[86] K. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep \ninto rectifiers: Surpassing human -level performance \non imagenet classification,\" in Proceedings of the \nIEEE international conference on computer vision , \n2015, pp. 1026-1034. \n[87] A. Paszke et al., \"Pytorch: An imperative style, high -\nperformance deep learning library,\" Advances in \nneural information processing systems, vol. 32, 2019. \n[88] Z. Zheng, L. Zheng, M. Garrett, Y. Yang, M. Xu, and \nY.-D. Shen, \"Dual -path convolutional image -text \nembeddings with instance loss,\" ACM Transactions \non Multimedia Computing, Communications, and \nApplications (TOMM), vol. 16, no. 2, pp. 1-23, 2020. \n[89] F. Radenović, G. Tolias, and O. Chum, \"Fine -tuning \nCNN image retrieval with no human annotation,\" \nIEEE transactions on pattern analysis and machine \nintelligence, vol. 41, no. 7, pp. 1655-1668, 2018. \n[90] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, \n\"An augmented linear mixing model to address \nspectral variability for hyperspectral unmixing,\" \nIEEE Transactions on Image Processing, vol. 28, no. \n4, pp. 1923-1938, 2018. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n24 \nGeoFormer: An Effective Transformer-based Siamese Network for UAV Geo-localization \n \n \nQingge Li received the M.S. degree in \nXi’an Shiyou  University, China, in \n2020. She is currently pursuing the \nPh.D. degree in  the College of Missile \nEngineering, Rocket Force University \nof Engineering, Xi'an, China.  \nHer research interests include visual \nnavigation, object detection and image \nprocessing. \n \n \n \n \nXiaogang Yang was born in Xi’an, \nShaanxi, China, in 1978. He received \nthe Ph.D. degree in control science \nfrom the Rocket Force University of \nEngineering, Xi’an, in 2006.  \nHe is currently a Faculty Member \nwith the Department of Control \nEngineering, Rocket Force University \nof Engineering. He is the author of 90 \narticles and 25 inventions. His research \ninterests include precision guidance \nand image processing. \n \n \n \nJiwei Fan  was born in Shulan, Jilin, \nPRC in 1990. He received the Ph.D. \ndegree in the PLA Rocket Force \nUniversity of Engineering, Xi'an, \nChina.  \nHis research interests include image \nprocessing, machine learning, \nprecision guidance, UAV target \ndetection and tracking. \n \n \n \n \n \nRuitao Lu received the Ph.D. degree \nin control science from the National \nUniversity of Defense Technology, \nChangsha, China, in 2016.  \nHe is currently a Faculty Member \nwith the Department of Control \nEngineering, Rocket Force University \nof Engineering, Xi’an, China. His \ncurrent research interests include \npattern recognition, image processing, \nand machine learning.  \n \n \n \n \n \nBin Tang  was born in Zhangjiajie, \nHunan, PRC in 2000. He is currently \npursuing the M.S. degree in the PLA \nRocket Force University of \nEngineering, Xi'an, China.  \nHis research interests include \nprecision guidance and image \nprocessing. \n \n \n \n \n \n \n \nSiyu Wang received the M.S. degree \nin Xi’an Shiyou University, China, in \n2021. He is currently pursuing the \nPh.D. degree in the PLA Rocket Force \nUniversity of Engineering, Xi'an, \nChina.  \nHis research interests include \npattern recognition, image processing, \nand machine learning. \n \n \n \n \nShuang Su received the M.S. degree \nin Northeast Electric Power University, \nJilin, China, in 2020. She is currently \npursuing the Ph.D. degree in the PLA \nRocket Force University of \nEngineering, Xi'an, China.  \nHer research interests include image \nprocessing, machine learning, \nprecision guidance and visual \nnavigation. \n \n \n \n \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3392812\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6374703645706177
    },
    {
      "name": "Transformer",
      "score": 0.5124171376228333
    },
    {
      "name": "Electrical engineering",
      "score": 0.23102322220802307
    },
    {
      "name": "Voltage",
      "score": 0.14886316657066345
    },
    {
      "name": "Engineering",
      "score": 0.12096211314201355
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801618472",
      "name": "PLA Rocket Force University of Engineering",
      "country": "CN"
    }
  ]
}