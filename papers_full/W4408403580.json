{
  "title": "LimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations",
  "url": "https://openalex.org/W4408403580",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5115940083",
      "name": "Ibrahim Al Azher",
      "affiliations": [
        "Northern Illinois University"
      ]
    },
    {
      "id": "https://openalex.org/A5067605207",
      "name": "Venkata Devesh Reddy Seethi",
      "affiliations": [
        "Northern Illinois University"
      ]
    },
    {
      "id": "https://openalex.org/A5065625279",
      "name": "Akhil Pandey Akella",
      "affiliations": [
        "Northern Illinois University"
      ]
    },
    {
      "id": "https://openalex.org/A5065954205",
      "name": "Hamed Alhoori",
      "affiliations": [
        "Northern Illinois University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2754549406",
    "https://openalex.org/W3018174791",
    "https://openalex.org/W4210446660",
    "https://openalex.org/W4394966852",
    "https://openalex.org/W2998199475",
    "https://openalex.org/W3172139812",
    "https://openalex.org/W2147988994",
    "https://openalex.org/W2160692068",
    "https://openalex.org/W2129647599",
    "https://openalex.org/W3098562592",
    "https://openalex.org/W2601243251",
    "https://openalex.org/W2271374128",
    "https://openalex.org/W3033889636",
    "https://openalex.org/W3084476365",
    "https://openalex.org/W2128488978",
    "https://openalex.org/W3156614760",
    "https://openalex.org/W2047604680",
    "https://openalex.org/W4386801024",
    "https://openalex.org/W4385572056",
    "https://openalex.org/W4391092830",
    "https://openalex.org/W2080190461",
    "https://openalex.org/W2168065722",
    "https://openalex.org/W3046874645",
    "https://openalex.org/W4200318245",
    "https://openalex.org/W3123004723",
    "https://openalex.org/W2963948106"
  ],
  "abstract": "The limitations sections of scientific articles play a crucial role in highlighting the boundaries and shortcomings of research, thereby guiding future studies and improving research methods. Analyzing these limitations benefits researchers, reviewers, funding agencies, and the broader academic community. We introduce LimTopic, a strategy where Topic generation in Limitation sections in scientific articles with Large Language Models (LLMs). Here, each topic contains the title and Topic Summary. This study focuses on effectively extracting and understanding these limitations through topic modeling and text summarization, utilizing the capabilities of LLMs. We extracted limitations from research articles and applied an LLM-based topic modeling integrated with the BERtopic approach to generate a title for each topic and Topic Sentences. To enhance comprehension and accessibility, we employed LLM-based text summarization to create concise and generalizable summaries for each topic Topic Sentences and produce a Topic Summary. Our experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and integrating BERTopic with LLM to generate topics, titles, and a topic summary. We also experimented with various LLMs with BERTopic for topic modeling and various LLMs for text summarization tasks. Our results showed that the combination of BERTopic and GPT 4 performed the best in terms of silhouette and coherence scores in topic modeling, and the GPT4 summary outperformed other LLM tasks as a text summarizer.",
  "full_text": "LimTopic: LLM-based Topic Modeling and Text Summarization\nfor Analyzing Scientific Articles limitations\nIbrahim Al Azher\nNorthern Illinois University\nDekalb, Dekalb, IL, USA\niazher1@niu.edu\nVenkata Devesh Reddy\nNorthern Illinois University\nDekalb, IL, USA\ndseethi@niu.edu\nHamed Alhoori\nNorthern Illinois University\nDekalb, IL, USA\nalhoori@niu.edu\nAkhil Pandey Akella\nNorthwestern University\nEvanston, IL, USA\nakhilpandey.akella@kellogg.northwestern.edu\nABSTRACT\nThe ‚Äúlimitations‚Äù sections of scientific articles play a crucial role in\nhighlighting the boundaries and shortcomings of research, thereby\nguiding future studies and improving research methods. Analyzing\nthese limitations benefits researchers, reviewers, funding agencies,\nand the broader academic community. We introduceLimTopic, a\nstrategy where Topic generation in Limitation sections in scien-\ntific articles with Large Language Models (LLMs). Here, each topic\ncontains the title and ‚ÄòTopic Summary. ‚Äô This study focuses on effec-\ntively extracting and understanding these limitations through topic\nmodeling and text summarization, utilizing the capabilities of LLMs.\nWe extracted limitations from research articles and applied an LLM-\nbased topic modeling integrated with the BERtopic approach to\ngenerate a title for each topic and ‚ÄòTopic Sentences. ‚Äô To enhance\ncomprehension and accessibility, we employed LLM-based text sum-\nmarization to create concise and generalizable summaries for each\ntopic‚Äôs Topic Sentences and produce a ‚ÄòTopic Summary. ‚Äô Our ex-\nperimentation involved prompt engineering, fine-tuning LLM and\nBERTopic, and integrating BERTopic with LLM to generate topics, ti-\ntles, and a topic summary. We also experimented with various LLMs\nwith BERTopic for topic modeling and various LLMs for text summa-\nrization tasks. Our results showed that the combination of BERTopic\nand GPT 4 performed the best in terms of silhouette and coherence\nscores in topic modeling, and the GPT4 summary outperformed\nother LLM tasks as a text summarizer. Our code and dataset are avail-\nable at https://github.com/IbrahimAlAzhar/LimTopic/tree/master.\nKEYWORDS\nResearch Limitations, Limitations sections, Large Language Models,\nInformation Extraction, Science of Science\n1 INTRODUCTION\nExamining the limitations sections of research articles offers nu-\nmerous benefits to researchers, institutions, and policymakers [21].\nThis analysis helps researchers learn from past shortcomings, iden-\ntify new research avenues, and set realistic project expectations,\nultimately fostering a more productive research environment across\ndiverse fields [18]. Early-career researchers, in particular, can ac-\ncelerate their learning by studying these sections [32]. They can\ndevelop novel approaches that advance their fields by uncover-\ning potential pitfalls and promising research gaps. Experienced\nresearchers can also gain deeper insights into their own research\nlimitations, which may lead to methodological innovations and\nthe development of new approaches that overcome existing short-\ncomings [1]. Recognizing these limitations can also catalyze cross-\ninstitutional and interdisciplinary collaborations to tackle complex\nissues [4, 12, 17].\nFurthermore, Editors and reviewers can expedite the review\nprocess by understanding the drawbacks of a particular field. For\ninstitutions and funding agencies [41] such as NSF, NIH, and DoE,\nunderstanding research limitations could support the strategic al-\nlocation of resources toward areas needing further exploration\n[8, 35, 48]. Additionally, policymakers can play a crucial role in ad-\ndressing identified limitations by implementing supportive policies\nand providing targeted funding [3, 34, 36, 38]. Overall, a thorough\nanalysis of limitations sections promotes transparency [ 16, 22],\nfacilitates collaboration, and ultimately enhances the quality and\nimpact of future research initiatives [13]. Researchers can utilize\nthis structured categorization to better understand current limi-\ntations in their field and direct future studies accordingly. This\nenhances the accessibility and usefulness of the information for\nresearchers and practitioners [5].\nIn this research, we propose a pipeline to process limitations sec-\ntions from research papers and produce a list of topics with clear,\naccessible descriptions. Our approach uses an LLM-based topic\nmodeling technique to generate insightful topics and concise sum-\nmaries that capture the essence of limitations discussed in research\narticles. We experimented with models like Latent Dirichlet Alloca-\ntion (LDA) and BERTopic [14], as well as various LLMs, including\nGPT-4, GPT-3.5 [6], Mistral, and Llama 2, utilizing different prompt-\ning methods. Each generated topic is titled with a meaningful label\n(Table 2). We applied BERTopic with LLMs to generate topics with\ntitles, and BERTopic generated Topic Sentence for each topic by\nanalyzing the topics (clusters) and selecting the most central docu-\nments for each topic. The Topic Sentences for each topic comprised\nmultiple lengthy, complex sentences that were difficult to interpret\nbecause it collected various sentences from the dataset where some\nsentences were too specific for particular limitations. To address\nthis issue, we implemented a text summarization approach using\nLLMs to create concise summaries for each topic, enhancing their\ncomprehensibility. We experimented with various LLMs for text\nsummarization, including GPT-4 [6], Llama 3, Claude 3.5 Sonnet,\nand GPT-3.5.\narXiv:2503.10658v1  [cs.CL]  8 Mar 2025\nI.A Azher et al.\nThe key contributions of this work are as follows:\n(1) We developed a structured approach to categorize research\nlimitations through topic modeling and to identify key\nthemes within the limitations.\n(2) We integrated LLMs with BERTopic for topic modeling\nand generating titles for each topic. We also applied LLMs\nto create easy-to-understand, concise, and generalizable\nsummaries for each topic by distilling the key details of\neach topic.\n(3) We conducted experiments and comparative analysis using\nvarious LLMs to determine the optimal performance for\ntopic modeling and text summarization and evaluated the\nresults.\n2 RELATED WORK\nScholarly articles have long been recognized as a rich source for\ndiverse types of information [45]. Over the years, numerous stud-\nies have developed sophisticated tools and methods for extracting\nelements such as abstracts [46], headers, keyphrases, methodolo-\ngies, figures, tables, acknowledgments [25], future work, and ref-\nerences from PDF documents. Initial approaches often combine\nrule-based methods with machine learning techniques to enhance\nthe accuracy and scope of extraction. For instance, Houngbo and\nMercer [19] focused on extracting methodological sections using a\nrule-based approach supplemented by machine learning. Further\ndevelopments saw the integration of specific tools to facilitate the\nextraction process. Recent advancements have utilized more sophis-\nticated algorithms and deep learning such as used an unsupervised\ngraph-based algorithm for keyphrase extraction [37]. The use of\nConvolutional Neural Networks (CNNs) marked a significant evo-\nlution, as demonstrated by Ling and Chen [29], who applied CNNs\nto extract both textual and non-textual content, showcasing the\npotential of deep learning in managing complex document struc-\ntures. Complementarily, Oelen et al . [33] focused on comparing\nand aligning similar research contributions, assisting researchers\nin navigating prevalent research themes.\nTopic modeling techniques, such as Latent Dirichlet Allocation\n(LDA), have been widely used across various fields [40]. While LDA\ntreats documents as a bag of words, it has notable limitations, in-\ncluding the need to specify a fixed number of topics in advance and\nits sensitivity to hyperparameters. To enhance the coherence of\ntopic representations, a bidirectional transformer model, BERT [10],\ncan be used, which captures the context of a word by considering\nsurrounding words. However, even with these improvements, inter-\npreting topics can be challenging when relying solely on keywords\nor topic words. To address this, we incorporate large language mod-\nels (LLMs) such as GPT with BERTopic [14] to generate meaningful\ntitles for each topic, making them more comprehensible than word\nrepresentations alone.\nThe application of Large Language Models (LLMs) to text sum-\nmarization has significantly transformed the process, often yielding\nresults that surpass human-generated summaries in terms of ef-\nficiency and cost-effectiveness [7, 39]. For example, the Long T5\nmodel has proven effective for summarizing multiple documents\nin literature reviews [ 47]. Similarly, BERT has been tailored to\nsummarize scientific texts [15], and newer models [42] are capable\nof producing both abstractive and extractive summaries within a\nunified framework. LLMs have also been used to facilitate topic\ngeneration by extracting and aggregating topics from sentences\nwithin documents. This process involves condensing the extracted\ntopics to provide a concise overview [43]. Further extending this\napplication, BERTopic has been integrated with LLMs for specific\nuses: Koloski et al. [27] applied BERTopic with Llama 2 for liter-\nature mining, and Kato et al . [24] utilized it alongside GPT for\nquantitative political analysis. These efforts have been enhanced\nby fine-tuning LLMs to optimize topic modeling capabilities [23].\nDespite these advancements, the field of extracting and generating\nlimitations from scientific articles remains relatively underexplored.\nWhile there have been initiatives like those by and Faizullah et al.\n[11] to generate limitations from scientific texts, and efforts to apply\nmultimodal LLMs for this purpose [2], the specific application of\nBERTopic combined with LLMs to model topics and summarize\nlimitation sections in scientific papers has not been widely pursued.\nThis gap highlights a critical area for future research, presenting\nopportunities to enhance the depth and accuracy of scientific liter-\nature analysis.\nRnR1\nTopics TitlesTopics Topics\nTopic\nSummary\nSummary Summary\nR1 Rn R1 Rn\nTraditional\napproach 1\nTraditional\napproach 2 LimTopic\nTopic\nSentences\nFeature\nTraditional\nApproach 1\nTraditional\nApproach 2\nLimTopic\nGlobal Context ‚Ä¢ ‚Ä¢\nLocal Context ‚Ä¢ ‚Ä¢\nContextual Details ‚Ä¢\nReadable Title ‚Ä¢\nFigure 1: Traditional approaches vs. LimTopic.\nFigure 1 illustrates the differences between our approach, Lim-\nTopic, and traditional methods. Here, R denotes a research paper,\nand n is the dataset‚Äôs total number of research papers. Traditional\nsummarization methods typically follow one of two approaches:\nTraditional Approach 1: This approach combines all research\npapers into a single dataset and applies a summarization algorithm\nto create topics and summaries. While it preserves global context,\nit often loses local context and relevant information specific to\nindividual papers.\nTraditional Approach 2: This approach generates individual sum-\nmaries for each paper and then aggregates them. Although it main-\ntains local context, it struggles to capture the global context, often\nresulting in redundant information and lacking cohesive insights\nacross documents. To address these limitations, our method, Lim-\nTopic, synthesizes both global and local contexts. We use BERTopic\nLimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations\nBERTopic LLM\nTopic words\nTopic\nSentences\nLLM text\nsummarizer\nTitle of\neach topic\nTopic\nSummary\nDataset\nRep. Model Count\nVectorizerUMAP Seed WordsSentence\nTransformer\nZero Shot Min\nSimilarityHDBSCAN\n(Components of BERTopic)\nRep. Model\nFigure 2: Details about BERTopic + LLM for generating title,\ntopic, and Topic Summary.\nwith LLMs to create cohesive topics and generate Topic Sentences\nfrom the entire dataset, ensuring global context and relational co-\nherence across papers. Next, we apply LLM-based text summa-\nrization to refine each topic‚Äôs Topic Sentences, producing concise\nsummaries that capture local context and maintain relevant details.\nThis process not only preserves contextual depth but also generates\nmeaningful, clear topic titles that offer cohesive insights across all\nresearch papers.\n3 DATASET COLLECTION AND\nPRE-PROCESSING\nData collection: We took ACL research papers from the year 2023,\nwhich comprise 2,896 papers consisting of 231 short papers, 1261\nlong papers, 976 findings papers, and 428 papers from the remaining\n24 categories. We extracted all of the ‚Äòlimitation‚Äô sections using a\nhybrid approach. We used the Science Parse1 tool to extract text\nin a JSON format, where section titles (e.g., abstract, introduction,\nlimitations) serve as keys and text of each section as the value. At\nfirst, we applied this tool to automatically detect limitations sections\nfrom scientific papers. We collected 1,406 papers with limitations\nsections. However, Science Parse did not detect many limitations\nsections because they were sub-sections within other sections.\nThe Science Parse tool can detect a section as a separate section\nif any number (e.g., 1, 2, 3) appears as a prefix in a section heading;\notherwise, it will treat it as a sub-section within the other section.\nSo, we applied a rule-based approach to detect the rest of the 1,490\nlimitations sections. We searched for the word ‚Äòlimitation‚Äô or ‚Äòlim-\nitations‚Äô in the ethics, conclusion, future work, broader impact,\ndiscussion, and other sections, except the abstract, introduction, re-\nlated work, methodology, and acknowledgments sections since we\nfound that these sections mostly didn‚Äôt discuss their limitations, and\nsometimes, they discussed other papers‚Äô limitations. We initiated\nthe extraction process when we encountered the word ‚Äòlimitation‚Äô\nor ‚Äòlimitations‚Äô and continued until we reached the beginning of\nthe subsequent section or sub-section.\nTable 1 shows the number of papers extracted explicitly using\nthe SciParse tool and the number of papers for which we need a\nrule-based approach in our dataset. Here, explicit limitation means\nlimitations sections are a separate section, and implicit limitations\n1https://github.com/allenai/science-parse\nCategories Explicit limitations Implicit limitations\nShort Papers 81 84\nLong Papers 368 544\nFindings ACL 573 510\nSemantic Eval. 281 38\nIndustry Track 46 31\nTable 1: Number of papers with Explicit and Implicit limita-\ntions sections.\nTitle Rank\nChallenges and Approaches in Multilingual Language\nProcessing 2\nLimitations in Dialogue System Quality Assessment\nand Improvement 3\nReasoning limitations in Large Language Models 5\nAdvances and Challenges in Summarization Tasks 6\nMulti-hop QA Model limitations and Challenges 7\nBias and limitations in AI-driven Hate Speech\nDetection 15\nGender Bias in Language Models 21\nTable 2: Some of the Titles of each topic after applying topic\nmodeling (BERTopic + GPT4). A higher rank means more\nprominent.\nmeans limitations sections aren‚Äôt separate; they are stored as a\nsub-section or contained inside the other sections.\nData Pre-processing: We removed all punctuation, newline, non-\nEnglish words, equations, and the string \"et al.. \" We also removed\nthe sentences that contained any links. We removed any limitations\nsection with a word count of less than 15, as we found that the\nScience Parse tool had extracted unnecessary words. Many papers\ncontain irrelevant sentences after the ‚ÄòLimitation‚Äô section, such as,\n\"Did you discuss any potential risks of your work?\" the science\nparse tool extracted. So, we checked and removed these types of\nsentences. Also, we found that when we extracted ‚ÄòLimitation‚Äô sec-\ntions, it extracted some other section‚Äôs text. So we checked whether\nany ‚Äòlimitations‚Äô section contained \"future work, \" \"ethics, \" \"grants, \"\n\"appendix, \" or \"section. \" If we find such a section, we proceed to\nremove that section from the ‚Äòlimitation‚Äô section.\n4 METHOD\nOur proposed model, LimTopic utilizes BERTopic combined with\nan LLM to analyze the limitations sections of research papers. As\nshown in Figure 2, BERTopic generates topic words and Topic\nSentences for each topic. To enhance clarity, we applied an LLM to\ngenerate meaningful titles for each topic and used the LLM again\nto summarize the Topic Sentences into concise ‚ÄòTopic Summaries.\nWe illustrate our workflow in Figure 3. Initially, we gathered ACL\npapers and extracted the limitations sectionsùêø= ùêø1,ùêø2,....ùêøùëõ where\nùëõ is the number of limitations from its corresponding research\npaper ùëÖ = ùëÖ1,ùëÖ2,....ùëÖùëõ. Then, we used topic modeling on all ùëõ\nlimitations sections and generated ùëó topics, denoted by ùëá1 to ùëáùëó .\nEach topic ùëáùëúùëùùëñùëêùëñ has a collection of topic words or keywords we\ncan denote as ùëáùëúùëùùëñùëêùëñ = ùëáùëò1,ùëáùëò2,.....ùëáùëò ùëò . Topic modeling uncovers\na distribution of words for each identified topic. However, these\ntopics can be hard to understand due to a lack of semantic meaning,\nI.A Azher et al.\nTopic \nText\nSummarization\nLimitations\nL1\nL2\n Ln\nResearch\nPapers\nR2\nR1\nRn\nTopics, Title and Topic Sentences\n(Topic-1) | Title (T1): Bias and limitation\nin AI-driven Hate Speech Detection.\nTopic Sentences (R1):One limitation of\nthis approach is the following: we areassuming that the human annotated\nlabels represent a reasonable groundtruth.However, it‚Äôs likely that theannotations have their own bias issues.\n....\n(Topic-j) | Title (Tj)\nTopic Sentences (Rj) : .....\nTopics, Title, and Topic\nSummary\n(Topic-1) | Title (T1): Bias\nand limitation...\nTopic Summary(TS1):One majorlimitation is the\nassumption that humanannotations represent areasonable ground truth,\nwhich may have their ownbiases\n(Topic-j) |  Title (Tj)\nTopic Summary (TSj).....\nModelingLimitations\nExtract\nText Summarization\nFigure 3: LimTopic for generating meaningful topic titles with summaries from a large collection of text-form limitations\nsections.\nDataset (Limitation sections)\n LLM prompt Fine tuned\nLLM BERTopic BERTopic + LLM\nGPT 3.5 GPT 4 Llama 2Mistral\nTitle for\neach\ntopic\nTopic words\nTopic\nSentences\nLLM text\nsummarizer\nTopic\nSummary\nLLM prompt to\ngenerate \"Topic\nSummary\"\na b c d\nd1 d2 d3 d4\nd21\nc1\nFigure 4: Experiment with various approaches to generate topic titles and Topic Summary. The X mark indicates poor results.\nleading to poor interpretation of the actual topic. Therefore, we\nused BERTopic along with LLM and generated meaningful title ùëáùëñ\nand Topic SentenceùëÖùëñ for each topicùëáùëúùëùùëñùëêùëñ (Equation 1). Here1 <=\nùëñ <= ùëó. Each Topic SentenceùëÖùëñ is very long and hard to interpret, so\nwe use text summarization and generated Topic SummaryùëáùëÜùëñ for\neach topicùëáùëúùëùùëñùëêùëñ (Equation 2). Hyperameter tuning of our proposed\nmodel‚Äôs LimTopic described in section 5.3 (Figure 4d2).\nùëá1,2,..ùëó = ùêµùê∏ùëÖùëáùëúùëùùëñùëê (ùêø1,2,..ùëõ ) (1)\nùëÜùëáùëó = ùêøùêøùëÄùë†ùë¢ùëöùëöùëéùëüùëñùëßùëí(ùëÖùëó ) (2)\n5 EXPERIMENTS\nWe experimented with four schemes after applying various pre-\nprocessing methods to the dataset.\n1. Topic Generation via LLM Prompting: LLM prompt engineer-\ning to generate topics and titles with a Topic summary (Figure 4a).\n2. Topic Generation with LLM Fine-tuning: Utilized fine-tuned\nLlama 2 to generate topics and titles with a Topic summary (Figure\n4b).\n3.Topic Modeling (BERTopic + LLM) with LLM Summariza-\ntion: Proposed Model (LimTopic)\na. Integrating BERTopic with LLM as a topic modeling to generate\ntopic, title, and Topic sentences. (Figure 4c), (Figure 3, blue box)\nb. LLM as a text summary on top of topic modeling (BERTopic +\nLimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations\nLLM) to generate a concise Topic summary (Figure 4d), (Figure 3,\ngreen box).\n5.1 Topic Generation via LLM Prompting\nOur initial approach used LLM prompt engineering to generate\npotential topics, titles, and summaries from the dataset containing\nlimitations sections. We leveraged the LangChain library 2 with\nOpenAI‚Äôs API to facilitate prompt engineering. We loaded the en-\ntire dataset and used GPT-3.5 with a prompt: ‚ÄúHow many topics\ncan you generate by emphasizing limitations and generating topic\ntitles?‚Äù GPT-3.5 initially identified around 1000 topics focused on\nlimitations in the dataset. To ensure consistency, we applied the\nself-consistency approach [44], using the same prompt again, which\nconfirmed the generation of 1000 topics. However, upon reviewing\nthese topics, we found them to be repetitive and overly generic,\nwith several topic titles appearing multiple times. We then applied\nthe same approach using GPT-4, which, in contrast, generated only\n5 distinct topics, offering more focused and relevant results. Next,\nwe set 35 as the optimal number of topics, aligning with the 35\ntopics generated by BERTopic. We prompted GPT-3.5 with: ‚ÄúCan\nyou generate 30-35 topics? Could you create a title and a Topic\nSummary for each topic within 250 words, with a particular focus\non its limitations?‚Äù GPT-3.5 generated 30 topics; however, many\ntitles were repetitive, and the summaries were too generic, lacking\ndetailed insights for each topic. Similarly, GPT-4 produced only five\ntopics, failing to capture the nuances of each limitation. To improve\nperformance, we implemented a sequential approach. First, we used\na prompt to generate a list of 35 concise topic names (one or two\nwords each), which we referred to as seed words (Table 7). Using\nthis seed words list, we prompted the model to generate detailed\ntopics, titles, and Topic Summaries based on these seed words, as\nshown in (Figure 5). However, the generated ‚ÄòTopic Summary‚Äô is\nsmall and lacks sufficient details.\nprompt= '''\nI have this topic list [seed word].\nCan you generate 30-35 topics to try each sentence that belongs\nto one of these classes? If not, make a new topic. Generate each\ntopic title and Topic Summaryof each topic within 150 words\n'''\nFigure 5: Prompt for topic generation using seed word\n5.2 Topic Generation with LLM Fine-tuning\nIn addition to prompting, our second approach investigated fine-\ntuning the pre-trained Llama 2 7b model using a 20% random sample\nof our dataset and making it a ‚Äòtrain dataset. ‚Äô We made our input as\na ‚ÄòQuestion-Answer‚Äô pair where ‚Äòquestions‚Äô is the topic title from\nBERTopic + LLM and ‚Äòanswers‚Äô is the text from limitations sections\nto train the Llama model. We employed a fine-tuned approach with\nlow-rank adaptation (LoRA) [ 20]. LoRA introduces two smaller\nmatrices where the original weight matrix remains frozen during\nfine-tuning, and these two smaller matrices will add in the original\nweights. The model will learn new patterns without changing the\npre-existing model structure. LoRA helps preserve knowledge by\nmodifying a small fraction of the model‚Äôs parameters. Then we\n2https://python.langchain.com/v0.2/docs/introduction/\nused QLoRA [9] for 4-bit quantization instead of 16 or 32 bits, and\nit reduced the precision of the numerical values in parameters for\nspeeding up computation. We set these parameters, ‚Äòbatch size‚Äô = 4,\n‚Äòlora attention dimension‚Äô = 64, ‚Äòlora alpha‚Äô = 16, and ‚Äòlora dropout‚Äô\n= 0.1, ‚Äòmaximum token length‚Äô = 512, and used the Adam optimizer\nwith a learning rate 2e-4. After 100 iterations, the loss dropped\nand converged. We fine-tuned the model using the ‚Äòtrain dataset‚Äô\nand created a new model and tokenizer. Next, we added test data\nto the new model and used prompts to generate topics, titles, and\nTopic Summary for each topic. We approached the prompt using\ntwo distinct methods: In our first approach, we used the prompt,\n‚ÄúBased on the above information, extract topics with a short label\nand Topic Summary. \" Here, we didn‚Äôt specify the number of topics.\nIn the second approach, we specified the number of topics to 30-\n35 and asked prompts to generate titles and Topic Summary for\neach topic. We chose this number because BERTopic generated this\nnumber of topics. However, the fine-tuned Llama model‚Äôs output\nwas not satisfactory. In both cases, it generated five topics, and the\nTopic Summary snippets were generic and lacked sufficient detail.\n5.3 (LimTopic) Topic Modeling (BERTopic +\nLLM) with LLM Summarization\nTo alleviate the problem in earlier approaches, such as LLM Prompt\n(Figure 4a) and Fine Tuning approach ( Figure 4b) we implemented\nBERTopic with LLM ( Figure 4d), and it shows better performance\nconsisting of better topics, titles, and Topic Summaries. It generates\nbetter human-understandable topic titles than ( Figure 4c) also,\nwhich generated topic words or keywords only.\n5.3.1 Experimenting with BERTopic. We initially experimented\nwith Linear Discriminant Analysis (LDA) for topic modeling, aim-\ning to capture specific limitations for each topic. However, LDA\nonly produces topic words, making it difficult to grasp the actual\nmeaning of each topic. Additionally, the topic summaries gener-\nated by LDA often covered unrelated or inconsistent limitations,\nas LDA grouped diverse sentences that did not always address the\nsame topic coherently. To address these issues, we implemented\nBERTopic, which, unlike LDA, gathers related sentences for each\ntopic as Topic Sentences, providing a more cohesive context. While\nBERTopic generates topic words, determining a clear title for each\ntopic remains challenging. To improve interpretability, we inte-\ngrated an LLM with BERTopic to generate descriptive titles (Figure\n3, blue area). Furthermore, we applied LLM-based text summariza-\ntion to refine Topic Sentences into focused summaries that clearly\nconvey each topic‚Äôs specific limitations (Figure 3, green area). We\nbegan experimenting with BERTopic using a pre-processed dataset\n(Figure 2). BERTopic has various elements where the tokenizer con-\nverts text into smaller tokens, UMAP [31] reduces the dimension of\ntext embedding for clustering, Count Vectorizer creates a matrix of\ntoken counts which calculates class-based TF-IDF score, and Sen-\ntence Transformers create dense embedding vectors with capturing\nsemantic meanings of the sentence. Figure 2 dotted line denotes\nthose elements are optional in BERTopic. We experimented with\nthose optional elements and found that integrating ‚Äòseed words‚Äô\n(Table 7) improves performance. We explored combinations of these\napproaches, such as:\na) with/without seed word, b) with/without UMAP and c) with/without\nI.A Azher et al.\nsystem prompt= '''\nYou are a helpful, respectful and honest assistant for labeling\ntopics.\n'''\nexample prompt= '''\nI have a topic that contains the following documents:\n- Traditional diets in most cultures were primarily plant-based\nwith a little meat on top, but with the rise of industrial-style meat\nproduction and factory farming, meat has become a staple food.\n- Meat, but especially beef, is the worst food in terms of emis-\nsions.\n- Eating meat doesn‚Äôt make you a bad person, not eating meat\ndoesn‚Äôt make you a good one.\nThe topic is described by the following keyword: ‚Äòmeat, beef, eat,\neating, emissions, steak, food, health, processed, chicken‚Äô.\nBased on the information about the topic above, please create a\nshort label or title for this topic. Make sure you only return the\nlabel and nothing more.\n'''\nmain prompt= '''\nI have a topic that contains the following documents:\n[DOCUMENTS]\nThe topic is described by the following keyword: [KEYWORD].\nBased on the information about the topic above, please create a\nshort label of this topic. Make sure you to only return the label\nand nothing more.\n'''\nprompt= system prompt + example prompt + main prompt\n'''\nOutput\nTitle= Challenges on Dense Retrieval Models in Information\nRetrieval\nTopic Sentences: However, it is unclear how it may be\nadapted for the single-representation dense retrieval prf\nmodel.In addition, in this...\nFigure 6: Prompt for topic modeling (BERTopic + GPT).\nHDBSCAN. Here, the seed word contains the prominent word for\neach topic in the dataset. We use these seed words for guided topic\nmodeling. To generate seed words, we used a GPT-4 prompt to\nproduce 30-35 topic names, each consisting of one or two words\n(Table 7). We then used this list of words to guide topic mod-\neling in BERTopic. We use UMAP for dimensionality reduction,\nwhich involves capturing lower dimensions from local and global\nhigh-dimensional space. We also experimented with HDBSCAN in\nBERTopic. However, when employing UMAP, it copies the example\nof the prompt and produces it in the topic summary for some topics.\nAfter reducing the dimensionality of input embeddings, we use\nHDBSCAN [30], a clustering process, to extract topics. However,\nHDBSCAN decreased the performance. One reason is that BERTopic\ngenerated high-dimensional data, whereas HDBSCAN used Eu-\nclidean distance, which doesn‚Äôt capture the semantic distance be-\ntween words. We experimented with the combination of UMAP,\nHDBSCAN, and seed word and measured the performance of the\nsilhouette and coherence scores as a quantitative study. Only in-\ncluding seed words with BERTopic improved performance, whereas\nUMAP and HDBSCAN led to decreased coherence and silhouette\nscores. BERTopic generated topic words or keywords for each topic.\nHowever, LLM can generate more coherent and descriptive titles\nfor each topic, making more straightforward interpretability, and\ndeeper contextual understanding can capture the breadth and nu-\nances of a topic. So, we integrate LLM with BERTopic to create\ntitles for each topic.\n5.3.2 Prompt Engineering on LLM for topic modeling. After\napplying BERTopic, we used a large language model (LLM) to gener-\nate topic titles. BERTopic allows customization through various pa-\nrameters, such as text embedding, dimensionality reduction (using\nUMAP), clustering (using HDBSCAN), and representation. In this\ncase, we used the LLM as the representation model, illustrated by\nthe label ‚ÄòRep. Model‚Äô in Figure 2. By using LLM-generated outputs\nfor topic modeling, we improved the quality of topic titles, moving\nbeyond simple keywords to meaningful, refined titles. Additionally,\nthe LLM refined the textual output provided by BERTopic. For this\nprocess, we designed two types of prompts for the LLM. In the first\napproach, we used prompts to generate concise topic titles (shown\nin Figure 6). In the second approach, we used prompts that created\nboth topic titles and a brief Topic Summary for each topic (Figure 7).\nWhile these summaries gave an overview of each topic, they were\nintentionally kept general and did not generate detailed content.\nOn the other hand, BERTopic also generated more comprehensive\n‚ÄôTopic Sentences‚Äô by aggregating content from relevant sections\nthroughout the text (Figure 3, blue area). These Topic Sentences\nwere lengthy and detailed, covering nuanced aspects of each topic.\nTo condense this information, we applied a text summarization\ntechnique with the LLM (section 5.3.3) to create clearer, focused\nTopic Summaries for each topic (Figure 3, green area). We also ex-\nperimented with few-shot and zero-shot prompting techniques [26].\nTesting with one to five examples for few-shot learning, we found\nthat providing one example worked best; using more examples\nsometimes led to overfitting, where the LLM would incorporate\ncontent from the example responses directly into the summaries,\nreducing the quality of the output.\nprompt = '''\nI have a topic that contains the following documents:\n[DOCUMENTS].\nThe topic is described by the following keyword: [KEY-\nWORD].\nBased on the information above, extract a short topic label\nand ‚ÄòTopic Summary‚Äô in the following format: topic: <topic\nlabel> Topic Summary: <texts>\"\n'''\nOutput\nTitle = Challenges and limitations in Clinical Data\nUsage and NLP Model Evaluation in Healthcare\nTopic Summary: In this text elucidates significant\nconstraints encountered in the use of clinical data\nfor NLP model evaluation, particularly in healthcare\nsettings. The study‚Äôs...\nFigure 7: Prompt for topic generations with Topic Summary.\n5.3.3 Experimenting on LLM for text summarization. In the\nprevious approach, we didn‚Äôt get a good ‚ÄòTopic Summary‚Äô for each\nLimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations\ntopic using the prompt in BERTopic + LLM (section 5.3.2, Figure:\n7, and BERTopic only (section 5.3.1). BERTopic generated topic\nsentences for each topic, which collects various texts related to\nthe topic across the dataset. Each topic‚Äôs Topic Sentences are long,\naveraging 800 words, making them lengthy and potentially en-\ncompassing multiple limitation types. So, we started with Topic\nSentences for each topic. To address this, we applied LLM-based\nsummarization for each topic‚Äôs Topic Sentences by employing the\nprompt in Figure 8 and generated a Topic summary for each topic.\nprompt = '''\n\"Can you summarize the following texts within 130-140\nwords, putting more emphasis on the limitations of the\n‚Äòtitle‚Äô?\" '''\nFigure 8: Prompt for text summarization.\nThis process yielded concise summaries offering a generalized\noverview of each topic‚Äôs specific limitations (See Tables 2, 11). We\nalso experimented with various LLMs for summarization, including\nGPT 4, GPT 3.5, Llama 7B, Llama 13B, and Mistral, and used GPT 4\nand Claude 3 as a Judge.\nCategory Optimal parameters\nnumber of neighbors (UMAP) 13\nnumber of components (UMAP) 7\nmin topic size 10\nzero shot min similarity 0.75\nTable 3: Optimal parameters for BERTopic.\n6 RESULTS AND ANALYSIS\nWe used coherence and silhouette scores to measure the quality of\ntopic modeling. The silhouette score, ranging from -1 to 1, evaluates\nhow well each cluster is formed by comparing intra-cluster similar-\nity with inter-cluster dissimilarity. The coherence score assesses\nsemantic similarity among frequently occurring words within a\ntopic, indicating how well the words and phrases align within a\ntopic‚Äôs context. For measuring the quality of text summarization,\nwe used ROUGE-1, ROUGE-2, ROUGE-L, BLEU, and BERTScore as\na quantitative method 10, and some other metrics such as Gram-\nmatically, Readability, so on as a LLM as a judge method (Table\n9).\n6.1 Topic Modeling:\nBERTopic generates topic words for which we can easily measure\nthe coherence score. However, measuring the coherence score us-\ning a title instead of topic words is impossible. To alleviate this\nproblem, we applied keyBERT to each topic‚Äôs Topic Sentences and\nextracted keywords using BERT-embedding and cosine similarity,\nwhich were the most similar to the documents. Next, we calcu-\nlate the average coherence score for each topic (Table 4). Table 4\nshows several topic-modeling models we used; some are traditional\ntopic models like LDA and BERTopic. We start the experiment with\nLDA, a probabilistic model with a 0.375 coherence score, whereas\nBERTopic gives a big jump to a 0.601 coherence score, and if we use\nUMAP, the coherence score drops. UMAP is one approach used for\ndimensionality reduction to speed up the training process, but its\ndrawback is that it requires assistance handling large amounts of\ndata. Then, we added LLM with BERTopic to generate titles for each\ntopic. We applied two prompts: one generates a ‚ÄòTopic and title, ‚Äô\nand the other generates a ‚ÄòTopic, title with the Topic Summary. ‚Äô\nWe found that the latter approach showed better coherence and\nsilhouette scores when we applied BERTopic with GPT 4. We also\nexperimented with a few shots and a zero-shot approach, and a\nfew shots achieved a 0.021 more coherence score and 0.059 more\nsilhouette score. Also, BERTopic + GPT 4 (few shots) achieves 0.016,\n0.19, 0.123, 0.098 more performance than BERTopic, BERTopic +\nGPT 3.5 (few shots), Llama 7b (few shots), Llama 13b (few shots) in\nterms of coherence score respectively.\nOur proposed BERTopic + GPT-4 model produced 35 topics with\ndescriptive titles with the rank. Here, a higher rank means more\nprominent this type of ‚ÄòLimitation‚Äô in the dataset. (See Table 2).\nIn GPT 4, we experimented with one, two, three, four, and five\nexamples in a few shots and found that one shot is better, whereas\nin other cases, the generated text mimics the example from the\nprompt. The performance of LLM in producing subject titles from\nthe input text is highly contingent on the size of the context win-\ndow. We used GPT 3.5 Turbo, which has a context window size of\n16,385 tokens, whereas GPT 4 Turbo has a larger context window\nof 128,000 tokens. Also, BERTopic + GPT4 has the capability to\nachieve a stronger understanding with more parameters trained on\nvast datasets, capturing the abstract themes of keywords provided\nby BERTopic, which ends up with GPT 4 outperforms other models\n(Table 4).\nIn other cases, the output was not sound when we integrated\nBERTopic with GPT 3.5, Llama 7b, Llama 13b, or Zephyr (Mistral\n7b). Here, Llama 7b, Llama 13 b, and Mistal generated topic word\n(keyword) for each topic. We used a prompt to create titles here, but\nthe topic titles must be more concise. GPT 3.5 produced very little\nTopic Summary; Llama 7b and 13b produced some Topic Summary\non certain topics; and Zephyr (Mistral 7b) also produced some\nirrelevant topic titles and Topic Summarys. One possible reason\nfor the poor performance of Zephyr (Mistral 7b) is that it‚Äôs got\n512 tokens as a context length when we integrate with BERTopic,\nwhere the input length is much higher (Table 4, Fig: 9). Llama 7b,\n13b produces some irrelevant topics and titles because of the smaller\nparameter size and context window compared to GPT.\nWe also experimented with GPT 4 and GPT 3.5 variants with\nBERTopic for topic modeling. Both models have famous variants,\nsuch as GPT 4/3.5 and GPT 4/3.5 turbo. We found that the GPT\nturbo version performs better than the GPT version for topic mod-\neling in Table 8. Here ‚Äògpt-4-turbo-preview‚Äô performs best (Tables\n4, 8). We found that ‚Äòall-MiniLM-L6-v2‚Äô achieves the best coherence\nscore, and ‚Äòall-MiniLM-L12-v2‚Äô achieves the best silhouette score\n(Table 5). One reason is that those embeddings are fine-tuned for\ngeneral purposes, capable of preserving key semantic information\nfor embedding, and trained on diverse datasets. Another important\nparameter is ‚Äòminimum topic size, ‚Äô which specifies a topic‚Äôs mini-\nmum size. We experimented with minimum topic sizes of 7, 10, and\n12 and found that 10 is optimal. The minimum topic size of less than\n10 will generate many overlapping topics, and more than 10 will\ngenerate fewer topics by combining two or more topics. In addition,\nI.A Azher et al.\nModel Name Topic and Title Topic, Title with Topic Summary\nSilhouette Score Coherence Score Silhouette Score Coherence Score\nLDA - - - 0.375\nBERTopic - - 0.577 0.601\nBERTopic + UMAP - - 0.586 0.581\nBERTopic + GPT 3.5 (zero-shot) 0.551 0.468 0.544 0.520\nBERTopic + GPT 3.5 (few-shot) 0.407 - 0.546 0.427\nBERTopic + GPT 4 (zero-shot) 0.561 0.487 0.529 0.596\nBERTopic + GPT 4 (few-shot) 0.530 0.509 0.588 0.617\nBERTopic + Llama 7b (zero-shot) - - 0.576 0.497\nBERTopic + Llama 7b (few-shot) - - 0.558 0.494\nBERTopic + Llama 13b (zero-shot) - - 0.578 0.517\nBERTopic + Llama 13b (few-shot) - - 0.558 0.519\nBERTopic + Mistral 7b (zero-shot) - - 0.524 0.551\nTable 4: Performance comparison of different models for Topic Modeling.\nSentence Transformer Min Topic Size Clusters Silhouette Score Coherence Score\nBge-base-en-v1.5 10 23 0.542 0.632\nallenai-specter 10 30 0.493 0.608\nparaphrase-MiniLM-L6-v2 10 28 0.502 0.544\nall-mpnet-base-v2 10 39 0.590 0.591\nall-MiniLM-L12-v2 10 33 0.601 0.581\nall-MiniLM-L12-v2 (without umap) 10 32 0.578 0.601\nall-MiniLM-L6-v2 10 31 0.528 0.715\nparaphrase-multilingual-MiniLM-L12-v2 10 28 0.460 0.521\nparaphrase-MiniLM-L6-v2 10 28 0.502 0.544\nbert-base-nli-mean-tokens 10 2 0.454 0.503\nall-distilroberta-v1 10 34 0.591 0.594\nmsmarco-distilbert-dot-v5 10 30 0.460 0.548\nmulti-qa-MiniLM-L6-cos-v1 10 25 0.498 0.627\nTable 5: Performance of various sentence transformers for topic modeling in BERTopic.\nYou are a very helpful, respectful assistant. Please take time to fully read and understand the\ncontext. Please select which \"Prompt\" is better in terms of Grammaticality, Cohesiveness, Understandability,\nLikability, Cohesiveness, Coherence, Likability, Relevance, Fluency, and describing a very good way of a particular\nlimitations. Note: Please take the time to read and understand the story fragment fully. Rate each prompt out\nof 5 ratings, where 5 is best and 1 is worst.\nTable 6: Prompt for GPT 4 to evaluate summary.\nLanguage Dataset Corpus Size Computational Machine Translation\nTokenization Costs Interpretability Morphology Semantic\nMemory Skewed Distributions Biased Distributions Small Size Generalizability\nBias Hyperparameter Hardware Real world Robustness\nNoisy Time Annotations Evaluation Diversity\nSegmentation Metrics - - -\nTable 7: Seed words: these words generated by LLM from the dataset are used as a chain approach for topic generation.\nModel GPT 4\nTurbo GPT 4 GPT 3.5\nTurbo GPT 3.5\nSilhouette\nScore 0.588 0.518 0.546 0.540\nTable 8: Topic modeling performance comparison in GPT\nvariants with BERTopic.\nwe experimented with another parameter, ‚Äòzero-shot min similarity, ‚Äô\nexperimenting with 0.7, 0.75, 0.8, and 0.85 and found 0.75 provides\nthe best coherence score. Additionally, we used ‚ÄòCountVectorizer‚Äô\nand removed the stopwords in the pre-processing stage. Further-\nmore, adding more data enhances the quality of topic word in the\nBERTopic + LLM (GPT 4, GPT 3.5, Llama 2), ensuring that each\ntopic‚Äôs word aligns well with their respective cluster. We also ex-\nperimented using BERTopic with UMAP. After varying the number\nof neighbors from 1 to 30, we found that 13 is the optimal number\nof neighbors, and 7 is the optimal number of components (Table\n3). We integrated optimized various parameters, such as Sentence\nTransformer, Count Vectorizer, UMAP, Seed words, Tokenzier, GPT\nLimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations\nFigure 9: Dataset size vs Performance\n4 Prompt 3 (Figure 6), Document Length, Embedding Model, and\nMinimum Topic Size with GPT 4 with BERTopic. We also analyzed\neach category of ACL 2023 papers to identify prominent limitations\nwithin each category. These limitations are in Table 12.\n6.2 Text summarization:\nAfter applying topic modeling with BERTopic + LLM, BERTopic\ngenerated Topic Sentences. We applied LLM as a text summarizer to\nsummarize each topic‚Äôs Topic Sentences concisely as we depict it as\na Topic Summary (see Table 11). We applied GPT 4, Claude 3.5 Son-\nnet, Llama 3, and GPT 3.5 to summarize each topic Topic Summary.\nAt first, we used ROUGE [28] score to measure the performance\nof each LLM as a summarizer. We used Topic Sentences generated\nby BERTopic from the dataset, containing texts of each topic. We\ntook these Topic Sentences as a reference text and considered the\nsummary from GPT 4, Claude 3.5 Sonnet, Llama 3, and GPT 3.5\nas candidates. We check the ROUGE-1, ROUGE-2, and ROUGE-L\nscores, which show the overlaps of unigrams, bigrams, and the\nlongest common subsequence between the candidate summary and\nthe reference text. BLEU measures the number of matching words\nand phrases between LLM-generated text and reference text with\nan overlap of different n-gram lengths.\nIn some cases, the GPT 4 and Claude 3.5 Sonnet summary rat-\nings are similar, providing detailed, cohesive, and fluent discussion.\nHowever, GPT 4 maintains a high level of detail without sacrificing\ntechnical depth. We found that the summary from GPT 3.5 achieves\nthe highest ROUGE-1 score, ROUGE-2, ROUGE-L score, and BLEU\nscore. So, a higher value means more overlapping words between\nactual and LLM-generated limitations. A greater degree of overlap\nmeans copying the word from the actual limitations as a summa-\nrizer, which is not good. The problem with these metrics is that they\nrely on exact matches between the LLM summary and reference\ntext. So, we applied another metric named BERTScore, which relies\non semantic similarity between generated and reference text tokens.\nBERTScore has precision, recall, and F1 scores, where recall mea-\nsures how much the content in the candidate sentence captures the\nessence of the reference using cosine similarity. So, we considered\n‚Äòrecall‚Äô in BERTScore as we used Topic Sentences as a ‚Äòreference\ntext, ‚Äô which consists of the actual limitations of each topic. Table 10\n3https://maartengr.github.io/BERTopic\nshows GPT 4 outperforms BERTScore (recall) compared to Claude\n3.5 Sonnet, Llama 3, and GPT 3.5.\nTo improve the quality of summaries, we opted to use human-\ngenerated summaries as ‚Äòreference text‚Äô or ground truth rather than\nrelying solely on text directly from the paper. To further enhance\nthis evaluation, we implemented an alternative method called LLM\njudgment, where GPT-4 and Claude 3.5 Sonnet served as evaluators\n[49]. First, we generated summaries using GPT-4, LLaMA 3, Claude\n3.5 Sonnet, and GPT-3.5, and then asked GPT-4 to rate each sum-\nmary on a scale from 1 to 5. The rating criteria included grammar,\nreadability, cohesiveness, understandability, likability, coherence,\nrelevance, fluency, and overall description quality (Table 9, 11). We\nrepeated this process using Claude 3.5 Sonnet as the evaluator, ask-\ning it to rate summaries from all the LLMs using the same metrics\n(Table 6). From the set of 35 summaries, we randomly selected 15\nand calculated the average scores from both GPT-4 and Claude 3.5\nSonnet. Table (11) provides a comparison of summaries generated\nby various LLMs along with their average ratings. Overall, GPT-4,\nas a summarizer, consistently received higher ratings than the other\nLLMs, indicating its superior performance (Table 9).\nMetrics GPT 4 Llama\n3 Claude 3.5 GPT\n3.5\nGrammatically 4.75 4.15 4.57 3.6\nReadability 4.11 3.6 4 2.95\nCohesiveness 4.75 3.55 4.3 2.8\nUnderstandability 4.11 3.65 4.05 2.8\nLikability 4.22 3.4 4 2.85\nCoherence 4.75 3.55 4.3 2.75\nRelevance 4.81 3.95 4.55 3.15\nFluency 4.36 3.65 4.2 2.8\nDescription\nQuality 4.8 3.55 4.45 2.7\nTable 9: Performance of summarization in various LLM\njudged by GPT 4 and Claude 3.5 Sonnet.\nModel ROUGE-\n1\nROUGE-\n2\nROUGE-\nL BLEU BERTScore\nGPT 4 34.13 9.34 16.95 5.18 83.61\nClaude\n3.5 Son-\nnet\n36.18 10.5 15.06 4.55 83.31\nLlama 3 45.34 16.96 16.38 3.86 82.47\nGPT 3.5 62.26 28.98 25.74 6.42 82.46\nTable 10: Performance of various LLM as a text summarizer.\nInput is Topic Sentences from BERTopic.\nFig 9 depicts that increasing the dataset size also increases the\nsilhouette score. Here, we started experimenting with one category\ndataset, which contains ‚Äôlong papers‚Äô, then we gradually added\nother category datasets such as ‚Äòshort papers, ‚Äô ‚Äòfindings ACL papers,\n‚Äòindustry track papers, ‚Äô and lastly, we included all 28 categories,\nand we found silhouette score is increasing in GPT 3.5, GPT 4, and\nLlama 2. A better silhouette score means the topic quality (cluster)\nis good. With the comparison of four LLMs, GPT 3.5 achieves a\n0.52 score, gradually increasing to 0.54 after adding each category\ndataset. Llama 2 is slightly inconsistent, where its performance\nI.A Azher et al.\ndrops at one point and increases later. The exception occurred\nby Mistral because the token limit is 512 tokens, and our dataset\nexceeds the limits. GPT 4, from 0.49 to 0.58, performs very well\nand is stable compared to GPT 3.5, Llama 2, and Mistral, where\nthe performance increases after adding each dataset category. GPT\n4 makes outstanding topics if we provide a sufficient number of\ndatasets.\nTitle: Challenges and limitations in Clinical Data\nUsage and NLP Model Evaluation in Healthcare\nResults\nGPT 4 summary : In this text elucidates signifi-\ncant constraints encountered in the use of clini-\ncal data for NLP model evaluation, particularly in\nhealthcare settings. The study‚Äôs reliance on a sin-\ngle clinical cohort limits its generalizability, raising\nconcerns about the representativeness of the data\ndue to possible biases in gender, age, race distribu-\ntions. While promising for cancer immunotherapy\nprognostic prediction, the study‚Äôs findings may not\nnecessarily apply to other clinical trials without\nfurther validation...\nGramm:\n5, Read-\nability:\n4, Cohe-\nsiveness:\n5, Under-\nstand-\nability: 4,\n...\nTable 11: Summary from GPT 4 and judge by GPT 4 and\nClaude 3.5 Sonnet.\nLimitation Category\nEnhancing medical summarization and\nmodel reliability. Medical NLP\nLimitations and challenges in evaluating\nconversations with LLMs. NLP conv AI\nSpeech-to-text translation challenges. IWSLT\nDialogue system development and\nevaluation. Finding ACL\nTable 12: Limitation in some categories of ACL 2023 papers.\n7 CONCLUSION\nIn this paper, LimTopic used BERTopic and GPT 4 for topic mod-\neling to analyze sections with limitations from scientific articles,\ncreating titles and topics with Topic Sentences. Later, we applied\ntext summarization to each topic‚Äôs Topic Sentences to generate a\nconcise Topic summary for each topic. We tried three approaches\n(Figure 4); firstly, we used our dataset and applied the LLM prompt\nto generate titles and Topic Summarys. Secondly, we applied LLM\nfine-tuning to create titles and Topic Summarys; we tried to prompt\nin BERTopic + LLM to generate Topic Summary. Here, BERTopic +\nGPT 4 generates a perfect title, and all the above methods generate\nconcise and generic Topic Summaries. We applied text summariza-\ntion of each topic‚Äôs Topic Sentences to create a Topic summary.\nHere, BERTopic created Topic Sentences that consist of various sen-\ntences pertinent to each topic from all over the dataset. So we took\nthe Topic Sentences, applied various LLM for summarization tasks,\nand created a better Topic summary for each topic. We also mea-\nsured the performance of each summary from LLM using Chatgpt\n4 and Claude 3.5 Sonnet. Our approach not only depicts the limita-\ntions but also suggests a structure to address the current limitations.\nIt promotes a thorough and critical approach to future research\nand a nuanced interpretation of existing works. Furthermore, be-\nyond LLMs‚Äô typical language processing ability, we evaluate several\nLLMs to do complex analytical tasks such as topic modeling.\nLIMITATIONS AND FUTURE WORK\nIn this work, we focused only on LLM-based topic modeling to\ngenerate topics and used a text summarization approach with LLMs\nto facilitate understanding of each topic‚Äôs details. Due to API costs,\nlarge-scale analysis with GPT could have been more feasible. Ad-\nditionally, GPU limitations restricted experimentation to Llama\n7b and Llama 13b models. Moreover, our analysis was limited to\nthe ACL 2023 dataset, which might affect the generalizability of\nfindings and some potential biases, such as ACL 2023 papers re-\nflecting the current methodologies and trends (e.g., LLMs), which\noverrepresents cutting-edge research that might not be relevant to\nearlier research. This dataset also focuses on particular areas that\nresearchers are interested in but might not represent the overall\nscenario. Our model shows a methodological bias toward promi-\nnent topics and titles, which leads to the underrepresentation of\nalternative methodologies. We rely on the LLMs to generate topic\nmodeling and text summarization, which might create bias from\nthe dataset on which LLMs are trained. Also, the GPT model trained\non a specific date and didn‚Äôt include information after their last\ntraining data update. LLM might have certain biases, such as lan-\nguage bias and demographic bias. Future work will also incorporate\nRetrieval Augmented Generation (RAG) with LLM, fine-tuning with\nLlama 3, ensuring different viewpoints, with more diverse dataset,\nand utilizing an iterative approach with a human feedback loop to\nalleviate these problems. We will extend to diverse datasets from\nvarious venues over a broader timeframe with different LLM fine-\ntuning approaches. We will take the human-generated summary as\na ground truth to evaluate text summarization. We will extend our\nwork using qualitative analysis for the performance of LLMs for\ntext summarization.\nREFERENCES\n[1] Herman Aguinis, Ravi S Ramani, and Nawaf Alabduljader. 2018. What you see is\nwhat you get? Enhancing methodological transparency in management research.\nAcademy of Management Annals12, 1 (2018), 83‚Äì110.\n[2] Ibrahim Al Azher and Hamed Alhoori. 2024. Mitigating Visual Limitations of\nResearch Papers. In 2024 IEEE International Conference on Big Data (BigData).\nIEEE, Online, 8614‚Äì8616. https://doi.org/10.1109/BigData62323.2024.10826112\n[3] Daniel M Altmann, Daniel C Douek, and Rosemary J Boyton. 2020. What policy\nmakers need to know about COVID-19 protective immunity. The Lancet395,\n10236 (2020), 1527‚Äì1529.\n[4] Ibrahim Al Azhar, Sohel Ahmed, Md. Saiful Islam, and Aisha Khatun. 2021.\nIdentifying Author in Bengali Literature by Bi-LSTM with Attention Mechanism.\nIn 2021 24th International Conference on Computer and Information Technology\n(ICCIT). IEEE, Online, 1‚Äì6. https://doi.org/10.1109/ICCIT54785.2021.9689840\n[5] Andrew Booth, Martyn-St James, Mark Clowes, Anthea Sutton, et al. 2021. Sys-\ntematic approaches to a successful literature review. SAGE Publications Ltd1, 1\n(2021), 3‚Äì8.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems33 (2020), 1877‚Äì1901.\n[7] Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2023. BooookScore: A\nsystematic exploration of book-length summarization in the era of LLMs. arXiv\npreprint arXiv:2310.007852, 11 (2023), 205.\n[8] Carrie Conaway, Venessa Keesler, and Nathaniel Schwartz. 2015. What research\ndo state education agencies really need? The promise and limitations of state\nlongitudinal data systems. Educational Evaluation and Policy Analysis37, 1_suppl\n(2015), 16S‚Äì28S.\nLimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations\n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\n2023. QLoRA: Efficient Finetuning of Quantized LLMs. In Advances in\nNeural Information Processing Systems, A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., On-\nline, 10088‚Äì10115. https://proceedings.neurips.cc/paper_files/paper/2023/file/\n1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171‚Äì4186. https://doi.\norg/10.18653/v1/N19-1423\n[11] Abdur Rahman Bin Mohammed Faizullah, Ashok Urlana, and Rahul Mishra. 2024.\nLimGen: Probing the LLMs for Generating Suggestive Limitations of Research\nPapers. In Machine Learning and Knowledge Discovery in Databases. Research\nTrack, Albert Bifet, Jesse Davis, Tomas Krilaviƒçius, Meelis Kull, Eirini Ntoutsi,\nand IndrÀôe ≈ΩliobaitÀôe (Eds.). Springer Nature Switzerland, Cham, 106‚Äì124.\n[12] Cole Freeman, Hamed Alhoori, and Murtuza Shahzad. 2020. Measuring the\ndiversity of Facebook reactions to research. Proceedings of the ACM on Human-\nComputer Interaction4, GROUP (2020), 1‚Äì17.\n[13] Gene V Glass. 1976. Primary, secondary, and meta-analysis of research. Educa-\ntional researcher5, 10 (1976), 3‚Äì8.\n[14] Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based\nTF-IDF procedure. arXiv preprint arXiv:2203.057942, 11 (2022), 205.\n[15] Yash Gupta, Pawan Sasanka Ammanamanchi, Shikha Bordia, Arjun Manoha-\nran, Deepak Mittal, Ramakanth Pasunuru, Manish Shrivastava, Maneesh Singh,\nMohit Bansal, and Preethi Jyothi. 2021. The Effect of Pretraining on Extractive\nSummarization for Scientific Documents. In Proceedings of the Second Workshop\non Scholarly Document Processing, Iz Beltagy, Arman Cohan, Guy Feigenblat,\nDayne Freitag, Tirthankar Ghosal, Keith Hall, Drahomira Herrmannova, Petr\nKnoth, Kyle Lo, Philipp Mayr, Robert M. Patton, Michal Shmueli-Scheuer, Anita\nde Waard, Kuansan Wang, and Lucy Lu Wang (Eds.). Association for Computa-\ntional Linguistics, Online, 73‚Äì82. https://doi.org/10.18653/v1/2021.sdp-1.9\n[16] Benjamin Haibe-Kains, George Alexandru Adam, Ahmed Hosny, Farnoosh Kho-\ndakarami, Massive Analysis Quality Control (MAQC) Society Board of Direc-\ntors Shraddha Thakkar 35 Kusko Rebecca 36 Sansone Susanna-Assunta 37 Tong\nWeida 35 Wolfinger Russ D. 38 Mason Christopher E. 39 Jones Wendell 40 Dopazo\nJoaquin 41 Furlanello Cesare 42, Levi Waldron, Bo Wang, Chris McIntosh, Anna\nGoldenberg, Anshul Kundaje, et al. 2020. Transparency and reproducibility in\nartificial intelligence. Nature 586, 7829 (2020), E14‚ÄìE16.\n[17] Noriko Hara, Paul Solomon, Seung-Lye Kim, and Diane H Sonnenwald. 2003.\nAn emerging view of scientific collaboration: Scientists‚Äô perspectives on collabo-\nration and factors that impact collaboration. Journal of the American Society for\nInformation science and Technology54, 10 (2003), 952‚Äì965.\n[18] Monique Hennink and Rob Stephenson. 2005. Using research to inform health\npolicy: barriers and strategies in developing countries. Journal of health commu-\nnication 10, 2 (2005), 163‚Äì180.\n[19] Hospice Houngbo and Robert E Mercer. 2012. Method mention extraction\nfrom scientific research papers. In Proceedings of COLING 2012. Association for\nComputational Linguistics, Online, 1211‚Äì1222.\n[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.096852, 11 (2021), 205.\n[21] Adnan A Hyder, Adrijana Corluka, Peter J Winch, Azza El-Shinnawy, Harith\nGhassany, Hossein Malekafzali, Meng-Kin Lim, Joseph Mfutso-Bengo, Elsa Se-\ngura, and Abdul Ghaffar. 2011. National policy-makers speak out: are researchers\ngiving them what they need? Health policy and planning26, 1 (2011), 73‚Äì82.\n[22] John PA Ioannidis. 2005. Why most published research findings are false. PLoS\nmedicine 2, 8 (2005), e124.\n[23] Roope Kajoluoto. 2024. Internet-scale Topic Modeling using Large Language\nModels. aaltodoc 3, 11 (2024).\n[24] Ken Kato, Annabelle Purnomo, Christopher Cochrane, and Raeid Saqur.\n2024. L (u) PIN: LLM-based Political Ideology Nowcasting. arXiv preprint\narXiv:2405.07320 12, 11 (2024).\n[25] Madian Khabsa, Pucktada Treeratpituk, and C Lee Giles. 2012. Ackseer: a repos-\nitory and search engine for automatically extracted acknowledgments from\ndigital libraries. In Proceedings of the 12th ACM/IEEE-CS joint conference on Digi-\ntal Libraries. Association for Computing Machinery, Online, 185‚Äì194.\n[26] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\nIwasawa. 2022. Large language models are zero-shot reasoners. Advances in\nneural information processing systems35 (2022), 22199‚Äì22213.\n[27] Boshko Koloski, Nada Lavraƒç, Bojan Cestnik, Senja Pollak, Bla≈æ ≈†krlj, and Andrej\nKastrin. 2024. AHAM: Adapt, Help, Ask, Model Harvesting LLMs for Litera-\nture Mining. In International Symposium on Intelligent Data Analysis. Springer,\nSpringer, Online, 254‚Äì265.\n[28] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out. Association for Computational Linguistics,\nOnline, 74‚Äì81.\n[29] Meng Ling and Jian Chen. 2020. DeepPaperComposer: A Simple Solution for\nTraining Data Preparation for Parsing Research Papers. In Proceedings of the\nFirst Workshop on Scholarly Document Processing, Muthu Kumar Chandrasekaran,\nAnita de Waard, Guy Feigenblat, Dayne Freitag, Tirthankar Ghosal, Eduard\nHovy, Petr Knoth, David Konopnicki, Philipp Mayr, Robert M. Patton, and Michal\nShmueli-Scheuer (Eds.). Association for Computational Linguistics, Online, 91‚Äì\n96. https://doi.org/10.18653/v1/2020.sdp-1.10\n[30] Leland McInnes, John Healy, Steve Astels, et al . 2017. hdbscan: Hierarchical\ndensity based clustering. J. Open Source Softw.2, 11 (2017), 205.\n[31] Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform man-\nifold approximation and projection for dimension reduction. arXiv preprint\narXiv:1802.03426 10, 11 (2018).\n[32] David Nicholas, Anthony Watkinson, Eti Herman, Carol Tenopir, Rachel Volen-\ntine, Suzie Allard, and Kenneth Levine. 2015. Do younger researchers assess\ntrustworthiness differently when deciding what to read and cite and where to\npublish? International Journal of Knowledge Content Development & Technology\n5, 2 (2015), 45‚Äì63.\n[33] Allard Oelen, Mohamad Yaser Jaradeh, Markus Stocker, and S√∂ren Auer. 2020.\nGenerate FAIR literature surveys with scholarly knowledge graphs. InProceedings\nof the ACM/IEEE Joint Conference on Digital Libraries in 2020. Association for\nComputing Machinery, Online, 97‚Äì106.\n[34] Kathryn Oliver, Theo Lorenc, Jane Tinkler, and Chris Bonell. 2019. Understanding\nthe unintended consequences of public health policies: the views of policymakers\nand evaluators. BMC public health19 (2019), 1‚Äì9.\n[35] Justin C Ortagus, Robert Kelchen, Kelly Rosinger, and Nicholas Voorhees. 2020.\nPerformance-based funding in American higher education: A systematic synthe-\nsis of the intended and unintended consequences. Educational Evaluation and\nPolicy Analysis42, 4 (2020), 520‚Äì550.\n[36] Laurence J O‚ÄôToole. 1986. Policy recommendations for multi-actor implementa-\ntion: An assessment of the field. Journal of public policy6, 2 (1986), 181‚Äì210.\n[37] Krutarth Patel and Cornelia Caragea. 2021. Exploiting position and contextual\nword embeddings for keyphrase extraction from scientific papers. In Proceedings\nof the 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume. Association for Computational Linguistics, Online,\n1585‚Äì1591.\n[38] Roger A Pielke Jr. 2007. The honest broker: making sense of science in policy and\npolitics. Cambridge University Press, Online.\n[39] Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023. Summarization is (almost) dead.\narXiv preprint arXiv:2309.095582, 11 (2023).\n[40] Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X Zhou, and Weihong Qian. 2009.\nTopic and keyword re-ranking for LDA-based topic modeling. InProceedings of\nthe 18th ACM conference on Information and knowledge management. Association\nfor Computing Machinery, Online, 1757‚Äì1760.\n[41] Mike Thelwall, Subreena Simrick, Ian Viney, and Peter Van den Besselaar. 2023.\nWhat is research funding, how does it influence research, and how is it recorded?\nKey dimensions of variation. Scientometrics 128, 11 (2023), 6085‚Äì6106.\n[42] Daniel Varab and Yumo Xu. 2023. Abstractive Summarizers are Excellent Extrac-\ntive Summarizers. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers). Association for Computa-\ntional Linguistics, Online, 330‚Äì339.\n[43] Han Wang, Nirmalendu Prakash, Nguyen Khoi Hoang, Ming Shan Hee, Usman\nNaseem, and Roy Ka-Wei Lee. 2023. Prompting Large Language Models for Topic\nModeling. In 2023 IEEE International Conference on Big Data (BigData). IEEE,\nOnline, 1236‚Äì1241. https://doi.org/10.1109/BigData59044.2023.10386113\n[44] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.111712, 11\n(2022).\n[45] Kyle Williams, Jian Wu, Sagnik Ray Choudhury, Madian Khabsa, and C Lee Giles.\n2014. Scholarly big data information extraction and integration in the citeseer\nùúí digital library. In 2014 IEEE 30th international conference on data engineering\nworkshops. IEEE, IEEE, Online, 68‚Äì73.\n[46] Jian Wu, Jason Killian, Huaiyu Yang, Kyle Williams, Sagnik Ray Choudhury,\nSuppawong Tuarob, Cornelia Caragea, and C Lee Giles. 2015. Pdfmef: A multi-\nentity knowledge extraction framework for scholarly documents and semantic\nsearch. In Proceedings of the 8th International Conference on Knowledge Capture.\nAssociation for Computing Machinery, Online, 1‚Äì8.\n[47] Benjamin Yu. 2022. Evaluating Pre-Trained Language Models on Multi-Document\nSummarization for Literature Reviews. In Proceedings of the Third Workshop on\nScholarly Document Processing, Arman Cohan, Guy Feigenblat, Dayne Freitag,\nTirthankar Ghosal, Drahomira Herrmannova, Petr Knoth, Kyle Lo, Philipp Mayr,\nMichal Shmueli-Scheuer, Anita de Waard, and Lucy Lu Wang (Eds.). Association\nfor Computational Linguistics, Gyeongju, Republic of Korea, 188‚Äì192. https:\n//aclanthology.org/2022.sdp-1.22\nI.A Azher et al.\n[48] Chin-Hsien Yu, Xiuqin Wu, Dayong Zhang, Shi Chen, and Jinsong Zhao. 2021.\nDemand for green finance: Resolving financing constraints on green innovation\nin China. Energy policy153 (2021), 112255.\n[49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information\nProcessing Systems36 (2024).",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.932822048664093
    },
    {
      "name": "Computer science",
      "score": 0.7259359359741211
    },
    {
      "name": "Information retrieval",
      "score": 0.4836257994174957
    },
    {
      "name": "Data science",
      "score": 0.43075641989707947
    },
    {
      "name": "Natural language processing",
      "score": 0.37844228744506836
    }
  ]
}