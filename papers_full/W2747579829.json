{
    "title": "Neural Networks Compression for Language Modeling",
    "url": "https://openalex.org/W2747579829",
    "year": 2017,
    "authors": [
        {
            "id": null,
            "name": "Grachev, Artem M.",
            "affiliations": [
                "National Research University Higher School of Economics",
                "Samsung (Russia)"
            ]
        },
        {
            "id": "https://openalex.org/A4287523323",
            "name": "Ignatov, Dmitry I.",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        },
        {
            "id": "https://openalex.org/A4221471602",
            "name": "Savchenko, Andrey V.",
            "affiliations": [
                "National Research University Higher School of Economics"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963991999",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W2755517410",
        "https://openalex.org/W1993482030",
        "https://openalex.org/W569478347",
        "https://openalex.org/W2119144962",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2553910756",
        "https://openalex.org/W2707890836",
        "https://openalex.org/W1899504021",
        "https://openalex.org/W2559813832",
        "https://openalex.org/W2963042606",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W2175402905",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W1798945469"
    ],
    "abstract": null,
    "full_text": "arXiv:1708.05963v1  [stat.ML]  20 Aug 2017\nNeural Networks Compression for Language\nModeling\nArtem M. Grachev 1,2, Dmitry I. Ignatov 2, and Andrey V. Savchenko 3\n1 Samsung R&D Institute Rus, Moscow, Russia\n2 National Research University Higher School of Economics, M oscow, Russia\n3 National Research University Higher School of Economics, L aboratory of\nAlgorithms and T echnologies for Network Analysis, Nizhny N ovgorod, Russia\ngrachev.art@gmail.com\nAbstract. In this paper, we consider several compression techniques\nfor the language modeling problem based on recurrent neural networks\n(RNNs). It is known that conventional RNNs, e.g, LSTM-based networks\nin language modeling, are characterized with either high sp ace complex-\nity or substantial inference time. This problem is especial ly crucial for\nmobile applications, in which the constant interaction wit h the remote\nserver is inappropriate. By using the Penn T reebank (PTB) da taset we\ncompare pruning, quantization, low-rank factorization, t ensor train de-\ncomposition for LSTM networks in terms of model size and suit ability\nfor fast inference.\nKeywords: LSTM, RNN, language modeling, low-rank factorization,\npruning, quantization\n1 Introduction\nNeural network models can require a lot of space on disk and in memory . They\ncan also need a substantial amount of time for inference. Thi s is especially im-\nportant for models that we put on devices like mobile phones. There are several\napproaches to solve these problems. Some of them are based on sparse compu-\ntations. They also include pruning or more advanced methods . In general, such\napproaches are able to provide a large reduction in the size o f a trained net-\nwork, when the model is stored on a disk. However, there are so me problems\nwhen we use such models for inference. They are caused by high computation\ntime of sparse computing. Another branch of methods uses diﬀ erent matrix-\nbased approaches in neural networks. Thus, there are method s based on the\nusage of T oeplitz-like structured matrices in [1] or diﬀere nt matrix decomposi-\ntion techniques: low-rank decomposition [1], TT-decompos ition (T ensor T rain\ndecomposition) [2,3]. Also [4] proposes a new type of RNN, ca lled uRNN (Uni-\ntary Evolution Recurrent Neural Networks).\nIn this paper, we analyze some of the aforementioned approac hes. The mate-\nrial is organized as follows. In Section 2, we give an overvie w of language mod-\neling methods and then focus on respective neural networks a pproaches. Next\nwe describe diﬀerent types of compression. In Section 3.1, w e consider the sim-\nplest methods for neural networks compression like pruning or quantization. In\nSection 3.2, we consider approaches to compression of neura l networks based on\ndiﬀerent matrix factorization methods. Section 3.3 deals w ith TT-decomposition.\nSection 4 describes our results and some implementation det ails. Finally , in Sec-\ntion 5, we summarize the results of our work.\n2 Language modeling with neural networks\nConsider the language modeling problem. W e need to compute t he probability\nof a sentence or sequence of words (w1, . . . , w T ) in a language L.\nP(w1, . . . , w T ) =P(w1, . . . , w T − 1)P(wT |w1, . . . , w T − 1) =\n=\nT∏\nt=1\nP(wt|w1, . . . , w t− 1) (1)\nThe use of such a model directly would require calculation P(wt|w1, . . . , w t− 1)\nand in general it is too diﬃcult due to a lot of computation ste ps. That is\nwhy a common approach features computations with a ﬁxed valu e of N and\napproximate (1) with P(wt|wt− N , . . . , w t− 1). This leads us to the widely known\nN-gram models [5,6]. It was very popular approach until the mi ddle of the 2000s.\nA new milestone in language modeling had become the use of rec urrent neural\nnetworks [7]. A lot of work in this area was done by Thomas Miko lov [8].\nConsider a recurrent neural network, RNN, where N is the number of timesteps,\nL is the number of recurrent layers, xt− 1\nℓ is the input of the layer ℓ at the moment\nt. Here t ∈ { 1, . . . , N }, ℓ ∈ { 1, . . . , L }, and xt\n0 is the embedding vector. W e can\ndescribe each layer as follows:\nzt\nℓ =Wℓxt\nℓ− 1 + Vℓxt− 1\nℓ + bl (2)\nxt\nℓ =σ(zt\nℓ), (3)\nwhere Wℓ and Vℓ are matrices of weights and σ is an activation function. The\noutput of the network is given by\nyt = softmax\n[\nWL+1xt\nL + bL+1\n]\n. (4)\nThen, we deﬁne\nP(wt|wt− N , . . . , w t− 1) =yt. (5)\nWhile N-gram models even with not very big N require a lot of space due\nto the combinatorial explosion, neural networks can learn s ome representations\nof words and their sequences without memorizing directly al l options.\nNow the mainly used variations of RNN are designed to solve th e problem of\ndecaying gradients [9]. The most popular variation is Long S hort-T erm Memory\n(LSTM) [7] and Gated Recurrent Unit (GRU) [10]. Let us descri be one layer of\nLSTM:\nit\nℓ = σ\n[\nW i\nl xt\nl− 1 + V i\nl xt− 1\nl + bi\nl\n]\ninput gate (6)\nft\nℓ = σ\n[\nW f\nl xt\nl− 1 + V f\nl xt− 1\nl + bf\nl\n]\nforget gate (7)\nct\nℓ = ft\nl ·ct− 1\nl + it\nl tanh\n[\nW c\nl xt\nl− 1 + Uc\nl xt− 1\nl + bc\nl\n]\ncell state (8)\not\nℓ = σ\n[\nW o\nl xt\nℓ− 1 + V o\nl xt− 1\nl + bo\nl\n]\noutput gate (9)\nxt\nℓ = ot\nℓ ·tanh[ct\nl ], (10)\nwhere again t ∈ { 1, . . . , N }, ℓ ∈ { 1, . . . , L }, ct\nℓ is the memory vector at the layer\nℓ and time step t. The output of the network is given the same formula 4 as\nabove.\nApproaches to the language modeling problem based on neural networks are\neﬃcient and widely adopted, but still require a lot of space. In each LSTM layer\nof size k × k we have 8 matrices of size k × k. Moreover, usually the ﬁrst (or\nzero) layer of such a network is an embedding layer that maps w ord’s vocabulary\nnumber to some vector. And we need to store this embedding mat rix too. Its size\nis nvocab × k, where nvocab is the vocabulary size. Also we have an output softmax\nlayer with the same number of parameters as in the embedding, i.e. k× nvocab. In\nour experiments, we try to reduce the embedding size and to de compose softmax\nlayer as well as hidden layers.\nW e produce our experiments with compression on standard PTB models.\nThere are three main benchmarks: Small, Medium and Large LST M models\n[11]. But we mostly work with Small and Medium ones.\n3 Compression methods\n3.1 Pruning and quantization\nIn this subsection, we consider maybe not very eﬀective but s till useful tech-\nniques. Some of them were described in application to audio p rocessing [12]\nor image-processing [13,14], but for language modeling thi s ﬁeld is not yet well\ndescribed.\nPruning is a method for reducing the number of parameters of N N. In\nFig 1. (left), we can see that usually the majority of weight v alues are con-\ncentrated near zero. It means that such weights do not provid e a valuable con-\ntribution in the ﬁnal output. W e can set some threshold and th en remove all\nconnections with the weights below it from the network. Afte r that we retrain\nthe network to learn the ﬁnal weights for the remaining spars e connections.\nQuantization is a method for reducing the size of a compresse d neural net-\nwork in memory . W e are compressing each ﬂoat value to an eight -bit integer\nrepresenting the closest real number in one of 256 equally-s ized intervals within\nthe range.\nFig. 1. Weights distribution before and after pruning\n−3 −2 −1 0 1 2 3\nValue\n0\n20000\n40000\n60000\n80000\n100000\n120000\n140000\n160000\n180000Frequency\n−3 −2 −1 0 1 2 3\nValue\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000\n8000\n9000Frequency\nPruning and quantization have common disadvantages since t raining from\nscratch is impossible and their usage is quite laborious. In pruning the reason\nis mostly lies in the ineﬃciency of sparse computing. When we do quantization,\nwe store our model in an 8-bit representation, but we still ne ed to do 32-bits\ncomputations. It means that we have not advantages using RAM . At least until\nwe do not use the tensor processing unit (TPU) that is adopted for eﬀective 8-\nand 16-bits computations.\n3.2 Low-rank factorization\nLow-rank factorization represents more powerful methods. F or example, in [1],\nthe authors applied it to a voice recognition task. A simple f actorization can be\ndone as follows:\nxt\nl = σ\n[\nW a\nℓ W b\nℓ xt\nℓ− 1 + Ua\nl Ub\nl xt− 1\nℓ + bl\n]\n(11)\nF ollowing [1] require W b\nl = Ub\nℓ− 1. After this we can rewrite our equation for\nRNN:\nxt\nl = σ\n[\nW a\nl mt\nl− 1 + Ua\nl mt− 1\nl + bl\n]\n(12)\nmt\nl = Ub\nl xt\nl (13)\nyt = softmax\n[\nWL+1mt\nL + bL+1\n]\n(14)\nF or LSTM it is mostly the same with more complicated formulas . The main\nadvantage we get here from the sizes of matrices W a\nl , Ub\nl , Ua\nl . They have the\nsizes r × n and n × r, respectively , where the original Wl and Vl matrices have\nsize n × n. With small r we have the advantage in size and in multiplication\nspeed. W e discuss some implementation details in Section 4.\n3.3 The T ensor T rain decomposition\nIn the light of recent advances of tensor train approach [2,3 ], we have also decided\nto apply this technique to LSTM compression in language mode ling.\nThe tensor train decomposition was originally proposed as a n alternative\nand more eﬃcient form of tensor’s representation [15]. The T T-decomposition\n(or TT-representation) of a tensor A ∈ Rn1× ...× nd is the set of matrices Gk[jk] ∈\nRrk− 1× rk , where jk = 1, . . . , n k, k = 1, . . . , d , and r0 = rd = 1such that each of\nthe tensor elements can be represented as A(j1, j2, . . . , j d) =G1[j1]G2[j2] . . . Gd[jd].\nIn the same paper, the author proposed to consider the input m atrix as a multi-\ndimensional tensor and apply the same decomposition to it. I f we have matrix A\nof size N × M, we can ﬁx d and such n1, . . . , n d, m1, . . . , m d that the following\nconditions are fulﬁlled: ∏ d\nj=1 nj = N, ∏ d\ni=1 mi = M. Then we reshape our ma-\ntrix A to the tensor A with d dimensions and size n1m1 × n2m2 × . . . × ndmd.\nFinally , we can perform tensor train decomposition with thi s tensor. This ap-\nproach was successfully applied to compress fully connecte d neural networks [2]\nand for developing convolution TT layer [3].\nIn its turn, we have applied this approach to LSTM. Similarly , as we describe\nit above for usual matrix decomposition, here we also descri be only RNN layer.\nW e apply TT-decomposition to each of the matrices W and V in equation 2 and\nget:\nzt\nℓ = TT(Wi)xt\nℓ− 1 + TT(Vl)xt− 1\nℓ + bℓ. (15)\nHere TT(W ) means that we apply TT-decomposition for matrix W . It is nec-\nessary to note that even with the ﬁxed number of tensors in TT- decomposition\nand their sizes we still have plenty of variants because we ca n choose the rank\nof each tensor.\n4 Results\nF or testing pruning and quantization we choose Small PTB Ben chmark. The\nresults can be found in T able 1. W e can see that we have a reduct ion of the size\nwith a small loss of quality .\nF or matrix decomposition we perform experiments with Mediu m and Large\nPTB benchmarks. When we talk about language modeling, we mus t say that the\nembedding and the output layer each occupy one third of the to tal network size.\nIt follows us to the necessity of reducing their sizes too. W e reduce the output\nlayer by applying matrix decomposition. W e describe sizes o f LR LSTM 650-\n650 since it is the most useful model for the practical applicati on. W e start with\nbasic sizes for W and V , 650 × 650, and 10000 × 650 for embedding. W e reduce\neach W and V down to 650 × 128 and reduce embedding down to 10000 × 128.\nThe value 128 is chosen as the most suitable degree of 2 for eﬃc ient device\nimplementation. W e have performed several experiments, bu t this conﬁguration\nis near the best. Our compressed model, LR LSTM 650-650, is even smaller\nthan LSTM 200-200 with better perplexity . The results of experiments can be\nfound in T able 2.\nIn TT decomposition we have some freedom in way of choosing in ternal ranks\nand number of tensors. W e ﬁx the basic conﬁguration of an LSTM -network with\nT able 1.Pruning and quantization results on PTB dataset\nModel Size No. of params T est PP\nLSTM 200-200 (Small benchmark) 18.6 Mb 4.64 M 117.659\nPruning output layer 90%\nw/o additional training 5.5 Mb 0.5 M 149.310\nPruning output layer 90%\nwith additional training 5.5 Mb 0.5 M 121.123\nQuantization (1 byte per number) 4.7 Mb 4.64 M 118.232\ntwo 600-600 layers and four tensors for each matrix in a layer . And we perform\na grid search through diﬀerent number of dimensions and vari ous ranks.\nW e have trained about 100 models with using the Adam optimize r [16]. The\naverage training time for each is about 5-6 hours on GeF orce G TX TIT AN X\n(Maxwell architecture), but unfortunately none of them has achieved acceptable\nquality . The best obtained result ( TT LSTM 600-600 ) is even worse than\nLSTM-200-200 both in terms of size and perplexity .\nT able 2.Matrix decomposition results on PTB dataset\nModel Size No. of params T est PP\nPTB LSTM 200-200 18.6 Mb 4.64 M 117.659\nBenchmarks LSTM 650-650 79.1 Mb 19.7 M 82.07\nLSTM 1500-1500 264.1 Mb 66.02 M 78.29\nOurs LR LSTM 650-650 16.8 Mb 4.2 M 92.885\nTT LSTM 600-600 50.4 Mb 12.6 M 168.639\nLR LSTM 1500-1500 94.9 Mb 23.72 M 89.462\n5 Conclusion\nIn this article, we have considered several methods of neura l networks compres-\nsion for the language modeling problem. The ﬁrst part is abou t pruning and\nquantization. W e have shown that for language modeling ther e is no diﬀerence\nin applying of these two techniques. The second part is about matrix decompo-\nsition methods. W e have shown some advantages when we implem ent models on\ndevices since usually in such tasks there are tight restrict ions on the model size\nand its structure. F rom this point of view, the model LR LSTM 650-650 has\nnice characteristics. It is even smaller than the smallest b enchmark on PTB and\ndemonstrates quality comparable with the medium-sized ben chmarks on PTB.\nAcknowledgements. This study is supported by Russian F ederation President\ngrant MD-306.2017.9. A.V. Savchenko is supported by the Lab oratory of Algo-\nrithms and T echnologies for Network Analysis, National Res earch University\nHigher School of Economics.\nReferences\n1. Lu, Z., Sindhwan, V., Sainath, T.N.: Learning compact rec urrent neural networks.\nAcoustics, Speech and Signal Processing (ICASSP) (2016)\n2. Novikov, A., Podoprikhin, D., Osokin, A., Vetrov, D.P .: T ensorizing neural net-\nworks. In: Advances in Neural Information Processing Syste ms 28: Annual Con-\nference on Neural Information Processing Systems 2015. (20 15) 442–450\n3. Garipov, T., Podoprikhin, D., Novikov, A., Vetrov, D.P .: Ultimate tensorization:\ncompressing convolutional and FC layers alike. CoRR/NIPS 2 016 workshop: Learn-\ning with T ensors: Why Now and How? abs/1611.03214 (2016)\n4. Arjovsky, M., Shah, A., Bengio, Y.: Unitary evolution rec urrent neural networks.\nIn: Proceedings of the 33nd International Conference on Mac hine Learning, ICML\n2016. (2016) 1120–1128\n5. Jelinek, F.: Statistical Methods for Speech Recognition . MIT Press (1997)\n6. Kneser, R., Ney, H.: Improved backing-oﬀ for m-gram langu age modeling. In\nProceedings of the IEEE International Conference on Acoust ics, Speech and Signal\nProcessing 1 (1995) 181–184.\n7. Hochreiter, S., Schmidhuber, J.: Long short-term memory . Neural Computation\n(9(8)) (1997) 1735–1780\n8. Mikolov, T.: Statistical Language Models Based on Neural Networks. PhD thesis,\nBrno University of T echnology (2012)\n9. Hochreiter, S., Bengio, Y., F rasconi, P ., Schmidhuber, J .: Gradient ﬂow in recurrent\nnets: the diﬃculty of learning long-term dependencies. S. C . Kremer and J. F.\nKolen, eds. A Field Guide to Dynamical Recurrent Neural Netw orks (2001)\n10. Cho, K., van Merrienboer, B., Bahdanau, D., Bengio, Y.: O n the proper-\nties of neural machine translation: Encoder-decoder appro aches. arXiv preprint\narXiv:1409.1259, 2014f (2014)\n11. Zaremba, W., Sutskever, I., Vinyals, O.: Recurrent neur al network regularization.\nArxiv preprint (2014)\n12. Han, S., Mao, H., Dally, W.J.: Deep compression: Compres sing deep neural net-\nworks with pruning, trained quantization and huﬀman coding . Acoustics, Speech\nand Signal Processing (ICASSP) (2016)\n13. Molchanov, P ., Tyree, S., Karras, T., Aila, T., Kaut, J.: Pruning convolu-\ntional neural networks for resource eﬃcient transfer learn ing. arXiv preprint\narXiv:1611.06440 (2016)\n14. Rassadin, A.G., Savchenko, A.V.: Deep neural networks p erformance optimiza-\ntion in image recognition. Proceedings of the 3rd Internati onal Conference on\nInformation T echnologies and Nanotechnologies (ITNT) (20 17)\n15. Oseledets, I.V.: T ensor-train decomposition. SIAM J. S cientiﬁc Computing 33(5)\n(2011) 2295–2317\n16. Kingma, D., Ba, J.: Adam: A method for stochastic optimiz ation. The Interna-\ntional Conference on Learning Representations (ICLR) (201 5)"
}