{
  "title": "Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice",
  "url": "https://openalex.org/W3174848559",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3117627924",
      "name": "Rongzhou Bao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103656757",
      "name": "Jiayi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112311038",
      "name": "Hai Zhao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970078867",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W3155936402",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2460937040",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2998604177",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W3104570147",
    "https://openalex.org/W3015189805",
    "https://openalex.org/W2967658867",
    "https://openalex.org/W2950048339",
    "https://openalex.org/W2965595599",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2963109131",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W3117582978",
    "https://openalex.org/W2963542245",
    "https://openalex.org/W3035164976",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2970449623",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W4288594304",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2803392236",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2963920068",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963491027",
    "https://openalex.org/W2964232431",
    "https://openalex.org/W2036430923",
    "https://openalex.org/W2911634294",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W4297792735",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2982054702"
  ],
  "abstract": "Pre-trained contextualized language models (PrLMs) have led to strong performance gains in downstream natural language understanding tasks.However, PrLMs can still be easily fooled by adversarial word substitution, which is one of the most challenging textual adversarial attack methods.Existing defence approaches suffer from notable performance loss and complexities.Thus, this paper presents a compact and performance-preserved framework, Anomaly Detection with Frequency-Aware Randomization (ADFAR).In detail, we design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples.Then, in order to defend adversarial word substitution, a frequency-aware randomization process is applied to those recognized adversarial input samples.Empirical results show that AD-FAR significantly outperforms those newly proposed defense methods over various tasks with much higher inference speed.Remarkably, ADFAR does not impair the overall performance of PrLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3248–3258\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3248\nDefending Pre-trained Language Models\nfrom Adversarial Word Substitution Without Performance Sacriﬁce\nRongzhou Bao, Jiayi Wang, Hai Zhao∗\nDepartment of Computer Science and Engineering, Shanghai Jiao Tong University\n2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\nMoE Key Lab of Artiﬁcial Intelligence, AI Institute,\nShanghai Jiao Tong University, Shanghai, China\nrongzhou.bao@outlook.com\n{wangjiayi 102 23}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn\nAbstract\nPre-trained contextualized language models\n(PrLMs) have led to strong performance gains\nin downstream natural language understand-\ning tasks. However, PrLMs can still be easily\nfooled by adversarial word substitution, which\nis one of the most challenging textual adver-\nsarial attack methods. Existing defence ap-\nproaches suffer from notable performance loss\nand complexities. Thus, this paper presents\na compact and performance-preserved frame-\nwork, Anomaly Detection with Frequency-\nAware Randomization (ADFAR). In detail, we\ndesign an auxiliary anomaly detection clas-\nsiﬁer and adopt a multi-task learning proce-\ndure, by which PrLMs are able to distin-\nguish adversarial input samples. Then, in or-\nder to defend adversarial word substitution,\na frequency-aware randomization process is\napplied to those recognized adversarial input\nsamples. Empirical results show that AD-\nFAR signiﬁcantly outperforms those newly\nproposed defense methods over various tasks\nwith much higher inference speed. Remark-\nably, ADFAR does not impair the overall per-\nformance of PrLMs. The code is available at\nhttps://github.com/LilyNLP/ADFAR.\n1 Introduction\nDeep neural networks (DNNs) have achieved re-\nmarkable success in various areas. However, pre-\nvious works show that DNNs are vulnerable to\nadversarial samples (Goodfellow et al., 2015; Ku-\nrakin et al., 2017; Wang et al., 2021), which are\ninputs with small, intentional modiﬁcations that\ncause the model to make false predictions. Pre-\ntrained language models (PrLMs) (Devlin et al.,\n*Corresponding author. This paper was partially sup-\nported by National Key Research and Development Pro-\ngram of China (No. 2017YFB0304100), Key Projects of\nNational Natural Science Foundation of China (U1836222\nand 61733011). This work was supported by Huawei Noah’s\nAsk Lab\n2019; Liu et al., 2019; Clark et al., 2020; Zhang\net al., 2020, 2019) are widely adopted as an es-\nsential component for various NLP systems. How-\never, as DNN-based models, PrLMs can still be\neasily fooled by textual adversarial samples (Wal-\nlace et al., 2019; Jin et al., 2019; Nie et al., 2020;\nZang et al., 2020). Such vulnerability of PrLMs\nkeeps raising potential security concerns, therefore\nresearches on defense techniques to help PrLMs\nagainst textual adversarial samples are imperatively\nneeded.\nDifferent kinds of textual attack methods have\nbeen proposed, ranging from character-level word\nmisspelling (Gao et al., 2018), word-level sub-\nstitution (Alzantot et al., 2018; Ebrahimi et al.,\n2018; Ren et al., 2019; Jin et al., 2019; Zang\net al., 2020; Li et al., 2020; Garg and Ramakr-\nishnan, 2020), phrase-level insertion and removal\n(Liang et al., 2018), to sentence-level paraphrasing\n(Ribeiro et al., 2018; Iyyer et al., 2018). Thanks to\nthe discrete nature of natural language, attack ap-\nproaches that result in illegal or unnatural sentences\ncan be easily detected and restored by spelling cor-\nrection and grammar error correction (Islam and\nInkpen, 2009; Sakaguchi et al., 2017; Pruthi et al.,\n2019). However, attack approaches based on adver-\nsarial word substitution can produce high-quality\nand efﬁcient adversarial samples which are still\nhard to be detected by existing methods. Thus,\nthe adversarial word substitution keeps posing a\nlarger and more profound challenge for the robust-\nness of PrLMs. Therefore, this paper is devoted to\novercome the challenge posed by adversarial word\nsubstitution.\nSeveral approaches are already proposed to miti-\ngate issues posed by adversarial word substitution\n(Zhou et al., 2019; Jia et al., 2019; Huang et al.,\n2019; Cohen et al., 2019; Ye et al., 2020; Si et al.,\n2021). Although these defense methods manage to\nalleviate the negative impact of adversarial word\n3249\nsubstitution, they sometimes reduce the prediction\naccuracy for non-adversarial samples to a notable\nextent. Given the uncertainty of the existence of\nattack in real application, it is impractical to sacri-\nﬁce the original prediction accuracy for the purpose\nof defense. Moreover, previous defense methods\neither have strong limitations over the attack space\nto certify the robustness, or require enormous com-\nputation resources during training and inference.\nThus, it is imperatively important to ﬁnd an efﬁ-\ncient performance-preserved defense method.\nFor such purpose, we present a compact\nand performance-preserved framework, Anomaly\nDetection with Frequency-Aware Randomization\n(ADFAR), to help PrLMs defend against adversar-\nial word substitution without performance sacriﬁce.\nXie et al. (2018) show that introducing randomiza-\ntion at inference can effectively defend adversarial\nattacks. Moreover, (Mozes et al., 2020) indicate\nthat the usual case for adversarial samples is replac-\ning words with their less frequent synonyms, while\nPrLMs are more robust to frequent words. There-\nfore, we propose a frequency-aware randomization\nprocess to help PrLMs defend against adversarial\nword substitution.\nHowever, simply applying a randomization pro-\ncess to all input sentences would reduce the predic-\ntion accuracy for non-adversarial samples. In order\nto preserve the overall performance, we add an aux-\niliary anomaly detector on top of PrLMs and adopt\na multi-task learning procedure, by which PrLMs\nare able to determine whether each input sentence\nis adversarial or not, and not introduce extra model.\nThen, only those adversarial input sentences will\nundergo the randomization procedure, while the\nprediction process for non-adversarial input sen-\ntences remains the same.\nEmpirical results show that as a more efﬁcient\nmethod, ADFAR signiﬁcantly outperforms previ-\nous defense methods (Ye et al., 2020; Zhou et al.,\n2019) over various tasks, and preserves the predic-\ntion accuracy for non-adversarial sentences. Com-\nprehensive ablation studies and analysis further\nprove the efﬁciency of our proposed method, and\nindicate that the adversarial samples generated by\ncurrent heuristic word substitution strategy can be\neasily detected by the proposed auxiliary anomaly\ndetector.\n2 Related Work\n2.1 Adversarial Word Substitution\nAdversarial word substitution (AWS) is one of\nthe most efﬁcient approaches to attack advanced\nneural models like PrLMs. In AWS, an attacker\ndeliberately replaces certain words by their syn-\nonyms to mislead the prediction of the target model.\nAt the same time, a high-quality adversarial sam-\nple should maintain grammatical correctness and\nsemantic consistency. In order to craft efﬁcient\nand high-quality adversarial samples, an attacker\nshould ﬁrst determine the vulnerable tokens to be\nperturbed, and then choose suitable synonyms to\nreplace them.\nCurrent AWS models (Alzantot et al., 2018;\nEbrahimi et al., 2018; Ren et al., 2019; Jin et al.,\n2019; Li et al., 2020; Garg and Ramakrishnan,\n2020) adopt heuristic algorithms to locate vulner-\nable tokens in sentences. To illustrate, for a given\nsample and a target model, the attacker iteratively\nmasks the tokens and checks the output of the\nmodel. The tokens which have signiﬁcant inﬂuence\non the ﬁnal output logits are regarded as vulnerable.\nPrevious works leverage word embeddings such\nas GloVe (Pennington et al., 2014) and counter-\nﬁtted vectors (Mrk ˇsi´c et al., 2016) to search the\nsuitable synonym set of a given token. Li et al.\n(2020); Garg and Ramakrishnan (2020) uses BERT\n(Devlin et al., 2019) to generate perturbation for\nbetter semantic consistency and language ﬂuency.\n2.2 Defense against A WS\nFor general attack approaches, adversarial train-\ning (Goodfellow et al., 2015; Jiang et al., 2020) is\nwidely adopted to mitigate adversarial effect, but\n(Alzantot et al., 2018; Jin et al., 2019) shows that\nthis method is still vulnerable to AWS. This is be-\ncause AWS models leverage dynamic algorithms to\nattack the target model, while adversarial training\nonly involves a static training set.\nMethods proposed by Jia et al. (2019); Huang\net al. (2019) are proved effective for defence against\nAWS, but they still have several limitations. In\nthese methods, Interval Bound Propagation (IBP)\n(Dvijotham et al., 2018), an approach to consider\nthe worst-case perturbation theoretically, is lever-\naged to certify the robustness of models. However,\nIBP-based methods can only achieve the certiﬁed\nrobustness under a strong limitation over the attack\nspace. Furthermore, they are difﬁcult to adapt to\nPrLMs for their strong reliance on the assumption\n3250\nFigure 1: Frequency-aware randomization examples.\nof model architecture.\nTwo effective and actionable methods (DISP\n(Zhou et al., 2019) and SAFER Ye et al. (2020))\nare proposed to overcome the challenge posed by\nAWS, and therefore adopted as the baselines for\nthis paper. DISP (Zhou et al., 2019) is a frame-\nwork based on perturbation discrimination to block\nadversarial attack. In detail, when facing adver-\nsarial inputs, DISP leverages two auxiliary PrLMs:\none to detect perturbed tokens in the sentence, and\nanother to restore the abnormal tokens to original\nones. Inspired by randomized smoothing (Cohen\net al., 2019), Ye et al. (2020) proposes SAFER,\na novel framework that guarantees the robustness\nby smoothing the classiﬁer with synonym word\nsubstitution. To illustrate, based on random word\nsubstitution, SAFER smooths the classiﬁer by av-\neraging its outputs of a set of randomly perturbed\ninputs. SAFER outperforms IBP-based approaches\nand can be easily applied to PrLMs.\n2.3 Randomization\nIn recent years, randomization has been used as\na defense measure for deep learning in computer\nvision (Xie et al., 2018). Nevertheless, direct exten-\nsions of these measures to defend against textual\nadversarial samples are not achievable, since the\ntext inputs are discrete rather than continuous. Ye\net al. (2020) indicates the possibility of extending\nthe application of the randomization approach to\nNLP by randomly replacing the words in sentences\nwith their synonyms.\n3 Method\n3.1 Frequency-aware Randomization\nSince heuristic attack methods attack a model by\nsubstituting each word iteratively until it success-\nfully alters the model’s output, it is normally dif-\nﬁcult for static strategies to defense such kind of\ndynamic process. Rather, dynamic strategies, such\nas randomization, can better cope with the problem.\nIt is also observed that replacing words with their\nmore frequent alternatives can better mitigate the\nadversarial effect and preserve the original perfor-\nmance. Therefore, a frequency-aware randomiza-\ntion strategy is designed to perplex AWS strategy.\nFigure 1 shows several examples of the\nfrequency-aware randomization. The proposed ap-\nproach for the frequency-aware randomization is\nshown in Algorithm 1, and consists of three steps.\nFirstly, rare words with lower frequencies and a\nnumber of random words are selected as substitu-\ntion candidates. Secondly, we choose synonyms\nwith the closest meanings and the highest frequen-\ncies to form a synonym set for each candidate\nword. Thirdly, each candidate word is replaced\nwith a random synonym within its own synonym\nset. To quantify the semantic similarity between\ntwo words, we represent words with embeddings\nfrom (Mrkˇsi´c et al., 2016), which is specially de-\nsigned for synonyms identiﬁcation. The semantic\nsimilarity of two words are evaluated by cosine\nsimilarity of their embeddings. To determine the\nfrequency of a word, we use a frequency dictionary\nprovided by FrequencyWords Repository*.\n3.2 Anomaly Detection\nApplying the frequency-aware randomization pro-\ncess to every input can still reduce the prediction\naccuracy for normal samples. In order to overcome\nthis issue, we add an auxiliary anomaly detection\nhead to PrLMs and adopt a multi-task learning pro-\ncedure, by which PrLMs are able to classify the\ninput text and distinguish the adversarial samples\nat the same time, and not introduce extra model.\nIn inference, the frequency-aware randomization\nonly applied to the samples that are detected as\nadversarial. In this way, the reduction of accuracy\nis largely avoided, since non-adversarial samples\nare not affected.\nZhou et al. (2019) also elaborates the idea of per-\nturbation discrimination to block attack. However,\ntheir method detects anomaly on token-level and\nrequires two resource-consuming PrLMs for detec-\ntion and correction, while ours detects anomaly on\nsentence-level and requires no extra models. Com-\n*https://github.com/hermitdave/FrequencyWords\n3251\nAlgorithm 1 Frequency-aware Randomization\nInput: Sentence X = {w1, w2, ..., wn}, word em-\nbeddings Emb over the vocabulary V ocab\nOutput: Randomized sentence Xrand\n1: Initialization: Xrand ←X\n2: Create a set Wrare of all rare words with\nfrequencies less than fthres, denote nrare =\n|Wrare|.\n3: Create a set Wrand by randomly selecting n ∗\nr −nrare words wj /∈Wrare, where r is the\npre-deﬁned ratio of substitution.\n4: Create the substitution candidates set,Wsub ←\nWrare + Wrand, and |Wsub|= n ∗r.\n5: Filter out the stop words in Wsub.\n6: for each word wi in Wsub do\n7: Create a set S by extracting the top ns syn-\nonyms using CosSim(Embwi , Embwword )\nfor each word in V ocab.\n8: Create a set Sfreq by selecting the top nf\nfrequent synonyms from S.\n9: Randomly choose one word ws from S.\n10: Xrand ←Replace wi with ws in Xrand.\n11: end for\npared to Zhou et al. (2019), our method is two times\nfaster in inference speed and can achieve better ac-\ncuracy for sentence-level anomaly detection.\n3.3 Framework\nIn this section, we elaborate the framework of AD-\nFAR in both training and inference.\n3.3.1 Training\nFigure 2 shows the framework of ADFAR in train-\ning. We extend the baseline PrLMs by three ma-\nFigure 2: Framework of ADFAR in training.\njor modiﬁcations: 1) the construction of training\ndata, 2) the auxiliary anomaly detector and 3) the\ntraining objective, which will be introduced in this\nsection.\nConstruction of Training Data As shown in\nFigure 2, we combine the idea of both adversar-\nial training and data augmentation (Wei and Zou,\n2019) to construct our randomization augmented\nadversarial training data. Firstly, we use a heuristic\nAWS model (e.g. TextFooler) to generate adver-\nsarial samples based on the original training set.\nFollowing the common practice of adversarial train-\ning, we then combine the adversarial samples with\nthe original ones to form an adversarial training\nset. Secondly, in order to let PrLMs better cope\nwith randomized samples in inference, we apply\nthe frequency-aware randomization on the adver-\nsarial training set to generate a randomized adver-\nsarial training set. Lastly, the adversarial training\nset and the randomized adversarial training set are\ncombined to form a randomization augmented ad-\nversarial training set.\nAuxiliary Anomaly Detector In addition to the\noriginal text classiﬁer, we add an auxiliary anomaly\ndetector to the PrLMs to distinguish adversarial\nsamples. For an input sentence, the PrLMs cap-\ntures the contextual information for each token by\nself-attention and generates a sequence of contex-\ntualized embeddings {h0, . . . hm}. For text clas-\nsiﬁcation task, h0 ∈RH is used as the aggregate\nsequence representation. The original text classi-\nﬁer leverages h0 to predict the probability that X\nis labeled as class ˆyc by a logistic regression with\nsoftmax:\nyc = Prob ( ˆyc|x),\n= softmax(Wc(dropout(h0)) +bc),\nFor the anomaly detector, the probability that X\nis labeled as class ˆyd (if X is attacked, ˆyd = 1;\nif X is normal, ˆyd = 0) is predicted by a logistic\nregression with softmax:\nyd = Prob ( ˆyd|x),\n= softmax(Wd(dropout(h0)) +bd),\nAs shown in Figure 2, the original text classiﬁer\nis trained on the randomization augmented adver-\nsarial training set, whereas the anomaly detector is\nonly trained on the adversarial training set.\n3252\nFigure 3: Framework of ADFAR in inference.\nTraining Objective We adopt a multi-task learn-\ning framework, by which PrLM is trained to clas-\nsify the input text and distinguish the adversarial\nsamples at the same time. We design two parallel\ntraining objectives in the form of minimizing cross-\nentropy loss: lossc for text classiﬁcation and lossd\nfor anomaly detection. The total loss function is\ndeﬁned as their sum:\nlossc = −[yc ∗log( ˆyc) + (1−yc) ∗log(1 −ˆyc)]\nlossd = −[yd ∗log( ˆyd) + (1−yd) ∗log(1 −ˆyd)]\nLoss = lossc + lossd\n3.3.2 Inference\nFigure 3 shows the framework of ADFAR in infer-\nence. Firstly, the anomaly detector predicts whether\nan input sample is adversarial. If the input sample\nis determined as non-adversarial, the output of the\ntext classiﬁer (Label A) is directly used as its ﬁnal\nprediction. If the input sample is determined as ad-\nversarial, the frequency-aware randomization pro-\ncess is applied to the original input sample. Then,\nthe randomized sample is sent to the PrLM again,\nand the second output of the text classiﬁer (Label\nB) is used as its ﬁnal prediction.\n4 Experimental Implementation\n4.1 Tasks and Datasets\nExperiments are conducted on two major NLP\ntasks: text classiﬁcation and natural language infer-\nence. The dataset statistics are displayed in Table\n1. We evaluate the performance of models on the\nnon-adversarial test samples as the original accu-\nracy. Then we measure the after-attack accuracy\nof models when facing AWS. By comparing these\ntwo accuracy scores, we can evaluate how robust\nthe model is.\nTask Dataset Train Test Avg Len\nClassiﬁcation\nMR 9K 1K 20\nSST2 67K 1.8K 20\nIMDB 25K 25K 215\nEntailment MNLI 433K 10K 11\nTable 1: Dataset statistics.\nText Classiﬁcation We use three text classiﬁca-\ntion datasets with average text lengths from 20 to\n215 words, ranging from phrase-level to document-\nlevel tasks. SST2 (Socher et al., 2013): phrase-\nlevel binary sentiment classiﬁcation using ﬁne-\ngrained sentiment labels on movie reviews. MR\n(Pang and Lee, 2005): sentence-level binary senti-\nment classiﬁcation on movie reviews. We take 90%\nof the data as training set and 10% of the data as test\nset as (Jin et al., 2019). IMDB (Maas et al., 2011):\ndocument-level binary sentiment classiﬁcation on\nmovie reviews.\nNatural Language Inference NLI aims at deter-\nmining the relationship between a pair of sentences\nbased on semantic meanings. We use Multi-Genre\nNatural Language Inference (MNLI) (Nangia et al.,\n2017), a widely adopted NLI benchmark with cov-\nerage of transcribed speech, popular ﬁction, and\ngovernment reports.\n4.2 Attack Model and Baselines\nWe use TextFooler†(Jin et al., 2019) as the major\nattack model for AWS. Moreover, we implement\n(Ren et al., 2019) and GENETIC (Alzantot et al.,\n2018) based on the TextAttack (Morris et al., 2020)\ncode base to further verify the efﬁciency of our\nproposed method.\nWe compare ADFAR with DISP (Zhou et al.,\n2019) and SAFER (Ye et al., 2020). The implemen-\ntation of DISP is based on the repository offered by\nZhou et al. (2019). For SAFER, we also leverage\nthe code proposed by Ye et al. (2020). Necessary\nmodiﬁcations are made to evaluate these methods’\nperformance under heuristic attack models.\n4.3 Experimental Setup\nThe implementation of PrLMs is based on Py-\nTorch‡. We leverage, BERT BASE (Devlin et al.,\n2019), RoBERTa BASE (Liu et al., 2019) and\nELECTRABASE (Clark et al., 2020) as baseline\n†https://github.com/jind11/TextFooler\n‡https://github.com/joey1993/bert-defender\n§https://github.com/lushleaf/Structure-free-certiﬁed-NLP\n3253\nModel MR SST2 IMDB MNLI\nOrig. Acc. Adv. Acc. Orig. Acc. Adv. Acc. Orig. Acc. Adv. Acc. Orig. Acc. Adv. Acc.\nBERT 86.2 16.9 93.1 39.8 92.4 12.4 84.0 11.3\nBERT + Adv Training 85.6 34.6 92.6 48.8 92.2 34.2 82.3 33.4\nBERT + DISP 82.0 42.2 91.6 70.4 91.7 82.0 76.3 35.1\nBERT + SAFER 79.0 55.4 91.3 75.6 91.3 88.1 82.1 54.7\nBERT + ADFAR 86.6 66.0 92.4 75.6 92.8 89.2 82.6 67.8\nTable 2: The performance of ADFAR and other defense frameworks using BERTBASE as PrLM and TextFooler as\nattack model. Orig. Acc. is the prediction accuracy of normal samples and Adv. Acc. is the after-attack accuracy\nof models when facing AWS. The results are based on the average of ﬁve runs.\nPrLMs. We use AdamW (Loshchilov and Hutter,\n2018) as our optimizer with a learning rate of 3e-5\nand a batch size of 16. The number of epochs is set\nto 5.\nFor the frequency-aware randomization process,\nwe set fthres = 200, ns = 20 and nf = 10. In\nthe adopted frequency dictionary, 5.5k out of 50k\nwords have a frequency lower than fthres = 200\nand therefore regarded as rare words. r is set to\ndifferent values for training ( 25%) and inference\n(30%) due to different aims. In training, to avoid\nintroducing excessive noise and reduce the predic-\ntion accuracy for non-adversarial samples, r is set\nto be relatively low. On the contrary, in inference,\nour aim is to perplex the heuristic attack mecha-\nnism. The more randomization we add, the more\nperplexities the attack mechanism receives, there-\nfore we set a relatively higher value for r. More\ndetails on the choice of these hyperparameters will\nbe discussed in the analysis section.\n5 Experimental Results\n5.1 Main results\nFollowing (Jin et al., 2019), we leverage BERTBASE\n(Devlin et al., 2019) as baseline PrLM and\nTextFooler as attack model. Table 2 shows the\nperformance of ADFAR and other defense frame-\nworks. Since randomization may lead to a variance\nof the results, we report the results based on the\naverage of ﬁve runs. Experimental results indicate\nthat ADFAR can effectively help PrLM against\nAWS. Compared with DISP (Zhou et al., 2019)\nand SAFER (Ye et al., 2020), ADFAR achieves the\nbest performance for adversarial samples. Mean-\nwhile, ADFAR does not hurt the performance for\nnon-adversarial samples in general. On tasks such\nas MR and IMDB, ADFAR can even enhance the\nbaseline PrLM.\nDISP leverages two extra PrLMs to discriminate\n¶https://github.com/huggingface\nand recover the perturbed tokens, which introduce\nextra complexities. SAFER makes the prediction\nof an input sentence by averaging the prediction\nresults of its perturbed alternatives, which multiply\nthe inference time. As shown in Table 3, compared\nwith previous methods, ADFAR achieves a signiﬁ-\ncantly higher inference speed.\nModel Parameters Inference Time\nBERTBASE 110M 15.7ms (100%)\nBERTBASE + DISP 330M 38.9ms (247%)\nBERTBASE + SAFER 110M 27.6ms (176%)\nBERTBASE + ADFAR 110M 18.1ms (115%)\nTable 3: Parameters and Inference Time statistics.\nThe inference time indicate the average inference time\nfor one sample in MR dataset using one NVIDIA\nRTX3090.\n5.2 Results with Different Attack Strategy\nSince ADFAR leverages the adversarial samples\ngenerated by TextFooler (Jin et al., 2019) in train-\ning, it is important to see whether ADFAR also\nperforms well when facing adversarial samples gen-\nerated by other AWS models. We leverage PWWS\n(Ren et al., 2019) and GENETIC (Alzantot et al.,\n2018) to further study the performance of ADFAR.\nAttack MR SST2\nBERT +ADFAR BERT +ADFAR\nAttack-Free 86.2 86.6 93.1 92.3\nPWWS 34.2 74.2 54.3 80.5\nGenetic 21.3 70.4 38.7 72.2\nTextFooler 16.9 66.0 39.8 73.8\nTable 4: Performance of BERT and BERT with AD-\nFAR when facing various AWS models. The results\nare based on the average of ﬁve runs.\nAs shown is Table 4, the performance of ADFAR\nis not affected by different AWS models, which\nfurther proves the efﬁcacy of our method.\n3254\n5.3 Results with Other PrLMs\nTable 5 shows the performance of ADFAR\nleveraging RoBERTaBASE (Liu et al., 2019) and\nELECTRABASE (Clark et al., 2020) as PrLMs. In\norder to enhance the robustness and performance\nof the PrLM, RoBERTa extends BERT with a\nlarger corpus and using more efﬁcient parameters,\nwhile ELECTRA applies a GAN-style architec-\nture for pre-training. Empirical results indicate\nthat ADFAR can further improve the robustness of\nRoBERTa and ELECTRA while preserving their\noriginal performance.\nPrLM MR SST2\nOrig. Acc. Adv. Acc. Orig. Acc. Adv. Acc.\nBERT 86.2 16.9 93.1 39.8\n+ADFAR 86.6 66.0 92.3 73.8\nRoBERTa 88.3 30.4 93.4 37.4\n+ADFAR 87.2 71.0 93.2 77.6\nELECTRA 90.1 33.6 94.2 40.4\n+ADFAR 90.4 71.2 95.0 83.0\nTable 5: Results based with various PrLMs.\n6 Analysis\n6.1 Ablation Study\nADFAR leverages three techniques to help PrLMs\ndefend against adversarial samples: adversar-\nial training, frequency-aware randomization and\nanomaly detection. To evaluate the contributions of\nthese techniques in ADFAR, we perform ablation\nstudies on MR and SST2 using BERTBASE as our\nPrLMs, and TextFooler as the attack model. As\nshown in Table 6, the frequency-aware randomiza-\ntion is the key factor which helps PrLM defense\nagainst adversarial samples, while anomaly detec-\ntion plays an important role in preserving PrLM’s\nprediction accuracy for non-adversarial samples.\nModel MR SST2\nOrig. Acc. Adv. Acc. Orig. Acc. Adv. Acc.\nBERT 86.2 16.9 93.1 39.8\n+ Adv 85.6 34.6 92.6 48.8\n+ FR 85.0 72.8 90.6 82.6\n+ AD 86.6 66.0 92.3 73.8\nTable 6: Ablation study on MR and SST2 us-\ning BERT BASE as PrLM, and TextFooler as attack\nmodel. Adv represents adversarial training, FR indi-\ncates frequency-aware randomization and AD means\nanomaly detection. The results are based on the aver-\nage of ﬁve runs.\n6.2 Anomaly Detection\nIn this section, we compare the anomaly detection\ncapability between ADFAR and DISP (Zhou et al.,\n2019). ADFAR leverages an auxiliary anomaly\ndetector, which share a same PrLM with the origi-\nnal text classiﬁer, to discriminate adversarial sam-\nples. DISP uses an discriminator based on an extra\nPrLMs to identify the perturbed adversarial inputs,\nbut on token level. For DISP, in order to detect\nanomaly on sentence level, input sentences with\none or more than one adversarial tokens identi-\nﬁed by DISP are regarded as adversarial samples.\nWe respectively sample 500 normal and adversar-\nial samples from the test set of MR and SST to\nevaluate the performance of ADFAR and DISP for\nanomaly detection.\nTable 7 shows the performance of ADFAR and\nDISP for anomaly detection. Empirical results\nshow that ADFAR can predict more precisely, since\nit achieves a signiﬁcantly higher F1 score than\nDISP. Moreover, ADFAR has a simpler framework,\nas its anomaly detector shares the same PrLM with\nthe classiﬁer, while DISP requires an extra PrLM.\nThe results also indicate that the current heuristic\nAWS strategy is vulnerable to our anomaly detector,\nwhich disproves the claimed undetectable feature\nof this very adversarial strategy.\nMethod MR SST2\nPrecision Recall F1 Precision Recall F1\nDISP 68.0 92.0 73.2 59.5 94.2 72.9\nADFAR 90.1 84.0 86.9 88.0 90.0 88.9\nTable 7: Performance for anomaly detection.\n6.3 Effect of Randomization Strategy\nAs the ablation study reveals, the frequency-aware\nrandomization contributes the most to the defense.\nIn this section, we analyze the impact of differ-\nent hyperparameters and strategies adopted by the\nfrequency-aware randomization approach, in infer-\nence and training respectively.\n6.3.1 Inference\nThe frequency-aware randomization process is ap-\nplied in inference to mitigate the adversarial effects.\nSubstitution candidate selection and synonym set\nconstruction are two critical steps during this pro-\ncess, in which two hyperparameters (r and ns) and\nthe frequency-aware strategy are examined.\n3255\nSelection of Substitution Candidates The inﬂu-\nence of different strategies for substitution candi-\ndate selection in inference is studied in this section.\nThe impact of two major factors are measured: 1)\nthe substitution ratio r and 2) whether to apply a\nfrequency-aware strategy. In order to exclude the\ndisturbance from other factors, we train BERT on\nthe original training set and ﬁx ns to 20. Firstly, we\nalter the value of r from 5% to 50%, without ap-\nplying the frequency-aware strategy. As illustrated\nby the blue lines in Figure 4, as r increases, the\noriginal accuracy decreases, while the adversarial\naccuracy increases and peaks when r reaches 30%.\nSecondly, a frequency-aware strategy is added to\nthe experiment, with fthres = 200. As depicted\nby the yellow lines in Figure 4, both original and\nadversarial accuracy, the general trends coincide\nwith the non-frequency-aware scenario, but overall\naccuracy is improved to a higher level. The highest\nadversarial is obtained when r is set to 30% using\nfrequency-aware strategy.\nFigure 4: Effect of the substitution ratio r and the\nfrequency-aware strategy in substitution candidate se-\nlection during inference.\nConstruction of Synonym Set The inﬂuence of\ndifferent strategies for synonym set construction in\ninference is evaluated in this section. The impact\nof two major factors are measured: 1) the size of\na single synonym set ns and 2) whether to apply a\nfrequency-aware strategy. In order to exclude the\ndisturbance from other factors, we train BERT on\nthe original training set and ﬁx r to 30% . Firstly,\nwe alter the value of ns from 5 to 50, without ap-\nplying the frequency-aware strategy. The resulted\noriginal and adversarial accuracy are illustrated by\nthe blue lines in Figure 5. Secondly, a frequency-\naware strategy is added to the experiment, with\nnf = 50%∗ns. As depicted by the yellow lines in\nFigure 5, the original accuracy and the adversarial\naccuracy both peaks when ns = 20, and the overall\naccuracy is improved to a higher level compared to\nthe non-frequency-aware scenario.\nFigure 5: Effect of the size of synonym set ns and the\nfrequency-aware strategy in construction of synonym\nset.\n6.3.2 Training\nThe frequency-aware randomization process is ap-\nplied in training to augment the training data, and\nhereby enables the PrLM to better cope with ran-\ndomized samples inference. Based on this pur-\npose, the frequency-aware randomization process\nin training should resemble the one in inference\nas much as possible. Therefore, here we set an\nidentical process for synonym set construction, i.e.\nns = 20 and nf = 50%∗ns. However, for the\nsubstitution selection process, to avoid introduc-\ning excessive noise and maintain the accuracy for\nthe PrLM, the most suitable substitution ratio r\nmight be different than the one in inference. Exper-\niments are conducted to evaluate the inﬂuence of\nr in training. We alter the value of r from 5% to\n50%. In Figure 6, we observe that r = 25%results\nin highest original and adversarial accuracy.\nFigure 6: Effect of the size of synonym set ns and the\nfrequency-aware strategy in construction of synonym\nset.\n7 Conclusion\nThis paper proposes ADFAR, a novel framework\nwhich leverages the frequency-aware randomiza-\ntion and the anomaly detection to help PrLMs de-\nfend against adversarial word substitution. Em-\npirical results show that ADFAR signiﬁcantly out-\nperforms those newly proposed defense methods\nover various tasks. Meanwhile, ADFAR achieves\na remarkably higher inference speed and does not\n3256\nreduce the prediction accuracy for non-adversarial\nsentences, from which we keep the promise for this\nresearch purpose.\nComprehensive ablation study and analysis indi-\ncate that 1) Randomization is an effective method\nto defend against heuristic attack strategy. 2) Re-\nplacement of rare words with their more com-\nmon alternative can help enhance the robustness of\nPrLMs. 3) Adversarial samples generated by cur-\nrent heuristic adversarial word substitution models\ncan be easily distinguished by the proposed auxil-\niary anomaly detector. We hope this work could\nshed light on future studies on the robustness of\nPrLMs.\nReferences\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2890–2896, Brussels, Belgium. Association\nfor Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter.\n2019. Certiﬁed adversarial robustness via random-\nized smoothing. In Proceedings of the 36th Inter-\nnational Conference on Machine Learning , pages\n1310–1320.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKrishnamurthy Dvijotham, Sven Gowal, Robert Stan-\nforth, Relja Arandjelovic, Brendan O’Donoghue,\nJonathan Uesato, and Pushmeet Kohli. 2018. Train-\ning veriﬁed learners with learned veriﬁers. arXiv\npreprint arXiv:1805.10265.\nJavid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018.\nOn adversarial examples for character-level neural\nmachine translation. In Proceedings of the 27th In-\nternational Conference on Computational Linguis-\ntics, pages 653–663, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nJ. Gao, J. Lanchantin, M. L. Soffa, and Y . Qi. 2018.\nBlack-box generation of adversarial text sequences\nto evade deep learning classiﬁers. In 2018 IEEE Se-\ncurity and Privacy Workshops (SPW), pages 50–56.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: BERT-based adversarial examples for text\nclassiﬁcation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6174–6181, Online. As-\nsociation for Computational Linguistics.\nIan J. Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2015. Explaining and harnessing adversar-\nial examples. arXiv preprint arXiv:1905.07129.\nPo-Sen Huang, Robert Stanforth, Johannes Welbl,\nChris Dyer, Dani Yogatama, Sven Gowal, Krish-\nnamurthy Dvijotham, and Pushmeet Kohli. 2019.\nAchieving veriﬁed robustness to symbol substitu-\ntions via interval bound propagation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4083–4093, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAminul Islam and Diana Inkpen. 2009. Real-word\nspelling correction using Google Web 1T 3-grams.\nIn Proceedings of the 2009 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1241–1249, Singapore. Association for Computa-\ntional Linguistics.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1875–1885, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nRobin Jia, Aditi Raghunathan, Kerem G ¨oksel, and\nPercy Liang. 2019. Certiﬁed robustness to adver-\nsarial word substitutions. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4129–4142, Hong Kong,\nChina. Association for Computational Linguistics.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2177–2190, Online. Asso-\nciation for Computational Linguistics.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2019. Is bert really robust? natural lan-\n3257\nguage attack on text classiﬁcation and entailment.\narXiv preprint arXiv:1907.11932.\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio.\n2017. Adversarial examples in the physical world.\narXiv preprint arXiv:1607.02533.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: Adversar-\nial attack against BERT using BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6193–6202, Online. Association for Computational\nLinguistics.\nBin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian,\nXirong Li, and Wenchang Shi. 2018. Deep text clas-\nsiﬁcation can be fooled. Proceedings of the Twenty-\nSeventh International Joint Conference on Artiﬁcial\nIntelligence.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nJohn Morris, Eli Liﬂand, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. TextAttack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in NLP. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations ,\npages 119–126, Online. Association for Computa-\ntional Linguistics.\nMaximilian Mozes, Pontus Stenetorp, Bennett Klein-\nberg, and Lewis D. Grifﬁn. 2020. Frequency-guided\nword substitutions for detecting textual adversarial\nexamples. arXiv preprint arXiv:2004.05887.\nNikola Mrk ˇsi´c, Diarmuid ´O S ´eaghdha, Blaise Thom-\nson, Milica Ga ˇsi´c, Lina Rojas-Barahona, Pei-\nHao Su, David Vandyke, Tsung-Hsien Wen, and\nSteve Young. 2016. Counter-ﬁtting word vec-\ntors to linguistic constraints. arXiv preprint\narXiv:1603.00892.\nNikita Nangia, Adina Williams, Angeliki Lazaridou,\nand Samuel R Bowman. 2017. The repeval 2017\nshared task: Multi-genre natural language inference\nwith sentence representations. In RepEval.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4885–4901, Online. Association\nfor Computational Linguistics.\nBo Pang and Lillian Lee. 2005. Seeing stars: Ex-\nploiting class relationships for sentiment categoriza-\ntion with respect to rating scales. In Proceed-\nings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 115–\n124, Ann Arbor, Michigan. Association for Compu-\ntational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lip-\nton. 2019. Combating adversarial misspellings with\nrobust word recognition. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5582–5591, Florence, Italy.\nAssociation for Computational Linguistics.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1085–1097, Florence, Italy. Association for Compu-\ntational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2018. Semantically equivalent adversar-\nial rules for debugging NLP models. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 856–865, Melbourne, Australia. Association\nfor Computational Linguistics.\nKeisuke Sakaguchi, Matt Post, and Benjamin\nVan Durme. 2017. Grammatical error correction\nwith neural reinforcement learning. In Proceedings\nof the Eighth International Joint Conference on\nNatural Language Processing (Volume 2: Short\nPapers), pages 366–372, Taipei, Taiwan. Asian\nFederation of Natural Language Processing.\nChenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan\nLiu, Yasheng Wang, Qun Liu, and Maosong Sun.\n2021. Better robustness by more coverage: Adver-\nsarial training with mixup augmentation for robust\nﬁne-tuning.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\n3258\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Lin-\nguistics.\nWenqi Wang, Run Wang, Lina Wang, Zhibo Wang, and\nAoshuang Ye. 2021. Towards a robust deep neural\nnetwork in texts: A survey.\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\nmentation techniques for boosting performance on\ntext classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou\nRen, and Alan Yuille. 2018. Mitigating adversar-\nial effects through randomization. arXiv preprint\narXiv:1711.01991.\nMao Ye, Chengyue Gong, and Qiang Liu. 2020.\nSAFER: A structure-free approach for certiﬁed ro-\nbustness to adversarial word substitutions. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3465–\n3475, Online. Association for Computational Lin-\nguistics.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu,\nMeng Zhang, Qun Liu, and Maosong Sun. 2020.\nWord-level textual adversarial attacking as combina-\ntorial optimization. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6066–6080, Online. Association\nfor Computational Linguistics.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020.\nSemantics-aware bert for language understanding.\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2019. Sg-net:\nSyntax-guided machine reading comprehension.\nYichao Zhou, Jyun-Yu Jiang, Kai-Wei Chang, and Wei\nWang. 2019. Learning to discriminate perturbations\nfor blocking adversarial attacks in text classiﬁcation.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4904–\n4913, Hong Kong, China. Association for Computa-\ntional Linguistics.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.9064005613327026
    },
    {
      "name": "Computer science",
      "score": 0.7916020154953003
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6541405916213989
    },
    {
      "name": "Inference",
      "score": 0.5815869569778442
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5706573128700256
    },
    {
      "name": "Substitution (logic)",
      "score": 0.5637750625610352
    },
    {
      "name": "Word (group theory)",
      "score": 0.5402026772499084
    },
    {
      "name": "Task (project management)",
      "score": 0.5197986364364624
    },
    {
      "name": "Language model",
      "score": 0.48590365052223206
    },
    {
      "name": "Natural language",
      "score": 0.4640108346939087
    },
    {
      "name": "Natural language processing",
      "score": 0.44216129183769226
    },
    {
      "name": "Speech recognition",
      "score": 0.383969247341156
    },
    {
      "name": "Machine learning",
      "score": 0.3771689534187317
    },
    {
      "name": "Linguistics",
      "score": 0.11327442526817322
    },
    {
      "name": "Engineering",
      "score": 0.07533818483352661
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}