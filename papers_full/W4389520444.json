{
  "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
  "url": "https://openalex.org/W4389520444",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2636416761",
      "name": "Denis McInerney",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2103058739",
      "name": "Geoffrey Young",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4201925681",
      "name": "Jan-Willem van de Meent",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2441726348",
      "name": "byron wallace",
      "affiliations": [
        "Universidad del Noreste"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6607520151",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4287271005",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4394659038",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "We propose CHiLL (Crafting High-Level Latents), an approach for natural-language specification of features for linear models. CHiLL prompts LLMs with expert-crafted queries to generate interpretable features from health records. The resulting noisy labels are then used to train a simple linear classifier. Generating features based on queries to an LLM can empower physicians to use their domain expertise to craft features that are clinically meaningful for a downstream task of interest, without having to manually extract these from raw EHR. We are motivated by a real-world risk prediction task, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and standard predictive tasks (e.g., 30-day readmission) to evaluate this approach. We find that linear models using automatically extracted features are comparably performant to models using reference features, and provide greater interpretability than linear models using \"Bag-of-Words\" features. We verify that learned feature weights align well with clinical expectations.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8477–8494\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCHiLL: Zero-shot Custom Interpretable Feature Extraction\nfrom Clinical Notes with Large Language Models\nDenis Jered McInerney\nNortheastern University\nmcinerney.de@northeastern.edu\nGeoffrey Young\nBrigham and Women’s Hospital\ngsyoung@bwh.harvard.edu\nJan-Willem van de Meent\nUniversity of Amsterdam\nj.w.vandemeent@uva.nl\nByron C. Wallace\nNortheastern University\nb.wallace@northeastern.edu\nAbstract\nWe propose CHiLL (Crafting High-Level La-\ntents), an approach for natural-language speci-\nfication of features for linear models. CHiLL\nprompts LLMs with expert-crafted queries\nto generate interpretable features from health\nrecords. The resulting noisy labels are then\nused to train a simple linear classifier. Gener-\nating features based on queries to an LLM can\nempower physicians to use their domain exper-\ntise to craft features that are clinically meaning-\nful for a downstream task of interest, without\nhaving to manually extract these from raw EHR.\nWe are motivated by a real-world risk predic-\ntion task, but as a reproducible proxy, we use\nMIMIC-III and MIMIC-CXR data and standard\npredictive tasks (e.g., 30-day readmission) to\nevaluate this approach. We find that linear mod-\nels using automatically extracted features are\ncomparably performant to models using refer-\nence features, and provide greater interpretabil-\nity than linear models using “Bag-of-Words”\nfeatures. We verify that learned feature weights\nalign well with clinical expectations.\n1 Introduction\nLLMs have greatly advanced few- and zero-shot ca-\npabilities in NLP, reducing the need for annotation.\nThis is especially exciting for the medical domain,\nin which supervision is often scant and expensive.\nHowever, given the high-stakes nature of clinical\nwork and the challenges associated with developing\nmodels (e.g., long-tail data distributions, weakly\ninformative supervision), predictions can rarely be\ntrusted blindly. Clinicians therefore tend to favor\nsimple models with interpretable predictors over\nopaque LLMs that rely on dense learned represen-\ntations. Risk prediction tools are often linear mod-\nels with handcrafted features. Such models have\nthe advantage of associating features with weights;\nthese can be inspected to ensure clinical tenability\nand may avoid undesired fragilities of large neural\nmodels. However, a downside to relying on inter-\nPatient has symptoms of … \nand history indicates type II \ndiabetes …\n…\nBP 133/81 | Wt 190 lbs\n…\nReferal(s): …\nRead the following text \nfrom a clinical note: \n<notes>\nDoes this patient have \na chronic illness?\nRaw patient notes Prompt LLM \n(Flan-T5)\n[0 0 … 1 … 0 1]\nchronic \nillness \nindicator \nx\nx\nYes\nLinear model over high-\nlevel features\np ( y =1 | x )= \u0000 ( ✓ · )\n<latexit sha1_base64=\"pL8OLL62kcKoVP/6HwrYJbyYgrQ=\">AAACF3icbVDLSgNBEJz1bXxFPXoZDEK8LLtR0UtA9OIxgkmEbAizk04yZGZ3mekVw5q/8OKvePGgiFe9+TdOHgdfBQ1FVTfdXWEihUHP+3RmZufmFxaXlnMrq2vrG/nNrZqJU82hymMZ6+uQGZAigioKlHCdaGAqlFAP++cjv34D2og4usJBAk3FupHoCM7QSq28mxQHZf/udp+WaWBEV7FigD1ARgPejpEGPZMwDpnnHoEa7rfyBc/1xqB/iT8lBTJFpZX/CNoxTxVEyCUzpuF7CTYzplFwCcNckBqwC/qsCw1LI6bANLPxX0O6Z5U27cTaVoR0rH6fyJgyZqBC26kY9sxvbyT+5zVS7Jw0MxElKULEJ4s6qaQY01FItC00cJQDSxjXwt5KeY9pxtFGmbMh+L9f/ktqJdc/cEuXh4XTs2kcS2SH7JIi8ckxOSUXpEKqhJN78kieyYvz4Dw5r87bpHXGmc5skx9w3r8AZtCeNw==</latexit>\nx\n<latexit sha1_base64=\"QXWNndi5NRieIutz8eBLFzeTjJc=\">AAAB6XicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGK/YA2lM120y7dbMLuRCyl/8CLB0W8+o+8+W/ctDlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRTea3Hrk2IlYPOE64H9GBEqFgFK10/1TslcpuxZ2BLBMvJ2XIUe+Vvrr9mKURV8gkNabjuQn6E6pRMMmnxW5qeELZiA54x1JFI278yezSKTm1Sp+EsbalkMzU3xMTGhkzjgLbGVEcmkUvE//zOimGV/5EqCRFrth8UZhKgjHJ3iZ9oTlDObaEMi3srYQNqaYMbThZCN7iy8ukWa1455Xq3UW5dp3HUYBjOIEz8OASanALdWgAgxCe4RXenJHz4rw7H/PWFSefOYI/cD5/ABwajRQ=</latexit>\nx\n<latexit sha1_base64=\"QXWNndi5NRieIutz8eBLFzeTjJc=\">AAAB6XicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGK/YA2lM120y7dbMLuRCyl/8CLB0W8+o+8+W/ctDlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRTea3Hrk2IlYPOE64H9GBEqFgFK10/1TslcpuxZ2BLBMvJ2XIUe+Vvrr9mKURV8gkNabjuQn6E6pRMMmnxW5qeELZiA54x1JFI278yezSKTm1Sp+EsbalkMzU3xMTGhkzjgLbGVEcmkUvE//zOimGV/5EqCRFrth8UZhKgjHJ3iZ9oTlDObaEMi3srYQNqaYMbThZCN7iy8ukWa1455Xq3UW5dp3HUYBjOIEz8OASanALdWgAgxCe4RXenJHz4rw7H/PWFSefOYI/cD5/ABwajRQ=</latexit>\nx\n<latexit sha1_base64=\"QXWNndi5NRieIutz8eBLFzeTjJc=\">AAAB6XicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rGK/YA2lM120y7dbMLuRCyl/8CLB0W8+o+8+W/ctDlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LRTea3Hrk2IlYPOE64H9GBEqFgFK10/1TslcpuxZ2BLBMvJ2XIUe+Vvrr9mKURV8gkNabjuQn6E6pRMMmnxW5qeELZiA54x1JFI278yezSKTm1Sp+EsbalkMzU3xMTGhkzjgLbGVEcmkUvE//zOimGV/5EqCRFrth8UZhKgjHJ3iZ9oTlDObaEMi3srYQNqaYMbThZCN7iy8ukWa1455Xq3UW5dp3HUYBjOIEz8OASanALdWgAgxCe4RXenJHz4rw7H/PWFSefOYI/cD5/ABwajRQ=</latexit>\nFigure 1: We propose to allow domain experts to specify\nhigh-level features for simple linear predictive models\nin natural language, and then extract these zero-shot\n(without supervision) using large language models.\npretable features is that one often has to manually\nextract them from patient records. This can be\nparticularly difficult when features are high-level\nand must be inferred from unstructured (free-text)\nfields within EHR.\nIn this work we investigate the potential of zero-\nshot extraction for definition and abstraction of\ninterpretable features from unstructured EHR to\nuse as inputs for simple (linear) models (Figure\n1). Recent work (Agrawal et al., 2022) has shown\nthat LLMs are capable zero-shot extractors of clin-\nical information, i.e., they can extract structured\ninformation from records without explicitly being\ntrained to do so. These can be high-level features\nspecified in natural language, which allows prac-\ntitioners to define features that they hypothesize\nmay be relevant to a downstream task. Such fea-\ntures, even if noisy, are likely to be more inter-\npretable than low-level features such as Bag-of-\nWords (BoW) representations. The motivation of\nthis work is to evaluate the viability of training\nsmall linear models over high-level features auto-\nmatically extracted via LLMs—without explicit\nsupervision—from unstructured data in EHR.\nA secondary concern that this paper seeks to ad-\ndress is any reliance on closed-source LLMs for\nhealthcare, which is undesirable for a number of\n8477\nreasons. First, such models are inherently opaque,\nprecluding interpretability analysis which is espe-\ncially important in healthcare. Second, EHR data is\nsensitive, and so submitting this to an API (e.g., as\nprovided by OpenAI) is potentially problematic. In\nthis work we show that despite the specialized do-\nmain, Flan-T5 (Chung et al., 2022; Wei et al., 2022)\nvariants—which can fit on a single GPU and be run\nlocally—can perform zero-shot feature extraction\nfrom EHR with reasonable accuracy.\nOur contributions are as follows. (1) We propose\nand evaluate a method for extracting high-level in-\nterpretable features from clinical texts using Flan-\nT5 given corresponding prompts, and we evaluate\nthe how well these extracted features align with\nground truth. (2) We demonstrate that we can ex-\nploit LLM calibration to improve performance, al-\nlowing models that use inferred features to perform\ncomparably to those using reference features. (3)\nWe show that the resulting linear models’ weights\nalign with clinician-annotated notions of how fea-\ntures should impact a prediction. (4) We investigate\nthe data- and feature- efficiency of our approach\nand find that it can achieve similar results with\nmuch less data and utilizes features efficiently.\nOur promising initial results suggest several av-\nenues for further exploration, e.g., modeling cor-\nrelations between inferred features, probing the\ndegree to which such predictors provide useful and\nreliable interpretability, and modifying trained lin-\near models directly based on expert judgement.\n2 Methods\nWe consider binary classification of patients on the\nbasis of free text from their EHR data. A now stan-\ndard approach to such tasks would entail adopting\na neural language encoder E to induce a fixed-\nlength d-dimensional distributed representation of\nan input x—e.g., the [CLS] embedding in BERT\n(Devlin et al., 2018; Alsentzer et al., 2019) models—\nand feeding this forward to a linear classification\nhead to yield a logit:\np(y= 1|x) =σ(wT E[CLS](x)) (1)\nwhere y ∈{0,1}, x ∈RL×V , and w ∈Rd. (For\nBERT,d= 768.) A drawback of this approach is\nthat it is not amenable to inspection. The prediction\nis made on the basis of a dense learned representa-\ntion from a pre-trained network, and it is unclear\nwhich patient attributes give rise to the prediction.\nA simpler and (at least arguably) more interpretable\napproach is to use a linear model defined over Bag-\nof-Words (BoW) representations:\npBoW(y= 1|x) =σ(wT xBoW). (2)\nThis approach operates over (transformations of)\ntoken counts, and therefore the learned w has a\nnatural correspondence to words in the vocabulary.\nLinear models that operate over tens of thousands\nof word predictors occupy an intermediate space\nbetween the interpretability afforded by simpler,\nsmaller models defined over high-level features and\nneural models which use opaque representations.\nFor some clinical tasks, however, BoW with large\nvocabularies can be competitive with respect to\ndownstream performance.\nIn this paper, we use instruction-tuned LLMs to\nperform zero-shot inference of intermediate, high-\nlevel, features f ∈RN using N expert-specified\nprompt templates t1,...,t N :\nfbinary\nn = I[argmaxz(LLM(z|tn(x))) =vyes]\n(3)\nwhere tn(x) denotes the prompt obtained by popu-\nlating the template tn with x, zrepresents the next\ntoken after the prompt andvyes represents the index\nof the token “yes” in the vocabulary. We then use a\nsimple linear model (with weights w∈RN ) over\nthese predicted features to predict the target:\np(y= 1) =σ(wT fbinary). (4)\nPredictions from the LLM for the high-level bi-\nnary features will be imperfect, and are naturally\nassociated with a confidence under the LLM: The\nprobability of the token “yes” normalized by the\nmass assigned to either “yes” or “no”. As a sim-\nple means of incorporating uncertainty in extracted\nfeatures, we can use this continuous value in place\nof binary feature indicators. For feature n(elicited\nusing template tn), the feature value fcont\nn is:\nLLM(z= vyes|tn(x))\nLLM(z= vyes|tn(x)) + LLM(z= vno|tn(x)).\n(5)\nInspired by previous work (Zhang et al., 2020),\nwe split text into chunks of a particular maximum\nlength and take the maximum (per-feature) of the\nfeature values of the chunks as the final features.\n3 Evaluation\nTo evaluate the proposed approach, we consider\nfour tasks on publicly available MIMIC data to\n8478\npermit reproducibility. We start with standard\nclinical predictive tasks in MIMIC-III (Johnson\net al. 2016b,a; Goldberger et al. 2000, Section 3.1):\nReadmission, mortality, and phenotype prediction.\nFor these we treat ICD codes as proxies for high-\nlevel features.1 This allows us to evaluate the accu-\nracy of the zero-shot extraction component, and to\ncompare the performance of a linear model defined\nover the true ICD codes as compared to the same\nmodel when operating over inferred features.\nWe then consider X-ray report classification\nusing the MIMIC-CXR dataset (Johnson et al.\n2019b,a,c; Johnson et al.; Goldberger et al. 2000;\nSection 3.2). In this setting, we elicited queries for\nintermediate feature descriptions from a radiolo-\ngist co-author. Queries take the form of questions\nthat a radiologist might a priori believe to be rele-\nvant to the report classification categories defined\nin CheXpert (e.g., “Does this patient have a normal\ncardiac silhouette?”; see Appendix Table B for all\nexamples). This demonstrates the flexibility of the\napproach—and the promise of allowing domain\nexperts to specify features in natural language—\nbut we are limited in our ability to evaluate the\nextraction performance directly in this case.\n3.1 Clinical predictive tasks on MIMIC-III\nFor the three standard clinical prediction tasks we\nconsider, we use ICD codes as proxies for high-\nlevel, interpretable features that one might want\nto infer. While this task is somewhat artificial, it\nallows us to evaluate how well the inferred features\nagree with the “true” features (i.e., ICD-code in-\ndicators). We use the top 10 most common ICD\ncodes in the training set as features. 2 We ask the\nLLM: “Does this mean the patient has <code>?”,\nwhere we replace <code>with the long descrip-\ntion of the ICD code. To illustrate the flexibility\nof the approach, we also consider two custom fea-\ntures which one might expect to be informative: (1)\nDoes the patient have a chronic illness? (2) Is the\ncondition life-threatening?\nReadmission prediction For 30-day readmission,\nwe follow the task definition, setup, and data pre-\nprocessing outlined in (Huang et al., 2019).\n1In practice one could of course use ICD codes directly as\npredictors; but this serves as an exemplary task using publicly\navailable data to show the potential of the proposed approach\nin a way that also allows us to evaluate models that have access\nto “ground-truth” high-level features, i.e., the ICD codes.\n2In preliminary experiments we found that including >10\nICD codes as predictors in linear models did not appreciably\nimprove AUROC for the three tasks considered.\nIn-hospital Mortality prediction This task in-\nvolves predicting if a patient will pass during a\nhospital stay with notes from the first 48 hours. We\nadapt preprocessing code from (Zhang et al., 2020).\nPhenotype prediction Zhang et al. (2020) also de-\nrive various phenotypes using ICD codes, and use\nthese to define a phenotype prediction task which\nwe also consider. (We again adapt their preprocess-\ning code for this task.)\n3.2 Chest X-ray report classification\nWe next consider chest X-ray report classifica-\ntion.3 This allows us to draw upon the exper-\ntise of the radiologist co-author. We use the\nMIMIC-CXR dataset (Johnson et al., 2019b,a,c;\nJohnson et al.; Goldberger et al., 2000) with CheX-\npert labels (Irvin et al., 2019)—again to ensure\nreproducibility—and evaluate performance on a\n12-label classification task.\nGiven that these labels can be automatically de-\nrived, the predictive task considered here is not in\nand of itself practically useful, but it is illustrative\nof how domain experts (here, a radiologist) can\ncraft bespoke features for a given task. We use\noutputs of the CheXpert automated report labeler\nas “labels”. We omit two downstream labels from\nour results—Consolidation and Pleural Effusion—\nbecause we included these as intermediate features\ninstead under the advisement of our radiologist co-\nauthor, and it does not make sense to include labels\nfor which there exists an exactly corresponding fea-\nture.4 (The names of the 12 labels we predict are\nshown in Table 2.)\nTo infer intermediate features, we asked the ra-\ndiologist co-author to provide natural binary ques-\ntions they would ask to categorize radiology reports\ninto the given classes (Appendix B). We refer to\nthese questions as queries, and use them as prompts\nfor the LLM to extract corresponding indicators (or\nprobabilities) for each answer. Because these are\nnovel predictors specified by a domain expert, we\ndo not have “reference” features to compare to (as\nwe did above where we treated ICD codes as high-\nlevel features). However, in Section 4.3 a domain\nexpert assesses the degree to which learned coeffi-\ncients for features align with clinical judgement.\n3We discard the images in this setting and only use text.\n4We did use these extra two labels in training, but this only\naffects BERT, which was trained in a ‘multi-task’ way with a\nshared encoder used (and updated) across all labels. We think\nit unlikely that this would impact performance much.\n8479\n0.4\n0.6\n0.8\n1.0AUROC\n...Chr. kidney disease...HyperlipidemiaDiabetes mellitus...\n...Use of anticoagulantsAcute kidney failureAcute respiratory failureCoronary Atherosclerosis...\nAtrial fibrillation...Heart Failure...Hypertention\n...Chr. kidney disease...HyperlipidemiaDiabetes mellitus...\n...Use of anticoagulantsAcute kidney failureAcute respiratory failureCoronary Atherosclerosis...\nAtrial fibrillation...Heart Failure...Hypertention\nContinuous Binary\n0.0\n0.2\n0.4\n0.6F1 Score\nFigure 2: Feature extraction performance for the readmission prediction task. AUROC for continuous ICD code\nfeatures (left) and F1 for binary ICD code features (right), compared to reference ICD codes.\n3.3 Experimental details\nTo extract features, we use FLAN -T5-XXL\n(Roberts et al., 2022) with fp16 enabled (for speed)\non a Quadro RTX 8000 with 46G of memory.\nWe use a maximum chunk size of 512 (as de-\nscribed in section 2) and use a maximum of 4\nchunks. To fit logistic regression models, we use\nthe sklearn (Pedregosa et al., 2011) package’s\nSGDClassifier with the logistic loss and the the\ndefault settings (this includes adding an intercept\nand an ℓ2 penalty). We show the full prompt tem-\nplate used for getting the features in appendix A.\n4 Results\nWe aim to address the following research questions.\nFeature extraction (4.1) How accurate are zero-\nshot extracted features, as compared to reference\n(manually extracted) predictors?\nDownstream classification performance (4.2)\nHow does classification based on exper our ap-\nproach compare to black box and simple models\non the ultimate downstream classification?\nInterpretability (4.3) Do the inferred features per-\nmit intuitive interpretation, and do the resultant\ncoefficients align with clinical expectations?\nData and feature efficiency (4.4) Do these fea-\ntures offer additional benefits in terms of data\nand/or feature efficiency?\n4.1 Feature extraction\nWe first measure the accuracy of automatically in-\nferred high-level features. In the case of ICD-code\nproxy features, we evaluate this directly using the\nactual ICD codes as reference labels with which\nto calculate precision, recall, and F1 for binary\nfeatures, and AUROC for our continuous features\n(although we cannot do this for the two custom\nfeatures considered). We present these metrics for\nAUROC\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n.92\nContinuous\nF1 Score Precision Recall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n.21 .19\n.34\nBinary\nFigure 3: Feature inference performance on labeled\nChest X-ray data. Histograms are over features. For\ncontinuous feature AUROCs, we omit features that do\nnot have any positive labels in the 50 labeled examples.\nFor F1 of binary features, we omit features that corre-\nspond to ill-defined precision (no positive predictions)\nand/or recall (no positive labels) scores are set to 0.\nthe readmission task in Figure 2.\nFor the radiology task, we have no reference\nfeatures to use for evaluation. Therefore, our radi-\nologist co-author annotated 50 test example reports\nwith a set of features applicable to each report. This\nallows us to create binary labels for each feature\nand report, in turn allowing evaluation with the\nsame metrics used above for binary and continuous\nfeature encodings. There are 105 features—too\nmany to present individually—so we report the\nfeature scores in violin plots (Figure 3).\nZero-shot (feature) extraction performs reason-\nably well here in general, as suggested by prior\nwork (Agrawal et al. 2022; although they used GPT-\n3, not Flan-T5). Scores are well above chance for\nMIMIC-III features. Given the zero-shot setting,\nthey are not on par with supervised approaches for\nICD code prediction (Mullenbach et al., 2018), but\nthis may be partially due to a limitation of the ICD\ncode reference features: ICD codes are known to\nbe noisy (Kim et al., 2021; Boag et al., 2022), so\neven correctly inferred features may not align well\n8480\nPhenotyping\nReadmission PredictionMortality Prediction\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0AUROC\nMIMIC-III T asks\nBERT\nTF-IDF (30k)\nGround Truth\nInferred Continuous\nInferred Continuous w/ Custom\nZero-Shot Downstream Prediction\nChest X-ray\nClassification\nFigure 4: Downstream classification performance. We\ncompare variants that use inferred continuous features\nwith models that use “ground truth” features (here, ICD\ncodes), and models that perform zero-shot prediction\ndirectly. We also show the performance of TF-IDF and\nBERT models (dashed horizontal lines) and 95% CIs.\nwith the labels. In contrast, our radiologist’s direct\nannotation of the Chest X-ray report features reveal\nmuch higher AUROCs for continuous features. A\nnovel aspect of this work, in contrast to (Agrawal\net al., 2022), is that in the case of the chest X-\nray task the features are extracted using custom\nqueries, specified as natural language questions\nprovided by a domain expert (radiologist). While\nfar from perfect, our results suggest that modern\nlarge instruction-tuned LMs are capable of infer-\nring arbitrary clinically salient high-level features\nfrom EHR with some fidelity.\n4.2 Downstream classification performance\nClassification performance in and of itself is not\nour primary objective; rather, our aim is to design\na method which allows for flexible construction of\nsmall and interpretable models using high-level fea-\ntures, with minimal expert annotation effort. But\nto contextualize the predictive performance of the\nmodels evaluated we also consider several base-\nlines that offer varying degrees of “interpretability”.\nSpecifically, we evaluate fine-tuned variants of (a)\nClinical BERT (Alsentzer et al., 2019), and (b) a\nlogistic regression model defined over BoW (TF-\nw/ Custom Pheno. Readmis. Mort. CXR Class.\n✗ -.054 -.025 -.052 -\n✔ -.050 -.023 -.088 -.060\nTable 1: Difference in AUROC between using Binary\nand Continuous features.\nIDF) features with varying vocabulary sizes. We\nreport AUROCs for phenotyping and X-ray report\nclassification, macro-averaged over labels.\nTo evaluate the degree to which zero-shot feature\ninference degrades performance, we also report\nresults for a logistic regression model defined over\nreference ICD codes (“Ground Truth”) for MIMIC\ntasks. And finally, we compare against direct zero-\nshot prediction of the downstream classification\nusing FLAN -T5-XXL (Roberts et al., 2022).\nFigure 4 demonstrates that across all tasks, the\nmodel variants with continuous intermediate fea-\ntures have significant signal (i.e., AUROC is sig-\nnificantly above chance 0.5). 5 Our method also\nperforms comparably to or better than “Ground\nTruth” features for all tasks except Phenotyping,\nwhere the ICD code features (real more-so than\ninferred6) have an advantage because the pheno-\ntypes considered were derived from ICD codes. 7\nWe also see that the addition of just two custom\nqueries does improve performance to varying de-\ngrees for MIMIC-III tasks relative to models that\nsolely employ ICD-code queries, indicating that\nthere is indeed a benefit that can be derived from\nemploying natural language queries to predict in-\ntermediate features.\nThat said, making downstream predictions using\nthese features performs worse than BERT and TF-\nIDF (30k) models. This is not surprising given that\nthe number of features for our method is in the\nrange of 10-105 as compared with 30k for TF-IDF\nand 100k+8 for ClinicalBERT.\nZero-shot prediction of the downstream task per-\nforms worse than (supervised) linear models on top\nof inferred features on Readmission prediction and\nChest X-ray report classification, equivalently on\nPhenotyping, and slightly better on Mortality pre-\ndiction. However, such predictions are completely\nblackbox (see Section 5 for discussion around inter-\npretability of zero-shot extracted features.) Finally,\nwe also find that using binary features instead of\ncontinuous features degrades performance signifi-\ncantly (Table 1); calibrating (un)certainty helps.\nWhen manually inspecting some example radi-\nology reports, it became apparent that downstream\nlabels are often verbalized in the radiology report.\n5Expanded set of results in Apppendix Table C.1.\n6Inferred codes are not expected to fully align with noisy\nICD codes (see section 4.1).\n7Performance is not 100% because only a subset of the\nICD codes used for phenotyping were used as features.\n8approximately, after the embedding layer\n8481\nCoronary atherosclerosis of unspecified type of vessel\nHypertensive chronic kidney disease\nAcute respiratory failure\ncustom_has_chronic\nconst\nCongestive heart failure\nOther and unspecified hyperlipidemia\nUnspecified essential hypertension\nAcute kidney failure\nLong-term (current) use of anticoagulants\nDiabetes mellitus without mention of complication\nAtrial fibrillation\ncustom_is_lifethreatening\nInferred Continuous w/ Custom\ntrach\nosh\nno\nesrd\ndialysis\nhd\nvarices\nhemodialysis\nrehab\ncomfort\nsubdural\ntracheostomy\nmetformin\nline\noverdose\nTF-IDF (30k)\nHypertensive chronic kidney disease\nAcute respiratory failure\nconst\nLong-term (current) use of anticoagulants\nCoronary atherosclerosis of unspecified type of vessel\nAcute kidney failure\nCongestive heart failure\nAtrial fibrillation\nDiabetes mellitus without mention of complication\nUnspecified essential hypertension\nOther and unspecified hyperlipidemia\nGround Truth\nFigure 5: Linear model coefficients for readmission prediction. Blue and red indicate features that support or\nrefute the label, respectively, and “const” refers to the values of the intercept. Though many feature weights (e.g.\n“Hypertensive Chronic Kidney Disease”) make a lot of sense in the context of readmission prediction, “Coronary\natherosclerosis [...],” which we see in the inferred features, does not make sense as a top feature. We suspect this is\ndue to imperfect feature inference as it is not a top feature when using the ground truth features.\nP@1 P@5 P@10 P@20 AUC\nNo Finding 1.00 0.40 0.20 0.10 0.62\nEnlar. Cardiom. 1.00 0.20 0.10 0.05 1.00\nCardiomegaly 1.00 0.80 0.60 0.35 0.61\nEdema 0.00 0.20 0.30 0.20 0.62\nPneumonia 0.00 0.20 0.20 0.10 0.64\nAtelectasis 0.00 0.20 0.20 0.10 0.69\nPneumothorax 1.00 0.60 0.30 0.15 0.84\nFracture 1.00 0.40 0.30 0.15 0.74\nLung Lesion 0.00 0.60 0.40 0.25 0.80\nLung Opacity 0.00 0.00 0.00 0.00 0.29\nPleural Other 0.00 0.20 0.40 0.20 0.73\nSupport Devices 0.00 0.40 0.30 0.30 0.49\nAverage 0.42 0.35 0.27 0.16 0.67\nTable 2: Precisions and AUCs of learned feature rank-\nings on the Chest X-ray classification task, evaluated\nagainst a priori relevancy judgements per class pro-\nvided by our radiologist collaborator. The model was\nnot trained to rank features but nevertheless implicitly\nlearned feature importance that aligns with intuition.\nThis makes sense given that the CheXpert labeler\nneeds to extract these from the report, and interme-\ndiate features are therefore not explicitly modeled.\nThis gives TF-IDF and BERT models a particu-\nlar advantage over inferred feature models that is\nunlikely to exist for natural tasks (which are not\ndefined by an automated labeler, and where inter-\nmediate features will probably be important).\n4.3 Interpretability\nGiven our primary motivation to offer\ninterpretability—specifically via small linear\nmodels over a small number of high-level\nfeatures—we next investigate how well learned\ncoefficients align with domain expert judgement.\nThe radiologist who specified features for the\nchest X-ray dataset also indicated for which task(s)\nbulging fissures\nstags antler sign\ndecreased lung volumes\nalveolar filling process\nconvex left atrial appendage\ncollapse of lung\nreticular opacity\ndense op air filling with pus water blood\nthickening of the fissures\nconst\nindistinct basilar opacity\nincrease subcarinal angle\napical lucency\nprominent right atrial contour\nperibronchiolar cuffing\nContinuous Feature Coefficients (Atelectasis)\n✔\n✔\n✔✔✔\n✔\n✗ \n✗ \n✗ ✗ \n✔\n✔\n✔✔\nFigure 6: Linear model coefficients for predicting At-\nelectasis in the Chest X-ray Dataset using the top 15\ncontinuous features. “const” represents a constant bias\nfeature and is therefore not annotated. ✔ and ✗ denote\npost-hoc annotations of the feature coefficient as align-\ning or not aligning with clinical expectations, respec-\ntively. Most features (both with negative and positive\ncoefficients) do align.\nthey judged each feature to be predictive. Conse-\nquently, we have a label for each of the 105 features\nthat indicates whether the domain expert believed\nthe feature to be likely supportive of, or likely not\nrelevant to, each of the 12 classes defined in the\nchest X-ray task. We use these “feature labels” to\nmeasure the degree to which the learned weights\nagree with the domain knowledge that they are\nintended to encode. In particular, we rank each\nfeature by coefficient and compute precision at k\nand AUC for all 12 classes (Table 2). For all la-\nbels except Lung Opacity and Support Devices, the\nrank of features in terms of relevance consistently\nagrees with expert judgement (AUC >.5).\nAs further evidence of the greater interpretability\nafforded by the proposed approach, we report the\ntop features for readmission prediction in Figure 5.\n8482\n\n Targets\n(CheXpert\nLabeler)\nModel\nPredictions\nSupport Devices\nLung Opacity\nEdema\nEdema Should\nbe\na\ntarget!Clinical Judgement:\nSupport Devices\nCardiomegaly\nCardiomegaly\nLung Opacity\nHISTORY:  Intubated, evaluate ET \ntube.\n \nFINDINGS: The ET tube is 3.5 cm \nabove the carina.  The NG tube tip \nis off the film, at least in the \nstomach.  Right IJ Cordis tip is in \nthe proximal SVC. The heart size \nis moderately enlarged. There is \nill-defined vasculature and \nalveolar infiltrate, right greater \nthan left. This is markedly \nincreased compared to the film \nfrom two hours prior and likely \nrepresents fluid overload.\nendotracheal tube\ntube\nenlarged heart\nenlarged cardiac silhouette\nalveolar fluid\nalveolar filling process\nesophageal tube\nnasogastric tube\ngastric tube\nenlarged cardiomediastinal silhouette\nT op Continuous Features\nconst\nalveolar filling process\nalveolar fluid\nperipheral lucency\nenlarged heart\nupper lobe pulmonary venous engorgement\nhazy perihilar opacity\ngastric tube\nesophageal tube\nrounded left heart border\n✔ ✔\nT op Decision-Impacting Features (Edema)\nFigure 7: Qualitative example of features and feature influence for predicting Edema in the case of an ostensible\n“false positive”. We selected this example because on cursory inspection Edema would seem to be applicable here.\nWe presented this to our radiologist co-author who confirmed that this report should in fact be labeled positive\nfor Edema. We show the report and the downstream reference and predicted labels on the left. In the middle, we\nreport top raw feature values, and on the right we show the top scores, i.e., feature values times the corresponding\ncoefficients for Edema. Bolded features were confirmed by the radiologist as aligning with clinical judgement.\nHere we compare the top-ranked features in linear\nmodels using TF-IDF (left), reference ICD codes\n(the “ground truth” high-level features here; center),\nand inferred high-level features (right). The top\npositive inferred high-level features align with the\ntop positive reference ICD code features.\nFor high-level features, coefficient magnitude\nmass is concentrated on the very top features,\nwhereas in the TF-IDF case mass is more uniformly\ndistributed. This held across many of the tasks and\nlabels (see Appendix Table C.2), and is likely an\nartifact of having many more TF-IDF features. It\nrenders such models difficult to interpret. We also\nsee that the custom “has_chronic” feature is among\nthe top features and the “is_lifethreatening” feature\nis at the bottom, aligning with intuition.\nWe also consider the coefficients for chest X-ray\nreport classification, enlisting our radiologist co-\nauthor to annotate these. It is important to do this\nanalysis post-hoc because the initial feature annota-\ntions used for the metrics in Table 2 are not neces-\nsarily exhaustive. Figure 6 reports assessments for\nthe linear model coefficients for “atelectasis”. Most\nof the feature influences agree with expectations.\nInterestingly, this analysis indicates the pres-\nence of certain known reporting biases present in\nthe reports. For example, the feature “apical lu-\ncency” specifically indicates possible pneumoth-\norax, a cause of passive atelectasis, and so ratio-\nnally should support the ‘atelectasis’ label, but is\nweighted to refute the label. We speculate that\nthis reflects ‘satisfaction of search’ bias, and other\nclosely aligned reporting biases; pneumothorax is\nsuch a critically important condition that radiolo-\ngists reporting pneumothorax will in many cases\nnot spend time searching for, or reporting the asso-\nciated atelectasis, which in this case is a secondary\nfeature of far lower independent importance.\nFigure 6 shows an example illustrating how a\nclinician might inspect a classification. Because the\npredicted “edema” label disagreed with the CheX-\npert labeler, our radiologist collaborator reviewed\nthis case to determine which features led to this\nostensible “false positive”. While inspecting the\nfeatures for a source of error, they determined that\nthe top positive features (“alveolar filling process”,\n“alveolar fluid”) accurately described the report and\ncould support Edema, but more commonly indicate\npneumonia or other causes. Subsequently, while\nreviewing the report, the radiologist concluded that\nthe model prediction of edema was actually cor-\nrect; the CheXpert label was a false negative. In\nthis case, the inferred features did correctly influ-\nence the ‘edema/no edema’ classification.\nAppendix Figure C.2 shows an example where\na feature influenced the model incorrectly, even\nthough the model made the correct prediction.\nThese examples illustrate two of many ways in\nwhich our approach might facilitate model interpre-\ntation and debugging.\n4.4 Data- and feature- efficiency\nFinally, we consider how our model fares compared\nto baselines in terms of data efficiency. Specifically,\nwe construct learning curves by increasing the per-\ncent of the available data used to train models. In\nFigure 8, we see that the performance of small\nmodels plateaus with relatively minimal supervi-\nsion. At low levels of supervision such models\nare competitive with—and in some cases better\n8483\nTrain Data Percentage (of 24457)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0AUROC\nPhenotyping\nTrain Data Percentage (of 26245)\nReadmission Prediction\nFeature T ype\nBERT\nGround Truth\nInferred Continuous\nInferred Continuous w/ Custom\nTF-IDF (30k)\n1% 3% 10% 30% 100%\nTrain Data Percentage (of 12681)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0AUROC\nMortality Prediction\n1% 3% 10% 30% 100%\nTrain Data Percentage (of 20000)\nChest X-ray Classification\nFigure 8: Learning curves for different methods.\nthan—larger models (e.g., based on Clinical BERT\nrepresentations), which benefit more from addi-\ntional supervision. We again emphasize, however,\nthat using more complex representations comes at\nthe expense of interpretability; this is true even for\nTF-IDF (see Figure 5).\nWe also explore the distribution of information\namong the features by pruning features from our\ncontinuous model after training in Figure 9. If we\nprune features randomly, we see a curve that indi-\ncates that we have not saturated our performance,\nand adding additional features will likely increase\nperformance further. In fact, while annotating 50\nexamples on the test set, our radiologist co-author\nnoted many features that were not included that\nwould likely increase performance.\nIf we prune features with the smallest-magnitude\ncoefficients first, we see that—in contrast to the\nmodel using binary features—the continuous fea-\nture model has a much more rounded curve, indi-\ncating that there are a large number of features that\nare actively contributing to a performance increase.\nWe also note that we use dramatically fewer in-\nferred features compared to TF-IDF. Indeed, as we\nreport in Appendix Table C.1, if we limit TF-IDF to\na vocabulary size of 100, using continuous inferred\nfeatures outperforms TF-IDF on all tasks.\n5 Discussion\nSmall linear models over well-defined features of-\nfer transparency in that one can read off coeffi-\ncients for predictors and inspect which most influ-\n0 20 40 60 80 100\nNumber of Features\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85AUROC\nPrune Randomly (10 runs)\n0 20 40 60 80 100\nNumber of Features\nPrune by Coefficient\nFeature T ype\nInferred Binary w/ Custom\nInferred Continuous w/ Custom\nChest X-Ray Feature Ablation\nFigure 9: Feature ablation for Chest X-ray classifica-\ntion. After training, we explore how pruning features\nrandomly (left) or based on coefficient magnitudes (av-\neraged over all classes) affects performance.\nenced the output for a given instance (e.g., Figure\n7). This is in stark contrast to deep neural net-\nwork models (like Clinical BERT; Alsentzer et al.\n2019), which operate over dense learned embed-\ndings. However, in healthcare high-level features\nmust often be manually extracted from unstruc-\ntured EHR. In this work we proposed to allow do-\nmain experts to define high-level predictors with\nnatural language queries that are used to infer fea-\nture values via LLMs. We then define small linear\nmodels over the resulting features.\nThe promise of this approach is that it enables\none to capitalize on LLMs while still building mod-\nels that provide interpretability. Our approach al-\nlows domain experts to craft features that align with\nquestions a clinician might seek to answer in order\nto make a diagnosis and then use the coefficients\nfrom a trained linear model to determine which\nfeatures inform predictions. Using such “abstract”\nfeatures in place of word frequency predictors con-\nfers greater interpretability (Figure 5).\n6 Conclusions\nWe have proposed using large language models\n(LLMs) to infer high-level features from unstruc-\ntured Electronic Health Records (EHRs) to be used\nas predictors in small linear models. These high-\nlevel features can be specified in arbitrary natural\nlanguage queries composed by domain experts, and\nextracted using LLMs without explicit supervision.\nOn three clinical predictive tasks, we showed\nthat this approach yields performance comparable\nto that achieved using “reference” high-level fea-\ntures (here, ICD codes). On a fourth task (X-ray re-\nport classification), we enlisted a radiologist collab-\norator to provide high-level features in natural lan-\n8484\nguage. We showed a model using these features as\nautomatically extracted via an LLM realized strong\npredictive performance, and—more importantly—\nprovided varieties of model transparency which\nwe confirmed aligned with clinical judgement, and\nwhich provided insights to the domain expert.\nThis work demonstrates the promise of using\nLLMs as high-level feature extractors for small\nlinear models, which may admit varieties of inter-\npretability. However, this is complicated by the\nissues discussed in the Limitations section, below.\nWe believe that more research into this hybrid ap-\nproach is therefore warranted.\nLimitations\nWhile the initial results reported above are encour-\naging, they also raise several questions as to how\none should design queries to yield intermediate\nfeatures that are interpretable, informative, and ver-\nifiable. One limitation of using LLMs to gener-\nate high-level features is that inference for these\nfeatures remains opaque. This inherent limitation\nsuggests a few immediate research questions for\nfuture work.\nThe slippery “interpretability” of inferred fea-\ntures; Is there a trade-off between interpretabil-\nity and predictive power? Not every query in-\nformative of the downstream prediction task will\nnecessarily aid interpretability. For example, when\npredicting mortality, the response to the query “Is\nthe patient at risk of death?” will likely corre-\nlate strongly with the downstream task (as can be\nseen in Figure 4, which reports performance for\nzero-shot prediction). But this query essentially\nparaphrases the original downstream task, and so\ndoes little to aid interpretability. It instead simply\nshifts the problem to explaining what elements of\nthe EHR give rise to the predicted “feature”. This\nsuggests that expert queries should be written to\nelicit features that correlate with, but are distinct\nfrom, the prediction task.\nHow do we know whether predicted features\nare faithful to their queries? A related compli-\ncation to this approach is that zero-shot feature\nextraction will not be perfect. While results for\nICD code proxies indicate good correlation, we\nwill in general not be able to easily verify whether\nthe indicator that is extracted by an LLM is in fact\npredicting the feature intended. The interpretabil-\nity of the resultant model will therefore depend on\nthe degree to which the LLM inferences align with\nthe domain experts’ intent regarding the respective\nfeatures.\nThis suggests that one might want to design\nqueries for high-level features that, whenever pos-\nsible, are easily verifiable upon inspection of the\nEHR. Asking if a patient is at risk of death is not\ndirectly verifiable, because it is necessarily a pre-\ndiction; asking if the patient has an “enlarged heart”\n(for example) probably is. However, even using\nverifiable features does not guarantee that clini-\ncians will hastily confirm or remain unbiased by\nunverified features in practice.\nCan we manually intervene when a model learns\na counter-intuitive dependency? Because we\nexploit features that have a direct relationship to\ndomain expert knowledge, it might be possible to\ndraw upon their judgement to intervene when a\nmodel learns a dependence on a predictor in way\nthat does not align with expectations (e.g. Figures\n6 and C.2). This may happen because either: (1)\nthe feature itself was incorrectly inferred, (2) there\nexists some spurious correlation in the data that\ndoes not hold in general. A less likely possibility\nis that (3) the expert prior was simply incorrect.\nFuture work might study if pruning this feature to\n“correct” the model improves generalization.\nEthics Statement\nOur approach raises ethical concerns because the\nextracted features may be incorrect. Though this\nwork does show promise in terms of the interpre-\ntations aligning with clinical expectations, it is far\nfrom ready to deploy because there is a danger that\nclinicians will make incorrect assumptions even if\nwarned of the model’s potential for making mis-\ntakes both in producing features. Just because the\nexplanation of the model makes sense clinically\ndoes not mean the underlying features for an in-\nstance are factual. Models like this may also play\ninto clinician bias given that the model is trained\ndata produced by clinicians. Therefore, we present\nthis work only for scientific exploration, and it is\nnot intended to be used in a deployed system in its\ncurrent form.\nAcknowledgements\nWe acknowledge partial funding for this work by\nNational Library of Medicine of the National In-\nstitutes of Health (NIH) under award numbers\n8485\nR01LM013772 and R01LM013891. The work\nwas also supported in part by the National Science\nFoundation (NSF) grant 1901117. The content is\nsolely the responsibility of the authors and does not\nnecessarily represent the official views of the NIH\nor the NSF.\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are zero-shot clinical information extractors.\nIn Proceedings of Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical BERT\nembeddings. In Proceedings of the 2nd Clinical Nat-\nural Language Processing Workshop, pages 72–78,\nMinneapolis, Minnesota, USA. Association for Com-\nputational Linguistics.\nWilliam Boag, Mercy Oladipo, and Peter Szolovits.\n2022. EHR Safari: Data is Contextual. In Proceed-\nings of the 7th Machine Learning for Healthcare\nConference, pages 391–408. PMLR.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jef-\nfrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark,\nJoseph E Mietus, George B Moody, Chung-Kang\nPeng, and H Eugene Stanley. 2000. Physiobank,\nphysiotoolkit, and physionet: components of a new\nresearch resource for complex physiologic signals.\ncirculation, 101(23):e215–e220.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and pre-\ndicting hospital readmission. arXiv:1904.05342.\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya,\net al. 2019. Chexpert: A large chest radiograph\ndataset with uncertainty labels and expert comparison.\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume 33, pages 590–597.\nA Johnson, T Pollard, R Mark, S Berkowitz, and\nS Horng. 2019a. Mimic-cxr database (version 2.0.0).\nphysionet.\nAlistair Johnson, Matt Lungren, Yifan Peng, Zhiyong\nLu, Roger Mark, Seth Berkowitz, and Steven Horng.\nMimic-cxr-jpg-chest radiographs with structured la-\nbels.\nAlistair E. W. Johnson, Tom J. Pollard, Seth J.\nBerkowitz, Nathaniel R. Greenbaum, Matthew P.\nLungren, Chih-ying Deng, Roger G. Mark, and\nSteven Horng. 2019b. MIMIC-CXR, a de-identified\npublicly available database of chest radiographs with\nfree-text reports. Scientific Data, 6(1):317.\nAlistair E. W. Johnson, Tom J. Pollard, and Roger G.\nMark. 2016a. MIMIC-III clinical database (version\n1.4).\nAlistair EW Johnson, Tom J Pollard, Nathaniel R Green-\nbaum, Matthew P Lungren, Chih-ying Deng, Yifan\nPeng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz,\nand Steven Horng. 2019c. Mimic-cxr-jpg, a large\npublicly available database of labeled chest radio-\ngraphs. arXiv preprint arXiv:1901.07042.\nAlistair E.W. Johnson, Tom J. Pollard, Lu Shen, Li-\nwei H. Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G. Mark. 2016b. MIMIC-III,\na freely accessible critical care database. Scientific\nData, 3(1):160035.\nYoungwoo Kim, Cheng Li, Bingyang Ye, Amir Tah-\nmasebi, and Javed Aslam. 2021. Supervised Learn-\ning in the Presence of Noise: Application in ICD-10\nCode Classification. ArXiv:2103.07808 [cs].\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018. Explainable predic-\ntion of medical codes from clinical text. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1101–1111, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Se-\nbastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\ncia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJonathan H. Clark, Stephan Lee, Dan Garrette, James\nLee-Thorp, Colin Raffel, Noam Shazeer, Marvin\nRitter, Maarten Bosma, Alexandre Passos, Jeremy\n8486\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. arXiv\npreprint arXiv:2203.17189.\nJason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew Mingbo Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nHaoran Zhang, Amy X. Lu, Mohamed Abdalla,\nMatthew McDermott, and Marzyeh Ghassemi. 2020.\nHurtful words: Quantifying biases in clinical con-\ntextual word embeddings. In Proceedings of the\nACM Conference on Health, Inference, and Learn-\ning, CHIL ’20, page 110–120, New York, NY , USA.\nAssociation for Computing Machinery.\n8487\nA Full Prompt\nTable A.1: The full prompts incorporate the input text and the questions, described in section 3 and appendix Table\nB according to the following templates.\nMIMIC Tasks Read the following text from a clinical note:\n————–\n<input>\n————–\n<question>\nChest X-ray Classification Task Read the following Chest X-ray report:\n————–\n<input>\n————–\n<question>\nB Hand-crafted Feature Queries for Chest X-ray Dataset\nTable B.2: Features grouped by the labels they may support. (Features may be written under more than one label.)\nNo Finding\nnormal Is this patient normal?\nclear lungs Does this patient have clear lungs?\nnormal cardiac silhouette Does this patient have a normal cardiac silhouette?\nsharp costophrenic angles Does this patient have sharp costophrenic angles?\nEnlarged Cardiomediastinum\nenlarged cardiomediastinal silhouette Does this patient have an enlarged cardiomediasti-\nnal silhouette?\nCardiomegaly\nenlarged cardiomediastinal silhouette Does this patient have an enlarged cardiomediasti-\nnal silhouette?\nincreased cardiothoracic ratio Does this patient have an increased cardiothoracic\nratio?\nprominent right atrial contour Does this patient have a prominent right atrial con-\ntour?\nenlarged heart Does this patient have an enlarged heart?\nglobular cardiac silhouette Does this patient have a globular cardiac silhou-\nette?\nrounded left heart border Does this patient have a rounded left heart border?\nuplifted cardiac apex Does this patient have an uplifted cardiac apex?\ndouble density Does this patient have a double density?\nsplaying of carina Is there splaying of the carina?\nincrease subcarinal angle Does this patient have an increased subcarinal an-\ngle?\nconvex left atrial appendage Does this patient have a convex left atrial ap-\npendage?\nthird mogul sign Does this patient have a third mogul sign?\nsuperior displacement of left mainstem bronchus Does this patient have superior displacement of\nleft mainstem bronchus?\n8488\nrounded cardiac apex Does this patient have a rounded cardiac apex?\nshmoo sign Does this patient have a shmoo sign?\nhoffman rigler sign Does this patient have a Hoffman-Rigler sign?\nEdema\nupper lobe pulmonary venous engorgement Does this patient have upper lobe pulmonary ve-\nnous engorgement?\nstags antler sign Does this patient have a stag’s antler sign?\nbilateral opacity Does this patient have bilateral opacity?\nsymmetric perihilar opacity Does this patient have a symmetric perihilar opac-\nity?\nbat wing opacity Does this patient have a bat wing opacity?\nenlarged cardiac silhouette Does this patient have an enlarged cardiac silhou-\nette?\nincreased cardiothoracic ratio Does this patient have an increased cardiothoracic\nratio?\nperibronchiolar cuffing Does this patient have peribronchiolar cuffing?\nhazy perihilar opacity Does this patient have a hazy perihilar opacity?\nseptal thickening Does this patient have septal thickening?\nkerley b lines Does this patient have Kerley B lines?\nthickening of the fissures Is there a thickening of the fissures?\nfluid in the fissures Does this patient have fluid in the fissures?\npleural effusion Does this patient have a pleural effusion?\nPneumonia\nreticular opacity Does this patient have a reticular opacity?\nhazy opacity Does this patient have a hazy opacity?\nconsolidation Does this patient have consolidation?\nsegmental opacity Does this patient have a segmental opacity?\nlobar opacity Does this patient have a lobar opacity?\nbulging fissures Does this patient have bulging fissures?\ncavitation Does this patient have cavitation?\nAtelectasis\nsubsegmental linear basilar opacity Does this patient have a subsegmental linear basi-\nlar opacity?\nlinear basilar opacity Does this patient have a linear basilar opacity?\nsubsegmental crescentic basilar opacity Does this patient have a subsegmental crescentic\nbasilar opacity?\ncrescentic basilar opacity Does this patient have a crescentic basilar opacity?\ndecreased lung volumes Does this patient have decreased lung volumes?\nindistinct basilar opacity Does this patient have indistinct basilar opacities?\nPneumothorax\napical lucency Does this patient have an apical lucency?\nperipheral lucency Does this patient have a peripheral lucency?\ncollapse of lung Is there a collapse of a lung?\nvisible visceral pleura Does this patient have visible visceral pleura?\nFracture\nlucency in a rib or clavicle Does this patient have a lucency in a rib or clavi-\ncle?\ndeformity of a rib or clavicle Does this patient have a deformity of a rib or clav-\nicle?\nwedge deformity of a vertebra Does this patient have a wedge deformity of a\nvertebra?\n8489\nstep off of a rib or clavicle Does this patient have a step-off of a rib or clavi-\ncle?\nLung Lesion\nrounded opacity Does this patient have a rounded opacity?\npulmonary calcification Does this patient have an pulmonary calcification?\ncavitation Does this patient have cavitation?\nabnormal high density Does this patient have an abnormal high density?\nopacity with indistinct borders Does this patient have an opacity with indistinct\nborders?\nspiculated opacity Does this patient have a spiculated opacity?\nhazy opacity Does this patient have a hazy opacity?\nLung Opacity\nalveolar blood Is there alveolar blood?\nalveolar fluid Is there alveolar fluid?\nPleural Other\npleural empyema Is there pleural empyema?\nhemothorax Does the patient have hemothorax?\npleural tumor Is there pleural tumor?\npleural thickening Is there pleural tumor?\ncalcificied pleural plaques Does this patient have a calcified pleural plaque?\nforeign body in pleural space Is there a foreign body in the pleural space?\nSupport Devices\nsurgical clip Does this patient have a surgical clip?\nmetallic density Does this patient have a metallic density?\ncurvilinear density Does this patient have a curvilinear density?\nforeign body Does this patient have a foreign body?\nmetal lead Does this patient have a metal lead?\nwire Does this patient have a wire?\ntube Does this patient have a tube?\nstent Does this patient have a stent?\nendotracheal tube Does this patient have an endotracheal tube?\nchest tube Does this patient have a chest tube?\ncentral venous catheters Does this patient have a central venous catheter?\npicc line Does this patient have a PICC line?\ntracheal stent Does this patient have a tracheal stent?\ncoronary stent Does this patient have a coronary stent?\naortic stent Does this patient have an aortic stent?\narterial stent Does this patient have an arterial stent?\npacemaker Does this patient have a pacemaker?\nicd Does this patient have an ICD?\naortic balloon pump Does this patient have an aortic balloon pump?\nlvad Does this patient have an lvad?\nclamshell closure device Does this patient have a clamshell closure device?\nSVC stent Does this patient have an SVC stent?\nIVC stent Does this patient have an IVC stent?\nIVC filter Does this patient have an IVC filter?\nOther Features\nalveolar hemorrhage Is there alveolar hemorrhage?\ndense op air filling with pus water blood Does this patient have a dense opacity that sug-\ngests the airspace filling with pus, water, or blood?\nesophageal tube Does this patient have an esophageal tube?\n8490\ng-tube Does this patient have a g-tube?\ngastric tube Does this patient have a gastric tube?\nloculated pleural effusion Is there a loculated pleural effusion?\nnasogastric tube Does this patient have a nasogastric tube?\n8491\nPhenotyping Readmission Mortality Chest X-ray\nBERT 0.820 ± 0.024 0.661 ± 0.019 0.874 ± 0.025 0.991 ± 0.008\nTF-IDF (30) 0.624 ± 0.031 0.573 ± 0.020 0.692 ± 0.034 0.760 ± 0.033\nTF-IDF (100) 0.697 ± 0.030 0.613 ± 0.020 0.787 ± 0.031 0.881 ± 0.027\nTF-IDF (1k) 0.813 ± 0.025 0.668 ± 0.019 0.863 ± 0.025 0.952 ± 0.014\nTF-IDF (30k) 0.830 ± 0.024 0.694 ± 0.019 0.872 ± 0.024 0.952 ± 0.013\nGround Truth 0.798 ± 0.027 0.645 ± 0.019 0.697 ± 0.037 -\nInferred Binary 0.651 ± 0.031 0.589 ± 0.020 0.699 ± 0.035 -\nInferred Binary w/ Custom 0.669 ± 0.031 0.603 ± 0.020 0.719 ± 0.033 0.828 ± 0.034\nInferred Continuous 0.705 ± 0.029 0.614 ± 0.020 0.752 ± 0.032 -\nInferred Continuous w/ Custom 0.719 ± 0.029 0.626 ± 0.020 0.806 ± 0.029 0.889 ± 0.026\nZero-Shot Downstream 0.711 ± 0.030 0.579 ± 0.020 0.816 ± 0.030 0.800 ± 0.030\nTable C.1: Expanded results of Downstream Classification Performance with 95% Confidence Intervals.\nPhenotype Readmission Mortality Chest X-ray\nTF-IDF (30) 3.05 3.41 1.00 1.98\nTF-IDF (100) 3.84 4.54 2.67 1.79\nTF-IDF (1k) 4.63 6.84 6.24 1.86\nTF-IDF (30k) 9.50 10.30 10.27 4.17\nGround Truth 1.30 2.39 2.00 -\nInferred Binary 1.96 2.39 1.85 -\nInferred Binary w/ Custom 2.03 2.56 1.50 3.98\nInferred Continuous 1.65 2.31 1.97 -\nInferred Continuous w/ Custom 1.75 2.51 1.13 3.08\nTable C.2: How is the magnitude mass distributed across coefficients? We report the entropy of the distribution\ndescribed by the coefficient magnitudes: H(softmax(|c|)) where c represents the vector of coefficients for a model\nand |·| takes the absolute value of each element. (Entropy is averaged over labels in the case of Phenotype and\nChest X-ray tasks.) This measure gives some insight into how uniform the coefficient magnitudes are. TF-IDF (30k)\nhas much higher entropy across the board indicating that many more features have high magnitude in this model.\nC Extra Figures and Tables\nNo Finding\nEnlarged Cardiomediastinum\nCardiomegaly\nEdema\nPneumoniaAtelectasis\nPneumothorax\nFracture\nLung LesionLung OpacityPleural Other\nSupport Devices\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0AUROC\nFeature Alignment with Clinical Expectations\nFigure C.1: Using a set of features annotated as supporting a particular label, we plot the AUROC obtained by using\nthe coefficient values (from the continuous features model) to predict this feature list for each label.\n8492\nTable C.3: AUC per Phenotyping label.\nBERT TF-IDF Ground Inferred Inferred Zero-Shot\n(30k) Truth Continuous Continuous Downstream\nw/ Custom\nConduction Disorders 0.856 0.847 0.712 0.678 0.676 0.758\nPneumonia (Except That\nCaused By Tuberculosis Or\nSexually Transmitted Disease)\n0.809 0.834 0.744 0.709 0.713 0.733\nDisorders Of Lipid Metabolism 0.785 0.763 1.000 0.705 0.723 0.639\nAcute Myocardial Infarction 0.867 0.862 0.785 0.788 0.796 0.805\nOther Lower Respiratory Dis-\nease\n0.696 0.719 0.684 0.580 0.630 0.649\nPleurisy; Pneumothorax; Pul-\nmonary Collapse\n0.663 0.722 0.583 0.593 0.627 0.661\nAny-Chronic 0.874 0.867 0.951 0.779 0.784 0.659\nDiabetes Mellitus With Compli-\ncations\n0.863 0.867 0.652 0.774 0.776 0.831\nCardiac Dysrhythmias 0.846 0.838 0.905 0.797 0.799 0.793\nAny-Acute 0.819 0.834 0.819 0.715 0.726 0.515\nCoronary Atherosclerosis And\nOther Heart Disease\n0.870 0.868 0.964 0.820 0.838 0.801\nHypertension With Complica-\ntions And Secondary Hyperten-\nsion\n0.873 0.870 0.864 0.749 0.769 0.670\nSepticemia (Except In Labor) 0.874 0.883 0.748 0.732 0.728 0.782\nRespiratory Failure; Insuffi-\nciency; Arrest (Adult)\n0.882 0.884 0.985 0.787 0.784 0.780\nAny-Disease 0.903 0.897 0.934 0.812 0.826 0.632\nCongestive Heart Failure; Non-\nhypertensive\n0.837 0.830 0.979 0.767 0.770 0.704\nChronic Obstructive Pulmonary\nDisease And Bronchiectasis\n0.777 0.789 0.670 0.658 0.670 0.716\nComplications Of Surgical Pro-\ncedures Or Medical Care\n0.712 0.756 0.589 0.569 0.581 0.701\nAcute And Unspecified Renal\nFailure\n0.830 0.848 0.956 0.703 0.714 0.646\nShock 0.873 0.872 0.756 0.715 0.726 0.694\nOther Upper Respiratory Dis-\nease\n0.831 0.833 0.642 0.686 0.698 0.737\nFluid And Electrolyte Disorders 0.734 0.760 0.691 0.648 0.650 0.618\nOther Liver Diseases 0.828 0.855 0.661 0.641 0.669 0.730\nGastrointestinal Hemorrhage 0.855 0.887 0.583 0.628 0.636 0.779\nDiabetes Mellitus Without Com-\nplication\n0.700 0.723 0.971 0.719 0.720 0.638\nAcute Cerebrovascular Disease 0.907 0.936 0.677 0.628 0.709 0.841\nEssential Hypertension 0.729 0.720 0.996 0.627 0.634 0.588\nChronic Kidney Disease 0.857 0.869 0.830 0.736 0.771 0.817\nTable C.4: AUC per CheXpert label.\nBERT TF-IDF Inferred Zero-Shot\n(30k) Continuous Downstream\nw/ Custom\nAtelectasis 0.997 0.968 0.888 0.944\nCardiomegaly 0.997 0.957 0.913 0.876\nEdema 0.997 0.961 0.910 0.954\nEnlarged Cardiom. 0.988 0.904 0.764 0.592\nSupport Devices 0.996 0.965 0.925 0.826\nPneumothorax 0.990 0.950 0.911 0.868\nFracture 0.999 0.987 0.948 0.977\nPneumonia 0.982 0.921 0.795 0.799\nPleural Other 0.995 0.965 0.804 0.566\nNo Finding 0.987 0.944 0.906 0.230\nLung Opacity 0.994 0.944 0.899 0.858\nLung Lesion 0.976 0.965 0.862 0.804\n8493\nFINDINGS: Two images of the chest \nshows a small consolidation at the \nright base, most consistent with \npneumonia. There are no other \nconsolidations. There is no evidence \nof interstitial edema. There are no \npleural effusions. The heart size is at \nthe upper limits of normal. The \nmediastinal contours are normal. \nThere are sternotomy wires in place.\n \nIMPRESSION: Consolidation in the \nright base is most consistent with \npneumonia.\nTargets\nPredictions\nCardiomegaly\nCardiomegaly\nPneumonia\nPneumonia\nconsolidation\nwire\nlobar opacity\nalveolar filling process\ndense op air filling with pus water blood\nnormal cardiac silhouette\ndense op\nlvad\nopacity with indistinct borders\nrounded opacity\nT op Continuous Features\n const\ndense op\nopacity with indistinct borders\nlobar opacity✔\n✗ \nalveolar filling process\nconsolidation\ndense op air filling with pus water blood\nrounded cardiac apex\nnormal\nnormal cardiac silhouette\nT op Decision-Impacting Features (Pneumonia)\nFigure C.2: Qualitative Example of Features and Feature Influence for Pneumonia. (Similar to Figure 7.) This\nseems to correctly predict Pneumonia, but this happens in spite of consolidation being incorrectly identified by the\nlinear model as not supporting Pneumonia. These plots make clear that this is not a mistake of the feature extractor\nat inference time because consolidation is among the top Features. The mistake comes from the linear model having\na negative coefficient for consolidation.\n8494",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.8783818483352661
    },
    {
      "name": "Computer science",
      "score": 0.7964671850204468
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6182547807693481
    },
    {
      "name": "Feature extraction",
      "score": 0.5642016530036926
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5440889596939087
    },
    {
      "name": "Machine learning",
      "score": 0.4834192991256714
    },
    {
      "name": "Task (project management)",
      "score": 0.44670945405960083
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43220382928848267
    },
    {
      "name": "Random forest",
      "score": 0.4240856468677521
    },
    {
      "name": "Natural language processing",
      "score": 0.42355257272720337
    },
    {
      "name": "Data mining",
      "score": 0.3659334182739258
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3340970277786255
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    },
    {
      "id": "https://openalex.org/I1283280774",
      "name": "Brigham and Women's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ]
}