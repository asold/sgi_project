{
  "title": "Limitations of Transformers on Clinical Text Classification",
  "url": "https://openalex.org/W3132259035",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2138294660",
      "name": "Gao Shang",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3041042378",
      "name": "Alawad Mohammed",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A4301856161",
      "name": "Young, M. Todd",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A4222326135",
      "name": "Gounley, John",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A4288581619",
      "name": "Schaefferkoetter, Noah",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A4222367110",
      "name": "Yoon Hong-Jun",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2973063049",
      "name": "Wu, Xiao-Cheng",
      "affiliations": [
        "Louisiana State University Health Sciences Center New Orleans"
      ]
    },
    {
      "id": "https://openalex.org/A4287248062",
      "name": "Durbin, Eric B.",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A4287248065",
      "name": "Doherty, Jennifer",
      "affiliations": [
        "Huntsman Cancer Institute",
        "University of Utah"
      ]
    },
    {
      "id": "https://openalex.org/A2901545985",
      "name": "Stroup, Antoinette",
      "affiliations": [
        "New Jersey State Library"
      ]
    },
    {
      "id": "https://openalex.org/A2750170115",
      "name": "Coyle, Linda",
      "affiliations": [
        "Information Management Services"
      ]
    },
    {
      "id": "https://openalex.org/A2748379327",
      "name": "Tourassi Georgia",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2805089815",
    "https://openalex.org/W2884001105",
    "https://openalex.org/W2768377508",
    "https://openalex.org/W6744560608",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W2789244308",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6774952039",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2611254939",
    "https://openalex.org/W6736878346",
    "https://openalex.org/W2981133974",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W6757721448",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2512963549",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W6761748628",
    "https://openalex.org/W2949479579",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2771477360",
    "https://openalex.org/W6767997687",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6726298457",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6776048684",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W2952087461",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W6768179808",
    "https://openalex.org/W6762235773",
    "https://openalex.org/W2966351171",
    "https://openalex.org/W2985301765",
    "https://openalex.org/W3039996909",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W2805185296",
    "https://openalex.org/W6770982027",
    "https://openalex.org/W3000238064",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3011718307",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2742947407",
    "https://openalex.org/W4293582125",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963126915",
    "https://openalex.org/W2939507640",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2962912956",
    "https://openalex.org/W3102251128",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W2963082289",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W4288104408",
    "https://openalex.org/W4288623406",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3102144021",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2990070951"
  ],
  "abstract": "Bidirectional Encoder Representations from Transformers (BERT) and BERT-based approaches are the current state-of-the-art in many natural language processing (NLP) tasks; however, their application to document classification on long clinical texts is limited. In this work, we introduce four methods to scale BERT, which by default can only handle input sequences up to approximately 400 words long, to perform document classification on clinical texts several thousand words long. We compare these methods against two much simpler architectures - a word-level convolutional neural network and a hierarchical self-attention network - and show that BERT often cannot beat these simpler baselines when classifying MIMIC-III discharge summaries and SEER cancer pathology reports. In our analysis, we show that two key components of BERT - pretraining and WordPiece tokenization - may actually be inhibiting BERT's performance on clinical text classification tasks where the input document is several thousand words long and where correctly identifying labels may depend more on identifying a few key words or phrases rather than understanding the contextual meaning of sequences of text.",
  "full_text": "3596 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\nLimitations of T ransformers on Clinical\nT ext Classiﬁcation\nShang Gao , Mohammed Alawad, M. T odd Y oung, John Gounley , Noah Schaefferkoetter,\nHong Jun Y oon, Xiao-Cheng Wu, Eric B. Durbin, Jennifer Doherty , Antoinette Stroup ,\nLinda Coyle, and Georgia T ourassi\nAbstract— Bidirectional Encoder Representations from\nTransformers (BERT) and BERT-based approaches are the\ncurrent state-of-the-art in many natural language process-\ning (NLP) tasks; however, their application to document\nclassiﬁcation on long clinical texts is limited. In this work,\nwe introduce four methods to scale BERT, which by de-\nfault can only handle input sequences up to approximately\n400 words long, to perform document classiﬁcation on\nclinical texts several thousand words long. We compare\nthese methods against two much simpler architectures – a\nword-level convolutional neural network and a hierarchical\nself-attention network – and show that BERT often cannot\nbeat these simpler baselines when classifying MIMIC-III dis-\ncharge summaries and SEER cancer pathology reports. In\nour analysis, we show that two key components of BERT\nManuscript received September 14, 2020; revised February 5, 2021;\naccepted February 22, 2021. Date of publication February 26, 2021;\ndate of current version September 3, 2021. This work was supported in\npart by the Joint Design of Advanced Computing Solutions for Cancer\n(JDACS4C) program established by the U.S. Department of Energy\n(DOE) and National Cancer Institute (NCI) of the National Institutes of\nHealth, in part by the auspices of the U.S. Department of Energy by Ar-\ngonne National Laboratory under Contract No. DE-AC02-06-CH11357,\nin part by Lawrence Livermore National Laboratory under Contract No.\nDEAC52-07NA27344, in part by Los Alamos National Laboratory under\nContract No. DE-AC5206NA25396, and in part by Oak Ridge National\nLaboratory under Contract No. DE-AC05-00OR22725. This work used\nresources of Oak Ridge Leadership Computing Facility at Oak Ridge\nNational Laboratory, which was supported by the Ofﬁce of Science of the\nU.S. Department of Energy under Contract No. DE-AC05-00OR22725.\nThis work was supported by Exascale Computing Project (17-SC-20-\nSC), a collaborative effort of the U.S. Department of Energy Ofﬁce of\nScience and National Nuclear Security Administration.(Corresponding\nauthors: Shang Gao; Georgia T ourassi.)\nShang Gao, Mohammed Alawad, M. T odd Y oung, John Gounley,\nNoah Schaefferkoetter, Hong Jun Y oon, and Georgia T ourassi are\nwith Oak Ridge National Laboratory, Oak Ridge, TN 37830 USA\n(e-mail: gaos@ornl.gov; alawadmm@ornl.gov; youngmt1@ornl.gov;\ngounleyjp@ornl.gov; schaefferknt@ornl.gov; yoonh@ornl.gov; tour-\nassig@ornl.gov).\nXiao-Cheng Wu is with Louisiana T umor Registry, Louisiana State\nUniversity Health Sciences Center, New Orleans, LA 70112 USA\n(e-mail: xwu@lsuhsc.edu).\nEric B. Durbin is with Kentucky Cancer Registry, University of Ken-\ntucky, Lexington, KY 40536 USA (e-mail: ericd@kcr.uky.edu).\nJennifer Doherty is with Utah Cancer Registry, University of Utah\nHealth Huntsman Cancer Institute, Salt Lake City, UT 84132 USA\n(e-mail: jen.doherty@hci.utah.edu).\nAntoinette Stroup is with New Jersey State Cancer Registry, T renton,\nNJ 08625 USA (e-mail: ams722@sph.rutgers.edu).\nLinda Coyle is with the Information Management Services Inc.,\nCalverton, MD 20705 USA (e-mail: coylel@imsweb.com).\nDigital Object Identiﬁer 10.1109/JBHI.2021.3062322\n– pretraining and WordPiece tokenization – may actually be\ninhibiting BERT’s performance on clinical text classiﬁcation\ntasks where the input document is several thousand words\nlong and where correctly identifying labels may depend\nmore on identifying a few key words or phrases rather\nthan understanding the contextual meaning of sequences\nof text.\nIndex Terms— BERT, clinical text, deep learning, natural\nlanguage processing, neural networks, text classiﬁcation.\nI. INTRODUCTION\nD\nOCUMENT classiﬁcation is an essential task in clinical\nnatural language processing (NLP). In the clinical setting,\nlabels are often available only at the document level rather than\nat the individual word level, such as when unstructured clinical\nnotes are linked to structured data from electronic health records\n(EHRs), and thus document classiﬁcation is an essential tool in\npractical automation of clinical workﬂows. Timely classiﬁcation\nof key data elements from clinical documents is extremely im-\nportant for applications such as precision medicine, population\nhealth surveillance, and research and policy. Unfortunately, in\nthe clinical setting, human annotation of EHRs can be extremely\ntime-consuming and expensive due to the technical nature of\nthe content and the expert knowledge required to parse it; thus,\neffective automated classiﬁcation of clinical text such as cancer\npathology reports and patient notes from hospital stays can make\nmeaningful contributions toward health-related outcomes [1].\nCurrently, Bidirectional Encoder Representations from Trans-\nformers (BERT) [2] and BERT-based approaches achieve state-\nof-the-art performance in many common tasks within the general\nNLP community such as question answering, natural language\nunderstanding, and text generation. BERT is a computationally\nexpensive deep learning approach that is ﬁrst pretrained on a very\nlarge corpus of unlabelled text on the order of 1 billion or more\nwords – this pretraining step typically takes hundreds to thou-\nsands of GPU or TPU hours [3], [4] and allows the model to learn\nnuanced linguistic patterns that may be useful for downstream\ntasks. Once pretrained, the model is then ﬁne-tuned on a speciﬁc\ntask of interest. To limit the vocabulary size and generalize better\nto new words outside the training vocabulary, BERT utilizes\nsubword-level WordPiece tokens rather that word-level tokens\nas input.\nAdapting BERT to the task of clinical document classiﬁ-\ncation poses non-trivial challenges. First, most BERT-based\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3597\nimplementations have a maximum input length of 512 Word-\nPiece tokens, which is roughly equal to 400 words. Unfor-\ntunately, clinical documents can very easily exceed this limit\n– the average discharge summary in the MIMIC-III dataset\nis approximately 2000 word tokens [5]. Second, to maximize\nperformance, BERT-based models must be pretrained on a text\ncorpus that is from a similar domain as the downstream applica-\ntion task. Therefore, clinical practitioners who wish to apply\nBERT-based models but do not have access to the compute\nor data necessary to pretrain their own models must rely on\ndownloading existing pretrained models such as BioBERT [6]\nor BlueBERT [7]. Some recent work, such as the Reformer [8]\nand LongFormer [9] models, adapt BERT for longer input texts;\nhowever, at the time of this study, there exist no publicly available\npretrained weights in the biomedical and/or clinical domain for\nthese models. For this reason, we utilize BlueBERT, which is\nthe original BERT model pretrained on sentences from Pubmed\nabstracts and MIMIC-III clinical notes, as the main model for\nthis work.\nIn this work, we test different methods to adapt BlueBERT for\ntext classiﬁcation on long clinical documents – these methods\nconsist of splitting long documents into smaller chunks, pro-\ncessing the chunks individually, and then combining the outputs\nusing max pooling or attention-based methods. We apply these\nmethods to both the single-label and multilabel classiﬁcation\nsettings. We compare the performance of BlueBERT against two\nstrong baselines – a shallow, word-level convolutional neural\nnetwork (CNN) [10] and a hierarchical self-attention network\n(HiSAN) [11], both of which have nearly two orders of mag-\nnitude fewer learnable parameters than BERT. We show that\nBERT actually achieves similar performance to the CNN and\nunderperforms the HiSAN on many of the clinical document\nclassiﬁcation tasks that we test on. Our contributions are as\nfollows:\nr We compare the effectiveness of different ways to adapt\nBERT for document classiﬁcation on long clinical texts\nup to several thousand words in length\nr We evaluate the effectiveness of BERT on clinical single-\nlabel and multilabel document classiﬁcation against two\nother strong baselines – the CNN and the HiSAN\nr We show that a much simpler deep learning model, the\nHiSAN, can obtain similar or better performance com-\npared to BERT on many of our clinical document classi-\nﬁcation tasks\nr To better understand the weaknesses of BERT in our tasks,\nwe analyze the attention weights within the HiSAN and\nBERT to understand how each model identiﬁes keywords\nand show that using WordPiece subword tokens may be\nmore difﬁcult than using word-level tokens\nII. RELA TEDWORK\nWhile BERT and BERT-based models have achieved state-\nof-the-art performance across a wide range of various NLP\ntasks including question answering [12], [13], information ex-\ntraction [14], [15], and summarization [16], [17], their applica-\ntions to long document classiﬁcation tasks have been extremely\nlimited. To our knowledge, there exists only one previous in-\ndepth study on strategies to adapt BERT for long document\nclassiﬁcation: Sun et al. [18] explore different techniques for\nusing BERT to classify moderate-length documents from IMDb\nreviews, Yelp reviews, Sogou News, and other similar datasets.\nThe study ﬁnds that the best overall classiﬁcation accuracy is\nachieved by using only the ﬁrst 128 and the last 382 tokens in\neach document as the input into BERT and dropping all interme-\ndiate content. While other works [19]–[21] have applied BERT\nto text classiﬁcation related tasks, none explore the problem of\nlong-document inputs that are longer than BERT’s default max\ninput length of 512 WordPiece tokens.\nThere are several reasons that the ﬁndings from [18] may\nnot hold in the clinical document domain. First, most of the\ndatasets tested in [18] are moderate in length – for example,\nonly 12.69% of the documents in IMDb exceed 512 tokens in\nlength, 4.60% in Yelp, and 46.23% in Sogou, and even in Sogou\nthe average length is only 737 tokens. Thus, it is uncertain how\nBERT will perform on datasets such as MIMIC-III where the\naverage discharge summary is over 2000 tokens long. Second, in\nclinical documents classiﬁcation tasks, the presence of a speciﬁc\nlabel may be indicated by only a short phrase that appears only\nonce in the entire document; therefore, using only the ﬁrst 128\nand the last 382 tokens may be more detrimental than in a task\nsuch as news classiﬁcation or sentiment analysis, where context\nclues may be scattered throughout the document.\nIn the clinical and biomedical domain, BERT has been applied\nto various tasks that do not include document-level classiﬁcation.\nBioBERT [6], which is pretrained on PubMed abstracts of PMC\nfull-text articles, showed superior performance on biomedical\nnamed entity recognition, relation extraction, and question an-\nswering tasks. ClinicalBERT [22], which starts with BioBert and\nthen further pretrains on MIMIC-III clinical notes, showed su-\nperior performance on clinical natural language inference tasks.\nBlueBERT [7], pretrained on PubMed abstracts and MIMIC-III\nclinical notes, achieved superior performance on biomedical and\nclinical sentence similarity, named entity recognition, relation\nextraction, and short document classiﬁcation tasks. Two com-\nmon characteristics of all these tasks are (1) input length is less\nthan or equal to 512 WordPiece tokens and (2) understanding\nsequences of words in context is generally critical to the task.\nIn [23], authors pretrained their own BERT model on Italian\nclinical text, applied it to Italian pathology report classiﬁcation,\nand found that BERT underperforms more simple architectures,\nbut the study focused on short inputs less than 512 WordPiece\ntokens in length. BERT has yet to be thoroughly tested under\nsettings where the input document is several thousand words\nlong and where correctly identifying labels may depend more on\nidentifying a few key words or phrases rather than understanding\nthe contextual meaning of sequences of text.\nThe current state-of-the-art approaches for clinical document\nclassiﬁcation are generally models that pre-date contextual word\nembedding-based approaches such as BERT. Clinical NLP ap-\nproaches often lag behind those used in the general NLP commu-\nnity partly due to the legal challenges of releasing open research\ndatasets to promote the development of new approaches [24],\n[25]. Recent approaches for clinical document classiﬁcation\n3598 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\ninclude rule-based methods [26], [27], traditional machine learn-\ning [28], [29], convolutional neural networks (CNNs) [30], [31],\nrecurrent neural networks (RNNs) [32], [33], and self-attention\nnetworks [11].\nIn this work, we compare different strategies to adapt BERT\nto long documents against existing strong baselines using dis-\ncharge summaries from the MIMIC-III dataset and cancer\npathology reports obtained from Louisiana Tumor Registry,\nKentucky Cancer Registry, Utah Cancer Registry, and New\nJersey State Cancer Registry. There are three multilabel clas-\nsiﬁcation tasks for MIMIC-III – diagnostic codes, diagnostic\ncategories, and procedure codes – and six single-label classiﬁ-\ncation tasks for the cancer pathology reports – identifying cancer\nsite, subsite, laterality, behavior, histology, and grade.\nIII. MA TERIALS ANDMETHODS\nA. BERT for Document Classiﬁcation\nIn this work, we begin with the assumption that end-users wish\nto apply BERT to their document classiﬁcation tasks but lack\nthe computational resources and/or training data on the order of\n1B+ words required to pretrain BERT from scratch; thus, users\nmust start from an available pretrained model. Because we are\nworking with clinical text documents, we utilize BlueBERT [7],\nwhich is the BERT model pretrained on PubMed abstracts and\nMIMIC-III clinical notes. As the architecture of BERT has been\nwidely described and explored in existing literature, we refer\nthe reader to those studies [2], [34], [35] to learn about the base\narchitecture of the BERT model.\nBecause the self-attention mechanism used in BERT has\nmemory requirements that scale quadratically based off the se-\nquence length, the original BERT model was primarily designed\nto handle sentence-length and paragraph-length inputs and has\na maximum input length of 512 WordPiece tokens, or roughly\n400 word tokens. As a result, subsequent BERT-based models\npretrained on different corpora, including BioBERT, Clinical\nBERT, and BlueBert, all share this same limitation on input\nlength. To adapt BERT for long document classiﬁcation, we\nexplore the following strategies (illustrated inFig. 1):\n1) First 510 WordPiece T okens Only:For any input docu-\nment, we convert the document into WordPiece tokens and use\nonly the ﬁrst 510 tokens. As standard practice for BERT-based\nmodels [2], each token sequence is prepended by the [CLS]\ntoken (used for classiﬁcation) and appended by the [SEP] token\n(marks the end of an input sequence for one or more input\nsequences), making a total of 512 tokens, the maximum input\nlength for BERT. As BERT is already preconﬁgured for a wide\nrange of tasks including sequence classiﬁcation [2], we use the\nstandard sequence classiﬁcation setup where the output of the\n[CLS] token is then fed into an intermediate dense layer and\na ﬁnal classiﬁcation layer. For single-label classiﬁcation, the\noutput logits from the classiﬁcation layer are fed into a softmax\nactivation, whereas for multilabel classiﬁcation, the logits are\nfed into a sigmoid activation.\nWe note that this strategy may discard a signiﬁcant portion of\ncontent for each document that may be useful for classiﬁcation;\ntherefore, we expect that this strategy may perform poorly due\nFig. 1. Process for splitting long documents into smaller chunks to feed\ninto BERT and methods for combining the resulting BERT outputs from\neach chunk into a single classiﬁcation decision.\nto information loss. However, we include this strategy as it is\nuseful to establish a baseline.\n2) Max Pool Over Logits:In order to capture the content from\nthe entire document, we utilize a hierarchical approach in which\nwe split long documents into smaller chunks and then process\neach chunk individually using BERT. After converting an input\ndocument into WordPiece tokens, we split the document into\nk segments of 510 tokens each. Each segment is prepended by\nthe [CLS] token and appended by the [SEP] token so that it is\n512 in length. We then utilize the standard BERT classiﬁcation\nsetup on each of thek segments, wherein the ﬁrst [CLS] token\nin each segment is passed to an intermediate dense layer and a\nﬁnal classiﬁcation layer. This generatesk logit vectors, one for\neach segment.\nPrior to the softmax or sigmoid activation function, we apply\na max pool operation across allk logits to reduce them into a\nsingle logit vector – this max pooled logit vector represents the\nmaximum logit value for each possible class across each of thek\nsegments. For single-label classiﬁcation, this ﬁnal max pooled\nlogit vector is passed to a softmax activation to predict class\nprobabilities, and for multilabel classiﬁcation it is passed to a\nsigmoid activation.\nWe note that max pooling is performed on the logit vector\nbecause the size of the logit vector is always equal to the number\nof possible classes and a higher logit value for a given class will\nalways indicate that particular class is more likely to be present.\nThis cannot be said about any other intermediate representation\ngenerated by BERT, where a large negative value may be just as\nimportant as a large positive value in identifying the presence of\na particular class. Thus, applying max pool to the logit vector\nminimizes unintentional information loss.\n3) T arget Attention:Similar to max pool over logits, we split\nthe document intok segments of 510 WordPiece tokens each.\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3599\nEach segment is prepended by the [CLS] token and appended\nby the [SEP] token so that it is 512 in length. We then utilize\nthe BERT model without the classiﬁcation setup such that for\neach of thek segments, we simply generate 512 new contextual\ntoken embeddings. From this, we drop the ﬁrst [CLS] and last\n[SEP] token embeddings from each of thek sequences, then\nconcatenate the k embedding sequences to form E ∈ Rl×d,\nwhere l is the total length of the document anddis the embedding\ndimension conﬁgured within BERT (768 in our case).\nNext, we utilize an attention mechanism to identify the token\nembeddings within E that are most relevant to the target task.\nTo do this, we utilize a modiﬁed version of scaled dot product\nattention [36], which is shown in 1:\nK = EW k + bk\nV = EW v + bv\nTarget-Attention(E,T )= softmax\n(TK ⊤\n√\nd\n)\nV (1)\nwhere Wk ∈ Rd×d and Wv ∈ Rd×d are learnable weight ma-\ntrices and bk ∈ Rd and bk ∈ Rd are learnable bias vectors.\nK ∈ Rl×d and V ∈ Rl×d are simple linear transformations of\nE. Finally, T ∈ R1×d is a randomly initialized vector that is\nlearned through training – this vector represents the information\nto look for given the current task.\nEssentially, our target attention operation compares each\ntoken embedding in E to the target vector T to identify the\nembeddings most relevant to the current task. The output of our\ntarget attention mechanism isD ∈ R1×d, the ﬁnal document em-\nbedding used for classiﬁcation, which is effectively a weighted\naverage of the most important embeddings fromE. We passD\nto a ﬁnal dense classiﬁcation layer; as the previous strategies, the\noutput logits from the classiﬁcation layer are fed into a softmax\nactivation for single-label classiﬁcation and a sigmoid activation\nfor multilabel classiﬁcation.\n4) Multilabel Attention: In the multilabel classiﬁcation set-\nting, we expand our target attention mechanism so that we use\na separate parallel attention mechanism for each possible label.\nThis increases the expressivity of the attention mechanism so\nthat the same attention target vector does not need to capture\ninformation for hundreds or thousands of possible labels.\nOnce again, we split the document intok segments of 510\nWordPiece tokens each. Each segment is prepended by the [CLS]\ntoken and appended by the [SEP] token so that it is 512 in length.\nWe use the same procedure from target attention to generateE ∈\nRl×d, which represents the contextual embeddings generated by\nBERT for the all tokens in the document. We then passE to a\nmodiﬁed version of scaled dot product attention, shown in 2:\nK = EW k + bk\nV = EW v + bv\nMultilabel-Attention(E,M )= softmax\n(MK ⊤\n√\nd\n)\nV\nLogits =( Multilabel-Attention(E,M )Wc)⊤ + bc (2)\nwhere Wk ∈ Rd×i and Wv ∈ Rd×i are learnable weight ma-\ntrices and bk ∈ Ri and bk ∈ Ri are learnable bias vectors.\nK ∈ Rl×i and V ∈ Rl×i are simple linear transformations of\nE. Unlike in target attention where the embedding dimension\nof K and V are set tod, the same asE, in multilabel attention\nthey are reduced to an intermediate dimensioni as we found this\nreduces overﬁtting. M ∈ Rc×i is a randomly initialized matrix\nthat is learned through training, wherec is the number of possible\nclasses – each row of this matrix represents the most important\ninformation for one class.\nWhile in target attention each embedding inE is compared\nto a single target vector to determine its relevance, in multilabel\nattention each embedding inE is simultaneously compared to a\ndifferent vector for each possible class to determine its relevance\nfor that class. The output of multilabel attention is a matrixO ∈\nRc×i, which we pass to a dense layer with weightsWc ∈ Ri×1\nand biasbc ∈ Rc to generate the logits. Because we only utilize\nmultilabel attention in the multilabel classiﬁcation setting, we\npass the output logits to a sigmoid activation to obtain the ﬁnal\nclass probabilities.\nB. Baseline Models\n1) Convolutional Neural Network:Our ﬁrst strong baseline is\na shallow word-level CNN based off [37]. Although a relatively\nsimple architecture that was originally developed in 2014, it is\nstill widely used for biomedical and clinical text classiﬁcation\nand has shown strong performance across a variety of tasks [10],\n[38]–[40]. For our CNN implementation, we represent each\ndocument using word level embeddings, which are passed to\nthree parallel 1D convolution layers; these examine three, four,\nand ﬁve consecutive words at a time to identify n-grams rele-\nvant to the given task. The outputs from the three convolution\nlayers are concatenated and passed to a max pool operation\nthat generates a ﬁxed-length document vector composed of\nthe most important n-grams in the document. This document\nvector is passed to a ﬁnal dense classiﬁcation layer that uses\nsoftmax for single-label classiﬁcation and sigmoid for multilabel\nclassiﬁcation.\nIn multilabel classiﬁcation settings, we also test a multilabel\nvariant of the CNN, which we refer to as CNN-multilabel (CNN-\nML). In this variant, after the outputs from the three convolution\nlayers are concatenated, instead of using a max pool operation,\nwe feed the output to the same multilabel attention setup that we\nuse for BERT.\n2) Hierarchical Self-Attention Network: Our second strong\nbaseline is the HiSAN network [11], which to our knowledge\nis the current state-of-the-art in classifying cancer pathology re-\nports. Like BERT, this architecture is also based off self-attention\noperations, but it is far simpler and has approximately 100x\nfewer learnable parameters. We use the exact same implementa-\ntion as [11] – ﬁrst, each document is represented using word level\nembeddings and then broken into chunks of ten words each. The\nHiSAN’s lower hierarchy uses a series of attention-based oper-\nations to generate a ﬁxed-length vector representation for each\nten-word chunk. These representations are then passed to the\n3600 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\nT ABLE I\nDA T ASETDESCRIPTIONS FOR EACH TASK.W E NOTE THA TDOCUMENT LENGTHS ARE MEASURED USING GENERIC WORD TOKENS RAT H E RTHAN BERT’S\nWORDPIECE TOKENS.C ONVERTING TO WORDPIECE TOKENS RESULTS INAPPROXIMA TEL Y25% MORE TOKENS\nHiSAN’s upper hierarchy, which uses another series of attention-\nbased operations to generate a ﬁxed-length vector representation\nof the entire document. This document vector is passed to a\nﬁnal dense classiﬁcation layer that uses softmax for single-label\nclassiﬁcation and sigmoid for multilabel classiﬁcation.\nLike with the CNN, in the multilabel classiﬁcation setting\nwe test a multilabel variant of the HiSAN, which we refer to\nas HiSAN-multilabel (HiSAN-ML). In this variant, we replace\ntarget attention mechanism in the HiSAN’s upper hierarchy with\nthe same multilabel attention setup that we use for BERT.\nC. Datasets\n1) MIMIC-III Discharge Summaries: The MIMIC-III dataset\nconsists of unstructured clinical notes as well as structured tables\nrelated to 49 785 distinct hospital admissions of 38 597 unique\nadult patients who stayed in the intensive care unit at Beth Israel\nDeaconess Medical Center between 2001 and 2012 [5]. Each\nunique admission is annotated by human experts with a set of\nICD-9 codes that describe the diagnoses and procedures that\noccurred during that particular stay. Each unique admission is\nalso associated with a discharge summary which summarizes\nthe information from the stay in a single document. For this\nstudy, we utilize the discharge summaries for three multilabel\nclassiﬁcation tasks – (1) predict the set of 5-character diagnoses\ncodes (DX 5-Char) associated with each discharge summary,\n(2) predict the set of unique 3-character (DX 3-Char) diagnoses\ncategories associated with each discharge summary, which con-\nsists of the ﬁrst three characters of the full 5-character diagnosis\ncode, and (3) predict the set of procedure codes associated with\neach discharge summary.\nWe note that some admissions have one or more addenda\nin addition to the discharge summary; in these situations we\nconcatenate the information from the addenda to the discharge\nsummary. Following [31], we perform train/val/test splitting\nbased off unique patient IDs so that the same patient does not\nappear in multiple splits. Statistics regarding this dataset are\navailable inTable I.\n2) SEER Cancer Pathology Reports: The National Cancer\nInstitute (NCI) Surveillance, Epidemiology, and End Results\n(SEER) program works with cancer registries across the United\nStates to collect and maintain cancer data in order to support\nnational cancer surveillance. We obtained 1 201 432 cancer\npathology reports from the Louisiana, Kentucky, New Jersey,\nand Utah SEER cancer registries. Each cancer pathology report\nis associated with a unique tumor ID; one or more cancer\npathology reports may be associated with the same tumor ID.\nFor each tumor ID, certiﬁed tumor registrars (CTRs) manually\nassigned ground truth labels for key data elements – including\ncancer site, subsite, laterality, behavior, histology, and grade;\nfor a given tumor ID, labels were assigned based off all data\navailable for that tumor ID. Because our ground truth labels are\nat the tumor level rather than the report level, there are cases\nwhere tumor IDs associated with multiple pathology reports\nhave labels which do not match the content within one or more\nof the individual pathology reports. Therefore, in this study\nwe only utilize tumor IDs associated with a single pathology\nreport, yielding a total of 200 352 pathology reports. We utilize\nthis dataset to perform six single-label document classiﬁcation\ntasks, one for each manually annotated data element. Statistics\nregarding this dataset are available inTable I.\nIV . EXPERIMENTS\nA. Evaluation Metrics\nFor multilabel classiﬁcation tasks on the MIMIC-III dataset,\nwe follow established metrics from previous work [30]–[32]\nand measure performance using precision, recall, and F1 score,\nwhere each possible text-code pair is treated as an independent\nprediction:\nPrecision = TruePositive\nTruePositives + FalsePositives (3)\nRecall = TruePositives\nTruePositives + FalseNegatives (4)\nF1 =2 ∗ Precision ∗Recall\nPrecision + Recall (5)\nSimilarly, for single-label classiﬁcation tasks on the cancer\npathology reports, we follow established metrics from previous\nwork [10], [11], [33], [41] and measure performance using\nclassiﬁcation accuracy and macro F1 score, in which the F1\nscore is calculated for each possible class label and then averaged\nacross all class labels:\nMacro F1= 1\n|C|\nC∑\nCi\nF1(Ci) (6)\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3601\nwhere Ci represents the subset of training samples belonging\nto class i, and |C| is the total number of possible classes.\nBecause of the extreme class imbalance inherent in the cancer\npathology report dataset, macro F1 score better captures model\nperformance on minority classes.\nFor all metrics, we bootstrap samples from our test set using a\nprocedure described in Appendix A to generate 95% conﬁdence\nintervals. Since computation speed may also be a consideration\nin some applications, we report the average inference time for\n1000 documents for each method on the MIMIC-III dataset\nutilizing a single Tesla V100 GPU.\nB. Dataset Cleaning\nFor both datasets, we lowercase all text, clean hex and\nunicode symbols, replace decimal values and integers larger\nthan 100, and clean up any deidentiﬁcation tokens; a more\ndetailed description is available in Appendix B. For BERT-\nbased approaches, we utilize the HuggingFace BERT tokenizer.1\nwith the vocabulary associated with the pretrained BlueBERT\nmodel2 For the CNN and HiSAN, we train 300-dimensional\nword2vec embeddings on each dataset, replacing unique words\nappearing in fewer than ﬁve documents in each dataset with an\n“unknown_word” token.\nC. Hyperparameter Optimization\nFor all BERT-based approaches, we start from pretrained\nweights from BlueBERT Base1 and implement all models using\nthe Huggingface library2. For the max pool over logits, target\nattention, and multilabel attention methods, we limit the number\nof segments per documentk to a maximum of 10; we note thatk\nis not a tuned hyperparameter but instead determined based off\nthe average length of our documents and the memory capacity\nof our Tesla V100 GPUs.\nHyperparameters for all approaches are optimized using the\nvalidation set of each dataset. Due to the high computational\ncost of some of our models, we use a hill-climbing strategy in\nwhich we change a single hyperparameter at a time and then\nretrain until model performance stops improving. We choose\nthe set of hyperparameters with the overall highest performance\nacross all tasks (average F1 for MIMIC and average accuracy\nfor pathology reports). We list the range of hyperparameters\nexplored as well as the optimal hyperparameters inTable II.\nD. Results\nFig. 2shows the results of our experiments on the MIMIC-II\ndataset dataset. First, we examine the performance of each model\nwhen limited to only the ﬁrst 510 WordPiece tokens. We note\nthat for models such as the CNN and HiSAN that use word\ntoken inputs, we convert the ﬁrst 510 WordPiece tokens back into\nword tokens which results in approximately 400 word tokens\nfor each document. We use this ﬁrst set of results to address two\nkey questions: (1) how well does each method perform when\nusing only the ﬁrst 510 WordPiece tokens compared to the full\n1https://huggingface.co/transformers/index.html\n2https://github.com/ncbi-nlp/bluebert\nT ABLE II\nHYPERP ARAMETERSEXPLORED FOR EACH MODEL.O PTIMAL\nHYPERP ARAMETERS AREMARKED WITH A * FOR THE MIMIC III TASKS AND\nAˆFOR THE PA THOLOGYREPORT TASKS\ndocument and (2) how well does BlueBERT compare to our\nstrong baselines when adaptive methods to ﬁt longer documents\nisn’t a performance factor?\nOur results inFig. 2 suggest that even if we limit all mod-\nels to short text segments that ﬁt within BERT’s default 512\nWordPiece input limit, BERT does not outperform our much\nsimpler baselines in two of the three tasks. The CNN model\nconsistently achieves the best precision scores by a wide margin\non all tasks. We expect that this because the CNN is designed to\nmemorize the 3-, 4-, and 5-gram word combinations associated\nwith each label as opposed to learning more complex sequential\npatterns; this limits the ability of the CNN to generalize beyond\nthe n-gram patterns it knows, but makes it very precise when\nit does encounter a previously seen n-gram. The HiSAN and\nBERT models can both learn more complex patterns than the\nCNN and achieve better recall than the CNN on all tasks at the\ncost of precision. The HiSAN model achieves the best recall and\nF1 scores on the diagnosis category and full code tasks, whereas\nBERT achieves the best recall and F1 score on the procedure\ntask.\nSecond, we examine the performance of each model using\nfull documents from the MIMIC-III dataset. We notice that\ncompared to using only the ﬁrst 510 WordPiece tokens, using\nthe full document results in signiﬁcantly improved performance\nacross all metrics. This makes intuitive sense, as on average, the\nﬁrst 510 WordPiece tokens captures approximately only the ﬁrst\n25% of each document and critical information may be located\nin the remainder of the document.\nWhen using full documents on the MIMIC-III dataset tasks,\nour BERT-based approaches do not signiﬁcantly outperform our\nmuch simpler baselines on any tasks. Once again, the CNN\nmodel consistently achieves the best precision scores by a wide\nmargin on all tasks. The HiSAN-based approaches achieve\nthe best recall and F1 scores on most tasks; while the BERT\nmultilabel attention approach achieves the best recall score on\nthe diagnostic category task, it is not signiﬁcantly better than\nthat of the HiSAN model.\n3602 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\nFig. 2. Precision, recall, and F1 scores for each model on the MIMIC-III dataset. 95% conﬁdence intervals are shown in red and calculated using\na bootstrapping procedure detailed in Appendix A.\nFig. 3. Accuracy (left) and macro F1 scores (right) for each model on the cancer pathology report dataset. 95% conﬁdence intervals are shown in\nred and calculated using a bootstrapping procedure detailed in Appendix A.\nWhen comparing the different methods for adapting BERT to\nlonger text documents, the max over logits method consistently\noutperforms the target attention method in all tasks and metrics\nexcept for precision score in the diagnostic code task. Interest-\ningly, using multilabel attention has mixed effects based on both\ntask and model. For the CNN and BERT models, multilabel\nattention increases recall at the cost of precision, whereas for\nthe HiSAN model it increases precision at the cost of recall.\nFor all models, multilabel attention appears to help most in the\ndiagnostic category and code tasks while having mixed results\nin the procedure task.\nFigure 3shows the results of our experiments on the cancer\npathology reports dataset. After taking into account conﬁdence\nintervals, BERT does not achieve statistically better accuracy or\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3603\nmacro F1 scores than the HiSAN on any of the six tasks. Similar\nto our results from the MIMIC-III dataset, the max over logits\napproach almost always performs better than the target attention\napproach on all tasks and metrics.\nFinally, Table IIIshows the average time in seconds to predict\non 1000 full documents from the MIMIC-III dataset. We see that\nthe BERT-based approaches are almost an order of magnitude\nslower than the base CNN and the HiSAN-based models. While\ninference time may not be the most critical factor for institutions\nthat only need to perform a single prediction pass on their data,\nit may be important for institutions that have millions of docu-\nments or need to regularly retrain their models on incoming data.\nV. DISCUSSION\nOur experiments show that BERT generally does not achieve\nthe best performance on our clinical text classiﬁcation tasks\ncompared to the much simpler CNN and HiSAN models. In this\nsection, we provide evidence for two potential explanations for\nthe weak performance of BERT – attention dilution and difﬁculty\nof subword tokens.\nFirst, one of the key components of BERT’s previous suc-\ncess is the masked-language modelling pretraining process, in\nwhich the BERT model may learn subtle and complex word\nrelationships between all possible words in a large unlabelled\ntext corpus. However, in clinical text classiﬁcation tasks on\ndocuments in which only very few words contribute toward a\nspeciﬁc label, most of these subtle word relationships may not be\nnecessary or even relevant to the task at hand. Therefore, BERT’s\nattention may actually be diluted away from the keywords most\ncritical to the task.\nTo demonstrate this phenomena, we generated three different\ntypes of attention visualizations. First, we multiplied the atten-\ntion weights through both hierarchies of the HiSAN to show\nexactly which words the HiSAN focuses on in each document\n(ﬁrst 510 WordPiece tokens only). Second, using our ﬁne-tuned\nBlueBERT model (ﬁrst 510 WordPiece tokens only), we visu-\nalized the attention weights from the very ﬁnal layer that are\nassociated with the [CLS] token used for classiﬁcation; these\nweights represent the most important subword tokens after they\nhave already incorporated contextual information from other\nsubword tokens based off the 12 self-attention layers of the main\nBERT model. Third, using our ﬁne-tuned BlueBERT model\n(ﬁrst 510 WordPiece tokens only), we started from the attention\nweights from the very ﬁnal layer that are associated with the\n[CLS] token and multiplied these attention weights through\nall 12 self-attention layers of the BERT model; these weights\nrepresent the most important subword tokens accounting for\nall the inter-word relationships captured during pretraining and\nﬁne-tuning. We provide an example of these visualizations in\nAppendix C.\nAfter examining these attention weights over a large number\nof documents, we noticed that in general, (1) the attention\nweights in the ﬁnal layer of BERT are more spread out and\nless focused on speciﬁc biomedical keywords than the attention\nweights from the HiSAN, and (2) the attention weights when\naccounting for all layers of BERT are even more diluted than\nT ABLE III\nAVERAGE TIME (IN SECONDS) TO PREDICT ON 1000 FULL DOCUMENTS\nFROM THE MIMIC-III DA T ASET.A LL TIMING IS PERFORMED ON ADGX\nMACHINE USING A SINGLE V100 GPU\nthose from the ﬁnal layer of BERT. While there is usually some\noverlap in the attention weights of the HiSAN and BERT, we\nfound that in a notable number of cases BERT places emphasis\non less relevant tokens such as punctuation and [SEP]. These\nvisualizations suggest that BERT’s attention is diverted toward\nword relationships learned during pretraining as opposed to the\nspeciﬁc keywords relevant to the downstream classiﬁcation task.\nSecond, while the HiSAN and CNN models utilize word-level\ntokens as input, BERT uses a WordPiece tokenizer that splits\neach word into one or more subword tokens. Whereas with\nword level tokens, the HiSAN and CNN can directly memorize\nkeywords or keyphrases important to each label, there is an\nadded layer of complexity with WordPiece tokens in that impor-\ntant keywords may be broken into multiple wordpiece tokens.\nThus, critical keywords or keyphrases will always be longer\nwhen represented as WordPiece tokens compared to word-level\nembeddings, thereby increasing the complexity of the token\ncombinations that a model must learn to recognize a particular\nlabel.\nTo test this hypothesis, we retrained the CNN and HiSAN\nmodels on the MIMIC-III dataset using the ﬁrst 512 subword\ntokens generated by the ﬁnal layer of the BlueBERT model\n(without any ﬁne-tuning on the MIMIC-III dataset) instead of\nusing word-level Word2Vec embeddings. Our results are shown\nin Fig. 4. We see that compared to using word-level tokens\nas input, both the CNN and HiSAN trained on subword token\ninputs perform worse in overall F1 score across all three tasks.\nThese results suggest that overall, it may be more difﬁcult to\nuse subword-level tokens for our MIMIC-III classiﬁcation tasks\nthan it is to use word-level tokens.\nFinally, we examined differences in the vocabulary and tok-\nenization setups between BERT and our baseline models as a\nsource of performance discrepancy. In our main experiments,\nwe used word embeddings trained directly on the target cor-\npus for our baseline models, eliminating word tokens appear-\ning fewer than ﬁve times, whereas for BlueBERT we tok-\nenized using the associated WordPiece vocabulary pretrained on\nPubmed abstracts and MIMIC-III. Therefore, we tested (1) the\nperformance of our baseline CNN and HiSAN when utilizing\npublicly available word embeddings pretrained on Pubmed and\nMIMIC-III [42], and (2) the performance of BlueBERT when\neliminating rare words appearing fewer than ﬁve times in the\n3604 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\nFig. 4. F1 scores for alternative vocabulary/tokenization setups on the\nMIMIC-III dataset (ﬁrst 510 WordPiece tokens only). 95% conﬁdence in-\ntervals are shown in red and calculated using a bootstrapping procedure\ndetailed in Appendix A.\ntarget corpus (mirroring the original tokenization process for\nthe CNN and HiSAN). The results of these experiments on the\nMIMIC-III dataset (ﬁrst 510 Wordpiece tokens) are shown in\nFig. 4. Using pretrained word vectors generally results in slightly\nworse F1 scores for the CNN and HiSAN, but both models\nstill outperformed BERT in the same two out of three tasks.\nEliminating rare tokens reduced BERT’s F1 score in two of the\nthree tasks, and in all three tasks the F1 score was worse than that\nof the HiSAN. Ultimately, we see that BlueBERT still does not\nconsistently outperform the CNN and HiSAN baselines under\nany of these alternative tokenization setups.\nVI. CONCLUSION\nIn this work, we compared four methods for adapting BERT,\nwhich by default can only take inputs of up to 510 WordPiece\nsubword tokens, to sequence classiﬁcation on long clinical texts\nup to several subword tokens in length; these methods include\nusing only the ﬁrst 510 WordPiece tokens, hierarchical max\npool over logits, hierarchical target attention, and hierarchical\nmultilabel attention. We compare these methods against two\nstrong baselines, the CNN and the HiSAN. We evaluted these\nmodels on two datasets. The MIMIC-III clinical notes dataset\nhas three multilabel classiﬁcation tasks: diagnosis codes, diag-\nnosis categories, and procedure codes; and the cancer pathology\nreports dataset has six single label classiﬁcation tasks: site,\nsubsite, laterality, histology, behavior, and grade.\nOur results showed that on most datasets and tasks, the BERT-\nbased methods performed equal to or worse than the simpler\nHiSAN baseline, and in some cases BERT performed equal to\nor worse than even the CNN. On the MIMIC-III dataset, when\nall models and baselines were limited to the ﬁrst 510 WordPiece\ntokens of each document only, BERT outperformed in only the\nrecall metric for the procedure code task. Once we utilized full\nlength documents, BERT outperformed on only the recall metric\nfor the diagnostic category task. On the cancer pathology report\ndataset, BERT was not statistically better than the HiSAN on any\nof the six tasks. Within the four different methods for adapting\nBERT to classiﬁcation on long texts, hierarchical multilabel\nattention had the overall strongest performance on multilabel\nclassiﬁcation and hierarchical max pool over logits had the\noverall strongest performance on single label classiﬁcation.\nIn our analysis, we presented evidence for two possible rea-\nsons why BERT underperforms in clinical text classiﬁcation\ntasks. First, our tasks generally have a low signal-to-noise ratio,\nin that the presence of a few keywords may be enough to\nindicate a particular label. In BERT’s pretraining process, BERT\nlearns complex and nuanced relations between all words in the\npretraining corpus; however, many of these relationships may\nbe irrelevant for the classiﬁcation task and may actually divert\nattention away from the critical keywords. Second, BERT’s\nWordPiece tokenizer breaks each word token into one or more\nsubword tokens. This increases the complexity of the classiﬁ-\ncation task, as now the model must learn to associate a larger\nnumber of subword tokens to each label compared to a lower\nnumber of word-level tokens.\nOur results suggest that a pretrained BERT model such as\nBioBERT or BlueBERT may not be the best choice for clinical\ntext classiﬁcation tasks, and a simple CNN or HiSAN model may\nachieve comparable or better accuracy/F1. However, recent work\nmay address some of BERT’s limitations that we illustrated. For\nexample, [43] utilizes a novel pretraining technique that forces\nBERT to focus on learning knowledge about entities rather than\nlearning generic syntax and grammar patterns; this may lead\nto better performance on downstream clinical and biomedical\nclassiﬁcation tasks which are often knowledge-oriented. Addi-\ntionally, [8], [9] adapt BERT for long texts without requiring hi-\nerarchical splitting methods, which may allow the model to learn\nuseful patterns over longer distances. Lastly, recent work [44]\nshows that a signiﬁcant weakness of BioBERT and BlueBERT\nis that they utilize the original WordPiece vocabulary from\nBERT, generated from Wikipedia and BooksCorpus; building\nthe WordPiece vocabulary directly on the domain of interest\nprevents important keywords from being split into multiple\nsubtokens and leads to higher accuracy in downstream tasks.\nUnfortunately, these approaches have yet to be pretrained on\nclinical corpora and then released for public use, and thus we\nleave further evaluation of these methods for future work. The\ncode used for the experiments in our paper will be made available\nonline after peer review.\nAPPENDIX\nA. Bootstrapping Conﬁdence Intervals\n1) For each model/task, save the model’s predictions on the\ntest set (hereon referred to as the original predictions)\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3605\n2) Randomly select samples from the test set along with\ntheir predicted labels (with replacement) to create a new\nset of samples and predicted labels of the same size as the\noriginal test set (hereon referred to as bootstrapped set)\n3) For cancer pathology reports, calculate accuracy and\nmacro F1 score on bootstrapped set; for MIMIC-III,\ncalculate precision, recall, and F1 score on bootstrapped\nset\n4) Repeat steps (2) and (3) 1000 times, saving the scores\neach time\n5) Calculate the 95% conﬁdence interval for each metric by\nﬁnding the 2.5 and 97.5 percentile entry for that metric\nwithin the 1000 runs (since precision, recall, and F1 score\nare not normally distributed)\nB. Data Preprocessing\n1) Replace hex and unicode characters with their string\nequivalents, removing any corrupted codes\n2) For pathology reports, remove identiﬁer segments (reg-\nistry ID, patient ID, document ID, etc) and XML tags\n3) For MIMIC-III, replace all deidentiﬁer tokens (e.g.,\n[**NAME**]) with the string “deindentiﬁed”\n4) Lowercase\n5) Replace all instances of decimal values with the string\n“ﬂoattoken”\n6) Replace all integers higher than 100 with the string\n“largeinttoken”\n7) Replace all nonalphanerics other than {. ? !, # :; () % / -\n+ = } with a spacesss\n8) If the same non-alphanumeric character repeats consecu-\ntively, replace it with a single copy of that character\n9) Add a space before and after every non-alphanumeric\ncharacter\nC. Attention Visualizations\nFig. A1. Attention weights and predictions on an example document from the MIMIC-III dataset for the diagnostic category task. In this ﬁgure, we\nmultiply the attention weights through both hierarchies of the HiSAN and show exactly which words the HiSAN focuses on in each document (ﬁrst\n510 WordPiece tokens only). For this visualization, we sum the attention weights across all attention heads.\n3606 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 25, NO. 9, SEPTEMBER 2021\nFig. A2. Attention weights and predictions on an example document from the MIMIC-III dataset for the diagnostic category task. In the top, using\nour ﬁne-tuned BlueBERT model (ﬁrst 510 WordPiece tokens only), we visualize the attention weights from the very ﬁnal layer that are associated\nwith the [CLS] token used for classiﬁcation; these weights represent the most important subword tokens after they have already incorporated\ncontextual information from other subword tokens based off the 12 self-attention layers of the main BERT model. In the bottom, we start from the\nattention weights from the very ﬁnal layer that are associated with the [CLS] token and multiply these attention weights through all 12 self-attention\nlayers of the BERT model; these weights represent the most important subword tokens accounting for all the inter-word relationships captured\nduring pretraining and ﬁne-tuning. For this visualization, we sum the attention weights across all attention heads.\nGAO et al.: LIMIT A TIONS OF TRANSFORMERS ON CLINICAL TEXT CLASSIFICA TION 3607\nREFERENCES\n[1] S. Wu et al., “Deep learning in clinical natural language processing:\nA methodical review,”J. Amer . Med. Informat. Assoc., vol. 27, no. 3,\npp. 457–470, 2020.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” inProc.\nNAACL-HLT: Annu. Conf. North Amer . Chapter Assoc. Comput. Linguis-\ntics, 2019, pp. 4171–4186.\n[3] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro,\n“Megatron-LM: Training multi-billion parameter language models using\nmodel parallelism,” 2019,arXiv:1909.08053.\n[4] R. Collobert and J. Weston, “A uniﬁed architecture for natural language\nprocessing: Deep neural networks with multitask learning,” inProc. 25th\nInt. Conf. Mach. Learn., 2008, pp. 160–167.\n[5] A. E. Johnsonet al., “Mimic-III, a freely accessible critical care database,”\nScientiﬁc Data, vol. 3, no. 1, p. 19, 2016.\n[6] J. Lee et al., “BioBert: A pre-trained biomedical language representa-\ntion model for biomedical text mining,”Bioinformatics, vol. 36, no. 4,\npp. 1234–1240, 2019.\n[7] Y . Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical natural\nlanguage processing: An evaluation of BERT and ELMO on ten bench-\nmarking datasets,” inProc. 18th Biomed. Natural Lang. Process. Workshop\nShared Task, 2019, pp. 58–65.\n[8] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efﬁcient trans-\nformer,” inProc. ICLR : 8th Int. Conf. Learn. Representations, 2020.\n[9] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document\ntransformer,” 2020,arXiv:2004.05150.\n[10] J. X. Qiu, H.-J. Yoon, P. A. Fearn, and G. D. Tourassi, “Deep learn-\ning for automated extraction of primary sites from cancer pathology\nreports,” IEEE J. Biomed. Health Informat., vol. 22, no. 1, pp. 244–251,\n2018.\n[11] S. Gaoet al., “Classifying cancer pathology reports with hierarchical self-\nattention networks,”Artif. Intell. Med., vol. 101, 2019, Art no. 101726.\n[12] A. Talmor and J. Berant, “MultiQA: An empirical investigation of gen-\neralization and transfer in reading comprehension,” inProc. ACL : 57th\nAnnu. Meeting Assoc. Comput. Linguistics, 2019, pp. 4911–4921.\n[13] C. Alberti, K. Lee, and M. Collins, “A BERT baseline for the natural\nquestions,” 2019,arXiv:1901.08634.\n[14] F. Petroni et al., “Language models as knowledge bases,” inProc. Conf.\nEmpirical Methods Natural Lang. Process., 2019, pp. 2463–2473.\n[15] L. B. Soares, N. FitzGerald, J. Ling, and T. Kwiatkowski, “Matching the\nblanks: Distributional similarity for relation learning,” inProc. ACL : 57th\nAnnu. Meeting Assoc. Comput. Linguistics, 2019, pp. 2895–2905.\n[16] Y . Liu and M. Lapata, “Text summarization with pretrained encoders,” in\nProc. Conf. Empirical Methods Natural Lang. Process., 2019, pp. 3728–\n3738.\n[17] H. Zhang, J. Cai, J. Xu, and J. Wang, “Pretraining-based natural language\ngeneration for text summarization,” inProc. 23rd Conf. Comput. Nat. Lang.\nLearn., 2019, pp. 789–797.\n[18] C. Sun, X. Qiu, Y . Xu, and X. Huang, “How to ﬁne-tune BERT for text\nclassiﬁcation?,” in Proc. China Nat. Conf. Chin. Comput. Linguistics,\n2019, pp. 194–206.\n[19] A. Adhikari, A. Ram, R. Tang, and J. Lin, “DocBERT: BERT for document\nclassiﬁcation,” 2019,arXiv:1904.08398.\n[20] N. Reimers, B. Schiller, T. Beck, J. Daxenberger, C. Stab, and I. Gurevych,\n“Classiﬁcation and clustering of arguments with contextualized word em-\nbeddings,” inProc. ACL : 57th Annu. Meeting Assoc. Comput. Linguistics,\n2019, pp. 567–578.\n[21] M. Ostendorff, P. Bourgonje, M. Berger, J. M. Schneider, and G. Rehm,\n“Enriching BERT with knowledge graph embeddings for document clas-\nsiﬁcation,” inProc. GermEval 2019 Workshop, Erlangen, Germany, 2019,\npp. 307–314.\n[22] E. Alsentzer et al., “Publicly available clinical BERT embeddings,”\nin Proc. 2nd Clinical Natural Language Processing Workshop, 2019,\npp. 72–78.\n[23] S. Martina, L. Ventura, and P. Frasconi, “Classiﬁcation of cancer pathology\nreports: A large-scale comparative study,”IEEE J. Biomed. Health Inf.,\nvol. 24, no. 11, pp. 3085–3094, 2020.\n[24] S. Sheikhalishahi, R. Miotto, J. T. Dudley, A. Lavelli, F. Rinaldi, and\nV . Osmani, “Natural language processing of clinical notes on chronic\ndiseases: Systematic review,”JMIR Med. Inform., vol. 7, no. 2, 2019, Art\nno. e12239.\n[25] J. R. A. Solareset al., “Deep learning for electronic health records: A.\ncomparative review of multiple deep neural architectures,”J. Biomed.\nInform., vol. 101, 2020, Art no. 103337.\n[26] J. Leeet al., “Automated extraction of biomarker information from pathol-\nogy reports,” BMC Med. Inform. Decis. Mak., vol. 18, no. 1, pp. 1–11,\n2018.\n[27] F. Xie, J. Lee, C. E. Munoz-Plaza, E. E. Hahn, and W. Chen, “Application of\ntext information extraction system for real-time cancer case identiﬁcation\nin an integrated healthcare organization,”J. Pathol. Inform., vol. 8, no. 1,\npp. 48–63, 2017.\n[28] A. Yalaet al., “Using machine learning to parse breast pathology reports,”\nBreast Cancer Res. Treat., vol. 161, no. 2, pp. 203–211, 2017.\n[29] W.-W. Yim, T. Denman, S. W. Kwan, and M. Yetisgen, “Tumor information\nextraction in radiology reports for hepatocellular carcinoma patients,”\nAMIA Summits Transl. Sci. Proc., vol. 2016, pp. 455–464, 2016.\n[30] S. Gehrmann et al., “Comparing deep learning and concept extraction\nbased methods for patient phenotyping from clinical narratives,”PLoS\nOne, vol. 13, no. 2, 2018, Art no. e0192360.\n[31] J. Mullenbach, S. Wiegreffe, J. Duke, J. Sun, and J. Eisenstein, “Explain-\nable prediction of medical codes from clinical text,” inProc. NAACL HLT:\n16th Annu. Conf. North Amer . Chapter Assoc. Comput. Linguistics: Hum.\nLang. Technol., vol. 1, 2018, pp. 1101–1111.\n[32] T. Baumel, J. Nassour-Kassis, R. Cohen, M. Elhadad, and N. Elhadad,\n“Multi-label classiﬁcation of patient notes a case study on ICD code\nassignment,” inProc. AAAI Workshops, 2017, pp. 409–416.\n[33] S. Gao et al., “Hierarchical attention networks for information extraction\nfrom cancer pathology reports,”J. Amer . Med. Inform. Assoc., vol. 25,\nno. 3, pp. 321–330, 2018.\n[34] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained\nmodels for natural language processing: A survey,”Sci. China Technol.\nSci., vol. 63, no. 10, pp. 1872–1897, 2020.\n[35] Q. Liu, M. J. Kusner, and P. Blunsom, “A survey on contextual embed-\ndings,” 2020,arXiv:2003.07278.\n[36] A. Vaswani et al., “Attention is all you need,” inProc. 31st Int. Conf.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[37] Y . Kim, “Convolutional neural networks for sentence classiﬁcation,” in\nProc. Conf. Empirical Methods Natural Lang. Process., 2014, pp. 1746–\n1751.\n[38] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep\nlearning based natural language processing,”IEEE Comput. Int. Mag.,\nvol. 13, no. 3, pp. 55–75, 2018.\n[39] C. Xiao, E. Choi, and J. Sun, “Opportunities and challenges in developing\ndeep learning models using electronic health records data: A systematic\nreview,” J. Amer . Med. Inform. Assoc., vol. 25, no. 10, pp. 1419–1428,\n2018.\n[40] M. Hughes, I. Li, S. Kotoulas, and T. Suzumura, “Medical text classiﬁca-\ntion using convolutional neural networks,”Stud Health Technol. Inform.,\nvol. 235, pp. 246–250, 2017.\n[41] M. Alawad et al., “Automatic extraction of cancer registry reportable\ninformation from free-text pathology reports using multitask convolutional\nneural networks,”J. Amer . Med. Informat. Assoc., vol. 27, no. 1, pp. 89–98,\nNov. 2019.\n[42] Y . Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec, improving\nbiomedical word embeddings with subword information and MeSH,”Sci.\nData, vol. 6, no. 1, pp. 1–9, 2019.\n[43] W. Xiong, J. Du, W. Y . Wang, and V . Stoyanov, “Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language model,” inProc. ICLR:\n8th Int. Conf. Learn. Representations, 2020.\n[44] Y . Guet al., “Domain-speciﬁc language model pretraining for biomedical\nnatural language processing,” 2020,arXiv:2007.15779.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8385399580001831
    },
    {
      "name": "Natural language processing",
      "score": 0.6991719603538513
    },
    {
      "name": "Artificial intelligence",
      "score": 0.66253662109375
    },
    {
      "name": "Lexical analysis",
      "score": 0.6112770438194275
    },
    {
      "name": "Transformer",
      "score": 0.5914262533187866
    },
    {
      "name": "Encoder",
      "score": 0.5058652758598328
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4851301610469818
    },
    {
      "name": "Document classification",
      "score": 0.4628554880619049
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1289243028",
      "name": "Oak Ridge National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I75420490",
      "name": "Louisiana State University Health Sciences Center New Orleans",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I143302722",
      "name": "University of Kentucky",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801159745",
      "name": "Huntsman Cancer Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223532165",
      "name": "University of Utah",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113128",
      "name": "New Jersey State Library",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210110061",
      "name": "Information Management Services",
      "country": "US"
    }
  ]
}