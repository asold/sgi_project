{
  "title": "VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling",
  "url": "https://openalex.org/W3217059257",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287441532",
      "name": "Fu, Tsu-Jui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225536662",
      "name": "Li, Linjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan, Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2915472489",
      "name": "Lin, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3091095485",
      "name": "Wang, William Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1578702133",
      "name": "Wang Li-juan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352283912",
      "name": "Liu Zicheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3152798676",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963541336",
    "https://openalex.org/W2765716052",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2078238240",
    "https://openalex.org/W3182683290",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2963017553",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W2989322838",
    "https://openalex.org/W2954199749",
    "https://openalex.org/W3186024896",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3207042189",
    "https://openalex.org/W2962949233",
    "https://openalex.org/W3203711169",
    "https://openalex.org/W2997805943",
    "https://openalex.org/W3168294587",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2885775891",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2897439619",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W3196867718",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W3034636873",
    "https://openalex.org/W3197828817",
    "https://openalex.org/W3104862079",
    "https://openalex.org/W2526286384",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3105232955",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3174564426",
    "https://openalex.org/W3034730770",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2963532523",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W3168640669",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W3010593057",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2606982687",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3211994343",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3166304536",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3102995547",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3145807616",
    "https://openalex.org/W3122640483",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W3023441976",
    "https://openalex.org/W3204670646",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2525778437"
  ],
  "abstract": "A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. Recent studies try to mitigate this disconnection via end-to-end training. To make it computationally feasible, prior works tend to \"imagify\" video inputs, i.e., a handful of sparsely sampled frames are fed into a 2D CNN, followed by a simple mean-pooling or concatenation to obtain the overall video representations. Although achieving promising results, such simple approaches may lose temporal information that is essential for performing downstream VidL tasks. In this work, we present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. Further, unlike previous studies that found pre-training tasks on video inputs (e.g., masked frame modeling) not very effective, we design a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. Specifically, the original video frame patches are \"tokenized\" into discrete visual tokens, and the goal is to recover the original visual tokens based on the masked patches. Comprehensive analysis demonstrates the effectiveness of both explicit temporal modeling via video transformer and MVM. As a result, VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks.",
  "full_text": "VIOLET\n : End-to-End Video-Language Transformers with\nMasked Visual-token Modeling\nTsu-Jui Fu†, Linjie Li‡, Zhe Gan‡, Kevin Lin‡, William Yang Wang†, Lijuan Wang‡, Zicheng Liu‡\n†UC Santa Barbara ‡Microsoft\n{tsu-juifu, william}@cs.ucsb.edu\n{lindsey.li, zhe.gan, keli, lijuanw, zliu}@microsoft.com\nAbstract\nA great challenge in video-language (VidL) modeling lies\nin the disconnection between ﬁxed video representations ex-\ntracted from image/video understanding models and down-\nstream VidL data. Recent studies try to mitigate this discon-\nnection via end-to-end training. To make it computation-\nally feasible, prior works tend to “imagify” video inputs,\ni.e., a handful of sparsely sampled frames are fed into a\n2D CNN, followed by a simple mean-pooling or concatena-\ntion to obtain the overall video representations. Although\nachieving promising results, such simple approaches may\nlose temporal information that is essential for performing\ndownstream VidL tasks. In this work, we present VIOLET,\na fully end-to-end VIdeO-LanguagE Transformer, which\nadopts a video transformer to explicitly model the temporal\ndynamics of video inputs. Further, unlike previous studies\nthat found pre-training tasks on video inputs (e.g., masked\nframe modeling) not very effective, we design a new pre-\ntraining task, Masked Visual-token Modeling (MVM), for\nbetter video modeling. Speciﬁcally, the original video frame\npatches are “tokenized” into discrete visual tokens, and the\ngoal is to recover the original visual tokens based on the\nmasked patches. Comprehensive analysis demonstrates the\neffectiveness of both explicit temporal modeling via video\ntransformer and MVM. As a result, VIOLET achieves new\nstate-of-the-art performance on 5 video question answering\ntasks and 4 text-to-video retrieval tasks.1\n1. Introduction\nHumans are born to perceive this world from multiple\nmodalities such as vision, sound, and touch. Video, con-\ntaining multiple modalities in nature, has been used as an\nepitome to test how AI systems perceive. Video-language\n(VidL) research aims at extending this ability to convey per-\n1Code is available at https : / / github . com / tsujuifu /\npytorch_violet\nFigure 1. End-to-end VIdeO-LanguagE T ransformer ( VIO-\nLET). VIOLET performs large-scale visual-text pre-training and\ncan be applied to various video question answering and text-to-\nvideo retrieval tasks.\nception via language. Popular VidL tasks were introduced,\nsuch as text-to-video retrieval [1–5], video question answer-\ning [6–9], text-based video moment retrieval [2, 4, 10, 11],\nand video captioning [1, 3, 12, 13].\nPrevious works [5, 6, 14–17] attempt cross-modal fusion\nover dense video features and text features to tackle VidL\ntasks, but suffer from domain disconnection due to ofﬂine\nfeature extraction [16, 18]. To address this issue, Clip-\nBERT [18] proposes to “imagify” the dense video frame\ninputs. First, it adopts a sparse sampling strategy to em-\nploy only a handful of frames from the entire video for ef-\nﬁcient end-to-end training. Second, the overall video rep-\nresentations are obtained through mean-pooling a sequence\nof frame features, individually computed by a 2D Convo-\nlutional Network. Although obtaining promising results,\nthe brash mean pooling over individual frame features for-\nfeits the crucial temporal information in video. To improve\ntemporal modeling, recent works [19, 20] concatenate all\nsparse-sampled frame features in chronological order, and\ndirectly enforce VidL learning along with the text inputs.\nHowever, these methods still treat video frames as static\nimages, and rely heavily on cross-modal fusion module to\ncapture both temporal dynamics in videos and the alignment\nbetween visual and textual elements simultaneously.\nWe propose fully end-to-end VIdeO-LanguagE Trans-\narXiv:2111.12681v2  [cs.CV]  16 Apr 2022\nformer ( VIOLET) to enhance video modeling for better\nVidL modeling from two perspectives: ( i) model architec-\nture, and (ii) pre-training task design. In terms of model ar-\nchitecture, instead of naive mean pooling or concatenation\nover a sequence of individual frame features,VIOLET con-\ntains Video Swin Transformer that models video temporal\nexplicitly for VidL learning [21,22]. Since the self-attention\nover spatial-temporal locality allows modeling variable se-\nquence lengths, our video transformer support ﬂexible VidL\nlearning from both videos and static images.\nIn terms of pre-training tasks, though the direct adoption\nof Masked Language Modeling [23] has proven effective in\npre-training vision-language models, the attempt on similar\nmasked modeling on vision inputs is not as successful. For\nexample, Masked Region Modeling (MRM) [24] or Masked\nFrame Modeling (MFM) [5] aim to recover masked image\nregions or video frames. Despite of the different variants\nof MRM/MFM that model object category or distilled re-\ngion/frame features, it suffers from imperfect patch labels,\nexcessive feature dimensions, rendering unsatisfactory per-\nformance [5,24]. Recent VidL works [18,19,25] even com-\npletely discard such pre-training tasks due to limited perfor-\nmance improvements.\nTo promote better video representations for VidL learn-\ning, we present a new pre-training task: Masked Visual-\ntoken Modeling (MVM), as shown in the left of Fig. 1.\nBy using the pre-trained discrete V AE [26] from DALL-\nE [27], we “tokenize” the video frames into discrete visual\ntokens, which can be used to reconstruct the original video\nframes. During pre-training, we mask out some proportions\nof the video input along both spatial and temporal dimen-\nsions, and the model learns to recover the discrete visual\ntokens of these masked patches. MVM improves over pre-\nvious MRM/MFM in two ways: ( i) MVM predicts over\na discrete space, which avoids falling ill with the similar\ntraining issues of excessive feature dimensions as in [5,24];\n(ii) MVM is based on latent visual tokens obtained from\na self-reconstruction training procedure, instead of distill-\ning from a well-supervised visual backbone. Our compre-\nhensive comparison shows that MVM enhances the model’s\ncapability to better understand video scenes and in turn ben-\neﬁts downstream VidL tasks.\nIn summary, our contributions are four-fold. ( i) We\npresent VIOLET, a fully end-to-end transformer to model\nthe spatial-temporal dynamic in videos for VidL learning.\n(ii) We propose a new pre-training task, Masked Visual-\ntoken Modeling, which recovers the masked video frame\npatches into a discrete visual token space. ( iii) VIOLET\nachieves state-of-the-art results on 4 text-to-video retrieval\nand 5 out of 8 video question answering tasks. (iv) Compre-\nhensive ablation studies demonstrate the necessity of tem-\nporal video modeling and the effectiveness of MVM across\ndifferent VidL pre-training settings.\n2. Related Work\nVideo-Language Understanding. Joint video-language\n(VidL) understanding [14, 28–32] aims at interpreting the\nphysical world via both vision and text perception. Re-\nsearchers have explored such capability on VidL tasks in-\ncluding text-based video retrieval [1–5], video question an-\nswering [6–9], moment retrieval [2, 4, 10, 11], and video\ncaptioning [1, 3, 12, 13]. Prior arts before the large-scale\npre-training era [9, 14, 33–36] leverage ofﬂine extracted\nvideo features [37–45]. Later on, VidL pre-trained mod-\nels [5, 15, 16, 46] built on the above pre-extracted features\nhave shown promising results. To enhance the performance,\nthere have been parallel interests in bringing in more modal-\nities from raw video inputs [31,47,48] and end-to-end train-\ning till the raw pixels [18,19,25,49], both aiming to elevate\nvideo representations for VidL modeling.\nOur work further explores the second direction for gen-\neral VidL understanding, in contrast to [25] focusing on\ntext-to-video retrieval only. Instead of encoding each video\nframe individually as static image and applying simple\nmean-pooling or concatenation along the temporal dimen-\nsion [18, 19], we demonstrate the necessity of temporal\nmodeling by our video transformer over the input video\nframes, even when they are sparsely sampled [18].\nMasked Visual Modeling. Aligned with the success of\ntransformer-based [50] language pre-training models [51–\n55], image-text pre-training [56–65] and video-text pre-\ntraining [66–68] have shown promising results on di-\nverse vision-language (VL) tasks. Popular VL pre-training\ntasks include Visual-Text Matching and Masked Language\nModeling, which are directly adapted from language pre-\ntraining [23]. Similar masked modeling on visual in-\nputs [5, 24] has also been introduced to VL pre-training,\nbut are not as useful. We propose Masked Visual-token\nModeling (MVM), adopting the latent codes of discrete\nV AE [26, 27, 69] as the reconstruction target for masked\npatches, which eases the auto-encoding prediction and can\nlead to a more signiﬁcant improvement.\nAmong the literature, BEiT [70] and VIMPAC [71] are\ntwo relevant studies of masked visual modeling for image\nclassiﬁcation [42] and action recognition [37]. Speciﬁcally,\nBEiT [70] proposes a BERT-like pre-training strategy to re-\ncover the original visual tokens from some masked image\npatches. Our MVM takes inspiration from BEiT, but ex-\ntends to more complex video inputs with an additional tem-\nporal dimension for VidL modeling. To prevent the model\nfrom taking shortcuts in recovering visual tokens from its\nspatial or temporal neighbors, we further introduce a combi-\nnation of blockwise masking and attended masking. VIM-\nPAC [71] takes a step further to completely remove the raw\npixel inputs from the training procedure. It employs visual\ntokens as the discrete representation of video inputs and ap-\nFigure 2. Overview of the proposed end-to-end VIdeO-LanguagE Transformer (VIOLET), with Video Swin Transformer, Language\nEmbedder, and Cross-modal Transformer. VIOLET adopts Discrete V AE to extract discrete visual tokens to perform Masked Visual-token\nModeling along with Visual-Text Matching and Masked Language Modeling during large-scale visual-text pre-training.\nplies a mask-then-predict pre-training task. The removal of\nraw pixel inputs renders a weaker baseline on popular action\nrecognition tasks. In our work, we leverage visual tokens as\nprediction targets for MVM, instead of replacing the raw\nvideo frame patches.\n3. VIOLET\n3.1. Model Architecture\nFig. 2 illustrates the overall architecture of our end-to-\nend video-language transformer (VIOLET). VIOLET con-\ntains 3 components: Video Swin Transformer (VT), Lan-\nguage Embedder (LE), and Cross-modal Transformer (CT).\nVIOLET takes video Vand sentence Xas inputs. Sparse-\nsampled frames {f1, f2, ...}from Vare ﬁrst processed by\nVT to compute video features v = {v1, v2, ...}. LE extracts\nthe word embeddings w = {w1, w2, ...}for each word to-\nken {x1, x2, ...}in X. Then CT performs cross-modal fu-\nsion on top of v and w to produce joint video-language\n(VidL) representations h for pre-training and downstream\nﬁnetuning. We explain each component in detail below.\nVideo Swin Transformer (VT). Instead of mean pooling\nor concatenating individual frame features, we adopt Video\nSwin Transformer [22] (VT) to model T sparse-sampled\nframes {ft}T\nt=1 along both spatial and temporal dimensions\nas video features {vt}T\nt=1. VT ﬁrst splits each frame as non-\noverlapping H ×W patches [72] and adopts a linear projec-\ntion layer to obtain the preliminary video patch embeddings\nu ∈RT×H×W×d:\nut = LinearProj(ft). (1)\nThe multi-layer 3D-shifted window [22] then considers dif-\nferent levels of spatial-temporal attention over these video\npatch embeddings. We add learnable positional embedding\npv to u, including spatial ps ∈RH×W×d and temporal or-\ndering pt ∈RT×d, and extracts the video features v:\npv\nt = ps + pt\nt,\nv = VT({ut + pv\nt}T\nt=1).\n(2)\nAll patches from the tth frame shares the same pt\nt and all\npatches with the same spatial position are given the sameps.\nIn particular, each 3D window is in the size ofT′×M ×M\nand considers video temporal acrossT′consecutive frames.\nBy adopting 3D windows upon blocks of video patches,\nVT can model image spatial and video temporal simulta-\nneously through the self-attended computation procedure.\nNote that we make a slight modiﬁcation to remove the tem-\nporal down-sampling from the original Video Swin Trans-\nformer and ensure the same temporal dimension as the in-\nput video for Masked Visual-token Modeling during pre-\ntraining (Sec. 3.2).\nVT enforces spatial-temporal modeling via 3D-shifted\nwindow to compute the initial video representations for\nVidL modeling. We demonstrate the advantages of VT over\nsimple mean-pooling or concatenation of “imagiﬁed” frame\nrepresentations under different VidL pre-training settings in\nSec. 4.3. In addition, as VT encodes video frame patches\nthrough a fully self-attended computation, it can support\na variable length of visual inputs. This video encoding\nenables VIOLET to carry out static images ( i.e., T = 1).\nWe discuss the ﬂexibility of pre-training VIOLET on both\nlarge-scale image-text data and video-text data in Sec. 4.3.\nLanguage Embedder (LE). For a language input X, we\nfollow WordPiece [73] and tokenize it into word tokens\n{xi}L\ni=1, where L is the number of tokens in X. LE em-\nbeds the discrete word tokenxi into high-dimensional word\nrepresentation wi ∈Rd :\n{wi}L\ni=1 = LE({xi}L\ni=1). (3)\nCross-modal Transformer (CT). Given video features v\nand word features w, CT performs cross-modal fusion over\nall {vi}T\ni=1 and {wi}L\ni=1 for joint VidL learning. We add\ndifferent positional embeddings pv or px to video features\nv or word features w, to incorporate sequence ordering and\ndistinguish between the two modalities. In particular, we\nreuse pv from VT, containing both spatial position and tem-\nporal ordering information. We concatenate the video and\ntext representations after position embedding as the input\nsequence to CT. In addition, a special[CLS] token is added\nto compute the global VidL representation, used in pre-\ntraining and downstream ﬁnetuning. The joint VidL fea-\ntures h ∈R(T+1+L)×d are computed as:\nh = CT([v + pv, [CLS], w+ px]),\n[hv, hc, hx] =h, (4)\n3.2. Pre-training Tasks.\nTo beneﬁt from large-scale data [19, 25, 74], we in-\ncorporate three pre-training tasks, including our proposed\nMasked Visual-token Modeling. Masked Language Mod-\neling [5, 23, 24] predicts the masked word tokens to im-\nprove language reasoning with the aid of visual perception.\nMasked Visual-token Modeling recovers the masked video\npatches to enhance the video scene understanding. Visual-\nText Matching [18, 24, 25] learns the alignments between\nvideo and text modality, improving the cross-modal fusion.\nMasked Language Modeling (MLM). In MLM, we ran-\ndomly mask out some word tokens with a probability of\n15%.2 The goal is to recover these masked tokens x from\nthe joint VidL features h modeled by Cross-modal Trans-\nformer (CT). Speciﬁcally, the corresponding hx for these\nmasked tokens are fed in a fully-connected (FC) layer\n(FCMLM) and projected to the discrete word token space for\nclassiﬁcation:\nx′\ni = FCMLM(hx\ni),\nLMLM = −E [ 1\n|MMLM|\n∑\ni∈MMLM\nlog P(xi |x′\ni)], (5)\nwhere MMLM denotes the index set of masked word tokens.\nVisual-Text Matching (VTM). VTM enhances the cross-\nmodal fusion via modeling the alignments between visual\nand textual inputs. At each training step, we randomly re-\nplace the corresponding text Xpos for a given video Vwith\nthe text description Xneg from a different video in the same\nbatch. Both the positive pair (V, Xpos) and negative pair\n(V, Xneg) are modeled by CT, and VTM is to tell them apart\nfrom the global VidL representationhc of the [CLS] token.\nIn particular, hc will be processed by a FC layer (FCVTM) to\n2Following BERT [23], We replace 80% of masked word tokens as the\n[MASK] token, 10% as a random token, and 10% as its original token.\nperform binary classiﬁcation:\nbpos = FCVTM(hc\npos), bneg = FCVTM(hc\nneg),\nLVTM = −E[log(bpos) + log(1−bneg)],\n(6)\nwhere hc\npos or hc\nneg is hc of positive or negative pairs.\nMasked Visual-token Modeling (MVM). Previous\nMasked Region Modeling (MRM) [24] and Masked\nFrame Modeling (MFM) [5] extends MLM to visual\ninputs but sometimes leads to unsatisfactory performance.\nDifferent from MRM and MFM, which rely on distilled\nvisual categories or features from a well-supervised vi-\nsual backbone [41, 45], we present Masked Visual-token\nModeling (MVM) to perform masked visual modeling in\na self-reconstruction scenario. We consider the discrete\nvariational autoencoder (dV AE) [26, 27] to quantize video\ninputs into masked prediction targets. dV AE is learned\nto tokenize images into discrete visual tokens q from a\nﬁnite vocabulary and then reconstruct the original visual\nscene based on q, where q should have a one-to-one\ncorrespondence with the input image patches spatially. We\nﬁrst adopt dV AE to tokenize thetth video frame ft into qt:\nqt = dV AE(ft). (7)\nSimilar to MLM, we mask out some video patches by re-\nplacing the pixel values with all zeros. MVM aims at re-\ncovering the visual tokens q of those masked video patches\nv from the corresponding joint VidL features hv. hv is fed\ninto a FC layer (FCMVM) and projected to the discrete visual\ntoken space for classiﬁcation:\nq′\nt,i = FCMVM(hv\nt,i), (8)\nLMVM = −E [\nT∑\nt=1\n1\n|MMVM\nt |\n∑\ni∈MMVM\nt\nlog P(qt,i |q′\nt,i)],\nwhere MMVM\nt is the index set of masked video patches for\nthe tth frame. Using discrete visual tokens as masked pre-\ndiction targets has two main advantages: ( i) The ﬁnite vo-\ncabulary size of these discrete visual tokens eases the learn-\ning of MVM, avoid the previous difﬁculty in model training\nwith MRM/MFM from imperfect patch categories or exces-\nsive feature dimensions; (ii) MVM does not require a well-\nsupervised visual backbone to distill the masking labels.\nThe latent visual tokens can be learned in a self-supervised\nway without human annotations.\n3.3. Masking Strategy of MLM and MVM\nWe introduce a combination of Blockwise Masking and\nAttended Masking to amplify the effectiveness of MLM and\nMVM, as shown in Fig. 3.\nBlockwise Masking (BM). Video usually presents anal-\nogous visual patterns in spatial-temporal neighbors ( i.e.,\nFigure 3. Masking Strategy of MLM and MVM, includingBlock-\nwise Masking (BM) and Attended Masking (AM).\nnearby patches within current frame or neighboring\nframes). While these neighbors make the masked video\npatches easy to recover, they may lead to spurious success\nin MVM evaluation. To make MVM more challenging,\nwe adopt Blockwise Masking [70, 71] that masks blocks of\nvideo patches along spatial-temporal dimension rather than\nindependently masking randomly sampled patches for each\nframe. Speciﬁcally, we randomly sample an (H′, W′, T′)\nas a masking block, where all H′×W′visual patches in\nthe following T′ consecutive frames will be masked; We\nrepeat this process until >15% of video patches are masked\nto perform MVM pre-training. The model cannot merely\nrely on similar neighboring visual cues but requires actual\nvisual reasoning to recover a group of missing patterns.\nAttended Masking (AM). The conventional practice is to\nsample masked visual patches or textual tokens with the\nsame probability over all visual and textual inputs. How-\never, the important elements (e.g., visual patches containing\nthe main object or content words) receive the same weight\nas the less relevant elements (e.g., scene background or stop\nwords) in masked modeling. Attended Masking tries to put\nmore weights on the more important elements based on the\nattention weights computed by Cross-modal Transformer\n(CT). A similar idea has been explored in [19] for MLM. In\nthis paper, we extend AM to both visual and textual modal-\nities. We ﬁrst keep the video-text inputs intact, feed them\ninto CT to compute the attention weights, to decide which\nportions in video and text are more important. We then se-\nlect the top 15% of most-attended tokens to be masked in\nboth video and text inputs to perform MVM and MLM.\n4. Experiments\n4.1. Experimental Setup\nPre-training Datasets. As mentioned in Sec. 3.1, VIO-\nLET is ﬂexible in taking both video and image as inputs.\nHence, We follow [25] to jointly pre-train our model on\nimage-text and video-text data, which we brieﬂy describe\nbelow. ( i) YT-Temporal-180M (YT-Temporal)[19] con-\ntains 6M YouTube videos with subtitle texts from Auto-\nmatic Speech Recognition (ASR). Following [19], we di-\nvide a long video into several video segments, with an av-\nerage length of 9.29 seconds. We treat every 4 consec-\nutive segments with their ASR as a video clip, leading\nto 180M video-subtitle pairs. ( ii) WebVid-2.5M (Web-\nVid) [25] scrapes 2.5M video-text pairs from the web. Dif-\nferent from YT-Temporal, text data in WebVid describes\nthe global video semantic. ( iii) ConceptualCaptions-3M\n(CC) [74] consists of 3.3M image-text pairs harvested from\nthe web. We compare the effects of different pre-training\ndata on downstream tasks in Sec. 4.3.\nDownstream Tasks. We evaluate VIOLET on both text-\nto-video retrieval and video question answering, across 12\ndownstream benchmarks. For text-to-video retrieval, we re-\nport performance of Recall at K (R@K) on MSRVTT [1],\nDiDeMo [79], YouCook2 [13] and LSMDC [3]. For\nvideo question answering, we consider 8 datasets in\nmultiple-choice and open-ended settings: TGIF-Action,\nTGIF-Transition and TGIF-Frame [6], MSRVTT-MC [80],\nMSRVTT-QA, MSVD-QA [7], LSMDC-MC and LSMDC-\nFiB [81]. Accuracy is used as evaluation metric. More\ndetails are provided in Appendix A.\nImplementation Details. We initialize our Video Swin\nTransformer with VideoSwin-Base [22], pre-trained on\nKinetics-400 [37]. Language Embedder and Cross-\nmodal Transformer are initialized from pre-trained BERT-\nBase [59]. We train VIOLET in a end-to-end manner for\nboth pre-training and downstream ﬁnetuning.\nDuring pre-training, we sparsely sample T = 4 video\nframes and resize them into 224x224 to split into patches\nwith H = W = 32. We use pre-trained DALL-E [27] as our\ndV AE to generate discrete visual tokens for MVM. For We-\nbVid [25] and CC [74], we perform VTM+MLM+MVM to\npre-train on videos or images with the globally-aligned alt-\ntext descriptions. We follow [19] to concatenate all ASR\ndescriptions for each middle frame as text input for YT-\nTemporal. VTM is performed for each pair of middle frame\nand its ASR text to learn the temporal reasoning over YT-\nTemporal video clips. Our implementation of VIOLET is\nbased on PyTorch [82]. We adopt AdamW [83] as the op-\ntimizer with an initial learning rate of 2e-5, betas of (0.9,\n0.98), and weight decay of 1e-3 for all pre-training experi-\nments. VIOLET follows a simple curriculum learning strat-\negy, where we ﬁrst pre-train on YT-Temporal with noisy\nASR text for 5 epochs and then on WebVid+CC with alt-\ntext descriptions for another 5 epochs.\nFor all downstream tasks, we adopt the same video frame\nsize (224x224) and patch size (32x32) but 5 sparse-sampled\nframes. Due to various data scales and domains, we use\ntask-speciﬁc learning rates and training epochs based on the\nperformance of the validation set for each downstream task.\nText-to-Video Retrieval\nMethod MSRVTT DiDeMo YouCook2 LSMDC\nModels using Pre-extracted Features\nHT100M [16] 14.9 / 40.2 / 52.8 - 08.2 / 24.5 / 35.3 07.1 / 19.6 / 27.9\nMMT [31] 26.6 / 57.1 / 67.1 - - 12.9 / 29.9 / 40.1\nHERO [5] 16.8 / 43.4 / 57.7 - - -\nA VLnet [47] 27.1 / 55.6 / 66.6 - 33.2 / 61.0 / 71.5 17.0 / 38.0 / 48.6\nSupport-Set [32] 30.1 / 58.3 / 69.3 - - -\nTACo [75] 28.4 / 57.8 / 71.2 - 29.6 / 59.7 / 72.7 -\nVideoCLIP [17] 30.9 / 55.4 / 66.8 - 32.2 / 62.6 / 75.0 -\nModels with End-to-end Training\nClipBERT [18] 22.0 / 46.8 / 59.9 20.4 / 48.0 / 60.8 - -\nFrozen [25] 32.5 / 61.5 / 71.2 31.0 / 59.8 / 72.4 - 15.0 / 30.8 / 39.8\nClip4Clip [76] 42.1 / 71.9 / 81.4 43.4 / 70.2 / 80.6 - 21.6 / 41.8 / 49.8\nVIOLET 34.5 / 63.0 / 73.4 32.6 / 62.8 / 74.7 35.7 / 66.7 / 78.2 16.1 / 36.6 / 41.2\n(a) Text-to-video Retrieval (Finetuned).\nZero-Shot Retrieval\nMethod MSRVTT DiDeMo\nModels using Pre-extracted Features\nHT100M [16] 07.5 / 21.2 / 29.6 -\nMMT [31] 0-0 / 06.9 / 0-0 -\nA VLnet [47] 19.6 / 40.8 / 50.7 -\nSupport-Set [32] 12.7 / 27.5 / 36.2 -\nTACo [75] 09.8 / 25.0 / 33.4 -\nVideoCLIP [17] 10.4 / 22.2 / 30.0 16.6 / 46.9 / 0-0\nModels with End-to-end Training\nMIL-NCE [49] 09.9 / 24.0 / 32.4 -\nV ATT [77] 0-0 / 00-0 / 29.7 -\nFrozen [25] 24.7 / 46.2 / 57.2 21.1 / 46.0 / 56.2\nCLIP [76, 78] 31.2 / 53.7 / 64.2 -\nVIOLET 25.9 / 49.5 / 59.7 23.5 / 49.8 / 59.8\n(b) Text-to-video Retrieval (Zero-shot).\nTable 1. Comparison with SOTA ontext-to-video-retrieval tasks under different settings: (a) pre-train then ﬁnetune and (b) pre-train then\nzero-shot evaluation. All resutls are reported on R@1 / R@5 / R@10. All models perform visual-text pre-training. Rows highlighted in\nblue use additional modalities such as sound and speech besides video frames.\n4.2. Comparison to Prior Arts\nText-to-Video Retrieval. Table 1a summarizes results on\ntext-to-video retrieval. VIOLET achieves signiﬁcant gain\nover existing VidL pre-trained models across all text-to-\nvideo retrieval datasets considered. Speciﬁcally, VIOLET\nsurpasses most previous methods focus on modeling multi-\nmodal fusion with pre-extracted video features. Notably,\nVIOLET is still competitive even when compared with\nMMT [31], HERO [5], and A VLnet [47] that use additional\nmodalities, such as sound and speech besides video frames.\nFor comparisons to end-to-end pre-trained models, VI-\nOLET outperforms ClipBERT [18] by +10% on R@1 on\nboth MSRVTT and DiDeMo, even though VIOLET uses\neven less frames (Ours: 5 frames vs. ClipBERT: 16 frames).\nThese results highlight the deﬁciency of ‘imagifying‘ video\nrepresentations. When compared with Frozen [25], de-\nsigned speciﬁcally for text-to-video retrieval tasks, VI-\nOLET can achieve notable performance improvements\nwith +2.0%, +1.6% and +1.1% on R@1 for MSRVTT,\nDiDeMo and LSMDC, respectively. We also include results\nfrom Clip4Clip [76] that leverages pre-trained CLIP [78] on\nover 400M image-text data, which is a few magnitude larger\nthan our pre-training data. VIOLET closes the gap between\nprevious end-to-end pre-trained models and Clip4Clip, and\nwe believe pre-training VIOLET with larger-scale data can\nfurther reduce the gap.\nZero-shot text-to-video retrieval. We further conduct\ngeneralizability evaluation under the zero-shot setting on\nMSRVTT and DiDeMo in Table 1b. Similarly, VIOLET\nachieves remarkable performance improvements over the\nexisting methods by large margins. Speciﬁcally, we ob-\nserve +6% gain on R@1 over previous models using pre-\nextracted video features and +1.2 −2% on R@1 over end-\nto-end pre-trained models, excluding CLIP [76, 78].\nTGIF MSRVTT LSMDC MSVD\nMethod Action Transition Frame MC QA MC FiB QA\nClipBERT [18] 82.8 87.8 60.3 88.2 37.4 - - -\nJustAsk [68] - - - - 41.5 - - 46.3\nMERLOT [19] 94.0 96.2 69.5 90.9 43.1 81.7 52.9 -\nVIOLET 92.5 95.7 68.9 91.9 43.9 82.8 53.7 47.9\nTable 2. Comparison with SOTA methods on video question an-\nswering. We gray out MERLOT due to its excessive computa-\ntional cost (e.g., 30K TPU hours vs. 2K GPU hours (ours) for pre-\ntraining and frame resolution 704 vs. 224 for downstream tasks).\nVideo Question Answering. We compare with prior arts\non video question answering (QA) tasks in Table 2. VI-\nOLET surpasses ClipBERT [18] with signiﬁcant perfor-\nmance gain of +9.7% on TGIF-Action, +7.9% on TGIF-\nTransition, +8.6% on TGIF-Frame, +3.7% on MSRVTT-\nMC and +6.5% on MSRVTT-QA. These results suggest the\nexplicit temporal modeling introduced by our video trans-\nformer is essential for video QA tasks, and pre-training with\nimage-text data alone may not be sufﬁcient for VidL mod-\neling. We provide more detailed discussions in Sec. 4.3.\nNote that both JustAsk [68] and MERLOT [19] speciﬁ-\ncally focus on video QA. JustAsk automatically generates\n69M video-question-answer triplets from narrated videos\nfor training, which is hardly extendable to text-to-video re-\ntrieval tasks. MERLOT is pre-trained for 40 epochs and\nwith extensive hyperparameter tuning on the frame resolu-\ntion from 384x704 to 704x704 for downstream tasks. The\noverall pre-training of MERLOT takes 30,720 TPU hours\non TPU v3. In contrast, we pre-train VIOLET for 5 epochs,\nwhich results in 2,240 GPU hours on V100 GPUs. We also\nadopt a much lower frame resolution of 224x224. With a\nmuch lower computational cost, VIOLET achieves around\n+1.0% performance gain over MERLOT on 4 video QA\ntasks for MSRVTT and LSMDC videos, while remains\nVideo TGIF- TGIF- MSRVTT- DiDeMo-\nEncoding Action Transition Retrieval Retrieval\nRandom initialized visual encoder\nMean 72.1 83.5 08.4 / 22.7 / 35.3 09.1 / 24.9 / 36.7\nConcat 72.9 83.7 09.0 / 23.5 / 35.5 09.4 / 25.8 / 38.1\nVT 73.6 84.6 09.2 / 24.0 / 35.8 10.3 / 30.1 / 40.5\nImageNet pre-trained visual encoder\nMean 77.5 86.5 09.6 / 26.7 / 39.5 09.5 / 27.5 / 40.9\nConcat 78.0 87.0 10.4 / 30.5 / 42.0 10.6 / 30.8 / 42.9\nVT 79.6 87.8 11.8 / 32.3 / 44.6 12.0 / 32.4 / 43.5\n+ Video-text pre-training on WebVid\nMean 80.3 88.7 20.8 / 44.9 / 58.1 17.9 / 43.5 / 51.3\nConcat 82.5 91.2 23.5 / 51.9 / 63.0 22.2 / 50.5 / 62.6\nVT 85.8 92.1 27.0 / 56.5 / 68.8 26.1 / 56.9 / 68.9\nTable 3. Impact of different temporal modeling methods over\nvideo inputs under different settings: (i) random initialized visual\nencoder; ( ii) ImageNet [84] pre-trained visual encoder and ( iii)\nAdding video-text pre-training on WebVid [25].\ncompetitive on TGIF. We believe VIOLET can further im-\nprove with larger frame resolution and longer pre-training\nepoch if computational resources permit.\n4.3. Analysis of VIOLET\nWe conduct ablation experiments on two video question\nanswering datasets (TGIF-Action and TGIF-Transition)\nand two text-to-video retrieval datasets (MSRVTT and\nDiDeMo) to study the factors leading toVIOLET’s success.\nImpact of Temporal Video Modeling. To demonstrate\nthe necessity of temporal modeling even under sparse sam-\npling, we compare three variants for temporal modeling in\nTable 3. (i) Mean: mean-pooling over independently com-\nputed frame features via ResNet-50 [43] as in [18]; ( ii)\nConcat: concatenation of the aforementioned frame fea-\ntures along the temporal dimension as in [19]; (iii) VT: en-\nforcing spatial-temporal modeling altogether on input video\nframe sequences via Video Swin Transformer in VIOLET.\nThe ﬁnal video representations are then concatenated with\ncorresponding text embeddings and fed in Cross-modal\nTransformer for downstream VidL modeling. We show re-\nsults under different settings: random-initialized visual en-\ncoder, ImageNet-pretrained visual encoder, and with addi-\ntional VidL pre-training on WebVid [25].\nVT consistently outperforms Mean and Concat over the\n4 datasets across all settings. The loss of temporal infor-\nmation in naive mean pooling (Mean) result in worst per-\nformance among the three. Although Concat can preserve\nthe temporal order of input video frames, it solely relies on\nCross-modal Transformer to model both the temporal dy-\nnamics in video and the correspondence between visual and\ntextual elements, brings unsatisfactory performance.\nWhen taking a closer look into different pre-training\nsettings, multimodal pre-training on WebVid signiﬁcantly\nboosts the model performance, compared to unimodal pre-\nPre-training TGIF- TGIF- MSRVTT- DiDeMo-\nTask Action Transition Retrieval Retrieval\nNone 81.9 88.5 13.0 / 36.5 / 49.6 18.3 / 46.4 / 56.5\nVTM+MLM 85.4 91.6 24.4 / 54.4 / 68.1 25.8 / 54.2 / 67.0\n+ MCM 85.0 91.6 26.0 / 56.0 / 68.4 25.8 / 55.9 / 68.1\n+ MFM 85.5 92.0 26.2 / 55.5 / 68.4 25.4 / 55.5 / 67.8\n+ MPM 85.0 91.8 26.6 / 56.2 / 68.4 26.0 / 56.5 / 68.0\n+ MVM 85.8 92.1 27.0 / 56.5 / 68.8 26.1 / 56.9 / 68.9\nTable 4. Impact of self-supervised pre-training on video inputs.\nAll pre-training are conducted on WebVid [25].\nFigure 4. MVM accuracy vs. downstream performance. We\nadopt only MVM during pre-training (using 0% (w/o MVM), 10%\n(8.9% MVM accuracy), 20% (13.4%), 50% (19.2%), and 100%\n(21.6%) of YT-Temporal videos).\ntraining of visual encoder on ImageNet. In addition, VT\nbeneﬁts more from VidL pre-training, leading to a bigger\nperformance gap when compared to Mean or Concat. As\nthe exposure to video data during pre-training utmostly en-\nhances the learning of temporal dynamics.\nEffectiveness of MVM. To demonstrate the effectiveness\nof MVM, we compare different variants of masked vi-\nsual modeling when pre-trained on WebVid [25] in Ta-\nble 4. First, we establish two baselines: without pre-training\n(None) and pre-training with only Visual-Text Matching\nand Masked Language Modeling (VTM+MLM) following\n[18, 19, 25]. Then we augment VTM+MLM with different\nvariants of masked visual modeling tasks. Masked Classi-\nﬁcation Modeling (MCM) mimics MRC in [24] to predict\nthe ImageNet [84] category of the masked image patch from\na pre-trained ResNet-50 [43]; Masked Feature Modeling\n(MFM) [5] distills the ﬁxed frame features extracted from a\npre-trained visual encoder. We use the output feature of the\nmasked patches from the last CNN layer of a ImageNet pre-\ntrained ResNet-50 and adopt linear regression as the train-\ning objective; Masked Patch Modeling (MPM) [5] dis-\ntinguishes the correct masked visual patch from negative\npatches in the same batch with Noise Contrastive Estima-\ntion loss [85], similar to MFM-NCE in [5].\nOur results suggest that not all masked visual modeling\nMethod TGIF- TGIF- MSRVTT- DiDeMo-\nAction Transition Retrieval Retrieval\nWithout Pre-training\nVIOLET 81.9 88.5 13.0 / 36.5 / 49.6 18.3 / 46.4 / 56.5\nPre-training on COCO+VG\nClipBERT [18] 82.8 87.8 22.0 / 46.8 / 59.9 20.4 / 48.0 / 60.8\nVIOLET 84.8 90.2 23.5 / 50.5 / 63.9 22.8 / 51.2 / 62.0\nPre-training on WebVid+CC\nFrozen [25] - - 31.0 / 59.5 / 70.5 31.0 / 59.8 / 72.4\nVIOLET 87.1 93.6 34.2 / 63.5 / 73.6 32.9 / 63.0 / 74.5\nPre-training on YTTemporal\nMERLOT [19] 94.0 96.2 - -\nVIOLET 91.0 94.7 25.4 / 54.3 / 64.6 26.7 / 56.4 / 64.6\nTable 5. Impact of using different pre-training data. We gray\nout MERLOT due to its excessive computational cost ( e.g., 30K\nTPU hours vs. 2K GPU hours (ours) for pre-training and frame\nresolution 384x704 vs. 224x224 (ours) for downstream tasks).\nmethods bring consistent improvement. MCM and MPM\ngive worse results on TGIF-Action over VTM+MLM; sim-\nilar trends have been observed in [5, 24]. MFM seems to\nfavor QA tasks, and MPM beneﬁts more on retrieval tasks.\nIn contrast, MVM leads to the best performance on all tasks,\nas it recovers masked patches into a ﬁnite discrete set, mak-\ning the learning of masked visual modeling easier.\nWe further investigate the relationship between MVM\nperformance and downstream performance. We pre-train\nVIOLET with MVM-only on 10%, 20%, 50%, and 100%\nof video scenes from YT-Temporal [19], discarding the\ncorresponding text. As illustrated in Fig. 4, such MVM\npre-training on video inputs only can greatly lift the per-\nformance on all 4 datasets, even without text informa-\ntion. Moreover, better MVM performance also leads to bet-\nter downstream performance. For example, with a 21.6%\nMVM accuracy on 100% YT-Temporal data, our model\nachieves +2.3% improvement on TGIF-Action and +2.5%\nR@5 increase on MSRVTT. In summary, results in Table 4\nand Fig. 4 suggest that MVM is vital in the success of VI-\nOLET, as it learns a better video representation to beneﬁt\ndownstream VidL tasks.\nImpact of different pre-training data. Table 5 estab-\nlishes a fair comparison to the recent SOTA methods, Clip-\nBERT [18], Frozen [25] and MERLOT [19], with the same\npre-training data, respectively. We also compare the effects\nof different pre-training data on downstream tasks.\nUnder fair comparison, our model consistently outper-\nforms ClipBERT and Frozen by large margins. When both\npre-trained on COCO+VG [44, 86], VIOLET surpasses\nClipBERT by >+2.0% on Video QA tasks, and >+1.5%\non R@1 for retrieval tasks. Frozen adopts a two-stream\narchitecture speciﬁcally designed for text-to-video retrieval\napplications. VIOLET not only is applicable to video QA\ntasks but also achieves a gain of >+1.9% on R@1 for re-\ntrieval tasks over Frozen, when both pre-trained on Web-\nFigure 5. Qualitative examples of self-reconstruction (high-\nlighted with orange bounding boxes) from predicted visual tokens\nduring our Masked Visual-token Modeling (MVM).\nVid+CC [25,74]. On YT-Temporal [19], VIOLET achieves\ncompetitive results with MERLOT on TGIF-Action and\nTGIF-Transition with a much lower training cost, as dis-\ncussed in Sec. 4.2. We further examine the effect of dif-\nferent pre-training data on downstream tasks with VIO-\nLET. YT-Temporal is designed to promote video temporal\nreasoning and not surprisingly leads to the best QA result.\nHowever, the noisy ASR descriptions lead to smaller gains\nin retrieval tasks, with a similar performance to COCO+VG,\nbut much worse than WebVid+CC with a smaller data size\n(5.5M vs. 180M). Therefore, we take advantage of both YT-\nTemporal and WebVid+CC as our ﬁnal pre-training corpus,\nwhich leads to strong performance on both video QA and\nretrieval tasks as presented in Sec. 4.2.\nQualitative Examples. Fig. 5 illustrates the qualitative ex-\namples of self-reconstruction from predicted visual tokens\nduring MVM, under both Blockwise Masking (BM) and\nAttended Masking (AM). As shown, BM masks blocks of\nvideo patches along with consecutive frames and AM masks\nthe most-attended video patches based on text input ( e.g.,\ndrawing with “hand” and “cartoon image” in the 2nd row\nor “chicken” and “ground” in the 4th row). VIOLET im-\nproves visual reasoning through this video reconstruction\nduring MVM, and the better video scene understanding fur-\nther beneﬁts downstream VidL tasks.\n5. Conclusion\nWe present VIOLET, a fully end-to-end VIdeO-\nLanguagE Transformer, which contains Video Swin Trans-\nformer to explicitly model the vital video temporal for\nvideo-language learning. We further enhance VIOLET\nwith a new pre-training task, Masked Visual-token Model-\ning (MVM), that learns video scene understanding through\na mask-the-predict procedure with self-reconstructable vi-\nsual tokens. Experiments on various text-to-video retrieval\nand video question answering tasks show that VIOLET\nachieves SOTA (or competitive) performance. Comprehen-\nsive ablation studies demonstrate the necessity of temporal\nvideo modeling and the effectiveness of MVM over previ-\nous MRM/MFM for video-language reasoning under differ-\nent pre-training settings.\nA. Experimental Setup of Downstream Tasks\nWe evaluate our pre-trained VIOLET on text-to-video\nretrieval and video question answering tasks across 12\ndownstream datasets. For text-to-video retrieval, we re-\nport model performance on MSRVTT [1], DiDeMo [79],\nYouCook2 [13], and LSMDC [3] and use Recall at K\n(R@K) as the evaluation metric. For video question answer-\ning, we consider datasets in both multiple-choice and open-\nended settings, including TGIF-Action, TGIF-Transition,\nTGIF-Frame [6], MSRVTT-MC, MSRVTT-QA, MSVD-\nQA [7], LSMDC-MC and LSMDC-FiB [81]. We evaluate\nour models using accuracy.\nWe follow the standard training/validation/testing splits\nof the original datasets. If not otherwise stated, we sparsely\nsample T = 5 video frames and adopt video frame size 224\nwith patch size 32. We use AdamW [83] to ﬁne-tune VI-\nOLET for each downstream task with an initial learning\nrate of 1.2e-5, betas of (0.9, 0.98), and weight decay of 1e-\n3. All ﬁnetuning experiments are conducted on Microsoft\nAzure [87] with 8 Nvidia V100 GPUs (32GB VRAM).\nA.1. Text-To-Video Retrieval\nFor text-to-video retrieval, similar to visual-text match-\ning (VTM) during pre-training, we treat corresponding\nvideo-text pairs as positives and all other pairwise com-\nbinations as negatives. We adopt a fully-connected (FC)\nlayer (FCT2V) over the global VidL representation hc of the\n[CLS] token to perform binary classiﬁcation:\nbpos = FCT2V(hc\npos), bneg = FCT2V(hc\nneg),\nLT2V = −E[log(bpos) + log(1−bneg)],\n(9)\nwhere hc\npos or hc\nneg is hc of positive or negative pairs. In\nparticular, we use pre-trained FC VTM for zero-shot text-to-\nvideo retrieval and to initialize FCT2V for further ﬁne-tuning\non each downstream text-to-video retrieval task.\nMSRVTT [1]contains 10K YouTube videos with 200K hu-\nman annotations. For fair comparison [18, 25], we train on\n9K training+validation splits and evaluate on the 1K-A test-\ning split. We adopt batch size 56 and train for 20 epochs.\nVideoQA Task #Option\nMultiple-\nChoice\nTGIF-Action [6] 5\nTGIF-Transition [6] 5\nMSRVTT-MC [7] 5\nLSMDC-MC [3] 5\nOpen-\nEnded\nTGIF-Frame [6] 1,540\nMSRVTT-QA [7] 1,500\nMSVD-QA [88] 1,000\nLSMDC-FiB [81] 908\nTable 6. Summary of video question answering tasks.\nDiDeMo [79] consists of 10K videos annotated with 40K\nsentences from Flickr. Following [18, 25], we concatenate\nall sentences from the same video into a paragraph and per-\nform paragraph-to-video retrieval for DiDeMo. We adopt\nbatch size 48 and train for 20 epochs.\nYouCook2 [13] contains 14K video clips from 2K cooking\nvideos and 89 recipes. Each clip is annotated with one sen-\ntence. We follow [16,17] to report retrieval performance on\nthe entire validation clips. We adopt batch size 56 and train\nfor 40 epochs.\nLSMDC [3] is built upon 118K video clips from 202\nmovies. Each clip has a caption from movie scripts or de-\nscriptive video services. Following [16, 25], we evaluate on\n1K testing clips that disjoint from the training+validation\nsplits. We adopt batch size 56 and train for 40 epochs.\nA.2. Video Question Answering\nWe test our model on video question answering (QA)\ntasks in both multiple-choice and open-ended settings,\nas summarized in Table 6. For multiple-choice QA\ntasks, we concatenate question with each answer option\nand add a separating blank token to form the input text\n(Q+[SEP]+A). We adopt a FC layer upon hc to predict the\nmodel conﬁdence on each answer option. Cross-entropy\nloss is used to train a classiﬁer over all answer options for\neach video-question pair. For open-ended QA tasks, we fol-\nlow the common practice to convert it to a classiﬁcation task\nwith a ﬁnite set of answer classes. We build a speciﬁc an-\nswer vocabulary that can cover most common answers in\nthe training split of each dataset. Similarly, our model pre-\ndicts the answer to a given question over all answer vocab-\nulary through a FC layer upon hc.\nTGIF-Action, TGIF-Transition, and TGIF-Frame [6]\nrequire spatial-temporal reasoning to answer questions re-\ngarding GIF videos in TGIF-QA [6] Speciﬁcally, we aim to\ntest our model along three dimensions: ( i) Action: to rec-\nognize the repeated action; ( ii) Transition: to identify the\ntransition between the before and after states; ( iii) Frame:\nto answer questions about a speciﬁc frame from the GIF\nvideo. Among them, TGIF-Action and TGIF-Transition are\ncollected under multiple-choice setting, and TGIF-Frame is\nan open-ended video QA task with free-form answers. In\nMasking TGIF- TGIF- MSRVTT- DiDeMo-\nStrategy Action Transition Retrieval Retrieval\nWithout pre-training\nNone 81.9 88.5 13.0 / 36.5 / 49.6 18.3 / 46.4 / 56.5\nPre-train on WebVid [25] with VTM+MLM+MVM\nRandom 83.7 90.8 24.3 / 54.8 / 66.7 24.2 / 53.5 / 67.6\nBM 85.4 91.8 27.0 / 56.2 / 68.6 25.8 / 56.8 / 68.8\nAM 85.5 91.6 26.8 / 56.5 / 68.7 26.0 / 56.8 / 68.6\nBM+AM 85.8 92.1 27.0 / 56.5 / 68.8 26.1 / 56.9 / 68.9\nTable 7. Impact of masking strategy in MVM and MLM.\nour implementation, we select 1,540 most common answers\nas answer candidates for TGIF-frame. We adopt batch size\n48 and train for 20 epochs.\nMSRVTT-MC and MSRVTT-QA [7]are created based on\nvideos and captions in MSRVTT [1]. MSRVTT-MC is a\nmultiple-choice task with videos as questions, and captions\nas answers. Each video contains 5 captions, with only one\npositive match. MSRVTT-QA contains 243K open-ended\nquestions over 10K videos. We select 1,500 most common\nanswers as the answer candidates. We adopt batch size 48\nand training epochs 20 for both datasets.\nMSVD-QA [7] consists of 47K open-ended questions over\n2K videos, based on video-caption pairs from MSVD [88].\nWe use 1,000 most common answers as the answer vocab-\nulary. We adopt batch size 80 and train for 40 epochs.\nLSMDC-MC and LSMDC-FiB [81] are built from\nLSMDC dataset [3]. Similar to MSRVTT-MC, LSMDC-\nMC requires the model to select the only positive caption\nthat describes the video from 5 caption candidates and for-\nmulates it as a multiple-choice QA task. LSMDC-FiB re-\nplaces a word in the question sentence with the [BLANK]\ntoken, and the model is to recover the missing word. We\nregard LSMDC-FiB as an open-ended Video QA task. In\nparticular, we use a FC layer over the joint VidL represen-\ntation h of the [BLANK] token to predict from 908 answer\ncandidates. We adopt batch size 80 and train for 40 epochs.\nB. Impact of Masking Strategy\nTo amplify the effectiveness of our MLM and MVM, we\nintroduce Blockwise Masking (BM) and Attended Mask-\ning (AM) in Sec. 3.3. Table 7 compares different masking\nstrategies when pre-trained on WebVid [25]. Speciﬁcally,\nwe compare 4 masking strategies, random masking, BM\nonly, AM only and BM+AM. Although improving from the\nnon-pretrained baseline, random masking results in the least\nperformance improvement on both video QA and retrieval\ntasks. In contrast, BM or AM alone brings more signiﬁ-\ncant performance improvements on both tasks, while seems\nto beneﬁt different tasks ( e.g., 91.8% on TGIF-Transition\nand 27.0% R@1 on MSRVTT-Retrieval for BM, and 85.5%\nMethod Frame Size VCR\nMERLOT [19] 384x704 75.1\nVIOLET 224x224 74.9\nVIOLET 384x384 76.3\nTable 8. Comparison with MERLOT [19] under the same pre-\ntraining epoch on VCR [89]. The pre-training are conducted on\nYT-Temporal [19] for 5 epochs.\non TGIF-Action and 26.0% R@1 on DideMo-Retrieval for\nAM). Finally, by leveraging both BM and AM (BM+AM),\nwe lead to the best performance among the four.\nUnlike random masking, BM cuts down the spurious\nsuccess in MVM evaluation through neighboring patches\nthat are visually similar to the masked patches, and AM puts\nmore masking weights on more important video-text ele-\nments based on the attention pattern from our Cross-modal\nTransformer. These results demonstrate that both BM and\nAM contribute to the success of VIOLET.\nC. Extending VIOLETto Image Question An-\nswering Task\nIn this section, we show that VIOLET is also ex-\ntendable to image question answering task by evaluating\nit on VCR [89], which requires commonsense reasoning\nabout the image content. We follow MERLOT [19] to\ndraw colored highlights around the referenced entity ( e.g.,\n[PERSON-1] and [CHAIR-2]) in the given image and\nreport performance on the multiple-choice Q →A subtask.\nTo ﬁnetune our model, we concatenate the question and\neach answer choice from the 4 possible answer candidates.\nSimilarly, a FC layer upon the global cross-modal represen-\ntation hc of the [CLS] token is adopted to predict the an-\nswer and cross-entropy loss is used to supervise the model\ntraining. We adopt batch size 48 and train for 20 epochs.\nThe results are shown in Table 8. For a fair compari-\nson, both VIOLET and MERLOT are pre-trained on YT-\nTemporal [19] for 5 epochs. Note that MERLOT adopts\na input image resolution of 384x704. With input image\nsize of 224x224, our VIOLET achieves comparable per-\nformance as MERLOT (74.9% vs. 75.1%). When in-\ncreasing the input image resolution to 384x384, though still\nsmaller than the input image size in MERLOT, VIOLET\ncan achieve a superior performance with an absolute gain\nof +1.2% over MERLOT.\nAs mentioned in Sec. 4.2, the full MERLOT pre-training\nrequires excessive computation power (30K TPU hours),\nwhile VIOLET, only pre-trained for 5 epochs (2K GPU\nhours), can achieve competitive performance on video-\nlanguage downstream tasks with lower input resolution\n(ours: 224x224, MERLOT:384x704). When comparing\nthe results from VIOLET with different input resolutions\nin Table 8, we observe that higher input resolution results\nin better downstream performance. It is also worth noting\nthat longer pre-training leads to monotonic performance im-\nprovement, as shown in [19]. Hence we believe VIOLET\ncan further improve with higher frame resolution and more\npre-training epochs if computational resources permit.\nD. Qualitative Examples of Zero-shot Text-to-\nVideo Retrieval\nWe visualize some qualitative examples of zero-shot\ntext-to-video retrieval in Fig. 6-9 on MSRVTT [1],\nDiDeMo [79], LSMDC [3], and YouCook2 [13], respec-\ntively. These examples show that pre-training on large-\nscale visual-text data (YT-Temporal [19], WebVid [25], and\nCC [74]) enables VIOLET to learn cross-modal alignment\nto perform text-to-video retrieval in a zero-shot scenario.\nFor MSRVTT (Fig. 6),“grand theft auto 5” (a video game)\nis not a commonly seen phrase, but we can still retrieve rel-\nevant video clips depicting the video game. For paragraph-\nto-video retrieval in DiDeMo (Fig. 7), the textual query is a\nconcatenation of multiple sentences, much longer than the\ninput text during pre-training. Surprisingly, VIOLET can\nstill retrieve videos that contain relevant semantics men-\ntioned in the textual query. For instance, the top-2/3/5 of\nthe retrieved videos on the upper left of Fig. 7 correspond to\nthe textual cues, such as “traveling away, comes back, red\nsigns ﬂaps”, Moreover, visualizations of zero-shot text-to-\nvideo retrieval on LSMDC (Fig. 8) and YouCook2 (Fig. 9)\nshow that VIOLET is generalizable to more speciﬁc video\ndomains, such as movie or cooking videos.\nE. Limitation and Broader Impact\nThe broader impact of this paper falls in applications\nof video-language (VidL) reasoning, including video ques-\ntion answering and text-to-video retrieval. Our end-to-end\nVIdeO-LanguagE Transformer (VIOLET) has the potential\nto be applied to various VidL tasks, such as video caption-\ning and video grounded dialogue, which is worth explo-\nration in future study. In addition, the newly introduced\nMasked Visual-token Modeling can further improve the per-\nformance when scaling up the pre-training to even larger-\nscale visual-text data. There are also several potential lim-\nitations of VIOLET that would make for promising av-\nenues of future work, including: 1) extending VIOLET to\nmodel the full-length videos with densely sampled frames\nfor downstream VidL tasks like TGIF-Count [6]; and 2) ex-\nploring extra input signals from videos, such as audio, into\nthe VIOLET framework for better performance.\nWe do not anticipate major ethical issues in our work.\nAs a data-driven system, the self-supervised method is sen-\nsitive to the distribution of the pre-training data. Therefore,\nwe consider diverse types of video, including VLOGs, in-\nstructional videos, short-duration GIFs, and even static im-\nages across YT-Temporal [19], WebVid [25], and CC [74].\nThe accompanying textual inputs used to train our model\nare from various sources, such as alt-text descriptions, hu-\nman annotations, and ASR outputs, at different levels, in-\ncluding temporally-speciﬁed and globally-aligned descrip-\ntions. We conduct a comprehensive downstream evaluation\nover 12 VidL tasks, trying to mitigate the bias of our learned\ncross-modal representation for better VidL reasoning.\nFigure 6. Qualitative examples of zero-shot text-to-video retrieval on MSRVTT [1].\nFigure 7. Qualitative examples of zero-shot text-to-video retrieval on DiDeMo [79].\nFigure 8. Qualitative examples of zero-shot text-to-video retrieval on LSMDC [3].\nFigure 9. Qualitative examples of zero-shot text-to-video retrieval on YouCook2 [13].\nReferences\n[1] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT:\nA Large Video Description Dataset for Bridging Video and\nLanguage. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016. 1, 2, 5, 9, 10, 11, 12\n[2] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and\nJuan Carlos Niebles. Dense-Captioning Events in Videos. In\nInternational Conference on Computer Vision (ICCV), 2017.\n1, 2\n[3] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt\nSchiele. A Dataset for Movie Description. In Conference on\nComputer Vision and Pattern Recognition (CVPR), 2015. 1,\n2, 5, 9, 10, 11, 14\n[4] Jie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal.\nTVR: A Large-Scale Dataset for Video-Subtitle Moment\nRetrieval. In European Conference on Computer Vision\n(ECCV), 2020. 1, 2\n[5] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng\nYu, and Jingjing Liu. HERO: Hierarchical Encoder for\nVideo+Language Omni-representation Pre-training. In Con-\nference on Empirical Methods in Natural Language Process-\ning (EMNLP), 2020. 1, 2, 4, 6, 7, 8\n[6] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and\nGunhee Kim. TGIF-QA: Toward Spatio-Temporal Reason-\ning in Visual Question Answering. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017. 1, 2, 5,\n9, 11\n[7] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,\nXiangnan He, and Yueting Zhuang. Video Question Answer-\ning via Gradually Reﬁned Attention over Appearance and\nMotion. In ACM Multimedia (ACMMM), 2017. 1, 2, 5, 9, 10\n[8] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg.\nTVQA: Localized, Compositional Video Question Answer-\ning. In Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), 2018. 1, 2\n[9] Jie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal.\nTVQA+: Spatio-Temporal Grounding for Video Question\nAnswering. In Annual Meeting of the Association for Com-\nputational Linguistics (ACL), 2020. 1, 2\n[10] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing Mo-\nments in Video with Natural Language. In International\nConference on Computer Vision (ICCV), 2017. 1, 2\n[11] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.\nTALL: Temporal Activity Localization via Language Query.\nIn International Conference on Computer Vision (ICCV) ,\n2017. 1, 2\n[12] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang\nWang, and William Yang Wang. V ATEX: A Large-Scale,\nHigh-Quality Multilingual Dataset for Video-and-Language\nResearch. In International Conference on Computer Vision\n(ICCV), 2019. 1, 2\n[13] Luowei Zhou, Chenliang Xu, and Jason J. Corso. To-\nwards Automatic Learning of Procedures from Web Instruc-\ntional Videos. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2018. 1, 2, 5, 9, 11, 15\n[14] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen\nTran. Hierarchical Conditional Relation Networks for Video\nQuestion Answering. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2020. 1, 2\n[15] Linchao Zhu and Yi Yang. ActBERT: Learning Global-Local\nVideo-Text Representations. In Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 2020. 1, 2\n[16] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand Tapaswi, Ivan Laptev, and Josef Sivic.\nHowTo100M: Learning a Text-Video Embedding by\nWatching Hundred Million Narrated Video Clips. In Inter-\nnational Conference on Computer Vision (ICCV) , 2019. 1,\n2, 6, 9\n[17] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettlemoyer, and\nChristoph Feichtenhofer. VideoCLIP: Contrastive Pre-\ntraining for Zero-shot Video-Text Understanding. InConfer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 2021. 1, 6, 9\n[18] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg,\nMohit Bansal, and Jingjing Liu. Less is More: ClipBERT\nfor Video-and-Language Learning via Sparse Sampling. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 1, 2, 4, 6, 7, 8, 9\n[19] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\nMERLOT: Multimodal Neural Script Knowledge Models.\nIn Conference on Neural Information Processing Systems\n(NeurIPS), 2021. 1, 2, 4, 5, 6, 7, 8, 10, 11\n[20] Peng Wu, Xiangteng He, Mingqian Tang, Yiliang Lv, and\nJing Liu. HANet: Hierarchical Alignment Networks for\nVideo-Text Retrieval. InACM Multimedia (ACMMM), 2021.\n1\n[21] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nSpace-Time Attention All You Need for Video Understand-\ning? In International Conference on Machine Learning\n(ICML), 2021. 2\n[22] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video Swin Transformer. In\narXiv:2106.13230, 2021. 2, 3, 5\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics (NAACL), 2019. 2, 4\n[24] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: UNiversal Image-TExt Representation Learning.\nIn European Conference on Computer Vision (ECCV), 2020.\n2, 4, 7, 8\n[25] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-\nman. Frozen in Time: A Joint Video and Image Encoder for\nEnd-to-End Retrieval. In International Conference on Com-\nputer Vision (ICCV), 2021. 2, 4, 5, 6, 7, 8, 9, 10, 11\n[26] Aaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural Discrete Representation Learn-\ning. In Conference on Neural Information Processing\nSystems (NeurIPS), 2017. 2, 4\n[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-Shot Text-to-Image Generation. In arXiv:2102.12092,\n2021. 2, 4, 5\n[28] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen,\nRohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang,\nWilliam Yang Wang, Tamara Lee Berg, Mohit Bansal,\nJingjing Liu, Lijuan Wang, and Zicheng Liu. V ALUE:\nA Multi-Task Benchmark for Video-and-Language Under-\nstanding Evaluation. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2021. 2\n[29] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zis-\nserman. Use What You Have: Video Retrieval Using Repre-\nsentations From Collaborative Experts. In British Machine\nVision Conference (BMVC), 2020. 2\n[30] Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and\nYue Gao. Divide and Conquer: Question-Guided Spatio-\nTemporal Contextual Attention for Video Question Answer-\ning. In AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\n2020. 2\n[31] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid. Multi-modal Transformer for Video Retrieval. In\nEuropean Conference on Computer Vision (ECCV), 2020. 2,\n6\n[32] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian\nMetze, Alexander Hauptmann, Joao Henriques, and Andrea\nVedaldi. Support-set bottlenecks for video-text representa-\ntion learning. In International Conference for Learning Rep-\nresentations (ICLR), 2021. 2, 6\n[33] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia.\nMotion-Appearance Co-Memory Networks for Video Ques-\ntion Answering. In Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2018. 2\n[34] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-Modal and\nHierarchical Modeling of Video and Text. InEuropean Con-\nference on Computer Vision (ECCV), 2018. 2\n[35] Jie Lei, Tamara L Berg, and Mohit Bansal. QVHighlights:\nDetecting Moments and Highlights in Videos via Natural\nLanguage Queries. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2021. 2\n[36] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang,\nChi Zhang, and Heng Huang. Heterogeneous Memory En-\nhanced Multimodal Attention Model for Video Question An-\nswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 2\n[37] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The Kinetics Human Action Video\nDataset. In arXiv:1705.06950, 2017. 2, 5\n[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal Seg-\nment Networks: Towards Good Practices for Deep Action\nRecognition. In European Conference on Computer Vision\n(ECCV), 2016. 2\n[39] Joao Carreira and Andrew Zisserman. Quo Vadis, Action\nRecognition? A New Model and the Kinetics Dataset. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 2\n[40] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking Spatiotemporal Feature Learn-\ning: Speed-Accuracy Trade-offs in Video Classiﬁcation. In\nEuropean Conference on Computer Vision (ECCV), 2018. 2\n[41] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. SlowFast Networks for Video Recognition.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019. 2, 4\n[42] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and\nLi Fei-Fei. ImageNet: a Large-Scale Hierarchical Image\nDatabase. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009. 2\n[43] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2016. 2, 7\n[44] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nFei-Fei Li. Visual Genome: Connecting Language and Vi-\nsion Using Crowdsourced Dense Image Annotations. In In-\nternational Journal of Computer Vision (IJCV), 2017. 2, 8\n[45] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-Up and Top-Down Attention for Image Captioning\nand Visual Question Answering. InConference on Computer\nVision and Pattern Recognition (CVPR), 2018. 2, 4\n[46] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. VideoBERT: A Joint Model for Video and\nLanguage Representation Learning. In International Confer-\nence on Computer Vision (ICCV), 2019. 2\n[47] Andrew Rouditchenko, Angie Boggust, David Harwath,\nBrian Chen, Dhiraj Joshi, Samuel Thomas, Kartik Au-\ndhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio Feris,\nBrian Kingsbury, Michael Picheny, Antonio Torralba, and\nJames Glass. A VLnet: Learning Audio-Visual Language\nRepresentations from Instructional Videos. In INTER-\nSPEECH, 2021. 2, 6\n[48] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui\nDing, and Zhongyuan Wang. HiT: Hierarchical Trans-\nformer with Momentum Contrast for Video-Text Retrieval.\nIn arXiv:2103.15049, 2021. 2\n[49] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. End-to-End\nLearning of Visual Representations from Uncurated Instruc-\ntional Videos. In Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2020. 2, 6\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention Is All You Need. In Conference on\nNeural Information Processing Systems (NeurIPS), 2017. 2\n[51] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. RoBERTa: A Robustly Op-\ntimized BERT Pretraining Approach. In arXiv:1907.11692,\n2019. 2\n[52] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V . Le. XLNet: General-\nized Autoregressive Pretraining for Language Understand-\ning. In Conference on Neural Information Processing Sys-\ntems (NeurIPS), 2019. 2\n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the Limits of Transfer Learning with\na Uniﬁed Text-to-Text Transformer. In arXiv:1910.10683,\n2020. 2\n[54] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. ALBERT: A Lite\nBERT for Self-supervised Learning of Language Represen-\ntations. In International Conference for Learning Represen-\ntations (ICLR), 2020. 2\n[55] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-\npher D Manning. ELECTRA: Pre-training Text Encoders\nas Discriminators Rather Than Generators. In International\nConference for Learning Representations (ICLR), 2020. 2\n[56] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-\nand-Language Transformer Without Convolution or Region\nSupervision. In International Conference on Machine Learn-\ning (ICML), 2021. 2\n[57] Karan Desai and Justin Johnson. VirTex: Learning Visual\nRepresentations from Textual Annotations. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n2\n[58] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 12-in-1: Multi-Task Vision and Lan-\nguage Representation Learning. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 2\n[59] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. In International Confer-\nence for Learning Representations (ICLR), 2020. 2, 5\n[60] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J. Corso, and Jianfeng Gao. Uniﬁed Vision-Language\nPre-Training for Image Captioning and VQA. In AAAI Con-\nference on Artiﬁcial Intelligence (AAAI), 2020. 2\n[61] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang,\nand Ming Zhou. Unicoder-VL: A Universal Encoder for Vi-\nsion and Language by Cross-modal Pre-training. In AAAI\nConference on Artiﬁcial Intelligence (AAAI), 2020. 2\n[62] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. VisualBERT: A Simple and Performant\nBaseline for Vision and Language. In arXiv:1908.03557,\n2019. 2\n[63] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\nand Jingjing Liu. Large-Scale Adversarial Training for\nVision-and-Language Representation Learning. In Confer-\nence on Neural Information Processing Systems (NeurIPS) ,\n2020. 2\n[64] Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei\nFang, and Jingjing Liu. LightningDOT: Pre-training Visual-\nSemantic Embeddings for Real-Time Image-Text Retrieval.\nIn Conference of the North American Chapter of the Associ-\nation for Computational Linguistics (NAACL), 2021. 2\n[65] Mingyang Zhou, Luowei Zhou, Shuohang Wang, Yu Cheng,\nLinjie Li, Zhou Yu, and Jingjing Liu. UC2: Universal Cross-\nlingual Cross-modal Vision-and-Language Pre-training. In\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 2\n[66] Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang,\nand Nojun Kwak. Self-supervised Pre-training and Con-\ntrastive Representation Learning for Multiple-choice Video\nQA. In AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\n2021. 2\n[67] Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta\nNakashima, and Haruo Takemura. BERT Representations\nfor Video Question Answering. InWinter Conference on Ap-\nplications of Computer Vision (WACV), 2020. 2\n[68] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Just Ask: Learning to Answer Questions\nfrom Millions of Narrated Videos. In International Confer-\nence on Computer Vision (ICCV), 2021. 2, 6\n[69] Jason Tyler Rolfe. Discrete Variational Autoencoders.\nIn International Conference for Learning Representations\n(ICLR), 2017. 2\n[70] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT Pre-\nTraining of Image Transformers. In arXiv:2106.08254,\n2021. 2, 5\n[71] Hao Tan, Jie Lei, Thomas Wolf, and Mohit Bansal. VIM-\nPAC: Video Pre-Training via Masked Token Prediction and\nContrastive Learning. In arXiv:2106.11250, 2021. 2, 5\n[72] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at\nScale. In International Conference for Learning Represen-\ntations (ICLR), 2021. 3\n[73] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim Krikun,\nYuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,\nApurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz\nKaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto\nKazawa, Keith Stevens, George Kurian, Nishant Patil, Wei\nWang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\nDean. Google’s Neural Machine Translation System: Bridg-\ning the Gap between Human and Machine Translation. In\narXiv:1609.08144, 2016. 3\n[74] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A Cleaned, Hypernymed, Im-\nage Alt-text Dataset For Automatic Image Captioning. In\nAnnual Meeting of the Association for Computational Lin-\nguistics (ACL), 2018. 4, 5, 8, 11\n[75] Jianwei Yang, Yonatan Bisk, and Jianfeng Gao. TACo:\nToken-aware Cascade Contrastive Learning for Video-Text\nAlignmentl. In International Conference on Computer Vi-\nsion (ICCV), 2021. 6\n[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,\nNan Duan, and Tianrui Li. CLIP4Clip: An Empirical\nStudy of CLIP for End to End Video Clip Retrieval. In\narXiv:2104.08860, 2021. 6\n[77] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\nV ATT: Transformers for Multimodal Self-Supervised Learn-\ning from Raw Video, Audio and Text. In arXiv:2104.11178,\n2021. 6\n[78] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Vi-\nsual Models From Natural Language Supervision. In\narXiv:2103.00020, 2021. 6\n[79] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing Mo-\nments in Video with Natural Language. In International\nConference on Computer Vision (ICCV), 2017. 5, 9, 11, 13\n[80] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A Joint Se-\nquence Fusion Model for Video Question Answering and\nRetrieval. In European Conference on Computer Vision\n(ECCV), 2018. 5\n[81] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning\nLanguage-Visual Embedding for Movie Understanding with\nNatural-Language. In arXiv:1609.08124, 2016. 5, 9, 10\n[82] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K ¨opf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An Im-\nperative Style, High-Performance Deep Learning Library.\nIn Conference on Neural Information Processing Systems\n(NeurIPS), 2019. 5\n[83] Ilya Loshchilov and Frank Hutter. Decoupled Weight De-\ncay Regularization. In International Conference for Learn-\ning Representations (ICLR), 2019. 5, 9\n[84] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImageNet Classiﬁcation with Deep Convolutional Neural\nNetworks. In Conference on Neural Information Processing\nSystems (NeurIPS), 2012. 7\n[85] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. Exploring the Limits of Language\nModeling. In arXiv:1602.02410, 2016. 7\n[86] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\ntam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick.\nMicrosoft COCO Captions: Data Collection and Evaluation\nServer. In arXiv:1504.00325, 2015. 8\n[87] Microsoft Azure. https://azure.microsoft.com/.\n9\n[88] David L. Chen and William B. Dolan. Collecting Highly Par-\nallel Data for Paraphrase Evaluation. In Annual Meetings of\nthe Association for Computational Linguistics (ACL) , 2011.\n9, 10\n[89] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nFrom Recognition to Cognition: Visual Commonsense Rea-\nsoning. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8159181475639343
    },
    {
      "name": "Security token",
      "score": 0.6595858931541443
    },
    {
      "name": "Transformer",
      "score": 0.5598228573799133
    },
    {
      "name": "Video processing",
      "score": 0.4983201026916504
    },
    {
      "name": "End-to-end principle",
      "score": 0.4979400634765625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4600832462310791
    },
    {
      "name": "Pooling",
      "score": 0.4521487057209015
    },
    {
      "name": "Language model",
      "score": 0.4503704905509949
    },
    {
      "name": "Computer vision",
      "score": 0.39529284834861755
    },
    {
      "name": "Speech recognition",
      "score": 0.3604074716567993
    },
    {
      "name": "Computer network",
      "score": 0.10152798891067505
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 89
}