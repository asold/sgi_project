{
  "title": "Multi-Scale Group Transformer for Long Sequence Modeling in Speech Separation",
  "url": "https://openalex.org/W3034398443",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2102282813",
      "name": "Yucheng Zhao",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2124221993",
      "name": "Chong Luo",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4221504550",
      "name": "Zheng-Jun Zha",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2134447754",
      "name": "Wenjun Zeng",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964058413",
    "https://openalex.org/W2962866211",
    "https://openalex.org/W2962905190",
    "https://openalex.org/W2221409856",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963317762",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W2792764867",
    "https://openalex.org/W1552314771",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2734774145",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2996969697"
  ],
  "abstract": "In this paper, we introduce Transformer to the time-domain methods for single-channel speech separation. Transformer has the potential to boost speech separation performance because of its strong sequence modeling capability. However, its computational complexity, which grows quadratically with the sequence length, has made it largely inapplicable to speech applications. To tackle this issue, we propose a novel variation of Transformer, named multi-scale group Transformer (MSGT). The key ideas are group self-attention, which significantly reduces the complexity, and multi-scale fusion, which retains Transform's ability to capture long-term dependency. We implement two versions of MSGT with different complexities, and apply them to a well-known time-domain speech separation method called Conv-TasNet. By simply replacing the original temporal convolutional network (TCN) with MSGT, our approach called MSGT-TasNet achieves a large gain over Conv-TasNet on both WSJ0-2mix and WHAM! benchmarks. Without bells and whistles, the performance of MSGT-TasNet is already on par with the SOTA methods.",
  "full_text": "Multi-Scale Group Transformer for Long Sequence Modeling\nin Speech Separation\nYucheng Zhao\u00031 , Chong Luo2 , Zheng-Jun Zhay1 and Wenjun Zeng2\n1University of Science and Technology of China\n2Microsoft Research Asia\nlnc@mail.ustc.edu.cn, cluo@microsoft.com, zhazj@ustc.edu.cn, wezeng@microsoft.com\nAbstract\nIn this paper, we introduce Transformer to the time-\ndomain methods for single-channel speech separa-\ntion. Transformer has the potential to boost speech\nseparation performance because of its strong se-\nquence modeling capability. However, its computa-\ntional complexity, which grows quadratically with\nthe sequence length, has made it largely inapplica-\nble to speech applications. To tackle this issue, we\npropose a novel variation of Transformer, named\nmulti-scale group Transformer (MSGT). The key\nideas are group self-attention, which signiﬁcantly\nreduces the complexity, and multi-scale fusion,\nwhich retains Transform’s ability to capture long-\nterm dependency. We implement two versions of\nMSGT with different complexities, and apply them\nto a well-known time-domain speech separation\nmethod called Conv-TasNet. By simply replac-\ning the original temporal convolutional network\n(TCN) with MSGT, our approach called MSGT-\nTasNet achieves a large gain over Conv-TasNet on\nboth WSJ0-2mix and WHAM! benchmarks. With-\nout bells and whistles, the performance of MSGT-\nTasNet is already on par with the SOTA methods.\n1 Introduction\nSpeech separation is a fundamental task in acoustic signal\nprocessing with a wide range of applications [Wang and\nChen, 2018 ]. The goal of speech separation is to separate\ntarget speech from interfering speech, non-speech noise, or\nboth. Thanks to the success of deep learning, the perfor-\nmance of single-channel speech separation system have been\ndramatically improved in recent years [Hershey et al., 2016;\nKolbæk et al., 2017]. In particular, a new category of speech\nseparation methods called time-domain methods [Luo and\nMesgarani, 2018; Luo and Mesgarani, 2019] begin to emerge.\nThese methods take the sampled data points from raw wave-\nform as input, use a learnable neural network layer as en-\ncoder, and adopt a sequence modeling tool for feature sepa-\nration before the separated feature is converted back to time\n\u0003This work is done when Yucheng Zhao is an intern in MSRA\nyCorresponding author\nFigure 1: A schematic diagram of the proposed MSGT with three\nscales. The cylinders represent group Transformers.\ndomain by a learnable decoder. The time-domain methods\nhave been shown to surpass ideal time-frequency (T-F) mag-\nnitude masking methods for speech separation.\nRecently, several investigations[Heitkaemper et al., 2019]\nreveal that the performance of time-domain methods is highly\ndependent on using short frame length in the encoder, which\nproduces extremely long sequence. For example, the frame\nlength used in Conv-TasNet [Luo and Mesgarani, 2019 ] is\n2ms, producing 4000 frames for a 4-second audio segment\nwith 1ms overlap. Such a long sequence is difﬁcult to model\nusing the conventional recurrent neural network (RNN) or\ntemporal convolutional network (TCN) [Bai et al., 2018] as\nboth of them have a long path before connecting all positions\n[Vaswani et al., 2017].\nTransformer [Vaswani et al., 2017] have recently shown\nits strong capability for sequence modeling in many natu-\nral language processing tasks [Devlin et al., 2018]. Com-\npared to RNN or TCN, Transformer uses self-attention (SA)\nto compute correlations between any input positions in par-\nallel, which can effectively capture global dependencies in\naddition to the local dependencies. Introducing Transformer\nto speech separation has the potential to model the long-range\ndependencies displayed in speech signals. But the main ob-\nstacle is the complexity. The computational complexity of\nTransformer grows quadratically with the sequence length,\nwhich is not affordable when the sequence length is on the\norder of thousands.\nIn order to solve this issue, we propose a novel architecture\ncalled multi-scale group Transformer (MSGT). We divide the\ninput sequence into groups and use group self-attention to\ncalculate correlations within each group. The complexity re-\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3251\nFigure 2: A schematic diagram of a deep-learning-based speech sep-\naration system.\nmains tractable as long as the group size is not too large. As\nwe down-sample the features, the group size in group Trans-\nformer is kept constant, so correlations in a longer range can\nbe captured. On the lowest scale, all data points are contained\nin a single group. As such, both local and global dependen-\ncies are captured and retained. Besides, this structure pays\nmore attention to local dependencies, which aligns with the\nphysical characteristics of acoustic signals.\nWe implement two versions of MSGT with different com-\nplexities and apply them to the Conv-TasNet speech sepa-\nration system. In particular, we replace TCN with MSGT\nfor sequence modeling. Experimental results show that the\nMSGT-based system, which we call MSGT-TasNet, achieves\nsigniﬁcant performance gain over Conv-TasNet in both noise-\nfree speech separation on WSJ0-2mix dataset [Hershey et\nal., 2016 ] and noisy speech separation on WHAM! dataset\n[Wichern et al., 2019].\nTo summarize, our work makes three main contributions:\n(1) To the best of our knowledge, we are the ﬁrst to inves-\ntigate Transformer for sequence modeling in speech separa-\ntion. (2) We propose multi-scale group Transformer which\nreduces the complexity of the standard Transformer without\nlosing its capability to model global dependencies. (3) We\nimplement two versions of MSGT and use them to build a\nspeech separation system MSGT-TasNet. Without bells and\nwhistles, the performance of MSGT-TasNet is already on par\nwith the state-of-the-art (SOTA) methods.\n2 Related Work\nIn this section, we provide the background of the speech sep-\naration task and brieﬂy review several variations of Trans-\nformer for long sequence modeling.\n2.1 Speech Separation\nThe goal of speech separation is to separate target speech\nfrom interference including interfering speech, non-speech\nnoise, or both. As shown in Fig.2, a deep learning-based\nspeech separation system is composed of three modules. Be-\nfore the pioneering work named TasNet[Luo and Mesgarani,\n2018], researchers have been using ﬁxed transformation, such\nas STFT and ISTFT, for the encoder and the decoder[Hershey\net al., 2016; Kolbæket al., 2017; Shi et al., 2018]. The sep-\naration is conducted on the two-dimensional time-frequency\nfeatures. However, the success of TasNet has ignited the in-\nterest of time-domain approaches, which directly take data\nsamples from time-domain raw waveform as input, and con-\nduct separation in a latent one-dimensional domain.\nOne typical feature of time-domain approaches is that\ntheir success rely on the effectiveness in modeling long\nsequences [Heitkaemper et al., 2019; Luo and Mesgarani,\n2019]. For Conv-TasNet, it is discovered that shorter frame\nlength achieves better performance than longer ones. There-\nfore, a frame length of 2ms is suggested for high perfor-\nmance. This length is an order of magnitude smaller than the\nlength used by conventional encoders. Given a ﬁxed-length\naudio input, such a short frame length also produces an ex-\ntremely long sequence, which is difﬁcult to model.\nIn Conv-TasNet [Luo and Mesgarani, 2019 ], the authors\nused a temporal convolution network to model long-range\ndependencies. However, it has been shown [Vaswani et al.,\n2017] that convolution is not as efﬁcient as Transformer in\nsequence modeling, as the latter is capable of capturing both\nlocal and global dependencies. This has been the motivation\nof our work. More recently, FurcaNeXt [Zhang et al., 2020]\nintroduced gated activation and ensemble learning into the\nframework to improve performance. However, they are still\nusing TCN for sequence modeling.\n2.2 Transformer\nTransformer is a strong sequence modeling tool but it is not\nreadily applicable to long sequences due to its quadratically\ngrowing computational complexity with the sequence length.\nSeveral pieces of work [Dai et al., 2019; Liu and Lapata,\n2019; Shen et al., 2018; Miculicich et al., 2018] in natural\nlanguage processing area have tried to tackle this limitation,\nso that Transformer can be applied to document-level tasks\nor having a higher efﬁciency. The main idea is to divide the\nsequence into a few conceptually meaningful sets or blocks,\nsuch as sentences and paragraphs, and then adopt a hierar-\nchical architecture to explore the intra-set and inter-set cor-\nrelations. To be more speciﬁc, local dependencies are cap-\ntured within each set, and global dependencies are computed\nthrough cross-set attention.\n[Shen et al., 2018 ] presents a bi-directional block self-\nattention network (SAN) that divides a sequence into blocks\nand sequentially applies intra-block SAN to each block and\ninter-block SAN across blocks. [Miculicich et al., 2018] uses\na hierarchical attention network structure for document-level\nmachine translation and [Liu and Lapata, 2019 ] uses a hi-\nerarchical Transformer for multi-document summarization.\nTransformer-XL [Dai et al., 2019] introduces a segment-level\nrecurrence mechanism that enables Transformer to learn de-\npendencies beyond a ﬁxed length.\nThe main difference between these hierarchical SAN and\nour work is that we do not calculate set-level dependencies.\nNLP tasks involve semantic units such as sentence, para-\ngraph, and document. However, speech separation is a low-\nlevel task without multiple semantic units. For such input\ndata, our model uses multi-scale fusion architecture to calcu-\nlate element-level dependencies at different resolutions.\nThere is also a work called Sparse Transformer [Child et\nal., 2019] for long sequence generation. They factorized full\nattention matrix by some sparse attention matrices to reduce\nthe complexity. Compared to our model, Sparse Transformer\nrelies on a highly optimized sparse matrix implementation\nand the complexity of it is O(np\nn), which is higher than\nours for long sequences.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3252\n(a) Dense Fusion\n (b) Light Fusion\nFigure 3: Two implementations of MSGT. Dense-fusion MSGT employs multiple group Transformers (GT) on each scale while light-fusion\nMSGT only uses one GT on each scale.\n3 Multi-Scale Group Transformer\nWe intend to design a long-sequence modeling tool based on\nTransformer. There are two design objectives. First, we shall\nreduce the computational complexity of Transformer so that\nit can scale with sequence length. Second, we shall maintain\nTransformer’s ability to model both short-term and long-term\ndependencies.\n3.1 The Proposed Architecture\nFig. 1 presents the architecture of the proposed multi-scale\ngroup Transformer (MSGT). The key innovations are the\ngroup self-attention and the multi-scale fusion.\nIn the original Transformer design, correlations are com-\nputed between any two positions in the input sequence. This\nbrings quadratic complexity with respect to the sequence\nlength. In contrast, the propose group self-attention restricts\nthe correlation computation within local regions. When the\ngroup size is ﬁxed at a constant, the number of groups grows\nlinearly with the sequence length, so does the computational\ncomplexity of group self-attention.\nHowever, group self-attention (GSA) does not consider\ncorrelations across groups, losing the capability to capture\nglobal dependencies. To retain this important capability and\nto balance the computation resources allocated between lo-\ncal and global dependencies, we propose the multi-scale ar-\nchitecture. On the high-resolution (large) scale, GSA cap-\ntures local dependencies, while on the low-resolution (small)\nscale, GSA captures long-range dependencies. As sequences\nin small scales are several times shorter than sequences in\nlarge scales, more computation resources are thus allocated\nto capture local dependencies. This is consistent with the in-\ntuition that local correlations are stronger and more important\nthan global correlations in audio signals.\nWe implement MSGT using two basic modules: opera-\ntion module and transition module. The operation module\nconducts multi-scale feature transformation. It may conduct\ndifferent transformations, including group Transformer and\nidentity mapping, on different scales. The transition module\nhandles feature resizing and feature fusion.\nGroup Self-Attention\nGiven an input sequence X 2 Rn\u0002d where d is the fea-\nture dimension and n is the sequence length, GSA ﬁrst di-\nvides the sequence into sequal-sized non-overlapped groups\nFigure 4: Illustration of feature fusion by transition modules. The\nlarge, medium, and small scales are shown from left to right.\nwith group size g. Then, standard self-attention is calculated\nwithin each group and the output of each group is concate-\nnated. We formulate the above process as follows:\nGroup(X) =fX1;X2;:::;X sg;Xi 2Rg\u0002d (1)\nGSA(X) =fSA(X1);SA(X2);:::; SA(Xs)g (2)\nThe standard self-attention, which proposed in[Vaswaniet\nal., 2017] is computed as follows:\nQ= XWQ;K = XWK;V = XWV (3)\nSA(X) =softmax(QKT\np\nd\n)V (4)\nwhere\np\ndis the scaling factor, WQ;WK;WV 2Rd\u0002d are\nparameter matrices of different linear transformations.\nFor simplicity, here we only describe the formula of single-\nhead group self-attention. The extension to multi-head group\nself-attention is straightforward.\nMulti-Scale Fusion\nMulti-scale fusion is conducted by the transition module. It\nexchanges information across multi-scale features. Let us\ntake 3 scales as a example, which is illustrated in Figure 4.\nFor the largest scale, we ﬁrst conduct up-sampling on two\nsmaller scales. Note we gradually up-sample the smallest\nscale by using two sequential up-sampling networks. Then,\nwe have three features with the same size and use addition to\nfuse them. For the medium scale, we up-sample the smallest\nscale, down-sample the largest scale and then add them to-\ngether. The operation on the smallest scale is similar. Note\nthat we do not have to fuse all scales in the transition module.\nIt is a design choice how many scales are fused. We imple-\nment two versions of MSGT with different complexities.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3253\nLayer Type Complexity per Layer Sequential Operations Maximum Path Length\nSelf-Attention O(n2 \u0001d) O(1) O(1)\nRecurrent O(n\u0001d2) O(n) O(n)\nConvolutional O(k\u0001n\u0001d2) O(1) O(logk(n))\nMSG Self-Attention (Light) O(g\u0001n\u0001d) O(1) O(log2(n=g))\nMSG Self-Attention (Dense) O(g\u0001n\u0001d\u0001log (n=g)) O(1) O(log2(n=g))\nTable 1: Per-layer complexity, minimum number of sequential operations and maximum path lengths for different layer types. The deﬁnition\nof n; dand g follows Section 3.1 and k is the convolution kernel size. The complexity per layer for MSG self-attention is the total complexity\ndivided by the number of layers as different scales have different complexities. Part of this table is adopted from [Vaswani et al., 2017].\n3.2 Two Implementations of MSGT\nWe present two concrete design instances of MSGT, namely\ndense-fusion MSGT and light-fusion MSGT, in Fig. 3. The\nfusion structures of these two implementations are inspired\nby the HRNet [Sun et al., 2019] and the U-Net [Ronneberger\net al., 2015] in the computer vision area.\nDense-Fusion MSGT\nThe architecture of dense-fusion MSGT is illustrated in Fig.\n3a. With dense fusion, multiple group Transformers are ap-\nplied on each scale. For simplicity, we fuse only the adjacent\nscale features in the transition module. In other words, an\ninput to the group Transformer is fused from no more than\nthree scales. The dense fusion has log-linear complexity with\nsequence length.\nLight-Fusion MSGT\nAs illustrated in Fig. 3b, the light fusion version only use one\nGT on each scale, although each GT may contain multiple\nlayers of transformation. The transition module does not do\nfusion as the features scale down. When the features scale\nup, only features from two adjacent scales are fused together.\nThe light fusion has linear complexity with sequence length.\n3.3 Analysis\nThe sequence modeling problem can be formulated as map-\nping one sequence fx1;x2;:::;x ngto another sequence of\nequal length fz1;z2;:::;z ng, where xi;zi 2Rd denote the\nfeature vector of a symbol in the sequence.\nThere are three desired properties for a sequence modeling\ntool: 1) Parallel computing; 2) Capability to capture long-\nrange dependencies; 3) Low-order complexity which allows\nit to scale to long sequences. The original Transformer paper\n[Vaswani et al., 2017] introduced three metrics to measure\nthese desired properties. They are complexity per layer, min-\nimum number of sequential operation required and the maxi-\nmum path length between any two input and output position.\nTable 1 provides a comparison of the tree metrics between\nthe proposed MSGT and other widely used sequence mod-\neling tools. A standard self-attention layer has complexity\nof O(n2 \u0001d), which is too high for long sequences. The\nmulti-scale group self-attention have maximum path length\nO(log2(n=g)) and complexity O(g \u0001n\u0001d) or O(g \u0001n\u0001d\u0001\nlog (n=g)) depending on the fusion choices. We will em-\npirically show this improvement in Section 5.4. For com-\npleteness, we also include recurrent and convolutional layers\nin this table, but they are inferior to self-attention which has\nbeen detailed in [Vaswani et al., 2017].\n4 MSGT for Speech Separation\nThe goal of speech separation is to separate target speech\nfrom a mixture signal. Following the convention, all the\nspeech signals appeared in the mixture are treated as target\nspeeches. The mixture can be denoted by a sequence y(t) in\nthe time domain. It can be decomposed as the summation of\nKspeech signals xk(t), and an additive noise n(t),\ny(t) =\nKX\nk=1\nxk(t) +n(t); (5)\nwhere tis the time index.\nRecent deep learning-based speech separation systems\nconsist of three main components, namely the encoder, the\ndecoder, and the separator. Fig.2 illustrates this framework.\nThe encoder processes time domain mixture signal y(t) by\nﬁrst dividing it into noverlapping frames yj 2R1\u0002L (j =\n1:::n), where Ldenotes frame length, and j is the frame in-\ndex. Then, each frame is transformed into d-dimensional fea-\ntures Yj 2R1\u0002d. Features of all the nframes are concate-\nnated to form Y 2Rn\u0002d. The separator use some sequence\nmodeling tool to estimate multiplicative masks Mk 2Rn\u0002d\nfor each signal and then multiply it on encoded features Y,\nproducing ^Xk as separated features. Finally, the decoder\ntransform ^Xk back to time domain and output the separated\nsignals ^xk(t).\nIt is worth noting that it is not necessary for the separated\nsignals to have the same permutation as the ground-truth la-\nbel. Utterance-level permutation invariant training (uPIT)\n[Kolbæk et al., 2017] can be applied to address this problem.\nWe adopt the well-known speech separation framework\nConv-TasNet [Luo and Mesgarani, 2019 ] and replace the\noriginal TCN with the proposed MSGT in its separator.\nThe encoder and the decoder are kept the same. Our sys-\ntem is still a time-domain method, so we name it MSGT-\nTasNet. We train MSTG-TasNet using the time domain scale-\ninvariant signal-to-distortion ratio (SI-SDR) as the training\ntarget, which is deﬁned as:\nSI-SDR = 10 log10( j\u000bsj2\nj^s\u0000\u000bsj2 ): (6)\nwhere ^s 2RT and s 2RT are the estimated and original\nclean signals, respectively.\u000b= ^sT s=jsj2 is the scaling factor.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3254\nSeparation Method SI-SDRi\n(dB)\nSDRi\n(dB) PESQ\nPSM 16.4 16.7 3.98\nDPCL++ (2016) 10.8 - -\nuPIT (2017) - 10.0 -\nchimera++ (2018) 12.6 13.1 -\nSign Prediction Net (2019) 15.3 15.6 3.36\nDeep CASA (2019) 17.7 18.0 3.51\nConv-TasNet (2019) 15.3 15.6 3.24\nFurcaNeXt (2020) - 18.4 -\nMSGT-TasNet (Light) 16.8 17.1 3.35\nMSGT-TasNet (Dense) 17.0 17.3 3.30\nTable 2: Performance comparison on WSJ0-2mix. Methods are\ngrouped into ideal masks, T-F domain and time-domain methods.\n5 Experiments\n5.1 Dataset and Evaluation Metric\nWe use two datasets for evaluation. The ﬁrst is the widely-\nused WSJ0-2mix dataset [Hershey et al., 2016]. WSJ0-2mix\ncontains 30 hours of training data, 10 hours of validation data\nand 5 hours of testing data. The validation set contains utter-\nances of the same speakers as in the training set, while the\ntesting set contains utterances of different speakers. Each\nmixture is artiﬁcially generated at a random signal-to-noise\nratio (SNR) between -5 and 5 dB. The second is the recently\nproposed WHAM! dataset [Wichern et al., 2019], which is an\nextension of the original WSJ0-2mix. The WHAM! dataset\nconsists of two speaker mixtures from the WSJ0-2mix dataset\ncombined with real ambient noise samples. This is a more\nchallenging dataset compared to the noise-free WSJ0-2mix.\nWe use the scale-invariant signal-to-distortion ratio (SI-\nSDR) improvement [Le Roux et al., 2019 ] and signal-to-\ndistortion ratio (SDR) [Vincent et al., 2006] improvement as\nthe main evaluation metrics. SI-SDR is also referred to as SI-\nSNR in some work [Luo and Mesgarani, 2019]. We also re-\nport perceptual evaluation of subjective quality (PESQ) [Rix\net al., 2001] to evaluate the quality of the separated mixtures.\n5.2 Experiment Conﬁguration\nWe train all models for 1M steps on 4-second segments with\nsample rate of 8K Hz. We use Adam optimizer with warm-\nup. The learning rate is initialized to 0.0003 and is adjusted\naccording to the following formula:\nlr = init\nlr \u0001min(step\u00000:3;step \u0001warmup steps\u00001:3) (7)\nWe choose warmup steps = 10000. We also use dropout to\nrelieve over-ﬁtting.\nIn all experiments, we use frame length of 2 ms. We choose\ngroup size of 1000 for noise-free speech separation and group\nsize of 500 for noisy speech separation. Following the nota-\ntions in [Vaswani et al., 2017], the Transformer parameters\nare dff = 1024,dmodel = 512, andh= 8. In the light fusion,\nwe use 8 layers of transformation for the GT in the smallest\nscale and 2 layers for the GT in the other scales. In the dense\nfusion, we use 3 layers of transformation for GT in all the\nscales. The output feature dimension of the encoder is 1024.\nSeparation Method SI-SDRi (dB)\nIRM 12.8\nIBM 13.4\nPSM 16.8\nchimera++ 9.9\nConv-TasNet* 12.0\nMSGT-TasNet (Light) 12.3\nMSGT-TasNet (Dense) 13.1\nTable 3: Performance comparison on WHAM!. The three results on\ntop are performance of different ideal masks. Conv-TasNet* is our\nre-implementation.\n5.3 Comparison with Other Methods\nPreviously, research on speech separation is more focused\non separating target speech from interfering speech [Luo and\nMesgarani, 2019; Liu and Wang, 2019]. In this paper, we use\nthe term noise-free speech separationto denote this conven-\ntional setting while we use another termnoisy speech separa-\ntion to refer to the task of separating target speech from both\ninterfering speech and non-speech noise. The WSJ0-2mix\ndataset is used for the former task while the WHAM! dataset\nis used for the latter task.\nSpeech Separation Results on WSJ0-2mix\nTable 2 shows speech separation results on WSJ0-2mix.\nDPCL++ [Isik et al., 2016 ], uPIT [Kolbæk et al., 2017 ],\nchimera++ [Wang et al., 2018 ] and Sing Prediction Net\n[Wang et al., 2019] are time-frequency (T-F) domain meth-\nods. Conv-TasNet [Luo and Mesgarani, 2019 ], FurcaNeXt\n[Zhang et al., 2020] and our proposed MSG-TasNet are time-\ndomain methods. The ﬁst row gives the performance of ideal\nphase-sensitive mask (PSM), which is a reasonable upper\nbound for all T-F domain methods.\nOur proposed two versions of MSGT-TasNet outperform\nConv-TasNet by a large margin only by replacing TCN with\nMSGT, showing that MSGT is a better sequence modeling\nmethod for speech separation. The SI-SDRi performance of\nthe light version is 0.2dB lower than that of the dense version,\nbut it uses 2x fewer memory and runs 2.8x faster.\nFurcaNeXt and Deep CASA achieve slightly higher SDRi\nor SI-SDRi than MSGT-TasNet. But the innovations in\nthese two methods can also be applied to our framework and\ncan potentially improve the performance of MSGT-TasNet.\nSpeciﬁcally, FurcaNext adopts gated activation and Deep\nCASA optimizes frame-level separation and speaker tracking\nin turn. We plan to integrate these two innovations into our\nmodel in the future.\nNoisy Speech Separation Results on WHAM!\nThe WHAM! dataset is more realistic and challenging since\nit contains noise. As this is a new dataset and only baseline\nresult is available, we add several idea masks to the compari-\nson. The ideal masks provide performance upper bounds for\nmost T-F domain methods. Results are presented in Table 3.\nIn noisy speech separation, MSGT-TasNet (Light) and\nMSGT-TasNet (Dense) get 0.3 dB and 1.1 dB SI-SDRi gain\nover Conv-TasNet, respectively. This gain is smaller than\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3255\n1;000 2; 000 3; 000 4; 000\n1\n2\n3\n4\n5\n6\n7\n8\nSequence Length\nGPU Mermory\n(GB)\na) GPU\nMemory Cost vs Sequence Length\nTransformer\nMSGT(D,1000)\nMSGT(D,500)\nMSGT(D,250)\nMSGT(L,1000)\nMSGT(L,500)\nMSGT(L,250)\n1;000 2; 000 3; 000 4; 000\n500\n1;000\n1;500\n2;000\n2;500\n3;000\nSequence Length\nThroughput (samples/min)\nb) Throughput\nvs Sequence Length\nTransformer\nMSGT(D,1000)\nMSGT(D,500)\nMSGT(D,250)\nMSGT(L,1000)\nMSGT(L,500)\nMSGT(L,250)\nFigure 5:\nEmpirical computation cost of Transformer and MSGT. D\nand L stand for dense and light, respectively. The number follows\nindicates group size. One sample is a 4s audio segment.\nwhat we get in noise-free speech separation, but the advan-\ntage of the dense version to the light version becomes sig-\nniﬁcant. Both results are due to the fact that noisy speech\nseparation is a more difﬁcult task than its noise-free counter-\npart. Note that, in this difﬁcult task, MSGT-TasNet (Dense)\nis the ﬁrst method that surpasses the ideal T-F mask IRM.\n5.4 Efﬁciency for Long Sequence\nMSGT reduces the time/space complexity from O(n2 \u0001d) to\nO(g\u0001n\u0001d) or O(g\u0001n\u0001d\u0001log (n=g)) in theory. We would like\nto compare the empirical GPU memory cost and throughput\nfor speech separation under the same conﬁguration. We use\n4s audio segment and choose different frame lengths to con-\ntrol the actual sequence length used in group Transformer.\nWe choose group size g as a hyper-parameter. For different\nsequence lengths, we keep the total number of group Trans-\nformers the same for light fusion and we keep the number\nof group Transformer in the largest scale the same for dense\nfusion. This constraint yields models with similar size for\ndifferent sequence length.\nAs Fig.5a shows, the GPU memory usage in MSGT is sig-\nniﬁcantly smaller than in Transformer, and it grows slower\nwith the increase of sequence length. Under the same group\nsize, the light version requires less GPU memory than the\ndense version. Within the same fusion model, the GPU mem-\nory cost grows with the group sizes g.\nFig.5b presents the throughput of different models. The\nnumbers are acquired on a single P100 GPU. MSGT (Dense)\nhas slightly lower throughput than Transformer for short se-\nquence because more scales are involved in computation.\nMSGT (Light) consistently achieves higher throughput than\nTransformer.\n5.5 Ablation Study\nAll ablation studies are carried out on the WSJ0-2mix speech\nseparation dataset.\nComparison of different sequence modeling tools. As\nshown in Table 4, using group Transformer without multi-\nscale fusion signiﬁcantly degrades the performance. The per-\nType SI-SDRi\n(dB)\nThroughput\n(samples/min)\nTCN 15.3 276\nSingle-scale GT 13.5 528\nMSGT (Light) 16.8 1066\nMSGT (Dense) 17.0 375\nTable 4: Comparison of different sequence modeling tools when\ngroup size is 1000.\n250 500 1000\n15\n16\n17\n18\n15:4 15:2\n16:816:3 16:8 17\nSI-SDRi (dB) Light Dense\nFigure 6: Inﬂuence of group size for MSGT-TasNet (Light) and\nMSGT-TasNet (Dense).\nformance of single-scale GT, which has only one scale and\nuses the same number of layers as MSGT (Light), is even\nworse than TCN. Using light fusion, we can achieve almost\ndoubled throughput and 3.3dB SI-SDRi gain over the single-\nscale version. We would also like to mention that MSGT\n(Light) achieve 1.5 dB gain over TCN with 3.9X speedup.\nInﬂuence of group size. The hyper-parameter g in group\nTransformer controls how many frames should be consider\nas a group. As shown in Fig.6, the largest group size achieves\nthe highest SI-SDRi for both fusion versions. In particular,\nlarge group size is essential for the light version to get good\nperformance, which may due to its inadequacy in exploring\nlong-range dependencies.\n6 Conclusion and Future Work\nIn this paper, we present the design and implementation\nof multi-scale group Transformer for long sequence mod-\neling. Through group self-attention and multi-scale fusion,\nMSGT signiﬁcantly reduces the computational complexity\nof Transformer without affecting its performance. Two ver-\nsions of MSGT with different complexities are implemented\nand applied in a well-known speech separation framework\ncalled Conv-TasNet. Experiment results show the proposed\nMSGT-TasNet achieves a large gain over Conv-TasNet on\nboth WSJ0-2min and WHAM! benchmarks. For the noisy\nspeech separation task, MSGT-TasNet is the ﬁrst approach\nsurpassing the performance of the ideal T-F mask.\nIn the future, we plan to apply neural architecture search\n(NAS) [Liu et al., 2018 ] to ﬁnd better fusion choices of\nMSGT. Besides, we plan to integrate the innovations of Deep\nCASA into MSGT-TasNet. We believe that such combination\ncan further advance the state-of-the-art of speech separation.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3256\nReferences\n[Bai et al., 2018] Shaojie Bai, J Zico Kolter, and Vladlen\nKoltun. An empirical evaluation of generic convolutional\nand recurrent networks for sequence modeling. arXiv\npreprint arXiv:1803.01271, 2018.\n[Child et al., 2019] Rewon Child, Scott Gray, Alec Radford,\nand Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang,\nWilliam W Cohen, Jaime Carbonell, Quoc V Le, and Rus-\nlan Salakhutdinov. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Heitkaemper et al., 2019] Jens Heitkaemper, Darius\nJakobeit, Christoph Boeddeker, Lukas Drude, and Rein-\nhold Haeb-Umbach. Demystifying tasnet: A dissecting\napproach. arXiv preprint arXiv:1911.08895, 2019.\n[Hershey et al., 2016] John R Hershey, Zhuo Chen, Jonathan\nLe Roux, and Shinji Watanabe. Deep clustering: Discrim-\ninative embeddings for segmentation and separation. In\nICASSP, pages 31–35, 2016.\n[Isik et al., 2016] Yusuf Isik, Jonathan Le Roux, Zhuo Chen,\nShinji Watanabe, and John R Hershey. Single-channel\nmulti-speaker separation using deep clustering. Inter-\nspeech, pages 545–549, 2016.\n[Kolbæk et al., 2017] Morten Kolbæk, Dong Yu, Zheng-Hua\nTan, Jesper Jensen, Morten Kolbaek, Dong Yu, Zheng-\nHua Tan, and Jesper Jensen. Multitalker speech sepa-\nration with utterance-level permutation invariant training\nof deep recurrent neural networks. IEEE/ACM TASLP,\n25(10):1901–1913, 2017.\n[Le Roux et al., 2019] Jonathan Le Roux, Scott Wisdom,\nHakan Erdogan, and John R Hershey. Sdr–half-baked or\nwell done? In ICASSP, pages 626–630, 2019.\n[Liu and Lapata, 2019] Yang Liu and Mirella Lapata. Hier-\narchical transformers for multi-document summarization.\narXiv preprint arXiv:1905.13164, 2019.\n[Liu and Wang, 2019] Yuzhou Liu and DeLiang Wang. Di-\nvide and conquer: A deep casa approach to talker-\nindependent monaural speaker separation. arXiv preprint\narXiv:1904.11148, 2019.\n[Liu et al., 2018] Hanxiao Liu, Karen Simonyan, and Yim-\ning Yang. Darts: Differentiable architecture search. arXiv\npreprint arXiv:1806.09055, 2018.\n[Luo and Mesgarani, 2018] Yi Luo and Nima Mesgarani.\nTasnet: time-domain audio separation network for real-\ntime, single-channel speech separation. In ICASSP, pages\n696–700, 2018.\n[Luo and Mesgarani, 2019] Yi Luo and Nima Mesgarani.\nConv-tasnet: Surpassing ideal time–frequency magni-\ntude masking for speech separation. IEEE/ACM TASLP,\n27(8):1256–1266, 2019.\n[Miculicich et al., 2018] Lesly Miculicich, Dhananjay Ram,\nNikolaos Pappas, and James Henderson. Document-level\nneural machine translation with hierarchical attention net-\nworks. In EMNLP, pages 2947–2954, 2018.\n[Rix et al., 2001] Antony W Rix, John G Beerends,\nMichael P Hollier, and Andries P Hekstra. Perceptual\nevaluation of speech quality (pesq)-a new method for\nspeech quality assessment of telephone networks and\ncodecs. In ICASSP, pages 749–752, 2001.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In MICCAI, 2015.\n[Shen et al., 2018] Tao Shen, Tianyi Zhou, Guodong Long,\nJing Jiang, and Chengqi Zhang. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence model-\ning. In ICLR, 2018.\n[Shi et al., 2018] Jing Shi, Jiaming Xu, Guangcan Liu, and\nBo Xu. Listen, think and listen again: Capturing top-down\nauditory attention for speaker-independent speech separa-\ntion. In IJCAI, pages 4353–4360, 2018.\n[Sun et al., 2019] Ke Sun, Bin Xiao, Dong Liu, and Jing-\ndong Wang. Deep high-resolution representation learning\nfor human pose estimation. In CVPR, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Vincent et al., 2006] Emmanuel Vincent, R ´emi Gribonval,\nand C ´edric F ´evotte. Performance measurement in blind\naudio source separation. TASLP, 14(4):1462–1469, 2006.\n[Wang and Chen, 2018] DeLiang Wang and Jitong Chen.\nSupervised speech separation based on deep learning: An\noverview. IEEE/ACM TASLP, 26(10):1702–1726, 2018.\n[Wang et al., 2018] Zhong-Qiu Wang, Jonathan Le Roux,\nDeLiang Wang, and John R Hershey. End-to-end speech\nseparation with unfolded iterative phase reconstruction.\nInterspeech, 2018.\n[Wang et al., 2019] Zhong-Qiu Wang, Ke Tan, and DeLiang\nWang. Deep learning based phase reconstruction for\nspeaker separation: A trigonometric perspective. In\nICASSP, pages 71–75, 2019.\n[Wichern et al., 2019] Gordon Wichern, Joe Antognini,\nMichael Flynn, Licheng Richard Zhu, Emmett McQuinn,\nDwight Crow, Ethan Manilow, and Jonathan Le Roux.\nWham!: Extending speech separation to noisy environ-\nments. Interspeech, pages 1368–1372, 2019.\n[Zhang et al., 2020] Liwen Zhang, Ziqiang Shi, Jiqing Han,\nAnyan Shi, and Ding Ma. Furcanext: End-to-end monau-\nral speech separation with dynamic gated dilated temporal\nconvolutional networks. In MMM, pages 653–665, 2020.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n3257",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7239500880241394
    },
    {
      "name": "Computer science",
      "score": 0.6706758737564087
    },
    {
      "name": "Quadratic growth",
      "score": 0.5698659420013428
    },
    {
      "name": "Speech recognition",
      "score": 0.4935966730117798
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3729652166366577
    },
    {
      "name": "Algorithm",
      "score": 0.3550816774368286
    },
    {
      "name": "Voltage",
      "score": 0.1433860957622528
    },
    {
      "name": "Engineering",
      "score": 0.12414529919624329
    },
    {
      "name": "Electrical engineering",
      "score": 0.07649597525596619
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 13
}