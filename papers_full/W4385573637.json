{
  "title": "ClinicalT5: A Generative Language Model for Clinical Text",
  "url": "https://openalex.org/W4385573637",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2786691325",
      "name": "Qiuhao Lu",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A2040419331",
      "name": "Dejing Dou",
      "affiliations": [
        "University of Oregon",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2134526459",
      "name": "Thien Nguyen",
      "affiliations": [
        "University of Oregon"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W4285417484",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3133965333",
    "https://openalex.org/W3172427031",
    "https://openalex.org/W3211384762",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4223492536",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W3174821868",
    "https://openalex.org/W4206094367",
    "https://openalex.org/W2147726942",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W4200544893",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3136215575",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2997702627",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3167002470",
    "https://openalex.org/W2911489562"
  ],
  "abstract": "In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5436–5443\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nClinicalT5: A Generative Language Model for Clinical Text\nQiuhao Lu1, Dejing Dou1,2, and Thien Huu Nguyen1\n1Dept. of Computer Science, University of Oregon, Eugene, OR, USA\n2Baidu Research\n{luqh, dou, thien}@cs.uoregon.edu\nAbstract\nIn the past few years, large pre-trained language\nmodels (PLMs) have been widely adopted in\ndifferent areas and have made fundamental im-\nprovements over a variety of downstream tasks\nin natural language processing (NLP). Mean-\nwhile, domain-specific variants of PLMs are\nbeing proposed to address the needs of domains\nthat demonstrate a specific pattern of writing\nand vocabulary, e.g., BioBERT for the biomed-\nical domain and ClinicalBERT for the clinical\ndomain. Recently, generative language models\nlike BART and T5 are gaining popularity with\ntheir competitive performance on text genera-\ntion as well as on tasks cast as generative prob-\nlems. However, in the clinical domain, such\ndomain-specific generative variants are still un-\nderexplored. To address this need, our work\nintroduces a T5-based text-to-text transformer\nmodel pre-trained on clinical text, i.e., Clini-\ncalT5. We evaluate the proposed model both\nintrinsically and extrinsically over a diverse set\nof tasks across multiple datasets, and show that\nClinicalT5 dramatically outperforms T5 in the\ndomain-specific tasks and compares favorably\nwith its close baselines.1\n1 Introduction\nIn the past few years, large pre-trained language\nmodels (PLMs), such as BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), GPT-3 (Brown et al.,\n2020), BART (Lewis et al., 2020), T5 (Raffel et al.,\n2020), etc., have achieved great success over a\nvariety of downstream tasks in natural language\nprocessing (NLP). These PLMs mainly depend\non self-supervised pre-training on large amounts\nof general-domain textual data, e.g., Wikipedia,\nnews articles, web crawl corpus, etc., and are\nwidely adopted in downstream applications. De-\nspite the superior performance of these PLMs\non general-domain text, their performance over\ndomain-specific text is relatively poor (Ma et al.,\n1We will release the models upon decision of the paper.\n2019). To bridge this gap, researchers propose to\nbuild domain-specific PLMs through fine-tuning or\npre-training from scratch over domain corpora. For\nexample, in the biomedical and clinical domains,\nvarious domain-specific PLMs have been explored\nand released, including BioBERT (Lee et al., 2020),\nSciBERT (Beltagy et al., 2019), BlueBERT (Peng\net al., 2019), ClinicalBERT (Huang et al., 2019),\nBioClinicalBERT2 (Alsentzer et al., 2019), umls-\nBERT (Michalopoulos et al., 2020), diseaseBERT\n(He et al., 2020), SciFive (Phan et al., 2021), and\nBioBART (Yuan et al., 2022).\nDomain-specific language models have been ex-\ntensively explored in different kinds of NLP-related\ndownstream applications, ranging from entity link-\ning (Bhowmik et al., 2021) to document classifica-\ntion (Allada et al., 2021). Generally, a typical and\npopular usage of the aforementioned PLMs is to\nleverage them to encode domain text, the learned\nrepresentations of which are then fed into some\ntask-specific structures for label prediction. Tak-\ning a complicated real-world task as an example,\n(Huang et al., 2019) predicts patients’ risk of read-\nmission within 30 days after discharge using clini-\ncal notes in the Electronic Health Records (EHRs).\nEssentially, they encode discharge summaries of\npatients with ClinicalBERT, and put the learned em-\nbeddings of the [CLS] token to a linear layer on top\nfor prediction, leading to better performance than\ntraditional models. Moreover, (Lu et al., 2021c)\nconstructs a document-level multi-view graph out\nof each clinical note and predicts patients’ 30-day\nreadmission risk with a graph-based model, and\nthey use BioClinicalBERT (Alsentzer et al., 2019)\nas the encoder within the graph model.\nRecently, generative language models, e.g.,\nBART (Lewis et al., 2020) and T5 (Raffel et al.,\n2020), have attracted attention since they are nat-\nurally effective for natural language generation\ntasks, such as document summarization (Chen and\n2Also known as ClinicalBERT.\n5436\nYang, 2021), question answering (Zhu et al., 2021;\nSachan et al., 2021), data augmentation (Lu et al.,\n2021b), etc. Meanwhile, a novel paradigm of lever-\naging generative language models has gained popu-\nlarity, where researchers cast non-generation tasks\nas generative problems, e.g., to directly generate\ntextual labels to incorporate their semantics, and re-\nport promising results (De Cao et al., 2021; De Cao\net al., 2022). However, such approaches are still\nunderexplored in certain domains due to lack of\ndomain-specific generative language models, i.e.,\nmost of the aforementioned domain-specific PLMs\nare notably domain-adapted BERT-style models.\nIn the biomedical domain, two generative language\nmodels SciFive (Phan et al., 2021) and BioBART\n(Yuan et al., 2022) have been released, but in the\nclinical domain, the situation is worse and no such\ngenerative models exist to our knowledge. Though\nthe two domains are relatively close, clinical text\nposes unique challenges compared to general and\nnon-clinical biomedical text due to its specific lin-\nguistic characteristics (Alsentzer et al., 2019). Pre-\nvious studies list some of the linguistic features of\nclinical text, e.g., heavy use of professional techni-\ncal terminology, abbreviations and acronyms, pas-\nsive verbs, omission of subjects and verbs, etc., and\nthese features make clinical text divergent from\nstandard language (Smith et al., 2014).\nAiming to fulfill this gap, we adapt T5 (Raf-\nfel et al., 2020) to the clinical domain by train-\ning a domain-specific variant using clinical text,\ni.e., ClinicalT5. We demonstrate the capabilities\nof the model by conducting both intrinsic and ex-\ntrinsic evaluations. For intrinsic evaluation, we\naim to evaluate its capability to capture the simi-\nlarity and relatedness of the Unified Medical Lan-\nguage System (UMLS) concept pairs, where we\nmeasure the correlation coefficient between the sim-\nilarity scores of the encoded representations for the\nconcept pairs and those judged by human experts.\nFor extrinsic evaluation, we evaluate the proposed\nmodel along with baselines over a diverse set of\nbenchmark datasets, ranging from document classi-\nfication (DC), named entity recognition (NER), to\nnatural language inference (NLI). Furthermore, we\nalso evaluate on three more complicated real-world\ntasks of clinical importance, i.e., patients’ 30-day\nreadmission risk, 30-day and 1-year mortality risk.\nWe show that ClinicalT5 dramatically outperforms\nT5 and compares favorably with its close baselines\nacross all of these tasks.\n2 Related Work\n2.1 Biomedical Domain-Adapted Models\nThe biomedical domain has been an active area\nof research in the NLP community for the past\nfew years. Many relevant studies have been pre-\nsented, ranging from domain-specific language\nmodels, external knowledge infusion, and vari-\nous downstream applications, etc. (Peng et al.,\n2019; Beltagy et al., 2019; Lee et al., 2020; He\net al., 2020; Michalopoulos et al., 2020; Lu et al.,\n2021a). Most of the biomedical language models\nare BERT (Devlin et al., 2019) variants fine-tuned\nto biomedical text, e.g., BioBERT is trained on\nPubMed abstracts and PMC full text articles (Lee\net al., 2020) and SciBERT is trained on the full\ntext of biomedical and computer science papers\nfrom the Semantic Scholar corpus (Beltagy et al.,\n2019). Besides, researchers inject external domain\nknowledge into adapted biomedical language mod-\nels due to the knowledge-intensive nature of this\ndomain, e.g., umlsBERT is directly trained using\nUMLS text (Michalopoulos et al., 2020), He et al.\ninfuse disease information from the correspond-\ning Wikipedia passages into language models (He\net al., 2020), and Lu et al. inject biomedical knowl-\nedge from multiple sources into language mod-\nels via adapters (Lu et al., 2021a). For genera-\ntive language models, SciFive is an adapted T5\nmodel pre-trained on PubMed abstracts and PMC\narticles (Phan et al., 2021) and BioBART is an\nadapted BART model pre-trained on PubMed ab-\nstracts (Yuan et al., 2022).\n2.2 Clinical Domain-Adapted Models\nIn the clinical domain, there are mainly two\npopular BERT models, i.e., ClinicalBERT (Huang\net al., 2019) and BioClinicalBERT (Alsentzer\net al., 2019), which are both trained on the clinical\nnotes in the MIMIC-III database (Johnson et al.,\n2016). For generative language models, however,\nthe topic is not well explored and this situation\nmotivates our work.\n3 ClinicalT5\nFollowing prior studies on clinical language mod-\nels (Huang et al., 2019; Alsentzer et al., 2019), we\nuse the textual notes in MIMIC-III to train Clini-\ncalT5, which consists of approximately 2 million\nnotes. Similarly, only minimal pre-processing is\nconducted where unnecessary tokens and charac-\nters are removed (Huang et al., 2019).\n5437\nIn particular, we initialize the weights from\nthe SciFive-PubMed-PMC model (base and large)\n(Phan et al., 2021) and further pre-train with the\nspan-mask denoising objective (Raffel et al., 2020)\non the pre-processed MIMIC-III notes. The base\nand large models have ∼ 220M parameters with\n12 layers and ∼ 770M parameters with 24 layers,\nrespectively. For each of the two versions, we fur-\nther pre-train ClinicalT5 on the unlabeled text for\nextra 10k steps, with a max sequence length of\n512, a batch size of 8, and a learning rate of 1e−4.\nThe pre-training is performed on 3 Nvidia Tesla\nV100-32GB GPUs. We provide a reproducibility\nchecklist in Appendix A, and we refer the readers\nto (Raffel et al., 2020) for more detailed treatment\nof the architecture and training objectives of T5.\n4 Experiments\nIn this section, we evaluate ClinicalT5 both intrin-\nsically and extrinsically, along with the following\ngenerative baselines (for both general and domain-\nspecific texts): BART (Lewis et al., 2020), Bio-\nBART (Yuan et al., 2022), T5 (Raffel et al., 2020),\nSciFive (Phan et al., 2021), to demonstrate the capa-\nbilities of ClinicalT5 across different applications.\n4.1 Intrinsic Evaluation\nWe conduct intrinsic evaluation on the datasets\nUMNSRS-Sim and UMNSRS-Rel (Pakhomov\net al., 2010), which consist of 566 and 587 UMLS\nterm pairs respectively. Each pair comes with a\nsimilarity score and a relatedness score that are\nmanually assigned by human experts. Similar to\nprevious work (Zhang et al., 2019), we encode the\nterms with ClinicalT5 and the baselines. Essen-\ntially, we use the mean-pooled vectors of the last\nhidden states of the encoders as the term embed-\ndings and calculate a cosine similarity score for\neach pair. Then we compute the Pearson’s correla-\ntion coefficient and Spearman’s correlation coeffi-\ncient between the computed scores and the expert-\nassigned scores. As shown in Table 1, ClinicalT5\ndemonstrates a better ability to capture the similar-\nity of UMLS terms than T5 and Scifive, indicating\nthe effectiveness of the training.\n4.2 Extrinsic Evaluation\nFor extrinsic evaluation, we consider three differ-\nent tasks, i.e., document classification (DC), named\nentity recognition (NER), and natural language in-\nference (NLI). To validate the models’ capability\nModel UMNSRS-Similarity UMNSRS-Relatedness\nPearson Spearman Pearson Spearman\nBART-base 0.1456 0.1300 0.0756 0.0625\nBioBART-base0.3753 0.3441 0.3101 0.2929\nT5-base 0.2050 0.1448 0.1056 0.0519\nSciFive-base 0.1941 0.1488 0.1359 0.0900\nClinicalT5-base 0.2126 0.1611 0.1478 0.0948\nBART-large 0.2234 0.1958 0.1706 0.1546\nBioBART-large0.4511 0.4302 0.3517 0.3400\nT5-large 0.2379 0.2018 0.1813 0.1564\nSciFive-large 0.3176 0.2642 0.3039 0.2618\nClinicalT5-large 0.3391 0.2847 0.2884 0.2468\nTable 1: Pearson’s and Spearman’s correlation coeffi-\ncient scores.\non clinical text, we select datasets that are closely\nrelevant to clinical targets rather than biomedical or\nchemical related data such as BC5CDR-chemical\n(Li et al., 2016). We fine-tune the evaluating mod-\nels on 4 corresponding datasets across these tasks in\na single-task text-to-text manner. For all the experi-\nments, we use a batch size of 16 and a learning rate\nof 1e−4. Due to different targets, we set the max\nsource text length to 256, and the max target text\nlengths to 52, 256, 256, 15 for the datasets HOC,\nNCBI, BC5CDR and MEDNLI, respectively.\n4.2.1 Document Classification\nWe conduct document classification on the HOC\ndataset (Baker et al., 2016), which consists of\n9, 972 samples for training and 4, 947 samples\nfor testing. Essentially, we fine-tune the evalu-\nating models to categorize the texts into 10 cat-\negories by directly generating the class labels,\ne.g., “empty”, “evading growth suppressors”,\n“genomic instability and mutation”, etc.\n4.2.2 Named Entity Recognition\nWe conduct named entity recognition on two popu-\nlar datasets, i.e., NCBI-disease (Do˘gan et al., 2014)\nand BC5CDR-disease (Li et al., 2016). The input\ntext sequence may contain a disease term and the\nterm should be identified and labeled in the target\ntext, e.g., for the input text “Genotype and phe-\nnotype in patients with dihydropyrimidine dehy-\ndrogenase deficiency”, the target is “Genotype and\nphenotype in patients with disease* dihydropy-\nrimidine dehydrogenase deficiency *disease”.\n4.2.3 Natural Language Inference\nWe conduct natural language inference evaluation\non the MEDNLI dataset (Romanov and Shivade,\n2018), which consists of 11, 232 training samples\n5438\nTasks HOC NCBI BC5CDR MEDNLI\nMetrics(%) P R F1 P R F1 P R F1 Acc\nBART-base 80.30 79.84 79.81 62.23 72.09 66.80 59.24 67.26 63.00 75.60\nBioBART-base 84.68 83.54 83.82 63.10 71.77 67.16 61.78 72.05 66.52 80.66\nT5-base 82.00 80.98 81.19 86.64 83.00 84.78 80.73 81.68 81.20 81.86\nSciFive-base 85.10 84.83 84.70 86.43 88.25 87.33 83.56 81.43 82.48 83.90\nClinicalT5-base 85.44 85.14 85.06 87.28 88.56 87.92 81.55 82.92 82.23 84.95\nBART-large 84.89 84.07 84.18 63.39 74.50 68.50 66.45 62.07 64.19 84.53\nBioBART-large 84.80 84.51 84.39 67.74 70.51 69.10 65.00 71.93 68.29 86.29\nT5-large 85.42 84.75 84.79 84.20 84.99 84.60 78.31 79.75 79.02 83.83\nSciFive-large 85.57 85.67 85.34 85.91 85.10 85.50 78.28 79.89 79.08 84.95\nClinicalT5-large 85.37 84.79 84.78 86.37 87.09 86.73 79.24 81.49 80.35 85.86\nTable 2: Performance comparison over document classification, named entity recognition, and medical natural\nlanguage inference.\nand 1, 422 testing samples. Essentially, we con-\nvert the premise-hypothesis pair to a sequence and\nprepend a task-specific prefix to it, e.g., “mednli:\npremise: [...]. hypothesis: [...].” We take the con-\nverted sequence as the input text and fine-tune the\nevaluating models to generate the target labels, i.e.,\n“contradiction”, “neutral”, “entailment”.\n4.2.4 Results\nThe results are shown in Table 2. Generally, Clin-\nicalT5 outperforms T5 and SciFive across most\nof these metrics, and the advantage indicates the\nsuccess of the training over clinical text. However,\nClinicalT5-large is on par with T5-large and has a\nslightly lower recall than SciFive-large on the HOC\ndataset. We conjecture that the large versions of\nBART and T5 already have enough capacity for\nthe task which makes domain-specific training less\nimpressive, as reflected by the fact that BioBART-\nlarge is only marginally better than BART-large.\nFor MEDNLI, ClinicalT5 consistently outperforms\nT5 and SciFive although BioBART-large achieves\nthe highest accuracy.\n4.3 Real-world Evaluation\nWe also evaluate the models on more complicated\nreal-world applications of clinical importance, i.e.,\n30-day unplanned ICU patient readmission risk,\n30-day and 1-year patient mortality risk. The exper-\niment is conducted based on the MIMIC-III dataset\n(Johnson et al., 2016). Following previous work\n(Zhang et al., 2020; Lu et al., 2021c), we extract\nthe discharge summaries from EHRs and generate\n48, 393 documents. Essentially, we take the evalu-\nating models to encode the last 512 tokens of each\nTasks 30-d Readmission 30-d Mortality 1-y Mortality\nMetrics(%) A.R. A.P. RP80 A.R. A.P. A.R. A.P.\nT5-base 77.10 52.24 16.97 80.03 23.62 78.52 45.72\nSciFive-base78.1253.95 18.87 80.38 24.16 78.95 45.38\nClinicalT5-base 77.9454.25 19.76 81.11 26.70 79.09 46.58\nA.R: AUC under ROC, A.P: AUC under PRC, RP80: recall at precision of80%\nTable 3: Performance on patients’ outcomes prediction.\nnote, the last hidden states of which are fed into\na linear layer on top for prediction. As shown in\nTable 3, ClinicalT5 shows the best results across\nalmost all the metrics, demonstrating its potential\nfor real-world applications in the clinical domain.\n5 Conclusion\nIn this study, we explore and propose ClinicalT5,\na T5-based text-to-text transformer model for clin-\nical text. We evaluate the proposed model both\nintrinsically and extrinsically, and the results show\nthat ClinicalT5 compares favorably with its close\nbaselines. We also test upon more complicated\npatient outcomes prediction tasks, the results of\nwhich indicates its potential for these real-world\ndownstream tasks in the clinical domain.\nLimitations\nIn this work we present a generative language\nmodel for clinical texts based on T5. Although\nour experiments demonstrate the effectiveness of\nour method, there are still some limitations that\ncan be improved in future work. First, our evalua-\ntion has not included question answering and other\nrelated tasks for clinical texts. These are impor-\ntant tasks (Phan et al., 2021) and can be further\n5439\nexplored in future work. Second, our pre-training\nmethod for ClinicalT5 has mainly inherited the ob-\njectives from T5 using direct unlabeled texts. As\nsuch, many important domain-specific knowledge\nfor clinical domain (e.g., knowledge bases, concept\ndefinition) has not been explored to improve our\ngenerative model, serving as a promising direction\nfor future research.\nEthics Statement\nAll datasets used in this research are publicly avail-\nable and are obtained according to each dataset’s\nrespective data usage policy. We avoid showing\nany direct excerpts of the data in the paper. We do\nnot attempt to identify or deanonymize users in the\ndata in any way during our research, thus prevent-\ning any bias in our methods toward any specific\nusers.\nMore specifically, the proposed models are\ntrained on the clinical notes of the public MIMIC-\nIII database, which are already deidentified in ac-\ncordance with Health Insurance Portability and Ac-\ncountability Act (HIPAA) standards using struc-\ntured data cleansing and date shifting. As such, all\nidentifying data elements in HIPAA, including pa-\ntient name, telephone number, address, and dates,\nare already removed (Johnson et al., 2016) from\nour training data to hinder attempts to retrieve per-\nsonal information from our models. Similar to\nexisting pre-trained and publicly available models\nfor the clinical domain, i.e., ClinicalBERT (Huang\net al., 2019) and BioClinicalBERT (Alsentzer et al.,\n2019), the proposed models serve as a resource to\nfacilitate future research on clinical text.\nAcknowledgement\nThis research has been supported by the Army Re-\nsearch Office (ARO) grant W911NF-21-1-0112\nand the NSF grant CNS-1747798 to the IU-\nCRC Center for Big Learning. This research is\nalso based upon work supported by the Office\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity (IARPA), via IARPA Contract No. 2019-\n19051600006 under the Better Extraction from Text\nTowards Enhanced Retrieval (BETTER) Program.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted as\nnecessarily representing the official policies, either\nexpressed or implied, of ARO, ODNI, IARPA, the\nDepartment of Defense, or the U.S. Government.\nThe U.S. Government is authorized to reproduce\nand distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.\nReferences\nAishwarya Krishna Allada, Yuanxin Wang, Veni Jin-\ndal, Morteza Babee, Hamid R Tizhoosh, and Mark\nCrowley. 2021. Analysis of language embeddings\nfor classification of unstructured pathology reports.\nIn 2021 43rd Annual International Conference of the\nIEEE Engineering in Medicine & Biology Society\n(EMBC), pages 2378–2381. IEEE.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop (Clini-\ncalNLP), pages 72–78.\nSimon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan\nHögberg, Ulla Stenius, and Anna Korhonen. 2016.\nAutomatic semantic classification of scientific litera-\nture according to the hallmarks of cancer. Bioinfor-\nmatics, 32(3):432–440.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3606–3611.\nRajarshi Bhowmik, Karl Stratos, and Gerard de Melo.\n2021. Fast and effective biomedical entity linking\nusing a dual encoder. In Proceedings of the 12th\nInternational Workshop on Health Text Mining and\nInformation Analysis, pages 28–37, online. Associa-\ntion for Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJiaao Chen and Diyi Yang. 2021. Structure-aware ab-\nstractive conversation summarization via discourse\nand action graphs. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1380–1391, Online. As-\nsociation for Computational Linguistics.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Representa-\ntions.\nNicola De Cao, Ledell Wu, Kashyap Popat, Mikel\nArtetxe, Naman Goyal, Mikhail Plekhanov, Luke\nZettlemoyer, Nicola Cancedda, Sebastian Riedel, and\n5440\nFabio Petroni. 2022. Multilingual autoregressive en-\ntity linking. Transactions of the Association for Com-\nputational Linguistics, 10:274–290.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (NAACL-HLT), pages 4171–4186.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nYun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James\nCaverlee. 2020. Infusing disease knowledge into\nbert for health question answering, medical inference\nand disease name recognition. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4604–4614.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntific data, 3(1):1–9.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nQiuhao Lu, Dejing Dou, and Thien Huu Nguyen. 2021a.\nParameter-efficient domain knowledge integration\nfrom multiple sources for biomedical pre-trained lan-\nguage models. In Findings of the Association for\nComputational Linguistics: EMNLP 2021 , pages\n3855–3865, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nQiuhao Lu, Dejing Dou, and Thien Huu Nguyen. 2021b.\nTextual data augmentation for patient outcomes pre-\ndiction. In 2021 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM), pages 2817–\n2821. IEEE.\nQiuhao Lu, Thien Huu Nguyen, and Dejing Dou. 2021c.\nPredicting patient readmission risk from medical text\nvia knowledge graph enhanced multiview graph con-\nvolution. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 1990–1994.\nXiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati,\nand Bing Xiang. 2019. Domain adaptation with bert-\nbased domain classification and data selection. In\nProceedings of the 2nd Workshop on Deep Learning\nApproaches for Low-Resource NLP (DeepLo), pages\n76–83.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alex Wong. 2020. Umls-\nbert: Clinical domain knowledge augmentation of\ncontextual embeddings using the unified medical\nlanguage system metathesaurus. arXiv preprint\narXiv:2010.10391.\nSerguei Pakhomov, Bridget McInnes, Terrence Adam,\nYing Liu, Ted Pedersen, and Genevieve B Melton.\n2010. Semantic similarity and relatedness between\nclinical terms: an experimental study. In AMIA an-\nnual symposium proceedings, volume 2010, page 572.\nAmerican Medical Informatics Association.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of bert and elmo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task (BioNLP), pages 58–65.\nLong N. Phan, James T. Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clini-\ncal domain. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1586–1596, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\n5441\nDevendra Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L. Hamil-\nton, and Bryan Catanzaro. 2021. End-to-end training\nof neural retrievers for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6648–6662, Online. Association for Computational\nLinguistics.\nKelly Smith, Beata Megyesi, Sumithra Velupillai, and\nMaria Kvist. 2014. Professional language in swedish\nclinical text: Linguistic characterization and com-\nparative studies. Nordic Journal of Linguistics ,\n37(2):297–323.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nHongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang,\nYutao Xie, and Sheng Yu. 2022. Biobart: Pretraining\nand evaluation of a biomedical generative language\nmodel. arXiv preprint arXiv:2204.03905.\nXiao Zhang, Dejing Dou, and Ji Wu. 2020. Learning\nconceptual-contextual embeddings for medical text.\nIn AAAI, pages 9579–9586.\nYijia Zhang, Qingyu Chen, Zhihao Yang, Hongfei Lin,\nand Zhiyong Lu. 2019. Biowordvec, improving\nbiomedical word embeddings with subword infor-\nmation and mesh. Scientific data, 6(1):1–9.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering. arXiv preprint\narXiv:2101.00774.\n5442\nA Reproducibility Checklist\n• Source code with specification of all de-\npendencies, including external libraries :\nThe models and source code along with a\nREADME file will be released upon decision\nof the paper.\n• Description of computing infrastructure\nused: In this work, we use 3 Nvidia Tesla\nV100-32GB GPUs for the pre-training and\none GPU for evaluations. PyTorch 1.8.1 and\nHuggingface-Transformer 4.18.0 (Wolf et al.,\n2019) are used for implementation.\n• Average runtime for each approach : We\npre-train the model on MIMIC-III for 3\nepochs which takes ∼ 8 hours, and the best\nvariant is chosen based on its performance on\nHOC.\n• Number of parameters in the model :\nClinicalT5-base has ∼ 220M parameters with\n12 layers and ClinicalT5-large has ∼ 770M\nparameters with 24 layers.\n• Explanation of evaluation metrics used,\nwith links to code : We use the same mea-\nsures and correctness criteria as in prior work\n(Zhang et al., 2019; Phan et al., 2021; Zhang\net al., 2020; Lu et al., 2021c) for fair com-\nparison. In particular, we use Pearson’s and\nSpearman’s correlation coefficients for intrin-\nsic evaluation, and use precision, recall, F1\nscore as well as accuracy for extrinsic eval-\nuation. We also use AUC of ROC, AUC of\nPRC and RP80 for the experiments of patient\noutcomes prediction.\n• Bounds for each hyper-parameter: For all\nthe experiments, we choose the learning rate\nfrom [1e-5, 1e-4, 1e-3] for the AdamW opti-\nmizer, the batch size from [4, 8, 16].\n5443",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.8580344915390015
    },
    {
      "name": "Computer science",
      "score": 0.8030288815498352
    },
    {
      "name": "Natural language processing",
      "score": 0.6851859092712402
    },
    {
      "name": "Vocabulary",
      "score": 0.6776546239852905
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6218690276145935
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6150128841400146
    },
    {
      "name": "Language model",
      "score": 0.6069913506507874
    },
    {
      "name": "Transformer",
      "score": 0.5825469493865967
    },
    {
      "name": "Generative model",
      "score": 0.5042270421981812
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5025012493133545
    },
    {
      "name": "Linguistics",
      "score": 0.16477695107460022
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}