{
  "title": "Deriving Language Models from Masked Language Models",
  "url": "https://openalex.org/W4385570199",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3036272606",
      "name": "Lucas Torroba Hennigen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124530043",
      "name": "Yoon Kim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W218065563",
    "https://openalex.org/W2114220616",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W162901985",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2013869878",
    "https://openalex.org/W1978575854",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2027060194",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W768466067",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4253647469",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W4285199989",
    "https://openalex.org/W2126398289",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4287125387",
    "https://openalex.org/W4313483387",
    "https://openalex.org/W4312091828"
  ],
  "abstract": "Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions from MLMs, focusing on distributions over two tokens, which makes it possible to calculate exact distributional properties. We find that an approach based on identifying joints whose conditionals are closest to those of the MLM works well and outperforms existing Markov random field-based approaches. We further find that this derived model’s conditionals can even occasionally outperform the original MLM’s conditionals.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1149–1159\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDeriving Language Models from Masked Language Models\nLucas Torroba Hennigen Yoon Kim\nMassachusetts Institute of Technology\nComputer Science and Artificial Intelligence Laboratory\nlucastor@mit.edu yoonkim@mit.edu\nAbstract\nMasked language models (MLM) do not ex-\nplicitly define a distribution over language, i.e.,\nthey are not language models per se. How-\never, recent work has implicitly treated them as\nsuch for the purposes of generation and scoring.\nThis paper studies methods for deriving explicit\njoint distributions from MLMs, focusing on dis-\ntributions over two tokens, which makes it pos-\nsible to calculate exact distributional properties.\nWe find that an approach based on identifying\njoints whose conditionals are closest to those\nof the MLM works well and outperforms ex-\nisting Markov random field-based approaches.\nWe further find that this derived model’s con-\nditionals can even occasionally outperform the\noriginal MLM’s conditionals.\n1 Introduction\nMasked language modeling has proven to be an\neffective paradigm for representation learning (De-\nvlin et al., 2019; Liu et al., 2019; He et al., 2021).\nHowever, unlike regular language models, masked\nlanguage models (MLM) do not define an explicit\njoint distribution over language. While this is not\na serious limitation from a representation learning\nstandpoint, having explicit access to joint distribu-\ntions would be useful for the purposes of genera-\ntion (Ghazvininejad et al., 2019), scoring (Salazar\net al., 2020), and would moreover enable evaluation\nof MLMs on standard metrics such as perplexity.\nStrictly speaking, MLMs do define a joint distri-\nbution over tokens that have been masked out. But\nthey assume that the masked tokens are condition-\nally independent given the unmasked tokens—an\nassumption that clearly does not hold for language.\nHow might we derive a language model from an\nMLM such that it does not make unrealistic inde-\npendence assumptions? One approach is to use the\nset of the MLM’s unary conditionals—the condi-\ntionals that result from masking just a single to-\nken in the input—to construct a fully-connected\nMarkov random field (MRF) over the input (Wang\nand Cho, 2019; Goyal et al., 2022). This resulting\nMRF no longer makes any independence assump-\ntions. It is unclear, however, if this heuristic ap-\nproach actually results in a good language model.1\nThis paper adopts an alternative approach which\nstems from interpreting the unary conditionals of\nthe MLM as defining a dependency network (Heck-\nerman et al., 2000; Yamakoshi et al., 2022).2 De-\npendency networks specify the statistical relation-\nship among variables of interest through the set of\nconditional distributions over each variable given\nits Markov blanket, which in the MLM case cor-\nresponds to all the other tokens. If the condition-\nals from a dependency network are compatible,\ni.e., there exists a joint distribution whose condi-\ntionals coincide with those of the dependency net-\nwork’s, then one can recover said joint using the\nHammersley–Clifford–Besag (HCB; Besag, 1974)\ntheorem. If the conditionals are incompatible, then\nwe can adapt approaches from statistics for deriving\nnear-compatible joint distributions from incompati-\nble conditionals (AG; Arnold and Gokhale, 1998).\nWhile these methods give statistically-principled\napproaches to deriving explicit joints from the\nMLM’s unary conditionals, they are intractable to\napply to derive distributions over full sequences.\nWe thus study a focused setting where it is tractable\nto compute the joints exactly, viz., the pairwise\nlanguage model setting where we use the MLM’s\nunary conditionals of two tokens to derive a joint\n1MRFs derived this way are still not language models\nin the strictest sense (e.g., see Du et al., 2022) because the\nprobabilities of sentences of a given length sum to 1, and hence\nthe sum of probabilities of all strings is infinite (analogous\nto left-to-right language models trained without an [EOS]\ntoken; Chen and Goodman, 1998). This can be remedied by\nincorporating a distribution over sentence lengths.\n2Recent work by Yamakoshi et al. (2022) has taken this\nview, focusing on sampling from the dependency network as\na means to implicitly characterize the joint distribution of an\nMLM. Here we focus on an explicit characterization of the\njoint.\n1149\nover these two tokens (conditioned on all the other\ntokens). Experiments under this setup reveal that\nAG method performs best in terms of perplexity,\nwith the the HCB and MRF methods performing\nsimilarly. Surprisingly, we also find that the unary\nconditionals of the near-compatible AG joint oc-\ncasionally have lower perplexity than the original\nunary conditionals learnt by the MLM, suggesting\nthat regularizing the conditionals to be compatible\nmay be beneficial insofar as modeling the distribu-\ntion of language.3\n2 Joint distributions from MLMs\nLet Vbe a vocabulary, T be the text length, and\nw ∈VT be an input sentence or paragraph. We\nare particularly interested in the case when a subset\nS ⊆[T] ≜ {1,...,T }of the input w is replaced\nwith [MASK] tokens; in this case we will use the\nnotation q{t}|S(·| wS) to denote the output distri-\nbution of the MLM at position t ∈S, where we\nmask out the positions in S, i.e., for all k∈Swe\nmodify w by setting wk = [MASK]. If S = {t},\nthen we call qt|t ≜ q{t}|{t} a unary conditional.\nOur goal is to use these conditionals to construct\njoint distributions qS|S(·| wS) for any S.\nDirect MLM construction. The simplest ap-\nproach is to simply mask out the tokens over which\nwe want a joint distribution, and define it to be the\nproduct of the MLM conditionals,\nqMLM\nS|S (wS |wS) ≜\n∏\ni∈S\nq{i}|S(wi |wS). (1)\nThis joint assumes that the entries of wS are con-\nditionally independent given wS. Since one can\nshow that MLM training is equivalent to learning\nthe conditional marginals of language (App. A),\nthis can be seen as approximating conditionals with\na (mean field-like) factorizable distribution.\nMRF construction. To address the condi-\ntional independence limitation of MLMs, prior\nwork (Wang and Cho, 2019; Goyal et al., 2022)\nhas proposed deriving joints by defining an MRF\nusing the unary conditionals of the MLM. Accord-\ningly, we define\nqMRF\nS|S (wS |wS) ∝\n∏\nt∈S\nqt|t(wt |wt), (2)\nwhich can be interpreted as a fully connected MRF,\nwhose log potential is given by the sum of the unary\n3Our code and data is available at:https://github.com/\nltorroba/lms-from-mlms.\nlog probabilities. One can similarly define a variant\nof this MRF where the log potential is the sum of\nthe unary logits. MRFs defined this way have a\nsingle fully connected clique and thus do not make\nany conditional independence assumptions. How-\never, such MRFs can have unary conditionals that\ndeviate from the MLM’s unary conditionals even\nif those are compatible (App. B). This is poten-\ntially undesirable since the MLM unary condition-\nals could be close to the true unary conditionals,4\nwhich means the MRF construction could be worse\nthan the original MLM in terms of unary perplexity.\nHammersley–Clifford–Besag construction.\nThe Hammersley–Clifford–Besag theorem (HCB;\nBesag, 1974) provides a way of reconstructing a\njoint distribution from its unary conditionals. With-\nout loss of generality, assume that S = {1,...,k }\nfor some k ≤ T. Then given a pivot point\nw′= (w′\n1,...,w ′\nk) ∈Vk, we define\nqHCB\nS|S (wS |wS) ∝\n∏\nt∈S\nqt|t(wt |w>t,w′\n<t)\nqt|t(w′\nt |w>t,w′\n<t), (3)\nwhere w′\n<i ≜ (w′\n1,...,w ′\ni−1), and similarly\nw>i ≜ (wi+1,...,w T). Importantly, unlike the\nMRF approach, if the unary conditionals of the\nMLM are compatible, then HCB will recover the\ntrue joint, irrespective of the choice of pivot.\nArnold–Gokhale construction. If we assume\nthat the unary conditionals are not compatible, then\nwe can frame our goal as finding a near-compatible\njoint, i.e., a joint such that its unary conditionals\nare close to the unary conditionals of the MLM.\nFormally, for any S and fixed inputs wS, we can\ndefine this objective as,\nqAG\nS|S(·| wS) = argmin\nµ\n∑\nt∈S\n∑\nw′∈V|S|−1\nJ(t,w′), (4)\nwhere J(t,w′) is defined as:\nKL(qt|S\\{t},S(·| w′,wS) ||µt|S\\{t},S(·| w′,wS)).\nWe can solve this optimization problem using\nArnold and Gokhale’s (1998) algorithm (App. C).\n2.1 Pairwise language model\nIn language modeling we are typically interested\nin the probability of a sequence p(w). However,\nthe above methods are intractable to apply to full\nsequences (except for the baseline MLM). For ex-\nample, the lack of any independence assumptions\n4As noted by https://machinethoughts.wordpress.\ncom/2019/07/14/a-consistency-theorem-for-bert/\n1150\nin the MRF means that the partition function re-\nquires full enumeration over VT sequences.5 We\nthus focus our empirical study on the pairwise set-\nting where |S|= 2.6 In this setting, we can calcu-\nlate qS|S(·| wS) with O(V) forward passes of the\nMLM for all methods.\n3 Evaluation\nWe compute two sets of metrics that evaluate the\nresulting joints in terms of (i) how good they\nare as probabilistic models of language and (ii)\nhow faithful they are to the original MLM con-\nditionals (which are trained to approximate the\ntrue conditionals of language, see App. A). Let\nD= {(w(n),S(n))}N\nn=1 be a dataset where w(n)\nis an English sentence and S(n) = (a(n),b(n)) are\nthe two positions being masked. We define the\nfollowing metrics to evaluate a distribution q′:\nLanguage model performance. We consider\ntwo performance metrics. The first is the pairwise\nperplexity (P-PPL) over two tokens,\nexp\n(\n−1\n2N\nN∑\nn=1\nlog q′\na(n),b(n)|S(n) (w(n)\na(n) ,w(n)\nb(n) |w(n)\nS(n) )\n)\nWe would expect a good joint to obtain lower\npairwise perplexity than the original MLM, which\n(wrongly) assumes conditional independence. The\nsecond is unary perplexity (U-PPL),\nexp\n(\n−1\n2N\nN∑\nn=1\n∑\n(i,j)∈\n{S(n),S(n)\nr }\nlog q′\ni|j,S(n) (w(n)\ni |w(n)\nj ,w(n)\nS(n) )\n)\nwhere for convenience we let S(n)\nr ≜ (b(n),a(n))\nas the reverse of the masked positions tuple S(n).\nNote that this metric uses the unary conditionals\nderived from the pairwise joint, i.e.,q′\ni|j,S, except in\nthe MLM construction case which uses the MLM’s\noriginal unary conditionals.\nFaithfulness. We also assess how faithful the\nnew unary conditionals are to the original unary\nconditionals by calculating the average conditional\nKL divergence (A-KL) between them,\nN∑\nn=1\n∑\nw′∈V\nD(S(n),w′,w(n)\nS(n) ) + D(S(n)\nr ,w′,w(n)\nS(n) )\n2N|V| .\n5We also tried estimating the partition through importance\nsampling with GPT-2 but found the estimate to be quite poor.\n6Concurrent work by Young and You (2023) also explores\nthe (in)compatibility of MLMs in the |S| = 2case.\nwhere we define D(S,w′,wS) ≜ KL(qa|b,S(· |\nw′,wS) ||q′\na|b,S(·| w′,wS)) for S = (a,b). If the\nnew joint is completely faithful to the MLM, this\nnumber should be zero. The above metric averages\nthe KL across the entire vocabulary V, but in prac-\ntice we may be interested in assessing closeness\nonly when conditioned on the gold tokens. We\nthus compute a variant of the above metric where\nwe only average over the conditionals for the gold\ntoken (G-KL):\nN∑\nn=1\nD(S(n),w(n)\nb(n) ,w(n)\nS(n) )+ D(S(n)\nr ,w(n)\na(n) ,w(n)\nS(n) )\n2N .\nThis metric penalizes unfaithfulness in common\ncontexts more than in uncommon contexts. Note\nthat if the MLM’s unary conditionals are compat-\nible, then both the HCB and AG approach should\nyield the same joint distribution, and their faithful-\nness metrics should be zero.\n3.1 Experimental setup\nWe calculate the above metrics on 1000 examples7\nfrom a natural language inference dataset (SNLI;\nBowman et al., 2015) and a summarization\ndataset (XSUM; Narayan et al., 2018). We con-\nsider two schemes for selecting the tokens to be\nmasked for each sentence: masks over two to-\nkens chosen uniformly at random (Random pairs),\nand also over random contiguous tokens in a sen-\ntence (Contiguous pairs). Since inter-token depen-\ndencies are more likely to emerge when adjacent\ntokens are masked, the contiguous setup magni-\nfies the importance of deriving a good pairwise\njoint. In addition, we consider both BERTBASE and\nBERTLARGE (cased) as the MLMs from which to ob-\ntain the unary conditionals.8 For the AG joint, we\nrun t= 50 steps of Arnold and Gokhale’s (1998)\nalgorithm (App. C), which was enough for conver-\ngence. For the HCB joint, we pick a pivot using\nthe mode of the pairwise joint of the MLM.9\n4 Results\nThe results are shown in Tab. 1. Comparing the\nPPL’s of MRF and MRFL (i.e., the MRF using log-\nits), the former consistently outperforms the latter,\n7Each example requires running the MLM over 28 000\ntimes, so it is expensive to evaluate on many more examples.\n8Specifically, we use the bert-base-cased and bert-large-\ncased implementations from HuggingFace (Wolf et al., 2020).\n9We did not find HCB to be too sensitive to the pivot in\npreliminary experiments.\n1151\nRandom pairs Contiguous pairs\nDataset Scheme U-PPL P-PPL A-KL G-KL U-PPL P-PPL A-KL G-KL\nB\nSNLI\nMLM 11.22 19 .01 1 .080 0 .547 13 .78 74 .68 4 .014 1 .876\nMRFL 13.39 71 .44 0 .433 0 .267 23 .45 13 568 .17 1 .543 0 .607\nMRF 12.30 21 .65 0 .658 0 .179 18 .35 126 .05 1 .967 0 .366\nHCB 12.51 22 .62 0 .593 0 .168 17 .71 589 .02 2 .099 0 .416\nAG 10.76 12 .68 0 .007 0 .085 13 .26 21 .59 0 .018 0 .181\nXSUM\nMLM 4.88 6 .12 0 .404 0 .227 4 .91 39 .33 4 .381 2 .128\nMRFL 5.17 9 .12 0 .148 0 .085 6 .55 2209 .94 1 .561 0 .383\nMRF 5.00 6 .23 0 .262 0 .049 5 .53 47 .62 2 .242 0 .185\nHCB 5.08 6 .21 0 .256 0 .052 6 .46 174 .32 2 .681 0 .328\nAG 5.00 5 .29 0 .003 0 .044 5 .27 8 .42 0 .016 0 .143\nL\nSNLI\nMLM 9.50 18 .57 1 .374 0 .787 10 .42 104 .12 4 .582 2 .463\nMRFL 11.52 76 .23 0 .449 0 .276 15 .43 8536 .92 1 .470 0 .543\nMRF 10.57 19 .54 0 .723 0 .193 13 .07 93 .33 1 .992 0 .359\nHCB 10.71 20 .70 0 .797 0 .215 14 .43 458 .25 2 .563 0 .552\nAG 8.57 10 .11 0 .007 0 .097 9 .64 15 .64 0 .019 0 .173\nXSUM\nMLM 3.80 5 .67 0 .530 0 .413 3 .91 103 .86 5 .046 3 .276\nMRFL 3.94 7 .06 0 .156 0 .068 4 .62 1328 .20 1 .441 0 .290\nMRF 3.87 4 .94 0 .322 0 .036 4 .16 36 .66 2 .258 0 .145\nHCB 3.91 5 .14 0 .346 0 .059 5 .67 164 .15 2 .954 0 .400\nAG 3.88 4 .13 0 .003 0 .042 4 .21 6 .62 0 .016 0 .126\nTable 1: Comparison of MRF, HCB and AG constructions on randomly sampled SNLI (Bowman et al., 2015)\nsentences and XSUM (Narayan et al., 2018) summaries. We apply the constructions to two MLMs: BERTBASE ( B )\nand BERTLARGE ( L ). We consider both masking tokens uniformly at random (Random pairs) and masking adjacent\ntokens uniformly at random (Contiguous pairs). For all metrics, lower is better.\nindicating that using the raw logits generally re-\nsults in a worse language model. Comparing the\nMRFs to MLM, we see that the unary perplexity\n(U-PPL) of the MLM is lower than those of the\nMRFs, and that the difference is most pronounced\nin the contiguous masking case. More surprisingly,\nwe see that the pairwise perplexity (P-PPL) is often\n(much) higher than the MLM’s, even though the\nMLM makes unrealistic conditional independence\nassumptions. These results suggest that the derived\nMRFs are in general worse unary/pairwise proba-\nbilistic models of language than the MLM itself,\nimplying that the MRF heuristic is inadequate (see\nApp. D for a qualitative example illustrating how\nthis can happen). Finally, we also find that the\nMRFs’ unary conditionals are not faithful to those\nof the MRFs based on the KL measures. Since\none can show that the MRF construction can have\nunary conditionals that have nonzero KL to the\nMLM’s unary conditionals even if they are com-\npatible (App. B), this gives both theoretical and\nempirical arguments against the MRF construction.\nThe HCB joint obtains comparable performance\nto MRF in the random masking case. In the con-\ntiguous case, it exhibits similar failure modes as\nthe MRF in producing extremely high pairwise\nperplexity (P-PPL) values. The faithfulness met-\nrics are similar to the MRF’s, which suggests that\nthe conditionals learnt by MLMs are incompatible.\nThe AG approach, on the other hand, outperforms\nthe MRFL, MRF and HCB approaches in virtually\nall metrics. This is most evident in the contiguous\nmasking case, where AG attains lower pairwise per-\nplexity than all models, including the MLM itself.\nIn some cases, we find that the AG model even\noutperforms the MLM in terms of unary perplex-\nity, which is remarkable since the unary condition-\nals of the MLM were trained to approximate the\nunary conditionals of language (App. A). This indi-\ncates that near-compatibility may have regularizing\neffect that leads to improved MLMs. Since AG\nwas optimized to be near-compatible, its joints are\nunsurprisingly much more faithful to the original\nMLM’s conditionals. However, AG’s G-KL tends\nto be on par with the other models, which suggests\nthat it is still not faithful to the MLM in the contexts\nthat are most likely to arise. Finally, we analyze\nthe effect of masked position distance on language\nmodeling performance, and find that improvements\nare most pronounced when the masked tokens are\nclose to each other (see App. E).\n5 Related work\nProbabilistic interpretations of MLMs.In one\nof the earliest works about sampling from MLMs,\nWang and Cho (2019) propose to use unary condi-\n1152\ntionals to sample sentences. Recently Yamakoshi\net al. (2022) highlight that, while this approach\nonly constitutes a pseudo-Gibbs sampler, the act of\nre-sampling positions uniformly at random guaran-\ntees that the resulting Markov chain has a unique,\nstationary distribution (Bengio et al., 2013, 2014).\nAlternatively, Goyal et al. (2022) propose defining\nan MRF from the MLM’s unary conditionals, and\nsample from this via Metropolis-Hastings. Concur-\nrently, Young and You (2023) conduct an empirical\nstudy of the compatibility of BERT’s conditionals.\nCompatible distributions. The statistics com-\nmunity has long studied the problem of assessing\nthe compatibility of a set of conditionals (Arnold\nand Press, 1989; Gelman and Speed, 1993; Wang\nand Kuo, 2010; Song et al., 2010). Arnold and\nGokhale (1998) and Arnold et al. (2002) explore al-\ngorithms for reconstructing near-compatible joints\nfrom incompatible conditionals, which we lever-\nage in our work. Besag (1974) also explores this\nproblem, and defines a procedure (viz., eq. 3) for\ndoing so when the joint distribution is strictly pos-\nitive and the conditionals are compatible. Lowd\n(2012) apply a version of HCB to derive Markov\nnetworks from incompatible dependency networks\n(Heckerman et al., 2000).\n6 Conclusion\nIn this paper, we studied four different methods for\nderiving an explicit joint distributions from MLMs,\nfocusing in the pairwise language model setting\nwhere it is possible to compute exact distributional\nproperties. We find that the Arnold–Gokhale (AG)\napproach, which finds a joint whose conditionals\nare closest to the unary conditionals of an MLM,\nworks best. Indeed, our results indicate that said\nconditionals can attain lower perplexity than the\nunary conditionals of the original MLM. It would\nbe interesting to explore whether explicitly regu-\nlarizing the conditionals to be compatible during\nMLM training would lead to better modeling of the\ndistribution of language.\n7 Limitations\nOur study illuminates the deficiencies of the MRF\napproach and applies statistically-motivated ap-\nproaches to craft more performant probabilistic\nmodels. However, it is admittedly not clear how\nthese insights can immediately be applied to im-\nprove downstream NLP tasks. We focused on mod-\nels over pairwise tokens in order to avoid sampling\nand work with exact distributions for the various\napproaches (MRF, HCB, AG). However this lim-\nits the generality of our approach (e.g., we cannot\nscore full sentences). We nonetheless believe that\nour empirical study is interesting on its own and\nsuggests new paths for developing efficient and\nfaithful MLMs.\nEthics Statement\nWe foresee no ethical concerns with this work.\nAcknowledgements\nWe thank the anonymous reviewers for their helpful\ncomments. This research is supported in part by\nfunds from the MLA@CSAIL initiative and MIT-\nIBM Watson AI lab. LTH acknowledges support\nfrom the Michael Athans fellowship fund.\nReferences\nBarry C. Arnold, Enrique Castillo, and José María Sara-\nbia. 2002. Exact and near compatibility of discrete\nconditional distributions. Computational Statistics &\nData Analysis, 40(2):231–252.\nBarry C. Arnold and Dattaprabhakar V . Gokhale. 1998.\nDistributions most nearly compatible with given fam-\nilies of conditional distributions. Test, 7(2):377–390.\nBarry C. Arnold and James S. Press. 1989. Compatible\nconditional distributions. Journal of the American\nStatistical Association, 84(405):152–156.\nYoshua Bengio, Éric Thibodeau-Laufer, Guillaume\nAlain, and Jason Yosinski. 2014. Deep generative\nstochastic networks trainable by backprop. In Pro-\nceedings of the 31st International Conference on Ma-\nchine Learning, volume 32 of Proceedings of Ma-\nchine Learning Research , pages 226–234, Bejing,\nChina. PMLR.\nYoshua Bengio, Li Yao, Guillaume Alain, and Pascal\nVincent. 2013. Generalized denoising auto-encoders\nas generative models. In Proceedings of the 26th\nInternational Conference on Neural Information Pro-\ncessing Systems, NIPS, page 899–907, Red Hook,\nNew York, USA. Curran Associates Inc.\nJulian Besag. 1974. Spatial interaction and the statisti-\ncal analysis of lattice systems. Journal of the Royal\nStatistical Society, 36(2):192–236.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\n1153\nStanley F. Chen and Joshua Goodman. 1998. An em-\npirical study of smoothing techniques for language\nmodeling. Technical report, Harvard University.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota, USA. Associa-\ntion for Computational Linguistics.\nLi Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara\nMeister, Jason Eisner, and Ryan Cotterell. 2022. A\nmeasure-theoretic characterization of tight language\nmodels.\nAndrew Gelman and Terence P. Speed. 1993. Character-\nizing a joint probability distribution by conditionals.\nJournal of the Royal Statistical Society, 55(1):185–\n188.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-Predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKartik Goyal, Chris Dyer, and Taylor Berg-Kirkpatrick.\n2022. Exposing the implicit energy networks behind\nmasked language models via Metropolis–Hastings.\nIn International Conference on Learning Representa-\ntions.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DeBERTa: Decoding-enhanced\nBERT with Disentangled Attention. In International\nConference on Learning Representations.\nDavid Heckerman, Max Chickering, Chris Meek,\nRobert Rounthwaite, and Carl Kadie. 2000. Depen-\ndency networks for inference, collaborative filtering,\nand data visualization. Journal of Machine Learning\nResearch, 1:49–75.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR.\nDaniel Lowd. 2012. Closed-form learning of markov\nnetworks from dependency networks. In Proceedings\nof the 28th Conference on Uncertainty in Artificial\nIntelligence, pages 533–542, Catalina Island, Califor-\nnia, USA. Association for Uncertainity in Artificial\nIntelligence.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\nTopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nChwan-Chin Song, Lung-An Li, Chong-Hong Chen,\nThomas J. Jiang, and Kun-Lin Kuo. 2010. Com-\npatibility of finite discrete conditional distributions.\nStatistica Sinica, 20(1):423–440.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a Markov random\nfield language model. In Proceedings of the Work-\nshop on Methods for Optimizing and Evaluating Neu-\nral Language Generation, pages 30–36, Minneapolis,\nMinnesota, USA. Association for Computational Lin-\nguistics.\nYuchung J. Wang and Kun-Lin Kuo. 2010. Compatibil-\nity of discrete conditional distributions with structural\nzeros. Journal of Multivariate Analysis, 101(1):191–\n199.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nTakateru Yamakoshi, Thomas Griffiths, and Robert\nHawkins. 2022. Probing BERT’s priors with serial re-\nproduction chains. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 3977–\n3992, Dublin, Ireland. Association for Computational\nLinguistics.\nTom Young and Yang You. 2023. On the inconsistencies\nof conditionals learned by masked language models.\n1154\nA MLMs as learning conditional marginals\nOne can show that the MLM training objective corresponds to learning to approximate the conditional\nmarginals of language, i.e., the (single-position) marginals of language when we condition on any\nparticular set of positions. More formally, consider an MLM parameterized by a vector θ ∈Θ and some\ndistribution µ(·) over positions to mask S ⊆[T]. Then the MLM learning objective is given by:\nˆθ = argsup\nθ\nE\nS∼µ(·)\nE\nw∼p(·)\n[\n1\n|S|\n∑\nt∈S\nlog qt|S(wt |wS; θ)\n]\n,\nwhere p(·) denotes the true data distribution. Analogously, let pS|S(· |wS) and pS(·) denote the\nconditionals and marginals of the data distribution, respectively. Then the above can be rewritten as:\nˆθ = argsup\nθ\nE\nS∼µ(·)\nE\nwS∼pS(·)\n[\n1\n|S|\n∑\nt∈S\nE\nwS∼pS|S(·)\n[\nlog qt|S(wt |wS; θ)\n]]\n= arginf\nθ\nE\nS∼µ(·)\nE\nwS∼pS(·)\n[\n1\n|S|\n∑\nt∈S\nKL(pt|S(·| wS) ||qt|S(·| wS; θ))\n]\n,\nThus, we can interpret MLM training as learning to approximate the conditional marginals of language,\ni.e., ∀S ⊆[T] and ∀t ∈S, in the limit we would expect that, for any observed context wS, we have\nqt|S(·| wS) ≈pt|S(·| wS).\nB Unfaithful MRFs\nHere we show that even if the unary conditionals used in the MRF construction are compatible (Arnold\nand Press, 1989), the unary conditionals of the probabilistic model implied by the MRF construction can\ndeviate (in the KL sense) from the true conditionals. This is important because (i) it suggests that we\nmight do better (at least in terms of U-PPL) by simply sticking to the conditionals learned by MLM, and\n(ii) this is not the case for either the HCB or the AG constructions, i.e., if we started with the correct\nconditionals, HCB and AG’s joint would be compatible with the MLM. Formally,\nProposition B.1. Let w1,w2 ∈V and further let p1|2(·| w2),p2|1(·| w1) be the true (i.e., population)\nunary conditional distributions. Define an MRF as\nq1,2(w1,w2) ∝p1|2(w1 |w2) p2|1(w2 |w1),\nand let q1|2(·| w2),q2|1(·| w1) be the conditionals derived from the MRF. Then there exists p1|2,p2|1\nsuch that\nKL(p1|2(·| w2) ||q1|2(·| w2)) >0.\nProof. Let w2 ∈V be arbitrary. We then have:\nq1|2(w1 |w2) = p1|2(w1 |w2) p2|1(w2 |w1)∑\nw′∈Vp1|2(w′|w2) p2|1(w2 |w′)\nNow, consider the KL between the true unary conditionals and the MRF unary conditionals:\nKL(p1|2(·| w2) ||q1|2(·| w2)) =\n∑\nw∈V\np1|2(w|w2) log p1|2(w|w2)\nq1|2(w|w2)\n=\n∑\nw∈V\np1|2(w|w2) log\n∑\nw′∈Vp1|2(w′|w2) p2|1(w2 |w′)\np2|1(w2 |w)\n= log Ew∼p1|2(·|w2)[p2|1(w2 |w)] −Ew∼p1|2(·|w2)[log p2|1(w2 |w)]\nThis term is the Jensen gap, and in general it can be non-zero. To see this, suppose V= {a,b}and\nconsider the joint\np1,2(w1,w2) =\n{\n97\n100 w1,w2 = a\n1\n100 otherwise\n1155\nwith corresponding conditionals p2|1(x|b) = p1|2(x|b) = 1\n2 for all x∈V and\np2|1(x|a) = p1|2(x|a) =\n{\n97\n98 x= a\n1\n98 x= b\nNow, take w2 = b. We then have\nKL(p1|2(·| b) ||q1|2(·| b))\n= log Ew∼p1|2(·|b)[p2|1(b|w)] −Ew∼p1|2(·|b)[log p2|1(b|w)]\n= log\n(1\n2\n(1\n98 + 1\n2\n))\n−1\n2\n(\nlog 1\n98 + log 1\n2\n)\n= log\n( 1\n196 + 1\n4\n)\n−1\n2\n(\nlog 1\n196\n)\n≈1.27\nwhich demonstrates that the KL can be non-zero.\nC Arnold–Gokhale algorithm\nArnold and Gokhale (1998) study the problem of finding a near-compatible joint from unary conditionals,\nand provide and algorithm for the case of |S|= 2 . The algorithm initializes the starting pairwise\ndistribution qAG(1)\na,b|S (·,·| wS) to be uniform, and performs the following update until convergence:\nqAG(t+1)\na,b|S (wa,wb |wS) ∝\nqa|b,S(wa |wb,wS) + qb|a,S(wb |wa,wS)\n(\nqAG(t)\na|S (wa |wS)\n)−1\n+\n(\nqAG(t)\nb|S (wb |wS)\n)−1 . (5)\nD Qualitative example of MRF underperformance\nThis example from SNLI qualitatively illustrates a case where both the unary and pairwise perplexities\nfrom the MRF underperforms the MLM: “The [MASK]1 [MASK]2 at the casino”, where the tokens “man is”\nare masked. In this case, both MRFs assign virtually zero probability mass to the correct tokens, while the\nMLM assigns orders of magnitude more (around 0.2% of the mass of the joint). Upon inspection, this\narises because q2|1,S(is |man) ≈0.02 and q1|2,S(man |is) ≈2 ×10−5, which makes the numerator of\nqMRF\n1,2|S(man,is) be ≈0. The MRF could still assign high probability to this pair if the denominator is also\n≈0, but in this case we have q2|1,S(was |man) ≈0.33 and q1|2,S(man |was) ≈0.03, which makes the\ndenominator well above 0. This causes the completion “man is” to have disproportionately little mass in\nthe joint compared other to combinations (“man was”) that were ascribed more mass by BERT’s unary\nconditionals.\nE Token distance analysis\nWe also explore the effect of the distance between masked tokens on the pairwise negative log-likelihood\n(PNLL, lower is better; note this is equivalent to the log PPPL) of the joints built using the different\napproaches we considered. We considered two different kinds of distance functions between tokens: (i)\nthe absolute difference in the positions between the two masked tokens, and (ii) their syntactic distance\n(obtained by running a dependency parser on unmasked sentences).\nWe plot the results in Fig. 1 (SNLI) and Fig. 2 (XSUM). Note that the black bars denote the number\nof datapoints with that distance between the two masked tokens, where a syntactic distance of 0 means\nthat the two masked tokens belong to the same word, whereas a token distance of 0 means that the two\nmasked tokens are adjacent. The graphs indicate that the language modeling performance improvement\n(compared to using the MLM joint) is most prominent when masked tokens are close together, which is\nprobably because when the masked tokens are close together they are more likely to be dependent. In\nthis case, AG tends to do best, HCB and MRF tend to do similarly, followed by MRF-L and, finally, the\nconditionally independent MLM, which follows the trends observed in the paper.\n1156\n0 2 4 6 8\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\n0\n50\n100\n150\n200\nToken distance\nAverage PNLL\nNumber of examples\n2 4 6\n2\n2.5\n3\n3.5\n4\n4.5\n0\n50\n100\n150\n200\n250\nSyntactic distance\nAverage PNLL\nNumber of examples\nFigure 1: Pairwise NLL (PNLL) as a function of the token and syntactic distance between masked positions for\njoints built using the methods: MLM, MRF (Logit), MRF, HCB, AG on SNLI (Bowman et al., 2015). The gray bars\nrepresent the number of examples on the dataset that had that degree of separation.\n0 5 10 15\n1.5\n2\n2.5\n3\n3.5\n4\n0\n20\n40\n60\n80\nToken distance\nAverage PNLL\nNumber of examples\n2 4 6 8\n1.5\n2\n2.5\n3\n0\n50\n100\n150\nSyntactic distance\nAverage PNLL\nNumber of examples\nFigure 2: Pairwise NLL (PNLL) as a function of the token and syntactic distance between masked positions for\njoints built using the methods: MLM, MRF (Logit), MRF, HCB, AG on XSUM (Narayan et al., 2018). The gray\nbars represent the number of examples on the dataset that had that degree of separation.\n1157\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAvailable online\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nConsistent with intended use\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAvailable online\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1158\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1159",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7255992293357849
    },
    {
      "name": "Language model",
      "score": 0.6704567670822144
    },
    {
      "name": "Joint probability distribution",
      "score": 0.5720670819282532
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5397146344184875
    },
    {
      "name": "Markov process",
      "score": 0.4805755615234375
    },
    {
      "name": "Field (mathematics)",
      "score": 0.467288076877594
    },
    {
      "name": "Markov random field",
      "score": 0.46665215492248535
    },
    {
      "name": "Natural language processing",
      "score": 0.44742658734321594
    },
    {
      "name": "Hidden Markov model",
      "score": 0.4236012399196625
    },
    {
      "name": "Mathematics",
      "score": 0.2358894944190979
    },
    {
      "name": "Statistics",
      "score": 0.1758948564529419
    },
    {
      "name": "Segmentation",
      "score": 0.0
    },
    {
      "name": "Image segmentation",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}