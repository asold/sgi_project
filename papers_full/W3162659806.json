{
  "title": "Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level",
  "url": "https://openalex.org/W3162659806",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222413860",
      "name": "Zhong, Ruiqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227589002",
      "name": "Ghosh, Dhruba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2615303194",
      "name": "Klein, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173099975",
      "name": "Steinhardt, Jacob",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099624838",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2110065044",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3114796327",
    "https://openalex.org/W3098467034",
    "https://openalex.org/W3034491534",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W3034408878"
  ],
  "abstract": "Larger language models have higher accuracy on average, but are they better on every single instance (datapoint)? Some work suggests larger models have higher out-of-distribution robustness, while other work suggests they have lower accuracy on rare subgroups. To understand these differences, we investigate these models at the level of individual instances. However, one major challenge is that individual predictions are highly sensitive to noise in the randomness in training. We develop statistically rigorous methods to address this, and after accounting for pretraining and finetuning noise, we find that our BERT-Large is worse than BERT-Mini on at least 1-4% of instances across MNLI, SST-2, and QQP, compared to the overall accuracy improvement of 2-10%. We also find that finetuning noise increases with model size and that instance-level accuracy has momentum: improvement from BERT-Mini to BERT-Medium correlates with improvement from BERT-Medium to BERT-Large. Our findings suggest that instance-level predictions provide a rich source of information; we therefore, recommend that researchers supplement model weights with model predictions.",
  "full_text": "Are Larger Pretrained Language Models Uniformly Better? Comparing\nPerformance at the Instance Level\nRuiqi Zhong Dhruba Ghosh Dan Klein Jacob Steinhardt\nComputer Science Division, University of California, Berkeley\n{ruiqi-zhong, djghosh13, klein, jsteinhardt}@berkeley.edu\nAbstract\nLarger language models have higher accu-\nracy on average, but are they better on ev-\nery single instance (datapoint)? Some work\nsuggests larger models have higher out-of-\ndistribution robustness, while other work sug-\ngests they have lower accuracy on rare sub-\ngroups. To understand these differences, we\ninvestigate these models at the level of indi-\nvidual instances. However, one major chal-\nlenge is that individual predictions are highly\nsensitive to noise in the randomness in train-\ning. We develop statistically rigorous meth-\nods to address this, and after accounting for\npretraining and ﬁnetuning noise, we ﬁnd that\nour BERT-LARGE is worse than BERT- MINI\non at least 1 −4% of instances across MNLI,\nSST-2, and QQP, compared to the overall ac-\ncuracy improvement of 2 −10%. We also\nﬁnd that ﬁnetuning noise increases with model\nsize, and that instance-level accuracy has mo-\nmentum: improvement from BERT- MINI to\nBERT-MEDIUM correlates with improvement\nfrom BERT-MEDIUM to BERT-LARGE . Our\nﬁndings suggest that instance-level predictions\nprovide a rich source of information; we there-\nfore recommend that researchers supplement\nmodel weights with model predictions.\n1 Introduction\nHistorically, large deep learning models (Peters\net al., 2018; Devlin et al., 2019; Lewis et al., 2020;\nRaffel et al., 2019) have improved the state of\nthe art on a wide range of tasks and leaderboards\n(Schwartz et al., 2014; Rajpurkar et al., 2016; Wang\net al., 2018), and empirical scaling laws predict\nthat larger models will continue to increase per-\nformance (Kaplan et al., 2020). However, little is\nunderstood about such improvement at the instance\n(datapoint) level. Are larger models uniformly bet-\nter? In other words, are larger pretrained models\nbetter at every instance, or are they better at some\ninstances, but worse at others?\nPrior works hint at differing answers. Hendrycks\net al. (2020) and Desai and Durrett (2020) ﬁnd\nthat larger pretrained models consistently improve\nout-of-distribution performance, which implies that\nthey might be uniformly better at a ﬁner level.\nHenighan et al. (2020) claim that larger pretrained\nimage models have lower downstream classiﬁca-\ntion loss for the majority of instances, and they\npredict this trend to be true for other data modal-\nities (e.g. text). On the other hand, Sagawa et al.\n(2020) ﬁnd that larger non-pretrained models per-\nform worse on rare subgroups; if this result gener-\nalizes to pretrained language models, larger models\nwill not be uniformly better. Despite all the in-\ndirect evidence, it is still inconclusive how many\ninstances larger pretrained models perform worse\non.\nA na¨ıve solution is to ﬁnetune a larger model,\ncompare it to a smaller one, and ﬁnd instances\nwhere the larger model is worse. However, this\napproach is ﬂawed, since model predictions are\nnoisy at the instance level. On MNLI in-domain\ndevelopment set, even the same architecture with\ndifferent ﬁnetuning seeds leads to different pre-\ndictions on ∼8% of the instances. This is due to\nunder-speciﬁcation (D’Amour et al., 2020), where\nthere are multiple different solutions that can mini-\nmize the training loss. Since the accuracy improve-\nment from our BERT-BASE 1 to BERT-LARGE is\n2%, most signals across different model sizes will\nbe dominated by noise due to random seeds.\nTo account for the noise in pretraining and ﬁne-\ntuning, we deﬁne instance accuracyas “how often\na model correctly predicts an instance” (Figure 1\nleft) in expectation across pretraining and ﬁnetun-\ning seeds. We estimate this quantity by pretraining\n10 models with different seeds, ﬁnetuning 5 times\nfor each pretrained models (Figure 1 middle), and\n1This is not the original release by Devlin et al. (2019); we\npretrained models ourselves.\narXiv:2105.06020v1  [cs.CL]  13 May 2021\naveraging across them.\nHowever, this estimate is still inexact, and we\nmight falsely observe smaller models to be better\nat some instances by chance. Hence, we propose\na random baseline to estimate the fraction of false\ndiscoveries (Section 3, Figure 1 right) and formally\nupper-bound the false discoveries in Section 4. Our\nmethod provides a better upper bound than the clas-\nsical Benjamini-Hochberg procedure with Fisher’s\nexact test.\nUsing the 50 models for each size and our im-\nproved statistical tool, we ﬁnd that, on the MNLI\nin-domain development set, the accuracy “decays”\nfrom BERT-LARGE to BERT-MINI on at least ∼4%\nof the instances, which is signiﬁcant given that the\nimprovement in overall accuracy is 10%. These\ndecaying instances contain more controversial or\nwrong labels, but also correct ones (Section 4.2).\nTherefore, larger pretrained language models are\nnot uniformly better.\nWe make other interesting discoveries at the in-\nstance level. Section 5 ﬁnds that instance-level\naccuracy has momentum: improvement from MINI\nto MEDIUM correlates with improvement from\nMEDIUM to LARGE . Additionally, Section 6 at-\ntributes variance of model predictions to pretrain-\ning and ﬁnetuning random seeds, and ﬁnds that\nﬁnetuning seeds cause more variance for larger\nmodels. Our ﬁndings suggest that instance-level\npredictions provide a rich source of information;\nwe therefore recommend that researchers supple-\nment model weights with model predictions. In this\nspirit, we release all the pretrained models, model\npredictions, and code here: https://github.com/\nruiqi-zhong/acl2021-instance-level .\n2 Data, Models, and Predictions\nTo investigate model behavior, we considered dif-\nferent sizes of the BERT architecture and ﬁne-\ntuned them on Quora Question Pairs (QQP 2),\nMulti-Genre Natural Language Inference (MNLI;\nWilliams et al. (2020)), and the Stanford Sen-\ntiment Treebank (SST-2; Socher et al. (2013)).\nTo account for pretraining and ﬁnetuning noise,\nwe averaged over multiple random initializations\nand training data order, and thus needed to pre-\ntrain our own models rather than downloading\noff the internet. Following Turc et al. (2019) we\ntrained 5 architectures of increasing size: MINI\n2https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\n(L4/H256, 4 Layers with hidden dimension 256),\nSMALL (L4/H512), MEDIUM (L8/H512), BASE\n(L12/H768), and LARGE (L24/H1024). For each\narchitecture we pre-trained models with 10 differ-\nent random seeds and ﬁne-tuned each of them 5\ntimes (50 total) on each task; see Figure 1 middle.\nSince pretraining is computationally expensive, we\nreduced the context size during pretraining from\n512 to 128 and compensated by increasing train-\ning steps from 1M to 2M. Appendix A includes\nmore details about pretraining and ﬁnetuning and\ntheir computational cost, and Appendix B veriﬁes\nthat our cost-saving changes do not affect accuracy\nqualitatively.\nNotation. We use i to index an instance in the\nevaluation set, sfor model sizes, P for pretraining\nseeds and F for ﬁnetuning seeds. cis a random\nvariable of value 0 or 1 to indicate whether the\nprediction is correct. Given the pretraining seed P\nand the ﬁnetuning seed F, cs\ni = 1 if the model of\nsize sis correct on instance i, 0 otherwise. To keep\nthe notation uncluttered, we sometimes omit these\nsuperscripts or subscripts if they can be inferred\nfrom context.\nUnless otherwise noted, we present results on\nthe MNLI in-domain development set in the main\npaper.\n3 Comparing Instance Accuracy\nTo ﬁnd the instances where larger models are worse,\na na¨ıve approach is to ﬁnetune a larger pretrained\nmodel, compare it to a smaller one, and ﬁnd in-\nstances where the larger is incorrect but the smaller\nis correct. Under this approach, BERT-LARGE is\nworse than BERT-BASE on 4.5% of the instances\nand better on 7%, giving an overall accuracy im-\nprovement of 2.5%.\nHowever, this result is misleading: even if we\ncompare two BERT- BASE model with different\nﬁnetuning seeds, their predictions differ on 8% of\nthe instances, while their accuracies differ only by\n0.1%; Table 1 reports this baseline randomness\nacross model sizes. Changing the pretraining seed\nalso changes around 2% additional predictions be-\nyond ﬁnetuning.\nTable 1 also reports the standard deviation of\noverall accuracy, which is about 40 times smaller.\nSuch stability starkly contrasts with the noisiness at\nthe instance level, which poses a unique challenge.\nSeed 1Seed 2Seed 3Instance AccuracyInstance 1X ✓ ✓ 66%Instance 2X X X 0%Instance 3✓ X X 33%Instance 4✓ ✓ ✓ 100%\nAverage across seeds\nF(*)1F(*)2\nP1 X✓P2 ✓XP3 X✓\nFinetuning Seeds\nPretrain-ing SeedsMINI \nLARGE\nInstance 1Instance i\nModelSizes\n… ……… …✓✓✓X✓✓\nX✓✓XXX\nXX✓X✓✓\nInstances\nFigure 1: Left: Each column represents the same architecture trained with a different seed. We calculate accuracy\nfor each instance (row) by averaging across seeds (column), while it is usually calculated for each model by\naveraging across instances. Middle: A visual layout of the model predictions we obtain, which is a binary-valued\ntensor with 4 axes: model size s, instance i, pretraining seeds P and ﬁnetuning seeds F. Right: for each instance,\nwe calculate the accuracy gain from MINI to LARGE and plot the histogram in blue, along with a random baseline\nin red. Since the blue distribution has a bigger left tail, smaller models are better at some instances.\nDiffFTune DiffPTrain Stdall\nMINI 7.2% 10.7% 0.2%\nSMALL 7.2% 10.7% 0.3%\nMEDIUM 8.0% 10.7% 0.3%\nBASE 8.5% 10.6% 0.2%\nLARGE 8.6% 10.1% 0.2%\nTable 1: Larger model sizes are at the bottom rows.\nDiffFTune: how much do the predictions differ, if two\nmodels have the same pretraining seed but different\nﬁnetuning seeds F? Diff PTrain: the difference if the\npretraining seeds P are different. Std all: the standard\ndeviation of overall accuracy, around 40 times smaller\nthan DiffFTune.\nInstance-Level Metrics To reﬂect this noisiness,\nwe deﬁne the instance accuracyAccs\ni to be how\noften models of size spredict instance icorrectly,\nAccs\ni := EP,F [cs\ni ]. (1)\nThe expectation is taken with respect to the pre-\ntraining and ﬁnetuning randomness P and F. We\nestimate Accs\ni via the empirical average ˆAcc\ns\ni ac-\ncross 10 pretraining ×5 ﬁnetuning runs.\nWe histogram ˆAcc\ns\ni in Figure 2 (a). On most\ninstances the model always predicts correctly or\nincorrectly ( ˆAcc = 0 or 1), but a sizable fraction\nof accuracies lie between the two extremes.\nRecall that our goal is to ﬁnd instances where\nlarger models are less accurate, which we refer\nto as decaying instances. We therefore study the\ninstance differencebetween two model sizess1 and\ns2, deﬁned as\ns1\ns2 ∆Acci := Accs2\ni −Accs1\ni , (2)\nwhich is estimated by the difference between the\naccuracy estimates ˆAcc\ns\ni , i.e.\ns1\ns2\nˆ∆Acci := ˆAcc\ns2\ni − ˆAcc\ns1\ni . (3)\nWe histogram BASE\nLARGE\nˆ∆Acci in Figure 2 (b). We\nobserve a unimodal distribution centered near 0,\nwith tails on both sides. Therefore, the estimated\ndifferences for some instances are negative.\nHowever, due to estimation noise, we might\nfalsely observe this accuracy decay by chance.\nTherefore, we introduce a random baseline\ns1\ns2 ∆Acc′to control for these false discoveries. Re-\ncall that we have 10 smaller pretrained models and\n10 larger ones. Our baseline splits these into a\ngroup Aof 5 smaller + 5 larger, and another group\nBof the remaining 5 + 5. Then the empirical accu-\nracies ˆAcc\nA\nand ˆAcc\nB\nare identically distributed,\nso we take our baseline s1\ns2 ∆Acc′to be the differ-\nence ˆAcc\nA\n− ˆAcc\nB\n. We visualize and compare\nhow to calculate s1\ns2\nˆ∆Acc and s1\ns2 ∆Acc′in Figure 3.\nWe histogram this baseline BASE\nLARGE ∆Acc′ in\nFigure 2 (b), and ﬁnd that our noisy estimate\nBASE\nLARGE\nˆ∆Acc has a larger left tail than the baseline.\nThis suggests that decaying instances exist. We\nsimilarly compare MINI to LARGE in Figure 2 (c)\nand ﬁnd an even larger left tail.\n4 Quantifying the Decaying Instances\nThe left tail of ˆ∆Acc noisily estimates the frac-\ntion of decaying instances, and the left tail of the\nrandom baseline ∆Acc′ counts the false discov-\nery fraction due to the noise. Intuitively, the true\nfraction of decaying instances can be captured by\nthe difference of these left tails, and we formally\nquantify this below.\n(a) BASE vs. LARGE , Acc\n (b) BASE vs. LARGE , ∆Acc\n (c) MINI vs. LARGE , ∆Acc\nFigure 2: (a) The distribution of instance accuracy ˆAcci. (b, c) Histogram of instance difference estimate (x-axis),\nˆ∆Acc (blue) and its baseline ∆Acc′ (red) compares BASE and LARGE . To better visualize, we truncated the\ndensity (y-axis) above 2. Since the blue histogram has a larger left tail than the red one, there are indeed instances\nwhere larger models are worse.\nF(*)1F(*)2 F(*)3\nP1 X✓✓P2 ✓XXP3 XXXP4 X✓✓\nMINI\nPretrain-ing Seeds\n̂AccA=0.58\n̂AccB=0.58MiniLargeΔAcc′\u0000 =0=\n̂AccMINI=0.42̂AccLARGE=0.75F(*)1F(*)2 F(*)3\nP1 X✓✓P2 ✓✓XP3 ✓✓XP4 ✓✓✓\nLARGE - MINILARGE ̂ΔAcc=.33=\n-\nGroup A\nGroup B\nFigure 3: The tables are model predictions with visual\nnotations established in Figure 1 middle. ˆ∆Acc (blue)\nis the mean difference between the left and the right\ntable, each corresponding to a model size. The random\nbaseline ∆Acc′ (red) is the mean difference between\ngroup A(orange) cells and group B(green), which are\nidentically and independently distributed.\nSuppose instance iis drawn from the empirical\nevaluation distribution. Then we can deﬁne the true\ndecaying fraction Decay as\nDecay := Pi[∆Acci <0]. (4)\nSince ∆Acci is not directly observable and\nˆ∆Acci is noisy, we add a buffer and only consider\ninstances with ˆ∆Acci ≤t, which makes it more\nlikely (but still uncertain) that the true ∆Acci <0.\nWe denote this “discovery fraction” ˆDecay(t) as\nˆDecay(t) := Pi[ ˆ∆Acci ≤t]. (5)\nSimilarly, we deﬁne a baseline control (false\ndiscovery fraction) Decay′(t) := Pi[∆Acc′\ni ≤t].\nHence, ˆDecay and Decay′are the cumulative dis-\ntribution function of ˆ∆Acc and ∆Acc′(Figure 4).\nWe have the following theorem, which we for-\nmally state and prove in Appendix D:\nTheorem 1 (Informal) If all the random seeds are\nindependent, then for all thresholdst,\nDecay ≥E[ ˆDecay(t) −Decay′(t)] (6)\nProof Sketch Suppose we observe cs1\nR1...2k\nand\ncs2\nR2k+1...4k\n, where there are 2k different random\nseeds for each model size 3. Then\nˆ∆Acci := 1\n2k(\n2k∑\nj=1\ncs1\nRj,i −\n4k∑\nj=2k+1\ncs2\nRj,i), (7)\nand hence the discovery rate ˆDecay(t) is deﬁned\nas\nˆDecay(t) := 1\n|T|\n|T|∑\ni=1\n1[ ˆ∆Acc ≤t]. (8)\nFor the random baseline estimator, we have\n∆Acc′\ni := 1\n2k(\nk∑\nj=1\ncs1\nRj,i +\n3k∑\nj=2k+1\ncs2\nRj,i (9)\n−\n2k∑\nj=k+1\ncs1\nRj,i −\n4k∑\nj=3k+1\ncs2\nRj,i),\nand the false discovery controlDecay′is deﬁned\nas\nDecay′(t) := 1\n|T|\n|T|∑\ni=1\n1[∆Acc′\ni ≤t]. (10)\nFormally, the theorem states that\nDecay ≥ER1...R4k[ ˆDecay(t)−Decay′(t)], (11)\nwhich is equivalent to\n|T|∑\ni=1\n(1[∆Acci <0] −P[ ˆ∆Acci ≤t] (12)\n+P[∆Acc′\ni ≤t]) ≥0\n3We assumed even number of random seeds since we will\nmix half of the models from each size to compute the random\nbaseline\n6%\nFigure 4: The cumulative distribution function of the\nhistogram in Figure 2 (c); only the negative x-axis is\nshown because it corresponds to decays. The maxi-\nmum difference between the two curves (6%) is a lower\nbound of the true decaying fraction.\nHence, we can declare victory if we can prove\nthat for all i, if ∆Acci ≥0,\nP[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t].\nThis is easy to see, since ∆Acc′\ni and ˆ∆Acci are\nboth binomial distributions with the same n, but\nthe ﬁrst has a larger rate. 4 □\nRoughly speaking, the true decaying fraction\nis at least the difference between ˆDecay(t) and\nDecay′(t) at every threshold t. Therefore, we take\nthe maximum difference between ˆDecay(t) and\nDecay′(t) to lower-bound the fraction of decaying\ninstances.5 For example, Figure 4 estimates the\ntrue decaying fraction between MINI and LARGE\nto be at least 6%.\nWe compute this lower bound for other pairs of\nmodel sizes in Table 2, and the full results across\nother tasks and model size pairs are in Appendix C.\nIn all of these settings we ﬁnd a non-zero fraction\nof decaying instances, and larger model size differ-\nences usually lead to more decaying instances.\nUnfortunately, applying Theorem 1 as above\nis not fully rigorous, since some ﬁnetuning runs\nshare the same pretraining seeds and hence are de-\npendent.6 To obtain a statistically rigorous lower\nbound, we slightly modify our target of interest. In-\nstead of examining individual ﬁnetuning runs, we\nensemble our model across 5 different ﬁnetuning\nruns for each pretraining seed; these predictions\n4More details are in Appendix D.\n5Adaptively picking the best threshold t depending on the\ndata may incur a slight upward bias. Appendix E estimates\nthat the relative bias is at most 10% using a bootstrap method.\n6Although we anticipate such dependencies do not cause a\nsubstantial difference, as discussed in Appendix D.1.\ns1 \\s2 MINI SMALL BASE LARGE\nMINI N/A 9% 18% 21%\nSMALL 3% N/A 14% 18%\nBASE 6% 5% N/A 10%\nLARGE 6% 5% 2% N/A\nTable 2: We lower-bound the fraction of instances that\nimprove when model size changes from s1 (row) to s2\n(column). For example, when model size decreases\nfrom LARGE to MINI , 6% of instances improve (i.e.\ndecays).\nThreshold ˆDecay Decay ′ Diff\nt= −0.4 4.22% 3.49e−3 3.87%\n. . . . . . . . . . . .\nt= −0.9 0.91% 1.44e−7 0.91%\nt= −1.0 0.48% 2.06e−8 0.48%\nTable 3: Comparing MINI vs. LARGE by calculating\nthe discovery fraction ˆDecay, the false discovery con-\ntrol Decay′, and their difference (Diff) under different\nthresholds t. LARGE is worse on at least ∼4% (maxi-\nmum Diff) of instances.\nare essentially the same as individual ﬁnetuning\nruns, except that the ﬁnetuning randomness is av-\neraged out. Hence we obtain 10 independent sets\nof model predictions with different random seeds,\nwhich allows us to apply Theorem 1.\nWe compare MINI to LARGE using these ensem-\nbles and report the discovery ˆDecay and the base-\nline Decay′in Table 3. Taking the maximum differ-\nence across thresholds, we estimate at least∼4% of\ndecaying instances. This estimate is lower than the\nprevious 6% estimate, which used the full set of 50\nmodels’ predictions assuming they were indepen-\ndent. However, this is still a meaningful amount,\ngiven that the overall accuracy improvement from\nMINI to LARGE is 10%.\n4.1 Fisher’s Test + Benjamini-Hochberg\nHere is a more classical approach to lower-bound\nthe decaying fraction. For each instance, we com-\npute a signiﬁcance levelαunder the null hypothesis\nthat the larger model is better, using Fisher’s exact\ntest. We sort the signiﬁcance levels ascendingly,\nand call the pth percentile αp. Then we pick a\nfalse discovery rate q(say, 25%), ﬁnd the largest\nps.t. αp <pq, and estimate the decaying fraction\nto be at least p(1 −q). This calculation is known\nas the Benjamini-Hochberg procedure (Benjamini\nand Hochberg, 1995).\nTo compare our method with this classical ap-\ns1 s2 2 6 10\nMINI LARGE ours 1.9% 3.1% 4.0%\nMINI LARGE BH 0.0% 0.9% 1.9%\nBASE LARGE ours 0.4% 0.9% 1.2%\nBASE LARGE BH 0.0% 0.0% 0.0%\nTable 4: We compare our method to the Fisher’s exact\ntest + Benjamin-Hochberg (BH) procedure described in\nSection 4. For all different model size pairs and number\nof pretrained models available, ours always provides a\nhigher (better) lower bound of the decaying fraction.\nproach, we estimate the lower bound of the decay-\ning fraction for different pairs of model sizes with\ndifferent numbers of pretrained models available.\nTo make sure our choice of the false discovery rate\nqdoes not bias against the classical approach, we\nadaptively choose qto maximize its performance.\nAppendix F includes the full results and Table 4 is\na representative subset.\nWe ﬁnd that our approach is more powerful, par-\nticularly when the true decaying fraction is likely\nto be small and only a few models are available,\nwhich is usually the regime of interest. For exam-\nple, across all pairs of model sizes, our approach\nonly needs 2 random seeds (i.e. pretrained models)\nto provide a non-zero lower bound on the decaying\nfraction, while the classical approach sometimes\nfails to do this even with 10 seeds. Intuitively, when\nfewer seeds are available, the smallest possible sig-\nniﬁcance level for each instance is larger than the\ndecaying fraction, hence hurting the classical ap-\nproach.\n4.2 Understanding the Decaying Instances\nWe next manually examine the decaying instances\nto see whether we can ﬁnd any interpretable pat-\nterns. One hypothesis is that all the decaying frac-\ntions are in fact mislabeled, and hence larger mod-\nels are not in fact worse on any instances.\nTo investigate this hypothesis, we examined the\ngroup of instances where MINI\nLARGE\nˆ∆Acci ≤ −0.9.\nMINI is almost always correct on these instances,\nwhile LARGE is almost always wrong, and the false\ndiscovery fraction is tiny. For each instance, we\nmanually categorize it as either: 1) Correct, if the\nlabel is correct, 2) Fine, if the label might be con-\ntroversial but we could see a reason why this label\nis reasonable, 3) Wrong, if the label is wrong, or\n4) Unsure, if we are unsure about how to label\nthis instance. Each time we annotate, with 50%\nprobability we randomly sample either a decaying\nCorrect Fine Wrong Unsure\nMNLID 66% 17% 9% 5%\nMNLIC 86% 5% 5% 1%\nSST-2D 55% 8% 10% 25%\nSST-2C 88% 4% 0% 6%\nQQPD 60% 26% 10% 2%\nQQPC 87% 10% 1% 0%\nTable 5: MINI vs. LARGE . We examine whether there\nare mislabels for theDecaying fractions (superscript D)\nand the rest of the dataset ( Control group C). The de-\ncaying fraction contains more mislabels, but includes\ncorrect labels as well.\ninstance or an instance from the remaining dataset\nas a control. We are blind to which group it comes\nfrom.\nFor each task of MNLI, QQP, and SST-2, the ﬁrst\nauthor annotated 100 instances (decay + control\ngroup) (Table 5). We present all the annotated\ndecaying instances in Appendix J.\nConclusion We ﬁnd that the decaying fraction\nhas more wrong or controversial labels, compared\nto the remaining instances. However, even after\nwe adjust for the fraction of incorrect labels, the\nDecay fraction still exceeds the false discovery\ncontrol. This implies that MINI models are bet-\nter than LARGE models on some correctly labeled\ninstances. The second author followed the same\nprocedure and reproduced the same qualitative re-\nsults.\nHowever, we cannot ﬁnd an interpretable pattern\nfor these correctly labeled decaying instances by\nsimply eyeballing. We discuss future directions to\ndiscover interpretable categories in Section 7.\n5 Correlation of Instance Difference\nWe next investigate whether there is a momen-\ntum of instance accuracy increase: for example,\nif the instance accuracy improves from MINI (s1)\nto MEDIUM (s2), is it more likely to improve from\nMEDIUM (s2) to LARGE (s3)?\nThe na¨ıve approach is to calculate the Pearson\ncorrelation coefﬁcient between MINI\nMEDIUM\nˆ∆Acc and\nMEDIUM\nLARGE\nˆ∆Acc, and we ﬁnd the correlation to be zero.\nHowever, this is partly an artifact of accuracies be-\ning bounded in [0,1]. If MEDIUM drastically im-\nproves over MINI from 0 to 1, there is no room for\nLARGE to improve over MEDIUM . To remove this\ninherent negative correlation, we calculate the cor-\nrelation conditioned on the accuracy of the middle-\n(s1,s2,s3) ↓Buckets→ 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00\nSMALL ,MEDIUM ,BASE 0.07 0.22 0.29 0.40 0.35 0.33 0.38 0.27 0.24 0.13\nMINI ,MEDIUM ,LARGE 0.03 0.15 0.18 0.33 0.17 0.16 0.22 0.20 0.19 0.09\nTable 6: Each row corresponds to a triplet of model sizes. Each column t represents a bucket that contains\ninstances with ˆAcc\ns2\n∈[t−0.1,t]. Within each bucket, we calculate the Pearson correlation coefﬁcient between\nthe estimated accuracy improvements: s1\ns2\nˆ∆Acc and s2\ns3\nˆ∆Acc. These correlations are positive and become higher\nwhen model size differences are small.\nsized model, ˆAcc\nMEDIUM\n.\nTherefore, we bucket instances by their esti-\nmated MEDIUM accuracy into intervals of size 0.1,\nand we ﬁnd the correlation to be positive within\neach bucket (Table 6, row 2). This ﬁxes the prob-\nlem with the na¨ıve approach by getting rid of the\nnegative correlation, which could have misled us\nto believe that improvements by larger models are\nuncorrelated.\nWe additionally ﬁnd that the correlations be-\ntween improvements become stronger when model\nsize differences are smaller. Table 6 row 1 re-\nports results for another model size triplet with\nsmaller size difference, i.e. (s1, s2, s3) = (SMALL ,\nMEDIUM , BASE ), and the correlation is larger for\nall buckets. Results for more tasks and size triplets\nare in Appendix G and the same conclusions hold\nqualitatively.\n6 Variance at the Instance Level\nSection 3 found that the overall accuracy has rela-\ntively low variance, but model predictions are noisy.\nThis section formally analyzes variance at the in-\nstance level. For each instance, we decompose its\nloss into three components: Bias2, variance due to\npretraining randomness, and variance due to ﬁne-\ntuning randomness. Formally, we consider the 0/1\nloss:\nLi := 1 −ci = (1 −ci)2, (13)\nwhere ci is a random variable 0/1 indicating\nwhether the prediction is correct or incorrect, with\nrespect to randomness in pretraining and ﬁnetun-\ning. Therefore, by bias-variance decomposition\nand total variance decomposition, we have\nLi = Bias2i + PretVari + FineVari, (14)\nwhere, by using P and F as pretraining and ﬁne-\ntuning random seeds:\nBias2i := (1 −EP,F [ci])2, (15)\nPretVari := VarP [EF [ci]],\nFineVari := EP [VarF [ci]],\nBias2 PretVar FineVar\nMINI 0.203 0.017 0.036\nSMALL 0.179 0.017 0.036\nMEDIUM 0.157 0.014 0.040\nBASE 0.134 0.010 0.043\nLARGE 0.111 0.007 0.043\nTable 7: The bias, pretraining variance, and ﬁnetuning\nvariance for each model size, averaged across all test\ninstances. Finetuning variance is much larger than pre-\ntraining variance; larger models have larger ﬁnetuning\nvariance.\ncapturing “how wrong is the average prediction”,\nvariance due to pretraining, and variance due to\nﬁnetuning seeds, respectively.\nWe can directly estimate FineVar by ﬁrst calcu-\nlating the sample variance across ﬁnetuning runs\nfor each pretraining seed, and then averaging the\nvariances across the pretraining seeds. Estimating\nPretVar is more complicated. A na¨ıve approach is\nto calculate the empirical variance, across pretrain-\ning seeds, of the average accuracy across ﬁnetuning\nseeds. However, the estimated average accuracy for\neach pretraining seed is noisy itself, which causes\nan upward bias on the PretVar estimate. We cor-\nrect this bias by estimating the variance of the esti-\nmated average accuracy and subtracting it from the\nna¨ıve estimate; see Appendix H for details, as well\nas a generalization to more than two sources of ran-\ndomness. Finally, we estimateBias2 by subtracting\nthe two variance estimates from the estimated loss.\nFor each of these three quantities, Bias2,\nPretVar and FineVar, we estimate it for each in-\nstance, average it across all instances in the evalua-\ntion set, and report it in Table 7. The variances at\nthe instance level are much larger than the variance\nof overall accuracy, by a factor of 1000.\nWe may conclude from Table 7 that larger mod-\nels have larger ﬁnetuning variance and smaller pre-\ntraining variance. However, lower bias also inher-\nently implies lower variance. To see this, suppose\na model has perfect accuracy and hence zero bias;\nFigure 5: The pretraining variance conditioned on\nBias2 (the level of correctness). Each curve represents\na model size. Larger models have lower pretraining\nvariance across all levels of bias.\nthen it always predicts the same label (the correct\none) and hence has zero variance. This might favor\nlarger models and “underestimate” their variance,\nsince they have lower bias. Therefore, we calculate\nand compare the variances conditioned on the bias,\ni.e. PretVar(b2) := Ei[PretVari|Bias2i = b2].\nWe estimate PretVars(b2) using Gaussian pro-\ncess regression and plot it against b2 in Figure 5.\nWe ﬁnd that larger models still have lower pre-\ntraining variance across all levels of bias on the\nspeciﬁc task of MNLI under the 0/1 loss. To fur-\nther check whether our conclusions are general, we\ntested them on other tasks and under the squared\nloss Li := (1 −pi)2, where pi is the probability\nassigned to the correct class. Below are the conclu-\nsions that generally hold across different tasks and\nloss functions.\nConclusion We ﬁnd that 1) larger models have\nlarger ﬁnetuning variance, 2) LARGE has smaller\npretraining variance than BASE ; however, the or-\ndering between other sizes varies across tasks and\nlosses, and 3) ﬁnetuning variance is 2−8 times as\nlarge as pretraining variance, and the ratio is bigger\nfor larger models.\n7 Discussion and Future Directions\nTo investigate model behaviors at the instance level,\nwe produced massive amounts of model predictions\nin Section 2 and treated them as raw data. To ex-\ntract insights from them, we developed better met-\nrics and statistical tools, including a new method\nto control the false discoveries, an unbiased estima-\ntor for the decomposed variances, and metrics that\ncompute variance and correlation of improvements\nconditioned on instance accuracy. We ﬁnd that\nlarger pretrained models are indeed worse on a non-\ntrivial fraction of instances and have higher vari-\nance due to ﬁnetuning seeds; additionally, instance\naccuracy improvements from MINI to MEDIUM cor-\nrelate with improvements from MEDIUM to LARGE\n.\nOverall, we treated model prediction data as the\ncentral object and built analysis tools around them\nto obtain a ﬁner understanding of model perfor-\nmance. We therefore refer to this paradigm as\n“instance level understanding as data mining ”.\nWe discuss three key factors for this paradigm to\nthrive: 1) scalability and the cost of obtaining pre-\ndiction data, 2) other information to collect for each\ninstance, and 3) better statistical tools. We analyze\neach of these aspects below.\nScalability and Cost of Data Data mining is\nmore powerful with more data. How easy is it\nto obtain more model predictions? In our paper,\nthe main bottleneck is pretraining. However, once\nthe pretrained models are released, individual re-\nsearchers can download them and only need to\nrepeat the cheaper ﬁnetuning procedure.\nFurthermore, model prediction data are under-\nshared: while many recent research papers share\ntheir code or even model weights to help reproduce\nthe results, it is not yet a standard practice to share\nall the model predictions. Since many researches\nfollow almost the same recipe of pretraining and\nﬁnetuning (McCoy et al., 2020; Desai and Durrett,\n2020; Dodge et al., 2020), much computation can\nbe saved if model predictions are shared. On the\nother hand, as the state of the art model size is\nincreasing at a staggering speed7, most researchers\nwill not be able to run inference on a single instance.\nThe trend that models are becoming larger and\nmore similar necessitate more prediction sharing.\nMeta-Labels and Other Predictions Data min-\ning is more powerful with more types of informa-\ntion. One way to add information to each instance\nis to assign “meta-labels”. In the HANS (McCoy\net al., 2019) dataset, the authors tag each instance\nwith a heuristic 8 that holds for the training distri-\nbution but fails on this instance. Naik et al. (2018a)\nand Ribeiro et al. (2020) associate each instance\n7e.g. BERT (Devlin et al., 2019) has 340M parameters,\nwhile Switch-Transformer has over 1 trillion parameters (Fe-\ndus et al., 2021).\n8For example, “the label [entailment] is likely if the\npremise and the hypothesis have signiﬁcant lexical overlap”.\nwith a particular stress test type or subgroup, for ex-\nample, whether the instance requires the model to\nreason numerically or handle negations. Nie et al.\n(2020) collects multiple human responses to esti-\nmate human disagreement for each instance. This\nmeta-information can potentially help us identify\ninterpretable patterns for the disagreeing instances\nwhere one model is better than the other. On the\nﬂip side, identifying disagreeing instances between\ntwo models can also help us generate hypothesis\nand decide what subgroup information to annotate.\nWe can also add performance information on\nother tasks to each instance. For example, Pruk-\nsachatkun et al. (2020) studied the correlation\nbetween syntactic probing accuracy (Hewitt and\nLiang, 2019) and downstream task performance.\nTurc et al. (2019) and Kaplan et al. (2020) studied\nthe correlation between language modelling loss\nand the downstream task performance. However,\nthey did not analyze correlations at the instance\nlevel. We may investigate whether their results\nhold on the instance level: if an instance is easier\nto tag by a probe or easier to predict by a larger\nlanguage model, is the accuracy likely to be higher?\nStatistical Tools Data mining is more powerful\nwith better statistical tools. Initially we used the\nBenjamini-Hochberg procedure with Fisher’s ex-\nact test, which required us to pretrain 10 models\nto formally verify that the decaying instances ex-\nist. However, we later realized that 2 is in fact\nenough by using our approach introduced in Sec-\ntion 4. We could have saved 80% of the compu-\ntation for pretraining if this approach was known\nbefore we started.\nFuture work can explore more complicated met-\nrics and settings. We compared at most 3 different\nmodel sizes at a time, and higher order comparisons\nrequire novel metrics. We studied two sources of\nrandomness, pretraining and ﬁnetuning, but other\nsources of variation can be interesting as well, e.g.\ndifferences in pretraining corpus, different model\ncheckpoints, etc. To deal with more sophisticated\nmetrics, handle different sources and hierarchies of\nrandomness, and reach conclusions that are robust\nto noises at the instance level, researchers need to\ndevelop new inference procedures.\nTo conclude, for better instance level understand-\ning, we need to produce and share more prediction\ndata, annotate more diverse linguistic properties,\nand develop better statistical tools to infer under\nnoises. We hope our work can inform researchers\nabout the core challenges underlying instance level\nunderstanding and inspire future work.\nAcknowledgement\nWe thank Steven Cao, Cathy Chen, Frances Ding,\nDavid Gaddy, Colin Li, and Alex Wei for giving\ncomments on the initial paper draft. We would also\nlike to thank the Google Cloud TPU team for their\nhardware support.\nReferences\nYoav Benjamini and Yosef Hochberg. 1995. Control-\nling the false discovery rate: a practical and pow-\nerful approach to multiple testing. Journal of the\nRoyal statistical society: series B (Methodological),\n57(1):289–300.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAlexander D’Amour, Katherine Heller, Dan Moldovan,\nBen Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisen-\nstein, Matthew D Hoffman, et al. 2020. Un-\nderspeciﬁcation presents challenges for credibil-\nity in modern machine learning. arXiv preprint\narXiv:2011.03395.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1224–1234, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E. Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. In International Conference on Machine Learn-\ning.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2020. BERTs of a feather do not generalize to-\ngether: Large variability in generalization across\nmodels with similar test set performance. In Pro-\nceedings of the Third BlackboxNLP Workshop on An-\nalyzing and Interpreting Neural Networks for NLP,\npages 217–227, Online. Association for Computa-\ntional Linguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018a.\nStress test evaluation for natural language inference.\nIn The 27th International Conference on Computa-\ntional Linguistics (COLING), Santa Fe, New Mex-\nico, USA.\nAakanksha Naik, Abhilasha Ravichander, Norman\nSadeh, Carolyn Rose, and Graham Neubig. 2018b.\nStress test evaluation for natural language inference.\nIn Proceedings of the 27th International Conference\non Computational Linguistics, pages 2340–2353,\nSanta Fe, New Mexico, USA. Association for Com-\nputational Linguistics.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143,\nOnline. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe\nPang, Clara Vania, Katharina Kann, and Samuel R.\nBowman. 2020. Intermediate-task transfer learning\nwith pretrained language models: When and why\ndoes it work? In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5231–5247, Online. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and\nPercy Liang. 2020. An investigation of why over-\nparameterization exacerbates spurious correlations.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings\nof Machine Learning Research, pages 8346–8356.\nPMLR.\nLane Schwartz, Timothy Anderson, Jeremy Gwinnup,\nand Katherine Young. 2014. Machine translation\nand monolingual postediting: The AFRL WMT-\n14 system. In Proceedings of the Ninth Workshop\non Statistical Machine Translation, pages 186–194,\nBaltimore, Maryland, USA. Association for Compu-\ntational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962v2.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAdina Williams, Tiago Pimentel, Hagen Blix, Arya D.\nMcCarthy, Eleanor Chodroff, and Ryan Cotterell.\n2020. Predicting declension class from form and\nmeaning. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6682–6695, Online. Association for Computa-\ntional Linguistics.\nA Pretraining and Finetuning Details\nHere we explain how to obtain the model predic-\ntions, which are analyzed in later sections. To ob-\ntain these predictions under the “pretraining and\nﬁnetuning” framework (Devlin et al., 2019), we\nneed to decide a model size, perform pretraining,\nﬁnetune on a training set with a choice of hyper-\nparameters, and test the model on an evaluation\nset. We discuss each bolded aspects below.\nSize Similar to Turc et al. (2019), we exper-\nimented with the following ﬁve model sizes,\nlisted in increasing order: MINI (L4/H256) 9 ,\nSMALL (L4/H512), MEDIUM (L8/H512), BASE\n(L12/H768), and LARGE (L24/H1024).\nPretraining We used the pretraining code from\nDevlin et al. (2019) and the pre-training corpus\nfrom Li et al. (2020). Compared to the original\nBERT release, we used context size 128 instead of\n512, since computation cost grows quadratically\nwith respect to context size; we also pretrained for\n2M steps instead of 1M.\nTraining Set We consider 3 datasets: Quora\nQuestion Pairs (QQP) 10, Multi-Genre Natural Lan-\nguage Inference (MNLI; Williams et al. (2020)),\nand the Stanford Sentiment Treebank (SST-2;\n(Socher et al., 2013)). For QQP we used the of-\nﬁcial training split. For MNLI we used 350K out\nof 400K instances from the original training split,\nand added the remaining 50K to the evaluation set,\nsince the original in-domain development set only\ncontains 10K examples. For SST-2, we mix the\ntraining and development set of the original split,\nsplit the instances into 5 folds, train on four of them,\nand evaluate on the remaining fold.\nHyperparameters As in Turc et al. (2019), we\nﬁnetune 4 epochs for each dataset. For each task\nand model size, we tune hyperparameters in the\nfollowing way: we ﬁrst randomly split our new\ntraining set into 80% and 20%; then we ﬁnetune on\nthe 80% split with all 9 combination of batch size\n[16, 32, 64] and learning rate [1e-4, 5e-5, 3e-5],\nand choose the combination that leads to the best\naverage accuracy on the remaining 20%.\nEvaluation Set After ﬁnetuning our pretrained\nmodels, we evaluate them on a range of in-domain,\n94 Layers with hidden dimension 256\n10https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\nout-of-domain, or challenging datasets to obtain\nmodel predictions. Models trained on MNLI are\nalso evaluated on Stanford Natural Language In-\nference (SNLI; (Bowman et al., 2015)), Heuristic\nAnalysis for NLI Systems (HANS; (McCoy et al.,\n2019)), and stress test evaluations (STRESS; (Naik\net al., 2018b)). Models trained on QQP are also\nevaluated on Twitter Paraphrase Database (Twit-\nterPPDB; (Lan et al., 2017)).\nSince pretraining introduces randomness, for\neach model size s, we pretrain 10 times with dif-\nferent random seed P; since ﬁnetuning also intro-\nduces noise, for each pretrained model we pretrain\n5 times with different random seed F; besides, we\nalso evaluate the model at the checkpoints after E\nepochs, where E ∈[3,31\n3 ,32\n3 ,4].\nPretraining 10 models for all 5 model sizes alto-\ngether takes around 3840 hours on TPU v3 with 8\ncores. Finetuning all of them 5 times for all three\ntasks in our paper requires around 1200 hours.\nB Compare Our Models to the Original\nSince we decreased the pre-training context length\nto save computation, these models are not exactly\nthe same as the original BERT release by Devlin\net al. (2019) and Turc et al. (2019). We need to\nbenchmark our model against theirs to ensure that\nthe performance of our model is still reasonable\nand the qualitative trend still holds. For each each\nsize and task, we ﬁnetune the original model 5\ntimes and calculate the average of overall accuracy.\nThe comparison can be seen in Table 8. We ﬁnd\nthat our model does not substantially differ from\nthe original ones on QQP and SST-2. On MNLI,\nthe performance of our BERT- BASE and BERT-\nLARGE is 2∼3% below the original release, but\nthe qualitative trend that larger models have better\naccuracy still holds robustly.\nC More Instance Difference Results\nSimilar to Figure 4, for all 10 pairs of model sizes\nand all in-distribution instances of MNLI, SST-2,\nand QQP, we plot the cumulative density of ˆ∆Acc\nand ∆Acc′, or say, ˆDecay(t) and Decay′(t) in Fig-\nure 6, 7, and 8.\nAdditionally, for each pair of model sizes s1 and\ns2, we estimate “how much instances are getting\nbetter/worse accuracy?” by taking the maximum\ndifference between the red curve and the blue curve.\nWe report these results for MNLI, SST-2, and QQP\nin Table 9. We ﬁnd that larger model size gaps\nQQP MNLI SST-2\nMINI orig 88.2% 74.6% 92.8%\nMINI ours 87.3% 74.3% 92.8%\nSMALL orig 89.1% 77.3% 93.9%\nSMALL ours 88.7% 76.7% 93.9%\nMEDIUM orig 89.8% 79.6% 94.2%\nMEDIUM ours 89.5% 78.9% 94.2%\nBASE orig 90.8% 83.8% 95.0%\nBASE ours 90.6% 81.2% 94.6%\nLARGE orig 91.3% 86.8% 95.2%\nLARGE ours 91.0% 83.8% 94.8%\nTable 8: Comparing our pretrained model (superscript\norig) to the original release by Devlin et al. (2019) and\nTurc et al. (2019) (superscript ours). All pretrained\nmodels are ﬁnetuned with the training set and tested on\nthe in-distribution evaluation set described in Appendix\nA.\nlead to larger decaying fraction, but also larger\nimproving fraction as well.\nD Proof of Theorem 1\nFormal Setup Our goal is to show that if all the\nrandom seeds are independent,\nDecay ≥E[ ˆDecay(t) −Decay′(t)] (16)\nMore concretely, suppose each instance is in-\ndexed by i, the set of all instances is T, and the\nrandom seed is R; then cs\nR ∈{0,1}|T| is a ran-\ndom |T| dimensional vector, where cs\nR,i = 1 if the\nmodel of size s correctly predicts instance i un-\nder the random seed R. We are comparing model\nsize s1 and s2, where s2 is larger; to keep nota-\ntion uncluttered, we omit these indexes whenever\npossible.\nSuppose we observe cs1\nR1...2k\nand cs2\nR2k+1...4k\n,\nwhere there are 2kdifferent random seeds for each\nmodel size 11. Then\nˆ∆Acci := 1\n2k(\n2k∑\nj=1\ncs1\nRj,i −\n4k∑\nj=2k+1\ncs2\nRj,i), (17)\nand hence the discovery rate ˆDecay(t) is deﬁned\nas\nˆDecay(t) := 1\n|T|\n|T|∑\ni=1\n1[ ˆ∆Acc ≤t]. (18)\n11We assumed even number of random seeds since we will\nmix half of the models from each size to compute the random\nbaseline\nFor the random baseline estimator, we have\n∆Acc′\ni := 1\n2k(\nk∑\nj=1\ncs1\nRj,i +\n3k∑\nj=2k+1\ncs2\nRj,i (19)\n−\n2k∑\nj=k+1\ncs1\nRj,i −\n4k∑\nj=3k+1\ncs2\nRj,i),\nand the false discovery controlDecay′is deﬁned\nas\nDecay′(t) := 1\n|T|\n|T|∑\ni=1\n1[∆Acc′\ni ≤t]. (20)\nTo reiterate, the deﬁnition of the true decay rate\nis\nDecay = 1\n|T|\n|T|∑\ni=1\n1[∆Acci <0]. (21)\nOur goal is to prove that\nDecay ≥ER1...R4k[ ˆDecay(t) −Decay′(t)] (22)\nProof By re-arranging terms and linearity of ex-\npectation, Equation 22 is equivalent to the follow-\ning\n|T|∑\ni=1\n(1[∆Acci <0] −P[ ˆ∆Acci ≤t] (23)\n+P[∆Acc′\ni ≤t]) ≥0\nHence, we can declare victory if we can prove\nthat for all i,\n1[∆Acci <0] −P[ ˆ∆Acci ≤t] (24)\n+P[∆Acc′\ni ≤t] ≥0\nTo prove Equation 24, we observe that ifAcci <\n0, since the probabilities are bounded by 0 and 1,\nits left-hand side must be positive. Therefore, we\nonly need to prove that\n∆Acci ≥0 (25)\n⇒P[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t],\nwhich will be proved in Lemma 1.□\nLemma 1\n∆Acci ≥0 (26)\n⇒P[∆Acc′\ni ≤t] ≥P[ ˆ∆Acci ≤t],\nMNLI s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.087 0.136 0.179 0.214\nSMALL 0.033 0.000 0.089 0.139 0.180\nMEDIUM 0.050 0.028 0.000 0.090 0.143\nBASE 0.060 0.048 0.026 0.000 0.101\nLARGE 0.059 0.052 0.040 0.021 0.000\nQQP s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.057 0.076 0.100 0.107\nSMALL 0.019 0.000 0.039 0.073 0.084\nMEDIUM 0.029 0.014 0.000 0.044 0.063\nBASE 0.034 0.027 0.016 0.000 0.032\nLARGE 0.036 0.031 0.027 0.016 0.000\nSST-2 s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.037 0.043 0.052 0.057\nSMALL 0.010 0.000 0.015 0.031 0.036\nMEDIUM 0.016 0.008 0.000 0.020 0.028\nBASE 0.019 0.014 0.009 0.000 0.014\nLARGE 0.020 0.017 0.015 0.008 0.000\nTable 9: On QQP, MNLI in domain development set and SST-2 we lowerbound the fraction of instances that\nimproves when model size changes from s1 (row) to s2 (column).\nFor m= 1,2, deﬁne\npsm\ni := ER[csm\ni ], (27)\nthen\nps1\ni ≤ps2\ni (28)\nSince cs1\ni and cs2\ni are both Bernoulli random vari-\nables with rate ps1\ni and ps2\ni respectively, we can\nwrite down the probability distribution of ˆ∆Acci\nand ∆Acc′\ni as the sum/difference of several bino-\nmial variables, i.e.\nˆ∆Acci ∼(Binom(k,ps2\ni ) + Binom(k,ps2\ni ) (29)\n−Binom(k,ps1\ni ) −Binom(k,ps1\ni ))/2k,\nand\n∆Acc′i ∼(Binom(k,ps2,i) + Binom(k,ps1,i)\n(30)\n−Binom(k,ps1,i) −Binom(k,ps2,i))/2k\nps1\ni ≤ps2\ni , Binom(k,ps2,i)) ﬁrst order stochas-\ntically dominates Binom(k,ps1,i). Therefore,\n∆Acc′i dominates ˆ∆Acci, hence completing the\nproof. □\nD.1 Independent Seed Assumption\nWe notice that Theorem 1 requires the seeds Rto\nbe independent. This assumption does not hold on\nour data, since some ﬁnetuning runs share the same\npretraining seeds. Therefore, the above proof no\nlonger holds. Speciﬁcally, Lemma 1 fails because\nˆ∆Acc and ∆Acc′are no longer binomial variables,\nand the later does not necessarily dominate the\nﬁrst. Here is a counter-example, if the seeds are\nnot entirely independent.\nHypothetically, suppose we are comparing a\nsmaller model s1 and a larger model s2. For the\nsmaller model, with .1 probability it ﬁnds a per-\nfect pretrained model that always predict correctly\nacross all ﬁnetuning runs and with .9 probability\nit ﬁnds a bad pretrained model that predict always\nincorrectly. For the larger model, with probability 1\nit ﬁnds an average pretrained model that predict cor-\nrectly for .2 fraction of ﬁnetuning runs. The larger\nmodel is on average better, because it has .2 >.1\nprobability to be correct. Hence, ∆Acc >0\nSuppose we observe 2 independent pretraining\nseeds for each size and inﬁnite number of ﬁne-\ntuning seeds for each pretraining seed, and let us\nconsider the threshold -0.8. Then\nP[ ˆ∆Acci ≤−0.8] (31)\n= 0.01 ≥0 = P[∆Acc′\ni ≤−0.8] (32)\nThe event that ˆ∆Acci ≤−0.8 happens with proba-\nbility 0.01 when both of the two small pretrained\nmodels have good pretraining seeds, and ∆Acc′\ni is\nat least -0.5 and will never be less than -0.8.\n(a)\n (b)\n (c)\n (d)\n (e)\n(f)\n (g)\n (h)\n (i)\n (j)\nFigure 6: Similar to Figure 4, on MNLI in-distribution development set, for each pair of model sizes, we plot the\ncumulative density function of instance differences.\n(a)\n (b)\n (c)\n (d)\n (e)\n(f)\n (g)\n (h)\n (i)\n (j)\nFigure 7: Similar to Figure 4, on SST-2, for each pair of model sizes, we plot the cumulative density function of\ninstance differences.\nThe key idea behind this counter-example is that\neven if the larger model has better average, the\ndistribution of average ﬁnetuning accuracy for dif-\nferent pretraining seeds might not stochastically\ndominate the one with lower average because of\noutliers. Hence, a priori, this is unlikely to happen\nin practice, since pretraining variance is generally\nsmall, and we have multiple pretraining seeds to\naverage out the outliers. Nevertheless, future work\nis needed to make a more rigorous argument.\nE Upward Bias of Adaptive Thresholds\nIn section 3 we picked the best threshold that can\nmaximize the lowerbound, which can incur a slight\nupward bias. Here we estimate that the bias is at\nmost 10% relative to the unbiased lowerbound with\na bootstrapping method.\nWe use the empirical distribution of 10 pre-\ntrained models as the ground truth distribution for\nbootstrapping. We ﬁrst compute a best threshold\nwith 10 sampled smaller and larger pretrained mod-\nels, and then compute the lowerbound Lwith this\nthreshold on another sample of 10 smaller and\nlarger models. Intuitively, we use one bootstrap\nsample (which contains 10 smaller pretrained mod-\nels and 10 larger pretrained models) as the devel-\nopment set to “tune the threshold”, and then use\nthis threshold on a fresh bootstrap sample to com-\npute the lowerbound. We refer to the lowerbound\nthat uses the best threshold as L∗, and compute the\nrelative error E[(L∗−L)]/E[L)], where the expec-\ntation is taken with respect to bootstrap samples.\nWe report all results in Table 10. In general, we\nﬁnd that the upward bias is negligible, which is at\nmost around 10%.\n(a)\n (b)\n (c)\n (d)\n (e)\n(f)\n (g)\n (h)\n (i)\n (j)\nFigure 8: Similar to Figure 4, on QQP in-domain development set, for each pair of model sizes, we plot the\ncumulative density function of instance differences.\nF Comparison with Signiﬁcance Testing\nWe also experimented with the classical approach\nthat calculates the signiﬁcance-level for each in-\nstance and then use the Benjamini-Hochberg proce-\ndure to lowerbound the decaying fraction. To make\nsure that we are comparing with this approach\nfairly, we lend it additional power by picking the\nfalse discovery rate that can maximize the true dis-\ncovery counts. We report the decaying fraction on\nMNLI in-domain development set found by this\nclassical method and compare it with our method\nfor different model size differences in Table 11;\nwe also simulate situations when we have fewer\nmodels.\nIn general, we ﬁnd that our method always pro-\nvide a tighter (higher) lowerbound than the classical\nmethod, and 2 models are sufﬁcient to verify the\nexistence (i.e. lowerbound >0) of the decaying\nfraction; in contrast, the classical method some-\ntimes fails to do this even with 10 models, e.g.,\nwhen comparing BASE to LARGE .\nIntuitively, our approach provides a better lower-\nbound because it better makes use of the infor-\nmation that on most instances, both the smaller\nand the larger models agree and predict completely\ncorrectly or incorrectly12. For an extreme exam-\nple, suppose we only observe 2 smaller models\nand 2 larger models, and inﬁnite number of dat-\napoints, whose predictions are independent. On\n99.98% datapoints, both models have instance ac-\ncuracy 1; on 0.01% datapoints, smaller model is\ncompletely correct while bigger completely wrong,\n12This is for intuition, though, and we do not need any\nassumption on the prior of instance accuracy, which requires\na Bayes interpretation.\nwhile on the rest 0.01% smaller completely wrong\nbut bigger completely correct. Setting threshold\nto be 2, our decay estimate ˆDecay is 0.01%, while\nDecay′ = 0: since the models either completely\npredict correct or wrongly, there is never a false\ndiscovery. Therefore, our method can provide the\ntightest lowerbound 0.01% in this case. On the\nother hand, since we only have 4 models in total,\nthe lowest signiﬁcance-level given by the ﬁsher ex-\nact test is 17% ≫0.1%, hence the discovery made\nby the Benjamin-Hochberg procedure is 0.\nG More Results on Momentum\nWe report more results on the correlation between\ninstance differences. Speciﬁcally, for one triplet\nof model sizes (e.g. MINI ⇒MEDIUM ⇒LARGE\n), for each group of instances that have similar\nˆAcc\nMEDIUM\n, we calculate the correlation between\ninstance differences, i.e. the Pearson-R score be-\ntween MINI\nMEDIUM ∆Acc and MEDIUM\nLARGE ∆Acc. All results\ncan be seen in Table 12.\nWe observe that\n• For nearly all buckets, the improvements are\npositively correlated.\n• When model size gap becomes larger (e.g.\nMINI ,MEDIUM ,LARGE has the largest model\nsize differences), the correlation decreases.\nH Loss Decomposition and Estimation\nIn this section, under the bias-variance decomposi-\ntion and total variance decomposition framework,\nwe decompose loss into four components: bias,\nvariance brought by pretraining randomness, by\nMNLI s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.031 0.027 0.026 0.020\nSMALL 0.108 0.000 0.027 0.023 0.019\nMEDIUM 0.095 0.116 0.000 0.028 0.023\nBASE 0.093 0.100 0.144 0.000 0.026\nLARGE 0.097 0.103 0.117 0.149 0.000\nQQP s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.025 0.022 0.021 0.020\nSMALL 0.127 0.000 0.040 0.020 0.020\nMEDIUM 0.093 0.146 0.000 0.032 0.031\nBASE 0.087 0.119 0.123 0.000 0.049\nLARGE 0.090 0.105 0.079 0.106 0.000\nSST-2 s1 \\s2 MINI SMALL MEDIUM BASE LARGE\nMINI 0.000 0.028 0.022 0.021 0.019\nSMALL 0.117 0.000 0.047 0.031 0.029\nMEDIUM 0.075 0.093 0.000 0.063 0.035\nBASE 0.068 0.067 0.085 0.000 0.071\nLARGE 0.071 0.067 0.060 0.098 0.000\nTable 10: The same table as 10, except that we are now calculating the relative upward biasE[(L∗−L)]/E[L)] as\ndescribed in Section E.\ns1 s2 method 2 4 6 8 10\nMINI SMALL ours 0.004 0.011 0.016 0.020 0.023\nMINI SMALL BH 0.000 0.000 0.000 0.000 0.002\nMINI MEDIUM ours 0.012 0.019 0.026 0.032 0.035\nMINI MEDIUM BH 0.000 0.000 0.003 0.008 0.011\nMINI BASE ours 0.019 0.028 0.035 0.040 0.042\nMINI BASE BH 0.000 0.000 0.008 0.014 0.020\nMINI LARGE ours 0.019 0.027 0.031 0.037 0.040\nMINI LARGE BH 0.000 0.000 0.009 0.015 0.019\nSMALL MEDIUM ours 0.002 0.006 0.010 0.015 0.017\nSMALL MEDIUM BH 0.000 0.000 0.000 0.000 0.000\nSMALL BASE ours 0.013 0.020 0.025 0.030 0.033\nSMALL BASE BH 0.000 0.000 0.002 0.006 0.011\nSMALL LARGE ours 0.015 0.021 0.026 0.031 0.033\nSMALL LARGE BH 0.000 0.000 0.005 0.009 0.013\nMEDIUM BASE ours 0.006 0.010 0.013 0.014 0.016\nMEDIUM BASE BH 0.000 0.000 0.000 0.000 0.001\nMEDIUM LARGE ours 0.010 0.014 0.019 0.022 0.023\nMEDIUM LARGE BH 0.000 0.000 0.002 0.004 0.006\nBASE LARGE ours 0.004 0.005 0.009 0.010 0.012\nBASE LARGE BH 0.000 0.000 0.000 0.000 0.000\nTable 11: We compare each pair of model sizes s1 and s2 and report the lower bound provided by our method and\nthe Benjamin-Hochberg (BH) procedure. The numbers in column name denote how many pretrained model we\nused to obtain the lower bounds.\nMNLI. Buckets ⇒ 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00\nMINI ,SMALL ,MEDIUM 0.00 0.18 0.19 0.18 0.23 0.26 0.24 0.23 0.20 0.12\nSMALL ,MEDIUM ,BASE 0.07 0.22 0.29 0.40 0.35 0.33 0.38 0.27 0.24 0.13\nMEDIUM ,BASE ,LARGE 0.05 0.09 0.17 0.33 0.20 0.30 0.12 0.13 0.16 0.09\nMINI ,MEDIUM ,LARGE 0.03 0.15 0.18 0.33 0.17 0.16 0.22 0.20 0.19 0.09\nQQP. Buckets ⇒ 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00\nMINI ,SMALL ,MEDIUM 0.03 0.21 0.18 0.21 0.21 0.25 0.18 0.16 0.10 0.06\nSMALL ,MEDIUM ,BASE 0.01 0.17 0.23 0.19 0.24 0.22 0.24 0.19 0.16 0.05\nMEDIUM ,BASE ,LARGE -0.02 0.16 0.09 0.23 0.17 0.10 0.14 0.14 0.09 -0.01\nMINI ,MEDIUM ,LARGE -0.01 0.07 0.14 0.09 0.16 0.09 0.16 0.07 0.10 0.07\nSST-2. Buckets ⇒ 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00\nMINI ,SMALL ,MEDIUM 0.09 0.26 0.43 0.22 0.28 0.24 0.27 0.35 0.20 0.12\nSMALL ,MEDIUM ,BASE 0.07 0.12 0.22 0.40 0.07 0.20 0.10 0.12 0.19 0.06\nMEDIUM ,BASE ,LARGE 0.01 0.24 0.29 0.35 0.19 0.19 0.26 0.39 0.15 0.03\nMINI ,MEDIUM ,LARGE 0.01 0.17 0.11 0.41 0.04 0.29 0.16 0.21 0.15 0.07\nTable 12: Sorted in ascending order, the model sizes are MINI , SMALL , MEDIUM , BASE , and LARGE . The\nthree model sizes listed for each row represents the model size of interest: for example, MINI ,MEDIUM ,LARGE\nmeans that we are calculating the correlation between MINI\nMEDIUM ∆Acc and MEDIUM\nLARGE ∆Acc. Each column trepresents\na bucket that contains instances with middle size accuracy in [t −0.1,t]. For example, if the row name is\nMINI ,MEDIUM ,LARGE , then the column 0.2 corresponds to a bucket where ˆAcc\nMEDIUM\ni is between 0.1 and 0.2.\nWe calculate the PearsonR correlation score between MINI\nMEDIUM\nˆ∆Acc and MEDIUM\nLARGE\nˆ∆Acc across all instances in the\nbucket.\nﬁnetuning randomness, and across different check-\npoints throughout training. We formally deﬁne the\nquantities we want to estimate in Appendix H.1,\npresent an unbiased estimator for these quantities\nin Appendix H.2, and show that our method can\nbe generalized to arbitrary number of source of\nrandomness in Appendix H.3.\nSpeciﬁcally, the main paper focused on scenar-\nios with 2 sources of randomness: pretraining and\nﬁnetuning. We discuss the case with 3 sources of\nrandomness in the appendix, rather than 2 as in the\nmain paper, because it is easier to understand the\ngeneral estimation strategy in the case of 3.\nH.1 Formalizing Decomposition\nRecall that P is the pretraining seed, F is the ﬁne-\ntuning seed, E represents a model checkpoint, i\nindexes each instance (datapoint). cs,i\nP,F,E = 1 if\nthe model of size swith pretraining seed pand ﬁne-\ntuning seed F, and trained for Eepochs is correct\non datapoint i, and 0 otherwise. Notice that we\nmove the instance index from the subscript to the\nsuperscript, since we now use subscript for random\nseeds, and instance index can be omitted in most\nof our derivations.\nThe expected squared loss Lof model son in-\nstance ican then be written as\nLs,i = EP,F,E [(1 −cs,i\nP,F,E )2] (33)\nSince we will analyze this term at a datapoint\nlevel, we drop the subscript s and i to keep the\nnotation uncluttered. By the standard bias variance\ndecomposition and total variance decomposition,\nwe decompose the loss Linto four terms:\nL=Bias2 + PretVar (34)\n+ FineVar + CkptVar.\nWe will walk through the meaning and deﬁnition\nof these four terms one by one. Bias2 captures how\nbad is the average prediction, deﬁned as\nBias2 = (1 −EP,F,E [cP,F,E ])2. (35)\nPretVar captures the variance brought by ran-\ndomness in pretraining, and is deﬁned as\nPretVar = VarP [EF,E[cP,F,E ]]. (36)\nSimilarly, we deﬁne the variance brought by ran-\ndomness in ﬁnetuning FineVar\nFineVar = EP [VarF [EE[cP,F,E ]]], (37)\nand that by ﬂuctuations across checkpoints e\nCkptVar = EP,F [VarE[cP,F,E ]]. (38)\nH.2 Unbiased Estimation\nWe ﬁrst describe the data on which we apply our\nestimator. Suppose we pretrain Pmodels with\ndifferent random seeds, for each of the P pre-\ntrained models we ﬁnetune with Fdifferent ran-\ndom seeds, and we evaluate at Edifferent check-\npoints. Then ∀j ∈[P],k ∈[F],l ∈[E] 13 , we\nobserve Pj,Fjk,Ejkl,and cPj,Fjk,Ejkl , where each\nobserved P,F and Eare i.i.d. distributed. Our goal\nis to estimate from cthe four quantities described\nin the previous section.\nH.2.1 Estimating CkptVar\nIt is straightforward to estimate CkptVar. The\nestimator ˆCkptVar deﬁned below is unbiased:\nˆCkptVar := 1\nPF\n∑\nj∈[P],k∈[F]\nˆVar\nPj,Fjk\nE , (39)\nwhere\nˆVar\nPj,Fjk\nE := 1\nE− 1\n∑\nl∈E\n(cPj,Fjk,Ejkl −¯cPj,Fjk )2,\n(40)\nand\n¯cPj,Fjk := 1\nE\n∑\nl∈[E]\ncPj,Fjk,Ejkl . (41)\nˆCkptVar is unbiased, since ˆVar\nPj,Fjk\nE is an un-\nbiased estimation of variance of c with ﬁxed Pj\nand Fjk, and randomness E, i.e.\nEEij(·) [ ˆVar\nPj,Fjk\nE ] = (42)\nVarE[cP,F,E |P = Pj,F = Fjk].\nTherefore, ∀j ∈[P],k ∈[F], we have\nEPj,Fjk [ ˆVar\nPj,Fjk\nE ] = CkptVar, (43)\nand hence by linearity of expectation\nEP(·),F(·),E(·) [ ˆCkptVar] = CkptVar. (44)\nH.2.2 Estimating FineVar\nAs before, by linearity of expectation, we can de-\nclare victory if we can develop an unbiased esti-\nmator for the following quantity and then average\nacross Pj:\nVarF [EE[cP,F,E ]|P = Pj], (45)\nwhich verbally means ”variance across different\nﬁnetuning seeds of the mean of c over different\n13[L] := {l : l ∈N, l∈[0, L−1]}\ncheckpoints E, conditioned on the pretraining seed\nPj.”\nSince Pj is ﬁxed for this estimator, we drop the\nsubscripts P to keep notation uncluttered. There-\nfore, we want to estimate\nVarF := VarF [EE[cF,E]] (46)\nA naive solution is to take ﬁrst take the mean\n¯cFk of cfor each Fk, i.e.\n¯cFjk := 1\nE\n∑\nl∈[E]\ncFk,Ekl, (47)\nand then calculate the sample variance ˜VarF of ¯c\nwith respect to F:\n˜VarF := 1\nF− 1\n∑\nk∈[F]\n(¯cFk −¯c)2, (48)\nwhere\n¯c:= 1\nF\n∑\nk∈[F]\n¯cFk (49)\nHowever, this would create an upward bias: the\nempirical mean ¯cFjk is a noisy estimate of the pop-\nulation mean EE[cFjk,E], and hence increases let\n˜VarF over-estimate the variance. Imagine a sce-\nnario where VarF is in fact 0; however, since ¯cFjk\nis a noisy estimate, ˜VarF will sometimes be posi-\ntive but never below 0. As a result, E[ ˜VarF ] >0,\nwhich is a biased estimator.\nWe introduce the following general theorem to\ncorrect this bias.\nTheorem 2 Suppose Dk,k ∈ [F] are indepen-\ndently sampled from the same distributionΞ, which\nis a distribution of distributions;ˆµk is an unbiased\nestimator ofEX∈Dk[X], andˆφk to be an unbiased\nestimator of the variance ofˆµk, then\nˆVarF = 1\nF− 1\n∑\nk∈[F]\n(ˆµk −ˆµ)2 (50)\n−1\nF\n∑\nk∈[F]\nˆφk\nis an unbiased estimator for\nV = VarD∼Ξ[EX∼D[X]], (51)\nwhere\nˆµ:= 1\nF\n∑\nk∈[F]\nˆµk (52)\nIn this estimator, the ﬁrst term “pretends” that ˆµ·\nare perfect estimator for the population mean and\ncalculate the variance, while the second term cor-\nrects for the fact that the empirical mean estimation\nis not perfect. Notice the theorem only requires\nthat ˆµand ˆφare unbiased, and is agnostic to the\nactual computation procedure by these estimators.\nProof We deﬁne the population mean of Dk to\nbe µk, i.e.\nµk := EX∼Dk[X], (53)\nand the population mean of µk across randomness\nin Dto be µ, i.e.\nµ:= ED∼Ξ[EX∼D[X]] (54)\nWe look at the ﬁrst term of the estimator in equa-\ntion 50:\n1\nF− 1E[\n∑\nk∈[F]\n(ˆµk −ˆµ)2] (55)\n= 1\nF− 1E[\n∑\nk∈[F]\n((ˆµk −µk) −(ˆµ−µ)\n+ (µk −µ))2]\n= 1\nF− 1E[\n∑\nk∈[F]\n[(ˆµk −µk)2 + (ˆµ−µ))2\n+ (µk −µ)2 −2(ˆµk −µk)(ˆµ−µ)\n−2( ˆµk −µk)(ˆµ−µ)]]\nThere are 5 summands within ∑\nk∈[F], and we\nlook at them one by one:\nE[\n∑\nk∈[F]\n(ˆµk −µk)2] = E[\n∑\nk∈[F]\nˆφk], (56)\nE[(ˆµ−µ)2] = E[(µ−1\nF\n∑\nk∈[F]\nµk) (57)\n+ 1\nF\n∑\nk∈[F]\n(µk −ˆµk))2]\n= 1\nFV + 1\nF2\n∑\nk∈[F]\nE[ˆφk]\nE[\n∑\nk∈[F]\n(µk −µ)2] = FV (58)\nE[−2\n∑\nk∈[F]\n(ˆµk −µk)(ˆµ−µ)] (59)\n= −2\nFE[\n∑\nk∈[F]\nˆφk].\nE[−2\n∑\nk∈[F]\n(ˆµk −µk)(ˆµ−µ)]\n= −2V. (60)\nPutting these ﬁve terms together, we continue\ncalculating Equation 55:\n1\nF− 1E[\n∑\nk∈[F]\n(ˆµk −ˆµ)2] (61)\n= 1\nF− 1E[\n∑\nk∈[F]\nˆφk\n+ F( 1\nFV + 1\nF2\n∑\nk∈[F]\nE[ˆφk])\n+ FV\n−2\nF\n∑\nk∈[F]\nˆφk\n−2V]\n= V + E[ 1\nF\n∑\nk\nˆφk]\nThen from Equation 50, we can tell that ˆVarF\nis unbiased. □\nNow we come back to the topic of developing an\nunbiased estimator for VarF as deﬁned in Equa-\ntion 46. To utilize Theorem 2, we need two compo-\nnents:\n• An unbiased estimator ˆµFk for EE[cF,E|F =\nFk]\n• An unbiased estimator ˆφFk for the variance of\nˆµFk, i.e. VarE(·)(ˆµFk)\n¯cFk is an unbiased estimator for EE[cF,E|F =\nFk], and its variance VarE[¯cF |F = Fk] is\nVarE(·) (¯cFk) = 1\nEVarE[cF,E|F = Fk]. (62)\nTherefore, to develop an unbiased estimator for\nVarE(¯cFjk ), it sufﬁces to have an unbiased esti-\nmate of VarE[cF,E|F = Fk]. We deﬁne\nˆφFk := 1\nE(F− 1)\n∑\nk∈[L]\n(cFk,Ekl −¯cFk)2, (63)\nand we can plug in ˆφFk and ˆµFk = ¯cFk into\nTheorem 2 as an unbiased estimator to obtain an\nunbiased estimator for VarF [EE[cP,F,E ]|P = Pj],\nand we average the estimation for eachPj to obtain\nan unbiased estimate.\nH.2.3 Estimating PretVar\nWe next estimate VarP [EF,E[cP,F,E ]] We can still\napply the idea from Theorem 2, which requires\n• An unbiased estimator ˆµPj for\nEF,E[cP,F,E |P = Pj]\n• An unbiased estimator ˆφPj for the variance of\nˆµPj , i.e. VarF,E[ˆµPj ].\nAgain, the ﬁrst is easy to obtain: ˆµPj = ¯cPj is\nan unbiased estimator for EF,E[cP,F,E |P = Pj],\nwhere\n¯cPj := 1\nFE\n∑\nk∈[F],l∈[E]\ncPj,Fjk,Ejkl (64)\nHowever, we cannot straightforwardly estimate\nVarF,E[ˆµPj ] as before, since samples cPj,Fjk,Ejkl\nare no longer independent. We need to use Equa-\ntion 57 to develop an unbiased estimator (the LHS\nis exactly what we want!), i.e.\nVarF,E(¯cPj ) = 1\nFVarF (EE[cP,F,E ]|P = Pj)\n(65)\n+ 1\nF2\n∑\nk∈[F]\nVarE(¯cPj,Fjk ),\nand we already know how to estimate these two\nsummands from the previous discussion on estimat-\ning FineVar.\nH.2.4 Estimating Bias2\nIt is easy to see that the following ˆLis an unbiased\nestimator for the loss L.\nˆL:= 1\nPFE\n∑\nj∈[P],k∈[F],l∈[E]\n(1 −cPj,Fjk,Ejkl )2,\n(66)\nand\nE[ ˆL] = L. (67)\nBy linearity of expectation and loss decomposi-\ntion in Equation 34,\nˆBias2 := ˆL− ˆPretVar (68)\n− ˆFineVar− ˆCkptVar\nis an unbiased estimator of Bias2.\nNotice that the na ¨ıve estimator that calculates\nthe expected bias and then squares it estimates\n(E[Bias])2 instead of E[Bias2].\nH.3 Generalization\nWe can generalize this estimation strategy to de-\ncompose variance into arbitrary number of random-\nness. In general, we want to estimate some quantity\nof the following form\nEr1...,rn−1 [Varrn[Ern+1...rN [cr1...,cN ]]], (69)\nfrom the data that has an hierarchical tree struc-\nture of randomness.\nFor the goal of developing an unbiased estimator,\nwe can get rid of the outer expectation r1 ...r n−1\neasily by linearity of expectation: simply estimate\nthe Variance conditioned on r1...n−1 and average\nthem together, as discussed in Section H.2.1.\nTo estimate\nVarrn[Ern+1...rN [cr1...,cN ]], (70)\nwe make use of Theorem 2, which requires\n• an unbiased estimator ˆµrn+1 for the quantity\nErn+1...rN [cr1...,cN ], which we can straightfor-\nwardly obtain by average the examples that\nhas the same random variablesr1...n (e.g. ¯cPj )\n• an unbiased estimator for the variance of\nˆµrn+1 . If N = n+1, we can directly compute\nthe sample variance of the cas our estimate\n(e.g. in Equation 63). Otherwise, we use\nEquation 57 to decompose the desired quan-\ntities into two, and estimate them recursively\nby applying Theorem 2 and Equation 57.\nFor readability we wrote the proof with the as-\nsumption that, in the tree of randomness, the num-\nber of branches for each node at the same depth\nis the same. However, our proof does not make\nuse of this assumption and can be applied to a gen-\neral tree structure of randomness as long as the the\nnumber of children is larger or equal to 2 for each\nnon-terminal node.\nI Variance Conditioned on Bias\nSince lower bias usually implies lower variance, to\ntease out the latent effect, we estimate the variance\n“given a ﬁxed level of bias Bias2 of b2 ∈[0,1]”,\ni.e.\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 9: The variance curve conditioned on Bias2 for\nin-domain development set of MNLI, QQP and SST-2.\nEach curve represents a model size. Left for pretraining\nvariance and right for ﬁnetuning variance.\nPretVar(b2) := Ei[PretVari|Bias2i = b2] (71)\nWe estimatePretVars(b2) and FineVars(b2) us-\ning gaussian process and plot them against b2 in\nFigure 9 for MNLI, QQP, and SST-2. We ﬁnd that\nlarger models usually have larger ﬁnetuning vari-\nance across all levels of biases (except forMEDIUM\nand MINI on SST-2), and BASE model always has\nlarger pretraining variance than LARGE .\nWe also experimented with the squared loss:\nLi = (1 −pi)2, (72)\nwhere pi is the probability the assigned to the cor-\nrect label for instance i. We plot the same curve in\nFigure 10 and observe the same trend.\nJ Example Decaying Instances\nWe manually examined the group of instances\nwhere MINI\nLARGE\nˆ∆Acci ≤−0.9 in Table 3. In other\nwords, MINI is almost always correct on these in-\nstances, while LARGE is almost always wrong. For\neach instance in this group, we manually catego-\nrize it into one of the four categories: 1) Correct,\nif the label is correct, 2) Fine, if the label might\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 10: The same ﬁgure as 9, except for using the\nsquared loss function L = (1 −p)2, where p is the\nprobability assigned to the correct label, instead of 0/1\nloss.\nbe controversial but we could see a reason why\nthis label is reasonable, 3) Wrong, if the label is\nwrong, and 4) Unsure, if we are unsure how to la-\nbel this instance. As a control, we also examined\nthe remaining fraction of the dataset. Each time\nwe annotate an instance, with 50% probability it is\nsampled from the decaying fraction or the remain-\ning fraction, and we do not know which group it\ncomes from.\nWe show below all the annotated instances from\nthis decaying fraction and their categories for\nMNLI (Section J.1), QQP , and SST-2(Section J.3).\nJ.1 MNLI\nMNLI is the abbreviation of Multi-Genre Natural\nLanguage Inference (Williams et al. (2020)). In\nthis task, given a premise and a hypothesis, the\nmodel needs to classify whether the premise\nentails/contradicts the hypothesis, or otherwise.\nThe instances can be seen below.\nPremise : and that you’re very much right\nbut the jury may or may not see it that way so you\nget a little anticipate you know anxious there and\ngo well you know\nHypothesis : Jury’s operate without the beneﬁt of\nan education in law.\nLabel : Neutral\nCategory : Correct\nPremise : In ﬁscal year 2000, it reported\nestimated improper Medicare Fee-for-Service\npayments of $11.\nHypothesis : The payments were improper.\nLabel : Entailment\nCategory : Fine\nPremise : is that what you ended up going\ninto\nHypothesis : So that must be what you chose to\ndo?\nLabel : Entailment\nCategory : Correct\nPremise : INTEREST RATE - The price\ncharged per unit of money borrowed per year,\nor other unit of time, usually expressed as a\npercentage.\nHypothesis : Interest rate is deﬁned as the total\namount of money borrowed.\nLabel : Entailment\nCategory : Wrong\nPremise : The analyses comply with the in-\nformational requirements of the sections including\nthe classes of small entities subject to the rule and\nalternatives considered to reduce the burden on the\nsmall entities.\nHypothesis : The rules place a high burden on the\nactivities of small entities.\nLabel : Contradiction\nCategory : Correct\nPremise : Isn’t a woman’s body her most\npersonal property?\nHypothesis : Women’s bodies belong to them-\nselves, they should decide what to do with it.\nLabel : Neutral\nCategory : Unsure\nPremise : The Standard , published a few\ndays before Deng’s death, covers similar territory.\nHypothesis : The Washington Post covers similar\nterritory.\nLabel : Neutral\nCategory : Correct\nPremise : Shoot only the ones that face us,\nJon had told Adrin.\nHypothesis : Jon told Adrin and the others to only\nshoot the ones that face us.\nLabel : Entailment\nCategory : Wrong\nPremise : But if you take it seriously, the\nanti-abortion position is deﬁnitive by deﬁnition.\nHypothesis : If you decide to be serious about\nsupporting anti-abortion, it’s a very run of the mill\nbelief to hold.\nLabel : Neutral\nCategory : Unsure\nPremise : yeah well that’s the other thing\nyou know they talk about women leaving the home\nand going out to work well still taking care of the\nchildren is a very important job and and someone’s\ngot to do it and be able to do it right and\nHypothesis : It is not acceptable for anybody to\nrefuse work in order to take care of children.\nLabel : Contradiction\nCategory : Correct\nPremise : The researchers found expected\nstresses like the loss of a check in the mail and the\nillness of loved ones.\nHypothesis : The stresses affected people much\ndiffferently than the researchers expected.\nLabel : Contradiction\nCategory : Correct\nPremise : so you know it’s something we\nwe have tried to help but yeah\nHypothesis : We did what we could to help.\nLabel : Entailment\nCategory : Correct\nPremise : Czarek was welcomed enthusias-\ntically, even though the poultry brotherhood was\npaying a lot of sudden attention to the newcomers\n- a strong group of young and talented managers\nfrom an egzemo-exotic chicken farm in Fodder\nBand nearby Podunkowice.\nHypothesis : Czarek was welcomed into the group\nby the farmers.\nLabel : Entailment\nCategory : Correct\nPremise : ’I don’t suppose you could forget\nI ever said that?’\nHypothesis : I hope that you can remember that\nforever.\nLabel : Contradiction\nCategory : Wrong\nPremise : Oh, my friend, have I not said to\nyou all along that I have no proofs.\nHypothesis : I told you from the start that I had no\nevidence.\nLabel : Entailment\nCategory : Correct\nPremise : I should put it this way.\nHypothesis : I should phrase it differently.\nLabel : Entailment\nCategory : Correct\nPremise : An organization’s activities, core\nprocesses, and resources must be aligned to\nsupport its mission and help it achieve its goals.\nHypothesis : An organization is successful if its\nactivities, resources, and goals align.\nLabel : Entailment\nCategory : Fine\nPremise : A more unusual dish is azure, a\nkind of sweet porridge made with cereals, nuts,\nand fruit sprinkled with rosewater.\nHypothesis : Azure is a common and delicious\nfood made with cereals, nuts and fruit.\nLabel : Entailment\nCategory : Wrong\nPremise : once you have something and it’s\nlike i was watching this program on TV yesterday\nin nineteen seventy six NASA came up with Three\nD graphics right\nHypothesis : I was watching a program about\ngardening.\nLabel : Contradiction\nCategory : Correct\nPremise : , First-Class Mail used by house-\nholds to pay their bills) and the household bill mail\n(i.e.\nHypothesis : Second-Class Mail used by house-\nholds to pay their bills\nLabel : Contradiction\nCategory : Unsure\nPremise : Rightly or wrongly, America is\nseen as globalization’s prime mover and head\ncheerleader and will be blamed for its excesses\nuntil we start paying ofﬁcial attention to them.\nHypothesis : America’s role in the globalization\nmovement is important whether we agree with it or\nnot.\nLabel : Entailment\nCategory : Correct\nPremise : After being diagnosed with can-\ncer, Carrey’s Kaufman decides to do a show at\nCarnegie Hall.\nHypothesis : Carrey’s Kaufman is only diagnosed\nwith cancer after doing a show at Carnegie Hall.\nLabel : Contradiction\nCategory : Correct\nPremise : Several pro-life Dems are mount-\ning serious campaigns at the state level, often\nagainst pro-choice Republicans.\nHypothesis : Serious campaigns are being run by\na few pro-life Democrats.\nLabel : Entailment\nCategory : Correct\nPremise : On the northwestern Alpine fron-\ntier, a new state had appeared on the scene,\ndestined to lead the movement to a united Italy.\nHypothesis : The unite Italy movement was\nwaiting for a leader.\nLabel : Neutral\nCategory : Fine\nPremise : well we bought this with credit\ntoo well we found it with a clearance uh down in\nMemphis i guess and uh\nHypothesis : We bought non-sale items in\nMemphis on credit.\nLabel : Contradiction\nCategory : Correct\nPremise : He slowed.\nHypothesis : He stopped moving so quickly.\nLabel : Entailment\nCategory : Correct\nPremise : As legal scholar Randall Kennedy wrote\nin his book Race, Crime, and the Law , Even\nif race is only one of several factors behind a\ndecision, tolerating it at all means tolerating it as\npotentially the decisive factor.\nHypothesis : Race is one of several factors in\nsome judicial decisions\nLabel : Entailment\nCategory : Correct\nPremise : Although all four categories of\nemissions are down substantially, they only\nachieve 50-75% of the proposed cap by 2007\n(shown as the dotted horizontal line in each of the\nabove ﬁgures).\nHypothesis : All of the emission categories\nexperienced a downturn except for one.\nLabel : Contradiction\nCategory : Correct\nPremise : He sat up, trying to free himself.\nHypothesis : He was trying to take a nap.\nLabel : Contradiction\nCategory : Correct\nPremise : Impossible.\nHypothesis : Cannot be done.\nLabel : Entailment\nCategory : Correct\nPremise : But, as the last problem I’ll out-\nline suggests, neither of the previous two\nobjections matters.\nHypothesis : I will not continue to outline any\nmore problems.\nLabel : Entailment\nCategory : Correct\nPremise : As the Tokugawa shoguns had\nfeared, this opening of the ﬂoodgates of Western\nculture after such prolonged isolation had a\ntraumatic effect on Japanese society.\nHypothesis : The Tokugawa shoguns had feared\nthat, because they understood the Japanese society\nvery well.\nLabel : Neutral\nCategory : Fine\nPremise : In the ancestral environment a\nman would be likely to have more offspring if he\ngot his pick of the most fertile-seeming women.\nHypothesis : Only a man who stayed with one\nfemale spread his genes most efﬁciently.\nLabel : Contradiction\nCategory : Fine\nPremise : Tommy was suddenly galvanized\ninto life.\nHypothesis : Tommy had been downcast for days.\nLabel : Neutral\nCategory : Correct\nPremise : Improved products and services\nInitiate actions and manage risks to develop\nnew products and services within or outside the\norganization.\nHypothesis : Managed risks lead to new products\nLabel : Entailment\nCategory : Fine\nPremise : Coast Guard rules establishing\nbridgeopening schedules).\nHypothesis : The Coast Guard is in charge of\nopening bridges.\nLabel : Entailment\nCategory : Correct\nPremise : The anthropologist Napoleon Chagnon\nhas shown that Yanomamo men who have killed\nother men have more wives and more offspring\nthan average guys.\nHypothesis : Yanomamo men who kill other men\nhave better chances at getting more wives.\nLabel : Entailment\nCategory : Fine\nPremise : The Varanasi Hindu University\nhas an Art Museum with a superb collection\nof 16th-century Mughal miniatures, considered\nsuperior to the national collection in Delhi.\nHypothesis : The Varanasi Hindu University\nhas an art museum on its campus which may be\nsuperior objectively to the national collection in\nDelhi.\nLabel : Entailment\nCategory : Correct\nPremise : Part of the reason for the differ-\nence in pieces per possible delivery may be due\nto the fact that ﬁve percent of possible residential\ndeliveries are businesses, and it is thought, but\nnot known, that a lesser percentage of possible\ndeliveries on rural routes are businesses.\nHypothesis : We all know that the reason for a\nlesser percentage of possible deliveries on rural\nroutes being businesses, is because of the fact that\npeople prefer living in cities rather than rural areas.\nLabel : Neutral\nCategory : Correct\nPremise : right oh they’ve really done uh\ngood job of keeping everybody informed of what’s\ngoing on sometimes i’ve wondered if it wasn’t\nalmost more than we needed to know\nHypothesis : I don’t think I have shared enough\ninformation with everyone.\nLabel : Contradiction\nCategory : Correct\nPremise : To reach any of the three Carbet\nfalls, you must continue walking after the roads\ncome to an end for 20 minutes, 30 minutes, or two\nhours respectively.\nHypothesis : There are three routes to the three\nCarbet falls, each a different length and all\ncontinue after the road seemingly ends.\nLabel : Entailment\nCategory : Correct\nPremise : But when the cushion is spent in\na year or two, or when the next recession arrives,\nthe disintermediating voters will ﬁnd themselves\nplaying the roles of budget analysts and tax wonks.\nHypothesis : The cushion will likely be spent in\nunder two years.\nLabel : Entailment\nCategory : Correct\nPremise : But, Slate protests, it was [Gates’]\nbyline that appeared on the cover.\nHypothesis : Slate was one hundred percent\npositive it was Gates’ byline on the cover.\nLabel : Neutral\nCategory : Correct\nPremise : But it’s for us to get busy and do\nsomething.”\nHypothesis : ”We don’t do much, so maybe this\nwould be good for us to bond and be together for\nthe ﬁrst time in a while.”.\nLabel : Neutral\nCategory : Fine\nPremise : Pearl Jam detractors still can’t\nstand singer Eddie They say he’s unbearably\nself-important and limits the group’s appeal by\nrefusing to sell out and make videos.\nHypothesis : A lot of people consider Eddie to be\na bad singer.\nLabel : Neutral\nCategory : Correct\nPremise : it’s the very same type of paint\nand everything\nHypothesis : It’s the same paint formula, it’s\ngreat!\nLabel : Entailment\nCategory : Fine\nPremise : Exhibit 3 presents total national\nemissions of NOx and SO2 from all sectors,\nincluding power.\nHypothesis : In Exhibit 3 there are the total\nregional emissions od NOx and SO2 from all\nsectors.\nLabel : Entailment\nCategory : Correct\nPremise : uh-huh and is it true i mean is it\num\nHypothesis : It’s true.\nLabel : Entailment\nCategory : Wrong\nPremise : When a GAGAS attestation en-\ngagement is the basis for an auditor’s subsequent\nreport under the AICPA standards, it would be\nadvantageous to users of the subsequent report\nfor the auditor’s report to include the information\non compliance with laws and regulations and\ninternal control that is required by GAGAS but not\nrequired by AICPA standards.\nHypothesis : The report is required by GAGAS\nbut not AICPA.\nLabel : Entailment\nCategory : Correct\nPremise : i’m on i’m in the Plano school\nsystem and living in Richardson and there is\na real dichotomy in terms of educational and\neconomic background of the kids that are going to\nbe attending this school\nHypothesis : The Plano school system only has\nchildren with poor intelligence.\nLabel : Contradiction\nCategory : Correct\nJ.2 QQP\nQQP is the abbreviation of Quora Question\nPairs14. Given two questions, the model needs\nto tell whether they have the same meaning (i.e.\nParaphrase/Non-paraphrase).\nQuestion 1 : Which universities for MS in CS\nshould I apply to?\nQuestion 2 : Which universities should I apply to\nfor an MS in CS?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What should I do to make life\nworth living?\nQuestion 2 : What makes life worth living?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : Why did Quora remove my\nquestion?\nQuestion 2 : Why does Quora remove questions?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How do I get thousands of fol-\nlowers on Instagram?\nQuestion 2 : How can I get free 10k real Instagram\nfollowers fast?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What is the basic knowledge\nof computer science engineers?\nQuestion 2 : What is basic syallbus of computer\nscience engineering?\nLabel : Non-paraphrase\nCategory : Fine\nQuestion 1 : How many mosquito bites\ndoes it take to kill a human being?\nQuestion 2 : How many times can a single\nmosquito bite a human within 8 hours?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : How does it feel to become at-\ntractive from unattractive?\nQuestion 2 : What does it feel like to go from\nphysically unattractive to physically attractive?\n14https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : Who is answering the ques-\ntions asked on Quora?\nQuestion 2 : Who can answer the questions asked\non Quora?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What machine learning theory\ndo I need to know in order to be a successful\nmachine learning practitioner?\nQuestion 2 : What do I need to know to learn\nmachine learning?\nLabel : Paraphrase\nCategory : Wrong\nQuestion 1 : If you could go back in time\nand change one thing, what would it be and why?\nQuestion 2 : If you could go back in time and do\none thing, what would it be?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : Will there be a civil war if\nTrump doesn’t become president?\nQuestion 2 : Will there be a second civil war if\nTrump becomes president?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : Do Quora contributors get\npaid?\nQuestion 2 : How do contributors get paid by\nQuora?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : Did India meet Abdul Kalam’s 2020\nvision so far?\nQuestion 2 : How far do you think India has\nreached on President APJ Kalam’s vision in the\nbook India 2020?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : How do I stop my dog from\nwhining after getting spayed?\nQuestion 2 : How do I stop my dog from whining?\nLabel : Paraphrase\nCategory : Wrong\nQuestion 1 : What difference are exactly\nbetween Euclidean space and non Euclidean\nspace?\nQuestion 2 : What is the difference between\nEuclidean and non-Euclidean?\nLabel : Non-paraphrase\nCategory : Wrong\nQuestion 1 : Why doesn’t Hillary Clinton\nwin the White House if she won the popular vote?\nQuestion 2 : How did Hillary Clinton win the\npopular vote but Donald Trump win the election?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How is public breastfeeding\nseen where you live?\nQuestion 2 : How is breastfeeding in public seen\nin your country?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What are some ways to change your\nNetﬂix password?\nQuestion 2 : How do you change your Netﬂix\npassword and email?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What do you think, is your\nbest answer on Quora?\nQuestion 2 : What is your best answer on Quora?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How can I travel to Mexico\nwithout a passport?\nQuestion 2 : Can I travel to Mexico without a\npassport?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How do modern Congolese\npeople view Mobutu in retrospect?\nQuestion 2 : How do Congolese currently view\nMobutu Sese Seko?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : How is Tanmay Bhat losing\nweight?\nQuestion 2 : Tanmay Bhat: How did you manage\nto reduce your fat?\nLabel : Non-paraphrase\nCategory : Unsure\nQuestion 1 : Is Xiaomi a brand to trust\n(comparing it with brands like Samsung and\nHTC)? What is better: Xiaomi MI3 or HTC Desire\n816?\nQuestion 2 : Is xiaomi a trusted brand?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : Why did Buddhism spread in\nEast Asia and not in its native land India?\nQuestion 2 : How was Buddhism spread in Asia?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : Can I become a multi billion-\naire betting on horses?\nQuestion 2 : How much money can I make betting\non horses? A month? Can I make 20,000 a month?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What is a diet for gaining\nweight?\nQuestion 2 : What is a way to gain weight?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : How do I use Instagram on\nmy computer?\nQuestion 2 : How can I get Instagram on my\ncomputer?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What is the legal basis of a\n”you break it, you buy it” policy?\nQuestion 2 : Is a ”you break it you buy it” policy\nactually legal?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How I should ﬁx my computer while\nit is showing no boot device found?\nQuestion 2 : How do I ﬁx the ”Boot device not\nfound” problem?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What innovative name can I\nuse for an interior designing ﬁrm?\nQuestion 2 : What can i name my interior\ndesigning ﬁrm?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What would it realistically\ncost to go to Tomorrowland?\nQuestion 2 : How much is a ticket to Tomorrow-\nland?\nLabel : Non-paraphrase\nCategory : Fine\nQuestion 1 : Is there a gender pay gap? If\nso why?\nQuestion 2 : Is the gender pay gap a myth?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How can I get rid of a canker\nsore on the bottom of my tongue?\nQuestion 2 : How can I get rid or a canker sore on\nthe tip of my tongue?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How can I sleep better and\nearly in night?\nQuestion 2 : How can I sleep better at night?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : Why did DC Change Captain\nMarvel’s name?\nQuestion 2 : Why did DC have to change Captain\nMarvel’s name but Marvel didn’t have to change\nScarecrow’s name?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : Should be there any difference\nbetween IIT and non IIT students in terms of\nplacement package from a company if both of\nthem are equally talented?\nQuestion 2 : Should be there any difference\nbetween IIT and non IIT students in terms of\nplacement package from a company if both of\nthem are equally capable?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What happened to The Joker\nafter The end of The Dark Knight?\nQuestion 2 : What happens to the Joker at the end\nof The Dark Knight (2008 movie)?\nLabel : Non-paraphrase\nCategory : Wrong\nQuestion 1 : I love my wife more then any-\nthing. Why do I fantasize about her with other\nmen?\nQuestion 2 : Why do I fantasize about other men\nhaving sex with my wife?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What is your opinion on the\nnew MacBook Pro Touch Bar?\nQuestion 2 : What do you think about the OLED\ntouch bar on the new MacBook Pro?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How do I get rid of my nega-\ntive alter ego?\nQuestion 2 : How do you get rid of your negative\nalter ego?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : How can I get wiﬁ driver for\nmy hp laptop with windows 7 os?\nQuestion 2 : How can I get wiﬁ driver for my\nlaptop with windows 7 os?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : What’s your attitude towards\nlife?\nQuestion 2 : What should be your attitude towards\nlife?\nLabel : Paraphrase\nCategory : Wrong\nQuestion 1 : What books would I like if I\nloved A Song of Ice and Fire?\nQuestion 2 : Are there books which are similar to\nA Song of Ice and Fire?\nLabel : Paraphrase\nCategory : Fine\nQuestion 1 : Why do Muslims think they\nwill conquer the whole world?\nQuestion 2 : Do you think Muslims will take over\nthe world?\nLabel : Non-paraphrase\nCategory : Correct\nQuestion 1 : Is dark matter a sea of mas-\nsive dark photons that ripple when galaxy clusters\ncollide and wave in a double slit experiment?\nQuestion 2 : Does a superﬂuid dark matter which\nripples when Galaxy clusters collide and waves in\na double slit experiment relate GR and QM?\nLabel : Paraphrase\nCategory : Correct\nQuestion 1 : What is Batman like?\nQuestion 2 : What is Batman’s personality like?\nLabel : Non-paraphrase\nCategory : Correct\nJ.3 SST-2\nSST-2 is the abbreviation of Stanford Sentiment\nTreebank (Socher et al., 2013). In this task, the\nmodel needs to recognize whether the phrases or\nsentences reﬂect positive or negative sentiments.\nInput : predictability is the only winner\nLabel : Negative\nCategory : Correct\nInput : abandon their scripts and go where\nthe moment takes them\nLabel : Negative\nCategory : Correct\nInput : chases for an hour and then\nLabel : Positive\nCategory : Unsure\nInput : provide much more insight than the\ninside column of a torn book jacket\nLabel : Negative\nCategory : Unsure\nInput : a children ’s party clown\nLabel : Negative\nCategory : Fine\nInput : perhaps even the slc high command\nfound writer-director mitch davis ’s wall of kitsch\nhard going .\nLabel : Negative\nCategory : Correct\nInput : own placid way\nLabel : Negative\nCategory : Correct\nInput : get on a board and , uh , shred\n,\nLabel : Negative\nCategory : Correct\nInput : asks what truth can be discerned\nfrom non-ﬁrsthand experience , and speciﬁcally\nquestions cinema ’s capability for recording truth .\nLabel : Positive\nCategory : Correct\nInput : puts the dutiful efforts of more dis-\nciplined grade-grubbers\nLabel : Positive\nCategory : Correct\nInput : ﬁlter out the complexity\nLabel : Positive\nCategory : Correct\nInput : told what actually happened as if it\nwere the third ending of clue\nLabel : Negative\nCategory : Correct\nInput : is more in love with strangeness\nthan excellence .\nLabel : Positive\nCategory : Wrong\nInput : i found myself howling more than\ncringing\nLabel : Positive\nCategory : Correct\nInput : goldbacher draws on an elegant vi-\nsual sense and a talent for easy , seductive pacing\n... but she and writing partner laurence coriat do\nn’t manage an equally assured narrative coinage\nLabel : Positive\nCategory : Unsure\nInput : for a thirteen-year-old ’s book re-\nport\nLabel : Negative\nCategory : Correct\nInput : a problem hollywood too long has\nignored\nLabel : Negative\nCategory : Correct\nInput : twisted sense\nLabel : Negative\nCategory : Correct\nInput : a stab at soccer hooliganism\nLabel : Negative\nCategory : Correct\nInput : sinuously plotted\nLabel : Negative\nCategory : Correct\nInput : shiner can certainly go the distance\n, but is n’t world championship material\nLabel : Positive\nCategory : Correct\nInput : holding equilibrium up\nLabel : Negative\nCategory : Wrong\nInput : i am highly amused by the idea that\nwe have come to a point in society where it has\nbeen deemed important enough to make a ﬁlm in\nwhich someone has to be hired to portray richard\ndawson .\nLabel : Positive\nCategory : Wrong\nInput : waters\nLabel : Negative\nCategory : Wrong\nInput : what might have emerged as hilari-\nous lunacy in the hands of woody allen or\nLabel : Positive\nCategory : Correct\nInput : of those airy cinematic bon bons\nwhose aims – and by extension , accomplishments\n– seem deceptively slight on the surface\nLabel : Positive\nCategory : Correct\nInput : do n’t blame eddie murphy but\nLabel : Negative\nCategory : Correct\nInput : melodramatic paranormal romance\nLabel : Negative\nCategory : Correct\nInput : could possibly be more contemptu-\nous of the single female population\nLabel : Negative\nCategory : Correct\nInput : cremaster 3 ” should come with the\nwarning “ for serious ﬁlm buffs only ! ”\nLabel : Negative\nCategory : Correct\nInput : softheaded metaphysical claptrap\nLabel : Negative\nCategory : Correct\nInput : owed to benigni\nLabel : Negative\nCategory : Unsure\nInput : to be a suspenseful horror movie or\na weepy melodrama\nLabel : Positive\nCategory : Correct\nInput : genuinely unnerving .\nLabel : Positive\nCategory : Correct\nInput : gaping enough to pilot an entire\nolympic swim team through\nLabel : Negative\nCategory : Correct\nInput : this is popcorn movie fun with equal doses\nof action , cheese , ham and cheek ( as well as a\nserious debt to the road warrior ) , but it feels like\nunrealized potential\nLabel : Positive\nCategory : Fine\nInput : feeling like it was worth your seven\nbucks , even though it does turn out to be a bit of a\ncheat in the end\nLabel : Negative\nCategory : Correct\nInput : pull it back\nLabel : Negative\nCategory : Correct\nInput : , this is more appetizing than a side\ndish of asparagus .\nLabel : Negative\nCategory : Correct\nInput : crime drama\nLabel : Negative\nCategory : Unsure\nInput : like most movies about the pitfalls\nof bad behavior\nLabel : Negative\nCategory : Fine\nInput : befallen every other carmen before\nher\nLabel : Positive\nCategory : Unsure\nInput : appeal to those without much inter-\nest in the elizabethans ( as well as rank frustration\nfrom those in the know about rubbo ’s dumbed-\ndown tactics )\nLabel : Negative\nCategory : Unsure\nInput : about existential suffering\nLabel : Negative\nCategory : Fine\nInput : , if uneven ,\nLabel : Negative\nCategory : Unsure\nInput : succumbs to sensationalism\nLabel : Positive\nCategory : Wrong\nInput : that turns me into that annoying\nspecimen of humanity that i usually dread encoun-\ntering the most\nLabel : Negative\nCategory : Fine\nInput : at least a minimal appreciation\nLabel : Positive\nCategory : Unsure\nInput : underlines even the dullest tangents\nLabel : Negative\nCategory : Correct\nInput : heard before\nLabel : Positive\nCategory : Unsure\nInput : i like my christmas movies with\nmore elves and snow and less pimps and ho ’s .\nLabel : Negative\nCategory : Unsure\nInput : can aspire but none can equal\nLabel : Negative\nCategory : Unsure\nInput : fathom\nLabel : Negative\nCategory : Unsure\nInput : attempt to bring cohesion to pamela\n’s emotional roller coaster life\nLabel : Negative\nCategory : Unsure\nInput : movie version\nLabel : Positive\nCategory : Wrong\nInput : of spontaneity in its execution and\na dearth of real poignancy\nLabel : Positive\nCategory : Correct",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.7012858986854553
    },
    {
      "name": "Randomness",
      "score": 0.6892204284667969
    },
    {
      "name": "Computer science",
      "score": 0.6623457074165344
    },
    {
      "name": "Noise (video)",
      "score": 0.5426270961761475
    },
    {
      "name": "Language model",
      "score": 0.4786180853843689
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44681575894355774
    },
    {
      "name": "Machine learning",
      "score": 0.39832162857055664
    },
    {
      "name": "Statistics",
      "score": 0.274492084980011
    },
    {
      "name": "Mathematics",
      "score": 0.14997833967208862
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}