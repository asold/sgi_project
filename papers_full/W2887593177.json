{
  "title": "Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition",
  "url": "https://openalex.org/W2887593177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224128455",
      "name": "Sachan, Devendra Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2388136758",
      "name": "Xie, Pengtao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222066820",
      "name": "Sachan, Mrinmaya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222901025",
      "name": "Xing, Eric P.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2414378847",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2021331223",
    "https://openalex.org/W1505083828",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2065963191",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2107005506",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2013989975",
    "https://openalex.org/W1971129545",
    "https://openalex.org/W2953173049",
    "https://openalex.org/W2757016069",
    "https://openalex.org/W2626176522",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2142384583",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2735585131",
    "https://openalex.org/W2964052092",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2743028754",
    "https://openalex.org/W2734608416",
    "https://openalex.org/W2964085268",
    "https://openalex.org/W2178441628"
  ],
  "abstract": "Biomedical named entity recognition (NER) is a fundamental task in text mining of medical documents and has many applications. Deep learning based approaches to this task have been gaining increasing attention in recent years as their parameters can be learned end-to-end without the need for hand-engineered features. However, these approaches rely on high-quality labeled data, which is expensive to obtain. To address this issue, we investigate how to use unlabeled text data to improve the performance of NER models. Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to \"pretrain\" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model. We evaluate our approach on four benchmark datasets for biomedical NER and show that it leads to a substantial improvement in the F1 scores compared with the state-of-the-art approaches. We also show that BiLM weight transfer leads to a faster model training and the pretrained model requires fewer training examples to achieve a particular F1 score.",
  "full_text": "Proceedings of Machine Learning Research 85:1–19, 2018 Machine Learning for Healthcare\nEﬀective Use of Bidirectional Language Modeling for\nTransfer Learning in Biomedical Named Entity Recognition\nDevendra Singh Sachan♠ devendra.singh@petuum.com\nPengtao Xie♠ pengtao.xie@petuum.com\nMrinmaya Sachan△ mrinmays@cs.cmu.com\nEric P. Xing♠ eric.xing@petuum.com\n♠Petuum Inc, Pittsburgh, PA, USA\n△Machine Learning Department, CMU, Pittsburgh, PA, USA\nEditor: ...\nAbstract\nBiomedical named entity recognition (NER) is a fundamental task in text mining of medical\ndocuments and has many applications. Deep learning based approaches to this task have\nbeen gaining increasing attention in recent years as their parameters can be learned end-\nto-end without the need for hand-engineered features. However, these approaches rely on\nhigh-quality labeled data, which is expensive to obtain. To address this issue, we investigate\nhow to use unlabeled text data to improve the performance of NER models. Speciﬁcally, we\ntrain a bidirectional language model (BiLM) on unlabeled data and transfer its weights to\n“pretrain” an NER model with the same architecture as the BiLM, which results in a better\nparameter initialization of the NER model. We evaluate our approach on four benchmark\ndatasets for biomedical NER and show that it leads to a substantial improvement in the\nF1 scores compared with the state-of-the-art approaches. We also show that BiLM weight\ntransfer leads to a faster model training and the pretrained model requires fewer training\nexamples to achieve a particular F1 score.\nKeywords: biomedical NER, language modeling, pretraining, bidirectional LSTM, char-\nacter CNN, CRF\n1. Introduction\nThe ﬁeld of biomedical text mining has received increased attention in recent years due to\nthe rapid increase in the number of publications, scientiﬁc articles, reports, medical records,\netc. that are available and readily accessible in electronic format. These biomedical data\ncontains many mentions of biological and medical entities such as chemical ingredients,\ngenes, proteins, medications, diseases, symptoms, etc. Figure 1 shows a medical text that\ncontains seven disease entities (highlighted in red) and four anatomical entities (highlighted\nin yellow). The accurate identiﬁcation of such entities in text collections is a very important\nsubtask for information extraction systems in the ﬁeld of biomedical text mining as it helps\nin transforming the unstructured information in texts into structured data. Search engines\ncan index, organize, and link medical documents using such identiﬁed entities and this\ncan improve medical information access as the users will be able to gather information\nfrom many pieces of text. The identiﬁcation of entities can also be used to mine relations\nand extract associations from the medical research literature, which can be used in the\nc⃝2018 D.S. Sachan, P. Xie, M. Sachan & E.P. Xing.\narXiv:1711.07908v3  [cs.CL]  15 Aug 2018\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nFigure 1: Example of disease and anatomical entities in medical text. Disease entities are high-\nlighted in red and anatomical entities are highlighted in yellow.\nconstruction of medical knowledge graphs (Rotmensch et al., 2017). We refer to this task\nof identiﬁcation and tagging of entities in a text as members of predeﬁned categories such\nas diseases, chemicals, genes, etc. as named entity recognition (NER).\nNER has been a widely studied task in the area of natural language processing (NLP)\nand a number of works have applied machine learning approaches to NER in the medical\ndomain. Building NER systems with high precision and high recall for the medical domain\nis quite a challenging task due to high linguistic variation in data. First, a dictionary-\nbased approach doing pattern matching will fail to correctly tag ambiguous abbreviations\nthat can belong to diﬀerent entity types. For example, the term CAT can refer to several\nphrases—“chloramphenicol acetyl transferase,” “computer-automated tomography,” “choline\nacetyltransferase,” or “computed axial tomography” (Stevenson and Guo, 2010). Second, as\nthe vocabulary of biomedical entities such as proteins is quite vast and is rapidly evolving, it\nmakes the task of entity identiﬁcation even more challenging and error-prone as it is diﬃcult\nto create labeled training examples having a wide coverage. Also, in contrast to natural\ntext, entities in the medical domain can have very long names as shown in Figure 1 that\ncan lead an NER tagger to incorrectly predict the tags. Lastly, state-of-the-art machine\nlearning approaches for NER task rely on high-quality labeled data, which is expensive to\nprocure and is therefore available only in limited quantity. Therefore, there is a need for\napproaches that can use unlabeled data to improve the performance of NER systems.\nNER can be devised as a supervised machine learning task in which the training data\nconsists of labels or tags for each token in the text. A typical approach for NER task is\nto extract word-level features followed by training a linear model for tag classiﬁcation. To\nextract features, our NER model makes use of pretrained word embeddings, learned char-\nacter features, and word-level bidirectional long short-term memory (BiLSTM). The word\nembeddings are learned from a large collection of PubMed abstracts and it improves the\nF1 score on NER datasets compared with randomly initialized word vectors. The BiLSTM\neﬀectively models the left and right context information around the center word for every\ntime step and this context based representation of a word can help in the disambiguation\nof abbreviations. The BiLSTM, when applied in combination with character features also\nmaps similar terms like “lymphoblastic leukemia,” “null-cell leukemia,” and its varied forms\nin a latent space that captures the semantic meaning in the phrases. This powerful repre-\nsentation of terms in a latent semantic space can also help in the correct classiﬁcation of\nunseen entities as entities with similar contexts are mapped closer together.\nIn this paper, we propose a transfer learning approach that makes use of unlabeled data\nto pretrain the weights of NER model using an auxiliary task. Speciﬁcally, we do language\nmodeling in both the forward and backward directions to pretrain the weights of NER\n2\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nFigure 2: Character CNN block diagram\nFigure 3: LSTM block diagram\nmodel that is later ﬁne-tuned using the supervised task-speciﬁc training data. We believe\nthat such generative model pretraining can prevent overﬁtting, improve model training,\nand its convergence speed. We show that such pretraining of weights helps to substantially\nincrease the F1 score on four benchmark datasets for biomedical NER compared with the\nstate-of-the-art approaches. We also observe that BiLM weight transfer leads to faster\nconvergence during the NER model ﬁne-tuning step. As an unsupervised method, our\ntransfer learning approach requires only unlabeled data and thus is generally applicable to\ndiﬀerent NER datasets compared with the supervised transfer learning approaches that rely\non task-speciﬁc labeled data to pretrain the model parameters (Lee et al., 2018).\nFollowing this Introduction, the remainder of this paper is organized as follows. Sec-\ntion 2 explains the NER model, its training methodology, and bidirectional language mod-\neling. Section 3 describes the experimental setup such as datasets, model architecture,\nand the training process. Section 4 reports the results on these datasets and analyzes the\nperformance of the pretrained NER model in detail. Section 5 reviews the related work\nfor biomedical NER. The conclusion, in section 6, summarizes our methods, results, and\ndiscusses the future work.\n2. Methods\nThe main building blocks of our neural network based NER model are: character-level con-\nvolutional neural network (CNN) layer, word embedding layer, word-level BiLSTM layer,\ndecoder layer, and sentence-level label prediction layer (see Figure 4). During model train-\ning, all the layer are jointly trained. Before training, we also pretrain the parameters of the\ncharacter-CNN, word embedding, and BiLSTM layers in the NER model using the learned\nparameters from a language model that has the same architecture. Speciﬁcally, we perform\nbidirectional language modeling (BiLM) to pretrain the weights of both the forward and\nbackward LSTMs in the NER model. Next, we will describe these layers in detail.\n3\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\n2.1. Character-Level CNN\nCNNs (LeCun et al., 1990) are widely used in computer vision tasks for visual feature\nextraction (Krizhevsky et al., 2012). In NLP, where the data is mostly sequential, successful\napplications of CNNs include tasks such as text classiﬁcation (Kim, 2014) and sequence\nlabeling (Collobert et al., 2011). In this paper, we use CNNs to extract features from\ncharacters (Kim et al., 2016) as they can encode morphological and lexical patterns observed\nin languages.\nSimilar to the concept of word embedding, each character is represented by an embed-\nding vector. These character embeddings are stored in a lookup table Wc ∈RVc×Dc , where\nVc is the character vocabulary, Dc is the dimensionality of character embeddings. To com-\npute character-level features, we perform 1D convolution along the temporal dimension. 1\nMathematically, this can be written as:\nzk[i] = f(Wk ∗X[:, i+ s−1] + bk),\nwhere ∗is the dot product operator, bk is the bias, X ∈RDc×wℓ is the character-based\nembedding representation of a word, wℓ is the length of a word, Wk are ﬁlter weights, s\nis the convolution stride, f can be any nonlinear function such as tanh or rectiﬁed linear\nunits (f(x) = max(0, x)). To capture the important features of a word, multiple ﬁlters of\ndiﬀerent strides are used. Finally, the maximum value is computed over the time dimension\nalso called max-pooling to get a single feature for every ﬁlter weight. All the features\nare concatenated to obtain character-based word representation vw\nchar. A block diagram of\ncharacter-level CNN is shown in Figure 2.\n2.2. Word-Level Bidirectional LSTM\nRecurrent neural network (Werbos, 1988) such as LSTM (Hochreiter and Schmidhuber,\n1997) is widely used in NLP because it can model the long-range dependencies in language\nstructure with their memory cells and explicit gating mechanism. The dynamics of an\nLSTM cell is controlled by an input vector ( xt), a forget gate ( ft), an input gate ( it), an\noutput gate (ot), a cell state ( ct), and a hidden state ( ht), which are computed as:\nit = σ(Wi ∗[ht−1,xt] + bi)\nft = σ(Wf ∗[ht−1,xt] + bf )\not = σ(Wo ∗[ht−1,xt] + bo)\ngt = tanh(Wg ∗[ht−1,xt] + bg)\nct = ft ⊙ct−1 + it ⊙gt\nht = ot ⊙tanh(ct),\nwhere ct−1 and ht−1 are the cell state and hidden state respectively from previous time\nstep, σ is the sigmoid function ( 1\n1+e−x ), tanh is the hyperbolic tangent function ( ex−e−x\nex+e−x ),\n1. To have a uniform length, each word is right-padded with a special padding token so that the length of\nevery word is the same as that of the longest word in every mini-batch. The embedding of the padding\ncharacter is always a zero vector.\n4\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\n⊙denotes element-wise multiplication. Figure 3 shows the block diagram of an LSTM cell.\nThe parameters of the LSTM are shared for all the time steps.\nIn our NER model, the word embeddings ( vw\nemb) and the character CNN features of\na word ( vw\nchar) are concatenated and is given as input to the sequence encoder ( xt =\n[vw\nemb,vw\nchar]). The sequence encoder consists of a forward LSTM and a backward LSTM,\nwhich is also known as bidirectional LSTM (BiLSTM) (Schuster and Paliwal, 1997). The\ninput to the backward LSTM cell is the reversed order of words in the sequence.\n2.3. Word-Level Likelihood\nFor every word, hidden state representations from BiLSTM are concatenated (ht = [− →ht,← −ht])\nand are fed to the decoder layer. The decoder layer computes an aﬃne transformation of\nthe hidden states\ndt = Wdht + b,\nwhere H is the dimensionality of the BiLSTM hidden states, T is the total number of tags,\nWd ∈RT×H and b are learnable parameters. Decoder outputs are referred to as logits in\nthe subsequent discussions. To compute the probability of a tag ( ˆ yt) for a word, softmax\nfunction is used\np(ˆyt = j |wt) = softmax(dt).\nLet y = {y1,y2,...,y N }denote the sequence of tags in the training corpus, then the cross-\nentropy loss is calculated as:\nCEner(y,ˆ y) = −\nN∑\nt=1\nT∑\nj=1\n1 (yt = ˆyt,j) log ˆyt,j\nFigure 4 shows the block diagram of our NER model. To learn the model parameters,\naverage cross-entropy loss is minimized by backpropagation through time (BPTT) (Werbos,\n1990). When the word-level likelihood is minimized to train the NER model, it is denoted\nas CNN-BiLSTM.\n2.4. Sentence-Level Likelihood\nA drawback of optimizing word-level likelihood is that it ignores the dependencies be-\ntween other neighboring tags in the sentence. A better strategy is to model the entire\nsentence structure using a conditional random ﬁeld (CRF). A CRF is a log-linear graphical\nmodel (Laﬀerty et al., 2001) that additionally considers the transition score from one tag to\nthe next tag. This encourages valid transition paths among the tags based on the learned\ntransition parameters (Wcrf ∈RT×T ). During training, we maximize the log-likelihood for\nthe entire sentence.2 Mathematically, this can be written as: 3\nlog p(y |d) = s(d,y) −log\n∑\ny′∈Sm\nes(d,y′),\n2. This can be done in polynomial time using theforward-backward algorithm (see Collins, 2013b).\n3. For a detailed derivation of CRF, see Collins (2013a).\n5\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nWord Embedding LayerCharacter CNN Layer\n'In', 'colon', 'carcinoma', 'cells'\nLSTMF LSTMB \nDecoder Layer \nCRF Layer \n'O',  'B-Disease',  'I-Disease',  'O'\nPredicted Named Entity Tags\nFigure 4: NER model architecture.\nShared Word Embedding + Character CNN Layer\n'In', 'colon', 'carcinoma', 'cells'\nShared Decoder Layer\n<begin>'In''colon''carcinoma''colon''carcinoma''cells'<eos>\nLSTMB \nLSTMF \nForward Language ModelBackward Language Model\nFigure 5: BiLM architecture.\nwhere Sm is the set containing all possible tag combinations for a sentence, s(d,y) is a\nscoring function deﬁned as:\ns(d,y) =\nN−1∑\nt=1\nWcrf\nyt,yt+1 +\nN∑\nt=1\ndt,yt .\nIn this paper, when the logits dt,yt are fed to the CRF layer to optimize sentence likelihood,\nwe call the NER model as CNN-BiLSTM-CRF. During inference, we use the Viterbi algo-\nrithm (Forney, 1973) to ﬁnd the best tag sequence that maximizes the sentence likelihood.\n2.5. Language Modeling\nHere, we provide a short description of language modeling, as its parameters are used to\ninitialize the NER model. In language modeling, the task is to train a model that maximizes\nthe likelihood of a given sequence of words. At every step, a language model computes the\nprobability of the next word in the sequence given all the previous words. If the sequence\nof words is w1,w2,...,w n, its likelihood is given as\npf (w1,w2,...,w n) =\nn+1∏\ni=2\np(wi |w1,...,w i−1),\nwhere wn+1 is a special symbol for the end of a sequence. LSTM can be used to predict\nthe probability of the next word given the current word and the previous sequence of\nwords (Graves, 2013). This is done by applying an aﬃne transformation to the hidden\nstates of LSTM at every time step to obtain the logits for all the words in the vocabulary.\nWe refer to this approach as the forward language model (LMf ).\n6\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nWe can also model the reversed sequence of words in a similar manner. In this, we\ncompute the probability of the reversed sequence as:\npb(wn,wn−1,...,w 1) =\ni=0∏\ni=n−1\np(wi |wi+1,...,w n),\nwhere w0 is a special symbol for the start of the sequence. We refer to this approach as\nthe backward language model (LMb). The network architecture of both LMf and LMb is\nsimilar to the NER model (see Figure 5). While training, both LMf and LMb share the\nparameters of the word embedding layer, character embedding layer, character CNN ﬁlters,\nand the decoder layer. We refer to this as the bidirectional language model (BiLM). To\nlearn the parameters of the BiLM, we perform joint training by minimizing the average\ncross-entropy losses of both the forward and backward language models\nCEℓm = −λℓm(log pf (w1:n) + logpb(wn:1)).\n3. Experimental Setup\nIn this section, we will ﬁrst describe the datasets, their preprocessing, and performance\nevaluation criteria. Next, we will discuss the architecture of NER model and language\nmodel followed by their training details.\n3.1. Dataset Preparation and Evaluation\nWe evaluate our proposed approach on four datasets: NCBI-disease (Do˘ gan and Lu, 2012),\nBioCreative V Chemical Disease Relation Extraction (BC5CDR) task (Li et al., 2016),\nBC2GM (Ando, 2007), and JNLPBA (Kim et al., 2004). For each dataset, we use the\ntraining, development, and test set splits according to Crichton et al. (2017). IV An overall\nsummary of these datasets such as the number of sentences, words, and entities is presented\nin Table 1. For each dataset, we use its training and development splits as unlabeled data\nfor language modeling task.\nWe use a special token for the numbers and preserve case information. In all our\nexperiments, we use IOBES tagging format (Borthwick, 1999) for the output tags. For\nevaluation, we report the precision, recall, and F1 scores for all the entities in the test set.\nWe do exact matching of entity chunks to compute these metrics. For each dataset, we tune\nthe hyperparameters of our model on the development set. Final training is done on both\nthe training and development sets. We use PyTorch framework (Paszke et al., 2017) for all\nour experiments.\n3.2. Model Architecture Details\nAs we use language model weights to initialize the parameters of the NER model, both\nthe models have identical conﬁgurations except for the top decoder layer. Dimensions of\ncharacter embeddings and word embeddings are set to 50 and 300 respectively. CNN ﬁlters\nIV. For our experiments, we use the datasets publicly available athttps://github.com/cambridgeltl/\nMTL-Bioinformatics-2016.\n7\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nProperties NCBI-disease BC5CDR BC2GM JNLPBA\nEntity type Disease Disease, Chemical Gene/Protein 5 NEs\n# Entity mentions 6,892 5,818 24,583 51,301\n# Sentences 7,295 13,907 20,000 24,806\n# Words 184,552 360,315 1,139,824 568,786\n# Train documents 593 500 - 1,800\n# Dev documents 100 500 - 200\n# Test documents 100 500 - 404\nTable 1: General statistics of the datasets used in this work. ‘#’ symbol stands for the term\n‘number of’. Entity types in JNLPBA dataset consists of protein, DNA, RNA, cell-type,\nand cell-line.\nhave widths (w) in the range from 1 to 7. The number of ﬁlters are computed as a function\nof ﬁlter width as min(200 , 50 ∗w). The hidden state of LSTM has 256 dimensions. As the\ndecoder layer is not shared between NER model and language model, the dimensions of the\ndecoder layer are diﬀerent for each of them. For NER model, as it concatenates the hidden\nstates of forward and backward LSTM to give input to the decoder layer, the dimensions\nof the decoder matrix are Wner\nd ∈R512×T . For language model, dimensions of the decoder\nmatrix are Wℓm\nd ∈R256×V where V is the vocabulary size.\n3.3. Language Model Training\nWe initialize the weights of word embedding layer for the BiLM task using pretrained word\nvectors. These vectors were learned using skip-gram (Mikolov et al., 2013) methodV applied\nto a large collection of PubMed abstracts. VI The embeddings of out-of-vocabulary words\nare uniformly initialized. LSTM parameters are also uniformly initialized in the range\n(−0.005,0.005). For all the other model parameters, we use Xavier initialization (Glorot\nand Bengio, 2010).\nFor model training, we use mini-batch SGD with a dynamic batch size of 500 words.\nAt the start of every mini-batch step, the LSTM starts from zero initial states. We do\nsentence-level language modeling and the network is trained using BPTT. We use Adam\noptimizer (Kingma and Ba, 2014) with default parameters settings and decay the learning\nrate by 0.5 when the model’s performance plateaus. We train the model for 20 epochs and\ndo early stopping if the perplexity doesn’t improve for 3 consecutive epochs. To regularize\nthe model, we apply dropout (Srivastava et al., 2014) with probability 0 .5 on the word\nembeddings and LSTM’s hidden states. To prevent the gradient explosion problem, we do\ngradient clipping by constraining its L2 norm to be less than 1 .0 (Pascanu et al., 2013).\n3.4. NER Model Training\nTo pretrain the NER model, we remove the top decoder layer of the BiLM and transfer\nthe remaining weights to the NER model with the same architecture. Next, we ﬁne-tune\nV. We learn word embeddings using theword2vec toolkit: https://code.google.com/p/word2vec/\nVI. These PubMed abstracts are available from the BioASQ Task 4a challenge (Tsatsaronis et al., 2015).\n8\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nDataset Metric Benchmark FFN BiLSTM MTM-CW BiLM-NER\nNCBI-disease\nPrecision 85.10 - 86.11 85.86 86.41\nRecall 80.80 - 85.49 86.42 88.31\nF1 82.90 80.46 85.80 86.14 87.34\nBC5CDR\nPrecision 89.21 - 87.60 89.10 88.10\nRecall 84.45 - 86.25 88.47 90.49\nF1 86.76 83.90 86.92 88.78 89.28\nBC2GM\nPrecision - - 81.57 82.10 81.81\nRecall - - 79.48 79.42 81.57\nF1 - 73.17 80.51 80.74 81.69\nJNLPBA\nPrecision 69.42 - 71.35 70.91 71.39\nRecall 75.99 - 75.74 76.34 79.06\nF1 72.55 70.09 73.48 73.52 75.03\nTable 2: Precision, recall, and F1 scores of our proposed BiLM pretrained NER model (last col-\numn) and recent state-of-the-art models. We use the CNN-BiLSTM-CRF architecture\nfor our NER model in all the experiments. Source of benchmark performance scores of\ndatasets are: NCBI-disease: Leaman and Lu (2016); BC5CDR: Li et al. (2015); JNLPBA:\nGuoDong and Jian (2004); MTM-CW was proposed in Wang et al. (2018a); FFN\n(Feed-forward network)was proposed in Crichton et al. (2017); BiLSTM was pro-\nposed in Habibi et al. (2017). The performance scores for these NER models are referred\nfrom Wang et al. (2018a).\nthe pretrained NER model using Adam optimizer (Kingma and Ba, 2014). In contrast to\nrandom initialization, during ﬁne-tuning, the pretrained weights act as the starting point\nfor the optimizer. We use mini-batch SGD with a dynamic batch size of 1 ,000 words and\ntrain the model for 50 epochs. Other settings are similar to the language model training\nprocedure as described above.\n4. Results\nIn this section, we ﬁrst evaluate the BiLM pretrained NER model on four biomedical\ndatasets and compare the results with the state-of-the-art models. Next, we analyze dif-\nferent variations of NER model pretraining and also do three experiments to study the\nproperties of pretrained NER model. Finally, in a case study on NCBI-disease dataset,\nwe analyze the model’s predictions on disease entities. We use the CNN-BiLSTM-CRF\narchitecture for NER model in all our the experiments unless speciﬁed otherwise.\n4.1. Performance on Benchmark Datasets\nWe compare our proposed BiLM pretrained NER model with state-of-the-art NER systems\nsuch as the multi-task models of Crichton et al. (2017), Wang et al. (2018a), and pretrained\nembedding based method of Habibi et al. (2017). We show the precision, recall, and F1\nscores of the models for all the above datasets in Table 2. From the results, we see that the\napproach of BiLM pretraining obtains the maximum F1 score for all the datasets. For NCBI-\ndisease dataset, the F1 score of our model is 87 .34%, which is an absolute improvement of\n9\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nDataset Metric No pretrain LMf pretrain LMb pretrain BiLM pretrain\nNCBI-disease\nPrecision 84.38 84.62 84.75 86.41\nRecall 87.37 87.89 88.00 88.31\nF1 85.35 86.22 86.34 87.34\nBC5CDR\nPrecision 88.95 88.67 88.12 88.10\nRecall 88.64 89.28 89.41 90.60\nF1 88.79 88.97 88.76 89.28\nBC2GM\nPrecision 81.40 82.00 81.04 81.81\nRecall 79.89 80.56 80.12 81.57\nF1 80.62 81.27 80.58 81.69\nJNLPBA\nPrecision 71.23 70.51 71.00 71.39\nRecall 76.52 77.11 76.98 79.06\nF1 73.78 73.66 73.87 75.03\nTable 3: Precision, recall, and F1 scores for diﬀerent variations of our proposed model.\n1.20% over the multi-task learning method of Wang et al. (2018a), in which they train the\nNER model jointly on all the datasets combined together. Similarly, for other datasets, we\ncan see that our proposed approach outperforms other benchmark systems by a signiﬁcant\nmargin. We want to mention here that our model was trained only on the provided data\nfor a particular dataset compared with the multi-task learning methods, which require a\ncollection of labeled data to improve their performance. This also highlights the importance\nof doing pretraining of the model weights as this can improve their generalization ability\non the test set.\n4.2. Model Variations Based on Weights Pretraining\nWe also compare the performance of the following methods that are based on diﬀerent\nparameter initialization strategies for the NER model.\n•No pretraining: We randomly initialize the parameters of the NER model except\nword embeddings followed by supervised training.\n•LMf pretraining: We initialize the parameters of the NER model using the forward\nlanguage model weights. The parameters of backward LSTM, decoder, and CRF are\nrandomly initialized.\n•LMb pretraining: We initialize the parameters of the NER model using the back-\nward language model weights. The parameters of forward LSTM, decoder, and CRF\nare randomly initialized.\n•BiLM pretraining: In this, the parameters of the NER model are initialized using\nthe bidirectional language model weights. The parameters of the decoder and CRF\nare randomly initialized.\nWe show the results of the above variations in model pretraining in Table 3. Our\nmodel gives an absolute improvement of around 2% and 0 .5% in F1 score on NCBI-disease\n10\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\n0.700 0.725 0.750 0.775 0.800 0.825 0.850 0.875\nRecall\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94Precision\nBiLM pretraining\nNo pretraining\n(a) NCBI-disease dataset\n0.700 0.725 0.750 0.775 0.800 0.825 0.850 0.875\nRecall\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96Precision\nBiLM pretraining\nNo pretraining (b) BC5CDR dataset\nFigure 6: Smoothed precision-recall curves for models with BiLM pretraining and no pretraining.\nBest viewed in color.\nand BC5CDR dataset respectively over the model with no pretraining. We note that for\nall the datasets, LMf pretraining and LMb pretraining also gives an improvement over\nno pretraining. From the results, we also observe that the BiLM pretraining achieves\nbetter F1 score and precision in comparison to LMf pretraining and LMb pretraining, thus\nhighlighting the importance of performing language modeling in both directions.\n4.3. Model Studies\nNext, we plot the precision-recall curve, convergence rate, and learning curve to gain ad-\nditional insights about the NER model with BiLM pretraining and compare it with the\nrandomly initialized model.\n4.3.1. Precision-Recall Curve\nIn Figure 6a and 6b, we plot the smoothed precision-recall curve for NCBI-disease and\nBC5CDR datasets. From both the plots, we see that the BiLM pretrained NER model is\nalways optimal as its area under the precision-recall curve is always more than that of a\nrandomly initialized NER model.\n4.3.2. Rate of Convergence\nWe monitor the overall clock time and the time taken per epoch required for the two\nmodels to converge. We follow the same training process as outlined above. A typical run\nfor both the models on NCBI-disease and BC5CDR dataset is shown in Figure 7a and 7b\nrespectively. For NCBI-disease dataset, the model with BiLM pretraining converges in 10\nepochs (≈500s) compared with the model with no pretraining, which typically converges\nin 14 epochs ( ≈700s). We observe a similar trend in the BC5CDR dataset where BiLM\npretraining results in convergence in 11 epochs (≈900s) whereas no pretraining takes around\n17 epochs ( ≈ 1150s). Thus, in terms of total time taken, we observe that pretraining\n11\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\n100 200 300 400 500 600 700\nTime (in seconds)\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0F1 Score\nBiLM pretraining\nNo pretraining\n(a) NCBI-disease dataset\n200 400 600 800 1000 1200\nTime (in seconds)\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0F1 Score\nBiLM pretraining\nNo pretraining (b) BC5CDR dataset\nFigure 7: F1 score versus time taken for training to converge for models with BiLM pretraining\nand no pretraining. Best viewed in color.\nusing BiLM weights results in faster convergence by about 28-35% compared with random\nparameter initialization setting. We also see that BiLM pretraining results in a better F1\nscore from ﬁrst epoch onwards for both the datasets.\n4.3.3. Learning Curve\nIn this setup, we analyze the F1 score of both the models by feeding them with an increasing\nnumber of examples during the training process (learning curve). The learning curve for\nboth the models on NCBI-disease and BC5CDR datasets is shown in Figure 8a and 8b\nrespectively. We can see that the BiLM pretrained model is always optimal (achieves higher\nF1 score) for any setting of the number of training examples.\n4.4. Case Study on NCBI-Disease Dataset\nWe will now discuss qualitative results of the BiLM pretrained NER model on NCBI-disease\ndataset. The NCBI-disease dataset consists of abstracts from medical research papers, which\nare written in a technical language and contains many complex entity names. In the NCBI-\ndisease dataset, combined training and development set contains 1 ,902 unique mentions of\ndisease entities. In its test set, there are 423 unique occurrences of disease names and the\nBiLM pretrained NER model is able to correctly predict 365 such diseases. Some examples\nof the longer disease names that are hard to recognize but our approach is able to correctly\npredict are “ sporadic breast , brain , prostate and kidney cancer ,” “deﬁciency of the ninth\ncomponent of human complement ,” “von hippel - lindau ( vhl ) tumor ,” and “deﬁciency of\nthe lysosomal enzyme aspartylglucosaminidase .”\nAmong the 423 unique mentions of diseases in the test set, 232 of them are unseen in the\ncombined training and development set. Our model was able to correctly predict around\n120 unseen disease entities in the test set. Some examples of unseen disease entities that\nare correctly predicted are “ deﬁciency of the lysosomal enzyme aspartylglucosaminidase ,”\n12\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\n0 500 1000 1500 2000 2500 3000 3500 4000\nNumber of Training Examples\n60\n65\n70\n75\n80\n85F1 Score\nBiLM pretraining\nNo pretraining\n(a) NCBI-disease dataset\n0 1000 2000 3000 4000 5000\nNumber of Training Examples\n65\n70\n75\n80\n85F1 Score\nBiLM pretraining\nNo pretraining (b) BC5CDR dataset\nFigure 8: F1 score versus increasing number of training examples for models with BiLM pretraining\nand no pretraining. Best viewed in color.\n“campomelic - metaphyseal skeletal dysplasia,” “atrophic benign epidermolysis bullosa,” and\n“ectopic intracranial retinoblastoma.” This can be attributed to the improved modeling of\nthe relationship among context words during bidirectional language modeling pretraining\nstep. Some examples of the disease entities where our model fails are “ bannayan - zonana\n( bzs ) or ruvalcaba - riley - smith syndrome ,” “ very - long - chain acyl - coenzyme a\ndehydrogenase deﬁciency,” “vwf - deﬁcient ,” and “diﬀuse mesangial sclerosis .” From these\nexamples, we see that the model makes an incorrect prediction when the disease entities\nhave longer names, which may also contain abbreviations.\n5. Related Work\nTraditionally, researchers have worked on carefully designing hand-engineered features to\nrepresent a word such as the use of parts-of-speech (POS) tags, capitalization information,\nuse of rules such as regular expressions to identify numbers, use of gazetteers, etc. A combi-\nnation of supervised classiﬁers using such features was used to achieve the best performance\non CoNLL-2003 benchmark NER dataset (Florian et al., 2003). Laﬀerty et al. (2001) popu-\nlarized the use of graphical models such as linear-chain conditional random ﬁelds (CRF) for\nNER tasks. Among the early approaches of NER systems in the biomedical domain include\nABNER (Settles, 2004), BANNER (Leaman and Gonzalez, 2008), and GIMLI (Campos\net al., 2013), which used a variety of lexical, contextual, and orthographic features as input\nto a linear-chain CRF.\nThe next generation of methods involves neural networks as they can be trained end-\nto-end using only the available labeled data without the need of manual task-speciﬁc fea-\nture engineering. In their seminal work, Collobert et al. (2011) trained window-based and\nsentence-based models for several NLP tasks and demonstrated competitive performance.\nFor NER task on newswire texts, Huang et al. (2015) uses word embeddings, spelling, and\ncontextual features that are fed to a BiLSTM-CRF model. To incorporate character fea-\n13\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\ntures, Lample et al. (2016) applies BiLSTM while Chiu and Nichols (2016); Ma and Hovy\n(2016) applies CNNs on character embeddings respectively. For biomedical NER task, Wei\net al. (2016) combines the output of BiLSTM and traditional CRF-based model using an\nSVM classiﬁer. Zeng et al. (2017) experiments with character-level and word-level BiLSTM\nfor the task of drug NER. Habibi et al. (2017) investigates the eﬀect of pretrained word\nembeddings on several biomedical NER datasets.\nPretraining the neural network model parameters using transfer learning has been widely\nstudied and has shown to improve results in a variety of tasks such as deep autoencoders\nfor dimensionality reduction (Hinton and Salakhutdinov, 2006), computer vision (Erhan\net al., 2010), text classiﬁcation (Dai and Le, 2015; Howard and Ruder, 2018), machine\ntranslation (Ramachandran et al., 2017; Qi et al., 2018), and question answering (Min\net al., 2017).\nFor sequence tagging tasks, supervised transfer learning to pretrain the model from the\nweights of another model that was trained on a diﬀerent labeled dataset has been applied\nto domain adaptation tasks (Qu et al., 2016), de-identiﬁcation of patient notes (Lee et al.,\n2018), NER task in tweets (von D¨ aniken and Cieliebak, 2017), and biomedical NER (Giorgi\nand Bader, 2018; Wang et al., 2018b). In contrast, we pretrain the weights of the NER\nmodel from a language model that is trained on unlabeled data and thus removing the hard\ndependency on the availability of larger labeled datasets for pretraining.\n6. Conclusion\nIn this paper, we present a transfer learning approach for the task of biomedical NER. In\nour NER model, we use CNNs with diﬀerent ﬁlters widths to extract character features\nand a word-level BiLSTM for sequence modeling which takes both word embeddings and\ncharacter features as inputs. We pretrain the NER model weights using a BiLM such that\nthe architectures of both the models are same except for the top decoder layer. The BiLM\nis trained in an unsupervised manner using only the unlabeled data.\nWe show that such pretraining of the NER model weights is a good initialization strategy\nfor the optimizer as it leads to substantial improvements in the F1 scores for four benchmark\ndatasets. Further, to achieve a particular F1 score, pretrained model requires less training\ndata compared with a randomly initialized model. A pretrained model also converges faster\nduring model ﬁne-tuning. We also observe gains in the recall score for both seen and unseen\ndisease entities.\nFor future work, we plan to train bigger sized language models on large collections of\nmedical corpora and use it for providing additional features to the NER model so that it can\nincorporate wider context while training. We also plan to use external medical knowledge\ngraphs to further improve the NER model’s performance.\nAcknowledgments\nThis work was supported by a generous research funding from CMU, MCDS students grant.\nWe would also like to thank the anonymous reviewers for giving us their valuable feedback\nthat helped to improve the paper.\n14\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nReferences\nRie Ando. Biocreative II gene mention tagging system at IBM Watson. In Second BioCre-\native Challenge Evaluation Workshop , volume 23, pages 101–103. Centro Nacional de\nInvestigaciones Oncologicas (CNIO) Madrid, Spain, 2007.\nAndrew Borthwick. A Maximum Entropy Approach to Named Entity Recognition . PhD\nthesis, New York, NY, USA, 1999.\nDavid Campos, S´ ergio Matos, and Jos´ e Oliveira. Gimli: open source and high-performance\nbiomedical name recognition. BMC Bioinformatics, 14(1):54, Feb 2013.\nJason Chiu and Eric Nichols. Named entity recognition with bidirectional LSTM-CNNs.\nTransactions of the Association for Computational Linguistics , 4:357–370, 2016.\nMichael Collins. Log-Linear Models, MEMMs, and CRFs. 2013a. URL http://www.cs.\ncolumbia.edu/~mcollins/crf.pdf.\nMichael Collins. The forward-backward algorithm. 2013b. URLhttp://www.cs.columbia.\nedu/~mcollins/fb.pdf.\nRonan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and\nPavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine\nLearning Research, 12:2493–2537, November 2011.\nGamal Crichton, Sampo Pyysalo, Billy Chiu, and Anna Korhonen. A neural network multi-\ntask learning approach to biomedical named entity recognition. BMC Bioinformatics, 18\n(1):368, 2017.\nAndrew Dai and Quoc Le. Semi-supervised sequence learning. In International Confer-\nence on Neural Information Processing Systems - Volume 2 , NIPS’15, pages 3079–3087,\nCambridge, MA, USA, 2015.\nRezarta Do˘ gan and Zhiyong Lu. An improved corpus of disease mentions in PubMed\ncitations. In Workshop on Biomedical Natural Language Processing , BioNLP ’12, pages\n91–99, Stroudsburg, PA, USA, 2012.\nDumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent,\nand Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of\nMachine Learning Research, 11:625–660, March 2010.\nRadu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. Named entity recognition\nthrough classiﬁer combination. In Conference on Natural Language Learning at HLT-\nNAACL 2003 - Volume 4 , CONLL ’03, pages 168–171, Stroudsburg, PA, USA, 2003.\nGeorge David Forney. The Viterbi algorithm. Proceedings of the IEEE , 61(3):268–278,\nMarch 1973.\nJohn M Giorgi and Gary D Bader. Transfer learning for biomedical named entity recognition\nwith neural networks. Bioinformatics, page bty449, 2018.\n15\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nXavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward\nneural networks. In International Conference on Artiﬁcial Intelligence and Statistics ,\nvolume 9 of Machine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia,\nItaly, 13–15 May 2010.\nAlex Graves. Generating sequences with recurrent neural networks. Computing Research\nRepository, arXiv:1308.0850, 2013.\nZhou GuoDong and Su Jian. Exploring deep knowledge resources in biomedical name recog-\nnition. In International Joint Workshop on Natural Language Processing in Biomedicine\nand Its Applications , JNLPBA ’04, pages 96–99, Stroudsburg, PA, USA, 2004.\nMaryam Habibi, Leon Weber, Mariana Neves, David Luis Wiegandt, and Ulf Leser. Deep\nlearning with word embeddings improves biomedical named entity recognition. Bioinfor-\nmatics, 33(14):i37–i48, 2017.\nGeoﬀrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural\nnetworks. Science, 313(5786):504–507, 2006.\nSepp Hochreiter and J¨ urgen Schmidhuber. Long short-term memory.Neural Computation,\n9(8):1735–1780, November 1997.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi-\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 328–339, 2018.\nZhiheng Huang, Wei Xu, and Kai Yu. Bidirectional LSTM-CRF models for sequence tag-\nging. Computing Research Repository, arXiv:1508.01991, 2015.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. Intro-\nduction to the bio-entity recognition task at JNLPBA. In International Joint Workshop\non Natural Language Processing in Biomedicine and Its Applications , pages 70–75, 2004.\nYoon Kim. Convolutional neural networks for sentence classiﬁcation. In Conference on\nEmpirical Methods in Natural Language Processing (EMNLP) , pages 1746–1751, Doha,\nQatar, October 2014.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. Character-aware neural\nlanguage models. In AAAI Conference on Artiﬁcial Intelligence , AAAI’16, pages 2741–\n2749. AAAI Press, 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Computing\nResearch Repository, arXiv:1412.6980, 2014.\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey Hinton. ImageNet classiﬁcation with deep\nconvolutional neural networks. In Proceedings of the 25th International Conference on\nNeural Information Processing Systems - Volume 1 , NIPS’12, pages 1097–1105, USA,\n2012.\n16\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nJohn Laﬀerty, Andrew McCallum, and Fernando Pereira. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling sequence data. In International Conference\non Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA, 2001.\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and\nChris Dyer. Neural architectures for named entity recognition. In Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies, pages 260–270, San Diego, California, June 2016.\nRobert Leaman and Graciela Gonzalez. BANNER: An executable survey of advances in\nbiomedical named entity recognition. Paciﬁc Symposium on Biocomputing. Paciﬁc Sym-\nposium on Biocomputing, pages 652–63, 2008.\nRobert Leaman and Zhiyong Lu. TaggerOne: joint named entity recognition and normal-\nization with semi-Markov models. Bioinformatics, 32(18):2839–2846, 2016.\nYan LeCun, Bernhard Boser, John Denker, R. Howard, Wayne Habbard, Lawrence Jackel,\nand Donnie Henderson. Handwritten digit recognition with a back-propagation network.\nIn Advances in Neural Information Processing Systems 2 , pages 396–404. Morgan Kauf-\nmann Publishers Inc., San Francisco, CA, USA, 1990.\nJi Young Lee, Franck Dernoncourt, and Peter Szolovits. Transfer learning for named-entity\nrecognition with neural networks. In Eleventh International Conference on Language\nResources and Evaluation, LREC 2018, Miyazaki, Japan , 2018.\nHaodi Li, Qingcai Chen, Kai Chen, and Buzhou Tang. Hitsz cdr system for disease and\nchemical named entity recognition and relation extraction. 2015.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman,\nAllan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. BioCreative\nV CDR task corpus: a resource for chemical disease relation extraction. Database, 2016.\nXuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-\nCNNs-CRF. In Association for Computational Linguistics (Volume 1: Long Papers) ,\npages 1064–1074, Berlin, Germany, August 2016.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeﬀrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. InInternational Confer-\nence on Neural Information Processing Systems - Volume 2 , NIPS’13, pages 3111–3119,\nUSA, 2013.\nSewon Min, Minjoon Seo, and Hannaneh Hajishirzi. Question answering through trans-\nfer learning from large ﬁne-grained supervision data. In Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 510–517, 2017.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the diﬃculty of training recur-\nrent neural networks. In International Conference on Machine Learning - Volume 28 ,\nICML’13, pages 1310–1318. JMLR.org, 2013.\n17\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\nDeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic dif-\nferentiation in pytorch. In NIPS-W, 2017.\nYe Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig.\nWhen and why are pre-trained word embeddings useful for neural machine translation?\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers) ,\npages 529–535, 2018.\nLizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, and Timothy Baldwin. Named en-\ntity recognition for novel types by transfer learning. In Conference on Empirical Methods\nin Natural Language Processing, pages 899–905, 2016.\nPrajit Ramachandran, Peter Liu, and Quoc Le. Unsupervised pretraining for sequence\nto sequence learning. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pages 383–391, 2017.\nMaya Rotmensch, Yoni Halpern, Abdulhakim Tlimat, Steven Horng, and David Sontag.\nLearning a health knowledge graph from electronic medical records. Scientiﬁc Reports,\n2017.\nMike Schuster and Kuldip Paliwal. Bidirectional recurrent neural networks. IEEE Trans-\nactions on Signal Processing , 45(11):2673–2681, November 1997.\nBurr Settles. Biomedical named entity recognition using conditional random ﬁelds and\nrich feature sets. In International Joint Workshop on Natural Language Processing in\nBiomedicine and Its Applications , JNLPBA ’04, pages 104–107, Stroudsburg, PA, USA,\n2004.\nNitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of\nMachine Learning Research, 15(1):1929–1958, January 2014.\nMark Stevenson and Yikun Guo. Disambiguation in the biomedical domain: The role of\nambiguity type. Journal of Biomedical Informatics , 43(6):972–981, December 2010.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias\nZschunke, Michael Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dim-\nitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick\nGallinari, Thierry Artieres, Axel Ngonga, Norman Heino, Eric Gaussier, Liliana Barrio-\nAlvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. An overview\nof the BIOASQ large-scale biomedical semantic indexing and question answering compe-\ntition. BMC Bioinformatics, 16:138, 2015.\nPius von D¨ aniken and Mark Cieliebak. Transfer learning and sentence level features for\nnamed entity recognition on tweets. In Workshop on Noisy User-generated Text , pages\n166–171, 2017.\n18\nBidirectional Language Modeling for Transfer Learning in Biomedical NER\nXuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis\nLanglotz, and Jiawei Han. Cross-type biomedical named entity recognition with deep\nmulti-task learning. Computing Research Repository, arXiv:1801.09851, 2018a.\nZhenghui Wang, Yanru Qu, Liheng Chen, Jian Shen, Weinan Zhang, Shaodian Zhang, Yimei\nGao, Gen Gu, Ken Chen, and Yong Yu. Label-aware double transfer learning for cross-\nspecialty medical named entity recognition. InConference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1–15, 2018b.\nQikang Wei, Tao Chen, Ruifeng Xu, Yulan He, and Lin Gui. Disease named entity recogni-\ntion by combining conditional random ﬁelds and bidirectional recurrent neural networks.\nDatabase, 2016.\nPaul Werbos. Generalization of backpropagation with application to a recurrent gas market\nmodel. Neural networks, 1(4):339–356, 1988.\nPaul Werbos. Backpropagation through time: what it does and how to do it. Proceedings\nof the IEEE , 78(10):1550–1560, Oct 1990.\nDonghuo Zeng, Chengjie Sun, Lei Lin, and Bingquan Liu. LSTM-CRF for drug-named\nentity recognition. Entropy, 19(6), 2017.\n19",
  "topic": "Named-entity recognition",
  "concepts": [
    {
      "name": "Named-entity recognition",
      "score": 0.9399325847625732
    },
    {
      "name": "Computer science",
      "score": 0.8339523077011108
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.8046635389328003
    },
    {
      "name": "Transfer of learning",
      "score": 0.7356832027435303
    },
    {
      "name": "Initialization",
      "score": 0.7291088104248047
    },
    {
      "name": "Task (project management)",
      "score": 0.6839515566825867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6542370319366455
    },
    {
      "name": "Language model",
      "score": 0.6315667629241943
    },
    {
      "name": "Labeled data",
      "score": 0.5490480065345764
    },
    {
      "name": "F1 score",
      "score": 0.5236836671829224
    },
    {
      "name": "Natural language processing",
      "score": 0.5229355096817017
    },
    {
      "name": "Machine learning",
      "score": 0.5076977014541626
    },
    {
      "name": "Deep learning",
      "score": 0.45163094997406006
    },
    {
      "name": "Training set",
      "score": 0.41152653098106384
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 32
}