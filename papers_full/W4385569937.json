{
  "title": "Lifting the Curse of Capacity Gap in Distilling Language Models",
  "url": "https://openalex.org/W4385569937",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100374101",
      "name": "Chen Zhang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100397486",
      "name": "Yang Yang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100347847",
      "name": "Jiahao Liu",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5100695181",
      "name": "Jingang Wang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5086726545",
      "name": "Yunsen Xian",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5057282504",
      "name": "Benyou Wang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A5050532724",
      "name": "Dawei Song",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285202066",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W3199246732",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W4285134706",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3131861520",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W4224296706",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3170796112",
    "https://openalex.org/W4295116917",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3138895808",
    "https://openalex.org/W4221156865",
    "https://openalex.org/W4226364033",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W2148603752",
    "https://openalex.org/W4304700893",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W4285410153",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4226515448",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W4226126941",
    "https://openalex.org/W3103884771",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W1480376833",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3004127093",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3204647170",
    "https://openalex.org/W4281842259",
    "https://openalex.org/W2264905057",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3204703845",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W4293390340",
    "https://openalex.org/W3015609966"
  ],
  "abstract": "Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, Dawei Song. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4535–4553\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLifting the Curse of Capacity Gap in\nDistilling Language Models\nChen Zhang♣, Yang Yang♦, Jiahao Liu♦, Jingang Wang♦, Yunsen Xian♦,\nBenyou Wang♥, Dawei Song♣∗\n♣Beijing Institute of Technology ♦Meituan NLP\n♥The Chinese University of Hong Kong, Shenzhen\nchenzhang9702@outlook.com\nAbstract\nPretrained language models (LMs) have shown\ncompelling performance on various down-\nstream tasks, but unfortunately they require\na tremendous amount of inference compute.\nKnowledge distillation finds a path to com-\npress LMs to small ones with a teacher-student\nparadigm. However, when the capacity gap\nbetween the teacher and the student is large, a\ncurse of capacity gap appears, invoking a defi-\nciency in distilling LMs. While a few studies\nhave been carried out to fill the gap, the curse\nis not yet well tackled. In this paper, we aim\nat lifting the curse of capacity gap via enlarg-\ning the capacity of the student without notably\nincreasing the inference compute. Largely mo-\ntivated by sparse activation regime of mixture\nof experts (MOE), we propose a mixture of min-\nimal experts (MINI MOE), which imposes extra\nparameters to the student but introduces almost\nno additional inference compute. Experimen-\ntal results on GLUE and CoNLL demonstrate\nthe curse of capacity gap is lifted by the magic\nof MINI MOE to a large extent. MINI MOE\nalso achieves the state-of-the-art performance\nat small FLOPs compared with a range of com-\npetitive baselines. With a compression rate as\nmuch as ∼50×, MINI MOE preserves ∼95%\nGLUE score of the teacher.1\n1 Introduction\nPretrained language models (LMs) have become a\npopular choice for various downstream tasks, e.g.,\ntext classification, token classification, and ques-\ntion answering (Devlin et al., 2019; Liu et al., 2019;\nRaffel et al., 2020). Unfortunately, appealing per-\nformance comes with a huge cost of inference com-\npute due to the scale of LMs. Knowledge distil-\nlation (Hinton et al., 2015; Sun et al., 2019), as\nan alternative to model pruning (Han et al., 2015)\nand quantization (Sung et al., 2015), discovers a\n∗Dawei Song is the corresponding author.\n1Code is available at https://github.com/GeneZC/\nMiniMoE\nTable 1: The curse of the capacity gap in terms of\nGLUE (Wang et al., 2019). The △denotes the perfor-\nmance difference of preceding two numbers. To ensure\nstudents at similar scales, the student/teacher scale ra-\ntios are properly reduced for some methods.\nMethod BERTbase BERTlarge △\nTeacher 86.7 88.3 +1.6\nKD10%/5%(2015) 81.3 80.8 −0.5\nDynaBERT15%/5%(2020) 81.1 79.2 −1.9\nMiniDisc10%/5%(2022a) 82.4 82.1 −0.3\nTinyBERT4L;312H(2020) 82.7 82.5 −0.2\nMiniLM3L;384H(2021b) 82.5 82.0 −0.5\nMiniMoE3L;384H(ours) 82.6 83.1 +0.5\nway to compress (Bucila et al., 2006) LMs with a\nteacher-student paradigm.\nHowever, in LM distillation, we recognize a\ncurse of capacity gap as:\n“Large teachers, poor students.”\nThe curse of capacity gap refers to a deficiency\nthat a larger teacher might unexpectedly result in a\npoorer student especially when the capacity gap be-\ntween the teacher and the student is large (Mirzadeh\net al., 2020; Cho and Hariharan, 2019), as illus-\ntrated in Table 1. Notably, this is the first veri-\nfication in LM distillation since previous studies\nrecognize the curse in vision model distillation. Al-\nthough a few studies (Wang et al., 2020; Zhang\net al., 2022a; Park et al., 2021a) have investigated\nto fill the gap, the curse is still not yet tackled.\nTo the demand, we aim at lifting the curse of\ncapacity gap via enlarging the capacity of the stu-\ndent without notably increasing the inference com-\npute. We propose a mixture of minimal experts\n(MINI MOE), inspired by the intuition of sparse ac-\ntivation of mixture of experts (MOE) (Shazeer et al.,\n2017). Thanks to that the activation process can\nbe parallel on either single or multiple devices (He\net al., 2021; Rajbhandari et al., 2022), MINI MOE\non the one hand imposes extra parameters to the\n4535\n0 2 4 6 8 10\nGFLOPs\n78\n80\n82\n84\n86GLUE\nBERT\nKD\nMiniDisc\nDynaBERT\nTinyBERT\nMoEBERT\nMiniLM\nMiniMoE\nFigure 1: GLUE v.s. GFLOPs.\nstudent, but on the other hand introduces negligibly\nadditional inference compute brought by routing\nalgorithm. To our best knowledge, this is the first\nwork aiming at lifting the curse completely.\nExperiments are conducted on GLUE (Wang\net al., 2019) and CoNLL (Sang and Meulder, 2003).\nThe results exhibit that MINI MOE largely lifts the\ncurse of the gap as in Table 1. MINI MOE also\nachieves state-of-the-art performance compared\nwith a range of competitive baselines, as shown\nin Figure 1. With compression as much as ∼50×,\nMINI MOE preserves ×95% GLUE score of the\nteacher. Thereby, we state that MINI MOE is a\nsmall yet nontrivial magic, making a great differ-\nence in lifting the curse.\n2 Curse of Capacity Gap\nThe curse of capacity gap is not new but is al-\nready recognized in studies on vision model dis-\ntillation (Mirzadeh et al., 2020; Cho and Hariha-\nran, 2019). While a hit-the-mind drawback of the\ncurse is that the performance of distilling to a small\nstudent can be dramatically worse than that of dis-\ntilling to a slightly larger one, a rather counter-\nintuitive deficiency is invoked as that the perfor-\nmance of distilling from a large teacher can be\nunexpectedly worse than that of distilling from a\nsmaller one (i.e., large teacher, poor student). We\nhere give a minor theoretical justification on the\ncurse, as a plus to the empirical justification.\nProposition 1 (VC dimension theory, Vapnik,\n1998). Assuming that the teacher function is fT ∈\nFT, the labeling function is f ∈F, and the data\nis D, we have:\nr(fT) −r(f) ≤ϵT + o(|FT|c\n|D| ),\nwhere r(·) is the risk function, |·|c is the function\nclass capacity measure, and |·| is the data scale\nmeasure. It should be highlighted that the approx-\nimation error ϵT is negatively correlated with the\ncapacity of the teacher model while the estimation\nerror o(·) is correlated with the learning optimiza-\ntion.\nProposition 2 (Generalized distillation the-\nory, Lopez-Paz et al., 2016). Additionally provid-\ning that the student function is fS∈FS, we have:\nr(fS) −r(fT) ≤ϵG+ o(|FS|c\n|D|α ),\nwhere the approximation error ϵGis positively cor-\nrelated with the capacity gap between the teacher\nand the student models, and 1/2 ≤α ≤1 is a\nfactor correlated to the learning rate.\nTheorem 1. The bound for the student function at\na learning rate can be written as:\nr(fS) −r(f) ≤ϵT + ϵG+ o(|FT|c\n|D| ) +o(|FS|c\n|D|α )\n≤ϵT + ϵG+ o(|FT|c + |FS|c\n|D|α ),\nProof. The proof is rather straightforward by com-\nbining Proposition 1 and 2.\nRemark 1. Under the same distillation setting, we\ncan ignore the estimation error. When we compare\ntwo students of different capacities distilled from\na teacher of the same capacity, the student of a\nsmaller capacity has a larger ϵG thus lower per-\nformance. When we compare two students of the\nsame capacities distilled from teachers of different\ncapacities, the student distilled from the teacher of\na larger capacity has a smaller ϵT yet a larger ϵG\nthus a tradeoff.\nRemark 1 basically tells that a tradeoff is associ-\nated with the increase of teacher capacity, implying\nthat increasing teacher capacity would first lead to\nimproved but then degraded student performance.\nThis tradeoff naturally corresponds with the curse.\nOn the other hand, it is accepted that large ca-\npacity gap is a pain and is processed in literature\nof LM distillation (Wang et al., 2020; Zhang et al.,\n2022a; Zhou et al., 2022). Being unaware of the\ncurse of capacity gap, these studies attempt to of-\nfer student-friendly teachers by either interpolat-\ning teacher assistants (Wang et al., 2020; Zhang\net al., 2022a) or adapting teacher knowledge (Zhou\net al., 2022; Yang et al., 2022). The unawareness\n4536\nis largely due to a fun fact that they only distil\nLMs like BERTbase, but neglect the scalability to\nLMs like BERTlarge especially when the student is\nsmall. Though the performance of student can be\nboosted in this way, the curse still remains in LM\ndistillation as in Figure 2. Other related work in\nknowledge distillation is given in Appendix E.\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nGFLOPs\n76\n78\n80\n82\n84GLUE\nBERTbase MiniLM\nBERTlarge MiniLM\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nGFLOPs\n76\n78\n80\n82\n84GLUE\nBERTbase MiniLM w/ TA\nBERTlarge MiniLM w/ TA\nFigure 2: The performance of MiniLM and MiniLM\nw/ TA across different student scales upon distilling\nBERTbase. We are glad to share checkpoints of an array\nof scales, together with those of MINI MOE, to facilitate\nthe development of related research. It should be noted\nthe unit of a vertical grid is comparably large.\nEmbarrassingly, while the curse is claimed to be\ntackled in vision model distillation (Zhu and Wang,\n2021; Park et al., 2021a; Zhao et al., 2022), our\npreliminary study (cf. Table 13 in Appendix H)\nindicates they are either expensive or not capable\nof LMs. The potential differences are as follows:\ntasks (e.g., ImageNet v.s. GLUE), backbones (e.g.,\nResNets v.s. transformers), and paradigms (e.g.,\nfrom scratch v.s. pretraining).\n3 MiniMoE\n3.1 Motivation\nEnlarging the capacity of the student is an intuitive\nsolution to lift the curse of capacity gap. How-\never, regarding the inference compute efficiency,\nthe increase of capacity should not introduce much\ninference compute.\nAn initial proposal can be using quantized back-\nbones (Zafrir et al., 2019; Bai et al., 2021). Quan-\ntized backbones may decrease the compute pre-\ncision, therefore maintaining inference compute\nTable 2: A comparison between MINI MOE and other\npossible alternatives.\nMethod Flexible\nHardware\nControllable\nCompute\nScalable\nCompute\nQuantization ✗ ✓ ✓\nDepth-adaptation ✓ ✗ ✓\nMoEfication ✓ ✓ ✗\nMINIMOE ✓ ✓ ✓\nconstant, along the course of enlarging the capacity.\nBut a vital portion of hardware-specific modifica-\ntions are needed to do so. We hence move on to\nnext possibility.\nAnother alternative is using dynamic net-\nworks (Han et al., 2021) based on the idea of con-\nditional computation (Bengio et al., 2015). MoE\ncomputation (Shazeer et al., 2017; Fedus et al.,\n2021) is an option derived upon the sparse activa-\ntion property to increase the scale with only minor\nlosses in compute efficiency. The other commonly\nused one is depth-adaptive computation (Xin et al.,\n2020; Zhou et al., 2020; Goyal et al., 2020; Kim\nand Cho, 2021) which involves layers into com-\nputation adaptively on either example (alias early\nexiting, Xin et al., 2020; Zhou et al., 2020) or to-\nken (alias token reduction, Goyal et al., 2020; Kim\nand Cho, 2021) level. A critical distinction between\nMoE and depth-adaptive models is that the com-\npute of an MoE model is accurately under control\nwhile that of a depth-adaptive model is not. We\nare impelled by the merits of MoE, and propose\na MINI MOE so that the capacity of the student\ncan be enlarged without much inference overhead\nincrement.\nAdditionally, we argue that MINI MOE is orthog-\nonal to alternatives mentioned above, and MIN-\nIMOE can be incorporated to these alternatives\nand makes it possible to serve more extreme sce-\nnarios. It is noteworthy that a certain stream of\nwork (Zhang et al., 2022b; Zuo et al., 2022) actu-\nally accelerates LMs via precisely converting them\ninto MoE models. Nonetheless, the moefication\nprocess is directly exerted to LMs with limited in-\nference compute improvements (cf. MoEBERT in\nFigure 1). Contrarily, MINI MOE is comprised of\nminimal experts, each of which can be extremely\nsmall. A comparison between mentioned possibili-\nties and MINI MOE is listed in Table 2. And other\nrelated work of interest is given in Appendix E.\n4537\n3.2 Implementation\nMinimal Language Models Typical language\nmodels are comprised of a stack of transformers\nlayers (Vaswani et al., 2017), and are pretrained\nwith language modeling tasks such as masked lan-\nguage modeling (Devlin et al., 2019). A trans-\nformer layer can be decomposed to a multi-head\nself-attention (MHA) block and a feed-forward net-\nwork (FFN) block. Concretely, given an n-length\nsequence of d-dimension input vectors X ∈Rn×d\nwith the i-th vector beingxi, the output of the MHA\nblock with Aindependent heads can be represented\nas:\nMHA(X) =\nA∑\nj=1\nAttn(X; WQ\nj ,WK\nj )XWV\nj WO\nj ,\nAttn(X;WQ\nj ,WK\nj ) =\nsoftmax(XWQ\nj WK⊤\nj X⊤/dA),\nwhere the j-th head is parameterized by WQ\nj , WK\nj ,\nWV\nj ∈Rd×dA\n, and WO\nj ∈RdA×d. On the other\nhand, the output of the FFN block is shown as:\nFFN(X) =GELU(XWI)WO,\nwhere two fully-connected layers are parameterized\nby WI ∈Rd×dI\nand WO ∈RdI×d respectively.\nDetails like biases, normalizations of a transformer\nlayer are omitted for brevity.\nTo reach an acceptable compute budget, pioneer-\ning studies either pretrain language models or distil\nones of small scales from LMs as in Figure 3. There\nare three lines of work in LM distillation: firstly,\ntask-specific distillation (Sun et al., 2019; Li et al.,\n2020; Sun et al., 2020a; Park et al., 2021b; Hou\net al., 2020; Xia et al., 2022) that conducts distilla-\ntion on a specific task at finetuning stage; secondly,\ntask-agnostic distillation (Turc et al., 2019; Sanh\net al., 2019; Sun et al., 2020b; Wang et al., 2021b)\nthat conducts distillation at pretraining stage; and\nthirdly, two-stage distillation (Jiao et al., 2020) that\ncombines the power of both task-agnostic and -\nspecific distillation. Here, the distilled language\nmodels only refer to language models distilled\nwith task-agnostic distillation regarding better task-\nscalability as the number of concerned tasks ex-\nplodes.\nWe formally define the distilled language models\nas minimal language models (MiniLMs, somehow\nabuse of notation with Wang et al., 2020) notated\nwith S. In contrast, LMs are notated with T. The\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA FFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nFFN\nFFN\nMHA\nFFN\nFFN\nFFN\nMHA\nFFN\nFFN\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFFN\nMHA\nFigure 3: Implementation of M INI MOE.\nlearning objective of MiniLMs can be abstracted\nas L(S; T,D), where Ddenotes the data. The\nspecific form of Lcan be adapted to arbitrary align-\nment strategies. We adopt a relation alignment\nstrategy (Wang et al., 2021b) as follows:\nL(S; T,D) =EX∼D\nR∑\nj=1\nKL(Reln(X; TWQ\nj ),Reln(X; SWQ\nj ))\n+ KL(Reln(X; TWK\nj ),Reln(X; SWK\nj ))\n+ KL(Reln(X; TWV\nj ),Reln(X; SWV\nj )),\nReln(X;TWQ\nj )\n= softmax(XTWQ\nj\nTWQ⊤\nj X⊤/dR),\nwhere KL stands for kullback-leibler divergence.\nEssentially, relation heads are derived by merging\nthe original A attention heads and then splitting\nthem to Rheads. TWQ\nj is the redistributed query\nparameter of the j-th relation head within totally\nR heads from the last layer of the LM, likewise\nTWK\nj and TWV\nj are the key and value parameters.\nAn auxiliary MHA block is employed as the last\nlayer of the MiniLM for better alignment follow-\ning Wang et al. (2021a). The MiniLM can be then\nfinetuned on any tasks.\nMixture of Minimal Experts Naturally, in or-\nder to enlarge the learning capacity gap of the stu-\ndent, we should add more parameters to the student.\nHowever, trivially adding parameters usually leads\nto a loss of inference compute efficiency.\nTo remedy this, a mixture of minimal experts\nis proposed as in Figure 3. Following prior litera-\nture (Shazeer et al., 2017, 2018), if we consider a\nFFN block in a MiniLM as a minimal expert, then\nextra parameters are exactly imposed as minimal\nexperts to be added to the FFN block. The FFN\n4538\nblock is enabled as a mixture of mminimal experts\nFFNMoE in an expert gating tactic as:\nFFNMoE(xi) =pk(xi) ·FFNk(xi),\npk(xi) = exp(xiwG\nk)∑m\nj=1 exp(xiwG\nj ),\nk= argmax p(xi),\nwhere the j-th gate is parameterized by wG\nj ∈Rd,\nand correspondingly the j-th minimal expert is\ndenoted as FFNj. We further follow Fedus et al.\n(2021) to only allow top-one gating (i.e., only the\nexpert with highest gating probability is reserved)\nbecause we want to keep the inference compute un-\ntouched. There are also diverse designs to achieve\nthe sparse routing, such as hashing (Roller et al.,\n2021) which we find performs worse (cf. Figure 5).\nSince only one minimal expert is activated dur-\ning the inference, the compute is only negligibly\nincreased by expert routing. As a complement, we\ncan also achieve, if necessary, a mixture of experts\nin an MHA block similarly.\nTo encourage a balanced load across minimal\nexperts, a differentiable load balancing objective\nB(S; D) is added from Lepikhin et al. (2021) as:\nB(S; D) =α·m\nm∑\nj=1\nfj ·Pj,\nfj = Exi∼D[I{argmax p(xi),j}],\nPj = Exi∼D[pj(xi)],\nwhere αis a coefficient that should be manually\ntune and is kept as 0.01 throughout this work fol-\nlowing (Fedus et al., 2021). While fj depicts the\nfraction of tokens dispatched to the j-th minimal\nexpert, Pj describes the fraction of the routing prob-\nability to the j-th minimal expert. And a multiplier\nmis used to make the magnitude of the objective\ninvariant to the number of minimal experts. The\nload balancing objective basically desires a uni-\nform routing so that the loss can be minimized.\nThe objective is added to the MiniLM not only at\ntask-agnostic distillation stage but also at finetun-\ning stage for practical concerns (cf. Figure 5).\n4 Experiments\n4.1 Data and Metrics\nWe conduct experiments on GLUE (Wang et al.,\n2019) and CoNLL (Sang and Meulder, 2003). The\nGLUE originally consists of two sequence clas-\nsification tasks, SST-2 (Socher et al., 2013), i.e.,\nCoLA (Warstadt et al., 2019), and seven sequence-\npair classification tasks, i.e., MRPC (Dolan and\nBrockett, 2005), STS-B (Cer et al., 2017), QQP,\nMNLI (Williams et al., 2018), QNLI (Rajpurkar\net al., 2016), RTE (Bentivogli et al., 2011),\nWNLI (Levesque et al., 2012). We exclude WNLI\nand CoLA due to the evaluation inconsistency (in\nother words, MiniLMs get dramatically worse re-\nsults while LMs get much better ones as found\nout in Xia et al., 2022) and use the left tasks.\nThe CoNLL is a token classification task. Follow-\ning BERT (Devlin et al., 2019), we report Accu-\nracy (Acc) on SST-2, MNLI, QNLI, RTE, Spear-\nman Correlation scores (SpCorr) on STS-B, and\nF1 on MRPC, QQP, CoNLL. Average score over\ntasks from GLUE (GLUE Score) is additionally\ncomputed. Results on development sets are re-\nported. GFLOPs are also attached as theoreti-\ncal speedup references. We adopt Wikipedia data\nfor task-agnostic disitllation. The detailed statis-\ntics, maximum sequence lengths, and metrics of\nGLUE, CoNLL, and Wikipedia are supplied in Ap-\npendix A.\n4.2 Hands-on Details\nExperiments are conducted upon distilling\nBERTbase and BERTlarge (Devlin et al., 2019).\nThe distillation carried out on eight Nvidia A100s.\nThe number of relation heads is set to 32. After the\ndistillation, finetuning is carried out on one Nvidia\nA100. The number of minimal experts mis default\nto 4 otherwise specified. Other details are supplied\nin Appendix B. All experiments are task-agnostic\nones, except those in Table 13.\n4.3 Baselines\nWe compare MINI MOE with several state-of-the-\nart baselines.\nConventional Distillation FT indicates direct\nfinetuning the student. KD (Hinton et al., 2015),\nPKD (Sun et al., 2019), and CKD (Park et al.,\n2021b) are methods with different distillation ob-\njectives, i.e., KD directly distills logits, PKD dis-\ntills both logits and hidden states, and CKD distills\nhigh-order relations. While above four methods\noriginally initialize student structures by dropping\nlayers, we enable them with a global pruning so\nthat they can adapt to students of small scales. Dyn-\naBERT (Hou et al., 2020) uses a two-step pruning\n4539\nTable 3: The results of comparison between distilling BERTbase and BERTlarge.\nMethod Teacher SST-2\nAcc\nMRPC\nF1\nSTS-B\nSpCorr\nQQP\nF1\nMNLI-m/mm\nAcc\nQNLI\nAcc\nRTE\nAcc\nGLUE\nScore\nCoNLL\nF1\nMiniLM6L;384H\nBERTbase 91.1 90.1 88.1 86.7 81.5/81.8 89.2 67.9 84.5 93.2\nBERTlarge⇑ 90.9 90.6 89.0 86.9 81.8/82.4 88.8 70.0 85.1 93.2\nw/ TA BERTbase 91.3 90.3 88.2 86.8 81.4/81.6 89.7 66.8 84.5 93.2\nBERTlarge⇑ 91.4 89.8 88.5 87.0 81.9/81.6 89.5 71.5 85.2 93.2\nBERTbase 91.3 90.2 88.6 86.5 81.6/81.5 89.5 68.6 84.7 93.3MINIMOE6L;384H BERTlarge⇑1 90.5 90.0 88.8 86.8 81.8/82.2 90.8 70.4 85.2 93.3\nMiniLM4L;384H\nBERTbase 90.0 88.6 87.2 86.1 80.0/80.3 87.9 67.2 83.4 91.5\nBERTlarge⇓ 89.3 87.5 88.1 85.9 79.9/80.2 87.6 67.2 83.2 91.2\nw/ TA BERTbase 90.0 88.5 87.3 86.3 80.1/80.7 88.0 66.4 83.4 91.8\nBERTlarge⇑ 90.6 88.7 88.1 86.3 80.5/80.7 87.9 69.0 84.0 92.2\nBERTbase 90.8 88.1 88.2 85.9 79.8/80.4 88.6 69.3 83.9 92.3MINIMOE4L;384H BERTlarge⇑ 90.5 88.0 88.7 86.7 80.9/80.9 89.2 69.0 84.2 92.4\nMiniLM3L;384H\nBERTbase 89.1 89.1 86.6 85.4 77.8/78.4 87.2 66.1 82.5 90.1\nBERTlarge⇓ 89.1 86.1 87.1 85.1 78.6/78.5 86.0 65.7 82.0 87.3\nw/ TA BERTbase 89.8 87.8 86.0 85.5 77.6/78.5 86.8 66.1 82.3 90.4\nBERTlarge⇓ 89.7 84.9 87.2 85.2 78.5/79.1 86.6 66.4 82.2 90.2\nBERTbase 89.3 87.4 87.8 85.6 78.2/78.7 87.2 67.0 82.6 90.7MINIMOE3L;384H BERTlarge⇑ 89.1 88.4 87.6 86.2 78.8/79.5 87.5 67.9 83.1 91.6\n1 ⇑is used to indicate the deficiency is tackled on both GLUE and CoNLL, otherwise⇓is used.\nto regulate student structures and a distillation ob-\njective akin to PKD. MoEBERT (Zuo et al., 2022)\nmoefies LMs by decomposing FFN blocks to MoE\nlayers. For these task-specific distillation methods,\nstudent structures are denoted either with *L for pre-\nserved number of layers in layer-dropping or with\n*% for preserved portion of parameters in pruning.\nAs aforementioned methods are task-specific dis-\ntillation ones, we then introduce task-agnostic ones.\nTinyBERT (Jiao et al., 2020) exploits a distillation\nobjective distilled with a combination of various\nfeature alignments. MiniLM (Wang et al., 2021b)\nstraightforwardly utilizes a distillation objective\nwith a deep relation alignment exactly the same\nwith ours. Since task-agnostic distillation allows\nboth dropping layers and hidden dimensions, stu-\ndent structures are denoted with *L;*H accordingly.\nCapacity-aware Distillation MiniLM w/\nTA (Wang et al., 2020) specifically incorporates\na teacher assistant to MiniLM. MiniDisc (Zhang\net al., 2022a) argues that the scale of the teacher\nassistant is crucial for student performance and\nproposes an automatic teacher assistant scheduler\nbased on properties of pruning. While MiniLM w/\nTA is only inspected under a task-agnostic setting,\nMiniDisc offers results under both task-specific\nand task-agnostic settings. Nevertheless, only\ntask-specific MiniDisc is selected since pruned\nMiniLMs can be unfair to compare with. There\nis scarce work in this direction in which we find\nthese two are the most comparable ones.\n4.4 Main Results\nFrom results in Table 3, we find that MINI MOE\nlifts the curse of capacity gap at all concerned times\nof compression. For example, MINI MOE3L;384H\ndisitlled from BERTlarge has an absolute 0.5 per-\nformance gain over that distilled from BERTbase\non GLUE, and the value on CoNLL is 0.9. On\nanother note, MiniLM is free of the curse only\nat small times of compression, and MiniLM w/\nTA can somewhat saves MiniLM from the curse\nat intermediate times of compression. For exam-\nple, both MiniLM 3L;384H and MiniLM3L;384H w/\nTA fail to improve the performance via replacing\nBERTbase with BERTlarge. Results on larger LMs\nlike BERTxlarge are supplied in Appendix F for\nscalability check.\nFrom results in Table 4, we also observe that\nMINI MOE generally outperforms both conven-\ntional and capacity-aware baselines and achieves\nnew state-of-the-art performance at all concerned\ntimes of compression. For example, MINI -\nMOE4L;192H has an absolute 0.8 performance im-\nprovement over MiniLM 4L;192H on GLUE. And\nthe reason why MINI MOE3L;384H slightly under-\n4540\nTable 4: The results of comparison between MINI MOE and baselines upon distilling BERTbase. The best results are\nboldfaced.\nMethod GFLOPs SST-2\nAcc\nMRPC\nF1\nSTS-B\nSpCorr\nQQP\nF1\nMNLI-m/mm\nAcc\nQNLI\nAcc\nRTE\nAcc\nGLUE\nScore\nCoNLL\nF1\nBERTbase 10.9 93.8 91.5 87.1 88.4 84.9/84.9 91.9 71.5 86.7 94.8\nKD15% 1.64 89.9 88.6 85.1 86.2 79.8/80.2 85.6 63.9 82.4 92.8\nPKD15% 1.64 90.0 88.2 85.5 86.4 80.4/79.6 85.9 63.9 82.5 92.9\nMoEBERT17%1 1.86 89.6 88.4 85.1 86.8 80.4/80.5 86.6 65.0 82.8 92.7\nDynaBERT15%2 1.64 89.1 85.1 84.7 84.3 78.3/79.0 86.6 61.4 81.1 -\nMiniDisc15%3 1.64 89.8 88.2 85.8 86.6 80.3/79.9 87.3 68.2 83.3 93.0\nMiniLM6L;384H 1.36 91.1 90.1 88.1 86.7 81.5/ 81.8 89.2 67.9 84.5 93.2\nw/ TA 1.36 91.3 90.3 88.2 86.8 81.4/81.6 89.7 66.8 84.5 93.2\nMINIMOE6L;384H 1.36\n6∼8×\n91.3 90.2 88.6 86.5 81.6/81.5 89.5 68.6 84.7 93.3\nKD10% 1.08 88.2 87.6 84.0 84.4 77.6/77.4 84.3 67.2 81.3 91.2\nMiniDisc10% 1.08 89.1 88.4 85.4 84.9 78.2/78.6 86.3 68.2 82.4 91.9\nMiniLM4L;384H 0.91 90.0 88.6 87.2 86.1 80.0/80.3 87.9 67.2 83.4 91.5\nw/ TA 0.91 90.0 88.5 87.3 86.3 80.1 /80.7 88.0 66.4 83.4 91.8\nMINIMOE4L;384H 0.91\n10∼12×\n90.8 88.1 88.2 85.9 79.8/80.4 88.6 69.3 83.9 92.3\nKD5% 0.54 85.6 84.0 83.8 82.5 72.6/73.2 81.6 63.2 78.3 83.1\nMiniDisc5% 0.54 86.9 87.6 84.8 83.5 72.7/74.5 84.0 66.8 80.1 85.6\nTinyBERT4L;312H4 0.60 88.5 87.9 86.6 85.6 78.9/79.2 87.3 67.2 82.7 -\nMiniLM3L;384H 0.68 89.1 89.1 86.6 85.4 77.8/78.4 87.2 66.1 82.5 90.1\nw/ TA 0.68 89.8 87.8 86.0 85.5 77.6/78.5 86.8 66.1 82.3 90.4\nMINIMOE3L;384H 0.68\n16∼20×\n89.3 87.4 87.8 85.6 78.2/78.7 87.2 67.0 82.6 90.7\nKD3% 0.32 85.2 83.6 81.9 82.1 71.9/72.7 81.9 57.4 77.1 74.3\nMiniDisc3% 0.32 85.9 85.7 83.6 83.1 72.9/73.6 81.9 58.1 78.1 80.5\nMiniLM4L;192H 0.23 86.9 86.4 85.4 84.3 77.5/77.5 85.9 65.3 81.2 90.0\nw/ TA 0.23 87.2 85.6 86.2 84.6 77.3/ 78.0 86.6 64.6 81.3 89.9\nMINIMOE4L;192H 0.23\n34∼47×\n88.1 86.1 86.2 84.8 77.7/77.8 86.6 68.6 82.0 91.3\n1 Each FFN is split to 8 experts and each MHA to 4 to reach the sparsity.\n2 The results are produced from the released code.\n3 The results are mainly taken from the original papers.\n4 The results are produced without additional task-specific distillation.\nperforms TinyBERT4L;312H is conjectured due to\nstructure discrepancy. Another observation is that\nthe larger times of compression, the larger the per-\nformance improvements are. For example, MIN-\nIMOE4L;384H yields an absolute 0.5 performance\nimprovement over MiniLM4L;384H in contrast to\nthat MINI MOE6L;384H only has an absolute 0.2\nperformance improvement over MiniLM 6L;384H\non GLUE. Two more notes are that, MoEBERT\nnearly reaches the compression upper bound, and\nTinyBERT is reproduced without additional task-\nspecific distillation for a fair comparison while the\nresults with additional task-specific distillation are\nsupplied in Appendix C.\n4.5 Analyses\nPractical Inference Compute Since GFLOPs\ncan only measure the theoretical inference com-\npute, we further provide throughput (i.e., tokens\nper micro second) as a practical inference compute\nmeasure. As in Table 5, 20×compression can real-\nize a significant inference compute gain in compar-\ning KD5% to BERTbase. The practical speedup is\napproximately 6.7×. Moreover, MINI MOE3L;384H\ncan retain most inference compute gain even if the\nrouting algorithm can slightly reduce the gain when\ncompared to MiniLM3L;384H. Although MINI MOE\nis seemingly memory-inefficient regarding the in-\ncreased parameter amount, we argue the potential\nof a memory-efficient MINI MOE with parameter\ndecomposition in Appendix G.\nTable 5: Practical inference compute with reference to\nBERTbase.\nMethod GFLOPs Throughput Params\nBERTbase 10.9 80.8 tokens/ms 109.5 M\nKD5% 0.54 544.7 tokens/ms 28.7 M\nMiniLM3L;384H 0.68 485.3 tokens/ms 17.2 M\nMINIMOE3L;384H 0.68 433.1 tokens/ms 28.3 M\n4541\nStudent Scale Following the behavior of Fig-\nure 2, we would like to showcase whether MINI -\nMOE can lift the curse across difference student\nscales. From Figure 4, the curse is lifted to a large\nextent by MINI MOE in comparison with MiniLM\nand MiniLM w/ TA. However,MINI MOE meets a\nbottleneck that distilling BERTlarge makes no dif-\nference from distilling BERTbase when the FLOPs\nis at an extreme value 0.04G ( ∼273×compres-\nsion from BERTbase, ∼968×compression from\nBERTlarge). We explore the extreme case by plug-\nging a TA toMINI MOE as supplied in Appendix D.\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\nGFLOPs\n78\n79\n80\n81\n82\n83\n84\n85GLUE\nBERTbase MiniMoE\nBERTlarge MiniMoE\nFigure 4: The performance of M INI MOE across differ-\nent student scales upon distilling BERTbase.\ngating hashing finetuning w/o load balance\nRouting\n81.0\n81.5\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5\n85.0GLUE\nFigure 5: The performance of different routing choices\nwith MiniMoE4L;384H upon distilling BERTbase.\nRouting Algorithm Routing algorithm is also a\ncrucial part benefiting from a nice design choice.\nWe compare our used gating with another fancy\nchoice hashing. We at the same time show the\neffect of using load balance at finetuning stage as\nwell. From the results in Figure 5, we see that\ngating outperforms hashing, and load balancing at\nboth distillation and finetuning stages is superior to\nthat at only distillation stage.\nExpert Number Regarding the expert numberm\nis a parameter of great importance for MINI MOE,\nwe here study its impact on the performance. The\nresults in Figure 6 reveal a first ascending then de-\nscending phenomenon while adding experts at a\ntime. The phenomenon suggests there is a trade-\noff when increasing the number of experts, and we\nconjecture the tradeoff accords with the famous\nbias-variance tradeoff (Hastie et al., 2001, Chapter\n7). That is, adding experts grows the parameter\nscale, thus decreasing bias yet increasing variance.\nAnother interesting notice is that smaller students\nfavor fewer experts. Based on the tradeoff con-\njecture, we hypothesize that smaller students are\nmore sensitive to variance increment, as the biases\nof smaller students can arrive at a minimum more\nquickly than those of larger ones.\n80.6\n80.8\n81.0\nMiniMoE12L; 96H\n79.5\n80.0\nMiniMoE6L; 96H\n77.5\n78.0\n78.5GLUE\n MiniMoE4L; 96H\n77.0\n77.5\nMiniMoE3L; 96H\n1,1E 1,4E 2,4E 1,6E 1,8E\n#Experts\n74\n76\nMiniMoE2L; 96H\nFigure 6: The impact of expert number on the perfor-\nmance upon distilling BERTbase, where x,yE denotes\nx experts in each MHA and y experts in each FFN. For\nexample, 1,1E is the original dense model, and 1,4E is\nthe MoE model used in Table 4.\n5 Conclusions\nIn this work, we uncover a curse of capacity gap\nin LM distillation, which is well discussed in pre-\nvious studies on vision model distillation but not\nrecognized in distilling LMs. While there are some\nstudies investigating to fill the gap, we find they\ncan hardly tackle the curse. Interestingly, existing\nsolutions in large vision language model distilla-\ntion which are stated to be able to lift the curse fail\nto achieve so for LMs. So we aim at lifting the\ncurse by proposing a well-motivated MINI MOE.\nThe MINI MOE can essentially enlarge the capac-\nity of the student but leave the inference compute\nnearly untouched. Our experimental results indi-\ncate that MINI MOE can not only lift the curse but\nalso realize new state of the arts.\nLimitations\nThe central limitation ofMINI MOE is the increased\nmemory footprint, which we could potentially ad-\ndress in the near future according to Appendix G.\n4542\nAcknowledgements\nWe thank the anonymous reviewers and chairs for\ntheir constructive suggestions. This research was\nsupported in part by Natural Science Foundation of\nBeijing (grant number: 4222036) and Huawei Tech-\nnologies (grant number: TC20201228005). Jin-\ngang Wang is funded by Beijing Nova Program\n(grant number: 20220484098).\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,\nXin Jiang, Qun Liu, Michael R. Lyu, and Irwin King.\n2021. Binarybert: Pushing the limit of BERT quanti-\nzation. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume\n1: Long Papers), Virtual Event, August 1-6, 2021 ,\npages 4334–4348.\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau,\nand Doina Precup. 2015. Conditional computa-\ntion in neural networks for faster models. arXiv,\n1511.06297.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2011. The seventh PASCAL recog-\nnizing textual entailment challenge. In Proceed-\nings of the Fourth Text Analysis Conference, TAC\n2011, Gaithersburg, Maryland, USA, November 14-\n15, 2011.\nCristian Bucila, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In\nProceedings of the Twelfth ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data\nMining, Philadelphia, PA, USA, August 20-23, 2006,\npages 535–541.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-\n2017 task 1: Semantic textual similarity multilingual\nand crosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic Eval-\nuation, SemEval@ACL 2017, Vancouver, Canada,\nAugust 3-4, 2017, pages 1–14.\nJang Hyun Cho and Bharath Hariharan. 2019. On the ef-\nficacy of knowledge distillation. In 2019 IEEE/CVF\nInternational Conference on Computer Vision, ICCV\n2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 4793–4801.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. arXiv, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv, abs/2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing, IWP@IJCNLP 2005, Jeju Island,\nKorea, October 2005, 2005.\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong,\nDmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret\nZoph, Liam Fedus, Maarten P. Bosma, Zongwei\nZhou, Tao Wang, Yu Emma Wang, Kellie Webster,\nMarie Pellat, Kevin Robinson, Kathleen S. Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V . Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2022. Glam: Efficient scaling of language mod-\nels with mixture-of-experts. In International Con-\nference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n5547–5569.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. arXiv,\n2101.03961.\n4543\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan T. Chakaravarthy, Yogish Sabhar-\nwal, and Ashish Verma. 2020. Power-bert: Accel-\nerating BERT inference via progressive word-vector\nelimination. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pages 3690–3699.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015. Learning both weights and connections for\nefficient neural networks. arXiv, 1506.02626.\nYizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui\nWang, and Yulin Wang. 2021. Dynamic neural net-\nworks: A survey. arXiv, 2102.04906.\nTrevor Hastie, Jerome H. Friedman, and Robert Tib-\nshirani. 2001. The Elements of Statistical Learning:\nData Mining, Inference, and Prediction . Springer\nSeries in Statistics.\nJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang,\nJidong Zhai, and Jie Tang. 2021. Fastmoe: A\nfast mixture-of-expert training system. arXiv,\n2103.13262.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\narXiv, 1503.02531.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic BERT\nwith adaptive width and depth. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinybert: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020, volume EMNLP 2020 ofFind-\nings of ACL, pages 4163–4174.\nGyuwan Kim and Kyunghyun Cho. 2021. Length-\nadaptive transformer: Train once with length drop,\nuse anytime with search. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021, pages 6501–6511.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nPrinciples of Knowledge Representation and Rea-\nsoning: Proceedings of the Thirteenth International\nConference, KR 2012, Rome, Italy, June 10-14, 2012.\nJianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng\nXu, Min Yang, and Yaohong Jin. 2020. BERT-EMD:\nmany-to-many layer mapping for BERT compression\nwith earth mover’s distance. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 3009–3018.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. arXiv, 1907.11692.\nDavid Lopez-Paz, Léon Bottou, Bernhard Schölkopf,\nand Vladimir Vapnik. 2016. Unifying distillation and\nprivileged information. In 4th International Confer-\nence on Learning Representations, ICLR 2016, San\nJuan, Puerto Rico, May 2-4, 2016, Conference Track\nProceedings.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distilla-\ntion via teacher assistant. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 5191–5198.\nDae Young Park, Moon-Hyun Cha, Changwook Jeong,\nDaesin Kim, and Bohyung Han. 2021a. Learning\nstudent-friendly teacher networks for knowledge dis-\ntillation. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 13292–13303.\nGeondo Park, Gyeongman Kim, and Eunho Yang.\n2021b. Distilling linguistic context for language\nmodel compression. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 7-11 November, 2021 ,\npages 364–378.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21:140:1–140:67.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Min-\njia Zhang, Reza Yazdani Aminabadi, Ammar Ah-\nmad Awan, Jeff Rasley, and Yuxiong He. 2022.\nDeepspeed-moe: Advancing mixture-of-experts in-\nference and training to power next-generation AI\nscale. In International Conference on Machine\n4544\nLearning, ICML 2022, 17-23 July 2022, Baltimore,\nMaryland, USA, volume 162 of Proceedings of Ma-\nchine Learning Research, pages 18332–18346.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nStephen Roller, Sainbayar Sukhbaatar, Arthur Szlam,\nand Jason Weston. 2021. Hash layers for large sparse\nmodels. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 17555–17566.\nErik F. Tjong Kim Sang and Fien De Meulder. 2003.\nIntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning, CoNLL 2003, Held in cooperation with\nHLT-NAACL 2003, Edmonton, Canada, May 31 -\nJune 1, 2003, pages 142–147.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv,\n1910.01108.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, Ryan Sepassi, and Blake A. Hechtman. 2018.\nMesh-tensorflow: Deep learning for supercomputers.\nIn Advances in Neural Information Processing Sys-\ntems 31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December\n3-8, 2018, Montréal, Canada, pages 10435–10444.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv, abs/1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631–1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 4322–\n4331.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020a. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 498–508.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020b. Mobilebert:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL 2020, Online, July 5-10, 2020, pages 2158–\n2170.\nWonyong Sung, Sungho Shin, and Kyuyeon Hwang.\n2015. Resiliency of deep neural networks under\nquantization. arXiv, 1511.06488.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. arXiv, 1908.08962.\nVladimir Vapnik. 1998. Statistical learning theory. Wi-\nley.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu\nDing, Weibao Gong, Shikun Feng, Junyuan Shang,\nYanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen,\nYuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai, Qiu-\nliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai\nYu, Yanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei\nZeng, Ge Li, Wen Gao, and Haifeng Wang. 2021a.\nERNIE 3.0 titan: Exploring larger-scale knowledge\nenhanced pre-training for language understanding\nand generation. arXiv, 2112.12731.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021b. Minilmv2: Multi-head self-\n4545\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Associa-\ntion for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 of Findings of ACL, pages 2140–\n2151.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions on Association for Computational Lin-\nguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 1 (Long Papers), pages 1112–1122.\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.\nStructured pruning learns compact and accurate mod-\nels. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 1513–1528.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July\n5-10, 2020, pages 2246–2251.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nYudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong\nYu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi,\nYiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang,\nWeijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian,\nYiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao,\nQipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang\nYang, Kyle Richardson, and Zhenzhong Lan. 2020.\nCLUE: A chinese language understanding evaluation\nbenchmark. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 4762–4772. International Committee on\nComputational Linguistics.\nFuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou,\nand Yang You. 2022. One student knows all experts\nknow: From sparse to dense. arXiv, 2201.10890.\nYi Yang, Chen Zhang, and Dawei Song. 2022. Sparse\nteachers can be dense with knowledge. arXiv,\nabs/2210.03923.\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding,\nXiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and\nJie Tang. 2021. Wudaocorpora: A super large-scale\nchinese corpora for pre-training language models. AI\nOpen, 2:65–68.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: quantized 8bit BERT.\nIn Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing - NeurIPS Edi-\ntion, EMC2@NeurIPS 2019, Vancouver, Canada, De-\ncember 13, 2019, pages 36–39.\nChen Zhang, Yang Yang, Qifan Wang, Jiahao Liu, Jin-\ngang Wang, Yunsen Xian, Wei Wu, and Dawei Song.\n2022a. Minidisc: Minimal distillation schedule for\nlanguage model compression. arXiv, 2205.14570.\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2022b. Moefication:\nTransformer feed-forward layers are mixtures of ex-\nperts. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 877–890.\nBorui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jia-\njun Liang. 2022. Decoupled knowledge distillation.\narXiv, 2203.08679.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian J.\nMcAuley, Ke Xu, and Furu Wei. 2020. BERT loses\npatience: Fast and robust inference with early exit.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nWangchunshu Zhou, Canwen Xu, and Julian J.\nMcAuley. 2022. BERT learns to teach: Knowledge\ndistillation with meta learning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 7037–\n7049.\nYichen Zhu and Yi Wang. 2021. Student customized\nknowledge distillation: Bridging the gap between\nstudent and teacher. In 2021 IEEE/CVF Interna-\ntional Conference on Computer Vision, ICCV 2021,\nMontreal, QC, Canada, October 10-17, 2021, pages\n5037–5046.\nSimiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He,\nTuo Zhao, and Weizhu Chen. 2022. Moebert: from\nBERT to mixture-of-experts via importance-guided\nadaptation. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 1610–1623.\nA Data Summary\nThe detailed statistics, maximum sequence lengths,\nand metrics for datasets we use are shown in Ta-\n4546\nble 6, where the Wikipedia corpus used for distilla-\ntion is also attached.\nB More Hands-on Details\nGeneral Guidelines The details of hyperparam-\neters for distillation and finetuning are shown in\nTable 7. We will be releasing our code and scripts\nin the final version for exact reproducibility. For\nall cases, students are always randomly initialized\nfollowing MiniLM.\nImplementation of MiniMoE We strictly fol-\nlow the design of SwitchTransformer (Fedus et al.,\n2021) and extend it to the design of our MINI -\nMOE. We also follow their associated appendices\nto implement an MoE for multihead attention. In\ndetail, based on the original design, we treat an\nFFN/MHA as an minimal expert, adopt top- one\ngating with load balancing, and employ a capac-\nity factor of 1.25 for a good tradeoff (where over-\nflowed tokens are dropped). For the parameter\neffect of adding an expert, we take expanding\nMiniLM4L;192H (11.3M) to MiniMoE4L;192H-1,2E\n(14.9M) as an example. The number of parame-\nters for embeddings is not changed (6.0M→6.0M),\nbut adding an expert ( 1,1E→1,2E) results in an\nincreased number of parameters for transformers\n(5.4M→9.0M).\nFurther, our design for HashLayer (Roller et al.,\n2021) also strictly follows the original random hash\ndesign, i.e., per-token hash is used. We strictly\nfollow the best configuration of DeKD as reported\nin their paper (Zhao et al., 2022), where αis 1.0\nand βis 8.0.\nC Results w/ Task-specific Distillation\nThe results with task-specific distillation are pro-\nduced from released checkpoints. The results in\nTable 8 demonstrate that TinyBERT is largely sup-\nported with data augmentation in the task-specific\ndistillation stage for great performance. Another\nintriguing observation is that data augmentation\nonly works for distillation but not for finetuning po-\ntentially due to the noise-resilience of distillation,\nso we preferably replace the finetuning stage with\na task-specific distillation stage in experimenting\nwith MiniLM.\nD M INI MOE at Extreme\nThe results in Table D witness that, MINI MOE\nsometimes struggles with extreme cases but can be\nenhanced with the help of TA.\nE Related Work\nKnowledge Distillation Distillation (Hinton\net al., 2015) is a de facto way to compression (Bu-\ncila et al., 2006) LMs by transferring the knowledge\nof LMs to small language models. During the distil-\nlation, a small language model serves as a student\nand treats a LM as a teacher to learn from. There\nare three lines of work in LM distillation: firstly,\ntask-specific distillation (Sun et al., 2019; Li et al.,\n2020; Sun et al., 2020a; Park et al., 2021b; Hou\net al., 2020; Xia et al., 2022) that conducts distilla-\ntion on a specific task at finetuning stage; secondly,\ntask-agnostic distillation (Turc et al., 2019; Sanh\net al., 2019; Sun et al., 2020b; Wang et al., 2021b)\nthat conducts distillation at pretraining stage; and\nthirdly, two-stage distillation (Jiao et al., 2020) that\ncombines the power of both task-agnostic and -\nspecific distillation. Though these methods real-\nize promising performance when distilling LMs\nlike BERTbase, they can come short of scalabil-\nity to LMs like BERT large especially when the\nstudent is of a small scale. In fact, driven by re-\ncent observations (Wang et al., 2020; Zhang et al.,\n2022a; Mirzadeh et al., 2020; Cho and Hariha-\nran, 2019), distillation with a small student can be\nfaced with two deficiencies due to the large capac-\nity gap. A few studies including teacher assistant-\nbased (Mirzadeh et al., 2020; Zhang et al., 2022a)\nand student-friendly (Park et al., 2021a; Zhou et al.,\n2022) distillation can alleviate the first but fail to\nresolve the second. It is noteworthy that some work\nstates they can tackle both deficiencies for vision\nmodels (Zhu and Wang, 2021; Zhao et al., 2022),\nbut preliminary studies have found that they are ei-\nther expensive or not capable of LMs. In our work,\nwe follow the line of task-agnostic distillation of\nLMs and aims at lifting both efficiencies for the\nfirst time.\nMixture of Experts Based on the idea of condi-\ntional computation (Bengio et al., 2015), MoE layer\nis proposed to scale-up LMs in a sparsely activated\nfashion (Shazeer et al., 2017). There are diverse\ndesigns to achieve the sparse routing, such as gat-\ning (Shazeer et al., 2018) and hashing (Roller et al.,\n2021), with necessary balance constraints (Lep-\nikhin et al., 2021). MoE layers are then joined\nto LMs in the past one or two years (Fedus et al.,\n2021; Du et al., 2022). Owing to the sparse activa-\ntion property, the scales of LMs are significantly\n4547\nTable 6: The statistics, maximum sequence lengths, and metrics.\nDataset #Train exam. #Dev exam. Max. length Metric\nSST-2 67K 0.9K 64 Accuracy\nMRPC 3.7K 0.4K 128 F1\nSTS-B 7K 1.5K 128 Spearman Correlation\nQQP 364K 40K 128 F1\nMNLI-m/mm 393K 20K 128 Accuracy\nQNLI 105K 5.5K 128 Accuracy\nRTE 2.5K 0.3K 128 Accuracy\nCoNLL 14k 3.3k 128 F1\nWikipedia 35M - 128 -\nTable 7: The hyperparameters for both distillation and finetuning. The search grids for GLUE and CoNLL are\nindicated differently.\nHyperparameter Distillation Finetuning\nBatch size 8 ×128=1024 {16,32}\nOptimizer AdamW AdamW\nLearning rate 3e-4 {1e-5,2e-5,3e-5}/{1e-4,2e-4,3e-4}\nTraining epochs 5 10\nEarlystop epochs - 5\nWarmup proportion 0.01 0.1\nWeight decay 0.01 0.01\nincreased with only minor losses in compute effi-\nciency on modern GPU devices so that the under-\nneath scaling laws can be uncovered in a compa-\nrably cheap manner (He et al., 2021; Rajbhandari\net al., 2022). In our work, we are impelled by\nthe merits of MoE, and propose a MINI MOE so\nthat the capacity of the student can be enlarged\nwithout much inference overhead increment. MIN-\nIMOE can be similar to a certain stream of meth-\nods (Zhang et al., 2022b; Zuo et al., 2022) that pur-\nsue accelerating LMs via precisely moefying them.\nNonetheless, the moefication process is exerted to\nLMs with limited inference compute improvements\ncompared to those advanced by MINI MOE. Note\nthat there are emergent work exploring compress-\ning MoE LMs (Xue et al., 2022) to dense students,\nwhich is walking down the same street in the op-\nposite side since we instead focus on compressing\ndense LMs to MoE students.\nF Results on BERT xlarge\nLM distillation, under either the task-agnostic set-\nting as in our paper or the task-specific setting, has\nseldom been investigated to distil LMs larger than\nBERTlarge. Even worse, there is only little work\nhas been investigated to distil BERTlarge under the\ntask-agnostic setting.\nIn the main results, we just follow the paces of\nthe task-agnostic setting, not only due to the huge\nscales of larger LMs like T5 and GPT3 but also\ndue to that task-agnostic LM distillation requires\nthe access to the original pretraining data of usu-\nally vast volume. What’s more, larger LMs like\nT5 can be incomparable to BERT owing to the ar-\nchitectural difference, and existing task-agnostic\nmethods including ours may easily fail.\nRegarding all the considerations mentioned\nabove, however, we try to check the existence of the\ncurse of capacity gap and examine MINI MOE un-\nder a comparably larger-scale setting, i.e., Chinese\nBERTbase v.s. BERTxlarge on some datasets from\nCLUE (Xu et al., 2020) (which can be viewed as\nthe Chinese GLUE). These datasets include a topic\nclassification dataset TNews, a similar question\nmatching dataset AFQMC, and a natural language\ninference dataset OCNLI. The preliminary results\nare shown in Table 10. As far as we know, while\nEnglish BERTxlarge with more than one billion\nparameters trained by Nvidia Megatron (Shoeybi\net al., 2019) is not publicly available, Chinese\nBERTxlarge can be easily downloaded through hug-\n4548\nTable 8: The results with and without task-specific distillation upon distilling BERTbase.\nMethod GFLOPs SST-2\nAcc\nMRPC\nF1\nSTS-B\nSpCorr\nQQP\nF1\nMNLI-m/mm\nAcc\nQNLI\nAcc\nRTE\nAcc\nGLUE\nScore\nTinyBERT4L;312H 0.60 88.5 87.9 86.6 85.6 78.9/79.2 87.3 67.2 82.7\nw/ tsd.+aug. 0.60 91.6 90.2 86.3 87.1 81.2/82.8 87.6 64.3 83.9\nMiniDisc5% 0.54 86.9 87.6 84.8 83.5 72.7/74.5 84.0 66.8 80.1\nw/ aug. 0.54 91.2 90.0 87.5 85.4 79.0/79.8 84.5 67.5 83.1\nMiniLM3L;384H 0.68 89.1 89.1 86.6 85.4 77.8/78.4 87.2 66.1 82.5\nw/ aug. 0.68 88.7 85.9 83.1 82.8 76.2/76.0 86.6 62.5 80.2\nw/ tsd.+aug.* 0.68 91.2 91.1 88.2 86.6 79.9/80.4 87.8 66.1 83.9\n* tsd. indicates task-specific distillation and aug. indicates distillation with data augmentation.\nTable 9: The results of MINI MOE at extreme upon distilling BERTbase and BERTlarge respectively.\nMethod GFLOPs SST-2\nAcc\nMRPC\nF1\nSTS-B\nSpCorr\nQQP\nF1\nMNLI-m/mm\nAcc\nQNLI\nAcc\nRTE\nAcc\nGLUE\nScore\nBERTbase 10.9 93.8 91.5 87.1 88.4 84.9/84.9 91.9 71.5 86.7\nMiniLM4L;96H 0.06 83.4 84.6 81.9 80.7 71.2/72.5 82.0 63.7 77.5\nw/ TA 0.06 84.5 83.9 82.2 80.5 70.8/72.4 81.6 63.7 77.5\nMINIMOE4L;96H 0.06 84.8 84.0 83.1 81.2 72.2/73.5 82.2 65.7 78.3\nw/ TA 0.06\n∼182×\n84.2 85.3 83.7 82.2 72.6/73.7 83.6 65.3 78.8\nMiniLM3L;96H 0.04 83.7 83.8 81.2 80.6 70.3/71.5 80.5 61.4 76.6\nw/ TA 0.04 82.6 83.3 81.2 80.3 70.3/71.9 80.7 61.4 76.5\nMINIMOE3L;96H 0.04 84.8 84.5 82.8 80.8 70.3/71.9 81.9 65.0 77.7\nw/ TA 0.04\n∼273×\n83.5 85.1 83.1 81.4 71.4/73.0 83.3 61.7 77.8\nBERTlarge 38.7 94.2 92.5 90.1 89.0 86.6/86.3 92.5 75.5 88.3\nMiniLM4L;96H 0.06 83.3 83.9 82.5 81.0 71.4/72.4 81.8 63.2 77.4\nw/ TA 0.06 84.1 85.8 82.4 81.3 71.9/73.4 82.3 64.3 78.2\nMINIMOE4L;96H 0.06 84.9 85.4 82.9 81.6 74.0/74.8 83.6 64.6 79.0\nw/ TA 0.06\n∼645×\n84.2 85.3 83.2 81.2 72.5/74.0 83.4 66.1 78.7\nMiniLM3L;96H 0.04 83.1 84.1 81.8 79.7 69.7/70.8 79.2 63.2 76.5\nw/ TA 0.04 83.0 83.2 81.2 80.3 69.3/70.7 81.8 60.7 76.3\nMINIMOE3L;96H 0.04 83.0 84.5 82.7 81.1 71.7/72.8 82.1 63.9 77.7\nw/ TA 0.04\n∼968×\n83.8 84.4 83.0 81.2 71.8/72.8 82.4 63.9 77.9\ngingface.2 It is noteworthy that Chinese BERTbase\nis trained on Chinese Wikipedia ( ∼15G) while\nChinese BERTxlarge is trained on Wudao Corpus\n(∼300G) (Yuan et al., 2021). We use Wikipedia\ndata as the default choice for distillation, but Wu-\ndao data seems to be a more suitable (though not\nthat fair) one for distilling Chinese BERTxlarge as\nwe have found that Wikipedia could not make the\ndistillation converge properly. Painfully, it con-\nsumes around one week to achieve one epoch of\ndistilling Chinese BERTxlarge using Wudao in con-\ntrast to five epochs of distilling Chinese BERTbase\non Wikipedia in one day. The results show that Chi-\nnese BERTxlarge is cursed to realize better students\n2https://huggingface.co/IDEA-CCNL/\nErlangshen-MegatronBert-1.3B.\nthan Chinese BERTbase does, and MINI MOE has\nthe potential to lift the curse under the larger-scale\nsetting.\nG Potential of Memory-efficient\nMINI MOE\nOne may argue that MINI MOE introduces much\nmore memory consumption than MiniLM does,\nlargely limiting the application scenarios for\nmemory-sensitive devices (e.g., mobile devices).\nHowever, there is no free lunch to enlarge the\ncapacity of the student. We should claim that,\nin order to increase the capacity, memory/space\nconsumption is a cheaper choice (e.g., more ex-\nperts) than latency/time consumption (e.g., more\noperations), and this is potentially the reason why\n4549\nTable 10: The results of comparison between distilling Chinese BERTbase and BERTxlarge.\nMethod Teacher TNews\nAcc\nAFQMC\nAcc\nOCNLI\nAcc\nCLUE\nScore\nTeacher BERTbase 57.0 74.8 75.4 69.1\nBERTxlarge⇑ 60.0 76.1 79.2 71.7\nMiniLM6L;384H\nBERTbase 55.5 72.0 71.0 66.2\nBERTxlarge⇓ 54.9 70.7 69.9 65.2\nBERTbase 55.9 72.9 70.8 66.5MINIMOE6L;384H BERTxlarge⇑1 56.7 72.4 71.0 66.7\n1 ⇑is used to indicate the deficiency is tackled on CLUE, otherwise⇓is used.\nlarge LMs like PaLM (Chowdhery et al., 2022) and\nFLAN (Chung et al., 2022) could become so pop-\nular. We should also highlight that scenarios that\nrequire rather limited memory consumption (e.g.,\nmobile scenarios) is currently not (though can be\nin the near future) the main concern of LMs. In\ncontrast, LMs are usually served in GPU scenarios,\nwhere memory/space is easy to access.\nLuckily, we find a potential path to address the\nmemory efficiency concern based on the idea of\nparameter decomposition (e.g., SVD). While em-\nbedding parameter decomposition is a general way\nto reduce the number of parameters for embed-\ndings and could not make MINI MOE as memory-\nefficient as MiniLM. We uncover that, without\nmuch performance sacrifice, transformer parameter\ndecomposition in MINI MOE can be easier in com-\nparison with that in MiniLM owing to the sparse\nactivation property of MoE. That is, transformer pa-\nrameters in MINI MOE have lower ranks than those\nin MiniLM, and this can be shown by analyzing\nthe magnitudes of the normalized singular values\nusing SVD. The preliminary results of the output\nmatrices of the last FFN layers separately from\nMiniLM3L;384H and MINI MOE3L;384H are shown\nin Table 11.\nWith this finding,MINI MOE can compress more\nparameters than MiniLM does using parameter de-\ncomposition and finally yield a similar memory\nefficiency to that of MiniLM.\nWe also explore another complementary solu-\ntion that views MINI MOE as teacher assistant and\nfurther distils from MINI MOE to its dense counter-\npart. The results are shown in Table 12, implying\nthat this only results in an acceptable performance\ndegradation on large datasets like MNLI and SST-2\nbut undesired performance degradation on small\ndatasets like RTE.\nH Failure of Vision Method\nWe examine in a preliminary study the effective-\nness of one of the vision model distillation meth-\nods (DeKD, Zhao et al., 2022) which can lift the\ncurse of capacity gap. From the results in Table 13,\nwe unfortunately discover that DeKD can only give\ncomparable performance in distilling BERT base,\nwhich even lags behind KD w/ TA. It hints that vi-\nsion model distillation methods are not that capable\nof LMs.\n4550\nTable 11: The SVD analysis to show the potential of memory-efficient MINI MOE.\nMethod %Value >0.2 %Value >0.1 %Value >0.05 Trm Params (Value >0.1)\nMiniLM3L;384Hdense 315/384=82% 356/384=93% 373/384=97% 5.3M →5.1M\nMiniMoE3L;384Hexpert #1 6/384=2% 82/384=21% 275/384=72% -\nMiniMoE3L;384Hexpert #2 34/384=9% 220/384=57% 361/384=94% -\nMiniMoE3L;384Hexpert #3 15/384=4% 175/384=46% 338/384=88% -\nMiniMoE3L;384Hexpert #4 24/384=6% 200/384=52% 357/384=93% -\nMiniMoE3L;384Hall experts 79/384/4=5% 677/384/4=44% 1331/384/4=87% 16.4M →8.2M\nTable 12: The results of further distilling from MINI MOE to its dense counterpart.\nMethod MNLI-m/mm SST-2 RTE\nMiniLM3L;384H 77.8/78.4 89.1 66.1\nMINIMOE3L;384H 78.2/78.7 89.3 67.0\nMINIMOE3L;384H⇒MiniLM3L;384H 78.1/78.4 89.5 64.3\nTable 13: The results of applying vision distillation methods upon BERTbase.\nMethod GLUE Method GLUE\nKD2L 72.9 KD 4L 81.8\nw/ TA 73.4 w/ TA 82.1\nDeKD2L 72.7 DeKD 4L 81.6\n4551\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nthe section following the conclusions.\n□\u0017 A2. Did you discuss any potential risks of your work?\nnot any known risks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nthe introduction.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nthe experiments.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nthe results in the experiments.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4552\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nthe hands-on details in the experiments.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nthe results in the experiments.\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nnot related packages used.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n4553",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.7164586782455444
    },
    {
      "name": "Zhàng",
      "score": 0.7068042755126953
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.5248140096664429
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.4640691876411438
    },
    {
      "name": "Curse",
      "score": 0.462091863155365
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.44914770126342773
    },
    {
      "name": "Mathematical economics",
      "score": 0.44590604305267334
    },
    {
      "name": "Computer science",
      "score": 0.4254702627658844
    },
    {
      "name": "Econometrics",
      "score": 0.3246183395385742
    },
    {
      "name": "Economics",
      "score": 0.3154844641685486
    },
    {
      "name": "Philosophy",
      "score": 0.26290208101272583
    },
    {
      "name": "Mathematics",
      "score": 0.2611812949180603
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2506520748138428
    },
    {
      "name": "Theology",
      "score": 0.22175729274749756
    },
    {
      "name": "Political science",
      "score": 0.12998545169830322
    },
    {
      "name": "Physics",
      "score": 0.1071636974811554
    },
    {
      "name": "Pure mathematics",
      "score": 0.08219575881958008
    },
    {
      "name": "Thermodynamics",
      "score": 0.07972630858421326
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}