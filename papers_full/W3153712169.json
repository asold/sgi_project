{
  "title": "Globalizing BERT-based Transformer Architectures for Long Document Summarization",
  "url": "https://openalex.org/W3153712169",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2565038797",
      "name": "Quentin Grail",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250791005",
      "name": "Julien Perez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2342961570",
      "name": "Eric Gaussier",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2606974598",
    "https://openalex.org/W1989420837",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W2964028111",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W3003186568",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2907644564",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963607157",
    "https://openalex.org/W2888556271",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2987751813",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3105055324",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2140189755",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3025151218",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2896807716",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2944833000",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2962716111",
    "https://openalex.org/W2997759614",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2101390659"
  ],
  "abstract": "Fine-tuning a large language model on downstream tasks has become a commonly adopted process in the Natural Language Processing (NLP) (CITATION). However, such a process, when associated with the current transformer-based (CITATION) architectures, shows several limitations when the target task requires to reason with long documents. In this work, we introduce a novel hierarchical propagation layer that spreads information between multiple transformer windows. We adopt a hierarchical approach where the input is divided in multiple blocks independently processed by the scaled dot-attentions and combined between the successive layers. We validate the effectiveness of our approach on three extractive summarization corpora of long scientific papers and news articles. We compare our approach to standard and pre-trained language-model-based summarizers and report state-of-the-art results for long document summarization and comparable results for smaller document summarization.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1792–1810\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1792\nGlobalizing BERT-based Transformer Architectures for Long Document\nSummarization\nQuentin Grail\nNA VER LABS Europe,\nMeylan, France\nUniv. Grenoble Alpes, CNRS, LIG,\nGrenoble, France\nquentin.grail@naverlabs.com\nJulien Perez\nNA VER LABS Europe,\nMeylan, France\njulien.perez@naverlabs.com\nEric Gaussier\nUniv. Grenoble Alpes, CNRS, LIG,\nGrenoble, France\neric.gaussier@imag.fr\nAbstract\nFine-tuning a large language model on down-\nstream tasks has become a commonly adopted\nprocess in the Natural Language Processing\n(NLP) (Wang et al., 2019). However, such\na process, when associated with the current\ntransformer-based (Vaswani et al., 2017) archi-\ntectures, shows several limitations when the\ntarget task requires to reason with long doc-\numents. In this work, we introduce a novel\nhierarchical propagation layer that spreads in-\nformation between multiple transformer win-\ndows. We adopt a hierarchical approach where\nthe input is divided in multiple blocks indepen-\ndently processed by the scaled dot-attentions\nand combined between the successive lay-\ners. We validate the effectiveness of our ap-\nproach on three extractive summarization cor-\npora of long scientiﬁc papers and news articles.\nWe compare our approach to standard and\npre-trained language-model-based summariz-\ners and report state-of-the-art results for long\ndocument summarization and comparable re-\nsults for smaller document summarization.\n1 Introduction\nLanguage model pre-training has become a key\ncomponent to improve performances on a major-\nity of Natural Language Processing (NLP) tasks\n(Wang et al., 2019). Most of the recent competi-\ntive architectures (Devlin et al., 2019; Lan et al.,\n2020; Liu et al., 2019b; Radford et al., 2018) are\nbased on the efﬁcient transformer layer introduced\nin Vaswani et al. (2017). BERT (Devlin et al.,\n2019) is one of these architectures that has been\nwidely adopted for comprehension and generation\ntasks. It is a multi-layer transformer network, pre-\ntrained with different self-supervised objectives.\nNumerous variations of transformer architectures\nhave been proposed to improve this approach (Lan\net al., 2020; Liu et al., 2019b; Radford et al., 2018).\nHowever, this type of process is only evaluated\non tasks composed of relatively short input text,\nGLUE (Wang et al., 2019), SQUAD (Rajpurkar\net al., 2016), SW AG (Zellers et al., 2018). Indeed,\nfor the tasks that require reasoning with longer doc-\numents, this approach exhibits several limitations.\nThe transformer self-attention memory quadrati-\ncally increases with the number of input tokens,\nmaking it technically impossible to compute on\ndocument-scale sequences. In addition, they usu-\nally require to deﬁne a ﬁxed maximum input length,\ntypically of 512 tokens, at the pre-training stage.\nOne solution is to pre-train the entire model on\nlonger sequences. However, this will still require a\nmassive computation power and will only push the\nlength limitation further. Other alternatives have\nbeen proposed to extend multi-layer transformers\narchitectures to longer sequences without modify-\ning this maximum length limitation. The ﬁrst one\nis to limit the input sequence to its ﬁrst tokens by\nremoving the text beyond the length limit. Obvi-\nously, it cannot be a reasonable solution to treat\nlong documents that are consistently longer than\nthis limit. The second alternative is to apply the\nmodel on a window that slides all over the docu-\nment. It has been used in Wolf et al. (2019) to deal\nwith SQUAD documents that are longer than the\n512 token limitation and in Joshi et al. (2019) for\na co-reference resolution task on long documents.\nThis approach can only work if the tokens need\nto be contextualized only in their surroundings be-\ncause there is no interaction between the different\nwindows. It seems to be a solution for co-reference\nresolution (Joshi et al., 2019) as they usually can be\nsolved with a reasonably sized window. Another\n1793\napproach adopted to deal with long documents or\nmulti-document is to select a sub-sample of the in-\nput that is small enough for the transformer model.\nMost of the state-of-the-art pipelines on the multi-\nhop question answering dataset HotpotQA (Yang\net al., 2018) use a ﬁrst model to retrieve the relevant\npieces of text before feeding them to a transformer-\nbased architecture (Fang et al., 2019a; Tu et al.,\n2019).\nWe argue that these solutions are not feasible to\ndeal with tasks that require a global understand-\ning of long documents. An example is extractive\nsummarization, where the decision for each sen-\ntence should be based on the information of the\ncomplete document. To address these challenges,\nwe propose a simple adaptation of the multi-layer\ntransformer architecture that can scale to long doc-\numents and beneﬁt from pre-trained parameters\nwith a relatively small length limitation. The gen-\neral idea is to independently apply a transformer\nnetwork on small blocks of a text, instead of a\nlong sequence, and to share information among the\nblocks between two successive layers. To the best\nof our knowledge, this is the ﬁrst attempt to intro-\nduce hierarchical components directly between the\nlayers of a pre-trained model and not only on top of\nit (Fang et al., 2019b; Zhang et al., 2019b; Tu et al.,\n2020). Between each of the transformer layers, we\nuse a Bidirectional Gated Recurrent Unit (BiGRU)\nnetwork (Cho et al., 2014) to spread global informa-\ntion across the blocks. Adding these propagation\nlayers between the transformer layers preserves\nthe original structure of the pre-trained model and\nmakes it possible to transfer parameter weights\nfrom a large pre-trained language model with only\nfew additional parameters to propagate information\nbetween blocks.\nThe contributions of this paper can be summa-\nrized as follows: (i) we propose a novel architecture\ndedicated to long documents which interweaves re-\ncurrent hierarchical modules with transformer lay-\ners and which exploits pre-trained language models\nlike BERT, and (ii) we demonstrate that this archi-\ntecture is able to build informative representations\nin the context of extractive summarization.\n2 Global BERT-based Transformer\nArchitecture\nIn this part, we brieﬂy recall the transformer layer\nfrom Vaswani et al. (2017) and its integration in\nthe BERT model (Devlin et al., 2019). Then we\ndescribe our modiﬁcations of this architecture that\nallow the model to read longer documents.\nTransformers: The transformer architecture,\nbased on a sequence of transformer layers, has been\ninitially introduced in Vaswani et al. (2017). The\nkey idea of this layer is to produce a contextualized\nrepresentation of an input sequence of tokens. It is\ncomposed of the succession of a multi-head self-\nattention, a ﬁrst normalizer, a feed-forward neural\nnetwork, and a second normalizer. This model,\nwhich has originally been introduced for machine\ntranslation, has then been adopted for most natural\nlanguage comprehension tasks. Most of the suc-\ncessful approaches (Devlin et al., 2019; Liu et al.,\n2019b; Lan et al., 2020) are composed of multiple\nstacked transformer layers. In the remainder, we de-\nnote by Tℓ the transformation corresponding to the\nℓth, 1 ≤ℓ≤L, transformer layer (Tℓ is a function\nfrom RN×h to RN×h, where N denotes the length\nof the sequence and hthe hidden dimension).\nBERT (Devlin et al., 2019) is a multi-layer\ntransformer encoder pre-trained on large text cor-\npora. Two BERT architectures have been proposed\nin Devlin et al. (2019): BERTBASE composed of\n12 stacked transformer layers with hidden dimen-\nsion of 768 (L = 12,h = 768) and BERTLARGE\ncomposed of 24 layers of hidden dimension 1024\n(L = 24,h = 1024). For both architectures, the\ninput length is limited to 512 WordPiece tokens\nand the pre-training includes two self-supervised\ntasks, namely masked language modeling and next\nsentence prediction. For masked language model-\ning, 15% of all the WordPiece tokens of the input\nsequence are masked or corrupted, and the model\nis used to predict the original token with a cross-\nentropy loss. For next sentence prediction, the\nmodel is trained as a classiﬁer to predict if two\nsentences are contiguous or not. The pre-training\nprocedure uses the BooksCorpus (Zhu et al., 2015)\nand documents from English Wikipedia. It re-\nquires 4 days of optimization on 16 TPU chips\nfor BERTBASE and 64 TPU chips for BERTLARGE.\n2.1 Stacked Propagation Layers\nWe propose a hierarchical structure that uses pre-\ntrained transformers to encode local text blocks that\nwill be used to compute document level representa-\ntions. The novel contribution of this work, depicted\nin Figure 1, is to incorporate recurrent hierarchical\nmodules between the different transformer layers\nand not only on top of the model, as proposed in\n1794\nBlock 1 Block 2 Block K\nCLS x1,1 x1,n1\nEmbedding Layer\n×L\nEmbedding Layer Embedding Layer\nPropagation Layer\nx2,1 x2,n2 xK,1 xK,nKSEP SEP CLS CLS SEP\nTransformer Layer\n Transformer Layer Transformer Layer\nBiGRU\nFFNN FFNNFFNN\nOutput Layer\nFigure 1: Our proposed modiﬁcation of a multi-layer transformer architecture. The input sequence is composed\nof K blocks of tokens. Each transformer layer is applied within the blocks, and a bidirectional GRU network\npropagates information in the whole document by updating the [CLS] representation of each block.\nseveral recent works (Fang et al., 2019b; Zhang\net al., 2019b; Tu et al., 2020). Because we con-\nstruct and propagate document level information\nbetween the layers, global and local information\nare fused at every level of the architecture. The text\nblocks can be sentences, paragraphs, or sections.\nWe experiment using sentences as blocks because\nit generally does not exceed the maximum length\nallowed by pre-trained models and because BERT\nhas demonstrated to be well adapted to represent\nsuch sequences.\nWe start by splitting the original sequence into\nmultiple blocks. Let Dbe a document composed of\nKblocks, D= {B1; B2; ··· ; BK}where a block\nBk, 1 ≤k≤K, is composed of nk tokens. To fol-\nlow the convention of BERT, special tokens[CLS]\nand [SEP] are respectively added at the beginning\nand end of each block of the document, so that:\nBk = {[CLS]; xk,1; xk,2; ···xk,nk; [SEP]}where\nxk,i is the index of the WordPiece token iof block\nk. In the remainder, the index 0 (resp. nk + 1) will\nbe used to refer to the representation of the [CLS]\n(resp. [SEP]) token in each block.\nEmbedding LayerBecause our goal is to reuse\nthe available pre-trained BERT parameters, token\nrepresentations are kept the same as in the original\nBERT and are composed of a token embedding, a\nsegment embedding, and a positional encoding that\nrepresents the position of the token in its block. We\nwill denote by Ek (Ek ∈R(nk+2)×h, 1 ≤k≤K)\nthe embedding representation of block k.\nPropagation Layers Our model is composed\nof L stacked identical hierarchical layers, called\npropagation layers, that comprise a transformer\nlayer, a BiGRU to propagate information across\nblocks and, ﬁnally, a feed-forward network. For\nany layer ℓ, 1 ≤ℓ ≤L, let Uℓ\nk ∈R(nk+2)×h be\nthe representation of block k after the (ℓ−1)th\nlayer, the representation for the ﬁrst layer being\ninitialized with the output of the embedding layer:\nU1\nk = Ek, ∀k∈{1,··· ,K}. We ﬁrst apply the\npre-trained transformer function Tℓ individually\non each block of the document to compute local,\ntoken-aware representations Vℓ\nk ∈R(nk+2)×h:\nVℓ\nk = Tℓ(Uℓ\nk), ∀k∈{1,··· ,K}.\nThe next step is to propagate information across\nall the blocks of the document in order to compute a\nglobal block-aware representation for the document\nat layer ℓ, denoted by Wℓ ∈RK×h, 1 ≤k ≤K.\nTo do so, we use a BiGRU network, fed with the\nrepresentation vectors of the different blocks, and\napply a feed-forward neural network to preserve the\nhidden dimension of the transformer. Each block\nkis represented by its [CLS] vector, i.e., the vector\n(represented by Vℓ\nk,0 ∈Rh) at the ﬁrst position in\nthe local representation of the block. These repre-\nsentations are then concatenated to form the input\nto the BiGRU. The global, block-aware representa-\ntion is then computed by applying the feed-forward\nneural network (FFNN) to all K outputs of the\nBiGRU:\nWℓ\nk = FFNN(BiGRUk([Vℓ\n1,0; ··· ; Vℓ\nK,0])),\n1795\navg. doc length avg. summary length\nDatasets sentences words sentences words\narXiv 204 5038 5.6 165\nPubMed 88 3235 6.8 205\nCNN/DM 32 757 4.1 57\nTable 1: Statistics on arXiv, PubMed and\nCNN/DailyMail validation datasets in terms of\ndocuments and summary lengths.\nwhere BiGRUk denotes the kth output of the Bi-\nGRU and [; ]is the concatenation operation.\nAt this stage, we have computed, for a given doc-\nument, local block representations Vℓ\nk (1 ≤k ≤\nK) and a global representation Wℓ. We combine\nthem to build the output representation of the layer:\nUℓ+1\nk = [Wℓ\nk; Vℓ\nk,1; ··· ; Vℓ\nk,nk+1], 1 ≤k≤K.\nAs one can note, Uℓ+1\nk ∈R(nk+2)×h is a rep-\nresentation of block k in which the [CLS] vector\nrepresentation has been enriched with document\nlevel information propagated from other blocks.\nUℓ+1\nk is then used as input for the next propagation\nlayer.\n2.2 Output Layer\nIn this work, we validate our approach on the task\nof extractive summarization described in Section\n3. This task can be considered as a binary classiﬁ-\ncation problem where each block has to be labeled\nas selected or not. We use a feed-forward neural\nnetwork followed by a Softmax function on the top\nof the block level representations after the last layer\nLto compute Y ∈RK×2.\nYk = Softmax(FFNN(WL+1\nk )).\nUsing a recurrent architecture to propagate informa-\ntion between blocks has two interesting properties.\nFirst, it allows our model to scale to long sequences\nof blocks without using an attention mechanism\nthat would not scale. Second, it does not require to\nimplement any positional encoding on block repre-\nsentations.\n3 Experiments\nWe evaluate our approach, which we refer to as\nGBT-EXTSUM (for ‘Global BERT-based Trans-\nformer for Extractive Summarization’), in the con-\ntext of extractive summarization, the goal of which\nbeing to identify and extract from a document the\npieces of text that are the most important (Kupiec\net al., 1995). We view this task as a sentence-\nlevel classiﬁcation problem where each sentence\nhas to be labeled according to its belonging to the\nsummary or not. To validate the effectiveness of\nour approach, we propose to test it on three sum-\nmarization datasets, namely ArXiv, PubMed and\nCNN/DailyMail:\n•The ArXiv and Pubmed datasets have been\nintroduced in Cohan et al. (2018). They con-\ntain long scientiﬁc documents from arXiv.\norg and PubMed.com and use their abstracts\nas the ground-truth summaries. We use\nthe original splits that respectively contain\n203,037/6,436/6,440 samples in the train-\ning, validation, and test sets for arXiv, and\n119,924/6,633/6,658 for PubMed.\n•The CNN/DailyMail dataset contains news\narticles associated with short summaries. We\nuse the splits of Hermann et al. (2015), where\nentities have not been anonymized. This\ndataset contains 287,226 training samples,\n13,368 validation samples, and 11,490 test\nsamples.\nTable 1 presents some statistics on these three\ndatasets. As one can note, for the scientiﬁc articles,\nthe average number of tokens in the documents\nto summarize is way beyond the capabilities of a\nstandard transformer pre-trained with BERT.\n3.1 Evaluation Metrics\nWe evaluate the quality of the extracted sum-\nmaries using the ROUGE metric (Lin, 2004), and\nmore particularly ROUGE-1 (overlap of unigrams),\nROUGE-2 (overlap of bigrams), ROUGE-3 (over-\nlap of trigrams) and ROUGE-L (longest common\nsubsequence between the produced summary and\nthe gold-standard one).\n3.2 Label Generation\nIn order to train extractive summarizers, one needs\nannotations in the form of sentence-level binary\nlabels. To compute such annotations, we follow\nthe work of Kedzie et al. (2018) and label all sen-\ntences by greedily optimizing the ROUGE-1 score\nof the extracted summary against the gold-standard\nsummary associated with each article. These la-\nbels are only used at training time, the evaluation\nof the extracted summaries being done against the\ngold-standard summaries provided in the datasets.\n1796\nPubMed arXiv\nSummarizer RG-1 RG-2 RG-3 RG-L RG-1 RG-2 RG-3 RG-L\nOracle 58.15 34.16 24.11 52.99 57.78 30.43 18.41 51.24\nLead 37.77 13.35 7.64 34.31 35.54 9.50 3.33 31.19\nAbstractive or Mix\nAttn-Seq2Seq (Nallapati et al., 2016) 31.55 8.52 7.05 27.38 29.30 6.00 1.77 25.56\nPntr-Gen-Seq2Seq (See et al.) 35.86 10.22 7.60 29.69 32.06 9.04 2.15 25.16\nDiscourse summarizer (Cohan et al., 2018) 38.93 15.37 9.97 35.21 35.80 11.05 3.62 31.80\nTLM-I+E (G,M) (Subramanian et al., 2019) 42.13 16.27 8.82 39.21 41.62 14.69 6.16 38.03\nDANCER PEGASUS (Gidiotis and Tsoumakas, 2020) 46.34 19.97 - 42.4245.01 17.60 - 40.56\nPEGASUS (Zhang et al., 2019a) 45.97 20.15 - 28.25 44.21 16.95 - 25.67\nBIGBIRD-Pegasus (Zaheer et al., 2020) 46.32 20.65 - 42.33 46.63 19.02 - 41.77\nExtractive\nSumBasic (Vanderwende et al., 2007) 37.15 11.36 5.42 33.43 29.47 6.95 2.36 26.30\nLexRank (Erkan and Radev, 2004) 39.19 13.89 7.27 34.59 33.85 10.73 4.54 28.99\nLSA (Steinberger and Jezek, 2004) 33.89 9.93 5.04 29.70 29.91 7.42 3.12 25.67\nSent-CLF (Subramanian et al., 2019) 45.01 19.91 12.13 41.16 34.01 8.71 2.99 30.41\nSent-PTR (Subramanian et al., 2019) 43.30 17.92 10.67 39.47 42.32 15.63 7.49 38.06\nBert Ranker (Nogueira and Cho, 2019) 43.67 18.00 10.74 39.22 41.65 13.88 5.92 36.40\nBERTSUMEXT(Liu and Lapata, 2019b) 41.09 15.51 8.64 36.85 41.24 13.01 5.26 36.10\nBERTSUMEXT(SW) (Liu and Lapata, 2019b) 45.01 20.00 12.05 40.43 42.93 15.08 6.01 37.22\nLongformer-Ext (Beltagy et al., 2020) 43.75 17.37 10.18 39.71 45.24 16.88 8.06 40.03\nReformer-Ext (Kitaev et al., 2020) 42.32 15.91 9.02 38.26 43.26 14.86 6.66 38.10\nGBT-EXTSUM(Ours) 46.87 20.19 12.11 42.68 48.08 19.21 9.58 42.68\nTable 2: Summarization results on PubMed and arXiv. Except for BERT-based approaches, for Reformer-Ext and\nfor Longformer-Ext, which we have reimplemented, the results of the baselines are taken from their associated\npaper as well as from Cohan et al. (2018). Bold results correspond to the best scores of extractive summarizers.\n3.3 Baseline Models\nWe compare our approach to several well known\npublished methods described below. These meth-\nods include SumBasic (Vanderwende et al., 2007),\nLexRank (Erkan and Radev, 2004), LSA (Stein-\nberger and Jezek, 2004), Attn-Seq2Seq (Nallapati\net al., 2016), Pntr-Gen-Seq2Seq (See et al.) and\nDiscourse-aware summarizer (Cohan et al., 2018).\nThe results for these models are the ones reported\nin the paper (Cohan et al., 2018). We also report\nthe results of Sent-CLF and Sent-PTR, which are\nhierarchical sentence pointer and classiﬁer, TLM-\nI+E (G,M) a mixed extractive/generative trans-\nformer language model from Subramanian et al.\n(2019), BIGBIRD (Zaheer et al., 2020), PEGASUS\n(Zhang et al., 2019a) and DANCER (Gidiotis and\nTsoumakas, 2020) which are three abstractive meth-\nods. Lastly, we developed several baseline models\nbased on BERT, Longformer (Beltagy et al., 2020)\nand Reformer (Kitaev et al., 2020):\nBERT Ranker:We used a BERT ranker, sim-\nilar to Nogueira and Cho (2019) in which each\nsentence of the document is processed individu-\nally. We apply BERT on each sentence1 and use a\nSigmoid layer, the input of which consists of the\n1This is possible as no sentence exceeds BERT token limi-\ntation.\n[CLS] representation of the sentence, to model the\nprobability of the sentence to be selected.\nBERTSUMEXT has been introduced in Liu and\nLapata (2019b). This model is an adaptation of\nBERT for extractive summarization. Because this\nmodel takes as input the concatenation of all the\ntokens of the document, it cannot scale to the arXiv\nand PubMed datasets. We propose two variants:\nthe ﬁrst one is to take as input only the ﬁrst 800 to-\nkens of the document, as suggested in the original\npaper. This solution is displayed as BERTSUMEXT\nin Table 2. The second is to apply BERTSUMEXT\nper sliding windows on the original document and\nto use, as a token representation, its representation\nin the window that maximizes its surrounding con-\ntext. We name this sliding window implementation\nBERTSUMEXT (SW) in Table 2.\nLongformer-Ext and Reformer-Ext : The\nLongformer and Reformer models were respec-\ntively introduced by Beltagy et al. (2020) and Ki-\ntaev et al. (2020). They both propose an adaptation\nof the Transformer self-attention that scale to long\nsequences. We add the same classiﬁcation head\nas the one used in our model on top of the contex-\ntualized representation of the ﬁrst token of each\nsentence to label them as selected or not in the\nsummary.\n1797\nWe also present the Oracle extractive results as\nan upper bound as well as the Lead baseline (which\nrespectively select the ﬁrst 3, 6, 7 sentences for\nCNN/DailyMail, arXiv and PubMed datasets). Sev-\neral models are reported only on CNN/DailyMail\ndataset and not on arXiv/Pubmed as they do not\nscale to long documents.\n3.4 Implementation details\nWe run all our experiments using the Pytorch li-\nbrary (Paszke et al., 2019). We built our model\nusing the ”bert-base-uncase” 2 version of BERT\nand its implementation in the HuggingFace library\n(Wolf et al., 2019). Our architecture is composed\nof L = 12propagation layers with a transformer\nhidden dimension of h = 768. The hidden di-\nmension of the BiGRU is set to 384 and we share\nits parameters among all the propagation layers.\nThe FFNN inside the propagation layers maps the\noutput of the BiGRU of dimension 2 ×384 to a\nvector of dimension 768. The FFNN of the output\nlayer is a binary classiﬁer that projects the sentence\nrepresentations of dimension 768 to an output of\ndimension 2. We ﬁne-tuned our model on the cross-\nentropy loss, for 5 epochs on 4 GPUs V100 and\nuse Adam optimizer (Kingma and Ba, 2015) with\nthe initial learning rate set to 3 ×10−5, β1 = 0.9,\nβ2 = 0.999, no learning rate warmup and a linear\ndecay of the learning rate. We describe implemen-\ntation details of BERTSUMEXT, Longformer-Ext\nand Reformer-Ext baselines in the Supplementary\nMaterial, Appendix A.\nWe usedTrigram Blockingto avoid the repetition\nof trigrams in the extracted summaries as suggested\nin Paulus et al. (2018). Given the extracted sum-\nmary so far, we only added candidate sentences\nthat had no overlapping trigram with the current\nsummary. We limited the summary to 3 sentences\nfor the CNN/DailyMail dataset, 6 sentences for\narXiv, and 7 for PubMed.\n3.5 Results\nOur main results are shown in Tables 2 and 3. On\nthe arXiv and PubMed datasets, our model out-\nperforms the baseline models on almost all of the\nreported metrics. Our approach manages to summa-\nrize long documents while preserving informative-\nness (evaluated by ROUGE-1) and ﬂuency (evalu-\nated by ROUGE-L) of the summaries. In addition\n2https://github.com/google-research/\nbert\nModel R-1 R-2 R-L\nOracle 56.2233.7452.19\nLead-3 40.1117.5436.32\nLATENT(Zhang et al., 2018) 41.0518.7737.54\nNEUSUM(Zhou et al., 2018) 41.5919.0137.98\nSUMO(Liu et al., 2019a) 41.0018.4037.20\nTransformerExt (Liu and Lapata, 2019b)40.9018.0237.17\nMASK-LMglobal(Chang et al., 2019) 41.2 19.1 37.6\nPNBERT (Zhong et al., 2019) 42.6919.6038.85\nBERT-ext + RL (Bae et al., 2019)42.7619.8739.11\nHIBERTM(Zhang et al., 2019b) 42.3719.9538.83\nBERTSUMEXT(Liu and Lapata, 2019b)43.2520.2439.63\nBERTSUMEXTw/o interval embedding43.2020.2239.59\nBERTSUMEXT(large) 43.8520.3439.90\nMatchSum (RoBERTa) (Zhong et al., 2020)44.4120.8640.55\nReformer-Ext (Kitaev et al., 2020)38.8516.4635.16\nLongformer-Ext (Beltagy et al., 2020)43.0020.2039.30\nGBT-EXTSUM(Ours) 42.9319.8139.20\nTable 3: Comparison of ROUGE scores on\nCNN/DailyMail wrt extractive models. All re-\nsults are taken from original papers but Reformer-Ext\nand Longformer-Ext which we have reimplemented.\nto the previously published methods, our approach\nalso improves over the BERT-based, Longformer-\nExt and Reformer-Ext baselines we have developed.\nAmong them, BERTSUMEXT, which focuses on a\ntruncated version of the document, is the less ef-\nfective. As documents are signiﬁcantly longer than\nthe 800 tokens limitation of this model, this result\nis not surprising. The sliding window adaptation\nof this model, that allows it to scale to long docu-\nments, is the one that achieves results that are the\nmost comparable to ours. Our approach still out-\nperforms this adaptation, demonstrating that sum-\nmaries require to propagate information beyond a\nsingle BERT window.\nOn the CNN/DailyMail dataset, one can see\nthat our model outperforms all the models that\ndo not use pre-trained parameters. This includes\nseveral transformer-based and hierarchical mod-\nels. However, while having comparable results,\nwe do not achieve stronger performance than the\ncurrent extractive state of the art from Zhong et al.\n(2020). This is not surprising as the majority of\nthe CNN/DailyMail examples contains their ora-\ncle summary sentences in the ﬁrst positions of the\narticles, as shown in the Supplementary Material,\nAppendix B.\nLastly, we evaluate the impact of several ele-\nments of our proposed model in Table 4. We\nﬁrst study the inﬂuence of the underlying lan-\nguage model by considering both RoBERTa (Liu\net al., 2019b) and PEGASUS (Zhang et al.,\n2019a) pre-trained models, respectively referred\nto as GBT-EXTSUM-RoBERTa and GBT-EXTSUM-\n1798\nPubMed arXivModel R-1 R-2 R-L R-1 R-2 R-L\nGBT-EXTSUM 46.8720.1942.6848.0819.2142.68\nGBT-EXTSUM-RoBERTa46.0219.2941.8447.4218.6242.03GBT-EXTSUM-PEGASUS44.1117.3440.0343.5015.3538.41\nGBT-EXTSUM-NoShare 46.8420.1942.6348.1119.3042.75\nGBT-EXTSUM-AveragePool45.2418.1340.9445.7117.3640.43GBT-EXTSUM-Transformer46.4619.6242.1747.6418.8242.22\nTable 4: Analysis of the inﬂuence of different key com-\nponents of our proposed architecture.\nPEGASUS. As one can see, the results show\nthat BERT-base architecture performs best in\nterms of ROUGE scores on both arXiv and\nPubMed. One major difference between PEGA-\nSUS and BERT/RoBERTA pre-trained models is\nthat BERT/RoBERTA are only encoders while PE-\nGASUS is a pre-trained encoder/decoder architec-\nture. This could explain why BERT/RoBERTA\noutperform PEGASUS on extractive summariza-\ntion tasks. We then compare an alternative of our\nimplementation of GBT-EXTSUM in which the pa-\nrameters of the BiGRU are not shared among all the\npropagation layers (GBT-EXTSUM-NoShare) and\nfound no clear difference with the version in which\nthe parameters are shared. Lastly, we compare\nthree architectures of propagation layers, including\nan average pooling of the [CLS] representations\nof the sentences, a Transformer layer between the\n[CLS] tokens (associated to a block position em-\nbedding), and a BiGRU layer. Among these three\nlayers, the average pooling layer, which introduces\nno additional trainable parameters, performs the\nworst. Furthermore, the BiGRU layer slightly out-\nperforms the Transformer layer in terms of ROUGE\nscores.\nAnalysis. In Figure 2, we compare the R-1 score\nof several models regarding the number of words\nin the source documents. One can see that GBT-\nEXTSUM consistently outperforms BERTSUMEXT\n(SW), Reformer-Ext and Longformer-Ext regard-\nless of the number of words in the source docu-\nments.\nWe present in Table 5 two example summaries of\na document from the PubMed test set (Kamio et al.,\n2009), respectively obtained by GBT-EXTSUM and\nBERTSUMEXT (SW). The numbers in the margin\nindicate the position of the sentences in the orig-\ninal document, which is composed of a total of\n78 sentences. As one can observe, GBT-EXTSUM\nextracts sentences from various parts of the docu-\nment whereas BERTSUMEXT (SW) mostly focuses\n1000 2000 3000 4000 5000 6000 7000 8000 900010000+\nDocument length\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70R-1 score\nOracle\nGBT-ExtSum\nLongformer-Ext\nReformer-Ext\nBertSum(SW)\nFigure 2: Average R-1 scores of extracted summaries\naccording to the number of words in the input docu-\nments from arXiv test dataset.\non the beginning of the document. Among the\nsentences selected by the two models, the most\nmeaningful one, in terms of ROUGE, is the last\none selected by GBT-EXTSUM. This sentence ap-\npears at position 66, in the last section (Discussion)\nof the original paper. In contrast, B ERTSUMEXT\n(SW) proposes sentences that are less relevant for\nsummarization purposes. Additional summaries of\nthe PubMed and arXiv articles are provided in the\nSupplementary Material, Appendices C and D.\nTo analyse the inﬂuence of the positions of the\nsentences in the input document, we present in\nFigure 3 the histograms of the positions of the sen-\ntences of the Oracle summary as well as that of\nthe predicted positions of different models, on the\nPubMed test set. One can see that if most rele-\nvant sentences appear at the beginning of a docu-\nment, other Oracle sentences are still relevant fur-\nther down the document. GBT-E XTSUM is the\nmodel that behaves the most closely to the Oracle,\nfollowed by BERTSUMEXT (SW), Reformer-Ext\nand Longformer-Ext. These last two models tend\nto over-select sentences from the beginning while\nfocusing less on the ones appearing later in the\ndocument. Our model remains inﬂuenced by the\nsentence position but is still able to select sentences\nfrom all over the document and is closer to the Or-\nacle distribution.\n4 Related Work\nHierarchical neural architectures have been\ncompetitive on a collection of NLP tasks that re-\nquire to reason over long or multiple documents\nsuch as aspect-based sentiment analysis (Paulus\net al., 2018), document summarization (Cheng and\nLapata, 2016), document segmentation (Koshorek\n1799\nGOLD\npurpose : to investigate whether the glc3a locus harboring the cyp1b1 gene is associated with normal tension glaucoma ( ntg ) in japanese\npatients.materials and methods : one hundred forty two japanese patients with ntg and 101 japanese healthy controls were recruited .\npatients exhibiting a comparatively early onset were selected as this suggests that genetic factors may show stronger involvement .\ngenotyping and assessment of allelic diversity was performed on 13 highly polymorphic microsatellite markers in and around the glc3a\nlocus.results:there were decreased frequencies of the 444 allele of d2s0416i and the 258 allele of d2s0425i in cases compared to controls (\np = 0.022 and p = 0.034 , respectively ) .\nhowever , this statistical signiﬁcance disappeared when corrected ( pc > 0.05 ) .\nwe did not ﬁnd any signiﬁcant association between the remaining 11 microsatellite markers , including d2s177 , which may be associated\nwith cyp1b1 , and ntg ( p > 0.05). conclusions : our study showed no association between the glca3 locus and ntg , suggesting that the\ncyp1b1 gene , which is reportedly involved in a range of glaucoma phenotypes , may not be an associated factor in the pathogenesis of ntg .\nGBT-EXTSUM\n1- primary open angle glaucoma ( poag ) is the most common type of glaucoma .\n15- we excluded individuals who were diagnosed under 20 or over 60 years of age and who had 8.0 d or higher myopic refractive error of\nspherical equivalence .\n17- the cases exhibiting a comparatively early onset were selected as they suggest that genetic factors may show stronger involvement . during\ndiagnosis ,\n30- the probability of association was corrected by the bonferroni inequality method , ie , by multiplying the obtained p values with the number\nof alleles compared .\n63-\nonly two adjacent markers , d2s0416i and d2s0425i , were signiﬁcantly positive , as shown in table 2 , and the frequency of the 444 allele\nof d2s0416i and the 258 allele of d2s0425i were decreased in cases compared to controls ( p = 0.022 , or = 0.59 and p = 0.034 , or = 0.42 ,\nrespectively ) .\n66-\nthe purpose of this study was to investigate whether the glc3a locus is associated with ntg in japanese subjects , based on results from recent\nstudies reporting that the cyp1b1 gene , located at the glc3a locus on chromosome 2p21 , could be a causative gene in poag as well as pcg .\nto this end , we genotyped 13 microsatellite markers in and around the glc3a locus . here\nBERTSUMEXT (SW)\n1- primary open angle glaucoma ( poag ) is the most common type of glaucoma .\n2-\nnormal tension glaucoma ( ntg ) is an important subset of poag ; while many poag patients have high iop,1 patients with ntg have statistically\nnormal iop.24 the prevalence of ntg is higher among the japanese population than among caucasians , and recent studies reported that 92%\nof poag patients in japan had ntg.58 the diagnosis of glaucoma is based on a combination of factors including optic nerve damage and\nspeciﬁc ﬁeld defects for which iop is the only treatable risk factor .\n7- of these subjects , 142 were diagnosed with ntg , and 101 were control subjects .\n20- genomic dna was extracted using the qiaamp dna blood mini kit ( qiagen , hilden , germany ) or the guanidine method . in this association\nstudy , we selected 13 highly polymorphic microsatellite markers that are located in and around the glc3a locus as shown in ﬁgure 1 .\n28- the number of microsatellite repeats was estimated automatically using the genescan 672 software ( applied biosystems ) by the local\nsouthern method with a size marker of gs500 tamra ( applied biosystems ) .\n22- polymerase chain reaction ( pcr ) was performed in a reaction mixture with a total volume of 12.5 l containing pcr buffer , genomic dna ,\n0.2 mm dinucleotide triphosphates ( dntps ) , 0.5 m primers , and 0.35 u taq polymerase .\nTable 5: An example of summary produced by our method compared to the gold summary and one produced by\nBERTSUMEXT (SW). With a red scale, we highlight the sentences with the highest ROUGE score when evaluated\nagainst the abstract. We show in the margin the position of the extracted sentence in the document. This document\n(Kamio et al., 2009) is 78 sentences long.\net al., 2018) and text classiﬁcation (Yang et al.,\n2016). The hierarchical structure enables the model\nto learn local contextualized token representations\nin its lower hierarchy level, while higher-level rep-\nresentations can capture long-distance dependen-\ncies within the document. Liu and Lapata (2019a)\nhave proposed a hierarchical modiﬁcation of the\ntransformer layer-based attention modules to model\nrelations between documents for abstractive sum-\nmarization but do not investigate parameter trans-\nfer form pre-trained language models. Chang et al.\n(2019) and Zhang et al. (2019b) suggested pre-\ntraining processes for hierarchical models, without\nhowever testing their approaches on long docu-\nment summarization nor releasing their pre-trained\nmodels. We have not included these models in our\ncomparison for this reason. Transformer-XH (Zhao\net al., 2020) introduced an eXtra Hop attention to\nmodel dependencies between different transformer\nwindows but requires a graph of related documents.\nLong-Document Transformers: Multiple stud-\nies have investigated different self-attention mech-\nanisms to extend transformers to long documents.\nTransformer-XL (Dai et al., 2019) introduced a re-\ncurrence between successive transformer windows\nwhich run from left to right through the document,\npreventing global information to bidirectionally\nﬂow through the document. Other approaches de-\nsign the self-attention as a sparse layer, as sparse\ntransformers (Child et al., 2019) or the recently pro-\nposed Longformer and BIGBIRD models (Beltagy\net al., 2020; Zaheer et al., 2020). One major differ-\nence with our work is that these models compute\nthe attention only between a limited set of ran-\ndomly or a priori chosen tokens. Reformer (Kitaev\net al., 2020) also tackles the problem of language\nmodeling for long sequences, but it does so by\ncomputing the self-attention only between similar\n1800\n0 25 50 75 100 125 150 175 200\nSentence position in document\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030Density\nOracle\nGBT-ExtSum\nBertSum(SW)\nLongformer-Ext\nReformer-Ext\nFigure 3: Proportion of the extracted sentences ac-\ncording to their position in the input document from\nPubMed test dataset.\ntokens, based on locality-sensitive hashing.\n5 Conclusion\nIn this paper, we have introduced a novel\ntransformer-based model for long document sum-\nmarization based on propagation layers that spread\ninformation between multiple transformer win-\ndows. This model preserves the architecture of\ncommonly used pre-trained language models, thus\nallowing the transfer of parameters. An evaluation,\nconducted on top of the BERT model in the con-\ntext of an extractive summarization task, further\nrevealed its effectiveness in dealing with long docu-\nments compared to other adaptations of BERT and\npreviously proposed models. In the future, we plan\nto adapt our model to other tasks that require under-\nstanding long documents, as question-answering\nand document-scale machine translation.\nAcknowledgments\nThis work was partially supported by\nMIAI@Grenoble Alpes, (ANR-19-P3IA-0003).\nReferences\nSanghwan Bae, Taeuk Kim, Jihoon Kim, and Sang-\ngoo Lee. 2019. Summary level training of sentence\nrewriting for abstractive summarization. CoRR,\nabs/1909.08752.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\nMing-Wei Chang, Kristina Toutanova, Kenton Lee, and\nJacob Devlin. 2019. Language model pre-training\nfor hierarchical document representations. CoRR,\nabs/1901.09128.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 484–494, Berlin, Germany. As-\nsociation for Computational Linguistics.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June 1-\n6, 2018, Volume 2 (Short Papers) , pages 615–621.\nAssociation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n2978–2988. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nG¨unes Erkan and Dragomir R. Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text\nsummarization. J. Artif. Int. Res., 22(1):457–479.\nYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang\nWang, and Jingjing Liu. 2019a. Hierarchical graph\nnetwork for multi-hop question answering. CoRR,\nabs/1911.03631.\nYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang\nWang, and Jingjing Liu. 2019b. Hierarchical graph\nnetwork for multi-hop question answering. CoRR,\nabs/1911.03631.\n1801\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A\ndivide-and-conquer approach to the summarization\nof long documents.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in Neural Informa-\ntion Processing Systems , volume 28, pages 1693–\n1701. Curran Associates, Inc.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel S. Weld. 2019. BERT for coreference reso-\nlution: Baselines and analysis. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 5802–5807. Association for\nComputational Linguistics.\nM Kamio, Akira Meguro, Masao Ota, N Nomura,\nKenji Kashiwagi, F Mabuchi, Hiroyuki Iijima,\nK Kawase, T Yamamoto, M Nakamura, Akira Negi,\nT Sagara, Teruo Nishida, M Inatani, Hidenobu Tani-\nhara, M Aihara, M Araie, Takeo Fukuchi, H Abe,\nand Nakamura Mizuki. 2009. Investigation of the\nassociation between the glc3a locus and normal ten-\nsion glaucoma in japanese patients by microsatellite\nanalysis. Clinical ophthalmology (Auckland, N.Z.) ,\n3:183–8.\nChris Kedzie, Kathleen McKeown, and Hal Daum´e III.\n2018. Content selection in deep learning models of\nsummarization. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1818–1828, Brussels, Belgium.\nAssociation for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nOmri Koshorek, Adir Cohen, Noam Mor, Michael Rot-\nman, and Jonathan Berant. 2018. Text segmentation\nas a supervised learning task. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Pa-\npers), pages 469–473, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nJulian Kupiec, Jan Pedersen, and Francine Chen. 1995.\nA trainable document summarizer. In Proceedings\nof the 18th Annual International ACM SIGIR Con-\nference on Research and Development in Informa-\ntion Retrieval, SIGIR ’95, page 68–73, New York,\nNY , USA. Association for Computing Machinery.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019a. Hierarchical\ntransformers for multi-document summarization. In\nProceedings of the 57th Conference of the Asso-\nciation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers , pages 5070–5081. Association for\nComputational Linguistics.\nYang Liu and Mirella Lapata. 2019b. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYang Liu, Ivan Titov, and Mirella Lapata. 2019a. Sin-\ngle document summarization as tree induction. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n1745–1755, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nC ¸ a˘glar Gulc ¸ehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. CoRR, abs/1901.04085.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K ¨opf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada,\npages 8024–8035.\n1802\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383–2392.\nThe Association for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\nGet To The Point: Summarization with Pointer-\nGenerator Networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083. Association for Computational Linguistics.\nJosef Steinberger and Karel Jezek. 2004. Using latent\nsemantic analysis in text summarization and sum-\nmary evaluation.\nSandeep Subramanian, Raymond Li, Jonathan Pilault,\nand Christopher J. Pal. 2019. On extractive and ab-\nstractive neural document summarization with trans-\nformer language models. CoRR, abs/1909.03186.\nMing Tu, Kevin Huang, Guangtao Wang, Jing Huang,\nXiaodong He, and Bowen Zhou. 2019. Select, an-\nswer and explain: Interpretable multi-hop reading\ncomprehension over multiple documents. CoRR,\nabs/1911.00484.\nMing Tu, Kevin Huang, Guangtao Wang, Jing Huang,\nXiaodong He, and Bowen Zhou. 2020. Select, an-\nswer and explain: Interpretable multi-hop reading\ncomprehension over multiple documents. national\nconference on artiﬁcial intelligence.\nLucy Vanderwende, Hisami Suzuki, Chris Brockett,\nand Ani Nenkova. 2007. Beyond sumbasic: Task-\nfocused summarization with sentence simpliﬁca-\ntion and lexical expansion. Inf. Process. Manag. ,\n43(6):1606–1618.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4,\n2018, pages 2369–2380. Association for Computa-\ntional Linguistics.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1480–1489, San Diego, California. Associa-\ntion for Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big bird: Transformers for\nlonger sequences.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adver-\nsarial dataset for grounded commonsense inference.\nCoRR, abs/1808.05326.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019a. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nXingxing Zhang, Mirella Lapata, Furu Wei, and Ming\nZhou. 2018. Neural latent extractive document sum-\nmarization. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 779–784, Brussels, Belgium. Association\nfor Computational Linguistics.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019b.\nHIBERT: document level pre-training of hierarchi-\ncal bidirectional transformers for document summa-\nrization. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 5059–5069. Association\nfor Computational Linguistics.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia\nSong, Paul N. Bennett, and Saurabh Tiwary. 2020.\nTransformer-xh: Multi-evidence reasoning with ex-\ntra hop attention. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\n1803\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuanjing Huang. 2020. Extrac-\ntive summarization as text matching. InProceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 6197–6208, On-\nline. Association for Computational Linguistics.\nMing Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu,\nand Xuanjing Huang. 2019. Searching for effec-\ntive neural extractive summarization: What works\nand what’s next. In Proceedings of the 57th Confer-\nence of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pages 1049–1058. As-\nsociation for Computational Linguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural doc-\nument summarization by jointly learning to score\nand select sentences. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 654–\n663, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\n1804\nA Baselines: Implementation Details\nBERTSUMEXT: For all experiments with\nBERTSUMEXT, we started with the original\nimplementation3 and adapted the code to build the\nsliding windows version. This implementation\nleverage bert-base-uncased pre-trained model and\nits associated hyperparameters. We use windows of\nwidth 800 with an overlap of 300 tokens between\ntwo following windows. If a sentence is in multiple\nwindows, we select its [CLS] representation in the\nwindow that maximizes the number of surrounding\ntokens. We ﬁnetune the model for 5 epochs using\nAdam optimizer with an initial learning rate of\n1 ×10−5, β1 = 0.9, β2 = 0.999.\nLongformer-Ext: We built the Longformer-Ext\nbaseline from the Longformer implementation\nreleased by HuggingFace 4. We use the ofﬁcial\nlongformer-base-4096 pre-trained model trained\nby AllenAI5. This model is based on RoBERTa-\nbase and its associated hyperparameters. To\nincrease the maximal position embedding, we drop\nthe pre-trained positional embedding parameters\nand train a novel token embedding layer to\nscale Longformer-Ext input up to 12294 tokens.\nThis model computes a sliding self-attention\nwith a window size of 512 tokens on all its 12\nTransformer layers. We ﬁnetune the model for\n5 epochs with only local attention because of\nmemory constraints, using Adam optimizer with\nan initial learning rate of 1 ×10−5, β1 = 0.9,\nβ2 = 0.999, no learning rate warmup and a linear\ndecay of the learning rate.\nReformer-Ext: We started from the HuggingFace\nimplementation of Reformer to build Reformer-Ext\nbaseline. We use a Reformer conﬁguration\ncomposed of six layers of attention. We use\nLocality-Sensitive Hashing Attention with 128\nbuckets on the input sequence and Local Self-\nattention on chunks of 64 tokens. We use hidden\nsates of dimension 256, a feed-forward layer\nof dimension 512, and 12 attention heads in\nTransformer encoders. We train this model for\n5 epochs using Adam optimizer with an initial\nlearning rate of 1 ×10−5, β1 = 0.9, β2 = 0.999,\nno learning rate warmup and a linear decay of the\n3https://github.com/nlpyang/PreSumm\n4https://github.com/huggingface/\ntransformers\n5https://github.com/allenai/longformer\nlearning rate.\nB Datasets Statistics\nFigure 4 presents the distribution of the document\nlengths in arXiv, PubMed and CNN/DailyMail, af-\nter tokenization with pretrained BERT-base tok-\nenizer. It also provides the histograms of the po-\nsition of the [CLS] tokens of the Oracle sentences\nin input documents. One can see that the three\ndatasets contain an important number of documents\nlonger than 512 tokens, the standard length limi-\ntation of pre-trained language models. However,\none can also notice that CNN/DailyMail contains\na large part of its Oracle sentences within this ﬁrst\nwindow of 512 tokens. As a consequence, a model\nthat is not able to ”read” beyond this limitation is\nnot penalized. It is also a reason why Lead baseline\nis quite strong on this dataset. On the contrary, on\narXiv and PubMed, one can see that a large part of\nOracle sentences occur beyond this 512 windows.\nThis explains why models capable of reading long\nsequences are required to achieve good results on\nthese datasets.\n1805\nArXiv\n0 5000 10000 15000 20000 25000 30000\nDocument length\n0\n100\n200\n300\n400Frequency\n0 2000 4000 6000 8000 10000 12000 14000\n[CLS] positions of oracle sentences\n0\n1000\n2000\n3000Frequency\nPubMed\n0 2000 4000 6000 8000 10000 12000 14000\nDocument length\n0\n200\n400Frequency\n0 1000 2000 3000 4000 5000 6000 7000 8000\n[CLS] positions of oracle sentences\n0\n2000\n4000Frequency\nCNN/DailyMail\n0 500 1000 1500 2000 2500 3000\nDocument length\n0\n200\n400\n600Frequency\n0 250 500 750 1000 1250 1500 1750 2000\n[CLS] positions of oracle sentences\n0\n2000\n4000\n6000Frequency\nFigure 4: Document lengths after tokenization with pretrained BERT-base tokenizer and position of the [CLS]\ntokens of Oracle sentences in the input documents.\n1806\nC PubMed Summaries\nGOLD\naim . to investigate incidental adrenal enlargement clinical characteristics and functional status and analyze functional lesion risk factors .\nmaterials and methods .\nthis retrospective study included 578 patients with adrenal imaging features showing enlargement .\nincidental adrenal enlargement cases ( 78 ) were considered eligible .\ndemographics , functional diagnosis , adrenal imaging features , and concomitant diseases were analyzed .\nresults .\nthe number of adrenal enlargements and proportion of incidental adrenal enlargement increased each year .\nmean patient age was 50.32 years .\nthirty - nine cases had unilateral enlargement on the left side and 3 on the right side ; 36 had bilateral enlargement .\nroutine medical checkup was found to have the greatest chance ( 43.59% ) of revealing clinical onsets leading to discovery .\nbiochemical and functional evaluation revealed 54 ( 69.23% ) cases of nonfunctional lesions , 12 ( 15.38% ) of subclinical cushing syndrome\n, 6 ( 7.69% ) of primary hyperaldosteronism , 1 ( 1.28% ) of metastasis , and 5 ( 6.41% ) of unknown functional status .\nnodular adrenal enlargement ( or , 7.306 ; 95% ci , 1.72728.667 ;\np = 0.006 ) was a risk factor for functional lesions .\nage and lesion location were not signiﬁcant factors .\nconclusion .\nincidental adrenal enlargement is a frequent radiographic ﬁnding and is accompanied by diverse clinical factors that require proper evalua-\ntion and management .\nnodular adrenal enlargement was a risk factor .\nGBT-EXTSUM\n8- data retrieved included patient demographics , ﬁnal functional diagnosis , adrenal imaging features , and concomitant diseases .\n14- smooth enlargement was deﬁned as enlargement of the gland with a smooth contour and no measureable or diffuse nodules . after obtaining\npatient history and physical examination , all patients underwent biochemical evaluation to assess their functional status .\n16- patients with an aldosterone - rennin ratio ( arr ) > 20 underwent any 1 of 3 conﬁrmatory tests ( saline infusion , captopril challenge , or\npostural stimulation ) to conﬁrm or exclude deﬁnitively primary hyperaldosteronism ( pa ) .\n25- as shown in table 1 , routine medical checkup was found to have the greatest chance ( 43.59% ) of revealing clinical onsets leading to the\ndiscovery of adrenal enlargement .\n29- nodular adrenal enlargement ( or 7.306 ; 95% ci , 1.72728.667 ; p = 0.006 ) was the risk factor for functional lesions .\n31- our study shows that the proportion of incidental adrenal enlargement has gradually increased by year .\n46- acth - independent macronodular hyperplasia ( aimah ) and primary pigmented nodular adrenal hyperplasia often manifest as adrenal\nhyperplasia . the clinical features of aimah tended to be atypical .\nBERTSUMEXT (SW)\n4-\nit is a common term for a variety of adrenal disorders , but its cause must be properly assessed so that patients needing treatment , such as\nthose with hormone hypersecretion or malignant disease , can receive appropriate care . however , there is a lack of literature on functional\nstatus and its follow - up to provide comprehensive insight to these ﬁndings .\n5- patients with incidental adrenal enlargement were evaluated in a tertiary referral hospital with endocrinological departments in china .\n7- this retrospective study included 578 patients with adrenal imaging features showing adrenal enlargement who were hospitalized at the\ndepartment of endocrinology in pla general hospital ( beijing , china ) between january 1993 and july 2013 .\n29- nodular adrenal enlargement ( or 7.306 ; 95% ci , 1.72728.667 ; p = 0.006 ) was the risk factor for functional lesions .\n36- in addition , smooth enlargement was more common , in 53 ( 83% ) cases , and together these statistics reﬂect the likelihood that adrenal\nenlargement will be bilateral , smooth , and found in men .\n37- however , our study did not show this tendency , likely because the research goals and thus , study populations , differed between the 2\nstudies .\n38- ’s study aimed to explore prevalence , while the present study aimed to evaluate functional status .\n1807\nGOLD\nbackground and objective .\nantimicrobial resistance is now a major challenge to clinicians for treating patients .\nhence , this short term study was undertaken to detect the incidence of multidrug - resistant ( mdr ) , extensively drug - resistant ( xdr ) ,\nand pandrug - resistant ( pdr ) bacterial isolates in a tertiary care hospital .\nmaterial and methods .\nthe clinical samples were cultured and bacterial strains were identiﬁed in the department of microbiology .\nthe antibiotic susceptibility proﬁle of different bacterial isolates was studied to detect mdr , xdr , and pdr bacteria .\nresults . the antibiotic susceptibility proﬁle of 1060 bacterial strains was studied .\n393 ( 37.1% ) bacterial strains were mdr , 146 ( 13.8% ) strains were xdr , and no pdr was isolated .\nall ( 100% ) gram negative bacterial strains were sensitive to colistin whereas all ( 100% ) gram positive bacterial strains were sensitive to\nvancomycin .\nconclusion .\nclose monitoring of mdr , xdr , or even pdr must be done by all clinical microbiology laboratories to implement effective measures to reduce\nthe menace of antimicrobial resistance .\nGBT-EXTSUM\n5- multidrug resistant ( mdr ) was deﬁned as acquired nonsusceptibility to at least one agent in three or more antimicrobial categories .\nextensively drug\n36- no mdr or xdr strain was isolated from streptococcus sp . all ( 100% ) gram positive cocci were sensitive to vancomycin and linezolid .\n38- e. coli was the commonest isolate 261 ( 35% ) , followed by pseudomonas aeruginosa 212 ( 28.4% ) .\n40-\nout of 200 klebsiella pneumoniae strains isolated , 75 ( 37.5% ) and 25 ( 12.5% ) were detected as mdr and xdr , respectively . out of 42\nacinetobacter and other nonfermenter species isolated , 19 ( 45.2% ) and 8 ( 19% ) were mdr and xdr strains , respectively . amongst 250\ngnb - mdr strains isolated ,\n62- , it has been reported that most frequent mdr pathogens were pseudomonas aeruginosa followed by e. coli .\n67-\nunless and until multidrug resistant organisms are detected and their incidence is known , the strategies for their control can not be adopted\nproperly in healthcare setup . hence , detection , prevention of transmission of mdros by following infection control practices , antimicrobial\nsurveillance , and stewardship are need of the hour .\n69- we hereby conclude that early detection and close monitoring of mdr , xdr , or even pdr bacterial strains must be started by all clinical\nmicrobiology laboratories to reduce the menace of antimicrobial resistance which is now a global problem .\nBERTSUMEXT (SW)\n9- this short term cross - sectional study was conducted in the department of microbiology from 15th of april to 15th of july , 2014 .\n10- the bacterial strains were isolated from different clinical samples and were identiﬁed by conventional methods .\n17-\nmethicillin resistant staphylococcus aureus ( mrsa ) strains were detected by meca - mediated oxacillin resistance using cefoxitin disk ( 30\ng ) on mueller hinton ( mh ) agar plate inoculated with test strains as per standard disk diffusion recommendations and incubated at 3335c\nfor 1618 hours .\n20- an increase in diameter of 5 mm with ceftazidime plus clavulanic acid as compared to ceftazidime disk alone was considered positive for\nesbl detection .\n36- no mdr or xdr strain was isolated from streptococcus sp . all ( 100% ) gram positive cocci were sensitive to vancomycin and linezolid .\n38- e. coli was the commonest isolate 261 ( 35% ) , followed by pseudomonas aeruginosa 212 ( 28.4% ) .\n65-\nthe limitation of this study is that this is a single center study for only three - month period in a tertiary care hospital in central india . to\nreﬂect the trend of infections caused by mdr and xdr strains of bacteria in the region , a multicenter study involving all types of healthcare\nsetups for a minimum period of one year\n1808\nGOLD\nbackground suicide is a grave public health issue that is responsible for a high mortality rate among individuals aged 1544 years .\nattitudes toward suicide among medical staff members have been associated with appropriate therapeutic responses to suicidal individuals .\nthe aim of this study was to examine the effects of parental rearing on attitudes toward suicide among japanese medical college stu-\ndents.methodswe examined the association between parental bonding and attitudes toward suicide in 160 medical college students in japan\n.\nthe parental bonding instrument was used to assess the attitudes and behaviors of parents .\nthe attitudes toward suicide were evaluated using the japanese version of the attitudes toward suicide questionnaire.resultsthe mean age of\nthe subjects was 25.24.0 years old .\nthe majority of the participants in our study agreed that anyone could commit suicide ( 88.8% ) and that suicide is preventable ( 86.3% ) .\nafter adjusting for age and sex , multivariate regression analysis revealed that maternal care approached a statistically signiﬁcant association\nwith the right to suicide attitude . under the same conditions ,\nmaternal care was shown to be signiﬁcantly associated with the common occurrence attitude .\nno other signiﬁcant relationships were observed between parental bonding and attitudes toward suicide.conclusionthis study suggests that\na higher level of maternal care ensures that children think that suicide occurs less commonly .\nthe promotion of best practices for suicide prevention among medical students is needed .\nchild rearing support might be associated with suicide prevention .\nGBT-EXTSUM\n3-\nprevious studies have shown that difﬁculties with parental bonding during childhood could be a predisposing factor for the onset of many\npsychiatric conditions , such as anxiety , depressive states , and maladjusted behaviors.68 parental bonding and premorbid personality traits\nplay an important role in shaping the developmental trajectory of an individual , including his / her ability to adjust to stressful events .\n5- the objective of this study was to investigate whether parental bonding is associated with attitudes toward suicide among medical college\nstudents in japan .\n8- the demographic data ( age and sex ) were obtained from self - questionnaires and interviews .\n14- higher scores on the care and protection dimensions reveal that participants perceive their parents to be more caring and/or protective .\n39- right to suicide was signiﬁcantly associated with common occurrence , unjustiﬁed behavior , and preventability / readiness to help .\n43- the majority of the participants in our study agreed that anyone could commit suicide ( 88.8% ) and that suicide is preventable ( 86.3% ) .\n44- in addition , the multiple regression analysis revealed that participants who reported a higher level of maternal care thought that suicide\nwas a common occurrence and tended to think that people do not have the right to commit suicide .\nBERTSUMEXT (SW)6- students in their ﬁfth year of medical school at hirosaki university , hirosaki , japan , participated in the study .\n7- the surveys were distributed to 226 medical students . of the distributed 226 surveys , 160 questionnaires ( 116 males and 44 females )\n13- the overprotection dimension of the pbi reﬂects parental overprotection and control in contrast to the encouragement of autonomy .\n14- higher scores on the care and protection dimensions reveal that participants perceive their parents to be more caring and/or protective .\n15- we employed the japanese version of the attitudes toward suicide questionnaire ( atts ) to assess the attitudes toward suicide held by the\nstudy participants.12 we employed a six factor model that was previously developed in studies of japanese attitudes , including\n16- common occurrence , suicidal expression as mere threat , unjustiﬁed behavior ,\n17- impulsiveness.12,13 each item , with the exception of items 10 and 28 , was scored on a ﬁve point scale from 1 ( strongly agree ) to 5 (\nstrongly disagree ) .\n1809\nD ArXiv Summaries\nGOLD\nin vivo calcium imaging through microscopes has enabled deep brain imaging of previously inaccessible neuronal populations within the\nbrains of freely moving subjects .\nhowever , microendoscopic data suffer from high levels of background ﬂuorescence as well as an increased potential for overlapping\nneuronal signals .\nprevious methods fail in identifying neurons and demixing their temporal activity because the cellular signals are often submerged in the\nlarge ﬂuctuating background . here\nwe develop an efﬁcient method to extract cellular signals with minimal inﬂuence from the background .\nwe model the background with two realistic components : ( 1 ) one models the constant baseline and slow trends of each pixel , and ( 2 )\nthe other models the fast ﬂuctuations from out - of - focus signals and is therefore constrained to have low spatial - frequency structure .\nthis decomposition avoids cellular signals being absorbed into the background term . after subtracting the background approximated with\nthis model , we use constrained nonnegative matrix factorization ( cnmf , @xcite ) to better demix neural signals and get their denoised and\ndeconvolved temporal activity .\nwe validate our method on simulated and experimental data , where it shows fast , reliable , and high quality signal extraction under a wide\nvariety of imaging parameters .\nGBT-EXTSUM\n1- . continued advances in optical imaging technology are greatly expanding the number and depth of neuronal populations that can be\nvisualized .\n2-\nspeciﬁcally , in vivo calcium imaging through microendoscopic lenses and the development of miniaturized microscopes have enabled\ndeep brain imaging of previously inaccessible neuronal populations of freely moving mice ( @xcite ) . while these techniques have been\nwidely used by neuroscientists ,\n20-\nlike the proposed cnmf in @xcite , our extended cnmf for microendoscopic data ( cnmf - e ) also has the capability of identifying neurons\nwith low signal - to - noise ratio ( snr ) and simultaneously denoising , deconvolving and demixing large - scale microendoscopic data . to\naccomplish this : ( 1 ) we replace the rank-1 nmf approximation of the background with a more sophisticated approximation , which can\nbetter account the complex background and avoid absorbing cellular signals , and ( 2 ) we develop an efﬁcient initialization procedure to\nextract neural activities with minimal inﬂuence from the background .\n71- @xmath56 is a template matching ﬁlter to detect spatial structures with similar shapes and sizes . for ﬂat structures in the small regions ,\nlike background , ﬁltering them with @xmath56\n134- in this paper , we proposed an efﬁcient method for extracting cellular signals from microendoscopic data ; such methods are in very high\ndemand in the neuroscience community .\n136- our method shows credible performances in recovering the real neuronal signals and outperforms the previous standard pca - ica method .\nBERTSUMEXT (SW)\n0- monitoring the activity of large - scale neuronal ensembles during complex behavioral states is fundamental to neuroscience research\n11- our work is based on a matrix factorization approach , which can simultaneously segment cells and estimate changes in ﬂuorescence in the\ntemporal domain .\n26- the video data we have are observations from the optical ﬁeld for a total number of @xmath2 frames .\n64-\nwe estimate the temporal component of one neuron @xmath15 from spatially ﬁltered data and then use it to extract the corresponding\nspatial footprint @xmath14 from the raw data . in the step of estimating @xmath14 , we re - order all frames to make nearby frames share\nthe similar local background levels and then take the temporal differencing to remove the background signals temporally .\n105- we also display @xmath98 tightly clustered neurons in the simulated data ( ﬁgure [ ﬁg : sim]e ) to demonstrate that our cnmf - e approach\ncan accurately detect and demix their activity ( ﬁgure [ ﬁg : sim]g ) .\n107- in contrast , pca - ica based detection can only detect two neurons and the calcium traces have high level of noise .\n1810\nGOLD\nstatistical learning theory chieﬂy studies restricted hypothesis classes , particularly those with ﬁnite vapnik - chervonenkis ( vc ) dimension\n.\nthe fundamental quantity of interest is the sample complexity : the number of samples required to learn to a speciﬁed level of accuracy .\nhere we consider learning over the set of all computable labeling functions .\nsince the vc - dimension is inﬁnite and a priori ( uniform ) bounds on the number of samples are impossible , we let the learning algorithm\ndecide when it has seen sufﬁcient samples to have learned . we ﬁrst show that learning in this setting is indeed possible , and develop a\nlearning algorithm .\nwe then show , however , that bounding sample complexity independently of the distribution is impossible .\nnotably , this impossibility is entirely due to the requirement that the learning algorithm be computable , and not due to the statistical nature\nof the problem .\nGBT-EXTSUM\n6- an alternative approach , and one we follow in this paper , is simply to consider a single learning model that includes all possible classiﬁca-\ntion methods .\n8- since the vc - dimension is clearly inﬁnite , there are no uniform bounds ( independent of the distribution and the target concept ) on the\nnumber of samples needed to learn accurately @xcite .\n10-\n, it is natural to allow the learning algorithm to decide when it has seen sufﬁciently many labeled samples based on the training samples\nseen up to now and their labels . since the above learning model includes any practical classiﬁcation scheme , we term it universal ( pac- )\nlearning .\n11- we ﬁrst show that there is a computable learning algorithm in our universal setting .\n19- our results imply that computable learning algorithms in the universal setting must waste samples ” in the sense of requiring more samples\nthan is necessary for statistical reasons alone .\n81- then we will contrast this to the case of an uncomputable learning algorithm .\nBERTSUMEXT (SW)50-\n( semantic requirements ) for any @xmath27 , for any concept @xmath8 , and distribution @xmath9 over @xmath2 , if the oracle returns\npairs @xmath28 for @xmath29 drawn iid from @xmath9 , then @xmath0 always halts , and with probability at least @xmath12 outputs a\nhypothesis @xmath13 such that @xmath30 ¡ {\nvarepsilon}$ ]\n64- suppose @xmath36 is an inﬁnite sequence of iid samples drawn from @xmath9 .\n75- the learning algorithm queries the oracle as necessary for new learning samples and their labeling .\n78- note that it seems necessary to expand the hypothesis space to include all partial recursive functions because the concept space of total\nrecursive functions does not have a recursive enumeration ( it is uncomputable whether a given program is total recursive or not ) .\n79- we will see in theorem [ thm : nobound ] that there is no bound @xmath55 on the number of samples queried by any computable learning\nalgorithm in our setting .\n80- let us obtain some intuition for why that is true for the above learning algorithm .\nGOLD\nin this paper , we propose majority voting neural networks for sparse signal recovery in binary compressed sensing .\nthe majority voting neural network is composed of several independently trained feedforward neural networks employing the sigmoid\nfunction as an activation function .\nour empirical study shows that a choice of a loss function used in training processes for the network is of prime importance .\nwe found a loss function suitable for sparse signal recovery , which includes a cross entropy - like term and an @xmath0 regularized term .\nfrom the experimental results\n, we observed that the majority voting neural network achieves excellent recovery performance , which is approaching the optimal perfor-\nmance as the number of component nets grows .\nthe simple architecture of the majority voting neural networks would be beneﬁcial for both software and hardware implementations .\nGBT-EXTSUM\n40- requires only several matrix - vector products to obtain an output signal , which is an estimate signal of the sparse vector @xmath12 .\n48- the signal propagates from left to right and the output signal @xmath17 eventually comes out from the output layer . the network should\nbe trained so that the output signal @xmath17 is an accurate estimation of the original sparse signal @xmath12 .\n168- in this paper , we proposed sparse signal recovery schemes based on neural networks for binary compressed sensing .\n169- our empirical study shows a choice of the loss function used for training neural networks is of prime importance to achieve excellent\nreconstruction performance .\n170- we found a loss function suitable for this purpose , which includes a cross entropy like term and an @xmath0 regularized term .\n173- the simple architecture of the majority voting neural network would be beneﬁcial for both software and hardware implementation .\nBERTSUMEXT (SW)\n19- the paper @xcite presents binary iterative hard thresholding ( biht ) algorithm by reforming iterative hard thresholding ( iht ) algorithm\n@xcite .\n20- although the known sparse recovery algorithms exhibit reasonable sparse recovery performance , it may not be suitable for applications in\nhigh speed wireless communications .\n48- the signal propagates from left to right and the output signal @xmath17 eventually comes out from the output layer . the network should\nbe trained so that the output signal @xmath17 is an accurate estimation of the original sparse signal @xmath12 .\n137-\nthe outputs from these neural network are combined by soft majority voting nodes and the ﬁnal estimation vector is obtained by rounding\nthe output from the soft majority voting nodes . combining a several neural networks to obtain improved performance is not a novel idea ,\ne.g. , @xcite , but it will be shown that the idea is very effective for our purpose . from statistics of reconstruction errors occurred in our\ncomputer experiments , we observed that many reconstruction error events ( i.e. , @xmath97 ) occur due to only one symbol mismatch .\n149- note that implementation of neural networks with fpga is recently becoming a hot research topic @xcite .\n151-\nthe length of the sparse signal is set to @xmath59 and the sparseness parameter is set to @xmath110 . ) , width=317 ] from ﬁg.[ﬁg\n: rr and m k6 ] , we can observe signiﬁcant improvement in recovery performance compared with the performance of the single neural\nnetwork . a single feedforward neural network discussed in the previous section",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9814281463623047
    },
    {
      "name": "Computer science",
      "score": 0.8373628258705139
    },
    {
      "name": "Transformer",
      "score": 0.7987555265426636
    },
    {
      "name": "Natural language processing",
      "score": 0.6269598603248596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5352191925048828
    },
    {
      "name": "Citation",
      "score": 0.5292690992355347
    },
    {
      "name": "Multi-document summarization",
      "score": 0.4730215072631836
    },
    {
      "name": "Language model",
      "score": 0.4715932607650757
    },
    {
      "name": "Information retrieval",
      "score": 0.4551108777523041
    },
    {
      "name": "Natural language",
      "score": 0.42139533162117004
    },
    {
      "name": "World Wide Web",
      "score": 0.1631879210472107
    },
    {
      "name": "Voltage",
      "score": 0.10087496042251587
    },
    {
      "name": "Engineering",
      "score": 0.0733206570148468
    },
    {
      "name": "Electrical engineering",
      "score": 0.06988218426704407
    }
  ],
  "institutions": []
}