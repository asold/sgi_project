{
    "title": "Target-driven Attack for Large Language Models",
    "url": "https://openalex.org/W4403487732",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2106046045",
            "name": "Chong Zhang",
            "affiliations": [
                "Xi’an Jiaotong-Liverpool University"
            ]
        },
        {
            "id": "https://openalex.org/A2103246445",
            "name": "Mingyu Jin",
            "affiliations": [
                "Rutgers Sexual and Reproductive Health and Rights"
            ]
        },
        {
            "id": "https://openalex.org/A2131509014",
            "name": "Dong Shu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2226116766",
            "name": "Taowen Wang",
            "affiliations": [
                "Rochester Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2109573734",
            "name": "Dong-fang Liu",
            "affiliations": [
                "Rochester Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2108239263",
            "name": "Xiaobo Jin",
            "affiliations": [
                "Xi’an Jiaotong-Liverpool University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3101449015",
        "https://openalex.org/W3097375194",
        "https://openalex.org/W3170403598",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W4387806231",
        "https://openalex.org/W4379958452",
        "https://openalex.org/W4387075795",
        "https://openalex.org/W4225858632",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W4386557007",
        "https://openalex.org/W2799007037",
        "https://openalex.org/W2964120615",
        "https://openalex.org/W3105604018",
        "https://openalex.org/W4385714464",
        "https://openalex.org/W2799194071",
        "https://openalex.org/W4388650587",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W4312643954",
        "https://openalex.org/W2963859254",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3134642945",
        "https://openalex.org/W4287210714",
        "https://openalex.org/W4380353722",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W4287393336",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4381245716",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W4309395891",
        "https://openalex.org/W4323709479"
    ],
    "abstract": "Current large language models (LLM) provide a strong foundation for large-scale user-oriented natural language tasks. Many users can easily inject adversarial text or instructions through the user interface, thus causing LLM model security challenges like the language model not giving the correct answer. Although there is currently a large amount of research on black-box attacks, most of these black-box attacks use random and heuristic strategies. It is unclear how these strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we propose our target-driven black-box attack method to maximize the KL divergence between the conditional probabilities of the clean text and the attack text to redefine the attack’s goal. We transform the distance maximization problem into two convex optimization problems based on the attack goal to solve the attack text and estimate the covariance. Furthermore, the projected gradient descent algorithm solves the vector corresponding to the attack text. Our target-driven black-box attack approach includes two attack strategies: token manipulation and misinformation attack. Experimental results on multiple Large Language Models and datasets demonstrate the effectiveness of our attack method.",
    "full_text": null
}