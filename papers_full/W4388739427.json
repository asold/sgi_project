{
  "title": "A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions",
  "url": "https://openalex.org/W4388739427",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5072048690",
      "name": "Luca Barbieri",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5071150969",
      "name": "Mattia Brambilla",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5042936635",
      "name": "Mario Stefanutti",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015089857",
      "name": "Ciro Romano",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5093276814",
      "name": "Niccolò De Carlo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5035547226",
      "name": "Manuel Roveri",
      "affiliations": [
        "Politecnico di Milano"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2811266402",
    "https://openalex.org/W2058401212",
    "https://openalex.org/W2587696068",
    "https://openalex.org/W2609848772",
    "https://openalex.org/W4283451731",
    "https://openalex.org/W3195104125",
    "https://openalex.org/W2903023342",
    "https://openalex.org/W3004207920",
    "https://openalex.org/W2565025742",
    "https://openalex.org/W2345132057",
    "https://openalex.org/W3048576163",
    "https://openalex.org/W2122646361",
    "https://openalex.org/W2030553727",
    "https://openalex.org/W2031366272",
    "https://openalex.org/W2134490011",
    "https://openalex.org/W1976331960",
    "https://openalex.org/W2129468158",
    "https://openalex.org/W2100537916",
    "https://openalex.org/W2296719434",
    "https://openalex.org/W2786535488",
    "https://openalex.org/W3117098906",
    "https://openalex.org/W2983029853",
    "https://openalex.org/W4256177618",
    "https://openalex.org/W2909745252",
    "https://openalex.org/W2107528096",
    "https://openalex.org/W3164952570",
    "https://openalex.org/W2883778034",
    "https://openalex.org/W2950361482",
    "https://openalex.org/W3178367256",
    "https://openalex.org/W4285299229",
    "https://openalex.org/W4385763767",
    "https://openalex.org/W4283318673",
    "https://openalex.org/W3040266635",
    "https://openalex.org/W4321020991",
    "https://openalex.org/W2963166639",
    "https://openalex.org/W3170937175",
    "https://openalex.org/W6784869275",
    "https://openalex.org/W2965433388",
    "https://openalex.org/W2911200746",
    "https://openalex.org/W3184127157",
    "https://openalex.org/W4290927906",
    "https://openalex.org/W4382360125",
    "https://openalex.org/W3215290057",
    "https://openalex.org/W4293577731",
    "https://openalex.org/W3216549012",
    "https://openalex.org/W4210263262",
    "https://openalex.org/W6802061597",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4205952419",
    "https://openalex.org/W6783236829",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W6760732026",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W4386566583",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6753640285",
    "https://openalex.org/W6764409202",
    "https://openalex.org/W3103522166",
    "https://openalex.org/W4383751021",
    "https://openalex.org/W3175915085",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2113107995",
    "https://openalex.org/W2490662969",
    "https://openalex.org/W2786827964",
    "https://openalex.org/W3170981104",
    "https://openalex.org/W2785362611",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W1970088130",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3106543020",
    "https://openalex.org/W3135550350",
    "https://openalex.org/W3098957257",
    "https://openalex.org/W2924902521"
  ],
  "abstract": "The widespread proliferation of Internet of Things (IoT) devices has pushed for the development of novel transformer-based Anomaly Detection (AD) tools for an accurate monitoring of functionalities in industrial systems. Despite their outstanding performances, transformer models often rely on large Neural Networks (NNs) that are difficult to be executed by IoT devices due to their energy/computing constraints. This paper focuses on introducing tiny transformer-based AD tools to make them viable solutions for on-device AD. Starting from the state-of-the-art Anomaly Transformer (AT) model, which has been shown to provide accurate AD functionalities but it is characterized by high computational and memory demand, we propose a tiny AD framework that finds an optimized configuration of the AT model and uses it for devising a compressed version compatible with resource-constrained IoT systems. A knowledge distillation tool is developed to obtain a highly compressed AT model without degrading the AD performance. The proposed framework is firstly analyzed on four widely-adopted AD datasets and then assessed using data extracted from a real-world monitoring facility. The results show that the tiny AD tool provides a compressed AT model with a staggering 99.93% reduction in the number of trainable parameters compared to the original implementation (from 4.8 million to 3300 or 1400 according to the input dataset), without significantly compromising the accuracy in AD. Moreover, the compressed model substantially outperforms a popular Recurrent Neural Network (RNN)-based AD tool having a similar number of trainable weights as well as a conventional One-Class Support Vector Machine (OCSVM) algorithm.",
  "full_text": "Received 24 November 2021; accepted 15 February 2022. Date of publication 4 March 2022;\ndate of current version 18 April 2022. The review of this article was arranged by Associate Editor Eric L. Miller.\nDigital Object Identiﬁer 10.1109/OJSP .2022.3154684\nCooperative Localization and Multitarget\nTracking in Agent Networks with the\nSum-Product Algorithm\nMATTIA BRAMBILLA 1 (Member, IEEE), DOMENICO GAGLIONE 3, GIOVANNI SOLDI 3,\nRICO MENDRZIK 4, GABRIELE FERRI 3 (Senior Member, IEEE), KEVIN D. LEPAGE 3,\nMONICA NICOLI 2 (Senior Member, IEEE), PETER WILLETT 5 (Fellow, IEEE),\nPAOLO BRACA 3 (Senior Member, IEEE), AND MOE Z. WIN 6 (Fellow, IEEE)\n1Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, 20133 Milan, Italy\n2Dipartimento di Ingegneria Gestionale (DIG), Politecnico di Milano, 20156 Milan, Italy\n3NATO STO Centre for Maritime Research and Experimentation (CMRE), 19126 La Spezia, Italy\n4Ibeo Automotive Systems GmbH, 22143 Hamburg, Germany\n5University of Connecticut, Storrs, CT 06269 USA\n6Laboratory for Information and Decision Systems (LIDS), Massachusetts Institute of Technology (MIT), Cambridge, MA 02139 USA\nCORRESPONDING AUTHOR: MATTIA BRAMBILLA (e-mail: mattia.brambilla@polimi.it)\nThis work was supported in part by the NATO Allied Command Transformation (ACT) under the DKOE Project and in part by the U.S. Army Research Ofﬁce\nthrough the MIT Institute for Soldier Nanotechnologies under Contract W911NF-13-D-0001. Parts of this paper were previously presented at ICASSP 2020,\nBarcelona, Spain, May 2020.\nABSTRACT This paper addresses the problem of multitarget tracking using a network of sensing agents\nwith unknown positions. Agents have to both localize themselves in the sensor network and, at the same\ntime, perform multitarget tracking in the presence of clutter and miss detection. These two problems are\njointly resolved using a holistic and centralized approach where graph theory is used to describe the statistical\nrelationships among agent states, target states, and observations. A scalable message passing scheme, based\non the sum-product algorithm, enables to efﬁciently approximate the marginal posterior distributions of both\nagent and target states. The proposed method is general enough to accommodate a full multistatic network\nconﬁguration, with multiple transmitters and receivers. Numerical simulations show superior performance\nof the proposed joint approach with respect to the case in which cooperative self-localization and multitarget\ntracking are performed separately, as the former manages to extract valuable information from targets. Lastly,\ndata acquired in 2018 by the NATO Science and Technology Organization (STO) Centre for Maritime\nResearch and Experimentation (CMRE) through a network of autonomous underwater vehicles demonstrates\nthe effectiveness of the approach in a practical application.\nINDEX TERMS Belief propagation, factor graph, maritime surveillance, message passing, probabilistic data\nassociation.\nI. INTRODUCTION\nA. BACKGROUND AND MOTIVATION\nDetecting unknown targets, understanding their intentions,\nand taking reactive countermeasures are common tasks in\nsituational awareness (SA) applications [1]–[7]. Depending\non the speciﬁc use case, different types of sensors (acoustic,\nradio frequency, optical, etc. [8]) may be used to sense the\nenvironment and provide the desired information. Most of\nSA applications use multiple cooperative sensors, rather than\na single one, to infer the presence and kinematics of targets.\nIndeed, cooperation dramatically increases the perception\ncapabilities of an SA system, as it relies on a larger dataset of\nobservations (or measurements) of the targets [9]–[13].\nExamples can be found in several domains such as\nunderwater surveillance networks [14]–[17], connected\nvehicles [18]–[20], and Internet of Things (IoT) [21]–[24].\nMobility of sensors can further improve the performance of\ntarget detection and localization by fusing spatial sensing\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME 3, 2022 169\nReceived XX Month, XXXX; revised XX Month, XXXX; accepted XX Month, XXXX; Date of publication XX Month, XXXX; date of\ncurrent version XX Month, XXXX.\nDigital Object Identifier 10.1109/XXXX.2023.1234567\nA Tiny Transformer-Based Anomaly\nDetection Framework for IoT Solutions\nLUCA BARBIERI1 (Member, IEEE), MATTIA BRAMBILLA1 (Member, IEEE),\nMARIO STEFANUTTI2, CIRO ROMANO2, NICCOL `O DE CARLO2,\nMANUEL ROVERI1 (Senior Member, IEEE)\n1Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, 20133 Milan, Italy\n2Sensoworks, 00192 Rome, Italy\nLuca Barbieri (email: luca1.barbieri@polimi.it)\nABSTRACT The widespread proliferation of Internet of Things (IoT) devices has pushed for the\ndevelopment of novel transformer-based Anomaly Detection (AD) tools for an accurate monitoring of\nfunctionalities in industrial systems. Despite their outstanding performances, transformer models often\nrely on large Neural Networks (NNs) that are difficult to be executed by IoT devices due to their\nenergy/computing constraints. This paper focuses on introducing tiny transformer-based AD tools to make\nthem viable solutions for on-device AD. Starting from the state-of-the-art Anomaly Transformer (AT)\nmodel, which has been shown to provide accurate AD functionalities but it is characterized by high\ncomputational and memory demand, we propose a tiny AD framework that finds an optimized configuration\nof the AT model and uses it for devising a compressed version compatible with resource-constrained IoT\nsystems. A knowledge distillation tool is developed to obtain a highly compressed AT model without\ndegrading the AD performance. The proposed framework is firstly analyzed on four widely-adopted AD\ndatasets and then assessed using data extracted from a real-world monitoring facility. The results show that\nthe tiny AD tool provides a compressed AT model with a staggering 99.93% reduction in the number of\ntrainable parameters compared to the original implementation (from 4.8 million to 3300 or 1400 according\nto the input dataset), without significantly compromising the accuracy in AD. Moreover, the compressed\nmodel substantially outperforms a popular Recurrent Neural Network (RNN)-based AD tool having a\nsimilar number of trainable weights as well as a conventional One-Class Support Vector Machine (OCSVM)\nalgorithm.\nINDEX TERMS Anomaly detection, machine learning, self-attention, knowledge distillation, Internet of\nThings, transformer, compression\nI. INTRODUCTION\nT\nHE recent integration of Internet of Things (IoT) sensor\nnetworks within everyday applications has enabled the\ncollection of large volumes of time series data, fostering\nthe development of accurate monitoring and intelligent con-\ntrol systems [1]–[4]. This is not only limited to industrial\nIoT setups but applies to vehicular systems as well [5],\n[6]. Regardless of the technological implementations and\nsystem architectures, a key task of IoT networks is to\nacquire data for monitoring and raising alarms or alerts\nwhen needed [7]. In this context, Anomaly Detection (AD)\ntools are fundamental to discover unusual or anomalous\npatterns as well as abrupt changes in data, possibly indicating\nfailures or malfunctions in the system being monitored [8]–\n[10]. Ideally, AD tools should also operate in real-time so\nas to provide up-to-date information, rapidly forward alert\nmessages, and consequently allow a reaction to anomalous\nevents in due time. Besides, all these functionalities should\nbe general enough to be seamlessly applied to multiple IoT\nscenarios [11].\nOn one hand, given the ever-increasing generation and ac-\nquisition of time-series data from IoT devices, conventional\nAD procedures based on historical and statistical analysis\nmay reveal to be inappropriate due to their underlying\nlimiting statistical and modeling assumptions [12]. On the\nother hand, Machine Learning (ML) techniques are being\nincreasingly used to learn descriptive relations from the\ncollected data, or even hidden relationships among them, that\nfacilitate the AD process. Well-known ML-based AD tools\nrely on Support Vector Machines (SVMs) and Decision Trees\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME , 1\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\n(DTs) algorithms that aim at detecting anomalies by building\na specific classifier [13]–[15] or by representing the time-\nseries data in a tree structure [16]–[18], respectively. Other\nwidely-used techniques include Isolation Forest (IF) [19],\n[20], Local Outlier Factor (LOF) [21], [22], and K-Nearest\nNeighbor (K-NN) [23], [24].\nMore recently, Neural Networks (NNs) have been increas-\ningly applied for solving AD tasks due to their outstanding\nability of capturing highly non-linear and complex rela-\ntionships from data, consequently facilitating the discovery\nof anomalies [25], [26]. Common architectures employed\nin this context include Convolutional Neural Networks\n(CNNs) [27], Recurrent Neural Networks (RNNs) [28],\nGraph Neural Networks (GNNs) [29], [30], transform-\ners [31], [32], or any combination thereof (for some ex-\namples we refer to [33]). These techniques typically target\nreconstructing the input time series at the output and discern\nthe anomalies based on the reconstruction error. Indeed,\nnormal data points should be reconstructed quite well while\nanomalous time series should lead to high reconstruction\nerrors at the output indicating a possible anomaly. Be-\nsides selecting a suitable ML model for the considered\nAD problem, another important aspect to take into account\nis the learning paradigm under which the models should\nbe trained. Supervised, semi-supervised, and unsupervised\nparadigms are the most utilized for AD [26]. In this paper,\nwe specifically focus on unsupervised methods as they are\nable to automatically discern anomalies without any external\nsupervision or labeled data [26], [34].\nBesides selecting a suitable ML model and learning\nparadigm, the integration of data-driven AD strategies into\nIoT setups faces additional challenges. Indeed, IoT devices\nare typically characterized by low power consumption and\nlimited computational capabilities, preventing the adoption\nof large ML models. Therefore, to harness the excellent\nperformances of ML-based AD tools in IoT systems, novel\nstrategies should be developed to optimize and/or compress\nthe NNs so that they can be executed on resource-constrained\ndevices. This paper tries to move in this direction by de-\nveloping a tiny AD framework providing highly accurate\nlearning-based AD strategies compatible with the (limited)\ncomputational and memory resources of IoT devices without\nsacrificing the AD capabilities.\nII. RELATED WORKS AND CONTRIBUTIONS\nThis section discusses the related works and details the main\ncontributions of the paper. We start by reviewing prior art on\nML-based unsupervised AD methods (Sec. A), followed by\na discussion on the compression strategies adopted to reduce\nthe computational/memory complexity of large transformer\nmodels (Sec. B). Lastly, Sec. C highlights the main contri-\nbutions of the paper.\nA. UNSUPERVISED ANOMALY DETECTION\nCommon approaches targeted at solving the AD task in\na fully unsupervised manner rely on RNNs that learn the\ntemporal dependency across multi-dimensional time series.\nFor example, the authors of [35] propose a Long Short Term\nMemory (LSTM)-based Variational AutoEncoder (V AE) to\nreconstruct time series and learn their posterior distribu-\ntions. Authors in [28] develop OmniAnomaly, a stochastic\nRNN framework that aims at producing accurate anomaly\nscores based on reconstruction probabilities. Similarly, In-\nterFusion [36] proposes a hierarchical V AE to faithfully\nmodel the relationships among the time series and exploit\ntheir representation to perform AD, whereas a temporal\none-class classification model introducing dilated RNNs\nis designed and proposed in [37]. Other ML approaches\nemploy Generative Adversarial Networks (GANs), where\nadversarially-generated time series are used to improve the\ndiscovery of anomalies [38], or a fusion of GANs with\nLSTMs [39]. More recently, new techniques based on the\ntransformer architecture have also been introduced thanks to\nthe increasing traction of the self-attention mechanism [32],\n[40]–[47]. Specifically, in [40] the authors propose a graph\nlearning with transformer for anomaly detection (GTA) that\njointly learns a graph structure and models the temporal\ndependencies of the time series through a transformer-based\nmodule, whereas TranAD, introduced in [32], improves the\naccuracy of AD while reducing training times. In [41], the\nauthors propose a root-square sparse transformer together\nwith a dynamically-adjusted learning strategy to address\nconcept drift in AD setups, while [42] develops ITran\nwhich incorporates knowledge about inductive biases to\nmake the solution effective even when a relatively small\namount of training data is available. On the other hand,\n[43] combines GANs with dilated convolutions to improve\nthe generalization capability of the developed transformer-\nbased AD tool. Other strategies instead focus on combining\ntransformers with V AE to provide more robust AD methods\n[45], [46]. Lastly, the Anomaly Transformer (AT) introduces\na novel association discrepancy metric and redesigns the\nself-attention mechanisms to work directly on time series\ndata [47]. Despite their competitiveness, transformers entail\nan excessive number of trainable parameters posing a major\nchallenge for their implementation on IoT devices which are\ntypically characterized by reduced memory and computa-\ntional abilities.\nB. COMPRESSION STRATEGIES FOR TRANSFORMERS\nTo reduce the computational burden of large trans-\nformer models, compression strategies have been developed\nthroughout the years [48], [49]. Most common approaches\nrely on quantization and pruning operations applied to the\nML model, where the former aims at representing the NN\nweights by using a lower number of bits [50], while the\nlatter removes redundant model parameters, possibly taking\ninto account also the structure of the multi-headed self-\n2 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nattention mechanism [51]. Another approach for transformer\ncompression is knowledge distillation, where a large pre-\ntrained model (called teacher) is used as a reference for train-\ning a much smaller model (called student) [52]. Knowledge\ndistillation strategies are typically characterized based on the\nnumber of teachers and/or students considered during the\ndistillation process [53], [54] or based on what information\nis distilled (e.g., the intermediate outputs, soft labels, and\nso forth) [55], [56]. Besides the aforementioned strategies,\nweight sharing can be also utilized to reduce transformer\ncomplexity. Under these methods, some trainable parameters\nare shared between different layers to reduce model com-\nplexity [57], [58]. Finally, methods based on matrix decom-\nposition (see e.g., [59], [60]) have been used to factorize\nlarge weight matrices into smaller representations. All these\ntechniques have shown remarkable compression capabilities\nspecifically for Natural Language Processing (NLP) tasks\nwhere encoder-decoder architectures are largely utilized.\nHowever, their use is largely underexplored when it comes\nto AD tasks. Based on these considerations, in the paper\nwe propose novel solutions for obtaining highly compressed\ntransformer-based AD tools specifically formulated for time-\nseries data. Note that other methods (see e.g., [61] for\na review) have been developed to obtain data-driven AD\nstrategies for time-series data compatible with resource-\nconstrained devices. Nevertheless, to the best of our knowl-\nedge, this is the first work that considers transformer models\nwhich poses additional challenges due to their extremely\nlarge model footprint.\nC. CONTRIBUTIONS\nIn this paper, we focus on providing highly compressed\ntransformer-based AD algorithms that can be employed\nin resource-constrained IoT setups. The goal is to obtain\nlightweight ML models that can support highly accurate AD\nfunctionalities over heterogeneous and complex time series\ndata. To this end, we propose a tiny AD framework that is\nresponsible for optimizing a large AD tool based on the self-\nattention mechanism, namely AT, and use it for producing\na substantially compressed version able to be executed on\nembedded devices. After optimizing the large AT model, a\nknowledge distillation policy is developed where the opti-\nmized AT algorithm is used to obtain a lighter student ML\narchitecture characterized by a substantially lower number\nof trainable parameters. This process is done through dis-\ntillation by matching the representations provided by the\nlarge teacher model with the ones attained by the smaller\nstudent model. Overall, the developed tiny AD framework is\nshown to produce transformer-based AD models supporting\nhighly-accurate AD functionalities while requiring minor\nmodifications compared to the original training process of\nAT, allowing for straightforward implementations. To sum-\nmarize, the detailed contributions are as follows:\n• we propose a tiny AD framework. The framework is\nresponsible for optimizing large AT models and using\nthem to produce highly compressed AD tools that can\nbe integrated into resource-constrained devices;\n• we develop a knowledge distillation strategy for com-\npressing AT and making it suitable for on-device AD.\nThe proposed distillation tool is general enough to\nbe applied even when the student and teacher have\na different number of layers and self-attention map\ndimensions without increasing the number of trainable\nparameters;\n• we extensively validate and compare the performances\nof the model obtained by the framework over AD\ndatasets used in the literature as well as using time-\nseries data collected in a real-world bridge infrastruc-\nture monitoring use case;\n• we compare the compressed model produced after\napplying the developed framework with a conventional\nOne-Class Support Vector Machine (OCSVM) algo-\nrithm as well as a state-of-the-art RNN AD tool.\nFor a fair comparison, the RNN-based AD strategy is\nconfigured so as to have roughly the same number of\ntrainable weights of the compressed model.\nExperimental results show that the proposed technique is\nable to provide a substantially compressed AT model, with\na remarkable 99.93% less trainable parameters compared\nto the original implementation, with negligible performance\nloss. Indeed, for the considered AD datasets, the F1 score\nobtained by the distilled model is slightly lower (less than\n2%) with respect to the original AT method. Similarly,\nthe F1 score obtained by the original AT and the distilled\nversion using the real-world monitoring time series data is\nnearly identical. The resulting model obtained after applying\nthe proposed AD frameworks enables a highly-accurate\ndiscovery of anomalies and outperforms the state-of-the-\nart RNN AD method when the two networks have similar\nnumber of trainable weights. Numerical results also show\nthat the compressed AT model substantially outperforms the\nconventional OCSVM AD strategy. Lastly, the analysis indi-\ncates that the distilled model is suitable for integration into\nresource-constrained environments, such as IoT or embedded\nsystems, thanks to the reduced model footprint, i.e., number\nof parameters [62].\nThe remainder of this paper is organized as follows.\nSection III reviews the AT model and its training process.\nSection IV details the proposed tiny AD framework. Sec-\ntion V highlights the numerical results characterizing the\ndistilled AT model using 4 widely-adopted AD datasets,\nwhile Section VI concentrates on the assessment of the\nproposed technique considering a real-world AD scenario.\nFinally, Section VII draws some conclusions.\nIII. TRANSFORMER-BASED ANOMALY DETECTION\nThis section briefly describes the AT method. At first,\nwe detail the model architecture (depicted in Figure 1)\ntogether with the multi-head anomaly-attention mechanism\n(Section A). Then, we present the learning strategy used\nVOLUME , 3\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nEm bedding module Anomaly attention La yer normalization F ully connected layer\nX\nX(0)\nX(1)\nX(ℓ−1)\nX(ℓ)\nX(L−1) X(L)\n1st\nlayer ℓ layer L layer\n1\nFIGURE 1. Anomaly Transformer architecture: the input X is sequentially processed by each layer of the architecture to extract rich features for AD by\nlearning to reconstruct the time series at the output. The anomaly-attention module is responsible for learning the prior and series association to\nfacilitate the discovery of anomalous data [47].\nfor optimizing the model parameters and the corresponding\ninference stage (Section B).\nA. ARCHITECTURE AND MULTI-HEAD\nANOMALY-ATTENTION MECHANISM\nGiven a d-dimensional time-series X ∈ RN×d of length\nN, the AT performs AD by reconstructing the original time\nseries at the output. The AT architecture is composed by L\nlayers, where the outputs X(ℓ) at layer ℓ, with 1 ≤ ℓ ≤ L,\nare computed as\nX(ℓ) = fLN\n\u0010\nZ(ℓ)WZ + Z(ℓ)\n\u0011\n, (1)\nbeing fLN(·) the processing done by a layer normalization\noperation and WZ are the weights associated to a fully con-\nnected layer that operates on the intermediate representation\nZ(ℓ) of layer ℓ, which is defined as\nZ(ℓ) = fLN\n\u0010\nfAT(X(ℓ−1)) +X(ℓ−1)\n\u0011\n, (2)\nwhere X(ℓ−1) are the outputs of layer ℓ−1 and fAT(·) denotes\nthe processing done by the anomaly-attention mechanism.\nNote that for ℓ = 0, the input time-series X is processed\nby an embedding function that initially converts X into a\nsequence of tokens and then sums the results to the output\ngiven by a positional encoding function to obtain the input\nsequence X(0) ∈ RN×dm , being dm the dimension of\nthe self-attention map, similarly to what is carried out in\ntraditional self-attention procedures [63]. On the other hand,\nfor ℓ = L we have X(L) = XR, i.e., the output of the model\nis the reconstructed time series XR of the input X.\nTo improve the accuracy of the architecture, a multi-head\nanomaly-attention mechanism is proposed to learn a robust\nnormal-abnormal association criteria. More specifically, two\nadditional learnable rules are introduced, namely the prior\nand series association using self-attention. The former is\ndesigned to learn the relative temporal distance of the time\nseries samples, while the latter learns the association across\ndifferent time series.\nThe multi-head anomaly-attention module is depicted in\nFigure 2 and works as follows. At first, the output X(ℓ−1)\nof the (ℓ − 1)-th layer is reorganized into disjoint matrices\n{X(ℓ−1)\nh ∈ RN×(dm/Nh)}Nh\nh=1. Each matrix X(ℓ−1)\nh is then\nfed to four fully connected layers to obtain queries, keys,\nKernel\nfitting\nRescaling\nScale\n&\nSoftmax\nX(ℓ−1)\nh\nΣ(ℓ)\nh\nP(ℓ)\nK(ℓ)\nh\nQ(ℓ)\nh\nS(ℓ)\nV(ℓ)\nh ˆZ(ℓ)\nh\nLtot,min Ltot,max\n1\nFIGURE 2. Multi-headed anomaly attention module at layer ℓ and for the\nh-th head. The outputs consist in the prior P(ℓ) and series S(ℓ)\nassociation matrices together with the intermediate representation of the\ntime-series bZ(ℓ).\nvalues, and scale matrices, denoted with Q(ℓ)\nh , K(ℓ)\nh , V(ℓ)\nh ∈\nRN×(dm/Nh) and Σ(ℓ)\nh ∈ RN×Nh , respectively, and defined\nas\nQ(ℓ)\nh = X(ℓ−1)\nh W(ℓ)\nQ , (3)\nK(ℓ)\nh = X(ℓ−1)\nh W(ℓ)\nK , (4)\nV(ℓ)\nh = X(ℓ−1)\nh W(ℓ)\nV , (5)\nΣ(ℓ)\nh = X(ℓ−1)\nh W(ℓ)\nΣ , (6)\nbeing W(ℓ)\nQ , W(ℓ)\nK , W(ℓ)\nV ∈ R(dm/Nh)×(dm/Nh) and W(ℓ)\nΣ ∈\nR(dm/Nh)×Nh the associated learnable weights. Next, the\nqueries and keys matrices for the h-th head are used to\ncompute the series association matrix S(ℓ) ∈ RN×N as\nS(ℓ) = Softmax\n \nQ(ℓ)\nh K(ℓ)T\nh√dm\n!\n, (7)\nwhile the entries of the prior association matrix P(ℓ) ∈\nRN×N are evaluated by fitting a Gaussian kernel to the\npairwise distances among the indices of the time series as\nfollows\n[P(ℓ)]i,j = 1q\n2πσ2\ni,ℓ,h\nexp\n \n−(i − j)2\n2σ2\ni,ℓ,h\n!\n. (8)\n4 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTo obtain a proper distribution, P(ℓ) is normalized, i.e.,\nP(ℓ) = P(ℓ) ⊘ (P(ℓ)1N ) , (9)\nwhere symbol ⊘ denotes the row-wise division. Lastly, the\nintermediate representation of the reconstructed time series\nfor the h-th head bZ(ℓ)\nh is computed as\nbZ(ℓ)\nh = S(ℓ)V(ℓ)\nh . (10)\nThis process is repeated for all Nh heads and the results\nare concatenated together to obtain the final intermediate\nrepresentation for the ℓ-th layer\nbZ(ℓ) =\nh\nbZ(ℓ)\n1 ··· bZ(ℓ)\nNh\ni\n. (11)\nB. OPTIMIZATION STRATEGY AND INFERENCE\nPROCESS\nThe main goal of the AT model is to reconstruct the time\nseries at the output by minimizing the reconstruction loss\nLrec = ∥X − XR∥2 , (12)\nwhere XR is the models’ output. Additionally, a sym-\nmetrized Kullback-Leibler (KL) divergence [64], represent-\ning an association discrepancy, is used to learn a robust\nnormal-abnormal discerning rule by optimizing over the\nfollowing loss term\nLsKL = 1\nL\nLX\nℓ=1\n\u0010\nKL(P(ℓ)∥S(ℓ)) +KL(S(ℓ)∥P(ℓ))\n\u0011\n, (13)\nwhere KL (·∥·) denotes the KL divergence between two\ndiscrete distributions. Note that LsKL is computed separately\nfor each row of the matrices P(ℓ) and S(ℓ) as each row is\nassumed to model a separate distribution. The total loss is\nthen evaluated as\nLtot,min = Lrec − λLsKL , (14)\nwhich is used to updated the NN weights.\nUsing only (14) for training the model makes the prior\nassociation not useful for discerning anomalies. Indeed, the\nmaximization of the association discrepancy in (14) leads\nto Gaussian kernels in (8) with extremely reduced standard\ndeviation [65]. To overcome this problem, AT introduces a\nmin-max learning strategy, where the prior association is\ninitially optimized to be as close as possible to the series\nassociation by minimizing (14). During this phase, the series\nassociation is kept constant and not backpropagated. Then,\nthe series association is updated so that the association\ndiscrepancy in (13) is maximized, leading to an higher ability\nof the model to recognize anomalous patterns in the time\nseries data. More specifically, keeping constant the prior\nassociation, the series association is updated considering the\nfollowing loss\nLtot,max = Lrec + λLsKL . (15)\nAdditionally, an early stopping criterion is used to prevent\nthe model to overfit. In particular, the training procedure is\nterminated when the losses (14) and (15) stop decreasing for\nmore than a pre-fixed number of consecutive epochs.\nUpon completion of the training process, AD is performed\nbased on the computation of an Anomaly Score (AS) that\nincorporates both the reconstruction quality and the value of\nthe association discrepancy. Specifically, given a new time\nseries X ∈ RN×d, the AS for each point of the time series\nis evaluated as\nAS = fSM(LsKL) ⊙\n\r\rX − XR\n\r\r2\n, (16)\nwhere fSM(·) and ⊙ denote the softmax operation and\nelement-wise multiplication, respectively. Then, a point is\nflagged as an anomaly if the AS is greater than a pre-defined\nthreshold δth.\nIV. TINY ANOMALY DETECTION FRAMEWORK\nThis section describes the proposed tiny AD framework,\nwhich is responsible for producing a highly compressed\nversion of AT. The procedure firstly optimizes an (uncom-\npressed) AT model and then uses a distillation method\nto incorporate the knowledge acquired by the optimized\nmodel into the distilled one characterized by much lower\ncomputational complexity. Given the limited computational\nand/or memory resources of embedded systems, such as\nmicrocontrollers or IoT devices, the use of the original (opti-\nmized) AT architecture might not be possible in many cases.\nIndeed, AT relies on a modified self-attention mechanism\nthat requires 3d2\nm trainable parameters associated with the\nlearnable weights of the queries, keys, and value matrices,\nwhile the scale matrix requires dmNh trainable parameters.\nAll these parameters then need to be multiplied by the\nnumber of layers L of the architecture. Considering that\nthe original implementation of AT [47] sets dm = 512,\nNh = 8, and L = 3layers, the resulting memory footprint is\nnot compatible with resource-constrained devices. Therefore,\nin what follows, we detail the main inner workings of\nthe proposed tiny AD framework and how it can provide\nhighly-accurate models suitable to be executed on resource-\nconstrained devices.\nAt first, the developed AD tool is responsible for pre-\ntraining a, possibly large, AT model according to the opti-\nmization strategies presented in Section B so as to maximize\nits AD performances. Then, it instantiates a new AT network\nwith much lower computational complexity (i.e., by limiting\nits number of layers L, its number of heads Nh, or by\nreducing its self-attention map dimension dm). Training\nfrom scratch the compressed model may lead to sub-optimal\nperformances due to its limited expressive capabilities. To\novercome this shortcoming, the tiny AD framework inte-\ngrates a knowledge distillation tool so that the knowledge\nof the optimized AT network, also referred to as teacher,\ncan be incorporated into the compressed AT model, referred\nto as student.\nKnowledge distillation strategies make use of intermediate\nor final model outputs to embed the knowledge acquired\nby a, possibly large, ML model into a much smaller NN.\nVOLUME , 5\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nThe choice about the specific outputs to be distilled is\ntypically made based on the architecture at hand. In our\ncase, we foresee three possible choices: ( i) distilling the\nfeature encodings provided by the embedding module, i.e.,\nX(0); ( ii) distilling the prior and series association matri-\nces, i.e., S(ℓ) and P(ℓ); ( iii) distilling the intermediate and\nfinal outputs of AT, namely X(ℓ). Clearly, one could also\ncombine the aforementioned strategies at the expense of\nlarger computational complexity. The first choice may not\ncapture the complex relationships required to reconstruct\nthe time-series at the output as only the feature encodings\nbetween teacher and student models are distilled. Therefore,\nthe reconstructed time-series at the output provided by the\nstudent may differ substantially from the one provided by the\nteacher, possibly leading to poor reconstruction results and\ninefficient AD. On the other hand, the second choice may\ninterfere with the min-max learning strategy of AT as the\nprior and series associations are alternatively optimized, as\ndetailed in Section III. This will make the convergence of the\ndistilled AT model difficult, possibly leading to degenerate\nprior/series association matrices with a subsequent decrease\nin AD performance. From these considerations, the strategy\nadopted in this paper for the distillation process is the\nthird one as it only constrains the intermediate and final\noutputs of teacher and student AT to be close to each other\nwithout explicitly enforcing the prior and series associations\nof the two models to be equal. This also guarantees that\nthe reconstructed time-series provided by the student closely\nmatches the one provided by the teacher.\nAccording to the previous discussion, the goal of the\ndeveloped distillation algorithm is to match the intermediate\nand final outputs of the teacher and student, as highlighted\nin Fig. 3. The teacher is configured to have L(T) layers,\nN(T)\nh heads, and a self-attention map dimensions of d(T)\nm .\nSimilarly, the student has L(S) ≤ L(T) layers, N(S)\nh ≤ N(T)\nh\nheads, and d(S)\nm ≤ d(T)\nm . Starting from the input time-series\nX, the teacher computes the outputs at each layer {X(ℓ)\nT }L(T)\nℓ=1\naccording to (1)-(2). In a similar manner, X is also used by\nthe student model to compute the outputs {X(ℓ)\nS }L(S)\nℓ=1 . Then,\nfully connected layers, exemplified by the green rectangles in\nFig. 3, are used to upscale/downscale the outputs of the two\narchitectures so that they have the same dimensions. This\nleads to the new representations { bX(ℓ)\nT ∈ RN×d}L(T)−1\nℓ=1 and\n{ bX(ℓ)\nS ∈ RN×d}L(S)−1\nℓ=1 for the teacher and student models,\nrespectively. Note that the fully connected layers reuse the\nweights provided by the last fully connected layer of the\nmodels. By doing so, the trainable parameters of both models\nremain the same.\nThe knowledge distillation process relies on a modified\nloss function compared to the ones presented in Section III.\nIn particular, the student is initially updated following the\nsame min-max optimization strategy described before in\n(14)-(15). Then, a distillation loss term is added to guide\nthe training from the teacher to the student such that the\nTeacher model\nStudent model\nDistillation loss\nLdistill = λD\n(∑L(S)−1\nℓ=1 ∥ˆX(ℓ)\nT −ˆX(ℓ)\nS ∥2 + ∥X(L(T))\nT −X(L(S))\nS ∥2\n)\nX\nX\nX(0)\nS\nX(0)\nT\nX(1)\nS\nX(1)\nT\nX(ℓ)\nS\nX(ℓ)\nT\nX(L(S))\nS\nX(L(T))\nT\nˆX(1)\nS ˆX(ℓ)\nS\nˆX(1)\nT ˆX(ℓ)\nT\n1\nFIGURE 3. Proposed distillation tool: the knowledge of the teacher (top)\nis incorporated into the student (bottom) by minimizing the difference\namong the outputs at different layers provided by the two models.\ndiscrepancy between the intermediate outputs of the two\nmodels is minimized, as highlighted in Fig. 3. Specifically,\nthis loss is evaluated as\nLdist = λD\n\n\nL(S)−1X\nℓ=1\n\r\r\rbX(ℓ)\nT − bX(ℓ)\nS\n\r\r\r\n2\n+\n\r\r\rX(L(T))\nT − X(L(S))\nS\n\r\r\r\n2\n\n.\n(17)\nThe value of λD is chosen so that the student is able to\nlearn a robust normal-abnormal discerning rule while also\nincorporating the knowledge provided by the teacher.\nV. NUMERICAL RESULTS ON LITERATURE DATASETS\nIn this section, we evaluate the performances of the proposed\ntiny AD framework. Section A details the main simulation\nparameters, while Sections B and C study the AD accuracy\nof different student model configurations and the impact\nof various distillation loss functions, respectively. Finally,\nSection D compares the performances of the model produced\nby the developed tiny AD framework with state-of-the-art\nand conventional baselines.\nA. SIMULATION PARAMETERS\nFor evaluation purposes, we consider the following widely-\nadopted AD datasets:\n• Server Machine Dataset (SMD): a 5-week long dataset\ncollected from a large internet company containing\ninformation about 28 different machines [28];\n6 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE 1. Statistics of literature datasets.\nDataset Dim.\nTraining Validation Testing Anomalies\n[%] [%] [%] [%]\nSMD 38 40 10 50 4.2\nSMAP 25 55.4 2.7 41.9 12.8\nMSL 55 35.3 8.8 55.9 10.5\nPSM 25 48.1 12 39.9 27.8\n• Soil Moisture Active Passive satellite (SMAP) and\nMars Science Laboratory rover (MSL): two datasets\npublished by NASA about telemetry of an aircraft\nsystem [66];\n• Pooled Service Metrics (PSM): a dataset by eBay\nrelated to several application server nodes [67].\nThe main characteristics of the datasets detailing the train-\ning/validation/testing split percentages, the dimension d of\nthe time series as well as the percentage of anomalies present\nin the testing split are summarized in Table 1.\nThe experiments focus on the comparison between the\ndetection abilities provided by the optimized model produced\nby the developed tiny AD tool and the ones attained by\nthe compressed model after distillation. In particular, the\noptimized AT employs L(T) = 3layers, N(T)\nh = 8heads, and\na self-attention map dimension d(T)\nm = 512, leading to ∼4.8\nmillion trainable parameters. The optimized model is trained\nfor 20 epochs using a batch size of 64 examples. Unless\nstated otherwise, each example refers to a windowed time-\nseries comprising N = 100data points. The NN weights are\nlearned via the Adam optimizer configured to have a linear\ndecaying learning rate with an initial value of 0.0001 and\nmomentum parameters of 0.9 and 0.999 while considering\nλ = 3 in (14) and (15). The number of epochs used for\nthe early stopping criterion is set to 5. This model is then\nexploited as teacher for the distillation process.\nAs performance metrics, we use the standard measure of\nprecision, recall, and F1 score for classification tasks, which\nare defined as\nPrecision = TP\nTP + FP , (18)\nRecall = TP\nTP + FN , (19)\nF1 = 2 Precision · Recall\nPrecision + Recall , (20)\nwhere TP, FP, TN, and FN denote the number of true\npositives, false positives, true negatives, and false negatives,\nrespectively. Additionally, we also consider the Receiver\nOperating Characteristics (ROC) and the Area Under the\nCurve (AUC) value to comprehensively characterize all the\nmethods.\nB. IMPACT OF STUDENT MODEL CONFIGURATIONS\nThis section studies how different student model configura-\ntions affect the AD performances. This analysis allows us\nto analyze the trade-off between the compression ratio of\nthe student model and its AD capabilities and subsequently\nchoose the best configuration. To do so, we pre-train the\nteacher using the parameters highlighted before and distill\nits knowledge considering students with L(S) ranging from\n1 up to 3 and with d(S)\nm ranging from 16 up to 256, while\nsetting N(S)\nh = N(T)\nh . The distillation process utilizes the loss\ndefined in (17). The student models are trained using the\nsame configuration parameters of the teacher with λD = 10.\nDuring the inference process, we select the threshold for flag-\nging an anomaly following the approach presented in [47]\nwhile also using the adjustment strategy proposed in [68].\nNote that the thresholds are optimized separately for each\ndataset and for each student configuration.\nThe results of the analysis are highlighted in Table 2 which\nreports the precision, recall and F1 scores for each literature\ndataset separately. Additionally, the same table highlights the\ncompression ratio achieved by the student model compared\nto the teacher. Note that the compression ratio varies slightly\nacross the datasets due to the different dimensions of the\ntime-series, thus we only report it for the SMD dataset.\nNumerical results show that the student is able to provide\naccurate AD performances even for large compression ratios.\nIndeed, the F1 score decreases slightly, i.e., less than 2%,\nwhen passing from L(S) = 3 and d(S)\nm = 256 to L(S) = 1\nand d(S)\nm = 16. Some configurations provide less accurate\nresults compared to others. For example, the combination\nd(S)\nm = 16 and L(S) = 3 should be avoided as responsible\nfor low F1 scores across most of the considered datasets.\nThis might indicate that the student AT should be configured\nwith a high enough d(S)\nm when L(S) is large to not incur in\nperformance degradation. Overall, the analysis shows that\nthe performance of the student model does not deteriorate\ntoo much for all analyzed configurations. Therefore, the best\ntrade-off between model complexity and AD accuracy is\nachieved when the student is configured with L(S) = 1and\nd(S)\nm = 16. Adopting such a configuration allows us to reduce\nthe number of parameters of the student by a staggering\n99.91% compared to the original AT without compromising\nthe AD accuracy of the compressed model.\nC. IMPACT OF DIFFERENT LOSS FUNCTIONS\nKnowledge distillation processes rely on dedicated and hand-\ncrafted loss functions to transfer the knowledge between\nteacher and student models. This section analyzes the impact\nof the specific loss function used during distillation. To do\nso, we consider three different loss functions: ( i) the L2 loss\ndefined in (17), ( ii) an L1 loss, and ( iii) a smooth L1 loss.\nVOLUME , 7\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nTABLE 2. Performance comparison over different AD literature datasets in terms of precision (P), recall (R), and F1 score for different student model\nconfigurations.\nConfiguration SMD SMAP MSL PSM\nL(S) d(S)\nm Compr. ratio P R F1 P R F1 P R F1 P R F1\n[#] [#] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%]\n3 256 74.48 88.82 93.29 91.00 93.58 99.23 96.32 91.96 93.36 92.65 97.13 98.75 97.93\n2 256 82.72 88.87 94.55 91.62 93.59 99.35 96.39 92.17 97.59 94.80 97.17 98.63 97.89\n1 256 90.96 88.74 92.32 90.50 93.60 98.76 96.11 91.39 95.15 93.23 97.21 98.64 97.92\n3 64 98.21 89.22 94.11 91.60 93.64 98.59 96.05 92.09 96.70 94.34 97.05 98.79 97.91\n2 64 98.74 89.62 95.04 92.25 93.06 91.46 92.25 92.10 96.70 94.35 97.16 98.20 97.68\n1 64 99.27 88.57 90.54 89.55 93.68 98.00 95.79 92.02 96.70 94.31 97.03 98.53 97.77\n3 16 99.84 86.12 74.72 80.02 93.54 98.51 95.96 91.63 93.07 92.35 97.17 98.86 98.01\n2 16 99.88 88.24 88.10 88.17 93.61 98.86 96.16 91.68 93.27 92.47 97.10 98.94 98.01\n1 16 99.91 88.30 90.80 89.53 93.66 99.22 96.36 91.77 95.94 93.81 97.12 98.10 97.61\nTABLE 3. Performance comparison over different AD literature datasets in terms of precision (P), recall (R), and F1 score for different loss functions.\nLoss\nSMD SMAP MSL PSM\nP R F1 P R F1 P R F1 P R F1\n[%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%]\nL2 loss 88.30 90.80 89.53 93.66 99.22 96.36 91.77 95.94 93.81 97.12 98.10 97.61\nL1 loss 87.89 86.78 87.33 93.65 98.99 96.25 91.94 97.50 94.64 97.19 98.12 97.65\nSmooth L1 loss 88.29 89.34 88.81 93.65 99.19 96.31 92.03 97.50 94.69 97.19 98.27 97.73\nAs done in Section IV, the L1 loss is evaluated as\nLdist = λD\n\n\nL(S)−1X\nℓ=1\n\r\r\rbX(ℓ)\nT − bX(ℓ)\nS\n\r\r\r+\n\r\r\rX(L(T))\nT − X(L(S))\nS\n\r\r\r\n\n,\n(21)\nOn the other hand, the smooth L1 loss is computed as\nL(SL1)\ndist = λD\n\n\nL(S)−1\nX\nℓ=1\nLs( bX(ℓ)\nT , bX(ℓ)\nS )+Ls(X(L(T))\nT , X(L(S))\nS )\n\n,\n(22)\nwhere Ls is defined in [69]. Note that the parameter β\nfor Ls is set to 1. According to the previous analysis,\nwe select the student model configuration with L(S) = 1,\nN(S)\nh = 8 and d(S)\nm = 16, while the teacher is configured\nas in Section A. The same training configurations detailed\nin Section A are also used here for updating the weights\nof the student and teacher models, while we set λD = 10\nfor all the losses. Finally, during inference, we optimize the\nthresholds according to the policy presented in [47]. This\nleads to setting δth = {0.398, 0.145, 0.157, 0.287} for the\nSMD, SMAP, MSL and PSM datasets when the L2 loss is\nconsidered, while we set δth = {0.395, 0.149, 0.152, 0.286}\nand δth = {0.398, 0.145, 0.158, 0.288} for the L1 and smooth\nL1 losses considering the same datasets, respectively.\nTable 3 reports the precision, recall, and F1 scores consid-\nering the three aforementioned loss functions. Comparing the\nresults, we can see that the L2 loss is advantageous for the\nSMD and SMAP datasets, while the smooth L1 has slightly\nhigher performances when MSL and PSM are considered.\nNevertheless, the results also highlight that the performances\nof the proposed approach are not that much affected by the\nspecific loss used for the distillation process. Indeed, the\ndifference among all losses in terms of precision, recall, and\nF1 score is only marginal. Based on this discussion, the loss\nfunction used for the distillation process for the following\nresults is (17).\nD. COMPARISON WITH OTHER BASELINES\nThis section evaluates the performances of the proposed\ntiny AD framework by comparing it with other baseline\napproaches. The knowledge distillation process considers as\nteacher the AT model set as in Section A while the student is\nconfigured according to the analysis provided in Section B.\nFor comparison purposes, we consider a widely-used RNN-\nbased AD method, namely LSTM-V AE [35], implemented so\n8 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE 4. Performance comparison over different AD literature datasets in terms of precision (P), recall (R), and F1 score.\nMethod\nSMD SMAP MSL PSM\nP R F1 P R F1 P R F1 P R F1\n[%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%] [%]\nOriginal AT 89.02 94.67 91.76 93.60 99.29 96.36 92.06 98.06 94.96 97.37 98.12 97.75\nDistilled AT (ours) 88.30 90.80 89.53 93.66 99.22 96.36 91.77 95.94 93.81 97.12 98.10 97.61\nLSTM-V AE 74.91 81.92 78.26 92.75 55.94 69.78 90.96 86.93 88.90 98.06 88.65 93.12\nOCSVM 42.18 74.89 53.96 48.81 50.32 49.55 58.63 84.22 69.13 63.31 82.29 71.56\n(a)\n (b)\n(c)\n (d)\nFIGURE 4. ROC curves obtained by the distilled AT model provided by the tiny AD framework and the LSTM-VAE method on different datasets. (a) SMD;\n(b) SMAP; (c) MSL; (d) PSM. The corresponding AUC values of the two methods are reported in the legends.\nas to roughly have the same number of trainable parameters\nof the student AT, as well as the conventional OCSVM\nAD strategy [70]. All models, apart OCSVM, are optimized\nusing the configuration parameters detailed in Section A\nwhile we set λD = 10in (17). As far as the inference process\nis concerned, the threshold δth is set separately for each\ndataset according to the policy in [47]. This process leads\nto δth = {0.016, 0.015, 0.012, 0.02} for the SMD, SMAP,\nMSL, and PSM datasets for the teacher, while for the student\nwe obtain δth = {0.398, 0.145, 0.157, 0.287}. On the other\nhand, the thresholds for LSTM-V AE are found via a non-\nexhaustive search over a grid of possible values.\nTable 4 provides a comparison between the models pro-\nduced by the developed tiny AD tool, LSTM-V AE and\nOCSVM in terms of precision, recall and F1 metrics. Over-\nall, the distilled AT model is able to closely match the per-\nformances of the original (optimized) AT architecture while\nrequiring a substantially lower computational complexity.\nIndeed, the F1 metric achieved by the student is only slightly\nlower than the one obtained by the teacher for all datasets.\nFocusing now on the detection abilities of LSTM-V AE, they\nare largely inferior compared to the ones achieved by the\ncompressed AT architecture despite having a similar number\nof trainable parameters. Specifically, LSTM-V AE provides\nvery poor detection abilities for the SMD and SMAP datasets\nwhere the F1 score is reduced more than 10% compared to\nall other methods, while for the MSL and PSM the accuracy\nreduction is not so severe (i.e., only a 4-5% reduction on\nthe F1 metric is observed). Similar results are also achieved\nby OCSVM which is shown to provide AD capabilities far\nVOLUME , 9\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nCM1 CM2\n(a)\n (b)\nFIGURE 5. AD case study on a real monitoring infrastructure: (a) sketch of the bridge being monitored with the installed IoT sensors; (b) time series\ndata from crack meters in a monitoring period of 9 months.\nbelow the ones attained by both LSTM-V AE and the distilled\nAT model. This suggests how conventional AD strategies,\nsuch as OCSVM, might not be adequate for heterogeneous\ntime-series data, and more complex data-driven AD tools are\nrequired for improving the performances.\nTo complement the analysis, we report in Figure 4 the\nROC curves obtained by the distilled AT model produced\nby the developed AD framework and the LSTM-V AE over\nall datasets used in the experiments and their corresponding\nAUC values. This set of results further confirms the findings\nof the previous analysis: the compressed AT model is able\nto outperform LSTM-V AE in all cases. The AUC values\nobtained by the distilled algorithm are superior when com-\npared with the ones attained by LSTM-V AE, especially if\nthe SMAP dataset is considered. Overall, the analysis shows\nthat the proposed tiny AD framework is able to provide\na highly-accurate distilled model that closely matches the\nperformances provided by the original (optimized) AT while\nalso outperforming a LSTM-V AE anomaly detector having\na similar number of trainable parameters.\nVI. CASE STUDY: ANOMALY DETECTION ON A BRIDGE\nINFRASTRUCTURE\nThis section analyzes the detection abilities of the models\nproduced by the developed AD tool using time series data\nacquired from a real-world bridge infrastructure monitoring\nsystem deployed in Italy. In the following, we describe\nthe main technical parameters of the dataset (Section A).\nNext, we evaluate the detection abilities of the teacher\nmodel trained under the proposed tiny AD framework by\nconsidering different input configurations in order to select\nthe one that provides the best performance (Section A).\nLastly, Section C characterizes the AD capabilities of the\ncompressed model which is distilled from the best teacher\nmodel selected in Section B according to the framework\ndescribed in Section IV.\nA. DATASET DESCRIPTION\nThe case study considers an IoT sensor network compris-\ning two crack-meters monitoring the status of a bridge\n(illustrated in Figure 5a) by measuring the variation of the\ndisplacement across cracks and/or joints over time. Com-\nmunication protocols, such as MQTT, are used to connect\nthe edge devices to a central software, where raw data\nare processed. The sampling period is configured so that a\nrecording is generated every two hours. The resulting time\nseries data recorded over 9 consecutive months, namely from\nDecember 2021 up to August 2022, comprises 8000 samples\nand it is shown in Figure 5b. We split this dataset into two\nparts: the first 6000 samples are used used for training, while\nthe remaining 2000 ones constitute the testing dataset.\nTo assess the performances of the proposed tiny AD\nframework, we introduce hand-crafted, yet realistic, anoma-\nlies in the testing set, while we assume that the training data\nis free from abnormal points. The following four types of\nperturbations are introduced in the testing time series:\n• Type I - point anomaly: a spike in the time series data;\n• Type II - step anomaly: a step function is superimposed\nto the values of the time series for NA consecutive data\npoints;\n• Type III - ascending exponential anomaly: an increasing\nexponential function is superimposed to the values of\nthe time series data for NA consecutive data points;\n• Type IV - descending exponential anomaly: a decreas-\ning exponential function is superimposed to the values\nof the time series data for NA consecutive data points.\nEach type of anomaly is introduced by adding one of the\nfunctions fA(t) provided in Table 5 to the raw sensors data.\nSpecifically, two spikes are added at timesteps tD = 900and\ntD = 1100, with B = 5, while σT denotes the empirical\nstandard deviation computed over the training dataset. Note\nthat we consider two different values of σT, one for each\ncrack-meter. For what concerns the other three anomaly\ntypes, i.e., step, increasing, and decreasing exponential, they\nare introduced in two non-overlapping windows comprising\n10 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nTABLE 5. Definition of anomaly functions.\nAnomaly fA(t)\nType I B σT δ(t − tD)\nType II\n\n\n\nB σT if tD1 ≤ t ≤ tD2\n0 otherwise\nType III\n\n\n\nB σT\nbt − 1\nb − 1 if tD1 ≤ t ≤ tD2\n0 otherwise\nType IV\n\n\n\n−B σT\nb−t − 1\nb−1 − 1 if tD1 ≤ t ≤ tD2\n0 otherwise\nNA = 30consecutive points each, and using the same values\nfor B and σT as before. The window starts at timestep\ntD1 = 900 and ends at tD2 = 930 for the first time series,\nwhile for the second one, the initial and final timesteps\nare chosen as tD2 = 1100 and tD2 = 1130. Additionally,\nthe exponential functions used for simulating the last two\nanomalies use b = 8. The resulting anomalous patterns of\nthe four types of anomalies are highlighted in Figure 6.\nIn the following, we use this database to initially assess\nthe ability of the original AT model (obtained from the AD\nframework detailed in Section IV) of detecting the four types\nof anomalies. Then, we will apply the distillation strategy\npresented in Section IV to compress the AT model and assess\nits AD performances.\nB. ANALYSIS OF THE TEACHER WITH DIFFERENT\nWINDOWS AND OVERLAPS\nThis section aims at studying the detection accuracy of the\noriginal AT model trained under the proposed AT framework\nusing the raw time series data acquired by the monitoring\nfacility. To achieve this goal, we consider different input\nconfigurations for the selection of the best performing (un-\ncompressed) AT model and use it to guide the student’s\ntraining. Recalling that AT accepts at the input a time series\nX with d dimensions and length N, we vary the number\nof data points N as well as the overlap among adjacent\nsegments and evaluate the performances accordingly. Specif-\nically, we consider two values of N, namely N = 6 and\nN = 12, which correspond to windows spanning half a\nday and one day of recording, and three overlaps, i.e., 0%,\n(no overlap exists between adjacent segments), 50% and\n80%. The original AT model is configured as in Section V,\nnamely it has L(T) = 3layers, N(T)\nh = 8heads, and a self-\nattention map with d(T)\nm = 512dimensions. Additionally, the\nAdam optimizer is used for updating the weights considering\na batch size of 64 examples, a learning rate of 0.001,\nand momentum parameters of 0.9 and 0.999. The model\nis trained for 200 epochs and it is stopped preemptively\nwhen the validation loss does not reduce over 10 consecutive\nepochs.\nFIGURE 6. Testing time series of crack meters with anomalies. The\nconsidered anomalies include (from top to bottom) point, step, ascending\nexponential and descending exponential perturbations.\nFigure 7 shows the AS obtained by the AT model at the\nend of the training process for the testing dataset containing\nthe point anomaly with N = 6 (Figure 7a) and N = 12\n(Figure 7b), and considering all overlaps. In particular, each\nfigure firstly shows the time series data of the testing dataset\nat the top, while the following subplots highlight the AS\nachieved considering 0%, 50%, and 80% overlaps.\nThe results indicate that the point anomaly can be detected\nfor all windows and overlaps considered in the analysis, even\nthough in some cases the AS is not particularly high (see\ne.g., the case with N = 12 and 50% overlap). Indeed, for\nall cases, two spikes are present in the area delimited by the\nlight gray boxes, which highlight the regions comprising the\nanomalies. Nevertheless, for N = 6 the model is able to\nrecognize the second anomaly fairly easily as the associated\nspike is more pronounced. On the other hand, for N = 12,\nthe AT is able to reliably detect the first anomaly. Besides\nchoosing an appropriate value for N, also the overlap has\nan impact on the overall performances. Indeed, for N = 6a\nhigh overlap, i.e., 80%, should be preferred to facilitate the\nAD process, while for N = 12the model seems to provide\nthe highest AS when the adjacent windows have no overlap.\nOverall, taking into account also the magnitude of the AS\nacross different input configurations, the model trained with\nN = 6 and 80% overlap is the one showing the highest\npeaks in the neighborhood of the point anomalies, therefore,\nit should be selected for facilitating the AD process.\nThe detection abilities provided by AT over the testing\ndataset with the step anomaly, whose results are reported in\nFigure 8a and Figure 8b for N = 6and N = 12, respectively,\nindicate that the AT model is able to recognize the anomalies\nalso in this case. However, N = 12is seen to provide more\nsparse peaks in the areas delimited by the gray boxes, while\nN = 6 obtains scores with high values that cover more\nuniformly the area containing the anomalies. The overlaps\nVOLUME , 11\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\n(a)\n(b)\nFIGURE 7. Analysis of the detection performance of the original AT for\ntype I anomaly with windows comprising: (a) 6 points and (b) 12 points.\nFrom top to bottom, each figure reports the testing time series and the AS\nobtained considering 0%, 50%, and 80% overlaps. The position of the\nanomalies is highlighted with a light gray box in all subfigures.\nare also shown to affect more the model trained with N = 6\nrather than that with N = 12. Indeed, 80% overlap should\nbe avoided when N = 6 as the peaks of the AS are not\nparticularly high while the results obtained for 0% and 50%\noverlaps are quite similar. On the other hand, for N = 12\ndifferent overlaps influence the ASs provided by the model\nonly negligibly. According to the results obtained, also under\nthis case N = 6 should be preferred when compared with\nN = 12as showing more distributed and higher AS values\nin the vicinity of the step anomalies, provided the overlap is\nbelow 80%.\nFigure 9 reports the results achieved by the AT model\nfor the testing dataset containing the ascending exponential\nanomaly with N = 6 (Figure 9a) and N = 12 (Figure 9b).\nThe results are in line with the ones found in the previous\nanalysis for the step anomaly: N = 6 generally provides\nAS with peaks more distributed in the neighborhood of the\n(a)\n(b)\nFIGURE 8. Analysis of the detection performance of the original AT for\ntype II anomaly with windows comprising: (a) 6 points and (b) 12 points.\nFrom top to bottom, each figure reports the testing time series and the AS\nobtained considering 0%, 50%, and 80% overlaps. The position of the\nanomalies is highlighted with a light gray box in all subfigures.\nanomalies, while N = 12 has fewer peaks but has some\nspikes with higher magnitude (see e.g., when the overlap is\n80%). Interestingly, when no overlap exists, the AS provided\nby the model trained with N = 12is quite low in the second\nwindow, indicating that under this input configuration AT is\nnot able to detect all anomalies. Overall, the best-performing\ninput configuration is N = 6with 50% overlap, as it shows\nan AS covering most of the anomalies in the testing dataset.\nThe results considering the last type of anomaly, i.e.,\nthe descending exponential, are presented in Figure 10a for\nN = 6 and in Figure 10b for N = 12. Compared to\nthe previous type, this anomaly seems to be easier to be\ndetected as the AS provided by the model under all input\nconfigurations shows a large number of peaks in the light\ngray boxes depicted in the figures. Again, using a window\nof N = 6 allows the detection of more anomalous points\ncompared to the case of N = 12. Similarly as before,\n12 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n(a)\n(b)\nFIGURE 9. Analysis of the detection performance of the original AT for\ntype III anomaly with windows comprising: (a) 6 points and (b) 12 points.\nFrom top to bottom, each figure reports the testing time series and the AS\nobtained considering 0%, 50%, and 80% overlaps. The position of the\nanomalies is highlighted with a light gray box in all subfigures.\nN = 6 should be selected in conjunction with 0% or 50%\noverlap as the AS magnitude is higher in the neighborhood\nof the anomalies, while when N = 12, the overlap should\nbe selected between 50% and 80% to improve the detection\nof the second anomaly. Considering the different input con-\nfigurations, N = 6is shown again to provide more accurate\ndetection results compared with N = 12, provided that the\noverlap is below 80%.\nTo finalize the analysis on the impact of the different input\nconfigurations for the (uncompressed) AT model, we report\nin Table 6 the number of correct predictions together with\nthe number of false positives obtained by AT considering\nall the combinations studied before. The results are obtained\nby numerically searching for the detection threshold δth that\ngives the highest and lowest number of correct predictions\nand false positives, respectively.\n(a)\n(b)\nFIGURE 10. Analysis of the detection performance of the original AT for\ntype IV anomaly with windows comprising: (a) 6 points and (b) 12 points.\nFrom top to bottom, each figure reports the testing time series and the AS\nobtained considering 0%, 50%, and 80% overlaps. The position of the\nanomalies is highlighted with a light gray box in all subfigures.\nFor the point anomaly, the performances are not affected\nby the specific choice of the window and the overlap.\nHowever, this does not hold when considering different types\nof anomalies. Generally, a window of N = 6 is seen to\nprovide the highest number of correct predictions while also\nhaving a slightly higher number of false positives compared\nwith N = 12. Therefore, N = 6 should be selected to\nachieve the highest AD accuracy. Focusing also on the\nperformances for different overlaps, the results show that\n80% should be avoided as responsible for low accuracy. On\nthe other hand, 0% overlap provides the least amount of false\npositives but it also detects a lower number of anomalies\nwhen compared with the 50% case. Considering all these\naspects, we finally select the input configuration with N = 6\nand 50% overlap and use the resulting model to perform the\ndistillation process as detailed in Section IV.\nVOLUME , 13\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nTABLE 6. Detection accuracy and false positive analysis for the original\nAT model.\nAnomaly\nWindow size N Overlap δth FP TP\n[#] [%] [#] [#] [#]\nType I\n6\n0 0.58 0 2\n50 0.47 0 2\n80 2.31 0 2\n12\n0 0.79 0 2\n50 0.26 0 2\n80 0.48 0 2\nType II\n6\n0 0.17 1 11\n50 0.53 0 10\n80 0.14 4 11\n12\n0 0.75 0 6\n50 1.51 0 5\n80 1.32 0 5\nType III\n6\n0 0.05 2 4\n50 0.03 4 7\n80 0.07 5 6\n12\n0 0.17 3 2\n50 0.05 8 3\n80 0.16 2 3\nType IV\n6\n0 0.31 2 9\n50 0.12 3 10\n80 0.03 3 10\n12\n0 0.69 1 4\n50 0.18 0 4\n80 0.31 1 5\nC. ASSESSMENT OF THE DISTILLED MODEL\nThis section aims at evaluating the AD performances of the\ncompressed AT model produced by the proposed AD frame-\nwork after distillation. According to the previous analysis,\nthe optimized AT model is pre-trained using N = 6 and\n50% overlap and using the same optimization parameters\npresented before. Regarding the compressed model, it is\nconfigured as in Section V, with L(S) = 1 layer, N(S)\nh = 8\nheads, and a self-attention map dimension d(S)\nm of 16, leading\napproximately to an overall number of 1400 parameters. It\nis also trained using a window of N = 6 and an overlap\nof 50%. The distillation process runs for 200 epochs with\nλD = 10 and it is early stopped if the validation loss does\nnot decrease for more than 5 consecutive epochs. Note that\nboth models do not have access to the simulated anomalies\nduring training (they are added only to the testing database),\nnor do they use labels to identify anomalies as we consider\na fully unsupervised learning setting.\nFigure 11 reports the results obtained by the compressed\nmodel over the testing dataset comprising the point (Fig-\nure 11a), step (Figure 11b), ascending exponential (Fig-\nure 11c) and descending exponential (Figure 11d) anomalies.\nEach figure shows at the top the testing time series data\ntogether with the introduced anomalies, while the bottom\nhighlights the AS obtained by the distilled AT model. To\nease the comparison, we also highlight the position of the\nanomalies with a light gray box. The AS for all figures shows\nthat the model is able to recognize fairly easily all anomaly\ntypes. Indeed, several spikes are reported in the positions de-\nlimited by the light gray boxes which indicate that the model\nis confident that the time series data contains anomalous\npoints. This demonstrates that the proposed AD framework\nintegrating the distillation strategy detailed in Section IV is\nable to provide a lightweight AT model that supports highly-\naccurate AD capabilities. When comparing the obtained\nresults with the ones achieved by the uncompressed model\n(Section B), it can be noticed that for some cases high AS\nvalues are reported outside of the areas delimited by the light\ngray boxes (see e.g., the timestep ranging from 300 up to\n360 in Figure 11a). This may be caused by the fact that the\nself-attention map of the compressed AT model is highly\nreduced making it more difficult for the anomaly attention\nmechanism to correctly learn the prior and series association\nand thus they do not fully capture the temporal dependency\nof the time series. Nevertheless, a careful optimization of the\ndetection threshold δth may be helpful in suppressing those\ncases.\nTo comprehensively characterize the performances of the\nmodel obtained by the proposed AD framework after the dis-\ntillation process, we report in Table 7 the number of correct\npredictions and false positives obtained by the distilled AT\nmodel considering the anomalies previously described. The\nresults are obtained by optimizing the detection threshold\nδth in order to maximize the number of correctly detected\nanomalies while minimizing the number of false positives.\nThe results obtained by the compressed AT model are in line\nwith the ones reported in Table 6 for the case with N = 6\nand 50% overlap: the number of correct predictions closely\nmatches the one provided by the uncompressed model for\nall anomaly types. The main difference with respect to\nthe previous case relies on the number of false positives\nprovided by the model trained under the proposed distilla-\ntion framework. Indeed, a slightly higher number of false\npositives is shown by the distilled model when compared\nwith the same number provided by the uncompressed AT\narchitecture. This is likely caused by the fact that the student\nhas a much smaller representation capacity compared to the\nteacher leading it to output a relatively high AS between\n14 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n(a)\n (b)\n(c)\n (d)\nFIGURE 11. Analysis of the detection performance of the distilled AT model considering the following anomaly types: (a) point, (b) step, (c) ascending\nexponential, (d) descending exponential. The top part of each figure reports the testing dataset highlighting the specific anomaly type, while the bottom\npart shows the AS provided by the model.\ntimesteps 200 and 400. This consequently causes the spikes\ndetected in that region of the testing time series to be\nflagged as anomalies. Nevertheless, the results still suggest\nthat the model obtained after distillation is capable of closely\nmatching the performances provided by the original AT\nimplementation.\nVII. CONCLUSION\nThis paper explored the problem of accurate AD in\nIoT setups characterized by devices having limited en-\nergy/computing capabilities. Transformer-based AD tools\nhave been demonstrated to provide outstanding performances\nin detecting anomalies over heterogeneous and streaming\ntime series data. Nevertheless, they generally comprise large\nand complex NNs, making them unsuitable for being de-\nployed in IoT devices due to energy and/or computing\nconstraints. To overcome such limitations, this paper pro-\nTABLE 7. Detection accuracy and false positive analysis for the proposed\ncompressed AT model.\nAnomaly\nδth FP TP\n[#] [#] [#]\nType I 0.68 1 2\nType II 1.90 0 11\nType III 0.38 5 6\nType IV 0.11 4 9\nposed an effective tiny AD framework based on knowledge\ndistillation. The developed tool aims at initially finding an\noptimized version of a state-of-the-art AD method, namely\nAT, and use it as input for the distillation process whose goal\nVOLUME , 15\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBarbieri et al.: A Tiny Transformer-Based Anomaly Detection Framework for IoT Solutions\nis to produce a substantially compressed AT model able to\nachieve accurate detection abilities.\nThe proposed framework is firstly assessed using widely\nadopted AD datasets showing its efficacy in providing a\nhighly-accurate AT model while reducing its trainable pa-\nrameters by roughly 99.93% (from 4.8 million to 3300\nor 1400 depending on the input dataset). Interestingly, the\nanalysis also shows that the compressed model provided by\nthe developed AD tool is able to substantially outperform an\nRNN-based state-of-the-art AD algorithm when the two have\nroughly the same computational complexity (i.e., number of\ntrainable parameters) as well as a conventional OCSVM AD\nstrategy. The AD framework is then deployed in a real-world\nAD scenario where an infrastructure is in charge of moni-\ntoring the physical parameters of a bridge via distributed\nIoT sensors placed on it. Under this scenario, the model\nproduced by the AD strategy after applying the knowledge\ndistillation tool is shown to closely match the performances\nof the original uncompressed model while only marginally\nincreasing the number of false positives.\nThe proposed tiny AD tool has been shown to be partic-\nularly suitable for dealing with complex and heterogeneous\ntime-series data revealing its potential to be applied to real-\nworld IoT setups. Nevertheless, the developed framework\ncould be further optimized to take into account other con-\nstraints, such as latency and reliability, that are likely to\nbe required when adapting the proposed solution to diverse\nscenarios, which may range from everyday applications to\nindustrial IoT services. In particular, the renovated self-\nattention mechanism of AT could be modified to introduce\nsparse computations, allowing to further scale down the\ninference time. Besides, the optimization of the training\npipelines for porting the distillation tool into physical devices\nis expected to bridge the gap between research and practice.\nFinally, it could be interesting to explore the integration of\nedge computing paradigms, including Federated Learning\n(FL) strategies, to improve the privacy of the proposed tiny\nAD framework.\nREFERENCES\n[1] E. Sisinni, A. Saifullah, S. Han, U. Jennehag et al. , “Industrial\ninternet of things: Challenges, opportunities, and directions,” IEEE\nTransactions on Industrial Informatics, vol. 14, no. 11, pp. 4724–4734,\n2018.\n[2] A. Zanella, N. Bui, A. Castellani, L. Vangelista et al. , “Internet of\nthings for smart cities,” IEEE Internet of Things Journal , vol. 1, no. 1,\npp. 22–32, 2014.\n[3] C. Arcadius Tokognon, B. Gao, G. Y . Tian, and Y . Yan, “Structural\nhealth monitoring framework based on internet of things: A survey,”\nIEEE Internet of Things Journal , vol. 4, no. 3, pp. 619–635, 2017.\n[4] C. Alippi and M. Roveri, “The (not) far-away path to smart cyber-\nphysical systems: An information-centric framework,” Computer,\nvol. 50, no. 4, pp. 38–47, 2017.\n[5] S. S. Musa, M. Zennaro, M. Libsie, and E. Pietrosemoli, “Conver-\ngence of information-centric networks and edge intelligence for IoV:\nChallenges and future directions,” Future Internet, vol. 14, no. 7, 2022.\n[6] L. Barbieri, S. Savazzi, M. Brambilla, and M. Nicoli, “Decentralized\nfederated learning for extended sensing in 6G connected vehicles,”\nVehicular Communications, vol. 33, p. 100396, 2022.\n[7] H. Xie, Z. Yan, Z. Yao, and M. Atiquzzaman, “Data collection for\nsecurity measurement in wireless sensor networks: A survey,” IEEE\nInternet of Things Journal , vol. 6, no. 2, pp. 2205–2224, 2019.\n[8] A. A. Cook, G. Mısırlı, and Z. Fan, “Anomaly detection for IoT time-\nseries data: A survey,” IEEE Internet of Things Journal , vol. 7, no. 7,\npp. 6481–6494, 2020.\n[9] C. Alippi, S. Ntalampiras, and M. Roveri, “Model-free fault detection\nand isolation in large-scale cyber-physical systems,” IEEE Transac-\ntions on Emerging Topics in Computational Intelligence , vol. 1, no. 1,\npp. 61–71, 2017.\n[10] C. Alippi, G. Boracchi, and M. Roveri, “Hierarchical change-detection\ntests,” IEEE Transactions on Neural Networks and Learning Systems ,\nvol. 28, no. 2, pp. 246–258, 2017.\n[11] F. Cauteruccio, L. Cinelli, E. Corradini, G. Terracina et al. , “A\nframework for anomaly detection and classification in multiple iot\nscenarios,” Future Generation Computer Systems , vol. 114, pp. 322–\n335, 2021.\n[12] V . Chandola, A. Banerjee, and V . Kumar, “Anomaly detection: A\nsurvey,” ACM Comput. Surv., vol. 41, no. 3, 2009.\n[13] T. Shon and J. Moon, “A hybrid machine learning approach to network\nanomaly detection,” Information Sciences, vol. 177, no. 18, pp. 3799–\n3821, 2007.\n[14] V . A. Sotiris, P. W. Tse, and M. G. Pecht, “Anomaly detection through\na bayesian support vector machine,” IEEE Transactions on Reliability,\nvol. 59, no. 2, pp. 277–286, 2010.\n[15] M. Amer, M. Goldstein, and S. Abdennadher, “Enhancing one-class\nsupport vector machines for unsupervised anomaly detection,” in\nProceedings of the ACM SIGKDD Workshop on Outlier Detection and\nDescription. Association for Computing Machinery, 2013, p. 8–15.\n[16] A. P. Muniyandi, R. Rajeswari, and R. Rajaram, “Network anomaly\ndetection by cascading K-means clustering and C4.5 decision tree\nalgorithm,” Procedia Engineering, vol. 30, pp. 174–182, 2012.\n[17] S. Thaseen and C. A. Kumar, “An analysis of supervised tree based\nclassifiers for intrusion detection system,” in 2013 International Con-\nference on Pattern Recognition, Informatics and Mobile Engineering ,\n2013, pp. 294–299.\n[18] J. Zhang, M. Zulkernine, and A. Haque, “Random-forests-based net-\nwork intrusion detection systems,” IEEE Transactions on Systems,\nMan, and Cybernetics, Part C (Applications and Reviews) , vol. 38,\nno. 5, pp. 649–659, 2008.\n[19] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008\nEighth IEEE International Conference on Data Mining, 2008, pp. 413–\n422.\n[20] D. Xu, Y . Wang, Y . Meng, and Z. Zhang, “An improved data anomaly\ndetection method based on isolation forest,” in 2017 10th International\nSymposium on Computational Intelligence and Design (ISCID), vol. 2,\n2017, pp. 287–291.\n[21] O. Alghushairy, R. Alsini, T. Soule, and X. Ma, “A review of local\noutlier factor algorithms for outlier detection in big data streams,” Big\nData and Cognitive Computing , vol. 5, no. 1, 2021.\n[22] Z. Cheng, C. Zou, and J. Dong, “Outlier detection using isolation forest\nand local outlier factor,” in Proceedings of the Conference on Research\nin Adaptive and Convergent Systems, ser. RACS ’19. Association for\nComputing Machinery, 2019, p. 161–168.\n[23] V . Hautamaki, I. Karkkainen, and P. Franti, “Outlier detection using\nk-nearest neighbour graph,” in Proceedings of the 17th International\nConference on Pattern Recognition, 2004. ICPR 2004. , vol. 3, 2004,\npp. 430–433.\n[24] Y . Djenouri, A. Belhadi, J. C.-W. Lin, and A. Cano, “Adapted K-\nnearest neighbors for detecting anomalies on spatio–temporal traffic\nflow,” IEEE Access, vol. 7, pp. 10 015–10 027, 2019.\n[25] S. Omar, A. M. Ngadi, and H. H. Jebur, “Machine learning techniques\nfor anomaly detection: An overview,” International Journal of Com-\nputer Applications, vol. 79, pp. 33–41, 2013.\n[26] A. B. Nassif, M. A. Talib, Q. Nasir, and F. M. Dakalbab, “Machine\nlearning for anomaly detection: A systematic review,” IEEE Access ,\nvol. 9, pp. 78 658–78 700, 2021.\n[27] D. Kwon, K. Natarajan, S. C. Suh, H. Kim et al., “An empirical study\non network anomaly detection using convolutional neural networks,” in\n2018 IEEE 38th International Conference on Distributed Computing\nSystems (ICDCS), 2018, pp. 1595–1598.\n[28] Y . Su, Y . Zhao, C. Niu, R. Liu et al., “Robust anomaly detection for\nmultivariate time series through stochastic recurrent neural network,”\nin Proceedings of the 25th ACM SIGKDD International Conference\n16 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\non Knowledge Discovery & Data Mining. Association for Computing\nMachinery, 2019, p. 2828–2837.\n[29] Y . Wu, H.-N. Dai, and H. Tang, “Graph neural networks for anomaly\ndetection in industrial internet of things,” IEEE Internet of Things\nJournal, vol. 9, no. 12, pp. 9214–9231, 2022.\n[30] Y . Feng, J. Chen, Z. Liu, H. Lv et al. , “Full graph autoencoder for\none-class group anomaly detection of IIoT system,” IEEE Internet of\nThings Journal, vol. 9, no. 21, pp. 21 886–21 898, 2022.\n[31] Q. Wen, T. Zhou, C. Zhang, W. Chen et al. , “Transformers in time\nseries: A survey,” arXiv e-prints, p. arXiv:2202.07125, 2022.\n[32] S. Tuli, G. Casale, and N. R. Jennings, “TranAD: Deep transformer\nnetworks for anomaly detection in multivariate time series data,” arXiv\npreprint arXiv:2201.07284, 2022.\n[33] G. Pang, C. Shen, L. Cao, and A. V . D. Hengel, “Deep learning for\nanomaly detection: A review,” ACM Comput. Surv., vol. 54, no. 2, mar\n2021.\n[34] Y . Liu, Y . Zhou, K. Yang, and X. Wang, “Unsupervised deep learning\nfor IoT time series,” IEEE Internet of Things Journal , pp. 1–1, 2023.\n[35] D. Park, Y . Hoshi, and C. C. Kemp, “A multimodal anomaly detector\nfor robot-assisted feeding using an LSTM-based variational autoen-\ncoder,” IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1544–\n1551, 2018.\n[36] Z. Li, Y . Zhao, J. Han, Y . Su et al. , “Multivariate time series\nanomaly detection and interpretation using hierarchical inter-metric\nand temporal embedding,” in Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining . Association\nfor Computing Machinery, 2021, p. 3220–3230.\n[37] L. Shen, Z. Li, and J. Kwok, “Timeseries anomaly detection using\ntemporal hierarchical one-class network,” in Advances in Neural\nInformation Processing Systems , vol. 33, 2020, pp. 13 016–13 026.\n[38] B. Zhou, S. Liu, B. Hooi, X. Cheng et al. , “BeatGAN: Anoma-\nlous rhythm detection using adversarially generated time series,” in\nProceedings of the Twenty-Eighth International Joint Conference on\nArtificial Intelligence, IJCAI-19 , 2019, pp. 4433–4439.\n[39] D. Li, D. Chen, B. Jin, L. Shi et al. , “MAD-GAN: Multivariate\nanomaly detection for time series data with generative adversarial\nnetworks,” in Artificial Neural Networks and Machine Learning –\nICANN 2019: Text and Time Series , 2019, pp. 703–716.\n[40] Z. Chen, D. Chen, X. Zhang, Z. Yuan et al., “Learning graph structures\nwith transformer for multivariate time-series anomaly detection in\nIoT,” IEEE Internet of Things Journal , vol. 9, no. 12, pp. 9179–9189,\n2022.\n[41] S. Zhang, Y . Liu, X. Zhang, W. Cheng et al., “CAT: Beyond efficient\ntransformer for content-aware anomaly detection in event sequences,”\nin Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining , ser. KDD ’22, 2022, p. 4541–4550.\n[42] X. Cai, R. Xiao, Z. Zeng, P. Gong et al., “Itran: A novel transformer-\nbased approach for industrial anomaly detection and localization,”\nEngineering Applications of Artificial Intelligence, vol. 125, p. 106677,\n2023.\n[43] Y . Li, X. Peng, J. Zhang, Z. Li et al. , “DCT-GAN: Dilated convo-\nlutional transformer-based GAN for time series anomaly detection,”\nIEEE Transactions on Knowledge and Data Engineering , vol. 35,\nno. 4, pp. 3632–3644, 2023.\n[44] C. Ding, J. Zhao, and S. Sun, “Concept drift adaptation for time series\nanomaly detection via transformer,” Neural Processing Letters, pp. 1–\n21, 2022.\n[45] H. Zhang, Y . Xia, T. Yan, and G. Liu, “Unsupervised anomaly detec-\ntion in multivariate time series through transformer-based variational\nautoencoder,” in 2021 33rd Chinese Control and Decision Conference\n(CCDC), 2021, pp. 281–286.\n[46] X. Wang, D. Pi, X. Zhang, H. Liu et al. , “Variational transformer-\nbased anomaly detection approach for multivariate time series,” Mea-\nsurement, vol. 191, p. 110791, 2022.\n[47] J. Xu, H. Wu, J. Wang, and M. Long, “Anomaly transformer: Time\nseries anomaly detection with association discrepancy,” arXiv preprint\narXiv:2110.02642, 2021.\n[48] K. Han, Y . Wang, H. Chen, X. Chen et al. , “A survey on vision\ntransformer,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 1, pp. 87–110, 2023.\n[49] M. Gupta and P. Agrawal, “Compression of deep learning models for\ntext: A survey,” ACM Trans. Knowl. Discov. Data , vol. 16, no. 4, jan\n2022.\n[50] Z. Yang, Y . Wang, K. Han, C. Xu et al. , “Searching for low-bit\nweights in quantized neural networks,” Advances in neural information\nprocessing systems, vol. 33, pp. 4091–4102, 2020.\n[51] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better\nthan one?” in Advances in Neural Information Processing Systems ,\nvol. 32, 2019.\n[52] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531 , 2015.\n[53] X. Liu, P. He, W. Chen, and J. Gao, “Improving multi-task deep neural\nnetworks via knowledge distillation for natural language understand-\ning,” arXiv preprint arXiv:1904.09482 , 2019.\n[54] R. Tang, Y . Lu, L. Liu, L. Mou et al. , “Distilling task-specific\nknowledge from BERT into simple neural networks,” arXiv e-prints ,\np. arXiv:1903.12136, Mar. 2019.\n[55] S. Sun, Y . Cheng, Z. Gan, and J. Liu, “Patient knowledge distillation\nfor BERT model compression,” arXiv e-prints , p. arXiv:1908.09355,\nAug. 2019.\n[56] J. Ko, S. Park, M. Jeong, S. Hong et al., “Revisiting intermediate layer\ndistillation for compressing language models: An overfitting perspec-\ntive,” in Findings of the Association for Computational Linguistics:\nEACL 2023 . Dubrovnik, Croatia: Association for Computational\nLinguistics, May 2023, pp. 158–175.\n[57] Z. Lan, M. Chen, S. Goodman, K. Gimpel et al. , “ALBERT: A lite\nBERT for self-supervised learning of language representations,” in\nInternational Conference on Learning Representations , 2020.\n[58] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit et al. , “Universal\ntransformers,” in International Conference on Learning Representa-\ntions, 2019.\n[59] X. Ma, P. Zhang, S. Zhang, N. Duan et al., “A tensorized transformer\nfor language modeling,” in Advances in Neural Information Processing\nSystems, vol. 32. Curran Associates, Inc., 2019.\n[60] O. Hrinchuk, V . Khrulkov, L. Mirvakhabova, E. Orlova et al. , “Ten-\nsorized Embedding Layers for Efficient Model Compression,” arXiv\ne-prints, p. arXiv:1901.10787, Jan. 2019.\n[61] Y . Abadade, A. Temouden, H. Bamoumen, N. Benamar et al. , “A\ncomprehensive survey on tinyml,” IEEE Access, vol. 11, pp. 96 892–\n96 922, 2023.\n[62] T. S. Ajani, A. L. Imoize, and A. A. Atayero, “An overview of\nmachine learning within embedded and mobile devices–optimizations\nand applications,” Sensors, vol. 21, no. 13, 2021.\n[63] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit et al. , “Attention is\nall you need,” in Advances in Neural Information Processing Systems ,\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach et al., Eds., vol. 30,\n2017.\n[64] A.-K. Seghouane and S.-I. Amari, “The AIC criterion and symmetriz-\ning the Kullback–Leibler divergence,” IEEE Transactions on Neural\nNetworks, vol. 18, no. 1, pp. 97–106, 2007.\n[65] C. M. Bishop, Pattern recognition and machine learning , ser. Infor-\nmation science and statistics. Springer, 2006.\n[66] K. Hundman, V . Constantinou, C. Laporte, I. Colwell et al., “Detecting\nspacecraft anomalies using LSTMs and nonparametric dynamic thresh-\nolding,” in Proceedings of the 24th ACM SIGKDD International Con-\nference on Knowledge Discovery& Data Mining , 2018, p. 387–395.\n[67] A. Abdulaal, Z. Liu, and T. Lancewicki, “Practical approach to asyn-\nchronous multivariate time series anomaly detection and localization,”\nin Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining , 2021, p. 2485–2494.\n[68] H. Xu, W. Chen, N. Zhao, Z. Li et al. , “Unsupervised anomaly\ndetection via variational auto-encoder for seasonal KPIs in web\napplications,” in Proceedings of the 2018 World Wide Web Conference,\n2018, p. 187–196.\n[69] R. Girshick, “Fast R-CNN,” in 2015 IEEE International Conference\non Computer Vision (ICCV) , 2015, pp. 1440–1448.\n[70] D. M. Tax and R. P. Duin, “Support vector data description,” Machine\nlearning, vol. 54, pp. 45–66, 2004.\nVOLUME , 17\nThis article has been accepted for publication in IEEE Open Journal of Signal Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJSP.2023.3333756\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7816014885902405
    },
    {
      "name": "Transformer",
      "score": 0.7697380781173706
    },
    {
      "name": "Anomaly detection",
      "score": 0.6989392042160034
    },
    {
      "name": "Internet of Things",
      "score": 0.6387346982955933
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5241004228591919
    },
    {
      "name": "Artificial neural network",
      "score": 0.49525609612464905
    },
    {
      "name": "Artificial intelligence",
      "score": 0.422727108001709
    },
    {
      "name": "Distributed computing",
      "score": 0.36043012142181396
    },
    {
      "name": "Embedded system",
      "score": 0.35857370495796204
    },
    {
      "name": "Machine learning",
      "score": 0.34592217206954956
    },
    {
      "name": "Real-time computing",
      "score": 0.3380991220474243
    },
    {
      "name": "Data mining",
      "score": 0.33647775650024414
    },
    {
      "name": "Engineering",
      "score": 0.11342394351959229
    },
    {
      "name": "Electrical engineering",
      "score": 0.08797633647918701
    },
    {
      "name": "Voltage",
      "score": 0.07970833778381348
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I93860229",
      "name": "Politecnico di Milano",
      "country": "IT"
    }
  ]
}