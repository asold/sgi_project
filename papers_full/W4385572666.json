{
    "title": "Adaptation Approaches for Nearest Neighbor Language Models",
    "url": "https://openalex.org/W4385572666",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2946176639",
            "name": "Rishabh Bhardwaj",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A2982905579",
            "name": "George Polovets",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3040509197",
            "name": "Monica Sunkara",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2140679639",
        "https://openalex.org/W3163073193",
        "https://openalex.org/W3170490008",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W179875071",
        "https://openalex.org/W4205694376",
        "https://openalex.org/W4313182274",
        "https://openalex.org/W3173360659",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4280534475",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3157700644",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2963123047",
        "https://openalex.org/W3100560913",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W4287887100",
        "https://openalex.org/W2963926728",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W4287649493",
        "https://openalex.org/W4300427681"
    ],
    "abstract": "Semi-parametric Nearest Neighbor Language Models (kNN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting kNN-LMs — 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot (kNN-LM) baselines that construct datastores from the adaptation data. On average, we see perplexity improvements of 17.1% and 16% for these respective baselines, across domains.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1135–1146\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAdaptation Approaches for Nearest Neighbor Language Models\nRishabh Bhardwaj1∗ George Polovets2\n1Singapore University of Technology and Design, Singapore\n2AWS AI Labs\nrishabhbhardwaj15@gmail.com\n{polovg, sunkaral}@amazon.com\nMonica Sunkara2\nAbstract\nSemi-parametric Nearest Neighbor Language\nModels (kNN-LMs) have produced impressive\ngains over purely parametric LMs, by lever-\naging large-scale neighborhood retrieval over\nexternal memory datastores. However, there\nhas been little investigation into adapting such\nmodels for new domains. This work attempts\nto fill that gap and suggests the following ap-\nproaches for adapting kNN-LMs — 1) adapt-\ning the underlying LM (using Adapters), 2)\nexpanding neighborhood retrieval over an ad-\nditional adaptation datastore, and 3) adapting\nthe weights (scores) of retrieved neighbors us-\ning a learned Rescorer module. We study each\nadaptation strategy separately, as well as the\ncombined performance improvement through\nablation experiments and an extensive set of\nevaluations run over seven adaptation domains.\nOur combined adaptation approach consistently\noutperforms purely parametric adaptation and\nzero-shot (kNN-LM) baselines that construct\ndatastores from the adaptation data. On aver-\nage, we see perplexity improvements of 17.1%\nand 16% for these respective baselines, across\ndomains.\n1 Introduction\nNatural Language Processing (NLP) has observed\nlarge performance improvements with recent ad-\nvancements in neural Language Models (LMs).\nThese models have enabled learning rich, semantic\ntext representations (Mikolov et al., 2010; Bengio\net al., 2000) that have facilitated a wide range of\ndownstream language tasks (Radford et al., 2018,\n2019). For the task of next-word prediction, para-\nmetric LMs utilize the rich contextual text repre-\nsentations as input to a classifier (output layer),\nwhich produces a distribution over the possible\nnext words.\nIn contrast to parametric LMs, k-Nearest Neigh-\nbor LMs (kNN-LMs) are semi-parametric models\n∗Work done during an internship at AWS AI Labs.\nFigure 1: An illustration of the proposedkNN-LM adap-\ntation approach. The current context is used as a query\n(q) for nearest-neighbor retrieval. The context is passed\nthrough the LM to obtain the query vector representa-\ntion ⃗ cq, which is then used to retrieve nearest neigh-\nbors from a large pretraining datastore and a smaller\nadaptation datastore (displayed in pink and yellow, re-\nspectively). The function f(·) represents merging of\ndatastores (Merge), followed by rescoring (Rank) of\nthe retrieved neighbors to obtain pkNN. The probabil-\nity distribution over the candidate next words is com-\nputed by the mixture of probabilities pkNN and pLM,\nwhere pLM denotes probabilities obtained from domain-\nadapted LM.\nthat maintain an external memory (i.e. datastore)\n(Khandelwal et al., 2019). This datastore is com-\nposed of key-value pairs, where the keys are con-\ntextual embeddings created from passing text data\nthrough an LM, and the values are the respective\nnext-word labels. The datastore can be used to re-\ntrieve k-nearest neighbors for the current context.\nThe retrieved values induce a probability distribu-\ntion over the next word, which is combined with\nthe LM probabilities.\nThis mixture of probabilities has produced im-\npressive gains over probabilities obtained from\npurely parametric LMs and has been shown to\ngenerate even larger improvements with the in-\n1135\ncrease in the scale of the datastore (Khandelwal\net al., 2019; Yogatama et al., 2021; He et al., 2021).\nWhile the dependency on a large-scale datastore is\neasy to satisfy when developing general-purpose\npretrained models, it is challenging to develop ef-\nfective kNN-LMs when it comes to specialized\ndomains. This is due to the scarcity of domain-\nspecific data, limiting the size of the corresponding\ndatastore.\nWe posit that large, general-purpose datastores,\nreferred to as the pretraining datastore , con-\ntain a significant amount of relevant information\nwhich can still be applied to specialized domains.\nThis information can be leveraged through nearest-\nneighbor retrieval and should prove especially use-\nful in situations where there is an insufficient\namount of domain-specific data to generate an ef-\nfective standalone datastore.\nUnlike parametric neural architectures which\ncan employ gradient-based finetuning for do-\nmain adaptation, it is less obvious how to adapt\nkNN-LMs primarily because of the non-parametric\nnature of datastores. One simple approach would\nbe to reconstruct the datastore, using domain-\nadapted LM representations. However, this comes\nat the cost of incurring a large memory footprint\nfor each adaptation domain. In this work, we in-\nstead choose to focus on adaptation strategies that\nare parameter and memory efficient. Given the\ncomplementary nature of the parametric and non-\nparametric components in a kNN-LM, we pursue\nadaptation strategies separately for each component\nand analyze their impact on the kNN-LM system’s\nadaptation performance.\n1. Adaptation of the parametric LM: Given that\nwe constrain ourselves to parameter-efficient\nadaptation techniques, we utilize Adapters\n(Houlsby et al., 2019) for finetuning the para-\nmetric LM because of their competitive perfor-\nmance with full model finetuning (Hou et al.,\n2022; Liu et al., 2022). We also investigate the\nimpact of adapting the parametric component\non the quality of retrieved neighbors from the\npretraining datastore.\n2. Adaptation of the non-parametric kNN: As a\nmemory-efficient alternative to reconstructing\nthe pretraining datastore with domain-adapted\nrepresentations, we formulate kNN adapta-\ntion as learning a domain-specific neighbor-\nhood scoring function (i.e. a Rescorer). This\nproposed Rescorer is trained to assign opti-\nmal weights to each retrieved neighbor for\na given domain. We also consider expand-\ning our neighborhood retrieval to include an\nadditional datastore referred to as the adapta-\ntion datastore, created purely from the target\ndomain. Relative to the pretraining datastore,\nthe addition of the adaptation datastore further\nincreases the memory footprint by an incre-\nmental amount.\nIn line with previous works, we focus our experi-\nments solely on the core Language Modeling task\nof next-word prediction (Khandelwal et al., 2019;\nYogatama et al., 2021). Results on seven adaptation\ndomains ranging from science and books, to con-\nversational text, demonstrate that our component-\nlevel strategies consistently improve over respec-\ntive parametric and semi-parametric baselines, and\nproduce even better results when combined to-\ngether. Specifically, we find that adaptation of the\nparametric component increases recall of ground-\ntruth labels found in the retrieved neighbors. We\nalso confirm that the large-scale pretraining data-\nstore contains relevant information for adaptation\ndomains, via its performance edge over models that\nexclude it. Finally, we observe that expanding the\nnearest neighbor search to include elements from\nthe adaptation datastore contributes to the best over-\nall performing strategy. Figure 1 demonstrates the\noverall approach using Wikipedia and US News\nas example pretraining and adaptation domains,\nrespectively.\n2 kNN-LMs\nFor a context ct defined by the sequence of words\n(w1,...,w t−1), the causal language modeling task\naims to model a probability distribution over the\nnext word1 wt. Let pLM(wt|ct) and pkNN(wt|ct)\nbe the probability masses computed by the LM\nand kNN components, respectively. Details on\nhow pkNN(wt|ct) is computed and combined with\npLM(wt|ct) to produce the final kNN-LM predic-\ntions, are outlined in the following sections.\nDatastore creation: Given a source domain train-\ning set Xs, let ci = (w1,...,w t−1) be a sequence\nin Xs. The datastore is defined as a set of Ds tuples\n{(⃗ ci,wi)}Ds\ni=1, where the key ⃗ ci ∈Rdh denotes the\ncontextual representation of ci, produced by the\n1We use “token” and “word” interchangeably.\n1136\nLM and value wi denotes the next word label in the\nsequence.\nk-Nearest neighbor retrieval: During inference,\nwe obtain a query vector ⃗ cq ∈Rdh for kNN re-\ntrieval by producing the contextual LM represen-\ntation for the current sequence of tokens cq. The\nneighborhood of ⃗ cq is constructed by retrieving\nits k nearest instances from the datastore. Let\nD(·) : R2dh →R refer to the distance measure 2.\nThe k-nearest neighbors of ⃗ cq can be obtained by:\nK:= arg min\nk\n{D(⃗ cq,⃗ ci)}i∈[Ds] (1)\nwhere k in the subscript denotes indices in\n[Ds]={1,...,D s}which corresponds to k smallest\ndistances. The score (weight) si of a neighbor key\n⃗ ci is defined as:\nsi := ||⃗ cq −⃗ ci||2,i ∈K (2)\nThus, the kNN probability of the next word can be\nobtained via:\npkNN(wt|ct) ∝\n∑\ni∈K\n1 [wi=wt] exp(−si). (3)\nUnifying kNN and LM: The probability distri-\nbution of the kNN-LM system can be obtained by\ninterpolating the component probabilities\npkNN-LM(wt|ct) =\nλpkNN(wt|ct) + (1−λ) pLM(wt|ct) (4)\nwhere λ∈[0,1].\nSince each probability distribution lies on a sim-\nplex spanning the token vocabulary, performing a\nconvex combination of the two maintains a valid\nprobability distribution.\n3 kNN-LM Adaptation\n3.1 Retrieval Quality Metrics\nBeyond tracking LM perplexity improvement, we\nalso introduce two simple metrics to measure the\nrelevance and quality of retrieved neighborhoods.\nFor neighborhood relevance, we define Recall as\nthe fraction of times a ground-truth word is in the\nretrieved set of neighbors. For neighborhood qual-\nity, we denote Precision as the fraction of times the\n2In practice large-scale datastores utilize approximate\nsearch methods for retrieval, detailed further in Section 4.1.\nkNN assigns more probability to the ground truth\ntoken than the LM. We define:\nPrecision =\nN∑\nt=1\n1 [pLM(w∗\nt|ct) <pkNN(w∗\nt|ct)]\nN ,\nRecall =\nN∑\nt=1\n1 [w∗\nt ∈Kt]\nN .\nwhere Kt := {⃗ wi : i∈[K]}, w∗\nt is the ground truth\nnext word for the context ⃗ ct, and N is the total\nnumber of words in the dataset.\n3.2 Parametric LM Adaptation\nWe follow a parameter-efficient adaptation ap-\nproach by keeping the pretrained LM fixed and\nlearning Adapter modules, attached to all the model\nlayers (Houlsby et al., 2019). Henceforth, we use\nLMa to denote the domain-adapted LM.\nWhile Adapter-based models have shown per-\nformance on par with model fine-tuning on vari-\nous adaptation tasks across domains (Pfeiffer et al.,\n2020), the impact of LM adaptation in a semi-\nparametric setting remains unclear. Given our con-\nstraint to keep the pretraining datastore keys static,\nupdates to the parametric LM could create a mis-\nmatch between contextual representations used for\nquerying and result in meaningless retrieved neigh-\nborhoods. We posit that Adapter-based LMs do not\nsuffer from this because they preserve the metric\nspace induced by the LM contextual encodings 3.\nAdapters tune representations in the original space\nsuch that they are more relevant to the adaptation\ndomain.\nHypothesis-1 : LM adaptation with\nAdapters not only assists the parametric\nmodels to perform better (↓perplexity),\nbut also improves the quality of neigh-\nborhoods retrieved from the pretraining\ndatastore (↑Recall).\n3.3 kNN Adaptation\nGiven that we choose to keep the memory foot-\nprint of our adaptation approaches small (relative\nto the pretraining footprint), we fix the pretraining\ndatastore representations (and thus the Recall of\nthe retrieved neighborhoods) and instead focus on\nimproving the Precision. This leads to our second\nhypothesis:\n3This is due to Adapters keeping the pretrained LM weight\nmatrices frozen, thus preserving the coordinate space that is\nprojected onto, when extracting contextual representations.\n1137\nHypothesis-2 : Using squared L2 dis-\ntance between the query and neighbor\nkey vectors is not the optimal neighbor\nscoring scheme. Instead, a more optimal\nscoring function can be learned for each\ndomain.\nWe propose learning a domain-optimized scor-\ning function (a Rescorer) that learns to assign\nmore weight to retrieved neighbors containing the\nground-truth label. We discuss the setup and ar-\nchitecture for the Rescorer in more detail subse-\nquently.\nRescorer Formulation: Given a query vector ⃗ cq\nobtained from LMa, we retrieve a large set of neigh-\nbors K. Each retrieved neighbor tuple (⃗ ci,wi) ∈K\nis passed through a neural module to obtain a\ndomain-optimized score sr\ni. Let fr(·) :Rdr →R\ndenote the Rescorer function. Its input is a set of\nthree vectors: query ⃗ cq, neighbor key vector ⃗ ci, to-\nken embedding of the neighbor value wi, as well\nas six features ⃗ xi = {x1,...,x 6}obtained from\nthe pairwise dot products and pairwise euclidean\ndistances between these three vectors4. The total\ninput dimension is; dr = 3dh+ 6where dh is the di-\nmension of the LM contextual representation. The\nfinal neighbor score s′\ni can be computed as5:\nsr\ni = fr([⃗ ci,⃗ cq, ⃗ wi,⃗ xi]) (5)\ns′\ni = sr\ni −si (6)\nRescorer Architecture: We employ a three-\nlayer fully-connected network for the Rescorer\narchitecture. The input vectors are first layer-\nnormalized and concatenated. They are then\npassed through two ReLU-activated dense layers\nwith a skip connection Rdr → R128 → R128\nand a final dense (regression) layer R128 →R1\nto generate the neighbor’s score. The overall\nRescorer workflow is shown in Figure 2.\nRescorer Training: We train the Rescorer to dis-\ncriminate neighbors containing the ground truth as\ntheir values, by employing Contrastive Learning.\nWe construct positive examples for retrieved neigh-\nbor tuples (⃗ ci,wi) if wi corresponds to the correct\nground-truth next word, otherwise they are treated\nas negatives. We collect contextual embeddings\n4We find that using these extra features produces the best\nquality Rescorer.\n5We empirically observed that combining the learned and\ndistance-based scores produces the best results.\nFigure 2: kNN-LM Rescorer workflow. Neighbors ex-\ntracted using the query contextual embedding are passed\nto a fully-connected Rescorer network. The scores\noutput from the network are used to produce adapted\nnearest-neighbor probabilities pkNN, which are used in\nthe final pw calculation.\nfor one million tokens from the adaptation domain\ntraining split6 {w1,...,w 1M}along with their near-\nest neighbors. Contrastive training examples are\ndiscarded if the ground-truth word is not found\nin the neighborhood values. From each neighbor-\nhood, the highest-scored (distance-based) positive\nneighbor is selected and 10 negative neighbors are\nrandomly sampled. Contrastive Loss (Oord et al.,\n2018) is used to learn the Rescorer parameters and\nis defined as:\nL= −log exp (\nsr\np\nτ )\nexp (\nsrp\nτ ) +∑\nn\nexp (srn\nτ )\n(7)\nwhere sr\np and sr\nn denote the Rescorer scores as-\nsigned to the positive and negative examples, re-\nspectively, and τ is a temperature hyperparameter.\n3.4 Merging kNNs\nWhile regenerating the pretraining datastore using\nan adapted LM (Section 3.2) is generally a very\nmemory-intensive procedure, creating a separate\ndatastore purely from adaptation training data is\nexpected to increase the memory footprint by a\nrelatively small amount 7. With the availability\nof both pretraining and adaptation datastores,\na natural extension is to utilize both during\nneighborhood retrieval. We extract the nearest\nneighbors independently from the pretraining\n6If the training set has less than one million tokens, we\nutilize all of its tokens.\n7In our experimental setup, this amounts to 1-10% relative\nincrease in memory footprint\n1138\ndatastore Kw and adaptation datastore Ka and\nmerge them to create Ka ∪Kw.\n3.5 Adaptation of kNN-LMs\nWe summarize the overall adaptation strategy out-\nlined in prior sections as follows:\n1. Updating the parametric LM using\nlightweight Adapter modules.\n2. Merging the retrieved neighbors from the pre-\ntraining and adaptation datastores into a single\nneighborhood.\n3. Training a Rescorer with Contrastive Loss,\nto learn domain-optimal scores for retrieved\nneighbors.\nIn the following results sections, we confirm\nthe validity of Hypothesis-1 and Hypothesis-2, as\nwell as the efficacy of our simple neighborhood\nmerging scheme through ablation studies. We also\ninvestigate the benefit of our collective adaptation\nstrategy on modeling downstream domains.\n4 Experiments\n4.1 Experimental Setup\nFor all of our experiments, we utilize the off-the-\nshelf GPT-2 (Radford et al., 2019) model from\nHuggingface Transformers (Wolf et al., 2019), as\nthe pretrained LM. This model contains 117 mil-\nlion parameters with a vocabulary size of 50,257\nword units, and directly matches the decoder-only\nconfiguration used in Khandelwal et al. (2019). For\nthe adaptation setting, Adapter modules are added\nto each layer of the pretrained GPT-2 resulting in\n0.7% extra parameters during finetuning. Training\nthe Rescorer also amounts to learning an incre-\nmental 320K parameters, or roughly 0.3% addi-\ntional parameters relative to that of GPT-2. The\nRescorer and Adapters are trained (separately) us-\ning AdamW optimizer with learning rates of 0.001\nand 0.0001, respectively and a weight decay of\n0.01. For sampling positive and negative exam-\nples during Rescorer training, we utilize a liberally\nsized neighborhood of size k=1000. Logging is\nperformed every 200 iterations and early stopping\nis performed if there is no validation performance\nimprovement for up to three logging steps.\nThe pretraining datastore is constructed from\nrunning GPT-2 on 1 billion tokens sampled from\nXsumSciQ arXivBookSumSAMSumXLSumGovSum\n99.5 1.0 77.3 4.5 0.4 100.0 10.5\nTable 1: Adaptation datastore size (in millions of en-\ntries).\nWikipedia8 (i.e. Kw) and any adaptation datas-\ntores are constructed from up to 100 million to-\nkens taken from the training split of adaptation\ndomains (i.e. Ka). We select seven datasets across\nmultiple domains to evaluate the performance of\nour adaptation strategies: XSum (Narayan et al.,\n2018) and XL-Sum (Hasan et al., 2021) covering\nthe news domain; SciQ (Johannes Welbl, 2017) and\narXiv (Cohan et al., 2018) for the science domain;\nBookSum (Kryscinski et al., 2021) for the liter-\nature domain, SAMSum (Gliwa et al., 2019) for\nthe conversational domain, and GovReport (Huang\net al., 2021) for the government domain. For any\nsummary-related datasets, we only utilize the orig-\ninal document text for our purposes and exclude\nsummary ground-truth text. Table 1 provides a\nbreakdown of the resulting adaptation datastore\nsizes.\nFor nearest neighbor retrieval, we use FAISS\n- a library designed for fast similarity search in\nhigh dimensional space (Johnson et al., 2019).\nSimilar to Khandelwal et al. (2019), we observe\nthat L2-based FAISS search obtains better re-\nsults than the inner-product, so we adopt this set-\nting for our work as well. For all experiments,\nwe perform hyperparameter search over k where\nk∈{1,2,4,..., 512,1000}and the kNN interpola-\ntion parameter λ∈{0.01,0.02,0.04,..., 0.98}.\n4.2 Models Used for Evaluation\nBecause our work is the first to explore the in-\ntersection of LM adaptation with semi-parametric\nLMs, we use relevant baselines from both individ-\nual methodologies to track our modeling improve-\nments. We provide the pretraining (w)kNN and\nadaptation (a)kNN neighborhood retrieval perplex-\nities for reference9, to illustrate relevance of the pre-\ntraining domain to target domains and relationship\nbetween retrieval quality and datastore size. For\nthe LM adaptation baseline, we compare against\nthe performance of parametric finetuning with\nAdapters LMa. For the semi-parametric LM base-\n8(Foundation)–https://huggingface.co/\ndatasets/wikipedia\n9(w)kNN is obtained by putting λ = 0.9999 in Equa-\ntion (4) to tackle cases where ground truth is not present in\nthe retrieved neighborhood.\n1139\nlines, we use two types of zero-shot evaluations\nof the kNN-LM. One applies zero-shot evaluation\nusing the pretrained datastore (w)kNN-LM and the\nother evaluates using a datastore constructed out of\nthe adaptation domain training data (a)kNN-LM.\nThe latter strategy, also presented in Khandelwal\net al. (2019), to the best of our knowledge, is the\nonly other work that utilizes adaptation data with\nkNN-LMs.\nBeyond these models, we perform extensive ex-\nperimentation with different combinations of data-\nstores to use for retrieval (Wikipedia -(w), Adap-\ntation training split - (a), Both - (w+a)), types of\nparametric LMs (Pretrained LM - LM, Adapted\nLM - LMa), and usage of Rescorers (Rescorer used\n- kNNr, No Rescorer used - kNN). These combi-\nnations provide precise ablations of our adaptation\ncomponent improvements and their interactions\nwith one another.\n5 Results and Discussions\n5.1 Hypothesis Pilot Studies\nWe first motivate our larger experimental ef-\nfort with pilot studies that test out Hypothesis-1,\nHypothesis-2, and the neighborhood merging strat-\negy. These studies are run on a subset of 100K test\ntokens taken from each adaptation domain. In our\ninitial pilot experiments, we find that using k=1000\nneighbors produces the best results. Any adapta-\ntion strategy requiring gradient-based optimization\nis performed on the respective adaptation training\nsplits.\nEvaluating Hypothesis-1: To test this hypothe-\nsis, we measure the impact of LM adaptation on\nretrieval quality from the pretraining , by observ-\ning changes to the kNN’s Recall value. Table 2\ndemonstrates that adaptation of the parametric LM\n(LMa) improves not only perplexity, but also re-\ntrieval Recall (retrieved neighbors using LMa are\ndenoted by kNN*, while neighbors retrieved with\nthe pretrained LM are denoted by kNN). This ap-\npears to support our hypothesis that techniques like\nAdapters, which preserve the LM representation\nspace, can also benefit the retrieval component.\nEvaluating Hypothesis-2: To test whether\nRescorer-generated scores improve over purely\ndistance-based scores, we contrast the resulting Pre-\ncision of both types of scoring methods. Table 3\nshows that the domain-adapted scores produced by\nthe Rescorer yield significantly higher neighbor-\nDomain Perplexity ( ↓) Recall ( ↑)\nLM LM a kNN kNN* kNN kNN*\nXSum 22.45 18.95 83.95 74.67 88.72 89.29\nSciQ 22.15 16.10 46.86 38.64 92.53 93.26\narXiv 56.83 24.97 513.44 270.11 77.54 79.89\nBookSum 21.15 20.45 64.92 62.15 90.14 90.34\nSAMSum 46.86 32.25 298.08 228.99 96.36 96.64\nXL-Sum 24.87 21.84 100.92 89.65 87.98 88.60\nGovReport 19.3114.72 83.62 66.91 88.55 89.47\nTable 2: Hypothesis -1 pilot study. Adapted LM repre-\nsentations improve both perplexity and retrieval Recall.\nPrecision (↑)\nDomain (w) kNN (w) kNNr (a)kNN (a) kNNr\nXSum 29.6 44.9 45.9 59.8\nSciQ 33.9 48.2 45.8 53.0\narXiv 25.6 38.2 52.8 65.4\nBookSum 33.1 54.7 33.7 50.1\nSAMSum 25.9 27.7 37.0 38.6\nXL-Sum 29.9 46.7 43.9 58.5\nGovReport 25.7 42.6 43.7 55.9\nTable 3: Hypothesis-2 pilot study. Applying the\nRescorer leads to Precision improvements in neighbors\nretrieved from both pretraining and adaptation datas-\ntores.\nhood Precision on average than those using purely\nL2-based scoring. This applies for neighbors re-\ntrieved from the pretraining datastore (w)kNNr, as\nwell as from datastores constructed from adapta-\ntion domain samples (a)kNNr. This suggests that\nthe Rescorer can act as a general-purpose improve-\nment over the standard kNN-LM setup, regardless\nof whether neighbors are retrieved from in-domain\nor out-of-domain datastores. The improvement in\nPrecision also confirms the efficacy of Contrastive\nLearning in producing a Rescorer that can discrimi-\nnate between neighbors containing the ground-truth\ntoken from those that don’t.\nEffectiveness of Neighborhood Merging To test\nthe effectiveness of the simple neighborhood merg-\ning strategy, we contrast the Recall of merged\nneighborhoods to those of standalone neighbor-\nhoods from each individual datastore. In this study,\nwe keep the total number of retrieved neighbors\nfixed and empirically find that retrieving 500 near-\nest neighbors from each datastore in the merging\nstrategy works best. The results of this study (Ta-\nble 4) show that the combined set of neighbors\nKa ∪Kw has a better Recall value than either indi-\nvidual neighborhood. Due to this observed Recall\nimprovement, we use this simple merging tech-\n1140\nRecall (↑)\nDomain (w) kNN (a) kNN (w+a) kNN\nXSum 89.3 92.7 93.1\nSciQ 92.5 91.5 94.7\narXiv 79.9 91.9 92.1\nBookSum 90.3 86.8 91.5\nSAMSum 84.3 85.7 88.9\nXL-Sum 88.6 92.1 92.5\nGovReport 89.5 92.5 93.6\nTable 4: Recall improvement from merging kNNs.\nMerged neighborhoods consistently obtain higher Re-\ncall than those obtained from the individual datastores.\nnique in our overall adaptation strategy. When\ntraining a Rescorer on these merged neighborhoods,\nwe pass an additional binary input feature to inform\nthe model on which datastore a particular neighbor\ncomes from.\n5.2 Domain Adaptation Evaluations\nTable 5 compares the perplexities of the various\nmodels evaluated on the seven adaptation test sets.\nFirst we note that while the adapted LM yields the\nexpected perplexity reductions over the pretrained\nLM (LMa < LM), we observe that zero-shot evalua-\ntion of the pretrainedkNN-LM also performs better\nthan the pretrained LM ((w)kNN-LM < LM). This\ncontinues to confirm the capacity of the pretraining\ndatastore to retrieve relevant neighbors for down-\nstream domains. We also find that in a majority of\nthe cases, zero-shot evaluation of a kNN-LM con-\nstructed over the adaptation datastore, outperforms\nparametric adaptation ((a)kNN-LM < LMa). This\ncorroborates the finding from Khandelwal et al.\n(2019), where utilizing data for neighborhood re-\ntrieval can outperform using it for LM training.\nThe results further support our Hypothesis-1,\nnamely that parametric LM adaptation improve-\nment is compounded when used in the kNN-LM\nsetting (e.g. (w)kNNr < LMa < (w)kNN-LMa).\nThey also add support for Hypothesis-2 where\nthe Rescorer acts as a general-purpose improve-\nment to kNN-LM models (by noting that kNNr-\nbased models outperform respective kNN-based\nmodels). We observe that merging neighborhoods\nfrom both datastores also provides some small per-\nplexity gains. Overall, our combined adaptation\napproach (last row of Table 5) produces an average\nof 17.1% and 16% perplexity improvement over the\nparametric adaptation LMa and semi-parametric\nbaselines (a)kNN-LM respectively.\nPretraining-datastore under low-resource adap-\ntation. We analyze the impact on Recall when\ncombining neighbors from the pretraining and\nadaptation datastores in a low-resource adaptation\nsetting (which is a common scenario). We utilize\nthe Xsum dataset (containing nearly 100M training\ntokens), to analyze the impact of merging retrieved\nneighborhoods for different sizes of the adaptation\ndatastore. In Figure 3-a), we observe that the Re-\ncall of retrieved neighbors significantly decreases\nas the adaptation datastore size decreases (green,\nKa). However, the merged neighborhood Recall\nenjoys a relatively flat curve (blue, Ka∪Kw). This\nsuggests that the pretraining datastore acts as an\nimportant buffer in maintaining high-quality neigh-\nborhoods for low-resource adaptation scenarios.\nA complementary study to consider for the low-\nresource setting is the impact of the size of the\npretraining datastore on the merged retrieval Re-\ncall. In this set of experiments, we fix the size of\nthe adaptation datastore to be 100K. From Figure 3-\nb), we observe that Recall monotonically increases\nwith the size of the pretraining datastore and may\ncontinue to improve even after the pretraining data-\nstore exceeds 1 billion tokens. Thus, scaling the\npretraining datastore can lead to improved retrieval\nquality on downstream domains.\nFigure 3: a) Change in Recall of merged neighborhoods\ncompared against the size of the adaptation datastore; b)\nchange in Recall of merged neighborhoods compared\nagainst the size of the pretraining datastore.\nWhich LM representations are better for data-\nstore construction? An important question to\nconsider, is which representations from GPT-2 are\nmost useful in constructing the datastore. To inves-\ntigate this, we experiment with using different lay-\ners from GPT-2 in constructing a Wikipedia-based\ndatastore. To increase the throughput experimen-\ntation, we use a smaller-sized datastore of size 10\nmillion. We consider the output of the penultimate\nTransformer block as well as the following layers\nfrom the last Transformer block in our analysis:\n1141\nConfiguration Perplexity ( ↓)\nSetting LM LMa (w)KNN (a)KNN rescoreXSum SciQ arXiv BookSum SAMSum XL-Sum GovReport\nBaseline\nLM (only) ✓ 22.45 22.15 56.83 21.15 46.86 24.87 19.32LMa(only) ✓ 18.95 16.09 24.97 20.45 32.26 21.84 14.72(w)kNN (only) ✓ 83.96 46.86 513.45 64.92 298.08 100.92 83.62(a)kNN (only) ✓ 38.38 57.82 87.58 109.87 229.89 47.75 40.39\nBaseline(2019) (w)kNN-LM ✓ ✓ 21.64 19.19 53.03 20.50 46.27 24.03 18.99(a)kNN-LM ✓ ✓ 17.01 14.71 24.38 20.60 39.99 19.39 14.87\nOurs\n(w)kNN-LMa ✓ ✓ 18.42 14.62 24.42 19.72 31.94 21.22 14.47(w)kNNr-LM ✓ ✓ ✓ 21.32 18.5 51.89 20.09 46.20 23.68 18.81(w)kNNr-LMa ✓ ✓ ✓ ✓ 18.23 14.22 24.19 19.35 31.92 20.98 14.36(a)kNN-LMa ✓ ✓ 15.30 12.88 17.81 20.22 31.48 18.12 13.08(a)kNNr-LMa ✓ ✓ ✓ 14.85 12.72 17.49 20.09 31.47 17.72 12.87(w+a)kNN-LMa ✓ ✓ ✓ 15.20 12.15 17.85 19.72 31.20 17.99 13.01(w+a)kNNr-LMa ✓ ✓ ✓ ✓ 14.71 11.95 17.47 19.42 31.18 17.53 12.79\nTable 5: Performance of different kNN-LM configurations. (w)kNN, (a)kNN, and (a+w)kNN denote neighborhood\nsearch from pretraining datastore, adaptation datastore, and equal contribution from both, respectively. LM and\nLMa denote standard GPT-2 and domain-adapted GPT-2.\nfirst layer norm (LN1), output of Multi-Headed At-\ntention (MHA), second layer norm (LN2), output\nof final feed-forward layer (FFN). Thus, each datas-\ntore differs only in its key vector representations⃗ cq\nfor a given context cq. The kNN-LM probability is\ncomputed as per Equation (4) where k is set to1000\nand λis a hyperparameter tuned via grid search in\nλ∈{0.01,0.02,0.04,..., 0.98}. Evaluation is per-\nformed on 100K test tokens obtained from unseen\nWikipedia documents.\nFigure 4: The figure shows the internals of GPT-2 final\nlayer. Perplexity (PPL) and Recall score of different\ncontext vector (⃗ cq) candidate representations for datas-\ntore construction.\nAs shown in Figure 4, we observe that using the\noutput of the LN2 layer creates the best represen-\ntation space for the datastore keys and produces\nthe best test perplexity of 23.61 and highest Recall\nof 0.86. We also observe that the best λreturned\nfor an LN2-based kNN-LM is 0.1, which is the\nhighest among context representation candidates\nconsidered.\nComputational cost. We compare our compu-\ntational overhead with respect to the standard\nkNN-LM proposed by Khandelwal et al. (2019).\nDuring inference, an Adapter increases the infer-\nence time of GPT-2 by about 1.2 milliseconds per\ntoken. The Rescorer takes about 60 milliseconds\nper token to score 1000 neighbors. We run the\nparametric model on a single GPU10 kNN and the\nRescorer on CPU.\n6 Related work\nOur proposed work investigates the intersec-\ntion of techniques used for parametric Language\nModel adaptation with semi-parametric systems\n(kNN-LMs). Therefore we discuss the related\nworks in each of these areas and contrast our re-\nspective contributions.\nParametric LM Adaptation Popularization of\nLarge-Scale Pretrained Language Models (PLMs)\nhas necessitated research into parameter-efficient\nadaptation methods, to avoid maintaining large\nmodels for each domain. Many parameter-efficient\nmethods keep the pretrained LM parameters frozen\nand learn additional layers during adaptation\n(Houlsby et al., 2019; Ben-Zaken et al., 2022),\nor modify the parameters of existing layers (Hu\net al., 2022; Hou et al., 2022). This work explores\nhow applying such techniques (namely Adapters)\ncan improve the semi-parametric LM adaptation\nperformance.\nSemi-Parametric KNN-LMs Previous works\nhave motivated that scaling the datastore for large-\nscale retrieval acts as a complimentary path to scal-\ning data used for LM training (Khandelwal et al.,\n2019; Borgeaud et al., 2022; Khandelwal et al.,\n2021). However, adaptation approaches of these\n10Tesla V100-SXM2-16GB\n1142\nsemi-parametric systems beyond zero-shot evalu-\nation (Khandelwal et al., 2019; Khandelwal et al.,\n2021) have not been explored up until this work.\nTo improve the quality of retrieval-enhanced\nmethods, neighborhood Rescorer techniques have\nbeen employed for other domains such as Q&A\n(Glass et al., 2022) and information retrieval\n(Nogueira and Cho, 2019). In contrast, this work\nexplores applications of Rescorer techniques for\nthe Language Modeling task and considers them\nfor lightweight adaptation of semi-parametric LMs.\n7 Conclusion\nWe proposed a multi-pronged strategy for adapting\nkNN-LM systems. Through our studies, we demon-\nstrated that a general-purpose pretraining datastore\ncontains relevant information, which can be uti-\nlized for downstream domains. We showed that\nparametric and non-parametric adaptation methods\ncomplement each other and that using the complete\nsemi-parametric adaptation strategy outperforms\nadapting just one of thekNN-LM components. Our\nmethods could further be extended by noting that\nthe Recall of retrieved neighborhoods is often im-\nperfect. Thus, a gate could be learned to predict\nwhether kNN retrieval should be triggered. While\nour study focused on the Language Modeling task,\nour approach could be applied towards other NLP\ntasks such as text generation and translation.\n8 Acknowledgement\nThe authors express their gratitude to Kyu Han\nand Shiva Sundaram for their continuous support\nthroughout this work. They are also appreciative\nof Omid Sadjadi, Sundararajan Srinivasan, and Ze-\njiang Hou for providing valuable feedback on the\npreliminary draft.\nReferences\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2022. Bitfit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. ArXiv, abs/2106.10199.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. Ad-\nvances in neural information processing systems, 13.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, T. W. Hennigan,\nSaffron Huang, Lorenzo Maggiore, Chris Jones, Al-\nbin Cassirer, Andy Brock, Michela Paganini, Geof-\nfrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and L. Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In ICML.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 615–621, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nWikimedia Foundation. Wikimedia downloads.\nMichael R. Glass, Gaetano Rossiello, Md. Faisal Mah-\nbub Chowdhury, Ankita Rajaram Naik, Pengshan\nCai, and A. Gliozzo. 2022. Re2g: Retrieve, rerank,\ngenerate. ArXiv, abs/2207.06300.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. arXiv preprint arXiv:2109.04212.\nZejiang Hou, Julian Salazar, and George Polovets.\n2022. Meta-learning the difference: Preparing large\nlanguage models for efficient adaptation. ArXiv,\nabs/2207.03509.\n1143\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2022. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization.\nMatt Gardner Johannes Welbl, Nelson F. Liu. 2017.\nCrowdsourcing multiple choice science questions.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535–547.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. ArXiv, abs/2010.00710.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nWojciech Kryscinski, Nazneen Fatema Rajani, Di-\nvyansh Agarwal, Caiming Xiong, and Dragomir R\nRadev. 2021. Booksum: A collection of datasets for\nlong-form narrative summarization.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efficient fine-tuning is\nbetter and cheaper than in-context learning. ArXiv,\nabs/2205.05638.\nTomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. In Interspeech,\nvolume 2, pages 1045–1048. Makuhari.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. ArXiv, abs/1808.08745.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with bert. ArXiv, abs/1901.04085.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. Adapterhub: A\nframework for adapting transformers. arXiv preprint\narXiv:2007.07779.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 9:362–373.\n1144\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1145\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n1146"
}