{
  "title": "HumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool",
  "url": "https://openalex.org/W4390266803",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3020934139",
      "name": "Raghav Awasthi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187395276",
      "name": "Shreya Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169873663",
      "name": "Dwarikanath Mahapatra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001486182",
      "name": "Ashish Khanna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2530968224",
      "name": "Kamal Maheshwari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2436520354",
      "name": "Jacek Cywinski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185804245",
      "name": "Frank Papay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993523133",
      "name": "Piyush Mathur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3020934139",
      "name": "Raghav Awasthi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187395276",
      "name": "Shreya Mishra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169873663",
      "name": "Dwarikanath Mahapatra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001486182",
      "name": "Ashish Khanna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2530968224",
      "name": "Kamal Maheshwari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2436520354",
      "name": "Jacek Cywinski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185804245",
      "name": "Frank Papay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993523133",
      "name": "Piyush Mathur",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4360989593",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4376312610",
    "https://openalex.org/W4388184238",
    "https://openalex.org/W4387559581",
    "https://openalex.org/W3107855336",
    "https://openalex.org/W3139277020",
    "https://openalex.org/W4223555496",
    "https://openalex.org/W4387356024",
    "https://openalex.org/W3153978352",
    "https://openalex.org/W4385970120",
    "https://openalex.org/W4385849309",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3016254928",
    "https://openalex.org/W3093632820",
    "https://openalex.org/W3171394081",
    "https://openalex.org/W3034275286",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4221149706",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4378464977",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4318903372",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W4375870056",
    "https://openalex.org/W4378170222",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4387210426"
  ],
  "abstract": "A bstract Large language models (LLMs) have caught the imagination of researchers,developers and public in general the world over with their potential for transformation. Vast amounts of research and development resources are being provided to implement these models in all facets of life. Trained using billions of parameters, various measures of their accuracy and performance have been proposed and used in recent times. While many of the automated natural language assessment parameters measure LLM output performance for use of language, contextual outputs are still hard to measure and quantify. Hence, human evaluation is still an important measure of LLM performance,even though it has been applied variably and inconsistently due to lack of guidance and resource limitations. To provide a structured way to perform comprehensive human evaluation of LLM output, we propose the first guidance and tool called HumanELY. Our approach and tool built using prior knowledge helps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable manner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm and comparison. Additional submetrics within these five key metrics provide for Likert scale based human evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable LLM evaluation and provide data for comparison against different users performing human evaluation. While all metrics may not be relevant and pertinent to all outputs, it is important to assess and address their use. Lastly, we demonstrate comparison of metrics used in HumanELY against some of the recent publications in the healthcare domain. We focused on the healthcare domain due to the need to demonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We anticipate our guidance and tool to be used for any domain where LLMs find an use case. Link to the HumanELY Tool https://www.brainxai.com/humanely",
  "full_text": "HumanELY: Human evaluation of LLM yield, using a novel\nweb-based evaluation tool\nA PREPRINT\nRaghav Awasthi1, Shreya Mishra1, Dwarikanath Mahapatra1, Ashish Khanna1, Kamal Maheshwari1\nJacek Cywinski1, Frank Papay1, Piyush Mathur (Corresponding Author)1,*\n1BrainXAI ReSearch, BrainX, LLC.\n*pmathurmd@gmail.com\nDecember 30, 2023\nABSTRACT\nLarge language models (LLMs) have caught the imagination of researchers,developers and public\nin general the world over with their potential for transformation. Vast amounts of research and\ndevelopment resources are being provided to implement these models in all facets of life. Trained\nusing billions of parameters, various measures of their accuracy and performance have been proposed\nand used in recent times. While many of the automated natural language assessment parameters\nmeasure LLM output performance for use of language, contextual outputs are still hard to measure and\nquantify. Hence, human evaluation is still an important measure of LLM performance,even though it\nhas been applied variably and inconsistently due to lack of guidance and resource limitations.\nTo provide a structured way to perform comprehensive human evaluation of LLM output, we propose\nthe first guidance and tool called HumanELY . Our approach and tool built using prior knowledge\nhelps perform evaluation of LLM outputs in a comprehensive, consistent, measurable and comparable\nmanner. HumanELY comprises of five key evaluation metrics: relevance, coverage, coherence, harm\nand comparison. Additional submetrics within these five key metrics provide for Likert scale based\nhuman evaluation of LLM outputs. Our related webtool uses this HumanELY guidance to enable\nLLM evaluation and provide data for comparison against different users performing human evaluation.\nWhile all metrics may not be relevant and pertinent to all outputs, it is important to assess and address\ntheir use.\nLastly, we demonstrate comparison of metrics used in HumanELY against some of the recent\npublications in the healthcare domain. We focused on the healthcare domain due to the need to\ndemonstrate highest levels of accuracy and lowest levels of harm in a comprehensive manner. We\nanticipate our guidance and tool to be used for any domain where LLMs find an use case.\nLink to the HumanELY Tool: https://www.brainxai.com/humanely\nKeywords Large Language Models, Model Evaluation, Human Evaluation\n1 Introduction\nWith the release of ChatGPT in November 2022, the number of similar foundation models and large language models\nhave increased exponentially. Foundation models (FM) are large-scale neural networks trained on broad data (Koubaa\net al. [2023]). They may handle multiple types of data (text, image, audio), making them a superset of Large Language\nModels (LLMs) (Bommasani et al. [2021]). These models which are also accelerating research and implementation\nat exponential pace are complex with varying levels of output. Although commonly expected to perform better as\nthe number of parameters increase, we are also seeing varying levels of performance as demonstrated on various\nleaderboards (Ope). The performance of these models varies not only based on the training data, but also on their\narchitecture and specific task. Many use cases and pilots to evaluate performance of these models are being undertaken\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nby researchers from across the world. Numerous evaluation metrics have been used for these models, which regularly\nleads to new winners being declared (Ope).\nEvaluation of LLMs is a challenging task with many proposed technical metrics and human evaluation methods\n(Kamalloo et al. [2023]) 1. As these models continue to evolve, their accurate evaluation is challenging. Most of the\nmodels have been assessed using technical methods, not universally designed to evaluate their performance, or do a\nnarrowly performance assessment on a few tasks or domains. There is also growing evidence that many models struggle\nwith robustness and potential for significant harm due to hallucatinations(Guo et al. [2023], Rawte et al. [2023]) 2.\nIn a recent comprehensive survey of LLMs categorized in three major groups, knowledge and capability evalua-\ntion,alignment evaluation and safety evaluation, the researchers assessed various aspects of model performance and\ntheir applications in certain key areas such as healthcare,finance,legal ,etc (Guo et al. [2023]). The authors emphasize\nthat as these LLMs rapidly advance in their capabilities, existing methods for evaluation are still lacking in their holistic\nassessments. They propose future directions for improvement in model development and evaluation to align with human\nvalues of helpfulness,harmlessness and honesty.\nWhile challenging to perform,the current gold standard for evaluation of these models beyond a few technical metrics, is\nstill human evaluation. Human evaluation provides for a comprehensive contextual benchmark beyond the task, domain\nand semantic capabilities oriented current technical approaches. Human evaluation also aligns with the human values\nthat many of these LLMs have been designed to replicate in their performance. But unlike technical metrics which\nhave a standard design, human evaluations have been designed and performed with significant variations. Multiple\nhuman evaluation methods have been proposed and used to assess NLP models, but there is no systematic method\nspecifically for evaluating LLMs (van der Lee et al. [2021], Shimorina and Belz [2021], Fuentes et al. [2022], Bojic\net al. [2023]). Additionally, these evaluation tools suffer from critical issues such as reproducibility and generalizability\nacross multiple tasks and domains, including healthcare, finance, education, etc (Bhatt et al. [2021], Ito et al. [2023],\nMahamood [2023], Li et al. [2023]).Hence,having a comprehensive,consistent,contextual,efficient and effective design\nof human evaluation is needed.\nTo address these challenges, we introduce HumanELY (Human Evaluation of LLM Yield), aiming to provide not only\nguidance for human evaluation of LLMs but also a state-of-the-art tool to facilitate these evaluations. We offer an\nexhaustive set of questions with clear definitions and details for each metric in the context of LLM evaluation. Through\nthese efforts, we aim to establish a better platform for human evaluation, enhancing reproducibility and generalizability\nin the evaluation process.’\nReview of existing evaluation metrics for LLMs\nAssessing LLMs is a challenging task, as there is no one-size-fits-all evaluation method (Zhang et al. [2023], Chan et al.\n[2023]). Researchers have utilized diverse approaches to evaluate these models, combining both quantitative and human\nevaluation techniques. Numerous quantitative metrics are developed for specific tasks like Text Classification, Natural\nLanguage Inference, Natural Language Generation, Text Summarization, Question Answering, Dialogue generation\nand Machine Translation (Table 2). For example, BLEU scores (Papineni et al. [2002]) are valuable for evaluating\ntasks like Machine Translation and Question Answering, measuring the similarity between generated and reference text.\nSimilarly, ROUGE scores Lin [2004] are used for text summarization by analyzing n-grams. Metrics like BLEURT\n(Sellam et al. [2020]) have emerged, incorporating BERT-based representations to assess semantic similarity between\ngenerated and reference text, allowing for a more nuanced evaluation.\nMetrics used for assessing dialogue quality include Deep-AM-FM (Zhang et al. [2021]), FlowScore (Li et al. [2021]).\nDeep-AM-FM measures dialog quality with Adequacy Metric (AM) and Fluency Metric (FM), utilizing BERT\nembeddings and language model probabilities. FlowScore calculates the dialogue quality by modeling dynamic\ninformation flow in dialogue history. For tasks like text classification, standard classification metrics like Accuracy, F1\nscore, Precision, Recall, and AUROC are typically used. However, since LLMs are capable of performing multiple\ntasks, benchmarking models requires more than task-specific metrics. Benchmarks like SuperGLUE (Sarlin et al.\n[2020]) have been used in models like GPT 3.0 (Brown et al. [2020a]) and BLOOM (Scao et al. [2022]) (Table 1) to\nshowcase model performance. SuperGLUE is a comprehensive evaluation benchmark for assessing natural language\nunderstanding models, featuring a range of challenging language tasks to assess language models’ understanding and\nreasoning capabilities. Its aggregated score offers a holistic measure of a model’s language understanding, making it\nvaluable for comparing LLMs.\n1https://toloka.ai/blog/evaluating-llms/\n2https://www.stateof.ai/\n2\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nThe Falcon model evaluation strategy (Penedo et al. [2023]) is inspired by The BigScience Architecture & Scaling\nGroup (Scao et al. [2022]) and has employed the unified framework LM-Eval3 developed by EleutherAI. This framework\nassesses the large language model across more than 200 tasks and supports models from Hugging Face Transformers4,\nincluding MegaTron-DeepSpeed5 and GPT-NeoX6. Hence, LM-Eval provides users with multi-environment flexibility.\nLLM Architecture Evaluation Metrics\nPaLM (Chowdhery et al. [2023]) Decoder BLEURT, MQM\nBLOOM (Workshop et al. [2022]) Decoder SuperGLUE, sacrebleu, ROUGE\nGPT 3.0 (Brown et al. [2020b]) Decoder Accuracy, Bleu, SuperGLEU\nFlan -T5 (Chung et al. [2022]) ‘Encoder-Decoder Accuracy, Normalised preferred metric\nGPT 4 7 Decoder Accuracy, Likelihood Estimation\nT0 (Sanh et al. [2021]) Encoder-Decoder Accuracy, rank classification\nLlama2 (Touvron et al. [2023]) Decoder ROUGE, Self-BLEU, Gwet’s ½, TruthfulQA,\nFalcon (Penedo et al. [2023]) Encoder-Decoder Zero-shot evaluation 8\nMedPalm2 (Singhal et al. [2023]) Decoder Accuracy, Paiwise preference Analysis, Overlap Analysis\ngatortron (Yang et al. [2022]) Decoder Precision, Recall, F1 score, Pearson Correlation, Exact Match\nTable 1: Key LLMs with some of the key Evaluation metrics used in\ntheir methods.\nWhile these metrics offer valuable insights into model performance, they may not fully capture the complexities\nof text, including issues like hallucinations, biases, and ethical concerns. Recognizing these complexities, human\nevaluation is increasingly seen as a complementary solution. Recent models like Llama2 (Touvron et al. [2023]) and\nMedPalm (Singhal et al. [2022]) combine human evaluation with quantitative assessment to present a more realistic\nand comprehensive evaluation of model performance. Additionally, evaluation benchmarks such as ChatbotArena and\nMT-Bench (Zheng et al. [2023]), proposed for evaluating LLMs and chatbots across multiple tasks, provide task-centric\nassessments. Chatbot Arena allows users to interact with anonymous LLMs and vote based on their preferences.\nWhereas, MT-Bench evaluates LLMs on multi-turn dialogues using a comprehensive set of questions tailored to assess\ntheir capabilities in handling conversations. Furthermore, a recent evaluation methodology, LLMEval (Lin and Chen\n[2023]), has introduced a unified approach to evaluation. This approach is based on the creation of a single prompt-based\nevaluation method that utilizes a unified evaluation framework to encompass various aspects of conversation quality\nwithin a single model query. Another noteworthy LLM evaluation metric, HELM, provides a comprehensive assessment\nof LLMs by evaluating them across various aspects, including language understanding, generation, coherence, context\nsensitivity, common-sense reasoning, and domain-specific knowledge (Liang et al. [2022]).\n3https://github.com/EleutherAI/lm-evaluation-harness\n4https://github.com/huggingface/transformers/\n5https://github.com/microsoft/Megatron-DeepSpeed/\n6https://github.com/EleutherAI/gpt-neox\n7https://arxiv.org/abs/2303.08774\n8https://zenodo.org/records/7413426\n3\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nConsidering the importance of human evaluation, the assessment by experts is still regarded as the gold standard.\nHuman evaluation is subject to variability and inconsistency due to different methods and criteria. This variability can\nintroduce bias in algorithm evaluation and make it challenging to compare different experiments, even though they all\nfall under the umbrella of \"human evaluation\". Various general-purpose human evaluation methods have been proposed,\nsuch as Microsoft Human Evaluation9 and LongEval (Krishna et al. [2023]), in their research found that the fine-grained\nannotations led to lower inter-annotator variance when compared with coarse-grained annotation. Additionally,they\nalso concluded that the partially annotating a summary can reduce annotator workload while maintaining accuracy.\nThey noted that these methods may have limited usefulness for evaluating long-form summaries. Therefore they\nrecommended using reference-free metrics as diagnostic tools for analyzing and understanding model behavior, rather\nthan relying solely on measures of how well models perform specific tasks.\nType of Metric Metric Explanation Strengths Weakness\nModel-free metrics\nPerplexity Perplexity measures the qual-\nity of the probability distribu-\ntion of words in a given cor-\npus by a model. Mathemat-\nically, perplexity is the geo-\nmetric mean of the inverse\nprobabilities of all the words\npredicted by language mod-\nels. Lower perplexity repre-\nsents the better performance\nof the model.\nEasy to compute Difficult to interpret\nBLEU The BLEU score is used to\nevaluate the model’s capa-\nbilities for machine transla-\ntion. The BLEU score is cal-\nculated by comparing the n-\ngrams of the ground truth ma-\nchine translation with those\nof the model-generated ma-\nchine translation. The BLEU\nscore ranges between 0 and 1,\nand a higher score represents\nbetter model performance.\nFrequently used in machine\ntranslation tasks.\nInability to un-\nderstand semantic\nsimilarity\nROUGE The ROUGE score is com-\nmonly used to evaluate sum-\nmarization tasks by calcu-\nlating the overlapping n-\ngrams between the model-\ngenerated summary and the\nreference summary. The\nROUGE score measures the\nrecall of n-grams in the\nmodel-generated summary\nand ranges from 0 to 1,\nwhere a higher score repre-\nsents better performance.\nFrequently used in summa-\nrization tasks.\nInability to recognize\nparaphrasing and con-\ntextual meaning\nACCURACY Accuracy is a simple mea-\nsure of correctness, com-\nmonly used in classifying\nword tokens. It is presented\nas a percentage of correct\npredictions.\nEasy to understand Limited scope in NLP\ntasks\n9https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/custom/how-to/test-evaluate\n4\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nModel-based metrics\nQuestEval QuestEval is a fact-oriented,\nreference-less method to\nevaluate the summarization\nquality of language models.\nQuestEval employs a diverse\nset of questions to assess\nthe correctness and appro-\npriateness of the generated\ntext.\nDo not require the ground\ntruth or reference text\nCan be biased\nMed-HALT Med-HALT is specifically\ndesigned for the medical do-\nmain and calculates the hal-\nlucination generated by large\nlanguage models.\nUseful for the medical do-\nmain, as the model is\ndomain-specific\nNot a method but pro-\nvides a new hallucina-\ntion benchmark and\ndataset\nBERTSCORE BERTSCORE uses the con-\ntextual embeddings gener-\nated by the BERT model\nat the token level for both\nthe model output and ref-\nerence text and then cal-\nculates the cosine similar-\nity between them. Based\non the cosine similarity,\nBERTSCORE computes the\nRecall, Precision, and F1\nscore.\nConsiders the contextual in-\nformation through embed-\ndings and can be used for any\ndownstream NLP task\nComputationally ex-\npensive because it\nuses embeddings\nTable 2: Evaluation metric with their strengths and weaknesses.\n1.1 Human Evaluation\nCurrently, human evaluation still remains the gold standard for the measuring performance of the LLM output,especially\nin specific domains such as healthcare (Reddy [2023]). Human evaluation can however be variable and inconsistent\nfollowing different methods and different criteria (Ziems et al. [2023]). Even though many of these methods use\nthe term human evaluation as a homogeneous entity, comparisons between different experiments hard to evaluate\nand can be biased. Krishna et al., described the numerous challenges with these varying human evaluation methods\nwhile evaluating LLM outputs, including inherent subjectivity,inter-rater correlation, labor intensiveness and design\nchallenges. Interestingly, in their comprehensive survey of 162 relevant publications,they found 101 (62%) publications\ndid not perform any human evaluation.They proposed guidelines for human evaluation of faithfulness in long form\nsummarization(150 words or more) (Krishna et al. [2023]). Their key guidelines,termed LongEval, include:\n• fine grained annotations have lower inter-annotator variance than COARSE-grained annotations.\n• partially annotating a summary reduces annotator workload while maintaining accuracy.\n• highlighting hints in the source document has limited usefulness for evaluating long-form summaries.\nWe use some of this guidance in development of our human evaluation tool,HumanELY, to address this and provide a\ntool for application.\n2 Proposed Method for consistent and comprehensive human evaluation of LLMs\nConsidering the importance of human evaluation, in this paper, we propose (Figure 1) a structured approach to\nconducting human evaluation of LLM outputs. We provide a comprehensive list of survey-based questions and majorly\ndivide the human evaluation practice into the following aspects: assessing the relevance of LLM-generated text,\nevaluating the coverage of LLM-generated text, assessing the coherence of LLM-generated text, considering potential\nharm associated with LLM-generated text, and emphasizing the significance of comparing human-generated text with\n5\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nLLM-generated text. It is important to understand that different evaluation metrics may overlap; for example, inaccurate\nanswers can be both harmful and unsafe (Figure 2).\nComparison\nRelevance\nHarm\nCoherence  \nCoverage\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nC\nContextualized \ncomposite of accuracy \nand resonsing\nGrammatical and full \nfrom organization\nCompares responses of \ndifferent entities\nHolistic and complete \nresponse\nHarmful component in \nresponse\nFinal Score\n0\n5\n10\n15\n20\n25\nRelevance Coherence Coverage Harm\nr1 coh1 cov1 h1Model 1\nr2 coh2 cov2 h2Model 2\nRelevanceCoherenceCoverage Harm\nr3 coh3 cov3 h3Model 3\nr4 coh4 cov4 h4 Model  4\ncom2\ncom1\ncom3\ncom4\nComparison\n           LLM  \n    generated response    Human Evaluation \n       using WebApp\n         Factors for \n      Human Evaluation\n            Explanation \n         of the factors\nResponse score Final Evaluation \n         Score \nFigure 1: Proposed Method for Human Evaluation Through the HumanELY Portal. We have proposed five\nmajor factors for conducting human evaluation: Relevance, Coverage, Coherence, Comparison, and Harm. We\nhave developed a set of survey-based questions to evaluate these five categories. Additionally, we are providing a\nWebApp that allows for evaluation by simply uploading a file with reference text and human-generated text.\nWe provide the linguistic definition of the evaluation metrics based on oxford languages 10 and the interpretation of\nthese terms in the context of LLM evaluation.\n2.1 Relevance\n2.1.1 Linguistic definition\nThe quality or state of being closely connected or appropriate.\n2.1.2 LLM evaluation context\nRelevance can be defined as a contextualized composite of accuracy and reasoning which is helpful to the user. For the\nLLM generated response to be relevant,it needs to be accurate in information it generates, have correct understanding,\nand reasoning as compared to the context and the query. The response beyond accuracy and reasoning metrics, needs to\nbe helpful to the user. We have designed following questions to evaluate the relevance of LLM output.\n• Is the LLM generated response accurate?\n• Is the LLM generated response correct in comprehension?\n• Does the LLM generated response have the reasoning mirroring the context?\n• Is the LLM generated response helpful to the user?\n2.2 Coverage\n2.2.1 Linguistic definition\nThe extent to which something deals with or applies to something else.\n10https://languages.oup.com/google-dictionary-en/\n6\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nFigure 2: Overlap among factors in human evaluation. The Venn diagram illustrates the relationships between\ndifferent factors. For example, there is an overlap indicating that inaccurate answers can be both harmful and\nunsafe.\n2.2.2 LLM evaluation context\nCoverage is defined by the holistic and completeness of the response. LLM generated response should address all the\nkey points,retrieve key knowledge elements without any significant missingness. We have designed following questions\nto evaluate the coverage.\n• Does the LLM generated response cover all the topics needed from the context?\n• Does the LLM generated response cover all the key aspects of the response based on the context?\n• Is the LLM generated response missing any significant parts of the desired response?\n2.3 Coherence\n2.3.1 Linguistic definition\nThe quality of being logical and consistent.the quality of forming a unified whole.\n2.3.2 LLM evaluation context\nCoherence is defined by grammatical and full form organization. The LLM generated response should have appropriate\nfluency, grammar, and organization to be complete in itself. We have designed following questions to evaluate the\ncoherence.\n• Is the LLM generated response fluent?\n• Is the LLM generated response grammatically correct?\n• Is the LLM generated response well organized?\n7\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\n2.4 Harm\n2.4.1 Linguistic definition\nPhysical injury, especially that which is deliberately inflicted. damage the health of. have an adverse effect on.\n2.4.2 LLM evaluation context\nHarm is defined by any hurtful component of the response. The LLM generated response should not have bias, toxic\nlanguage or interpretation, private data and hallucinations. We have designed following questions to evaluate harm.\n• Does the LLM generated response have any amount of biasness?\n• Does the LLM generated response have any amount of toxicity?\n• Does the LLM generated response violate any privacy?\n• Does the LLM generated response have any amount of hallucinations?\n2.5 Comparison\n2.5.1 Linguistic definition\nA consideration or estimate of the similarities or dissimilarities between two things or people.\n2.5.2 LLM evaluation context\nComparison compares responses by different entities or expectations. This metrics compares a LLM generated response\nwith a human response or another LLM response.\n• Is the generated response distinguishable from human response?\n• How does the generated response compare with the human response?\n• How does the generated response compare to the other LLM responses?\n3 Comparison with existing methods\nWe provide comparative examples (Table 3) of the current state of human evaluation from a few different publications\nusing large language models for summarization and question answering task against our proposed HumanELY method.\nWe specifically, evaluated some of the key publications related to different LLM methods and evaluations in healthcare.\nThere is a consensus that for use of LLM generated responses in healthcare, they need to be assessed with the most\nqualified assessment metrics to provide high quality patient care and prevention of harm.\n(Singhal et al. [2022]) used an evaluation methodology which is one of the most comprehensive one. Their evaluation\nincludes the use of two human user evaluation groups and a high number of diverse evaluation metrics. Amongst the\nclinician evaluation metrics, their evaluation method included all the criteria for relevance and harm as proposed by\nthe HumanELY evaluation method. While they did assess for coverage using the key metrics, key points from the\ncontext were not specifically addressed,although some of them might have been assessed when looking for missing\ncontent. They also did not specifically check for grammar in their evaluation.Amongst the lay users, the evaluation\nprimarily looked for intent and helpfulness of the answer. They specifically did not address comparison of response\nagainst human generated responses or evaluation by another LLM.\nFor evaluation of Gatotron, human evaluation performed by (Peng et al. [2023]), readability of the response which aligns\nwith coherence metrics in HumanELY including fluency,grammar and comprehension was assessed. By addressing\nrelevance and consistency they addressed the accuracy and reasoning sub measures of the relevance metric in HumanELY .\nConsistency also probably addresses coverage of key points in the response evaluation. However, they did not assess\nany of the harm metrics. Comparison was performed against human responses.\n(Kung et al. [2023]), in their evaluation of LLM generated responses, did assess for accuracy and reasoning within the\nrelevance metric and retrieval of key instances of text amongst the coverage metric of HumanELY . They specifically did\nnot assess metrics for harm,coherence or make any comparison assessments.\n(Tang et al. [2023]), addressed accuracy and comprehension submetics of relevance metric in their evaluation. They\nalso specifically addressed harmfulness, though it was specifically missing assessment of privacy data.With their\n8\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\ncomprehensiveness evaluation,they do address all the facets of coverage evaluating for sufficient information in the\nresponse.Within coherence,they did assess for fluency and organization, but did not specifically address the grammar.\nThey also did not make any comparisons against human response or other LLM evaluation.\n(Xiong et al. [2023]), primarily looked at helpfulness and honesty which address two of the key submetics of relevance\nincluding accuracy and helpfulness. But specifically,they did not address comprehension and reasoning of the response.\nWhile assessing for harmlessness, they assessed for various parameters of harm but it is unclear if privacy was addressed.\nThey do not make any specific assessments for coherence, though one could presume some components of coherence\ncould be included while assessing for helpfulness.All these parameters were assessed for human preference but did not\nspecifically compare LLM responses against human generated response.\nMetrics Sub-Metrics Study1\n(Singhal et al. [2022]) Study2\n(Peng et al. [2023]) Study3\n(Kung et al. [2023]) Study4\n(Tang et al. [2023]) Study5\nXiong et al. [2023] Humanely\nRele\nvance\nAccurac\ny ✓ ✓ ✓ ✓ ✓ ✓\nComprehension ✓ ✓ ✓\nReasoning ✓ ✓ ✓ ✓\nHelpfulness ✓ ✓ ✓\nCo\nverage\nK\ney points ✓ ✓ ✓ ✓\nRetrie\nval ✓ ✓ ✓ ✓\nMissingness ✓ ✓ ✓\nCoher\nence\nFluenc\ny ✓ ✓ ✓ ✓\nGrammer ✓ ✓\nOr\nganization ✓ ✓ ✓ ✓\nHarm\nBias ✓ ✓ ✓ ✓\nT\noxicity ✓ ✓ ✓ ✓\nPri\nvacy ✓ ✓ ✓\nHallucinations ✓ ✓ ✓ ✓\nComparison\nHuman ✓ ✓\nLLM ✓\nTable 3: Comparison of Human Evaluation Factors used in different studies.\n4 Use of HumanELY tool\nTo begin the evaluation process (Figure 3), commence by uploading a comma-separated file containing two columns:\nthe first column labeled \"Reference Text,\" which should contain the original text, and the second column labeled\n\"Generated Text,\" which should contain the summary generated by the Language Learning Model (LLM). After\nuploading the data in the portal, users can see a table within the portal with 4 columns \"Reference\", \"Generated\",\n\"Status\", and \"Evaluate\". The Status column shows the evaluation status for the corresponding row - whether it is\ncompleted (shown by a green tick) or uncompleted (shown by a red clock symbol). The Evaluate column allows users\nto select rows to evaluate the corresponding row.\nSubsequently, choose a summary by clicking the corresponding icon in the \"Evaluate\" column. The text from the\nselected row will be displayed in expanded form under \"Reference Text (Selected)\" and \"Generated Text (Selected).\"\nNext, assess the generated text based on the parameters of \"Relevance,\" \"Coverage,\" \"Coherence,\" , \"Harm\" and\n\"Comparison\" using a Likert scale ranging from 1 to 5. After completing the evaluation, click the \"submit\" button to\nsubmit your assessment. The status of the selected row will change to \"Done\" upon successful submission. Additionally,\nit is possible to modify the evaluation parameters by selecting the respective row from the \"Evaluate\" column. After\nthoroughly evaluating each row (samples of reference text and generated text), users have the option to download their\nraw evaluation scores in the form of CSV and Excel files.\nThe rationale behind providing the raw evaluation scores is to enable users to conduct additional analyses, such as score\naggregation from multiple evaluators and model evaluation, in their own unique manner. This flexibility allows users to\ntailor their assessments to specific cases, fostering a more personalized and insightful understanding of the generated\ncontent.\n9\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\n1.\n2.\n3.\n4.\n5.\n.\n.\n.\n.\n.\n.\n6.\n7.\nFigure 3: Step by Step guide to use HumanELY tool.\n10\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\n5 Data privacy Statement\nWe do not collect any data which users upload in the HumanELY portal. However, in pursuit of enhancing our tools and\nconducting future research, we store the user feedback (as indicated by choosing the options in Figure 3) and evaluation\nscores related to Human Evaluation on our servers.\n6 Limitations\nGenerative AI is not new and the assessment of generated responses has now been done for a significant period of time\nbut have lacked consistent definitions and comprehensiveness. Our method is derived out of these prior related work.\nAs the field of generative AI evolves, new evaluation methods are likely to be proposed and adopted. Although,our\napproach provides for consistent and comprehensive human assessment of the LLM generated response, we do plan on\nadopting new assessments and changing HumanELY metrics as they evolve. Our interpretation of human evaluation\nmethods of various recent publications may lack direct and quantifiable correlation with the HumanELY metrics.\nHowever, we believe we have made the best possible assessment based on their application context.\nNot all HumanELY evaluation metrics are needed for assessment of each LLM response. However, we do recommend\nan appropriate consideration or exemption of using these metrics for human evaluation of LLM responses on a case by\ncase basis.\nWe also do not address the important question of “who” should assess the LLM generated response while performing\nhuman evaluation. Clearly, this is an important question and needs to be addressed based on the purpose of the LLM\ngenerated response and the defined consumer response. As assessed in MedPalm, it might be appropriate to assess\ndifferent metrics from different perspectives of more than one consumer,physician and lay person, in their instance.\nLastly, it is important to use the quantified automated metrics along with our proposed metrics to supplement LLM\nevaluations.\n7 Conclusion\nWith the exponential increase in research,development and adoption of LLMs,there is a critical need to have a highly\nreliable human evaluation methodology, to ensure delivery of accurate and safe output. Human evaluation of LLM\noutput needs a comprehensive, standardized, quantifiable and comparable standard for measurement. Our goal in\ndevelopment of HumanELY is to provide researchers, developers, and policymakers with a tool that can assist with\nhuman evaluation of LLM outputs following these standards.\nReferences\nAnis Koubaa, Wadii Boulila, Lahouari Ghouti, Ayyub Alzahem, and Shahid Latif. Exploring chatgpt capabilities and\nlimitations: A critical review of the nlp game changer. Preprints, 2023.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021.\nLlm leaderboard - a hugging face space by huggingfaceh4. URL https://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\nEhsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei. Evaluating open-domain question answering in\nthe era of large language models. arXiv preprint arXiv:2305.06984, 2023.\nZishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi\nXiong, et al. Evaluating large language models: A comprehensive survey. arXiv preprint arXiv:2310.19736, 2023.\nVipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, SM Tonmoy, Aman Chadha, Amit P Sheth,\nand Amitava Das. The troubling emergence of hallucination in large language models–an extensive definition,\nquantification, and prescriptive remediations. arXiv preprint arXiv:2310.04988, 2023.\nChris van der Lee, Albert Gatt, Emiel van Miltenburg, and Emiel Krahmer. Human evaluation of automatically\ngenerated text: Current trends and best practice guidelines. Computer Speech & Language, 67:101151, 2021.\nAnastasia Shimorina and Anya Belz. The human evaluation datasheet 1.0: A template for recording details of human\nevaluation experiments in nlp. arXiv preprint arXiv:2103.09710, 2021.\n11\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nBelén Saldías Fuentes, George Foster, Markus Freitag, and Qijun Tan. Toward more effective human evaluation for\nmachine translation. In Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval), pages\n76–89, 2022.\nIva Bojic, Jessica Chen, Si Yuan Chang, Qi Chwen Ong, Shafiq Joty, and Josip Car. Hierarchical evaluation framework:\nBest practices for human evaluation. arXiv preprint arXiv:2310.01917, 2023.\nShaily Bhatt, Rahul Jain, Sandipan Dandapat, and Sunayana Sitaram. A case study of efficacy and challenges in\npractical human-in-loop evaluation of nlp systems using checklist. In Proceedings of the Workshop on Human\nEvaluation of NLP Systems (HumEval), pages 120–130, 2021.\nTakumi Ito, Qixiang Fang, Pablo Mosteiro, Albert Gatt, and Kees van Deemter. Challenges in reproducing human\nevaluation results for role-oriented dialogue summarization. In Proceedings of the 3rd Workshop on Human\nEvaluation of NLP Systems, pages 97–123, 2023.\nSaad Mahamood. Reproduction of human evaluations in:“it’s not rocket science: Interpreting figurative language in\nnarratives”. In Proceedings of the 3rd Workshop on Human Evaluation of NLP Systems, pages 204–209, 2023.\nYiru Li, Huiyuan Lai, Antonio Toral, and Malvina Nissim. Same trends, different answers: Insights from a replication\nstudy of human plausibility judgments on narrative continuations. In Proceedings of the 3rd Workshop on Human\nEvaluation of NLP Systems, pages 190–203, 2023.\nXinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider\nand deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval:\nTowards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages\n311–318, 2002.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages\n74–81, 2004.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text generation. arXiv preprint\narXiv:2004.04696, 2020.\nChen Zhang, Luis Fernando D’Haro, Rafael E Banchs, Thomas Friedrichs, and Haizhou Li. Deep am-fm: Toolkit for\nautomatic dialogue evaluation. Conversational Dialogue Systems for the Next Decade, pages 53–69, 2021.\nZekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not flat: Modeling the dynamic\ninformation flow across dialogue utterances. arXiv preprint arXiv:2106.02227, 2021.\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature\nmatching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4938–4947, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages\n1877–1901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/\n2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné, Alexan-\ndra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.\nJournal of Machine Learning Research, 24(240):1–113, 2023.\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\n12\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint \nHumanELY: Human evaluation of LLM yield, using a novel web-based evaluation tool A PREPRINT\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877–1901, 2020b.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud\nStiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv\npreprint arXiv:2110.08207, 2021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\nKaran Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather\nCole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models.arXiv\npreprint arXiv:2305.09617, 2023.\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb E Smith, Christopher Parisien, Colin Compas,\nCheryl Martin, Mona G Flores, Ying Zhang, et al. Gatortron: A large clinical language model to unlock patient\ninformation from unstructured electronic health records. arXiv preprint arXiv:2203.03540, 2022.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv\npreprint arXiv:2212.13138, 2022.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint\narXiv:2306.05685, 2023.\nYen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain\nconversations with large language models. arXiv preprint arXiv:2305.13711, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint\narXiv:2211.09110, 2022.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. Longeval:\nGuidelines for human evaluation of faithfulness in long-form summarization. arXiv preprint arXiv:2301.13298,\n2023.\nSandeep Reddy. Evaluating large language models for use in healthcare: A framework for translational value assessment.\nInformatics in Medicine Unlocked, page 101304, 2023.\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models\ntransform computational social science? arXiv preprint arXiv:2305.03514, 2023.\nCheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin, Mona G\nFlores, Ying Zhang, Tanja Magoc, et al. A study of generative large language model for medical research and\nhealthcare. arXiv preprint arXiv:2305.13523, 2023.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria\nMadriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of chatgpt on usmle: Potential\nfor ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198, 2023.\nLiyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor, Ali Soroush, Pierre A Elias, Ziyang Xu, Ying Ding, Greg\nDurrett, Justin F Rousseau, et al. Evaluating large language models on medical evidence summarization. npj Digital\nMedicine, 6(1):158, 2023.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,\nKarthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv\npreprint arXiv:2309.16039, 2023.\n13\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 30, 2023. ; https://doi.org/10.1101/2023.12.22.23300458doi: medRxiv preprint ",
  "topic": "Yield (engineering)",
  "concepts": [
    {
      "name": "Yield (engineering)",
      "score": 0.6239427328109741
    },
    {
      "name": "Computer science",
      "score": 0.5131474137306213
    },
    {
      "name": "World Wide Web",
      "score": 0.33681488037109375
    },
    {
      "name": "Data science",
      "score": 0.3260268568992615
    },
    {
      "name": "Materials science",
      "score": 0.06933343410491943
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    }
  ],
  "institutions": []
}