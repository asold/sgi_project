{
  "title": "Retrieval-Augmented Generative Question Answering for Event Argument Extraction",
  "url": "https://openalex.org/W4385573027",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2676585443",
      "name": "Xin-Ya Du",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A2127420617",
      "name": "Heng Ji",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2075655036",
    "https://openalex.org/W4285125060",
    "https://openalex.org/W3024298906",
    "https://openalex.org/W2475245295",
    "https://openalex.org/W4285162231",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W3170137603",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3035229828",
    "https://openalex.org/W3170759063",
    "https://openalex.org/W3199177860",
    "https://openalex.org/W3102925419",
    "https://openalex.org/W2964206023",
    "https://openalex.org/W2974604908",
    "https://openalex.org/W2984582583",
    "https://openalex.org/W2084238990",
    "https://openalex.org/W2165962657",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2211728022",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W182831726",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3159518688"
  ],
  "abstract": "Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models' capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example's context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4649–4666\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRetrieval-Augmented Generative Question Answering\nfor Event Argument Extraction\nXinya Du\nDepartment of Computer Science\nThe University of Texas at Dallas\nxinya.du@utdallas.edu\nHeng Ji\nDepartment of Computer Science\nUniversity of Illinois Urbana-Champaign\nhengji@illinois.edu\nAbstract\nEvent argument extraction has long been stud-\nied as a sequential prediction problem with\nextractive-based methods, tackling each ar-\ngument in isolation. Although recent work\nproposes generation-based methods to capture\ncross-argument dependency, they require gener-\nating and post-processing a complicated target\nsequence (template). Motivated by these obser-\nvations and recent pretrained language models’\ncapabilities of learning from demonstrations.\nWe propose a retrieval-augmented generative\nQA model (R-GQA) for event argument ex-\ntraction. It retrieves the most similar QA pair\nand augments it as prompt to the current ex-\nample’s context, then decodes the arguments\nas answers. Our approach outperforms sub-\nstantially prior methods across various settings\n(i.e. fully supervised, domain transfer, and few-\nshot learning). Finally, we propose a clustering-\nbased sampling strategy (JointEnc) and conduct\na thorough analysis of how different strategies\ninfluence the few-shot learning performance.1\n1 Introduction\nMany documents report sequences of events corre-\nsponding to common situations in the real world.\nArguments of different roles provide fine-grained\nunderstanding of the event (e.g. INDIVIDUALS , OR-\nGANIZATIONS , LOCATIONS ) and also influence the\ndetermination of the event type (Grishman, 2019).\nAs compared to detecting the trigger (usually verbs)\nof an event, extracting arguments involve recogniz-\ning mention spans (consisting of multiple words) of\nvarious roles across sentences (Jurafsky and Martin,\n2018). We list an example in Figure 1, given the\ncontext and the event type (nomination), all argu-\nments for the three roles (i.e PERSON , POSITION ,\nAGENT ) should be extracted.\n1The implementations will be released at https://\ngithub.com/xinyadu/RGQA.\nContext: One of those difficult judges [John M.]\nis nominated (Type: nomination) by Adam to be \n[chief justice] in 2000….\n✅ Context: [Greg L.] was elected (Type: Elect) \nby  Randy as [mayor of Columbus] in 1999….\nRole Question Answers/\nExtractions\nPerson   who is the person nominated? John M.\nPostion   what position is the person \nnominated for? chief justice\nAgent   who is the norminating agent? Adam\nRole Question Answers/\nExtractions\nPerson   who is the person elected? Greg L.\nPostion   what position is the person \nelected for?\nmayor of \nColumbus\nAgent   who is the electing agent? Randy\n❌ Context: [John N.] borrowed (Type: \nTransfer-Money) a large amount of cash to \nto buy shares in 2000 ….\nRole Question Answers/\nExtractions\nRecipient Who is recipient agent? John N.\nGiver Who is the donating agent? N/A\n… … …\nRetrieved\nDemos (from \nTraining)\nCurrent\nExample\nFigure 1: Current/test example’s context and question\nfor each role have great similarities to the retrieved\ndemonstrations (context and QA pairs).\nTo overcome the error propagation of extractive\nmodels (Li et al., 2013; Du and Cardie, 2020b)\nand efficiently capture the cross-role dependencies,\nend-to-end template generation-based information\nextraction approaches (Li et al., 2021; Huang et al.,\n2021; Du et al., 2021) have been proposed. How-\never, they (1) suffer from the dense output template\nformat (fewer training instances) and cannot fully\nexploit semantic relations between roles with the\nconstrained templates; (2) are unable to unleash the\nexcellent analogical capability of large pre-trained\nmodels (Brown et al., 2020) on similar input-output\n4649\npairs to produce extraction results.\nBased on our observations in the real circum-\nstances, examples often bear great similarities (in\nterms of both syntax and semantics) with other ex-\namples (Figure 1). In this Figure, we have current\ninput context “... difficult judges John M. is nomi-\nnated ...” for a nomination event. When searching\nthrough examples in the large store (e.g. training\nset) for demonstrations (input-output pairs2), the\ntwo most similar examples’ input-output pairs are\npresented. Both of the retrieved examples’ contexts\nhave large semantic similarities with the context of\nthe current example. The first retrieved example’s\nquestions (for each role) also match the input ex-\namples’. The second example’s questions do not.\nThus, to help the model determine “how much” to\nlearn from the demonstrations is also important.\nMotivated by the weaknesses of previous meth-\nods and our observations, we introduce a retrieval-\naugmented generative question answering model\n(R-GQA) for event argument extraction. Firstly,\nour formulation for event extraction as a generative\nquestion answering task enables the model to take\nadvantage of both question answering (exploiting\nlabel semantics) and text generation, and there’s no\nneed for threshold tuning. We conduct experiments\non two settings (1) fully-supervised setting3 and (2)\ndomain transfer setting4. Empirically, our method\noutperforms previous methods (extraction QA and\ntemplate generation-based methods) substantially\n(Contribution 1).\nTo enable our generative model based on large\npretrained model to explicitly learn (“reason”)\nfrom similar demonstrations as prompt, we add\nto our model a retrieval component. It uses sim-\nilarity/analogy score to decide how much to rely\non retrieved demonstrations. It significantly out-\nperforms the generative QA model (our proposed\nbaseline without the retrieval component) in both\nsettings (Contribution 2). What’s more, we also\ninvestigate various models’ performance in the few-\nshot extraction setting. As far as we know, there’s\na large variance in terms of performance when the\nexamples for training/evaluation are randomly sam-\npled, causing different methods not comparable.\nThus (1) we investigate models’ behavior in the\nfew-shot event extraction setting on different sam-\npling strategies (e.g. random, clustering-based) and\n2In our QA setting, input consists of the context and ques-\ntion (for each argument role), output consists of the arguments.\n3train and test both on ACE05 (Doddington et al., 2004).\n4train on ACE05 and test on WikiEvent (Li et al., 2021).\nhow the model performance and distribution dis-\ntance (between true data and sampled data) cor-\nrespond; (2) we design a clustering-based sam-\npling strategy (JointEnc), which selects the most\nrepresentative (unlabeled) examples by leveraging\nboth context & trigger embedding. It is better\nthan random sampling and one-round active learn-\ning. Our discussions on sampling methods help\nimprove benchmarking models’ few-shot setting\nperformance (Contribution 3).\n2 Problem and Definitions\nEvent Ontology, Templates, and Questions We\nfocus on extracting event arguments from a se-\nquence of words. An event consists of (1) a trigger\nand the type (E) of the event; (2) corresponding ar-\nguments {argE\n1 ,argE\n2 ,...}for event type E. Both\nthe event type and argument roles are pre-defined in\nthe ontology. Apart from the event types and argu-\nment roles, the ontology also provides definitions\nand templates for the argument roles. For example,\nwhen E = Movement-Transportation-Evacuation,\nthe template for the argument roles is provided,\n[arg1] transported [arg2] in [arg3]\nfrom [arg4] place to [arg5] place.\nBased on the definitions of argument roles and the\ntemplates in the ontology, we can generate the\nnatural questions for each argument role based\non the mechanism proposed in Du and Cardie\n(2020b). For example, in this example, arg1\n(TRANSPORTER ):“who is responsible for trans-\nport”, arg2 (PASSENGER ):“who is being trans-\nported”, arg3 (VEHICLE ):“what is the vehicle\nused”, arg4 (ORIGIN ):“where the transporting orig-\ninated”, arg5 (DESTINATION ):“where the trans-\nporting is directed”5.\nDemonstrations Store Brown et al. (2020) pro-\nposed to use in-context demonstrations (input-\noutput pairs) as prompt to test the zero-shot perfor-\nmance of large pretrained language models. For\nour retrieval-augmented approach, we denote the\nset of demonstrations/prompts to choose from ST.\nIn this work, we initiate ST with the training set.6\nData and Sampling Strategy In the fully-\nsupervised setting, we use the entire training set (1)\nto train the models; (2) as the demonstration store.\n5For the full list of questions for WikiEvent argument roles,\nplease refer to the Appendix Sec E.\n6Other external resources can also be added to ST .\n4650\nAugmented Encoder\n            <S>         John         …\nConditional Decoder\n   John       M.        …                          \nNominate-Person\nY/N—> answer: Greg L. \nQuestion: who is the person nominated?\nInput context: … judge John M. is [trg]nominated[trg] by … \nRetrieve  \nDemonstration  \nanalogy lossseq2seq loss\nℒanaℒseq\nQuestion: who is the person elected?\nCurrent Example’s Input Context and Question\nContext: Greg L. was elected by … as …\nRetrieved Demonstration (prompt)\nFigure 2: Our Retrieval-Augmented Generative Question Answering Model.\nIn the few-shot setting, motivated by the need to re-\nduce annotation cost, we assume that there is only\na fixed budget for annotating K examples’ argu-\nments for training, and call the annotated subset\nSfew . Then we use Sfew as both the training set\nand demonstration store.\n3 Methodology\nWe first describe the retrieval-augmented gener-\native question answering model (Figure 2), in-\ncluding (1) the generation model and how to con-\nstruct the demonstration (prompt) as well as the\nfinal input&target sequence; (2) training, decod-\ning, post-processing details; and how they differ\nfrom template-generation based models. Then we\nintroduce our clustering-based sampling strategy\nto diversify the training examples for the few-shot\nsetting.\n3.1 Retrieval-Augmented Generative QA\nBART (Lewis et al., 2020a) is a large pre-trained\nencoder-decoder transformer architecture based on\nVaswani et al. (2017). Its pretraining objective is to\nreconstruct the original input sequence (denoising\nautoencoder). Prior work reports that this objective\nhelps the extraction problems (Li et al., 2021; Du\net al., 2022). Thus we use pre-trained BART as\nour base model. It is presented in Figure 2. For\neach argument role, the R-GQA model’s input x is\nconditioned on (1) the current example’s context;\n(2) question for the role and (3) the demonstra-\ntion store ST. We will explain the details below.\nThe ground truth sequence y is based on the gold-\nstandard argument spans for the current training\ninstance. The goal is to find ˆ ysuch that,\nˆ y= arg max\ny\np(y|x) (1)\nwhere p(y|x) is the conditional log-likelihood of\nthe predicted argument sequence y given input x.\nTo construct x and y, apart from the special to-\nkens in the vocabulary of BART – including the\nseparation token [sep], and start/end token of a se-\nquence (i.e. < S >and < /S >), we add three\nnew tokens: [demo], [tgr] and [sep_arg]. More\nspecifically, [demo] denotes which part of the in-\nput sequence is the demonstration/prompt, [trg]\nmarks the trigger of the event in the input context,\n[sep_arg] is used as the separator token gold argu-\nments.\nGiven an example (including context and the\nevent trigger), for each argument role of the event\ntype E, the input format is as follows, where we\ninstantiate all components to obtain the final input\nsequence:\nx = <S > [demo] Demonstration [demo]\nQuestion [sep] Input Context </S >\nwhere “Question” is from the question set derived\nfrom respective ontology (Section 2); for “Input\nContext”, we mark up the current example’s trigger\nword with [trg] token for emphasizing. For the\nexample in Figure 2, the input context would be “...\nJohn M is [trg] nominated [trg] by ...”.\nAs for the “Demonstration”, we first retrieve it\nfrom the demonstration store (ST = {d1,d2,...})\ndr which is most similar to current question and\ninput context, it is a (<Question, Context>, Argu-\nments) pair. We concatenate the components (with\n4651\nthe separation tokens in between them) as the final\ndemonstration sequence.\nDemonstration dr = Qr [sep] Cr [sep]\nThe answer is: Ar\nWe use S-BERT (Reimers and Gurevych, 2019) to\ncalculate the similarity scores between the current\ninstance and all demonstrations in ST. S-BERT is\na modification of the BERT model (Devlin et al.,\n2019) that uses siamese and triplet network struc-\ntures to obtain semantically meaningful embed-\ndings for word sequences7.\nTo construct the target (sequence), we first de-\ntermine how much to learn from the demonstration\n– if the similarity score is above a threshold (deter-\nmined on the development set), and the demonstra-\ntion and current instance both have a non-empty an-\nswer, then we assign 1 (Yes) to yanalogy, otherwise\n0 (No). Then we concatenate all argument spans of\nthe role with [sep_arg] to construct yseq2seq,\nyseq2seq = <s> Argument1\n[sep_arg] Argument2 [sep_arg] ...</s>\nThe final y includes yseq2seq and yanalogy.\n3.2 Training and Inference\nTraining After the preparation for S =\n{(x(i),y(i))}|S|\ni=1, we minimize the joint loss func-\ntion during training,\nL= Lseq2seq + Lanalogy\nLseq2seq = −\n|S|∑\ni=1\nlog p(y(i)\nseq2seq|x(i); θ)\n= −\n|S|∑\ni=1\n|y(i)\nseq2seq|∑\nj=1\nlog p(y(i)\nj |x(i),y(i)\n<j; θ)\n(2)\nwhere Lseq2seq is the cross-entropy loss between\nthe decoder’s output and the target sequence\nyseq2seq. Lanalogy is the binary cross-entropy loss\ncalculated with the final hidden state of the final\ndecoder token.\nInference and Post-processing At test time, we\nconduct greedy decoding to obtain the target se-\nquence, then we split the decoded sequence with\n7The SentenceTransformer library ( https://www.\nsbert.net/docs/quickstart.html) supports calcu-\nlations in batch.\nrespect to [seq_arg]. Since it is also required to ob-\ntain the offsets of the argument in the input context,\nwe automatically match the candidate argument’s\nspan with the input context. Then, if there’s no\nmatched span, we discard the candidate argument;\nif there are multiple matches, we select the one clos-\nest to the trigger word. For example, if the input\ncontext is “One of those difficult judges [John M.]\nis nominated (Type: nomination) by Adam to be\nchief justice in 2000.. [John M.] started office on ...”\nand there are two appearances of the candidate ar-\ngument (in brackets) for the role PERSON , then we\nuse the first candidate’s offsets. Different from our\nmethods, the template-based generation method\ngenerates a sequence similar to the one in Section 2\n– causing the model to (1) not fully exploit the se-\nmantic relations of roles across event types; (2)\nrequire more complicated post-processing includ-\ning an additional step to obtain arguments from the\ngenerated template.\n3.3 Few-shot Setting and Sampling Strategy\nAlgorithm 1: Our Strategy for Obtaining Sfew\nInput :|S|Unlabeled Examples, Sample Size N\n1 k ←# event types (based on ontology);\n2 Sfew ←[ ];\n// obtain embeddings for all\nunlabeled instances\n3 for i ←1 to |S|do\n4 repi ←[enc(contexti), enc(trigger_texti)];\n5 add repi to all_reps;\n6 end\n7 clusters = k_means(all_reps);\n// add instances to samples\n8 for i ←1 to k do\n9 #instance = length(clusters[i])\n|S| ∗N;\n10 instances =\nsample(clusters[i], #instances);\n11 add instances to Sfew ;\n12 end\nIn the few-shot setting, we assume that we have\na budget to obtain annotations for a limited number\nof examples’ arguments (5%-20% of all examples)\nfor training. We denote the set of few training\nexamples as Sfew . We study (1) how different\nsampling strategies affect the Sfew ’s distributions\nand models’ performance; (2) how to select the\nbest set of examples (in zero or one round 8) and\nhave them annotated for training, to achieve better\nperformance at test time.\nWe propose a sampling method called JointEnc.\nIt uses k-means clustering upon the embeddings\n8One-round active learning setting (Wang et al., 2021).\n4652\nACE05 WikiEvent\nTrain Dev Test Train Dev Test\n# event types 33 22 31 49 35 34\n# arg. roles 22 22 21 57 32 44\n# docs 529 40 30 206 20 20\n# sentences 17172 923 832 5262 378 492\navg # events\nper doc 9.26 16.71 10.58 15.73 17.25 18.25\nTable 1: Dataset Statistics.\nof both input context and trigger text. This is eas-\nier to implement as compared to the one-round\nactive learning setting since our method does not\nrequire iterative training/testing for selecting unla-\nbeled examples. Details of how we obtain Sfew\nare illustrated in Algorithm 1. Specifically, we\nfirst obtain embeddings of context and trigger text\nfor each unlabeled example (line 3-6). Then we\nconduct k_means based clustering upon the embed-\ndings (line 7). Finally, we calculate the proportions\nof examples across all clusters9; and add the cor-\nresponding number of examples of each cluster to\nSfew (line 8-12).\n4 Experiments and Analysis\nWe conduct experiments and compare our model\nto baselines in three settings on two datasets: (1)\nfull supervision setting; domain transfer setting; as\nwell as (3) few-shot training setting (Section 4.5).\n4.1 Datasets Statistics and Evaluation\nFor the fully-supervised experiments, we use ACE\n2005 corpus for evaluation, it contains documents\ncrawled between year 2003 and 2005 from a va-\nriety of areas. We use the same data split and\npreprocessing steps as in previous work (Wadden\net al., 2019; Du and Cardie, 2020b). For the do-\nmain transfer setting, we conduct training on the\nACE05 training set and test on the WikiEvent test\nset. WikiEvent contains real-world news articles\nannotated with the DARPA KAIROS ontology10.\nMost of the event/argument types of WikiEvent’s\nontology do not appear in the ontology of ACE05\n(e.g. Disaster, Cognitive, Disease).\nThe statistics of the datasets are shown in Table 1.\nWe use the same test set as in Li et al. (2021) in the\ndomain transfer setting. As for the preprocessing\nstep of WikiEvent, since we train the models on the\n9We also try adding average number of examples for each\ncluster but performance is substantially worse.\n10https://www.darpa.mil/news-events/\n2019-01-04\nACE05 (including only arguments in the sentence\nwhere each trigger appears), we also use arguments\nwithin a maximum context window of the length\nequal to the average of ACE05 sentence length).\nAs for the evaluation, we use the same criteria as\nin previous work (Li et al., 2013) to judge whether\nan extracted argument is correct. We consider an\nargument mention to be correctly identified if its\noffsets match any of the reference arguments of\nthe current event (i.e. argument identification, or\nArg Id. for short); and an argument is correctly\nclassified if its role also matches (i.e. argument\nclassification or Arg C.).\nWhen comparing the extracted argument spans\nwith the gold-standard ones, in addition to using\nextract match (EM), we also consider head noun\nphrase match (HM). It is more lenient than EM\nsince it does not require the boundary/offsets to\nbe matched correctly (Huang and Riloff, 2012; Du\nand Cardie, 2020a). For example, “the John M.”\nand “John M.” match under the HM metric. Our\nresults are reported with Precision (P), Recall (R),\nand F-measure (F1) scores.\n4.2 Baselines\nWe compare our model to several representa-\ntive and competitive baselines (extractive meth-\nods and generation-based methods). EEQA (Du\nand Cardie, 2020b) uses the pretrained BERT as\nthe base model and add a linear layer on top, to\nobtain the beginning and end offsets of the an-\nswer/argument spans in the input context for each\nrole. GenIE (Li et al., 2021) use template-based\ngeneration for argument extraction. Its objective is\nto generate the template (including the arguments)\nand post-process the generated template to obtain\nthe argument mentions (Section 2). Sometimes the\ngenerated sequences don’t conform to the original\ntemplate thus affecting the performance. Genera-\ntive QA is our own baseline without the retrieval\ncomponent – it directly encodes the question for the\ncurrent argument role and input context to generate\nthe candidate argument spans.\n4.3 Fully-Supervised Setting Results\nIn Table 2, we report results for the fully super-\nvised setting. The score for Argument identifica-\ntion is strictly higher than Arg. classification since\nit only requires both the mention span match and\nrole match. We denote our proposed framework\nas R-GQA. To find out how the explicit modeling\nof the analogical relations (semantic relatedness)\n4653\nEM Arg Identification Arg Classification\nP R F1 P R F1\nEEQA\n(Du and Cardie, 2020b) 69.16 62.65 65.74 66.51 60.47 63.34\nGenIE (Li et al., 2021) 71.13 68.75 69.92 67.82 65.55 66.67\nGenerative QA 75.40±.70 72.10 ±.26 73.71 ±.20 71.92±.88 69.09 ±.59 70.47 ±.12\nR-GQA 76.90±1.04 74.17 ±.73 75.51 ±.58 74.10±.97 71.46 ±.47 72.75 ±.36\nAblations\nw/o analogy loss 76.20±1.27 72.04 ±.97 74.06 ±.33 73.90±1.39 69.87 ±.73 71.82 ±.32\nHM Arg Identification Arg Classification\nP R F1 P R F1\nGenIE (Li et al., 2021) 72.85 69.12 70.94 69.92 66.50 68.17\nGenerative QA 75.45±.58 73.70 ±.21 74.56 ±.18 71.88±.76 70.20 ±.00 71.03 ±.37\nR-GQA 76.95±1.34 74.93 ±.52 75.93 ±.91 74.04±1.00 72.10 ±.21 73.05 ±.59\nAblations\nw/o analogy loss 77.04±1.32 71.88 ±.52 74.36 ±.34 74.86±1.26 69.84 ±.51 72.26 ±.31\nTable 2: Fully-supervised setting experimental results (in %) on ACE05 data. The upper table is based on Exact\nMatch (EM) and the bottom table is based on Head Head (HM).\nbetween the demonstration and the current instance\nhelps, we also report ablation study results. More\nspecifically, we use BART-Large for all methods\nthat use BART as the base model to ensure they are\ncomparable. For our own model and its variations,\nwe conduct three runs, and calculate the average of\ntheir performance and standard deviations.\nWe observe that: (1) all the text generation-\nbased approaches outperform substantially EEQA\n(the extractive question answering based approach)\nin both precision&recall; Plus, generation-based\nmethods require only one pass and are faster than\nextractive-based method which has O(n2) com-\nplexity for span enumeration; (2) Our methods\nbased on generative QA (with 17621 gold QA\npairs) substantially improve over the pure template-\ngeneration based method (with 4419 gold tem-\nplates), we see that the better F1 mainly comes\nfrom consistently increase of precision&recall\n(~3%-4% for EM, ~1.5%-2% for HM). It makes\nsense considering in the template generation setting\n(I) hallucination happens; and (II) the generation\nsequence is longer, as compared to generating ar-\nguments for only one role in one pass; (3) Our\nR-GQA method benefits greatly from the retrieved\ndemonstrations (prompts). We see that the bet-\nter performance mainly comes from the increase\nin recall (smaller variance). Moreover, as for the\nfunctionality of explicitly model analogy relation\n(Lanalogy), we find that it provides a boost of recall\nof around 3% without sacrificing precision. These\nto a certain extent prove that the demo’s QA pair\nencourages the model to generate more arguments\nfor the current instance.\n4.4 How Does R-GQA perform in the domain\ntransfer setting\nTo mimic the real-world setting, we examine the\nportability of the models to test set of a new on-\ntology (event types and argument types) such as\nin Li et al. (2021). More specifically, we conduct\ntraining on ACE05 (with 33 event types) and test\non WikiEvent dataset (with 50 event types).\nIn Table 3, we present the domain transfer re-\nsults. For this new setting, the best methods’ per-\nformance on WikiEvent are around 20% lower (F1)\nas compared to the fully supervised setting (Du\net al., 2022). Mainly because: (1) the WikiEvent\ndataset is harder as compared to ACE05 – with a\nperformance drop around 5-10% F1 across mod-\nels; (2) the test set of WikiEvent includes many\nevent/argument types that are distinct from exist-\ning ones from ACE05. Accordingly, we find that\nperformance on the subset of data of distinctly\nevent types largely drops. We list the types in Ap-\npendix B. When comparing QA-based generation\nmodel and GenIE, we observe that (1) recall of the\nQA-based models is substantially higher (>10%)\n– leading to large argument identification perfor-\nmance improvement; while our models do not have\n4654\nEM HM\nModels Arg Id. Arg C. Arg Id. Arg C.\nP R F1 P R F1 P R F1 P R F1\nGenIE\n(Li et al., 2021) 49.96 23.47 31.88 44.92 21.09 28.66 52.87 24.84 33.74 46.94 22.04 29.95\nGenerative QA 47.12 35.61 40.57 32.32 24.42 27.82 49.71 37.57 42.79 34.20 25.84 29.44\nR-GQA 44.88 40.68 42.63 31.42 28.42 29.82 47.65 43.17 45.25 33.10 29.93 31.41\nTable 3: Domain transfer setting results (in %).\n200 300 400 500 600 700 800 900 1000\nModels (4.8%) (7.1%) (9.5%) (11.9%) (14.3%) (16.7%) (19.0%) (21.4%) (23.8%)\nGenIE 29.13 38.19 44.19 49.09 50.26 46.85 54.41 58.47 59.94\nOurs (R-GQA) 38.79 47.64 52.55 56.97 56.40 58.90 61.24 58.77 61.41\nTable 4: Few-shot performance comparison (F1 in %).\nan advantage in precision and even drops a bit, but\nthe general performance (F1) is consistently higher;\n(2) Our R-GQA model’s retrieval component helps\nthe model generate more arguments and improves\nR and F1.\n4.5 How Does R-GQA perform in Few-shot\nSetting and What is Sampling Strategy’s\nInfluence\nFirstly, in Table 4, we present comparisons be-\ntween GenIE and R-GQA in the few-shot setting\non ACE05. To obtain the few-shot training ex-\namples, we use the sampling strategy proposed in\nSection 3.3. The # examples varies from 200 (5%)\nto 1k (20%). We observe the trend that when the\nnumber of examples is smallest, the performance\ngap is largest (around 10% F1). While as the exam-\nple number grows, generally the gap minimizes –\nfrom 10% (200), to 6% (600), to around 2% (1k).\nNext, we report results for different sampling\nmethods (including the one-round active learning\nsetting) to find out what are the more important\nfactors for the event argument extraction task’s an-\nnotation (with a fixed budget). Namely, we sam-\nple from “unlabeled” examples with the following\nstrategies: Random picks the examples randomly\nwhich (nearly) match the distribution of event types\nin the test set; AL is the one-round active learning\nbased approach – basically, a model is trained on\nthe 100 examples with annotations and unlabeled\nexamples that are most challenging (model most\nuncertain about) are selected. Our JointEnc strat-\negy first conducts clustering on unlabeled examples\n(based on both input context and trigger text) and\nselects from each cluster # examples proportional\nto the size of each cluster; Context also conducts\nclustering based sampling similar to JointEnc but\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n200 300 400 500 600 700 800 900 1000\nRandom AL Context JointEnc\nFigure 3: Distance (Y-axis) between event type distribu-\ntions of (1) sampled examples with different sampling\nstrategies and (2) real data. X-axis: sampling size.\nonly embeds each example based on its context.\nFor the few-shot setting with increasing sam-\npling size, we calculate the Hellinger distance (Be-\nran, 1977) between distributions of examples sam-\npled from each strategy and the true data distribu-\ntion (represented by training data with labels). The\ndistances are presented in Figure 3. We observe\nthat (1) the distances between distributions of sam-\npled examples and true data distribution decrease,\nas the sampling size grows; (2) sampled data based\non JointEnc is generally closest to true data dis-\ntribution across different sampling sizes. Corre-\nspondingly, Figure 4 reports the performances of\nR-GQA trained on samples from each strategy. The\nmodel trained on examples from our JointEnc out-\nperforms other strategies’, demonstrating the ben-\nefit of JointEnc. Moreover, we find that there is\n4655\n20.00\n30.00\n40.00\n50.00\n60.00\n70.00\n200 300 400 500 600 700 800 900 1000\nRandom AL Context JointEnc Full Train\nFigure 4: R-GQA’s few-shot performance under differ-\nent sampling strategies.\na correlation between distribution distances and\nfew-shot experimental results – the smaller the dis-\ntances are, models trained on the sampled set have\nbetter performance. This phenomenon is especially\nobvious when the sampling size is small (5%–10%\nof training data). We also provide an analysis of\neach event type in Appendix (Section D).\n5 Related Work\nEvent Extraction and Extractive&Generation-\nbased Approaches Traditionally, researchers\nhave been investigating extractive approaches for\nevent/information extraction. Specifically, one\nbranch of work use B-I-O sequence labeling based\napproaches using CRF or structured SVM mod-\nels (Björne et al., 2009; Yang and Mitchell, 2016;\nLin et al., 2020), and more recently with neural net-\nworks (Chen et al., 2015; Nguyen et al., 2016). An-\nother branch of extractive approaches includes us-\ning span enumeration (Wadden et al., 2019), as well\nas using question answering to encourage transfer\nbetween argument roles (Du and Cardie, 2020b).\nRecently, generation-based approaches have\nbeen proposed. Among them more generally,\nTANL (Paolini et al., 2020) proposes to use\ntranslation-based approaches for structured predic-\ntion. More specifically, it constructs decoding tar-\ngets by inserting text markers and labels around\nentity mentions in the input sentence. To better\ncapture cross-entity dependencies. Huang et al.\n(2021); Li et al. (2021); Du et al. (2021); Huang\net al. (2022) propose template-generation based ap-\nproaches. They fill in the role slots in the template\n(e.g. Sec 2) with arguments to construct the gold\nsequences. As compared to TANL and template\ngeneration-based methods, our R-GQA is designed\nto be a QA-based generative model with a simpler\ngeneration objective. Plus, it augments the current\nexample’s context with the most similar demon-\nstration in the training set as prompt. It gets the\nbest of both worlds (i.e. question answering and\ngenerative models).\nRetrieval-augmented Text Generation and In\nContext Learning Recent studies have shown\nthe effectiveness ofretrieval augmentation in many\ngenerative NLP tasks, such as knowledge-intensive\nquestion answering (Lewis et al., 2020b; Guu et al.,\n2020) and dialogue response generation (Cai et al.,\n2019). They mainly retrieve additional knowl-\nedge or relevant information, but not demonstra-\ntions (input-output pairs). Another closely rele-\nvant branch of work is in-context learning, it’s a\ntuning-free approach that adapts to a new task by\nproviding demonstrations (input-output pairs) as\nprompts to generate the “answer” (Brown et al.,\n2020). GPT-3 proposes to use random examples as\ndemonstrations. Liu et al. (2022) refines the strat-\negy by proposing to retrieve demonstrations that\nare semantically-similar to the current example as\nprompt. They show the capability of PLM to learn\nfrom similar examples.\nDifferent from the work above, our work draws\ninsights from both retrieval-augmented text genera-\ntion and in-context learning. It (1) retrieves from\nthe training set the most similar demonstration (QA\npair) and uses it as a prompt; (2) uses gradient de-\nscent to optimize the model. Plus, it focuses on the\nspecific argument extraction problem – our model\nnot only augments the input context with demon-\nstration but also determines how much to learn\nfrom it (by training with analogical loss).\n6 Conclusions\nIn this work, we introduce a retrieval-augmented\ngenerative question answering framework (R-\nGQA) for event argument extraction. Our model\ngenerates arguments (answers) for each role, con-\nditioned on both the current input context and\nthe analogical demonstration prompt (based on\ntheir semantic similarity). Empirically, we show\nthat R-GQA outperforms current competitive base-\nlines with large margins in fully-supervised, cross-\ndomain and few-shot learning settings. We conduct\na thorough analysis and benchmark how different\n4656\nsampling strategies influence models’ performance\nin the few-shot learning setting. We find that for\nevent argument extraction, diversifying the exam-\nples makes the sampling distribution closer to the\ntrue distribution and contributes to models’ better\nperformance.\nLimitations\nThis work has certain limitations.\n• Firstly, since the pre-trained model we use\n(BART-Large) has many parameters, one\nmodel’s training will nearly occupy one\nNVIDIA Tesla V100 16GB GPU; As for in-\nference, it takes about 1GB of space.\n• Although the BART-based models (GenIE and\nR-GQA) are end-to-end and have a great per-\nformance boost, the inference time (about 2\nexamples/s) is slightly longer as compared to\nmanual-feature based approaches.\n• In the real domain transfer setting, the gen-\neral performance of models is still lower than\n40% (F1), making the systems not compet-\nitive in real circumstances. In the future, it\nis worth investigating how to tackle this chal-\nlenge by both more general ontology design-\ning and stronger&robust methods.\nAcknowledgement\nWe thank the anonymous reviewers helpful sug-\ngestions. This research is based upon work sup-\nported by U.S. DARPA KAIROS Program No.\nFA8750-19-2-1004, U.S. DARPA AIDA Program\nNo. FA8750-18-2-0014 and LORELEI Program\nNo. HR0011-15-C-0115. The views and conclu-\nsions contained herein are those of the authors and\nshould not be interpreted as necessarily represent-\ning the official policies, either expressed or implied,\nof DARPA, or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for governmental purposes notwithstand-\ning any copyright annotation therein.\nReferences\nRudolf Beran. 1977. Minimum hellinger distance esti-\nmates for parametric models. The annals of Statistics,\npages 445–463.\nJari Björne, Juho Heimonen, Filip Ginter, Antti Airola,\nTapio Pahikkala, and Tapio Salakoski. 2009. Extract-\ning complex biological events with rich graph-based\nfeature sets. In Proceedings of the BioNLP 2009\nWorkshop Companion Volume for Shared Task, pages\n10–18.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\naojiang Liu, and Shuming Shi. 2019. Retrieval-\nguided dialogue response generation via a matching-\nto-generation framework. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866–1875.\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and\nJun Zhao. 2015. Event extraction via dynamic multi-\npooling convolutional neural networks. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , Beijing, China.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGeorge Doddington, Alexis Mitchell, Mark Przybocki,\nLance Ramshaw, Stephanie Strassel, and Ralph\nWeischedel. 2004. The automatic content extrac-\ntion (ACE) program – tasks, data, and evaluation. In\nProceedings of the Fourth International Conference\non Language Resources and Evaluation (LREC’04),\nLisbon, Portugal. European Language Resources As-\nsociation (ELRA).\nXinya Du and Claire Cardie. 2020a. Document-level\nevent role filler extraction using multi-granularity\ncontextualized encoding. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 8010–8020, Online. Association\nfor Computational Linguistics.\nXinya Du and Claire Cardie. 2020b. Event extraction by\nanswering (almost) natural questions. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 671–683,\nOnline. Association for Computational Linguistics.\nXinya Du, Sha Li, and Heng Ji. 2022. Dynamic global\nmemory for document-level argument extraction. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\n4657\nLong Papers), pages 5264–5275, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nXinya Du, Alexander Rush, and Claire Cardie. 2021.\nTemplate filling with generative transformers. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 909–914, Online. Association for Computa-\ntional Linguistics.\nRalph Grishman. 2019. Twenty-five years of infor-\nmation extraction. Natural Language Engineering,\n25(6):677–692.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, pages 3929–3938.\nKuan-Hao Huang, I-Hung Hsu, Prem Natarajan, Kai-\nWei Chang, and Nanyun Peng. 2022. Multilin-\ngual generative language models for zero-shot cross-\nlingual event argument extraction. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4633–4646, Dublin, Ireland. Association for\nComputational Linguistics.\nKung-Hsiang Huang, Sam Tang, and Nanyun Peng.\n2021. Document-level entity-based extraction as tem-\nplate generation. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5257–5269, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nRuihong Huang and Ellen Riloff. 2012. Modeling tex-\ntual cohesion for event extraction. In Proceedings of\nthe Twenty-Sixth AAAI Conference on Artificial Intel-\nligence, July 22-26, 2012, Toronto, Ontario, Canada.\nAAAI Press.\nDaniel Jurafsky and James H Martin. 2018.\nSpeech and language processing. prepa-\nration [cited 2020 June 1] Available from:\nhttps://web.stanford.edu/˜jurafsky/slp3.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nQi Li, Heng Ji, and Liang Huang. 2013. Joint event\nextraction via structured prediction with global fea-\ntures. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 73–82, Sofia, Bulgaria.\nAssociation for Computational Linguistics.\nSha Li, Heng Ji, and Jiawei Han. 2021. Document-level\nevent argument extraction by conditional generation.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 894–908, Online. Association for Computa-\ntional Linguistics.\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA joint neural model for information extraction with\nglobal features. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, Online. Association for Computational Lin-\nguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (Dee-\nLIO 2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114.\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Grish-\nman. 2016. Joint event extraction via recurrent neural\nnetworks. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 300–309, San Diego, California.\nAssociation for Computational Linguistics.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie\nMa, Alessandro Achille, RISHITA ANUBHAI, Ci-\ncero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2020. Structured prediction as translation be-\ntween augmented natural languages. In International\nConference on Learning Representations.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nDavid Wadden, Ulme Wennberg, Yi Luan, and Han-\nnaneh Hajishirzi. 2019. Entity, relation, and event\nextraction with contextualized span representations.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n4658\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5784–\n5789, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTianhao Wang, Si Chen, and Ruoxi Jia. 2021. One-\nround active learning. CoRR, abs/2104.11843.\nBishan Yang and Tom M. Mitchell. 2016. Joint extrac-\ntion of events and entities within a document context.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 289–299, San Diego, California. Association\nfor Computational Linguistics.\nA Hyperparameters\ntrain batch size 4\neval batch size 4\nlearning rate 3e-5\naccumulate grad batches 4\ntraining epoches 6\nwarmup steps 0\nweight decay 0\n# gpus 1\nTable 5: Hyperparameters for Training R-GQA.\nB Distinct Event Types in WikiEvent\nOntology (as Compared to ACE05)\nHierachy L1 Hierachy L2 Hierachy L3\nArtifactExistenceDamageDestroyDisableDismantleDamage\nArtifactExistenceDamageDestroyDisableDismantleDestroy\nArtifactExistenceDamageDestroyDisableDismantleDisableDefuse\nArtifactExistenceDamageDestroyDisableDismantleDismantle\nArtifactExistenceDamageDestroyDisableDismantleUnspecified\nArtifactExistenceManufactureAssemble Unspecified\nCognitive IdentifyCategorize Unspecified\nCognitive Inspection SensoryObserve\nCognitive Research Unspecified\nCognitive TeachingTrainingLearningUnspecified\nDisaster DiseaseOutbreak Unspecified\nDisaster FireExplosion Unspecified\nGenericCrimeGenericCrime GenericCrime\nJustice InvestigateCrime Unspecified\nLife Consume Unspecified\nLife Illness Unspecified\nLife Infect Unspecified\nMedical Diagnosis Unspecified\nMedical Intervention Unspecified\nMedical Vaccinate Unspecified\nMovement Transportation PreventPassage\nTransaction Donation Unspecified\n4659\nC Further Findings and (Error) Analysis\nError Cases and Remaining Challenges We conduct an analysis on the error cases and summarize\nrepresentative causes and provide examples below:\n• Lack of contextual understanding. For example, “Earlier documents in the case have included embar-\nrassing details about perks [Welch]Person received as part of his retirement package from GE ..”. The\nmodel predicts the pronoun “his” which is closer to the trigger word as the final PERSON argument\nfor the retiring event, ignoring the better option “Welch” which is more informative. Also with the\ndocument-level contextual knowledge of the person “Welch” that appears frequently, it would be easier\nfor the model to decide.\n• Complex language usage such as idioms and metaphors (e.g. for the event with “swept out of power” as\nthe trigger, the arguments’ recall is very low). Addressing these phenomena is difficult since it requires\nricher knowledge about the background/culture. Plus, the special tokenization process further (e.g. BPE:\nByte-Pair Encoding) further hurts the performance of extracting certain words that rarely appear.\n• Inherent imperfectness of the datasets. The inter-annotator agreement for ACE05/WikiEvent is limited\n(under 85%), so theoretically there is an upper bound for human performance as well. For example,\nwe see that the head noun match (HM) score is strictly higher than the exact match (EM) in Section\n4, and the gap mitigates as the performance gets higher (over 70% F1). This demonstrates there is an\nambiguity in determining the argument’s boundary. Moreover, for the example in the first bullet point,\npredicting pronoun does not get credit – while in a certain amount of training data it’s permitted.\nInfluence of Similarity-based Retrieval In Figure 5, we provide insights on how the similarity between\nthe demonstration and current context affects the model’s performance. We divide the original test set\ninto five subsets, corresponding to the example’s similarity score. It is observed there is a trend that when\nthe similarity score grows, performance of the model also grows, especially when the similarity is over\n0.7. This to a certain extent shows the benefits of augmenting the current context with a more similar\ndemonstration as the prompt.\nSimilarity Score\nF1 (%)\n60\n70\n80\n90\n100\n<0.5 0.5-0.6 0.6-0.7 0.7-0.8 >0.8\nFigure 5: R-GQA’s performances on subsets of dataset (as the similarity scores grow).\n4660\nD Distribution Distances for Each Event Type (ACE05)\nEvent Type Random AL Context JointEnc\nMovement:Transport 0.38 1.21 0.37 0.28\nPersonnel:Elect 0.38 0.75 0.18 0.26\nPersonnel:Start-Position 0.34 0.78 0.39 0.46\nPersonnel:Nominate 0.43 0.52 0.49 0.31\nPersonnel:End-Position 0.66 0.99 0.23 0.34\nConflict:Attack 0.30 0.16 0.34 0.23\nContact:Meet 0.40 0.20 0.43 0.40\nLife:Marry 0.54 0.32 0.25 0.24\nTransaction:Transfer-Money 0.38 0.38 0.41 0.42\nConflict:Demonstrate 0.26 0.53 0.43 0.37\nBusiness:End-Org 0.67 0.25 0.64 0.28\nJustice:Sue 0.63 1.09 0.47 0.48\nLife:Injure 0.37 0.46 0.47 0.32\nLife:Die 0.32 0.94 0.34 0.22\nJustice:Arrest-Jail 0.42 0.29 0.45 0.46\nContact:Phone-Write 0.24 0.31 0.33 0.23\nTransaction:Transfer-Ownership 0.24 0.32 0.30 0.22\nBusiness:Start-Org 0.78 0.86 0.45 0.30\nJustice:Execute 0.72 0.32 0.81 0.32\nJustice:Trial-Hearing 0.20 0.38 0.46 0.28\nLife:Be-Born 0.77 0.31 0.41 0.28\nJustice:Charge-Indict 0.27 0.68 0.44 0.27\nJustice:Convict 0.47 0.55 0.49 0.48\nJustice:Sentence 0.13 0.41 0.34 0.57\nBusiness:Declare-Bankruptcy 0.27 0.84 0.37 0.30\nJustice:Release-Parole 0.38 0.22 0.46 0.46\nJustice:Fine 0.42 0.22 0.43 0.41\nJustice:Pardon 0.41 0.45 0.43 0.48\nJustice:Appeal 0.62 0.35 0.31 0.63\nJustice:Extradite 0.37 0.83 0.55 0.56\nLife:Divorce 0.32 1.01 0.30 0.20\nBusiness:Merge-Org 0.60 0.47 0.73 0.42\nJustice:Acquit 0.59 0.71 0.49 0.57\nSum 14.65 19.36 14.39 12.31\nAverage 0.43 0.55 0.42 0.36\n4661\nE Generated Questions for Argument Roles in WikiEvent Ontology\nEvent Type Argument Role Question\nArtifactExistence.DamageDestroyDisableDismantle.Damage Damager who is the damaging agent?\nArtifact what is being damaged?\nInstrument what is the instrument used in the damage?\nPlace where the damage takes place?\nArtifactExistence.DamageDestroyDisableDismantle.Destroy Destroyer who is the destroying agent?\nArtifact what is being destroyed?\nInstrument what is the instrument used in the destroy?\nPlace where the destroy takes place?\nArtifactExistence.DamageDestroyDisableDismantle.DisableDefuse Disabler who is the disable agent?\nArtifact what is being disabled?\nInstrument what is the instrument used in the disable?\nPlace where the disable takes place?\nArtifactExistence.DamageDestroyDisableDismantle.Dismantle Dismantler who is the dismantle agent?\nArtifact what is being dismantled?\nInstrument what is the instrument used in the dismantle?\nComponents who is being dismantled?\nPlace where the dismantle takes place?\nArtifactExistence.DamageDestroyDisableDismantle.Unspecified DamagerDestroyer who is the damaging agent?\nArtifact what is being destroyed\nInstrument what is the instrument used in the destroy\nPlace where the destroy takes place?\nArtifactExistence.ManufactureAssemble.Unspecified ManufacturerAssembler what is the manufacutring agent?\nArtifact what is being manufactured?\nComponents what is the components used for the manufacture?\nInstrument what is the instrument used in the manufacture?\nPlace where the manufacutring takes place?\nBusiness:Declare-Bankruptcy Org What declare bankruptcy?\nPlace Where the merger takes place?\nBusiness:End-Org Org What is ended?\nPlace Where the event takes place?\nBusiness:Merge-Org Org What is merged?\nBusiness:Start-Org Agent Who is the founder?\nOrg What is started?\nPlace Where the event takes place?\nCognitive.IdentifyCategorize.Unspecified Identifier who is the identifier?\nIdentifiedObject what is being identified?\nIdentifiedRole what is being identified as?\nPlace where the identifiying takes place?\nCognitive.Inspection.SensoryObserve Observer who is the observer?\nObservedEntity what is being observed?\nInstrument what is the instrument used in the observe?\nPlace where the observe takes place?\nCognitive.Research.Unspecified Researcher who is the researcher?\nSubject what is being researched?\nMeans what is being used for the research?\nPlace where the research takes place?\nCognitive.TeachingTrainingLearning.Unspecified TeacherTrainer who is the teaching agent?\nFieldOfKnowledge what is being taught?\nLearner who is being taught?\nMeans what is being used for the teaching\nInstitution where is the teaching at institution\nPlace where the teaching takes place?\nConflict.Attack.DetonateExplode Attacker Who is the denotating agent?\nTarget who is the target of the attack?\nInstrument What is the instrument used in the attack?\nExplosiveDevice what is the explosive device?\nPlace Where the detonation takes place?\n4662\nConflict.Demonstrate.DemonstrateWithViolence Demonstrator who is demonstrating agent?\nRegulator who is the regulator?\nVisualDisplay what is the visual display?\nTopic what is the topic for the demonstration?\nTarget who is the target of the demonstration?\nPlace where the demonstration takes place?\nConflict.Demonstrate.Unspecified Demonstrator who is demonstrating agent?\nRegulator who is the regulator?\nVisualDisplay what is the visual display?\nTopic what is the topic for the demonstration?\nTarget who is the target of the demonstration?\nPlace where the demonstration takes place?\nConflict:Attack Attacker Who is the attacking agent?\nInstrument What is the instrument used in the attack?\nPlace Where the attack takes place?\nTarget Who is the target of the attack?\nVictim Who is the target of the attack?\nConflict:Demonstrate Entity Who is demonstrating agent?\nPlace Where the demonstration takes place?\nContact.Contact.Broadcast Communicator who is communicating agents?\nRecipient who is the recipient?\nInstrument What is the instrument used in the communication?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact.Contact.Correspondence Participant who is communicating agents?\nInstrument What is the instrument used in the communication?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact.Contact.Meet Participant Who are meeting?\nTopic what is the topic of the meeting\nPlace Where the meeting takes place?\nContact.Contact.Unspecified Participant who is communicating agents?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact.Prevarication.Unspecified Communicator who is communicating agents?\nRecipient who is communicating agents?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact.RequestCommand.Unspecified Communicator who is communicating agents?\nRecipient who is communicating agents?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact.ThreatenCoerce.Unspecified Communicator who is communicating agents?\nRecipient who is communicating agents?\nTopic what is the communicating topic?\nPlace Where it takes place?\nContact:Meet Entity Who are meeting?\nPlace Where the meeting takes place?\nContact:Phone-Write Entity Who is communicating agents?\nPlace Where it takes place?\nControl.ImpedeInterfereWith.Unspecified Impeder who is the impeder agent?\nImpededEvent what is the impede event?\nPlace where the impede takes place?\nDisaster.Crash.Unspecified DriverPassenger Who is responsible for the transport event?\nVehicle What is the vehicle used to transport the person or artifact?\nCrashObject what is being crashed into?\nPlace where the transport takes place?\nDisaster.DiseaseOutbreak.Unspecified Disease what broke out?\nVictim Who is the harmed person?\nPlace Where the disease takes place?\nDisaster.FireExplosion.Unspecified FireExplosionObject what caught fire?\nInstrument What is the instrument used in the explosion?\nPlace where the explosion takes place?\nGenericCrime.GenericCrime.GenericCrime Perpetrator who committed a crime?\nVictim Who is the target of the crime?\nPlace Where the crime takes place?\n4663\nJustice.Acquit.Unspecified JudgeCourt What is the judge?\nDefendant Who is the defendant?\nCrime what is the crime being acquitted?\nPlace Where the acquit takes place?\nJustice.ArrestJailDetain.Unspecified Jailer Who is the arresting agent?\nDetainee Who is jailed or arrested?\nCrime what is the crime being arrested?\nPlace Where the person is arrested?\nJustice.ChargeIndict.Unspecified Prosecutor Indicated by whom?\nDefendant Who is indicted?\nJudgeCourt Who was the judge or court?\nCrime what is the crime being charged?\nPlace Where the indictment takes place?\nJustice.Convict.Unspecified JudgeCourt Who is the judge or court?\nDefendant Who is convicted?\nCrime what is the crime being convicted?\nPlace Where the conviction takes place?\nJustice.InvestigateCrime.Unspecified Investigator Who is the investigator?\nDefendant Who is investigated?\nCrime what is the crime being investigated?\nPlace Where the investigation takes place?\nJustice.ReleaseParole.Unspecified JudgeCourt Who will release?\nDefendant Who is released?\nCrime what is the crime being released?\nPlace Where the release takes place?\nJustice.Sentence.Unspecified JudgeCourt Who is the judge or court?\nDefendant Who is sentenced?\nCrime what is the crime being sentenced?\nSentence what is the sentence?\nPlace Where the sentencing takes place?\nJustice.TrialHearing.Unspecified Prosecutor Who is the prosecuting agent?\nDefendant Who is on trial?\nJudgeCourt Who is the judge or court?\nCrime what is the crime being tried?\nPlace Where the trial takes place?\nJustice:Acquit Adjudicator Who was the judge or court?\nDefendant Who was acquitted?\nJustice:Appeal Adjudicator Who was the judge or court?\nPlace Where the appeal takes place?\nPlaintiff What is the plaintiff?\nJustice:Arrest-Jail Agent Who is the arresting agent?\nPerson Who is jailed or arrested?\nPlace Where the person is arrested?\nJustice:Charge-Indict Adjudicator Who was the judge or court?\nDefendant Who is indicted?\nPlace Where the indictment takes place?\nProsecutor Indicated by whom?\nJustice:Convict Adjudicator Who is the judge or court?\nDefendant Who is convicted?\nPlace Where the conviction takes place?\nJustice:Execute Agent Who carry out the execution?\nPerson Who was executed?\nPlace Where the execution takes place?\nJustice:Extradite Agent Who is the extraditing agent?\nPerson Who is being extradited\nDestination Where the person is extradited to?\nOrigin Where is original location of the person being extradited?\nJustice:Fine Adjudicator Who do the fining?\nEntity What was fined?\nPlace Where the fining Event takes place?\nJustice:Pardon Adjudicator Who do the pardoning?\nDefendant Who was pardoned?\nPlace Where the pardon takes place?\nJustice:Release-Parole Entity Who will release?\nPerson Who is released?\nPlace Where the release takes place?\nJustice:Sentence Adjudicator Who is the judge or court?\nDefendant Who is sentenced?\nPlace Where the sentencing takes place?\nJustice:Sue Adjudicator Who is the judge or court?\nDefendant Who is sued against?\nPlace Where the suit takes place?\nPlaintiff Who is the suing agent?\n4664\nJustice:Trial-Hearing Adjudicator Who is the judge or court?\nDefendant Who is on trial?\nPlace Where the trial takes place?\nProsecutor Who is the prosecuting agent?\nLife.Consume.Unspecified ConsumingEntity what is the consuming agent?\nConsumedThing what is consumed?\nPlace where the consuming takes place?\nLife.Die.Unspecified Victim Who died?\nPlace Where the death takes place?\nKiller Who is the attacking agent?\nMedicalIssue what is the medical issue\nLife.Illness.Unspecified Victim who is victim?\nDeliberateInjurer who is the deliberate injurer\nDisease what is the disease or sickness?\nPlace where the event takes place?\nLife.Infect.Unspecified Victim who is victim?\nInfectingAgent who infected?\nSource what is the infect from?\nPlace where the event takes place?\nLife.Injure.Unspecified Victim Who is the harmed person?\nInjurer Who is the attacking agent?\nInstrument What is the device used to inflict the harm?\nBodyPart what is the body part being harmed?\nMedicalCondition what is the medical issue?\nPlace Where the injuring takes place?\nLife:Be-Born Person Who is born?\nPlace Where the birth takes place?\nLife:Die Agent Who is the attacking agent?\nInstrument What is the device used to kill?\nPlace Where the death takes place?\nVictim Who died?\nLife:Divorce Person Who are divorced?\nPlace Where the divorce takes place?\nLife:Injure Agent Who is the attacking agent?\nInstrument What is the device used to inflict the harm?\nPlace Where the injuring takes place?\nVictim Who is the harmed person?\nLife:Marry Person Who are married?\nPlace Where the marriage takes place?\nMedical.Diagnosis.Unspecified Treater who diagnosed the patient?\nPatient who is diagnosed?\nSymptomSign what is the symptom?\nMedicalCondition what is the medical condition?\nPlace where the event takes place?\nMedical.Intervention.Unspecified Treater what treated the patient?\nPatient who is treated?\nMedicalIssue what is the medical issue?\nInstrument What is the instrument used in the treatment?\nPlace Where the treatment takes place?\nMedical.Vaccinate.Unspecified Treater what treated the patient?\nPatient who is treated?\nVaccineTarget who is the target of the vaccination?\nVaccineMethod what is the method of the vaccination?\nPlace Where the vaccination takes place?\nMovement.Transportation.Evacuation Transporter Who is responsible for the transport event?\nPassengerArtifact Who is being transported?\nVehicle What is the vehicle used to transport the person or artifact?\nOrigin Where the transporting originated?\nDestination Where the transporting is directed?\nMovement.Transportation.IllegalTransportation Transporter Who is responsible for the transport event?\nPassengerArtifact Who is being transported?\nVehicle What is the vehicle used to transport the person or artifact?\nOrigin Where the transporting originated?\nDestination Where the transporting is directed?\nMovement.Transportation.PreventPassage Transporter Who is responsible for the transport event?\nPassengerArtifact Who is being transported?\nVehicle What is the vehicle used to transport the person or artifact?\nPreventer who is preventing the transport?\nOrigin Where the transporting originated?\nDestination Where the transporting is directed?\nMovement.Transportation.Unspecified Transporter Who is responsible for the transport event?\nPassengerArtifact Who is being transported?\nVehicle What is the vehicle used to transport the person or artifact?\nOrigin Where the transporting originated?\nDestination Where the transporting is directed?\n4665\nMovement:Transport Agent Who is responsible for the transport event?\nArtifact Who is being transported?\nDestination Where the transporting is directed?\nOrigin Where the transporting originated?\nVehicle What is the vehicle used to transport the person or artifact?\nPersonnel.EndPosition.Unspecified Employee Who is the employee?\nPlaceOfEmployment Who is the employer?\nPosition what is the position?\nPlace Where the employment relationship ends?\nPersonnel.StartPosition.Unspecified Employee Who is the employee?\nPlaceOfEmployment Who is the employer?\nPosition what is the position?\nPlace Where the employment relationship begins?\nPersonnel:Elect Entity Who voted?\nPerson Who was elected?\nPlace Where the election takes place?\nPersonnel:End-Position Entity Who is the employer?\nPerson Who is the employee?\nPlace Where the employment relationship ends?\nPersonnel:Nominate Agent Who is the nominating agent?\nPerson Who are nominated?\nPersonnel:Start-Position Entity Who is the employer?\nPerson Who is the employee?\nPlace Where the employment relationship begins?\nTransaction.Donation.Unspecified Giver Who is the donating agent?\nRecipient Who is the recipient?\nBeneficiary Who benefits from the transfer?\nArtifactMoney what is being donated?\nPlace Where the transaction takes place?\nTransaction.ExchangeBuySell.Unspecified Giver Who is the selling agent?\nRecipient Who is the buying agent?\nAcquiredEntity Who was bought or sold?\nPaymentBarter how much was the selling?\nBeneficiary Who benefits from the transaction?\nPlace Where the sale takes place?\nTransaction:Transfer-Money Beneficiary Who benefits from the transfer?\nGiver Who is the donating agent?\nPlace Where the transaction takes place?\nRecipient Who is the recipient?\nTransaction:Transfer-Ownership Artifact Who was bought or sold?\nBeneficiary Who benefits from the transaction?\nBuyer Who is the buying agent?\nPlace Where the sale takes place?\nSeller Who is the selling agent?\n4666",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7802995443344116
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.6503674983978271
    },
    {
      "name": "Question answering",
      "score": 0.6069237589836121
    },
    {
      "name": "Generative grammar",
      "score": 0.6038998961448669
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5716040730476379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5617079138755798
    },
    {
      "name": "Cluster analysis",
      "score": 0.5418418645858765
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5275582671165466
    },
    {
      "name": "Dependency (UML)",
      "score": 0.48020896315574646
    },
    {
      "name": "Decodes",
      "score": 0.441417396068573
    },
    {
      "name": "Natural language processing",
      "score": 0.41606754064559937
    },
    {
      "name": "Machine learning",
      "score": 0.4005263149738312
    },
    {
      "name": "Algorithm",
      "score": 0.1301136612892151
    },
    {
      "name": "Decoding methods",
      "score": 0.127566397190094
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162577319",
      "name": "The University of Texas at Dallas",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 29
}