{
  "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
  "url": "https://openalex.org/W4385572867",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2111695320",
      "name": "Boshi Wang",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2101262251",
      "name": "Xiang Deng",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2105996224",
      "name": "Huan Sun",
      "affiliations": [
        "The Ohio State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3115947671",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3035164567",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2970023150",
    "https://openalex.org/W4287854486",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W3173566921",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W3125238517",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4285080136",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4297778521",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W3045683288",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4287393336"
  ],
  "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a \"chain of thought\" for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step's contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714–2730\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nIteratively Prompt Pre-trained Language Models for Chain of Thought\nBoshi Wang, Xiang Deng and Huan Sun\nThe Ohio State University, Columbus, OH\n{wang.13930,deng.595,sun.397}@osu.edu\nAbstract\nWhile Pre-trained Language Models (PLMs)\ninternalize a great amount of world knowl-\nedge, they have been shown incapable of re-\ncalling these knowledge to solve tasks requir-\ning complex & multi-step reasoning. Similar\nto how humans develop a “chain of thought”\nfor these tasks, how can we equip PLMs with\nsuch abilities? In this work, we explore an\niterative prompting framework, a new prompt-\ning paradigm which progressively elicits rele-\nvant knowledge from PLMs for multi-step in-\nference. We identify key limitations of existing\nprompting methods, namely they are either re-\nstricted to queries with a single identifiable re-\nlation/predicate, or being agnostic to input con-\ntexts, which makes it difficult to capture vari-\nabilities across different inference steps. We\npropose an iterative context-aware prompter,\nwhich addresses these limitations by learning\nto dynamically synthesize prompts conditioned\non the current step’s contexts. Experiments on\nthree datasets involving multi-step reasoning\nshow the effectiveness of the iterative scheme\nand the context-aware prompter design.1\n1 Introduction\nHumans can develop a “chain of thought” for com-\nplex decision making. For example, when asked\nthe question (Q) shown in Figure 1, which involves\ncomposition, an important type of multi-step rea-\nsoning, humans apply two consecutive steps to de-\nrive the final answer: 1) find “father” of the topic\nentity “Gwilym Lloyd George” (C1); 2) find “birth-\nplace” of the entity returned in the first step (C2).\nRecently, large-scale pre-trained language mod-\nels (PLMs) have been shown capable of internal-\nizing a great amount of simple factual knowledge\nsuch as C1 and C2, yielding competitive perfor-\nmance on a range of knowledge-intensive tasks\nwithout resorting to any external knowledge source\n1Our source code is available at https://github.\ncom/sunlab-osu/IterPrompt.\nIterative Prompting\nStandard Probing\nC1: “David Lloyd George is the \nfather of Gwilym Lloyd George”\nQ: “What is the place of birth of Gwilym Lloyd George’s father?”\n(Answer: Manchester)\nPLM\n“Who is the father of \nGwilym Lloyd George?”\n“David Lloyd George”\n“Unknown”\nQPrompter\nQ Q & C1Prompter\nC2: “Manchester is the place of \nbirth of David Lloyd George”\nFigure 1: Our Iterative Prompting approach (on the\nright), compared with Standard Probing (on the left). In\nStandard Probing, a question is directly fed to the PLM\nto output the final answer, which could work for simple\nfactual questions but fails for complex questions that\nrequire multi-step reasoning. In contrast, we augment\nthe PLM with a Prompter, which learns to iteratively\nprompt the PLM to recall a series of knowledge and\nderive a “chain of thought”.\n(Petroni et al., 2019; Shin et al., 2020; Zhong et al.,\n2021; Roberts et al., 2020; Lee et al., 2020). How-\never, work such as (Talmor et al., 2020a; Kassner\net al., 2020; Rae et al., 2021) reveals that PLMs\nface difficulties in complex, multi-step reasoning.\nFor example, they struggle with answering complex\nquestions like Q without using external sources, no\nmatter whether they are fine-tuned based on QA\npairs or simply prompted to produce the answer\n(where even if they have memorized C1 and C2).\nIn this paper, we study the following question:\nHow to shepherd a PLM to recall a series of stored\nknowledge (e.g., C1 and C2) that is necessary for\nmulti-step inference (e.g., answeringQ), analogous\nto how humans develop a “chain of thought” for\ncomplex decision making?\nA direct way would be to fine-tune the PLM to\ngenerate the series of knowledge all at once (as-\nsuming such supervision is available), but soon\none realizes the practical issue in this approach:\nPLMs which internalize a great amount of knowl-\nedge are inevitably large in scale, and fine-tuning\n2714\nall their parameters would become more and more\ncostly as they keep scaling up. There is also the\nconcern that fine-tuning PLMs may interfere with\ntheir implicit knowledge storage, a phenomenon\nobserved in (Wang et al., 2021) which is more gen-\nerally related to the catastrophic forgetting problem\nof deep learning models (McCloskey and Cohen,\n1989; Kirkpatrick et al., 2017; Howard and Ruder,\n2018). Therefore, lightweight methods such as\nprompting (Liu et al., 2021) which keep a PLM’s\nparameters intact would be preferable for our pur-\npose of eliciting knowledge. However, we find that\nno matter whether it is fine-tuned or prompted to\ngenerate the series of knowledge all at once, the\nPLM tends to lose its “chain of thought” during\nthe process, generating irrelevant facts or suffering\nfrom hallucination.\nMotivated by the iterative nature of multi-\nstep reasoning problems, we explore an iterative\nprompting framework in this paper, which elicits\nknowledge from PLMs step by step for a given\ninference task. We have two desiderata in itera-\ntive prompting: (1) At different inference steps, the\nprompts need to focus on different components of\nthe complex query. (2) The prompts should appro-\npriately integrate knowledge gathered in previous\nsteps into the current step; for instance, during the\n2nd step in the example in Figure 1, the prompts\nneed to combine the entity “David Lloyd George”\n(from knowledge recalled in the 1st step) with the\nunresolved part “What is the place of birth of” in\nthe query.\nA natural thought is to directly apply existing\nprompting methods in an iterative fashion. Un-\nfortunately, their prompts are either restricted to\nqueries with a single, identifiable relation/predicate\n(Jiang et al., 2020; Petroni et al., 2019; Zhong et al.,\n2021; Shin et al., 2020; Qin and Eisner, 2021), or\nbeing agnostic and insensitive to step-wise inputs\n(Lester et al., 2021; Li and Liang, 2021; Brown\net al., 2020), and hence not ideal for our desiderata.\nWe design a novel iterative prompting method\ntowards that end. We augment the PLM with an\niterative Context-Aware Prompter, a model which\nlearns to dynamically synthesize prompts based on\nthe current step context. At each step, the Prompter\nlearns to process the query and previously gathered\nevidence, and composes a prompt which steers the\nPLM to recall the next piece of knowledge. Like\nother prompting methods, the PLM is kept fixed\nthroughout the learning process. In addition, as\nthe PLM size increases, the number of trainable\nparameters in our method scales comparably with\nor slower than previous prompting methods.\nWe conduct experiments on three datasets in-\nvolving multi-step reasoning, including two recent\nmulti-hop Question Answering datasets: 2Wiki-\nMultiHopQA (Ho et al., 2020) and R4C (Inoue\net al., 2020), and a scientific dataset (Talmor et al.,\n2020b) for reasoning over taxonomic relations. Our\nexperimental results show (1) effectiveness of the\niterative scheme; (2) our proposed Context-Aware\nPrompter design outperforms existing prompting\nmethods by notable margins; (3) quantitative and\nqualitative analysis which reveal the faithfulness of\nour learned prompter.\n2 Methodology\nIn this section, we first formalize our problem setup\n(§2.1), and then introduce our iterative prompting\nframework (§2.2), followed by our context-aware\nprompter design (§2.3) which addresses key limita-\ntions of previous prompting methods when applied\nin this iterative scheme.\n2.1 Problem Setup\nGiven a complex query q, our goal is to drive a\nPLM Mto recall a sequence of simple knowledge\nstatements Cq = [ c1, ..., cnq ] which is sufficient\nfor deciding the response to q. In particular, we\nfocus on developing prompting methods, where the\nparameters of Mare fixed and we aim to construct\nprompt T which steer Mto recall Cq. Note that\nhere we treat T as a variable, which may or may\nnot depend on other variables based on different\nmodelings. Writing M(T) as Maugmented with\nprompt T, our training objective is to learn how to\nfind T which could maximize the log-likelihood\nL(T) =\nN∑\ni=1\nlog P(Cqi |qi; M(T))\nwith a set of training data {qi, Cqi }N\ni=1.\nOur formulation here is general and applicable\nto all prompting-based methods, where the settings\nin previous work such as (Zhong et al., 2021; Shin\net al., 2020; Lester et al., 2021; Li and Liang, 2021;\nQin and Eisner, 2021) correspond to the reduced\ncase where |Cq|= 1 for any query q. In our ex-\nperiments, we also consider PLM fine-tuning, in\nwhich case there’s no promptT in the pipeline, and\ninstead the parameters of Mare optimized.\n2715\n2.2 Iterative Prompting Framework\nInspired by the sequential nature of multi-step infer-\nence tasks, we approach the problem in an iterative\nway:\nP(Cq|q; M(T)) =\nnq∏\nj=1\nP(cj|q, c1, ..., cj−1; M(T))\nwhere at each step j, M(T) recalls the next piece\nof knowledge cj conditioned on the query q and all\npreviously gathered knowledge c1, ..., cj−1 (con-\ncatenated with q).\n2.3 Context-Aware Prompter\nPrevious prompting methods which take single-\nrelation inputs clearly fail to apply in this iterative\nsetting due to the complexity of the input context\nq, c1, ..., cj−1. Task-level prompting methods such\nas Prompt-Tuning (Lester et al., 2021) and Prefix-\nTuning (Li and Liang, 2021) are applicable here,\nwhere T is treated as a static parameter. However,\nas described earlier, this modeling is not ideal forT\nto fully capture variabilities across different infer-\nence steps. In this work, we model T as the output\nof our Prompter, a learnable function mapping fW\nwhich dynamically synthesizes T w.r.t. the current\nstep input context:\nT = fW (q, c1, ..., cj−1), ∀j\nPrompter Instantiation. While there are many\nplausible design choices for the Prompter fW , here\nwe instantiate it with a transformer-based language\nmodel (shown in Figure 2). The prompts are de-\nsigned to be contextualizations (by the Prompter)\nof a set of special tokensw.r.t.the current step input\ncontext, linearly projected into the PLM’s embed-\nding space by a trainable matrix (omitted in the\nfigure due to space limit). In this work, we adopt\nan Encoder-Decoder PLM and use prefix-prompts\nin the implementation; hence we have prompts that\nare prepended to both the PLM’s encoder inputs\nand decoder inputs. Note that our design could\nbe easily adapted to other types of PLMs (e.g.,\nencoder-only/decoder-only models) and different\nprompt positionings (e.g., infix, postfix).\nComparison with Prompt/Prefix-Tuning. Both\nPrompt-Tuning (Lester et al., 2021) and Prefix-\nTuning (Li and Liang, 2021) model the prompt T\nas a context-agnostic parameter. In Prompt-Tuning,\nT has the same identity as in our approach which\nis a set of virtual input tokens ( Encoder Prompts\nEncoder \nSpecial Tokens\nEncoder Prompts Decoder Prompts\n…\nPLM Encoder Block (fixed) PLM Decoder Block (fixed)\nPrompter\nDecoder \nSpecial Tokens\n…\n…\nFigure 2: Our context-aware prompter design. The\nprompter contextualizes a set of special tokens w.r.t. the\ncurrent step context q, c1, ..., cj−1 to get the resulting\nprompts, which steers the PLM to recall the next piece\nof knowledge cj .\n& Decoder Prompts in Figure 2). In Prefix-Tuning,\nT is modeled to be the set of activations (keys &\nvalues in the transformer attention blocks) of the\nvirtual prompt tokens across all PLM layers. Let\nD be the embedding dimension of the PLM, h be\nthe number of layers in the PLM, d be the embed-\nding dimension of the Prompter (d ≤D), and l be\nthe length of the prompt tokens (both encoder &\ndecoder prompts). Then the number of trainable pa-\nrameters is Θ(d·(D+l)) for our proposed method,\nΘ(l ·D) for Prompt-Tuning and Θ(l ·h ·D) for\nPrefix-Tuning. It can thus be seen that our proposed\nmethod scales comparatively with Prompt-Tuning,\nslower than Prefix-Tuning, and overall maintains\na manageable amount of trained parameters as the\nPLM scales up (which increases D and h).\nContinuous v.s. Discrete Prompts. While model-\ning T as discrete tokens in the PLM’s vocabulary\ncould increase the readability of the prompts, a dis-\ncrete space is much less expressive than its contin-\nuous counterpart, and optimization over a discrete\nspace could be highly inefficient. Also, despite\nbeing inside the vocabulary, the searched discrete\nprompts could still have low interpretability as seen\nby the given examples in (Shin et al., 2020). Hence,\nwe follow prior work (Zhong et al., 2021; Li and\nLiang, 2021; Lester et al., 2021; Qin and Eisner,\n2021) and model the prompts to be continuous vir-\ntual tokens instead of discrete tokens.\n2.4 Learning and Inference\nWe use teacher-forcing for model training, namely,\nat each step, the ground truth contexts at that step\n2716\n(query and previous knowledge pieces) are pre-\nsented to the model. We maximizeL(T) using stan-\ndard sequence-to-sequence (seq2seq) objectives.\nDuring inference, we proceed autoregressively by\nfeeding the recalled knowledge at step t −1 as the\nadditional context at step t, and execute for some\npredefined number of steps. We also explore jointly\ntraining the prompter with a “stopper” which learns\nto stop the knowledge recall process when it de-\ncides that the recalled evidence is adequate enough;\ndetails are included in Appendix A.4.\n3 Experimental Setup\nOur research question is how to shepherd a PLM\nto recall a series of knowledge and derive a “chain\nof thought\" for multi-step reasoning. To this end,\nwe conduct experiments on several datasets that\nrequire complex multi-step reasoning and com-\npare different methods to guide the PLM via\nprompt/prefix tuning, fine-tuning, and our prompter\ndesign. We use both intrinsic and extrinsic metrics\nto evaluate the quality of recalled knowledge, con-\nsidering both end answer accuracy and coverage of\nintermediate evidence.\n3.1 Datasets & Preprocessing\nWe conduct experiments on three datasets involv-\ning multi-step reasoning which include annotations\nfor knowledge statements relevant to the queries:\n2WikiMultiHopQA (abbreviated as 2Wiki) (Ho\net al., 2020), R4C (Inoue et al., 2020), and a scien-\ntific commonsense reasoning dataset (abbreviated\nas LoT2) constructed by (Talmor et al., 2020b).\n2WikiMultiHopQA (Ho et al., 2020). 2Wiki is\na recent large scale multi-hop QA dataset, which\ncontains in total over 192k (167k train, 12.5k de-\nvelopment, and 12.5k test) samples constructed\njointly from Wikipedia and Wikidata. Since the\ntest set is private, we randomly split the original de-\nvelopment set into our development & test set (6k\nsamples each). The dataset format largely follows\nHotpotQA (Yang et al., 2018), but includes more\ndiverse reasoning types of questions and detailed\nannotations of evidence paths for each question.\nHere, an evidence path is an ordered list of (sub-\nject entity, relation, object entity) knowledge base\ntriplets. We use the question as the query q, and\nuse a simple template to convert each triplet in the\nevidence path into a natural language statement,\n2The abbreviation here comes from the phrase “Leap-of-\nThought” in the paper title of (Talmor et al., 2020b).\nforming Cq. Due to the large training set size and\nlimited computing budget, we randomly sample\n10% of the training data to form our final training\nset, which has the side benefit of largely reducing\nthe test/train overlap (more details in §4.2).\nR4C (Inoue et al., 2020). R4C is another re-\ncent multi-hop QA dataset containing annotated\nevidence paths. The dataset contains 4.6k exam-\nples (2.4k train, 2.2k development) constructed on\ntop of HotpotQA, where the authors used crowd-\nsourcing efforts to collect the evidence paths in\nthe form of simple subject-verb-object natural lan-\nguage sentences. Again, we randomly split the\ndevelopment set (there’s no test set given) into our\ndevelopment and test set (1.1k samples each). We\nuse the question as our query q and use the anno-\ntated evidence sentences as Cq.\nLoT (Talmor et al., 2020b). The dataset involves\nreasoning over a set of taxonomic relations, con-\nstructed from ConceptNet and WordNet. Each ex-\nample consists of a hypothesis (e.g., “A whale has\na belly button”) which we treat as queryq, and a set\nof simple facts including hypernym rules (e.g., “A\nwhale is a mammal”, “A whale is a vertebrate”) and\nproperties (e.g., “A mammal has a belly button”, “A\nvertebrate has a tail”). By reasoning over the facts\nand selecting the correct chain of hypernym rule &\nproperty (“A whale is a mammal”, “A mammal has\na belly button”), one could verify or deny the given\nhypothesis. One subtle issue of directly using the\ngold hypernym rule and property as Cq is, during\nthe first step, it would be difficult to directly iden-\ntify the correct object entity without looking ahead\non the properties in the second step. Therefore, for\nthe first step, we concatenate all the hypernymic\nobjects appearing in the dataset w.r.t. to the same\nsubject to form c1. We drop samples from the orig-\ninal training set where the relevant facts are not (or\nonly partially) provided, and obtain 9.4k/1.2k/1.2k\nsamples for training/development/testing.\nFor 2Wiki and R4C, the number of steps during\ninference is set to be 4 since over 99% of the sam-\nples have less or equal to 4 inference steps. For\nLoT, we set the number of inference steps to be 2.\nOverall, we regard 2Wiki as our “major” evaluation\ndataset due to its largest scale (despite our down-\nsampling) and diverse types of queries, and use it to\nconduct a faithfulness study of prompting in §4.2.\nSome examples of the processed data samples are\nshown in Appendix A.6.\n2717\n3.2 Compared Methods\nWe compare our proposed iterative Context-Aware\nPrompter ( iCAP) along with Prompt Tuning\n(Prompt-T), Prefix Tuning ( Prefix-T) and PLM\nfine-tuning ( PLM-FT) under both non-iterative\nand iterative setting. The iterative setting is de-\nscribed in §2.2 and for the non-iterative setting, we\nsimply concatenate all the knowledge statements in\nCq to form one single piece of knowledge for each\nquery. In extrinsic evaluation, we also compare\nwith fine-tuning the PLM on (query, answer) pairs\nwithout knowledge recall (PLM-QA), which mea-\nsures how much the PLM can solve these multi-step\ninference problems directly, a skill which PLMs\nare poor at as shown by previous work. We addi-\ntionally report final inference results when feeding\nground truth contexts to the reader (Oracle-RD) as\nan upper bound for extrinsic evaluation. Relation-\nspecific prompting methods such as (Shin et al.,\n2020; Zhong et al., 2021; Petroni et al., 2019) are\nnot included since they’re not directly applicable\nto our problem setup as discussed earlier.\nOur focus in this work is on knowledge elici-\ntation from PLMs , and hence we do not aim to\ncompare with previous dataset-specific methods\nwhich typically have different problem formula-\ntions & focus than ours and utilize other attributes\nin the datasets which we do not use (e.g., gold &\ndistractor evidence paragraphs).\n3.3 Evaluation Metric\nWe use both intrinsic and extrinsic metrics to eval-\nuate the PLM recalled knowledge.\nIntrinsic Evaluation. Here, we directly measure\nthe quality of recalled knowledge. While there are\nstandard metrics for evaluating text generation such\nas BLEU and ROUGE, these metrics generally fail\nto capture the entity-centric nature of the recalled\nknowledge we wish to examine (more details are in-\ncluded in Appendix A.3). Therefore, we propose a\nset of measures that are better suited for the tasks in\nour experiments. For 2Wiki and R4C, we evaluate\nthe ratio where the recalled knowledge contains the\nanswer entity (Ans.R); we also compute the ratio\namong only those samples where the answer entity\ndoes not appear in the query (Ans. ˆR). For 2Wiki\nand LoT, we also evaluate the evidence coverage\nof recalled contexts by computing the average ratio\nof gold evidence appearing in the recalled context\n(Evi.R) and the ratio of samples where all gold\nevidence are recalled ( Evi.R∗) as a more strict\nmeasure. For 2Wiki, we use the entities from the\nannotated KB triples as evidence. For LoT, we\nconsider the hypernym rule/property as evidence,\nwhere in the 1st step, we deem the hypernym rule\nas correct if the gold object is recalled, and use\nexact match for the recalled property in the 2nd\nstep.\nExtrinsic Evaluation. We also conduct extrin-\nsic evaluation by measuring how much the recalled\nknowledge help find the response to the query. Sim-\nilar to reading comprehension, we concatenate all\nrecalled knowledge as the contexts, and use a reader\nwhich tries to infer the answer given the query and\ncontexts. For 2Wiki and R4C, we first pre-train the\nreader using the ground truth contexts, and then\nfine-tune it on the recalled contexts3; for LoT, we\nuse a rule-based reader directly4. We report Exact\nMatch (EM) and Answer F1 scores for 2Wiki &\nR4C, and EM score for LoT where the answer is\nrestricted to yes/no.\n3.4 Implementation Details\nArchitectures & hyperparameters. We use\nBART-large (Lewis et al., 2020) for our PLM and\nRoBERTa-base (Liu et al., 2019) for our prompter,\nwhich is several times smaller than the PLM5. We\nalso include some results & discussion for different\nprompter scales in Appendix A.7. We use another\nBART-large for the reader in extrinsic evaluation6.\nOur implementation is based on Hugging Face\nTransformers (Wolf et al., 2020). We use AdamW\noptimizer (Loshchilov and Hutter, 2019) and a lin-\near learning rate scheduler with a warmup ratio of\n0.06 for optimization. For hyperparameters, we use\na batch size of 32, 128, 32 for 2Wiki, LoT and R4C\nrespectively, and tune the learning rate from {4e-5,\n8e-5, 4e-4, 8e-4, 4e-3, 8e-3, 4e-2} & length of en-\ncoder/decoder prompts7 from {15, 30, 45, 60, 80,\n100}; more details are included in Appendix A.1.\nWe run most experiments with three random seeds\nand report the mean scores.\n3We found in our preliminary experiments that this ap-\nproach gives the best results across different methods.\n4LoT is constructed using templates, and therefore a rule-\nbased reader can perfectly solve the inference task (100%\naccuracy when ground truth contexts are given, see Table 2).\n5While our prompter is also initialized using a Pre-trained\nLanguage Model, we’ll use the term “PLM” to refer only to\nthe larger & more knowledgeable one.\n6For the reader, we intentionally choose the same architec-\nture with the PLM for a fair comparison with PLM-QA.\n7We set the length of encoder & decoder prompts to be\nthe same, as we do not observe improvements otherwise in\npreliminary experiments.\n2718\n2Wiki LoT R4C\nEvi.R∗ Evi.R Ans.ˆR Ans.R Evi.R∗ Evi.R Ans.ˆR Ans.R\nPLM-FT 10.3 33.8 12.3 45.3 41.8 70.8 38.1 43.9\nPLM-FT (Iter) 26.3 48.9 35.4 60.6 41.3 70.1 43.1 48.5\nPrompt-T 5.5 22.3 6.6 41.3 35.3 62.8 28.2 33.4\nPrompt-T (Iter) 10.8 27.5 16.7 46.2 33.3 63.4 30.6 36.0\nPrefix-T 6.7 25.9 7.6 44.2 31.8 64.0 27.2 33.9\nPrefix-T (Iter) 14.8 33.9 22.5 53.2 31.6 64.9 33.7 39.8\niCAP 22.0 42.1 28.6 54.6 34.1 65.0 36.8 41.5\nTable 1: Results for Intrinsic Evaluation, where “(Iter)” indicates the iterative setting. All metrics are defined in §3.3\nand overall measure the gold (answer) entity/object coverage of the recalled knowledge from different perspectives.\n2Wiki LoT R4C\nEM F1 EM EM F1\nOracle-RD 97.8 98.9 100.0 75.7 86.8\nPLM-QA 24.1 29.3 68.3 22.6 28.8\nPLM-FT 33.6 37.8 76.0 25.3 36.8\nPLM-FT (Iter) 45.5 50.9 77.8 32.2 42.5\nPrompt-T 26.9 31.0 65.9 16.6 25.9\nPrompt-T (Iter) 25.0 30.2 68.8 22.4 30.4\nPrefix-T 31.6 35.6 69.0 19.2 29.2\nPrefix-T (Iter) 31.1 36.4 72.6 24.0 34.2\niCAP 42.8 47.9 73.8 25.7 35.2\nTable 2: Results for Extrinsic Evaluation, where the\nrecalled knowledge of each method is used for final\ninference, except for Oracle-RD and PLM-QA.\nKnowledge Enhancement for PLM. Since our fo-\ncus is on how to make PLMs better at recalling rel-\nevant knowledge for multi-step inference, we need\nto make sure the PLM actually memorizes all the\nrelevant knowledge in the first place, so that the re-\nsults can be attributed solely to the effectiveness of\nknowledge recall. Hence, we conduct knowledge\nenhancement for the PLM, where we additionally\npre-train the PLM to recover separately masked\nelements in the triplets which form the knowledge\nstatements, a strategy similar to salient span mask-\ning (Roberts et al., 2020; Guu et al., 2020). More\ndetails could be found in Appendix A.2. Note the\nsame PLM after knowledge enhancement is used\nacross different compared methods.\n4 Results\n4.1 Effectiveness of iCAP\nThe results for intrinsic & extrinsic evaluation are\nsummarized in Table 1 and 2 respectively, which\nare highly consistent. We elaborate on the results\nin what follows.\nEffectiveness of Iterative Scheme & Context-\nAware Prompter. Across different datasets, it\ncan be seen that most compared methods bene-\nfit from the iterative setting (Iter) over the non-\niterative setting. Moreover, our proposed itera-\ntive Context-Aware Prompter (iCAP) further out-\nperforms Prompt/Prefix Tuning by notable gains\nacross different datasets and metrics, approaching\nthe performance of PLM fine-tuning (PLM-FT);\nin particular, on the 2Wiki dataset which has the\nlargest scale and diversity of reasoning types,iCAP\nachieves more than 15% and 10% absolute gains\nin F1 over Prompt-Tuning & Prefix-Tuning respec-\ntively. Overall, the results clearly show the effec-\ntiveness of the iterative scheme and our proposed\ncontext-aware prompter design. However, we note\nthat even the best results (prompting based or fine-\ntuning based) still far lag behindOracle-RD which\nuses ground truth contexts as input, which suggests\na large room for improvements with better methods\nfor knowledge elicitation from PLMs. Some failure\ncases of iCAP are included in Appendix A.6.\nHelpfulness of Knowledge Recall for Multi-step\nInference. The result obtained by fine-tuning the\nPLM on (query, answer) directly without knowl-\nedge recall (PLM-QA) is outperformed by almost\nall other compared methods, verifying the previous\nfindings that PLMs face difficulties in using their\nstored knowledge to perform multi-step inference\ntasks. The large gain obtained from methods based\non knowledge recall shows the helpfulness of de-\nriving a “chain of thought” (especially iteratively)\nfrom PLMs for multi-step inference.\n4.2 Faithfulness of Prompting\n(Zhong et al., 2021) raised and studied some im-\n2719\nRandom Model Random Embedding\nEvi.R∗ Evi.R Ans. ˆR Ans.R Evi.R∗ Evi.R Ans. ˆR Ans.R\nPLM-FT 1.77 5.20 3.76 37.48 4.10 11.47 6.52 37.18\nPrompt-T 0.0 0.0 0.0 0.0 0.006 0.013 0.003 0.002\nPrefix-T 0.001 0.0 0.0 0.0 0.009 0.014 0.004 0.002\niCAP 0.001 0.001 0.0 0.0 1.49 2.83 0.98 0.59\nTable 3: Intrinsic Evaluation Results on Random Control Experiments. Here we only focus on the iterative setting\nusing the 2Wiki dataset.\nportant questions in optimization-based prompting\nmethods: Are the prompts “really” doing prompt-\ning? Is it possible that they capture dataset reg-\nularities too? The issue is related to the notion\nof test-train overlap (Lewis et al., 2021), where\nthe dataset may contain some underlying spurious\npatterns that the model exploits, and thus standard\nevaluations could not truthfully measure their gen-\neralization behaviors. Here, we take this concern\nseriously and conduct a series of analysis to faith-\nfully interpret the results we obtained. We focus on\n2Wiki under iterative setting for our analysis.\nTest-Train Overlap. For each development & test\nsample, we compute the ratio of knowledge state-\nments in Cq that also appear in the training set,\nmeaning that during certain steps for some train-\ning samples, the model has “seen” the exact same\npiece of knowledge. Note that this is a rather strict\nmeasure: even if all the knowledge pieces in Cq\nare seen during training, they may come from com-\npletely different samples & steps and hence orga-\nnized in different ways. We summarize the over-\nlapping ratios of development & test set samples\nin Table 7 in Appendix A.5. It can be seen that\nthe down-sampling has the side benefit of greatly\nreducing the test-train overlap; in particular, the\npercentage of examples where all knowledge state-\nments are seen during training is reduced from\nalmost 30% to less than 2%, and more importantly,\nover 70% of the samples have no overlap. This sug-\ngests a rather low risk for the existence of strong\nspurious regularities in our setup.\nRandom Control Experiments. Examining the\ndata-level statistics is helpful, but still not sufficient\nin terms of revealing the spurious regularities that\ndifferent methods may capture. Hence, we follow\n(Zhong et al., 2021) to conduct two random control\nexperiments. In the Random Model experiment,\nwe re-initialize all parameters of the PLM to clean\nout its internal knowledge, and proceed with the\nsame training procedure as earlier. In this way, any\npositive signal obtained could only be attributed to\ndataset regularities captured by the method. In the\nRandom Embedding experiment, we re-initialize\nonly the input embeddings of the PLM, a setting\nanalogous to the control task introduced in (He-\nwitt and Liang, 2019) (more discussions can be\nfound in (Zhong et al., 2021)). Here we only pro-\nceed with the iterative setting and conduct intrinsic\nevaluation, where the results are summarized in\nTable 3. It can be seen that 1) PLM fine-tuning cap-\ntures significantly more regularities in the dataset\nthan prompting-based methods; 2) While our pro-\nposed method captures a bit more regularities than\nPrompt/Prefix Tuning, they still remain at a very\nsmall level. Overall, our random control experi-\nments show that the exploitation of spurious dataset\npatterns by the evaluated prompting methods is\nrather mild, and that by PLM fine-tuning could\npotentially be larger.\nPrompter Attention Visualization. To see\nwhether our proposed iCAP behaves in the way we\nexpect, one direct approach is to examine the inner\nworkings of the prompter. Towards this end, we vi-\nsualize the attentions during the prompter forward\npass at different steps. We randomly choose exam-\nples in the development/test set, and use BertViz\n(Vig, 2019) to visualize the attentions within the for-\nward pass of the prompter after the following pro-\ncessing steps: 1) we aggregate the attention weights\nof different attention heads within the same trans-\nformer layer; 2) to better view the prompt tokens as\none single unit, we average the attentions across dif-\nferent prompt tokens to form one “master” prompt\ntoken; 3) we drop all special tokens (BOS, EOS) for\ncleaner visualization. One example (the same ex-\nample which we use in Figure 1) is in Figure 3, and\nwe include more examples in Appendix A.8. As\nbriefly illustrated earlier in §1, during the 1st step\ntowards solving this query, the prompter should\n2720\nFigure 3: Prompter Attention Visualization. Attentions\nduring the forward pass for the 1st & 2nd step are shown\non the left & right respectively. Different colors corre-\nspond to different transformer layers. More examples of\ndifferent reasoning types are included in Appendix A.8.\nfocus on the part concerning “father” of “Gwilym\nLloyd George”; during the 2nd step, the prompter\nshould integrate the answer “David Lloyd George”\nfrom the 1st step evidence and the “place of birth”\npart in the query to synthesize the prompt. We can\nsee that the attention distributions at different steps\naccord well with our expectations. However, we\nnote that attention visualization is only a qualitative\napproach; more systematic ways for examining the\ninner working behaviors of transformers remain an\nopen challenge.\n5 Related Work\nMemorization and Reasoning in PLMs. With the\nrecent success of large-scale pre-trained language\nmodels (PLMs), there has been growing interest in\ninvestigating what is captured by these PLMs dur-\ning pre-training (Talmor et al., 2020a; Rogers et al.,\n2020; Kassner et al., 2020). Studies have shown\nthat in addition to learning linguistic knowledge\nabout language use, PLMs are capable of memo-\nrizing a great amount of world knowledge (Rogers\net al., 2020), yielding competitive performance on\nknowledge probing (Petroni et al., 2019; Shin et al.,\n2020; Zhong et al., 2021) and other knowledge-\nintensive tasks such as question answering (Roberts\net al., 2020) and fact checking (Lee et al., 2020),\nwithout resorting to any external knowledge source.\nOn the other hand, other work such as (Talmor\net al., 2020a; Kassner et al., 2020; Rae et al., 2021)\nreveals that PLMs face difficulties in recalling their\nstored knowledge for multi-step inferences (such as\nanswering complex, multi-hop questions), which\nis also verified in our experiments.\nPrompt Learning. One type of method for elicit-\ning knowledge from PLMs is prompting (Liu et al.,\n2021), which is gaining increasing research inter-\nests & potential recently. Prompting methods seek\nto re-frame queries into prompts which accord with\nthe PLM’s input format, and extract useful infor-\nmation from the predicted results. The benefit of\nnot needing to tune PLMs makes prompting es-\npecially appealing as PLMs scale up in size. In\nthis work, we are interested in developing prompt-\ning methods which could enable PLMs to recall a\nseries of relevant knowledge for multi-step infer-\nence. Previous work along this direction mainly\nuse manually designed prompts/templates suited\nfor specific datasets (Paranjape et al., 2021; Mishra\net al., 2021; Shwartz et al., 2020); instead, we seek\nto develop a general method which can learn to con-\nstruct appropriate prompts automatically. Concur-\nrent to our work, Chain-of-Thought (CoT) prompt-\ning (Wei et al., 2022b) shares similar high-level\nideas as ours, where the authors propose to provide\nintermediate reasoning steps in the prompts to en-\ncourage the PLM to perform step-by-step inference.\nWhile CoT shows great successes, we note it is one\nof the emergent abilities of large language mod-\nels (Wei et al., 2022a) and only works well with\nextremely large PLMs (>100B typically) such as\nGPT-3 (Brown et al., 2020) and PaLM (Chowdhery\net al., 2022). In our work, we use PLMs that are\nseveral orders of magnitude smaller than those used\nin CoT and demand much less computing resources.\nWe hope our efforts could contribute towards de-\nveloping LM-based systems with better multi-step\nreasoning abilities but also moderate scale.\nFor existing work on learning-based prompting,\n(Shin et al., 2020) proposes to use gradient-guided\nsearch to find appropriate discrete prompt tokens\nin the PLM’s vocabulary to form prompt templates.\nWhile the resulting prompts are readable, most of\nthem have very low fluency and interpretability.\n(Zhong et al., 2021; Qin and Eisner, 2021) pro-\npose to optimize the prompts in continuous space\ninstead, which shows large benefits in both ef-\nfectiveness and optimization efficiency. (Zhong\net al., 2021) also raises and studies the question\nof whether learning-based prompting could exploit\n2721\nspurious dataset regularities which would weaken\nthe validity of standard evaluation results, a con-\ncern we seriously address in our work. (Lester et al.,\n2021; Li and Liang, 2021) follow the continuous\nprompting paradigm, and tune task-level prompts\nfor lightweight adaptation of PLMs. Overall, exist-\ning prompt learning methods are either restricted\nto cases where there exists a single & identifiable\nrelation/predicate within the query (Zhong et al.,\n2021; Qin and Eisner, 2021; Shin et al., 2020), or\nbeing static and not sensitive to sample-wise inputs\n(Lester et al., 2021; Li and Liang, 2021).\nIterative Knowledge Retrieval. We are also in-\nspired by methods that iteratively retrieve knowl-\nedge from explicit knowledge sources for multi-\nstep reasoning, such as (Xiong et al., 2021; Qi\net al., 2019; Khattab et al., 2021; Mo et al., 2022b).\nOur problem setting could be viewed as iterative\nretrieval over implicit knowledge in PLMs, instead\nof from explicit knowledge sources.\n6 Conclusion & Future Work\nWe explore an iterative prompting framework to-\nwards driving a “chain of thought” from PLMs for\nmulti-step reasoning tasks. We show the superi-\nority of this iterative scheme, and also the effec-\ntiveness of our proposed context-aware prompter\ndesign, which addresses key limitations of previ-\nous prompting methods when applied in this new\nscheme. In addition, we conduct both quantitative\n& qualitative analysis on the faithfulness of the\nlearned prompting behaviors. In the future, we aim\nto further extend and apply our ideas to language\nmodel pretraining, with the hope that PLMs can\nbe inherently equipped with stronger multi-step\nreasoning capabilities. The iterative framework\nwe explore here also opens the possibility of hu-\nman intervention and interaction during inference;\nnamely, a human can track along the PLM’s chain\nof thought and make edits and corrections at differ-\nent steps, similarly as in (Mo et al., 2022a), which\nimproves the transparency and trustworthiness of\ninference and also helps reduce error propagation\nalong the reasoning process. We leave these inves-\ntigations as future work.\nLimitations\nExperiments with larger-scale models. We ex-\nplored a novel framework to prompt (or elicit\nknowledge from) PLMs for multi-step inference.\nAlthough our iterative prompting approach outper-\nforms the baselines by a large margin, there is still\nmuch room to improve. One promising direction\nis to experiment with PLMs larger than what is\nused in our experiments (i.e., BART-large), which\nhave better capacities for internalizing knowledge.\nHowever, when the models get larger, the associ-\nated computational cost will increase accordingly,\nwhich was also the main obstacle for us to pursue\nthis front. We intend to conduct such experiments\nin the future when we have access to better com-\nputing resources.\nDatasets with noisy knowledge statements. We\nused three recently released datasets (2Wiki, R4C,\nLoT) that require multi-step inference for our exper-\niments. Compared with alternative datasets such as\nHotpotQA and StrategyQA (Geva et al., 2021), they\ninclude knowledge statements that have cleaner for-\nmats and are much more suitable for multi-step\ninference (in fact, this is one of the main motiva-\ntions behind the construction of 2Wiki & R4C).\nFor HotpotQA & StrategyQA, the knowledge state-\nments are given as raw sentences from the evidence\nparagraphs and include information irrelevant to\nthe original question. We exercised our best ef-\nfort to process them (e.g., resolving coreferences,\nsimplifying & decomposing nested sentences, etc.)\ninto our desired formats, but the resulting knowl-\nedge statements are still very noisy. All methods\nincluding ours cannot be trained well under such\nknowledge statements. How to use such naturally\noccurring but noisy knowledge statements as super-\nvision to guide PLMs to develop a chain of thought\nis an interesting topic to study in the future.\nExploring alternative architectural designs. An-\nother limitation is that we only implemented an\nintuitive and simple instantiation (Figure 2) of our\nproposed context-aware prompter to illustrate its\npromises. It is an interesting future direction to\nfurther explore various design choices for iterative\nprompting, e.g., alternative design for the Prompter-\nPLM interface, dynamic prompt length across dif-\nferent inference steps, etc.\nAcknowledgements\nThe authors would like to thank colleagues from\nthe OSU NLP group for their thoughtful comments.\nThis research was supported in part by Google Fac-\nulty Award, Google Research Scholar Award, NSF\nIIS 1815674, NSF CAREER 1942980, and Ohio\nSupercomputer Center (Center, 1987).\n2722\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554–3565, Online. As-\nsociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901.\nOhio Supercomputer Center. 1987. Ohio supercomputer\ncenter.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nJohn Hewitt and Percy Liang. 2019. Designing and in-\nterpreting probes with control tasks. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), Hong Kong, China. Association\nfor Computational Linguistics.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609–6625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nNaoya Inoue, Pontus Stenetorp, and Kentaro Inui. 2020.\nR4C: A benchmark for evaluating RC systems to get\nthe right answer for the right reason. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6740–6750, Online.\nAssociation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner, Benno Krojer, and Hinrich Schütze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 552–564, Online. Association for Computa-\ntional Linguistics.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Baleen: Robust multi-hop reasoning at scale\nvia condensed retrieval. Advances in Neural Infor-\nmation Processing Systems, 34.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nNayeon Lee, Belinda Z. Li, Sinong Wang, Wen-tau\nYih, Hao Ma, and Madian Khabsa. 2020. Language\nmodels as fact checkers? In Proceedings of the\nThird Workshop on Fact Extraction and VERifica-\ntion (FEVER), pages 36–41, Online. Association for\nComputational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\n2723\nPatrick Lewis, Pontus Stenetorp, and Sebastian Riedel.\n2021. Question and answer test-train overlap in open-\ndomain question answering datasets. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 1000–1008, Online. Association for\nComputational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Gordon H. Bower,\neditor, Psychology of Learning and Motivation, vol-\nume 24, pages 109–165. Academic Press.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\ninstructional prompts to gptk’s language. arXiv\npreprint arXiv:2109.07830.\nLingbo Mo, Ashley Lewis, Huan Sun, and Michael\nWhite. 2022a. Towards transparent interactive seman-\ntic parsing via step-by-step correction. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 322–342.\nLingbo Mo, Zhen Wang, Jie Zhao, and Huan Sun. 2022b.\nKnowledge transfer between structured and unstruc-\ntured sources for complex question answering. In\nProceedings of the Workshop on Structured and Un-\nstructured Knowledge Integration (SUKI), pages 55–\n66.\nBhargavi Paranjape, Julian Michael, Marjan\nGhazvininejad, Hannaneh Hajishirzi, and Luke\nZettlemoyer. 2021. Prompting contrastive explana-\ntions for commonsense reasoning tasks. In Findings\nof the Association for Computational Linguistics:\nACL-IJCNLP 2021 , pages 4179–4192, Online.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nPeng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and\nChristopher D. Manning. 2019. Answering complex\nopen-domain questions through iterative query gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2590–2602, Hong Kong, China. Association for Com-\nputational Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615–4629, Online. Association for Computa-\ntional Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020a. olmpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\n2724\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020b. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n20227–20237.\nJesse Vig. 2019. A multiscale visualization of attention\nin the transformer model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 37–42,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3241–3251, Online.\nAssociation for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nWenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick\nLewis, William Yang Wang, Yashar Mehdad, Scott\nYih, Sebastian Riedel, Douwe Kiela, and Barlas\nOguz. 2021. Answering complex open-domain ques-\ntions with multi-hop dense retrieval. In International\nConference on Learning Representations.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\n2725\n2Wiki LoT R4C\nlr pt_len lr pt_len lr pt_len\nPrompt-T 8e-3 80 4e-3 80 4e-3 60\nPrefix-T 8e-4 80 4e-4 60 4e-4 80\nPLM-FT 4e-5 - 4e-5 - 4e-5 -\nPLM-QA 4e-5 - 8e-5 - 4e-5 -\niCAP 8e-5 30 8e-5 60 8e-5 30\nTable 4: Hyperparameter settings for all compared meth-\nods. lr: learning rate, pt_len: prompt length.\nRouge (R) BLEU\nR-1 R-2 R-L\nPLM-FT 74.3 62.4 72.7 52.9\nPLM-FT (Iter) 83.6 76.3 82.3 70.8\nPrompt-T 68.7 55.5 66.4 45.4\nPrompt-T (Iter) 74.5 64.7 73.7 56.7\nPrefix-T 70.8 57.8 68.9 48.7\nPrefix-T (Iter) 79.0 70.3 77.6 64.0\niCAP 79.2 70.5 78.3 64.9\nTable 5: Intrinsic evaluation on 2Wiki using standard\ntext generation metrics (ROUGE & BLEU).\nA Appendix\nA.1 Additional Details on Experiments\nHyperparameters. We set the batch size to be 32,\n128, 32 and train for 70, 50, 40 epochs for 2Wiki,\nLoT & R4C respectively. Table 4 summarizes other\nhyperparameters used in our experiments.\nA.2 More details on PLM Knowledge\nEnhancement\nTo make sure the PLM knows all the relevant\nknowledge for subsequent recall, we further pre-\ntrain the PLM to recover separately masked ele-\nments in the triplets which form the knowledge\nstatements. For 2Wiki and LoT, we also addition-\nally include knowledge statements that are not used\nin the dataset to make the setting more challenging;\none can think of these extra knowledge statements\nas “distractors”. For 2Wiki, we filter from the pro-\ncessed Wikidata triples provided by (Agarwal et al.,\n2021) by keeping those with subject entities appear-\ning in the original knowledge statements, and in the\nend, we obtain 383k extra knowledge statements\nv.s. 240k original ones (note that while we down-\nsample the training set during our main experiment,\nthe knowledge enhancement step is performed on\nthe full dataset). For LoT, we directly use the pro-\nvided distractor knowledge in the original dataset.\nWe don’t add distractors for R4C because the pro-\nEvi.R∗ Evi.R Ans.ˆR Ans.R\niCAP 20.0 39.1 26.5 54.0\niCAP (with stopper) 18.4 37.5 22.9 51.8\nTable 6: Intrinsic evaluation results from jointly train-\ning the Prompter and the Stopper which learns to stop\nthe knowledge recall process when it decides that the\nrecalled knowledge is adequate enough for answering\nthe query.\nvided knowledge statements are in natural language\nand it’s hard to retrieve high quality knowledge\nstatements as such. We verified that the PLM after\nknowledge enhancement can indeed recover the\nmasked elements in the knowledge statements in\nnear-perfect accuracy.\nA.3 Standard Metrics for Intrinsic Evaluation\nThe intrinsic evaluation results obtained by using\nstandard text generation metrics (ROUGE 1/2/L &\nBLEU) for 2Wiki are shown in Table 5. Comparing\nwith results using our proposed metrics (Table 1), it\ncould be seen that while overall they show the same\ntrend, the standard evaluation results tend to group\ncloser due to their lack of focus on the important\nparts (e.g., entities) of the recalled evidence.\nA.4 Prompter with Automatic Stopping\nHere we explore augmenting an additional Stop-\nper module which could learn to decide to stop the\nknowledge recall process appropriately when the\nrecalled evidence pieces are enough to answer the\nquery. Since the representations from the Prompter\nare already rich, we design the Stopper module to\nbe a simple feed-forward DNN on top of the [CLS]\nembedding of the Prompter. The DNN has two\nhidden layers of dimensions 500 and 100 respec-\ntively, and outputs the probability of stopping the\nknowledge recall process. The loss for the Stopper\nis standard binary classification loss, which is com-\nbined with the original Prompter loss with weight\nfactor 0.1. The Prompter and Stopper are jointly\ntrained under this combined objective.\nWe experiment on 2Wiki only and run the exper-\niment once due to efficiency considerations. We\nfirst evaluate the frequency that the Stopper decides\nto stop the recall at the same number of steps as in\nthe ground truth knowledge pieces. Note that this is\nnot a perfect measure, as the actual recalled knowl-\nedge is different from the ground truth knowledge.\nThe frequency is 98.5%, which indicates that the\nstopper can learn to stop the recall process appro-\n2726\n0% 1%-20% 21%-40% 41%-60% 61%-80% 81-99% 100%\n2wiki (full) 36.0% 0.0% 0.5% 28.4% 5.2% 0.0% 29.8%\n2wiki (down-sampled) 71.4% 0.1% 8.1% 16.2% 2.6% 0.0% 1.6%\nTable 7: Test/Train simple knowledge overlap on 2Wiki. The horizontal bar represents the percentage range of\nsimple knowledge statements appearing in the training set, and the content values are the percentages of development\n& test set examples that fall into the corresponding range.\nBERT-tiny BERT-small\nEvi.R∗ Evi.R Ans. ˆR Ans.R Evi.R∗ Evi.R Ans. ˆR Ans.R\n6.0 17.7 9.0 35.3 21.4 41.2 29.1 54.2\nTable 8: 2Wiki intrinsic evaluation results with two smaller-scale prompter instantiations.\npriately. Then we use our intrinsic measures to see\nthe quality of the recalled evidence after truncation\nby the Stopper; the results are shown in Table 6.\nNote that here, the “iCAP” setting (top row) is dif-\nferent from that in Table 1 (despite having the same\nname) since the prompter is trained together with\nthe stopper for fair comparison. It can be seen from\nthe results that there’re small performance drops\nafter truncating by the Stopper, which suggests that\nthe Stopper can learn to stop the knowledge recall\nprocess rather appropriately but not perfectly.\nA.5 Test-Train Overlap\nTable 7 shows the 2Wiki Test-Train knowledge\nstatement overlap, where 2Wiki (full) corresponds\nto the statistics using the full training set, and\n2Wiki (down-sampled) corresponds to the down-\nsampled training set that we used in our actual ex-\nperiment. The inference steps in 2Wiki are mostly\n2 or 4, so overall there’re higher chances for the\ncoverage ratio to be 50%.\nA.6 Examples of processed data samples &\nfailure cases of iCAP\nTable 9 shows examples of our processed data sam-\nples for each dataset and each sub-category, along\nwith some failure cases of our proposed method.\nA.7 Variants of Prompter Scales\nWhile we used RoBERTa-base to instantiate the\nprompter in our main experiments, it is also inter-\nesting to see how the performance varies along dif-\nferent scales of the prompter. Towards this end, we\nconducted experiments on 2Wiki with two smaller\nscale prompters: BERT-small (28.8 million param-\neters) & BERT-tiny (4.4 million parameters). The\nintrinsic evaluation results are shown in Table 8.\nIt can be seen that the performance grows as the\nprompter scale grows; in addition, BERT-small can\nalso achieve an impressive performance (under-\nperforming RoBERTa-base used in our main ex-\nperiments by just a small gap) while BERT-tiny\nbasically fails. This suggests that the prompter\nstill needs to be larger than a certain scale for our\nmethod to work well.\nA.8 More Examples on Prompter Attention\nVisualizations\nFigure 4, 5, 6, 7 show additional example prompter\nattention visualizations in the 2Wiki dataset, each\ncorresponding to a different reasoning type as indi-\ncated in the captions.\nFigure 4: Prompter attention visualization. Reasoning\ntype: Composition.\n2727\nFigure 5: Prompter attention visualization. Reasoning\ntype: Comparison.\nFigure 6: Prompter attention visualization. Reasoning\ntype: Inference.\n2728\nq: Which film whose director is younger, Khoon Ka Khoon or Idaho Transfer?\nCq: [ Sohrab Modi is director of Khoon Ka Khoon, \nPeter Fonda is director of Idaho Transfer,\n2 November 1897 is date of birth of Sohrab Modi,\nFebruary 23, 1940 is date of birth of Peter Fonda ]\nFigure 7: Prompter attention visualization. Reasoning type: Bridge-comparison.\n2729\nQuery (2Wiki[Composition]) What is the place of birth of the performer of song La Terre Est Ronde?\nGold Knowledge Orelsan is performer of La terre est ronde; Alençon is place of birth of Orelsan\nRecalled Knowledge Basshunter is performer of La Terre est ronde; Havana is place of birth of\nBasshunter\nQuery (2Wiki[Comparison]) Who was born first out of Emma Kealy and Viktor Podloucký?\nGold Knowledge 29 May 1977 is date of birth of Emma Kealy; December 3, 1950 is date of birth\nof Viktor Podloucký\nRecalled Knowledge 30 March 1977 is date of birth of Emma Kealy; 9 October 1964 is date of birth\nof Viktor Podloucký\nQuery (2Wiki[Inference]) Who is the maternal grandfather of Vyacheslav Yaroslavich?\nGold Knowledge Ingegerd Olofsdotter of Sweden is mother of Vyacheslav Yaroslavich; Olof\nSkötkonung is father of Ingegerd Olofsdotter of Sweden\nRecalled Knowledge Yaroslavlava of Avidia is mother of Vyacheslav Yaroslavich; Sovatoslav is\nfather of Yaroslavlava of Avidia\nQuery (2Wiki[Bridge comparison])Which film has the director died later, One Day In The Life Of Andrei Ar-\nsenevich or Wolves Of The Range?\nGold Knowledge Chris Marker is director of One Day in the Life of Andrei Arsenevich; Sam\nNewfield is director of Wolves of the Range; 29 July 2012 is date of death of\nChris Marker; November 10, 1964 is date of death of Sam Newfield\nRecalled Knowledge Chris Marker is director of One Day in the Life of Andrei Arsenevich; Wallace\nFox is director of Wolves of the Range; 21 January 2013 is date of death of\nChris Marker; March 30, 1999 is date of death of Andrei Arsenevich\nQuery (LoT) A evergreen is a important food source.\nGold Knowledge A evergreen is a plant; A plant is not a important food source\nRecalled Knowledge A evergreen is a material, tree; A tree is a important food source\nQuery (R4C[Comparison]) Which documentary was filmed first, Almost Sunrise or Hail! Hail! Rock ’n’\nRoll?\nGold Knowledge Almost Sunrise was filmed in 2016; Hail! Hail! Rock ’n’ Roll was filmed in\n1986\nRecalled Knowledge Almost Sunrise (album) is credited to American singer-songwriter Taylor Swift;\nRock ’n’ Roll is filmed in the 1990s\nQuery (R4C[Bridge]) Who was the chief executive officer of the second largest US car rental company\nby sales?\nGold Knowledge The Hertz Corporation is the second-largest US car rental company; Robert L.\nStone was chief executive officer of The Hertz Corporation\nRecalled Knowledge The Hertz Corporation is the second-largest US car rental company; Enterprise\nRent-A-Car founder Jack Taylor was chief executive officer of Hertz\nTable 9: Examples of our processed data samples for each dataset and sub-category (indicated in brackets), along\nwith failure cases of our method.\n2730",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8295578360557556
    },
    {
      "name": "Inference",
      "score": 0.7045342922210693
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6245003938674927
    },
    {
      "name": "Predicate (mathematical logic)",
      "score": 0.5109555125236511
    },
    {
      "name": "Context (archaeology)",
      "score": 0.510314404964447
    },
    {
      "name": "Machine learning",
      "score": 0.4786902070045471
    },
    {
      "name": "Language model",
      "score": 0.4722197353839874
    },
    {
      "name": "Language understanding",
      "score": 0.4463747441768646
    },
    {
      "name": "Key (lock)",
      "score": 0.44110891222953796
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.43239012360572815
    },
    {
      "name": "Natural language processing",
      "score": 0.4144444763660431
    },
    {
      "name": "Programming language",
      "score": 0.10551929473876953
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    }
  ],
  "cited_by": 52
}