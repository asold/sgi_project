{
    "title": "Glaucoma Classification using Light Vision Transformer",
    "url": "https://openalex.org/W4386930973",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2582676543",
            "name": "Piyush Bhushan Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2136078981",
            "name": "Pawan Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1619579587",
            "name": "Harsh Dev",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2133534834",
            "name": "Anil Tiwari",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5114084196",
            "name": "Devanshu Batra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2159102910",
            "name": "Brijesh Kumar Chaurasia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2582676543",
            "name": "Piyush Bhushan Singh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2136078981",
            "name": "Pawan Singh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A1619579587",
            "name": "Harsh Dev",
            "affiliations": [
                "Indian Institute of Technology Kanpur",
                null
            ]
        },
        {
            "id": "https://openalex.org/A2133534834",
            "name": "Anil Tiwari",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5114084196",
            "name": "Devanshu Batra",
            "affiliations": [
                "Indian Institute of Technology Kanpur"
            ]
        },
        {
            "id": "https://openalex.org/A2159102910",
            "name": "Brijesh Kumar Chaurasia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2034742711",
        "https://openalex.org/W4308750663",
        "https://openalex.org/W2160605010",
        "https://openalex.org/W4367050303",
        "https://openalex.org/W3104201899",
        "https://openalex.org/W2980444207",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W4313289651",
        "https://openalex.org/W4296068600",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W4285334919",
        "https://openalex.org/W4323521234",
        "https://openalex.org/W4312102467",
        "https://openalex.org/W4381611346",
        "https://openalex.org/W4380084166",
        "https://openalex.org/W4372049017",
        "https://openalex.org/W4312610639"
    ],
    "abstract": "INTRODUCTION: Nowadays one of the primary causes of permanent blindness is glaucoma. Due to the trade-offs, it makes in terms of portability, size, and cost, fundus imaging is the most widely used glaucoma screening technique. OBJECTIVES:To boost accuracy,focusing on less execution time, and less resources consumption, we have proposed a vision transformer-based model with data pre-processing techniques which fix classification problems. METHODS: Convolution is a “local” technique used by CNNs that is restricted to a limited area around an image. Self-attention, used by Vision Transformers, is a “global” action since it gathers data from the whole image. This makes it possible for the ViT to successfully collect far-off semantic relevance in an image. Several optimizers, including Adamax, SGD, RMSprop, Adadelta, Adafactor, Nadam, and Adagrad, were studied in this paper. We have trained and tested the Vision Transformer model on the IEEE Fundus image dataset having 1750 Healthy and Glaucoma images. Additionally, the dataset was preprocessed using image resizing, auto-rotation, and auto-adjust contrast by adaptive equalization. RESULTS: Results also show that the Nadam Optimizer increased accuracy up to 97% in adaptive equalized preprocessing dataset followed by auto rotate and image resizing operations. CONCLUSION: The experimental findings shows that transformer based classification spurred a revolution in computer vision with reduced time in training and classification.",
    "full_text": "EAI Endorsed Transactions  \non Pervasive Health and Technology Research Article \n \n \n  1      \nGlaucoma Classification using Light Vision Transformer  \nPiyush Bhushan Singh1, *, Pawan Singh1, Harsh Dev2, Anil Tiwari3, Devanshu Batra4 and Brijesh Kumar \nChaurasia5 \n1 Department of Computer Science and Engineering, Amity School of Engineering and Technology   \n   Lucknow, Amity University Uttar Pradesh, India \n2 Department of Computer Science and Engineering, Pranveer Singh Institute of Technology, Kanpur, India \n3 Amity School of Engineering and Technology Lucknow, Amity University Uttar Pradesh, India  \n4 Department of Information Technology, Pranveer Singh Institute of Technology, Kanpur, India \n5 Department of Computer Science and Engineering, Pranveer Singh Institute of Technology, Kanpur, India \n  \nAbstract \n \nINTRODUCTION: Nowadays one of the primary causes of permanent blindness is glaucoma. Due to the trade-offs, it makes \nin terms of portability, size, and cost, fundus imaging is the most widely used glaucoma screening technique.  \nOBJECTIVES:To boost accuracy ,focusing on less execution time, and less resources consumption, we have proposed a \nvision transformer-based model with data pre-processing techniques which fix classification problems. \nMETHODS: Convolution is a “local” technique used by CNNs that is restricted to a limited area around an image. Self -\nattention, used by Vision Transformers, is a “global” action since it gathers data from the whole image. This makes it possible \nfor the ViT to successfully collect far -off semantic relevance in an image. Several optimizers, including Adamax, SGD, \nRMSprop, Adadelta, Adafactor, Nadam, and Adagrad, were studied in this paper. We have trained and tested the Vision \nTransformer model on the IEEE Fundus image dataset having 1750 Healthy and Glaucoma images. Additionally, the dataset \nwas preprocessed using image resizing, auto-rotation, and auto-adjust contrast by adaptive equalization. \nRESULTS: Results also show that the Nadam Optimizer increased accuracy up to 97% in adaptive equalized preprocessing \ndataset followed by auto rotate and image resizing operations. \nCONCLUSION: The experimental findings shows that transformer based classification spurred a revolution in computer \nvision with reduced time in training and classification. \nKeywords: Glaucoma, CNN models, Vision Transformer Model, Optimizers, Fundus imaging \nReceived on 11 July 2023, accepted on 05 September 2023, published on 21 September 2023 \n \nCopyright © 2023 Singh et al., licensed to EAI. This is an open access article distributed under the terms of the CC BY -NC-SA 4.0, \nwhich permits copying, redistributing, remixing, transformation, and building upon the material in any medium so long as the original \nwork is properly cited. \n \ndoi: 10.4108/eetpht.9.3931 \n \n*Corresponding author. Email: Piyush.bhadauria@gmai.com  \n \n1. Introduction \nGlaucoma is a condition of the eyes that damages the optic \nnerves because of elevated intraocular pressure within the \nretina [1]. Without early diagnosis, glaucoma progresses to \npermanent sightlessness of the human eye and eventually \nbecomes irredeemable [ 2]. It is projected that there will be \n111.8 million glaucoma sufferers globally by 2040 [3], which \nis a grave issue for the elderly because the condition          \nfrequently affects older persons. Therefore, priority should be \ngiven to disease early- stage detection. The obstruction of the \ndrainage canal in the eye is where the illness starts. The \nintraocular pressure (IOP) will rise as a result of this \nobstruction. IOP elevation will eventually damage optic \nnerve fibers, causing the retinal nerve fiber la yer (RNFL) to \nthicken. The diagnosis of a glaucomatous eye has been made \nrapid and simple using computer vision and deep learning \ntechniques. \nDue to its superior removal capabilities and effective \ncomputation, ML and CNN has been the best option for \nimage classification over the past ten years [4- 6], [16 -18]. \nAlexNet obtains ground -breaking performance on the \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nSingh et. al. \n 2 \nImageNet image categorization at the beginning of deep \nlearning [7, 8]. In this study, the most recent developments in \nVision models using transformer architecture for glaucoma \nclassification utilizing the whole fundus image dataset are \nexamined. \nA novel class of neural network is called Transformer based \nmodel. It mostly makes use of the self -attention mechanism \nto extract intrinsic characteristics [9], and it has a lot of \npromise for being widely applied in AI applications. The \nVision Transformer (ViT) attracted a lot of scientific interest \nin 2020. Before outperforming SOTA CNNs in more difficult \ntasks like object identification and segmentation, it showed \npromising results by outperforming them in numerous image \nrecognition tests. \nTransformers are particularly effective structures for data that \ncan be modeled as a series (for instance, a sentence is a \nsequence of words), and they were originally developed for \nNatural Language Processing jobs. It addresses numerous \nproblems that recurrent neural networks and other sequential \nmodels confront. Stacks of transformer bricks make up \ntransformers. These blocks are multilayer networks made up \nof basic linear layers, feed -forward networks, and self-\nattention layers as illustrated in Fig. 2. \nTo summarize our contributions in this paper \n• T\no the best of our knowledge, very few works pertaining\nto the glaucoma classification problem utilizing vision\ntransformers on fundus images are available. We are\nproposing Transformer learning for the classification of\nGlaucoma diseases.\n• We have also analyzed our work on adaptive and\nwithout adaptive equalization preprocessing data set.\n• We have trained and tested datasets using various\noptimizers.\n• The empirical results can achieve an accuracy of up to\n97%.\nThe remaining sections of the paper are arranged as follows. \nWe’ll introduce deep learning methodologies and concepts \nalong with a few transformer learning usages in Section 2. \nThe transformers we employed in our studies are presented in \nSection 3. The experiments we carried out and the outcomes \nare displayed in Section 4. The conclusion and future scope \nare followed by section 5.  \n2. Related Work\nNowadays, image categorization has found a lot of success in \nboth the academic and industrial worlds. A deep learning \nmodel is also used for feature classification and selection. In \n[10]\nt\nhe authors provide a technique for optimizing a vision\ntransformer (ViT) model using a small lung X- ray dataset.\nThey first fine-tune a ViT pre-training model on famous lung\ndatasets as opposed to directly fine -tuning a ViT pre-trained\nmodel. Later, the small dataset is used to retrain the trained\nmodel. The precision & accuracy of the R50-B/16 ViT model\nis 87.57% and 88.02%, respectively. Resnet50, on the other \nhand, boasts accuracy & precision of 86.57% and 86.51%, \nrespectively. In [11] the lungs image classification task, \nauthors compare the CNN -based and the transformer -\nbaseline model. The outcomes dem onstrate that, in terms of \naccuracy, the suggested strategy was competitive with \nfamous CNN algorithms. In terms of model interpretation, the \nsuggested strategy outperforms CNN methods as well. The \nbackbone network, high/ m id-level vision, low -level vision, \nand video processing are the primary areas they examine [12]. \nImplementing Transformers in actual device -based \napplications, additionally, give effective Transformers \napproaches. As the foundation of the transformers, the  \nauthors also take a quick look at the self -attention process in \ncomputer vision. There are various ways to further enhance \nvisual transformers, in addition to the methods already \ndescribed, such as positional encoding, normalization \ntechnique, shortcut connection, and attention removal. They \nobserve that when CNN and transformer are combined, the \nperformance is greater, demonstrating their complementarity \nthrough both local and global connections. The goal of fine -\ngrained visual categorization (FGVC) is to  identify an \nobject’s subclass is discussed in [13]. The FGVC task is \nextended to use the vision transformer, resulting in advanced \nperformance. These transformer- based line structures, \nhowever, ultimately cause harm to the discriminative zones \nand have a large computational cost. To address these \nproblems, the pooling layer was added to a transformer \nencoder to build a multi -stage hierarchical structure in the \npooling layer -based vision transformer architecture with \noverlapping patches (OP -ViT). For fine -grained visual \ncategorization, OP -ViT, a multi -phase vision transformer \nmodel is suggested. To better maintain local region \ninformation, a sliding window with a reduced step size is \nemployed. The transformer is a multi -phase hierarchical \nstructure that is f requently used in CNN due to the pooling \nlayer’s architecture demonstrated [13]. This makes it possible \nfor their model to cope with the higher computing demand \nbrought on by overlap patches. In this study, the author \nutilizes a light weighted module that integrates the \ninformation joint of window self-attention with the inductive \nbias of the Convolution Neural Network model (CNN) to \nproduce a backbone architecture known as the Convolution \nLite Transformer model (CLT) based on the ViT model. \nAdditionally, e xperimental findings demonstrate that CLT \noutperforms the conventional CNN model in accuracy on \nImageNet, achieving 79.21% accuracy with fewer \nparameters. Light weight deployment than the ViT model and \nmore efficient than the conventional CNN model is presented \nin [14]. This work introduces a light fusion model for image \nclassification called CLT that is built upon the junction of \nCNNs and Transformers. It makes use of ViT’s information \nadvantage over the world and CNN’s effectiveness in local \nprocessing. Inductive bias and self-attention are combined to \nhelp CLT strike a balance between accuracy and parameter. \nTo develop efficient glaucoma classifiers on the fundus \nimage datasets REFUGE, RIM-ONE DL, and DRISHTI-GS, \nthe most recent Transformer architecture-based Models used \nin deep learning are fine -tuned and utilizing the appropriate \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nGlaucoma Classification using Light Vision Transformer \n3 \nselection of hyperparameters. According to the results, Vision \nSwin Transformer performs best on the aggregated data set \nand can be used as a classifier for predicting glaucoma from \nunidentified fundus images of the Eye. Transformer performs \nbest performance on REFUGE & RIM ONE datasets. The \ncategorization of a fundus picture [19] utilizing an ensemble \nof vision transformers is investigated. Six complete fundus \nphotos dataset that are freely available were combined into \none sizable dataset for the purpose of glaucoma identification. \nThey presented a thorough assessment of more than seven \ndistinct basic Vision Transformers models. The need for a lot \nof data to effectively train vision transformer models \npresented the first difficulty. In the second, there wer e more \nsamples from patients without glaucoma than from those who \nhad the disease as the imbalanced dataset. Swin Transformer \nexhibits excellent performance both in solo models and in the \ntop-performing ensembles, and it provides excellent snapshot \nensemble results.  \nIn existing approaches [15- 18], the impact of data pre -\nprocessing on learning and result accuracy is an issue. \nTherefore, accurate glaucoma disease detection and data pre-\nprocessing is essential. We have addressed all issues using \ntransformer models along with data preprocessing to increase \naccuracy. Additionally, our suggested model can lower \ncomputation costs in terms of resources and time which can \nbe used in mobile devices as well. \n3. Proposed Methodology\nThe ViT architecture is relatively simple, and all of its\ncalculations may be summed up as follows: From the input \npicture, the first layer of ViTs extracts a predetermined \nnumber of patches. \na) B\nefore patching          b) After Patching\nFigure 1. The Patching on IEEE Dataset [20]\nA special class token vector is then added to the series of \nembedding vectors after the patches are projected to linear \nembedding. The sequence is then transferred into the \ntransformer blocks after the vectors holding positional \ninformation have been added to the embedding and the class \ntoken. The final classification is produced by an MLP head \nonce the class token vector has been taken from the output of \nthe last transformer block. \nA transformer block is made up of many layers. Layer \nNormalization is implemented at the top layer. The vital centre \nof ViTs’ multi- head attention, which is in charge of ViTs’ \nperformance, comes next. Two arrows may be seen on the \ntransformer block illustration in Fig. 2. These are the so-called \nresidual skip connections. After Layer Normalization, the \nresults of the multi- head attention are examined once again. \nFinally, an MLP that was initially developed using the GeLU \nactivation function serves as the output layer.  \nThe convolutional kernel in the case of CNNs would be an \ninductive bias. By eliminating their inductive biases, ViTs \nmoved in the opposite direction of CNNs and LSTMs to \nbecome more broad architectures. Since MLPs do not alter \ntheir weights for different inputs after training, a ViT can be \nthought of as a generalization of MLPs. The attention weights \nof ViTs, on the other hand, are computed “runtime” based on \nthe specific input. \nThe preprocessing phases of the proposed model are \ndiscussed as follows: \nIEEE Dataset and Preprocessing \n This set contains 1450 fundus images with 899 glaucoma \ndata and 551 normal data [20]. This dataset is distributed \nin train, test, and valid dataset. We have tested the model \non test dataset which have 291 images as healthy and \nGlaucoma images Fig.3. \nF\nig. 3a IEEE original  Fig. 3b IEEE original \n (Glaucoma)  (Healthy) \nT\nhe steps of preprocessing are as follows \n1. I\nmage resizing: The image will be stretched to fit your\nscreen while being resized to 224 × 224.\n2. Auto Rotate: This step enables a device’s screen\norientation to adjust automatically. We utilize the Portrait\nMode, which automatically converts any photographs that \nare not in the correct rotation to portrait mode after\nrotating them in accordance with the auto-rotation chosen\nhere. To display the pixels and rotate them by 90 or 180\ndegrees while presenting the image, it simply flips one bit\nto the viewer.\n3. Auto Adjust Contrast: In this case, I'm utilizing\nadaptive equalization, which adapts the photos from the\ndata set. Sample images after preprocessing are displayed\nin Fig. 4.\nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nSingh et. al. \n \n  4      \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n    \n \nFig. 4a IEEE Adaptive     Fig. 4b IEEE Adaptive  \nEqualization dataset      Equalization (Healthy) \nimage after (Glaucoma) \n4. Results and Analysis  \nIn this section, we evaluate the glaucoma classification of \nproposed Transformer Learning with different optimizers. \nWe train and validate the model on the IEEE dataset [20]. \nFig. 5 shows the confusion matrix of five optimizers on the \nIEEE dataset without adaptive. The best performance of the \nconfusion matrix shown in Fig. 5(d) is of Nadam optimizer. \nIt is observed that the images correctly classified as \nGlaucoma are 168 and images correctly classified as \nhealthy are 90. On the other hand, the number of images \nmisclassified as Glaucoma is 11, and healthy eye images \nare 22. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n   \n \n(a) Confusion Matrix of   (b) Confusion Matrix of \n      Adam Optimizer         Adamax Optimizer \n \n \n \n(c) Confusion Matrix of   ( d) Confusion Matrix of \n      AdamW Optimizer          Nadam Optimizer \n \nEmbedded Patches  \nNormalization  \nMulti-Head    \nAttention  \nMLP  \nNormalization  \n \nPatch + Position Embedding \nClasses: \nGlaucoma \nHealthy \nMLP Head \nTransformer Encoder \n \nLinear Patches of Flattened Patches \nFigure 2. The Workflow of the Proposed Model \n \n \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nGlaucoma Classification using Light Vision Transformer \n \n \n \n5 \n \n(e) Confusion Matrix of RMSProp Optimizer  \nFigure 5. The Confusion Matrix of Optimizers on \nIEEE Dataset (Without Adaptive equalized) \n \nThe confusion matrix of five optimizers on the IEEE \ndataset with adaptive is shown in Fig. 6. The Nadam \noptimizer performs the confusion matrix in Fig. 6(d) with \nthe best results. It has been noted that 167 images were \nappropriately identified as glaucoma, while 107 images \nwere correctly identified as healthy. On the other side, there \nwere 2 images were incorrectly labeled as glaucoma and 5 \nimages of healthy eyes.\n \n \n \n(a) Confusion Matrix of     (b) Confusion Matrix of \n      Adam Optimizer         Adamax Optimizer \n \n \n \n(c) Confusion Matrix of     (d) Confusion Matrix of \n      AdamW Optimizer          Nadam Optimizer \n \n \n(e) Confusion Matrix of  RMSProp Optimizer \nFigure 6. The Confusion Matrix of Optimizers on \nIEEE Dataset (Adaptive Equalized) \nFigs. 7 and 8. Show the performance of the proposed \nTransformer model using different parameters on the IEEE \nDataset without and with Adaptive equalization \nrespectively. The results of the proposed model have \nachieved accuracy up to 97%, however, existing work is \nable to achieve only up to 93% [2]. In addition to the same \naccuracy is achieved in [10] over chest X-ray images. \n \nFigure 7. The performance of the proposed \nTransformer Learning model on various parameters \nover IEEE Dataset without and with Adaptive \nrespectively. \n0\n20\n40\n60\n80\n100\nAdamWAdamAdagradNadamAda\nFactor\nAda\nDelta\nRMS \nPROPSGDAdamax\n Accuracy of Transformer Learning Model without Adaptive IEEE Dataset\n Accuracy of Transformer Learning Model with Adaptive IEEE Dataset\nAccuracy (%)\nOptimizers  \nFigure 8. The Performance of Transformer Learning \non IEEE Dataset \n \nFigs. 9 (a) & (b) show that the training and validation \naccuracy and validation loss of the proposed NAdam \noptimizer over adaptive IEEE dataset and without adaptive. \nWe have computed our proposed study on up to 100 \nepochs. \n \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nSingh et. al. \n \n  6      \n \n \n(a) Learning curve IEEE Without Adaptive \nEqualization Adam Optimizer   \n \n \n \n(b) Learning curve IEEE With Adaptive    \n     Equalization NAdam Optimizer \n \nFigure 9. Learning Curve with the best accuracy on \nIEEE Dataset with and without Preprocessing as \nadaptive equalization \n5. Conclusion and Future Work  \nIn this work, transformer-based architectures for glaucoma \neye classification are studied. We have analyzed the several \noptimizers on the dataset, and it has been presented that all \noptimizers provide better results in the preprocessed \ndataset with adaptive equalization and others as compared \nto the original dataset. Empirical results proved that the \nvision transformer works best on the N adam optimizer, \nwhich improves the accuracy to 97.6%; however, CNN \nmodels are achieved up to 88% [7] using learning. In future \nwork, we will explore hybrid CNN with machine learning \nand ViT on large private dataset. \nReferences \n[1] Weinreb R N,  Aung T,  Medeiros FA. The pathophysiology \nand treatment of glaucoma: a review. In JAMA, 311(18), \n1901–1911) \nDOI: https://doi.org/10.1001/jama.2014.3192.  \n[2] Singh PB, Singh P, Dev H. Optimized convolutional neural \nnetwork for glaucoma detection with improved Optic -Cup \nsegmentation. Advances in Engineering Software \n175(2023), 1-13 (2022) \nDOI: https://doi.org/10.1016/j.advengsoft.2022.103328 \n[3] Tham, Y.C., Li, X., Wong, T.Y., Quigley, H. A., Aung, T., \nCheng, C.Y. Global prevalence of glaucoma and projections \nof glaucoma burden through 2040: a systematic review and \nmeta-analysis. Ophthalmology,121(11), 2081–2090 (2014) \nDOI: 10.1016/j.ophtha.2014.05.013 \n[4] Bajpai S, Sharma K, Chaurasia BK.  Intrusion Detection \nFramework in IoT Networks. Springer Nature Computer \nScience Journal, Special Issue on Machine Learning and \nSmart Systems, 4(350), 1-17 (2023)  \nDOI: https://doi.org/10.1007/s42979-023-01770-9 \n[5] Courtie, E., Veenith, T., Logan, A.: Retinal blood flow in \ncritical illness and systemic disease: A Review. Annals of \nIntensive Care 10(152), 1-18 (2020)  \nDOI: https://doi.org/10.1186/s13613-020-00768-3 \n[6] Bajpai S, Sharma K, Chaurasia BK.  Intrusion Detection \nSystem in IoT Network using ML. In NeuroQuantology \n20(13), 3597-3601 (2022) \nDOI: 10.14704/nq.2022.20.13.NQ88441\n \n[7] Qummar S, Khan FG, Shah S, Khan A,Shamshirband S, \nRehman Z U, Khan IA, Jadoon W (2019) A Deep Learning \nEnsemble Approach for Diabetic Retinopathy Detection. In \nIEEE Access, 7:150530- 150539 \n DOI: 10.1109/ACCESS.2019.2947484 \n[8] Krizhevsky A, Sutskever I, Hinton GE. ImageNet \nclassification with deep convolutional neural networks. In \nCommunications of the ACM, 60, 84–90 (2017) \nDOI:10.1145/3065386. \n[9] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, \nGomez AN,  Kaiser L, Polosukhin I. Attention is all you \nneed,  In Proc. Conf. Neural Informat. Process. Syst., 6000–\n6010 (2017) \n[10] Nguyen MH, Quang  KN. A Study of Vision Transformer for \nLung Diseases Classification. In 6th International \nConference on Green Technology and Sustainable \nDevelopment (GTSD), 116-121 (2022) \nDOI: 10.1109/GTSD54989.2022.9989100 \n[11] Okolo GI, Katsigiannis S. Ramzan, N. IEViT: An enhanced \nvision transformer architecture for chest X-ray image \nclassification, In Computer Methods and Programs in \nBiomedicine, 226 (107141), 1-11 (2022). \nDOI:10.1016/j.cmpb.2022.107141 \n[12] Han K, Wang, Y, Chen H, Chen X, Guo J, Liu Z, Tang Y, \nXiao AuC, Xu Y, Yang Z, Zhang Y, Tao D. A Survey on \nVision Transformer. IEEE Transactions on Pattern Analysis \nand Machine Intelligence, 45 (1), 87-110 (2023) \nDOI: 10.1109/TPAMI.2022.3152247 \n[13] Huang Z, Du Ji -X, Zhang H -Bo. A Multi-Stage Vision \nTransformer for Fine -grained Image Classification. In 11\nth \nInternational Conference on Information Technology in \nMedicine and Education (ITME), 191-195 (2021) \nDOI: 10.1109/ITME53901.2021.00047 \n[14] Han X, Wang K, Tu S, Zhou W. Image Classification Based \non Convolution and Lite Transformer. 4\nth Internati-onal \nConference on Applied Machine Learning (ICAML), 3-7 \n(2022)DOI: 10.1109/ICAML57167.2022.00009 \n[15] Mallick S, Paul J, Sengupta N, Sil J. Study of Different \nTransformer based Networks for Glaucoma Detection. In \nIEEE Region 10 Conference (TENCON), 1-6 (2022) \nDOI:  10.1109/TENCON55691.2022.9977730 \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9\nGlaucoma Classification using Light Vision Transformer \n \n \n \n7 \n[16] Tripathi A, Misra A, Kumar K, Chaurasia BK.Optimized \nMachine Learning for classifying colorectal tissues. Springer \nNature Computer Science Journal, Special Issue on Machine \nLearning and Smart Systems, 1-26 (2023) \nDOI : 10.1007/s42979-023-01882-2 \n[17] Chaurasia BK, Raj H, Rathour SS ,Singh PB. Transfer \nLearning driven Ensemble Model for Detection of Diabetic \nRetinopathy Disease. In Medical & Biological Engineering \n& Computing, Springer, 1-22 (2023) \nDOI : 10.1007/s11517-023-02863-6 \n[18] Tripathi A, Misra A, Kumar K, Chaurasia BK. Colon Cancer \nclassification using Machine Learning. IEEE ISCON, 1-6 \n(2023). \nDOI: 10.1109/ISCON57294.2023.10112181 \n[19] Wassel M, Hamdi AM, Adly N, Torki M. Vision \nTransformers Based Classification for Glaucomatous Eye \nCondition. In 26\nth International Conference on Pattern \nRecognition (ICPR), 5082-5088 (2022) \nDOI: 10.1109/TENCON55691.2022.9977730 \n[20] IEEE Dataset, Online Available at: https://ieee -\ndataport.org/documents/1450-fundus-images- 899-\nglaucoma-data-and-551-normal-data \nEAI Endorsed Transactions on \nPervasive Health and Technology \n2023 | Volume 9"
}