{
  "title": "Bridging the Gap for Tokenizer-Free Language Models",
  "url": "https://openalex.org/W2970748231",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4298742986",
      "name": "Choe, Dokook",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282221971",
      "name": "Al-Rfou, Rami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225839094",
      "name": "Guo, Mandy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A596017358",
      "name": "Lee Heeyoung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281801133",
      "name": "Constant, Noah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W2026487812",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2117065474",
    "https://openalex.org/W2952339051",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2254973503",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W2137735870",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2053921957",
    "https://openalex.org/W46679369"
  ],
  "abstract": "Purely character-based language models (LMs) have been lagging in quality on large scale datasets, and current state-of-the-art LMs rely on word tokenization. It has been assumed that injecting the prior knowledge of a tokenizer into the model is essential to achieving competitive results. In this paper, we show that contrary to this conventional wisdom, tokenizer-free LMs with sufficient capacity can achieve competitive performance on a large scale dataset. We train a vanilla transformer network with 40 self-attention layers on the One Billion Word (lm1b) benchmark and achieve a new state of the art for tokenizer-free LMs, pushing these models to be on par with their word-based counterparts.",
  "full_text": "Bridging the Gap for Tokenizer-Free Language Models\nDokook Choe∗ Rami Al-Rfou∗ Mandy Guo\nGoogle AI\n1600 Amphitheatre Parkway\nMountain View, California 94043\n{choed, rmyeid, xyguo, hylee, nconstant}@google.com\nHeeyoung Lee Noah Constant\nAbstract\nPurely character-based language models\n(LMs) have been lagging in quality on large\nscale datasets, and current state-of-the-art\nLMs rely on word tokenization. It has been\nassumed that injecting the prior knowledge\nof a tokenizer into the model is essential to\nachieving competitive results. In this paper,\nwe show that contrary to this conventional\nwisdom, tokenizer-free LMs with sufﬁcient\ncapacity can achieve competitive performance\non a large scale dataset. We train a vanilla\ntransformer network with 40 self-attention\nlayers on the One Billion Word ( lm1b)\nbenchmark and achieve a new state of the\nart for tokenizer-free LMs, pushing these\nmodels to be on par with their word-based\ncounterparts.\n1 Introduction\nThere has been a recent surge of improvements\nin language modeling, powered by the introduc-\ntion of the transformer architecture (Vaswani et al.,\n2017). These gains stem from the ability of\nthe transformer self-attention mechanism to bet-\nter model long context (as compared to RNN net-\nworks), spanning hundreds of characters (Al-Rfou\net al., 2019) or words (Baevski and Auli, 2018;\nRadford et al., 2019). These approaches con-\nsider language modeling as a classiﬁcation prob-\nlem with the aim of predicting the next token\ngiven a ﬁxed-size preceding context. To support\nvariable-length context, Dai et al. (2019) adds re-\ncurrence to a transformer model, improving the\nstate-of-the-art further.\nCurrent word-based language models (LMs) de-\npend on a series of preprocessing steps that in-\nclude lowercasing, tokenization, normalization,\nand out-of-vocabulary handling. This preprocess-\ning stage is language dependent and can add sig-\n∗Equal contribution.\nniﬁcant complexity to real applications. As such,\nit is appealing to shift to more general LMs that\nprocess raw text at the character level. Processing\nlanguage at the character level allows us to model\nmorphological variants of a word, assign reason-\nable likelihood to out-of-vocabulary words, and\nlearn subword-level language abstractions. This\nopen vocabulary modeling is quite important for\nlanguages with complex morphology such as Ara-\nbic, Turkish, or Finnish (Lee et al., 2016; Bo-\njanowski et al., 2017; Kim et al., 2016).\nWhile character- and word-based LMs have\nboth improved in their performance over time,\npurely character-based LMs have continued to lag\nin performance compared to models that leverage\na tokenizer. Al-Rfou et al. (2019) report inferior\nperformance from character-level modeling on a\nlarge scale word-level benchmark, lm1b (Chelba\net al., 2013). Similarly, Radford et al. (2019) ob-\nserve that a character-level LM is harder to train to\ncompetitive performance on their huge WebText\ncorpus, as compared with subword segmentation\nusing byte pair encoding (BPE) (Sennrich et al.,\n2016; Gage, 1994).\nSub-word tokenization approaches like BPE\nrepresent a middle ground for text segmentation.\nOn one hand, they can help with better modeling\nopen vocabulary. On the other hand, they still de-\npend on a tokenizer, adding complexity to the ﬁ-\nnal system. Moreover, the preprocessing stage is\nnot jointly optimized with learning the task objec-\ntive. This last point is especially relevant given\nthat LMs are increasingly used for their ability\nto produce pretrained representations that will be\nﬁne-tuned for a downstream task (Devlin et al.,\n2018; Howard and Ruder, 2018; Radford et al.,\n2018; Peters et al., 2018). Since word-based LMs\nuse closed vocabulary and sub-word models adopt\na segmentation that targets the pretraining corpus,\nthere is little space to adapt the vocabulary or op-\narXiv:1908.10322v1  [cs.CL]  27 Aug 2019\ntimize the segmentation to ﬁt the ﬁnal task data\ndistribution.\nThe rest of this paper is organized as follows.\nIn Section 2, we describe our model architecture,\nwhich is a vanilla deep transformer byte-level LM.\nSection 3 describes thelm1b dataset and our eval-\nuation methodology. Section 4 presents our re-\nsults and how our model compares to the previous\nwork. In Section 5 we analyze the representations\nlearned by the network at different depths using\nword-similarity benchmarks. For this analysis to\nbe feasible we propose a strategy to extract word\nrepresentations from a character model.\nTo summarize our contributions:\n•We develop a competitive tokenizer-free lan-\nguage model on a large scalable dataset.\n•We probe the performance of our model’s\nlearned intermediate representations on word\nsimilarity tasks.\n2 Modeling\nLanguage models (LMs) assign a probability dis-\ntribution over a sequence x0:t by factoring out the\njoint probability from left to right as follows\nP(x0:t) =P(x0)\nt∏\ni=1\nP(xi|x<i). (1)\nInstead of reading in the tokenized input text, our\nmodel reads raw utf-8 bytes. For English text\nin the ASCII range, this is equivalent to process-\ning characters as individual tokens. Non-ASCII\ncharacters (e.g. accented characters, or non-Latin\nscripts) are typically two or three utf-8 bytes. We\nuse a standard “transformer decoder” (a stack of\ntransformer layers with a causal attention mask)\nto process the sequence x0:i−1 and predict the fol-\nlowing byte xi. The model’s prediction is an esti-\nmate of the probability distribution over all possi-\nble 256 byte values. Our input byte embedding\nmatrix has dimensionality 256. Our byte-level\ntransformer model has 40 standard transformer\nlayers with hidden size 1024, ﬁlter size 8192, and\n16 heads. The model has around 836M parame-\nters, of which only 66K are byte embeddings.\n2.1 Training\nWe sample random byte sequences of length 512.\nThis sampling process does not respect the sen-\ntence boundary. Therefore, one example might\nspan complete and partial sentences. We dropout\nboth timesteps of self-attention layers and features\nof relu activations across timesteps with a proba-\nbility of 0.3. We use the Adam optimizer (Kingma\nand Ba, 2014) with initial learning rate 10−4 and\nbatch size 1024. The training runs for two mil-\nlion steps, and at every 10,000 steps we decay the\nlearning rate geometrically by 0.99.\n2.2 Windowed Prediction\nTo score each byte prediction, we need to process\nan entire 512-byte context from scratch, which is\ncomputationally intensive. To speed up develop-\nment, for each window of context size c, we score\n(stride = c/2) characters in parallel (the second\nhalf of the window). This leads to a tractable run-\nning time for our development evaluation process.\nWhile this setup is sub-optimal for our model, we\ndid not observe any signiﬁcant regression in our\nmetrics. For example, the ﬁnal bits/byte value of\n0.874055 (stride = 1) only grows to 0.87413 with\nstride = 256. Our ﬁnal test evaluation is reported\nwith stride = 1.\n3 Experimental Setup\nThere are no large scale datasets that are heav-\nily studied for both word and character language\nmodeling. Typically, a speciﬁc dataset will be con-\nsidered under just one level of segmentation. For\nour efforts to be comparable with the literature, we\nuse a word LM dataset. This puts our model at\na disadvantage; the dataset is tokenized and our\nmodel will not utilize the given word boundary\ninformation. Our approach is able to model rare\nwords and estimate their appropriate likelihoods,\nhowever, they have been replaced with a special\ntoken to produce closed vocabulary text that is\nappropriate for word-level modeling. Hence, the\nmetrics we report are meant to provide a lower\nbound on the utility of our approach in realistic\nsettings.\nshards # of words # of bytes\ntrain 01-99 798,947,561 4,132,763,660\ndev 01 of 00 165,560 856,742\ntest 00 of 00 159,658 826,189\nTable 1: Word and byte counts in lm1b. The word\ncount includes end of sentence tokens, and byte count\nincludes newline characters.\nSegmentation Context Length # of params Perplexity Bits/Byte\nShazeer et al. (2017) Word Fixed 6.0B 28.0 0.929\nShazeer et al. (2018) Word-Piece Fixed 4.9B 24.0 0.886\nBaevski and Auli (2018) Word Fixed 1.0B 23.0 0.874\nDai et al. (2019) Word Arbitrary 0.8B 21.8 0.859\nAl-Rfou et al. (2019) Byte Fixed 0.2B 40.6 1.033\nOurs Byte Fixed 0.8B 23.0 0.874\nTable 2: Comparing recent language model results on lm1b.\n3.1 LM1B\nWe use the One Billion Word benchmark (Chelba\net al., 2013) to compare LM performance. The\ndataset consists of shufﬂed short sentences, and\ndoesn’t require modeling long contexts (95% of\nthe training sentences are under 256 bytes and over\n99.78% are under 512 bytes). The corpus is tok-\nenized, and a small percentage of rare words are\nreplaced with UNK tokens. The data gets split into\n100 shards, and the ﬁrst one (00) is held out while\nthe rest (01-99) are used for training. The holdout\nset is split again into 50 shards, and historically\nshard 00 of the holdout has been used as the test\nset. There is no standard dev set, so we use shard\n01 of the holdout as dev. See the corpus statistics\nin Table 1 for details.\n3.2 Metrics\nWord LMs typically report their results in terms of\nperplexity per word (ppl) while byte LMs report\ntheir results in bits per byte (bpb). We report both\nmetrics to make our results more accessible.\nConversion between those metrics are based on\nthe following observation: The amount of infor-\nmation in the test dataset is the same independent\nof segmentation.\nI(text) =|words|× bits\nword = |bytes|× bits\nbyte,\n(2)\nwhere I(x) is the information contained in x,\nwhich is −log2 P(x; model). Equation 2 allows\nus to convert bits/word to bits/byte. Then straight-\nforwardly, using Equation 3 we can convertbpbto\nppl:\nperplexity = 2\n|bytes|\n|words|×bpb (3)\nWe train our model to minimize bpb over the\ntraining set and convert bpbon the test set to ppl\nfor comparison. For the test dataset, we use the\n|words|and |bytes|values reported in Table 1.\n4 Results and Discussion\nTable 2 shows the perplexity of several models on\nlm1b. We observe that tokenizer-free LM perfor-\nmance improves signiﬁcantly (40.6 to 23.0) when\nthe model capacity is increased from 0.2B to 0.8B\nparameters. With sufﬁcient capacity our byte-level\nLM is competitive with word based models (rang-\ning from 21.8 to 28.0). Note, our model is able to\nachieve comparable performance without any ex-\nplicit signal of word boundaries.\nBecause of the large symbol space that word-\nbased LMs address, they rely on sparse opera-\ntions running on heterogeneous devices to run ef-\nﬁciently (e.g. running sparse embedding lookups\non CPU as opposed to GPU/TPU). By contrast,\nbyte LMs are dense, and all operations can be ex-\necuted on specialized accelerators efﬁciently. We\nexpect that with advances in accelerated hardware,\nbyte-level text processing will become a popular\nchoice.\nOf all the baseline models we reference, only\nDai et al. (2019) uses recurrence to model arbitrary\nlength history. This technique could be added to\ntokenizer-free models as well. Indeed, we expect\nthis approach to be particularly well-suited to byte\nand character models where text gets mapped onto\nlonger token sequences, as Dai et al. (2019) show\nthat adding recurrence increases the length of con-\ntext their model can effectively use.\n5 Extracting Word Representations\nIn this section, we test our model’s ability to pro-\nduce meaningful word-level representations. We\ninvestigate this by feeding the model single words,\nand evaluating its intermediate activations on word\nsimilarity tasks.\nSince our model is trained to predict each in-\ndividual character, activations within a word only\nhave partial information about that word. To get\na word representation, we append an empty space\nFigure 1: Performance on word similarity tasks described in Shazeer et al. (2016): Spearman’s ρ measuring\ncorrelation between human annotation and cosine similarities on word representations generated from activations\non different transformer layers.\ncharacter at the end of the input word. The activa-\ntion at the space position from the transformer’s\nfeed-forward layer takes all characters into ac-\ncount, given the causal attention. To predict what\nfollows the space, the model must have a good un-\nderstanding of the preceding word, so this activa-\ntion can be used as a proxy for a word representa-\ntion.\nTo evaluate our extracted word representations,\nwe use the word similarity tasks described in\nSwivel (Shazeer et al., 2016). Following their\nevaluation methodology, we score word pairs us-\ning cosine similarity, and then measure the corre-\nlation with human ratings using Spearman’sρ.\nWe do not expect these results to be competi-\ntive, given that our model is never trained to repre-\nsent words. Moreover, the Swivel model is trained\non a combination of Wikipedia and the Gigaword5\ncorpus (Parker et al., 2011) which is composed of\n3.3 billion lowercased words with discarded punc-\ntuation. They discard out-of-vocabulary words\nfor evaluation, while we use all word pairs in the\nbenchmark. Nevertheless, this evaluation is valu-\nable for comparing the relative quality of repre-\nsentation across different layers.\nFigure 1 shows Spearman’s ρ across different\nlayers of the model. We observe two main phases\nof performance. In the ﬁrst phrase (layers 1-\n10), all task metrics improve with depth. In the\nsecond phase (layers 11-40), performance either\nplateaus or degrades slightly with depth. We sus-\npect that the earlier layers learn general-purpose\nfeatures which are linguistically relevant, while\nthe ﬁnal layers ﬁne-tune speciﬁcally to the task\nof next character prediction. Interestingly, the\nRare Word and SimLex999 datasets do not fol-\nlow this paradigm. Their performance drops be-\ntween layers 4-6, but picks up again and improves\nwith depth (layers 6-40). We hypothesize that the\nmodel may be storing words at different depths ac-\ncording to their frequency. It would be interesting\nto investigate to what degree the improved perfor-\nmance of deeper LMs is due to better modeling of\nrare words/phrases.\nTable 3 shows the best performance of our\nmodel across all layers compared to the state-of-\nthe-art model on word similarity. The gap here is\na reminder that work remains to be done on im-\nproving methods for extracting word representa-\ntions from character models.\nDataset Ours Swivel\nmen (Bruni et al., 2012) 0.35 0.76\nmturk (Radinsky et al., 2011) 0.32 0.72\nrarewords (Luong et al., 2013) 0.46 0.48\nsimlex999 (Hill et al., 2014) 0.31 0.40\nws353 (Finkelstein et al., 2002) 0.38 -\nws353rel (Zesch et al., 2008) 0.31 0.62\nws353sim (Agirre et al., 2009) 0.43 0.75\nTable 3: Spearman’s ρfor different datasets using our\nmodel activations. We report the best value achieved\nacross all layers versus Swivel (Shazeer et al., 2016).\n6 Conclusion\nWe show that a tokenizer-free language model\nwith sufﬁcient capacity can achieve results that\nare competitive with word-based LMs. Our model\nreads raw byte-level input without the use of any\ntext preprocessing. As such, the model has no di-\nrect access to word boundary information. Finally,\nwe show that our model’s intermediate represen-\ntations capture word-level semantic similarity and\nrelatedness across layers.\nReferences\nEneko Agirre, Enrique Alfonseca, Keith Hall, Jana\nKravalova, Marius Pas ¸ca, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distribu-\ntional and wordnet-based approaches. In Proceed-\nings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics,\nNAACL ’09, pages 19–27, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2019. Character-level lan-\nguage modeling with deeper self-attention. In\nThirty-Third AAAI Conference on Artiﬁcial Intelli-\ngence.\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nElia Bruni, Gemma Boleda, Marco Baroni, and Nam-\nKhanh Tran. 2012. Distributional semantics in tech-\nnicolor. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Long Papers - Volume 1, ACL ’12, pages 136–\n145, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv\npreprint arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\ntan Ruppin. 2002. Placing search in context: The\nconcept revisited. ACM Trans. Inf. Syst., 20(1):116–\n131.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users J., 12(2):23–38.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2014.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. CoRR, abs/1408.3456.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In Proceedings of the Thirtieth AAAI\nConference on Artiﬁcial Intelligence , AAAI’16,\npages 2741–2749. AAAI Press.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJason Lee, Kyunghyun Cho, and Thomas Hof-\nmann. 2016. Fully character-level neural machine\ntranslation without explicit segmentation. CoRR,\nabs/1610.03017.\nThang Luong, Richard Socher, and Christopher Man-\nning. 2013. Better word representations with recur-\nsive neural networks for morphology. In Proceed-\nings of the Seventeenth Conference on Computa-\ntional Natural Language Learning, pages 104–113,\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nRovert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English gigaword ﬁfth edi-\ntion. Philadelphia. Linguistic Data Consortium.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1:8.\nKira Radinsky, Eugene Agichtein, Evgeniy\nGabrilovich, and Shaul Markovitch. 2011. A\nword at a time: Computing word relatedness using\ntemporal semantic analysis. In Proceedings of the\n20th International Conference on World Wide Web,\nWWW ’11, pages 337–346, New York, NY , USA.\nACM.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin\nTran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff\nYoung, et al. 2018. Mesh-tensorﬂow: Deep learning\nfor supercomputers. In Advances in Neural Infor-\nmation Processing Systems, pages 10414–10423.\nNoam Shazeer, Ryan Doherty, Colin Evans, and Chris\nWaterson. 2016. Swivel: Improving embeddings by\nnoticing what’s missing. CoRR, abs/1602.02215.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nTorsten Zesch, Christof M ¨uller, and Iryna Gurevych.\n2008. Using wiktionary for computing semantic\nrelatedness. In Proceedings of the 23rd National\nConference on Artiﬁcial Intelligence - Volume 2 ,\nAAAI’08, pages 861–866. AAAI Press.",
  "topic": "Bridging (networking)",
  "concepts": [
    {
      "name": "Bridging (networking)",
      "score": 0.9622533321380615
    },
    {
      "name": "Computer science",
      "score": 0.44579625129699707
    },
    {
      "name": "Computer security",
      "score": 0.16896846890449524
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 14
}