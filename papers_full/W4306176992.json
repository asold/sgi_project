{
  "title": "MedJEx: A Medical Jargon Extraction Model with Wiki’s Hyperlink Span and Contextualized Masked Language Model Score",
  "url": "https://openalex.org/W4306176992",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2508661792",
      "name": "Sunjae Kwon",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A3005683294",
      "name": "Zonghai Yao",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2616132694",
      "name": "Harmon Jordan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1824406339",
      "name": "David Levy",
      "affiliations": [
        "University of Massachusetts Lowell"
      ]
    },
    {
      "id": "https://openalex.org/A2098924331",
      "name": "Brian Corner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111112795",
      "name": "Hong Yu",
      "affiliations": [
        "United States Department of Veterans Affairs",
        "University of Massachusetts Chan Medical School",
        "University of Massachusetts Amherst",
        "University of Massachusetts Lowell"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2902446432",
    "https://openalex.org/W2398669368",
    "https://openalex.org/W2887280559",
    "https://openalex.org/W3004015233",
    "https://openalex.org/W1550291595",
    "https://openalex.org/W2178441628",
    "https://openalex.org/W2592420893",
    "https://openalex.org/W2891311366",
    "https://openalex.org/W2982906145",
    "https://openalex.org/W4287746114",
    "https://openalex.org/W112847953",
    "https://openalex.org/W2776226510",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W2100627415",
    "https://openalex.org/W2335791510",
    "https://openalex.org/W2145561180",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3108819963",
    "https://openalex.org/W2004311568",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3172427031",
    "https://openalex.org/W2604748391",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3150670579",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2114668172",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W3158665049",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2908840510",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W274221391",
    "https://openalex.org/W2784948075",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2129958862",
    "https://openalex.org/W4285209661",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2994782398",
    "https://openalex.org/W2765441486",
    "https://openalex.org/W2071879021",
    "https://openalex.org/W2141086962",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2164861117",
    "https://openalex.org/W2808393080",
    "https://openalex.org/W2582146834",
    "https://openalex.org/W4221006722",
    "https://openalex.org/W1550258693",
    "https://openalex.org/W1986211833",
    "https://openalex.org/W2770304476",
    "https://openalex.org/W2064418625",
    "https://openalex.org/W1841433433",
    "https://openalex.org/W2141936101",
    "https://openalex.org/W2885588848",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W3158890082"
  ],
  "abstract": "This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms from 18K+ EHR note sentences ($MedJ$). Then, we introduce a novel medical jargon extraction ($MedJEx$) model which has been shown to outperform existing state-of-the-art NLP models. First, MedJEx improved the overall performance when it was trained on an auxiliary Wikipedia hyperlink span dataset, where hyperlink spans provide additional Wikipedia articles to explain the spans (or terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that a contextualized masked language model score was beneficial for detecting domain-specific unfamiliar jargon terms. Moreover, our results show that training on the auxiliary Wikipedia hyperlink span datasets improved six out of eight biomedical named entity recognition benchmark datasets. Both MedJ and MedJEx are publicly available.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11733–11751\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nMedJEx: A Medical Jargon Extraction Model with Wiki’s Hyperlink Span\nand Contextualized Masked Language Model Score\nSunjae Kwon1, Zonghai Yao1, Harmon S. Jordan2,\nDavid A. Levy3, Brian Corner2, Hong Yu1,3,4,5\n1UMass Amherst, 2Health Research Consultant,\n3UMass Lowell, 4UMass Medical School, 5U.S. Department of Veterans Affairs\nsunjaekwon@umass.edu, zonghaiyao@umass.edu, harmon.s.jordan@gmail.com,\ndavid_levy@uml.edu, brian.corner@umassmed.edu, hong_yu@uml.edu\nAbstract\nThis paper proposes a new natural language\nprocessing (NLP) application for identifying\nmedical jargon terms potentially difficult for\npatients to comprehend from electronic health\nrecord (EHR) notes. We first present a novel\nand publicly available dataset with expert-\nannotated medical jargon terms from 18K+\nEHR note sentences ( MedJ ). Then, we\nintroduce a novel medical jargon extraction\n(MedJEx ) model which has been shown to\noutperform existing state-of-the-art NLP mod-\nels. First, MedJEx improved the overall per-\nformance when it was trained on an auxiliary\nWikipedia hyperlink span dataset, where hyper-\nlink spans provide additional Wikipedia arti-\ncles to explain the spans (or terms), and then\nfine-tuned on the annotated MedJ data. Sec-\nondly, we found that a contextualized masked\nlanguage model score was beneficial for detect-\ning domain-specific unfamiliar jargon terms.\nMoreover, our results show that training on\nthe auxiliary Wikipedia hyperlink span datasets\nimproved six out of eight biomedical named en-\ntity recognition benchmark datasets. MedJEx\nis publicly available 1.\n1 Introduction\nAllowing patients to access their electronic health\nrecords (EHRs) represents a new and personal-\nized communication channel that has the poten-\ntial to improve patient involvement in care and\nassist communication between physicians, patients,\nand other healthcare providers (Baldry et al., 1986;\nSchillinger et al., 2009). However, studies showed\nthat patients do not understand medical jargon in\ntheir EHR notes (Chen et al., 2018).\nTo improve patients’ EHR note comprehension,\nit is important to identify medical jargon terms\nthat are difficult for patients to understand. Un-\nlike the traditional concept identification or named\n1https://github.com/MozziTasteBitter/\nMedJEx\nentity recognition (NER) tasks, where the tasks\nmainly center on semantic salient entities, detect-\ning such medical jargon terms takes into consider-\nation the perspective of user comprehension. Tra-\nditional NER approaches such as using compre-\nhensive clinical terminological resources (e.g., the\nUnified Medical Language System (UMLS) (Bo-\ndenreider, 2004)) would identify terms such as \"wa-\nter\" and \"fat\", which are not considered difficult for\npatients to comprehend. Meanwhile, using term fre-\nquency (TF) as the proxy for medical jargon term\nidentification will miss outliers such as \"shock,\"\nwhich is a term frequently used in the open do-\nmain with its common sense: \"a sudden upsetting\nor surprising event or experience.\" However, EHR\nnotes incorporate its uncommon sense: \"a medi-\ncal condition caused by severe injury, pain, loss of\nblood, or fear that slows down the flow of blood.\"\n(Shock, 2022). Thus, \"shock\" should be identified\nas a jargon term from EHR notes since it would be\ndifficult for patients to comprehend, even though\nits TF is high. In this study, we propose a natural\nlanguage processing (NLP) system that can iden-\ntify such outlier jargon from EHR notes through a\nnovel method for homonym resolution.\nWe first expert-annotated de-identified EHR note\nsentences for medical jargon terms judged to be\ndifficult to comprehend. This resulted in the Med-\nical Jargon Extraction for Improving EHR Text\nComprehension (MedJ) dataset, which comprises\n18,178 sentences and 95,393 medical jargon terms.\nWe then present a neural network-based medical\njargon extraction (MedJEx) model to identify the\njargon terms.\nTo ameliorate the limited training-size issue, we\npropose a novel transfer learning-based framework\n(Tan et al., 2018) utilizing auxiliary Wikipedia\n(Wiki) hyperlink span datasets (WikiHyperlink),\nwhere the span terms link to different Wiki articles\n(Mihalcea and Csomai, 2007). Although medical\njargon extraction and WikiHyperlink recognition\n11733\nseem to be two different applications, they share\nsimilarities. The role of hyperlinks is to help a\nreader to understand an Wiki article. Thus, \"dif-\nficult to understand\" concepts in the Wiki article\nmay be more likely to have hyperlinks. Therefore,\nwe hypothesize that large-scale hyperlink span in-\nformation from Wiki can be advantageous for our\nmodels of medical jargon extraction. Our results\nshow that models trained on WikiHyperlink span\ndatasets indeed substantially improved the perfor-\nmance of MedJEx. Moreover, we also found that\nsuch auxiliary learning improved six out of the\neight benchmark datasets of biomedical NER tasks.\nTo detect outlier homonymous terms such as\n\"shock\", we deployed an approach inspired by\nmasking probing (Petroni et al., 2019), a method\nfor evaluating linguistic knowledge of large-scale\npre-trained language models (PLMs). Meister et al.\n(2022) suggests PLMs are beneficial for predicting\nthe reading time, with longer reading time indicates\ndifficult for indicating difficulty in understanding.\nIn our work, we propose a contextualized masked\nlanguage model (MLM) score feature to tackle the\nhomonym challenge. Note that models will recog-\nnize the sense of a word or phrase using contextual\ninformation. Since PLMs calculate the probability\nof masked words in consideration of context, we\nhypothesize that PLMs trained in the open-domain\ncorpus would predict poorly masked medical jar-\ngon if senses are distributed differently between\nthe open domain and clinical domain corpora.\nWe conducted experiments on four state-of-the-\nart PLMs, namely BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), BioClinicalBERT\n(Alsentzer et al., 2019b) and BioBERT (Lee et al.,\n2020). Experimental results show that when both\nof the methods are combined, the medical jargon\nextraction performance is improved by 2.44%p in\nBERT, 2.42%p in RoBERTa, 1.56%p in BioClini-\ncalBERT, and 1.19%p in BioBERT.\nOur contributions are as follows:\n• We propose a novel NLP taskfor identifying\nmedical jargon terms potentially difficult for\npatients to comprehend from EHR notes.\n• We construct MedJ, an expert-curated 18K+\nsentence dataset for the MedJEx task.\n• We introduce MedJEx, a medical jargon ex-\ntraction model. Herein, MedJEx was first\ntrained with the auxiliary WikiHyperlink span\ndataset before being fine-tuned on the MedJ\ndataset. It uses MLM score feature for\nhomonym resolution.\n• The experimental results show that training\non the Wiki’s hyperlink span datasets consis-\ntently improved the performance of not only\nMedJ but also six out of eight BioNER bench-\nmarks. In addition, our qualitative analyses\nshow that the MLM score can complement\nthe TF score for detecting the outlier jargon\nterms.\n2 Related Work\nIn principle, MedJEx is related to text simplifica-\ntion (Kandula et al., 2010). None of the previ-\nous work (Abrahamsson et al., 2014; Qenam et al.,\n2017; Nassar et al., 2019) identified terms that im-\nportant for comprehension.\nOn the other hand, MedJEx is relevant to\nBioNER, a task for identifying biomedical named\nentities such as disease, drug, and symptom\nfrom medical text. There are several benchmark\ncorpora, including i2b2 2010 (Patrick and Li,\n2010), ShARe/CLEF 2013 (Zuccon et al., 2013),\nand MADE (Jagannatha et al., 2019), all of which\nwere developed solely based on clinical importance.\nIn contrast, MedJ is patient-centered, taking into\nconsideration of patients’ comprehension. Identi-\nfying BioNER from medical documents has been\nan active area of research. Earlier work such as\nthe MetaMap (Aronson, 2001), used linguistic pat-\nterns, either manually constructed or learned semi-\nautomatically, to map free text to external knowl-\nedge resources such as UMLS (Lindberg et al.,\n1993). The benchmark corpora have promoted su-\npervised machine learning approaches including\nconditional random fields and deep learning ap-\nproaches (Jagannatha et al., 2019).\nKey phrase extraction in the medical domain is\nanother related task. It identifies important phrases\nor clauses that represent topics (Hulth, 2003). In\nprevious studies, key phrases were extracted using\nfeatures such as TF, word stickiness, and word cen-\ntrality (Saputra et al., 2018). Chen and Yu (2017)\nproposed an unsupervised learning based method\nto elicit important medical terms from EHR notes\nusing MetaMap (Demner-Fushman et al., 2017)\nand various weighting features such as TextRank\n(Mihalcea and Tarau, 2004) and term familiarity\nscore (Zeng-Treitler et al., 2007). In another work,\nChen et al. (2017) proposed an adaptive distant su-\n11734\nUMLSQuickUMLS\nWeighted Score Feature\nBinary Feature\nWiki_trained LM\nTokenizer\nCRF Layer\nMLPMLP\nMLP Binary Feature Extraction\nBiomedical Concepts\nTerm WeightingInitialize with trained weights\nWikiHyperlink Training Auxiliary  Feature ExtractionTarget  Model\nInput\nHidden\nWeighted emissionEmissionFinal emission\nWordFreq\n… exacerbated by his shock liver …\n… ‘exacerbated by’'shock’‘liver’\nPretraiend LM…  'ex', '##ace', '##rb', '##ated’, 'by', 'his’, [MASK], 'liver\n…  'ex', '##ace', '##rb', '##ated’, 'by', 'his', 'shock', 'liver’Masking\nMLM Score: 0.6018TF Score: 0.5608MLM Score\nTF ScoreWikiHyperlink\nPretraiend LM\nWikipedia Articles\nData Processing\nSource Label\nMedJ\n1. Other common side effects include fatigue, headache,myalgia(muscle pain), andarthralgia(joint pain)2. as shown by an increase inserum creatinine3. The discomfort may occasionally feel like heartburn4.Shockis the state of insufficientblood flowto thetissues…\nExamples of WikiHyperlink\nFigure 1: This figure demonstrates the overall architecture of MedJEx. There are three components in MedJEx: 1)\nWikiHyperlink training, 2) auxiliary feature extraction and 3) target model. First, in WikiHyperlink training, we\nextract hyperlink spans from Wikipedia articles. The examples shows that hyperlink spans (blue colored) represent\nmedical jargons, and ignore easier medical terms such as \"fatigue\" and \"headache\". Then, the pretrained language\nmodel (LM) is trained with WikiHyperlink. In auxiliary feature extraction, we can see that MLM score of medical\njargon \"shock\" shows relatively high TF and MLM scores, indicating that the MLM score can help detect the\nmedical jargon. Finally, the weight parameters of Wiki-trained LM in the target model are initialized with trained\nparameters of pretrained LM of WikiHyperlink training. Then, the model is finetuned with MedJ.\npervision based medical term extraction approach\nthat utilizes consumer health vocabulary (Zeng and\nTse, 2006) and a heuristic rule to distantly label\nmedical jargon training datasets. A key phrase\nextraction method using a large-scale pretrained\nmodel is being actively studied (Soundarajan et al.,\n2021).\nUnlike the previous BioNER or key phrase iden-\ntification applications, identifying medical jargon\nterms is important for patients’ comprehension of\ntheir EHR notes and represents a novel NLP ap-\nplication. However, not all medical entities are\nunfamiliar to patients. The brute force approach\nof capturing every medical entity, the approaches\nof existing BioNER and key phrase identification\napplications, may bring about confusion to patients.\nOn the other hand, undetected medical jargon terms\nwill reduce patients’ EHR note comprehension. In\nthis paper, we propose MedJEx, a novel application\nthat identifies medical jargon terms important for\npatients’ comprehension. Once jargon terms are\nidentified, interventions such as linking the jargon\nterms to lay definitions can help improve compre-\nhension.\n3 Dataset Construction\nThis work has two different datasets: 1) MedJ for\nmedical jargon extraction and 2) Wiki’s hyperlink\nspan (WikiHyperlink) dataset for transfer learning.\n3.1 MedJ\n3.1.1 Data Collection\nThe source of the dataset is a collection of publicly\navailable deidentified EHR notes from hospitals\naffiliated with the University of Pittsburg Medical\nCenter. Herein, 18,178 sentences were randomly\nsampled and domain-experts then annotated the\nsentences for medical jargon 2.\n3.1.2 Data Annotation\nDomain-experts read each sentence and identified\nas medical jargon terms that would be considered\ndifficult to comprehend for anyone no greater than\na 7th grade education 3. Overall, 96,479 medical\njargon terms have been annotated by complying\nwith the following annotation guideline.\nAnnotation Guideline The dataset was anno-\ntated for medical jargon by six domain experts from\nmedicine, nursing, biostatistics, biochemistry, and\nbiomedical literature curation 4. Herein, the anno-\n2Using these data requires a license agreement.\n3The rule of thumb is that if a candidate term has a lay def-\ninition comprehensible to a 4-7th grader as judged by Flesch-\nKincaid Grade Level (Solnyshkina et al., 2017), the candidate\nterm is included as a jargon term.\n4The annotator agreement scores can be found in Ap-\npendix A.1.\n11735\ntators applied the following rules for identifying\nwhat was jargon:\nRule 1. Medical terms that would not be recog-\nnized by about 4 to 7th graders , or that have a\ndifferent meaning in the medical context than\nin the lay context (homonym) were labeled. For\nexample:\n• accommodate: When the eye changes focus\nfrom far to near.\n• antagonize: A drug or substance that stops the\naction or effect of another substance.\n• resident: A doctor who has finished medical\nschool and is receiving more training.\n• formed: Stool that is solid.\nRule 2. Terms that are not strictly medical, but are\nfrequently used in medicine. For example:\n• \"aberrant\", \"acute\", \"ammonia\", \"tender\", \"in-\ntact\", \"negative\", \"evidence\"\nRule 3. When jargon words are commonly used\ntogether, or together they mean something dis-\ntinct or are difficult to quickly understand from\nthe individual parts were labeled. For example:\n• vascular surgery: Medical specialty that per-\nforms surgery on blood vessels.\n• airway protection: Inserting a tube into the\nwindpipe to keep it wide open and prevent\nvomit or other material from getting into the\nlungs.\n• posterior capsule: The thin layer of tissue be-\nhind the lens of the eye. It can become cloudy\nand blur vision.\n• right heart: The side of the heart that pumps\nblood from the body into the lungs.\n• intracerebral hemorrhage: A stroke.\nRule 4. Terms whose definitions are widely\nknown (e.g., by a 3rd grader) do NOT need to\nbe labeled. For example:\n• “muscle”, “heart”, “pain”, “rib”, “hospital”\nRule 4.1 When in doubt, label the term. For\nexample:\n• “colon”, “immune system”\n3.1.3 Data Cleaning\nFirst, we cleaned up overlapped (tumor suppres-\nsor gene, gene deletion) or nested (vitamin D, 25-\nhydroxy vitamin D) jargon. We chose the longest\njargon terms among nested or overlapped jargon\nterms. For example, we chose \"tumor suppressor\ngene\" as a jargon term, not its nested term \"tumor.\"\nIn all, MedJ contains a total of 95,393 context-\ndependent jargon terms which we used as the gold\nstandard for training and evaluation of the MedJEx\nmodel. The 95,393 jargon terms represent a total\nof 12,383 unique jargon terms.\n3.2 WikiHyperlink\nFrom a Wiki dump data 5, we first cleaned and\nelicited text by using Wikiextractor (Attardi, 2015).\nThen, we extracted hyperlink spans with the Beauti-\nfulSoup (Richardson, 2007) module. Wiki articles\nwere split into sentences with the Natural Language\nToolkit (Bird et al., 2009), then the sentences were\nsplit into tokens with the PLM tokenizer. Overall,\nWikiHyperlink contains more than 114M sentences,\n13B words, and 99M hyperlink spans. Finally, the\nsource data consists of the sequence input of the\ntoken and hyperlink labels represented in the stan-\ndard BIOES format (Yang et al., 2018).\n4 MedJEx Model\nFigure 1 is an overview of MedJEx. First, we\ntrained PLMs with WikiHyperlink (Wiki-trained).\nThen, the Wiki-trained model was transferred to\nthe target model that we propose by initializing\nthe target model with the weight parameters of the\nWiki-trained model. Finally, we fine-tuned the tar-\nget model with our expert-annotated dataset. Note\nthat, since the pretrain corpora of PLMs used in\nthis work include the Wiki corpus, we noticed that\nthe performance change should derive from the\nadded labels (hyperlink spans). Herein, we ex-\ntracted UMLS concepts and used them as auxiliary\nfeatures.\n4.1 Wiki’s Hyperlink Span Prediction for\nTransfer Learning Framework\nAlthough MedJ is a high-quality and a large scale\nexpert-labeled dataset, deep learning models could\nimprove performance with additional data. How-\never, annotation is very expensive. Transfer learn-\ning is one of the effective ways to mitigate the\n5https://dumps.wikimedia.org/enwiki/\n20211001/\n11736\nchallenge (Ruder, 2019; Mao, 2020). In this paper,\nwe propose to utilize Wiki’s articles and hyperlink\nspan as source data. We assumed that hyperlink\nspans are similar to medical jargon: readers need to\nread the hyperlinked articles to understand the span\nconcepts (Mihalcea and Csomai, 2007). Indeed,\nin the example sentence in Figure 1, we can see\nthat some difficult biomedical concepts are hyper-\nlinked, but easier concepts such as \"fatigue\" and\n\"headache\" were not linked. We also expect that\nthis approach can also be generalized for BioNER\ntasks since hyperlinks are often associated with a\nbiomedical concept.\nTraining We fine-tune a PLM with WikiHyper-\nlinik by following the standard protocol for fine-\ntuning PLMs for sequence labeling tasks (Devlin\net al., 2019). Herein, for a given N number of sen-\ntences, a PLM calculated the probability distribu-\ntion for the C classes of each token in the sentence\ncomposed of S tokens. The model was trained to\noptimize cross-entropy (CE) loss of Eq. 1, where\nyn,s,c and ˆyn,s,c indicate the label and the model’s\noutput, respectively, for the sth token’s probability\nof the nth sentence belonging to the class c respec-\ntively.\nLCE = 1\nNSC\nN∑\nn=1\nS∑\ns=1\nC∑\nc=1\nyn,s,c ·log ˆyn,s,c (1)\n4.2 Neural Network-based Medical Jargon\nExtraction Model\nOur jargon prediction model consists of the fol-\nlowing two parts: 1) additional learning feature\nextraction, 2) target model. This section explains\nthe additional feature extraction at first. Then, we\ndescribes the structure of the target model.\n4.2.1 Auxiliary Feature Extraction\nThe UMLS concepts incorporate important clinical\ndomain-specific jargon, including disease, surgery,\ndrug, etc (Katona and Farkas, 2014; Chen et al.,\n2018). Therefore, in this study, we extracted the\nUMLS concepts from input sentences and then\nused them as features for medical jargon term ex-\ntraction. We elicited UMLS concepts from in-\nput sentences with QuickUMLS, an unsupervised\nUMLS concept matching tool (Soldaini and Go-\nharian, 2016). Then, we represented the positions\nof concepts in binary feature extraction in BIOES\nbinary encoding. The weighting score feature was\nexpressed by multiplying the binary encoding of a\nconcept by the term weighting.\nIn this study, we employed the widely used\nTF score and the masked language model (MLM)\nscore as term weighting methods. We normalized\nMLM scores and TF scores to values between [0,\n1] by Min-Max scaling (Al Shalabi and Shaaban,\n2006). Details on the expression of additional fea-\ntures are described in Appendix B.\nContextualized MLM Score Frequency score-\nbased methods have been widely used to extract\nunfamiliar or important terms, since some jar-\ngon terms can be rarely observed in the gen-\neral corpus (Chen et al., 2018). However, term\nfrequency-based approaches do not consider con-\ntextual factors, and therefore tend to underestimate\nthe homonym issue. Otherwise, a language model\nis a probability distribution over a sequence of\nwords. We can calculate the probability of phrases\nor words for a given context. In particular, in\nMLMs, it is known that we can understand whether\nknowledge of a specific concept is included in\nPLMs by masking part of the sentence (Petroni\net al., 2019; Kwon et al., 2019a; Zhong et al., 2021).\nWe proposed a MLM score that is the negative\nlikelihood of the masked tokens from a text. Eq. 2\nis the MLM score of a UMLS medical conceptc for\na given sentence S. Suppose, Tc is the token length\nof c and p is the starting position of c. Herein,\nwe mask concept tokens Sp...Sp+Tc−1 with a spe-\ncial token \"[MASK]\" then input the masked will\nbe ˜S to PLMs. As a result, we can get the prob-\nability P( ˜Si = Si|˜S) of the ith masked token ˜Si\nwill be Si from ˜S. Then, we calculated the MLM\nscore of c (MLM (c, S)) by averaging negative log\nlikelihoods of masked tokens. Overall, when the\nMLM of the model is low, it means that the masked\nconcept can be predicted easily.\nMLM (c, S) =−1\nTc\np+Tc−1∑\ni=p\nlogP ( ˜Si = Si|˜S) (2)\n4.2.2 Target Model\nFirst, an input sentence was split into subword units\nthrough a PLM tokenizer. Binary features were in-\nput to a multi-layer perceptron (MLP), mapped into\nthe same dimension as the token embedding vector,\nand then added to the output of the tokenizer. The\nadded input is input to a Wiki-trained LM then we\ncan get hidden. In the Wiki’s hyperlink step, the\ninitial parameters of the PLM trained were set to\nthe weight parameters of Wiki-trained LM. Then,\nthe output of the Wiki-trained LM (Hidden) was\n11737\ninput to an MLP to create an emission score. Si-\nmultaneously, the weighted scores and the Hidden\nwere concatenated and then input to another MLP\nto create the weighted emission score. We got the\nfinal emission score by adding the emission score\nand the weighted emission score. Then, the final\nemission was input to the conditional random field\n(Lafferty et al., 2001) layer. Suppose P is the fi-\nnal emission, and y is a sequence of output labels.\nHerein, we calculated the score of the sequence y\ndefined as Eq. 3 with the transition matrix A. Then,\nwe picked the optimal output sequence ˆy from all\npossible sequences of labels Y by jointly decoding\nthrough Viterbi searching (Viterbi, 1967).\ns(y) =exp\n( n∑\ni=0\nAyiyi+1 +\nn∑\ni=0\nPiyi\n)\n(3)\nˆy = argmax\n˜y∈Y\ns(˜y) (4)\n5 Experiment\n5.1 Experimental Set Up\nThe experiments on the WikiHyperlink span predic-\ntion were conducted on the following settings. The\nWiki data consists of approximately 26M articles,\nwhich correspond to 150M sentences with 99M hy-\nperlinks. We used BERT-cased base (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019) base models.\nWe also utilized BioClinicalBERT (Alsentzer et al.,\n2019b) and BioBERT (Lee et al., 2020) which are\nthe state-of-the-art PLMs pretrained on biomedical\ntext. We mainly set the hyper-parameters of the\nPLMs by following Gururangan et al. (2020)’s post-\ntraining setting. Meanwhile, each input consisted\nof up to 128 tokens and the learning rate was set\nto 5e-4. The parameters were updated every 2,048\ninputs. We trained PLMs up to 50K update steps,\nwhich is slightly less than 1 epoch (about 56K; 7\ndays). For the remaining hyper-parameters, we\nused the default setting of the Transformers library\n(Wolf et al., 2020).\nTo fine-tune medical jargon extract models, we\nused the following settings. First, the batch size and\nmaximum epoch were set to 32 and 3, respectively,\naccording to the PLMs’ standard training setting.\nWe set the learning rate as 5e-5 for all models. Fi-\nnally, we randomly split the dataset into a 14,542\ntraining set (80%), a 1,817 validation set (10%),\nand a 1,819 test set (10%). Hyper-parameters and\nexperimental models were selected with the high-\nest performance in the validation set, and detailed\n73.4473.7774.3974.4074.7174.5775.1875.4775.8075.9576.0076.3777.6777.9577.9478.0977.9678.5778.1978.6178.6978.6578.8078.79\n72.5073.5074.5075.5076.5077.5078.5079.50\n0 10K20K30K40K50K\nRoBERTaBERTBioClinicalBERTBioBERT\nFigure 2: The F1 scores of the vanilla models with the\nupdate step of the WikiHyperlink training.\nresults are described in Appendix C.\nMoreover, a Wordfreq library (Speer et al., 2018)\nwas adopted to calculate TF of the candidate UMLS\nconcepts. We performed Student’s t-test (Student,\n1908) to assess whether the change in performance\nbetween experimental results was statistically sig-\nnificant. Finally, we used the F1 score (Kwon et al.,\n2019b) to evaluate the performance of the model.\n5.2 Experimental Results\nThe PLMs can be categorized as the following two\ntypes: 1) pretrained models were initialized with\nstandard pretrained models and 2) Wiki-trained\nmodels were initialized with the Wiki’s hyperlink\ntrained models. Vanilla models do not incorporate\nthe UMLS features. The binary model has only\nthe binary features. +TF and +MLM indicate that\nadding the TF score feature and MLM score fea-\nture, respectively. +TF+MLM concatenates two\nfeatures as the weighted input. Finally, The En-\nsemble is a weighted voting of the predictions of\nfour models (Binary, +TF, +MLM, +MLM+TF)\ndesigned to reflect various aspects of the features.\nThe algorithm for Ensemble is described in Ap-\npendix E.\n5.2.1 Experimental Results on the Hyperlink\nTraining Step\nFigure 2 is the fine-tuning performance of the\nvanilla models for every 10K update step on the\ntest set. Herein, step 0 indicates the pretrained set-\nting. The results show that the medical jargon term\nextraction performance tends to be improved as\nthe update step increases. The performance was\nimproved by 1.19%p (p<1e-4) in BERT, 1.13%p\n(p<1e-3) in RoBERTa, 0.90%p ( p=0.11) in Bio-\nClinicalBERT and 0.60%p (p=0.07) in BioBERT,\nalthough the models were trained with less than\n1 epoch of Wiki data. Considering that the pre-\n11738\nModel\nBERT RoBERTa BioClinicalBERT BioBERT\nPretrained Wiki-trained Pretrained Wiki-trained Pretrained Wiki-trained Pretrained Wiki-trained\nPrec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1\nVanilla 75.08 75.27 75.18 75.65 77.11 76.37 72.00 74.94 73.44 73.06 76.14 74.57 77.29 78.06 77.67 77.39 79.78 78.57 77.79 78.59 78.19 78.76 79.40 78.79\nBinary 76.44 77.06 76.75 76.31 78.51 77.39 74.00 75.10 74.54 74.63 76.31 75.46 78.47 78.85 78.66 78.80 79.38 79.09 78.88 78.34 78.61 78.76 79.40 79.08\n+TF 75.96 78.04 76.99 75.92 79.31 77.58 73.90 76.50 75.18 74.29 76.44 75.35 78.16 79.60 78.88 78.63 79.80 79.21 78.73 79.51 79.12 78.69 80.01 79.35\n+MLM 76.05 78.09 77.06 75.83 79.16 77.46 72.91 76.00 74.42 74.36 76.45 75.39 78.15 79.65 78.89 78.75 79.70 79.22 78.68 79.45 79.06 78.65 80.04 79.34\n+TF+MLM 76.27 77.39 76.83 77.67 77.26 77.26 73.06 76.14 74.57 74.12 76.676 75.37 78.16 79.02 78.59 78.37 79.09 78.73 78.49 78.78 78.64 78.60 79.36 78.98\nEnsemble 76.07 78.62 77.33 76.39 78.90 77.62 74.09 76.83 75.44 74.80 76.96 75.86 77.93 79.88 78.89 78.67 79.81 79.23 78.73 79.52 79.12 78.71 80.06 79.38\nTable 1: The precision (Prec), recall (Rec) and F1 scores of MedJEx models.\ntraining corpora of all models include the English\nWikipedia corpus (Liu et al., 2019), we can infer\nthat the improvements are due to the hyperlink\nspan information rather than Wiki’s text. Other-\nwise, the performances of biomedical BERTs are\nmarginally enhanced. We speculate that this is\nbecause the models have already been trained on\nthe biomedical literature, so the effect of the task\ntransfer through learning WikiHyperlink span in-\nformation is relatively small. Nevertheless, these\nresults imply that the task transfer is effective albeit\nWiki data are a general corpus. Overall, we can see\nthat the Wiki-training is beneficially transferred to\nmedical jargon extraction models, supporting our\nassumption.\n5.2.2 Impact of the Proposed Methods\nTable 1 contains the experimental results for eval-\nuating the impact of the proposed methods. Com-\npared to the vanilla models, the Wiki-trained en-\nsemble models outperformed by 2.44%p in BERT\n(p<1e-11), 2.42%p (p<1e-9) in RoBERTa, 1.56%p\n(p<1e-5) in BioClinicalBERT and 1.19%p (p<1e-\n3) in BioBERT. We can see that the Wiki-trained\nmodels improved performance in all 24 cases com-\npared to the pretrained models. This means that\nthe WikiHyperlink span’s information is helpfully\ntransferred to training medical jargon. In addi-\ntion, the binary models demonstrate better perfor-\nmance compared to the Vanilla models. Compared\nto Binary, TF and MLM features improve perfor-\nmance marginally in BERT, BioClinicalBERT and\nBioBERT. On the other hand, in the RoBERTa\nmodel, while the TF feature improves the perfor-\nmance in the pretrained model, it can be seen that\nthe performance is slightly decreased in other cases.\nIn addition, when both TF and MLM features are\nincluded, the performance is marginally changed\ncompared to using each feature. The ensemble\nmodels lead to the highest performance in all cases.\nPrec. Rec. F1\nQuickUMLS 21.69 62.21 32.16\nMedCAT 45.89 32.32 37.93\nTable 2: The precision (Prec), recall (Rec) and F1 scores\non UMLS concept extraction systems.\nType Datasets Pretrained Wiki-trained\nDisease NCBI disease 87.92 89.21\nBC5CDR disease 83.93 84.87\nDrug &\nChem.\nBC5CDR Chem. 92.07 91.88\nBC4CHEMD 90.06 90.27\nGene &\nProtein\nBG2GM 82.30 83.06\nJNLPBA 74.95 77.95\nSpecies LINNAEUS 87.59 89.87\nS800 74.95 74.85\nTable 3: F1 score of BioBERT V anillamodels on Pre-\ntrained and Wiki-trained settings in BioNER datasets.\nChem. indicates ‘chemical.’\n5.3 Comparison with UMLS Concept\nExtractors\nTo verify that our task is different from existing\nUMLS concept extraction task. For this, we eval-\nuated the performance of existing UMLS concept\nextractors, QuickUMLS and MedCAT (Kraljevic\net al., 2019), in MedJ’s test dataset. The results\nin Table 2 show that the performance of UMLS\nextractors was substantially inferior to our models.\nEven though QuickUMLS extracted all possible\nUMLS concepts, the recall score was 62.21, indi-\ncating that a substantial amount of medical jargon\nterms (37.79%) in EHR notes is not included in the\nUMLS concepts. To put it differently, since UMLS\nconcept extractors mainly concentrated on specific\ntypes of medical terms, there are some representa-\ntive jargon types that the UMLS concept extractors\nfrequently fail to predict: 1) abbreviations (e.g., yo:\nyears old, s/p: status post ...), 2) special numerical\nterms (e.g., 20/40: vision test results, 2-0: a heavy\nthread used for stitching ...).\n11739\n5.4 Impact of the Wiki’s Hyperlink Span\nTraining on BioNER Datasets\nWe assessed the generalizability of Wiki training by\nconducting experiments on eight BioNER bench-\nmarks used in Lee et al. (2020)’s setting 6. We\nevaluated the performance of the BioBERTV anilla\nmodel on each data in the pretrained and Wiki-\ntrained settings. Table 3 shows F1 scores on\nthe datasets. We can see that Wiki training pos-\nitively affected five datasets while it marginally im-\npacted three datasets (S800, BC5CDR Chem. and\nBC4CHEMD). Especially, in JNLPBA, the perfor-\nmance improveed by 3%p, and in LINNAEUS, the\nperformance improved by 2.29%p. Meanwhile,\nsince BioNER benchmarks are targeted to elicit\nspecific medical concepts, there is some medical\njargon that BioNER cannot cover, such as metric\nunits (millliter, mg, ...) and medical techniques\n(flushing: to use fluid to clean out a catheter) and\nso on. Detailed experimental settings and results\nare described in Appendix F.\n6 Discussion\n6.1 Feature Analysis\nMLM Figure 3 represents the histograms of the\nbiomedical concepts on MLM scores. The blue-\ncolored histogram indicates the UMLS biomedical\nconcepts that are not jargon, while the red-colored\nhistogram indicates UMLS biomedical concepts\nthat are jargon. We can notice the heavily tail of the\nhistogram of non-jargon concepts indicating MLM\nscores are lower. The heavy tail includes concepts\nthat are relatively easy to understand (e.g. shoulder,\nchest pain, wound management ...). These results\nshow that the MLM score can be an appropriate\nfeature to determine whether a concept will be the\nmedical jargon. Additional analyses for the MLM\nscore are in Appendix G.\nTF and MLM We conducted a case study to ana-\nlyze the impact of the TF and MLM for identifying\nmedical jargon from EHR notes. Specifically, we\ncalculated the TF and MLM scores of candidate\nUMLS concepts that had been mapped to the med-\nical jargon in our EHR note sentences. Then, we\ncategorized the concepts according to their scores.\nWe used MLM of the pretrained BERT, and the\n6NBCI disease (Dogan and Lu, 2012), BC5CDR disease\n(Wei et al., 2016), BC5CDR chemical (Wei et al., 2016),\nBC4CHEMD chemical, (Krallinger et al., 2015), BC2GM\n(Smith et al., 2008), JNLPBA (Kim et al., 2004), LINNAEUS\n(Gerner et al., 2010), S800 (Pafilis et al., 2013)\nFigure 3: Histograms of the MLM score feature for\nUMLS biomedical concepts. Red: jargon concepts;\nblue: non-jargon concepts\nconcepts were categorized as a high score (> 0.5;\n↑) and a low score (< 0.5; ↓). Note that a high\nMLM score means that the BERT failed to predict\nthe concept. A high TF score means that the con-\ncept was frequently observed in the general corpus.\nThe following is the combination of MLM and TF\ncategories and notable examples.\n1. ↑TF, ↑MLM: \"shock\", \"drainage\", \"tissue\"\n2. ↓TF, ↑MLM: \"Vancomycin B\", \"Seroquel\",\n\"subdural hematoma\"\n3. ↑TF, ↓MLM: \"coma\", \"gene\", \"wound\"\n4. ↓TF, ↓MLM: \"pneumonia\", \"membrane\", \"vi-\nral\"\nThe first case is a word frequently observed in a\ngeneral corpus (↑TF), but it is a concept that fails\nto predict in the BERT ( ↑MLM). This concept\nincludes rare senses used in medical contexts. For\nexample, \"drainage\" is used as a synonym for sewer\nin the general context, while in the medical domain\nit may mean \"extra liquid that is removed from\nthe body.\" The second case is the most unfamiliar\nwords. The concepts are composed of multiple\ntokens and medical entities such as disease or drug\nnames. The third case consists of relatively easy-\nto-understand concepts. The fourth case contains\nrelatively short medical jargon composed of 1 to 2\ntokens. We can infer that MLM and TF cannot only\nbe complementary but also can be used together to\nhelp solve the challenging homonym issue.\n6.2 Error Analysis\nWe manually examined the outputs. The most com-\nmon type of false negatives errors was abbrevia-\ntions, such as \"ENT\" for \"ear, nose, and throat\"\nand \"or\" for \"operating room\", and \"p.o.\" for \"per\nos\". Another type of error was signs with special\nmeanings such as \"q.6 h\" for \"per every 6 hours\",\n\"x2\" for \"two times\", and \"3+\" for \"very strong\"\n11740\nfall into this type. Other notable errors were epoy-\nmous person name-based medical concepts (e.g.,\n\"Azzopardi effect\") and device names (e.g., \"Bi-\nPap\"). MedJEx failed to detect the aforementioned\ntypes of medical jargon due to the data sparsity\nchallenge.\n6.3 Prospective Downstream Applications\nMedical jargon extraction task has divers potential\napplications. It could be a preprocessing part of\nBioNLP pipelines and used for downstream medi-\ncal AI application systems. For example, it could\nbe adapted to medical concept linking systems such\nas NoteAid (Polepalli Ramesh et al., 2013). In ad-\ndition, a chatbot-based self-diagnosis system (You\nand Gui, 2020) could use our approach for the ex-\nplanation of medical jargons to avoid generating\njargons.\n6.4 Merits\nProspective downstream applications can promote\neffective communication between clinicians and\ntheir patients by increasing patients’ EHR compre-\nhension ability. This, in turn, can help the patients\nin self-management of their illness (Adams, 2010).\nEffective communication is also beneficial for pre-\nventing physicians’ burnout (Aaronson et al., 2019).\nThus, we can expect this new task will contribute\nnot only to improve the patients’ outcomes.\n6.5 Limitations\nThis task defined medical jargon at a single\ndifficulty-level, disregarding diverse educational\nlevels of users. In particular, setting the difficulty\nof each medical jargon term will help this task con-\ntribute to improving the performance of machines\nas well as patients, and further educate and support\nclinicians. Moreover, we did not analyze the jargon\ntypes such as acronyms but merely identified the\npresence of medical jargon, which can limit further\nanalyses.\n7 Conclusion\nWe introduce a novel NLP tasked named MedJEx\nand present an expert-curated MedJ dataset for the\ntask. We propose two innovative methods: 1) Pre-\ntraining Wiki’s hyperlink span, and 2) Contextu-\nalized MLM score feature for extracting medical\njargon from EHR notes. The experimental results\nshow that the Wiki’s hyperlink span can be effec-\ntively transferred to the medical jargon extraction\nmodel, leading to a significant performance im-\nprovement. Wiki’s hyperlink span training also\nbeneficial in six out of eight BioNER benchmarks.\nFinally, in a qualitative evaluation, the MLM score\nfeature complements the TF feature to identify com-\nmon terms (or terms with high TFs) used in the\nclinical domain (homonyms).\nEthical Consideration\nIn this study, we legitimately obtained a licensed\naccess to the University of Pittsburgh Medical Cen-\nter EHR repository, and all EHR notes used were\nfully de-identified.The experiments described in\nAppendix A.1 and D were performed in accordance\nwith the recommendations laid out in the World\nMedical Association Declaration of Helsinki. The\nstudy protocol was approved by the institutional\nreview boards of a medical school in the US.\nIn addition, our model used BERT and its fam-\nilies, so it over-relies on a contextual embedding\nfeature that can cause mis-classification. Specifi-\ncally, even with the same terminology, the predic-\ntion of a model may be different depending on the\ncontext.\nAcknowledgement\nResearch reported in this study was in part sup-\nported by the National Institutions of Health\nR01DA045816 and R01MH125027. The content is\nsolely the responsibility of the authors and does not\nnecessarily represent the official views of National\nInstitutes of Health.\nReferences\nEmily L Aaronson, Benjamin A White, Lauren Black,\nDavid F Brown, Theodore Benzer, Allison Castagna,\nAli S Raja, Jonathan Sonis, and Elizabeth Mort. 2019.\nTraining to improve communication quality: an ef-\nficient interdisciplinary experience for emergency\ndepartment clinicians. American Journal of Medical\nQuality, 34(3):260–265.\nEmil Abrahamsson, Timothy Forni, Maria Skeppstedt,\nand Maria Kvist. 2014. Medical text simplification\nusing synonym replacement: Adapting assessment\nof word difficulty to a compounding language. In\nProceedings of the 3rd Workshop on Predicting and\nImproving Text Readability for Target Reader Popu-\nlations (PITR), pages 57–65.\nRobert John Adams. 2010. Improving health outcomes\nwith better patient understanding and education. Risk\nmanagement and healthcare policy, 3:61.\n11741\nHerman Aguinis, Isabel Villamor, and Ravi S Ramani.\n2021. Mturk research: Review and recommendations.\nJournal of Management, 47(4):823–837.\nLuai Al Shalabi and Zyad Shaaban. 2006. Normaliza-\ntion as a preprocessing engine for data mining and\nthe approach of preference matrix. In 2006 Inter-\nnational conference on dependability of computer\nsystems, pages 207–214. IEEE.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019a. Publicly available clinical BERT\nembeddings. In Proceedings of the 2nd Clinical Nat-\nural Language Processing Workshop, pages 72–78,\nMinneapolis, Minnesota, USA. Association for Com-\nputational Linguistics.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019b. Publicly available clin-\nical bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop, pages\n72–78.\nAlan R Aronson. 2001. Effective mapping of biomed-\nical text to the umls metathesaurus: the metamap\nprogram. In Proceedings of the AMIA Symposium,\npage 17. American Medical Informatics Association.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nMolly Baldry, Carol Cheal, Brian Fisher, Myra Gillett,\nand Val Huet. 1986. Giving patients their own\nrecords in general practice: experience of patients\nand staff. Br Med J (Clin Res Ed), 292(6520):596–\n598.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. \" O’Reilly Media,\nInc.\".\nOlivier Bodenreider. 2004. The unified medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nJinying Chen, Emily Druhl, Balaji Polepalli Ramesh,\nThomas K Houston, Cynthia A Brandt, Donna M\nZulman, Varsha G Vimalananda, Samir Malkani, and\nHong Yu. 2018. A natural language processing sys-\ntem that links medical terms in electronic health\nrecord notes to lay definitions: system development\nusing physician reviews. Journal of medical Internet\nresearch, 20(1):e26.\nJinying Chen, Abhyuday N Jagannatha, Samah J Fodeh,\nand Hong Yu. 2017. Ranking medical terms to\nsupport expansion of lay language resources for pa-\ntient comprehension of electronic health record notes:\nadapted distant supervision approach. JMIR medical\ninformatics, 5(4):e8531.\nJinying Chen and Hong Yu. 2017. Unsupervised en-\nsemble ranking of terms in electronic health record\nnotes based on their importance to patients. Journal\nof biomedical informatics, 68:121–131.\nAntonio Cuevas, Manuel Febrero, and Ricardo Fraiman.\n2004. An anova test for functional data. Computa-\ntional statistics & data analysis, 47(1):111–122.\nDina Demner-Fushman, Willie J Rogers, and Alan R\nAronson. 2017. Metamap lite: an evaluation of\na new java implementation of metamap. Journal\nof the American Medical Informatics Association ,\n24(4):841–844.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1).\nRezarta Islamaj Dogan and Zhiyong Lu. 2012. An\nimproved corpus of disease mentions in pubmed cita-\ntions. In BioNLP: Proceedings of the 2012 Workshop\non Biomedical Natural Language Processing, pages\n91–99.\nMartin Gerner, Goran Nenadic, and Casey M Bergman.\n2010. Linnaeus: a species name identification sys-\ntem for biomedical literature. BMC bioinformatics,\n11(1):1–17.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nAnette Hulth. 2003. Improved automatic keyword ex-\ntraction given more linguistic knowledge. In Pro-\nceedings of the 2003 conference on Empirical meth-\nods in natural language processing, pages 216–223.\nAbhyuday Jagannatha, Feifan Liu, Weisong Liu, and\nHong Yu. 2019. Overview of the first natural lan-\nguage processing challenge for extracting medica-\ntion, indication, and adverse drug events from elec-\ntronic health record notes (made 1.0). Drug safety,\n42(1):99–111.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H\nLehman, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi,\nand Roger G Mark. 2016. Mimic-iii, a freely accessi-\nble critical care database. Scientific data, 3(1):1–9.\nSasikiran Kandula, Dorothy Curtis, and Qing Zeng-\nTreitler. 2010. A semantic and syntactic text simplifi-\ncation tool for health content. In AMIA annual sym-\nposium proceedings, volume 2010, page 366. Ameri-\ncan Medical Informatics Association.\nMelinda Katona and Richárd Farkas. 2014. Szte-nlp:\nClinical text analysis with named entity recognition.\nIn Proceedings of the 8th International Workshop\n11742\non Semantic Evaluation (SemEval 2014), pages 615–\n618.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduction to\nthe bio-entity recognition task at jnlpba. In Proceed-\nings of the international joint workshop on natural\nlanguage processing in biomedicine and its applica-\ntions, pages 70–75. Citeseer.\nZeljko Kraljevic, Daniel Bean, Aurelie Mascio, Lukasz\nRoguski, Amos Folarin, Angus Roberts, Rebecca\nBendayan, and Richard Dobson. 2019. Medcat–\nmedical concept annotation tool. arXiv preprint\narXiv:1912.10166.\nZeljko Kraljevic, Thomas Searle, Anthony Shek, Lukasz\nRoguski, Kawsar Noor, Daniel Bean, Aurelie Mas-\ncio, Leilei Zhu, Amos A Folarin, Angus Roberts,\net al. 2021. Multi-domain clinical natural language\nprocessing with medcat: the medical concept anno-\ntation toolkit. Artificial Intelligence in Medicine ,\n117:102083.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu, Robert\nLeaman, Yanan Lu, Donghong Ji, Daniel M Lowe,\net al. 2015. The chemdner corpus of chemicals and\ndrugs and its annotation principles. Journal of chem-\ninformatics, 7(1):1–17.\nSunjae Kwon, Cheongwoong Kang, Jiyeon Han, and\nJaesik Choi. 2019a. Why do masked neural language\nmodels still need common sense knowledge? arXiv\npreprint arXiv:1911.03024.\nSunjae Kwon, Youngjoong Ko, and Jungyun Seo. 2019b.\nEffective vector representation for the korean named-\nentity recognition. Pattern Recognition Letters ,\n117:52–57.\nJohn D Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random fields: Proba-\nbilistic models for segmenting and labeling sequence\ndata. In Proceedings of the Eighteenth International\nConference on Machine Learning, pages 282–289.\nJohn P Lalor, Wen Hu, Matthew Tran, Hao Wu,\nKathleen M Mazor, and Hong Yu. 2021. Eval-\nuating the effectiveness of noteaid in a commu-\nnity hospital setting: Randomized trial of electronic\nhealth record note comprehension interventions with\npatients. Journal of medical Internet research ,\n23(5):e26354.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157.\nDonald AB Lindberg, Betsy L Humphreys, and Alexa T\nMcCray. 1993. The unified medical language system.\nYearbook of Medical Informatics, 2(01):41–51.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHuanru Henry Mao. 2020. A survey on self-supervised\npre-training for sequential transfer learning in neural\nnetworks. arXiv preprint arXiv:2007.00800.\nClara Meister, Tiago Pimentel, Thomas Clark, Ryan\nCotterell, and Roger Levy. 2022. Analyzing wrap-\nup effects through an information-theoretic lens. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 20–28.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alexander Wong. 2021. Umlsbert:\nClinical domain knowledge augmentation of contex-\ntual embeddings using the unified medical language\nsystem metathesaurus. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1744–1753.\nRada Mihalcea and Andras Csomai. 2007. Wikify!\nlinking documents to encyclopedic knowledge. In\nProceedings of the sixteenth ACM conference on Con-\nference on information and knowledge management,\npages 233–242.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In Proceedings of the 2004 con-\nference on empirical methods in natural language\nprocessing, pages 404–411.\nIslam Nassar, Michelle Ananda-Rajah, and Gholamreza\nHaffari. 2019. Neural versus non-neural text simpli-\nfication: A case study. In Proceedings of the The\n17th Annual Workshop of the Australasian Language\nTechnology Association, pages 172–177.\nEvangelos Pafilis, Sune P Frankild, Lucia Fanini,\nSarah Faulwetter, Christina Pavloudi, Aikaterini\nVasileiadou, Christos Arvanitidis, and Lars Juhl\nJensen. 2013. The species and organisms resources\nfor fast and accurate identification of taxonomic\nnames in text. PloS one, 8(6):e65390.\nJon Patrick and Min Li. 2010. High accuracy infor-\nmation extraction of medication information from\nclinical notes: 2009 i2b2 medication extraction chal-\nlenge. Journal of the American Medical Informatics\nAssociation, 17(5):524–527.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\n11743\nBalaji Polepalli Ramesh, Thomas Houston, Cynthia\nBrandt, Hua Fang, and Hong Yu. 2013. Improv-\ning patients’ electronic health record comprehension\nwith noteaid. In MEDINFO 2013, pages 714–718.\nIOS Press.\nBasel Qenam, Tae Youn Kim, Mark J Carroll, Michael\nHogarth, et al. 2017. Text simplification using con-\nsumer health vocabulary to generate patient-centered\nradiology reporting: translation and evaluation. Jour-\nnal of medical Internet research, 19(12):e8536.\nLeonard Richardson. 2007. Beautiful soup documenta-\ntion. April.\nSebastian Ruder. 2019. Neural transfer learning for\nnatural language processing. Ph.D. thesis, NUI Gal-\nway.\nIlham Fathy Saputra, Rahmad Mahendra, and Al-\nfan Farizki Wicaksono. 2018. Keyphrases extraction\nfrom user-generated contents in healthcare domain\nusing long short-term memory networks. In Proceed-\nings of the BioNLP 2018 workshop, pages 28–34.\nDean Schillinger, Margaret Handley, Frances Wang, and\nHali Hammer. 2009. Effects of self-management sup-\nport on structure, process, and outcomes among vul-\nnerable patients with diabetes: a three-arm practical\nclinical trial. Diabetes care, 32(4):559–566.\nShock. 2022. in: Cambridge Dictionary.\nLarry Smith, Lorraine K Tanabe, Cheng-Ju Kuo,\nI Chung, Chun-Nan Hsu, Yu-Shi Lin, Roman Klinger,\nChristoph M Friedrich, Kuzman Ganchev, Manabu\nTorii, et al. 2008. Overview of biocreative ii gene\nmention recognition. Genome biology, 9(2):1–19.\nLuca Soldaini and Nazli Goharian. 2016. Quickumls:\na fast, unsupervised approach for medical concept\nextraction. In MedIR workshop@SIGIR, pages 1–4.\nMarina Solnyshkina, Radif Zamaletdinov, Ludmila\nGorodetskaya, and Azat Gabitov. 2017. Evaluating\ntext complexity and flesch-kincaid grade level. Jour-\nnal of social studies education research , 8(3):238–\n248.\nSanjay Soundarajan, Sachira Kuruppu, Ashutosh Singh,\nJongchan Kim, and Monalisa Achalla. 2021. Spar-\nclink: an interactive tool to visualize the impact of\nthe sparc program. bioRxiv.\nRobyn Speer, Joshua Chin, Andrew Lin, Sara Jewett,\nand Lance Nathan. 2018. Luminosoinsight/wordfreq:\nv2.2.\nStudent. 1908. The probable error of a mean.\nBiometrika, pages 1–25.\nChuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang,\nChao Yang, and Chunfang Liu. 2018. A survey\non deep transfer learning. In International confer-\nence on artificial neural networks , pages 270–279.\nSpringer.\nAndrew Viterbi. 1967. Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE transactions on Information Theory,\n13(2):260–269.\nChih-Hsuan Wei, Yifan Peng, Robert Leaman, Allan Pe-\nter Davis, Carolyn J Mattingly, Jiao Li, Thomas C\nWiegers, and Zhiyong Lu. 2016. Assessing the state\nof the art in biomedical relation extraction: overview\nof the biocreative v chemical-disease relation (cdr)\ntask. Database, 2016.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nJie Yang, Shuailong Liang, and Yue Zhang. 2018. De-\nsign challenges and misconceptions in neural se-\nquence labeling. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 3879–3889.\nYue You and Xinning Gui. 2020. Self-diagnosis through\nai-enabled chatbot-based symptom checkers: user\nexperiences and design considerations. In AMIA An-\nnual Symposium Proceedings , volume 2020, page\n1354. American Medical Informatics Association.\nQing T Zeng and Tony Tse. 2006. Exploring and devel-\noping consumer health vocabularies. Journal of the\nAmerican Medical Informatics Association, 13(1):24–\n29.\nQing Zeng-Treitler, Sergey Goryachev, Hyeoneui Kim,\nAlla Keselman, and Douglas Rosendale. 2007. Mak-\ning texts in electronic health records comprehensible\nto consumers: a prototype translator. In AMIA An-\nnual Symposium Proceedings , volume 2007, page\n846. American Medical Informatics Association.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033.\nGuido Zuccon, Alexander Holloway, Bevan Koopman,\nand Anthony Nguyen. 2013. Identify disorders in\nhealth records using conditional random fields and\nmetamap aehrc at share/clef 2013 ehealth evalua-\ntion lab task 1. In Proceedings of the CLEF 2013\nWorkshop on Cross-Language Evaluation of Methods,\nApplications, and Resources for eHealth Document\nAnalysis, pages 1–8. The CLEF Initiative (Confer-\nence and Labs of the Evaluation Forum).\n11744\nAppendices\nA Data Annotation\nA.1 Evaluation of the Annotation\nTo evaluate the annotators’ reliability in identifying jargon, an observational study was performed to\nassess the agreement of the dataset annotators with each other and with laypeople. Note that, this work\nis a part of unpublished manuscript.\nA.1.1 Data Collection and Setting\nFor evaluation, twenty sentences were randomly selected from deidentified inpatient EHR notes from the\nUniversity of Pittsburgh Medical Center EHR repository. Sentences that consisted only of administrative\ndata, sentences whose length was less than ten words, and sentences substantially indistinguishable from\nanother sentence were filtered out.\nNote that, the annotators had never seen the sampled sentences. The twenty sentences were made\nup of 904 words in total. Common words were discarded so as not to inflate the calculated agreement.\nThese consisted of all pronouns, conjunctions, prepositions, numerals, articles, contractions, months,\npunctuation, and the most common 25 verbs, nouns, adverbs, and adjectives. Terms occurring more\nthan one time in a sentence were counted only once. Furthermore, to ameliorate double-counting issue,\nmulti-word terms were counted as single terms. Multi-word terms were determined by two members\nof the research team by consensus. In this work, multi-word terms were defined as adjacent words that\nrepresented a distinct medical entity (examples: “PR interval”, “internal capsule”, “acute intermittent\nporphyria”), were commonly used together (examples: “hemodynamically stable”, “status post”, “past\nmedical history”) and terms that were modified by a minor word (examples: “trace perihepatic fluid”,\n“mild mitral regurgitation”, “rare positive cells”, “deep pelvis”). After applying these rules, 325 candidate\nmedical jargon terms were utilized. The laypeople consisted of 270 individuals recruited from Amazon\nMechanical Turk (MTurk) (Aguinis et al., 2021).\nA.1.2 Annotation Reliability\nThe results showed that there was good agreement among annotators (Fleiss’ kappa = 0.781). The\nannotators had high sensitivity (91.7%) and specificity (88.2%) in identifying jargon terms as determined\nby the laypeople (the gold standard).\nB Details on Feature Representations\ni i+1i+2i+3i+4i+5B 1 0 0 0 0 0I 0 1 0 0 0 0O 0 0 0 1 1 0E 0 0 1 0 0 0S 0 0 0 0 0 1\n(a) Binary feature\ni i+1i+2i+3i+4i+5B 0.970 0 0 0 0I 0 0.970 0 0 0O 0 0 0 0 0 0E 0 0 0.970 0 0S 0 0 0 0 0 0.11 (b) Weighted score feature\nFigure 1: Examples of a binary feature and a weighted score feature.\nThis section explains details on the binary and weighted score features with an example. Suppose a\nconcept c1 starts with the ith token and the length is 3, and another concept c2 starts with the i + 5th token\nand the length is 1. In this case, binary encoding features can be expressed as a 5-dimensional vector (\"B\",\n\"I\", \"O\", \"E\", \"S\") as shown in Figure 1(a). Since c1 starts at i and ends at i + 2, the ith \"B\" dimension\nand i + 2th \"E\" dimension are set to 1. In addition, because the i + 1th token is the inner of the c1, the\ni + 1th ‘I’ dimension will be 1. Then, c2 starts and ends at i + 5. On the other hand, further assume that\nthe term weighting score of c1 and c2 are 0.97 and 0.11\n11745\nC Details of the Experimental Setting\nIn the experiments, we set the models’ hyper-parameters with the performance on the validation set via\nthe grid-search. Once, determining hyper-parameters in vanilla models, the same values were used in the\nother models. We trained WikiHyperlink for 50K update steps. Herein, the number of parameters of all\nexperimental models are about 108M. In all experiments, the random seed was set to 0. All experiments\nwere conducted in the Centos Linux 7 environment using one RTX-8000 GPU, Intel Xeon E5-2620 CPU,\nand 64GB RAM.\nC.1 Hyper-parameter Setting\nIn the case of MLP, all hidden sizes were set equal to the default hidden size of PLM. Also, the activation\nfunction used a hyperbolic tangent function by following Gururangan et al. (2020)’s setting.\nLearning Rate BERT RoBERTa BioClinicalBERT BioBERT\n6e-5 67.46 62.80 67.07 70.48\n1e-5 71.02 66.91 73.06 73.97\n5e-5 75.92 73.64 78.53 78.51\nTable 1: The F1 scores of the finetuned vanilla models for each learning rate.\nIn the fine-tuning on the task, we choose the best learning rate of the vanilla models on the validation\nset among the following set of the candidate learning rates {5e-6, 1e-5, 5e-5}. Overall, the results in\nTable 1 show that we could achieve the best performances on validation set when the learning rate was set\n5e-5.\nC.2 Model Selection\nBioClinicalBERT BioBERT BioMedRoBERTa BioClinicalRoBERTa\n78.53 78.51 73.89 76.27\nTable 2: The F1 scores of the fine-tuned pretrained biomedical PLMs on vanilla setting.\nIn this work, we utilize contextualized PLMs to make jargon prediction models. For this, we use two\nrepresentative PLMs: BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). In addition, there\nare several recent state-of-the-art models pretrained in biomedical domains, recently. To be specific,\nBioBERT additionally trained BERT with biomedical text corpora (Lee et al., 2020). BioClinicalBERT\n(Alsentzer et al., 2019b) further trained BioBERT with clinical notes from MIMIC-III (Johnson et al.,\n2016). In addition, there are some studies, such as BioMedRoBERTa (Gururangan et al., 2020) or\nBioClinicalRoBERTa (Lewis et al., 2020) that suggested training the RoBERTa model with biomedical\ntext corpora or clinical notes. On the other hand, Michalopoulos et al. (2021) proposed UmlsBERT that\nintegrates UMLS semantic type embedding as an additional input feature during the pretraining step.\nUmlsBERT is similar to our suggestion in that it uses UMLS concept as an embedding feature. However,\nour method is slightly different in that it uses span information instead of the UMLS semantic type.\nMoreover, we show that the performance can be improved using the UMLS features only in fine-tuning.\nIn this paper, we selected two biomedical PLMs by comparing the performances of state-of-the-art\nbiomedical PLMs in the vanilla models on the validation set. Table 2 presents the experimental comparison\namong the four representative biomedical PLMs: BioClinicalBERT (Alsentzer et al., 2019a), BioBERT\n(Lee et al., 2020), BioMedRoBERTa and BioClinicalRoBERTa. The results show that BioBERT and\nBioClinicalBERT showed no differences (p >0.05) but the other RoBERTa-based models presented\ninferior scores. Therefore, we choose the BERT-basedbiomedical PLMs for further experiments.\n11746\nC.3 Experimental Results of UMLS concept extractors\nPrec. Rec. F1\nQuickUMLS 21.10 60.74 31.32\nMedCAT 45.89 32.32 37.93\nTable 3: The precision (Prec), recall (Rec) and F1 scores on UMLS concept extraction systems.\nSetting Concept Extractor Prec. Rec. F1\nBERTBinary QuickUMLS 75.51 77.37 76.43\nBERTBinary MedCAT 74.90 77.00 75.93\nTable 4: The precision (Prec), recall (Rec) and F1 scores on BERT with a binary setting on the different concept\nextractors.\nSince experimental models rely on a UMLS concept extractor, it is also important to choose appropriate\nUMLS concept extractors. There are several concept extractors that have been introduced including\nMetaMap (Demner-Fushman et al., 2017), QuickUMLS (Soldaini and Goharian, 2016), cTAKES (Saputra\net al., 2018), and MedCAT (Kraljevic et al., 2021). We compared two extractors, QuickUMLS and\nMedCAT, which are state-of-the-art concept extractors. Table 3 presents the performance of the concept\nextractors. Herein, we can see that MedCAT achieved better performance in terms of precision and F1 but\nQuickUMLS had better recall performance. We preferred higher recall, since a concept extractor was\nused for candidate concept extraction. Indeed, the performances on the BERT with the binary setting in\nTable 4 demonstrates that using QuickUMLS led to higher performance than that of using MedCAT.\nC.4 Experimental Results on Tagging Schemes\nTagging scheme F1\nBIO 74.25\nBIOES 75.92\nTable 5: Experimental comparison on BIO and BIOES tagging schemes.\nFinally, to select a sequence labeling tagging scheme, we compared two representative tagging schemes:\nBegin, Inside and Outside (BIO) and Begin, Inside, Outside, End, and Singleton (BIOES) (Yang et al.,\n2018). Table 5 presents the experimental results on the validation set of the BERT’s vanilla setting. The\nresults show that the validation performance with the BIO scheme is lower than that of the BIOES scheme.\n11747\nD The Impact on the Understandability to Patients\nThis section introduces an experiment to verify that providing medical jargon and the corresponding lay\ndefinitions can be beneficial to the comprehension of patients. Note that all experimental settings and\nresults are part of Lalor et al. (2021)’s work.\nD.1 Experimental Setting\nThe authors recruited 174 patients from a community hospital in the USA. Herein, the participants took\na web-based EHR comprehension test and the participants were randomly assigned to a control (n=85)\ngroup or intervention (n=89) group to take the test without or with the support of the medical jargons\nidentification and the corresponding lay definitions, respectively. In addition, 200 participants from MTurk\nwere engaged to take the test (100 participants were assigned to a control group and the other 100 were\nallocated to an intervention group).\nD.1.1 EHR Comprehension Test\nFigure 2: An example question of the EHR comprehension test. Herein, you can see that identifying medical jargon\n\"ferritin\" and providing its definition can be helpful to understand that the bold text describes a blood iron test.\nTo assess a user’s comprehension of EHR notes, we conducted the EHR comprehension test. Table 2 is\nan example of the EHR comprehension test. This test consists of 14 paragraphs extracted from de-identified\nEHR notes and relevant multiple-choice questions curated by physicians. In previous work, it has been\nverified that the EHR comprehension test reflects the participant’s education level and understandability\nof the medical literature. In this experiment, we provided definitions for medical jargon only to the\nintervention group.\nD.2 Experimental Results\nSource Condition control Intervention\nMTurk 0.756±0.246 0.830±0.201\nLocal hospital 0.646±0.179 0.727±0.191\nTable 6: Experimental evaluation on patients’ understandability. Herein, the values in the table indicate the average\nscore and standard deviation of each group on the EHR comprehension test.\nTable 6 presents the experimental results of the evaluation of the customers’ understandability. The\nresults of ANOV A (Cuevas et al., 2004) show that providing medical jargon and the corresponding lay\ndefinitions significantly enhances the patients’ comprehension of the EHR notes in both groups (p <0.01).\n11748\nE Algorithm for the Ensemble Model\nAlgorithm 1 Pseudo code for the weighted voting ensemble prediction\nRequire: Validation Set (V), Test Set (T), Trained Models (M)\n1: S ←∅\n2: for Mi in M do\n3: L ←∅\n4: for Vj in V do\n5: XVj , yVj ←Vj\n6: L ←L||Mi(XVj )\n7: end for\n8: S ←S||F1(L, V)\n9: end for\n10: O ←∅\n11: for Tj in T do\n12: L ←∅\n13: for Mi in M do\n14: XTj , yTj ←Tj\n15: L ←L + Si ×Mi(XTj )\n16: end for\n17: for Lk in L do\n18: O ←O||argmax Lk\n19: end for\n20: end for\n21: Return O\nWe first evaluated the performance on the validation set of each model and then set this as the weight of\neach model (line 2 to 11). In the line 1, we set of models’ F1 scores S as ∅. Herein, in line 7, we got the\njth sequence of optimal predicted labels of a model (Mi(XVj )) and appended it to the list of predictions\nL. Then, we calculated F1 score of each model (Mi) for the given validation set V in line 8.\nIn the test set, weighted voting was performed based on the scores of the models (line 11 to 20). In line\n15, we got the result of multiplying the model’s score by the predicted sequence of labels (Si ×Mi(XTj ))\nfor a test input (XTj ). After that, the label with the highest weighted score was selected from each token\nin line 18. Finally, the selected labels O are returned in line 21.\n11749\nF Impact of the Proposed Methods on BioNER Benchmarks\nSetting Entity Type Datasets Vanilla Binary TF MLM TF+MLM\nPretrained\nDisease NCBI disease 87.92 87.62 88.31 88.24 87.53\nBC5CDR disease 83.93 84.10 84.62 84.74 83.96\nDrug/chem BC5CDR chemical 92.07 91.58 92.08 92.09 91.64\nBC4CHEMD 90.06 90.30 90.04 89.97 90.14\nGene/protein BG2GM 82.30 82.87 82.83 82.81 82.67\nJNLPBA 74.95 78.04 77.51 77.41 77.90\nSpecies LINNAEUS 87.59 87.86 85.69 85.81 85.92\nSpecies-800 74.95 75.11 76.88 77.12 76.67\nWiki-trained\nDisease NCBI disease 89.21 85.86 87.90 87.85 88.24\nBC5CDR disease 84.87 85.47 85.23 85.35 85.62\nDrug/chem BC5CDR chemical 91.88 92.17 92.09 92.13 91.84\nBC4CHEMD 90.27 90.27 90.21 90.13 90.50\nGene/protein BG2GM 83.06 83.23 83.07 83.26 83.17\nJNLPBA 77.95 78.34 77.69 78.33 77.70\nSpecies LINNAEUS 89.87 88.67 89.10 88.83 85.85\nSpecies-800 74.85 75.76 75.97 76.12 75.10\nTable 7: Experimental results on proposed methods on BioNER benchmarks. Herein, the values are presented in\nbold if performance is improved in Wiki_setting.\nWe examined the impact of our suggestions on BioNER benchmarks. Herein, we mainly compare\nthe impact of WikiHyperlink span training method. Furthermore, we kept the hyper-parameters of other\nexperiments and we used the F1 score as the main evaluation criterion.\nTable 7 presents the experimental results. We can see that the performances were enhanced in almost all\ncases. Among 40 experimental settings performances were improved on 30 settings. In addition, when we\nconducted a t-test on the performance of the experimental settings, we found that the mean and standard\ndeviation of the settings were significantly different (p <0.005). However, in Species-800 and NCBI\ndisease, overall performance marginally decreased after Wiki_training application, and it can be found\nthat the performance marginally changed in the BC4CHEMED data.\nOn the other hand, additional features did not the affect performance improvement. This is due to\nthe structure of our model. Note that the candidate medical terms are extracted by QuickUMLS and\nencoded as a binary form (see Appendix B). This can be advantageous in the MedJ task that extracts\ncomprehensive medical terms. However, the BioNER task aims to extract entities of specific semantic\ntypes. Therefore, our approach can confuse the NER models. For instance, \"V on Willebrand’s factor\ndeficiency\" is a syndrome name and part of it, \"V on Willebrand’s factor\", is a protein name. In our setting,\n\"V on Willebrand’s factor deficiency\" is input as a medical jargon. However, if a task is gene/protein\nname extraction, the input signal can mislead the model. To ameliorate this issue, we can utilize semantic\ntype information from biomedical concept extractors. Specifically, we can use semantic type embedding\n(Michalopoulos et al., 2021) or a semantic-type span feature (Kwon et al., 2019b) as an additional input.\n11750\nG Additional Analysis of the MLM Score Feature\n(a) MLM scores of UMLS concepts which are jargon\n (b) MLM scores of UMLS concepts which are not jargon\nFigure 3: Histograms of the MLM score feature. The x-axis is the normalized MLM score, and the y-axis is the\nnumber of observations.\nIn this section, we show that MLM features can be valid for extracted UMLS biomedical concepts.\nFigure 3 is a histogram of biomedical concepts for MLM scores. In this case, Figure 3(a) is a histogram\nfor medical jargons, and Figure 3(b) is a histogram for non-jargons. As a result of the experiment, we\nconfirmed that, in the case of non-medical jargon, the histogram showed a heavily tailed distribution in the\nsection with the low MLM score. On the other hand, medical jargon was observed relatively infrequently\nat low MLM scores.\nThe mean and standard deviation of the jargon’s MLM score were 0.43 ±0.14, and the mean and\nvariance of non-jargon concepts were 0.32 ±0.17. As a result of performing the statistical test, we can\nsee that the two distributions were significantly different (p <0.01).\n11751",
  "topic": "Jargon",
  "concepts": [
    {
      "name": "Jargon",
      "score": 0.9588145017623901
    },
    {
      "name": "Hyperlink",
      "score": 0.9020127058029175
    },
    {
      "name": "Computer science",
      "score": 0.8250447511672974
    },
    {
      "name": "Natural language processing",
      "score": 0.6608299612998962
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.578521192073822
    },
    {
      "name": "Language model",
      "score": 0.5222166776657104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5216941237449646
    },
    {
      "name": "Information retrieval",
      "score": 0.49540746212005615
    },
    {
      "name": "Named-entity recognition",
      "score": 0.47014859318733215
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.4405383765697479
    },
    {
      "name": "Span (engineering)",
      "score": 0.42844730615615845
    },
    {
      "name": "World Wide Web",
      "score": 0.29448139667510986
    },
    {
      "name": "Linguistics",
      "score": 0.16391441226005554
    },
    {
      "name": "Web page",
      "score": 0.1386030614376068
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I133738476",
      "name": "University of Massachusetts Lowell",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I166722992",
      "name": "University of Massachusetts Chan Medical School",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1322918889",
      "name": "United States Department of Veterans Affairs",
      "country": "US"
    }
  ]
}