{
  "title": "Pipelines for Social Bias Testing of Large Language Models",
  "url": "https://openalex.org/W4285183888",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2203505503",
      "name": "Debora Nozza",
      "affiliations": [
        "Bocconi University"
      ]
    },
    {
      "id": "https://openalex.org/A2124569955",
      "name": "Federico Bianchi",
      "affiliations": [
        "Bocconi University"
      ]
    },
    {
      "id": "https://openalex.org/A310222905",
      "name": "Dirk Hovy",
      "affiliations": [
        "Bocconi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2769358515",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3011762034",
    "https://openalex.org/W3204712960",
    "https://openalex.org/W2963381846",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2997588435",
    "https://openalex.org/W3216415775",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W4226462293",
    "https://openalex.org/W2942160782",
    "https://openalex.org/W3177468621",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W2954275542",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W3175487198",
    "https://openalex.org/W3101004475",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2903795157",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2980350050",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W4285210581",
    "https://openalex.org/W3175919386",
    "https://openalex.org/W3173465197",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3199411432",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W3174685870"
  ],
  "abstract": "The maturity level of language models is now at a stage in which many companies rely on them to solve various tasks. However, while research has shown how biased and harmful these models are, systematic ways of integrating social bias tests into development pipelines are still lacking. This short paper suggests how to use these verification techniques in development pipelines. We take inspiration from software testing and suggest addressing social bias evaluation as software testing. We hope to open a discussion on the best methodologies to handle social bias testing in language models.",
  "full_text": "Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 68 - 74\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nPipelines for Social Bias Testing of Large Language Models\nDebora Nozza, Federico Bianchi, Dirk Hovy\nBocconi University\nVia Sarfatti 25\nMilan, Italy\n{debora.nozza,f.bianchi,dirk.hovy}@unibocconi.it\nAbstract\nThe maturity level of language models is now\nat a stage in which many companies rely on\nthem to solve various tasks. However, while\nresearch has shown how biased and harmful\nthese models are, systematic ways of integrat-\ning social bias tests into development pipelines\nare still lacking. This short paper suggests how\nto use these verification techniques in devel-\nopment pipelines. We take inspiration from\nsoftware testing and suggest addressing social\nbias evaluation as software testing. We hope to\nopen a discussion on the best methodologies to\nhandle social bias testing in language models.\n1 Introduction\nCurrent language models are now primarily de-\nployed on large infrastructures (e.g., HuggingFace\nrepository1) and used by many practitioners and\nresearchers with few lines of code. This releasing\nmechanism has brought tremendous value to the\ncommunity as researchers everywhere can access\nmodels, download them on their laptops, and run\nexperiments. However, these models are quickly\nadopted without complete understanding their pos-\nsible limitations (Bianchi and Hovy, 2021).\nRecent literature is now rich of papers that\ndemonstrate how social bias is embedded in large\nlanguage models and propose many different ver-\nification and validation datasets (e.g., May et al.,\n2019; Nozza et al., 2021; Nadeem et al., 2021, in-\nter alia). Researchers and practitioners can use all\nthese contributions to understand if a model is safe\nto use or not. We will refer to these works and\nthe datasets used as verification as social bias tests\nfrom this point on.\nThis literature often misses the long-term goal.\nWhat is the point of having so many social bias\ntests that effectively capture different aspects of\nthe problem if we do not find a systematic way\nof using them? Indeed, this work is also inspired\n1https://huggingface.co/\nby the recent approaches and methodologies de-\nfined to provide more comprehensive evaluations\nof models (Ribeiro et al., 2020; Chia et al., 2022).\nIndeed, other computer science fields have devel-\noped insights into how to handle testing. Software\ndevelopment has long been wrestling with the need\nfor good evaluation practices for source code. For\nexample, Continuous Integration and Continuous\nDeployment (CI/CD) is a general methodology in\nsoftware development. It assumes frequent test-\ning to ensure that the product under development\npasses specific qualitative tests that guarantee it\nis working. In this direction, frequent testing of\nlanguage models can be part of the solution.\nThe main contribution of this short paper is first\nto identify the main recurring themes and the pri-\nmary methodologies of social bias literature. We\nthen suggest a more practical and developmental\ndirection: all these methods can be used the same\nway as tests in software testing pipelines. Unsta-\nble/unsafe software should not go into production,\nwhich is also true for language models.\nWe are aware that a single social bias test can-\nnot provide a complete picture of the problems\nand that we cannot treat a model that passes the\ntests as entirely safe. Nonetheless, we believe that\nsome frequent tests are better than no tests. As a\ncommunity, we need to come together and work\nclosely to stress test these models even during the\ndevelopment phase.\nContributions Our contribution is twofold: we\nfirst give an overview of the literature on social bias\ntests and explore the main themes and methods.\nWe then suggest that this literature can be used in\npractical contexts to frequently evaluate language\nmodels to understand better how the tools we use\ncan be harmful. With this work, we hope to start\na discussion on the best methodologies to handle\nsocial bias testing in language models as we believe\nthis is a fundamental step to sustain the future and\ncorrect usage of these technologies.\n68\n2 Existing Social Bias tests\nAn overview of bias in NLP has been presented\nin several work (Blodgett et al., 2020; Shah et al.,\n2020; Hovy and Prabhumoye, 2021; Sheng et al.,\n2021; Stanczak and Augenstein, 2021). Here, we\nfocus on the approaches proposed for contextual\nembeddings. We illustrate the main themes that\nhave driven the developed of social bias tests. The\ncategories we are going to describe are not mutually\nexclusive, however they showcase in a coherent\nmanner what has been done in the literature.\n2.1 Word List-based\nSeveral studies have been conducted to analyse and\ndetermine the level of bias in static word embed-\ndings in binary and multi-class scenarios (Boluk-\nbasi et al., 2016; Caliskan et al., 2017; Garg et al.,\n2018; Swinger et al., 2019; Manzini et al., 2019;\nLauscher and Glavaš, 2019; Gonen and Goldberg,\n2019). Several works applied these bias evaluations\nto contextualized models by extracting static word\nembeddings for them (Basta et al., 2019; Lauscher\net al., 2021; Wolfe and Caliskan, 2021).\nInspired by gender bias metrics for word embed-\ndings, May et al. (2019) proposed the Sentence En-\ncoder Association Test (SEAT), a template-based\ntest founded on the Word Embedding Association\nTest (WEAT) (Caliskan et al., 2017). Afterward,\nLiang et al. (2020) used SEAT for measuring bias,\nalso considering the religious dimension.\n2.2 Template-based\nTemplate-based approaches exploit the fact that\nBERT-like models are trained using a masked lan-\nguage modeling objective. I.e., given a sentence\nwith omitted tokens indicated as [MASK], they\npredict the masked tokens. The predictions for\nthese [MASK] tokens may provide us with some\ninsight into the bias embedded in the actual rep-\nresentations. We can generate templates in two\ndifferent ways. First, by accounting for certain\ntargets (e.g., gendered words) and attributes (e.g.\ncareer-related words) (Kurita et al., 2019; Zhang\net al., 2020; Dev et al., 2020). This enable, for\nexample, to compute the association between the\ntarget male gender and the attribute programmer,\nby feeding “[MASK] is a programmer” to BERT,\nand compute the probability assigned to the sen-\ntence “he is a programmer”. Another option is\nto create templates coupling protected group tar-\ngets with neutral predicates (e.g., “works as”, “is\nknown for”). For example, we can ask BERT to\ncomplete “the woman is known for [MASK]” or\n“the girl worked as [MASK].” Then, it is possible to\nexploit lexicons (Nozza et al., 2021, 2022), or hate\nspeech (Ousidhoum et al., 2021; Sheng et al., 2019)\nand sentiment classifiers Hutchinson et al. (2020);\nHuang et al. (2020) to obtain a social bias score\nfrom the template-based generated text. Ideally,\nusing a classifier lets us test the data more easily\nand accurately than lexicons.\nThe same approach can be applied to natural\nlanguage generation models (Sheng et al., 2019;\nHuang et al., 2020). The models are not fed with\na masked token but are asked to complete the tem-\nplate. So, instead of a single word, they return a set\nof words.\nAn interesting case has been proposed by\nChoenni et al.. They look into what kinds of stereo-\ntyped information are collected by LLMs exploit-\ning a dataset comprising stereotypical attributes\nfor various social groups. The dataset was cre-\nated by feeding search engines queries that already\nimply a stereotype about a specific social group\n(e.g., ‘Why are Asian parents so’). Then, the au-\nthors count how many of the stereotypes found by\nthe search engines are also encoded in the LLMs\nthrough masked language modeling.\n2.3 Crowdsourced-based\nFew works have collected datasets to compute bias\nscores. Nadeem et al. (2021) presented StereoSet,\na crowdsourced English dataset to measure stereo-\ntypical biases in four domains: gender, profession,\nrace, and religion. Nangia et al. (2020) introduced\nCrowS-Pairs, a crowdsourced benchmark compris-\ning 1508 examples that cover stereotypes dealing\nwith nine types of bias. Both Nadeem et al. (2021);\nNangia et al. (2020) proposed a metric to measure\nfor how many examples the model prefers stereo-\ntyped sentences over less stereotyped sentences.\n2.4 Social Media-based\nBarikeri et al. (2021) propose a bias evaluation\nframework for conversational LLMs usingREDDIT -\nBIAS , an English conversational data set grounded\nin real-world human conversations from Reddit.\nThe authors propose a perplexity-based bias mea-\nsure meant to quantify the amount of bias in genera-\ntive language models along several bias dimensions.\nGehman et al. (2020) focus on collecting prompts\nfrom the OpenWebText Corpus (Gokaslan and Co-\nhen, 2019) and annotating them with the Perspec-\ntive API to evaluate the toxicity of the messages.\n69\nThese messages are then split in half (a prompt and\na continuation) and are used to study, for exam-\nple, whether a model generates toxic continuations\nfrom a non-toxic prompt.\n2.5 Discussion\nWhile many social bias tests have been provided\nin the literature, they differ in methodology, cov-\nered languages, and protected groups. Most works\nare on English. Only (Nozza et al., 2021; Ousid-\nhoum et al., 2021) considered languages beyond\nEnglish. The majority of work focused on gender\nbias, and only a few investigated an extensive range\nof targets (Nangia et al., 2020; Nadeem et al., 2021;\nOusidhoum et al., 2021; Barikeri et al., 2021). We\nalso found that Hutchinson et al. (2020); Huang\net al. (2020) did not provide data or code publicly.\nBlodgett et al. (2021) presented a critical review\nof some social bias tests and found significant is-\nsues with noise, unnaturalness, and reliability of\nthe some work (Nangia et al., 2020; Nadeem et al.,\n2021). Finally, it is important to highlight that so-\ncial biases are different depending on the cultural\nand historical context of application of the language\nmodel.\nThis brief analysis demonstrates that no existing\nsocial bias test is universal. While we may fill this\nresearch gap in the future, for now, we suggest\nusing more than one test has to be used to measure\nbias.\n3 Integration\nWe describe the different modalities that can be\nused to integrate social bias tests into development\npipelines.\n3.1 Continuous Social Bias Verification\nSoftware testing is at the heart of software devel-\nopment. Without good evaluation, software easily\nbreaks in production, causing economic damage to\ncompanies.\nMost of the checks currently run to test language\nmodels are structural. For example, does it produce\noutputs correctly? Once fine-tuned, are the results\nwe get in a sensible range? We suggest that tests\nshould cover social biases.\nWe take inspiration from software testing and\nsuggest testing methodologies for language models.\nIn a CI/CD (continuous integration and continuous\ndevelopment) setting, code is continuously pushed\ninto the repository and tested to ensure the model\nis stable. Software is deployed if and only if tests\nare correctly passed. We believe that we should\nreplicate this pipeline in the development of lan-\nguage models. Every time a new model is released,\nwe can run tests to verify if and how the model is\nhurtful.\nNote that this is indeed a real problem. Many\npipelines are now based on HuggingFace APIs that\ndirectly download the model from the HuggingFace\nHub. Users might not know what happens on the\nbackend: what happens when a model is updated,\nand the user downloads it thinking it is the same\nas the older version? We are not sure how many\nusers keep track of commits and changelogs, and\nthis might create a misunderstanding about which\nmodel is being used and with which training setup.\n3.2 Badge System\nPublishers may help maintain the fairness of the re-\nsearch ecosystem by establishing a badging mecha-\nnism. This approach would increase the likelihood\nthat an LLM will be tested in advance for social\nbiases and that end-users will pay attention to this\nissue.\nHere, we propose a badging system based on the\nACM one2 and the one proposed for the NAACL\n2022 reproducibility track 3. We identified three\npossible badges: Social Bias Evaluated, Social Bias\nAvailable, and Results Validated.\nSocial Bias Evaluated This badge is given to\nLLMs who have successfully run the social bias\ntests. This badge does not require the scores to be\nmade publicly available.\nSocial Bias Available This badge is given to\nLLMs that made the results of social bias tests\nretrievable. We propose to design one badge for\neach implemented social bias test and to show it\nalong with the associated score. We discourage us-\ning badges as binary (i.e., test passed or test failed)\nfor these particular cases. Considering the prob-\nlem as binary might imply that a passing model is\nentirely free of bias, even if this is not the case.\nResults Validated This badge is given to\nLLMs in which the social bias test results were\nsuccessfully attained by a person or team other\nthan the author.\n2https://www.acm.org/\npublications/policies/\nartifact-review-and-badging-current\n3https://2022.naacl.org/blog/\nreproducibility-track/\n70\nThe woman is a [MASK]\nThe boy should work as a [MASK]\n…\nModel is pushed \non an online \nrepository.\nScore\nModel is \ndeveloped \nGitHub Actions\nSocial Bias Evaluation Engine\nNewBERT\nModel is released \nwith badges\nFigure 1: The figure shows an example of the possible integration of Social Bias tests into a development pipeline.\nA model can be developed and trained on a server and pushed online. Then we can use an automation tool (e.g.,\nGithub Actions) to start an evaluation engine that will eventually generate the predictions for the models. Once\nscored, the model can be released online with badges identifying possible issues that one might encounter with the\nmodel.\nBadging is also a standard and straightforward\nsystem to showcase software validity in an online\nrepository. These badges are often used to show\ninformation about the number of downloads, the\ntest coverage, the quality of the documentation and\nallow users to understand the quality of what they\nare using with a quick look.\nFigure 1 shows a possible integration of testing\nfor harms in development pipelines. We can de-\nvelop the models on a local server and push this\nmodel online after training is finished (with Git\nLSF, for example). Pushing should automatically\nstart an evaluation pipeline (something close to\nGithub Actions) that starts an evaluation engine:\nthis engine should load the models and run the\nsocial bias tests. Once the results are collected,\nand the metrics have been scored, the model can\nfinally appear on online repositories with badges\nthat identify if and how the test have been run with\nthe respective scores.\n3.3 Limits of this Integration\nAn open question is if the test should be available\nto the developer of the models. On the one hand,\nreleasing the tests makes it easier for everyone to\nevaluate their models internally before release. On\nthe other hand, this makes it easier to “train on test”\nand hack the system to obtain better scores.\nHiding the test sets from the developer is closer\nto standard Quality And Assurance developers in\ncompanies that are meant to test the interfaces and\nthe code that the developer has built. This approach\nis also in line with challenges that do not share\ntest data and in which models are submitted using\ndocker containers that are then internally evaluated\nand scored. As Goodhart’s law states, “When a\nmeasure becomes a target, it ceases to be a good\nmeasure”. Thus we should be aware that social bias\ntests cannot be the panacea for language models\nproblems. We cannot rely only on a test to assess\nthe validity of a model.4\nAnother point in discussion is that the pipelines\nwe have designed are meant to evaluate intrinsic\nbias in language models. Unfortunately, this does\nnot consider the verification of bias in downstream\napplication: this extrinsic bias has been found to\nbe poorly correlated with the original bias of lan-\nguage models (Goldfarb-Tarrant et al., 2021). How-\never, we want to point out the an additional set of\napplication-specific tests could be used to evaluate\nthe models adapted for these tasks: for example, re-\nsearchers could use hate speech check tests (Dixon\net al., 2018; Nozza et al., 2019; Röttger et al., 2021)\nto verify social biases in hate speech detection mod-\nels.\n4Albeit, this comment is true for any measure we use in\nthe field.\n71\n4 Conclusion\nThis paper proposes to use social bias tests in model\ndevelopment pipelines. We believe that our work\ncan be helpful to make the development of these\nmodels fairer and easier to sustain from an ethical\npoint of view. Future work is needed to answer\nseveral questions about this system. For example,\nwho creates the tests and how can we make sure\nthat these tests can be trusted? It becomes critical\nto involve marginalized communities to develop\nmore sustainable and effective social bias tests.\nAcknowledgements\nThis project has partially received funding from the\nEuropean Research Council (ERC) under the Eu-\nropean Union’s Horizon 2020 research and innova-\ntion program (grant agreement No. 949944, INTE-\nGRATOR), and by Fondazione Cariplo (grant No.\n2020-4288, MONICA). Debora Nozza, Federico\nBianchi, and Dirk Hovy are members of the Mi-\nlaNLP group, and the Data and Marketing Insights\nUnit of the Bocconi Institute for Data Science and\nAnalysis.\nEthical Statements\nWe understand that providing social bias tests as a\nquantifiable indicator for bias carries a significant\nrisk. A low score on a social bias test might be used\nto assert that a model is fully devoid of bias. As\nNangia et al. (2020), we strongly advise against this.\nTests can be an indication of issues. Conversely,the\nabsence of a high score does not necessarily entail\nthe absence of bias. Neither do replace a thorough\ninvestigation of the data.\nReferences\nSoumya Barikeri, Anne Lauscher, Ivan Vuli´c, and Goran\nGlavaš. 2021. RedditBias: A real-world resource for\nbias evaluation and debiasing of conversational lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1941–1955, Online. Association for\nComputational Linguistics.\nChristine Basta, Marta R. Costa-jussà, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 33–39, Florence, Italy. Association\nfor Computational Linguistics.\nFederico Bianchi and Dirk Hovy. 2021. On the gap be-\ntween adoption and understanding in NLP. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3895–3901, Online.\nAssociation for Computational Linguistics.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona,\nSpain, pages 4349–4357.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nPatrick John Chia, Jacopo Tagliabue, Federico Bianchi,\nChloe He, and Brian Ko. 2022. Beyond ndcg: behav-\nioral testing of recommender systems with reclist. In\nCompanion Proceedings of the Web Conference.\nRochelle Choenni, Ekaterina Shutova, and Robert van\nRooij. 2021. Stepmothers are mean and academics\nare pretentious: What do pretrained language models\nlearn about you? In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1477–1491, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nSunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Sriku-\nmar. 2020. On measuring and mitigating biased in-\nferences of word embeddings. In The Thirty-Fourth\nAAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of\nArtificial Intelligence Conference, IAAI 2020, pages\n7659–7666. AAAI Press.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classification. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society, AIES ’18, page 67–73, New York, NY ,\nUSA. Association for Computing Machinery.\n72\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2018. Word embeddings quantify 100\nyears of gender and ethnic stereotypes. Proceedings\nof the National Academy of Sciences, 115(16):E3635–\nE3644.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1926–1940, Online. Association\nfor Computational Linguistics.\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nDirk Hovy and Shrimai Prabhumoye. 2021. Five\nsources of bias in natural language processing. Lan-\nguage and Linguistics Compass, 15(8):e12432.\nPo-Sen Huang, Huan Zhang, Ray Jiang, Robert Stan-\nforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani\nYogatama, and Pushmeet Kohli. 2020. Reducing sen-\ntiment bias in language models via counterfactual\nevaluation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 65–83,\nOnline. Association for Computational Linguistics.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Den-\nton, Kellie Webster, Yu Zhong, and Stephen Denuyl.\n2020. Social biases in NLP models as barriers for\npersons with disabilities. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5491–5501, Online. Association\nfor Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nAnne Lauscher and Goran Glavaš. 2019. Are we con-\nsistently biased? multidimensional analysis of biases\nin distributional word vectors. In Proceedings of the\nEighth Joint Conference on Lexical and Computa-\ntional Semantics (*SEM 2019) , pages 85–91, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020. Towards debiasing sentence\nrepresentations. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5502–5515, Online. Association for\nComputational Linguistics.\nThomas Manzini, Lim Yao Chong, Alan W Black, and\nYulia Tsvetkov. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing multi-\nclass bias in word embeddings. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 615–621, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2021.\nHONEST: Measuring hurtful sentence completion\nin language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2398–2406, Online.\nAssociation for Computational Linguistics.\n73\nDebora Nozza, Federico Bianchi, Anne Lauscher, and\nDirk Hovy. 2022. Measuring Harmful Sentence Com-\npletion in Language Models for LGBTQIA+ Individ-\nuals. In Proceedings of the Second Workshop on\nLanguage Technology for Equality, Diversity and In-\nclusion. Association for Computational Linguistics.\nDebora Nozza, Claudia V olpetti, and Elisabetta Fersini.\n2019. Unintended bias in misogyny detection. WI\n’19, page 149–155, New York, NY , USA. Association\nfor Computing Machinery.\nNedjma Ousidhoum, Xinran Zhao, Tianqing Fang,\nYangqiu Song, and Dit-Yan Yeung. 2021. Probing\ntoxic content in large pre-trained language models.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4262–4274, Online. Association for Computational\nLinguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nPaul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak\nWaseem, Helen Margetts, and Janet Pierrehumbert.\n2021. HateCheck: Functional tests for hate speech\ndetection models. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 41–58, Online. Association for\nComputational Linguistics.\nDeven Santosh Shah, H. Andrew Schwartz, and Dirk\nHovy. 2020. Predictive biases in natural language\nprocessing models: A conceptual framework and\noverview. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5248–5264, Online. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. Societal biases in language\ngeneration: Progress and challenges. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4275–4293, Online.\nAssociation for Computational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\narXiv preprint arXiv:2112.14168.\nNathaniel Swinger, Maria De-Arteaga, Neil Thomas\nHeffernan IV , Mark DM Leiserson, and Adam Tau-\nman Kalai. 2019. What are the biases in my word\nembedding? In Proceedings of the 2019 AAAI/ACM\nConference on AI, Ethics, and Society , AIES ’19,\npage 305–311, New York, NY , USA. Association for\nComputing Machinery.\nRobert Wolfe and Aylin Caliskan. 2021. Low frequency\nnames exhibit bias and overfitting in contextualizing\nlanguage models. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 518–532, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nHaoran Zhang, Amy X. Lu, Mohamed Abdalla,\nMatthew McDermott, and Marzyeh Ghassemi. 2020.\nHurtful words: Quantifying biases in clinical con-\ntextual word embeddings. In Proceedings of the\nACM Conference on Health, Inference, and Learn-\ning, CHIL ’20, page 110–120, New York, NY , USA.\nAssociation for Computing Machinery.\n74",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7103931307792664
    },
    {
      "name": "Pipeline transport",
      "score": 0.6331450939178467
    },
    {
      "name": "Software",
      "score": 0.47775617241859436
    },
    {
      "name": "Capability Maturity Model",
      "score": 0.47554442286491394
    },
    {
      "name": "Development testing",
      "score": 0.44314807653427124
    },
    {
      "name": "Software development",
      "score": 0.4191858172416687
    },
    {
      "name": "Data science",
      "score": 0.3826374113559723
    },
    {
      "name": "Software quality",
      "score": 0.3792836368083954
    },
    {
      "name": "Software engineering",
      "score": 0.36607372760772705
    },
    {
      "name": "Programming language",
      "score": 0.15749385952949524
    },
    {
      "name": "Engineering",
      "score": 0.143521249294281
    },
    {
      "name": "Environmental engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71209653",
      "name": "Bocconi University",
      "country": "IT"
    }
  ]
}