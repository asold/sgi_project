{
    "title": "Zero-shot Visual Question Answering with Language Model Feedback",
    "url": "https://openalex.org/W4385572364",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5001058167",
            "name": "Yifan Du",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5100363220",
            "name": "Junyi Li",
            "affiliations": [
                "Université de Montréal",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5030566556",
            "name": "Tianyi Tang",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5037145565",
            "name": "Wayne Xin Zhao",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5025631695",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963101081",
        "https://openalex.org/W4377121433",
        "https://openalex.org/W4281944818",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W3167118264",
        "https://openalex.org/W4281392527",
        "https://openalex.org/W3133644679",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W4285300583",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4312107707",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4226277721",
        "https://openalex.org/W4296001058",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W4320561490",
        "https://openalex.org/W4226452284",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4214717370",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4226321975",
        "https://openalex.org/W3016211260",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W4385573003",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W4296566403",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W4385574156",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3207493267",
        "https://openalex.org/W4312846625",
        "https://openalex.org/W2947312908",
        "https://openalex.org/W4221166832",
        "https://openalex.org/W4292945941",
        "https://openalex.org/W4281633595"
    ],
    "abstract": "In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM. To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM). Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available at https://github.com/RUCAIBox/LAMOC.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 9268–9281\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nZero-shot Visual Question Answering with Language Model Feedback\nYifan Du1,4, Junyi Li1,3, Tianyi Tang1, Wayne Xin Zhao1,4 /Letterand Ji-Rong Wen1, 2,4\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3DIRO, Université de Montréal\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\n{yifandu1999, batmanfly}@gmail.com\nlijunyi@ruc.edu.cn, steventianyitang@outlook.com\nAbstract\nIn this paper, we propose a novel language\nmodel guided captioning approach, LAMOC ,\nfor knowledge-based visual question answer-\ning (VQA). Our approach employs the gener-\nated captions by a captioning model as the con-\ntext of an answer prediction model, which is a\nPre-trained Language model (PLM). As the ma-\njor contribution, we leverage the guidance and\nfeedback of the prediction model to improve\nthe capability of the captioning model. In this\nway, the captioning model can become aware\nof the task goal and information need from the\nPLM. To develop our approach, we design two\nspecific training stages, where the first stage\nadapts the captioning model to the prediction\nmodel (selecting more suitable caption propo-\nsitions for training) and the second stage tunes\nthe captioning model according to the task goal\n(learning from feedback of the PLM). Exten-\nsive experiments demonstrate the effectiveness\nof the proposed approach on the knowledge-\nbased VQA task. Specifically, on the challeng-\ning A-OKVQA dataset, LAMOC outperforms\nseveral competitive zero-shot methods and even\nachieves comparable results to a fine-tuned\nVLP model. Our code is publicly available\nat https://github.com/RUCAIBox/LAMOC.\n1 Introduction\nRecently, pre-trained language models (PLMs) (De-\nvlin et al., 2019; Brown et al., 2020), especially\nlarge language models (Zhao et al., 2023) have\ndemonstrated excellent capabilities in solving tasks\nthat require background knowledge or complex\nreasoning, such as commonsense reasoning (Sap\net al., 2019; Rajani et al., 2019) and logical reason-\ning (Wei et al., 2022; Kojima et al., 2022). Inspired\nby these successes, recent studies have proposed\nutilizing PLMs1 to solve complex vision-language\n/LetterCorresponding author.\n1In this paper, PLMs refer to the models trained on text-\nonly corpus, instead of the text encoder/decoder in vision-\nlanguage pre-trained (VLP) models, which typically have a\nweaker reasoning capacity in linguistic content.\nCaptioningModel\nA person taking a picture of cupcakes on a camera.\nPLM\nWhat is the white substance on top of the cupcakes? Biscuit\nFigure 1: An example that a captioning model (BLIP)\nfails to provide suitable descriptions for a prediction\nmodel (FLAN-T5) of a question in A-OKVQA dataset.\ntasks, exemplified by the task of knowledge-based\nvisual question answering (VQA) that aims to an-\nswer open-ended questions given an image based\non outside knowledge (Schwenk et al., 2022). It has\nbeen shown that PLM-enhanced approaches (Gui\net al., 2022; Lin et al., 2022) typically lead to an im-\nproved performance on the knowledge-based VQA\ntask than pure vision-language pre-trained (VLP)\nmodels (Schwenk et al., 2022).\nIn the literature, existing PLM-enhanced VQA\napproaches can be roughly categorized into two\nlines. The first line of research focuses on adapting\nPLMs to the vision modality by introducing spe-\ncific modular networks or training objectives (Tsim-\npoukelli et al., 2021; Liang et al., 2022; Alayrac\net al., 2022). However, they usually incur a high\ncomputational cost during pre-training in order\nto effectively integrate a vision encoder into the\nPLM. As another line of research, several studies\naim to reduce the cost of tuning PLMs in vision-\nlanguage tasks by utilizing PLMs in a zero-shot or\nfew-shot manner. They typically generate a cap-\ntion for an image using a captioning model (e.g., a\nfine-tuned VLP model), and employ the generated\ncaption as the context (e.g., prompt) to assist PLMs\nin question answering (Yang et al., 2022; Tiong\net al., 2022; Guo et al., 2022). Such an approach\nis training-free and can be generally applied with\n9268\nvarious PLMs.\nHowever, in these existing zero-shot or few-shot\nmethods, the captioning model is unaware of both\ntask goal and information need for the integrated\nPLM. They directly reuse the captioning model\nfine-tuned on caption datasets. As a result, the gen-\nerated captions tend to be less informative for the\nVQA task, even irrelevant to the question. Figure 1\npresents an example that an inappropriate caption\nleads to an incorrect answer generated by the PLM.\nAs we can see, the question is highly related to key-\nwords “icing” or “frosting”, while the captioning\nmodel misses these information and generates a\ngeneral description.\nTo address this issue, we propose LAMOC : a\nnovel LAnguage MOdel guided Captioning ap-\nproach for the VQA task. The key idea is to lever-\nage the guidance and feedback of the prediction\nmodel (i.e., the PLM) to improve the capability\nof the captioning model, so that it can be aware\nof the task goal and information need, and assist\nthe prediction model in answer prediction. Our\napproach is specially designed with two gradual\ntraining stages. At the first stage, the captioning\nmodel is trained to align to the prediction model,\nin which the prediction model selects captions that\nare more pertinent to a given question from multi-\nple propositions generated by the captioning model.\nThese selected captions are informative and can be\nused to fine-tune the captioning model to generate\ninformative captions. At the second stage, since\nthe generated caption is used by the PLM as direct\nevidence for VQA, we employ the feedback from\nthe PLM as reward signals to train the captioning\nmodel via reinforcement learning. During training,\nonly the captioning model is tuned while the PLM\nis fixed, which significantly reduces the compu-\ntational costs. Meanwhile, since the feedback is\nfrom PLM, both training stages do not require any\nlabeled data.\nOur contributions can be summarized as follows:\n(1) We proposeLAMOC , a novel approach for train-\ning captioning models to generate informative cap-\ntions that can assist PLMs in VQA tasks; (2) Using\na small number of randomly sampled unlabeled\n(image, question) pairs, LAMOC consistently out-\nperforms several competitive zero/few-shot base-\nlines without PLM feedback on two knowledge-\nbased VQA datasets: OK-VQA and A-OKVQA;\n(3) We have demonstrated the effectiveness of our\nmethod on PLMs of varying scales, from 223M\nto 11B. This not only confirms the robustness of\nour approach but also demonstrates its potential for\ngeneralization to Large Language Models (LLMs).\n2 Related Work\nPLMs for VQA. After training on large corpora,\nPLMs exhibit surprising abilities, such as chain-\nof-thought reasoning (Wei et al., 2022), in-context\nlearning (Brown et al., 2020), and instruction fol-\nlowing (Chung et al., 2022), which cannot be ob-\ntained by vision-language pre-training. Thus, some\nworks adopt PLM to perform VQA and obtain\npromising results. One line of research combines\na PLM and a vision encoder and trains them end-\nto-end. Frozen (Tsimpoukelli et al., 2021) and\nLiang et al. (2022) train a visual encoder or a mod-\nular network and keep the PLM frozen to retain its\npowerful abilities. Flamingo (Alayrac et al., 2022)\nelaborates the model architecture to combine the\nvision and language models and scales the model\nsize to 80B. Another line of research tries to deploy\nPLMs on VQA tasks in a few-shot/zero-shot man-\nner. PICa (Yang et al., 2022) and Img2Prompt (Guo\net al., 2022) translate the image to captions or tags\nand employ GPT-3 to answer a question by in-\ncontext learning. PNP-VQA (Tiong et al., 2022)\ngenerates question-related captions and utilizes a\nQA model (Khashabi et al., 2022) for answer pre-\ndiction. This type of work does not require extra\ntraining and can be adapted to new PLMs. Our\nwork follows the second paradigm and is an exten-\nsion of these works.\nLearning from Feedback. A regular paradigm\nto train a model is defining a loss function and op-\ntimizing it. However, certain objectives, such as\ncoherence, diversity, and toxicity in text generation,\nmay not be easily incorporated into the loss func-\ntion and learned in an end-to-end manner (Paulus\net al., 2018; Pang and He, 2021). Thus, explicit\nfeedback on model output is regarded as a learn-\ning signal to assist in training. Campos and Sh-\nern (2022) utilize a PLM’s refinement and human\nfeedback to fine-tune a summary model. Wang\net al. (2022c) leverage compiler feedback to im-\nprove the compilability of programs generated by\nthe language model. Ouyang et al. (2022) align a\nlanguage model with the user’s intention through\nreinforcement learning from human feedback. We\nborrow idea from these works, but our feedback\ncomes from a PLM instead of humans, thus saving\nthe annotation cost.\n9269\n3 Method\nIn this section, we present the proposed LAMOC :\nLAnguage MOdel guided Captioning method for\nVQA. The overall architecture of LAMOC is de-\npicted in Figure 2.\n3.1 Overview of Our Approach\nIn this work, we study the task of visual question\nanswering (VQA). Given an image-question pairx:\n⟨xi,xq⟩, the task goal is to predict a correct answer\nyto the question xq given the image xi. Following\nprior studies (Yang et al., 2022; Tiong et al., 2022),\nwe adopt a captioning-based approach for VQA,\nin which a captioning model generates auxiliary\ncaptions for helping answer prediction. Formally,\nwe represent the above idea in a probabilistic way:\np(y|xi,xq) (1)\n=\n∑\nz∈Z\np(z|xi,xq; ΘC)  \ncaption generation\n·p(y|xq,z; ΘP )  \nanswer prediction\n,\nwhere a captioning model ΘC firstly generates an\nauxiliary captions z, and then a prediction model\nΘP predicts an answer candidate y based on the\ncaption z and the question xq. We evaluate this\nprobability by iterating over a set of generated cap-\ntions. Here, we consider an unsupervised setting:\nno labeled answer data is available. Although there\nis no labeled answers, we assume that a small num-\nber of image-question pairs can be obtained for\ntraining (no overlapping with the task dataset).\nTo instantiate this probabilistic approach, we\nadopt a vision-language pre-trained (VLP) model,\ni.e., BLIP (Li et al., 2022b), as the captioning\nmodel, and a pre-trained language model (PLM),\ni.e., FLAN-T5-XXL (Chung et al., 2022), as the\nprediction model. The prediction model ΘP is ex-\npected to fulfill the task by accurately predicting\nthe answer, while the captioning model ΘC plays\nan assisted role by providing informative evidence\nfor ΘP . In our approach, the captioning model\nΘC can be tuned while the prediction model ΘP is\nfixed during optimization. By leveraging the unla-\nbeled image-question pairs (without the labeled an-\nswers), we let the two models cooperate with each\nother: the captioning model generates informative\nevidence for helping answer prediction, and the\nprediction model provides task-specific guidance\nand feedback to improve the captioning model.\nTo optimize our approach, we design a gradual\ntraining process including two stages: (1) caption-\ning adaptation aims to adjust ΘC to produce infor-\nmative captions that are suitable for ΘP (§3.2.1),\nand (2) feedback-based learning aims to opti-\nmize ΘC according to task-specific feedback from\nΘP (§3.2.2). Once the captioning model is well\ntrained, we employ the prediction model for pre-\ndicting the final answer as in Eq. (1), based on the\ncaptions provided by the captioning model (§3.3).\nNext, we introduce these parts in details.\n3.2 Language Model Guided Captioning\nThe key of our approach (Eq. (1)) is to train an\neffective captioning model ΘC for improving the\ncapability of the prediction model ΘP on VQA.\nConsidering that there are no labeled answers, we\nemploy the prediction model to provide guidance\nand feedback to optimize the captioning model.\n3.2.1 Captioning Adaptation\nSince the captioning model is originally intended\nto describe the given image, it may not be in suited\nform to assist the prediction model. Thus, we pro-\npose a captioning adaptation strategy that tunes the\ncaptioning model to fit the prediction model.\nCaption Propositions. We first sample nimage-\nquestion pairs from VQAv2 (Goyal et al., 2017),\nwhich is a large VQA dataset containing more than\n1M questions and does not overlap with our task\ndataset. Then we employ the captioning model\nto propose kcaptions for each image by nucleus\nsampling (Holtzman et al., 2019). Among these\ncaptions, some may be better suited for the predic-\ntion model than the rest. We would like to identify\nsuch captions and use them to refine the captioning\nmodel.\nInstruction-based Captions Selection. Since the\nprediction model is developed based on the FLAN-\nT5-XXL, it has encoded a large amount of knowl-\nedge in a massive number of parameters. We design\nthe following instruction to prompt FLAN-T5-XXL\nto identify more informative captions:\n“Question: [QUESTION] Caption: [CAPTION]\\n\nTo what degree does the caption relate to the ques-\ntion:\\n A: 0%\\n B: 25%\\n C: 50%\\n D:75%”.\nGiven the above prompt, FLAN-T5-XXL will\ngenerate a corresponding option among the set\n{A,B,C,D }. Such an option reflects the corre-\nlation between the caption and question, and the\ncaptions with the predicted option “ D:75%” are\nmore relevant to the question. Since the options are\nmade by the prediction model itself, they tend to be\n9270\n1. The indiancommunity of some people…2. Many people covered by white cloth…3. Mendressed in uniforms carrying whiteSheets under their hats…\nCaptioningModel\n1. A boy with a beanie is playing soccer.2. A young man with a ball in a green field.3. A young man in a blue uniform kicks the soccer ball on the field.…\nFilter\nFine-tuneInformative Caption Dataset\nReinforcement LearningReward\nStage I: Captioning Adaptation\nStage II: Feedback-based Learning\nQuestion: What color is his uniform?\nCaptioningModel Question: What region of the world is this?\nAnswer: Asia0.50.30.8\nIndiaAfricaAsia\nPLM\nPLM-Guided Training\nZero-shot Inference Majority V oting\nPLM\nFigure 2: Overview of our proposed approach LAMOC . In captioning adaption, we utilize a PLM to select\ninformative captions and fine-tune the captioning model on them. When learning from PLM feedback, we regard\nthe feedback from the PLM as reward signals and perform reinforcement learning on the captioning model.\nmore useful for answer prediction. Thus, we keep\nthe captions with the predicted option “ D:75%”\nand discard the rest.\nCaptioning Model Fine-tuning. Via the above\ncaption selection, we can obtain a set of more in-\nformative captions, which are judged by the pre-\ndiction model. Further, we use them to fine-tune\nthe captioning model by optimizing the following\ncross-entropy loss:\nLFT = −1\nT\nT∑\nt=1\nlog p(zt|xi,z<t), (2)\nwhere T is the length of caption, zt denotes the t-th\ntoken of the informative caption selected by FLAN-\nT5-XXL, z<t represents the generated token up to\nthe (t−1)-th step. After fine-tuning, the captioning\nmodel can be better suited for the prediction model.\n3.2.2 Feedback-based Learning\nThough adapting to the prediction model, the cap-\ntioning model is still unaware of the answer pre-\ndiction task for VQA. Thus, we further propose\nconstruct pseudo supervision signals based on the\nPLM feedback from the prediction model. Since\nthe captioning model is only involved as an interme-\ndiate component for answer prediction, we design\na reinforcement learning method for optimizing it.\nReward From PLM Feedback. A key design\nconsideration of reinforcement learning is the def-\ninition of the reward function. In our approach,\ninstead of only generating relevant captions for the\nimages, the effectiveness of the captioning model\nshould be measured by how well it helps find the\ncorrect answer. To achieve this goal, we design the\nfollowing two kinds of reward signals.\n• Prompt-based Reward: A heuristic method is\nutilizing the prompt in §3.2.1 to instruct FLAN-T5-\nXXL to obtain a relevance score, and regard this\nrelevance score as the reward signal:\nr(xq,z) = arg max\ns∈{0,0.25,0.5,0.75}\np(s|xq,z; ΘP ), (3)\nA higher score indicates a more informative cap-\ntion, which is encouraged.\n• Confidence-based Reward: Since there is no\nground-truth answer during training, following\nEq.(1), we employ the probability score of the pre-\ndicted answer (the most confident candidate) given\nby the prediction model as the reward:\nr(xq,z) = p(ˆy|xq,z; ΘP ), (4)\nwhere z is the generated caption by the caption-\ning model and ˆyis the predicted answer from the\nprediction model. In this way, the PLM ( i.e., the\nprediction model) can inform the captioning model\n9271\nabout the informativeness of the generated caption:\nthe larger probability score, the more informative\na caption is, and vice versa. We will verify the\nreliability of these reward designs in §5.1.\nPolicy Gradient. In the framework of reinforce-\nment learning, caption generation can be viewed\nas a sequential decision-making process over the\nwhole vocabulary space. Each generated caption\nwith T tokens is treated as an individual episode\nof length T in this process. At the t-th time step,\nthe state (xi,z<t) is the combination of the image\nand caption generated up to the (t−1)-th token,\nand the action zt is the t-th token to be generated.\nWe employ the policy gradient algorithm (Sutton\nand Barto, 2018) and perform gradient descent to\noptimize the following objective function:\nLRL = −\nT∑\nt=1\nr(xq,z) logp(zt|xi,z<t; Θcap),\n(5)\nwhere z = ⟨z1,...,z t,...,z T ⟩is the caption, and\nr(xq,z) is the reward given by the PLM. Finally,\nwe jointly optimize the two loss functions:\nL= (1 −α) ·LFT + α·LRL, (6)\nwhere αis a weight factor to balance the two parts.\nTo fully exploit the online feedback provided by\nFLAN-T5-XXL, we only optimize the captioning\nadaptation loss function LFT in the initial epoch,\nwhile the reinforcement learning loss functionLRL\nis optimized throughout the training process.\n3.3 Answer Prediction\nAt inference time, we utilize the updated caption-\ning model to assist the prediction model in an-\nswering questions, by calculating the probability\np(y|xq,z; ΘP ). To increase the diversity of cap-\ntions and the coverage of answers, we first ran-\ndomly sample 20% patches from the whole image\nat each time and apply top-ksampling (Fan et al.,\n2018) to generate a caption for these patches with\nthe updated captioning model. We repeat this pro-\ncess mtimes to generate mdiverse captions. Then\nwe concatenate each of them with the correspond-\ning question to construct the following prompt:\n“Please answer the following ques-\ntion.\\n[CAPTION]. [QUESTION]”.\nBased on this prompt, the FLAN-T5-XXL is in-\nstructed to propose an answer with greedy decod-\ning. We can take the max-voting strategy over all\nthe generated answers.\nDifferent from previous work on learning from\nfeedback (Campos and Shern, 2022; Wang et al.,\n2022c; Ouyang et al., 2022), our proposed ap-\nproach explores the guidance and feedback from\nthe prediction model instead of human annotations.\nAs we will see in §5.1, our empirical study shows\nthat there exists a negative correlation between the\nnegative log likelihood assigned by a PLM and the\nVQA score of a generated answer. This finding sug-\ngests that the reward r(xq,z) given by PLM can\npotentially serve as a substitute for labeled data to\nimprove the captioning model for the VQA task.\n4 Experiment\nThis section shows the experimental setup and then\nhighlights the main conclusions of our results.\n4.1 Experimental Setup\nTask Datasets. Since our goal is to improve the per-\nformance of PLMs on visual commonsense tasks,\nwe choose two knowledge-based VQA datasets to\nevaluate our method: (1) OK-VQA (Marino et al.,\n2019) contains 5,046 questions in the test set that\nrequire external knowledge resources to answer.\n(2) A-OKVQA (Schwenk et al., 2022) is an aug-\nmented dataset based on OK-VQA, which requires\nadditional types of world knowledge compared to\nOK-VQA. Since the test set of A-OKVQA is not\npublic, we evaluate our method on the validation\nset. We do not test on VQAv2 (Goyal et al., 2017)\nbecause the majority of questions in this dataset\nare largely focused on recognition and simple vi-\nsual detection tasks, which can be done without\nmuch logical reasoning or external knowledge, and\na fine-tuned VLP model could obtain surprising re-\nsults (Wang et al., 2022b,a). We do not use training\ndata to make a fair comparison with other methods.\nBaselines. We divide previous methods into two\ncategories: (1) Methods without extra large-\nscale Vision-Language (V-L) pre-training, which\nmeans the models have not been pre-trained on\nlarge-scale V-L datasets, including PICa (Yang\net al., 2022), PNP-VQA (Tiong et al., 2022),\nImg2Prompt (Guo et al., 2022). LAMOC also be-\nlongs to this category. (2) Methods with extra\nlarge-scale V-L pre-training, which means that\nthe PLM and the vision encoder are jointly trained\non V-L datasets (although the PLM may be fixed,\nit obtains the ability to understand images), includ-\ning VL-T5 (Cho et al., 2021), FewVLM (Jin et al.,\n9272\nEvaluation\nSetting Method Parameters Use\nExtra PLM?\nWith extra V-L\nPre-training?\nOK-VQA\ntest\nA-OKVQA\nval\nModels fine-tuned on training set.\nSupervised\nlearning\nBLIP† 226M ✗ ✗ 37.6 38.5\nPromptCap 175B ✔ ✗ 58.8 58.0\nModels without fine-tuning.\nFew-shot\nFewVLMbase 288M ✗ ✔ 15.0 -\nFewVLMlarge 804M ✗ ✔ 23.1 -\nPICa 175B ✔ ✗ 48.0 -\nZero-shot\nVL-T5no-VQA 288M ✗ ✔ 5.8 -\nVLKDViT-B/16 494M ✔ ✔ 10.5 -\nVLKDViT-B-L/14 713M ✔ ✔ 13.3 -\nFlamingo3B 3B ✔ ✔ 41.2 -\nFlamingo9B 9B ✔ ✔ 44.7 -\nFlamingo80B 80B ✔ ✔ 50.6 -\nFrozen 7B ✔ ✔ 5.9 -\nPNP-VQA3B 3.9B ✔ ✗ 34.1 33.4\nPNP-VQA11B 11.9B ✔ ✗ 35.9 36.0\nImg2Prompt6.7B 8.3B ✔ ✗ 38.2 33.3\nImg2Prompt13B 14.6B ✔ ✗ 39.9 33.3\nLAMOC 11B (Ours) 11.4B ✔ ✗ 40.3 37.9\nTable 1: Results on OK-VQA and A-OKVQA. The methods are categorized by whether they use extra PLM and\nwhether carry out V-L pre-training. The methods in the upper part have been fine-tuned on the training set, while\nthose in the middle and bottom parts have not. All methods using extra PLM keep it frozen. † Instead of first\nfine-tuning BLIP on VQAv2 and then performing task-specific fine-tuning, we directly fine-tune BLIP on two target\ndatasets for a fair comparison.\n2022), VLKD (Dai et al., 2022), Frozen (Tsim-\npoukelli et al., 2021), and Flamingo (Alayrac et al.,\n2022). The above methods do not use or use few\nlabeled data (zero-shot/few-shot). Besides, we in-\nclude two methods, i.e., BLIP (Li et al., 2022b) and\nPromptCap (Hu et al., 2022), which are fine-tuned\non large amounts of labeled data.\nImplementation details. For image captioning, we\nadopt BLIP (Li et al., 2022b) with 446M parame-\nters and load the released checkpoint that has been\nfine-tuned on the COCO 2014 training set (Lin\net al., 2014), which has no overlap with both the\nOK-VQA and A-OKVQA evaluation datasets. For\nthe PLM, we utilize FLAN-T5-XXL (Wei et al.,\n2022), which has been fine-tuned on more than\n1,800 tasks through instructions and stores consid-\nerable world knowledge. We also carry out ex-\nperiments on PLMs with other sizes, from 223M\nto 11B parameters, to demonstrate the robustness\nand generalizability of our approach across PLMs\nwith different sizes. It is noteworthy that the in-\nformative caption dataset used in the captioning\nadaptation stage is selected by FLAN-T5-XXL, be-\ncause the relevance score given by smaller models\nis not reliable, as will be illustrated in §5.1. When\ntraining the captioning model, we select 1,000 (im-\nage, question) pairs without labels from VQAv2\n(about 10% of the amount of training data for our\ntarget datasets), which has no overlap with the OK-\nVQA and A-OKVQA. It is worth noting that these\n1,000 image-question pairs can be sampled from\nany datasets or even be generated, we sample from\nVQAv2 for the sake of reproducibility. The an-\nswers are generated by the PLM auto-regressively,\nwithout access to the pre-defined answer list. We\nconduct experiments with 5 random seeds and re-\nport the average VQA score according to official\nevaluation protocols.\n4.2 Main Results\nTable 1 displays the results of our methods and\nbaselines on OK-VQA and A-OKVQA.\nFirst, LAMOC outperforms all the zero-shot base-\nlines without V-L pre-training on both datasets.\nCompared to previous state-of-the-art, LAMOC\nachieves prominent gains on the challenging A-\nOKVQA dataset (37.9 vs 36.0) and OK-VQA\ndataset (40.3 vs 39.9). Compared to these base-\nlines, our approach does not require additional\nimage-question matching or question generation\nmodules, thus speeding up the inference speed.\nSince Flamingo has been trained on a massive V-L\ndataset, it achieves the best performance among\n9273\nzero-shot methods. It has been reported that large-\nscale V-L pre-training can develop a mapping be-\ntween images and knowledge concepts that can aid\nin knowledge-based VQA (Tiong et al., 2022).\nSecond, LAMOC narrows the gap between meth-\nods with and without fine-tuning, and even achieves\ncomparable results with the fine-tuned VLP model,\ni.e., BLIP. For example, the performance gap be-\ntween PNP-VQA11B and BLIP is 2.5, and has been\ndecreased to 0.6 by LAMOC , which implies the\nimportance of language model feedback.\nFinally, we report the results of our methods\nwith different model sizes in Table 2. When in-\ncreasing the model scale from 223M to 11B, we\nobserve a 1-2 point improvement in VQA scores on\nthe challenging A-OKVQA dataset. This indicates\nthat a larger PLM can not only store more world\nknowledge to assist with question answering, but\nalso provide more accurate feedback to refine the\ncaptioning model. This is further supported by the\nablation study in §5.1.\n5 Analysis\n5.1 The Reliability of Feedback From PLM\n0% 25% 50% 75%\nRelevance score\n(a)\n0\n10\n20\n30\n40\n50VQA score\nbase\nlarge\nXL\nXXL\n[0,0.5] [0.5,1] [1,1.5] [1.5,2]\nNLL of the generated answer\n(b)\n0\n10\n20\n30\n40\n50VQA score\nbase\nlarge\nXL\nXXL\nFigure 3: The relationship between the caption’s re-\nward and the corresponding answer’s VQA score on\nA-OKVQA validation set. Figure (a) reflects the relia-\nbility of prompt-based reward while figure (b) reflects\nthe reliability of confidence-based reward.\nThe main idea of our work is leveraging the\nfeedback of a PLM to guide caption generation,\nso a critical aspect is the reliability of the feed-\nback. L AMOC involves two types of feedback: (1)\nprompt-based reward and (2) confidence-based re-\nward, which will be evaluated independently.\nTo evaluate the reliability of the first type of feed-\nback, we analyze the relation between the VQA\nscore and the relevance score provided by the PLM\non A-OKVQA validation set (Figure 3(a)). We\ncan observe that as the relevance score provided\nby FLAN-T5-XXL increases, the VQA score also\nincreases, indicating that FLAN-T5-XXL is a suit-\nable prediction model for providing accurate feed-\nback and the relevance scores can be regarded as\nreward signals. However, this trend is not observed\nfor the other three models, implying that their feed-\nback is unreliable. As a result, we only use FLAN-\nT5-XXL to select informative captions during cap-\ntioning adaptation.\nTo evaluate the reliability of the second type\nof feedback, we prompt FLAN-T5 to answer the\nquestion conditioned on the captions and plot the\nrelationship between the negative log-likelihood\n(NLL) of the generated answer and its correspond-\ning VQA score. As Figure 3(b) shows, there is a\nnegative correlation between the NLL of the gener-\nated answers and their VQA scores, suggesting that\ncaptions with lower NLL are more informative and\nrelevant to the questions. Therefore, the probability\nof the generated answer is a reliable feedback and\ncan be used as the reward signal during reinforce-\nment learning.\n5.2 The Effectiveness of Two-stage Training\nWhen training the captioning model, we adopt two\ngradual training stages: captioning adaptation and\nfeedback-based learning. In this part, we study\nthe effectiveness of this training strategy and ex-\nplore whether one training stage is more effective\nthan the other. As illustrated in Table 2, different\nmodels benefit from different training objectives.\nFor example, the captioning adaptation stage is\nmore beneficial for FLAN-T5-large, leading to an\nimprovement of about 4 points on OK-VQA. On\nthe other hand, FLAN-T5-XXL benefits the most\nfrom reinforcement learning with prompt-based re-\nwards and obtains more than 4 points improvement\non A-OKVQA. Moreover, the results show that\njointly training the two objectives further boosts\nperformance, highlighting the effectiveness of the\nproposed two-stage training approach.\n5.3 Case Study\nFigure 4 displays three instances of the captions\ngenerated by BLIP and LAMOC , along with the cor-\nresponding answers generated by FLAN-T5-XXL.\nSince LAMOC is trained on the basis of BLIP, the\ndifference can reflect the effect of our method. As\ncan be observed, the captions generated byLAMOC\nare longer and more comprehensive, containing key\ninformation relevant to the question. For example,\nin Figure 4(a), LAMOC generates captions that in-\nclude specific details such as “frosting” and “choco-\nlate”, while BLIP only generates general captions\nabout “donuts” and “box”, without sufficient infor-\n9274\nMethod FLAN-T5-base (223M) FLAN-T5-large (738M) FLAN-T5-XL (3B) FLAN-T5-XXL (11B)\nOK-VQA A-OKVQA OK-VQA A-OKVQA OK-VQA A-OKVQA OK-VQA A-OKVQA\nBLIP caption 20.42 19.46 23.86 28.86 32.36 31.21 38.48 35.06\n+ adaptation 19.72 18.71 27.43 29.19 32.22 31.07 38.35 35.30\n+ RL (prompt) 21.24 19.25 27.29 29.73 32.28 30.63 38.74 37.62\n+ RL (confidence) 21.14 19.74 25.09 28.98 32.02 32.10 40.31 37.85\n+ adaptation + RL 19.72 20.63 24.82 29.84 32.77 32.00 39.72 37.09\nTable 2: Results of different model sizes and different training objectives. “BLIP caption” means feeding captions\ngenerated by BLIP to PLM without captioning adaptation and feedback-based learning. “adaptation” means\ncaptioning adaptation, while “RL (prompt)” means RL with prompt-based reward, and “RL (confidence)” means\nRL with confidence-based reward.\nQ: What kind of coating has been used?\nA: frosting, chocolate, icing, paint \nBLIP captions:\n1. there are a box of assorted donuts in \nit.\n2. a box of seven different type of \ndoughnuts.\nBLIP answer prediction: sugar\n ❌\nLAMOC captions:\n1. box of chocolate and sprinkle glazed \ndonuts each with assorted sprinkled \nsugar, vanilla, and chocolate frosting.\n2. a box of assorted chocolate, pink, \nchocolate frosted doughnuts.\nLAMOC answer prediction: chocolate \nfrosting, chocolate \n ✅\nQ: What is the stuffed animal touching \ninstead of the tennis player?\nA: television, television screen\nBLIP captions:\n1. a teddy bear in a pair of scissors and \na tennis racket.\n2. a stuffed teddy bear wearing a tennis \nuniform.\nBLIP answer prediction : a tennis ball\n ❌\nLAMOC captions:\n1. man in business clothes swinging \ntennis racquet at television player on \nglass background.\n2. a man on television wearing the foot \nof a tennis player.\nLAMOC answer prediction: television\n ✅\nQ: What region of the world is this?\nA: middle east, asia, china, Africa\nBLIP captions:\n1. person standing over by various \npeople sitting.\n2. a group of people that are looking at \nhats.\nBLIP answer prediction : South America, \nNorth America, Europe \n❌\nLAMOC captions:\n1. many people in matching hats are \ncovered by white cloth.\n2. men dressed in uniforms wearing \nmatching printed hats and carrying \nwhite sheets of fabric under their hats.\nLAMOC answer prediction: Asia, Middle \nEast \n✅\n(a) Case 1                                                            (b) Case 2                                                                (c) Case 3\nFigure 4: Example captions and predictions generated by BLIP and L AMOC .\nmation to help answer the question. These results\nhighlight the importance of training the captioning\nmodel under the guidance of PLMs.\nOne concern is that the PLM may generate cor-\nrect answers due to the language bias, not attribut-\ning to the relevant information contained in the\ncaptions. For example, in Figure 4(a), the PLM\nmay generate the answer “chocolate”, even if the\ncaptions do not mention chocolate (Li et al., 2023).\nHowever, since chocolate often co-occurs with\ndonuts in the training corpora, the PLM may asso-\nciate chocolate with donuts and generate it as the\nanswer. In order to check how often such a situa-\ntion happens, we randomly sample 100 questions\nwhere the prediction model gives correct answers.\nFor each question, we manually assess whether\ntheir answer is derived from the caption. Our anal-\nysis reveals that only 6 out of 100 captions are\nirrelevant to the questions, indicating the reliability\nof the captions.\nAnother interesting phenomenon is that the sen-\ntences generated by LAMOC can be grammatically\nincoherent and sometimes incomplete. This indi-\ncates that PLM prompting may not always conform\nto human language patterns, which is consistent\nwith previous studies (Webson and Pavlick, 2022;\nDeng et al., 2022).\nThe ablation study of the level of relevance, the\nnumber of captions, and the influence of different\nprompt designs can be found in appendix B.\n6 Conclusion\nIn this paper, we propose LAMOC , a language\nmodel guided captioning method that improves\na captioning model to generate comprehensive\ncaptions for an image to help answer the ques-\ntion. In order to train such a model, we first\nperform captioning adaptation on a self-generated\ndataset filtered by FLAN-T5-XXL, and then fine-\n9275\ntune the updated captioning model through re-\ninforcement learning from PLM feedback. Our\nmethod, LAMOC , generates captions that are both\ninformative and able to assist PLMs in VQA\ntasks, as demonstrated through experiments on\ntwo knowledge-based VQA datasets. On the chal-\nlenging A-OKVQA dataset, LAMOC substantially\noutperforms previous zero-shot methods and even\nachieves comparable results to a fine-tuned VLP\nmodel. Additionally, we show that LAMOC is gen-\neralizable to PLMs of varying sizes, from 223M to\n11B parameters, demonstrating its potential to be\napplied to LLMs, which we leave as future work.\n7 Limitations\nIn our study, we have demonstrated the effective-\nness of our proposed method on FLAN-T5 with\ndifferent sizes. However, we have not yet evalu-\nated its performance on LLMs, which possess an\neven greater number of parameters and have been\npre-trained on larger corpora, thus potentially pro-\nviding more accurate feedback for both caption\nadaptation and reinforcement learning. Meanwhile,\nit is worth noting that PLMs may contain certain\nbiases, and training based on their feedback may\namplify these biases. As future work, we aim to in-\nvestigate the scalability of our method to LLMs, as\nwell as strategies to mitigate the potential negative\neffects of biases present in PLMs.\nAckownledgement\nThis work was partially supported by National\nNatural Science Foundation of China under Grant\nNo. 62222215, Beijing Natural Science Founda-\ntion under Grant No. 4222027, and Beijing Out-\nstanding Young Scientist Program under Grant No.\nBJJWZYJH012019100020098. Xin Zhao is the\ncorresponding author.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJon Ander Campos and Jun Shern. 2022. Training lan-\nguage models with language feedback. In ACL Work-\nshop on Learning with Natural Language Supervi-\nsion. 2022.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In International Conference on Machine Learn-\ning, pages 1931–1942. PMLR.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nWenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun\nLiu, and Pascale Fung. 2022. Enabling multimodal\ngeneration on CLIP via vision-language knowledge\ndistillation. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 2383–2395. Association for\nComputational Linguistics.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\narXiv preprint arXiv:2205.12548.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889–898. Association for Computational Lin-\nguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904–6913.\n9276\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alexan-\nder Hauptmann, Yonatan Bisk, and Jianfeng Gao.\n2022. KAT: A knowledge augmented transformer\nfor vision-and-language. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022, pages 956–968. As-\nsociation for Computational Linguistics.\nJiaxian Guo, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Boyang Li, Dacheng Tao, and\nSteven CH Hoi. 2022. From images to textual\nprompts: Zero-shot vqa with frozen large language\nmodels. arXiv preprint arXiv:2212.10846.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nYushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi,\nNoah A Smith, and Jiebo Luo. 2022. Promptcap:\nPrompt-guided task-aware image captioning. arXiv\npreprint arXiv:2211.09699.\nWoojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen,\nand Xiang Ren. 2022. A good prompt is worth\nmillions of parameters: Low-resource prompt-based\nlearning for vision-language models. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n2763–2775. Association for Computational Linguis-\ntics.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Ha-\njishirzi. 2022. Unifiedqa-v2: Stronger general-\nization via broader cross-format training. CoRR,\nabs/2202.12359.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil-\nvio Savarese, and Steven C. H. Hoi. 2022a. LA VIS:\nA library for language-vision intelligence. CoRR,\nabs/2209.09019.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022b. BLIP: bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 12888–12900.\nPMLR.\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Eval-\nuating object hallucination in large vision-language\nmodels. CoRR, abs/2305.10355.\nSheng Liang, Mengjie Zhao, and Hinrich Schütze. 2022.\nModular and parameter-efficient multimodal fusion\nwith prompting. In Findings of the Association for\nComputational Linguistics: ACL 2022, Dublin, Ire-\nland, May 22-27, 2022, pages 2976–2985. Associa-\ntion for Computational Linguistics.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nYuanze Lin, Yujia Xie, Dongdong Chen, Yichong\nXu, Chenguang Zhu, and Lu Yuan. 2022. RE-\nVIVE: regional visual representation matters in\nknowledge-based visual question answering. CoRR,\nabs/2206.01201.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition , pages\n3195–3204.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nRichard Yuanzhe Pang and He He. 2021. Text genera-\ntion by learning from demonstrations. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 4932–4942. Association\nfor Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 33, pages\n3027–3035.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\n9277\nA-OKVQA: A benchmark for visual question answer-\ning using world knowledge. In Computer Vision -\nECCV 2022 - 17th European Conference, Tel Aviv,\nIsrael, October 23-27, 2022, Proceedings, Part VIII,\nvolume 13668 of Lecture Notes in Computer Science,\npages 146–162. Springer.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction. MIT press.\nAnthony Meng Huat Tiong, Junnan Li, Boyang Li, Sil-\nvio Savarese, and Steven CH Hoi. 2022. Plug-and-\nplay vqa: Zero-shot vqa by conjoining large pre-\ntrained models with zero training. arXiv preprint\narXiv:2210.08773.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 200–212.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. OFA: unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, ICML\n2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning\nResearch, pages 23318–23340. PMLR.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subho-\njit Som, and Furu Wei. 2022b. Image as a foreign\nlanguage: Beit pretraining for all vision and vision-\nlanguage tasks. CoRR, abs/2208.10442.\nXin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li,\nPingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun\nLiu. 2022c. Compilable neural code generation with\ncompiler feedback. In Findings of the Association\nfor Computational Linguistics: ACL 2022, Dublin,\nIreland, May 22-27, 2022, pages 9–19. Association\nfor Computational Linguistics.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 2300–2344. Association for\nComputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 36, pages 3081–\n3089.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\n9278\nAppendix\nA Training Details and Artifacts\nFor LAMOC training, we adopt the officially re-\nleased BLIP captioning checkpoint2 for model ini-\ntialization. For both captioning adaptation and\nreinforcement learning, we adopt the following\nhyper-parameters: learning rate 2e−6, warmup\n600 steps, weight decay 0.05, batch size 8. The\nbalance factor αis set to 0.9. We train the model\nfor 10 epochs and choose the one with the highest\nreward (without labels from the validation set). All\nthe experiments are conducted based on LA VIS (Li\net al., 2022a) under BSD 3-Clause License. The\nA-OKVQA is under the Apache License 2.0.\nB Additional Ablation Study\nB.1 Level of Relevance\nWhen prompting the PLM to give a correlation\nscore for the caption, the level of relevance is part\nof the prompt, thus can influence the result. We try\ndifferent levels for the prompt-based reward and\nthe results are in Table 3. Since four levels gives\nthe highest vqa score, we use four levels in our\nprompt-based reinforcement learning.\nLevel A-OKVQA\nA: 0%; B: 100% 27.25\nA: 0%; B: 50%; C: 100% 28.29\nA: 0%; B: 25%; C: 50%; D: 75% 28.98\nA: 0%; B: 25%; C: 50%; D: 75%; E: 100% 27.96\nTable 3: VQA score of models trained with different\nlevels of prompt-based rewards.\nB.2 Number of Captions\nSince the PLM is \"blind,\" all visual information is\ncarried by the captions. Thus, the number of cap-\ntions is critical for the PLM to answer the question.\nIn Figure 5, we explore the influence of the number\nof captions. Our results indicate that utilizing a\nlarger number of captions leads to improved per-\nformance across various model sizes. Performance\ngains continue to accumulate even when utilizing\n10 captions, leading us to posit that incorporating\nan even greater number of captions would result in\nfurther improvements.\n2https://storage.googleapis.com/\nsfr-vision-language-research/BLIP/models/model_\nlarge_caption.pth\n1 2 3 4 5 6 7 8 9 10\nNumber of captions\n0\n10\n20\n30\n40\n50VQA score\nbase\nlarge\nXL\nXXL\nFigure 5: VQA score with different number of captions\nin the A-OKVQA validation set.\nB.3 Prompt Design\nAnother critical design of our method is instruct-\ning the FLAN-T5 to provide feedback and answer\nquestions, so we explore the effects of different\nformats of instruction in Table 4. We can observe\nthat prompt design has a great impact on the results\nTable 4, which is in line with the conclusion of\nprevious works (Wei et al., 2022).\nPrompt OK-VQA A-OKVQA\nAnswer the following question in\none word. Q: [caption]. [question] 29.53 29.84\nPlease answer the following\nquestion. [caption]. [question] 28.22 29.73\n[caption]. [question] 27.59 27.99\n[caption]. [question] Let’s think\nstep by step. 18.08 28.72\nTable 4: VQA score of the answers generated by FLAN-\nT5-large conditioned on different prompts.\n9279\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9280\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4, Appendix\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n9281"
}