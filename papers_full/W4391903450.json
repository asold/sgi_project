{
    "title": "Accepting the Familiar: The Effect of Perceived Similarity with AI Agents on Intention to Use and the Mediating Effect of IT Identity",
    "url": "https://openalex.org/W4391903450",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3128674739",
            "name": "Naif Alawi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A731681699",
            "name": "Triparna de Vreede",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3170466053",
            "name": "Gert-Jan de Vreede",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3012220842",
        "https://openalex.org/W3142805939",
        "https://openalex.org/W2077233711",
        "https://openalex.org/W2110506823",
        "https://openalex.org/W1889373108",
        "https://openalex.org/W2202316322",
        "https://openalex.org/W3091183950",
        "https://openalex.org/W35427003",
        "https://openalex.org/W2117012826",
        "https://openalex.org/W2145482311",
        "https://openalex.org/W1791587663",
        "https://openalex.org/W2499526086",
        "https://openalex.org/W6669071275",
        "https://openalex.org/W1987258130",
        "https://openalex.org/W6757226841",
        "https://openalex.org/W2052284320",
        "https://openalex.org/W2947308709",
        "https://openalex.org/W6602550886",
        "https://openalex.org/W1994279947",
        "https://openalex.org/W1519180031",
        "https://openalex.org/W2735622052",
        "https://openalex.org/W3092521076",
        "https://openalex.org/W7021068485",
        "https://openalex.org/W4205420544",
        "https://openalex.org/W2913916101",
        "https://openalex.org/W1983271703",
        "https://openalex.org/W6652658619",
        "https://openalex.org/W581221035",
        "https://openalex.org/W2396826584",
        "https://openalex.org/W6683034941",
        "https://openalex.org/W6684011503",
        "https://openalex.org/W6682145910",
        "https://openalex.org/W2105753174",
        "https://openalex.org/W6737305540",
        "https://openalex.org/W2967737202",
        "https://openalex.org/W6647975832",
        "https://openalex.org/W3121406068",
        "https://openalex.org/W2034367814",
        "https://openalex.org/W2904901514",
        "https://openalex.org/W2008542123",
        "https://openalex.org/W4254114958",
        "https://openalex.org/W62824994",
        "https://openalex.org/W4235150884",
        "https://openalex.org/W2998583581",
        "https://openalex.org/W4235678817",
        "https://openalex.org/W4232685510",
        "https://openalex.org/W2153082786",
        "https://openalex.org/W2018669908",
        "https://openalex.org/W1592478671",
        "https://openalex.org/W4230237252",
        "https://openalex.org/W2161410357",
        "https://openalex.org/W4244008358"
    ],
    "abstract": "With the rise and integration of AI technologies within organizations, our understanding of the impact of this technology on individuals remains limited. Although the IS use literature provides important guidance for organization to increase employees’ willingness to work with new technology, the utilitarian view of prior IS use research limits its application considering the new evolving social interaction between humans and AI agents. We contribute to the IS use literature by implementing a social view to understand the impact of AI agents on an individual’s perception and behavior. By focusing on the main design dimensions of AI agents, we propose a framework that utilizes social psychology theories to explain the impact of those design dimensions on individuals. Specifically, we build on Similarity Attraction Theory to propose an AI similarity-continuance model that aims to explain how similarity with AI agents influence individuals’ IT identity and intention to continue working with it. Through an online brainstorming experiment, we found that similarity with AI agents indeed has a positive impact on IT identity and on the intention to continue working with the AI agent.",
    "full_text": "Accepting the Familiar: The Effect of Perceived Similarity with AI Agents on Intention to Use and the Mediating Effect of IT Identity  Naif Alawi Umm Al-Qura University Saudi Arabia naalawi@uqu.edu.sa \nTriparna de Vreede Muma College of Business University of South Florida tdevreede@usf.edu \nGert-Jan de Vreede Muma College of Business University of South Florida gdevreede@usf.edu  Abstract With the rise and integration of AI technologies within organizations, our understanding of the impact of this technology on individuals remains limited. Although the IS use literature provides important guidance for organization to increase employees’ willingness to work with new technology, the utilitarian view of prior IS use research limits its application considering the new evolving social interaction between humans and AI agents. We contribute to the IS use literature by implementing a social view to understand the impact of AI agents on an individual’s perception and behavior. By focusing on the main design dimensions of AI agents, we propose a framework that utilizes social psychology theories to explain the impact of those design dimensions on individuals. Specifically, we build on Similarity Attraction Theory to propose an AI similarity-continuance model that aims to explain how similarity with AI agents influence individuals’ IT identity and intention to continue working with it. Through an online brainstorming experiment, we found that similarity with AI agents indeed has a positive impact on IT identity and on the intention to continue working with the AI agent. 1. Introduction  The advent of Industry 4.0 and the ensuing integration of advanced AI technologies within the organizational workforce has significantly changed the organizational work landscape. The increased proliferation of AI artifacts and Internet-of-Things (IOT) artifacts has strengthened concerns about automation and its impact on the workforce (Müller, 2019). Surveys show that most US citizens (about 85%) favor imposing regulatory restrictions on automation to limit job losses (Gramlich, 2017). This fear of job loss is an important factor leading to resistance to technology implementation in general (Joshi, 1991; Lapointe & Rivard, 2005; Marakas & Hornik, 1996) and AI implementation in particular, and it remains a persistent challenge for organizations (You & Robert Jr, 2018).  As AI technologies become increasingly integrated in human work processes in organizations, behavioral AI research is expanding IS Use theories beyond the utilitarian view by creating new paradigms for human – \nAI collaboration (Baird & Maruping, 2021). The utilitarian view focuses on the human usage of technology as a passive tool rather than humans collaborating with technology that has a certain level of agency itself (Baird & Maruping, 2021; Mouakket, 2015). The human – AI collaborative paradigm necessitates examining the relationship between human and machine by shifting focus towards a collaborative social view where humans work with AI agents as partners. Recognizing the importance of careful adoption of advanced AI agents (see e.g., Lebovitz, et al., 2021; Lebovitz, et al., 2022), we utilize a social lens to investigate the factors that impact humans’ behavior and performance while collaborating with an AI agent. Specifically, our research question focuses on one of the main design dimensions of AI: AI representation: How does an AI agent’s representation influence individuals’ intention to continue working with these agents? Building on Similarity – Attraction theory (SAT) (Byrne, 1971) we propose an AI similarity-continuance model (SCM) that attempts to explain the impact of an AI agent’s representation on individuals’ intentions to continue working with it. With the increased adoption and application of agentic AI in collaborative Human-AI relationships, there is a need to gain a deeper understanding of the intricacies of these relationships. Our research aims to contribute to the growing body of IS research that takes a collaborative, social perspective on the relationship between users and agentic IS artifacts in order to inform designers so that they can (1) create more effective AI systems, and (2) provide users with more effective guidance and support on how to work together with AI most productively.  2. Background 2.1. Towards an Agentic View of AI agents Due to the economic impact of implementation failures on organizations, factors impacting the continued use of information systems has been studied extensively over the past decades (Chuttur, 2009). This resulted in the proliferation of models that investigated and explained individuals’ acceptance of new IS \nProceedings of the 56th Hawaii International Conference on System Sciences | 2023\nPage 206\nURI: https://hdl.handle.net/10125/102653\n978-0-9981331-6-4\n(CC BY-NC-ND 4.0)\n\ntechnologies (Davis, 1989; Venkatesh & Davis, 2000; Venkatesh et al., 2003; Rogers, 2010; Thompson, Higgins, & Howell, 1991; Bhattacherjee, 2001). While many of these models and theories remain relevant, the advent of AI-integrated technology revealed a need for further elaboration of the relationship between humans and AI (Baird & Maruping, 2021; Gursoy, Chi, Lu, & Nunkoo, 2019; Van den Broek, et al., 2021). For instance, the relationship between humans and AI agents is moving from an ‘individual use’ towards a more collaborative relationship (Gursoy et al., 2019; Lu, Cai, & Gursoy, 2019). Also, there is an emerging need for additional constructs that describe the collaborative relationships between humans and AI agents. Finally, a collaborative relationship between humans and AI agents necessitates an investigation into how an AI agent’s actions could influence individuals’ attitudes and behaviors (Baird & Maruping, 2021). Accordingly, the nature of human-AI research is progressively expanding the utilitarian view to include the agentic perspective as well.  2.2. Human–AI Interactions through a Social Lens While the concept of people applying social rules, norms, and etiquettes to computer interactions has been studied for a long time, people’s interaction with AI agents has transformed over the past decade and these interactions are becoming more social (Baird & Maruping, 2021; Eyssel, De Ruiter, Kuchenbrandt, Bobinger, & Hegel, 2012; Schuetz & Venkatesh, 2020; Warta, Kapalo, Best, & Fiore, 2016). For instance, people commonly personify virtual agents by using pronouns typically reserved for humans such as “she” (Gao, Pan, Wang, & Chen, 2018; Purington, Taft, Sannon, Bazarova, & Taylor, 2017). They even compare between Alexa and their family members such as wives or mothers (Gao et al., 2018). Since Nass, Steuer, and Tauber (1994) introduced the Computers Are Social Actors (CASA) paradigm, a great body of knowledge focused on social psychology related factors such as computer anthropomorphism and its impact on several factors (i.e., acceptance, trust, and compliance) to illuminate the boundaries according to which people apply social heuristics when interacting with computers. For instance, Parise, Kiesler, Sproull, and Waters (1999) found that a computer agent’s interface has a significant effect on people’s willingness to cooperate with the agent. Other studies also examined the impact of agents’ anthropomorphic features and found positive effects on compliance (Adam, Wessel, & Benlian, 2020), trust (De Visser et al., 2016) and teamwork (De Visser et al., 2017). Despite increasing research in human-computer interaction (HCI), integrating the social lens into IS use \nand acceptance research has remained limited or narrowly focused. For instance, various studies (Kääriä, 2017; Rietz, Benke, & Maedche, 2019; Suh, Kim, & Suh, 2011; Wagner, Nimmermann, & Schramm-Klein, 2019) have highlighted the role of anthropomorphism and similarity on the acceptance of different types of AI agents such as voice assistants, chatbots, or avatars. However, these studies focused on the “use” aspect of the relationship to explain the effects of anthropomorphism and similarity on acceptance, consequently relegating AI agents to be viewed as tools rather than as collaborative actors (Baird & Maruping, 2021; Schuetz & Venkatesh, 2020). To address this disconnect, we draw from the social-psychology and HCI literature to inform our study. In social psychology, a key concept is that people’s perceptions and behaviors are influenced by others that they interact with (Principles of social psychology, 2015). Building on this notion, we argue that people’s perceptions and behavior can be influenced by how AI agents are represented and by their appearance. We use Similarity Attraction Theory as a theoretical lens since this theory is concerned with appearance and how it affects others’ perceptions. 2.3. Similarity Attraction Theory Similarity Attraction Theory (SAT) posits that individuals are likely to be more attracted to others who hold similar attitudes and beliefs (Byrne, 1971). Similarity is a core construct that has an impact on social interactions. Similarity between individuals positively impacts interactions, cohesion, performance, likeness, and perceived competence (Guéguen, Martin, & Meineri, 2011; Harrison, Price, & Bell, 1998; Phillips, Northcraft, Neale, & relations, 2006; Singh et al., 2015). Similarity can be examined from two levels: surface-level similarity and deep-level similarity (Harrison et al., 1998; Kacmar, Harris, Carlson, & Zivnuska, 2009; Phillips et al., 2006). Surface-level similarity, also known as demographic similarity, refers to the similarity based on the salient characteristics (i.e., gender and ethnicity) between individuals. Deep-level similarity, or attitudinal similarity, refers to the shared beliefs, attitudes, and opinions among individuals (Harrison et al., 1998; Phillips et al., 2006). SAT has been tested in multiple fields and dimensions of similarity including demographical, physical, and preference similarities on attraction (Moon, 1996). SAT has been used in HCI research as well. Y. E. Moon (1996) applied SAT to analyze the influence of computer attitudinal similarity on four different dimensions of attractions (Intellectual Attraction, Social Attraction, Emotional Satisfaction, and Utility). Although limited to attitudinal similarity, \nPage 207\nthis study supports positive relations between similarity and attraction in an HCI context. In another study, You and Robert Jr (2018) studied the impact of a robot’s surface and deep level similarity on people’s trust. Results showed significant effects for deep-level similarity on an individual’s trust. They also found a positive effect of trust on intention to work with a robot. Other HCI studies also suggest a positive relation between attitudinal similarity and positive perceptions towards an AI agent (Bernier & Scassellati, 2010; Park, Jin, & del Pobil, 2012). These findings highlight the importance of considering similarity as a design criterion when investigating the antecedents to intention to work with an AI agent. 2.4. Model and Hypotheses Development Building on SAT, we conceptualize an AI Similarity Continuance Model that aims to explain the impact of the AI agent’s representation on an individual’s perceptions. Specifically, the model links how an AI agent is represented to perceptual factors like identity and perceived value which, in turn, influence an individual’s intention to continue working with the AI agent.  Similarity and Identity: The attraction aspect of SAT has been broadly studied, with some studies using ‘liking’ as an indication for attraction, and others suggesting that the concept of attraction is multidimensional (McCroskey & McCain, 1972; Y. E. Moon, 1996). According to McCroskey and McCain (1972), attraction consist of three dimensions including social (related to closeness), physical (related to likeness), and task (related to work collaboration and dependence). This concept of attraction is analogous to the theoretical concept of identification, or IT identity, in HCI. According to Carter (2012), IT identity refers to “the set of meanings an individual attaches to the self in relation to IT”. The concept of IT identity does not mean ownership, nor does it mean a person’s virtual identity communicated through IT (Carter, Petter, Grover, & Thatcher, 2020). Rather, it can be viewed as a form of attachment that represents positive affect and response toward IT, parallel to the positive affect an individual experiences when being attracted to another person. Like attraction, IT identity is multidimensional, and it can be manifested through relatedness, emotional energy, and dependence as interconnected dimensions (Carter 2012). Relatedness in IT identity reflects the feeling of connectedness with the technology. Emotional energy reflects the likeness and attachment to the technology and dependence represents an individual’s reliance on a technology. As an individual perceives an AI agent to be like them, we argue that they \nare more likely to identify with the AI agent. Accordingly, we posit: H1: Perceived similarity with an AI agent is positively related to an individuals’ perceived IT identity. IT Identity and Intention to Use: IT identity has also been linked to technology acceptance. According to Carter (2012), as individuals identify with a technology, this identification engenders a positive attitude towards using this technology. Though the focus was on technology use, Carter (2012) showed that IT identity has a positive relation in explaining positive attitude towards using the technology and toward the intention to continue to use it. Drawing from these findings, we posit that identification with an AI agent will positively impact an individual’s intention to continue working with this AI agent. Thus, we hypothesize:  H2: IT identity is positively related to an individual’s behavioral intention to continue working with an AI agent. H3: IT identity will mediate the relationship between similarity and the individual’s behavioral intention to continue working with AI agent. IT identity and Perceived value: When individuals interact with an AI agent, they contemplate the advantages and disadvantages of working with that technology. This process of contemplation and evaluation to forming an attitude towards the technology is referred to as perceived value (Briggs et al., 1998). In the Technology Transition Model (TTM), Briggs et al. (1998) suggest that individuals can derive a value assessment of the technology they work with from a variety of dimensions such as affective or cognitive values. Thus, perceived value is a holistic construct that reflects a general assessment that is formed based on working with the technology. In addition, perceived value accounts for a broad range of positive or negative outcomes from the interaction with the technology. Thus, one can view perceived usefulness as a single dimension of value among many other dimensions such as political, affective, cognitive, or social values within perceived value (Briggs et al., 1998). Given the wide variability in the forms of interaction between individuals and AI agents, we posit that perceived value is a more suitable construct that encompasses different positive and negative value dimensions that individuals can derive from working with a technology. Accordingly, we argue that individuals contemplate and synthesize the benefits and negative outcomes from working with an AI agent and form an overall perception of the value of using the Artifact. We argue that this perceived value is derived from their identification with the AI agent. Thus, we posit: H4: IT identity is positively related to an individual’s perceived value of AI agents. \nPage 208\nPerceived Value and Intention to Use: Perceived value also affects technology acceptance. According to TTM, behavioral intention to use is a function of perceived value. Briggs et al. (1998) suggest that users typically synthesize the benefits and costs of using a technology subconsciously to derive an overall perceived value. When deciding whether to use the technology or not, users would then recall the positive or negative aspects to decide on using the technology. Accordingly, perceived value can be viewed as a factor that is directly related to behavioral intention to use and as a mediator for other factors that would generate this perception of value. As discussed earlier, we expect individuals working with an AI agent to synthesize a perceived value of the technology from their identification with the technology. This perceived value would in turn be a direct and mediating factor that influences an individual’s intention to continue working with an AI agent. Therefore, we postulate: H5: Perceived value of an AI agent is positively related to an individual’s intention to continue working with the AI agent. H6: Perceived value of an AI agent mediates the relationship between IT identity and behavioral intention to continue working with the AI agent. It should be noted that we use the term similarity in its general sense that encompasses both demographical and attitudinal forms of similarity for our conceptual model. In addition, we recognize that the form of interaction between individuals and AI agents has several variations in terms of the number of parties interacting (i.e., single or multiple individuals and AI agents). Thus, we limit the scope of our model to the context of a single individual directly interacting with a single AI agent for the purpose of our research model. The AI similarity-continuance model is depicted in Figure 1.  \n Figure 1. AI similarity-continuance model. 3. Method AI Teammate Platform: We developed a custom online AI Teammate platform that enables participants to collaborate with an AI agent (chatbot in this context). The platform was developed using different \ntechnologies, including PHP and HTML. The database was created with MySQL. The platform was hosted on a Google Cloud Virtual Machine Instance. It connected to Qualtrics, an online survey software, for managing and collecting the survey side of the experiment. Since we investigated the impact of an AI agent’s (chatbot) representation on individuals, the platform allowed us to control the representational dimension of the AI agent in terms of its similarity with the individual working with it. Accordingly, the platform accommodated a High and Low Similarity experimental design. In the High Similarity conditions, participants were paired with an avatar with similar gender (Male or Female), ethnicity (e.g., White, Black, Asian), and an avatar picture that showed a human image and a name that matched the individual’s gender and ethnicity. For instance, a white-male participant in a high similarity condition would be matched with a chatbot named Jake and an image of a white male person (Figure 2). In the low similarity conditions, the participants were matched with a chatbot with a generic robot image named Smart Bot (Figure 2).  \n \n Figure 2. Brainstorming session with high and low similarity AI teammate.  \nPerceived similarity with AI agentIT IdentityIntention to Use\nPerceived value of AI agent\nDependencyEmotional EnergyRelatedness\nPage 209\nIn the experiment, the AI agent (chatbot) was introduced as an AI teammate that is empowered by state-of-the-art AI and ML capabilities to recognize and understand textual problems and suggest ideas. However, the chatbot was developed with predefined ideas that it contributed during the brainstorming session. The ideas were updated and changed depending on the problem or the task assigned to the participants during the brainstorming session. Experimental Design: Participants who provided consent to participate were redirected to a survey page that collected demographic information including gender, ethnicity, age, work experience, and education. Next, the system randomly assigned participants to one of the two treatment conditions or the control group. Subsequently, participants were redirected to the ‘introduction’ page where they met their AI teammate. The ‘introduction’ page consisted of a description of the AI agent as well as an image that represented the AI agent. In the high similarity condition, the AI agent’s avatar either looked like them (same gender and ethnicity), while in the low similarity condition the AI agent’s avatar looked different than the participant (different gender and ethnicity). This representation served to manipulate the low-level similarity between the participants and the AI agent. Next, participants were redirected to a moral choice task where they were asked to analyze a situation that has a moral dilemma, and they were asked to choose the answer that they agree with the most. Depending on the condition (High or Low Similarity), the AI agent provided an answer that was either similar (in the High Similarity condition) or different (in the Low Similarity conditions) from the participant’s answer. This manipulation served to manipulate the deep level similarities beyond appearances between the participants and the avatar.  Then, the participants were redirected to the Brainstorming Session page. The page contained instructions about how to participate in the brainstorming session as well as a problem description for participants to generate ideas. The brainstorming page contained a 10-minute countdown session timer that was visible to participants throughout the session. In addition, an ideas counter was displayed which tracked the number of ideas that participants generated and the number of ideas generated by their AI teammate. The participants could generate as many ideas as they wished during the available time. Once the timer stopped, the brainstorming session ended, and participants were redirected to a survey page. The order of the survey questions was randomized to avoid biases related to order, recency, or fatigue.   Participants and Procedure: We deployed the AI platform through CloudResearch, a platform for managing online data collection, to gather data from \nAmazon Mechanical Turkers (MTurkers). Each MTurker was paid a fixed amount, regardless of their productivity in terms of idea generation. To determine the eligibility criteria and ensuring data quality, we followed research recommendations such that workers could only complete the task if they previously completed more than 1000 hits with a 95% or higher approval rate, lived in the United States, and were fluent in English (Peer, Vosgerau, & Acquisti, 2014). Based on these criteria, we collected data from a total of 450 workers. However, we identified 24 participants who did not complete the study and thus were disqualified, resulting in 426 valid participants. About 42.5% of the participants identified themselves as female and 57.5% as male. Most participants identified themselves as white (74.4%), followed by black (10.1%), Asian (8.2%), Hispanic/Latin American (5.2%), Indian (1.9), and Native Hawaiian/Pacific Islander (0.2%). The minimum age was 19, and the maximum age was 71. About 65% of participants were between 30 and 50 years old while the remaining two age groups (below 30 or above 50) were 17.5% each. The treatment conditions’ sample size was balanced; the control group consisted of 50 participants (Table 1).  Table 1: Sample size per treatment. Condition Participants High Similarity  185 Low Similarity 191 Control 50  Measures: To test the proposed model and hypotheses, we used a host of measures. Besides the similarity measure, all other measures were adopted from the literature. The items of these measures were adjusted accordingly to fit the context of this study.  Similarity: To measure perceived similarity, participants were asked two 5-point Likert scale questions. The first question addressed their perceived physical similarity with the chatbot. The second question addressed their perceived attitudinal similarity with the chatbot. Afterwards, we computed a composite similarity score by calculating the average of participants’ demographical and attitudinal similarity responses to get an overall perceived similarity score for each participant. Through these steps, we were able to consider the different levels of similarities (surface and deep) in the study model while ensuring model parsimony. IT Identity: Since IT identity is a second-order construct (Omega: 0.80) that has three dimensions, we adopted the measures proposed by (Carter, 2012) to measure each of these dimensions. The instrument for each of the dimensions (dependence (Alpha: 0.96), \nPage 210\nrelatedness (Alpha: 0.95), and emotional energy (Alpha: 0.92)) consisted of 4 (5-point Likert scale) items. Perceived Value: The purpose of this scale is to measure an individual’s perceived value of their AI teammate. The scale used for measuring this construct was adopted based on the instrument proposed by (de Vreede, de Vreede, Reiter-Palmon, & Ashley, 2011). The scale consisted of 4 items with a 5-point Likert scale format (Alpha: 0.97). Behavioral Intention: With a focus on collaboration rather than use, we adopted the behavior intention instrument by Venkatesh et al., (2003) and made appropriate adjustments to reflect the human – AI collaboration context. The adjusted instrument consisted of 3 items (5-point Likert scale) (Alpha: 0.97). Manipulation Checks: All similarity manipulations worked as intended. For demographical similarity, participants in the high similarity condition (M = 2.978, SD = 1.35) reported higher scores of perceived demographical similarities, t(368.01) = 6.106, p < 0.00. Equally, results pertaining attitudinal similarity show that participants in the high similarity condition (M = 3.464, SD = 1.23) perceived the AI agent to be more similar to them compared to the low similarity conditions (M = 2.738, SD = 1.16), t(370.95) = 5.851, p < 0.00. 4. Results To determine the measurements’ reliability, we computed the Alpha Coefficients and Composite Reliabilities following (Cronbach & Shavelson, 2004). The recommended threshold of 0.7 was found to be exceeded in both statistics indicating acceptable measurement reliability (Table 2) (Cortina, 1993; Fornell & Larcker, 1981). CFA was also performed to determine for construct validity. The results indicated that all items loaded in their respective constructs. Two items were removed as they were slightly below the recommended 0.7 cutoff (Hair, 2009): Relatedness item 2 and Emotional Energy item 2. Measures were also tested for discriminant validity and the square root of AVE was found greater than inter-construct correlations (Table 3) (Fornell & Larcker, 1981). To test our hypotheses, we performed structural equation modeling using the “Lavaan” package in “R”. The overall model fit shows support for the conceptualized model postulated in this study (Figure 3). Further, the results provide support (Est= 0.58, p= 0.00*) for the positive relationship between similarity and IT identity (H1). The model also shows support for the positive effect of IT identity and behavioral intention to use AI teammates (Est= 0.31, p= 0.00*) as well as the mediation effect of IT identity for similarity and behavioral intention to use AI teammates (Est= 0.17, p= \n0.00*) (H2 & H3). The positive relationship between IT identity and the perceived value of AI teammate (Est= 0.77, p= 0.00*) as well as the mediation effect of perceived value between IT identity and behavioral intention to use AI teammate (Est= 0.43, p= 0.00*) was also found to be significant (H4 & H5). Finally, the model also shows support (Est= 0.57, p= 0.00*) for the positive relationship between perceived value and behavioral intention to use AI teammates (H5).  Table 2: Confirmatory factor analysis (CFA). Item PV DEP EE REL BI PV 1 0.88 0.03 0.05 0 0.04 PV 2 0.93 0.03 -0.01 0 0.03 PV 3 0.93 0.04 -0.03 0.01 0 PV 4 0.9 -0.01 0.04 0 0.02 BI 1 0.02 -0.01 -0.01 0.03 0.94 BI 2 0.05 0 0.04 -0.01 0.89 BI 3 -0.03 0 -0.01 -0.01 1.02 DEP 1 0.1 0.79 0 0.07 0.03 DEP 2 0.02 0.78 0.05 0.02 0.06 DEP 3 0.03 0.94 0.02 0 -0.03 DEP 4 -0.01 0.97 0.02 -0.01 0 REL 1 0.06 0.01 0.87 0.03 -0.01 REL 2 0.22 -0.08 0.68 0.03 0.05 REL 3 -0.09 0.12 0.88 0.01 0.03 REL 4 -0.01 0 0.93 -0.01 0.02 EE 1 -0.09 0.14 0.03 0.78 0.08 EE 2 0.13 -0.21 0.13 0.69 -0.05 EE 3 0.04 0 -0.02 0.92 0 EE 4 -0.02 0.05 -0.01 0.92 0.02 Note1: PV – Perceived Value; BI – Behavioral Intention; COM – Competence; DEP – Dependence; EE – Emotional energy; REL – Relatedness; Note2: Values equal or greater than 0.7 are boldened    Table 3: Composite reliability, AVE, and inter-constructs correlations. \n \nConstruct CR AVE Latent constructs \n1 2 3 4 5 \nPerceived Value (1) 0.925 0.902 0.950     \nBehavioral Intention (2) 0.927 0.927 0.806 0.963    \nRelatedness (3) 0.858 0.854 0.741 0.719 0.924   \nEmotional Energy (4) 0.802 0.825 0.515 0.500 0.470 0.908  \nDependance (5) 0.884 0.854 0.537 0.521 0.671 0.700 0.924 \nNote: Bold values on the diagonal are √AVE \n \nPage 211\n  \n \n Figure 3. Structural equation model (SEM) results.   5. Discussion & Conclusions This study examined the impact of AI agent representation on perceptual and behavioral factors related to individuals through a social lens. Through an experiment where individuals collaborated with a chatbot to brainstorm ideas, we were able to derive some important theoretical findings. Drawing from SAT, we proposed a Similarity Continuance Model which predicts a positive relationship between similarity and IT identity with the technology in the context of human – AI agent interaction. By manipulating the representational form of the AI teammate in the experiment, we were able to analyze the effect of similarity on identification. The results of our study highlight the important role of AI agent’s similarity on individual’s identification with the technology (IT identity). This finding is in line with our hypothesis. Additionally, this finding expands prior literature that established a positive relationship between similarity and identification for other types of non- technologies (i.e., personal avatar) (Suh et al., 2011).  We further explored the role of IT identity as an important factor that affects an individual’s intention to continue working with the AI agent. Additionally, we assessed the mediating role of IT identity in the relationship between similarity and continuance intention. As hypothesized, the results of our analysis support the important positive role of IT identity in affecting an individual’s intention to continue working with an AI agent. In addition, our results highlighted IT \nidentity as an important mediator in the relationship between similarity and continuance intention. Prior literature established a relation between IT identity and use (Carter, 2012; Carter & Grover, 2015; Carter et al., 2020). However, our results expand the effect of IT identity to the collaboration domain between individuals and AI agents where the technology is viewed as a peer rather than as a tool. Thus, we infer that IT identity is a critical factor for understanding an individual’s continuance intention to work together with a technology agent.  In view of the persistent challenge that organizations face regarding workers resistance to collaborate with an AI agent, our results provide some important practical contributions that can help those organization to overcome this challenge. First, our study provides guidance to developers of organizational AI agents to focus on integrating design features that enhance the perception of similarity with individuals. With the enhanced graphical and AI capabilities, developers should utilize these technologies to create AI agents that have similar representational features to individuals. Although we limited our study to a certain set of demographical and attitudinal similarities, we expect that expanding this set towards creating AI agents that are similar to the individual would enhance individuals' experience and their intention to continue working with the chatbot. By doing so, organizations could possibly increase the willingness of their employees to work with the technology in a collaborative manner. While this study opens doors to the critical social design perspective of Human – AI interaction, our study \n*Sig. at p>0.01; Chisq: 232.93; DF:128; Robust CFI:0.984; Robust RMSEA:0.054; Robust Upper CI RMSEA: 0.064; Robust Upper CI RMSEA: 0.043; SRMR: 0.034\nPage 212\nalso has limitations which offer exciting opportunities for future research. First, our study used the concept of similarity in its general sense, encompassing both attitudinal and demographical similarities. Previous literature, however, suggested that different levels and combinations of similarities could have different impacts on individuals (Suh et al., 2011; You & Robert Jr, 2018). Thus, we plan to conduct a future study that separates the two types of similarity and expands our proposed model by (1) determining deep-level similarity in a multi-dimensional fashion and (2) by exploring the effect of different combinations of surface-level and deep-level similarities on individuals’ perceptions. The second limitation in our study is that we utilized a cross sectional experiment to examine our model. As suggested by Carter (2012) and others, the concept of IT identity could have a deeper effect in longer relations with the technology. In essence, we expect that as individuals work more intensely and longer with AI agents, their identification with the technology would further be enhanced. Accordingly, we recommend that future research examine or expand on our model by utilizing longitudinal studies.  Technology poses another limitation for this study. While we confirmed the effects of our manipulations, we recognize that different advanced technologies could be used to create enhanced AI agents to further improve individuals’ perceived similarity with the chatbot. Additionally, NLP capabilities can be embedded within the chatbot to create a conversational environment where the chatbot and individuals can discuss their attitudinal similarity in a much more human-like approach. Thus, we recommend that future research should expand on the scope of the technology used to further enhance our understanding about the effects of similarity and chatbot capabilities on individuals’ perceptions and behavior. With the dynamic change in human – AI interaction, we studied the impact of AI agent representation and capability on perceptual and behavioral factors related to individuals through a social lens. Drawing from SAT, we proposed a model of IS Social Continuance. By operationalizing an experiment where individuals collaborate with a chatbot to brainstorm ideas, we were able to derive some important theoretical and practical findings that will help with the design and implementation of AI teammates in organizations. References Adam, M., Wessel, M., & Benlian, A. (2021). AI-based chatbots in customer service and their effects on user compliance. Electronic Markets, 31(2), 427-445. Baird, A., & Maruping, L. M. (2021). The Next Generation of Research on IS Use: A Theoretical Framework of \nDelegation to and from Agentic IS Artifacts. MIS quarterly, 45(1). Bernier, E. P., & Scassellati, B. (2010, August). The similarity-attraction effect in human-robot interaction. In 2010 IEEE 9th International Conference on Development and Learning (pp. 286-290). IEEE. Bhattacherjee, A. (2001). Understanding information systems continuance: An expectation-confirmation model. MIS Quarterly, 351-370. Briggs, R. O., Adkins, M., Mittleman, D., Kruse, J., Miller, S., & Nunamaker Jr, J. F. (1998). A technology transition model derived from field investigation of GSS use aboard the USS Coronado. Journal of Management Information Systems, 15(3), 151-195.  Byrne, D. E. (1971). The attraction paradigm (Vol. 462): Academic press.  Carter, M. (2012). Information technology (IT) identity: A conceptualization, proposed measures, and research agenda. (Doctor of Philosophy (PhD)), Clemson University, Retrieved from https://tigerprints.clemson.edu/all_dissertations/901  Carter, M., & Grover, V. (2015). Me, myself, and I (T) conceptualizing information technology identity and its implications. MIS Quarterly, 39(4), 931-958.  Carter, M., Petter, S., Grover, V., & Thatcher, J. B. (2020). Information technology identity: A key determinant of it feature and exploratory usage. MIS Quarterly, 44(3), 983-1021.  Chuttur M.Y. (2009). Overview of the Technology Acceptance Model: Origins, Developments and Future Directions. Sprouts: Working Papers on Information Systems, 9(37). Indiana University, USA http://sprouts.aisnet.org/9-37  Cortina, J. M. (1993). What is coefficient alpha? An examination of theory and applications. Journal of Applied Psychology, 78(1), 98.  Cronbach, L. J., & Shavelson, R. J. (2004). My current thoughts on coefficient alpha and successor procedures. Educational Psychological Measurement, 64(3), 391-418.  Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Quarterly, 319-340.  De Visser, E. J., Monfort, S. S., Goodyear, K., Lu, L., O’Hara, M., Lee, M. R., . . . Krueger, F. (2017). A little anthropomorphism goes a long way: Effects of oxytocin on trust, compliance, and team performance with automated agents. Human Factors, 59(1), 116-133.  De Visser, E. J., Monfort, S. S., McKendrick, R., Smith, M. A., McKnight, P. E., Krueger, F., & Parasuraman, R. (2016). Almost human: Anthropomorphism increases trust resilience in cognitive agents. Journal of Experimental Psychology: Applied, 22(3), 331.  De Vreede, T., De Vreede, G.J., Reiter-Palmon, R., & Ashley, G. (2011, October), A Model of Technology Transition: Scale Development and Factor Analysis, Midwest Academy of Management 2011, Omaha, October 20-22, 2011.  Eyssel, F., De Ruiter, L., Kuchenbrandt, D., Bobinger, S., & Hegel, F. (2012). ‘If you sound like me, you must be more human’: On the interplay of robot and user \nPage 213\nfeatures on human-robot acceptance and anthropomorphism. Paper presented at the 2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI).  Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobservable variables and measurement error. Journal of Marketing Research, 18(1), 39-50.  Gao, Y., Pan, Z., Wang, H., & Chen, G. (2018). Alexa, my love: Analyzing reviews of amazon echo. Paper presented at the 2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation.  Gramlich, J. (2017). Most Americans would favor policies to limit job and wage losses caused by automation. Pew Research Center. Retrieved from https://www.pewresearch.org/fact-tank/2017/10/09/most-americans-would-favor- policies-to-limit-job-and-wage-losses-caused-by-automation/  Guéguen, N., Martin, A., & Meineri, S. (2011). Similarity and social interaction: When similarity fosters implicit behavior toward a stranger. The Journal of Social Psychology, 151(6), 671-673.  Gursoy, D., Chi, O. H., Lu, L., & Nunkoo, R. (2019). Consumers acceptance of artificially intelligent (AI) device use in service delivery. International Journal of Information Management, 49, 157-169.  Hair, J. F. (2009). Multivariate data analysis: A Global Perspective. 7th ed. Upper Saddle River: Prentice Hall. Harrison, D. A., Price, K. H., & Bell, M. P. (1998). Beyond relational demography: Time and the effects of surface-and deep-level diversity on work group cohesion. Academy of Management Journal, 41(1), 96-107.  Joshi, K. D. (1991). A Model of Users' Perspective on Change: The Case of Information Systems Technology Implementation. MIS Quarterly, 15(2), 229-242. Kääriä, A. (2017). Technology acceptance of voice assistants: Anthropomorphism as factor. Master’s thesis, University of Jyväskylä, Finland.  Kacmar, K. M., Harris, K. J., Carlson, D. S., & Zivnuska, S. (2009). Surface-level actual similarity vs. deep-level perceived similarity: Predicting leader-member exchange agreement. Journal of Behavioral Applied Management, 10(3), 315-334.  Lapointe, L., & Rivard, S. (2005). A Multilevel Model of Resistance to Information Technology Implementation. MIS Quarterly, 29(3), 461-491.  Lebovitz, S., Levina, N., & Lifshitz-Assaf, H. (2021). Is AI Ground Truth Really ‘True’? The Dangers of Training and Evaluating AI Tools Based on Experts’ Know-What. The Dangers of Training and Evaluating AI Tools Based on Experts’ Know-What. MIS Quarterly, 45(3), 1501-1525. Lebovitz, S., Lifshitz-Assaf, H., & Levina, N. (2022). To engage or not to engage with AI for critical judgments: How professionals deal with opacity when using AI for medical diagnosis. Organization Science, 33(1), 126-148. \nLu, L., Cai, R., & Gursoy, D. (2019). Developing and validating a service robot integration willingness scale. International Journal of Hospitality Management, 80, 36-51.  Marakas, G. M., & Hornik, S. (1996). Passive resistance misuse: overt support and covert recalcitrance in IS implementation. European Journal of Information Systems, 5(3), 208-219.  McCroskey, J. C., & McCain, T. A. (1972). The Measurement of Interpersonal Attraction. Speech Monographs, 41(3), 261-266.  Moon, Y. E. (1996). Similarity effects in human-computer interaction: Effects of user personality, computer personality, and user control on attraction and attributions of responsibility. Dissertation, Stanford University. Mouakket, S. (2015). Factors influencing continuance intention to use social network sites: The Facebook case. Computers in Human Behavior, 53, 102-110.  Müller, J. M. (2019). Assessing the barriers to Industry 4.0 implementation from a workers’ perspective. IFAC-PapersOnLine, 52(13), 2189-2194.  Nass, C., Steuer, J., & Tauber, E. R. (1994). Computers are social actors. Paper presented at the Proceedings of the SIGCHI conference on Human factors in computing systems.  Parise, S., Kiesler, S., Sproull, L., & Waters, K. (1999). Cooperating with life-like interface agents. Computers in Human Behavior, 15(2), 123-142.  Park, E., Jin, D., & del Pobil, A. P. (2012). The law of attraction in human-robot interaction. International Journal of Advanced Robotic Systems, 9(2), 35.  Peer, E., Vosgerau, J., & Acquisti, A. (2014). Reputation as a sufficient condition for data quality on Amazon Mechanical Turk. Behavior Research Methods, 46(4), 1023-1031.  Phillips, K. W., Northcraft, G. B., & Neale, M. A. (2006). Surface-level diversity and decision-making in groups: When does deep-level similarity help? Group Processes & Intergroup Relations, 9(4), 467-482. Purington, A., Taft, J. G., Sannon, S., Bazarova, N. N., & Taylor, S. H. (2017). \"Alexa is my new BFF\" Social Roles, User Satisfaction, and Personification of the Amazon Echo. Paper presented at the Proceedings of the 2017 CHI conference extended abstracts on human factors in computing systems.  Rietz, T., Benke, I., & Maedche, A. (2019). The impact of anthropomorphic and functional chatbot design features in enterprise collaboration systems on user acceptance. 14th International Conference on Wirtschaftsinformatik, February 24-27, 2019, Siegen, Germany Rogers, E. M. (2010). Diffusion of innovations. Simon and Schuster.  Schuetz, S., & Venkatesh, V. (2020). The rise of human machines: How cognitive computing systems challenge assumptions of user-system interaction. Journal of the Association for Information Systems, 21(2), 460-482.  Singh, R., Wegener, D. T., Sankaran, K., Singh, S., Lin, P. K., Seow, M. X., . . . Shuli, S. (2015). On the importance of trust in interpersonal attraction from attitude similarity. \nPage 214\nJournal of Social Personal Relationships, 32(6), 829-850.  Suh, K.-S., Kim, H., & Suh, E. K. (2011). What if your avatar looks like you? Dual-congruity perspectives for avatar use. MIS Quarterly, 711-729.  Thompson, R. L., Higgins, C. A., & Howell, J. M. (1991). Personal computing: Toward a conceptual model of utilization. MIS Quarterly, 125-143.  Van den Broek, E., Sergeeva, A., & Huysman, M. (2021). When the Machine Meets the Expert: An Ethnography of Developing AI for Hiring. MIS Quarterly, 45(3), 1557-1580. Venkatesh, V., & Davis, F. D. (2000). A theoretical extension of the technology acceptance model: Four longitudinal field studies. Management Science, 46(2), 186-204.  Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. MIS Quarterly, 425-478.  Wagner, K., Nimmermann, F., & Schramm-Klein, H. (2019, January). Is it human? The role of anthropomorphism as a driver for the successful acceptance of digital voice assistants. In proceedings of the 52nd Hawaii International Conference on System Sciences. Warta, S. F., Kapalo, K. A., Best, A., & Fiore, S. M. (2016). Similarity, complementarity, and agency in HRI: Theoretical issues in shifting the perception of robots from tools to teammates. Paper presented at the Proceedings of the Human Factors and Ergonomics Society Annual Meeting.  You, S., & Robert Jr, L. P. (2018). Human-robot similarity and willingness to work with a robotic co-worker. Paper presented at the Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction.   \nPage 215"
}