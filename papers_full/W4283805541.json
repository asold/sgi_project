{
  "title": "Self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation",
  "url": "https://openalex.org/W4283805541",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2114053840",
      "name": "Sangjoon Park",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127186396",
      "name": "Gwang-Hyun Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108885784",
      "name": "Yu-Jin Oh",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127373208",
      "name": "Joon Beom Seo",
      "affiliations": [
        "Asan Medical Center",
        "Ulsan College",
        "University of Ulsan"
      ]
    },
    {
      "id": "https://openalex.org/A2108408998",
      "name": "Sang Min Lee",
      "affiliations": [
        "University of Ulsan",
        "Asan Medical Center",
        "Ulsan College"
      ]
    },
    {
      "id": "https://openalex.org/A2117171669",
      "name": "Jin Hwan Kim",
      "affiliations": [
        "Chungnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2118321303",
      "name": "Sung-Jun Moon",
      "affiliations": [
        "Yeungnam University"
      ]
    },
    {
      "id": "https://openalex.org/A2190705000",
      "name": "Jae Kwang Lim",
      "affiliations": [
        "Kyungpook National University"
      ]
    },
    {
      "id": "https://openalex.org/A2166847905",
      "name": "Chang Min Park",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2100734166",
      "name": "Jong Chul Ye",
      "affiliations": [
        "International Graduate School of English",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2114053840",
      "name": "Sangjoon Park",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127186396",
      "name": "Gwang-Hyun Kim",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108885784",
      "name": "Yu-Jin Oh",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127373208",
      "name": "Joon Beom Seo",
      "affiliations": [
        "Asan Medical Center",
        "University of Ulsan",
        "Ulsan College"
      ]
    },
    {
      "id": "https://openalex.org/A2108408998",
      "name": "Sang Min Lee",
      "affiliations": [
        "Asan Medical Center",
        "Ulsan College",
        "University of Ulsan"
      ]
    },
    {
      "id": "https://openalex.org/A2117171669",
      "name": "Jin Hwan Kim",
      "affiliations": [
        "Chungnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2118321303",
      "name": "Sung-Jun Moon",
      "affiliations": [
        "Yeungnam University Medical Center",
        "Yeungnam University"
      ]
    },
    {
      "id": "https://openalex.org/A2190705000",
      "name": "Jae Kwang Lim",
      "affiliations": [
        "Kyungpook National University"
      ]
    },
    {
      "id": "https://openalex.org/A2166847905",
      "name": "Chang Min Park",
      "affiliations": [
        "New Generation University College"
      ]
    },
    {
      "id": "https://openalex.org/A2100734166",
      "name": "Jong Chul Ye",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2886281300",
    "https://openalex.org/W2770241596",
    "https://openalex.org/W2799723178",
    "https://openalex.org/W2786204509",
    "https://openalex.org/W2893693469",
    "https://openalex.org/W2608231518",
    "https://openalex.org/W2939788146",
    "https://openalex.org/W2972112160",
    "https://openalex.org/W3194687576",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3176055902",
    "https://openalex.org/W3033586327",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3123344745",
    "https://openalex.org/W2924319258",
    "https://openalex.org/W2964744899",
    "https://openalex.org/W3024371423",
    "https://openalex.org/W2904183610",
    "https://openalex.org/W1904878066",
    "https://openalex.org/W2912664121",
    "https://openalex.org/W3034922525",
    "https://openalex.org/W6912683627",
    "https://openalex.org/W3098394437"
  ],
  "abstract": null,
  "full_text": "ARTICLE\nSelf-evolving vision transformer for chest X-ray\ndiagnosis through knowledge distillation\nSangjoon Park 1, Gwanghyun Kim1, Yujin Oh 1, Joon Beom Seo2, Sang Min Lee 2, Jin Hwan Kim3,\nSungjun Moon 4, Jae-Kwang Lim5, Chang Min Park 6 & Jong Chul Ye 1,7✉\nAlthough deep learning-based computer-aided diagnosis systems have recently achieved\nexpert-level performance, developing a robust model requires large, high-quality data with\nannotations that are expensive to obtain. This situation poses a conundrum that annually-\ncollected chest x-rays cannot be utilized due to the absence of labels, especially in deprived\nareas. In this study, we present a framework named distillation for self-supervision and self-\ntrain learning (DISTL) inspired by the learning process of the radiologists, which can improve\nthe performance of vision transformer simultaneously with self-supervision and self-training\nthrough knowledge distillation. In external validation from three hospitals for diagnosis of\ntuberculosis, pneumothorax, and COVID-19, DISTL offers gradually improved performance as\nthe amount of unlabeled data increase, even better than the fully supervised model with the\nsame amount of labeled data. We additionally show that the model obtained with DISTL is\nrobust to various real-world nuisances, offering better applicability in clinical setting.\nhttps://doi.org/10.1038/s41467-022-31514-x OPEN\n1 Department of Bio and Brain Engineering, KAIST, Daejeon, Korea.2 Asan Medical Center, University of Ulsan College of Medicine, Seoul, South Korea.\n3 College of Medicine, Chungnam National Univerity, Daejeon, South Korea.4 College of Medicine, Yeungnam University, Daegu, South Korea.5 School of\nMedicine, Kyungpook National University, Daegu, South Korea.6 College of Medicine, Seoul National University, Seoul, South Korea.7 Kim Jaechul Graduate\nSchool of AI, KAIST, Daejeon, Korea.✉email: jong.ye@kaist.ac.kr\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 1\n1234567890():,;\nW\nith the early success of deep learning for medical\nimaging1– 3, the application of arti ﬁcial intelligence\n(AI) for medical images has rapidly accelerated in\nrecent years4– 6. In particular, many deep learning-based com-\nputer-aided diagnosis (CAD) software have been introduced into\nroutine practice7– 10 for various imaging modalities including\nchest X-ray (CXR). These deep learning-based AI models have\ndemonstrated the potential to dramatically reduce the workload\nof clinicians in a variety of contexts if used as an assistant,\nleveraging their power to handle a large corpus of data in parallel.\nThe advantage can be maximized in resource-limited settings\nsuch as in underdeveloped countries, where various diseases like\ntuberculosis prevail while the number of experts to provide\naccurate diagnosis is scanty.\nCurrently, most of the existing AI tools are based on the\nconvolutional neural network (CNN) models built with super-\nvised learning. However, collecting large and well-curated data\nwith the ground truth annotation is rather dif ﬁcult in the\nunderprivileged areas where the amount of available data itself is\nabundant. In particular, although the size of data increases in\nnumber every year in these areas, the lack of ground truth\nannotation hinders the use of increasing data to improve\nthe performance of AI model. Given the limitation in\nlabel availability, an important line of machine learning research\nis to obtain a robust model relying less on manually annotated\nlabels.\nSeveral classes of approaches have been developed to address\nthis problem, by learning the underlying patterns of data without\nor with only a small amount of pre-existing labels. In self-\nsupervised learning, the large unlabeled data corpus itself pro-\nvides the supervisory signal which enables the model to learn\ntask-agnostic visual representation and to adapt to the down-\nstream tasks with a small number of labeled data. In self-\ntraining11, which is a representative semi-supervised learning\nmethod, a learner (teacher) obtained with supervised learning via\nsmall labeled data keeps on labeling large unlabeled data, gen-\nerating pseudo-labels, which can be used for retraining a new\nmodel (student) with enlarged data corpus. This teacher-student\nlearning paradigm is often called knowledge distillation. Since this\nconﬁguration is suitable for the framework with a siamese design\nwhere one model learns from the prediction of the other model\ninstead of labels, some lines of semi- and self-supervised works\nhave utilized knowledge distillation and suggested the possibility\nthat the model performance can be similar or even better than the\nfully supervised one\n11– 13\nSpeciﬁcally, in self-training with noisy student method 11\n(Fig. 1a), the key idea is to match the predictions of a more\ncorrupted student to the pseudo-label obtained with an uncor-\nrupted teacher. Speciﬁcally, in this method, the teacher isﬁrst\ntrained with supervised learning on the small number of labeled\ndata to generate the pseudo-labels for the separated set of large\nunlabeled data corpus. During the training of the student with\nboth labeled and pseudo-labeled data, strong noise is applied both\nfor the input and model architecture of the student to improve\nthe robustness to nuisances like adversarial or natural perturba-\ntions. These processes are iteratively done a few times by treating\nthe trained student as a new teacher to generate new pseudo-\nlabels for unlabeled data. In distillation with no label (DINO)12\n(Fig. 1b), two networks with the same Vision Transformer (ViT)\nmodels14, one deﬁned as a student and one as a teacher, take\ninputs from two sets of views from the same image: two large\npatches containing the global idea of the image (global crops),\nand the multiple small patches that offer a local representation of\nthe image (local crops). Then, all crops are passed to the student\nmodel while only the global crops are passed to the teacher so that\nthe two networks come to understand the semantic meaning with\nself-attention that the local and the global crops represent the\nsame subject, albeit seemingly disparate. More detailed explana-\ntion of the existing approaches is provided in Supplementary\nmaterial and Supplementary Fig. 1.\nAlthough self-training with noisy student and DINO are see-\nmingly different, they share fundamental similarities of knowl-\nedge distillation that the teacher outputs more accurate result\nwith less distorted or more informative input, and the student\nlearns to match the teacher’s prediction using more distorted or\nless informative input. Focusing on this, here we introduce a self-\nevolving framework, dubbed distillation for self-supervised and\nself-train learning (DISTL), that can gradually improve the per-\nformance by the generation of pseudo-labels that reconcile the\ndistinct strengths of self-supervised learning and self-training\nunder knowledge distillation.\nFigure 1c illustrates the proposed DISTL framework. The two\nidentical models, teacher and student, are utilized. However,\ndifferent from the existing methods, the student is jointly trained\nwith self-supervised learning for semantic features and self-\ntraining for pseudo-labels with the knowledge distilled from the\nteacher. These two components play different roles in the\nlearning process. Speciﬁcally, self-supervised learning empowers\nthe model to learn task-agnostic semantic information about the\ngiven image, like overall structural features consisting the CXR\nimage (Supplementary Fig. S2). On the other hand, the self-\ntraining enables the model to directly learn the task-speci ﬁc\ninformation such as the diagnosis of tuberculosis, by encouraging\nthe student to match its noised prediction obtained from a given\nCXR to the clean pseudo-label of the teacher. In addition,\ninspired by the learning process of human non-expert reader, the\nadditional correction with the initial small labeled data is done\nper the pre-deﬁned steps to prevent the student from being biased\nby the imperfect estimation of the teacher. More detailed\ndescriptions for the DISTL method and ablation study to verify\nthe architecture are provided in Supplementary Material and\nSupplementary Fig. S3.\nWith the proposed DISTL method, we have shown that the AI\nmodel performance can be gradually improved with increasing\nunlabeled data by maximally utilizing the common ground of\nknowledge distillation from self-supervision and self-training. Of\nnote, it even outperforms the supervised model trained with the\nsame amount of labeled data. Furthermore, the model obtained\nwith the DISTL method has shown to be substantially robust to\nthe real-world data corruptions as well as offers a more\nstraightforward localization of lesions with the model’s attention.\nWe argue that the distillation of knowledge through self-\nsupervised learning and self-training, even without knowledge\nof the lesion, leads to a high correlation of attention with the\nlesion, which may also be the reason for the superior diagnostic\nperformance.\nResults\nPractical simulation for increasing unlabeled data over time.\nFigure 2 illustrates the experimental scenario of clinical applica-\ntion of the proposed DISTL framework. With the small number\nof labeled data, the initial model isﬁrst obtained, and the student\nmodel is trained in large unlabeled data with this teacher. During\nthis process, the teacher is slowly co-distilled from the updated\nstudent. Then, the updated models are used as the starting points\nof the next-generation models with increasing timeT, similar to\nthe self-training with noisy student11. We evaluated the proposed\nframework in three CXR tasks including the diagnosis of tuber-\nculosis by using only a small corpus of labeled data for super-\nvision and gradually increasing the amount of unlabeled data\nsimulating the real-world data accumulation over time.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x\n2 NATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications\nIn particular, to conﬁrm whether our AI model can gradually\nself-evolve in the data-abundant but the label-insuf ﬁcient\nsituation, we set our main task as the diagnosis of tuberculosis,\nas it is highly demanded in clinics after World Health\nOrganization has identiﬁed the use of AI-based CAD for CXR\nscreening of tuberculosis as a potential solution in resource-\nlimited settings15. We collected the normal and tuberculosis\nCXRs from both the publicly available open-source and the\ninstitutional datasets for the model development and internal\nvalidation (Supplementary Table 1 and Details of datasets for\ndiagnosis section). After collection, a total of 35,985 CXRs were\nfurther divided into 3598 labeled (10% of total data) and 32,387\nunlabeled subsets (90% of total data). Next, assuming the\nsituation in the clinic that the number of unlabeled cases\nClass \nhead\n߀Repeat\nSemantic features\nLocal crops\nOriginal view\nGlobal view\nLocal \nviews\nCXR image\nTeacher \nbackbone\nStudent \nbackbone\nFeature\nhead\nClass \nhead\nClass \nhead\nFeature\nhead\nGlobal crops\nTeacher\nStudent\nPseudolabel\nClass prediction\nc\nSemantic features\n޾Self-train޽Self-supervision\nSupervision \nfrom small labels޿Correction\na b\nSemantic features\nLocal crops\nOriginal view\nGlobal view\nLocal \nviews\nCXR image\nTeacher \nbackbone\nStudent \nbackbone\nFeature\nhead\nGlobal crops\nTeacher\nStudent\nSemantic features\nSelf-supervisionRepeat\nTeacher \nbackbone\nStudent \nbackbone\nClass \nhead\nTeacher\nStudent\nPseudolabel\nClass prediction\nSelf-train\n+Noise\nCXR image\nFeature\nhead\n+Noise\nFig. 1 Comparison between the existing methods and the proposed framework for the self-evolving AI model. aSelf-training with noisy student method.\nb Self-supervised learning with distillation with no label (DINO) method.c The proposed distillation for self-supervised and self-train learning (DISTL)\nmethod is mainly composed of the two components, for self-supervision and self-training.\n…\n…\nxu\nlabel\nxl yl\nSmall labeled data\nStudent Student Student\nTeacher\nTeacher\nTeacher\nTeach\nCorrect Correct Correct\nInitial \nmodel\nT=1T = initial T=2 T=t\nTeach\nCorrect\nTeach\nCorrect\nlabel\nxl yl\nSmall labeled data\nlabel\nxl yl\nSmall labeled data\nlabel\nxl yl\nSmall labeled data\nIncreasing \nunlabeled data\n…\nxu\nIncreasing \nunlabeled data\n…\nxu\nIncreasing \nunlabeled data\nFig. 2 Simulation of clinical application for increasing data over time.The initial model is trained with small labeled data. Then, using this initial model as\nteacher and small initial data for correction, the student is trained with the DISTL method under an increasing amount of data over timeT.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 3\nincreases as time goes, the unlabeled subset was further divided\ninto three. Then, using these three folds, we increased the total\namount of available unlabeled data to be 30%, 60%, 90% of total\ndata, supposing the timeT = 1, 2, 3 goes. During this process, the\nsubset of labeled data remainsﬁxed to the initial 3598 CXRs (10%\nof total data). The performances of the proposed self-evolving AI\nmodel at each timeT were evaluated in the external validation\ndata collected and labeled by board-certiﬁed radiologists in three\ndifferent hospitals (Chonnam National University Hospital\n[CNUH], Yeungnam University Hospital [YNU], and Kyung-\npook National University Hospital [KNUH]), to validate the\ngeneralization capability for different devices and image acquisi-\ntion settings (Table1, upper).\nFor pneumothorax diagnosis, we used the SIIM-ACR pneu-\nmothorax data\n16 for the model development and internal\nvalidation. As it contains the CXRs and the segmentation mask\nfor either pneumothorax or normal cases, we adopted it to be the\npneumothorax diagnosis task, as a binary classiﬁcation problem.\nSimilar to the tuberculosis diagnosis task, we partitioned this data\ninto a labeled and unlabeled subsets, and the unlabeled subset was\nfurther divided into three to simulate the gradually accumulating\ndata with time. For external validation of the trained model, we\nalso collected the CXRs of pneumothorax patients in the three\nhospitals (CNUH, YNU, KNUH), which were labeled by board-\ncertiﬁed radiologists (Table1, middle).\nFor COVID-19 diagnosis, we utilized two publicly available\nCOVID-19 datasets17,18 for the model development and internal\nvalidation by gradually increasing the amount of unlabeled data\nwith increasing time similar to other tasks, while the CXRs of\npolymerase chain reaction (PCR) positive COVID-19 cases were\ndeliberately collected for the external validation in the three\nhospitals (CNUH, YNU, KNUH) (Table1, lower).\nOur tuberculosis diagnosis model can self-evolve with\nincreasing unlabeled data.W e ﬁrst investigated whether the\nperformance of tuberculosis diagnosis can gradually be improved\nwith the proposed DISTL framework given the increasing num-\nbers of unlabeled data. As shown in Fig. 3a, in the external\nvalidation, the performance of the model trained with the pro-\nposed framework improved as the number of unlabeled data\nincreased, from an AUC of 0.948 to 0.974. Of note, the improved\nperformance was even better than the supervised model trained\nwith the same amount of data with labels, which improved to the\nAUC of 0.958 atT = 2 but decreased to 0.950 atT = 3, showing\nthe sign of overﬁtting. In detail, theﬁnal model showed the AUCs\nof 0.974, 0.965, 0.985, 0.980, sensitivities of 92.7%, 92.9%, 93.0%,\n95.0%, speciﬁcities of 92.0%, 90.3%, 96.0%, 93.5%, accuracies of\n92.2%, 90.4%, 95.3%, 94.0% in the pooled test set and three\ninstitutions (Supplementary Table 2), which con ﬁrmed the\nexcellent generalization capability in clinical situation with dif-\nference devices and settings.\nNot conﬁned to the metric itself, we observed an interesting\nﬁnding that the model attention of the ViT model gets reﬁned\nwith increasing time T (Fig. 3b). As the AI model evolves with\nincreasing time T, the self-attention of AI gets reﬁned to better\nlocalize the target lesion as well as semantic structures within\nthe given CXR image. Notably, the gradual improvement of\nperformance was more prominent for the ViT model equipped\nwith self-attention than the standard CNN-based models without\nself-attention (Fig. 4a). The ViT model showed a linear increase\nas well as the best performance among the models, although other\nCNN-based models also showed performance improvement with\nthe proposed framework under increasing unlabeled data. In\naddition, the ViT model showed no sign of overﬁtting which was\nobserved in some CNN-based models at laterT.\nWe further evaluated whether the existing self-supervised and\nsemi-supervised learning methods, which can also be utilized for\nthe plenty of unlabeled data with increasingT, can improve the\nperformance of the AI model gradually similar to our DISTL\nframework (Fig. 4b). With the same experimental settings, the\nexisting methods presented the signi ﬁcant degradation of\nperformance at T = 1 where the number of unlabeled data is\nrelatively small, while the performances slightly improve with\nmore data with increasing T. Even with this increase in\nperformance, none of the existing self-supervised and semi-\nsupervised methods showed prominent performance improve-\nment compared with the initial model, while the model built with\nthe DISTL framework showed stably improving performance\nwith increasing unlabeled data.\nRobustness to unseen classes and label corruption. In a real-\nworld setting, the data of unseen classes not included in training\ndata may be included when collecting the unlabeled cases, or\nincorrectly labeled data may be added by the mistake of a prac-\ntitioner. Therefore, we performed experiments to verify the\nrobustness of the proposed framework given these situations.\nFirst, we collected data on four other classes (nodule, effusion,\ninterstitial lung disease, bacterial infection) that are commonly\nTable 1 Details of data partitioning used for the experiments of three chest X-ray (CXR) tasks.\nClass Training and internal validation External validation\nT = initial T= 1 T = 2 T = 3\nLabeled Unlabeled Total CNUH YNU KNUH\nTuberculosis diagnosis\nNormal 3033 9013 18,047 27,059 1100 400 300 400\nTuberculosis 565 1782 3543 5328 328 28 100 200\nTotal 3598 10,795 21,590 32,387 1428 428 400 600\nPneumothorax diagnosis\nNormal 984 2920 5816 8726 1100 400 300 400\nPneumothorax 224 707 1438 2155 120 30 40 50\nTotal 1208 3627 7254 10,881 1220 430 340 450\nCOVID-19 diagnosis\nNormal 3015 9017 17,999 27,077 1100 400 300 400\nCOVID-19 503 1538 3111 4590 659 81 286 292\nTotal 3518 10,555 21,110 31,667 1759 481 586 692\nT time, CNUH Chungnam national university hospital,YNU Yeungnam University Hospital,KNUH Kyungpook National University Hospital;COVID-19 coronavirus disease 2019.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x\n4 NATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.948 0.962 0.973 0.974 \nSupervised 0.948 0.950*\n(p=0.002)\n0.958*\n(p<0.001)\n0.950*\n(p<0.001)\na\n b\nCXR image Initial (supervised)\nT=1 T=2 T=3\nFig. 3 Tuberculosis diagnosis results with gradually increasing unlabeled data. aGradually evolving performance with the proposed framework under an\nincreasing amount of data. Compared with the supervised model using the same amount of data with labels, the model trained with the proposed\nframework without labels showed even better performances.b Gradual attention change of the evolving model for a tuberculosis case. For the exempliﬁed\ntuberculosis case, the attention of the Vision Transformer (ViT) model gets gradually reﬁned to better catch the target lesions and semantic structures as\nthe model evolves with increasing timeT. Data are presented with calculated area under the receiver operating characteristics curves (AUCs) in the study\npopulation (center lines) ±95% conﬁdence intervals (CIs) calculated with the DeLong's method (shaded areas). The AUCs of the proposed method and the\nsupervised learning method were compared at each time pointT with the DeLong test to evaluate statistical signiﬁcance, except for theT =initial where\nthe two methods start from the same baseline. * denotes statistically signiﬁcant (p < 0.050) superiority of the proposed framework. All statistical tests\nwere two-sided. CXR, chest X-ray.\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.948 0.962 0.973 0.974 \nSimCLR (linear) 0.948 0.852* \n(p<0.001)\n0.857* \n(p<0.001)\n0.864* \n(p<0.001)\nSimCLR (fine-tune) 0.948 0.881* \n(p<0.001)\n0.888* \n(p<0.001)\n0.905* \n(p<0.001)\nNoisy student 0.948 0.943* \n(p<0.001)\n0.944* \n(p<0.001)\n0.950* \n(p<0.001)\nFixMatch 0.948 0.945* \n(p<0.001)\n0.948* \n(p<0.001)\n0.948* \n(p<0.001)\nMethods T = initial T = 1 T = 2 T = 3\nViT (proposed) 0.948 0.962  0.973  0.974  \nResNet-50 0.913* \n(p<0.001)\n0.930* \n(p<0.001)\n0.959* \n(p=0.009)\n0.943* \n(p<0.001)\nResNext-50 0.944 \n(p=0.440)\n0.963 \n(p=0.931)\n0.971 \n(p=0.676)\n0.964* \n(p=0.040)\nDenseNet-201 0.945 \n(p=0.642)\n0.964 \n(p=0.760)\n0.965* \n(p=0.049)\n0.964* \n(p=0.013)\nEfficientNet-B4 0.838* \n(p<0.001)\n0.870* \n(p<0.001)\n0.927* \n(p<0.001)\n0.933* \n(p<0.001)\nba\nFig. 4 Tuberculosis diagnosis results for comparison with the other models and methods. aCompared to other convolutional neural networks (CNN)-\nbased models, the Vision Transformer (ViT) model showed a linear increase as well as the best performance among the models.b Unlike the model trained\nwith the proposed framework, none of the existing self-supervised and semi-supervised methods showed a prominent improvement in performance with\nincreasing time T. Data are presented with calculated area under the receiver operating characteristics curves (AUCs) in the study population (center\nlines) ±95% conﬁdence intervals calculated with the DeLong's method (shaded areas). The AUCs of the proposed method were compared with the other\nmodels or methods at each time pointT with the DeLong test to evaluate statistical signiﬁcance, except for theT = initial where all methods of comparison\nstart from the same baseline inb. * denotes statistically signiﬁcant (p < 0.050) superiority of the proposed framework. All statistical tests were two-sided.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 5\nencountered in clinics from a hospital (Asan Medical Center\n[AMC]). These other class data were added in the same manner\nwhen increasing the number of unlabeled data over time (Sup-\nplementary Table 3). Notably, the performance was stably\nimproved the same as in the experiments without adding these\nother class data (Fig.5a), suggesting the robustness of the pro-\nposed framework to assure that the AI model is not confused by\nthese unseen classes to the initial model trained only with normal\nand tuberculosis data. Secondly, we randomly made the label\nwrong with a probability of 5% for the supervised learning and\nevaluated whether the performance decreases (Supplementary\nTable 4). The model trained with supervised learning using the\ncorrupted label showed signiﬁcant deterioration in performance,\nwhile that with the proposed framework was not altered as it does\nnot depend on the label for increased data (Fig. 5b). Taken\ntogether, these results suggest the impressive reliability of the\nproposed framework which is required in real clinical\napplications.\nComparison of lesion localization performances . Under the\nhypothesis that ViT’s direct attention can provide better locali-\nzation than CNN’s indirect attention via the Gradient-weighted\nClass Activation Mapping (GradCAM)19, we quantiﬁed the loca-\nlization performance with model attention. A total of 30 CXRs in\nthe external validation data for tuberculosis diagnosis were\nselected, and manually annotated by the clinician. The predictions\nfrom model attention were generated by applying the threshold\nvalues after normalization to best localize the target lesions (0.1 for\nViT and 0.6 for CNN models). As the ViT model has multiple\nheads to be visualized, the best performing head was selected for\nevaluation. The dice similarity coef ﬁcients were calculated to\nassess the consistency between the predictions and labels.\nWithout any supervision during the training, the direct\nvisualization of ViT attention offered better localization of the\ntarget lesion than the indirect attention visualization of CNN-\nbased models using GradCAM, providing a mean dice similarity\ncoefﬁcient of 0.622 (standard deviation [STD] of 0.168) compared\nwith that of 0.373 (STD of 0.259) for a CNN-based model. Of\nnote, the indirect attention using GradCAM either attends to the\nunimportant location (upper ﬁgure) or fails to localize the\nmultiple lesions (lowerﬁgure) (Fig. 6).\nVerifying applicability of the proposed framework in other\ntasks. We further analyze whether the gradual performance\nimprovement with the proposed framework can also be observed\nin the CXR tasks other than tuberculosis diagnosis. First, for\npneumothorax diagnosis, similar to the observation in the\ntuberculosis diagnosis task, the model trained with the proposed\nframework improved gradually over increasing timeT (Fig. 7a).\nNotably, the performance of the model with the proposed fra-\nmework was lower than the supervised one when available\nunlabeled data are relatively small (T = 1) but it ultimately out-\nperformed the supervised model with the increased numbers of\nunlabeled data (T = 3). Similarly, for COVID-19 diagnosis, the\nproposed framework provided a stable performance improvement\nover time, whereas the model trained with the same amount of\nlabeled data showed a substantial performance drop at laterT in\nthe external validation (Fig. 7b), suggesting that overﬁtting to\ntraining data degraded the generalization performance of the\nsupervised model.\nValidation in a cohort of bacteriological laboratory test con-\nﬁrmed tuberculosis cases. As the diagnostic gold standard of\ntuberculosis is bacteriological laboratory test, we further validated\na b\nMethods T = initial T = 1 T = 2 T = 3\nProposed 0.948 0.962 0.973 0.974\nSupervised\n(corrupted) 0.948 0.948* \n(p=0.001)\n0.940* \n(p<0.001)\n0.939* \n(p<0.001)\nSupervised\n(not corrupted) 0.948 0.950* \n(p=0.002)\n0.958* \n(p<0.001)\n0.950* \n(p<0.001)\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.948 0.959 0.971 0.974\nFig. 5 Tuberculosis diagnosis results with real-world data collection and label corruption. aEven after adding unseen class data that are commonly\nencountered in clinics, the performance was stably improved with increasing timeT, even though these other class data were not included for the training\nof the initial model.b In the simulation for label corruption, the model trained with the proposed framework was not compromised, while the model trained\nwith supervised learning using corrupted labels showed signiﬁcant deterioration in performance. Data are presented with calculated area under the receiver\noperating characteristics curves (AUCs) in the study population (center lines) ±95% conﬁdence intervals calculated with the DeLong's method (shaded\nareas). The AUCs of the proposed method and the supervised learning method with and without corruptions were compared at each time pointT with the\nDeLong test to evaluate statistical signiﬁcance, except for theT =initial where all methods start from the same baseline. * denotes statistically signiﬁcant\n(p < 0.050) superiority of the proposed framework. All statistical tests were two-sided.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x\n6 NATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications\nthe developed model in the external data from another corelab\n(Seoul National University Hospital [SNUH]) containing con-\nﬁrmed tuberculosis cases with the reference standard. To this end,\nCXR images of 727 normal and 535 bacteriological laboratory test\nconﬁrmed tuberculosis cases were collected. In this cohort, the\nresults were remarkably consistent. Given the increasing number\nof unlabeled data, the model obtained with DISTL frameworks\nprovided gradually improving performance without any sign of\noverﬁtting even at laterT, which was superior to those of a fully\nsupervised model trained with the same amount of labeled data\n(Fig. 8). In detail, the ﬁnal model with the DISTL framework\nshowed a diagnostic performance with an AUC of 0.952, a sen-\nsitivity of 87.7%, speciﬁcity of 89.1%, and accuracy of 88.5%\n(Supplementary Table 5) demonstrating the robust generalization\nperformance in cohort with the reference standard.\nDiscussion\nGiven the impressive results of early studies that AI can keep up\nor even surpass the performance of the experienced clinician in\nvarious applications for medical imaging\n1,2, we have confronted\nthe era of ﬂooding AI models for medical imaging. However,\nCXR image Label ViT (direct attention) ResNet-50 (GradCAM)\nFig. 6 Exempliﬁed tuberculosis lesion localization with attention by Vision Transformer (ViT) and convolutional neural network (CNN)-based models.\nThe direct visualization of ViT attention offers better localization of the target lesion than indirect attention visualization of CNN-based models using\nGradient-weighted Class Activation Mapping (GradCAM).\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.832 0.882  0.902 0.913\nSupervised 0.832 0.905 \n(p=0.048)\n0.909 \n(p=0.602)\n0.891 \n(p=0.142)\nba\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.828 0.942  0.965 0.966 \nSupervised 0.828 0.953 \n(p=0.008)\n0.913* \n(p<0.001)\n0.866* \n(p<0.001)\nFig. 7 Other CXR diagnosis results including pneumothorax and coronavirus disease 2019 (COVID-19) diagnoses. aWhen applied for pneumothorax\ndiagnosis, the model trained with the proposed framework improved gradually over increasing timeT, while the supervised model showed the sign of\noverﬁtting at laterT. b Likewise, the model trained with the proposed framework outperformed the supervised model for the COVID-19 diagnosis task,\nwhich was more prominent under the increasing amount of unlabeled data. Data are presented with calculated area under the receiver operating\ncharacteristics curves (AUCs) in the study population (center lines) ±95% conﬁdence intervals calculated with the DeLong's method (shaded areas). The\nAUCs of the proposed method and the supervised learning method were compared at each time pointT with the DeLong test to evaluate statistical\nsigniﬁcance, except for theT = initial where the two methods start from the same baseline.† denotes statistically inferior performance, while * denotes\nstatistically signiﬁcant superiority of the proposed framework (p < 0.050). All statistical tests were two-sided.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 7\nthese models share a common drawback that they highly depend\non the quantity and quality of labels as well as the data. If the\nlabeled corpus does not contain sufﬁcient data points to represent\nthe entire distribution, the resulting model can be biased and the\ngeneralization performance can unpardonably deteriorate. In the\nﬁeld of medical imaging, a large number of raw data is being\naccumulated each year, but it is difﬁcult to utilize this large data\ncorpus with supervised learning approaches due to the absence\nof labels. Therefore, several methods based on unsupervised\nlearning20,21, self-supervised learning 22, and semi-supervised\nlearning23 have been proposed to cope with this problem, but\ntheir performances are still sub-optimal.\nTo cope with this problem, the proposed DISTL framework\nstands based on two key components: self-supervised learning\nand noisy self-training through teacher-student knowledge dis-\ntillation. The ﬁrst component, in our method, is similar to that\nproposed in DINO12, which encourages the model to learn the\ntask-agnostic semantic information of the image by the local-\nglobal view correspondence. In our preliminary experiment, the\nmodel built only with this self-supervision attends well to the\nimage layout, and particularly, object boundaries as shown in\nSupplementary Fig. S2. Secondly, the self-training component\nenables the model to directly learn the task-speciﬁc features, the\ndiagnosis of tuberculosis, similar to the noisy student self-\ntraining11. Under the continuity and clustering assumption24,\nlearning with a soft pseudo-label along with student-side noise\nincreases not only the performance but also the robustness to\nadversarial samples.\nInterestingly, we have found an analogy between the proposed\nDISTL framework and the training process of radiologists during\ntheir junior years (Fig. 9). When a junior radiologist learns to\nread CXR, a common practice is toﬁrst read CXR and afﬁrm it\nwith computed tomography image of the same patient which\nusually offers a more accurate diagnosis. This procedure is ana-\nlogous to the learning process of the student in our framework in\nwhich the model learns to match the prediction from noisy\naugmented or cropped image containing less information to that\nfrom the clean original image containing more information by the\nteacher which offers a more accurate diagnosis. In addition, it is\nalso a common practice that the junior radiologist learns referring\nto the senior radiologist’s reading, which is similar to the teacher-\nstudent distillation used in our framework. Finally, during the\nlearning process, the junior radiologists occasionally refer to the\ntextbook containing small but typical cases, which prevents from\nbeing biased from recently seen atypical cases. In our framework,\nthe correction step with the small number of initial labeled data\nplays a similar role. As a result, the DISTL framework, unlike the\nexisting self-supervised and semi-supervised learning approaches,\noffered gradually evolving performance simply by increasing the\namount of unlabeled data, with the substantial robustness to the\ncorruptions from data of unseen classes or wrong labels. In\naddition, we found in the experiments extending the application\nof our framework to the pneumothorax and COVID-19 diagnosis\nthat it provides the beneﬁt generally applicable to a variety of\ntasks.\nPractically, our method holds great potential for the screening\nof diseases like tuberculosis, especially when applied in under-\nprivileged areas. In the simulation of the application of the pro-\nposed framework, it shows a negatively predicted portion of\n72.5%, a negative predictive value (NPV) of 0.977, and a false-\npositive rate (FPR) of 0.080 (Supplementary Table 6). That is to\nsay, 72.5% of the screened population are indeed negative with a\nprobability of 97.7%, and therefore can be safely excluded from\nthe further reading by the clinician, having only 8.0% of cases as\nfalse positive. Consequently, this will substantially reduce the\nworkload, while not signi ﬁcantly increasing the false alarm.\nMoreover, the AI model can self-evolve with the proposed DISTL\nmethod in the increasing unlabeled data corpus without any\nfurther supervision from a human expert. This is another\nimportant merit to be used in underprivileged areas, where plenty\nof data are available due to the high prevalence of diseases but the\nexperts to label data are scanty.\nThis study has several limitations. First, the details concerning\nthe patient demographics and CXR characteristics were not\navailable in open-source data used for the training and internal\nvalidation. Second, although we simulated the robustness to\nunseen class data assuming real-world data collection in the\nexperiment, it was not possible to consider all the other minor\nclasses that can be encountered in real-world data accumulation.\nThird, we utilized a total of 35,985 CXRs to demonstrate the\nbeneﬁt of the proposed framework by using after dividing them\ninto the small labeled and large unlabeled subsets, but the\nnumber may be insufﬁcient to draw aﬁrm conclusion. Further\nstudies are warranted to verify the proposed framework in a data\ncorpus large enough to represent the general distribution.\nNevertheless, with the data-abundant but label-insuf ﬁcient\ncondition being common for medical imaging, we believe that\nit may offer great applicability to a broad ﬁeld of medical\nimaging.\nMethods\nEthics committee approval. This study was approved by the institutional review\nboards (IRBs) of Asan Medical Center, Chungnam National University Hospital,\nYeungnam University Hospital, Kyungpook National University Hospital, Seoul\nMethods T=i n i t i a l T=1 T=2 T=3\nProposed 0.905 0.933 0.951 0.952\nSupervised 0.905 0.927\n(p=0.197)\n0.934*\n(p=0.001)\n0.930*\n(p<0.001)\nFig. 8 Tuberculosis diagnosis results for validation in a cohort of\nbacteriological laboratory-conﬁrmed tuberculosis cases.In a cohort from\nanother corelab consisting of 727 normal cases and 535 conﬁrmed tuberculosis\ncases, the model obtained with the DISTL framework offered performances\nsuperior to those of the fully supervised model trained with the same amount of\nlabeled data. Data are presented with calculated area under the receiver\noperating characteristics curves (AUCs) in the study population (center\nlines) ±95% conﬁdence intervals (CIs) calculated with the DeLong's method\n(shaded areas). The AUCs of the proposed method and the supervised learning\nmethod were compared at each time pointT with the DeLong test to evaluate\nstatistical signiﬁcance, except for theT =initial where the two methods start\nfrom the same baseline. * denotes statistically signiﬁcant (p <0 . 0 5 0 )\nsuperiority of the proposed framework. All statistical tests were two-sided.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x\n8 NATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications\nNational University Hospital, and the requirement for informed consent was\nwaived due to the retrospective study design.\nDetails of datasets for pre-training. In this work, the pre-trained model was used\nfor all models, to offer the feature extracting capacity with prior knowledge of CXR.\nTo pre-train the model to learn task-relevant CXR feature in a large corpus of\nCXRs, we used the CheXpert dataset\n25 containing 10 common CXR classes: no\nﬁnding, cardiomegaly, lung opacity, consolidation, edema, pneumonia, atelectasis,\npneumothorax, pleural effusion, and support device. Among the 10 classes,ﬁve\nclasses including lung opacity, consolidation, edema, pneumonia, and pleural\neffusion, considered to be related to the manifestation of infectious disease, were\nselected as task-relevant CXR features. Consequently, the model wasﬁrst trained to\nclassify these ﬁve classes with the CheXpert data. With a total of 223,648 CXRs\nfrom 65,240 subjects, 29,453 posterior-anterior (PA) and 161,759 anterior-\nposterior (AP) view CXRs were used after excluding the 32,436 lateral view CXRs.\nThanks to the huge number of cases, the model was able to be a robust extractor for\nthe task-relevant CXR features, without depending upon the variation in patients\nand the setting for image acquisition. As suggested in the ablation study of pre-\ntraining (see Supplementary Fig. S3), this pre-training step has brought us a sub-\nstantial increase in performance and is one of the key components of our model.\nDetails of datasets for diagnosis. First, for the tuberculosis diagnosis, we used\nboth public and institutional datasets for the model development and internal\nvalidation. Speciﬁcally, data deliberately collected from a hospital (Asan Medical\nCenter [AMC]) as well as publicly available data (National Institutes of Health\n[NIH]\n26, Valencian Region Medical ImageBank [BIMCV]17, CheXpert25, India27,\nMontgomery, Shenzen28, Belarus29, PAthology Detection in Chest radiograph\n[PADChest]30, Tuberculosis X-ray 11K [TBX 11K]31) were integrated (Supple-\nmentary Table 1), to have a total of 35,985 CXRs containing 5893 tuberculosis and\n30,092 normal cases. For pneumothorax diagnosis, the SIIM-ACR pneumothorax\ndata16 were utilized for the model development and internal validation. The ori-\nginal SIIM-ACR pneumothorax data contains 12,089 CXR images with or without\ncorresponding masks for the 2379 pneumothorax or 9710 normal cases. Therefore,\nwe adapted the problem into a binary classiﬁcation for the diagnosis of pneu-\nmothorax by deﬁning CXR as a pneumothorax positive case if a segmentation\nmask contains a positive value and as a negative case if not. Finally, for COVID-19\ndiagnosis, two publicly available open-sourced datasets were used for the model\ndevelopment and internal validation\n17,18. As these two datasets contain a small\nnumber of normal CXRs, we also utilized the normal CXRs in the tuberculosis\ndiagnosis task as the normal cases,ﬁnally yielding a total of 35,185 CXRs consisting\nof 5093 COVID-19 and 30,092 normal cases.\nFor all the three CXR diagnosis tasks, data deliberately collected from three\nhospitals (Chonnam National University Hospital [CNUH], Yeungnam University\nHospital [YNU], and Kyungpook National University Hospital [KNUH], labeled\nby board-certiﬁed radiologists, were used to externally validate the generalization\ncapability for different devices and image acquisition settings. In detail, 328\ntuberculosis and 1100 normal CXRs for the tuberculosis diagnosis task, 120\npneumothorax and 1100 normal CXRs for the pneumothorax diagnosis task, and\n120 COVID-19 and 1100 normal CXRs are evaluated for the external validation of\neach task.\nDetails of Implementation. The CXR images underwent preprocessing including\nhistogram equalization, Gaussian blurring, and normalization, and resized to\n256 × 256. As the backbone partf of the network, we used the ViT small model (12\nlayers and six heads) with the patch size of 8 × 8, and the CNN-based models\n(ResNet-50, ResNext-50, DenseNet-201, and EfﬁcientNet-B4) were used for\ncomparison. For the classiﬁcation and projection heads, the three-layered multi-\nlayer perceptron was utilized. For pre-training, an Adam optimizer was used with\na learning rate of 0.0001. The model was pre-trained for 5 epochs with a step\ndecay scheduler, with a batch size of 16. Weak data augmentation including\nrandom ﬂipping, rotation, and translation were performed to increase the data\nvariability during the pre-training\n32. As the loss functions, binary cross-entropy\n(BCE) losses were used for each class label. For supervised training of the initial\nmodel and the iterative training of the models with the DISTL method, AdamW\noptimizer\n33 were used along with cosine decay scheduler with a maximum\nlearning rate of 0.00005. The model was trained for 5 epochs with one epoch for\nthe warm-up step. The correction step is performed per 500 updates. Similar to\npre-training, a batch size of 16 was used and random data augmentation was\nperformed during the training. Since the differences between the image classes in\nCXR are less discernible than those in natural image, the cropped area should be\nlarger to capture the distinctive information between the CXRs. Therefore, we\nused a larger range of crop window sizes for both global and local crops with\n75– 100% and 20– 60% of the entire image, respectively, compared with those of\n40– 100% and 5– 40% for the DINO method. The cross-entropy loss was used as\nthe loss function for both the classiﬁcation and the self-supervising losses. All\nexperiments including preprocessing, model development, and evaluation were\nperformed using Python 3.8.5, Pytorch 1.8.0, Numpy 1.22.2, Pillow 9.0.1, Opencv-\npython 4.5.5.62, timm 0.5.4, scikit-learn 1.0.2 libraries and CUDA 11.1 on NVI-\nDIA Quadro 6000, GeForce RTX 3090, and RTX 2080 Ti.\nDetails of evaluation. For evaluation of the overall model performances, the three\nindependent test sets were pooled and then used to evaluate the overall perfor-\nmance of the model, while the model performances in three individual test sets\nwere reported separately. The area under the receiver operating characteristics\ncurve (AUC) was used as the primary evaluation metric, and the sensitivity, spe-\nciﬁcity, and accuracy were also calculated to meet the pre-deﬁned sensitivity value\n≥80% by adjusting the thresholds, if possible. To statistically compare the proposed\nmethod with others, the DeLong test was performed to estimate 95% conﬁdence\nintervals (CIs) andp-values. Statistically signiﬁcant differences were deﬁned as\np < 0.050. All statistical tests were two-sided.\nDirect visualization of the model attention is another merit of the ViT model.\nSimilar to the approach introduced in a self-supervised learning approach for\nViT\n12, we used the attention weights of multi-head in the last layer of the\nTransformer encoder to visualize attention. For comparison, the model attention\nwas visualized indirectly with the Grad-CAM\n19 that generates the model attention\nwith the linear combination determined by the gradients of the output with regard\nto the last layer feature map, for the CNN-based models.\nReporting summary. Further information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nData availability\nPart of CXRs is compiled from publicly available open-source data repositories. The\nCheXpert repository is available athttps://stanfordmlgroup.github.io/competitions/\nchexpert/, The BIMCV repository is available athttps://github.com/BIMCV-CSUSP/\nBIMCV-COVID-19. The India tuberculosis repository can be found athttps://www.\na b\nTeacher model Student model\nMore \ninformative view\nLess informative \nview\nPseudo-label Prediction\nMatch\nSmall \nLabeled data\nLarge unlabeled data\nlabel\nxl yl\nCorrect\nSenior reader Junior reader\nMore informative \nmodality\nLess informative \nmodality\nReference Reading\nMatch\nTypical case \nin textbook\nLarge clinical cases\nCorrect\n“Representative case \nactive tuberculosis”\nFrom same CXR From same case\nFig. 9The analogy between the artiﬁcial intelligence (AI) model training with the proposed DISTL method and human reader training.a The learning\nprocess of AI model trained with the proposed DISTL method.b The learning process of a radiologist. CXR, chest X-ray.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 9\nkaggle.com/raddar/chest-xrays-tuberculosis-from-india. Montgomery and Shenzen data\ncan be requested via the contact on the follwing webpagehttps://openi.nlm.nih.gov.\nBelarus tuberculosis repository is available athttps://github.com/frapa/tbcnn/tree/\nmaster/belarush. The PADChest repository is available athttps://github.com/auriml/Rx-\nthorax-automatic-captioning. The TBX 11K repository can be accessed athttps://www.\nkaggle.com/usmanshams/tbx-11. NIH normal data can be found athttps://cloud.google.\ncom/healthcare-api/docs/resources/public-datasets/nih-chest and NIH tuberculosis data\ncan be available athttps://tbportals.niaid.nih.gov/downloaddata after getting permission\nfrom TB portal. The SIIM-ACR Pneumohtorax Segmentation dataset is available at\nhttps://www.kaggle.com/c/siim-acr-pneumothorax-segmentation. Brixia COVID-19 data\nrepository can be found athttps://brixia.github.io/. Other part of institutional data, which\nwere used with institutional permission through IRB approval for this study, are not\npublicly available due to the patient privacy obligation. Interested users can request the\naccess to these data for research, by contacting the corresponding author J.C.Y.\n(jong.ye@kaist.ac.kr). Any access to de-identiﬁed institutional data requires IRB approval\nat the requesting institution along with the signed agreement on data transfer and usage.\nReplies to initial request will be made within 10 working days and follow-up based on the\nanswers will be made within institutional review cycles. Use of data is limited to research\npurposes and redistribution of data is not allowed. Source data are provided with\nthis paper.\nCode availability\nThe Pytorch codes for the DISTL used in this study is available at the following Github\nrepository at https://doi.org/10.5281/zenodo.6623257\n34.\nReceived: 3 February 2022; Accepted: 16 June 2022;\nReferences\n1. Gulshan, V. et al. Development and validation of a deep learning algorithm for\ndetection of diabetic retinopathy in retinal fundus photographs.JAMA 316,\n2402– 2410 (2016).\n2. De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral\nin retinal disease.Nat. Med. 24, 1342– 1350 (2018).\n3. Rajpurkar, P. et al. CheXNet: radiologist-level pneumonia detection on chest\nX-rays with deep learning. Preprint athttps://arxiv.org/abs/1711.05225\n(2017).\n4. Ting, D. S. et al. AI for medical imaging goes deep.Nat. Med. 24, 539– 540\n(2018).\n5. Giger, M. L. Machine learning in medical imaging.J. Am. College Radiol.15,\n512– 520 (2018).\n6. Pesapane, F., Codari, M. & Sardanelli, F. Artiﬁcial intelligence in medical\nimaging: threat or opportunity? Radiologists again at the forefront of\ninnovation in medicine.Eur. Radiol. Exp.2,1 – 10 (2018).\n7. Lakhani, P. & Sundaram, B. Deep learning at chest radiography: automated\nclassiﬁcation of pulmonary tuberculosis by using convolutional neural\nnetworks. Radiology 284, 574– 582 (2017).\n8. Pasa, F., Golkov, V., Pfeiffer, F., Cremers, D. & Pfeiffer, D. Efﬁcient deep\nnetwork architectures for fast chest X-ray tuberculosis screening and\nvisualization. Sci. Rep. 9,1 – 9 (2019).\n9. Harris, M. et al. A systematic review of the diagnostic accuracy of artiﬁcial\nintelligence-based computer programs to analyze chest X-rays for pulmonary\ntuberculosis. PLoS ONE 14, e0221339 (2019).\n10. Qin, Z. Z. et al. Tuberculosis detection from chest X-rays for triaging in a high\ntuberculosis-burden setting: an evaluation ofﬁve artiﬁcial intelligence\nalgorithms. Lancet Digital Health3, e543– e554 (2021).\n11. Xie, Q., Luong, M.-T., Hovy, E. & Le, Q. V. Self-training with noisy student\nimproves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition10687– 10698 (2020).\n12. Caron, M. et al. Emerging properties in self-supervised vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition 9650– 9660 (2021).\n13. Bhat, P., Arani, E. & Zonooz, B. Distill on the go: online knowledge distillation\nin self-supervised learning. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition2678–\n2687 (2021).\n14. Dosovitskiy, A. et al. An image is worth 16×16 words: transformers for image\nrecognition at scale. InInternational Conference on Learning Representations\n(2021).\n15. World Health Organization. WHO Consolidated Guidelines on Tuberculosis\n(2021).\n16. SIIM-ACR. SIIM-ACR pneumothorax segmentation∣ Kaggle. https://www.\nkaggle.com/c/siim-acr-pneumothorax-segmentation (2021).\n17. Vayá, M. d. l. I. et al. BIMCV COVID-19+: a large annotated dataset of RX\nand CT images from COVID-19 patients. Preprint athttps://arxiv.org/abs/\n2006.01174 (2020).\n18. Signoroni, A. et al. End-to-end learning for semiquantitative rating of\nCOVID-19 severity on Chest X-rays. Preprint athttps://arxiv.org/abs/2006.\n04603 (2020).\n19. Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via\ngradient-based localization. InProceedings of the IEEE International\nConference on Computer Vision618– 626 (2017).\n20. Raza, K. & Singh, N. K. A tour of unsupervised deep learning for medical\nimage analysis. Curr. Med. Imaging17, 1059– 1077 (2021).\n21. Ahn, E., Kumar, A., Feng, D., Fulham, M. & Kim, J. Unsupervised deep\ntransfer feature learning for medical image classiﬁcation. In 2019 IEEE 16th\nInternational Symposium on Biomedical Imaging (ISBI 2019)1915– 1918\n(IEEE, 2019).\n22. Chen, L. et al. Self-supervised learning for medical image analysis using image\ncontext restoration. Med. Image Anal.58, 101539 (2019).\n23. Liu, Q., Yu, L., Luo, L., Dou, Q. & Heng, P. A. Semi-supervised medical image\nclassiﬁcation with relation-driven self-ensembling model.IEEE Trans. Med.\nImaging 39, 3429– 3440 (2020).\n24. Kim, G. Recent deep semi-supervised learning approaches and related works.\nPreprint at https://arxiv.org/abs/2106.11528 (2021).\n25. Irvin, J. et al. Chexpert: a large chest radiograph dataset with uncertainty labels\nand expert comparison.Proc. AAAI Conf. Artif. Intell.33, 590– 597 (2019).\n26. National Institutes of Health. TB portals - home.https://tbportals.niaid.nih.\ngov/ (2021).\n27. radder. TBXPredict - browse/data at sourceforge.net.https://www.kaggle.com/\nraddar/chest-xrays-tuberculosis-from-india (2021).\n28. Jaeger, S. et al. Two public chest X-ray datasets for computer-aided screening\nof pulmonary diseases.Quant. Imag. Med. Surg.4, 475 (2014).\n29. Pasa. tbcnn/belarus at master⋅frapa/tbcnn⋅github. https://github.com/frapa/\ntbcnn/tree/master/belarus (2021).\n30. Bustos, A., Pertusa, A., Salinas, J.-M. & de la Iglesia-Vayá, M. Padchest: a large\nchest X-ray image dataset with multi-label annotated reports.Med. Image\nAnal. 66, 101797 (2020).\n31. Liu, Y., Wu, Y.-H., Ban, Y., Wang, H. & Cheng, M.-M. Rethinking computer-\naided tuberculosis diagnosis. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition2646– 2655 (2020).\n32. Ye, W., Yao, J., Xue, H. & Li, Y. Weakly supervised lesion localization\nwith probabilistic-cam pooling. Preprint athttps://arxiv.org/abs/2005.14480\n(2020).\n33. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations(2021).\n34. sangjoon park. sangjoon-park/AI-Can-Self-Evolve: Code for“DISTL:\ndistillation for self-supervised and self-train learning”. https://doi.org/10.5281/\nzenodo.6623233 (2022).\nAcknowledgements\nWe are grateful to Yisak Kim (Seoul National University Hospital) for his help in\ncollecting validation data. This research was supported by the National Research\nFoundation (NRF) of Korea under Grant NRF-2020R1A2B5B03001980, the Korea\nMedical Device Development Fund grant funded by the Korea government (the Ministry\nof Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health &\nWelfare, the Ministry of Food and Drug Safety) (Project Number: 1711137899,\nKMDF_PR_20200901_0015), Institute of Information & communications Technology\nPlanning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-\n0-00075, Artiﬁcial Intelligence Graduate School Program (KAIST)), Institute of Infor-\nmation & communications Technology Planning & Evaluation (IITP) grant funded by\nthe Korea government (MSIT) (No. 2021-0-02068, Artiﬁcial Intelligence Innovation\nHub), the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information\nTechnology Research Center) support program (IITP-2022-2020-0-01461) supervised by\nthe IITP (Institute for Information & communications Technology Planning & Eva-\nluation) and the KAIST Key Research Institute (Interdisciplinary Research Group)\nProject.\nAuthor contributions\nS.P. performed all experiments, wrote the extended code, and prepared the manuscript.\nG.K. and Y.O. contributed in data preprocessing. J.B.S., S.M.L., J.H.K., S.M., and J.K.L.\ncollected and labeled data. C.M.P. advised the project in conception. J.C.Y. supervised the\nproject in conception and discussion, and prepared the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x\n10 NATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41467-022-31514-x.\nCorrespondence and requests for materials should be addressed to Jong Chul Ye.\nPeer review informationNature Communications thanks Euijoon Ahn and the other,\nanonymous, reviewer(s) for their contribution to the peer review of this work. Peer\nreviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-31514-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3848 | https://doi.org/10.1038/s41467-022-31514-x | www.nature.com/naturecommunications 11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7476668357849121
    },
    {
      "name": "Transformer",
      "score": 0.663650631904602
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5858524441719055
    },
    {
      "name": "Machine learning",
      "score": 0.47868281602859497
    },
    {
      "name": "Distillation",
      "score": 0.4529291093349457
    },
    {
      "name": "Deep learning",
      "score": 0.4265858829021454
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}