{
  "title": "GMAT: Global Memory Augmentation for Transformers",
  "url": "https://openalex.org/W3033182847",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2184172997",
      "name": "Gupta, Ankit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221504776",
      "name": "Berant, Jonathan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2953163841",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3015883388",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W2995359496",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2896197082",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the $\\textit{pairwise dot-product}$ attention that has a large $Ω(L^2)$ memory requirement for length $L$ sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable $O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.",
  "full_text": "GMAT: Global Memory Augmentation for\nTransformers\nAnkit Gupta\nTel Aviv University\nankitgupta.iitkanpur@gmail.com\nJonathan Berant\nTel Aviv University, Allen Institute for AI\njoberant@cs.tau.ac.il\nAbstract\nTransformer-based models have become ubiquitous in natural language processing\nthanks to their large capacity, innate parallelism and high performance. The contex-\ntualizing component of a Transformer block is the pairwise dot-product attention\nthat has a large Ω(L2) memory requirement for length Lsequences, limiting its\nability to process long documents. This has been the subject of substantial interest\nrecently, where multiple approximations were proposed to reduce the quadratic\nmemory requirement using sparse attention matrices. In this work, we propose to\naugment sparse Transformer blocks with a dense attention-based global memory\nof length M (≪L) which provides an aggregate global view of the entire input\nsequence to each position. Our augmentation has a manageable O(M ·(L+ M))\nmemory overhead, and can be seamlessly integrated with prior sparse solutions.\nMoreover, global memory can also be used for sequence compression, by represent-\ning a long input sequence with the memory representations only. We empirically\nshow that our method leads to substantial improvement on a range of tasks, includ-\ning (a) synthetic tasks that require global reasoning, (b) masked language modeling,\nand (c) reading comprehension.\n1 Introduction\nThe Transformer architecture [26] has been widely successful in achieving state-of-the-art perfor-\nmance on a wide range of natural language processing (NLP) tasks, including machine translation\n[4], language modeling [ 25], question-answering [ 8], and many more. In particular, Transform-\ners pre-trained on large amounts of text with a language modeling (LM) objective, have become\nthe de-facto standard in NLP, exhibiting surprising amounts of linguistic and world knowledge\n[18, 3, 13, 12, 19, 6, 24]. Moreover, Transformers with tailored attention patterns have been success-\nfully used to replace convolutions in computer vision [16, 15], and have also been useful in music\ngeneration [7], symbolic mathematics [11] and other modalities.\nOne of the most powerful features in Transformers is (pairwise) self-attention, where all positions\nin an input sequence aggregate information from the entire sequence in parallel. However, this\nrequires computing a similarity score for all pairs of positions simultaneously, leading to a Ω(L2)\nmemory requirement for length Lsequences, which is prohibitively expensive for long sequences.\nTo alleviate this issue, severalsparsiﬁcations of vanilla self-attention have been recently proposed;\neach restricting the number of positions that a given position can attend to [28, 2, 21, 30, 20, 9, 1].\nFor example, in BLOCK BERT [20], the sequence is split into L\nM chunks of length M and positions\nin chunk ionly attend to positions in chunk σ(i) for some pre-determined permutation σ, thereby\nhaving a O(M ·L) memory requirement. The REFORMER [9] uses locality-sensitive hashing (LSH)\nto arrange similar vectors close to one another and then chunks them. Each chunk then attends to\nonly a couple of chunks leading to a O(M ·L) memory requirement. While such sparsiﬁcations\noften lead to performance that is comparable to vanilla Transformers, they have some undesirable\nconsequences:\nPreprint. Under review.\narXiv:2006.03274v1  [cs.LG]  5 Jun 2020\nFigure 1: (a) GMAT: For each attention head, tokens of the main sequence X (blue) use approximate/sparse\nattention for attending to X, and vanilla attention for attending to the global memory XM . Tokens of XM (red)\nattend to all tokens using vanilla attention. (b) chunked self-attention: tokens of the main sequence use vanilla\nattention for attending to their respective chunk and global memory, but do not attend to other chunks.(c) GMAT\nfor sequence compression for a model with N = Nc + Nm + Nd layers.\n• A position can require many layers to accumulate information from the entire input, and thus\nstruggle when aggregating global information is necessary. For example, in §3.1, we show that a\nLSH Transformer (REFORMER without reversible connections) struggles on a simple tagging task\nthat requires information from the entire sequence.\n• Most sparsiﬁcation schemes pose an inductive bias based on the locality of natural language, by\nrestricting a position to only attend to its nearby tokens. While this is often a reasonable assumption,\nit raises several concerns. First, it is trivial to construct examples where locality is violated. For\nexample, vanilla attention is invariant to input permutation, and thus can handle tasks such as\n“word deshufﬂing”, where a randomly shufﬂed sequence needs to be mapped to the original order.\nOn the other hand, any locality-based inductive bias would be detrimental. Second, progress in\nnatural language understanding has led to increasing interest in handling global dependencies in\nlong documents and even entire books [10], where a locality-based inductive bias is sub-optimal.\nIn this work, we propose Global Memory Augmentation for Transformers (GMAT). We augment\nsparse variants of the Transformer with a small global memory which is read and updated by all the\npositions using vanilla attention. Speciﬁcally, we preﬁx every input sequence (of length L) with a list\nof M memory tokens. At each multi-head attention layer, for each head (Figure 1a), the Ltokens1 of\nthe main sequence attend to other tokens of the main sequence using any sparse variant of attention,\nwhereas they attend to theM memory tokens using vanilla dense attention. Moreover, theM memory\ntokens attend to all M+ Ltokens using vanilla attention. This results in a O(M·(L+ M)) memory\noverhead which is manageable for M ≪L. Because the number of parameters in Transformers\ndoes not depend on the length of the input (modulo learned positional embeddings), the number of\nparameters grows by only a negligible M ·Eparameters, for an embedding size E.\nWe propose also to use GMAT for sequence compression (Figure 1c). After encoding an input\nsequence with Nc GMAT layers, we discard the vectors corresponding to the main sequence X, and\nkeep only the global memory vectors, which are now a compressed representation of the entire input.\nThe memory vectors are then processed and decompressed using Nd layers back to the original input\nlength. The sequence can now be stored using only M(≪L) vectors, and decompression is done\nwith a small number (Nd) of GMAT layers.\nWe evaluate GMAT on a wide range of tasks and show: (a) large improvements on synthetic tasks\nwhere global reasoning is required, (b) it improves masked langauge modeling (MLM) accuracy\n(used in Transformer pre-training), (c) improvements on two reading comprehension (RC) tasks, and\nlast (d) moderate reductions in MLM and RC performance when using GMAT for compression.\nTo summarize, GMAT is a simple extension of the Transformers architecture, that can be seamlessly\ncombined with any sparse attention variant. We show GMAT is useful for reducing memory require-\nments as well as for sequence compression, and demonstrate performance enhancements on a wide\nrange of tasks. Our code and data can be downloaded from https://github.com/ag1988/gmat.\n1For brevity, we use the word token to refer both to the input token and its contextualized representation\ninterchangeably.\n2\n2 Global-Memory Augmented Transformers\nA Transformer [26] is a stack of layers each consisting of sub-layers such as multi-head attention,\nfeed-forward, etc. Its contextualizing component is the multi-head attention deﬁned as follows.\nMulti-head Attention Given a query Q∈RLQ×d, key K ∈RLK×d and value V ∈RLK×d, the\noutput of scaled dot-product attention is deﬁned as:\nAttention(Q,K,V ) = softmax\n(QKT\n√\nd\n)\nV. (1)\nIn multi-head attention, instead of computing a single attention output with dmodel dimensional keys,\nqueries, and values, these are linearly projected down in parallel htimes to d= dmodel/hdimensions,\nusing different learned projection matrices. Attention is applied to each of the hnew queries, keys\nand values, yielding ddimensional outputs which are concatenated and again projected to obtain the\ndmodel-dimensional output.\nThe attention function (Eq. 1) requires the computation of QKT containing LQ ·LK entries and can\nbe expensive for long sequences. To alleviate this issue, sparse attention variants [2, 30, 20, 9, 1]\nrelax this requirement and compute only a few entries of QKT , masking out the rest. For a binary\nmask2 B ∈{0,−∞}LQ×LK ,\nSparseAttention(Q,K,V,B ) = softmax\n(QKT\n√\nd\n+ B\n)\nV. (2)\nGlobal Memory As explained in §1, sparse attention variants have some undesirable properties. To\nremedy this, we augment such models with a small global memory which is read and updated by all\nthe positions using vanilla attention (Figure 1a). Speciﬁcally, we preﬁx every token sequence X (of\nlength L) with a sequence of M memory tokens [m1],..., [mM]. At each multi-head attention layer\nof the model, at each head, the Lrepresentations Xcorresponding to the tokens of the main sequence\nattend to the other positions in X using any sparse attention variant, but attend to the representations\nof all memory tokens XM normally (Eq. 3). Moreover, the memory tokens attend to all the M + L\ntokens normally.\n˜X = SparseAttention\n(\nX,\n[\nX\nXM\n]\n,\n[\nX\nXM\n]\n,\n[\nB\n0\n])\n, ˜XM = Attention\n(\nXM,\n[\nX\nXM\n]\n,\n[\nX\nXM\n])\n.\n(3)\nThis results in a O(M ·(L+ M)) memory overhead (manageable for M ≪L). Moreover, this does\nnot add any parameters to the model, except for a negligible M ·Eparameters used to embed the M\nnew memory tokens with an embedding size of E.\nChunked self-attention To explore the limits of GMAT and highlight its ability to contextualize\nover multiple fragments via a memory, we work with chunked self-attention (Figure 1b), a simple\nsparsiﬁcation method. In C×kchunked self-attention (Figure 1b), a sequence of length C·kis\npartitioned into k contiguous chunks of length C. Each token within a given chunk uses vanilla\n(multi-head) attention to attend to tokens in its chunk in addition to the global memory but does not\nattend to other chunks. Hence, chunks interact with each otheronly via the memory. Without memory,\ntraining with C ×k attention is equivalent to training with vanilla Transformer over a length- C\nsequence. While more complex sparsiﬁcation schemes are possible, this setup focuses on the ability\nto contextualize disjoint segments through the global memory only. Note that a single model can\nbe used with different values of Cand k, as Transformer models are invariant to the length of their\ninput, aside for positional embeddings, which we handle below. We use the notation (C×k,M) to\ndenote a chunked self-attention model where the input sequence X has length C·kwith a global\nmemory of size M.\nPositional Embeddings As attention is invariant to order, it is important to supply the model with\npositional information corresponding to the individual tokens. Rather than have a distinct learnable\nvector for each position [26], we represent a position pas a tuple (q,r) where r= p(mod 512), and\nq = ⌊p/512⌋. Each 0 ≤r <512 and 0 ≤q <64 has a distinct learnable vector.3 The positional\n2The sparsity of B can be leveraged via customized implementations of matrix product [2, 1].\n3These particular values allow us to initialize the vectors for r with the learned 512 positional embeddings\nof pre-trained LMs such as BERT.\n3\nembedding of pis represented by the sum of the vectors corresponding to qand rand allows us to\nmodel positions up to 215. Memory tokens have a ﬁxed position, and thus positional embeddings are\nused only for the main sequence X.\n2.1 Sequence Compression\nContextualized word representations [18, 3] improve performance compared to ﬁxed word embed-\ndings such as GloVe [ 17]. Unfortunately, some large models that compute contextualized repre-\nsentations do not ﬁt on popular GPUs and need specialized hardware [22]. Instead of leaving this\ncomputation to the users, an appealing option might be to release pre-computed contextualized\nrepresentations, at least for popular benchmarks, similar to word embeddings [17]. However, storing\na vector for each position in a large corpus is expensive. A second natural motivation for GMAT is for\nsequence compression, i.e. using a small memory to represent a long sequence. This can dramatically\nreduce the overhead in storing pre-computing contextualized representations for large corpora.\nConsider a N-layer GMAT with a memory of size M. We apply the N model layers in 3 steps as\nshown in Figure 1c. Given a length Linput sequence, let W and P denote its word and positional\nembeddings. First, the bottom Nc layers are applied for compressing the entire information of the\ninput sequence into just M memory vectors X(c)\nM . The next Nm layers are then applied only on the\nM memory vectors resulting in richer representations X(m)\nM . This length M sequence is restored to\nthe original length M + Lby concatenating the positional embeddings P and, ﬁnally, the remaining\nNd = N −Nc −Nm layers are applied for decompressing the information packed in X(m)\nM into the\nﬁnal representations. Here, the positional embeddings P act as queries for restoring the original input\ninformation from X(m)\nM .\nThe M(≪L) vectors X(m)\nM can be efﬁciently serialized for later use and are decompressed using\nminimal post-processing into representations of length L. In §4.3 and §4.4, we show that using the\ndecompressed representations leads to only a small performance degradation on masked language\nmodeling and reading comprehension. We use positional embeddings P in the decompression step\nand not the contextualized representation X(c) (Figure 1c), since we want the output to depend only\non M vectors instead of M + L.\nComparison to Longformer In contemporaneous work [1], the authors demonstrate the effective-\nness of a sparse sliding window attention pattern combined with a global memory on multiple natural\nunderstanding tasks. In contrast to our work, the contribution of the global memory component is not\nevaluated, and is designed on a task-by-task basis. Moreover, attention scores for the memory and\nthe main sequence are computed using separate sets of projection matrices, thereby introducing new\nparameters. In comparison, we demonstrate the utility of a global memory for various sparse attention\npatterns on synthetic, NLP and compression tasks, introducing a minimal set of new parameters.\n3 Global Reasoning on Synthetic Data\nWe consider two synthetic datasets, where global reasoning over the entire input is required.\n3.1 Majority Tagging\nRecently proposed sparse attention schemes exploit the locality of natural language (§1). One\nexception is locality sensitive hashing (LSH) attention [ 9]. While in LSH attention, tokens are\nnot bound to attend only within their proximity, it does limit the number of positions a token can\nattend to at each head. We examine the utility of adding GMAT to a LSH Transformer [9], that is, a\nTransformer that uses LSH attention, for a sequence tagging task.\nMAJORITY(L,p): Let X = (x1,...,x L) be a sequence of Lintegers from the set {1,..., 2p}.\nLet Ni denote the number of occurrences of iin X. For every even integer iin the domain, deﬁne\nmaj(i−1) = maj(i) =\n{i−1 Ni−1 ≥Ni\ni otherwise .\nThen the majority sequence is deﬁned to be (maj(x1),..., maj(xL)). Note that for p= 1, the task\nrequires tagging all tokens of X by their mode.\n4\n(a) 2-layers\n (b) 12-layers\nFigure 2: Evaluation exact match (EM) of LSH Transformer on theMAJORITY(L, p) task of §3.1. M denotes\nthe memory size. Models in the same ﬁgure use the same hyperparameters.\nTo create the data, we sampled the elements of X independently and uniformly from the domain.\nAfter training a 2-layer LSH Transformer on the above task (Figure 2a) we compared its performance\nwith and without a global memory of size M = 8. Hyperparamaters for LSH attention (bucket size,\nrounds of hashing, etc.) were used as suggested by [9] and the other hyperparameters (hidden size,\netc.) were taken from BERT-base (without dropout). As shown in Table 1, we found that LSH\nattention failed to do well on the MAJORITY(8192,1) task. On much shorter inputs of length 512,\nit managed to do well on MAJORITY(512,1) but again struggled on a slightly more complex task\nof MAJORITY(512,3). Conversely, GMAT obtains near perfect performance on these tasks.\nMAJORITY(8192, 1) MAJORITY(512, 1) MAJORITY(512, 3)\nM = 0 0.15 0.92 0.86\nM = 8 0.98 1.0 0.98\nTable 1: Evaluation exact match (EM) of a 2-layer LSH Transformer on majority tagging. EM for an example is\n1 iff every position is tagged correctly. M denotes the memory size.\nTo determine if GMAT also leads to lower sample complexity in deeper models, we trained (Figure 2b)\na 12-layer model on MAJORITY(1792,4) and found GMAT obtains better performance with lower\nsample complexity. This suggests that in LSH attention, while a certain level of sparsity works well\nwhen information is mostly local, it can be too restrictive for inherently global tasks.\n3.2 Numerical Reasoning Over Text\nHaving shown the utility of GMAT on a combinatorial task, we now transition towards language tasks.\nWe create a pseudo-textual task that requires global mathematical reasoning by generating examples\nfrom a rule-based generator from [5]. The generator generates examples that include a passage P,\ndescribing a sequence of related events, and a question Qthat requires aggregating information from\nvarious parts of P and performing numerical reasoning to arrive at a numeric answer A(see Table 2).\nP: The commander recruited 358 households and 9669 Italian troops. The commander lost 812 of the\nhouseholds. The commander recruited 542 households in France. The commander recruited 3075 households\nin France . The commander recruited 2843 households and 5413 Native American troops.\nQ: How many more Italian troopsdid the commander have than Native American troops? A: 4256\nTable 2: An example from the textual synthetic data used in §3.2.\nFollowing [5], we train a generative encoder-decoder model, GENBERT, on the generated data\nafter replacing the encoder self-attention [ 26] with chunked self-attention (§2), and compare the\nperformance before and after GMAT augmentation (see training and data details in §A.1). We\nsummarize the results in Table 3. Compared to vanilla attention over the entire input (i.e.,(140×1,0)),\nchunking the encoder input into 2 chunks signiﬁcantly reduced the performance (i.e., (70 ×2,0)),\nin line with the global nature of the task. Surprisingly, adding a global memory of size 30 reduced\naccuracy even further. We hypothesize this is due to the strict weight-tying strategy employed by\nGENBERT, where the parameters of the Transformer encoder and decoder are tied, leading to\nunderﬁtting. To handle that, we untie the parameters of the projection matrices that update the\nmemory representations XM in all attention heads (Eq. 3, right), initializing them randomly. This\nseparates the attention heads that update the main sequence from the heads that update the memory.\nIn this setup, accuracy improved substantially, almost recovering the performance of vanilla attention.\n5\n(70 × 2, 0) (70 × 2, 30) (70 × 2, 30) (untied) (140 × 1, 0)\n62.2 51.4 86.7 93.9\nTable 3: Evaluation exact match (EM) of GENBERT on the textual synthetic data. EM for a sample is 1 iff every\ndigit of the desired number is predicted correctly. All models use same hyperparamters.\n(a) Wikipedia (random init)\n (b) Wikipedia (BERT init)\n (c) PG19 (BERT init)\nFigure 3: Evaluation error for the MLM task in 3 different setups.\n4 Masked Language Modeling\nOne of the biggest success stories of Transformers is as an architecture for pre-training LMs. We now\ninvestigate pre-training GMAT with a masked language modeling objective, as a memory-efﬁcient\nreplacement for models such as BERT [3, 13]. Past work has shown strong correlation between\nperformance on the MLM task and that on downstream applications [13, 12]. For our experiments,\nwe use the BERT -base architecture [3] after making the modiﬁcations described in §2.\nWe form examples by sampling sequences of length Lfrom English Wikipedia and the PG19 dataset,\nand replacing sub-words with the [MASK] token following the procedure in [3] (details in §A.2). The\nmodel is trained to maximize the log probability of the masked out tokens. We evaluate the error of\nthe model as the fraction of tokens predicted incorrectly, and the MLM “perplexity” as the reciprocal\nof the geometric mean of probabilities of all masked out tokens.4 PG19 contains 29K long books,\nand is thus likely to beneﬁt from modeling long context, while in Wikipedia most articles are short\nand can ﬁt into the 512 word pieces that are the input of BERT. We experiment with training a\nTransformer from scratch, as well as initializing with BERT-base.\n4.1 Random Initialization\nAs shown in Figure 3a, we train 3 models on Wikipedia. The setting (512 ×1,0) corresponds to\nstandard MLM training on instances of length 512 without global memory. Similarly, (1024 ×1,0)\ndenotes training with vanilla attention over a context of size 1024, incurring a large memory penalty.5\nLastly, in (512 ×4,64), a 2048-long context is chunked into four 512-chunks that have to interact\nvia a global memory of size 64. Increasing the context size to 1024 improves MLM performance on\nWikipedia (Table 4). Using global memory improves sample complexity and performance compared\nto training on 512-long instances, albeit only moderately. Thus, the chunks are able to leverage global\nmemory to exchange information, alleviating the context fragmentation problem [28].\nsetting (512 × 1, 0) (512 × 4, 64) (1024 × 1, 0)\nbest evaluation error / perplexity 33.67 / 5.14 33.25 / 5.11 31.53 / 4.64\nTable 4: MLM training on Wikipedia from random initialization for 431K steps. Error denotes the percentage of\nmasked tokens predicted incorrectly.\n4.2 BERT Initialization\nTo show that GMAT can be easily integrated into existing pre-trained LMs, we take a pre-trained\nBERT-base model, and further train it using GMAT. Because BERT was pre-trained on Wikipedia,\nimproving performance on Wikipedia itself could be difﬁcult, as it already has high conﬁdence on\ntokens from this corpus. Hence, we also train on PG19, which was not part of BERT’s training data.\n4Equivalently, the natural exponential of the average loss over the development set.\n5We do not train in the (2048 × 1, 0) setting due to memory constraints.\n6\nTable 5 summarizes the results. On Wikipedia, increasing the context size to 1024 provides a\nsigniﬁcant improvement (Figure 3b), but global memory does not improve performance compared to\nstandard MLM training on 512-long instances. However, on PG19 (Figure 3c) using global memory\nsubstantially improves perplexity from 4.4 →4.35, closing roughly half the gap from using a context\nof size 1024, which obtains an MLM perplexity of 4.3. This hints that the lack of improvement on\nthe Wikipedia data might be due to the fact that BERT was pre-trained on Wikipedia.\nsetting BERT (no training) (512 × 4, 0) (512 × 4, 64) (1024 × 2, 0) (8 × 64, 0) (8 × 64, 64)\nWikipedia 35.2 / 6.856 29.163 / 3.953 29.15 / 3.956 28.74 / 3.87 53.11 / 20.68 32.98 / 4.94\nPG19 42.5 / 10.57 31.90 / 4.40 31.72 / 4.35 31.44 / 4.30 - -\nTable 5: Evaluation error / perplexity for MLM training.. Models are initialized with BERT-base, except for\n(8 ×64, 0), (8 ×64, 64), which are initialized with the trained models (512 ×4, 0), (512 ×4, 64) respectively.\nThe above results indicate that disjoint text segments can exchange useful information via global\nmemory. However, because natural language has a locality bias, the utility of memory diminishes\nas the chunk length C increases. To determine the efﬁcacy of GMAT when C is small, where\ncontextualization should highly depend on the memory, we experiment with chunks of size 8. As\nexpected, without access to a reasonably-large surrounding context, the model (8 ×64,0) fails\nto predict masked tokens (Table 5). Interestingly, a small global memory of size 64 signiﬁcantly\nimproves performance (53.11 →32.98 error, 20.68 →4.94 perplexity), reaching performance that\nis close to (512 ×4,0). We further evaluate the pre-trained GMAT models on reading comprehension\ntasks in §4.4.\n4.3 Sequence Compression\nWe turn to sequence compression, where our goal is to compress a sequence of length Linto M\nvectors that can be saved and later decompressed back into a sequence of length L, with minimal\ndrop in performance. Using the setup described in §2.1, we use Nc compression layers, followed by\nNd = N −Nc decompression layers, and train with the same data and MLM objective as above on\nWikipedia. As shown in Table 6, we found that Nc = 9 outperforms Nc = 3 (which also happens to\nbe well-aligned with our need for a small number of decompression layers). Compared to a model\nwithout compression, we observe a moderate degradation in performance (29.163 →32.98 error, and\n3.953 →5.017 MLM perplexity), showing that a global memory of size just 64 provides a compact\nand useful representation for the entire sequence of length 512.\nsetting (512 × 4, 0) (512 × 1, 64), Nc = 3 (512 × 1, 64), Nc = 9\ninitialization BERT (512 × 4, 64) (512 × 4, 64)\nbest evaluation error / perplexity 29.163 / 3.953 33.44 / 5.112 32.98 / 5.017\nTable 6: MLM training on Wikipedia with compression. Compressed models were initialized with the (512 ×\n4, 64) model trained in §4.2 and further trained for 440K steps.\n4.4 Reading Comprehension Performance\nWhile MLM performance is known to correlate well with downstream applications [ 13, 22], we\ntake Wikipedia-based GMAT models trained with the MLM objective in §4.2 and §4.3 and further\nﬁne-tune them on reading comprehension (RC) tasks.\nSQUAD We ﬁrst ﬁne-tune on SQUAD v1 [23], using the simple sliding-window based approach of\n[3]. Similar to past work [12], we limit the input size to 384 tokens, as most paragraphs are relatively\nshort. We train all models using identical hyperparameters. Summarized in Table 7, the model\n(512 ×4,64) improves performance over BERT (88.6 →89.2 F1), indicating global memory helps\neven with vanilla self-attention. The performance of (512 ×4,0) is very similar to BERT, ruling out\nthe possibility that the performance of (512 ×4,64) was a result of extra pre-training on Wikipedia.\nSurprisingly, the model (8 ×64,64) reported 84.2 F1, a moderate drop in performance given that,\nwith chunks of size 8, the contextualization depends almost entirely on the memory. Interestingly, the\ncompression model with Nc = 9 reported 87.1 F1 (compared to BERT’s88.6) an impressive score\ngiven that, after 9 layers, the information of the entire input must pass through only 64 vectors.\n7\nBERT (512 × 4, 0) (512 × 4, 64) (8 × 64, 64) (8 × 64, 0) (512 × 1, 64)\nNc = 3\n(512 × 1, 64)\nNc = 9\n81.09 / 88.60 80.93 / 88.47 81.77 / 89.16 75.64 / 84.17 9.82 / 14.60 76.59 / 84.58 79.55 / 87.05\nTable 7: Evaluation EM/F1 on SQUAD v1.\nHOTPOT QA To investigate the utility of GMAT for long-range reasoning, we ﬁne-tune our models\non HOTPOT QA [29], an RC dataset focusing on multi-paragraph reasoning. InHOTPOT QA, examples\ncomprise of a question Q, 2 gold paragraphs G1,G2 required for answering Q, and 8 distractor\nparagraphs. Each gold paragraph contains supporting facts: the sentences relevant for answering Q.\nAs we are interested in analyzing if models can aggregate information dispersed in a long context, we\norder the paragraphs such that one of G1,G2 is among the ﬁrst 3 paragraphs, and the other is among\nthe last 3. We refer to this setting as the “gold-seperated” setting, denoted by subscriptGS. To reuse\nthe sliding-window setup from SQUAD, we concatenate the 10 paragraphs (details in §A.3) into a\nsingle long context D. Following [3], each input instance consists of Qconcatenated with a window\nP of contiguous text from Dwhose length is limited according to the maximum sequence length\nallowed by the model (512 for BERT, 2048 for (512 ×4,0) and (512 ×4,64)). We leverage the\nsupporting facts supervision provided in the dataset, and include a binary tagging task (denoted by\nSF) of independently tagging each token of the input by 1/0 indicating if it is part of a supporting fact\n(§A.3). Moreover, for GMAT models we replaced the input memory tokens by the ﬁrst 64 tokens of\nQ, as this improved performance.\nmodel BERT (512 × 4, 64)\nSF task included no yes no yes\nall 66.3 66.3 67.6 68.3\nonly-comparison 61.6 58.2 65.3 65.6\nTable 8: F1 scores on HOTPOT QAGS development set.\nThe results after ﬁnetuning on HOTPOT QAGS are summarized in Table 8. With the ability to\ncontextualize over a much larger context containing bothG1,G2, the model (512×4,64) reported an\nimproved performance compared to BERT (66.3 →68.3 F1). The improvement on the “comparison”\ntype of questions is even more pronounced (61.6 →65.6) as such questions usually require comparing\nquantities from each G1,G2 and are hard if either of G1,G2 is missing from the input. We omit\nresults for (512 ×4,0), because chunked self-attention without memory means that the chunk that\ncontains the question has no access to the entire context, leading to low performance.\nTo fairly compare (512 ×4,0) and (512 ×4,64) and study the effect of global memory, we create\na new setting, “question-repeated” (QR), in which, while forming the context D, Qis appended\nafter each paragraph thereby ensuring that all four 512-chunks of (512 ×4,0) have access to Q.\nMoreover, we repeat this experiment after creating an adversarial version ADV-HOTPOT QA of\nHOTPOT QA by following [14]. The results after ﬁne-tuning on these datasets are summarized in\nTable 9. Global memory improves model performance on both HOTPOT QAGS+QR (66.4 →67.4 F1)\nand ADV-H OTPOT QAGS+QR (64 →64.6 F1) reafﬁrming its utility for long-range reasoning.\nmodel (512 × 4, 0) (512 × 4, 64)\nSF task included no yes no yes\nHOTPOT QAGS+QR 65.4 66.4 66.0 67.4\nADV-HOTPOT QAGS+QR 62.6 64.0 64.0 64.6\nTable 9: F1 on HOTPOT QAGS+QR and ADV-H OTPOT QAGS+QR development sets.\n5 Conclusion\nIn this work, we proposed GMAT, a simple extension to the Transformer architecture that allows a bet-\nter trade-off between compute and performance and can be naturally used for sequence compression.\nOur approach can be seamlessly integrated with the increasingly-popular sparse Transformer variants.\nWe show GMAT (a) leads to performance improvements on a wide range of tasks, and (b) can be\nused to compress long sequences by factor of 8×with only a small degradation in performance.\n8\nBroader Impact\nTransformers have become a popular architecture for sequence processing and generation in natural\nlanguage processing and outside of it. The goal of this paper it to reduce the memory requirement\nand thereby allow for longer sequences to be processed. Moreover, our compression technique\ncan facilitate the use of pre-computed contextualized representations, allowing users access to an\napproximation of these representations even if they cannot compute the representations from scratch\nthemselves. As such, we consider a positive impact of this work to be the ability of more users\nwith constraints on their computational resources to use the Transformer architecture and its pre-\ntrained representations. Moreover, being able to process long documents can open the door to new\napplications in natural language processing, such as multiple-document understanding, and perhaps\nalso processing of sequences outside of NLP, for example in Biology. As Transformers are becoming\nubiquitous in machine learning, naturally any negative impact that can be attributed to Transformers\n(fake news generation, classiﬁers in sensitive domains such as the justice system and healthcare) are\nalso inherited by our approach, and perhaps enhanced when long sequences need to be processed.\nAcknowledgments and Disclosure of Funding\nWe thank Shimi Salant, Yoav Goldberg and Mor Geva for helpful discussions and constructive\nsuggestions. This research was partially supported by The Israel Science Foundation grant 942/16,\nThe Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the\nEuropean Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800).\nReferences\n[1] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020.\n[2] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers.arXiv\npreprint arXiv:1904.10509, 2019.\n[3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers\nfor language understanding. In North American Association for Computational Linguistics (NAACL) ,\npages 4171–4186, Minneapolis, Minnesota, June 2019.\n[4] S. Edunov, M. Ott, M. Auli, and D. Grangier. Understanding back-translation at scale. In Empirical\nMethods in Natural Language Processing (EMNLP), 2018.\n[5] M. Geva, A. Gupta, and J. Berant. Injecting numerical reasoning skills into language models. In ACL,\n2020.\n[6] J. Hewitt and C. D. Manning. A structural probe for ﬁnding syntax in word representations. In North\nAmerican Association for Computational Linguistics (NAACL), pages 4129–4138, 2019.\n[7] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman,\nM. Dinculescu, and D. Eck. Music transformer. In International Conference on Learning Representations,\n2019.\n[8] V . Karpukhin, B. O˘guz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for\nopen-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n[9] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. In International Conference\non Learning Representations, 2020.\n[10] T. Koˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis, and E. Grefenstette. The narrativeqa\nreading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–\n328, 2018.\n[11] G. Lample and F. Charton. Deep learning for symbolic mathematics. In International Conference on\nLearning Representations, 2020.\n[12] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised\nlearning of language representations. In International Conference on Learning Representations, 2020.\n[13] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov.\nRoberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[14] S. Min, E. Wallace, S. Singh, M. Gardner, H. Hajishirzi, and L. Zettlemoyer. Compositional questions do\nnot necessitate multi-hop reasoning. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4249–4257, 2019.\n9\n[15] N. Parmar, P. Ramachandran, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens. Stand-alone self-attention\nin vision models. In Advances in Neural Information Processing Systems, pages 68–80, 2019.\n[16] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, and A. Ku. Image transformer. CoRR,\nabs/1802.05751, 2018.\n[17] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word representation. In Empirical\nMethods in Natural Language Processing (EMNLP), pages 1532–1543, 2014.\n[18] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contex-\ntualized word representations. In North American Association for Computational Linguistics (NAACL),\n2018.\n[19] F. Petroni, T. Rocktäschel, P. Lewis, A. Bakhtin, Y . Wu, A. Miller, and S. Riedel. Language models\nas knowledge bases? In Proceedings of the Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2019.\n[20] J. Qiu, H. Ma, O. Levy, S. W.-t. Yih, S. Wang, and J. Tang. Blockwise self-attention for long document\nunderstanding. arXiv preprint arXiv:1911.02972, 2019.\n[21] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for\nlong-range sequence modelling. In International Conference on Learning Representations, 2020.\n[22] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683,\n2019.\n[23] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine comprehension\nof text. In Empirical Methods in Natural Language Processing (EMNLP), 2016.\n[24] A. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of a\nlanguage model? ArXiv, abs/2002.08910, 2020.\n[25] A. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efﬁcient content-based sparse attention with routing\ntransformers. arXiv preprint arXiv:2003.05997, 2020.\n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\nAttention is all you need. In Advances in Neural Information Processing Systems (NIPS), pages 5998–6008,\n2017.\n[27] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Fun-\ntowicz, and J. Brew. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv,\nabs/1910.03771, 2019.\n[28] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized autoregressive\npretraining for language understanding. In Advances in neural information processing systems , pages\n5754–5764, 2019.\n[29] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. R. Salakhutdinov, and C. D. Manning. Hotpotqa: A\ndataset for diverse, explainable multi-hop question answering. In EMNLP, 2018.\n[30] Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang. Bp-transformer: Modelling long-range context via binary\npartitioning. arXiv preprint arXiv:1911.04070, 2019.\n10\nA Supplemental Material\nA.1 Details of the Numerical Reasoning Task\nFor creating the textual synthetic data for the generative QA task of §3.2, we used the data generation\nset-up of [5] (§4.2 of [5]). Default templates and vocabulary were used to create passages containing\n5 sentences. While instantiating the templates, the probability of sampling from one of the previously\nused values was set to 0.999 to promote inter-sentence dependencies. This gave us 629906/15K\ntrain/dev passage-question-answer triples. Among these, we only kept the samples where the answer\nwas a number not appearing in the passage, and discarded the rest. This gave us 223067 training and\n5146 evaluation instances.\nWe only kept the decoder/generative head ofGENBERT (§3 of [5]) and allowed the decoder to attend\nto all the encoder outputs in the cross-attention layers. As the weights of the encoder and decoder are\ntied, we used segment ids 0 for the encoder input sequence and 1 for the decoder inputs.\nA.2 Data for Masked LM task\nThe instances for the MLM task (§4) were formed separately using 5.2M pages from English\nWikipedia (October 2017 dump) and the training set of PG19 dataset containing ∼29K books from\nProject Gutenberg [ 21]. For each dataset, after appending a special symbol at the end of each\ndocument, the documents were arranged in a random order and concatenated into a single long text\nwhich was then tokenized into a list of tokens. Depending upon the input length Lof the experiment\n(512/1024/etc) this list was chunked into full length L−2 sequences which were then masked\nrandomly following [3] and enclosed within [CLS] and [SEP] tokens. For each dataset, the ﬁrst\n2.55B tokens (i.e. 510 ×5M) were used to form the respective training set, next 10.20M tokens\n(510 ×20K) the dev set and the rest were discarded.\nA.3 Finetuning on HOTPOT QA\nGiven the question Qand 10 arranged paragraphs Pi’s, each Pi is extended by preﬁxing it with\nits title. Moreover, to handle yes/no questions, a special string <a> yes no </a> is also preﬁxed.\nThe context Dis formed by simply concatenating the resulting paragraphs. Following [3], given a\nwindow/chunk P from tokenized D, the corresponding instance is formed as [CLS] Q [SEP] P\n[SEP].\nSupporting Facts Tagging Task (SF): Besides the standard span extraction loss, we also include\nanother task using the supporting facts supervision. Contextualized representations of the model are\nlinearly projected to 2 scores (for 0/1) per token and normalized to obtain log-probabilities. For an\ninput, loss is computed as negative log probability of the correct tag averaged over the positions. As\nsupporting facts positions are fewer, log-probabilities are weighted according to the respective class\n(0/1) size.\nAdversarial data generation: After training the single-paragraph model of [14] on HOTPOT QA, for\neach sample in the training and development sets, we retrieved top 50 introductory paragraphs from\nWikipedia according their TF-IDF similarity with the question. The 50 paragraphs were then re-\nranked using the \"no-answer-logit\" score predicted by the trained model and 8 adversarial distractors\nwere chosen accordingly. When evaluated on the adversarial version of the development set the\nperformance of the trained model reduced from 64.4 →57.8 F1. Re-training on the adversarial\ndata increased the performance to 61.3. In both cases, we trained for 10 epochs with batch size 36,\nmaximum sequence length 300 and learning rate 5e-5 with linear warmup proportion 0.1.\nA.4 Hyperparameters\nFor all our experiments, we used an older version of Hugging Face’s Transformers library [27]. For\nconvenience, we denote the training hyperparameters using the following abbreviations, INS: number\nof training instances, BSZ: number of instances in a batch, ISZ: instance size, SQL: ﬁnal input\nsequence length, LR: learning rate, WRM: linear LR warm-up proportion, EP: number of epochs,\nSTP: number of optimizer steps, GAC: gradient accumulation steps, POSq: whether (y/n) qpart is\nincluded in positional embeddings deﬁned in §2.\n11\nThe hyperparameters for majority tagging are in Table 12, for GENBERT ﬁnetuning in Table 13, for\nMLM trainings in Table 10, for SQUAD ﬁnetuning in Table 11 and for HOTPOT QA ﬁnetuning in\nTable 14.\nsection model(s) init BSZ ISZ SQL LR WRM EP STP GAC POSq\n§4.1 (512 × 1, 0) random 60 512 512 5e-5 0.1 9 431K 1 n\n§4.1 (512 × 4, 64) random 60 512 2048 5e-5 0.1 9 431K 1 y\n§4.1 (1024 × 1, 0) random 60 512 1024 5e-5 0.1 9 431K 1 y\n§4.2\n(512 × 4, 0)\n(512 × 4, 64)\n(1024 × 2, 0)\nWikipedia , PG19\nBERT 16 512 2048 2e-5 0.001 5 350K 1 y\n§4.2 (8 × 64, 0) (512 × 4, 0) 24 512 512 5e-5 0.01 5 270K 1 y\n§4.2 (8 × 64, 64) (512 × 4, 64) 24 512 512 5e-5 0.01 5 270K 1 y\n§4.3 (512 × 1, 64), Nc = 3\n(512 × 1, 64), Nc = 9 (512 × 4, 64) 24 512 512 5e-5 0.01 5 440K 1 y\nTable 10: Training hyperparameters for MLM training (§4). Common parameters: INS=5M, dropout-rate=0.1,\noptimizer=Bert-Adam, weight-decay=0.01, max-grad-norm=1.0, seed=42. If STP speciﬁed, training is termi-\nnated after STP-many optimizer steps.\nmodel(s) BSZ ISZ SQL LR WRM EP GAC\nall 12 384 384 3e-5 0.1 3 1\nTable 11: Training hyperparameters for SQUAD v1 ﬁnetuning (§4.4). POSq for a model is same as during its\npre-training. Common parameters: maximum query length=64, window-stride-length=128, dropout-rate=0.1,\noptimizer=Bert-Adam, weight-decay=0.01, max-grad-norm=1.0, seed=42.\ntask num. of layers INS BSZ ISZ SQL LR WRM EP GAC POSq\nL = 8192 , p = 1 2 200K 4 8192 8192 2e-6 0.01 1 1 y\nL = 512 , p = 1, 3 2 200K 80 512 512 2e-6 0.01 2 1 n\nL = 1792 , p = 4 12 300K 8 1792 1792 2e-6 0.01 2 1 y\nTable 12: Training hyperparameters for majority tagging task (§3.1). Common parameters: init=random,\ndropout-rate=0.0, optimizer=Bert-Adam, weight-decay=0.01, max-grad-norm=1.0, seed=42.\nmodel(s) BSZ ISZ SQL LR WRM EP GAC\nall 68 140 140 3e-5 0.1 15 1\nTable 13: Training hyperparameters for GENBERT ﬁnetuning (§3.2). Common parameters: init= BERT,\nINS=223067, POSq=n, dropout-rate=0.1, optimizer=Bert-Adam, weight-decay=0.01, max-grad-norm=1.0,\nseed=42.\nmodel(s) dataset-setting window-stride-length BSZ ISZ SQL LR WRM EP GAC POSq\nBERT GS 128 32 512 512 2.5e-5 0.1 3 1 n\n(512 × 4, 0)\n(512 × 4, 64) GS 128 12 2048 2048 3.5e-5 0.1 6 1 y\n(512 × 4, 0)\n(512 × 4, 64) GS + QR 256 8 2048 2048 3e-5 0.1 4 2 y\nTable 14: Training hyperparameters for ﬁnetuning on HOTPOT QA variants (§4.4). Common parameters:\nmaximum query length=64, dropout-rate=0.1, optimizer=Bert-Adam, weight-decay=0.01, max-grad-norm=1.0,\nseed=42.\n12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4164547622203827
    },
    {
      "name": "Computer science",
      "score": 0.3324540853500366
    },
    {
      "name": "Electrical engineering",
      "score": 0.10694423317909241
    },
    {
      "name": "Engineering",
      "score": 0.08502805233001709
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    }
  ],
  "cited_by": 21
}