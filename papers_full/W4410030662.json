{
  "title": "Leveraging long context in retrieval augmented language models for medical question answering",
  "url": "https://openalex.org/W4410030662",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2114730160",
      "name": "Gongbo Zhang",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2100770557",
      "name": "Zihan Xu",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2091663374",
      "name": "Qiao Jin",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2102288034",
      "name": "Fangyi Chen",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A3164167264",
      "name": "Yilu Fang",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2024876568",
      "name": "Yi Liu",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2787694935",
      "name": "Justin F. Rousseau",
      "affiliations": [
        "The University of Texas Southwestern Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2155053510",
      "name": "Ziyang Xu",
      "affiliations": [
        "New York University",
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2114730160",
      "name": "Gongbo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100770557",
      "name": "Zihan Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2091663374",
      "name": "Qiao Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102288034",
      "name": "Fangyi Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3164167264",
      "name": "Yilu Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2024876568",
      "name": "Yi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2787694935",
      "name": "Justin F. Rousseau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155053510",
      "name": "Ziyang Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4388409299",
    "https://openalex.org/W4361000349",
    "https://openalex.org/W4377820139",
    "https://openalex.org/W4392686512",
    "https://openalex.org/W4404203830",
    "https://openalex.org/W4392757369",
    "https://openalex.org/W4401100256",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4402348202",
    "https://openalex.org/W4396529913",
    "https://openalex.org/W3102645206",
    "https://openalex.org/W4393147129",
    "https://openalex.org/W3172669006",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4394688596",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4402671835",
    "https://openalex.org/W4402670290",
    "https://openalex.org/W4402684028",
    "https://openalex.org/W2743151379",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2157355837",
    "https://openalex.org/W4328114912",
    "https://openalex.org/W4400921863",
    "https://openalex.org/W4391011686",
    "https://openalex.org/W4396724815",
    "https://openalex.org/W4400066572",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W3088056511",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4392688329",
    "https://openalex.org/W3083410900"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01651-w\nLeveraging long context in retrieval\naugmented language models for medical\nquestion answering\nCheck for updates\nGongbo Zhang1,Z i h a nX u2,Q i a oJ i n3,F a n g y iC h e n1,Y i l uF a n g1,Y iL i u4, Justin F. Rousseau5,6,Z i y a n gX u7,\nZhiyong Lu3, Chunhua Weng1 & Yifan Peng2\nWhile holding great promise for improving and facilitating healthcare through applications of medical\nliterature summarization, large language models (LLMs) struggle to produce up-to-date responses on\nevolving topics due to outdated knowledge or hallucination. Retrieval-augmented generation (RAG) is\na pivotal innovation that improves the accuracy and relevance of LLM responses by integrating LLMs\nwith a search engine and external sources of knowledge. However, the quality of RAG responses can\nbe largely impacted by the rank and density of key information in the retrieval results, such as the“lost-\nin-the-middle” problem. In this work, we aim to improve the robustness and reliability of the RAG\nworkﬂow in the medical domain. Speciﬁcally, we propose a map-reduce strategy, BriefContext, to\ncombat the“lost-in-the-middle” issue without modifying the model weights. We demonstrated the\nadvantage of the workﬂow with various LLM backbones and on multiple QA datasets. This method\npromises to improve the safety and reliability of LLMs deployed in healthcare domains by reducing the\nrisk of misinformation, ensuring critical clinical content is retained in generated responses, and\nenabling more trustworthy use of LLMs in critical tasks such as medical question answering, clinical\ndecision support, and patient-facing applications.\nLarge language models (LLMs) areﬁnding their way into an expanding\nrange of healthcare domains, holding tremendous potential for improving\npatient care, enhancing communication and education, and facilitating\nclinical workﬂow effectiveness\n1– 7. LLMs are useful for answering common\nqueries related to diseases or personal risk, interpreting laboratory results,\nand getting advice on medical condition management\n8– 12. Despite the\npotential of LLMs, the deployment of LLMs in healthcare faces signiﬁcant\nsafety threats. LLMs struggle to generate accurate and up-to-date responses\non current topics, due to outdated knowledge, lack of domain-speciﬁc\nexpertise, or hallucination13– 17.\nRetrieval-Augmented Generation (RAG) is a pivotal innovation to\nenhance the quality and relevance of responses in LLMs18– 21. Typically, a\nRAG system consists of a retrieval module and a generative module. When a\nuser query is provided as input, the systemﬁrst uses the retrieval module to\nfetch relevant documents or data snippets by searching through external\ndata sources. Next, the generative module takes the retrieved information as\ninput and produces a response to the user query. With the help of the\nretrieval module, the generative module can provide more accurate and\nfactual answers without the need for continual training orﬁne-tuning. As\nsuch, RAG poses a promising direction for applications requiring high\nfactual accuracy and speciﬁcity\n14,22.\nHowever, prompting LLMs with contextual information has trade-offs.\nOn the one hand, providing contextual information enhances the model’s\nability to perform the downstream tasks by augmenting LLMs with external\ndomain-speciﬁc knowledge that is under-represented in their pretraining\ndata. On the other hand, the input of LLMs is bounded by the limit of their\ncontext windows. Even though recently released models can process an\nincreasing number of tokens, the increased amount of content to reason over\ncan still hinder model performance\n23. The quality of RAG completion also\ndepends on the retrieval results, sucha st h ed e n s i t yo rp o s i t i o n so fq u e r y -\nrelevant information14,22,24– 26. As retrieval systems are still imperfect, it is\ninevitable to retrieve information irrelevant to the user query14.\n1Department of Biomedical Informatics, Columbia University, New York, NY, USA.2Department of Population Health Sciences, Weill Cornell Medicine, New York,\nNY, USA.3Division of Intramural Research, National Library of Medicine, National Institutes of Health, Bethesda, MD, USA.4Division of Endocrinology, Department\nof Medicine, Diabetes and Metabolism, Weill Cornell Medical College, New York, NY, USA.5Department of Neurology, University of Texas Southwestern Medical\nCenter, Dallas, TX, USA.6Peter O’Donnell Jr. Brain Institute, University of Texas Southwestern Medical Center, Dallas, TX, USA.7Department of Dermatology, NYU\nGrossman School of Medicine, New York, NY, USA.e-mail: cw2384@cumc.columbia.edu; yip4002@med.cornell.edu\nnpj Digital Medicine|           (2025) 8:239 1\n1234567890():,;\n1234567890():,;\nA recent study reports an issue of“lost-in-the-middle”, i.e., the position\nof key information in the LLM context impacts the quality of the model\ncompletions24. This issue occurs when a lengthy context of information is\nretrieved, and the highly relevant information is not ranked at the top or\nbottom of the retrieval results. We refer to these positions as spotlight\npositions, and the document containing key information as the key docu-\nment. If not ranked at the top, the key information may be neglected by the\ngenerative module, resulting in incomplete or inaccurate responses to the\nuser queries\n24. How to effectively utilize contextual information in RAG\napplications remains to be an open research question. Current studies\nattribute this issue to positional attention bias, i.e., more attention weights\nare allocated more to information at spotlight positions than others\n27– 29.T o\naddress the issue, existing methods mainly focus on adjusting the model\nweights, either byﬁne-tuning LLMs27 or directly adjusting the attention\nweights28,29. However, adjusting model weights can lead to catastrophic\nforgetting30,31,i . e . ,t h eo v e r a l lp e r f o r m a n c eo fL L M sd e g r a d e su p o na d o p t i n g\nnew information on a speciﬁct a s k .\nIn this research, we aim to address the“lost-in-the-middle” issue\nwithout modifying model weights. Ourstrategy involves increasing the\ndensity of key information within the context, rather than modifying model\nweights. The lower bound for RAG is closed-book settings, where LLMs\nhave access only to the question withno extra information. The upper\nbound is Oracle settings, where only relevant key information is provided in\nthe context. Compared to closed-book settings, LLMs perform signiﬁcantly\nbetter in Oracle settings. These two scenarios represent opposite ends of the\nspectrum concerning key information density. In closed-book settings, the\ndensity is essentially zero because no external information is provided. In\ncontrast, Oracle settings boast nearly 100% density, as only relevant infor-\nmation is supplied. RAG sits in the middle, where relevant information is\noften mixed with irrelevant content.We hypothesize that the density of key\ninformation affects downstream model performance.\nTherefore, we propose a novel framework, BriefContext, to transform\nthe long-context reasoning task into multiple short-context reasoning tasks.\nThe core of the framework leverages the map-reduce concept\n32– 37, originally\ndesigned for processing massive data in parallel. In our workﬂow, we divide\nthe long context into multiple partitions and dispatch them to multiple LLM\nsessions. The additional LLM service requests incur extra costs. However,\nsuppose the key document is returned at the top of the ranking. In that case,\nthe extra cost is unnecessary since the downstream generative module can\nalready take advantage of the key information at spotlight positions. To\navoid unnecessary costs, we introduce a preﬂight mechanism to predict the\noccurrence of“lost-in-the-middle”. Such a task is challenging since the key\ndocument is unknown beforehand. Here, we employ a heuristic based on\nconsistency across different ranking results to predict the occurrence of\nthe issue.\nWe evaluated the proposed framework via both controlled experi-\nments and integration testing. In particular, we evaluate general-purpose\nLLMs on answering medical QA questions that require domain knowledge\nin depth\n1,26,38– 41. This choice of models and dataset exempliﬁes the scenarios\nwhere knowledge encoded from pretraining data is insufﬁcient to answer\nthe questions well. Our controlled experiments changed the position of key\ninformation and compared BriefContext with a regular RAG pipeline. In the\nintegration testing, we use the ranking order from the real-world retrieval\nresults. These experiments demonstrate that BriefContext consistently\noutperforms the RAG baseline by a substantial margin when the key\ninformation is placed in the middle. BriefContext also improves the model\nperformance when the key information is placed in spotlight positions.\nFurthermore, to understand how BriefContext improves the RAG\npipeline, we investigate the followingquestions, each of which corresponds\nto a module in the pipeline:\n(1) Can LLMs resolve conﬂicts correctly when the LLM context contains\nconﬂicting information? Weﬁnd LLMs can correctly resolve 74.7% of\ncases with conﬂicting information in the context window. Conse-\nquently, BriefContext achieved a higher overall accuracy than the\nvanilla RAG.\n(2) Do LLMs utilize short context more effectively than long context?\nHere, we prove the hypothesis that with the same key information in\nthe context, LLMs perform better at reasoning over shorter contexts\nthan longer ones. We controlled the number of documents in the\ncontext information and evaluated LLMs in different settings. Weﬁnd\nthat model performance decreases as the number of documents\nincreases, even though the same key information is present in the\ncontext. This conﬁrms that short context is utilized more effectively\nthan long context in RAG. Furthermore, since LLMs perform better at\nreasoning over shorter contexts than longer ones, the problem of\nreasoning over long context can be divided into multiple subtasks of\nreasoning over short context, and the correct answer can be more easily\nlocated in one of the subtasks.\n(3) How well does the preﬂight check predict the occurrence of“lost-in-\nthe-middle”? We show that the preﬂight check can predict the issue\noccurrence with a recall of 92.61%but a precision of 50.18%. About\n35.7% of true-negative cases can be correctly ﬁltered by the\npreﬂight check.\n(4) What is the relationship between the retrieval results and the positional\nattention bias? We show that positional attention bias is triggered when\nthe key documents contain similar vocabulary to other documents in\nthe context that do not provide supporting information to the\nuser query.\nResults\nBrief context overview\nOur goal is to mitigate the issue of“lost-in-the-middle”,w h i c ha f f e c t st h e\nperformance of RAG in QA tasks. This issue arises when the sequence of\ndocument retrieval inﬂuences the quality of the information extracted and\nused in generating responses.\nOur proposed BriefContext consists of four modules (Fig.1): Retrieval,\nPreﬂight check, Context Map, and Context Reduce. The Model Develop-\nment section provides a detailed description. The workﬂow initiates when a\nuser inputs a query. This query is converted into an encoded representation\nexpressed as numerical vectors and used to search a knowledge base, where\ndocuments have been previously encoded into vectors using the same\nencoder (Retrieval module). Then, the retrieved documents are sorted using\ntwo distinct algorithms: MedCPT and BM25. It is important to note that\nMedCPT is also used in the primary retrieval module. By comparing the two\nrankings, we develop the Preﬂight check module to conjecture the existence\nof the“lost-in-the-middle” issue. If the issue is detected, the ContextMap\nmodule engages. Here, the retrieved documents are partitioned. Using\npartitions created in the ContextMap step, the LLMs are prompted to\nextract relevant information from eachpartition. Furthermore, the extrac-\nted responses are collected and injected into the ContextReduce module.\nHere, the aggregated responses undergo a summarization process to distill\nthe most pertinent information. Finally, the summarized information is\nformatted into a coherent response and provided to the user as theﬁnal\nanswer.\nThis workﬂow is designed to minimize the detrimental effects of\nretrieval order by reshaping how information is processed and integrated\nfrom various sources. By doing so, theBriefContext enhances the accuracy\nand reliability of responses in QA tasks, ensuring that users receive precise\nand relevant information regardlessof how the initial data was retrieved.\nWe tested the workﬂow on multiple-choice questions, which allow\nscalable evaluation. The multiple-choice questions are all publicly available.\nSpeciﬁc a l l y ,w ec h o s et h eM I R A G Eb e n c h m a r kf o rt h i sp u r p o s e .F o ra\ncomprehensive test, we also evaluated the workﬂow on open-ended ques-\ntions generated using publicly available education materials. The details are\ndescribed in the Method section.\nCan we address the issue of“lost-in-the-middle” without chan-\nging model weights?\nTo answer this question, we evaluated BriefContext in both controlled\nstudies with synthetic rankings and integration testing with real-world\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 2\nrankings. In the controlled study, we used all of the PubMed articles and a\ncollection of textbooks39 that are widely used by medical students as the\nk n o w l e d g e b a s e .W h i l eap o r t i o no ft h ek n o w l e d g eb a s eo rc o r p u sw h e r et h e\ndataset was derived (e.g., PubMed abstracts or textbooks) is probably\nincluded in the pre-training of LLMs, we deem the comparison remains fair,\nsince we used the same backbone LLMs for RAG and BriefContext. We\nselected 20% of questions from PubMedQA\n41,a n dM e d C P T2 as the primary\nsearch engine. The evaluation metric is accuracy, which is the ratio of\ncorrectly answered questions. As shown in Fig.2 and Supplementary\nTable 2, BriefContext utilizes the external information in the middle of the\ncontext more effectively than the baseline RAG workﬂow (p values shown in\nFig. 2 captions). Using Mixtral-7x8b\n42 as the LLM backbone, the accuracy\naveraged over different positions was improved from 57.66 to 60.41 when\ntop_k = 16 (p << 0.05). Using GPT-3.5-turbo43 as the LLM backbone, the\naccuracy was improved from54.82 to 58.11 when top_k = 8 (p << 0.01), and\n52.12 to 58.51 when top_k = 16 (p << 0.01).\nWe used the baseline Chain-of-Thought (CoT) and RAG accuracies\nreported in the MIRAGE benchmark. The results of integration testing\ns h o w ni nF i g .3 and Supplementary Table 3 demonstrate that BriefContext\nhas improved the overall accuracy across different LLM backbones. With\nLLama2-70B-chat, the accuracy was improved from 55.81 to 66.47; with\nLLama3-70B-instruct, the accuracywas improved from 76.75 to 79.03; with\nMixtral-7x8b, the accuracy was improved from 70.52 to 72.20; with GPT-\n3.5-turbo-0125, the accuracy was improved from 69.19 to 72.51. We also\ninvited three medical experts to evaluate model responses to 48 open-ended\nmedical questions. Out of the 48 questions, our method generates better\nanswers than the RAG baseline for 29.2% of questions and worse answers\nfor 12.5% of questions. For the remaining 58.3% of questions, our method\nand the RAG baseline produced the same responses.\nCan LLMs resolve the conﬂicts in the retrieved external knowl-\nedge in the ContextReduce step?\nIn the BriefContext workﬂow, we divided the long text into multiple par-\ntitions. One issue is that LLM answers based on different context partitions\nare not always the same. We refer to such a situation as context with conﬂict\ninformation. It’s unclear how LLMs deal with such a context. To investigate\nthis problem, we used 20% of PubMedQA questions with synthesized\nrankings. The experimental setup, including the knowledge base, search\nengine, and backbone LLMs, is the same as the above control studies of\nBriefContext.\nThe results are shown in Fig.4.O v e r a l l ,M i x t r a l - 7 x 8 br e s o l v e d1 7 1o u t\nof 217 cases with conﬂicting contextual information correctly; GPT-3.5-\nturbo resolved 225 out of 313 cases. We also reported the win/tie/lose ratio\n(deﬁned in the Method section) details in Supplementary Table 3. Overall,\nBriefContext consistently demonstrates a higher win rate than lose rate,\nwhich indicates the advantages of BriefContext handling context with\nconﬂict information. The advantages are more manifested in 25\nth,5 0th,7 5th\npercentile of positions than the others. This also highlights that the key\ninformation is under-utilized by the vanilla RAG, especially when the\ncontext contains conﬂicting information.\nDo LLMs favor short context over long context in the\nContextMap step?\nTo answer this, we used the same questions, knowledge base, and search\nengine as in the question above with synthetic rankings. We strategically\nplaced key documents at different positions in the context (i.e., retrieved\nPubMed abstracts) and reported the average accuracy. We evaluated the\nsame 4 LLM backbones using various numbers (top_k) of documents in the\ncontext. Figure5 shows that the LLMs favor short over long context.\n(1) User Query\n(10) Answer with\nSupporting Evidence\nRetrieval Module\nGeneration Module\n(2) Query\nEncodings\n(3) Lists of documents\nranked by relevance\nSearching\nAlgorithm\nReranking\nAlgorithm\nBackbone LLMs\n(4a) Search results\n(4b) Rerank results\nPreflight check:\n(5) Divide the\ncontext or not?\nContext Map Context Reduce\nYes\nNo\nExtracted info\nfrom each split\n(6) Request: \ninfo extraction\n(7) Response:\ninfo extraction\n(8) Request: \nsummarization\n(9) Response: \nsummarization\nFig. 1 | Workﬂow of BriefContext.In the Context Map operation (1), the retrieved documents are divided into multiple partitions to create multiple RAG subtasks. In the\nContext Reduce operation (2), the responses were collected from the previous step and summarized into aﬁnal response.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 3\nCan the occurrence of“lost-in-the-middle” be predicted by the\nPreﬂight check?\nIt typically remains unknown which documents contain the key informa-\ntion. It’s thus unclear whether the“lost-in-the-middle” issue happens or not.\nTo predict the occurrence of the issue, we used the consistency across\ndifferent ranking results as a heuristic( s e ed e t a i l si nt h eM e t h o d ss e c t i o n ) .\nWe evaluate how well the heuristic can predict the issue. We deﬁne con-\nsistency as the IoU rate between rankings from MedCPT and BM25. The\nthreshold is set to 0.2. When the IoUis larger than 0.2, we posit that\nMedCPT has placed the key document at top positions, i.e., the ranking issue\ndoes not occur. To validate this hypothesis, we used precision, recall, and F1,\nw h e r eat r u e - p o s i t i v ei sd eﬁn e da sa ni s s u eo f“lost-in-the-middle” that\nhappened and was successfully captured using the IoU score. The test was\nperformed using queries from PubMedQA and BioASQ datasets, and\nresults from PubMed were retrieved using MedCPT. The IoU heuristic\nachieved 50.18% precision, 92.61% recall, and 65.09% F1 (Supplementary\nTable 5). Based on the confusion matrix, 35.7% of true-negative cases can be\ncorrectlyﬁltered, which avoids unnecessary procedure calls of BriefContext.\nBased on these numbers, we estimate the preﬂight check can help reduce\nextra overhead by up to 35%.\nWhat is the relationship between positional attention bias and\nretrieval results?\nRecent studies27,28 pointed out that lost-in-middle-issue is attributed to the\npositional attention bias, i.e., models exhibit U-shaped attention patterns\nwhere documents at the beginning or end of the inputs receive higher\nattention values, regardless of their relevance. We argue that positional\nattention bias is related to inaccurate retrieval results that are irrelevant to\nthe user query but contain vocabularysimilar to the key documents. Recall\nthat most modern LLM architectures employ self-attention, which calcu-\nlates pair-wise inner product of embeddings as attention weights\n44.E a c h\nposition is typically represented as a concatenation of position and text\nembedding vectors\n45,46. We hypothesize that positional attention bias is\ntriggered only when the text embeddings of key documents are similar to\nother documents in the context. In other words, the positional attention bias\nwill disappear when the key document can be associated with the query\nsuccessfully and distinguished clearly from other retrieved documents in the\ncontext.\nTo prove this hypothesis, we randomly selected 20% of multiple-choice\nquestions (n = 105) from the PubMedQA dataset. We set up two search\nengines to retrieve documents relevant to the questions. In the control\ngroup, we used MedCPT as the search engine and retrieved the top 16\ndocuments from the external knowledge base using the input query. In the\nexperimental group, we synthesize retrieval results by mixing the key\ndocuments with documents randomly selected at random from the\nknowledge base. The randomly selected documents were highly likely\nirrelevant to the input query. To manifest the lost-in-the-middle issue, we\nplace the key document right in the middle of the LLM context for both\ngroups. We provide the two retrieval results to downstream LLMs as con-\ntextual information and report the accuracy. Figure6 shows that the\naccuracy is higher when the key documents are mixed with random\ndocuments (experimental group) as compared to relevant documents\n(control group), even though the key documents are placed right in the\nmiddle of the context. These results prove that positional attention bias is\noverpowered by text-embedding-based attention when the key information\nis distinguishable from other documents in the context. Furthermore, this\nobservation highlights a limitation of search engines based on embedding\nrepresentation or dense retrieval. These search engines sometimes return\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nns\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nns\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nns\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nns\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nRAG BC\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\n0th 25th 50th 75th 100th\n(a) GPT3.5-turbo, top_k=8\n(b) GPT3.5-turbo, top_k=16\n(c) Mixtral-7x8b, top_k=8\n(d) Mixtral-7x8b, top_k=16\nFig. 2 | Relationship between QA accuracy and positions of key information in\nthe LLM context.We show the average and standard deviation of accuracy of:\na, b GPT-3.5-Turbo, c, d Mixtral-7x8b. The quartiles refer to the positions where the\nkey document is located. Signiﬁcance levels:*p < 0.05;**p < 0.01;***p < 0.001;\n****p < 0.0001; ns Not signiﬁcant.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 4\nirrelevant documents that manifest a high resemblance to the query\nvocabulary.\nDiscussion\nOur experiments demonstrate that BriefContext improves the robustness\nregarding the order of retrieved documents in the RAG paradigm without\nadjusting model weights. Our proposed workﬂow improved accuracy on\nseveral biomedical QA datasets. Thisis demonstrated via both controlled\nstudies and integration testing, as shown in Figs.2 and 3.W h e nc o nﬂicting\ninformation is present in the context, Mixtral-7x8b correctly resolved 78.8%\nof the cases with conﬂicting information in the context, while GPT-3.5-\nturbo resolved 71.8% of the cases, as shown in Supplementary Table 5. As\nsuch, the BriefContext can better utilize the key document than RAG,\nmainly when the context contains conﬂicting information. However, LLMs\ndo not always correctly resolve the conﬂict information. Here, we illustrate\no n ee x a m p l ew h e r eB r i e f C o n t e x tf a i l s, but vanilla RAG succeeds. Consider\nthe question with ID 18507507 in PubMedQA,“The promise of specialty\npharmaceuticals: are they worth the price?”. The publication record (PMID\n18507507), labeled as the key information, supports a positive answer. Other\nretrieved records present irrelevant information, which results in an answer\nwith a lower level of certainty (e.g., PMID 28911475, PMID 24991326). Such\nretrieved records can lead to a positive answer in one partition and an\nuncertain answer in another. In the phase of ContextReduce, the backbone\nLLM favored the uncertain answer, leading to errors. Despite this, in most\ncases, the conﬂicting information can be resolved correctly.\nIn addition to the LLM capabilities of correctly resolving most con-\nﬂicting information, we also show that key information can be better utilized\nin a short than a long context. To prove this, we construct various sets of\ncontext information with varying numbers of documents but the same key\ninformation. As shown in Fig.6, the QA accuracy decreases as the number of\ndocuments is increased. By dividing a long list of documents into multiple\nbatches, we decompose a challenging RAG task into multiple subtasks with\nshorter context. Resolving the“lost-in-the-middle” issue is also attributed to\nthis division operation, which is deﬁned as the ContextMap operation in our\npipeline. In cases where the key documents are ranked at the spotlight\npositions, the vanilla RAG workﬂow can already utilize the key information.\nHowever, it’s challenging to predict where the key document is ranked\nwithout knowing which document contains the key information. To combat\nthis issue, we propose a preﬂight check mechanism to predict the“lost-in-\nthe-middle” occurrence. Supplementary Table 5 shows the preﬂight check\nachieves 50.18% precision, 92.61% recall, and 65.09% F1. About 35.7% of\ntrue-negative cases can be correctlyﬁltered by the preﬂight check.\nEarlier studies pointed out that the issue of“lost-in-the-middle” is\nattributed to positional attention bias\n27,28. In this study, we show that posi-\ntional attention bias only manifests when the key documents are not dis-\ntinguishable from other documents inthe context based on topic similarity\nto the query. The positional attention bias can be overpowered by the\nsegment embeddings when the key documents are distinguishable. As\ns h o w ni nF i g .6 (experimental group), the key documents can be effectively\nutilized even if placed right in the middle of the context. This highlights the\nlimitations of embedding-based search engines, which mainly rely on\nsuperﬁcial lexical similarity to performthe retrieval task without deeply\nunderstanding the relationship between user queries and the returned\ndocuments\n47.\nWe identify the following sources of medical QA errors in the RAG\nparadigm. First, LLMs sometimes resolve conﬂicting information incor-\nrectly. Although about 78.8% and 71.8% of conﬂicting information were\nresolved by Mixtral-7x8b and GPT-3.5, respectively, they failed to provide\ncorrect answers for the rest of the cases, resulting in wrongﬁnal answers.\nSecond, although the RAG paradigm improves LLMs via external knowl-\nedge sources, we show that LLMs may still fail to answer questions correctly\neven in the oracle settings, where only key documents were provided as the\nFig. 3 | Integration testing of BriefContext with\ndifferent LLM backbones.We show the accuracy of\nvarious settings with different foundation models:\na Llama3-70B-instruct, b Llama2-70B-chat,\nc Mixtral-7x8b, andd GPT-3.5-turbo-0125. BC\nBrief Context. RAG Retrieval-augmented genera-\ntion. CoT Chain-of-Thought. Signiﬁcance levels:\n*p < 0.05;**p < 0.01;***p < 0.001;****p < 0.0001;\nns Not signiﬁcant.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 5\nc o n t e x t .W h i l et h i si s s u ei sb e y o n dt h es c o p eo ft h e“lost-in-the-middle”,t h i s\nhighlights the gap between the RAG paradigm and the strict requirement for\naccuracy in the medical domain.\nOur experiment has a few limitations. Firstly, due to the lack of open-\nended questions annotated with key documents, we cannot quantitatively\nevaluate the impact of key document positioning on QA responses. How-\never, we addressed this by conducting a controlled experiment using\nmultiple-choice questions where the key document was strategically placed\nat various positions within the prompt context. Secondly, our choice of the\noff-the-shelf LLMs without any modiﬁcations presents another limitation.\nA future direction of this work could explore the context map-reduce\nparadigm withﬁne-tuned or task-speciﬁc LLMs. Lastly, our current focus is\non QA tasks in the medical domain. In future studies, we plan to explore the\napplication of LLMs to other tasks and QA tasks in other scientiﬁc domains.\nAnother future direction is to incorporate the BriefContext mechanism into\nconversational agents to allow furthervalidation using real-word clinical\nqueries.\nIn summary, we propose BriefContext, a map-reduce approach, to\neffectively utilize long context in RAG workﬂow for answering questions in\nthe medical domain. First, we showed that LLMs can better utilize short\ncontext than long context. Next, by dividing the long context into several\nsubtasks, we improve the model performance on biomedical QA tasks\nwithout adjusting model weights. To avoid unnecessary extra costs on LLMs\nservice, we then introduced a preﬂight check mechanism to prognose the\nranking issue without knowing which document contains key information.\nWe show LLMs can correctly resolve 74.7% of cases with conﬂicting\ninformation in the context window. BriefContext takes advantage of this\ncapability of LLMs and shorter context, which explains how BriefContext\nimproves biomedical QA accuracy in RAG workﬂow. Lastly, we discussed\nwhen positional attention bias is triggered. We hope this assists future\nresearch on the root cause of the positional attention bias.\nWhile our proposed BriefContext framework was evaluated only\nwithin the biomedical question-answering in this study, it shows promise\nfor generalizing to tasks that require effective processing of long-context\ndata, such as extracting pertinent datafrom lengthy, duplicative electronic\nhealth records, legal document analysis, historical research, or technical\nreport summarization. Future studies could explore these applications to\nevaluate the generalizability and adaptability of BriefContext in addressing\ndiverse and complex information retrieval challenges.\nMethods\nWe describe the methods in detail in four main sections, aligning with the\nstudy aims and the Results section.\nTo develop the model and ensure its scalable evaluation, we used\nmultiple-choice questions, where the correctness of model outputs can be\ndetermined without necessitating further expert feedback. We chose the\nMIRAGE\n26 benchmark for this purpose, which consists of three medical\nexamination QA subsets (MMLU-Med48,M e d Q A - U S39,a n dM e d M C Q A49)\nand two biomedical research QA subsets (PubMedQA41 and BioASQ-Y/N40)\n(Supplementary Table 6).\nGiven that our goal is to improve RAG pipelines, we speciﬁcally used\ntwo biomedical subsets (PubMedQA and BioASQ-Y/N), due to their reli-\nance on external knowledge databases that can augment the capabilities of\nLLMs. Furthermore, to maintain a diversity of question types, we used\nMedMCQA, the largest medical examination QA dataset. We particularly\nfocus on PubMedQA and BioASQ-Y/N, which contain explicit\n05 0 1 0 0\n0th\n25th\n50th\n75th\n100th\n%\nAnswer Position\n17 / 23\n15 / 20\n16 / 21\n16 / 21\n18 / 22\n13.04 78.26\n30 60\n38.10\n61.9028.57\n47.62\n22.73 72.73\nWin Tie Lose\n05 0 1 0 0\n0th\n25th\n50th\n75th\n100th\n%\n17 / 20\n17 / 21\n19 / 25\n16 / 21\n20 / 238.7\n47.62\n86.96\n42.86\n40 40\n42.86 47.62\n45 45\n0 50 100\n0th\n25th\n50th\n75th\n100th\n%\nAnswer Position\n25 / 34\n23 / 33\n18 / 28\n23 / 33\n22 / 355.71 74.29\n36.36 48.48\n28.57 67.86\n33.33 54.55\n23.53 58.82\n05 0 1 0 0\n0th\n25th\n50th\n75th\n100th\n%\n26 / 33\n23 / 28\n15 / 25\n24 / 32\n26 / 3215.62 84.38\n28.12 62.5\n28 60\n42.86 42.86\n30.3 54.55\nModel: Mixtral-7x8b\ntop_K = 8\nModel: GPT-3.5-turbo-0125\nWin Tie Lose\na\nb\ntop_K = 8\ntop_K = 16\ntop_K = 16\nFig. 4 | Analysis of cases with conﬂicting context information.Number of cases (red) with conﬂict information provided to LLMs and number of correctly resolved cases\n(green): a Mixtral-7x8b, b GPT-3.5-turbo-0125.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 6\nspeciﬁcations of the key documents. This information allows us to perform a\ndeep analysis the relationship between the key document position and the\npipeline accuracy.\nIn the real-world practice of medical QA, questions always arise\nwithout predeﬁned options, reﬂecting the open-ended nature of real-world\nscenarios. As such, we present MedQ, a dataset comprising 48 open-ended\nquestions. We created these questions using StatPearls\n50,as o u r c et h a t\nsummarizes up-to-date medical knowledge and practice across various\nspecialties. In particular, we selectedarticles focusing on neurology, endo-\ncrinology, and dermatology.\nTo formulate the questions, we prompt GPT-4 to generate pairs of\nPICO (participant, intervention/comparison, and outcomes) questions and\nanswers. The generated QA pairs were then reviewed by three specialties\n(dermatology, neurology, and endocrinology) to ensure their accuracy and\nrelevance.\nFollowing the practice of this benchmark work\n26, we built a knowl-\nedge base with components: (1) The entire collection of abstracts indexed\nin PubMed, and (2) a set of 18 medical textbooks\n39 (available athttps://\ngithub.com/jind11/MedQA) that are widely used by medical students and\nserve as preparation materials for the USMLE exams (Supplementary\nTable 7).\nGiven a large collection of documentsD, the main goal of the retrieval\nmodule is to select a subset of documentsD\nr ¼f d1; d2; ::: dkg/C26 Drelevant\nto the user queryQ,w h e r ek is the number of retrieved documents. To\nperform an effective and efﬁcient retrieval, weﬁrst encode each documentdi\nand the queryQ into numerical vectors of the sameﬁxed dimension,\ndenoted asembedðdiÞ and embedðQÞ, respectively.\nT h ec o l l e c t i o ni st h e ns o r t e db yt h er e l e v a n c et ot h eq u e r y .W ed e n o t e\nthe resultant ranking asRLLM ¼½ r1\nLLM; r2\nLLM; :::; rk\nLLM/C138 . where the rele-\nvance of a documentdi to a query is determined by the inner product of the\ntwo embedding vectors ri\nLLM ¼ embedðdiÞ>embedðQÞ: Based on the\nranking results, we discuss two possible outcomes. First, when the key\ninformation is ranked at the top positions, the generative module can take\nadvantage of the retrieved information. In this case, there is no need to\ninclude too many articles in the context. Several results ranked at the top\nprovide enough information to answerthe question. Second, when the key\ninformation is ranked beyond the spotlight positions, the key information is\nprobably to be neglected.\nTo combat ranking related issues, a common approach is to employ\nhybrid rankings, which ensemble several ranking results into a new order\nusing reciprocal ranking fusion (RRF). While RRF demonstrated advan-\ntages in end-to-end RAG evaluation, there is no guarantee that documents\nwith key information will always be placed at top positions in the hybrid\nranking results. The hybrid ranking results still leaves the“lost-in-the-\nmiddle” issue unresolved. It’s also unrealistic to expect any retrieval system\nto always place the documents of interest at the veryﬁrst position.\nFig. 5 | Medical QA accuracy of LLMs with various numbers of documents as\ncontext information.We show the mean and standard deviation of accuracy with\ndifferent number of documents in the context window. The top solid line shows the\nperformance in the Oracle settings. The bottom dotted line shows the performance\nof CoT. With the same key document in the context, the accuracy decreases as the\nnumber of documents increases.a Llama3-70B-instruct, b Llama2-70B-chat,\nc Mixtral-7x8b, andd GPT-3.5-turbo-0125. BC Brief Context. RAG Retrieval-\naugmented generation. CoT Chain-of-Thought. Signiﬁcance levels:*p < 0.05;\n**p < 0.01;***p < 0.001;****p < 0.0001; ns Not signiﬁcant.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 7\nInspired by the hybrid ranking algorithms, we use the consistency\namong different retrieval systems to conject the occurrence of ranking issues\nwithout knowing which documents contain key information. In particular,\nwe calculate the intersection-over-union (IoU) rate between the topn results.\nIn addition to the retrieval system based on dense representation of docu-\nments, we use another ranking algorithm, BM25, to rerank the documents in\nR\nLLM. The new ranking is denoted asRBM25 ¼½ r1\nBM; r2\nBM; :::; rk\nBM/C138 .N e x t ,\nwe conduct a preﬂight check to determine whether to invoke the Brief-\nContext subroutine in the RAG pipeline. The preﬂight check is formally\ndeﬁned as an indicator function,\n1 RLLM; RBM25; n\n/C0/C1\n¼ 1i f RLLM :n½/C138 \\ RBM :n½/C138\nRLLM :n½/C138 \\ RBM :n½/C138 >δ\n0o t h e r w i s e\n(\nð1Þ\nThe choice of the threshold is crucial in balancing the trade-off between\nprecision and recall. While it’s possible to further optimize the precision and\noverall F1 by adjusting the threshold, we chose a value that ensures high\nrecall. This decision is based on the fact that false positive errors result in\nextra cost, while false negative errors could leave the errors in vanilla RAG\nunaddressed. To better demonstrate the effectiveness of our methods, we\nprioritize achieving high recall over high precision.\nThe ContextMap operation dividesDr into a partitionPðDrÞ (i.e., the\nsets inP are subsets ofDr, and the elements ofP are mutually exclusive) and\nconverts each subset as a prompt. Here, each subset has the same number of\ndocuments, denoted asD\nr\ns 2 P. The output is a list of prompts with the\nsame instruction and user query, as outlined in Supplementary Note 1.\nConsider a partition ofD\nr ¼f d1; d2; ::: d8g as Dr\n1 ¼f d1; d2; d3; d4g and\nDr\n2 ¼f d5; d6; ; d7; d8g, the resultant prompts are“{instruction} {query}\n[doc 1]d1 [doc 2] d2…” and “{instruction} {query} [doc 1]d5 [doc 2] d6…”.\nThe only difference between the prompts is the contextualized documents.\nIt has been pointed out that decoder-only models cannot attend to query\ntokens if the query is only placed behind the contextual information, since\ndecoder-only models only attend toprior tokens by each timestamp. To\ncombat this effect, we adopt query-aware contextualization, where a prompt\nconsists of instruction, context information, and the user query placed\nbefore the context. Since not all documents inD\nr are necessarily related to\nthe queryQ, we instruct the model to either extract the relevant information\nControl Experimental\n0.5\n0.6\n0.7\n0.8Accuracy\nControl Experimental\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nControl Experimental\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\nControl Experimental\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy\n(b) Llama2-70B-chat(a) Llama3-70B-instruct\n(c) Mixtral-7x8b (d) GPT-3.5-turbo-0125\n0.3\n0.4\nFig. 6 | Relationship between QA accuracy and different context information.We\nshow the average mean and standard deviation of accuracy with the real retrieval and\ncontrolled settings as the context. In the Control group, all documents come from\nresults returned by MedCPT. In the experimental group, the context consists of key\ndocuments and others selected at random from the knowledge base.a Llama3-70B-\ninstruct, b Llama2-70B-chat, c Mixtral-7x8b, andd GPT-3.5-turbo-0125ontext.\nRAG retrieval-augmented generation. CoT chain-of-thought. Signiﬁcance levels:\n*p < 0.05;**p < 0.01;***p < 0.001;****p < 0.0001; ns Not signiﬁcant.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 8\nor truthfully report no detection of anyrelevant information. The operation\nof ContextMap can be processed in parallel via multi-threading, where each\nthread formats a prompt. This batchprocessing is straightforward to\nimplement since the prompt formatting subroutine only requires read\naccess to the context.\nAfter the context mapping, we next query the backbone LLM to extract\nrelevant information from the context and answer the user query, as out-\nlined in Supplementary Note 2. The relevant information is autoregressively\nsampled from the probability distribution over the model vocabulary con-\nditioned on the instruction, query, and provided context:\ny\nt\ninfo /C24 pθðQ; Drs\nS; Ie; y0:t/C0 1\ninfoÞ; ð2Þ\nw h e r ew ed e n o t et h em o d e lw e i g h t sa sθ, extraction instructionIe,q u e r yQ,\nshard of contextDrs\nS,a n dyt\ninfo the sampled information. The invocations to\nextraction can also be streamlined in parallel. The extracted information is\nthen used to generate a summarization prompt, where we provide\ninstructionsI\ns to ignore empty information. Theﬁnal answer is also directly\nsampled from the probability distribution over the model vocabulary con-\nditioned on the summarization instruction, extracted information, and\nquery:\ny\nt\nanswer /C24 pθðQ; yinfo; Is; y0:t/C0 1\nanswerÞ: ð3Þ\nAs in a typical map-reduce workﬂow, the long context of relevant\ndocuments isﬁrst divided and dispatched to worker LLMs to create requests\nfor extracting relevant information. After all the worker LLMsﬁnish their\nprocessing jobs, they return to the LLMallocator to aggregate the individual\nresults.\nBelow, we discuss the extra cost incurred by invoking the BriefContext\nsubroutine. Here, we use the pricing model of most proprietary LLMs, e.g.,\nGPTs, where users are charged by the number of input and output tokens.\nWe denote the number of tokens in the prompt instruction and context as\nN\nins and Ncon, and the maximum number of output tokens asNout,\nrespectively. The prices of input and output per token are denoted aspinput\nandpoutput. The context is divided into M partitions. The cost of vanilla RAG\nis\nOðNcon /C1 pinput þ Nins /C1 pinput þ Nout /C1 poutÞ ð4Þ\nwhile the cost of BriefContext invocation is\nOðNcon /C1 pinput þ M /C1 Nins /C1 pinput þð M þ 1Þ/C1 Nout /C1 poutÞ ð5Þ\nSince the lengths of instruction and output are much shorter than the\ncontext information, the extra cost incurred by BriefContext invocations is\nnot signiﬁcant in scale.\nIn our cost analysis, we adopted the big-O notation, proving that\nBriefContext and vanilla RAG are at the same level in terms of theoretical\ncomplexity. However, in real-world scenarios, constant factors do play a\nrole. While these extra tokens do not impact the big-O analysis, they would\nstill increase the actual costs. It’s challenging to accurately quantify the\npercentage increase in cost since this varies by the speciﬁc prompt, retrieved\ndocuments, and batch size in BriefContext. In case the context is as short as a\nprompt instruction, the lost-in-the-middle issue would not appear, thus not\nrequiring the BriefContext procedure.\nA n o t h e rf a c t o rt oc o n s i d e ri st h eo c c u r r e n c eo f“lost-in-the-middle”\nissues, which can vary by the queries, corpus, and choice of retrieval models.\nTo help understand the frequency of these issues, we reported the average\nnumber of tokens per request, with 8 publication records retrieved for each\nquery. In BriefContext, the average numbers of input and output tokens per\nrequest are 5496.5 and 247.5, respectively. In vanilla RAG, these numbers\nare 3066.0 and 183.0.\nCan we address the issue of“lost-in-the-middle” without chan-\nging model weights?\nTo answer this question, we evaluated BriefContext in both controlled\nstudies with synthetic rankings and integration testing with real-world\nrankings. In the controlled study, we used the same experimental setup as\nthe above question, i.e., the knowledge base of PubMed articles and text-\nbooks, the 20% subset of questions from PubMedQA, and MedCPT as the\nprimary search engine. The evaluation metric is accuracy. We synthesized\nrankings by placing key information at different positions, including 0th,\n25th, 50th, 75th, and 100th percentile ofpositions in the context. We used\nMixtral-7x8B and GPT-3.5-turbo as LLM backbones since these two models\nbeneﬁt more from retrieval augmentation than others (Fig.2). We com-\npared the BriefContext with the vanilla RAG workﬂow using the same\nbackbone LLM and external knowledge as the context in the prompts.\nIn the ﬁrst integration testing, we used all questions from\nMedMCQA\n49, PubMedQA41,a n dB i o A S Q - Y / N40 from the MIRAGE26\nbenchmark dataset. The evaluation metric is accuracy. We selected LLama2-\n70B-chat51, LLama3-70B-instruct52, Mixtral-7x8b42, and GPT-3.5-turbo43 as\nbackbone LLMs, all of which have beenused in the published benchmark\nresults26. We used the baseline closed-book (CoT) and RAG accuracies that\nwere reported in the MIRAGE benchmark results26. We used the same\nknowledge base as in the controlled studies. The knowledge base is a subset\nof the corpus that was used in the MIRAGE benchmark results reported by\nXiong et al.\n26. Our knowledge base thus contains no extra information as\ncompared to theirs, which makes a fair comparison between BriefContext\nand RAG. In BriefContext, we usedMedCPT as the search engine. The\norder of retrieved documents by MedCPT was kept the same when the\nprompt context was constructed. The top_k is set to 16. In the second\nintegration testing, we invited three medical experts to help curate 48 open-\nended question-answer pairs from their specialty domain and compare our\nmethod with the RAG baseline.\nCan LLMs resolve the conﬂicts in the retrieved external knowl-\nedge in the ContextReduce step?\nTo investigate this problem, we used 20% of PubMedQA questions with\nsynthesized rankings. The experimental setup, including the knowledge\nbase, search engine, and the backbone LLM, is the same as the above\nexperiments. We deﬁne the occurrence of conﬂict information as an event in\nwhich LLMs return inconsistent answers given different context partitions\nof the same query results. We further deﬁne that the conﬂict is correctly\nresolved if theﬁnal answer is correct. We report the number of cases with\nconﬂict information and how many cases were correctly resolved by our\nproposed workﬂow. We also compare BriefContext with the vanilla RAG,\nwhich has the same backbone LLM in these cases. The comparison results\nconsist of three possible outcomes: (1) our method wins the comparison if it\nresolves the conﬂict information correctly while the RAG baseline answers\nthe question incorrectly; (2) the lose outcome is deﬁned similarly; or (3) the\noutcome is a tie when both BriefContext and RAG answer the question\neither correctly or incorrectly.\nDo LLMs favor short context over long context in the\nContextMap step?\nTo answer this, we used the same questions, knowledge base, and search\nengine as in the question above with synthetic rankings. We strategically\nplaced key documents at the 0th, 25th, 50th, 75th, and 100th percentile of the\npositions in the context (i.e., retrieved PubMed abstracts) and calculated the\naccuracy averaged over theﬁve positions.\nCan the occurrence of“lost-in-the-middle” be predicted by the\nPreﬂight check?\nTo predict the occurrence of the issue, we used the consistency across\ndifferent ranking results as a heuristic. We evaluate how well the heuristic\ncan predict the issue. In this experiment, we selected all questions from\nPubMedQA and BioASQ, where the answers were also annotated with the\nPMID of articles that contained the key information. The issue occurrence is\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 9\ndeﬁned as an event where the key document is ranked beyond the top N\npositions. The threshold N is set to 3 since model performance drops sig-\nniﬁcantly when N becomes larger than 3, according to earlier studies24,27,28.W e\nu s e dt h es a m ek n o w l e d g eb a s ea si nt h ea b o v eq u e s t i o n s .W eu s e dM e d C P Ta s\nthe primary search engine and BM25 as the secondary search engine to rerank\nthe retrieval results from MedCPT. We deﬁne consistency as the IoU rate\nbetween rankings from MedCPT and BM25. The threshold is set to 0.2.\nWhat is the relationship between positional attention bias and\nretrieval results?\nIn our study, we decouple the impact of segment embeddings on attention\nweights from the impact of positional embeddings. Recall that Transformer\narchitecture adopts the self-attention mechanism, where the weight is cal-\nculated as an inner-product between each pair of embeddings\n44.E a c h\nembedding consists of positional, token, and segment embeddings, which\nencode position and semantics, respectively20,45. We randomly selected 20%\nof multiple-choice questions (n = 105) from the PubMedQA dataset. Each\nquestion is a multiple-choice question, and the evaluation metric is accu-\nracy. The knowledge base consists of two components. One is all of the\nabstracts indexed at PubMed (https://pubmed.ncbi.nlm.nih.gov/), and the\nother is a collection of 18 textbooks\n39 that medical students widely use for\npreparing USMLE. Scientiﬁc literature has certain limitations, such as\npublication bias and lack of generalizability across different population\ngroups. Another potential supplementary information source is real-world\nclinical data, such as electronic health records. However, due to the sensitive\nnature, retrieval augmented LLMs based on clinical data is out of the scope\nof this study. We used MedCPT2 as the search engine to obtain relevant\ninformation from the knowledge base. MedCPT was speciﬁcally pretrained\non biomedical literature using user click information2.\nData availability\nThe results are provided within the supplementary informationﬁles.\nCode availability\nThe code is available athttps://github.com/ebmlab/BriefContextunder the\nMIT license.\nReceived: 13 September 2024; Accepted: 19 April 2025;\nReferences\n1. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n2. Jin, Q. et al. MedCPT: contrastive pre-trained transformers with large-\nscale PubMed search logs for zero-shot biomedical information\nretrieval. Bioinformatics 39, btad651 (2023).\n3. Haupt, C. E. & Marks, M. AI-Generated Medical Advice— GPT and\nBeyond. JAMA 329, 1349–1350 (2023).\n4. Peng, Y., Rousseau, J. F., Shortliffe, E. H. & Weng, C. AI-generated\ntext may have a role in evidence-based medicine.Nat. Med.29,\n1593–1594 (2023).\n5. Zhang, G. et al. A span-based model for extracting overlapping PICO\nentities from randomized controlled trial publications.J. Am. Med.\nInform. Assoc.31, 1163–1171 (2024).\n6. Idnay, B. et al. Mini-mental status examination phenotyping for\nAlzheimer’s disease patients using both structured and narrative\nelectronic health record features.J. Am. Med. Inform. Assoc. https://\ndoi.org/10.1093/jamia/ocae274 (2024).\n7. Jin, Q. et al. Demystifying large language models for medicine: a\nprimer. arXiv [cs.AI](2024).\n8. Spotnitz, M. et al. A survey of clinicians’views of the utility of large\nlanguage models.Appl. Clin. Inform.15, 306–312 (2024).\n9. Zelin, C., Weng, C., Jeanne, M., Zhang, G. & Weng, C. Rare disease\ndiagnosis using knowledge guided retrieval augmentation for\nChatGPT. J. Biomed. Inform.157, 104702 (2024).\n10. Tian, S. et al. Opportunities and challenges for ChatGPT and large\nlanguage models in biomedicine and health.Brief. Bioinform. 25,\nbbad493 (2023).\n11. Zhang, G. et al. Closing the gap between open source and commercial\nlarge language models for medical evidence summarization.NPJ\nDigit. Med.7, 239 (2024).\n12. Park, J. et al. Criteria2Query 3.0: leveraging generative large language\nmodels for clinical trial eligibility query generation.J. Biomed. Inform.\n154, 104649 (2024).\n13. Cao, M., Dong, Y., Wu, J. & Cheung, J. C. K. Factual error correction\nfor abstractive summarization models. InProc. 2020 Conf. Empirical\nMethods in Natural Language Processing (EMNLP)6251–6258\n(Association for Computational Linguistics, 2020).https://doi.org/10.\n18653/v1/2020.emnlp-main.506.\n14. Chen, J., Lin, H., Han, X. & Sun, L. Benchmarking large language\nmodels in retrieval-augmented generation.AAAI 38, 17754–17762\n(2024).\n15. Raunak, V., Menezes, A. & Junczys-Dowmunt, M. The curious case of\nhallucinations in neural machine translation. InProc. 2021 Conf. of the\nNorth American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (NAACL-HLT)\n1172–1183 (Association for Computational Linguistics, 2021).https://\ndoi.org/10.18653/v1/2021.naacl-main.92.\n16. Ji, Z. et al. Survey of hallucination in natural language generation.ACM\nComput. Surv.55,1 –38 (2023).\n17. Zhang, G. et al. Leveraging generative AI for clinical evidence\nsynthesis needs to ensure trustworthiness.J. Biomed. Inform.153,\n104640 (2024).\n18. Guu, K., Lee, K., Tung, Z., Pasupat, P. & Chang, M. Retrieval\naugmented language model pre-training. InProc. 37th International\nConference on Machine Learning(eds. Iii, H. D. & Singh, A.)\n3929–3938 (PMLR, 2020).\n19. Lewis, P., Perez, E., Piktus, A. & Petroni, F. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks.Adv. Neural Inform.\nProcess. Syst.33, 9459–9474 (2020).\n20. Borgeaud, S. et al. Improving language models by retrieving from\ntrillions of tokens. InProc. 39th International Conference on Machine\nLearning (eds. Chaudhuri, K. et al.)2206–2240 (PMLR, 2022).\n21. Izacard, G. & Grave, E. Leveraging passage retrieval with generative\nmodels for open domain question answering. InProc. 16th Conf. of\nthe European Chapter of the Association for Computational\nLinguistics (EACL)874–880 (Association for Computational\nLinguistics, 2021).https://doi.org/10.18653/v1/2021.eacl-main.74.\n22. Ding, Y. et al. A Survey on RAG Meets LLMs: towards retrieval-\naugmented large language models.arXiv [cs.CL](2024).\n23. Li T, Zhang G, Do QD, Yue X, Chen W. Long-context LLMs struggle\nwith long in-context learning.arXiv [csCL]. 2024.\n24. Liu, N. F. et al. Lost in the middle: how language models use long\ncontexts. Trans. Assoc. Comput. Linguist.12, 157–173 (2023).\n25. Jiang, H. et al. LongLLMLingua: Accelerating and enhancing LLMs in\nlong context scenarios via prompt compression. InProc. 62nd Annu.\nMeet. Assoc. Comput. Linguistics (ACL)1658–1677 (Association for\nComputational Linguistics, 2024).https://doi.org/10.18653/v1/2024.\nacl-long.91.\n26. Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking retrieval-\naugmented generation for medicine. InFindings of the Association for\nComputational Linguistics ACL 20246233–6251 (2024).https://doi.\norg/10.18653/V1/2024.FINDINGS-ACL.372.\n27. He, J. et al. Never lost in the middle: improving large language models\nvia attention strengthening question answering.arXiv [cs.CL](2023).\n28. Hsieh, C.-Y. et al. Found in the middle: Calibrating positional attention\nbias improves long context utilization. InFindings of the Association\nfor Computational Linguistics (ACL)14982–\n14995 (Association for\nComputational Linguistics, 2024).https://doi.org/10.18653/v1/2024.\nﬁndings-acl.890.\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 10\n29. Wu, T., Zhao, Y. & Zheng, Z. An efﬁcient recipe for long context\nextension via middle-focused positional encoding. InAdv. Neural Inf.\nProcess. Syst. 38(NeurIPS, 2024).\n30. Kemker, R., McClure, M., Abitino, A., Hayes, T. & Kanan, C.\nMeasuring catastrophic forgetting in neural networks. InProceedings\nof the AAAI conference on artiﬁcial intelligence, Vol. 32, No. 1,\n3390–3398 (2018).\n31. Kirkpatrick, J. et al. Overcoming catastrophic forgetting in neural\nnetworks. Proc. Natl. Acad. Sci. USA.114, 3521–3526 (2017).\n32. Dean, J. & Ghemawat, S. MapReduce.Commun. ACM53,7 2–77\n(2010).\n33. Zhang, H., Li, P., Meng, F., Fan, W. & Xue, Z. MapReduce-based\ndistributed tensor clustering algorithm.Neural Comput. Appl.35,\n24633–24649 (2023).\n34. Bergui, M., Hourri, S., Najah, S. & Nikolov, N. S. Predictive modelling of\nMapReduce job performance in cloud environments using machine\nlearning techniques.J. Big Data11, 98 (2024).\n35. Senthamil Selvi, R., Sankari, V., Ramya, N. & Selvi, M. Ensemble\nmodel for stock price forecasting: MapReduce framework for big data\nhandling: an optimal trained hybrid model for classiﬁcation. J. Circuits\nSyst. Comput. 33, 2450202 (2024).\n36. Mv, K. et al. Survey on MapReduce scheduler algorithms in Hadoop\nframework. Int. J. Innov. Res. Inf. Secur.10, 314–319 (2024).\n37. Luo, Y. & Li, J. Face Image Encryption Using Fuzzy K2DPCA and\nChaotic MapReduce.Tehnički vjesnik31, 1143–1153 (2024).\n38. Liévin, V., Hother, C. E., Motzfeldt, A. G. & Winther, O. Can large\nlanguage models reason about medical questions?Patterns 5,\n100943 (2024).\n39. Jin, D. et al. What disease does this patient have? A large-scale open\ndomain question answering dataset from medical exams.NATO Adv.\nSci. Inst. Ser. E Appl. Sci.11, 6421 (2021).\n40. Tsatsaronis, G. et al. An overview of the BIOASQ large-scale\nbiomedical semantic indexing and question answering competition.\nBMC Bioinformatics16, 138 (2015).\n41. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. & Lu, X. PubMedQA: a dataset\nfor biomedical research question answering. InProc. 2019\nConference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP)2567–2577 (Association for\nComputational Linguistics, 2019).\n42. Jiang, A. Q. et al. Mixtral of Experts.arXiv [cs.LG](2024).\n43. Brown, T. et al. Language models are few-shot learners.Adv. Neural\nInf. Process. Syst.33, 1877–1901 (2020).\n44. Vaswani, A. et al. Attention is all you need.Adv. Neural Inf. Process.\nSyst. 30, 5998–6008 (2017).\n45. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In\nProc. 2019 Conf. of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (NAACL-\nHLT) 4171–4186 (Association for Computational Linguistics, 2019).\nhttps://doi.org/10.18653/v1/n19-1423.\n46. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving\nlanguage understanding by generative pre-training. OpenAI.Preprint,\npp.1–12, (2018).\n47. Steck, H., Ekanadham, C. & Kallus, N. Is cosine-similarity of\nembeddings really about similarity? inCompanion Proceedings of the\nACM on Web Conference 2024. https://doi.org/10.1145/3589335.\n3651526 (ACM, 2024).\n48. Hendrycks, D. et al. Measuring massive multitask language\nunderstanding. InProc. 9th Int. Conf. Learn. Represent. (ICLR)\n(OpenReview.net, 2021).\n49. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: a large-scale\nmulti-subject multi-choice dataset for medical domain question\nanswering. InProc. Conference on Health, Inference, and Learning\n(eds. Flores, G., Chen, G. H., Pollard, T., Ho, J. C. & Naumann, T.)\n248–260 (PMLR, 2022).\n50. StatPearls Publishing. (n.d.).StatPearls. Retrieved September 5th,\n2024, fromhttps://www.ncbi.nlm.nih.gov/books/NBK430685/.\n51. Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat\nModels. arXiv [cs.CL](2023).\n52. Introducing Meta Llama 3: The most capable openly available LLM to\ndate. Meta AIhttps://ai.meta.com/blog/meta-llama-3/.\nAcknowledgements\nThis project was sponsored by the National Library of Medicine grant\nR01LM009886, R01LM014344, and R01LM014573, the National Center for\nAdvancing Clinical and Translational Science awards UL1TR001873 and\nUL1TR002384. Q.J. and Z.L are supported by the NIH Intramural Research\nProgram, National Library of Medicine. We also want to express our\ngratitude to Amazon Web Services (AWS) for providing the computational\nresources used in our research.\nAuthor contributions\nStudy concepts/study design, G.Z., C.W., and Y.P.; manuscript drafting or\nmanuscript revision for important intellectual content, G.Z., Z.X., Q.J., F.C.,\nY.F., Y.L., J.F.R., Z.X., Z.L., C.W., and Y.P.; approval ofﬁnal version of the\nsubmitted manuscript, G.Z., Z.X., Q.J., F.C., Y.F., Y.L., J.F.R., Z.X., Z.L.,\nC.W., and Y.P.; agrees to ensure any questions related to the work are\nappropriately resolved, G.Z., Z.X., Q.J., F.C., Y.F., Y.L., J.F.R., Z.X., Z.L.,\nC.W., and Y.P.; literature research, G.Z. and Y.P.; experimental studies, G.Z.,\nZ.X., Q.J., F.C., and Y.F.; human evaluation, Y.L., J.F.R., and Z.X.; data\ninterpretation and statistical analysis, G.Z. and Y.P.; and manuscript editing,\nG.Z., Z.X., Q.J., F.C., Y.F., Y.L., J.F.R., Z.X., Z.L., C.W., Y.P.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01651-w\n.\nCorrespondenceand requests for materials should be addressed to\nChunhua Weng or Yifan Peng.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01651-w Article\nnpj Digital Medicine|           (2025) 8:239 11",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8109740018844604
    },
    {
      "name": "Computer science",
      "score": 0.6606183052062988
    },
    {
      "name": "Information retrieval",
      "score": 0.6457220315933228
    },
    {
      "name": "Context (archaeology)",
      "score": 0.638237476348877
    },
    {
      "name": "Natural language processing",
      "score": 0.508897066116333
    },
    {
      "name": "World Wide Web",
      "score": 0.3545607328414917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3318425416946411
    },
    {
      "name": "History",
      "score": 0.15108197927474976
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4387153466",
      "name": "Weill Cornell Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800548410",
      "name": "United States National Library of Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I867280407",
      "name": "The University of Texas Southwestern Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 10
}