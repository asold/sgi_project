{
  "title": "Application of Transformer-Based Language Models to Detect Hate Speech in Social Media",
  "url": "https://openalex.org/W4224113206",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4381428723",
      "name": "Swapnanil Mukherjee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109634504",
      "name": "Sujit Das",
      "affiliations": [
        "Maulana Azad National Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4381428723",
      "name": "Swapnanil Mukherjee",
      "affiliations": [
        "Ashoka University"
      ]
    },
    {
      "id": "https://openalex.org/A2109634504",
      "name": "Sujit Das",
      "affiliations": [
        "National Institute of Technology Warangal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3106580412",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3009899658",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W3090108214",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2971050273",
    "https://openalex.org/W3119860226",
    "https://openalex.org/W4288400169",
    "https://openalex.org/W3195209315",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3021411204",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2806872289",
    "https://openalex.org/W2963790884",
    "https://openalex.org/W3032985518",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Detecting and removing hateful speech in various online social media is a challenging task. Researchers tried to solve this problem by using both classical and deep learning methods, which are found to have limitations in terms of the requirement of extensive hand-crafted features, model architecture design, and pretrained embeddings, that are not very proficient in capturing semantic relations between words. Therefore, in this paper, we tackle the problem using Transformer-based pretrained language models which are specially designed to produce contextual embeddings of text sequences. We have evaluated two such models—RoBERTa and XLNet—using four publicly available datasets from different social media platforms and compared them to the existing baselines. Our investigation shows that the Transformer-based models either surpass or match all of the existing baseline scores by significant margins obtained by previously used models such as 1-dimensional convolutional neural network (1D-CNN) and long short-term memory (LSTM). The Transformer-based models proved to be more robust by achieving native performance when trained and tested on two different datasets. Our investigation also revealed that variations in the characteristics of the data produce significantly different results with the same model. From the experimental observations, we are able to establish that Transformer-based language models exhibit superior performance than their conventional counterparts at a fraction of the computation cost and minimal need for complex model engineering. Received: 28 August 2021 | Revised: 20 October 2021 | Accepted: 29 October 2021 Conflicts of Interest The authors declare that they have no conflicts of interest to this work.",
  "full_text": "Received: 28 August 2021 | Revised: 20 October 2021 | Accepted: 29 October 2021 | Published online: 17 December 2021\nRESEARCH ARTICLE\nApplication of Transformer-Based\nLanguage Models to Detect Hate\nSpeech in Social Media\nSwapnanil Mukherjee1 and Sujit Das2,*\n1Department of Computer Science, Ashoka University, India\n2Department of Computer Science and Engineering, National Institute of Technology Warangal, India\nAbstract: Detecting and removing hateful speech in various online social media is a challenging task. Researchers tried to solve this problem\nby using both classical and deep learning methods, which are found to have limitations in terms of the requirement of extensive hand-crafted\nfeatures, model architecture design, and pretrained embeddings, that are not very proficient in capturing semantic relations between words.\nTherefore, in this paper, we tackle the problem using Transformer-based pretrained language models which are specially designed to produce\ncontextual embeddings of text sequences. We have evaluated two such models—RoBERTa and XLNet—using four publicly available\ndatasets from different social media platforms and compared them to the existing baselines. Our investigation shows that the\nTransformer-based models either surpass or match all of the existing baseline scores by significant margins obtained by previously used\nmodels such as 1-dimensional convolutional neural network (1D-CNN) and long short-term memory (LSTM). The Transformer-based\nmodels proved to be more robust by achieving native performance when trained and tested on two different datasets. Our investigation\nalso revealed that variations in the characteristics of the data produce significantly different results with the same model. From the\nexperimental observations, we are able to establish that Transformer-based language models exhibit superior performance than their\nconventional counterparts at a fraction of the computation cost and minimal need for complex model engineering.\nKeywords: transformer, hate speech, social media, RoBERTa, XLNet, fine-tuning, natural language processing\n1. Introduction\nWith the exponential rise in Internet usage in the last decade, the\npopularity and adoption of social media platforms have also risen\ngreatly. These platforms provide an open space for individuals and\ncommunities to voice their opinions and conduct business. But due\nto this substantial degree of freedom granted to users, they often\nmisuse such open platforms to pejorate, demean, and intimidate\nother users through the usage of offensive and hateful language.\nAlthough there is no formal and universally accepted definition of\nhate speech, from general consensus, we can define hate speech as\nsomething along the lines of a direct attack on an individual or a\ngroup of individuals based on their personal characteristics such as\nrace, ethnicity, gender, religious affiliation, disability, etc.; this\nmay or may not always include swear, curse, or abusive\nterminology. Due to a heavy increase in such content, social media\ncompanies are often faced with the strenuous task of detecting and\ntimely removing such content from their platforms. In the recent\npast, many approaches concerning the problem of hate speech\ndetection have been explored, which include classical machine\nlearning models as well as deep learning approaches. But to the\nbest of our knowledge, the research works that leverage state-of-\nthe-art Transformer models for the task of hate speech detection\nare limited.\nThe Transformer (Vaswani et al.,2017) architecture with the\nself-attention mechanism, since its inception, has formed the basis\nof many cutting-edge language models (LMs) such as BERT\n(Devlin et al., 2019) and GPT (Radford et al., 2018). These\nmodels, having millions of parameters, are trained on vast text\ncorpora (sometimes including the entire Wikipedia) and have\noutperformed all existing deep learning approaches in all major\nnatural language processing (NLP) tasks (Vaswani et al.,2017)\n(classification, generation, summarization, etc.) and established\nbenchmarks due to their semantic representation of text sequences.\nCurrently, Transformer-based models achieve state-of-the-art\nperformance in all sequence classification tasks. Hence, there arises\nan obvious need to investigate the applicability of Transformer\nmodels in the framework of hate detection study. Some significant\ncontributions in hate speech detection are narrated below. In the\ndomain of classical machine learning models, Davidson et al.\n(2017) proposed a machine learning approach based on Logistic\nRegression (LR) and Support Vector Machine (SVM) classifier to\nclassify a corpus of≈25k tweets divided into three categories—\nHate, Offensive, and Neither. They focused on the distinction\nbetween tweets that contain hate speech and ones that are offensive\nby arguing that the mere presence of offensive words or language\ndoes not constitute“hate speech” since the use of such language is\n*Corresponding author: Sujit Das, Department of Computer Science and\nEngineering, National Institute of Technology Warangal, India. Email: sujit.\ndas@nitw.ac.in\nJournal of Computational and Cognitive Engineering\n2023, Vol. 2(4) 278– 286\nDOI: 10.47852/bonviewJCCE2022010102\n© The Author(s) 2021. Published by BON VIEW PUBLISHING PTE. LTD. This is an open access article under the CC BY License (https://creativecommons.org/\nlicenses/by/4.0/).\n278\ncharacteristic to many groups and is also quite prevalent in popular\nculture. They noted that due to the similarity in content and structure\nof hateful and offensive tweets, and the limited number of hate\ntweets (5%) in the corpus as compared to offensive tweets (70%),\nmany tweets belonging to the hate class were misclassified as\noffensive. An important aspect of their study was that they used\nmanually extracted features from the corpora, instead of generating\nvector representations of the text samples. Next, Sreelakshmi et al.\n(2020) used different machine learning models such as SVM and\nRandom Forests for classification for detecting hate speech text in\nEnglish– Hindi code-mixed data using embeddings obtained from\npretrained models such as fastText and doc2vec. In the space of\ndeep learning models, Gomez et al. (2020) and Undirwade and Das\n(2021) focused on hate speech detection in multimodal publications,\nwhich contain images along with text. They created a multimodal\ndataset consisting of 150k samples, each containing a text and an\nimage. For the text classification task, a 150-layer long short-term\nmemory (LSTM) was used in conjunction with pretrained GloVe\n(Pennington et al.,2014) embeddings. Zhang et al. (2018)c r e a t e da\ncustom dataset of tweets concerning particular communities and\nclassified them into “hate” and “nonhate” categories with their\nproposed model, which was a combination of convolutional neural\nnetwork (CNN) and gated recurrent unit (GRU) models using\nGoogle’s word2vec (Mikolov et al.,2013) pretrained on the Google\nNews Corpus. They evaluated the performance of their model on\nother existing datasets, and it outperformed all existing baselines by\nsignificant margins. Qian et al. (2019) introduced two hate speech\ndatasets which—instead of Twitter like most other publicly available\ndatasets—were sourced from Reddit and Gab containing≈22k and\n≈34k samples, respectively. In addition to detecting hate speech,\nanother objective task was automatic generative intervention\nwhenever hate speech was detected. This was aimed at actively\ndiscouraging and reprimanding online hate speech in addition to\ndetecting and removing such content. For the binary detection task,\nthey experimented with an ensemble of classical machine learning\nmodels (LR and SVM) and deep learning models (CNN and GRU).\nIt is important to note in this regard that all classical models (SVM,\nRandom Forests, etc.) use handcrafted features such as TF-IDF and\nN-Grams while deep learning methods use pretrained vectors\ngenerated from static representation models such as word2vec and\nGloVe, which do not produce contextual embeddings. Recent study\nwith Transformer models is given in Alshalan and Al-Khalifa\n(2020), where Alshalan and Al-Khalifa, (2020) investigated hate\nspeech detection in Arabic tweets (in the Saudi context) by\nevaluating four deep learning models, one of which was the\nTransformer-based BERT. Despite BERT being the state-of-the-art\nmodel among those that were tested, it performed worse than other\ndeep learning models such as GRU and 1D-CNN. This was\nattributed to the fact that BERT is pretrained on the English\nWikipedia database and the problem concerned tweets in Arabic.\nRoy et al. (2020), in their study, used a customized RoBERTa\nm o d e l( L i ue ta l . ,2019) to achieve the best results on a multilingual\nTwitter dataset sourced by the organizers of FIRE-HASOC 2020.\nThe authors, focusing exclusively on the Transformer-based\nRoBERTa, demonstrated that large pretrained Transformer LMs are\nhighly potent in classification tasks even when the dataset size is\nvery small as compared to most other deep learning applications.\nMutanga et al. (2020) also evaluated an ensemble of Transformer\nmodels, focusing mainly on the DistilBERT model, which is a\nscaled-down version of the original BERT with increased speed\nwhile maintaining considerable consistency in performance.\nThrough thisstudy, we aimtoleverage the pre-encoded knowledge\nin such models to detect hateful content on social media based on a\nfine-tuning approach. We observed that much of the existing research\nfocused only on datasets sourced from Twitter with slight variations\nin them and used classical machine learning models alongside\nextensive hand-crafted features or deep learning methods with\npretrained language embeddings, which are not often semantically\naccurate. Therefore, extending the existing work in this domain, our\ncontributions in this paper are twofold. First, we evaluate two\nTransformer-based LMs—XLNet (Yang et al.,2019) and RoBERTa\n(Liu et al.,2019) and compare their performance to the existing state-\nof-the-art models to determine if they yield better results without\nrequiring extensive model architecture or manual feature engineering.\nSecond, we have evaluated the models over four datasets, two of\nwhich are from Reddit and Gab and the other two from Twitter, to\nbroaden the scope of the study and analyze the effect the data from\ndifferent social media platforms have on model performance, due to\ndifference in their characteristics. We perform a binary classification\ntask on three datasets (Hate or Non-Hate) and a multiclass\nclassification task on one dataset (Hate, Offensive, or Neither).\nThe remainder of the paper has been structured in the following\nway: Section2 contains background information mainly regarding\nthe datasets. Method and experiments are presented in Section3\nfollowed by results and discussion in Section4. Key conclusions\nare given in Section5.\n2. Backgrounds\nThis section briefly discusses the publicly available datasets that\nare used in this study, which includes an overview of each dataset, the\ntechniques used for preparing the data in a usable format, and statistics\nof samples in each dataset after data cleaning and preparation.\n2.1. Twitter dataset\nDavidson et al. (2017) introduced a tweet dataset that was\ncrawled from Twitter, based on terms from a hate speech lexicon\nobtained from Hatebase. This dataset contains 24,802 tweets\ndistributed into 3 classes: Hate, Offensive, and Neither. The\ndistribution of tweets in the three classes is shown in Figure1.\nThe dataset was manually labeled by workers from\nCrowdFlower with at least three workers annotating every tweet,\nso as to avoid any personal bias. A characteristic of this dataset is\nthat the data are highly skewed with the majority class containing\n75% of the samples, and such severe data imbalance often makes\nFigure 1\nDistribution of tweets in the Davidson Twitter Dataset: Hate\n(5%), Offensive (76%), and Neither (16.6%)\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n279\nit very difficult for a neural network to learn the representations\nappropriately. Nevertheless, no data augmentation techniques\nwere applied to normalize the class imbalance since it was also\nnot implemented in the original text.\n2.2. Reddit dataset\nThis dataset along with the Gab dataset was first introduced in\nQian et al. (2019) and contains 22,304 comments sourced from some\nof the “most toxic subReddits ” such as r/DankMemes, r/\nImgoingtohellforthis, r/KotakuInAction, and r/MensRights. The\ndata were then crowd-sourced to Amazon Mechanical Turk\nworkers, with each comment being labeled by three workers. The\nclass of each comment was assigned by majority voting of the\nlabels assigned by the workers. A detailed account of the data\ncollection process is provided in Qian et al. (2019). The class\ndistribution of comments in the Reddit dataset is depicted in Figure2.\nThis dataset, similar to the Davidson tweets dataset, has a\nskewed class distribution but the skewness is not as severe.\nCharacteristically, the comments in this dataset are much longer\nthan the ones in the other three datasets used in this task as\ndepicted in Table1.\n2.3. Gab dataset\nThe Gab dataset was introduced in Qian et al. (2019), but it is\nlarger than the other datasets containing around 33,776 comments\nand the class distribution of the data can be considered to be\nalmost balanced for deep learning applications. The procedure of\ndata collection was the same as outlined in Section 2.2.A n\ninteresting characteristic of the Gab dataset is the near-balanced\nclass distribution itself. The fact that this specific dataset contains\nalmost 3 times more hate comments than the other datasets\nevidently points to the abundant availability of such content on\nthe Gab platform. This can be easily attributed to Gab’s policies\nabout free speech and content moderation on their platform and\nthe user base it fosters (Brandt & Dean, 2021). The class\ndistribution for this dataset is shown in Figure3.\n2.4. HASOC 2020 dataset\nHate Speech and Offensive Content (HASOC) identification is\na competition aimed at promoting research into hateful content\ndetection in online platforms. The dataset released by the\norganizers for the 2020 iteration of the competition is a\nmultilingual one, consisting of tweets in three languages: English,\nGerman, and Hindi. The data are first divided into two classes:\nHOF (Hate and Offensive) and NOT (Non-Hate-Offensive). The\nHOF data are further subdivided into three classes: HATE (Hate),\nOFFN (Offensive), and PRFN (Profane), giving rise to two\nsubtasks. Subtask A is about the coarse-grained classification of\ntweets into Hate and Non-Hate categories and Subtask B is related\nto the fine-grained classification of tweets containing Hate Speech\ninto the three subcategories as mentioned earlier.\nFigure 2\nClass distribution of Reddit comments: Hate (30.80%)\nand Non-Hate (69.20%)\nFigure 3\nClass distribution of the Gab comments: Hate (43.26%)\nand Non-Hate (56.73%)\nTable 1\nStatistical analysis of all datasets\nDataset Tokenizer Avg sequence length Avg sentence length (p)Std. of sequence lengths\nNo. of samples having\nseq length> avg + 2*pstd\nDavidson Tweets Bert 18.04 12.71 8.18 670\nRoBERTa 16.98 7.41 746\nXLNet 18.64 8.42 729\nReddit Bert 52.18 44.58 81.03 744\nRoBERTa 49.36 76.44 753\nXLNet 52.28 80.21 756\nGab Bert 36.79 27.10 43.50 575\nRoBERTa 34.34 43.31 565\nXLNet 37.62 54.19 563\nHASOC (train) Bert 19.47 14.17 9.27 81\nRoBERTa 18.12 8.06 51\nXLNet 19.84 9.11 75\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n280\nFor the sake of maintaining consistency across the datasets, in\nthis study, we focus on the English tweets dataset and Subtask A. The\norganizers provide prebuilt train and test splits of the English dataset,\nand the train set contains a total number of 3,708 tweets out of which\n1,856 tweets are categorized as HOF and the rest NOT. Figure4\nshows the class distribution for the English HASOC 2020 dataset.\nThe test split contains a total of 815 tweets out of which 423 are\nHATE and 391 are NOT.\n2.5. Data preparation\nSince we are using Transformer models for our classification\ntask, the raw data need to be preprocessed in a manner similar to\nthe way the pretraining data were prepared during the training of\nthat particular language model. If that is not done, the\nrepresentations generated by the LM might be different than what\nwe expect due to characteristic differences in the input data, which\nmight lead to suboptimal performance on downstream tasks. For\npreparing the data in all four datasets, we broadly perform the\nfollowing operations along with some platform-specific alterations\ndue to the obvious differences between the nature of the raw data\nfrom different platforms.\n All rows with missing values were dropped.\n All usernames (e.g., @mike334) were removed.\n All platform-specific artifacts such as“RT” in Twitter and“\\t”, “r/”\nin Reddit and Gab were removed.\n Hashtag symbols (#) were removed but the hashtag contents were\npreserved (e.g., ###BlackLivesMatter was changed to\nBlackLivesMatter).\n All punctuations, links, URLs, and special characters were removed.\n The data were not tokenized during preprocessing. The\ncorresponding tokenizer of each LM was used to tokenize the\ndata at runtime.\nFor the Reddit and Gab dataset, comments from each conversation\nwere separated out as individual samples. A Python package named\nRedditcleaner (Leitner, 2021) was used to clean the comments. All\nthe datasets contain emojis in the input sentences, with the number\nbeing comparatively higher in the Twitter datasets than the Reddit\nand Gab datasets. We conducted experiments with emojis\nremoved as well as preserved. In the cases where emojis\nwere preserved, we changed the emojis into their text transcript\n(e.g.,\n is converted to face_with_tears_of_joy) using the emoji\n(Kim, 2021) Python package.\n2.6. Data statistics\nIn this subsection, we give an account of the statistical analysis of\nthe data in all four datasets, which are important while setting the\nvarious hyperparameters of the language models for generating\naccurate representations. We analyze the data with three tokenizers:\nBertTokenizer (WordPiece), RoBERTaTokenizer (WordPiece), and\nXLNetTokenizer (SentencePiece). Although we do not compare the\nBERT model for the classification task in this work, the\nBertTokenizer has been included to provide a baseline context.\n2.7. Models\nWe have used two language models in this study—XLNet (Yang\net al.,2019) and RoBERTa (Liu et al.,2019). Although both of these\nmodels are based on the original BERT, some changes have been\nincorporated in their training schema to alleviate some of the\ndrawbacks of BERT. XLNet employs Transformer-XL (Dai et al.,\n2019) as its backbone, an upgraded version of the original\nTransformer where the “XL” stands for extra large. Transformer\nXL is capable of effectively handling sequences much longer than\n512 tokens (the limit for the original Transformer) and introduces\ntwo techniques (Recurrence Mechanism and Relative Positional\nEncoding) which enable it to do so. It is also 1800+ times faster\nthan the vanilla Transformer and beats it on all benchmarks due to\nthese improved training paradigms. Consequently, XLNet proved to\nbe a much better model than its predecessors.\nRoBERTa was introduced by Facebook AI in 2019. One of its\nkey differences from BERT is that it removed the Next Sentence\nPrediction pretraining objective from its training scheme. Some of\nthe model hyperparameters were also changed; the learning rate\nand mini batch size were increased, and it was trained for much\nlonger on almost 10x more data than BERT. Finally, it yielded\nstate-of-the-art results and matched the leading score (XLNet’s)\non the GLUE benchmark.\n3. Method and Experiments\nIn this paper, we try to determine the efficacy of pretrained\nTransformer-based language models through fine-tuning. To\ndetermine how good these models perform in their bare form, without\nany custom architectural addition or algorithmic enhancement, we use\nthe vanilla Transformer models along with a simple classification head.\n3.1. Setup\nAll experiments were conducted on Google Colab and Kaggle\nKernels. We used the Transformers (Wolf et al.,2020) library, which\ncontains HuggingFace ’s implementation of Transformer-based\nlanguage models. The code was implemented in PyTorch (Paszke\net al., 2019) and SkLearn (Pedregosa et al., 2011). All\nexperiments on the Davidson dataset were conducted on the\nmulticlass distribution.\n3.2. Fine-tuning\nWe evaluated two Transformer language models in this study:\nXLNet and RoBERTa. Both of them are based on BERT, where\nsome of the shortcomings of BERT are mitigated through approaches\nexplained in Khan (2021). Since our problem focuses on“classifying”\ntext into two or more categories, we used theXLNetForSequence\nClassification and RobertaForSequenceClassification models\nprovided by the HuggingFace API, which adds a classification head\nto the base models. We did not use a base model and then add a\nFigure 4\nClass distribution of the train split of the HASOC 2020 English\ndataset: Hate (50%) and Non-Hate (50%)\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n281\ncustom classification head to it as our aim was to evaluate to what\ndegree the sole Transformer models could perform without any\nspecial enhancements. But it is often done when more control on\nthe model output and architecture of the classification head is\ndesired (Roy et al.,2020).\nSince the LMs are already pretrained on vast amounts of data,\nwe are only required to fine-tune them for the task at hand. There are\nstandard strategies for fine-tuning Transformer models for\nclassification, the most common of which are outlined as follows:\n1. Freezing all the layers of the Transformer and only fine-tuning the\nweights of the classifier layer\n2. Freezing some of the initial layers of the Transformer model and\nfine-tuning the remaining layers along with the classifier layer\n3. Fine-tuning all the layers of the Transformer.\nThere is no consensus regarding which of these strategies work best,\nand it usually depends on the data used for the given problem. Thus,\nwe tested on all of them to determine which of them yielded the best\nresults for our task. For this, we trained all models on randomly\nshuffled 85% of the data in each dataset (in separate experiments)\nand tested model performance on the remaining 15% data. The\nresults of these experiments are presented in Tables3 and 4.\nA common practice in classification problems where the class\ndistribution is skewed is to useclass weights, where weights are\nassigned to each class based on the number of samples belonging\nto a particular class, which is factored into the loss function so\nthat the model is penalized more when it makes an incorrect\nprediction on the minority class(es), which reduces the bias of the\nmodel toward the majority class. Since the dataset given in\nDavidson et al. (2017) was skewed much in comparison to the\nother datasets, we compared the effect on model performance with\nand without adding class weights to the loss function. We\nobtained lower F1 scores when using class weights. A discussion\nof the same has been undertaken in Section4.\n3.3. Hyperparameter selection\nOne of the most crucial aspects of training deep neural network\nmodels is the selection of optimal hyperparameters. Since we were\ndealing with Transformer models, which, without being supplied the\ncorrect hyperparameters, can easily perform worse than the simplest\nmodels, hyperparameter’s selection was especially important. After\ndetermining the best approach for fine-tuning the models from the\nexperiments in Section3.2 with a fixed set of hyperparameters, we\ntested different hyperparameters that affected model performance the\nmost while the others were left to their default values. Adam\noptimizer, provided by the Transformers library, was used in all\nexperiments.\nThe learning rate is, by far, the most important hyperparameter,\nwhich can drastically affect results if not selected appropriately. We\nexperimented with different learning rates, and the LR of 3e-5\nworked optimally for us yielding the best results on all datasets.\nThe learning rate of 2e-5 also performed quite well, producing\nslightly lower results than the former, but in some cases, it failed\nto converge, which led us to use the rate of 3e-5 for all other\nexperiments. After determining the best model configuration for\nthe fine-tuning strategy, we used a linear schedule for the learning\nrate during cross-validation and when compared against cross-\nvalidation with a constant LR, it outperformed the constant rate in\nmost cases by small margins.\nBatch size is another factor that influences the cost of calculation,\nand subsequently the optimization process of neural networks. We\ntested batch sizes of 32, 64, and 128 but they made no noticeable\ndifference. Hence, the default batch size of 32 was used to prevent\nrunning out of memory especially while training on the Reddit and\nGab datasets, which have significantly long sequences.\nElaborating on sequence lengths, Transformer models can\nonly take inputs of a fixed specified length, the default and\nmaximum in most cases being 512. The tokenizer for every\nmodel served by HuggingFace has an attribute “max_length, ”\nwhich specifies the maximum length of the input the model can\ntake. Shorter sequences are padded and longer sequences are\ntruncated to that length. This hyperparameter is of utmost\nsignificance since it alters the data being inputted to the model,\nthus affecting model performance. We determined the max\nlength parameters for each dataset based on the distribution of\nsequence lengths in that particular dataset. Apart from the\neffect on the model performance, it is interesting to note that\nthe computational time increases in proportional to the\nsequence length of the data. The Reddit and Davidson tweets\ndatasets approximately contain the same number of samples but\nthe max_length for the Reddit dataset was ≈6.5 times the\nmax_length for the Davidson tweets dataset, and this is\nreflected in the training time. It took us around 4 mins to\ntrain 1 epoch on the tweets dataset, whereas it took 30 mins for\nthe same on the Reddit dataset ( ≈7× the time for the tweets\ndataset).\nFor the parameter weight decay, we experimented with two\nweight decay rates of 0.1 and 0.01 and discovered that they both\nproduced similar results, the latter being slightly more stable\nthroughout training. Both these values worked almost at par for\nour datasets and the difference in results was marginal.\nWe trained all models for all experiments to the point where\ntheir performance on the test and validation sets started to drop,\nso as to gain a definite idea about the number of epochs to train\nthe models where their performances peak. In most of the\nexperiments, we noticed that the performance was highest after\ntraining for three epochs, and in some cases, two epochs. On\nfurther training, the model started to overfit and the validation and\ntest accuracy dropped steadily. We present a summary of\nthe best hyperparameters for both models on all four datasets in\nTable 2.\nAfter determining the best configuration of each model and for\nevery dataset, we performed 5-fold cross-validation with the selected\nconfiguration(s) to determine the ability of the models to generalize\nto unseen data and to validate whether the results achieved in the\nprevious experiments were coincidental and reproducible only for\nTable 2\nSelected hyperparameters for each dataset for the classification task\nLearning rate (LR) Batch size Weight decay Number of epochs\n3e-5 (Linear schedule) 32 0.1 3\nDavidson tweets Reddit Gab HASOC\nSequence length 40 260 128 50\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n282\nthe particular data split. In addition to this, we trained a model on the\nentire Gab dataset and made predictions on the Reddit dataset and\nvice versa , and the results were in line with all the past\nexperiments showing that the Transformer-based models are\nextremely robust, even to the point of achieving performance at\npar with models natively trained and tested on data belonging to\nonly one source distribution.\n4. Results and Discussion\nIn this section, we showcase the results of the experiments\noutlined in the preceding section to show that the Transformer\nmodels perform significantly better than existing approaches on\nall the datasets that we tested. Since the datasets in consideration\nare unbalanced, we settled upon the F1 score as the primary\nmeasure of model performance as accuracy can sometimes be\nvery misleading in the case of unbalanced data. Hence, we only\nreport the F1 scores (expressed as a percentage) accounting for\nclass imbalance and make all comparisons with previous works in\nrespect to the same.\n4.1. Emojis\nAs introduced in Section2.5, we compared the results before\nand after removing emojis from the Twitter datasets, and the latter\nyielded better performance. This is surprising because emojis\nprovide useful context in a piece of text, and previous works have\nshown that model performance (Roy et al.,2020) was benefited\nby incorporating emojis. A possible explanation for this could be\nthe way the emojis are passed on to the model. Roy et al. (2020)\nused emo2vec (Wang et al.,2020) to convert the emojis directly\ninto vector representations and then concatenated them to the\n(cleaned) text representation obtained from the Transformer\nmodel, which was then inputted to the classifier. On the other\nhand, our preprocessing approach replaced the emojis with their\nrespective textual transcript, which creates an abrupt disruption in\nthe otherwise natural flow of a sentence. Since the Transformer\nlooks at the neighboring context of every token in the sequence,\nthe sudden interruption by the emoji transcription within the\nsentence may have created confusion for the model.\n4.2. Class weights\nAs introduced in Section3.2, surprisingly, the usage of class\nweights negatively impacted model performance. A possible\nexplanation for this occurrence is that class weights penalize\nthe loss function more for making incorrect predictions on the\nminority class compared to making incorrect predictions on\nthe majority class. The number of samples in the “hate” and\n“neither” classes is very less as compared to the“offensive” class,\nand as reported in Davidson et al. ( 2017), there is a lot of\nlinguistic and semantic overlap between the hate and offensive\nclass, which makes the data ambiguous in respect to the\npretrained representations of the Transformer, and consequently,\nthe classification task also becomes harder. Hence, the model\nwhile not being able to improve performance on the minority\nclasses also performs worse on the majority class due to increased\npenalty. Thus, the overall performance is going down.\n4.3. Fine-tuning strategy\nThe best results achieved on a particular test split for each fine-\ntuning strategy tested are presented in Tables3 and 4.\nFrom Tables3 and 4, a general trend can be noticed that model\nperformance increases with the number of encoder layers being fine-\ntuned, and the best performance was achieved when all layers of the\nmodel were fine-tuned. It is notable in this regard that the difference\nof performance between freezing the first seven layers and the first\nfive layers is not very pronounced. This can be attributed to the fact\nthat each encoder layer of the Transformer encodes different\ninformation about the input, but fine-tuning three more layers,\nwhich are directly next to the layers fine-tuned in the previous\nexperiment, will not provide a big performance improvement\nbecause they are likely to encode a similar kind of information.\n4.4. Cross-validation\nAfter determining the optimal fine-tuning strategy and values\nfor the hyperparameters from the initial experiments, we\nperformed 5-fold cross-validation on three datasets with the data\ndistributed proportionally in each fold. We also used a linear\nschedule on the learning rate, which proved to yield slightly better\nresults than the constant rate. We left the HASOC dataset out of\nthis experiment since it already contained a predefined test split.\nTherefore, cross-validating the model to observe generalization\nability was not necessary as we were only required to maximize\nperformance on the given test split. From Table 5, it can be\nobserved that the cross-validation F1 scores are lower than the\nscores on the individual test split, which shows that a model\nmight perform exceptionally well on a data split due to the\ndistribution of that particular split but it may not necessarily\nTable 4\nPerformance of different fine-tuning strategies for RoBERTa\nStrategy\nDavidson\ntweets Reddit Gab HASOC\nClassifier layer only 87.34 89.42 89.45 85.21\nClassifier layer with last 2\nencoder layers\n91.07 91.78 91.60 88.20\nClassifier layer with last 5\nencoder layer\n90.38 92.15 91.75 88.43\nAll layers 91.66 92.40 92.75 89.80\nTable 3\nPerformance of different fine-tuning strategies for XLNet\nStrategy\nDavidson\nTweets Reddit Gab HASOC\nClassifier layer only 88.41 89.16 88.67 84.52\nClassifier layer with last 2\nencoder layers\n90.70 92.00 91.31 86.60\nClassifier layer with last 5\nencoder layers\n90.50 91.87 91.34 86.85\nAll layers 90.87 92.34 92.06 88.94\nTable 5\nF1 scores for 5-fold cross-validation on each dataset\nModel Davidson tweets Reddit Gab\nXLNet 90.69 91.48 91.76\nRoBERTa 90.75 91.81 92.21\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n283\nperform so well on unseen data. In our case, the cross-validation\nscores, although lower, are quite close to the best scores on the\ntest split so it can be inferred that these models are capable of\ngeneralizing quite well to new data.\n4.5. Cross testing\nWe studied a step beyond cross-validating models on different\nfolds of the training data and trained models on a particular dataset\nonly to assess its performance on an entirely different dataset. This\nwas done to test if these huge Transformer-based LMs, which already\nencode a lot of general textual information owing to their pretraining,\nare truly robust enough to perform decently on new data if fine-tuned\nappropriately for that task, independent of their pretraining data. For\nthis, we trained two RoBERTa models each on the Gab and Reddit\ndatasets, respectively, and tested them on the other dataset. The\nReddit and Gab datasets were chosen due to the similarity in their\nplatforms and the characteristics of the data. The model trained on\nReddit data achieved an F1 score of 90.35 on the entire Gab\ndataset and the model trained on Gab data achieved an F1 score\nof 91.31 on the Reddit dataset. The lower score for the model\ntrained on the Reddit data is supported by the fact that the class\ndistribution of the Reddit dataset is quite imbalanced in\ncomparison to that of the Gab data. But overall, it can be noticed\nthat the F1 scores are quite close to those achieved in the previous\nexperiments where models were trained and validated on different\nsplits of the same dataset. It can be thus ascertained that if\nTransformer-based LMs are fine-tuned on sufficient amounts of\nhigh quality for a particular task, they are very likely to perform\nquite good on new data, which might be from an entirely different\nsource given the new data are characteristically and linguistically\nnot very different from the data used for fine-tuning.\n4.6. Comparison and analysis\nThe section compares the performance of the Transformer LMs\nwith the best performance attained in Davidson et al. (2017), Qian\net al. (2019), and Roy et al. (2020) on the respective datasets. The\nresults presented in Table 6 are the best scores selected from\nmultiple runs of the 5-fold cross-validation.\nIt is clear from the results that both the Transformer LMs,\nXLNet and RoBERTa, have outperformed previous baselines,\nwhich are based on classical and deep learning methods. We now\nundertake an analysis of the results and compare them to the\nprevious baselines.\nIt might seem at first glance that the improvement in the\nDavidson tweets dataset is insignificant for Transformer models\nand as compared to other datasets, but this can be explained\nthrough evidence provided in the original dataset. The biggest\ndrawback of this dataset is the extreme skewness of the classes.\nOnly 5% of the samples are labeled as hate speech when that is\nthe class that the model is trying to detect and the phenomenon of\nthis severe class imbalance is a huge problem for deep neural\nnetwork models (Zhang & Luo,2019). Moreover, the final scores\nreported by Davidson et al. (2017) showed the result by training\nthe model on the entire dataset, whereas we report the best scores\nafter cross-validation, which keeps aside 20% of the data. Hence,\nit can be believed that the performance will increase if trained on\nthe entire dataset, which will give it exposure to more training\ndata. Nevertheless, the F1 score was reported to be 0.51 for the\n“hate” class in Davidson et al. (2017), whereas the highest we\nwere able to achieve was 0.53 on the RoBERTa model —an\nimprovement of two percentage points. Another important factor\nto consider is the “inter-coder” agreement score provided by\nCrowdFlower, the service which was used to annotate the dataset.\nIn Davidson et al. (2017), it was mentioned that the intercoder\nagreement was 92%, which means that only 92% of the time the\nannotators agreed upon a common label for a particular data\nsample. Since no model of present times can be expected to have\nperformance better than that, it can thus be considered as the\nperformance upper limit.\nOn the other hand, there has been quite an improvement in the\nReddit dataset over the baseline score and that can be attributed to the\nexperimental setup and adopted approaches mentioned in Qian et al.\n(2019). Recurrent Neural Networks (RNNs), while being able to\nprocess sequential data efficiently, fail to provide optimal\nperformance when the length of the sequences becomes\nexcessively large. They were used in conjunction with pretrained\nword embeddings obtained from word2vec, which do not take\ncontext into account when generating word embeddings.\nTransformers are well equipped to overcome these problems since\nthey can (1) handle comparatively longer sequences better than\nRNNs and (2) the multiheaded self-attention is the key strength of\nthe Transformer architecture, which takes into context neighboring\ntokens for generating contextualized embeddings. This gives the\nTransformer models a strong edge over the RNN, especially in the\nReddit dataset, where the sequences are very long.\nTransformer models also made a good improvement on the Gab\ndataset, though not as pronounced as on the Reddit dataset. This can\nbe attributed to the fact that the CNN model performed quite well in\n(Qian et al.,2019) because the sequences in the Gab dataset are half\nthe length of those in the Reddit dataset, which reduces the chances\nof poisoning of the model by artifacts and unwanted noise. CNN\nmodel also serves as a better“feature extractor” in comparison to\nRNN, which paired with a nearly balanced dataset provided quite\ngood results. Hence, making dramatic improvements on the Gab\ndataset were not a very likely event.\nLastly, the HASOC dataset is the only one where we could not\nimprove our results over existing ones. This is easily explained by the\nfact that the authors in Roy et al. (2020) used an XLM-RoBERTa,\nthough with a different architecture, and thus we were not\nexpecting any major improvements on that dataset although both\nTable 6\nComparison of results achieved by the Transformer models with the best results obtained by original authors.\nBest results are highlighted in bold\nRoBERTa XLNet Author ’s best\nDavidson tweets 90.84 90.69 90.00 (SVM)\nReddit 91.81 91.48 77.5 (RNN + word2vec)\nGab 92.21 91.76 89.6 (CNN + word2vec)\nHASOC 89.80 88.94 90.29 (RoBERTa + custom classification head)\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n284\nour works used the same model, and it is important to note the\nsignificant differences in model architecture and training\nmethodology. The authors of Roy et al. ( 2020) used a base\nmultilingual RoBERTa model to generate the embeddings for the\ninput data and then fed it to a 12-layer feedforward network for\nclassification. They also used a self-adjusting learning rate where\nthe LR would be automatically decreased to lower values based\non the macro F1 score for the current epoch instead of the\nconventional validation loss, and a model reinstatement\nmechanism would automatically revert the model to the last best\nstate if the current training epoch performs worse than the last\nbest. Their incorporation of emojis was also quite different from\nours as described in Section 4.1. In contrast, we evaluated the\nperformance of the standard Transformer classification models\nprovided by the HuggingFace API, which attaches a two-layer\ndense network on top of the vanilla Transformer for classification.\nWe also did not make any other custom changes to suit this\nparticular dataset to maintain parity with the other datasets.\nNevertheless, our performance was very close to the performance\nof Roy et al. (2020) on much simpler model training constraints\nand architecture, thus saving a lot of computational power and\nmodel engineering complexity.\nApart from comparing Transformer LMs to other deep learning\nand classical models, it would be only fair to pit the models against\nthemselves to determine which is better suited for this particular task.\nFrom the results, it is evident that RoBERTa beats XLNet in almost\nall experiments, even if by small margins. This might come across as\nslightly surprising since RoBERTa inherited a lot from BERT\nincluding most of the core pretraining setup and XLNet had\nbeaten BERT in all benchmarks due to itsPermutation Language\nModelling mechanism. But what made the crucial difference is the\ndata on which the models were pretrained. As highlighted in\nSection II.G, RoBERTa used almost 10 times more training data\nthan XLNet most of which consisted of Internet data such as\nreviews, comments, and web articles. Therefore, RoBERTa had\nalready encoded a lot more information about the kind of data\ncommonly available on social media platforms than XLNet during\nits pretraining phase, which made it easier to fine-tune for our\ntask and consequently achieved better scores.\n4.7. Limitations\nAlthough Transformer-based LMs are proved to be very\nefficient in their results and brought several advantages over\nconventional methods, they have some inherent limitations. The\nTransformer is a very complex neural network, and like other\nneural network models, it works as a black box. That is, we can\nonly supply it with the input and receive the corresponding\noutput, but the input is transformed into the corresponding output\ninside the various layers of the network using some complex\nprocess. Although we may have the knowledge of how a certain\nnetwork functions, it is very difficult to explain the predictions of\nsuch complex systems because of the lack of metadata on the\ninput in the intermediary stages of the network. Consequently, we\nare limited in our power to achieve complete transparency in the\nsystems that we develop, which might give rise to ethical and\nphilosophical issues. Another disadvantage of these Transformer\nmodels is also one of its biggest advantages —the rigidity of\nmodel architecture. These models, unlike conventional neural\nnetwork models, cannot be re-engineered at will to suit a specific\nproblem or scenario. Their design and pretraining scheme is very\nspecifically developed following highly complex methods, and if\none were to try to modify the structure of the model beyond the\npermitted specifications, it may result in a catastrophic failure of\nthe entire system. Thus, there is very little flexibility as to what\ncan be done with the model architecture-wise.\n5. Conclusion and Future Work\nIn this study, we have investigated the performance of two\nTransformer language models, RoBERTa and XLNet, on four\ndifferent datasets three of which were from different social media\nplatforms and compared them to the existing deep learning and\nclassical architectures. Both the models surpassed the existing\nbaseline scores on three of the four datasets and performed nearly\nas well on the fourth dataset, which used a RoBERTa model. From\nour experiments, it can be concluded that these state-of-the-art\nmodels, which have revolutionized the domain of natural language\nprocessing, prove to be equally effective and promising for the task\nof hate speech detection. This approach brings some clear\nadvantages—(1) The computation cost for the whole process is\nlargely minimized because these models are pretrained on vast\namounts of data. (2) There is no need to expedite effort on\ndeveloping new architectural designs of models and validate their\nefficiency for this particular task because the Transformer-based\nLMs use the same underlying architecture, which is very efficient\nfor the purpose it was developed. (3) Since the effort on model\nengineering is minimized, it gives a greater scope to procure and\nexperiment with the data, which is usually not the point of focus in\nsuch studies. Although we tested only two models in this work,\nresearch in this domain is moving at a fast pace and new and better\nmodel architectures are being developed. Hence, there may exist\nother Transformer architectures that perform better than the ones\nexplored in this work, which will require further investigation and\nexperimenting to ascertain. Furthermore, we only considered\ntextual data consisting of tweets and comments, but the problem of\nmultimodal hate speech analysis, where text data are often\naccompanied by data from other modalities such as images or\nvideo, is a tougher challenge. Therefore, Transformer-based\nmodels can be used in conjunction with image-processing models\nto take into account both modalities to make more accurate\npredictions. Recent application of the Transformer architecture to\nimage processing resulted in Vision Transformer (Dosovitskiy\net al.,2020), which has beaten the best CNN models to become the\nnew state of the art. Thus, the performance of conventional CNN\nmodels can be compared to Vision Transformers. The multimodal\nCNN– Transformer combination can also be compared to other\nconventional multimodal models. Another very important\nobservation we made through this study is that the data used to\nfine-tune the model is a more important contributor toward the\nresults achieved than the model itself; smaller but quality datasets\nyielded results as good as large datasets of relatively poorer\nquality. Given that the Transformer is a very complex model with a\ncarefully designed architecture, which due to its pretraining already\nencodes most of the necessary information, it might be a good\nopportunity for the research community to gravitate toward\nsourcing better data, both in scale and quality, for the fine-tuning\nphase, and shift their focus to data engineering rather than model\nengineering in support of the emerging trend of“Data-Centric AI.”\nConflicts of Interest\nThe authors declare that they have no conflicts of interest to this\nwork.\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n285\nReferences\nAlshalan, R., & Al-Khalifa, H. (2020). A deep learning approach for\nautomatic hate speech detection in the saudi twittersphere.\nApplied Sciences, 10 (23), 8614. https://doi.org/10.3390/\napp10238614.\nBrandt, L., & Dean, G. (2021). Gab, a social-networking site popular\namong the far right, seems to be capitalising on Twitter bans\nand Parler being forced offline. It says it’s gaining 10,000\nnew users an hour.Business Insider, 11.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., & Salakhutdinov, R.\n(2019). Transformer-XL: Attentive language models beyond a\nfixed-length context. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\n2978– 2988. https://doi.org/10.18653/v1/P19-1285.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai,\nX., Unterthiner, T., ::: , & Houlsby, N. (2020). An image\nis worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint: 2010.11929. https://doi.org/10.\n48550/arXiv.2010.11929\nDavidson, T., Warmsley, D., Macy, M., & Weber, I. (2017).\nAutomated hate speech detection and the problem of\noffensive language. In Proceedings of the International\nAAAI Conference on Web and Social Media, 11 (1),\n512– 515. https://doi.org/10.1609/icwsm.v11i1.14955\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT:\nPre-training of deep bidirectional Transformers for language\nunderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\n1, 4171– 4186. https://doi.org/10.18653/v1/N19-1423.\nGomez, R., Gibert, J., Gomez, L., & Karatzas, D. (2020). Exploring\nhate speech detection in multimodal publications. In 2020\nIEEE Winter Conference on Applications of Computer Vision,\n1470–1478.https://doi.org/10.1109/WACV45572.2020.9093414.\nKhan, S. (2021).BERT, RoBERTa, DistilBERT, XLNet—Which one to\nuse? Retrieved from: https://towardsdatascience.com/bert-\nRoBERTa-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8\nKim, T. (2021). Emoji. Retrieved from: https://github.com/\ncarpedm20/emoji\nLeitner, L. (2021). Redditcleaner. Retrieved from: https://github.\ncom/LoLei/redditcleaner\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,::: &\nStoyanov, V. (2019). RoBERTa: A robustly optimized BERT\npretraining approach.arXiv preprint: 1907.11692.https://doi.\norg/10.48550/arXiv.1907.11692\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J.\n(2013). Distributed representations of words and phrases and\ntheir compositionality. Advances in Neural Information\nProcessing Systems, 26.\nMutanga, R. T., Naicker, N., & Olugbara, O. O. (2020). Hate speech\ndetection in twitter using Transformer methods.International\nJournal of Advanced Computer Science and Applications,\n11(9). https://doi.org/10.14569/IJACSA.2020.0110972.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,\nG., ::: , & Chintala, S. (2019). Pytorch: An imperative style,\nhigh-performance deep learning library.Advances in Neural\nInformation Processing Systems, 32.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B.,\nGrisel, O.,::: , & Duchesnay, E. (2011). Scikit-learn: Machine\nlearning in Python. The Journal of Machine Learning\nResearch, 12, 2825– 2830.\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global\nvectors for word representation. InProceedings of the 2014\nConference on Empirical Methods in Natural Language\nProcessing, 1532– 1543. https://doi.org/10.3115/v1/D14-1162.\nQian, J., Bethke, A., Liu, Y., Belding, E., & Wang, W. Y. (2019). A\nbenchmark dataset for learning to intervene in online hate\nspeech. InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing, 4754– 4763. https://doi.org/10.18653/v1/D19-\n1482.\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I.\n(2018). Improving language understanding by generative pre-\ntraining.R e t r i e v e df r o m :https://blog.openai.com/language-\nunsupervised/\nRoy, S. G., Narayan, U., Raha, T., Abid, Z., & Varma, V. (2020).\nLeveraging multilingual transformers for hate speech\ndetection. In Proceedings of FIRE 2020 Working Notes,\nForum for Information Retrieval Evaluation.\nSreelakshmi, K., Premjith, B., & Soman, K. P. (2020). Detection of\nhate speech text in Hindi-English code-mixed data.Procedia\nComputer Science, 171, 737– 744. https://doi.org/10.1016/j.\nprocs.2020.04.080.\nUndirwade, A., & Das, S. (2021). Image anonymization using deep\nconvolutional generative adversarial network. Machine\nLearning Algorithms and Applications,305– 330. https://doi.\norg/10.1002/9781119769262.ch17.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., ::: , & Polosukhin, I. (2017). Attention is all you need.\nAdvances in Neural Information Processing Systems, 30.\nWang, S., Maoliniyazi, A., Wu, X., & Meng, X. (2020). Emo2Vec:\nLearning emotional embeddings via multi-emotion category.\nACM Transactions on Internet Technology , 20(2), 1– 17.\nhttps://doi.org/10.1145/3372152.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A.,\n::: , & Rush, A. M. (2020). Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, 38– 45. https://doi.org/\n10.18653/v1/2020.emnlp-demos.6.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., &\nLe, Q. V. (2019). Xlnet: Generalized autoregressive pretraining\nfor language understanding.Advances in Neural Information\nProcessing Systems,32.\nZhang, Z., Robinson, D., & Tepper, J. (2018). Detecting hate speech\non twitter using a convolution-gru based deep neural network.\nIn A. Gangemi, R. Navigli, M.-E. Vidal, P. Hitzler, R. Troncy,\nL. Hollink, A. Tordai, & M. Alam (Eds.),The semantic web,\n(pp. 745– 760). Springer International Publishing.https://doi.\norg/10.1007/978-3-319-93417-4_48.\nZhang, Z., & Luo, L. (2019). Hate speech detection: A solved problem?\nThe challenging case of long tail on twitter.Semantic Web, 10(5),\n925– 945. https://doi.org/10.3233/SW-180338.\nHow to Cite:Mukherjee, S. & Das, S. (2023). Application of Transformer-Based\nLanguage Models to Detect Hate Speech in Social Media. Journal of\nComputational and Cognitive Engineering , 2(4), 278 – 286, https://doi.org/\n10.47852/bonviewJCCE2022010102\nJournal of Computational and Cognitive EngineeringVol. 2 Iss. 4 2023\n286",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8309903144836426
    },
    {
      "name": "Computer science",
      "score": 0.7764534950256348
    },
    {
      "name": "Language model",
      "score": 0.6391681432723999
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5607351660728455
    },
    {
      "name": "Computation",
      "score": 0.5481838583946228
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5167385935783386
    },
    {
      "name": "Architecture",
      "score": 0.45029139518737793
    },
    {
      "name": "Natural language processing",
      "score": 0.44048643112182617
    },
    {
      "name": "Deep learning",
      "score": 0.4353407025337219
    },
    {
      "name": "Machine learning",
      "score": 0.42298778891563416
    },
    {
      "name": "Social media",
      "score": 0.4144389033317566
    },
    {
      "name": "Speech recognition",
      "score": 0.4059753119945526
    },
    {
      "name": "Engineering",
      "score": 0.08895072340965271
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Algorithm",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91277730",
      "name": "Maulana Azad National Institute of Technology",
      "country": "IN"
    }
  ],
  "cited_by": 21
}