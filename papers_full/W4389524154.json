{
  "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
  "url": "https://openalex.org/W4389524154",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098730636",
      "name": "Xiaohua Wang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2797806351",
      "name": "Yuliang Yan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2786938257",
      "name": "Longtao Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2130394239",
      "name": "Xiaoqing Zheng",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2049019309",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2970716846",
    "https://openalex.org/W4386566667",
    "https://openalex.org/W4298856952",
    "https://openalex.org/W4377372342",
    "https://openalex.org/W4389519041",
    "https://openalex.org/W1988520084",
    "https://openalex.org/W4379986648",
    "https://openalex.org/W2068457373",
    "https://openalex.org/W2055011753",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W4226101361",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2016702698",
    "https://openalex.org/W2329632867",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W2092391357",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4385571920",
    "https://openalex.org/W4248931877",
    "https://openalex.org/W4300476846",
    "https://openalex.org/W2484571116"
  ],
  "abstract": "Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed “hallucinations”, remains a significant challenge. Current hallucination detection methods often necessitate the retrieval of great numbers of relevant evidence, thereby increasing response times. We introduce a unique framework that leverages statistical decision theory and Bayesian sequential analysis to optimize the trade-off between costs and benefits during the hallucination detection process. This approach does not require a predetermined number of observations. Instead, the analysis proceeds in a sequential manner, enabling an expeditious decision towards “belief” or “disbelief” through a stop-or-continue strategy. Extensive experiments reveal that this novel framework surpasses existing methods in both efficiency and precision of hallucination detection. Furthermore, it requires fewer retrieval steps on average, thus decreasing response times.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15361–15371\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHallucination Detection for Generative Large Language Models by\nBayesian Sequential Estimation\nXiaohua Wang1,2, Yuliang Yan1,2, Longtao Huang3, Xiaoqing Zheng1,2,∗, Xuanjing Huang1,2\n1School of Computer Science, Fudan University, Shanghai, China\n2Shanghai Key Laboratory of Intelligent Information Processing\n3Alibaba Group\n{xiaohuawang22,ylyan21}@m.fudan.edu.cn\n{zhengxq,xjhuang}@fudan.edu.cn\nAbstract\nLarge Language Models (LLMs) have made re-\nmarkable advancements in the field of natural\nlanguage generation. However, the propensity\nof LLMs to generate inaccurate or non-factual\ncontent, termed “hallucinations”, remains a sig-\nnificant challenge. Current hallucination detec-\ntion methods often necessitate the retrieval of\ngreat numbers of relevant evidence, thereby in-\ncreasing response times. We introduce a unique\nframework that leverages statistical decision\ntheory and Bayesian sequential analysis to op-\ntimize the trade-off between costs and bene-\nfits during the hallucination detection process.\nThis approach does not require a predetermined\nnumber of observations. Instead, the analysis\nproceeds in a sequential manner, enabling an\nexpeditious decision towards “belief” or “disbe-\nlief” through a stop-or-continue strategy. Exten-\nsive experiments reveal that this novel frame-\nwork surpasses existing methods in both effi-\nciency and precision of hallucination detection.\nFurthermore, it requires fewer retrieval steps\non average, thus decreasing response times1.\n1 Introduction\nIn the era of information overload and the prolif-\neration of misleading or false information, auto-\nmatic fact checking has become an essential tool\nfor verifying the veracity of claims and combating\nmisinformation. Large Language Models (LLMs),\nsuch as GPT-4 (OpenAI, 2023),PaLM (Chowdhery\net al., 2022) and LLaMA (Touvron et al., 2023),\nhave made significant advancements in the field of\nnatural language generation (NLG). However, the\ninherent tendency of LLMs to generate inaccurate\nor non-factual content, commonly referred to as\n“hallucinations” (Bang et al., 2023; Ji et al., 2022),\ncontinues to present a significant challenge.\nOne previous work utilizes the sampled-based\napproach for detecting hallucination (Manakul\n1Our code is available at https://github.com/\nxhwang22/HallucinationDetection.\nEvidenceClaim\nResponse: “Giuseppe Mariani \nwas an Italian professional \nfootballer who played as a \nforward. He was born in Milan, \nItaly, and died in Rome, Italy. \n[truncated]” Document Retrieval\nEvidenceClaim\nFaculty\nHallucination\nPrior work: Retrieve consistent \nnumber of documents\nOur method: Decide whether to \nretrieve additional evidence via \nBayesian sequential analysis\n①\n②\n③\n④Bayesian Risk \nDecision\nHallucination \nClassifier\nHallucination \nClassifier\nFigure 1: Evidence-based hallucination detection based\non Bayesian sequencial analysis. Prior works often re-\ntrieve a predetermined number of documents and over-\nlooked the variability among different inputs. In our\nframework, (1) we first retrieve one document from\nthe search engine; (2) External evidence and the claim\ngenerated by the LLM are input into the hallucination\nclassifier to calculate the veracity score; Then we make\na decision on whether to stop observing and make a\nveracity judgment based on the available evidence or to\ncontinue observing additional evidence. This decision is\nmade based on the Bayesian risk decision and sequential\nanalysis. (3) If stopping and making a determination\ncarries a higher risk, we choose to continue observing\nadditional evidence; (4) Otherwise, we choose to deter-\nmine the veracity of the claim.\net al., 2023), they assume that LLMs are capable\nof producing similar responses and contain consis-\ntent facts related to a given concept. However, this\nmethod necessitates many samplings of the LLM,\nleading to tremendous costs in practical applica-\ntions. Another commonly employed strategy in\nfact checking entails retrieving relevant evidence\nfrom various sources, such as texts (Thorne et al.,\n2018; Augenstein et al., 2019), knowledge graphs\n(Kim et al., 2023) and the webs (Chen et al., 2023;\nKamoi et al., 2023) to validate or challenge claims.\nThese methods usually retrieve a predetermined\nnumber of documents across diverse cases. How-\never, the effectiveness of these approaches heavily\ndepends on the selection of the number of docu-\nments to retrieve. While these methods can ensure\n15361\na certain level of consistency, in practical applica-\ntions, different claims may require varying amounts\nof external evidence for verification. This should be\ndetermined by the nature of the claim itself and the\ndocuments retrieved, rather than predetermined pa-\nrameters. The optimal number of external evidence\nsources needed to validate a claim should be dy-\nnamically determined based on the specific context\nand characteristics of the claim, taking into account\nfactors such as complexity, ambiguity, and the avail-\nability of relevant information. This study aims to\ninvestigate the feasibility of gradually collecting\nrelevant information and adopting a data-driven\napproach to document retrieval in the context of\nhallucination detection for large language models\n(LLMs) in real-world scenarios. By employing\nsuch an approach, we can enhance retrieval effi-\nciency and reduce unnecessary retrieval attempts.\nIn this paper, we propose a novel framework that\nleverages Bayesian sequential analysis (Wetherill,\n1961; Arrow et al., 1949; Carlin et al., 1998) to en-\nhance the retrieval efficiency for detecting whether\nclaims generated by LLMs are hallucinated or fac-\ntual in the wild. Recognizing that a single sentence\nmay contain a mixture of factual and nonfactual\ninformation, we first decompose a claim into sub-\nclaims, each focusing on a single entity and its\nassociated attributes (Kamoi et al., 2023). In the\nsubsequent stage, we employ a search engine to\nretrieve relevant documents for each subclaim in-\ndividually. These retrieved documents, in conjunc-\ntion with their corresponding subclaims, are then\ninput into a classifier to estimate their veracity (Nie\net al., 2020). As illustrated in Figure 1, we con-\nsider the retrieved documents as a sequence and\nmake informed decisions on whether additional\ninformation should be retrieved for a given case\nvia Bayesian sequential analysis through a stop-or-\ncontinue strategy. Once the information gained is\nenough to decide whether claims are hallucinations,\nwe stop and make an evaluation. Otherwise, we\nretrieve additional documents and make a decision\nin the next step. Finally, we aggregate the assess-\nments of the subclaims to determine the overall\nveracity of the input claim.\nTo evaluate the effectiveness of our framework,\nwe conducted experiments on the dataset of Self-\nCheckGPT (Manakul et al., 2023), containing\n1,908 sentences from 238 articles generated by\nGPT-3, each labeled with its corresponding verac-\nity. The experimental results demonstrate that our\nframework outperforms sample-based methods in\nterms of efficiency and accuracy in hallucination\ndetection. In the task of sentence-level hallucina-\ntion detection, our framework achieves comparable\nperformance to the baseline approach. However, in\nthe passage-level task, we observe an improvement\nof 6.43% in terms of Spearman’s correlation coef-\nficient. Furthermore, across different sets of hyper-\nparameter configurations, our framework consis-\ntently reduces the number of retrieved documents\ncompared to the approach with a fixed number of\nretrieved documents. These results indicate the\nsuperiority of our approach in detecting hallucina-\ntions with improved efficiency and precision, show-\ncasing its potential for practical applications.\n2 Related Work\n2.1 Hallucination Detection for Large\nLanguage Models\nSeveral studies have explored the phenomenon of\nhallucination (Su et al., 2022; Lee et al., 2022; Dai\net al., 2022) in large language models and have pro-\nposed various approaches for its causation and de-\ntection. Bang et al. (2023) conducted an evaluation\nof ChatGPT’s hallucination on fact-checking test\nsets and knowledge-based QA tasks, highlighting\nthe model’s susceptibility to extrinsic hallucination\ncompared to intrinsic hallucination (Ji et al., 2022).\nIn terms of detection, Kadavath et al. (2022)\nand Mündler et al. (2023) conducted a study to\ninvestigate whether language models can assess\nthe validity of their own claims, aiming to explore\nthe self-evaluative capabilities of language models.\nAzaria and Mitchell (2023) employed the internal\nstate of LLMs to determine the truthfulness of state-\nments by leveraging the hidden-layer activations\nas input to the classifier. Manakul et al. (2023) in-\ntroduced a sample-based detection method for hal-\nlucination detection. They emphasized that when\na language model is well-acquainted with a given\nconcept, the sampled responses are likely to be\nsimilar and contain consistent facts. This method\nrelies on analyzing the consistency of the generated\nsamples to identify potential hallucinations. Unlike\ntheir approach, we employ an evidence-based fact-\nchecking method, where we verify the correctness\nof claims based on external documents.\n2.2 Evidence-based Fact Checking\nEvidence-based fact-checking has gained signifi-\ncant attention as an effective approach to combat\n15362\nmisinformation and ensure information accuracy.\nFEVER (Thorne et al., 2018) utilized Wikipedia ev-\nidence for fact verification, while Augenstein et al.\n(2019) collected evidence from fact-checking web-\nsites and proposed a method that combines veracity\npredictions and evidence ranking.\nRecent approaches include the use of knowledge\ngraphs (KG) by Kim et al. (2023), where a classi-\nfier predicts relations and hops related to the claim,\nand related sequences are retrieved from the KG\nas evidence. Kamoi et al. (2023) put forward a\ndataset constructed using real-world claims and\nevidence. Their work focused on enhancing the\nperformance of entailment models by reducing the\ncomplexity of claims through a decomposition pro-\ncess. By breaking down the claims into simpler\ncomponents, they aim to facilitate a more effective\nevaluation of entailment and improve the overall\nperformance of the models. Chen et al. (2023)\npresented an automated pipeline for fact-checking\nreal-world claims, retrieving raw evidence from\nthe web. However, these methods retrieve a fixed\nnumber of documents for all instances, whereas\nour work focuses on adaptively retrieving a varying\nnumber of documents for specific instances.\n2.3 Bayesian Sequential Analysis\nSequential analysis (Wald, 1947) grounded in de-\ncision theory and Bayesian inference (Gunst and\nShcherbakova, 2008; Box and Tiao, 1973), pro-\nvides a valuable framework for making informed\ndecisions based on accumulating evidence. The\ngoal is to select decisions that minimize the ex-\npected loss, considering the associated costs. The\ngeneral approach to Bayesian sequential decision\nproblems involves backward induction (DeGroot,\n2005; Berger, 2013), also known as stochastic dy-\nnamic programming (Ross, 2014). However, as the\ncomplexity of the problem increases, this method\ncan become computationally intensive. To address\nthis issue, Cardillo and Fu (1968) proposed an ap-\nproximation by assuming that the subsequent stage\nis terminal, leading to a suboptimal solution. Ex-\npanding on the backward induction method, Brock-\nwell and Kadane (2003) introduced a grid approx-\nimation approach. This technique estimates the\nexpected loss at each decision time, significantly\nreducing computation time to a linear complexity\nrelative to the number of decision stages.\nIn this paper, we utilize Bayesian sequential anal-\nysis to implement a stop-or-continue strategy for\nretrieving external documents. We treat the re-\ntrieved documents as a sequence and dynamically\ndetermine whether to continue or stop the retrieval\nprocess based on the accumulated evidence. This\nstop-or-continue strategy allows for efficient and\neffective information retrieval, optimizing the pro-\ncess by adapting to the evolving evidence and min-\nimizing unnecessary retrieval attempts.\n3 Method\nIn this paper, we propose a framework for detect-\ning hallucinations of LLMs by leveraging external\nevidence.\nGiven a claim generated by the LLMs, we first\ndecompose it into subclaims that contain basic\nknowledge and proceed to evaluate the veracity\nof each subclaim individually.\nSubsequently, we employ a search engine to re-\ntrieve web documents for each subclaim as external\nevidence. Rather than predefining the number of\ndocuments to retrieve, we adopt a dynamic retrieval\nprocess that involves retrieving one document at\none step and determining the need for additional\nretrieval based on the content of the retrieved docu-\nment.\nThen, we utilize a specific classification model\nto compute the entailment score between each doc-\nument and subclaim which represents the extent to\nhow well the evidence supports a given claim. The\nentailment score is processed into features, and a\nNaive Bayes classifier (NBC) is applied to calcu-\nlate the probability of the subclaim’s veracity based\non these features.\nNext, we adopt a stop-or-continue strategy, em-\nploying Bayesian risk decision to choose among\nthree options: (1) stopping the retrieval process and\nclassifying the subclaim as faculty, (2) stopping the\nretrieval process and classifying the subclaim as\na hallucination, (3) or continuing to retrieve addi-\ntional documents and make a choice in the next\nstep. After making the decision to stop the retrieval\nprocess and determine the veracity or reaching the\nmaximum retrieval times, we obtain the probability\nthat the subclaim is faculty.\nFinally, by aggregating the probabilities of the\nsubclaims’ veracity, we evaluate the overall verac-\nity of the original claim and assess its potential as\na hallucination.\nIn the following subsections, we discuss in more\ndetail our method. In Section 3.1, we demonstrate\nthe process of claim decomposition. In Section\n15363\n3.2, we describe details of using a search engine to\nretrieve web documents. Section 3.3 explains how\nthe entailment score is calculated and processed\ninto features required by the Naive Bayes Classifier\n(NBC). In Section 3.4, we provide a specific de-\nscription of the stop-or-continue strategy’s details.\nFinally, in Section 3.5, we elaborate on how the\nveracity probabilities of subclaims are aggregated.\n3.1 Claim Decomposition\nGiven a claim generated by LLMs, the direct ve-\nracity validation of the entire claim becomes chal-\nlenging due to the presence of multiple knowledge\ninstances within the generated content. So we first\ndecompose the original claim into subclaims, with\neach subclaim specifically focusing on a single\nentity and its associated attributes. Kamoi et al.\n(2023) show that similar method allows for a more\nfine-grained analysis, enabling us to examine and\nevaluate each entity and its attributes individually.\nSimultaneously, we replace pronouns in sentences\nwith their corresponding entities. This approach is\nimplemented to enhance the retrieval of relevant\nevidence from web documents, making the search\nprocess more effective.\nWe employ a zero-shot prompt-based approach\nusing the GPT-3.5 (text-davinci-003) model to\nperform claim decomposition and the full prompt\ncan be seen in Appendix A. For sentences that\nhave not been successfully decomposed, we solely\nperform entity replacements for pronouns.\n3.2 External Documents Retrieval\nTo validate the real-world efficacy of our frame-\nwork, we utilize the subclaims derived from the\nprevious decomposition step to retrieve external\ndocuments from search engines. We input the sub-\nclaims as queries into the Bing Search API 2 to\nobtain URLs. We then utilize the newspaper3k3 to\nextract web page content from the retrieved URLs.\nNote that the external documents are treated as a\nfinite sequence with maximum length K and are\nretrieved one by one. Therefore, we don’t pre-\ndetermine a fixed number of retrieval times. We\ndisregard web content that is inaccessible, such as\nPDF files or protected websites.\n2http://www.microsoft.com/en-us/bing/apis/\nbing-web-search-api\n3https://github.com/codelucas/newspaper\nAlgorithm 1:Hallucination detection\nInput: C : A claim generated by LLMs for\nhallucination detection;\nπ1(0) : Initial probability of Cbeing factual;\nK : Maximum retrieval times;\nCFA : Cost of false alarm;\nCM : Cost of miss;\nCretrieve: Cost of retrieving an document;\nOutput: Pfactual (C): Probability of Cbeing\nfactual;\n1 {C1,C2,··· ,CL}← ClaimDecompose(C);\n2 for i←1 to Ldo\n3 n←1 ;\n4 while n≤kdo\n5 En ←RetrieveDocument(Ci);\n6 fn ←CalEntailmentFeature(En, Ci);\n7 π1(n) ←NBC(π1(n−1),fn);\n8 Rstop(n) ←min((1 −π1(n))CM ,\n(1 −π0(n))CFA );\n9 Rcontinue(n) ←\nCretrieve + Efn+1 (Rstop(n+ 1));\n10 if Rstop(n) <Rcontinue(n) then\n11 break;\n12 else\n13 n←n+ 1;\n14 end\n15 end\n16 Pi\nfactual = π1(n);\n17 end\n18 Pfactual (C) = min\ni\nPfactual (Ci);\n19 Return: Pfactual (C)\n3.3 Entailment Score Calculation and\nDiscretization\nWe utilize a specific classification model to calcu-\nlate the entailment score between each document\nand subclaim which represents the extent to how\nwell the evidence supports the given subclaim. A\nhigher entailment score indicates stronger support\nfrom the external document for the corresponding\nsubclaim. In this study, we use the DeBERTa-v3\n(He et al., 2021) model fine-tuned on Natural Lan-\nguage Inference (NLI) dataset (Laurer et al., 2022)\nto perform a two-label classification (entailment\nand not entailment). Let Crefer to a subclaim and\nE refer to a retrieved document, the entailment\nscore s(C,E) is the probability of the entailment\nlabel obtained by inputting the claim and evidence\ninto DeBERTa-v3. Therefore, s(C,E) ∈(0,1)\nand s(C,E) →1 if the evidence completely sup-\nport the claim.\nTo overcome the limitation of the model’s input\nlength, we divide the document Einto text spans\n{E1,E2,··· ,El}. Each text span comprises m\ntokens, with a step size of n(n<m ) tokens. This\nmeans that we divide the document into text spans\nof length m tokens and move the segmentation\n15364\nwindow by ntokens for each step. We calculate\nthe entailment score between each text span and\nthe corresponding subclaim, and select the highest\nentailment score as the entailment score for the\noriginal document:\ns(C,E) = max\ni\n(s(C,Ei)) (1)\nThis approach ensures that the document’s over-\nall entailment score reflects the strongest evidence\nfound within it.\nScore Discretization\nThe entailment score is indeed a continuous value\nranging between 0 and 1. To simplify the repre-\nsentation of the document characteristics, we trans-\nform the entailment score into a discrete entailment\nfeature based on the assumption that two docu-\nments with similar entailment scores possess the\nsame feature:\nf(C,E) =⌊10 ·s(C,E)⌋ (2)\nwhere sis the continuous value of entailment score\nand f ∈{0,1,2,··· ,8,9}is the discrete entail-\nment feature given claim C and evidence E. The\nentailment feature is then input into a Naive Bayes\nClassifier (NBC) to calculate the veracity of the\nclaim in Equation 4.\n3.4 Stop-or-Continue Retrieval Strategy\nGiven a subcliam Cthat needs to be evaluated for\nveracity, and a finite sequence of retrieved external\ndocuments {E1,E2,··· ,EK}, we use the entail-\nment features of each document to access the verac-\nity θof C. We denote θ= θ0 to represent that Cis\na hallucination and θ= θ1 if Cis factual. At time\nn<K , we use π1(n) to represent the probability\nof Cbeing factual, given features f1,f2,··· ,fn:\nπ1(n) =P(θ= θ1|f1:n) (3)\nwhere the f1:n is the entailment features of E1:n\nand Ccalculated using Equation 2.\nAt step n+1, we use π1(n) as the prior probabil-\nity and calculate the posterior probabilityπ1(n+1)\nbased on π1(n) and fn+1:\nπ1(n+ 1) = π1(n)P(fn+1|θ1)\n(1 −π1(n))P(fn+1|θ0) + π1(n)P(fn+1|θ1)\n(4)\nwhich comes by recursively applying Bayes’ rule\n(Appendix B). This is the probability that Cis fac-\ntual, given features f1,f2,··· ,fn,fn+1, with an\nassumption of independence between the features.\nIn our approach, we assume that the content gen-\nerated by the large language model (LLM) has an\nequal prior probability of being hallucinations or\nfactual information and set π1(0) = 0.5.\nOne of the difficulties in calculating the\nabove probability is the conditional probability\nP(fn+1|θ1) and P(fn+1|θ0). We use a sampling\nmethod to compute this term required in Equation 4.\nWe sampled a set that consists of sfactual claims\nand s nonfactual claims. For each claim, we re-\ntrieved a piece of external evidence and computed\nthe entailment features of the claim and evidence.\nThen we estimate P(fn+1|θ1) and P(fn+1|θ0) by\ncounting the factual and nonfactual claims (plus a\nsmoothing term) that have the entailment feature\nequal to fn+1.\nBayesian Sequential Analysis\nFor a subcliam C, external document comes one by\none in sequence. At step n<K , we retrieve docu-\nments {E1,E2,··· ,En}and calculate the proba-\nbility that Cis factual using Equation 4. Then we\nhave three options:\n• Decide θ= θ1: We stop observing additional\nevidence and determine the subclaim Cis fac-\ntual.\n• Decide θ= θ0: We stop observing additional\nevidence and determine the subclaim C is a\nhallucination.\n• Keep test: The previous evidence is not\nenough to evaluate the veracity of the sub-\nclaim Cand decide to retrieve additional doc-\numents.\nIf we choose the first two options, we stop observ-\ning additional external evidence. However, if we\nchoose to continue with “keep test”, we retrieve an\nadditional document and make the choice among\nthe three options again at step n+ 1. We assume\nthat the document sequence is finite, and when the\nmaximum step Kis reached, we no longer choose\n“keep test”.\nWe make decisions based on the minimization\nof Bayesian risk. In this study, we consider three\npossible risk costs:\n• CFA : Cost of false alarm, we declare θ= θ0\nbut C is factual. In this condition, we incur\na cost when mistakenly classifying factual in-\nformation as hallucinations.\n15365\n• CM : Cost of miss, we declare θ = θ1 but C\nis a hallucination. In this condition, we incur\na cost when mistakenly classifying hallucina-\ntions as factual information.\n• Cretrieve: Cost of retrieve an external evi-\ndence. In this work, we assume that the cost of\nretrieving external evidence is equal for each\nretrieval.\nWe choose to stop only when the cost of stopping\nis less than the minimum expected cost of retrieving\none more document. If we stop and declare θ,\nthe cost due to “miss” would be π1(n)CM and\nthe cost due to “false alarm” would be π0(n)CFA .\nTherefore, if we make the decision to stop at step\nn, the expected risk Rstop(n) is:\nRstop(n) = min((1 −π1(n))CM ,(1 −π0(n))CFA ) (5)\nwhere Rstop(n) is the expected cost of choose the\nstop option. If the cost due to “miss” is smaller than\nthe cost due to “false alarm”, we declareθ= θ1 and\ndetermine that Cis factual. Otherwise, we declare\nθ= θ0 and determine that Cis a hallucination.\nIf we choose to retrieve an additional document\nand make a choice in the next step, because we do\nnot know the retrieved document En+1 at step n,\nso we calculate the minimum expected risk:\nRcontinue(n) = Cretrieve +\n9∑\nfn+1=0\nR(n+1)·P(fn+1) (6)\nwhere fn+1 is the entailment feature of En+1 and\nC. Rcontinue is the minimum expected risk to re-\ntrieve an additional document and make a choice in\nthe next step. Ris the overall minimum expected\ncost of the three options:\nR(n) =min(Rcontinue(n),Rstop(n)) (7)\nWe use this recursive equation to obtain the op-\ntimal solution. We stop and determine the prob-\nability of C being factual is π1(n) and evaluate\nthe veracity of Cif this action results in minimum\nexpected cost Rstop(n). Specifically, if Rstop(n)\nresults in the cost of miss, we consider C to be\nfactual. Otherwise, we consider C to be a hallu-\ncination. On the other hand, if the measurement\nof Rcontinue(n) yields the minimum expected cost,\nthe process continues and an additional document\nEn+1 is retrieved.\nTo obtain the optimal Bayesian decision in the fi-\nnite sequential setting at any step other than the\nlast, one must consider the probability of data\nvalues that have not yet been observed. This re-\nquires working the problem backward (from the\nmax step K back to time 0), a process referred\nto as backward induction (DeGroot, 2005; Berger,\n2013). However, computing Equation 6 is a bit\nawkward, as we need to consider all discrete values\nfor subsequent steps. The computation complexity\nthat implements the backward induction solution\nincreases exponentially in the number of backward\nsteps. Therefore, we use the truncation approxima-\ntion made one-step-ahead (Cardillo and Fu, 1968)\nwhich leads to the suboptimal solution. In each\nstep, the choice is made based on the assumption\nthat the next step is the final step, meaning that only\nthe first two options are considered. In this case,\nEquation 5 still hold, but R(n+ 1)is replaced by\nRstop(n+ 1)in Equation 6:\nRcontinue(n) = Cretrieve +\n9∑\nfn+1=0\nRstop(n+1) ·P(fn+1)\n(8)\nBy using this approach, the computational complex-\nity decreases from exponential to linear. Specifi-\ncally, it reduces from O(10K) to O(K).\n3.5 Aggregation\nFinally, we calculate the original claim’s probabil-\nity of factual by considering the minimum proba-\nbility of factual among the subclaims:\nPfactual (C) = min\ni\nPfactual (Ci) (9)\nWhere Pfactual is the probability of factual and\nthe original claim C is decomposed into L sub-\nclaims {C1,C2,··· ,CL}. This is based on the\nassumption that if any portion of the original claim\nis nonfactual, we consider the whole sentence to be\na hallucination.\n4 Experiments\nWe conducted experiments on the hallucination\ndetection classification task. First, we compared\nour framework with SelfCheckGPT to evaluate the\neffectiveness of our approach in hallucination de-\ntection at both the sentence-level and passage-level.\nNext, we performed a comparative experiment us-\ning a fixed number of retrieved documents as ex-\nternal evidence to validate the efficiency of our\nframework in improving retrieval efficiency and\nreducing the number of external evidence. Finally,\nwe conducted ablation study to evaluate the neces-\nsity of the claim decomposition step.\n15366\nMethod Sentence-level (AUC-PR) Passage-level (Corr.)\nEvidence Num Nonfact Factual Acc Pearson Spearman\nSelf-Detection - - - 31.01 - -\nw/ BERTScore 20 81.96 44.23 - 58.18 55.90\nSelfCheckGPT w/ QA 20 84.26 48.14 - 61.07 59.29\nw/Unigram (max) 20 85.63 58.47 - 64.71 64.91\nCombination 60 87.33 61.83 - 69.05 67.77\nOur Framework CM = 14, CFA = 24 3.05 82.42 57.01 80.24 71.37 64.55\nCM = 28, CFA = 96 6.22 86.45 61.96 82.39 81.18 74.20\nTable 1: Follow prior work (Manakul et al., 2023), we report the Area Under the Precision-Recall Curve (AUC-PR)\nat the sentence-level and the Pearson and Spearman’s rank correlation coefficient w.r.t human judgments at the\npassage-level. Additionally, we report the number of external evidence. For SelfCheckGPT, “Evidence Num”\nrepresents the number of samples obtained from GPT-3. For our framework, “Evidence Num” represents the average\nnumber of retrieved external documents. Furthermore, for our framework, we also report the sentence-level accuracy.\nBERTScore, QA and Unigram are three metrics to evaluate the consistency of samples in SelfCheckGPT.\n4.1 Experiment setup\nDataset We use the dataset of SelfCheckGPT\n(Manakul et al., 2023) which contains 1,908 sen-\ntences from 298 article generated by GPT-3. Each\nsentence is labeled with one of the three verac-\nity labels: “Accurate”, “Minor Inaccurate”and\n“Major Inaccurate”. In the experiments, “Ma-\njor Inaccurate” and “Minor Inaccurate” sentences\nare labeled as non-factual class, while sentences\nlabeled as “Accurate” are considered factual class.\nBaseline SelfCheckGPT samples from GPT-3\nmultiple times and utilizing consistency between\nsamples to identify hallucinations. We use Self-\nCheckGPT as the baseline to evaluate the perfor-\nmance of our framework for hallucination detection\nat sentence-level and passage-level. We also query\nChatGPT itself to determine whether it constitutes\nhallucination as an additional baseline(Mündler\net al., 2023).\nHyperparameters Settings For calculation of\nthe conditional probability of Equation 4, we used\ns= 200and a smoothing term of 1 in the experi-\nments reported here. In section 4.3, we adjust CFA\nand CM on our framework for hyperparameter ex-\nperiments. For the experiments conducted outside\nof that section, we set the value of the three possible\ncosts of Bayesian sequential analysis CFA = 24,\nCM = 14and Cretrieve = 1. For the length of text\nspans and the step size during document segmenta-\ntion, we set m= 400and n= 100. Additionally,\nwe set the maximum retrieval time K = 10.\nEvaluation Metrics Following SelfCheckGPT,\nwe report AUC-PR for sentence-level detection\ntasks. For passage-level detection, we calculate\nthe average score of all sentences in a passage to\nobtain the passage’s veracity score. Passage-level\nranking performances are measured by Pearson\ncorrelation coefficient and Spearman’s rank corre-\nlation coefficient w.r.t. human judgments.\nFor our framework, we also report average\nsearch time and accuracy (ACC) for sentence-level\ndetection tasks. To validate the advantage of our\nmethod in terms of sample cost, we report the num-\nber of external evidence. For SelfCheckGPT, we\nconsider the number of times they perform sam-\npling from GPT-3 as the quantity of external evi-\ndence. For our framework, we report the average\nnumber of retrieved documents at the sentence-\nlevel. To validate our advantage in terms of sam-\npling cost, we assume that SelfCheckGPT incurs\nthe same cost for one GPT-3 sample as it does for\nsearching an external document.\n4.2 Results of Hallucination Detection\nThe results of our experiments can be found in\nTable 1, which provides an overview of the per-\nformance of our framework in hallucination de-\ntection. When the average number of retrieved\ndocuments per sentence is 3.05, our framework out-\nperforms SelfCheckGPT with single consistency\nmetric, which rely on 20 external evidence, in terms\nof passage-level performance. As the average num-\nber of retrieved documents increased to 6.22, our\nframework shows slightly lower performance in\nNonfact detection but uses less external evidence.\nIn Factual detection and passage-level evaluation,\nwe achieve significant improvements over the base-\nline. These results demonstrate the effectiveness of\nour framework in hallucination detection.\n4.3 Effectiveness for Reducing Retrieval\nTimes\nIn this section, we conducted comparative experi-\nments using a fixed number of retrieved documents.\n15367\n1 2 3 4 5 6 7 8\nThe number of documents retrieved\n50\n55\n60\n65\n70\n75Spearman's RankCC\nNaive Bayes Classifier (NBC)\nAverage\nMaximum\nBayesian sequential analysis\nFigure 2: The performance of Bayesian sequential anal-\nysis method on ranking passages (Spearman’s) versus\nstrategies with fixed number of retrieval documents.\nFor the case of using a fixed number of retrieved\ndocuments, we considered three methods to aggre-\ngate the features of these documents:\n• Naive Bayes Classifier (NBC): We treated the\ndocuments as a fixed number of features and\nused a Naive Bayes classifier, as described in\nEq.4, for hallucination detection.\n• Average: For each document, we compute an\nentailment score and then average the scores\nof all the documents to obtain the veracity\nscore of the claim.\n• Maximum: We calculated an entailment score\nfor each document and selected the highest\nscore as the veracity score for the sentence.\nThis approach is based on the assumption that\nhallucinations usually lack any supporting ev-\nidence in external documents. Therefore, if\nthere are documents that can validate a sen-\ntence, we consider the sentence factual.\nThe CM and CFA hyperparameters represent\nthe costs involved in making Bayesian risk deci-\nsions. Different values of CM and CFA can lead\nto different decisions by the strategy, for the same\nπ1(n) and Cretrieve, thereby influencing the aver-\nage number of retrieved documents. To evaluate\nthe effectiveness of our framework in improving re-\ntrieval efficiency and reducing search frequency, we\nconduct comparative experiments using the fixed\nnumber of retireved documents. We randomly se-\nlect five sets of CM and CFA values and compare\nthe results with the fixed search times approach.\nThe results obtained from the experiments are\npresented in Figure 2, which illustrates the rela-\ntionship between the average number of retrieved\ndocuments in our framework and the Spearman’s\ncorrelation coefficient at the passage-level halluci-\nnation detection. Based on the results, we observe\nthat as the average number of retrieved documents\nin our framework approaches the fixed number of\nretrievals, there is a higher Spearman’s correlation\ncoefficient at the passage-level hallucination de-\ntection. This implies that our framework achieves\nbetter performance in hallucination detection with\nfewer retrievals compared to the fixed number of\nretrievals. These findings indicate that our frame-\nwork effectively reduces the number of retrievals re-\nquired while maintaining or even improving the per-\nformance of hallucination detection. This demon-\nstrates the efficacy of our approach in enhancing\nretrieval efficiency and reducing search frequency\nin the context of hallucination detection tasks.\nMethod Sentence-level (AUC-PR)\nNonfact Factual Acc\nw/o Decomposition 80.04 53.71 79.19\nw Decomposition 82.42 57.01 80.24\nTable 2: The performance at the sentence-level by\ndirectly using the original sentence for hallucination\ndetection.\n4.4 The Effect of Claim Decomposition\nIn this section, we validate the contribution of\nthe claim decomposition step to the overall per-\nformance. We consider the scenario where only the\ndecomposed subclaims are used for document re-\ntrieval and directly perform hallucination detection\non the original sentence.\nThe results in Table 2 demonstrate that when\nsubclaims are only used for document retrieval, the\nperformance of the framework decreases to some\nextent at both the sentence level and passage level.\nThis indicates that directly determining the verac-\nity of a sentence that contains multiple pieces of\nknowledge is challenging. It highlights the neces-\nsity of the problem decomposition step.\n5 Conclusions\nIn this study, we propose a framework that utilizes\nBayesian sequential analysis for detecting hallu-\ncinations in large language models (LLMs). We\nconsider the retrieved documents as a sequence\nand employ a stop-or-continue strategy to make in-\n15368\nformed decisions on whether to retrieve additional\ninformation for a given case. By applying Bayesian\nsequential analysis, our framework achieves com-\npetitive results on annotated GPT-3 responses, sur-\npassing sample-based methods and retrieval strate-\ngies with a fixed number of search documents. Ad-\nditionally, we validate the practical applicability of\nour framework by conducting searches for relevant\ndocuments on the web, demonstrating the feasi-\nbility of our approach in corroborating the claims\nmade by LLMs with real-world information.\nLimitations\nIn this study, the implemented strategy amalga-\nmates individual pieces of information extracted\nfrom a search engine at each iterative stage. An\ninherent limitation of this methodology is the po-\ntentially extended duration required for inference.\nFor future studies, it would be better to consider\nthe integration of information drawn from multiple\ndocuments concurrently, which could significantly\nenhance the speed of the inference process.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for their valuable comments. This work\nwas supported by National Natural Science Foun-\ndation of China (No. 62076068), and Shanghai\nMunicipal Science and Technology Project (No.\n21511102800).\nReferences\nKenneth J Arrow, David Blackwell, and Meyer A Gir-\nshick. 1949. Bayes and minimax solutions of sequen-\ntial decision problems. Econometrica, Journal of the\nEconometric Society, pages 213–244.\nIsabelle Augenstein, Christina Lioma, Dongsheng\nWang, Lucas Chaves Lima, Casper Hansen, Christian\nHansen, and Jakob Grue Simonsen. 2019. Multifc: A\nreal-world multi-domain dataset for evidence-based\nfact checking of claims. In Conference on Empirical\nMethods in Natural Language Processing.\nAmos Azaria and Tom M. Mitchell. 2023. The inter-\nnal state of an llm knows when its lying. ArXiv,\nabs/2304.13734.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. ArXiv, abs/2302.04023.\nJames O Berger. 2013. Statistical decision theory and\nBayesian analysis. Springer Science & Business\nMedia.\nG. E. P. Box and George C. Tiao. 1973. Bayesian infer-\nence in statistical analysis. International Statistical\nReview, 43:242.\nAnthony E Brockwell and Joseph B Kadane. 2003. A\ngridding method for bayesian sequential decision\nproblems. Journal of Computational and Graphi-\ncal Statistics, 12(3):566–584.\nGerald P Cardillo and King-Sun Fu. 1968. On subop-\ntimal sequential pattern recognition. IEEE Transac-\ntions on Computers, 100(8):789–792.\nBradley P Carlin, Joseph B Kadane, and Alan E Gelfand.\n1998. Approaches for optimal sequential decision\nanalysis in clinical trials. Biometrics, pages 964–975.\nJifan Chen, Grace Kim, Aniruddh Sriram, Greg Dur-\nrett, and Eunsol Choi. 2023. Complex claim veri-\nfication with evidence retrieved in the wild. ArXiv,\nabs/2305.11859.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. ArXiv, abs/2204.02311.\nWenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale\nFung. 2022. Plausible may not be faithful: Probing\nobject hallucination in vision-language pre-training.\nIn Conference of the European Chapter of the Asso-\nciation for Computational Linguistics.\nMorris H DeGroot. 2005. Optimal statistical decisions.\nJohn Wiley & Sons.\nM. C. M. Gunst and O. Shcherbakova. 2008. Asymp-\ntotic behavior of bayes estimators for hidden markov\nmodels with application to ion channels. Mathemati-\ncal Methods of Statistics, 17:342–356.\n15369\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai,\nAndrea Madotto, and Pascale Fung. 2022. Survey of\nhallucination in natural language generation. ACM\nComputing Surveys, 55:1 – 38.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez,\nand Greg Durrett. 2023. Wice: Real-world en-\ntailment for claims in wikipedia. arXiv preprint\narXiv:2303.01432.\nJiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James\nThorne, and Edward Choi. 2023. Factkg: Fact veri-\nfication via reasoning on knowledge graphs. ArXiv,\nabs/2305.06590.\nMoritz Laurer, W v Atteveldt, Andreu Casas, and\nKasper Welbers. 2022. Less annotating, more\nclassifying–addressing the data scarcity issue of su-\npervised machine learning with deep transfer learning\nand bert-nli.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2022.\nFactuality enhanced language models for open-ended\ntext generation. ArXiv, abs/2206.04624.\nPotsawee Manakul, Adian Liusie, and Mark John Fran-\ncis Gales. 2023. Selfcheckgpt: Zero-resource black-\nbox hallucination detection for generative large lan-\nguage models. ArXiv, abs/2303.08896.\nNiels Mündler, Jingxuan He, Slobodan Jenko, and Mar-\ntin T. Vechev. 2023. Self-contradictory hallucinations\nof large language models: Evaluation, detection and\nmitigation. ArXiv, abs/2305.15852.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics. As-\nsociation for Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nSheldon M Ross. 2014. Introduction to stochastic dy-\nnamic programming. Academic press.\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read before\ngenerate! faithful long form question answering with\nmachine reading. ArXiv, abs/2203.00343.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nverification. ArXiv, abs/1803.05355.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nAbraham Wald. 1947. Foundations of a general the-\nory of sequential decision functions. Econometrica,\nJournal of the Econometric Society, pages 279–313.\nGB Wetherill. 1961. Bayesian sequential analysis.\nBiometrika, 48(3/4):281–292.\nA Claim Decomposition Prompts\nThe zero-shot prompt we use to decompose the\nclaim is shown in Figure 3.\nFollow these steps to process the text in the triple delimiter:\nStep 1: Rewrite the text while use the original names of entities instead of pronouns when referring to \nthem.\nStep 2: Decompose the text into basic knowledge, requiring each piece of knowledge to contain only \none entity and one attribute of that entity and one value of the attribute.\nStep 3: Express each decomposed piece of knowledge in a natural language sentence.\nStep 4: Output which sentence in step three was obtained from the original text.\nOutput the result in JSON format, the key of JSON is 'step1', 'step2', 'step3’ and 'step4', the value of \nstep3 should be a list of sentences and the value of step4 should be pairs of sentence in step3 and \noriginal sentence in the original text and the index of the sentence in the original text.\n```{ passage }```\nOutput:\nFigure 3: The zero-shot prompt for decompose the sen-\ntence of the passage into subclaims.\nB Derivation of Equation 4\nGiven a subcliam Cthat needs to be evaluated for\nveracity, and a finite sequence of retrieved external\ndocuments {E1,E2,··· ,EK}, we use the entail-\nment features of each document to access the verac-\nity θof C. We denote θ= θ0 to represent that Cis\na hallucination and θ= θ1 if Cis factual. At time\nn<K , we use π1(n) to represent the probability\nof Cbeing factual, given features f1,f2,··· ,fn:\nπ1(n) =P(θ= θ1|f1:n) (10)\n15370\nwhere the f1:n is the entailment features of E1:n\nand Ccalculated using Equation 2.\nπ1(n+ 1) (11)\n= P(θ= θ1|f1:n+1) (12)\n= P(θ1,f1:n+1)\nP(f1:n+1) (13)\n= P(θ1)P(f1:n+1|θ1)\nP(f1:n+1|θ1)P(θ1) + P(f1:n+1|θ0)P(θ0) (14)\n= P(θ1)Πn+1\ni=1 P(fi|θ1)\nP(θ1)Πn+1\ni=1 P(fi|θ1) + P(θ0)Πn+1\ni=1 P(fi|θ0) (15)\n= P(fn+1|θ1)P(f1:n,θ1)\nP(fn+1|θ1)P(f1:n,θ1) + P(fn+1|θ0)P(f1:n,θ0)\n(16)\n= P(fn+1|θ1)P(θ1|f1:n)\nP(fn+1|θ1)P(θ1|f1:n) + P(fn+1|θ0)P(θ0|f1:n)\n(17)\n= π1(n)P(fn+1|θ1)\n(1 −π1(n))P(fn+1|θ0) + π1(n)P(fn+1|θ1) (18)\nDeriving Equation 15 from Equation 14 based on\nthe assumption of independence between the fea-\ntures.\nC Self-Detection Prompts\nFor querying ChatGPT itself to see whether it’s\nhallucination, we use the prompt shown in Figure\n4.\nI give you one statement, please conclude whether the statement \nis nonfactual with Yes or No.\nStatement: {Hypothesis}\nFigure 4: The zero-shot prompt for querying ChatGPT\nitself to see whether it’s hallucination.\n15371",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6960091590881348
    },
    {
      "name": "Bayesian probability",
      "score": 0.6252477169036865
    },
    {
      "name": "Machine learning",
      "score": 0.5423383116722107
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5339685082435608
    },
    {
      "name": "Generative grammar",
      "score": 0.5001494884490967
    },
    {
      "name": "Bayes' theorem",
      "score": 0.4991908073425293
    },
    {
      "name": "Generative model",
      "score": 0.4686927795410156
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4100872874259949
    },
    {
      "name": "Natural language processing",
      "score": 0.3241957426071167
    },
    {
      "name": "Mathematics",
      "score": 0.10969159007072449
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}