{
    "title": "GraphFormers: GNN-nested Language Models for Linked Text Representation.",
    "url": "https://openalex.org/W3157746834",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5054790743",
            "name": "Junhan Yang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100423656",
            "name": "Zheng Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5044147794",
            "name": "Shitao Xiao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5037831162",
            "name": "Chaozhuo Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100932403",
            "name": "Guangzhong Sun",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5044651577",
            "name": "Xing Xie",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2945623882",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W2131876387",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2971455676",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3100848837",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W3016482365",
        "https://openalex.org/W2971193649",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W3098400973",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2296073425",
        "https://openalex.org/W2894175828",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2949547296",
        "https://openalex.org/W2963460103",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W3122775348",
        "https://openalex.org/W2242161203",
        "https://openalex.org/W3152893301",
        "https://openalex.org/W2976561662",
        "https://openalex.org/W3154091824",
        "https://openalex.org/W3157563153",
        "https://openalex.org/W3154229486",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2662348822",
        "https://openalex.org/W3039695075",
        "https://openalex.org/W2970096436",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2950336186",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3021282678",
        "https://openalex.org/W3015883388"
    ],
    "abstract": "Linked text representation is critical for many intelligent web applications, such as online advertisement and recommender systems. Recent breakthroughs on pretrained language models and graph neural networks facilitate the development of corresponding techniques. However, the existing works mainly rely on cascaded model structures: the texts are independently encoded by language models at first, and the textual embeddings are further aggregated by graph neural networks. We argue that the neighbourhood information is insufficiently utilized within the above process, which restricts the representation quality. In this work, we propose GraphFormers, where graph neural networks are nested alongside each transformer layer of the language models. On top of the above architecture, the linked texts will iteratively extract neighbourhood information for the enhancement of their own semantics. Such an iterative workflow gives rise to more effective utilization of neighbourhood information, which contributes to the representation quality. We further introduce an adaptation called unidirectional GraphFormers, which is much more efficient and comparably effective; and we leverage a pretraining strategy called the neighbourhood-aware masked language modeling to enhance the training effect. We perform extensive experiment studies with three large-scale linked text datasets, whose results verify the effectiveness of our proposed methods.",
    "full_text": null
}