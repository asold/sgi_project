{
  "title": "Offline Pre-trained Multi-agent Decision Transformer",
  "url": "https://openalex.org/W4362590704",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2151736631",
      "name": "Linghui Meng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Shandong Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A3198045821",
      "name": "Muning Wen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3196768618",
      "name": "Chenyang Le",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2150510018",
      "name": "Xiyun Li",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2166055967",
      "name": "Dengpeng Xing",
      "affiliations": [
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2119756314",
      "name": "Weinan Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2099142219",
      "name": "Ying Wen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2127103437",
      "name": "Haifeng Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A754385464",
      "name": "Jun Wang",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2113349909",
      "name": "Yaodong Yang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2074677540",
      "name": "Bo Xu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2151736631",
      "name": "Linghui Meng",
      "affiliations": [
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A3198045821",
      "name": "Muning Wen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3196768618",
      "name": "Chenyang Le",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2150510018",
      "name": "Xiyun Li",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2166055967",
      "name": "Dengpeng Xing",
      "affiliations": [
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2119756314",
      "name": "Weinan Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2099142219",
      "name": "Ying Wen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2127103437",
      "name": "Haifeng Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shandong Institute of Automation",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A754385464",
      "name": "Jun Wang",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A2113349909",
      "name": "Yaodong Yang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2074677540",
      "name": "Bo Xu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Shandong Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3107615218",
    "https://openalex.org/W2530849036",
    "https://openalex.org/W3093963693",
    "https://openalex.org/W6767785798",
    "https://openalex.org/W2914154006",
    "https://openalex.org/W6747941106",
    "https://openalex.org/W2803005587",
    "https://openalex.org/W2756196406",
    "https://openalex.org/W3196792970",
    "https://openalex.org/W3198396204",
    "https://openalex.org/W2904246096",
    "https://openalex.org/W2411690432",
    "https://openalex.org/W2981187905",
    "https://openalex.org/W2786036274",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2886647275",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W3193711978",
    "https://openalex.org/W3169871130",
    "https://openalex.org/W3171362638",
    "https://openalex.org/W3022566517",
    "https://openalex.org/W3208313084",
    "https://openalex.org/W3197189015",
    "https://openalex.org/W6758846586",
    "https://openalex.org/W3016943563",
    "https://openalex.org/W3136237689",
    "https://openalex.org/W6772005887",
    "https://openalex.org/W2947150733",
    "https://openalex.org/W2904453761",
    "https://openalex.org/W3034084488",
    "https://openalex.org/W3130091361",
    "https://openalex.org/W3174153107",
    "https://openalex.org/W3189584127",
    "https://openalex.org/W3213789840",
    "https://openalex.org/W6791661883",
    "https://openalex.org/W3138460475",
    "https://openalex.org/W6779109570",
    "https://openalex.org/W2794643322",
    "https://openalex.org/W2911743772",
    "https://openalex.org/W3035478219",
    "https://openalex.org/W3123636359",
    "https://openalex.org/W2946606218",
    "https://openalex.org/W6800682094",
    "https://openalex.org/W3200561352",
    "https://openalex.org/W6802126314",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6799166919",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W4286748781",
    "https://openalex.org/W3016525976",
    "https://openalex.org/W3085267010",
    "https://openalex.org/W2539402368",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3203079382",
    "https://openalex.org/W4287126489",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W4301501993",
    "https://openalex.org/W4383112908",
    "https://openalex.org/W3032916997",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W4387171915",
    "https://openalex.org/W3204192621",
    "https://openalex.org/W2622408375",
    "https://openalex.org/W2996037775"
  ],
  "abstract": "Abstract Offline reinforcement learning leverages previously collected offline datasets to learn optimal policies with no necessity to access the real environment. Such a paradigm is also desirable for multi-agent reinforcement learning (MARL) tasks, given the combinatorially increased interactions among agents and with the environment. However, in MARL, the paradigm of offline pre-training with online fine-tuning has not been studied, nor even datasets or benchmarks for offline MARL research are available. In this paper, we facilitate the research by providing large-scale datasets and using them to examine the usage of the decision transformer in the context of MARL. We investigate the generalization of MARL offline pre-training in the following three aspects: 1) between single agents and multiple agents, 2) from offline pretraining to online fine tuning, and 3) to that of multiple downstream tasks with few-shot and zero-shot capabilities. We start by introducing the first offline MARL dataset with diverse quality levels based on the StarCraftII environment, and then propose the novel architecture of multi-agent decision transformer (MADT) for effective offline learning. MADT leverages the transformer’s modelling ability for sequence modelling and integrates it seamlessly with both offline and online MARL tasks. A significant benefit of MADT is that it learns generalizable policies that can transfer between different types of agents under different task scenarios. On the StarCraft II offline dataset, MADT outperforms the state-of-the-art offline reinforcement learning (RL) baselines, including BCQ and CQL. When applied to online tasks, the pre-trained MADT significantly improves sample efficiency and enjoys strong performance in both few-short and zero-shot cases. To the best of our knowledge, this is the first work that studies and demonstrates the effectiveness of offline pre-trained models in terms of sample efficiency and generalizability enhancements for MARL.",
  "full_text": " \nOffline Pre-trained Multi-agent Decision Transformer\nLinghui Meng 1,2*          Muning Wen 3*          Chenyang Le 3          Xiyun Li 1,4\nDengpeng Xing 1,2          Weinan Zhang 3          Ying Wen 3          Haifeng Zhang 1,2\nJun Wang 6          Yaodong Yang 5          Bo Xu 1,2\n1 Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China\n2 School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China\n3 Shanghai Jiao Tong University, Shanghai 200240, China\n4 School of Future Technology, University of Chinese Academy of Sciences, Beijing 100049, China\n5 Institute for AI, Peking University, Beijing 100871, China\n6 Department of Computer Science, University College London, London WC1E 6BT, UK\n \nAbstract:   Offline reinforcement learning leverages previously collected offline datasets to learn optimal policies with no necessity to\naccess the real environment. Such a paradigm is also desirable for multi-agent reinforcement learning (MARL) tasks, given the combin-\natorially increased interactions among agents and with the environment. However, in MARL, the paradigm of offline pre-training with\nonline fine-tuning has not been studied, nor even datasets or benchmarks for offline MARL research are available. In this paper, we facil-\nitate the research by providing large-scale datasets and using them to examine the usage of the decision transformer in the context of\nMARL. We investigate the generalization of MARL offline pre-training in the following three aspects: 1) between single agents and mul-\ntiple agents, 2) from offline pretraining to online fine tuning, and 3) to that of multiple downstream tasks with few-shot and zero-shot\ncapabilities. We start by introducing the first offline MARL dataset with diverse quality levels based on the StarCraftII environment,\nand then propose the novel architecture of multi-agent decision transformer (MADT) for effective offline learning. MADT leverages the\ntransformer′s modelling ability for sequence modelling and integrates it seamlessly with both offline and online MARL tasks. A signific-\nant benefit of MADT is that it learns generalizable policies that can transfer between different types of agents under different task scen-\narios. On the StarCraft II offline dataset, MADT outperforms the state-of-the-art offline reinforcement learning (RL) baselines, includ-\ning BCQ and CQL. When applied to online tasks, the pre-trained MADT significantly improves sample efficiency and enjoys strong per-\nformance in both few-short and zero-shot cases. To the best of our knowledge, this is the first work that studies and demonstrates the ef-\nfectiveness of offline pre-trained models in terms of sample efficiency and generalizability enhancements for MARL.\nKeywords:    Pre-training model, multi-agent reinforcement learning (MARL), decision making, transformer, offline reinforcement\nlearning.\nCitation:   L. Meng, M. Wen, C. Le, X. Li, D. Xing, W. Zhang, Y. Wen, H. Zhang, J. Wang, Y. Yang, B. Xu. Offline pre-trained multi-\nagent decision transformer. Machine Intelligence Research, vol.20, no.2, pp.233–248, 2023. http://doi.org/10.1007/s11633-022-1383-7\n \n 1   Introduction\nMulti-agent  reinforcement  learning  (MARL)  al-\ngorithms[1] play  an  essential  role  in  solving  complex  de-\ncision-making tasks by learning from the interaction data\nbetween  computerised  agents  and  (simulated)  physical\nenvironments.  It  has  been  typically  applied  to  self-driv-\ning[2−4],  order  dispatching[5, 6],  modelling  population  dy-\nnamics[7],  and  gaming  AIs[8, 9].  However,  the  scheme  of\nlearning  policy  from  experience  requires  the  algorithms\nwith  high  computational  complexity[10] and  sample  effi-\nciency due to the limited computing resources and high\ncost resulting from the data collection[11−14]. Furthermore,\neven in domains where the online environment is feasible,\nwe  might  still  prefer  to  utilize  previously-collected  data\ninstead; for example, if the domain′s complexity requires\nlarge  datasets  for  effective  generalization.  In  addition,  a\npolicy  trained  on  one  scenario  usually  cannot  perform\nwell on another even under the same task. Therefore, a\nuniversal policy is critical for saving the training time of\ngeneral reinforcement learning (RL) tasks.\nNotably, the recent advance of supervised learning has\nshown that the effectiveness of learning methods can be\nmaximized when they are provided with very large mod-\nelling  capacity,  trained  on  very  large  and  diverse\ndatasets[15−17]. The surprising effectiveness of large, gener-\nic  models  supplied  with  large  amounts  of  training  data,\nsuch  as  GPT-3[18],  spurs  the  community  to  search  for\nways  to  scale  up  thus  boosting  the  performance  of  RL\nmodels. Towards this end, Decision transformer[19] is one\n \nResearch Article\nSpecial Issue on Large-scale Pre-training: Data, Models, and Fine-\ntuning\nManuscript received July 6, 2022; accepted October 18, 2022\nRecommended by Associate Editor Zhi-Yuan Liu\n \n*These authors contribute equally to this work\n© The Author(s) 2023\n \nMachine Intelligence Research\nwww.mi-research.net\n20(2), \nApril 2023, 233-248\nDOI:\n 10.1007/s11633-022-1383-7\n \n\nof the first models that verifies the possibility of solving\nconventional (offline) RL problems by generative traject-\nory modelling, i.e., modelling the joint distribution of the\nsequence of states, actions, and rewards without tempor-\nal difference learning.\nThe  technique  of  transforming  decision-making  prob-\nlems into sequence modelling problems has opened a new\ngate for solving RL tasks. Crucially, this activates a nov-\nel pathway toward training RL systems on diverse data-\nsets[20−22] in  much  the  same  manner  as  in  supervised\nlearning, which is often instantiated by offline RL tech-\nniques[23]. Offline RL methods have recently attracted tre-\nmendous attention since they enable agents to apply self-\nsupervised or unsupervised RL methods in settings where\nonline collection is infeasible. We, thus, argue that this is\nparticularly  important  for  MARL  problems  since  online\nexploration in multi-agent settings may not be feasible in\nmany settings[24], but learning with unsupervised or meta-\nlearned[25] outcome-driven  objectives  via  offline  data  is\nstill possible. However, it is unclear yet whether the ef-\nfectiveness of sequence modelling through transformer ar-\nchitecture also applies to MARL problems.\nIn this paper, we propose multi-agent decision trans-\nformers (MADT), an architecture that casts the problem\nof  MARL  as  conditional  sequence  modelling.  Our  man-\ndate  is  to  understand  if  the  proposed  MADT  can  learn\nthrough pre-training a generalized policy on offline data-\nsets,  which  can  then  be  effectively  used  to  other  down-\nstream  environments  (known  or  unknown).  As  a  study\nexample,  we  specifically  focus  on  the  well-known  chal-\nlenge  for  MARL  tasks:  the  StarCraft  multi-agent  chal-\nlenge (SMAC)[26], and demonstrate the possibility of solv-\ning  multiple  SMAC  tasks  with  one  big  sequence  model.\nOur  contribution  is  as  follows:  We  propose  a  series  of\ntransformer variants for offline MARL by leveraging the\nsequential modelling of the attention mechanism. In par-\nticular,  we  validate  our  pre-trained  sequential  model  in\nthe challenging multi-agent environment for its sample ef-\nficiency and transferability. We built a dataset with dif-\nferent  skill  levels  covering  different  variations  of  SMAC\nscenarios. Experimental results on SMAC tasks show that\nMADT enjoys fast adaptation and superior performance\nvia learning one big sequence model.\nThe main challenges in our offline pre-training and on-\nline fine-tuning problems are the out-of-distribution and\ntraining  paradigm  mismatch  problems.  We  tackle  these\ntwo problems with the sequential model and pre-train the\nglobal critic model offline.\n 2   Related work\nOffline  deep  reinforcement  learning. Recent\nworks  have  successfully  applied  RL  in  robotics  con-\ntrol[27, 28] and gaming AIs[29] online. However, many works\nattempt to reduce the cost resulting from online interac-\ntions  by  learning  with  neural  networks  from  an  offline\ndataset  named  offline  RL  methods[23].  There  are  two\nclasses to divide the offline RL methods: constraint-based\nand sequential model-based methods. For the constraint-\nbased methods, a straightforward method is to adopt the\noff-policy algorithm and regard the offline datasets as a\nreplay  buffer  to  learn  a  policy  with  promising  perform-\nance. However, experience existing in offline datasets and\ninteraction with online environments have different distri-\nbutions, which causes the overestimation in the off-policy\n(value-based)  method[30].  Substantial  works  presented  in\noffline RL aim at resolving the distribution shift between\nthe static offline datasets and the online environment in-\nteractions[30−32].  In  addition,  depending  on  the  dynamic\nplanning  ability  of  the  transition  model,  Matsushima  et\nal.[33, 34] learn different models offline and regularize the\npolicy  efficiently.  In  particular,  Yang  et  al.[35, 36] con-\nstrain  off-policy  algorithms  in  the  multi-agent  field.  Re-\nlated  to  our  work  for  the  improvement  of  sample  effi-\nciency,  Nair  et  al.[37] derive  the  Karush  Kuhn-Tucker\n(KKT)  conditions  of  the  online  objective,  generating  an\nadvantage weight to avoid the out-of-distribution (OOD)\nproblem.  For  the  sequential  model-based  methods,  De-\ncision transformer outperforms many state-of-the-art off-\nline RL algorithms by regarding the offline policy train-\ning  process  as  a  sequential  modelling  and  testing  it\nonline[19, 38].  In  contrast,  we  show  a  transformer-based\nmethod  in  the  multi-agent  field,  attempting  to  transfer\nacross many scenarios without extra constraints. By shar-\ning  the  sequential  model  across  agents  and  learning  a\nglobal  critic  network  offline,  we  conduct  a  pre-trained\nmulti-agent  policy  that  can  be  continuously  fine-tuned\nonline.\nMulti-agent reinforcement learning. As a natur-\nal  extension  from  single-agent  RL,  MARL[1] attracts\nmuch  attention  to  solve  more  complex  problems  under\nMarkov games. Classic algorithms often assume multiple\nagents to interact with the environment online and col-\nlect the experience to train the joint policy from scratch.\nMany  empirical  successes  have  been  demonstrated  in\nsolving  zero-sum  games  through  MARL  methods[8, 39].\nWhen  solving  decentralized  partially  observable  Markov\ndecision processes (Dec-POMDPs) or potential games[40],\nthe  framework  of  centralized  training  and  decentralized\nexecution (CTDE) is often employed[41−45] where a cent-\nralized critic is trained to gather all agents′ local observa-\ntions  and  assign  credits.  While  CTDE  methods  rely  on\nthe  individual-global-max  assumption[46],  another  thread\nof work is built on the so-called advantage decomposition\nlemma[47],  which  holds  in  general  for  any  cooperative\ngame; such a lemma leads to provably convergent multi-\nagent trust-region methods[48] and constrained policy op-\ntimization methods[49].\nTransformer. Transformer[50] has  achieved  a  great\nbreakthrough  to  model  relations  between  the  input  and\noutput sequence with variable length, for the sequence-to-\nsequence problems[51], especially in machine translation[52]\nand speech recognition[53]. Recent works even reorganize\nthe  vision  problems  as  the  sequential  modelling  process\nand  construct  the  state-of-the-art  (SOTA)  model  with\npretraining,  named  vision  transformers  (ViT)[16, 54, 55].\n 234 Machine Intelligence Research 20(2), April 2023\n \n\nDue to the Markovian property of trajectories in offline\ndatasets, we can utilize Transformer as that in language\nmodelling.  Therefore,  Transformer  can  bridge  the  gap\nbetween supervised learning in the offline setting and re-\ninforcement learning in online interaction because of the\nrepresentation capability. We claim that the components\nin  Markov  games  are  sequential,  then  utilise  the  trans-\nformer for each agent to fit a transferable MARL policy.\nFurthermore,  we  fine-tune  the  learned  policy  via  trial-\nand-error.\n 3   Methodology\nIn this section, we demonstrate how the transformer is\napplied  to  our  offline  pre-training  MARL  framework.\nFirst,  we  introduce  the  typical  paradigm  and  computa-\ntion  process  for  the  multi-agent  reinforcement  learning\nand attention-based model. Then, we introduce an offline\nMARL  method,  in  which  the  transformer  sequentially\nmaps between the local observations and actions of each\nagent in the offline dataset via parameter sharing. Then\nwe leverage the hidden representation as the input of the\nMADT to minimize the cross-entropy loss. Furthermore,\nwe  introduce  how  to  integrate  the  online  MARL  with\nMADT  in  constructing  our  whole  framework  to  train  a\nuniversal MARL policy. To accelerate the online learning,\nwe load the pre-trained model as a part of the MARL al-\ngorithms and learn the policy based on experience in the\nlatest buffer stored from the online environment. To train\na universal MARL policy quickly adapting to other tasks,\nwe bridge the gap between different scenarios from obser-\nvations, actions, and available actions, respectively. Fig. 1\noverviews our method from the perspective of offline pre-\ntraining  with  supervised  learning  and  online  fine-tuning\nwith  MARL  algorithms.  The  main  contributions  of  this\nwork are summarized as follows: 1) We conducted an off-\nline  dataset  for  multi-agent  offline  pre-training  on  the\nwell-known challenging task, SMAC; 2) To improve the\nsample efficiency online, we propose fine-tuning the pre-\ntrained multi-agent policy instantiated with the sequence\nmodel  by  sharing  policy  among  agents  and  show  the\nstrong capacity of sequence modelling for multi-agent re-\ninforcement  learning  in  the  few-shot  and  zero-shot  set-\ntings; 3) We propose pre-training an actor and a critic to\nfine-tune  with  the  policy-based  network.  In  contrast  to\nthe imitation learning that only fits a policy network off-\nline,  MADT  trains  the  actor  and  critic  offline  together\nand  fine-tunes  them  online  in  the  RL-style  training\nscheme. We also give some empirical conclusions, such as\nthe effect of reward-to-go in the online fine-tuning stage\nand the multi-task padding method on SMAC.\n⟨S; A;\nR; P; n; \r ⟩\nS\nn\nS1 \u0002 S2 \u0001 \u0001\n\u0001 \u0002Sn ! S\nAi\ni\nP : Si \u0002 Ai !\nP D(\nSi)\nA\nRi : S \u0002 Ai ! R\n\u0019(ajs) 2 \u0005i : S ! P D(\nA)\n\u0005i\n\u0005i\ni\na 2 Ai\ns 2 Si\n∑\nt \rtrt\ni\nrt\ni 2 Ri\ni\nt\n\r\nri\nr\nMulti-agent  reinforcement  learning. For  the\nMarkov  game,  which  is  a  multi-agent  extension  of  the\nMarkov decision process (MDP), there is a tuple repres-\nenting the essential elements , where \n  de-\nnotes  the  state  space  of  agents, .\n is  the  action  space  of  each  agent , \n denotes the transition function emitting the dis-\ntribution\n over the state space and  is  the joint action\nspace,  is  the  reward  function  of  each\nagent  and  takes  action  following  their  policies\n from  the policy space , where\n denotes the policy space of agent , , and .\nEach\n agent  aims  to  maximize  its  long-term  reward\n, where  denotes the reward of agent  in\ntime \n and  denotes the discount factor. In the cooperat-\nive\n setting,  we  also  denote  the  with  shared  among\nagents for the simplification.\nQ 2 Rtq\u0002dq\nK 2 Rtk\u0002dk\nV 2 Rtv\u0002dv\nt\u0003\nd\u0003\ntk = tv\ndq = dk\nAttention-based model. The attention-based mod-\nel has shown its stable and strong representation capabil-\nity.  The  scale  dot-production  attention  uses  the  self-at-\ntention mechanism demonstrated in [50]. Let \nbe the quries,  be\n  the keys, and \nbe the values, where  are  the element numbers of differ-\nent inputs and  are  the corresponding element dimen-\nsions. Normally,  and . The outputs of self-\nattention are computed as\nAttention(Q; K; V ) = softmax\n( QKT\npdk\n)V (1)\n1/pdk\nwhere  the  scalar  is  used  to  prevent  the  softmax\nfunction  from  entering  regions  that  have  very  small\ngradients.  Then,  we  introduce  the  multi-head  attention\nprocess as follows:\nMultiHead(Q; K; V ) = Concat\n(head1; \u0001 \u0001 \u0001 ; headh)WO\n(2)\nheadi = Attention(QWQ\ni ; KW K\ni ; V\nWV\ni ): (3)\n \nDataset\nOnline\nenvironment\nOffline dataset\nTrainer MADT\nSOTA policy/\nDemonstration\nBuffer\nShared data structure\nOffline\nOnline\nRollout\nCollect data\nData\nStore data\nLoad \ndata\nTrain\nFig. 1     Overview of the pipeline for pre-training the general policy and fine-tuning it online\n \nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 235 \n \n\ndmodel\ndff\nThe  position-wise  feed-forward  network  is  another\ncore module of the transformer. It consists of two linear\ntransformations with a ReLU activation function. The di-\nmensionality of inputs and outputs is , and\n  that of\nthe feed forward layer is . Specially,\nFFN(x) = max\n(0; xW1 + b1)W2 + b2 (4)\nW1 2 Rdmodel\u0002dff\nW2 2 Rdff \u0002\ndmodel\nb1 2 Rdff\nb2 2 Rdmodel\nwhere  and  are  the\nweights, and  and  are the biases.\nAcross different positions are the same linear transform-\nations. Note that the position encoding for leveraging the\norder of the sequence is as follows:\nPE(pos; 2i) = sin\n(pos/10 0002i/dmodel )\nPE(pos; 2i + 1) = cos(pos/10 0002i/dmodel ): (5)\n 3.1   Multi-agent decision transformer\ni\n\u001ct\ni\nt\nht\ni = ( h1;\nh2; \u0001 \u0001 \u0001 ; hl)\nht\nt\nAlgorithm  1  shows  the  offline  training  process  for  a\nsingle-task  of  MADT,  in  which  we  autoregressively  en-\ncode  the  trajectories  from  the  offline  datasets  in  offline\npre-trained  MARL  and  train  the  transformer-based  net-\nwork with supervised learning. We carefully reformulate\nthe  trajectories  as  the  inputs  of  the  causal  transformer\nthat  are  different  from  those  in  the  Decision  transfor-\nmer[19]. We deprecate the reward-to-go and actions that\nare encoded with states together in the single-agent DT.\nWe will interpret the reason for this in the next section.\nSimilar  to  the  seq2seq  models,  MADT  is  based  on  the\nautoregressive  architecture  with  reformulated  sequential\ninputs across timescales. The left part of Fig. 2 shows the\narchitecture. The causal transformer encodes the agent ′s\ntrajectory\n sequence  at  the time step  to a hidden rep-\nresentation  with  a  dynamic  mask.\nGiven , the output at the time step  is based on the\nprevious\n data and then consumes the previously emitted\nactions  as  additional  inputs  when  predicting  a  new  ac-\ntion.\nAlgorithm  1. MADT-Offline:  Multi-agent  decision\ntransformer\nD : f\u001ci : ⟨st\ni; ot\ni;\nat\ni; vt\ni; dt\ni;\nrt\ni⟩T\nt=1gn\ni=1\nvt\ni\n1) Input: Offline  dataset \n  ,  denotes the available action\n\u0012\n2) Initialize  for the Causal transformer\n\u000b\nC\nn\n3) Initialize  as  the  learning  rate,  as  the  context\n  length, and  as the maximum agent number\n\u001c = f\u001c1; \u0001 \u0001\n\u0001 ; \u001ci; \u0001 \u0001 \u0001 ; \u001cng\nin\nD\n4) for    do\n\u001ci = frt\ni; st\ni;\nat\nigt21:C\nwhere\nC\ndt\ni\n5)　　Chunk  the  trajectory  into  as\n \n the ground truth samples,   is  the context\n  length, and mask the trajectory when  is true\n\u001ct\ni = f\u001c1\ni \u0001 \u0001\n\u0001 \u001ct\ni \u0001 \u0001 \u0001 \u001cC\ni g\nin\n\u001ci\n6)　　for    do\nP(at\nij\u001c<t\ni ; \u0012\n′\n) =\n0 if\nvt\ni is true\n7)　　　Mask  illegal  actions  via \n  \n^at\ni = arg maxai P(\naij\u001c<t\ni ; \u0012\n′\n)\n8)　　　Predict  the  action \n\u0012\n\u0012\narg max\n\u0012′\n1\nC\n∑C\nt=1 P(at\ni)\u0002\nlog P(^at\nij\n\u001c<t\ni ; \u0012\n′\n)\n9)　　　Update  with  = \n  \n10)　　end for\n11) end for\nxt\n⟨global_state; local_observation⟩\nTrajectories  reformulation  as  input. We  model\nthe lowest granularity at each time step as a modelling\nunit  from the static offline dataset for the concise rep-\nresentation.\n MARL  has  many  elements,  such  as\n, different  from the single\nagent. It is reasonable for sequential modelling methods\nto  model  them  in  a  MDP.  Therefore,  we  formulate  the\ntrajectory as follows:\n\u001ci = ( x1; \u0001\n\u0001 \u0001 ; xt; \u0001 \u0001 \u0001 ; xC) ; where xt = ( st; oi\nt; ai\nt)\nsi\nt\noi\nt\ni\nt\nai\nt\nxt\nwhere  denotes the global shared state,  denotes the\nindividual\n observation for agent  at  time step , and \ndenotes the action. We regard  as  a token and process\nthe whole sequence similar to the scheme in the language\nmodelling.\nOutput sequence construction. To bridge the gap\nbetween  training  with  the  whole  context  trajectory  and\n \nOffline datasets\nN ×\nMulti-head \nattention\nAdd & Norm\nFeed forward\nAdd & Norm\n… … Actor network\nPadded agent ation embedding\nPadded agent observation embedding\nCritic network\nActor loss\ne.g., PPO loss\nCritic loss\ne.g., Huber loss\nLocal observation\nAgent′s action\nGlobal state\nAll agents′ actions\nCausal transformer\nn agent\naval\nt−1 avalt\nt − 1 t + 1t t\na\na a o\nt − 1 t\nt\nt − 1 t\na\na\na\nA\nA\ns\ns\no\no\no\nt t + 1\na\navalt+1\n×\nFig. 2     Detailed model structure for offline and online MADT\n \n 236 Machine Intelligence Research 20(2), April 2023\n \n\nt\n⟨1; \u0001 \u0001\n\u0001 ; t \u0000 1⟩\ntesting  with  only  previous  data,  we  mask  the  context\ndata  to  autoregressively  output  in  the  time  step  with\nprevious\n data  in .  Therefore,  MADT  pre-\ndicts  the  sequential  actions  at  each  time  step  using  the\ndecoder as follows:\ny = at = arg max\na\np\u0012(\naj\u001c; a1; \u0001 \u0001 \u0001 ; at\u00001) (6)\n\u0012\n\u001c\ns\no\nt\np\u0012\nv\nwhere  denotes the parameters of MADT and  denotes\nthe\n trajectory  including  the  global  state ,  local\nobservation \n before  the time step ,  is the distribution\nover the legal action space under the available action .\nCore module description. MADT differs from the\ntransformers  in  conventional  sequence  modelling  tasks\nthat  take  inputs  with  position  encoding  and  decode  the\nencoded  hidden  representation  autoregressively.  We  use\nthe masking mechanism with a lower triangular matrix to\ncompute the attention:\nAttention(Q; K; V ) = softmax\n(QKT\npdk\n+ M\n)\nV (7)\nM\nt\n⟨1; \u0001 \u0001\n\u0001 ; t \u0000 1⟩\nv\nwhere  is the mask matrix that ensures that the input\nat\n the time step  can  only correlate with the input from\n.  We  employ  the  cross-entropy  (CE)  as  the\ntotal  sequential  prediction  loss  and  utilize  the  available\naction  to ensure agents take those illegal actions with a\nprobability\n of  zero.  The  CE  loss  can  be  represented  as\nfollows:\nLCE (\n\u0012) = 1\nC\nC∑\nt=1\nP(at) log P(^atj\n\u001ct; ^a<t; \u0012) (8)\nat\n\u001ct\nfs1:t; o1:\ntg\n^a\nwhere C is  the  context  length,  is  the  ground  truth\naction,  includes .  denotes  the  output  of\nMADT.  The  cross-entropy  loss  shown  above  aims  to\nminimize the distribution distance between the prediction\nand the ground truth.\n 3.2   Multi-agent decision transformer with\nPPO\nThe method above can fit the data distribution well,\nresulting  from  the  sequential  modelling  capacity  of  the\ntransformer.  However,  it  fails  to  work  well  when  pre-\ntraining on the offline datasets and improves continually\nby interacting with the online environment. The reason is\nthe  mismatch  between  the  objectives  of  the  offline  and\nonline phase. In the offline stage, the imitation-based ob-\njective conforms to a supervised learning style in MADT\nand  ignores  measuring  each  action  with  a  value  model.\nWhen the pre-trained model is loaded to interact with the\nonline  environment,  the  buffer  will  only  collect  actions\nconforming  to  the  distributions  of  the  offline  datasets\nrather  than  those  corresponding  to  high  reward  at  this\n\u0019(oi; ai)\nVϕ(s)\n\u0019i(oi; ai)\nVϕ(s)\nstate. That means the pre-trained policy is encouraged to\nchoose an action to be identical to the distribution in the\noffline  dataset,  even  though  it  leads  to  a  low  reward.\nTherefore, we need to design another paradigm, MADT-\nPPO,  to  integrate  RL  and  supervised  learning  for  fine-\ntuning in Algorithm 2. Fig. 2 shows the pre-training and\nfine-tuning  framework.  A  direct  method  is  to  share  the\npre-trained  model  across  each  agent  and  implement  the\nREINFORCE algorithm[56]. However, only actors result in\nhigher variance, and the employment of a critic to assess\nstate values is necessary. Therefore, in online MARL, we\nleverage  an  extension  of  PPO,  the  state-of-the-art  al-\ngorithm on tasks of StarCraft, multi-agent particle envir-\nonment  (MPE),  and  even  the  return-based  game\nHanabi[57].  In  the  offline  stage,  we  adopt  the  strategy\nmentioned  before  to  pre-train  an  offline  policy  for  each\nagent  and\n  additionally  use  the  global  state  to\npre-train  a  centralized  critic .  In  the  fine-tuning\nstage, we first load the offline pre-trained sharing policy\nas each agent′s online initial policy . When\n  the\ncritic is pre-trained, we instantiate the centralized critic\nwith the pre-trained model as . To  fine-tune the pre-\ntrained  multi-agent  policy  and  critic  model,  multiple\nagents  clear  the  buffer  and  interact  with  the  environ-\nment  to  learn  the  policy  via  maximizing  the  following\nPPO objective:\nn∑\ni=1\nE s\u0018\u001a\u0012old ; a\u0018\u0019\u0012old\n[\nmin(! ^A(s; a); clip(!;\n1\u0000ϵ; 1+ϵ) ^A(s; a))] (9)\n! = \u0019\u0012(oi; ai)/\n\u0019\u0012old (oi; ai)\n^A\nϵ\nwhere  denotes  the  importance\nweight  using  in  PPO-style  algorithms,  denotes  the\nadvantage  function  computed  by  reward  and  the  critic\nmodel  and  denotes  the  clip  parameter.  The  detailed\nfine-tuning pipeline can be found in Algorithm 2.\nAlgorithm  2. MADT-Online:  Multi-agent  decision\ntransformer with PPO\nD\n\u0012\n1) Input: Offline dataloader , Pretrained  MADT policy\n  with parameter \n\u0012\nϕ\n\u0019\u0012(aijoi)\nVϕ(s)\n2) Initialize  and  are  the  parameters  of  an  actor\n   and  critic  respectively, which could be\n  inherited directly from pre-trained models\nn\n\r\nϵ\n3) Initialize  as  the  agent  number,  as  the  discount\n  factor, and  as clip ratio.\n\u001c = f\u001c1; \u0001 \u0001\n\u0001 ; \u001ci; \u0001 \u0001 \u0001 ; \u001cng\nin\nD\n4) for    do\n\u001ci = fst; ot\ni;\nat\nigt21:C\nwhere\nC\n5)　　Sample  as  the  ground  truth,\n    is the context length\n^A(s; ai)\n=\n∑\nt \rtr(s; ai) \u0000 Vϕ(\ns)\n6)　　Compute  the  advantage  function \n  \nw = \u0019\u0012(oi; ai)/\n\u0019\u0012old (oi; ai)\n7)　　Compute  the  important  weight \n  \n\u0012i\ni 2 1; \u0001 \u0001\n\u0001 ; n\n8)　　Update  for  via:\n\u0012i\narg maxEs\n\u0018\u001a\u0012old ;a\u0018\u0019\u0012old\n[clip(w; 1 \u0000 ϵ; 1+\nϵ) ^A(s; ai)]\n9)  　  =　 \n \n \nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 237 \n \n\nLϕ = 1\n2 [∑\nt \rtrt \u0000 Vϕ(s)]2\n10)　 Compute  the  MSE  loss  \nϕ = arg min\nϕ\nLϕ\n11)　 Update the critic network via \n12) end for\n 3.3   Universal model across scenarios\nTo train a universal policy for each of the scenarios in\nthe SMAC which might vary with agent number in fea-\nture space, action space, and reward ranges, we consider\nthe modification list below.\nParameters  sharing  across  agents. When  offline\nexamples  are  collected  from  multiple  tasks  or  the  test\nphase owns the different agent numbers from the offline\ndatasets, the difference in agent numbers across tasks is\nan intractable problem for deciding the number of actors.\nThus, we consider sharing the parameters across all act-\nors with one model as well as attaching one-hot agent IDs\ninto observations for compatibility with a variable num-\nber of agents.\nFeature encoding. When the policy needs to gener-\nalize  to  new  scenarios  that  arise  from  different  feature\nshapes, we propose encoding all features into a universal\nspace by padding zero at the end and mapping them to a\nlow-dimensional space with fully connected networks.\nAction masking. Another  issue  is  the  different  ac-\ntion spaces across scenarios. For example, fewer enemies\nin a scenario means fewer potential attack options as well\nas  fewer  available  actions.  Therefore,  an  extra  vector  is\nutilized  to  mute  the  unavailable  actions  so  that  their\nprobabilities are always zero during both the learning and\nevaluating processes.\nReward  scaling. Different  scenarios  might  vary  in\nreward  ranges  and  lead  to  unbalanced  models  during\nmulti-task offline learning. To balance the influence of ex-\namples from different scenarios, we scale their rewards to\nthe  same  range  to  ensure  that  the  output  models  have\ncomparable performance across different tasks.\n 4   Experiments\nWe  show  three  experimental  settings:  offline  MARL,\nonline MARL by loading the pre-trained model, and few-\nshot or zero-shot offline learning. For the offline MARL,\nwe  expect  to  verify  the  performance  of  our  method  by\npre-training the policy and directly testing on the corres-\nponding  maps.  In  order  to  demonstrate  the  capacity  of\nthe pre-trained policy on the original or new scenarios, we\naim to demonstrate the fine-tuning in the online environ-\nment.  Experimental  results  in  offline  MARL  show  that\nour MADT-offline in Section 3.1 outperforms the state-of-\nthe-art  methods.  Furthermore,  MADT-online  in  Section\n3.2  can  improve  the  sample  efficiency  across  multiple\nscenarios.  Besides,  the  universal  MADT  trained  from\nmulti-task  data  with  MADT-online  generalizes  well  in\neach scenario in a few-shot or even zero-shot setting.\n 4.1   Offline datasets\n\u001c := (\nst; ot; at; rt; donet; vt)T\nt=1\nThe  offline  datasets  are  collected  from  the  running\npolicy,  MAPPO[58],  on  the  well-known  SMAC  task[26].\nEach  dataset  contains  a  large  number  of  trajectories:\n.  Different  from  D4RL[59],\nour  datasets  conform  to  the  property  of  DecPOMDP,\nwhich  owns  local  observations  and  available  actions  for\neach agent. In the appendix, we list the statistical proper-\nties of the offline datasets in Tables A1 and A2.\n 4.2   Offline multi-agent reinforcement lear-\nning\n2s3z\n3s5z\n3s5z VS.\n3s6z\ncorridor\nIn  this  experiment,  we  aim  to  validate  the  effective-\nness  of  the  MADT  offline  version  in  Section  3.1  as  a\nframework for offline MARL on the static offline datasets.\nWe  train  a  policy  on  the  offline  datasets  with  various\nqualities  and  then  apply  it  to  an  online  environment,\nStarCraft[26]. There are also baselines under this setting,\nsuch  as  behavior  cloning  (BC),  as  a  kind  of  imitation\nlearning  method  showing  stable  performance  on  single-\nagent offline RL. In addition, we employ the convention-\nal  effective  single-agent  offline  RL  algorithms,  BCQ[32],\nCQL[31], and ICQ[35], and then use the extension method\nby simply mixing each agent value network proposed by\n[35] for multi-agent setting, denoting it as “xx-MA”. We\ncompare  the  performance  of  the  MADT  offline  version\nwith  other  abovementioned  offline  RL  algorithms  under\nonline  evaluation  in  the  MARL  environment.  To  verify\nthe quality of our collected datasets, we chose data from\ndifferent  levels  and  trained  the  baselines  as  well  as  our\nMADT. Fig. 3 shows the overall performance on various\nquality datasets. The baseline methods enhance their per-\nformance stably, indicating the quality of our offline data-\nsets.  Furthermore,  our  MADT  outperforms  the  offline\nMARL baselines and converges faster across easy, hard,\nand  super  hard  maps  ( , \n , ,\n).  From  the  initial  performance  in  the  evalu-\nation\n period, our pretrained model gives a higher return\nthan baselines in each task. Besides, our model can sur-\npass the average performance in the offline dataset.\n 4.3   Offline pre-training and online fine-\ntuning\nThe experimental designed in this subsection intends\nto answer the question: Is the pre-training process neces-\nsary for online MARL? First, we compare the online ver-\nsion  of  MADT  in  Section  3.2  with  and  without  loading\nthe pre-trained model. If training MADT only by online\nexperience,  we  can  view  it  as  a  transformer-based\nMAPPO  replacing  the  actor  and  critic  backbone  net-\nworks  with  the  transformer.  Furthermore,  we  validate\nthat  our  framework  MADT  with  the  pre-trained  model\ncan  improve  sample  efficiency  on  most  easy,  hard,  and\n 238 Machine Intelligence Research 20(2), April 2023\n \n\nsuper hard maps.\nNecessity of the pretrained model. We train our\nMADT based on the datasets collected from a map and\nfine-tune it on the same map online with the MAPPO al-\ngorithm. For comparison fairness, we use the transformer\nas both actor and critic networks with and without the\npre-trained model. Primarily, we choose three maps from\neasy, hard, and super hard maps to validate the effective-\nness of the pre-trained model in Fig. 4. Experimental res-\nults  show  that  the  pre-trained  model  converges  faster\nthan  the  algorithm  trained  from  scratch,  especially  in\nchallenging maps.\nImproving  sample  efficiency. To  validate  the\nsample efficiency improvement by loading our pre-trained\nMADT and fine-tuning it with MAPPO, we compare the\noverall  framework  with  the  state-of-the-art  algorithm,\nMAPPO[58], without the pre-training phase. We measure\nthe  sample  efficiency  in  terms  of  the  time  to  threshold\nmentioned in [60], which denotes the number of online in-\nteractions (timesteps) to achieve a predefined threshold in\nTable 1, and our pre-trained model needs much less than\nthe traditional MAPPO to achieve the same win rate.\n 4.4   Generalization with multi-task pre-\ntraining\nExperiments in this section explore the transferability\nof the universal MADT mentioned in Section 3.3, which\nis pre-trained with mixed data from multiple tasks. De-\npending on whether the downstream tasks have been seen\nor not, the few-shot experiments are designed to validate\nthe adaptability of the seen tasks. In contrast, the zero-\nshot experiments are designed for the held-out maps.\nFew-shot  learning. The  results  in Fig. 5(a)  show\nthat our method can utilize multi-task datasets to train a\nuniversal  policy  and  generalize  to  all  tasks  well.  Pre-\ntrained MADT can achieve higher returns than the mod-\nel  trained  from  scratch  when  we  limit  the  interactions\nwith the environment.\nZero-shot learning. Fig. 5(b) shows that our univer-\nsal  MADT  can  surprisingly  improve  performance  on\ndownstream tasks even if it has not been seen before (3\nstalkers VS. 4 zealots).\n 4.5   Ablation study\nThe experiments in this subsection are designed to an-\nswer the following research questions: RQ1: Why should\nwe  choose  MAPPO  for  the  online  phase?  RQ2:  Which\nkind  of  input  should  be  used  to  make  the  pre-trained\nmodel beneficial for the online MARL? RQ3: Why can-\nnot the offline version of MADT be improved in the on-\nline fine-tuning period after pre-training?\nSuitable online algorithm. Although  the  selection\nof  the  MARL  algorithm  for  the  online  phase  should  be\nflexible  according  to  specific  tasks,  we  design  experi-\nments to answer RQ1 here. As discussed in Section 3, we\ncan train Decision Transformer for each agent and fine-\ntune  it  online  with  an  MARL  algorithm.  An  intuitive\nmethod is to load the pre-trained transformer and take it\nas  the  policy  network  for  fine-tuning  with  the  policy\ngradient method, e.g., REINFORCE[56]. However, for the\n \nAverage return\n20\n15\n10\n5\n0\n0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1E+3\nTimesteps\nAverage return\n0 0.2 0.4 0.6 0.8 1.0\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n1E+3\nAverage return\nMADT\nBC\nBCQ-MA\nCQL-MA\nICQ-MA\n0 0.2 0.4 0.6 0.8 1.0\n20\n18\n16\n14\n12\n10\n8\nTimesteps 1E+3\nAverage return\n0 0.2 0.4 0.6 0.8 1.0\n14\n12\n10\n8\n6\n4\n2\nTimesteps 1E+2\nAverage return\n0 0.2 0.4 0.6 0.8 1.0\n6\n5\n4\n3\n2\n1\nTimesteps 1E+2\nAverage return\n10\n8\n6\n4\n2\n0 0.2 0.4 0.6 0.8\n1.0\nTimesteps 1E+2\nAverage return\n0 0.2 0.4 0.6 0.8 1.0\n14\n12\n10\n8\n6\n4\n2\nTimesteps\n1E+2\nAverage return\n20\n15\n10\n5\n0 0.2 0.4 0.6 0.8\n1.0\nTimesteps 1E+2\nAverage return\n20\n15\n10\n5\n0\n0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1E+2\nAverage return\n20\n15\n10\n5\n0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1E+2\n0 0.2 0.4 0.6 0.8 1.0\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nTimesteps 1E+2\nAverage return\nAverage return\n0 0.2 0.4 0.6 0.8 1.0\nTimesteps 1E+2\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0\n(a) 2s3z (Easy) (b) 3s5z (Hard) (c) 3s5z VS. 3s6z (Super hard) (d) Corridor (Super hard)\nFig. 3     Performance of offline MADT compared with baselines on four easy or (super-)hard SMAC maps. The dotted lines represent the\nmean values in the training set. Columns (a)–(d) are average returns from (poor, medium, good) datasets from top to the bottom.\n \nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 239 \n \n\nreason of the high variance mentioned in Section 3.2, we\nchoose MAPPO as the online algorithm and compare its\nperformance  in  improving  the  sample  efficiency  during\nthe online period in Fig. 6(a).\n \n20%; 40%; 60%; 80% and 100\n%\n1\nTable 1    Number of interactions needed to achieve the win rate   for  the training policy with\n(MAPPO/pre-trained MADT). “–” means no more samples are needed to reach the target win rate.\n“ ” represents  that policies cannot reach the target win rate.\nMaps\n# Samples to achieve the win rate\nMaps\n# Samples to achieve the win rate\n20% 40% 60% 80% 100% 20% 40% 60% 80% 100%\n2 m VS. 1 z\n(Easy) 8E+4/– 1E+5/ – 1.3E+5/– 1.5E+5/– 1.6E+5/– 3 s VS. 5 z\n(Hard) 8E+5/– 8.5E+5/ – 8.7E+5/\n1.5E+4\n9E+5/\n5E+4\n2E+6/\n1.5E+5\n3 m\n(Easy) 3.2E+3/– 8.3E+4/– 3.2E+5/– 4E+5/– 7.2E+5/– 2 c VS. 64 zg\n(Hard) 2E+5/– 3E+5/– 4E+5/\n8E+4\n5E+5/\n1E+5\n1.8E+6/\n5E+5\n2 s VS. 1 sc\n(Easy) 1E+4/– 2.5E+4/– 3E+4/– 8E+4/\n4E+4\n3E+5/\n1.2E+5\n8 m VS. 9 m\n(Hard) 3E+5/– 6E+5/– 1.4E+6/\n2E+4\n2E+6/\n8E+4 ∞/2.2E+6\n3 s VS. 3 z\n(Easy) 2.5E+5/ – 3E+5/– 6.2E+5/\n1E+4\n7.3E+5/\n1.5E+5\n8E+5/\n2.9E+5\n5 m VS. 6 m\n(Hard)\n1.5E+6/\n2E+5\n2.5E+6/\n8E+5\n5E+6/\n2E+6 ∞/∞ ∞/∞\n3 s VS. 4 z\n(Easy) 3E+5/– 4E+5/ – 5E+5/– 6.2E+5/– 1.5E+6/\n1.8E+5\n3 s5 z\n(Hard)\n8E+5/\n6.3E+4\n1.3E+6/\n1E+5\n1.5E+6/\n4E+5\n1.9E+6/\n1E+6\n2.5E+6/\n2E+6\nSo many\nbaneling\n(Easy)\n3.2E+4/\n8E+3\n1E+5/\n4E+4\n3.2E+5/\n7E+4\n5E+5/\n8E+4\n1E+6/\n6.4E+5\n10 m VS. 11 m\n(Hard) 2E+5/– 3.5E+5/– 4E+5/\n2.8E+4\n1.7E+6/\n1.2E+5\n4E+6/\n2.5E+5\n8 m\n(Easy) 4E+5/– 5E+6/– 5.6E+5/– 5.6E+5E+\n5/1.6E+5\n8.8E+5/\n2.4E+5\nMMM2\n(Super hard) 1E+6/ 1.8E+6/ 2.3E+6/ 4E+6/ ∞/∞\nMMM\n(Easy) 5.2E+4/– 8E+4/– 3E+5/– 4.5E+5/– 1.8E+6/\n6E+5\n3 s5z VS. 3 s6 z\n(Super\n hard) 1.8E+6/ – 2.5E+6/– 3E+6/\n8E+5\n5E+6/\n1E+6 ∞/∞\nBane VS.\nbane\n(Easy)\n3.2E+3/\n– 3.2E+3/– 3.2E+5/– 4E+5/– 5.6E+5/– Corridor\n(Super hard) 1.5E+6/ – 1.8E+6/– 2E+6/– 2.8E+6/– 7.8E+6/\n4E+5\n \n \nTimesteps\n(a) So many baneling (Easy)\n1E+6\nAverage return\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0\n0 0.5 1.0 1.5 2.0 2.5\nOnline MARL w/ pretrained model\nOnline MARL w/o pretrained model\n(b) 5 m VS. 6 m (Hard)\nAverage return\nTimesteps 1E+6\n0 0.5 1.0 1.5 2.0 2.5\n14\n12\n10\n8\n6\n4\n2\n0\n−2\nOnline MARL w/ pretrained model\nOnline MARL w/o pretrained model\n(c) 3s5z VS. 3s6z (Super hard)\nAverage return\nTimesteps 1E+6\n0 0.5 1.0 1.5 2.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\nOnline MARL w/ pretrained model\nOnline MARL w/o pretrained model\nFig. 4     Average returns with and without the pre-trained model\n \n \nAverage return\nFrom scratch\nFine-tuned from universal MADT25\n20\n15\n10\n5\n0\n2m VS. \n1z\n2s VS. 1sc 3s VS. \n3z\n3s VS. \n4z3m\n(a)\n1E+6Times (teps)\nAverage return\nUniversal MADT (ours)\nTrain from scratch\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0\n0 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00\n(b)\n3s VS.\n4z\n2m VS.\n1z\n2s VS.\n1sc\n3m\n3s VS.\n3z\nFig. 5     Few-shot and zero-shot validation results. (a) shows the average returns of the universal MADT pre-trained from all five tasks\ndata and the policy trained from scratch, individually. We limit the environment interaction to 2.5 M steps. (b) shows the average\nreturns of a held-out map ( ),   where the universal MADT is trained from data on ( ,  ,  ,  ).\n \n 240 Machine Intelligence Research 20(2), April 2023\n \n\nDropping  reward-to-go  in  MADT. To  answer\nRQ2,  we  compare  different  inputs  embedded  into  the\ntransformer, including the combination of state, reward-\nto-go,  and  action.  We  find  reward-to-go  harmful  to  on-\nline  fine-tuning  performance,  as  shown  in Fig. 6(b).  We\nsuppose the distribution of reward-to-go is the mismatch\nbetween offline data and online samples. That is, the re-\nwards of online samples are usually lower than those of\noffline  data  due  to  stochastic  exploration  at  the  begin-\nning  of  the  online  phase.  It  deteriorates  the  fine-tuning\ncapability  of  the  pre-trained  model,  and  based  on\nFig. 6(b),  we  only  choose  states  as  our  inputs  for  pre-\ntraining and fine-tuning.\nIntegrating online MARL with MADT. To an-\nswer RQ3, we directly apply the offline version of MADT\nfor pre-training and fine-tune it online. However, Fig. 6(c)\nshows  that  it  cannot  be  improved  during  the  online\nphase. We analyse the results caused by the absence of\nmotivation for chasing higher rewards and conclude that\noffline MADT is supervised learning and tends to fit its\ncollected experience even with unsatisfactory rewards.\n 5   Conclusions\nIn  this  work,  we  propose  MADT,  an  offline  pre-\ntrained  model  for  MARL,  which  integrates  the  trans-\nformer  to  improve  sample  efficiency  and  generalizability\nin  tackling  SMAC  tasks.  MADT  learns  a  big  sequence\nmodel  that  outperforms  the  state-of-the-art  methods  in\noffline  settings,  including  BC,  BCQ,  CQL,  and  ICQ.\nWhen applied in online settings, the pre-trained MADT\ncan  drastically  improve  the  sample  efficiency.  We  ap-\nplied MADT to train a generalizable policy over a series\nof  SMAC  tasks  and  then  evaluated  its  performance  un-\nder  both  few-shot  and  zero-shot  settings.  The  results\ndemonstrate  that  the  pre-trained  MADT  policy  adapts\nquickly to new tasks and improves performance on differ-\nent downstream tasks. To the best of our knowledge, this\nis  the  first  work  that  demonstrates  the  effectiveness  of\noffline pre-training and the effectiveness of sequence mod-\nelling through transformer architectures in the context of\nMARL.\n Appendix A   Properties of datasets\nWe list the properties of our offline datasets in Tables A1\nand A2.\n Appendix B   Details of hyper-parameters\nDetails  of  hyper-parameters  used  for  MADT  experi-\nments are listed from Tables B1–B5.\n \nTable A1    Properties for our offline dataset collected from the experience of multi-agent PPO on the easy maps of SMAC\nMaps Difficulty Data quality # Samples Reward distribution (mean (±std))\n3m Easy 3m-poor 62 528 6.29 (± 2.17)\n3m-medium – –\n3m-good 1 775 159 19.99  (± 0.18)\n8m Easy 8m-poor 157 133 6.43 (± 2.41)\n8m-medium 297 439 11.95  (± 0.94)\n8m-good 2 781 145 20.00  (± 0.16)\n2s3z Easy 2s3z-poor 147 314 7.69 (± 1.77)\n2s3z-medium 441 474 12.85  (± 1.37)\n2s3z-good 4 177 846 19.93  (± 0.67)\n \n0 0.2 0.4 0.6 0.8 1.0\n1E+6\nMADT with MAPPO\nMADT with REINFORCE\n14\n12\n10\n8\n6\n4\n2\nTimesteps\nAverage return\n(a)\n0 0.2 0.4 0.6 0.8 1.0\n1E+6\nState_only\nState & action\nReward-to-go & state & action\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0\nTimesteps\nAverage return\n(b)\n0 1 2 3 4 5 6 7 8\n1E+5\n14\n12\n10\n8\n6\n4\n2\nAverage return\nTimesteps\n(c)\nMADT with MAPPO for fine-tuning\nPure MADT for fine-tuning\n5m_vs_6m\nFig. 6     Ablation results on a hard map,  , for  validating the necessity of (a) MAPPO in MADT-online, (b) Input formulation,\n(c) online version of MADT.\n \nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 241 \n \n\nTable A1 (continued) Properties for our offline dataset collected from the experience of multi-agent PPO on the easy maps of SMAC\nMaps Difficulty Data quality # Samples Reward distribution (mean (±std))\n2s_vs_1sc Easy 2s_vs_1sc-poor 12 887 6.62 (± 2.74)\n2s_vs_1sc-medium 33 232 11.70 (± 0.73)\n2s_vs_1sc-good 1 972 972 20.23  (± 0.02)\n3s_vs_4z Easy 3s_vs_4z-poor 216 499 7.58 (± 1.45)\n3s_vs_4z-medium 335 580 12.13 (±m 1.38)\n3s_vs_4z-good 3 080 634 20.19  (± 0.40)\nMMM Easy MMM-poor 326 516 7.64 (± 2.05)\nMMM-medium 648 115 12.23  (± 1.37)\nMMM-good 2 423 605 20.08  (± 1.67)\nSo_many_baneling Easy So _many_baneling-poor 1  542 9.08 (± 0.66)\nSo_many_baneling-medium 59 659 13.31 (± 1.14)\nSo_many_baneling-good 1  376 861 19.46  (± 1.29)\n3s_vs_3z Easy 3s_vs_3z-poor 52 807 8.10 (± 1.37)\n3s_vs_3z-medium 80 948 11.87 (± 1.19)\n3s_vs_3z-good 149 906 20.02  (± 0.09)\n2m_vs_1z Easy 2m_vs_1z-poor 25 333 5.20 (± 1.66)\n2m_vs_1z-medium 300 11.00 (± 0.01)\n2m_vs_1z-good 120 127 20.00  (± 0.01)\nBane_vs_bane Easy Bane_vs_bane-poor 63 1.59 (± 3.56)\nBane_vs_bane-medium 3 507 14.00  (± 0.93)\nBane_vs_bane-good 458 795 19.97  (± 0.36)\n1c3s5z Easy 1c3s5z-poor 52 988 8.10 (± 1.65)\n1c3s5z-medium 180 357 12.68  (± 1.42)\n1c3s5z-good 2 400 033 19.88  (± 0.69)\n \n \nTable A2    Properties for our offline dataset collected from the experience of multi-agent\nPPO on the hard and super hard maps of SMAC\nMaps Difficulty Data quality # Samples Reward distribution (mean (±std))\n5m_vs_6m Hard 5m_vs_6m-poor 1 324 213 8.53 (± 1.18)\n5m_vs_6m-medium 657 520 11.03  (± 0.58)\n5m_vs_6m-good 503 746 20 (± 0.01)\n10m_vs_11m Hard 10m_vs_11m-poor 140 522 7.64 (± 2.39)\n10m_vs_11m-medium 916 845 12.72  (± 1.25)\n10m_vs_11m-good 895 609 20 (± 0.01)\n2c_vs_64zg Hard 2c_vs_64zg-poor 10 830 8.91 (± 1.01)\n2c_vs_64zg-medium 97 702 13.05 (± 1.37)\n2c_vs_64zg-good 2 631 121 19.95  (± 1.24)\n8m_vs_9m Hard 8m_vs_9m-poor 184 285 8.18 (± 2.14)\n8m_vs_9m-medium 743 198 12.19  (± 1.14)\n8m_vs_9m-good 911 652 20 (± 0.01)\n 242 Machine Intelligence Research 20(2), April 2023\n \n\nTable A2 (continued) Properties for our offline dataset collected from the experience of multi-agent\nPPO on the hard and super hard maps of SMAC\nMaps Difficulty Data quality # Samples Reward distribution (mean (±std))\n3s_vs_5z Hard 3s_vs_5z-poor 423 780 6.85 (± 2.00)\n3s_vs_5z-medium 686 570 12.12  (± 1.39)\n3s_vs_5z-good 2 604 082 20.89  (± 1.38)\n3s5z Hard 3s5z-poor 365 389 8.32 (± 1.44)\n3s5z-medium 2 047 601 12.61  (± 1.32)\n3s5z-good 1 448 424 18.45  (± 2.03)\n3s5z_vs_3s6z Super hard 3s5z _vs_3s6z-poor 594 089 7.92 (± 1.77)\n3s5z_vs_3s6z-medium 2 214 201 12.56  (± 1.37)\n3s5z_vs_3s6z-good 1 542 571 18.35  (± 2.04)\n27m_vs_30m Super hard 27m _vs_30m-poor 102 003 7.18 (± 2.08)\n27m_vs_30m-medium 456 971 13.19  (± 1.25)\n27m_vs_30m-good 412 941 17.33  (± 1.97)\nMMM2 Super hard MMM2-poor 1  017 332 7.87 (± 1.74)\nMMM2-medium 1 117 508 11.79  (± 1.28)\nMMM2-good 541 873 18.64  (± 1.47)\nCorridor Super hard Corridor-poor 362  553 4.91 (± 1.71)\nCorridor-medium 439 505 13.00  (± 1.32)\nCorridor-good 3 163 243 19.88  (± 0.99)\n \n \n3m\nTable B1    Common hyper-parameters for all MADT experiments for pre-training on a map, taking   (easy)  as an example\nHyper-parameter Value Hyper-parameter Value Hyper-parameter Value\nOffline_train_critic True Max _timestep 400 Eval _epochs 32\nn\n_layer 2\nn _head 2\nn _embd 32\nOnline_buffer_size 64 Model _type State_only Mini_batch_size 128\n \n \nTable B2    Hyper-parameters for MADT experiments in Fig. 3\nMaps offline_episode_num offline_lr\n2s3z 1 000 1E –4\n3s5z 1\n 000 1E –4\n3s5z\n_vs_3s6z 1 000 5E –4\nCorridor 1\n 000 5E –4\n \n \nTable\n B3    Hyper-parameters for MADT experiments in Figs. 4, 6 and Table 1\nMaps Offline_episode_num Offline_lr Online _lr Online_ppo_epochs\n2c\n_vs_64zg 1 000 5E–4 5E–4 10\n10m\n_vs_11m 1 000 5E–4 5E–4 10\n8m\n_vs_9m 1 000 1E–4 5E–4 10\n3s\n_vs_5z 1 000 1E–4 5E–4 10\n3s5z 1 000 1E–4 5E–4 10\n3m 1 000 1E–4 5E–4 15\nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 243\n \n \n\n Acknowledgements   \nLinghui Meng was supported in part by the Strategic\nPriority  Research  Program  of  the  Chinese  Academy  of\nSciences (No. XDA27030300). Haifeng Zhang was suppor-\nted in part by the National Natural Science Foundation\nof China (No. 62206289).\n Open Access\nThis article is licensed under a Creative Commons At-\ntribution  4.0  International  License,  which  permits  use,\nsharing, adaptation, distribution and reproduction in any\nmedium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to\nthe  Creative  Commons  licence,  and  indicate  if  changes\nwere made.\nThe images or other third party material in this art-\nicle  are  included  in  the  article′s  Creative  Commons  li-\ncence,  unless  indicated  otherwise  in  a  credit  line  to  the\nmaterial. If material is not included in the article′s Creat-\nive  Commons  licence  and  your  intended  use  is  not  per-\nmitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the\ncopyright holder.\nTo  view  a  copy  of  this  licence,  visit http://creative-\ncommons.org/licenses/by/4.0/.\nReferences\n Y. D. Yang, J. Wang. An overview of multi-agent rein-\nforcement  learning  from  game  theoretical  perspective.\n[Online],  Available:  https://arxiv.org/abs/2011.00583,\n2020.\n[1]\n S. Shalev-Shwartz, S. Shammah, A. Shashua. Safe, multi-\nagent,  reinforcement  learning  for  autonomous  driving.\n[Online],  Available:  https://arxiv.org/abs/1610.03295,\n2016.\n[2]\n M. Zhou, J. Luo, J. Villella, Y. D. Yang, D. Rusu, J. Y.\nMiao, W. N. Zhang, M. Alban, I. Fadakar, Z. Chen, A. C.\nHuang, Y. Wen, K. Hassanzadeh, D. Graves, D. Chen, Z.\nB. Zhu, N. Nguyen, M. Elsayed, K. Shao, S. Ahilan, B. K.\nZhang, J. N. Wu, Z. G. Fu, K. Rezaee, P. Yadmellat, M.\nRohani,  N.  P.  Nieves,  Y.  H.  Ni,  S.  Banijamali,  A.  C.\nRivers,  Z.  Tian,  D.  Palenicek,  H.  bou  Ammar,  H.  B.\nZhang, W. L. Liu, J. Y. Hao, J. Wang. Smarts: Scalable\nmulti-agent  reinforcement  learning  training  school  for\nautonomous  driving.  [Online],  Available:  https://arxiv.\norg/abs/2010.09776, 2020.\n[3]\n H. F. Zhang, W. Z. Chen, Z. R. Huang, M. N. Li, Y. D.\nYang,  W.  N.  Zhang,  J.  Wang.  Bi-level  actor-critic  for\nmulti-agent  coordination.  In \nProceedings of the 34th\nAAAI Conference on Artificial Intelligence,  New  York,\nUSA, pp. 7325–7332, 2020.\n[4]\n M. N. Li, Z. W. Qin, Y. Jiao, Y. D. Yang, J. Wang, C. X.\nWang, G. B. Wu, J. P. Ye. Efficient ridesharing order dis-\npatching with mean field multi-agent reinforcement learn-\ning.  In  Proceedings of World Wide Web Conference,\nACM, San Francisco, USA, pp. 983–994, 2019. DOI: 10.\n1145/3308558.3313433.\n[5]\nTable B3 (continued) Hyper-parameters for MADT experiments in Figs. 4, 6 and Table 1\nMaps Offline_episode_num Offline_lr Online _lr Online_ppo_epochs\n2s_vs_1sc 1 000 1E–4 5E–4 15\nMMM 1 000 1E–4 1E–4 5\nSo_many_baneling 1  000 1E–4 1E–4 5\n8m 1 000 1E–4 1E–4 5\n3s_vs_3z 1 000 1E–4 1E–4 5\n3s_vs_4z 1 000 1E–4 1E–4 5\nBane_vs_bane 1 000 1E–4 1E–4 5\n2m_vs_1z 1 000 1E–4 1E–4 5\n2c_vs_64zg 1 000 1E–4 1E–4 5\n5m_vs_6m 1 000 1E–4 1E–4 10\nCorridor 1 000 1E–4 1E–4 10\n3s5z_vs_3s6z 1 000 1E–4 1E–4 10\n \n \nTable B4    Hyper-parameters for MADT\nexperiments in Fig. 5(a)\nHyper-parameter Value\nOffline_map_lists [3s_vs _4z, 2m_vs_1z, 3m, 2s_vs_1sc, 3s_vs_3z]\nOffline_episode_num [200, 200, 200, 200, 200]\nOffline_lr 5E–4\nOnline_lr 1E–4\nOnline_ppo_epochs 5\n \n \nTable B5    Hyper-parameters for MADT\nexperiments in Fig. 5(b)\nHyper-parameter Value\nOffline_map_lists [2m _vs_1z, 3m, 2s_vs_1sc, 3s_vs_3z]\nOffline_episode_num [250, 250, 250, 250]\nOffline_lr 5E–4\nOnline_lr 1E–4\nOnline_ppo_epochs 5\n \n 244 Machine Intelligence Research 20(2), April 2023\n \n\n Y. D. Yang, R. Luo, M. N. Li, M. Zhou, W. N. Zhang, J.\nWang. Mean field multi-agent reinforcement learning. In\nProceedings of the 35th International Conference on Ma-\nchine Learning, Stockholm, Sweden, pp. 5571–5580, 2018.\n[6]\n Y. D. Yang, L. T. Yu, Y. W. Bai, Y. Wen, W. N. Zhang, J.\nWang. A study of AI population dynamics with million-\nagent reinforcement learning. In Proceedings of the 17th\nInternational Conference on Autonomous Agents and\nMultiAgent Systems, ACM, Stockholm, Sweden, pp. 2133–\n2135, 2018.\n[7]\n P. Peng, Y. Wen, Y. D. Yang, Q. Yuan, Z. K. Tang, H. T.\nLong,  J.  Wang.  Multiagent  bidirectionally-coordinated\nnets: Emergence of human-level coordination in learning\nto play StarCraft combat games. [Online], Available: ht-\ntps://arxiv.org/abs/1703.10069, 2017.\n[8]\n M. Zhou, Z. Y. Wan, H. J. Wang, M. N. Wen, R. Z. Wu,\nY. Wen, Y. D. Yang, W. N. Zhang, J. Wang. MALib: A\nparallel framework for population-based multi-agent rein-\nforcement learning. [Online], Available: https://arxiv.org/\nabs/2106.07551, 2021.\n[9]\n X. T. Deng, Y. H. Li, D. H. Mguni, J. Wang, Y. D. Yang.\nOn the complexity of computing Markov perfect equilibri-\num in general-sum stochastic games. [Online], Available:\nhttps://arxiv.org/abs/2109.01795, 2021.\n[10]\n T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha,\nJ. Tan, V. Kumar, H. Zhu, A. Gupta, P. Abbeel, S. Levine.\nSoft  actor-critic  algorithms  and  applications.  [Online],\nAvailable: https://arxiv.org/abs/1812.05905, 2018.\n[11]\n R. Munos, T. Stepleton, A. Harutyunyan, M. G. Belle-\nmare. Safe and efficient off-policy reinforcement learning.\nIn Proceedings of the 30th Conference on Neural Informa-\ntion Processing Systems,  Barcelona,  Spain,  pp. 1054–\n1062, 2016.\n[12]\n L.  Espeholt,  R.  Marinier,  P.  Stanczyk,  K.  Wang,  M.\nMichalski. SEED RL: Scalable and efficient deep-RL with\naccelerated central inference. In Proceedings of the 8th In-\nternational Conference on Learning Representations, Ad-\ndis Ababa, Ethiopia, 2020.\n[13]\n L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih,\nT. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, S.\nLegg,  K.  Kavukcuoglu.  IMPALA:  Scalable  distributed\ndeep-RL with importance weighted actor-learner architec-\ntures. In Proceedings of the 35th International Conference\non Machine Learning, Stockholm, Sweden, pp. 1407–1416,\n2018.\n[14]\n K. M. He, X. L. Chen, S. N. Xie, Y. H. Li, Dollár, R. Gir-\nshick. Masked autoencoders are scalable vision learners. In\nProceedings of IEEE/CVF Conference on Computer Vis-\nion and Pattern Recognition, IEEE, New Orleans, USA,\npp. 15979–15988,  2021.  DOI:  10.1109/CVPR52688.2022.\n01553.\n[15]\n Z. Liu, Y. T. Lin, Y. Cao, H. Hu, Y. X. Wei, Z. Zhang, S.\nLin,  B.  N.  Guo.  Swin  transformer:  Hierarchical  vision\ntransformer  using  shifted  windows.  In  Proceedings of\nIEEE/CVF International Conference on Computer Vision,\nIEEE, Montreal, Canada, pp. 9992–10002, 2021. DOI: 10.\n1109/ICCV48922.2021.00986.\n[16]\n S. Kim, J. Kim, H. W. Chun. Wave2Vec: Vectorizing elec-\ntroencephalography bio-signal for prediction of brain dis-\nease.  International Journal of Environmental Research\nand Public Health,  vol. 15,  no. 8,  Article  number  1750,\n2018. DOI: 10.3390/ijerph15081750.\n[17]\n T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP.  Dhariwal,  A.  Neelakantan,  P.  Shyam,  G.  Sastry,  A.\nAskell,  S.  Agarwal,  A.  Herbert-Voss,  G.  Krueger,  T.\nHenighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C.\n[18]\nWinter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,\nI. Sutskever, D. Amodei. Language models are few-shot\nlearners. In Proceedings of the 34th International Confer-\nence on Neural Information Processing Systems,  Van-\ncouver, Canada, Article number 159, 2020. DOI: 10.5555/\n3495724.3495883.\n L. L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M.\nLaskin,  P.  Abbeel,  A.  Srinivas,  I.  Mordatch.  Decision\ntransformer: Reinforcement learning via sequence model-\ning.  [Online],  Available:  https://arxiv.org/abs/2106.\n01345, 2021.\n[19]\n Y. D. Yang, J. Luo, Y. Wen, O. Slumbers, D. Graves, H.\nbou  Ammar,  J.  Wang,  M.  E.  Taylor.  Diverse  auto-cur-\nriculum  is  critical  for  successful  real-world  multiagent\nlearning systems. In Proceedings of the 20th International\nConference on Autonomous Agents and Multi-agent Sys-\ntems, ACM, pp. 51–56, 2021.\n[20]\n N. Perez-Nieves, Y. D. Yang, O. Slumbers, D. H. Mguni,\nY.  Wen,  J.  Wang.  Modelling  behavioural  diversity  for\nlearning in open-ended games. In Proceedings of the 38th\nInternational Conference on Machine Learning, pp. 8514–\n8524, 2021.\n[21]\n X. Y. Liu, H. T. Jia, Y. Wen, Y. J. Hu, Y. F. Chen, C. J.\nFan, Z. P. Hu, Y. D. Yang. Unifying behavioral and re-\nsponse  diversity  for  open-ended  learning  in  zero-sum\ngames. In Proceedings of the 35th Conference on Neural\nInformation Processing Systems, pp. 941–952, 2021.\n[22]\n S. Levine, A. Kumar, G. Tucker, J. Fu. Offline reinforce-\nment learning: Tutorial, review, and perspectives on open\nproblems.  [Online],  Available:  https://arxiv.org/abs/\n2005.01643, 2020.\n[23]\n R.  Sanjaya,  J.  Wang,  Y.  D.  Yang.  Measuring  the  non-\ntransitivity  in  chess.  Algorithms,  vol. 15,  no. 5,  Article\nnumber 152, 2022. DOI: 10.3390/a15050152.\n[24]\n X. D. Feng, O. Slumbers, Y. D. Yang, Z. Y. Wan, B. Liu,\nS.  McAleer,  Y.  Wen,  J.  Wang.  Discovering  multi-agent\nauto-curricula  in  two-player  zero-sum  games.  [Online],\nAvailable: https://arxiv.org/abs/2106.02745, 2021.\n[25]\n M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N.\nNardelli, T. G. J. Rudner, C. M. Hung, P. H. S. Torr, J.\nFoerster,  S.  Whiteson.  The  StarCraft  multi-agent  chal-\nlenge. In Proceedings of the 18th International Conference\non Autonomous Agents and MultiAgent Systems,\nMontreal, Canada, pp. 2186–2188, 2019.\n[26]\n Z. Li, S. R. Xue, X. H. Yu, H. J Gao. Controller optimiza-\ntion for multirate systems based on reinforcement learn-\ning. International Journal of Automation and Computing,\nvol. 17, no. 3, pp. 417–427, 2020. DOI: 10.1007/s11633-020-\n1229-0.\n[27]\n Y. Li, D. Xu. Skill learning for robotic insertion based on\none-shot demonstration and reinforcement learning. Inter-\nnational Journal of Automation and Computing, vol. 18,\nno. 3, pp. 457–467, 2021. DOI: 10.1007/s11633-021-1290-3.\n[28]\n C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak,\nC. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, et\nal. Dota 2 with large scale deep reinforcement learning.\n[Online],  Available:  https://arxiv.org/abs/1912.06680,\n2019.\n[29]\n A. Kumar, J. Fu, G. Tucker, S. Levine. Stabilizing off-\npolicy  Q-learning  via  bootstrapping  error  reduction.  In\nProceedings of the 33rd Conference on Neural Information\nProcessing Systems, Vancouver, Canada, pp. 11761–11771,\n2019.\n[30]\n A. Kumar, A. Zhou, G. Tucker, S. Levine. Conservative Q-[31]\nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 245 \n \n\nlearning for offline reinforcement learning. In Proceedings\nof the 34th International Conference on Neural Informa-\ntion Processing Systems,  Vancouver,  Canada,  Article\nnumber 100, 2020. DOI: 10.5555/3495724.3495824.\n S. Fujimoto, D. Meger, D. Precup. Off-policy deep rein-\nforcement learning without exploration. In Proceedings of\nthe 36th International Conference on Machine Learning,\nLong Beach, USA, pp. 2052–2062, 2019.\n[32]\n T. Matsushima, H. Furuta, Y. Matsuo, O. Nachum, S. X.\nGu. Deployment-efficient reinforcement learning via mod-\nel-based offline optimization. In Proceedings of the 9th In-\nternational Conference on Learning Representations, 2021.\n[33]\n D. J. Su, J. D. Lee, J. M. Mulvey, H. V. Poor. MUSBO:\nModel-based uncertainty regularized and sample efficient\nbatch optimization for deployment constrained reinforce-\nment learning. [Online], Available: https://arxiv.org/abs/\n2102.11448, 2021.\n[34]\n Y. Q. Yang, X. T. Ma, C. H. Li, Z. W. Zheng, Q. Y. Zhang,\nG. Huang, J. Yang, Q. C. Zhao. Believe what you see: Im-\nplicit constraint approach for offline multi-agent reinforce-\nment learning. [Online], Available: https://arxiv.org/abs/\n2106.03400, 2021.\n[35]\n J. C. Jiang, Z. Q. Lu. Offline decentralized multi-agent re-\ninforcement  learning.  [Online],  Available:  https://arxiv.\norg/abs/2108.01832, 2021.\n[36]\n A. Nair, M. Dalal, A. Gupta, S. Levine. Accelerating on-\nline reinforcement learning with offline datasets. [Online],\nAvailable: https://arxiv.org/abs/2006.09359, 2020.\n[37]\n M.  Janner,  Q.  Y.  Li,  S.  Levine.  Offline  reinforcement\nlearning as one big sequence modeling problem. [Online],\nAvailable: https://arxiv.org/abs/2106.02039, 2021.\n[38]\n L. C. Dinh, Y. D. Yang, S. McAleer, Z. Tian, N. P. Nieves,\nO. Slumbers, D. H. Mguni, H. bou Ammar, J. Wang. On-\nline double oracle. [Online], Available: https://arxiv.org/\nabs/2103.07780, 2021.\n[39]\n D. H. Mguni, Y. T. Wu, Y. L. Du, Y. D. Yang, Z. Y.\nWang, M. N. Li, Y. Wen, J. Jennings, J. Wang. Learning\nin nonzero-sum stochastic games with potentials. In Pro-\nceedings of the 38th International Conference on Machine\nLearning, pp. 7688–7699, 2021.\n[40]\n Y. D. Yang, Y. Wen, J. Wang, L. H. Chen, K. Shao, D.\nMguni, W. N. Zhang. Multi-agent determinantal Q-learn-\ning. In Proceedings of the 37th International Conference\non Machine Learning, pp. 10757–10766, 2020.\n[41]\n T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J.\nFoerster, S. Whiteson. QMIX: Monotonic value function\nfactorisation for deep multi-agent reinforcement learning.\nIn  Proceedings of the 35th International Conference on\nMachine Learning,  Stockholm,  Sweden,  pp. 4295–4304,\n2018.\n[42]\n Y. Wen, Y. D. Yang, R. Luo, J. Wang, W. Pan. Probabil-\nistic  recursive  reasoning  for  multi-agent  reinforcement\nlearning. In Proceedings of the 7th International Confer-\nence on Learning Representations,  New  Orleans,  USA,\n2019.\n[43]\n Y. Wen, Y. D. Yang, J. Wang. Modelling bounded ration-\nality in multi-agent interactions by generalized recursive\nreasoning. In Proceedings of the 29th International Joint\nConference on Artificial Intelligence,  Yokohama,  Japan,\npp. 414–421, 2020. DOI: 10.24963/ijcai.2020/58.\n[44]\n S. Hu, F. D. Zhu, X. J. Chang, X. D. Liang. UPDeT: Uni-\nversal  multi-agent  reinforcement  learning  via  policy  de-\ncoupling with transformers. [Online], Available: https://\narxiv.org/abs/2101.08001, 2021.\n[45]\n K.  Son,  D.  Kim,  W.  J.  Kang,  D.  E.  Hostallero,  Y.  Yi.\nQTRAN: Learning to factorize with transformation for co-\noperative multi-agent reinforcement learning. In Proceed-\nings of the 36th International Conference on Machine\nLearning, Long Beach, USA, pp. 5887–5896, 2019.\n[46]\n J. G. Kuba, M. N. Wen, L. H. Meng, S. D. Gu, H. F.\nZhang, D. H. Mguni, J. Wang, Y. D. Yang. Settling the\nvariance of multi-agent policy gradients. In Proceedings of\nthe 35th Conference on Neural Information Processing\nSystems, pp. 13458–13470, 2021.\n[47]\n J. G. Kuba, R. Q. Chen, M. N. Wen, Y. Wen, F. L. Sun, J.\nWang, Y. D. Yang. Trust region policy optimisation in\nmulti-agent reinforcement learning. In Proceedings of the\n10th International Conference on Learning Representa-\ntions, 2022.\n[48]\n S.  D.  Gu,  J.  G.  Kuba,  M.  N.  Wen,  R.  Q.  Chen,  Z.  Y.\nWang, Z. Tian, J. Wang, A. Knoll, Y. D. Yang. Multi-\nagent constrained policy optimisation. [Online], Available:\nhttps://arxiv.org/abs/2110.02793, 2021.\n[49]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, I. Polosukhin. Attention is all you\nneed. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, Long Beach,\nUSA, pp. 6000–6010, 2017. DOI: 10.5555/3295222.3295349.\n[50]\n I. Sutskever, O. Vinyals, Q. V. Le. Sequence to sequence\nlearning with neural networks. In Proceedings of the 27th\nInternational Conference on Neural Information Pro-\ncessing Systems, Montreal, Canada, pp. 3104–3112, 2014.\nDOI: 10.5555/2969033.2969173.\n[51]\n Q. Wang, B. Li, T. Xiao, J. B. Zhu, C. L. Li, D. F. Wong,\nL. S. Chao. Learning deep transformer models for machine\ntranslation. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, Florence,\nItaly, pp. 1810–1822, 2019. DOI: 10.18653/v1/P19-1176.\n[52]\n L. H. Dong, S. Xu, B. Xu. Speech-transformer: A no-recur-\nrence sequence-to-sequence model for speech recognition.\nIn  Proceedings of IEEE International Conference on\nAcoustics, Speech and Signal Processing,  Calgary,\nCanada,  pp. 5884–5888,  2018.  DOI:  10.1109/ICASSP.\n2018.8462506.\n[53]\n K. Han, Y. H. Wang, H. T. Chen, X. H. Chen, J. Y. Guo,\nZ. H. Liu, Y. H. Tang, A. Xiao, C. J. Xu, Y. X. Xu, Z. H.\nYang, Y. M. Zhang, D. C. Tao. A survey on vision trans-\nformer.  [Online],  Available:  https://arxiv.org/abs/2012.\n12556, 2020.\n[54]\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. H. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G.\nHeigold, S. Gelly, J. Uszkoreit, N. Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In Proceedings of the 9th International Conference\non Learning Representations, 2020.\n[55]\n R.  J.  Williams.  Simple  statistical  gradient-following  al-\ngorithms  for  connectionist  reinforcement  learning.  Ma-\nchine Learning, vol. 8, no. 3, pp. 229–256, 1992. DOI: 10.\n1007/BF00992696.\n[56]\n I. Mordatch, P. Abbeel. Emergence of grounded composi-\ntional  language  in  multi-agent  populations.  In  Proceed-\nings of the 32nd AAAI Conference on Artificial Intelli-\ngence and Thirtieth Innovative Applications of Artificial\nIntelligence Conference and the 8th AAAI Symposium on\nEducational Advances in Artificial Intelligence, New Or-\nleans,  USA,  Article  number  183,  2018.  DOI:  10.5555/\n3504035.3504218.\n[57]\n C. Yu, A. Velu, E. Vinitsky, J. X. Gao, Y. Wang, A. Bay-\nen, Y. Wu. The surprising effectiveness of PPO in cooper-\native, multi-agent games. [Online], Available: https://arx-\n[58]\n 246 Machine Intelligence Research 20(2), April 2023\n \n\niv.org/abs/2103.01955, 2021.\n J.  Fu,  A.  Kumar,  O.  Nachum,  G.  Tucker,  S.  Levine.\nD4RL: Datasets for deep data-driven reinforcement learn-\ning. [Online], Available: https://arxiv.org/abs/2004.07219,\n2020.\n[59]\n Z. D. Zhu, K. X. Lin, A. K. Jain, J. Zhou. Transfer learn-\ning  in  deep  reinforcement  learning:  A  survey.  [Online],\nAvailable: https://arxiv.org/abs/2009.07888, 2020.\n[60]\n \n Linghui Meng received the B. Sc. degree\nin  rail  traffic  signal  control  from  Beijing\nJiao Tong University, China in 2019. He is\ncurrently a Ph.D. degree candidate in pat-\ntern recognition and intelligent system at\nboth  Institute  of  Automation,  Chinese\nAcademy  of  Sciences,  China,  and  Uni-\nversity  of  Chinese  Academy  of  Sciences,\nChina.\n     His  interests  include  theoretical  research  on  reinforcement\nlearning, pre-training models, multi-agent system and speech re-\ncognition.\n     E-mail: menglinghui2019@ia.ac.cn\n     ORCID iD: 0000-0002-5826-8072\n \n Muning Wen is currently a Ph.D. degree\ncandidate in computer science and techno-\nlogy  at  Shanghai  Jiao  Tong  University,\nChina.\n     His research interests include reinforce-\nment  learning,  deep  learning  and  multi-\nagent system.\n     E-mail: muning.wen@sjtu.edu.cn\n \n Chenyang  Le  is  an  undergraduate  in\ncomputer  science  and  technology  at\nShanghai Jiaotong University, China.\n     His research interests include reinforce-\nment learning, automatic speech recogni-\ntion and speech translation.\n     E-mail: nethermanpro@sjtu.edu.cn\n \n Xiyun Li is currently a Ph. D. degree can-\ndidate  in  pattern  recognition  and  intelli-\ngent  system  at  Institute  of  Automation,\nChinese Academy of Sciences, China.\n     His research interests include reinforce-\nment  learning  and  brain-inspired  cognit-\nive models.\n     E-mail: lixiyun2020@ia.ac.cn\n \n Dengpeng  Xing  received  the  B. Sc.  de-\ngree  in  mechanical  electronics  and  the\nM. Sc.  degree  in  mechanical  manufactur-\ning  and  automation  from  Tianjin  Uni-\nversity, China in 2002 and 2006, respect-\nively, and the Ph. D. degree in control sci-\nence and engineering from Shanghai Jiao\nTong University, China in 2010. He is cur-\nrently  an  associate  professor  in  the  Re-\nsearch Center for Brain-inspired Intelligence, Institute of Auto-\nmation, Chinese Academy of Sciences, China.\n     His  research  interests  include  reinforcement  learning  and\nbrain-inspired robotics.\n     E-mail: dengpeng.xing@ia.ac.cn \nWeinan  Zhang  received  the  B. Eng.  in\ncomputer  science  and  technology  from\nACM  Class  of  Shanghai  Jiao  Tong  Uni-\nversity, China in 2011, and the Ph. D. de-\ngree  in  computer  science  and  technology\nfrom  University  College  London,  UK  in\n2016. He is now a tenure-track associate\nprofessor  at  Shanghai  Jiao  Tong  Uni-\nversity, China. He has published over 150\nresearch papers on international conferences and journals and\nhas  been  serving  as  an  area  chair  or  (senior)  PC  member  at\nICML, NeurIPS, ICLR, KDD, AAAI, IJCAI, SIGIR, etc, and a\nreviewer at JMLR, TOIS, TKDE, TIST, etc.\n     His  research  interests  include  reinforcement  learning,  deep\nlearning and data science with various real-world applications of\nrecommender systems, search engines, text mining & generation,\nknowledge graphs and game AI.\n     E-mail: wnzhang@sjtu.edu.cn \nYing  Wen  received  the  B. Eng.  degree\nfrom Beijing University of Posts and Tele-\ncommunications,  China,  the  B. Eng.  de-\ngree  with  first  class  honor  from  Queen\nMary, University of London, UK in 2015,\nthe  M. Sc.  degree  with  distinction  honor\nfrom  University  College  London,  UK  in\n2016, and the Ph. D. degree from Depart-\nment of Computer Science, University Col-\nlege London, UK. is currently a tenure-track Assistant Professor\nwith the John Hopcroft Center for Computer Science, Shanghai\nJiao Tong University, China. He has published over 20 research\npapers about machine learning on top-tier international confer-\nences  (ICML,  NeurIPS,  ICLR,  IJCAI,  and  AAMAS).  He  has\nbeen serving as a PC member at ICML, NeurIPS, ICLR, AAAI,\nIJCAI, ICAPS and a reviewer at TIFS, Operational  Research,\netc. He was granted Best Paper Award in AAMAS 2021 Blue\nSky Track and Best System Paper Award in CoRL 2020.\n     His research interests include machine learning, multi-agent\nsystems and human-centered interactive systems.\n     E-mail: ying.wen@sjtu.edu.cn \nHaifeng Zhang received the B. Sc. degree\nin computer science and economics and the\nPh. D.  degree  in  computer  science  from\nPeking  University,  China  in  2012  and\n2018, respectively. He was a visiting scient-\nist at Center on Frontiers of Computing\nStudies  (CFCS),  Peking  University,\nChina. Earlier, he was a research fellow at\nUniversity College London, UK. He is an\nassociate professor at  Institute of Automation,  Chinese Academy\nof Sciences (CASIA), China.\n     He  has  published  research  papers  on  international  confer-\nences  ICML,  NeurIPS,  AAAI,  IJCAI,  AAMAS,  etc.  He  has\nserved as a Reviewer for AAAI, IJCAI, TNNLS, Acta Automat-\nica  Sinica, and Co-Chair for IJCAI competition, IJTCS, DAI\nWorkshop, etc.\n     His research interests include reinforcement learning, game\nAI, game theory and computational advertising.\n     E-mail: haifeng.zhang@ia.ac.cn \nL. Meng et al. / Offline Pre-trained Multi-agent Decision Transformer 247 \n \n\nJun Wang received the B. Sc. degree from\nSoutheast University, China in 1997, the\nM. Sc. degree from National University of\nSingapore,  Singapore  in  2003,  the  Ph. D.\ndegree  from  Delft  University  of  Techno-\nlogy,  The  Netherlands  in  2007.  is  cur-\nrently a professor with Computer Science\nDepartment,  University  College  London,\nUK.  He  has  published  over  200  research\narticles. His team won the First Global Real-Time Bidding Al-\ngorithm Contest with more than 80 participants worldwide. He\nwas the winner of multiple best paper awards. He was a recipi-\nent of the Beyond Search—Semantic Computing and Internet\nEconomics Award by Microsoft Research and the Yahoo! FREP\nFaculty Award. He has served as an Area Chair for ACM CIKM\nand ACM SIGIR. His recent service includes the Co-Chair for\nArtificial  Intelligence,  Semantics,  and  Dialog  in  ACM  SIGIR\n2018.\n     His research interests are in the areas of AI and intelligent sys-\ntems, covering (multiagent) reinforcement learning, deep gener-\native models, and their diverse applications on information re-\ntrieval, recommender systems and personalization, data mining,\nsmart cities, bot planning, and computational advertising.\n     E-mail: jun.wang@cs.ucl.ac.uk (Corresponding author)\n     ORCID iD: 0000-0001-9006-7951\n \n Yaodong  Yang  received  the  B. Sc.  de-\ngree in electronic engineering & informa-\ntion science from University of Science and\nTechnology of China, China in 2013, the\nM.Sc. degree in science (Quant. Biology/\nBiostatistics)  degree  from  Imperial  Col-\nlege London, UK 2014, and the Ph.D. de-\ngree in computer science from University\nCollege London, UK in 2021. He is a ma-\nchine learning researcher with ten-year working experience in\nboth academia and industry. Currently, he is an assistant pro-\nfessor at Peking University, China. Before joining Peking Uni-\nversity, he was an assistant professor at King′s College London,\nUK. Before KCL, he was a principal research scientist at Hua-\nwei UK. Before Huawei, he was a senior research manager at\nAIG, working on AI applications in finance. He has maintained a\ntrack record of more than forty publications at top conferences\nand journals, along with the Best System Paper Award at CoRL\n2020 and the Best Blue-sky Paper Award at AAMAS 2021.\n     His  research  interests  include  reinforcement  learning  and\nmulti-agent systems.\n     E-mail: yaodong.yang@pku.edu.cn \nBo Xu received the B. Sc. degree in elec-\ntrical  engineering  from  Zhejiang  Uni-\nversity, China in 1988, and the M. Sc. and\nPh. D. degrees in pattern recognition and\nintelligent system from Institute of Auto-\nmation,  Chinese  Academy  of  Sciences,\nChina in 1992 and 1997, respectively. He is\na  professor,  the  director  of  Institute  of\nAutomation,  Chinese  Academy  of  Sci-\nences, China, and also deputy director of Center for Excellence\nin Brain Science and Intelligence Technology, Chinese Academy\nof Sciences, China.\n     His  research  interests  include  brain-inspired  intelligence,\nbrain-inspired cognitive models, natural language processing and\nunderstanding, and brain-inspired robotics.\n     E-mail: xubo@ia.ac.cn (Corresponding author)\n     ORCID iD: 0000-0002-1111-1529\n 248 Machine Intelligence Research 20(2), April 2023\n \n",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8093050718307495
    },
    {
      "name": "Computer science",
      "score": 0.7570086717605591
    },
    {
      "name": "Transformer",
      "score": 0.6784180402755737
    },
    {
      "name": "Online and offline",
      "score": 0.5674434304237366
    },
    {
      "name": "Machine learning",
      "score": 0.5573837161064148
    },
    {
      "name": "Offline learning",
      "score": 0.5570682287216187
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5188178420066833
    },
    {
      "name": "Marl",
      "score": 0.4588596820831299
    },
    {
      "name": "Task (project management)",
      "score": 0.4277452230453491
    },
    {
      "name": "Generalization",
      "score": 0.41803044080734253
    },
    {
      "name": "Online learning",
      "score": 0.23910892009735107
    },
    {
      "name": "Engineering",
      "score": 0.12092176079750061
    },
    {
      "name": "Multimedia",
      "score": 0.10768535733222961
    },
    {
      "name": "Structural basin",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210094879",
      "name": "Shandong Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}