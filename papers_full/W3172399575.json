{
  "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
  "url": "https://openalex.org/W3172399575",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096181728",
      "name": "Yue Yu",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2903413907",
      "name": "Simiao Zuo",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2472586093",
      "name": "Haoming Jiang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2795917994",
      "name": "Wendi Ren",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2234658305",
      "name": "Tuo Zhao",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096826587",
      "name": "Chao Zhang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2959716049",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3037311587",
    "https://openalex.org/W4287815528",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3092571094",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2994976222",
    "https://openalex.org/W3127622310",
    "https://openalex.org/W3128654100",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3101606352",
    "https://openalex.org/W2581377246",
    "https://openalex.org/W2079057609",
    "https://openalex.org/W3035097673",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W2964159205",
    "https://openalex.org/W601603264",
    "https://openalex.org/W2056584528",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W2964074409",
    "https://openalex.org/W3099142828",
    "https://openalex.org/W2012273848",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W3103981637",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4235216760",
    "https://openalex.org/W2145287260",
    "https://openalex.org/W3012687255",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952831501",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3006963874",
    "https://openalex.org/W3122924117",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3007685714",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W3092642435",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W2911424454",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W3100445485",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4287614078",
    "https://openalex.org/W3104223418",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2970297748",
    "https://openalex.org/W2953070460",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2592691248",
    "https://openalex.org/W2606901057",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3034588688",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2157364932",
    "https://openalex.org/W2769041395",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W4287867774"
  ],
  "abstract": "Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, Chao Zhang. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1063‚Äì1077\nJune 6‚Äì11, 2021. ¬©2021 Association for Computational Linguistics\n1063\nFine-Tuning Pre-trained Language Model with Weak Supervision:\nA Contrastive-Regularized Self-Training Approach\nYue Yu‚àó Simiao Zuo‚àó Haoming Jiang Wendi Ren Tuo Zhao Chao Zhang\nGeorgia Institute of Technology, Atlanta, GA, USA\n{yueyu,simiaozuo,jianghm,wren44,tourzhao,chaozhang}@gatech.edu\nAbstract\nFine-tuned pre-trained language models (LMs)\nhave achieved enormous success in many nat-\nural language processing (NLP) tasks, but they\nstill require excessive labeled data in the Ô¨Åne-\ntuning stage. We study the problem of Ô¨Åne-\ntuning pre-trained LMs using only weak super-\nvision, without any labeled data. This prob-\nlem is challenging because the high capacity\nof LMs makes them prone to overÔ¨Åtting the\nnoisy labels generated by weak supervision.\nTo address this problem, we develop a con-\ntrastive self-training framework, COSINE, to\nenable Ô¨Åne-tuning LMs with weak supervision.\nUnderpinned by contrastive regularization and\nconÔ¨Ådence-based reweighting, our framework\ngradually improves model Ô¨Åtting while effec-\ntively suppressing error propagation. Experi-\nments on sequence, token, and sentence pair\nclassiÔ¨Åcation tasks show that our model outper-\nforms the strongest baseline by large margins\nand achieves competitive performance with\nfully-supervised Ô¨Åne-tuning methods. Our\nimplementation is available on https://\ngithub.com/yueyu1030/COSINE.\n1 Introduction\nLanguage model (LM) pre-training and Ô¨Åne-tuning\nachieve state-of-the-art performance in various nat-\nural language processing tasks (Peters et al., 2018;\nDevlin et al., 2019; Liu et al., 2019; Raffel et al.,\n2019). Such approaches stack task-speciÔ¨Åc layers\non top of pre-trained language models, e.g., BERT\n(Devlin et al., 2019), then Ô¨Åne-tune the models with\ntask-speciÔ¨Åc data. During Ô¨Åne-tuning, the semantic\nand syntactic knowledge in the pre-trained LMs is\nadapted for the target task. Despite their success,\none bottleneck for Ô¨Åne-tuning LMs is the require-\nment of labeled data. When labeled data are scarce,\nthe Ô¨Åne-tuned models often suffer from degraded\nperformance, and the large number of parameters\ncan cause severe overÔ¨Åtting (Xie et al., 2019).\n‚àóEqual Contribution.\nTo relieve the label scarcity bottleneck, we Ô¨Åne-\ntune the pre-trained language models with only\nweak supervision. While collecting large amounts\nof clean labeled data is expensive for many NLP\ntasks, it is often cheap to obtain weakly labeled\ndata from various weak supervision sources, such\nas semantic rules (Awasthi et al., 2020). For ex-\nample, in sentiment analysis, we can use rules\n‚Äòterrible‚Äô‚ÜíNegative (a keyword rule) and\n‚Äò* not recommend *‚Äô‚ÜíNegative (a pat-\ntern rule) to generate large amounts of weak labels.\nFine-tuning language models with weak supervi-\nsion is nontrivial. Excessive label noise,e.g., wrong\nlabels, and limited label coverage are common and\ninevitable in weak supervision. Although existing\nÔ¨Åne-tuning approaches (Xu et al., 2020; Zhu et al.,\n2020; Jiang et al., 2020) improve LMs‚Äô generaliza-\ntion ability, they are not designed for noisy data and\nare still easy to overÔ¨Åt on the noise. Moreover, ex-\nisting works on tackling label noise are Ô¨Çawed and\nare not designed for Ô¨Åne-tuning LMs. For exam-\nple, Ratner et al. (2020); Varma and R√© (2018) use\nprobabilistic models to aggregate multiple weak\nsupervisions for denoising, but they generate weak-\nlabels in a context-free manner, without using LMs\nto encode contextual information of the training\nsamples (Aina et al., 2019). Other works (Luo\net al., 2017; Wang et al., 2019b) focus on noise tran-\nsitions without explicitly conducting instance-level\ndenoising, and they require clean training samples.\nAlthough some recent studies (Awasthi et al., 2020;\nRen et al., 2020) design labeling function-guided\nneural modules to denoise each sample, they re-\nquire prior knowledge on weak supervision, which\nis often infeasible in real practice.\nSelf-training (Rosenberg et al., 2005; Lee, 2013)\nis a proper tool for Ô¨Åne-tuning language models\nwith weak supervision. It augments the training set\nwith unlabeled data by generating pseudo-labels for\nthem, which improves the models‚Äô generalization\npower. This resolves the limited coverage issue in\n1064\nweak supervision. However, one major challenge\nof self-training is that the algorithm still suffers\nfrom error propagation‚Äîwrong pseudo-labels can\ncause model performance to gradually deteriorate.\nWe propose a new algorithm COSINE1 that\nÔ¨Åne-tunes pre-trained LMs with only weak supervi-\nsion. COSINE leverages both weakly labeled and\nunlabeled data, as well as suppresses label noise\nvia contrastive self-training. Weakly-supervised\nlearning enriches data with potentially noisy labels,\nand our contrastive self-training scheme fulÔ¨Ålls the\ndenoising purpose. SpeciÔ¨Åcally, contrastive self-\ntraining regularizes the feature space by pulling\nsamples with the same pseudo-labels close while\npushing samples with different pseudo-labels apart.\nSuch regularization enforces representations of\nsamples from different classes to be more distin-\nguishable, such that the classiÔ¨Åer can make bet-\nter decisions. To suppress label noise propaga-\ntion during contrastive self-training, we propose\nconÔ¨Ådence-based sample reweighting and regular-\nization methods. The reweighting strategy em-\nphasizes samples with high prediction conÔ¨Ådence,\nwhich are more likely to be correctly classiÔ¨Åed,\nin order to reduce the effect of wrong predictions.\nConÔ¨Ådence regularization encourages smoothness\nover model predictions, such that no prediction\ncan be over-conÔ¨Ådent, and therefore reduces the\ninÔ¨Çuence of wrong pseudo-labels.\nOur model is Ô¨Çexible and can be naturally ex-\ntended to semi-supervised learning, where a small\nset of clean labels is available. Moreover, since we\ndo not make assumptions about the nature of the\nweak labels, COSINE can handle various types of\nlabel noise, including biased labels and randomly\ncorrupted labels. Biased labels are usually gener-\nated by semantic rules, whereas corrupted labels\nare often produced by crowd-sourcing.\nOur main contributions are: (1) A contrastive-\nregularized self-training framework that Ô¨Åne-tunes\npre-trained LMs with only weak supervision. (2)\nConÔ¨Ådence-based reweighting and regularization\ntechniques that reduce error propagation and pre-\nvent over-conÔ¨Ådent predictions. (3) Extensive ex-\nperiments on 6 NLP classiÔ¨Åcation tasks using 7\npublic benchmarks verifying the efÔ¨Åcacy of CO-\nSINE. We highlight that our model achieves com-\npetitive performance in comparison with fully-\nsupervised models on some datasets, e.g., on the\n1Short for Contrastive Self-Training for Fine-Tuning Pre-\ntrained Language Model.\nYelp dataset, we obtain a 97.2% (fully-supervised)\nv.s. 96.0% (ours) accuracy comparison.\n2 Background\nIn this section, we introduce weak supervision and\nour problem formulation.\nWeak Supervision. Instead of using human-\nannotated data, we obtain labels from weak super-\nvision sources, including keywords and semantic\nrules2. From weak supervision sources, each of the\ninput samples x‚ààX is given a label y‚ààY‚à™{‚àÖ} ,\nwhere Yis the label set and ‚àÖdenotes the sample\nis not matched by any rules. For samples that are\ngiven multiple labels, e.g., matched by multiple\nrules, we determine their labels by majority voting.\nProblem Formulation. We focus on the weakly-\nsupervised classiÔ¨Åcation problems in natural lan-\nguage processing. We consider three types of tasks:\nsequence classiÔ¨Åcation, token classiÔ¨Åcation, and\nsentence pair classiÔ¨Åcation. These tasks have a\nbroad scope of applications in NLP, and some ex-\namples can be found in Table 1.\nFormally, the weakly-supervised classiÔ¨Åcation\nproblem is deÔ¨Åned as the following: Given weakly-\nlabeled samples Xl = {(xi,yi)}L\ni=1 and unlabeled\nsamples Xu = {xj}U\nj=1, we seek to learn a classi-\nÔ¨Åer f(x; Œ∏) :X‚ÜíY . Here X= Xl ‚à™Xu denotes\nall the samples and Y= {1,2,¬∑¬∑¬∑ ,C}is the label\nset, where Cis the number of classes.\n3 Method\nOur classiÔ¨Åer f = g‚ó¶BERT consists of two parts:\nBERT is a pre-trained language model that outputs\nhidden representations of input samples, and gis\na task-speciÔ¨Åc classiÔ¨Åcation head that outputs a\nC-dimensional vector, where each dimension cor-\nresponds to the prediction conÔ¨Ådence of a speciÔ¨Åc\nclass. In this paper, we use RoBERTa (Liu et al.,\n2019) as the realization of BERT.\nThe framework ofCOSINE is shown in Figure 1.\nFirst, COSINE initializes the LM with weak labels.\nIn this step, the semantic and syntactic knowledge\nof the pre-trained LM are transferred to our model.\nThen, it uses contrastive self-training to suppress\nlabel noise propagation and continue training.\n3.1 Overview\nThe training procedure of COSINE is as follows.\nInitialization with Weakly-labeled Data. We\nÔ¨Åne-tune f(¬∑; Œ∏) with weakly-labeled data Xl by\n2Examples of weak supervisions are in Appendix A.\n1065\nWeak Supervision\nUnmatched Samples ùëã!InputCorpus\nMatched Samples ùëã\" ùëì(#;ùúÉ)InitializeBERTBERT\nLow-confSamples\nHigh-confSamples ùê∂\nAll Samplesùëã\"\nConfidence RegularizationùëÖ!---------Threshold ùúâ---------\nContrastiveLoss ùëÖ#Soft Pseudo-Label %ùë¶\nPredictionClassification Lossùêø$ Feature\nKnowledge Bases\nPatterns &Dictionaries\nSemanticRules\nSharpen\nFigure 1: The framework of COSINE. We Ô¨Årst Ô¨Åne-tune the pre-trained language model on weakly-labeled data\nwith early stopping. Then, we conduct contrastive-regularized self-training to improve model generalization and\nreduce the label noise. During self-training, we calculate the conÔ¨Ådence of the prediction and update the model\nwith high conÔ¨Ådence samples to reduce error propagation.\nFormulation Example Task Input Output\nSequence ClassiÔ¨Åcation Sentiment Analysis, Topic ClassiÔ¨Åcation,\nQuestion ClassiÔ¨Åcation [x1,..., xN ] y\nToken ClassiÔ¨Åcation Slot Filling, Part-of-speech Tagging,\nEvent Detection [x1,...,x N ] [ y1,...,y N ]\nSentence Pair ClassiÔ¨Åcation Word Sense Disambiguation, Textual Entailment,\nReading Comprehension [x1,x2] y\nTable 1: Comparison of different tasks. For sequence classiÔ¨Åcation, input is a sequence of sentences, and we output\na scalar label. For token classiÔ¨Åcation, input is a sequence of tokens, and we output one scalar label for each token.\nFor sentence pair classiÔ¨Åcation, input is a pair of sentences, and we output a scalar label.\nsolving the optimization problem\nmin\nŒ∏\n1\n|Xl|\n‚àë\n(xi,yi)‚ààXl\nCE (f(xi; Œ∏),yi) , (1)\nwhere CE(¬∑,¬∑) is the cross entropy loss. We adopt\nearly stopping (Dodge et al., 2020) to prevent the\nmodel from overÔ¨Åtting to the label noise. However,\nearly stopping causes underÔ¨Åtting, and we resolve\nthis issue by contrastive self-training.\nContrastive Self-training with All Data. The\ngoal of contrastive self-training is to leverage all\ndata, both labeled and unlabeled, for Ô¨Åne-tuning, as\nwell as to reduce the error propagation of wrongly\nlabelled data. We generate pseudo-labels for the\nunlabeled data and incorporate them into the train-\ning set. To reduce error propagation, we introduce\ncontrastive representation learning (Sec. 3.2) and\nconÔ¨Ådence-based sample reweighting and regular-\nization (Sec. 3.3). We update the pseudo-labels\n(denoted by Àúy) and the model iteratively. The pro-\ncedures are summarized in Algorithm 1.\n‚ãÑUpdate Àúy with the current Œ∏. To generate the\npseudo-label for each sample x‚ààX, one straight-\nforward way is to use hard labels (Lee, 2013)\nÀúyhard = argmax\nj‚ààY\n[f(x; Œ∏)]j . (2)\nNotice that f(x; Œ∏) ‚ààRC is a probability vector\nand [f(x; Œ∏)]j indicates the j-th entry of it. How-\never, these hard pseudo-labels only keep the most\nlikely class for each sample and result in the prop-\nagation of labeling mistakes. For example, if a\nsample is mistakenly classiÔ¨Åed to a wrong class,\nassigning a 0/1 label complicates model updating\n(Eq. 4), in that the model is Ô¨Åtted on erroneous\nlabels. To alleviate this issue, for each sample x\nin a batch B, we generate soft pseudo-labels3 (Xie\net al., 2016, 2019; Meng et al., 2020; Liang et al.,\n2020) Àúy ‚ààRC based on the current model as\nÀúyj =\n[f(x; Œ∏)]2\nj/fj\n‚àë\nj‚Ä≤‚ààY[f(x; Œ∏)]2\nj‚Ä≤/fj‚Ä≤\n, (3)\nwhere fj = ‚àë\nx‚Ä≤‚ààB[f(x‚Ä≤; Œ∏)]2\nj is the sum over soft\nfrequencies of class j. The non-binary soft pseudo-\nlabels guarantee that, even if our prediction is in-\naccurate, the error propagated to the model update\nstep will be smaller than using hard pseudo-labels.\n‚ãÑUpdate Œ∏ with the current Àúy. We update the\nmodel parameters Œ∏by minimizing\nL(Œ∏; Àúy) =Lc(Œ∏; Àúy) +R1(Œ∏; Àúy) +ŒªR2(Œ∏), (4)\nwhere Lc is the classiÔ¨Åcation loss (Sec. 3.3),\nR1(Œ∏; Àúy) is the contrastive regularizer (Sec. 3.2),\nR2(Œ∏) is the conÔ¨Ådence regularizer (Sec. 3.3), and\nŒªis the hyper-parameter for the regularization.\n3More discussions on hard vs.soft are in Sec. 4.5.\n1066\nAlgorithm 1: Training Procedures of COSINE.\nInput: Training samples X; Weakly labeled samples\nXl ‚äÜX; Pre-trained LM f(¬∑; Œ∏).\n// Fine-tune the LM with weakly-labeled data.\nfor t= 1,2,¬∑¬∑¬∑ ,T1 do\nSample a minibatch Bfrom Xl.\nUpdate Œ∏by Eq. 1 using AdamW.\n// Conduct contrastive self-training with all data.\nfor t= 1,2,¬∑¬∑¬∑ ,T2 do\nUpdate pseudo-labels Àúy by Eq. 3 for all x‚ààX.\nfor k= 1,2,¬∑¬∑¬∑ ,T3 do\nSample a minibatch Bfrom X.\nSelect high conÔ¨Ådence samples Cby Eq. 9.\nCalculate Lc by Eq. 10, R1 by Eq. 6, R2 by\nEq. 12, and Lby Eq. 4.\nUpdate Œ∏using AdamW.\nOutput: Fine-tuned model f(¬∑; Œ∏).\nContrastiveLearning\nHigh-confidence Sample PairsCompact Clusters of Samples \nFigure 2: An illustration of contrastive learning. The\nblack solid lines indicate similar sample pairs, and the\nred dashed lines indicate dissimilar pairs.\n3.2 Contrastive Learning on Sample Pairs\nThe key ingredient of our contrastive self-training\nmethod is to learn representations that encourage\ndata within the same class to have similar repre-\nsentations and keep data in different classes sepa-\nrated. SpeciÔ¨Åcally, we Ô¨Årst select high-conÔ¨Ådence\nsamples (Sec. 3.3) Cfrom X. Then for each pair\nxi,xj ‚ààC, we deÔ¨Åne their similarity as\nWij =\n{ 1, if argmax\nk‚ààY\n[Àúyi]k = argmax\nk‚ààY\n[Àúyj]k\n0, otherwise\n,\n(5)\nwhere Àúyi, Àúyj are the soft pseudo-labels (Eq. 3) for\nxi, xj, respectively. For each x‚ààC, we calculate\nits representation v = BERT(x) ‚ààRd, then we\ndeÔ¨Åne the contrastive regularizer as\nR1(Œ∏; Àúy) =\n‚àë\n(xi,xj)‚ààC√óC\n‚Ñì(vi,vj,Wij), (6)\nwhere\n‚Ñì= Wijd2\nij + (1‚àíWij)[max(0,Œ≥ ‚àídij)]2. (7)\nHere, ‚Ñì(¬∑,¬∑,¬∑) is the contrastive loss (Chopra et al.,\n2005; Taigman et al., 2014), dij is the distance 4\nbetween vi and vj, and Œ≥is a pre-deÔ¨Åned margin.\nFor samples from the same class, i.e. Wij = 1,\nEq. 6 penalizes the distance between them, and\n4We use scaled Euclidean distancedij = 1\nd ‚à•vi ‚àívj‚à•2\n2 by\ndefault. More discussions on Wij and dij are in Appendix E.\nfor samples from different classes, the contrastive\nloss is large if their distance is small. In this way,\nthe regularizer enforces similar samples to be close,\nwhile keeping dissimilar samples apart by at leastŒ≥.\nFigure 2 illustrates the contrastive representations.\nWe can see that our method produces clear inter-\nclass boundaries and small intra-class distances,\nwhich eases the classiÔ¨Åcation tasks.\n3.3 ConÔ¨Ådence-based Sample Reweighting\nand Regularization\nWhile contrastive representations yield better de-\ncision boundaries, they require samples with high-\nquality pseudo-labels. In this section, we introduce\nreweighting and regularization methods to suppress\nerror propagation and reÔ¨Åne pseudo-label qualities.\nSample Reweighting. In the classiÔ¨Åcation task,\nsamples with high prediction conÔ¨Ådence are more\nlikely to be classiÔ¨Åed correctly than those with\nlow conÔ¨Ådence. Therefore, we further reduce label\nnoise propagation by a conÔ¨Ådence-based sample\nreweighting scheme. For each sample xwith the\nsoft pseudo-label Àúy, we assign x with a weight\nœâ(x) deÔ¨Åned by\nœâ= 1‚àí H(Àúy)\nlog(C), H(Àúy) =‚àí\nC‚àë\ni=1\nÀúyilog Àúyi, (8)\nwhere 0 ‚â§H(Àúy) ‚â§log(C) is the entropy of Àúy.\nNotice that if the prediction conÔ¨Ådence is low, then\nH(Àúy) will be large, and the sample weight œâ(x)\nwill be small, and vice versa. We use a pre-deÔ¨Åned\nthreshold Œæ to select high conÔ¨Ådence samples C\nfrom each batch Bas\nC= {x‚ààB| œâ(x) ‚â•Œæ}. (9)\nThen we deÔ¨Åne the loss function as\nLc(Œ∏,Àúy) = 1\n|C|\n‚àë\nx‚ààC\nœâ(x)DKL (Àúy‚à•f(x; Œ∏)) , (10)\nwhere\nDKL(P‚à•Q) =\n‚àë\nk\npklog pk\nqk\n(11)\nis the Kullback‚ÄìLeibler (KL) divergence.\nConÔ¨Ådence regularization The sample reweight-\ning approach promotes high conÔ¨Ådence samples\nduring contrastive self-training. However, this strat-\negy relies on wrongly-labeled samples to have low\nconÔ¨Ådence, which may not be true unless we pre-\nvent over-conÔ¨Ådent predictions. To this end, we\npropose a conÔ¨Ådence-based regularizer that encour-\nages smoothness over predictions, deÔ¨Åned as\nR2(Œ∏) = 1\n|C|\n‚àë\nx‚ààC\nDKL (u‚à•f(x; Œ∏)) , (12)\n1067\nwhere DKL is the KL-divergence and ui = 1/C\nfor i= 1,2,¬∑¬∑¬∑ ,C. Such term constitutes a regu-\nlarization to prevent over-conÔ¨Ådent predictions and\nleads to better generalization (Pereyra et al., 2017).\n4 Experiments\nDatasets and Tasks. We conduct experiments on\n6 NLP classiÔ¨Åcation tasks using 7 public bench-\nmarks: AGNews (Zhang et al., 2015) is a Topic\nClassiÔ¨Åcation task; IMDB (Maas et al., 2011) and\nYelp (Meng et al., 2018) are Sentiment Analysis\ntasks; TREC (V oorhees and Tice, 1999) is a Ques-\ntion ClassiÔ¨Åcation task; MIT-R (Liu et al., 2013) is a\nSlot Filling task; Chemprot (Krallinger et al., 2017)\nis a Relation ClassiÔ¨Åcation task; and WiC (Pilehvar\nand Camacho-Collados, 2019) is a Word Sense Dis-\nambiguation (WSD) task. The dataset statistics are\nsummarized in Table 2. More details on datasets\nand weak supervision sources are in Appendix A5.\nBaselines. We compare our model with different\ngroups of baseline methods:\n(i) Exact Matching (ExMatch) : The test set is\ndirectly labeled by weak supervision sources.\n(ii) Fine-tuning Methods: The second group of\nbaselines are Ô¨Åne-tuning methods for LMs:\n‚ãÑRoBERTa (Liu et al., 2019) uses the RoBERTa-\nbase model with task-speciÔ¨Åc classiÔ¨Åcation heads.\n‚ãÑ Self-ensemble (Xu et al., 2020) uses self-\nensemble and distillation to improve performances.\n‚ãÑFreeLB(Zhu et al., 2020) adopts adversarial train-\ning to enforce smooth outputs.\n‚ãÑMixup (Zhang et al., 2018) creates virtual training\nsamples by linear interpolations.\n‚ãÑSMART (Jiang et al., 2020) adds adversarial\nand smoothness constraints to Ô¨Åne-tune LMs and\nachieves state-of-the-art result for many NLP tasks.\n(iii) Weakly-supervised Models: The third group\nof baselines are weakly-supervised models6:\n‚ãÑSnorkel (Ratner et al., 2020) aggregates different\nlabeling functions based on their correlations.\n‚ãÑWeSTClass (Meng et al., 2018) trains a classi-\nÔ¨Åer with generated pseudo-documents and use self-\ntraining to bootstrap over all samples.\n‚ãÑImplyLoss (Awasthi et al., 2020) co-trains a rule-\nbased classiÔ¨Åer and a neural classiÔ¨Åer to denoise.\n‚ãÑDenoise (Ren et al., 2020) uses attention network\nto estimate reliability of weak supervisions, and\nthen reduces the noise by aggregating weak labels.\n5Note that we use the same weak supervision signals/rules\nfor both our method and all the baselines for fair comparison.\n6All methods use RoBERTa-base as the backbone unless\notherwise speciÔ¨Åed.\n‚ãÑUST (Mukherjee and Awadallah, 2020) is state-\nof-the-art for self-training with limited labels. It\nestimates uncertainties via MC-dropout (Gal and\nGhahramani, 2015), and then select samples with\nlow uncertainties for self-training.\nEvaluation Metrics. We use classiÔ¨Åcation accu-\nracy on the test set as the evaluation metric for\nall datasets except MIT-R. MIT-R contains a large\nnumber of tokens that are labeled as ‚ÄúOthers‚Äù. We\nuse the micro F1 score from other classes for this\ndataset.7\nAuxiliary. We implement COSINE using Py-\nTorch8, and we use RoBERTa-base as the pre-\ntrained LM. Datasets and weak supervision de-\ntails are in Appendix A. Baseline settings are in\nAppendices B. Training details and setups are in\nAppendix C. Discussions on early-stopping are in\nAppendix D. Comparison of distance metrics and\nsimilarity measures are in Appendix E.\n4.1 Learning From Weak Labels\nWe summarize the weakly-supervised leaning re-\nsults in Table 3. In all the datasets, COSINE out-\nperforms all the baseline models. A special case is\nthe WiC dataset, where we use WordNet9 to gen-\nerate weak labels. However, this enables Snorkel\nto access some labeled data in the development set,\nmaking it unfair to compete against other methods.\nWe will discuss more about this dataset in Sec. 4.3.\nIn comparison with directly Ô¨Åne-tuning the pre-\ntrained LMs with weakly-labeled data, our model\nemploys an ‚Äúearlier stopping‚Äù technique10 so that\nit does not overÔ¨Åt on the label noise. As shown,\nindeed ‚ÄúInit‚Äù achieves better performance, and it\nserves as a good initialization for our framework.\nOther Ô¨Åne-tuning methods and weakly-supervised\nmodels either cannot harness the power of pre-\ntrained language models, e.g., Snorkel, or rely on\nclean labels, e.g., other baselines. We highlight\nthat although UST, the state-of-the-art method to\ndate, achieves strong performance under few-shot\nsettings, their approach cannot estimate conÔ¨Ådence\nwell with noisy labels, and this yields inferior per-\nformance. Our model can gradually correct wrong\npseudo-labels and mitigate error propagation via\ncontrastive self-training.\nIt is worth noticing that on some datasets, e.g.,\n7The Chemprot dataset also contains ‚ÄúOthers‚Äù type, but\nsuch instances are few, so we still use accuracy as the metric.\n8https://pytorch.org/\n9https://wordnet.princeton.edu/\n10We discuss this technique in Appendix D.\n1068\nDataset Task Class # Train # Dev # Test Cover Accuracy\nAGNews Topic 4 96k 12k 12k 56.4 83.1\nIMDB Sentiment 2 20k 2.5k 2.5k 87.5 74.5\nYelp Sentiment 2 30.4k 3.8k 3.8k 82.8 71.5\nMIT-R Slot Filling 9 6.6k 1.0k 1.5k 13.5 80.7\nTREC Question 6 4.8k 0.6k 0.6k 95.0 63.8\nChemprot Relation 10 12.6k 1.6k 1.6k 85.9 46.5\nWiC WSD 2 5.4k 0.6k 1.4k 63.4 58.8\nTable 2: Dataset statistics. Here cover (in %) is the fraction of instances covered by weak supervision sources in\nthe training set, and accuracy (in %) is the precision of weak supervision.\nMethod AGNews IMDB Yelp MIT-R TREC Chemprot WiC (dev)\nExMatch 52.31 71 .28 68 .68 34 .93 60 .80 46 .52 58 .80\nFully-supervised Result\nRoBERTa-CL‚ãÑ(Liu et al., 2019) 91.41 94 .26 97 .27 88 .51 96 .68 79 .65 70 .53\nBaselines\nRoBERTa-WL‚Ä†(Liu et al., 2019) 82.25 72 .60 74 .89 70 .95 62 .25 44 .80 59 .36\nSelf-ensemble (Xu et al., 2020) 85.72 86 .72 80 .08 72 .88 66 .18 44 .62 62 .71\nFreeLB (Zhu et al., 2020) 85.12 88 .04 85 .68 73 .04 67 .33 45 .68 63 .45\nMixup (Zhang et al., 2018) 85.40 86 .92 92 .05 73 .68 66 .83 51 .59 64 .88\nSMART (Jiang et al., 2020) 86.12 86 .98 88 .58 73 .66 68 .17 48 .26 63 .55\nSnorkel (Ratner et al., 2020) 62.91 73 .22 69 .21 20 .63 58 .60 37 .50 - - -‚àó\nWeSTClass (Meng et al., 2018) 82.78 77 .40 76 .86 - - -‚äó 37.31 - - -‚äó 48.59\nImplyLoss (Awasthi et al., 2020) 68.50 63 .85 76 .29 74 .30 80 .20 53 .48 54 .48\nDenoise (Ren et al., 2020) 85.71 82 .90 87 .53 70 .58 69 .20 50 .56 62 .38\nUST (Mukherjee and Awadallah, 2020) 86.28 84 .56 90 .53 74 .41 65 .52 52 .14 63 .48\nOurCOSINEFramework\nInit 84.63 83 .58 81 .76 72 .97 65 .67 51 .34 63 .46\nCOSINE 87.52 90.54 95.97 76.61 82.59 54.36 67.71\n‚ãÑ: RoBERTa is trained with clean labels. ‚Ä†: RoBERTa is trained with weak labels. ‚àó: unfair comparison. ‚äó: not applicable.\nTable 3: ClassiÔ¨Åcation accuracy (in %) on various datasets. We report the mean over three runs.\n40 50 60 70 80\nCorruption Ratio (in %)\n40\n60\n80\n100Accuracy (in %)\nInit\nSMART\nUST\nCOSINE\nFully supervised\nFigure 3: Results of label corruption on TREC. When\nthe corruption ratio is less than 40%, the performance\nis close to the fully supervised method.\nAGNews, IMDB, Yelp, and WiC, our model\nachieves the same level of performance with mod-\nels (RoBERTa-CL) trained with clean labels. This\nmakes COSINE appealing in the scenario where\nonly weak supervision is available.\n4.2 Robustness Against Label Noise\nOur model is robust against excessive label noise.\nWe corrupt certain percentage of labels by ran-\ndomly changing each one of them to another class.\nThis is a common scenario in crowd-sourcing,\nwhere we assume human annotators mis-label each\nsample with the same probability. Figure 3 summa-\nrizes experiment results on the TREC dataset. Com-\nModel Dev Test #Params\nHuman Baseline 80.0 - - -\nBERT (Devlin et al., 2019) - - - 69.6 335M\nRoBERTa (Liu et al., 2019) 70.5 69.9 356M\nT5 (Raffel et al., 2019) - - - 76.9 11,000M\nSemi-Supervised Learning\nSenseBERT (Levine et al., 2020) - - - 72.1 370M\nRoBERTa-WL‚Ä†(Liu et al., 2019) 72.3 70.2 125M\nw/ MT‚Ä†(Tarvainen and Valpola, 2017)73.5 70.9 125M\nw/ V AT‚Ä†(Miyato et al., 2018) 74.2 71.2 125M\nw/ COSINE‚Ä† 76.0 73.2 125M\nTransductive Learning\nSnorkel‚Ä†(Ratner et al., 2020) 80.5 - - - 1M\nRoBERTa-WL‚Ä†(Liu et al., 2019) 81.3 76.8 125M\nw/ MT‚Ä†(Tarvainen and Valpola, 2017)82.1 77.1 125M\nw/ V AT‚Ä†(Miyato et al., 2018) 84.9 79.5 125M\nw/ COSINE‚Ä† 89.5 85.3 125M\nTable 4: Semi-supervised Learning on WiC. VAT (Vir-\ntual Adversarial Training) and MT (Mean Teacher) are\nsemi-supervised methods. ‚Ä†: has access to weak labels.\npared with advanced Ô¨Åne-tuning and self-training\nmethods ( e.g. SMART and UST) 11, our model\nconsistently outperforms the baselines.\n4.3 Semi-supervised Learning\nWe can naturally extend our model to semi-\nsupervised learning, where clean labels are avail-\n11Note that some methods in Table 3, e.g., ImplyLoss and\nDenoise, are not applicable to this setting since they require\nweak supervision sources, but none exists in this setting.\n1069\nable for a portion of the data. We conduct exper-\niments on the WiC dataset. As a part of the Su-\nperGLUE (Wang et al., 2019a) benchmark, this\ndataset proposes a challenging task: models need\nto determine whether the same word in different\nsentences has the same sense (meaning).\nDifferent from previous tasks where the labels\nin the training set are noisy, in this part, we utilize\nthe clean labels provided by the WiC dataset. We\nfurther augment the original training data of WiC\nwith unlabeled sentence pairs obtained from lexi-\ncal databases (e.g., WordNet, Wictionary). Note\nthat part of the unlabeled data can be weakly-\nlabeled by rule matching. This essentially creates a\nsemi-supervised task, where we have labeled data,\nweakly-labeled data and unlabeled data.\nSince the weak labels of WiC are generated by\nWordNet and partially reveal the true label informa-\ntion, Snorkel (Ratner et al., 2020) takes this unfair\nadvantage by accessing the unlabeled sentences\nand weak labels of validation and test data. To\nmake a fair comparison to Snorkel, we consider the\ntransductive learning setting, where we are allowed\naccess to the same information by integrating unla-\nbeled validation and test data and their weak labels\ninto the training set. As shown in Table 4, CO-\nSINE with transductive learning achieves better\nperformance compared with Snorkel. Moreover,\nin comparison with semi-supervised baselines (i.e.\nV AT and MT) and Ô¨Åne-tuning methods with extra\nresources (i.e., SenseBERT), COSINE achieves\nbetter performance in both semi-supervised and\ntransductive learning settings.\n4.4 Case Study\nError propagation mitigation and wrong-label\ncorrection. Figure 4 visualizes this process. Be-\nfore training, the semantic rules make noisy predic-\ntions. After the initialization step, model predic-\ntions are less noisy but more biased,e.g., many sam-\nples are mis-labeled as ‚ÄúAmenity‚Äù. These predic-\ntions are further reÔ¨Åned by contrastive self-training.\nThe rightmost Ô¨Ågure demonstrates wrong-label cor-\nrection. Samples are indicated by radii of the circle,\nand classiÔ¨Åcation correctness is indicated by color,\ni.e., blue means correct and orange means incorrect.\nFrom inner to outer tori specify classiÔ¨Åcation accu-\nracy after the initialization stage, and the iteration\n1,2,3. We can see that many incorrect predictions\nare corrected within three iterations. To illustrate:\nthe right black dashed line means the corresponding\nsample is classiÔ¨Åed correctly after the Ô¨Årst iteration,\nand the left dashed line indicates the case where the\nsample is mis-classiÔ¨Åed after the second iteration\nbut corrected after the third. These results demon-\nstrate that our model can correct wrong predictions\nvia contrastive self-training.\nBetter data representations. We visualize sam-\nple embeddings in Fig. 7. By incorporating the\ncontrastive regularizer R1, our model learns more\ncompact representations for data in the same class,\ne.g., the green class, and also extends the inter-class\ndistances, e.g., the purple class is more separable\nfrom other classes in Fig. 7(b) than in Fig. 7(a).\nLabel efÔ¨Åciency. Figure 8 illustrates the number\nof clean labels needed for the supervised model to\noutperform COSINE. On both of the datasets, the\nsupervised model requires a signiÔ¨Åcant amount of\nclean labels (around 750 for Agnews and 120 for\nMIT-R) to reach the level of performance as ours,\nwhereas our method assumes no clean sample.\nHigher ConÔ¨Ådence Indicates Better Accuracy.\nFigure 6 demonstrates the relation between predic-\ntion conÔ¨Ådence and prediction accuracy on IMDB.\nWe can see that in general, samples with higher\nprediction conÔ¨Ådence yield higher prediction ac-\ncuracy. With our sample reweighting method, we\ngradually Ô¨Ålter out low-conÔ¨Ådence samples and as-\nsign higher weights for others, which effectively\nmitigates error propagation.\n4.5 Ablation Study\nComponents of COSINE. We inspect the impor-\ntance of various components, including the con-\ntrastive regularizer R1, the conÔ¨Ådence regularizer\nR2, and the sample reweighting (SR) method, and\nthe soft labels. Table 5 summarizes the results and\nFig. 9 visualizes the learning curves. We remark\nthat all the components jointly contribute to the\nmodel performance, and removing any of them\nhurts the classiÔ¨Åcation accuracy. For example, sam-\nple reweighting is an effective tool to reduce error\npropagation, and removing it causes the model to\neventually overÔ¨Åt to the label noise,e.g., the red bot-\ntom line in Fig. 9 illustrates that the classiÔ¨Åcation\naccuracy increases and then drops rapidly. On the\nother hand, replacing the soft pseudo-labels (Eq. 3)\nwith the hard counterparts (Eq. 2) causes drops in\nperformance. This is because hard pseudo-labels\nlose prediction conÔ¨Ådence information.\nHyper-parameters of COSINE. In Fig. 5, we ex-\namine the effects of different hyper-parameters,\nincluding the conÔ¨Ådence threshold Œæ (Eq. 9), the\n1070\nInitIter 1Iter 2Iter 3\nCorrectIncorrect\nFigure 4: ClassiÔ¨Åcation performance on MIT-R. From left to right: visualization of ExMatch, results after the\ninitialization step, results after contrastive self-training, and wrong-label correction during self-training.\n0.4 0.5 0.6 0.7 0.8\n80\n85\n90\n95Accuracy (in %)\nAGNews IMDB\n(a) Effect of Œæ.\n80 120 160 240 320 400\nT1 / Steps (IMDB)\n70\n80\n90Accuracy (in %)\nInit\nCOSINE\nClean label (b) Effect of T1.\n10 50 150 250 350 450\nT3 / Steps\n84\n86\n88\n90\n92Accuracy (in %)\nAGNews IMDB (c) Effect of T3.\nFigure 5: Effects of different hyper-parameters.\n0 0.2 0.4 0.6 0.8 1\nConfidence Score (IMDB)\n20\n40\n60\n80\n100Accuracy (in %)\nFigure 6: Accuracy vs. Con-\nÔ¨Ådence score.\n(a) Embedding w/o R1.\n (b) Embedding w/ R1.\nFigure 7: t-SNE (Maaten and Hinton, 2008) visualiza-\ntion on TREC. Each color denotes a different class.\n0 200 500 1000 1500\n# of Human Annotated Samples\n80\n82\n84\n86\n88\n90Accuracy (in %)\nSupervised w/ Roberta\nWeak Labels w/ COSINE\nWeak Labels w/ Init\n(a) Results on Agnews.\n0 50 100 200 300\n# of Human Annotated Samples\n40\n50\n60\n70\n80\n90Accuracy (in %)\nSupervised w/ Roberta\nWeak Labels w/ COSINE\nWeak Labels w/ Init (b) Results on MIT-R.\nFigure 8: Accuracy vs. Number of annotated labels.\nstopping time T1 in the initialization step, and the\nupdate period T3 for pseudo-labels. From Fig. 5(a),\nwe can see that setting the conÔ¨Ådence threshold\ntoo big hurts model performance, which is because\nan over-conservative selection strategy can result\nin insufÔ¨Åcient number of training data. The stop-\nping time T1 has drastic effects on the model. This\nis because Ô¨Åne-tuning COSINE with weak labels\nfor excessive steps causes the model to unavoid-\nably overÔ¨Åt to the label noise, such that the con-\ntrastive self-training procedure cannot correct the\n0 500 1000 1500 2000 2500\nNumber of Iterations\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy COSINE\nw/o R1\nw/o R2\nw/o SR\nw/o R1/R2\nw/o R1/R2/SR\nDirect Training\nFigure 9: Learning curves on TREC with different set-\ntings. Mean and variance are calculated over 3 runs.\nMethod AGNews IMDB Yelp MIT-R TREC\nInit 84.63 83.58 81.76 72.97 66.50\nCOSINE 87.52 90.54 95.97 76.61 82.59\nw/o R1 86.04 88.32 94.64 74.11 78.28\nw/o R2 85.91 89.32 93.96 75.21 77.11\nw/o SR 86.72 87.10 93.08 74.29 79.77\nw/o R1/R2 86.33 84.44 92.34 73.67 76.95\nw/o R1/R2/SR 86.61 83.98 82.57 73.59 74.96\nw/o Soft Label 86.07 89.72 93.73 73.05 71.91\nTable 5: Effects of different components. Due to space\nlimit we only show results for 5 representative datasets.\nerror. Also, with the increment of T3, the update\nperiod of pseudo-labels, model performance Ô¨Årst\nincreases and then decreases. This is because if we\nupdate pseudo-labels too frequently, the contrastive\nself-training procedure cannot fully suppress the\nlabel noise, and if the updates are too infrequent,\nthe pseudo-labels cannot capture the updated infor-\nmation well.\n1071\n5 Related Works\nFine-tuning Pre-trained Language Models. To\nimprove the model‚Äôs generalization power dur-\ning Ô¨Åne-tuning stage, several methods are pro-\nposed (Peters et al., 2019; Dodge et al., 2020; Zhu\net al., 2020; Jiang et al., 2020; Xu et al., 2020; Kong\net al., 2020; Zhao et al., 2020; Gunel et al., 2021;\nZhang et al., 2021; Aghajanyan et al., 2021; Wang\net al., 2021), However, most of these methods fo-\ncus on fully-supervised setting and rely heavily on\nlarge amounts of clean labels, which are not al-\nways available. To address this issue, we propose a\ncontrastive self-training framework that Ô¨Åne-tunes\npre-trained models with only weak labels. Com-\npared with the existing Ô¨Åne-tuning approaches (Xu\net al., 2020; Zhu et al., 2020; Jiang et al., 2020),\nour model effectively reduce the label noise, which\nachieves better performance on various NLP tasks\nwith weak supervision.\nLearning From Weak Supervision. In weakly-\nsupervised learning, the training data are usually\nnoisy and incomplete. Existing methods aim to\ndenoise the sample labels or the labeling functions\nby, for example, aggregating multiple weak super-\nvisions (Ratner et al., 2020; Lison et al., 2020;\nRen et al., 2020), using clean samples (Awasthi\net al., 2020), and leveraging contextual informa-\ntion (Mekala and Shang, 2020). However, most of\nthem can only use speciÔ¨Åc type of weak supervi-\nsion on speciÔ¨Åc task, e.g., keywords for text clas-\nsiÔ¨Åcation (Meng et al., 2020; Mekala and Shang,\n2020), and they require prior knowledge on weak\nsupervision sources (Awasthi et al., 2020; Lison\net al., 2020; Ren et al., 2020), which somehow\nlimits the scope of their applications. Our work\nis orthogonal to them since we do not denoise the\nlabeling functions directly. Instead, we adopt con-\ntrastive self-training to leverage the power of pre-\ntrained language models for denoising, which is\ntask-agnostic and applicable to various NLP tasks\nwith minimal additional efforts.\n6 Discussions\nAdaptation of LMs to Different Domains.When\nÔ¨Åne-tuning LMs on data from different domains,\nwe can Ô¨Årst continue pre-training on in-domain\ntext data for better adaptation (Gururangan et al.,\n2020). For some rare domains where BERT trained\non general domains is not optimal, we can use\nLMs pretrained on those speciÔ¨Åc domains ( e.g.\nBioBERT (Lee et al., 2020), SciBERT (Beltagy\net al., 2019)) to tackle this issue.\nScalability of Weak Supervision. COSINE can\nbe applied to tasks with a large number of classes.\nThis is because rules can be automatically gener-\nated beyond hand-crafting. For example, we can\nuse label names/descriptions as weak supervision\nsignals (Meng et al., 2020). Such signals are easy to\nobtain and do not require hand-crafted rules. Once\nweak supervision is provided, we can create weak\nlabels to further apply COSINE.\nFlexibility. COSINE can handle tasks and weak\nsupervision sources beyond our conducted exper-\niments. For example, other than semantic rules,\ncrowd-sourcing can be another weak supervision\nsource to generate pseudo-labels (Wang et al.,\n2013). Moreover, we only conduct experiments\non several representative tasks, but our framework\ncan be applied to other tasks as well, e.g., named-\nentity recognition (token classiÔ¨Åcation) and reading\ncomprehension (sentence pair classiÔ¨Åcation).\n7 Conclusion\nIn this paper, we propose a contrastive regular-\nized self-training framework, COSINE, for Ô¨Åne-\ntuning pre-trained language models with weak su-\npervision. Our framework can learn better data\nrepresentations to ease the classiÔ¨Åcation task, and\nalso efÔ¨Åciently reduce label noise propagation by\nconÔ¨Ådence-based reweighting and regularization.\nWe conduct experiments on various classiÔ¨Åcation\ntasks, including sequence classiÔ¨Åcation, token clas-\nsiÔ¨Åcation, and sentence pair classiÔ¨Åcation, and the\nresults demonstrate the efÔ¨Åcacy of our model.\nBroader Impact\nCOSINE is a general framework that tackled the la-\nbel scarcity issue via combining neural nets with\nweak supervision. The weak supervision provides\na simple but Ô¨Çexible language to encode the domain\nknowledge and capture the correlations between\nfeatures and labels. When combined with unla-\nbeled data, our framework can largely tackle the\nlabel scarcity bottleneck for training DNNs, en-\nabling them to be applied for downstream NLP\nclassiÔ¨Åcation tasks in a label efÔ¨Åcient manner.\nCOSINE neither introduces any social/ethical\nbias to the model nor amplify any bias in the data.\nIn all the experiments, we use publicly available\ndata, and we build our algorithms using public\ncode bases. We do not foresee any direct social\nconsequences or ethical issues.\n1072\nAcknowledgments\nWe thank anonymous reviewers for their feedbacks.\nThis work was supported in part by the National\nScience Foundation award III-2008334, Amazon\nFaculty Award, and Google Faculty Award.\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021. Better Ô¨Åne-tuning by reducing representa-\ntional collapse. In International Conference on\nLearning Representations.\nLaura Aina, Kristina Gulordava, and Gemma Boleda.\n2019. Putting words in context: LSTM language\nmodels and lexical ambiguity. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3342‚Äì3348.\nAbhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal,\nand Sunita Sarawagi. 2020. Learning from rules\ngeneralizing labeled exemplars. In International\nConference on Learning Representations.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiÔ¨Åc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615‚Äì\n3620.\nSumit Chopra, Raia Hadsell, and Yann LeCun. 2005.\nLearning a similarity metric discriminatively, with\napplication to face veriÔ¨Åcation. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR‚Äô05), volume 1, pages 539‚Äì546.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. CoRR, abs/2002.06305.\nYarin Gal and Zoubin Ghahramani. 2015. Bayesian\nconvolutional neural networks with bernoulli\napproximate variational inference. CoRR,\nabs/1506.02158.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Veselin\nStoyanov. 2021. Supervised contrastive learning for\npre-trained language model Ô¨Åne-tuning. In Interna-\ntional Conference on Learning Representations.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342‚Äì8360.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efÔ¨Åcient Ô¨Åne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2177‚Äì2190.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model Ô¨Åne-tuning for in- and out-of-\ndistribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1326‚Äì1340.\nMartin Krallinger, Obdulia Rabal, Saber A Akhondi,\net al. 2017. Overview of the biocreative VI\nchemical-protein interaction track. In Proceedings\nof the sixth BioCreative challenge evaluation work-\nshop, volume 1, pages 141‚Äì146.\nDong-Hyun Lee. 2013. Pseudo-label: The simple and\nefÔ¨Åcient semi-supervised learning method for deep\nneural networks. In Workshop on challenges in rep-\nresentation learning, ICML, volume 3, page 2.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234‚Äì1240.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. SenseBERT:\nDriving some sense into BERT. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4656‚Äì4667.\nChen Liang, Yue Yu, Haoming Jiang, Siawpeng Er,\nRuijia Wang, Tuo Zhao, and Chao Zhang. 2020.\nBond: Bert-assisted open-domain named entity\nrecognition with distant supervision. In Proceed-\nings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, page\n1054‚Äì1064.\nPierre Lison, Jeremy Barnes, Aliaksandr Hubin, and\nSamia Touileb. 2020. Named entity recognition\nwithout labelled data: A weak supervision approach.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n1518‚Äì1533.\nJingjing Liu, Panupong Pasupat, Yining Wang, Scott\nCyphers, and James R. Glass. 2013. Query un-\nderstanding enhanced by hierarchical parsing struc-\ntures. In 2013 IEEE Workshop on Automatic\n1073\nSpeech Recognition and Understanding , pages 72‚Äì\n77. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nBingfeng Luo, Yansong Feng, Zheng Wang, Zhanxing\nZhu, Songfang Huang, Rui Yan, and Dongyan Zhao.\n2017. Learning with noise: Enhance distantly super-\nvised relation extraction with dynamic transition ma-\ntrix. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 430‚Äì439.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142‚Äì150.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579‚Äì2605.\nDheeraj Mekala and Jingbo Shang. 2020. Contextual-\nized weak supervision for text classiÔ¨Åcation. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 323‚Äì333.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei\nHan. 2018. Weakly-supervised neural text classiÔ¨Åca-\ntion. In Proceedings of the 27th ACM International\nConference on Information and Knowledge Manage-\nment, page 983‚Äì992.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\nclassiÔ¨Åcation using label names only: A language\nmodel self-training approach. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9006‚Äì9017.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii. 2018. Virtual adversarial training:\na regularization method for supervised and semi-\nsupervised learning. IEEE transactions on pat-\ntern analysis and machine intelligence, 41(8):1979‚Äì\n1993.\nSubhabrata Mukherjee and Ahmed Hassan Awadallah.\n2020. Uncertainty-aware self-training for text clas-\nsiÔ¨Åcation with few labels. CoRR, abs/2006.15315.\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Regu-\nlarizing neural networks by penalizing conÔ¨Ådent out-\nput distributions. CoRR, abs/1701.06548.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227‚Äì\n2237.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP (RepL4NLP-2019), pages 7‚Äì14.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning representa-\ntions. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n1267‚Äì1273.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nAlexander Ratner, Stephen H. Bach, Henry R. Ehren-\nberg, Jason A. Fries, Sen Wu, and Christopher R√©.\n2020. Snorkel: rapid training data creation with\nweak supervision. VLDB Journal, 29(2):709‚Äì730.\nWendi Ren, Yinghao Li, Hanting Su, David Kartchner,\nCassie Mitchell, and Chao Zhang. 2020. Denoising\nmulti-source weak supervision for neural text classi-\nÔ¨Åcation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3739‚Äì3754.\nChuck Rosenberg, Martial Hebert, and Henry Schnei-\nderman. 2005. Semi-supervised self-training of ob-\nject detection models. WACV/MOTION, 2.\nYaniv Taigman, Ming Yang, Marc‚ÄôAurelio Ranzato,\nand Lior Wolf. 2014. Deepface: Closing the gap\nto human-level performance in face veriÔ¨Åcation. In\nThe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nAntti Tarvainen and Harri Valpola. 2017. Mean teach-\ners are better role models: Weight-averaged consis-\ntency targets improve semi-supervised deep learning\nresults. In Advances in Neural Information Process-\ning Systems, pages 1195‚Äì1204.\nParoma Varma and Christopher R√©. 2018. Snuba: Au-\ntomating weak supervision to label training data.\nProc. VLDB Endowment, 12(3):223‚Äì236.\nEllen M. V oorhees and Dawn M. Tice. 1999. The\nTREC-8 question answering track evaluation. In\nProceedings of The Eighth Text REtrieval Confer-\nence, volume 500-246.\n1074\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, pages 3261‚Äì3275.\nAobo Wang, Cong Duy Vu Hoang, and Min-Yen\nKan. 2013. Perspectives on crowdsourcing annota-\ntions for natural language processing. Language re-\nsources and evaluation, 47(1):9‚Äì31.\nBoxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan,\nRuoxi Jia, Bo Li, and Jingjing Liu. 2021. InfoBERT:\nImproving robustness of language models from an\ninformation theoretic perspective. In International\nConference on Learning Representations.\nHao Wang, Bing Liu, Chaozhuo Li, Yan Yang, and\nTianrui Li. 2019b. Learning with noisy labels for\nsentence-level sentiment classiÔ¨Åcation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 6286‚Äì6292.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. 2016.\nUnsupervised deep embedding for clustering analy-\nsis. In Proceedings of The 33rd International Con-\nference on Machine Learning, pages 478‚Äì487.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang\nLuong, and Quoc V . Le. 2019. Unsupervised\ndata augmentation for consistency training. CoRR,\nabs/1904.12848.\nYige Xu, Xipeng Qiu, Ligao Zhou, and Xuan-\njing Huang. 2020. Improving BERT Ô¨Åne-tuning\nvia self-ensemble and self-distillation. CoRR,\nabs/2002.10345.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin,\nand David Lopez-Paz. 2018. mixup: Beyond empir-\nical risk minimization. In International Conference\non Learning Representations.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT Ô¨Åne-tuning. In International Confer-\nence on Learning Representations.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiÔ¨Åcation. In Advances in Neural Information Pro-\ncessing Systems 28, pages 649‚Äì657.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Sch√ºtze. 2020. Masking as an efÔ¨Åcient alter-\nnative to Ô¨Ånetuning for pretrained language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226‚Äì2241.\nWenxuan Zhou, Hongtao Lin, Bill Yuchen Lin, Ziqi\nWang, Junyi Du, Leonardo Neves, and Xiang Ren.\n2020. NERO: A neural rule grounding framework\nfor label-efÔ¨Åcient relation extraction. In Proceed-\nings of The Web Conference 2020, page 2166‚Äì2176.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced ad-\nversarial training for natural language understanding.\nIn International Conference on Learning Represen-\ntations.\n1075\nA Weak Supervision Details\nCOSINE does not require any human annotated\nexamples in the training process, and it only needs\nweak supervision sources such as keywords and\nsemantic rules. According to some studies in exist-\ning works Awasthi et al. (2020); Zhou et al. (2020),\nsuch weak supervisions are cheap to obtain and are\nmuch efÔ¨Åcient than collecting clean labels. In this\nway, we can obtain signiÔ¨Åcantly more labeled ex-\namples using these weak supervision sources than\nhuman labor.\nThere are two types of semantic rules that we\napply as weak supervisions:\n‚ãÑKeyword Rule : HAS(x, L) ‚Üí C. If x\nmatches one of the words in the list L, we\nlabel it as C.\n‚ãÑPattern Rule: MATCH(x, R) ‚Üí C. If x\nmatches the regular expression R, we label\nit as C.\nIn addition to the keyword rule and the pattern rule,\nwe can also use third-party tools to obtain weak\nlabels. These tools (e.g. TextBlob12) are available\nonline and can be obtained cheaply, but their pre-\ndiction is not accurate enough (when directly use\nthis tool to predict label for all training samples,\nthe accuracy on Yelp dataset is around 60%).\nWe now introduce the semantic rules on each\ndataset:\n‚ãÑAGNews, IMDB, Yelp: We use the rule in Ren\net al. (2020). Please refer to the original paper\nfor detailed information on rules.\n‚ãÑMIT-R, TREC: We use the rule in Awasthi et al.\n(2020). Please refer to the original paper for\ndetailed information on rules.\n‚ãÑChemProt: There are 26 rules. We show part\nof the rules in Table 6.\n‚ãÑWiC: Each sense of each word in WordNet has\nexample sentences. For each sentence in the\nWiC dataset and its corresponding keyword,\nwe collect the example sentences of that word\nfrom WordNet. Then for a pair of sentences,\nthe corresponding weak label is ‚ÄúTrue‚Äù if their\ndeÔ¨Ånitions are the same, otherwise the weak\nlabel is ‚ÄúFalse‚Äù.\n12https://textblob.readthedocs.io/en/\ndev/index.html.\nB Baseline Settings\nWe implement Self-ensemble, FreeLB, Mixup\nand UST based on their original paper. For other\nbaselines, we use their ofÔ¨Åcial release:\n‚ãÑ WeSTClass (Meng et al., 2018): https:\n//github.com/yumeng5/WeSTClass.\n‚ãÑ RoBERTa (Liu et al., 2019): https:\n//github.com/huggingface/transformers.\n‚ãÑ SMART (Jiang et al., 2020): https:\n//github.com/namisan/mt-dnn.\n‚ãÑ Snorkel (Ratner et al., 2020): https:\n//www.snorkel.org/.\n‚ãÑ ImplyLoss (Awasthi et al., 2020): https:\n//github.com/awasthiabhijeet/\nLearning-From-Rules.\n‚ãÑ Denoise (Ren et al., 2020): https:\n//github.com/weakrules/\nDenoise-multi-weak-sources .\nC Details on Experiment Setups\nC.1 Computing Infrastructure\nSystem: Ubuntu 18.04.3 LTS; Python 3.7; Pytorch\n1.2. CPU: Intel(R) Core(TM) i7-5930K CPU @\n3.50GHz. GPU: GeForce GTX TITAN X.\nC.2 Hyper-parameters\nWe use AdamW (Loshchilov and Hutter, 2019) as\nthe optimizer, and the learning rate is chosen from\n1 √ó10‚àí5,2 √ó10‚àí5,3 √ó10‚àí5}. A linear learning\nrate decay schedule with warm-up 0.1 is used, and\nthe number of training epochs is 5.\nHyper-parameters are shown in Table 7. We use\na grid search to Ô¨Ånd the optimal setting for each\ntask. SpeciÔ¨Åcally, we search T1 from 10 to 2000,\nT2 from 1000 to 5000, T3 from 10 to 500, Œæfrom\n0 to 1, and Œªfrom 0 to 0.5. All results are reported\nas the average over three runs.\nC.3 Number of Parameters\nCOSINE and most of the baselines (RoBERTa-\nWL / RoBERTa-CL / SMART / WeSTClass / Self-\nEnsemble / FreeLB / Mixup / UST) are built on\nthe RoBERTa-base model with about 125M param-\neters. Snorkel is a generative model with only a\nfew parameters. ImplyLoss and Denoise freezes\nthe embedding and has less than 1M parameters.\nHowever, these models cannot achieve satisfactory\nperformance in our experiments.\n1076\nRule Example\nHAS (x, [amino acid,mutant,\nmutat, replace] ) ‚Üí part_of\nA major part of this processing requires endoproteolytic cleavage at spe-\nciÔ¨Åc pairs of basic [CHEMICAL]amino acid[CHEMICAL] residues,\nan event necessary for the maturation of a variety of important bio-\nlogically active proteins, such as insulin and [GENE]nerve growth\nfactor[GENE].\nHAS (x, [bind, interact,\naffinit] ) ‚Üí regulator\nThe interaction of [CHEMICAL]naloxone estrone azine[CHEMICAL]\n(N-EH) with various [GENE]opioid receptor[GENE] types was studied\nin vitro.\nHAS (x, [activat, increas,\ninduc, stimulat, upregulat]\n) ‚Üí upregulator/activator\nThe results of this study suggest that [CHEMI-\nCAL]noradrenaline[CHEMICAL] predominantly, but not exclusively,\nmediates contraction of rat aorta through the activation of an\n[GENE]alphalD-adrenoceptor[GENE].\nHAS (x, [downregulat,\ninhibit, reduc, decreas]\n) ‚Üí downregulator/inhibitor\nThese results suggest that [CHEMICAL]prostacyclin[CHEMICAL]\nmay play a role in downregulating [GENE]tissue factor[GENE] expres-\nsion in monocytes, at least in part via elevation of intracellular levels\nof cyclic AMP.\nHAS (x, [ agoni, tagoni]*\n) ‚Üí agonist * (note the leading\nwhitespace in both cases)\nAlprenolol and BAAM also caused surmountable antagonism\nof [CHEMICAL]isoprenaline[CHEMICAL] responses, and this\n[GENE]beta 1-adrenoceptor[GENE] antagonism was slowly reversible.\nHAS (x, [antagon] ) ‚Üí\nantagonist\nIt is concluded that [CHEMICAL]labetalol[CHEMICAL] and dilevalol\nare [GENE]beta 1-adrenoceptor[GENE] selective antagonists.\nHAS (x, [modulat,\nallosteric] ) ‚Üí modulator\n[CHEMICAL]Hydrogen sulÔ¨Åde[CHEMICAL] as an allosteric modu-\nlator of [GENE]ATP-sensitive potassium channels[GENE] in colonic\ninÔ¨Çammation.\nHAS (x, [cofactor] ) ‚Üí\ncofactor\nThe activation appears to be due to an increase of [GENE]GAD[GENE]\nafÔ¨Ånity for its cofactor, [CHEMICAL]pyridoxal phos-\nphate[CHEMICAL] (PLP).\nHAS (x, [substrate, catalyz,\ntransport, produc, conver]\n) ‚Üí substrate/product\nKinetic constants of the mutant [GENE]CrAT[GENE] showed modi-\nÔ¨Åcation in favor of longer [CHEMICAL]acyl-CoAs[CHEMICAL] as\nsubstrates.\nHAS (x, [not] ) ‚Üí not [CHEMICAL]Nicotine[CHEMICAL] does not account for the CSE\nstimulation of [GENE]VEGF[GENE] in HFL-1.\nTable 6: Examples of semantic rules on Chemprot.\nHyper-parameter AGNews IMDB Yelp MIT-R TREC Chemprot WiC\nDropout Ratio 0.1\nMaximum Tokens 128 256 512 64 64 400 256\nBatch Size 32 16 16 64 16 24 32\nWeight Decay 10‚àí4\nLearning Rate 10‚àí5 10‚àí5 10‚àí5 10‚àí5 10‚àí5 10‚àí5 10‚àí5\nT1 160 160 200 150 500 400 1700\nT2 3000 2500 2500 1000 2500 1000 3000\nT3 250 50 100 15 30 15 80\nŒæ 0.6 0.7 0.7 0.2 0.3 0.7 0.7\nŒª 0.1 0.05 0.05 0.1 0.05 0.05 0.05\nTable 7: Hyper-parameter conÔ¨Ågurations. Note that we only keep certain number of tokens.\nD Early Stopping and Earlier Stopping\nOur model adopts the earlier stopping strategy dur-\ning the initialization stage. Here we use ‚Äúearlier\nstopping‚Äù to differentiate from ‚Äúearly stopping‚Äù,\nwhich is standard in Ô¨Åne-tuning algorithms. Early\nstopping refers to the technique where we stop\ntraining when the evaluation score drops. Earlier\nstopping is self-explanatory, namely we Ô¨Åne-tune\nthe pre-trained LMs with only a few steps, even\nbefore the evaluation score starts dropping. This\ntechnique can efÔ¨Åciently prevent the model from\noverÔ¨Åtting. For example, as Figure 5(b) illustrates,\non IMDB dataset, our model overÔ¨Åts after 240 itera-\ntions of initialization with weak labels. In contrast,\nthe model achieves good performance even after\n400 iterations of Ô¨Åne-tuning when using clean la-\nbels. This veriÔ¨Åes the necessity of earlier stopping.\n1077\nDistance d Euclidean Cos\nSimilarity W Hard KL-based L2-based Hard KL-based L2-based\nAGNews 87.52 86.44 86.72 87.34 86.98 86.55\nMIT-R 76.61 76.68 76.49 76.55 76.76 76.58\nTable 8: Performance of COSINE under different settings.\nE Comparison of Distance Measures in\nContrastive Learning\nThe contrastive regularizer R1(Œ∏; Àúy) is related to\ntwo designs: the sample distance metric dij and the\nsample similarity measure Wij. In our implemen-\ntation, we use the scaled Euclidean distance as the\ndefault for dij and Eq. 5 as the default for Wij13.\nHere we discuss other designs.\nE.1 Sample distance metric d\nGiven the encoded vectorized representations vi\nand vj for samples iand j, we consider two dis-\ntance metrics as follows.\nScaled Euclidean distance (Euclidean): We cal-\nculate the distance between vi and vj as\ndij = 1\nd‚à•vi ‚àívj‚à•2\n2 . (13)\nCosine distance (Cos) 14: Besides the scaled Eu-\nclidean distance, cosine distance is another widely-\nused distance metric:\ndij = 1‚àícos (vi,vj) = 1‚àí‚à•vi ¬∑vj‚à•\n‚à•vi‚à•‚à•vj‚à•. (14)\nE.2 Sample similarity measures W\nGiven the soft pseudo-labels Àúyi and Àúyj for samples\niand j, the following are some designs for Wij. In\nall of the cases, Wij is scaled into range [0,1] (we\nset Œ≥ = 1in Eq. 7 for the hard similarity).\nHard Similarity: The hard similarity between two\nsamples is calculated as\nWij =\n{ 1, if argmax\nk‚ààY\n[Àúyi]k = argmax\nk‚ààY\n[Àúyj]k,\n0, otherwise.\n(15)\nThis is called a ‚Äúhard‚Äù similarity because we obtain\na binary label, i.e., we say two samples are similar\nif their corresponding hard pseudo-labels are the\nsame, otherwise we say they are dissimilar.\n13To accelerate contrastive learning, we adopt the doubly\nstochastic sampling approximation to reduce the computa-\ntional cost. SpeciÔ¨Åcally, the high conÔ¨Ådence samples Cin\neach batch Byield O(|C|2) sample pairs, and we sample |C|\npairs from them.\n14We use Cos to distinguish from our model name CO-\nSINE.\nSoft KL-based Similarity: We calculate the simi-\nlarity based on KL distance as follows.\nWij = exp\n(\n‚àíŒ≤\n2\n(\nDKL(Àúyi‚à•Àúyj) +DKL(Àúyj‚à•Àúyi)\n))\n,\n(16)\nwhere Œ≤is a scaling factor, and we set Œ≤ = 10by\ndefault.\nSoft L2-based Similarity: We calculate the simi-\nlarity based on L2 distance as follows.\nWij = 1‚àí1\n2||Àúyi ‚àíÀúyj||2\n2, (17)\nE.3 COSINE under different dand W.\nWe show the performance of COSINE with dif-\nferent choices of dand W on Agnews and MIT-R\nin Table 8. We can see that COSINE is robust\nto these choices. In our experiments, we use the\nscaled euclidean distance and the hard similarity\nby default.",
  "topic": "Zh√†ng",
  "concepts": [
    {
      "name": "Zh√†ng",
      "score": 0.7055253386497498
    },
    {
      "name": "Computer science",
      "score": 0.6829426288604736
    },
    {
      "name": "Linguistics",
      "score": 0.5462127327919006
    },
    {
      "name": "Computational linguistics",
      "score": 0.5141911506652832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5039765238761902
    },
    {
      "name": "Natural language processing",
      "score": 0.503330647945404
    },
    {
      "name": "Language model",
      "score": 0.49100759625434875
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4541747272014618
    },
    {
      "name": "Speech recognition",
      "score": 0.3588053584098816
    },
    {
      "name": "Philosophy",
      "score": 0.1611124873161316
    },
    {
      "name": "History",
      "score": 0.10292655229568481
    },
    {
      "name": "Physics",
      "score": 0.10022363066673279
    },
    {
      "name": "China",
      "score": 0.09655576944351196
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}