{
  "title": "KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships",
  "url": "https://openalex.org/W3214536449",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2122874018",
      "name": "Lei He",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2134013486",
      "name": "Suncong Zheng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1951789788",
      "name": "Tao Yang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104000146",
      "name": "Feng Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964222246",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3010563230",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W2991612931",
    "https://openalex.org/W4287547182",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W2759136286",
    "https://openalex.org/W2889583850"
  ],
  "abstract": "Interactions between entities in knowledge graph (KG) provide rich knowledge for language representation learning. However, existing knowledge-enhanced pretrained language models (PLMs) only focus on entity information and ignore the fine-grained relationships between entities. In this work, we propose to incorporate KG (including both entities and relations) into the language learning process to obtain KG-enhanced pretrained Language Model, namely KLMo. Specifically, a novel knowledge aggregator is designed to explicitly model the interaction between entity spans in text and all entities and relations in a contextual KG. An relation prediction objective is utilized to incorporate relation information by distant supervision. An entity linking objective is further utilized to link entity spans in text to entities in KG. In this way, the structured knowledge can be effectively integrated into language representations. Experimental results demonstrate that KLMo achieves great improvements on several knowledge-driven tasks, such as entity typing and relation classification, comparing with the state-of-the-art knowledge-enhanced PLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4536–4542\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n4536\nKLMo: Knowledge Graph Enhanced Pretrained Language Model with\nFine-Grained Relationships\nLei He, Suncong Zheng, Tao Yang, Feng Zhang\nTencent AI Platform Department, China\n{bettyleihe,congzheng,rigorosyang,jayzhang}@tencent.com\nAbstract\nInteractions between entities in knowledge\ngraph (KG) provide rich knowledge for lan-\nguage representation learning. However, exist-\ning knowledge-enhanced pretrained language\nmodels (PLMs) only focus on entity informa-\ntion and ignore the ﬁne-grained relationships\nbetween entities. In this work, we propose\nto incorporate KG (including both entities and\nrelations) into the language learning process\nto obtain KG-enhanced pretrained Language\nModel, namely KLMo. Speciﬁcally, a novel\nknowledge aggregator is designed to explicitly\nmodel the interaction between entity spans in\ntext and all entities and relations in a contex-\ntual KG. An relation prediction objective is\nutilized to incorporate relation information by\ndistant supervision. An entity linking objec-\ntive is further utilized to link entity spans in\ntext to entities in KG. In this way, the struc-\ntured knowledge can be effectively integrated\ninto language representations. Experimental\nresults demonstrate that KLMo achieves great\nimprovements on several knowledge-driven\ntasks, such as entity typing and relation clas-\nsiﬁcation, comparing with the state-of-the-art\nknowledge-enhanced PLMs.\n1 Introduction\nKnowledge Graph (KG) with entities and relations\nprovides rich knowledge for language learning\n(Wang et al., 2017, 2014). Recently, researchers\nhave explored to incorporate KG information into\nPLMs (Devlin et al., 2018; Radford et al.) to en-\nhance language representations, such as ERNIE-\nTHU (Zhang et al., 2019), WKLM (Xiong et al.,\n2019), KEPLER (Wang et al., 2019), KnowBERT\n(Peters et al., 2019), BERT-MK (He et al., 2019)\nand KALM (Rosset et al., 2020), . However, they\nonly utilize entity information and ignore the ﬁne-\ngrained relationships between entities. The ﬁne-\ngrained semantic information of relations between\nentities is also critical to language representation\nIn 2001, Lang Lang has attended to BBC Proms, however, he became \npopular in China until his appearance in Trio of Happiness in 2012.\nBBC\nProms\nTrio of\nHappiness\nLang Lang\nGuest\nPerformer\nis_a\nis_a\nis_a\nTrio of\nHappiness\nTV Show\nPianist\nConcert\nConcert\nFigure 1: An illustrative example of incorporating\nknowledge into PLMs. The relations in KG is critical\nto correctly predict the type of Trio of Happiness.\nlearning. Taking Figure 1 as example, for entity\ntyping, without explicitly knowing the ﬁne-grained\nrelation Guest between Lang Lang and Trio of Hap-\npiness, which is different from the relation Per-\nformer between Lang Lang and BBC Proms, it’s\nimpossible to correctly predict the type of Trio of\nHappiness as TV Show, since the input sentence\nliterally implies that Trio of Happiness belongs to\nthe same type as BBC Proms. The ﬁne-grained re-\nlations between entities in KG provide speciﬁc con-\nstraint on entities, thus can play an important role\nin language learning for knowledge-driven tasks.\nTo explicitly incorporate entities and ﬁne-\ngrained relations in KG into PLMs, one main chal-\nlenge we are faced with is the Text-Knowledge\nAlignment (TKA) problem: it’s difﬁcult to make\ntoken-relation and token-entity alignments for the\nfusion of text and knowledge. To handle this prob-\nlem, the KG-enhanced pretrained language model\n(KLMo) is proposed to integrate KG (i.e. both en-\ntities and ﬁne-grained relations) into the language\nrepresentation learning. The main component of\nKLMo is a knowledge aggregator, which is respon-\nsible for text and knowledge information fusion\nfrom two individual embedding spaces, i.e. to-\nken embedding space and KG embedding space.\nThe knowledge aggregator models the interaction\nbetween entity spans in text and all entities and re-\nlations in a contextual KG via an entity span-level\ncross-KG attention to make tokens attend to highly\n4537\nrelated entities and relations in KG. Based on the\nKG-enhanced token representations, a relation pre-\ndiction objective is utilized to predict the relation\nof each pair of entities in text based on the distant\nsupervision of KG. Furthermore, an entity linking\nobjective is utilized to predict entities in KG based\non the corresponding entity spans in text. The rela-\ntion prediction and entity linking objectives are the\nkey to the integration of KG information into text\nrepresentations.\nWe conduct experiments on two Chinese\nknowledge-driven NLP tasks, i.e. entity typing\nand relation classiﬁcation. The experimental re-\nsults demonstrate that KLMo obtains large im-\nprovements over BERT and existing knowledge-\nenhanced PLMs, by taking full advantage of a struc-\ntured KG including both entities and ﬁne-grained\nrelations. We also will publish a Chinese entity\ntyping dataset for the evaluation of Chinese PLMs.\n2 Model Description\nAs shown in Figure 2, KLMo is designed as a multi-\nlayer Transformer-based (Vaswani et al., 2017)\nmodel, which accepts a token sequence and the\nentities and relations in its contextual KG as in-\nput. The token sequence is ﬁrstly encoded by a\nmulti-layer Transformer-based text encoder. The\noutput of the text encoder is further used as input\nfor the knowledge aggregator that fuses the knowl-\nedge embeddings of entities and relations into the\ntoken sequence to obtain KG-enhanced token rep-\nresentations. Based on the KG-enhanced represen-\ntations, novel relation prediction and entity linking\nobjectives are jointly optimized as the pre-training\nobjectives, which help incorporate high-related en-\ntity and relation information in the KG into the text\nrepresentations.\n2.1 Knowledge Aggregator\nAs shown in Figure 2, the knowledge aggregator\nis designed as an M-layer knowledge encoder to\nintegrate knowledge in KG into language represen-\ntation learning. It accepts the hidden embeddings\nof the token sequence and the knowledge embed-\ndings of the entities and relations in KG as input,\nand fuses text and KG information from two indi-\nvidual embedding spaces. The knowledge aggre-\ngator contains two separate multi-head attentions:\ntoken-level self-attention and knowledge graph at-\ntention (Veliˇckovi´c et al., 2017), which encodes the\ninput text and the KG independently. The entity\n[CLS]      失 孤 是 刘 德 华 饰 雷 泽 宽 … [SEP]\nt\" t# t$ t#%t& t' …\nText Encoder\nKnowledge Aggregator\n(\" (& ('(#\nToken-level Self-Attention Knowledge Graph Attention\n…\nt) t*t% t+\n,# ,& ,' -# -& \t-'\n()\n(\"\n/ (&\n/ ('\n/ ($/(#\n/ ()\n/0̂#\n/ 0̂$\n/0̂&\n/ 0̂)\n/ 0̂%\n/ 0̂2\n/0̂*\n/ 0̂#\"\n/\nt#\"t2\ne1 e2 e3\n…\n-&/ -'\n/-#\n/\n0̂)\n/ 0̂%/ 0̂2\n/0̂*\n/ 0̂#\"\n/\n-&\n/ -'\n/ -'\n/-&\n/ -'\n/\nKG-enhanced Token Representations\nLost and Love is a movie starring Andy Lau (playing Lei Zekuan) \ne1 e2 e3\n0̂$\n/\n-&\n/\n0̃$\n/\n0̂&\n/\n-#\n/\n0̃&\n/ 0̃)\n/ 0̃%\n/ 0̃2\n/0̃*\n/ 0̃#\"\n/… … … …\n… 0̂#\n/\n-#\n/\n0̃#\n/\n… …\nAndy Lau 33165\nLost and Love 24405\n… …\nLei Zekuan 885776\n………\nEntity-level Cross-KG Attention\nr s o\n,# Actor -# -&\n,& Role -# -'\n,' Paly -' -&\n()\nContextual KG\n,#\n,&\n,'\n-#\n-&\n-'\n,# ,& ,' -#-&-'\ninvisible\nEntity Spans\n(2) Entity Linking\nMLP\n(1) Relation Prediction\nMLP\nActor 33165\nFigure 2: Overview of the model architecture.\nrepresentation is computed by pooling over all to-\nkens in an entity-span. Then the aggregator models\nthe interaction between entity spans in text and all\nentities and relations in a contextual KG through\nan entity-level cross-KG attention to incorporate\nknowledge into the text representations.\nKnowledge Graph Attention As the entities\nand relations in a KG composes a graph, it’s critical\nto considering the graph structure during knowl-\nedge representation learning. We ﬁrst represent\nentities and relations in the contextual KG by\nTransE (Bordes et al., 2013) and then translate\nthem into an entity and relation embedding se-\nquence {z0,z1,..., zq}, served as the input for the\nknowledge aggregator. Then the knowledge aggre-\ngator encodes the entity and relation sequence by\na knowledge graph attention which considers its\ngraph structure by importing a visible matrix M\ninto the traditional self-attention mechanism (Liu\net al., 2020). The visible matrix M only allows\nadjacent entities and relations in the KG to be visi-\nble to each other during representation learning, as\nshown in the right bottom of Figure 2.\nEntity-level Cross-KG Attention To com-\npute the KG-enhanced entity representa-\ntions, given an entity mention list Ce =\n{(e0,start0,end0),..., (em,startm,endm)}, the\nknowledge aggregator ﬁrst computes the entity\nspan representations {ˆei\n0,..., ˆei\nm} by pooling over\nall tokens in an entity-span with self-attentive span\npooling method from (Lee et al., 2017). The entity\nspan embeddings {ˆei\n0,..., ˆei\nm} can be expanded\nto all tokens {ˆei\n0,..., ˆei\nn} by making ˆei\nj = ˆti\nj for\ntokens not in any entity spans, where ˆti\nj denotes\nthe representation of the j-th token from the\n4538\ntoken-level self-attention.\nIn order to model the interaction between entity\nspans in text and all entities and relations in a con-\ntextual KG, the aggregator performs an entity-level\ncross-KG attention to allow tokens attend to highly\nrelated entities and relations in KG, thus computes\nthe KG-enhanced entity representations. Specif-\nically, the entity-level cross-KG attention in the\ni-th aggregator is performed by contextual multi-\nhead attention between the entity span embeddings\n{ˆei\n0,..., ˆei\nn} as the query and the entity and relation\nembeddings {zi\n0,..., zi\nq} as the key and value.\nKG-enhanced Token Representations To in-\nject the KG-enhanced entity information into the\ntoken representations, the i-th layer of the knowl-\nedge aggregator computes the KG-enhanced token\nrepresentations {˜ti\n0,..., ˜ti\nn} by adopting an infor-\nmation fusion operation between {ˆti\n0,..., ˆti\nn} and\n{ei\n0,..., ei\nn}. For the j-th token, the fusion opera-\ntion is deﬁned as follows:\nui\nj = σ(W i\nuˆti\nj + W i\neei\nj + bi\nu)\n˜ti\nj = σ(W i\nt ui\nj + bi\nt)\n(1)\nwhere ui\nj represents the hidden state integrating the\ninformation from both token and entity. σis a non-\nlinear activation function. W i\n∗ and bi\n∗ are learnable\nweights and biases respectively. The KG-enhanced\ntoken representation {˜ti\n0,..., ˜ti\nn} is fed into the next\nlayer of knowledge aggregator as input.\n2.2 Pre-training Objectives\nTo incorporate KG knowledge into the language\nrepresentation learning, KLMo adopts a multi-task\nloss function as the training objective:\nL = LRP + LEL + LMLM (2)\nIn addition to the loss of masked language model\nLMLM (Devlin et al., 2018; Li et al., 2020), an\nrelation prediction loss LRP and an entity linking\nloss LEL are integrated to predict the entities in KG\nbased on the corresponding KG-enhanced tokens\nrepresentations {˜tM\n0 ,..., ˜tM\nn }.\nFor each pair of entity spans, we utilize the re-\nlation between their corresponding entities in the\nKG as the distant supervision for relation predic-\ntion. The relation prediction and entity linking\nobjectives are the key to the integration of rela-\ntions and entities in KG into the text. Since the\nnumber of entities in KG is quite large for the\nSoftmax operation in entity-linking objective, we\nModal Precision Recall F1 Acc\nBERT 81.76 80.11 80.92 80.06\nWKLM 82.71 80.28 81.47 80.17\nERNIE 82.66 81.39 82.02 81.18\nKLMo 82.68 84.33 83.50 81.75\nTable 1: Results on Entity Typing.\nhandle this problem by only predicting entities in\nthe same batch instead of all entities in KG. To pre-\nvent KLMo from completely remembering entity\nmentions while predicting rather than relying on\ntextual contexts, we randomly mask10% of entities\nwith a special [MASK] token in the input text.\n3 Experiments\nThis section presents the details of KLMo\npre-training and its ﬁnetuning on two speciﬁc\nknowledge-driven NLP tasks: entity typing and\nrelation classiﬁcation. We pretrain KLMo by a Chi-\nnese corpus of Baidu Baike’s webpages and the\nBaike Knowledge Graph. Details of the pretraining\ncorpus and experimental settings are described in\nAppendix A. 1\n3.1 Baselines\nWe compare KLMo with the state-of-the-art PLMs\npretrained on the same Baidu Baike corpus: (1)\nBERT-Base Chinese (Devlin et al., 2018), which\nis further pretrained on the Baidu Baike corpus for\none epoch.(2) ERNIE-THU (Zhang et al., 2019),\na pioneering and typical work in this ﬁeld, which\nincorporates entity knowledge into the PLM. (3)\nWKLM (Xiong et al., 2019), a weakly supervised\nKnowledge-enhanced PLM using entity replace-\nment predictions to incorporate the entity knowl-\nedge, which provides the state-of-the-art results on\nseveral knowledge-driven tasks.\n3.2 Entity Typing\nDataset In this work, we create a Chinese entity\ntyping dataset, which is a completely manually-\nannotated dataset containing 23,100 sentences and\n28,093 annotated entities distributed in 15 ﬁne-\ngrained categories of media works, such as Movie,\nShow and TV Play. We split the dataset into a train-\ning set with 15,000 sentences and a test set with\n8,100 sentences. The detail statistics of the dataset\n1Our code and datasets are available at: https://\ngithub.com/lei-nlp/KLMo\n4539\nModal Precision Recall F1\nCNN - - 20.56\nBERT 15.94 35.12 21.93\nWKLM 16.32 36.96 22.64\nERNIE 18.18 34.29 23.76\nKLMo 20.90 31.24 25.05\nTable 2: Results on Relation Classiﬁcation.\nModal Precision Recall F1\nBERT 81.76 80.11 80.92\nKLMo 82.68 84.33 83.50\nw/o KG 82.30 83.02 82.66\nTable 3: Ablation study on Entity Typing.\nand the ﬁnetuning settings are shown in Appendix\nB.1.\nResults We evaluate various pretrained models\nfor entity typing under precision, recall, micro-F1\nand accuracy metrics. The results are shown in\nTable 1. The following observations can be found:\n(1) All knowledge-enhanced PLMs generally per-\nform much better than the BERT baseline on all\nmeasures, which shows that entity knowledge is\nbeneﬁcial to entity type predication with limited\nannotated resources. (2) Compared with the ex-\nisting knowledge-enhanced PLMs, KLMo largely\nimproves the recall score over WKLM and ERNIE,\nleading to an improvement of 1.58 and 0.57 on\nmicro-F1 respectively. This indicates that ﬁne-\ngrained relationships between entities help KLMo\nto predict appropriate categories for more entities.\n3.3 Relation Classiﬁcation\nDataset The CCKS 2019 Task 3 Inter-Personal\nRelational Extraction (IPRE) dataset (Han et al.,\n2020) is used for the evaluation on relation classi-\nﬁcation. The training set is automatically labeled\nby distant supervision, and the test set is manu-\nally annotated. There are 35 relations (including a\nnull-relation class “NA”), where “NA” accounts for\nnearly 86% in the training set and 97% in the test\nset. The detail statistics of the dataset and ﬁnetun-\ning settings are shown in Appendix B.2.\nResults We adopt precision, recall and micro-\nF1 as the evaluation measures. The results are\nshown in Table 2. In addition to BERT baseline, we\nalso compare KLMo with an ofﬁcial CNN baseline,\nwhich gets CNN output as the sentence embedding\nand feed it into a relation classiﬁer. From Table\n2, we can see that both CNN and BERT baseline\nmodels do not perform well, which indicates the\nhigh difﬁculty of the dataset. This ascribes to the\nlarge number of noisy labels in the training set\nautomatically generated by distant supervision.\nAlthough the dataset are very difﬁcult, we can\nstill observe that: (1) All knowledge-enhanced\nPLMs largely improve the precision and micro-\nF1 scores over BERT baseline, which shows that\nboth entity information and KG information can\nenhance language representations and accordingly\nprompt the performance of relation classiﬁcation.\n(2) KLMo largely improves the precision score\nover WKLM and ERNIE, leading to an improve-\nment of 2.41 and 1.29 on micro-F1 respectively,\nwhich demonstrates that ﬁne-grained relations in\nKG help KLMo avoid ﬁtting on noisy labels and\npredict relations correctly.\n3.4 Effects of KG Information\nMost NLP tasks only provide text inputs and the\nentity linking itself is a hard task. Thus, we inves-\ntigate the effects of KG entities and relations for\nKLMo on entity typing. w/o KG refers to ﬁnetun-\ning KLMo without the input of KG entities and\nrelations. Table 3 shows the results of the ablation\nstudy. Without KG input for funetuning, KLMo\nstill largely outperforms BERT on both precision\nand recall scores, leading to an improvement by\n1.74 on micro-F1. Compared with KLMo ﬁne-\ntuning with KG, KLMo without KG witnesses a\nsmall decrease of 0.84 on micro-F1 measure. This\ndemonstrates that KG information has been inte-\ngrated into KLMo during pre-training. For most\nspeciﬁc NLP tasks, KLMo can be ﬁnetuned in a\nsimilar way as BERT.\n4 Conclusion\nIn this paper, we propose a novel KG-enhanced\npretrained language model KLMo to explicitly in-\ntegrate KG entities and ﬁne-grained relations into\nthe language representation learning. Accordingly,\nthe novel knowledge aggregator is designed to han-\ndle the heterogeneous information fusion and text-\nknowledge alignment problems. Further, the re-\nlation prediction and entity linking objectives are\njointly optimized to encourage the knowledge in-\nformation integration. The experiment results show\nthat KLMo outperforms the other state-of-the-art\nknowledge-enhanced PLMs, which validates the\nintuition that ﬁne-grained relationships in KG can\n4540\nenhance the language representation learning and\nbeneﬁt some knowledge-driven NLP tasks.\nReferences\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Neural Information Processing\nSystems (NIPS), pages 1–9.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nXianpei Han, Zhichun Wang, Jiangtao Zhang, Qinghua\nWen, Wenqi Li, Buzhou Tang, Qi Wang, Zhi-\nfan Feng, Yang Zhang, Yajuan Lu, et al. 2020.\nOverview of the ccks 2019 knowledge graph eval-\nuation track: Entity, relation, event and qa. arXiv\npreprint arXiv:2003.03875.\nXu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. Openke: An\nopen toolkit for knowledge embedding. In Proceed-\nings of the 2018 conference on empirical methods\nin natural language processing: system demonstra-\ntions, pages 139–144.\nBin He, Di Zhou, Jinghui Xiao, Qun Liu, Nicholas Jing\nYuan, Tong Xu, et al. 2019. Integrating graph\ncontextualized knowledge into pre-trained language\nmodels. arXiv preprint arXiv:1912.00147.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference resolu-\ntion. arXiv preprint arXiv:1707.07045.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang.\n2020. Unimo: Towards uniﬁed-modal understand-\ning and generation via cross-modal contrastive learn-\ning. arXiv preprint arXiv:2012.15409.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert:\nEnabling language representation with knowledge\ngraph. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 34, pages 2901–2908.\nMatthew E Peters, Mark Neumann, Robert L Lo-\ngan IV , Roy Schwartz, Vidur Joshi, Sameer Singh,\nand Noah A Smith. 2019. Knowledge enhanced\ncontextual word representations. arXiv preprint\narXiv:1909.04164.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding\nby generative pre-training.\nCorby Rosset, Chenyan Xiong, Minh Phan, Xia\nSong, Paul Bennett, and Saurabh Tiwary. 2020.\nKnowledge-aware language model pretraining.\narXiv preprint arXiv:2007.00655.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. arXiv preprint\narXiv:1710.10903.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\n2017. Knowledge graph embedding: A survey of\napproaches and applications. IEEE Transactions\non Knowledge and Data Engineering, 29(12):2724–\n2743.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. Kepler: A\nuniﬁed model for knowledge embedding and pre-\ntrained language representation. arXiv preprint\narXiv:1911.06136.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 28.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. arXiv preprint arXiv:1912.09637.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: En-\nhanced language representation with informative en-\ntities. arXiv preprint arXiv:1905.07129.\nA Pre-training Settings\nA.1 Pretraining Corpus\nBaike Knowledge Graph Baike Knowledge\nBase is a generic-domain Chinese knowledge base,\nwhich contains 226 concept types, more than 100\nmillion entities, and 2.2 billion triples. Each entity\nin Baike Knowledge Base is aligned to a webpage\nfrom a variety of sources, such as Baidu Baike, So-\ngou Baike and douban. To pretrain a KG-enhanced\nChinese language model, we extract a subset of\nthis knowledge base to build a Baike KG using\nthe following rules: 1) removing entities not from\nBaidu Baike articles; 2) removing low-popular enti-\nties (smaller than 200); 3) only keeping fact triples\nwhose both entities are Baidu Baike entities. The\nﬁnal Baike KG contains 2,466,069 entities, 390\nrelations and 9,859,314 triples.\nChinese Pretraining Corpus KLMo mainly\nadopts Baidu Baike’s webpages for pre-training,\nwhich contains encyclopedia articles written in for-\nmal Chinese language. Entities in articles can be\n4541\nDataset Entity Typing Dataset Linked Entity Typing Dataset\n#Sentences #Entities #Types #Linked_Sentences #Linked_Entities Sent_Ratio\nTraining Set 15,000 18,180 15 7606 11,313 50.7%\nTesting Set 8100 9913 15 4165 6268 51.4%\nTable 4: Statistics of the Chinese Entity Typing Dataset.\n#Sentences #Relations #Linked_Sentences #Linked_Entities Sent_Ratio\nTraining Set 287,351 35 132,739 232,882 46.2%\nTesting Set 38,417 35 15,906 27,233 41.4%\nTable 5: Statistics of Chinese Relation Classiﬁcation Dataset.\nextracted by anchor links and aligned to Baike KG\nentities. After preprocessing the corpus, a large\nformatted dataset containing 7.8B tokens, 174M\nsentences, 21M entities and 1.2M relations is gen-\nerated for the pre-training of KLMo. Sentences\nhaving less than 5 words or 2 entities are discarded.\nA.2 Implementation Details\nIn the experiment, we ﬁrst obtain the knowledge\nrepresentations trained on Baike KG triples by\nTransE (Bordes et al., 2013) algorithm using the\nOpenKE toolkit (Han et al., 2018). The represen-\ntations are used to initialize the entity and relation\nembeddings in KLMo. The embedding dimension\nis set to 100 and the epoch number is set to 5000.\nAs for the pre-training of KLMo, due to the\nexpensive cost of pre-training from scratch, we\ninherit the parameters of BERT-Base Chinese to\ninitialize the Transformer blocks for token encod-\ning, while the parameters for entity and relation\nencoding modules are all randomly initialized. The\nnumber of text encoder layers Land knowledge\naggregator layers M are both 6. The hidden size\nof token embeddings dt, knowledge embeddings\ndz and entity span embeddings de are set to 768,\n100 and 100. The number of token-oriented atten-\ntion heads At, KG-oriented attention heads Az and\nentity span-level attention heads Ae are set to 12,\n4 and 12 respectively. The pre-training of KLMo\nruns 3 epochs on 4 NVIDIA Tesla V100 (32GB)\nGPUs with the batch size of 128, the max sequence\nlength of 512 and the learning rate of 5e-5.\nB Finetuning Settings\nB.1 Entity Typing\nTo evaluate the performance of KLMo, two\nknowledge-driven tasks, i.e. entity typing and re-\nlation classiﬁcation, are performed in this work.\nGiven a sentence with an entity mention, the entity\ntyping task is to label the mention with its ﬁne-\ngrained semantic type.\nDataset Entity typing is not a new task. However,\nto our best knowledge, there is no public bench-\nmark dataset available on Chinese ﬁne-grained en-\ntity typing. Therefore, In this work, we create a\nChinese entity typing dataset, which is a completely\nmanually-annotated dataset containing 23,100 sen-\ntences and 28,093 annotated entities distributed in\n15 ﬁne-grained categories of media works, such as\nMovie, Show and TV Play. We split the dataset\ninto a training set with 15,000 sentences and a test\nset with 8,100 sentences. The detail statistics of the\ndataset are shown in Table 4.\nFinetuning The Chinese entity typing dataset\nlacks of KG entity annotations, thus we ﬁrst use an\nentity linker tool accompanied with Baike Knowl-\nedge Base to recognize entity mentions in sentences\nand link them to their corresponding Baike KG en-\ntities. The statistics of the linked entity typing\ndataset are shown in Table 4. Over 50% of sen-\ntences contain at least one linked KG entity men-\ntion in both training set and test set. To ﬁnetune\nKLMo for entity typing, we use the representation\nof the ﬁrst token of each entity span to predict its\nentity type. The model is ﬁnetuned for 10 epochs\non the training set with the batch size of 128, the\nmax sequence length of 256 and the learning rate\nof 2e-5.\nB.2 Relation Classiﬁcation\nWe also compare the results of various pretrained\nmodels on the task of relation classiﬁcation. Given\na pair of entities in a sentence, the relation classiﬁ-\ncation task is to determine the relation type between\nthe pair of entities.\n4542\nDataset The CCKS 2019 Task 3 Inter-Personal\nRelational Extraction (IPRE) dataset (Han et al.,\n2020) is used for the evaluation on relation classi-\nﬁcation. The training set is automatically labeled\nby distant supervision, and the test set is manu-\nally annotated. There are 35 relations (including a\nnull-relation class “NA”), where “NA” accounts for\nnearly 86% in the training set and 97% in the test\nset. The detail statistics of the dataset are shown in\nTable 5.\nFinetuning The IPRE dataset also lacks of KG\nentity annotations, and we recognize and link entity\nmentions to their corresponding Baike KG entities\nin the same way as we do for the entity typing\ndataset. The statistics of the linked dataset are\nshown in Table 5. Over 40% of sentences contain\nat least one linked KG entity mention. To ﬁnetune\nKLMo for relation classiﬁcation, we concatenate\nthe representations of the ﬁrst token of the two\ncandidate entity spans. The model is ﬁnetuned\nfor 10 epochs with the batch size of 128, the max\nsequence length of 256 and the learning rate of\n2e-5.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8511059284210205
    },
    {
      "name": "Knowledge graph",
      "score": 0.6451905965805054
    },
    {
      "name": "Natural language processing",
      "score": 0.6207199692726135
    },
    {
      "name": "Relation (database)",
      "score": 0.5402161478996277
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5383096933364868
    },
    {
      "name": "Graph",
      "score": 0.5123621225357056
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.48680683970451355
    },
    {
      "name": "Entity linking",
      "score": 0.4429362714290619
    },
    {
      "name": "Knowledge base",
      "score": 0.21262067556381226
    },
    {
      "name": "Data mining",
      "score": 0.11390799283981323
    },
    {
      "name": "Theoretical computer science",
      "score": 0.10031157732009888
    }
  ]
}