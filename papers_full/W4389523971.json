{
  "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization",
  "url": "https://openalex.org/W4389523971",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2120194892",
      "name": "Janghwan Lee",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2114836216",
      "name": "Minsoo Kim",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099569748",
      "name": "Seungcheol Baek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2220402958",
      "name": "Seok Hwang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116200700",
      "name": "Wonyong Sung",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2102209727",
      "name": "Jung-Wook Choi",
      "affiliations": [
        "Hanyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3204625459",
    "https://openalex.org/W4379539370",
    "https://openalex.org/W4378770729",
    "https://openalex.org/W3101100041",
    "https://openalex.org/W4379548477",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3100710793",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W4385574133",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3210580311",
    "https://openalex.org/W4375868853",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4283313765",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3202028501",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4226539834",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W2970821029",
    "https://openalex.org/W4221148635"
  ],
  "abstract": "Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency—a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2× hardware efficiency improvement compared to 8-bit integer MAC unit.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14726–14739\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnhancing Computation Efficiency in Large Language Models\nthrough Weight and Activation Quantization\nJanghwan Lee1∗, Minsoo Kim1∗, Seungcheol Baek2, Seok Joong Hwang2,\nWonyong Sung3 and Jungwook Choi1†\n{hwanii0288, minsoo2333}@hanyang.ac.kr, {bsc11235, nzthing}@sapeon.com\nwysung@snu.ac.kr, choij@hanyang.ac.kr\n1Hanyang University, 2SAPEON Korea Inc., 3Seoul National University\nRepublic of Korea\nAbstract\nLarge Language Models (LLMs) are proficient\nin natural language processing tasks, but their\ndeployment is often restricted by extensive\nparameter sizes and computational demands.\nThis paper focuses on post-training quantiza-\ntion (PTQ) in LLMs, specifically 4-bit weight\nand 8-bit activation (W4A8) quantization, to\nenhance computational efficiency—a topic less\nexplored compared to weight-only quantiza-\ntion. We present two innovative techniques:\nactivation-quantization-aware scaling (AQAS)\nand sequence-length-aware calibration (SLAC)\nto enhance PTQ by considering the combined\neffects on weights and activations and align-\ning calibration sequence lengths to target tasks.\nMoreover, we introduce dINT, a hybrid data\nformat combining integer and denormal repre-\nsentations, to address the underflow issue in\nW4A8 quantization, where small values are\nrounded to zero. Through rigorous evalua-\ntions of LLMs, including OPT and LLaMA, we\ndemonstrate that our techniques significantly\nboost task accuracies to levels comparable with\nfull-precision models. By developing arith-\nmetic units compatible with dINT, we further\nconfirm that our methods yield a 2×hardware\nefficiency improvement compared to 8-bit inte-\nger MAC unit.\n1 Introduction\nLarge language models (LLMs) have achieved\nbreakthroughs in many natural language processing\ntasks such as translation, summarization, reason-\ning, and conversation, often matching or exceeding\nhuman performance (Zhang et al., 2022; Touvron\net al., 2023; Chowdhery et al., 2022; Brown et al.,\n2020; OpenAI, 2023). However, the extensive pa-\nrameters of LLMs present deployment challenges\ndue to the high memory bandwidth needed for high\nthroughput inference. Post-training quantization\n(PTQ) addresses this by \"compressing\" weight pa-\n∗equal contribution †corresponding author\nrameters, significantly reducing memory require-\nments and enhancing GPU performance by alle-\nviating memory bandwidth bottlenecks (Frantar\net al., 2023; Lin et al., 2023; Lee et al., 2023a).\nNevertheless, LLMs’ computational complexity re-\nmains a concern. For example, GPT-3 (Brown\net al., 2020) requires at least 350 GFLOPs of com-\nputation for a single token, but PTQ methods of-\nten revert compressed weights to higher precisions\nlike 16-bit floating-point (FP16) for computation,\nwhich is inefficient given the resource demands of\nmultiply-accumulate (MAC) operations. With com-\nputing platforms evolving through high-bandwidth\nmemory (Gurumurthi et al., 2021) and processing-\nin-memory (Kim et al., 2021; He et al., 2020) to\nresolve the memory bandwidth bottleneck, address-\ning LLMs’ computational needs becomes more\nimperative.\nA PTQ strategy that effectively quantizes both\nweights and activations is thus appealing as it\nreduces the hardware complexity of MAC units,\nenhancing computational throughput (Sun et al.,\n2019; Dettmers et al., 2022; Xiao et al., 2022). PTQ\nresearch specific to LLM’s computation efficiency\nis growing, focusing on utilizing INT8-INT8 MAC\nunits, common in GPUs (Andersch et al., 2022).\nLLM.Int8 (Dettmers et al., 2022), for instance, used\nINT8 quantization for weights and activations, but\ndirected activation outliers through an FP16 dat-\napath, isolating them. SmoothQuant (Xiao et al.,\n2022) extended this by employing activation chan-\nnel scaling to target outliers and adjusting corre-\nsponding weights for balanced quantization. How-\never, these studies do not address challenges faced\nwhen weights are reduced to 4 bits, revealing an\nunexplored area for combined effects on weight\nand activation quantization.\nThis paper delves into the challenges of post-\ntraining quantization (PTQ) for both weights and\nactivations in large language models (LLMs). We\npinpoint two primary hurdles in achieving efficient\n14726\n4-bit weight and 8-bit activation (W4A8) quantiza-\ntion. First, LLMs like OPT (Zhang et al., 2022) and\nLLaMA (Touvron et al., 2023) have distinct weight\nand activation range characteristics, making exist-\ning PTQ methods unsuitable for universal use. For\nexample, AWQ’s (Lin et al., 2023) activation-aware\nscaling makes activations prone to quantization er-\nrors, while OPTQ’s (Frantar et al., 2023) weight\ncalibration struggles with varying activation ranges.\nWe propose two novel solutions for this first hurdle:\nactivation-quantization-aware scaling (AQAS) and\nsequence-length-aware calibration (SLAC). AQAS\noptimizes quantization scales by jointly consid-\nering weights and activations, yielding balanced\nquantization. SLAC aligns the sequence length of\nthe application task with that of the PTQ calibra-\ntion dataset, mitigating the impact of variations in\nactivation diversity, which significantly affects the\nPTQ calibration process.\nSecond, we observe that underflow, where small-\nmagnitude values round to zero, severely impacts\nW4A8 quantization in LLMs because the quan-\ntization error associated with values rounding to\nzero constitutes a significant portion of the out-\nput error. While underflow is a well-known issue\nin reduced-precision formats for deep neural net-\nworks (DNNs) (Sun et al., 2019, 2020; Chmiel\net al., 2022; Jin et al., 2022), previous PTQ research\nin LLMs mainly focuses on outliers, neglecting\nunderflow. We discover that standard INT4 repre-\nsentation discards crucial small-magnitude weights\nwhen multiplied with activations. As existing data\nformats like integer, floating-point, or logarithmic\nformats are inadequate for this underflow issue,\nwe introduce dINT, a new integer format with de-\nnormal representation. dINT merges the uniform\ncoverage of integers with the denormal of floating-\npoints, effectively mitigating underflow and im-\nproving accuracy. We also propose a MAC unit\nsupporting dINT to ensure hardware efficiency.\nWe evaluate AQAS, SLAC, and dINT on OPT\nand LLaMA, focusing on language modeling, zero-\nshot reasoning, and 5-shot in-context learning. The\nresults show that integrating these methods for\nW4A8 PTQ significantly improves task accuracies\nfor both OPT and LLaMA across a diverse set\nof benchmarks (Wikitext, Common Sense Ques-\ntion Answering (CSQA), and Massive Multitask\nLanguage Understanding (MMLU)) and the model\nsizes ranging from 125M to 65B parameters.\n2 Background\n2.1 Weight-only PTQ for LLMs\nVarious weight-only PTQ techniques have emerged\nto alleviate memory-bandwidth constraints in LLM\ninference by compressing weights to 4 bits while\nmaintaining accuracy (Park et al., 2023; Kwon\net al., 2022; Frantar et al., 2023; Lin et al., 2023;\nLee et al., 2023a). For example, OPTQ (Fran-\ntar et al., 2023) reduces output distortion from\ncolumn-wise weight quantization by sequentially\nupdating unquantized weights using activation Hes-\nsians. AWQ (Lin et al., 2023) scales weights\naccording to activation magnitudes for improved\nquantization, while OWQ (Lee et al., 2023a) and\nSPQR (Dettmers et al., 2023) isolate sensitive\nweights, retaining them at higher precision. How-\never, these approaches entail high-precision com-\nputations and complex arithmetic units. We demon-\nstrate that these weight compression methods are\nsub-optimal for activation quantization in common\nLLMs, often exacerbating challenges by ignoring\nactivation dynamics. Consequently, we introduce\nadvanced techniques specifically designed to ad-\ndress these intricacies, enhancing weight quantiza-\ntion accuracy when the activation is also quantized.\n2.2 Weight and Activation PTQ for LLMs\nQuantizing both weights and activations enables\nthe use of lower-precision MAC units, signif-\nicantly saving logic area and power consump-\ntion (Horowitz, 2014). As such, many studies\naim to reduce DNN’s computational burden (Sun\net al., 2019; Lee et al., 2023b), especially in\nLLMs (Dettmers et al., 2022; Xiao et al., 2022;\nLiu et al., 2023; Bondarenko et al., 2021). For\ninstance, LLM.Int8 (Dettmers et al., 2022) and\nSmoothQuant (Xiao et al., 2022) employ GPU-\nsupported INT8-INT8 MAC operations for effi-\nciency, with LLM.Int8 processing outliers sepa-\nrately and SmoothQuant adjusting activations and\nweights. Additionally, (Liu et al., 2023; Bon-\ndarenko et al., 2021) employ quantization-aware\nfine-tuning for further reductions to W4A8 or\nW4A4, but face noticeable accuracy losses despite\nexpensive fine-tuning. This paper proposes novel\nsolutions that address the accuracy drop in com-\nbined weight and activation quantization with bit-\nprecision down to W4A8, achieving superior re-\nsults compared to prior works without fine-tuning.\n14727\nFigure 1: (a) Illustration of fused-layernorm (fused-LN) in OPT (top) and layernorm (LN) in LLaMA (bottom)\ncomputation patterns within a Transformer layer. Note that two computation patterns yield ths same output if\ncomputed in full-precision, but they deviate when activation and weight are quantized. (b) Min-Max range of input\nactivations (left) and weight (right) as operands of matrix multiplication. (c) Min-Max range of input activation\nvarying sequence length from 128 to 2048 (Orange: OPT-6.7B, Blue: LLaMA-7B). (d) Max values of per-channel\ninput activation for OPT-6.7B (left) and LLaMA-7B (right) for different input sequence lengths (32 and 2048).\n2.3 Underflow for Reduced-Precision LLMs\nUnderflow, the numerical error from small values\nrounding to zero due to limited bit-precision, has\nbeen actively studied as a critical issue in reduced-\nprecision DNN training. For instance, (Sun et al.,\n2019) counters underflow in 8-bit floating-point by\nadjusting the exponent bias, (Sun et al., 2020) uti-\nlizes a radix-4 format to represent wider magnitude\nranges in 4-bit floating-point (FP4), and (Chmiel\net al., 2022) uses stochastic underflow to address\nbiased quantization in FP4 gradients. In fixed-point\nrepresentation, (Jin et al., 2022) explores optimal\nformats by analyzing underflow and overflow trade-\noffs based on fractional length. Contrary to these\nstudies focusing on the training phase, our paper in-\nvestigates underflow’s impact on PTQ of LLMs for\nthe first time and introduces an enhanced integer\nformat to combat it.\n3 Improving PTQ for Weight and\nActivation Quantization\nWe aim to advance LLM quantization beyond the\nrealms of 4-bit weight-only PTQ or W8A8 PTQ by\ninvestigating the combined effects of weight and ac-\ntivation quantization. When quantizing both weight\nand activation, it is important to note that LLMs dis-\nplay distinct weight and activation characteristics.\nFor example, OPT has been found to have 0.1% ac-\ntivation outliers by (Dettmers et al., 2022), whereas\nGLM-130B (Zeng et al., 2023) reported 30% of\noutliers in its model. In the context of weight, due\nto varied weight distributions across models, OPT-\n66B experiences a substantial perplexity increase\nin the wikitext benchmark with INT4 weights, soar-\ning from 9.34 to 110 (Frantar et al., 2023), whereas\nGLM-130B shows no performance degradation on\nthe MMLU benchmark when INT4 weights are\napplied (Zeng et al., 2023). We posit that these\ndiscrepancies arise from variances in pre-training\nconfigurations such as datasets, learning rates, layer\nstructures, and self-attention directionality, as well\nas options designed for efficient inference, such\nas operation fusion techniques like layernorm fu-\nsion. Significantly, existing PTQ research has over-\nlooked these unique traits intrinsic to each model\nthat are pivotal for the combined optimization of\nactivation and weight quantization. Therefore, we\ndelve into the weight and activation distributions of\nwidely-used OPT and LLaMA models during quan-\ntization to understand PTQ limitations and develop\nnovel methods to address them.\n3.1 Model Analysis: OPT vs. LLaMA\nTo understand the adverse effects of quantization\non restricting dynamic range, we examine the\nminimum and maximum values (Min-Max range)\nacross the layers of LLMs. Fig. 1(a) illustrates\nthe computation patterns within a layer of LLMs\nand Fig. 1(b) displays Min-Max range of activa-\ntions (left) and weights (right) as operands of ma-\ntrix multiplication for each FC layer in OPT and\nLLaMA. Notably, there are contrasting trends in\n14728\nMin-Max ranges; OPT has a broad activation range\nbut a narrow weight range, while LLaMA exhibits\nthe opposite. This distinction stems from the way\nthese LLMs process activations at layernorm. As\ndepicted in Fig. 1(a), in OPT, the layernorm pa-\nrameters are fused to the subsequent FC layer’s\nweights (Fig. 1(a) top), allowing only normalized\nactivation to enter the FC layer. Conversely, lay-\nernorm is not fused in LLaMA (Fig. 1(a) below),\nresulting in scaled activation as input to FC layers.\nAlthough layernorm fusion preserves functionality\nin full-precision computation, this presence or ab-\nsence of layernorm fusion in activation processing\ncontributes to significantly distinct behaviors under\nquantization, as will be discussed in the following\nsections.\nAnother insightful finding from our model anal-\nysis is the variation in activation diversity based on\nsequence lengths. Fig. 1(c) displays the Min-Max\nrange as sequence length varies from 128 to 2048\n(Orange: OPT-6.7B, Blue: LLaMA-7B). Notably,\nOPT’s activation range remains stable across se-\nquence lengths, while LLaMA’s activation range\nexpands, suggesting challenges in range calibration\nfor quantization. Fig. 1(d) contrasts maximum val-\nues per channel for OPT and LLaMA at varying\nsequence lengths. OPT displays consistent out-\nliers at the same channels, dominating its activa-\ntion dynamic ranges. In contrast, LLaMA’s outliers\nincrease in magnitude and shift across channels,\nindicating varied activation dynamic ranges. This\ndistinction in activation diversity is significant for\nquantization. While PTQ generally presumes con-\nsistent dynamic ranges for calibrating quantization\nranges \"offline\", these findings emphasize the ne-\ncessity of considering distinct activation dynamic\nrange and incorporating sequence length into cali-\nbration. The following sections discuss methods to\noptimize weight and activation quantization, build-\ning on these model-specific insights.\n3.2 Activation-Quantization-Aware Scaling\nThe distinct properties of outliers in weights and ac-\ntivations illustrated in Fig. 1(b) pose challenges of\napplying prior scaling techniques. Fig. 2 illustrates\nthe absolute maximum of (a) input activations and\n(b) weights at the \"Key\" layers (for self-attention)\nin OPT-6.7B when different scaling methods are\napplied. Specifically, SmoothQuant (Xiao et al.,\n2022) (SQ) scales activation for 8-bit quantization,\nbut descales weights, resulting in a more diverse\nFigure 2: Absolute max value of (a) input activation and\n(b) weight after scaling by each method (OPT-6.7B).\nWe observed that these trends were significantly pro-\nnounced in OPT models due to large outliers. (See\nFig. 6 for the same plot for LLaMA.)\nand quantization-sensitive range for weight. On the\nother hand, AWQ (Lin et al., 2023) scales weights\nfor 4-bit quantization but significantly increases\nactivation diversity, making activation quantization\nproblematic. In other words, the existing scaling-\nbased PTQ techniques such as SQ and AWQ cannot\nresolve the issue of conflicting trends in activation\nand weight outliers. To address this, we introduce\nactivation-quantization-aware scaling (AQAS), a\nhybrid of SQ and AWQ. AQAS aims to find scal-\ning values that minimize the output error caused by\nquantized weights and activations. We use mean\nsquared error (MSE) loss as the objective function,\naligning with previous studies on layer-wise opti-\nmization (Nagel et al., 2020; Frantar et al., 2022).\nOur objective function is as follows:\nargmin\ns\n||Q(W·diag(s))Q(diag(s)−1 ·X)−WX||2\n2\n(1)\nWe define the weight W ∈RM×C, scale factor\ns ∈RC, and activation X ∈RC×T , where M rep-\nresents the output feature dimension, Crepresents\nthe input feature dimension, and T denotes the\nnumber of tokens. Fig. 2 demonstrates that AQAS\nconsiders activation quantization’s impact to adjust\nactivation magnitudes, easing activation quantiza-\ntion. Additionally, as compared to SQ, AQAS ad-\njusts weight magnitudes more moderately, making\n4-bit weight quantization feasible.\n3.3 Sequence-Length-Aware Calibration\nAs shown in Fig. 1(c), variation in activation di-\nversity depending on the sequence length affects\nthe quantization performance. Specifically, weight-\nupdate-based quantization like OPTQ (Frantar\net al., 2023) struggles with models like LLaMA\nthat have increasing activation diversity during cal-\nibration. To delve deeper into this phenomenon,\n14729\nSequence Length 64 128 512 2048 std\nFP (LLaMA-7B) 70.92 -\nINT4 69.37 -\nINT4 OPTQ 68.14 69.89 65.96 65.19 2.54\nINT4 AQAS+OPTQ69.99 70.48 68.92 70.20 0.68\nTable 1: W4A8 Quantization zero-shot evaluation of\nCommonSenseQA (average score of PIQA, Winogrande\nand Arc_easy, default calibration sequence length is\n2048)\nwe analyze the approach adopted by OPTQ, which\nemploys weight adjustments in response to quan-\ntization error using activation Hessian, formulated\nas follows (Frantar et al., 2023):\nδF = −wq −quant(wq)\n[H−1\nF ]qq\n·(H−1\nF ):,q (2)\nHi = ∂2E\n∂W2\ni,:\n= 2XXT , (3)\nwhere X denotes the layer input activation, W\nis weights of linear layer, wq is weight element\nto quantize, and δdenotes optimal weight update\nrecovering quantization error. We examine the\nweight update ratio, (H−1\nF ):,q/[H−1\nF ]qq, represent-\ning the second derivative of quantization error (E),\nto assess changes in weights due to OPTQ. Fig. 3(a)\nshows the weight update ratio for OPT and LLaMA\nwith varying calibration sequence lengths. OPT\nremains relatively consistent, while LLaMA dis-\nplays varying weight update ratios for varying se-\nquence length, suggesting activation diversity af-\nfects OPTQ’s weight updates.\nThis sensitivity of OPTQ updates prompts us to\nfurther explore its implications for performance.\nWe evaluate the zero-shot performance of OPTQ\nfor W4A8 quantization by varying the calibra-\ntion sequence length on PIQA, Winogrande, and\nArc_easy tasks from CSQA (Bisk et al., 2019; Sak-\naguchi et al., 2019; Clark et al., 2018), which have\nsequence lengths ranging from tens to hundreds\n(note that the type of calibration dataset was kept\nconsistent). Table 1 reveals that when the cali-\nbration sequence length (e.g., 512 or 2048) sig-\nnificantly deviates from task’s sequence lengths,\nOPTQ’s performance suffers (up to 4% degrada-\ntion), even falling below basic nearest-rounding\nquantization. However, when the sequence lengths\nare aligned (e.g., 64 or 128), OPTQ performs ex-\nceptionally well.\nFigure 3: (a) Comparison of weight update ratio in Eq. 2\nin OPT-6.7B, LLaMA-7B, and LLaMA-7B with AQAS\nscaling. (b) Minimum input activation range for the\nquery layer in three models: W4A8 (calibrated with 128\nand 2048 sequence lengths) and full-precision (FP), all\nevaluated under an input sequence length of 128.\nThe large standard deviation in accuracies for\nmatching versus non-matching sequence lengths\nsuggests that LLaMA’s activation diversity substan-\ntially impacts OPTQ’s accuracy. To mitigate this,\nwe propose the sequence-length-aware calibration\n(SLAC) method. This approach involves determin-\ning the expected sequence length during the target\ntask’s inference phase and aligning the sequence\nlength of the calibration dataset accordingly. Such\na task-specific PTQ calibration process enhances\nthe robustness and accuracy of the model’s infer-\nence. The efficacy of SLAC, particularly in the\nCSQA benchmark, is substantiated by experiments\ndetailed in Sec. 5.3.\nThe effectiveness of the SLAC method is evi-\n14730\ndent when comparing the dynamic range of quan-\ntized models with their full-precision counterparts.\nFig. 3 (b) demonstrates that using calibration data\naligned with the input sequence length (calib-128)\nresults in a dynamic range more consistent with\nthat of the full-precision model (FP), unlike mod-\nels calibrated with mismatched sequence lengths\n(calib-2048).\nIntegrating SLAC with AQAS effectively en-\nhances weight and activation quantization. As il-\nlustrated in Fig. 3(a), AQAS efficiently mitigates\nthe sensitivity to input sequence length regarding\nweight updates. Moreover, Table 1 shows that\nthe standard deviation related to the calibration\ndataset’s length is significantly reduced from 2.54\nto 0.68 through AQAS. Consequently, combining\nAQAS with OPTQ proves advantageous for infer-\nences across diverse sequence lengths, and employ-\ning the SLAC method for calibration according to\nthe target dataset’s sequence length further bolsters\nperformance.\n4 Overcoming PTQ Underflow for LLMs\nBy employing AQAS to address activation quanti-\nzation errors in weight scaling, and utilizing SLAC\nto align the sequence length of the calibration\ndataset with that of the target inference, we achieve\na substantial improvement in the performance of\nour W4A8 models. However, we encounter persis-\ntent performance degradation issues. In this sec-\ntion, we unveil \"underflow\" issues as a previously\noverlooked cause of accuracy degradation in PTQ\napplied to LLMs and propose a new numerical for-\nmat to mitigate this problem.\n4.1 Observations\nWe identify underflow as a main contributor to per-\nformance degradation. To dissect the causes of\ndegradation when converting the weights of the\nscaled model to 4-bit, we split the quantization\nerror into two parts: rounding error (∆r) and un-\nderflow error (∆u). The rounding error accounts\nfor the error when the quantized value is non-zero,\nwhereas the underflow error represents the error\noccurring when the quantized value rounds to zero.\nBy considering the total error (∆) induced by quan-\ntization as a combination of ∆u and ∆r, we can\nexpress the expected output quantization error as\nfollows:\nE[(WX −(W + ∆u + ∆r)X)2] (4)\n= E[(∆uX)2] + E[(∆rX)2] + E[2(∆uX∆rX)].\nFig. 4(a) exemplifies the underflow issues, illustrat-\ning the distinct impacts of quantization errors on\nfinal model accuracy, measured as perplexity. The\nfigure highlights that setting small values near zero\nto exactly zero, while leaving other values unquan-\ntized, impairs performance. In contrast, quantizing\nlarger values and precisely representing those near\nzero significantly improve accuracy. Fig. 4(b) pro-\nvides a breakdown of error terms across layers in\nOPT W4A8, indicating a correlation between high\ntotal error and substantial underflow error. This un-\nderlines the necessity for a method that effectively\naddresses underflow errors.\n4.2 Integer with Denormal Representation\nInspired by our observations and the denormal num-\nbers in floating-point representation, we introduce\na new integer format called integer with denormal\nrepresentation (dINT). As illustrated by Fig. 4(c),\ndINT uses two bins around zero to ensure lower\nmagnitudes are effectively represented. In b-bit\nquantization, two values are reserved for special\ncases, so the quantization range represents integers\nfrom 0 to 2b −3. These special values in dINT have\nmagnitudes equal to half of the chosen step size ,\nwhich is a power of two to enable computation by\nsimple bit-shift operations. Our experimental find-\nings have confirmed that this choice of half-step\nsize consistently delivers the most robust perfor-\nmance, surpassing other special values designed\nfor bit shifting, as elaborated in Appendix A.5.\nThe quantization and dequantization procedures\nfor dINT are detailed below:\nXint =\n\n\n\nc1,for s\n4 <n ≤3s\n4\nc2,for −3s\n4 ≤n< −s\n4\nclamp\n(\n⌈X\ns ⌋+ z,0,p\n)\n,else\n(5)\nXq =\n\n\n\ns\n2 ,for Xint = c1\n−s\n2 ,for Xint = c2\n(Xint −z) ·s,else\n(6)\nwhere prepresents the number of uniform steps,\ncalculated as p= 2b −3 for a given bit number b.\nThe step size sis obtained by dividing the quanti-\nzation range by p, and zis the zero-point for asym-\nmetric quantization. c1 and c2 denote the positive\n14731\nFigure 4: (a) INT4 without rounding sets small values near zero to zero, preserving the rest and causing performance\ndegradation. INT4 without underflow preserves only values near zero, improving performance. (b) Impact of\nunderflow error and rounding error on the output error. Significant impact of underflow error on the output error in\nINT4. (c) Proposed dINT4 preserves two small values near zero, preventing performance degradation. (d) Using the\nproposed dINT4 to reduce underflow error leads to a significant reduction in output error.\nFigure 5: (Blue) Values to be quantized. (Orange) INT4\nquantized values, evenly spaced. (Green) FP4 quantized\nvalues, dense resolution for small values but coarse\nresolution for large magnitudes. (Red) Proposed dINT4\nformat, balanced quantization range with a separate\nspecial value for small values.\nand negative special values in dINT that represent\nsmall magnitudes. These values are encoded with\ndistinct bits, analogous to encoding inf or NaN.\nDuring dequantization, if the value corresponds to\nc1 or c2, it is represented as a special value; other-\nwise, dequantization proceeds as in standard integer\nformats. Fig. 5 shows that dINT4 strikes a balance\nbetween INT4, which has uniform dynamic range\ncoverage but underflow issues, and FP4, which\ndensely represents small values to avoid underflow\nbut coarsely covers the dynamic range.\n4.3 Advantages\nFig. 4(d) showcases the benefits of dINT in reduc-\ning output quantization error. By plotting each term\nof Eq. 4, we observe that dINT primarily mitigates\nunderflow error, which substantially lowers the out-\nput error. Although in instances like layer 9, the\noutput error slightly increases due to a widened\nstep size causing a rise in rounding error, the mag-\nnitude of this increment is minimal. On the whole,\nHW Performance MAC Input Formats\nPrecision INT8 × INT8 dINT4 × INT8 (Savings)\nArea (µm2) 86.33 44.57 (1.93 ×)\nPower (mW) 0.1595 0.0624 (2.56 ×)\nTable 2: Evaluation of hardware performance of MAC\nunits (7nm, 1GHz).\ndINT is effective in most scenarios. Furthermore,\nwe design and synthesize a MAC unit using dINT\nand compare it to a traditional 8-bit integer MAC\nunit using Synopsys Design Compiler and a com-\nmercial 7nm technology (1GHz) for area efficiency\nevaluation. As shown in Table 2, dINT achieves\n1.93×and 2.56×savings in area and power con-\nsumption, respectively. This underscores dINT’s\neffectiveness in tackling underflow issues with min-\nimal output errors and its hardware implementation\nefficiency.\n5 Experimental Results\n5.1 Experimental Settings\nIn our experimental settings, we implement a com-\nprehensive evaluation to assess the effectiveness\nof our AQAS, SLAC, and dINT4 techniques in\nLLMs. This involves conducting quantized in-\nference with 8-bit activations and 4-bit weights\nacross a spectrum of tasks, encompassing language\nmodeling, reasoning, and the MMLU benchmark.\nTo enhance both computational and memory ef-\nficiency in activation quantization, we broaden\nour approach to incorporate the quantization of\n\"Value (for attention map calculation)\", which are\n14732\nWeight OPTQ A-bits W/V-bits OPT Family LLaMA Family\nScaling 125M 1.3B 2.7B 6.7B 13B 30B 7B 13B 30B\nBaseline 31.95 16.41 14.32 12.29 11.50 10.67 5.68 5.09 4.10\n- -\nINT8 INT4\n42.78 35.17 24.03 15.52 36.43 150.94 6.87 6.14 5.52\n✓ 36.04 19.35 15.61 15.16 15.82 25.23 7.57 5.85 5.13\nSQ - 41.31 23.48 33.86 1,596.83 897.25 19.43 7.08 6.27 6.17\n✓ 36.22 17.70 15.22 12.82 11.93 11.06 8.26 5.91 5.11\nAWQ - 44.17 25.04 15.88 322.85 670.00 4,246.25 6.75 5.84 5.26\n✓ 39.10 19.35 16.00 432.74 183.83 4,848.16 6.52 5.78 5.02\nAQAS\n- 36.57 17.68 15.34 13.42 12.19 11.08 6.69 5.81 5.14\n✓ 35.62 17.48 15.08 12.97 12.08 11.04 6.60 5.71 5.07\n✓ dINT4 34.92 17.28 15.03 12.89 11.98 11.04 6.48 5.67 4.72\nTable 3: PPL results of W4A8V4 (Weight-4bit, Activation-8bit,Value-4bit) at standard language modeling evaluation\nwith OPT and LLaMA family models, applying OPTQ (Frantar et al., 2023) with various weight scaling techniques\nand two numerical formats.\nspecifically cached to expedite the inference stage\nduring generation (Kwon et al., 2023). We com-\npare our methods against baseline techniques, in-\ncluding weight scaling of SQ (Xiao et al., 2022),\nAWQ (Lin et al., 2023), and weight update based\nmethod, OPTQ (Frantar et al., 2023). Task details,\nmodels, calibration methods, and quantization tech-\nniques used in the experiments are outlined in Ap-\npendix A.1, and an ablation study exploring aspects\nsuch as reducing precision to 3-bit, weight-only\nquantization with dINT, and other 4-bit formats is\ndetailed in Appendix A.6.\n5.2 Evaluation on Language Modeling Task\nWe first evaluate perplexity (PPL) as the language\nmodeling performance for various PTQ methods.\nTable 3 presents W4A8 quantization results with\ndifferent PTQ combinations. For OPT models,\nOPTQ generally reduces perplexity. However,\nwhen combined with weight scaling methods like\nSQ and AWQ for 4-bit weight quantization, there’s\na significant accuracy drop (i.e., spikes in PPL),\nwhich OPTQ cannot fully mitigate in most OPT\nmodels (except 6.7B and 13B). AQAS effectively\ncurtails the accuracy drop of 4-bit weight quan-\ntization, and combining it with OPTQ enhances\naccuracy. Utilizing dINT4 for 4-bit quantization\nfurther lowers perplexity, maintaining a gap of less\nthan 1.0 compared to the full-precision baseline,\nwith the exception of OPT-125M, which is sensi-\ntive to quantization. For LLaMA models, OPTQ\nwith 4-bit weight quantization raises perplexity due\nto increased activation diversity, as discussed in\nSec. 3.3. Weight scaling methods like AWQ and\nAQAS aid in performance recovery, and dINT4\nWeight OPTQ W/V-bits OPT LLaMA\nScaling 6.7B 13B 7B 13B\nBaseline 69.11 69.38 70.92 74.48\n- -\nINT4\n63.23 48.67 69.37 72.22\n✓ 64.42 56.62 64.94 71.69\nSQ ✓ 67.80 68.96 68.23 72.63\nAQAS ✓ 67.79 68.80 69.04 73.50\n✓ dINT4 68.19 68.96 68.74 73.31\nAQAS* ✓ INT4 67.48 68.24 70.48 73.50\n✓ dINT4 67.92 68.80 71.01 73.64\nTable 4: Average accuracy for CommonSense QA\n(CSQA) tasks including PIQA, Winogrande, and\nARC_easy. AQAS* denotes the AQAS method with\nthe SLAC approach (calibrating the dataset’s sequence\nlength to 128).\nfurther minimizes perplexity, keeping it within 1.0\nof the baseline. We detail the results of applying\nour proposed AQAS and dINT4 strategies to mod-\nels with over 60 billion parameters, specifically\nOPT-66B and LLaMA-65B, in Appendix A.2.\n5.3 Evaluation on Zero-shot Reasoning Tasks\nWe carry out experiments for the zero-shot Com-\nmonSense QA (CSQA) (Bisk et al., 2019; Sak-\naguchi et al., 2019; Clark et al., 2018) benchmark\nby comparing different quantization options, akin\nto prior tests. As noted in Sec. 3.3, LLaMA models\nundergo performance decline with OPTQ without\nweight scaling, whereas OPT models, less affected\nby activation diversity, show performance gains us-\ning OPTQ even without scaling. Among weight\nscaling techniques, AQAS exhibits superior perfor-\nmance, and employing the dINT4 format further\nenhances results.\n14733\nWeightOPTQA-bitsW/V-bits LLaMA Family\nScaling 7B 13B 30B 65B\nBaseline 35.20 47.15 58.50 63.60\n- -\nINT8 INT4\n28.05 40.82 48.40 57.20\n✓ 27.05 42.95 53.30 58.50\nSQ ✓ 29.32 43.12 52.83 59.30\nAQAS ✓ 30.8144.2353.6759.60\n✓ dINT4 31.00 44.73 55.50 61.40\nTable 5: Average MMLU accuracy. The detailed accu-\nracy for each item can be found in Table 7.\nDue to the shorter input sentences in zero-shot\nCSQA compared to the default OPTQ calibra-\ntion dataset, employing SLAC, which considers\nthe LLaMA models’ activation diversity based on\nsequence length, improves performance for both\nINT4 and dINT4 formats. However, aligning the\ncalibration length with the target task’s sequence\nlength for the OPT models does not result in signif-\nicant improvements. This can be attributed to the\nOPT models’ lower sensitivity to weight updates\ndue to activation diversity during the calibration\nprocess, as discussed in Section 3.3, which differs\nfrom the behavior of the LLaMA models.\nAs a result, we attain performance within 1%\nof full precision for both OPT and LLaMA mod-\nels using 8-bit activation and 4-bit weight, notably\nachieving full precision-equivalent performance in\nLLaMA-7B by comprehensively accounting for the\nmodel’s activation characteristics.\n5.4 Evaluation on In-Context Learning Tasks\nWe evaluate the MMLU benchmark on several op-\ntions that exhibited strong performance in language\nmodeling. To assess the efficacy of our proposed\nmethod in in-context learning, we conduct 5-shot\ninference. Given that OPT models are deemed\nunsuitable for the MMLU benchmark (Lin et al.,\n2023), we restrict the experiments to LLaMA mod-\nels. Consistent with language modeling results,\nAQAS, accounting for both weight and activation\nquantization errors, delivers the best performance.\nMoreover, effectively managing underflow error\nbolsters performance across all models, with a no-\ntable 2% performance enhancement observed in the\nLLaMA-30B model. To evaluate the efficacy of our\napproach on large-scale models, we further expand\nthe experiment to LLaMA-65B. The results demon-\nstrate that dINT4 significantly enhances MMLU\naccuracy by conserving small-magnitude values.\nDetailed results for each category within MMLU\nare provided in the Appendix A.3.\n6 Conclusion\nWe address Post-training Quantization (PTQ) in\nLarge Language Models (LLMs), specifically tar-\ngeting 4-bit weight and 8-bit activation (W4A8)\nquantization to boost computational efficiency.\nWe present Activation-Quantization-Aware Scal-\ning (AQAS) and Sequence-Length-Aware Calibra-\ntion (SLAC), refining PTQ by taking into account\nweights and activations, and aligning sequence\nlengths. To combat the underflow issue in W4A8\nquantization, where small magnitudes are rounded\ndown to zero, we introduce dINT, a hybrid for-\nmat blending integer and denormal representations.\nThrough extensive evaluations on LLMs such as\nOPT and LLaMA, we demonstrate marked im-\nprovements in task accuracy and adaptability. Addi-\ntionally, with the development of MAC units com-\npatible with dINT, we achieve a twofold increase\nin hardware efficiency.\n7 Limitation\nWe conducted a thorough analysis of model-\nspecific characteristics in LLMs and identified lim-\nitations in current PTQ methods. However, fur-\nther investigation is needed to understand the spe-\ncific phenomena observed in certain LLM mod-\nels during the pre-training process. Additionally,\nexploring more advanced collaborations of PTQ\ntechniques at lower bit precision for weights and\nactivations holds promise for future research.\nAcknowledgement\nThis work was supported by Institute of Infor-\nmation & communications Technology Planning\n& Evaluation(IITP) grants funded by the Korea\ngovernment(MSIT)(2020-0-01305, Development\nof AI Deep-Learning Processor and Module for\n2,000 TFLOPS Server, 2020-0-01297, Develop-\nment of Ultra-Low Power Deep Learning Pro-\ncessor Technology using Advanced Data Reuse\nfor Edge Applications), the Technology Innova-\ntion Program (1415178807, Development of In-\ndustrial Intelligent Technology for Manufacturing,\nProcess, and Logistics) funded By the Ministry\nof Trade, Industry & Energy(MOTIE, Korea), and\nthe National Research Foundation of Korea (NRF)\ngrant funded by the Korea government(MSIT)(No.\n2021R1A2C1013513).\n14734\nReferences\nMichael Andersch, Greg Palmer, Ronny Krashinsky,\nNick Stam, Vishal Mehta, Gonzalo Brito, and Sridhar\nRamaswamy. 2022. Nvidia hopper architecture in-\ndepth. https://developer.nvidia.com/blog/\nnvidia-hopper-architecture-in-depth/ .\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2019. Piqa: Reasoning about\nphysical commonsense in natural language.\nYelysei Bondarenko, Markus Nagel, and Tijmen\nBlankevoort. 2021. Understanding and overcoming\nthe challenges of efficient transformer quantization.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7947–7969, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nBrian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaa-\ncov, and Daniel Soudry. 2022. Logarithmic unbiased\nquantization: Simple 4-bit training in deep learning.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question\nanswering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457v1.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multi-\nplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\nTim Dettmers, Ruslan Svirschevski, Vage Egiazarian,\nDenis Kuznedelev, Elias Frantar, Saleh Ashkboos,\nAlexander Borzunov, Torsten Hoefler, and Dan Alis-\ntarh. 2023. Spqr: A sparse-quantized representation\nfor near-lossless llm weight compression.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2022. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\narXiv preprint arXiv:2210.17323.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh. 2023. OPTQ: Accurate quantization for\ngenerative pre-trained transformers. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nSudhanva Gurumurthi, Kijun Lee, Munseon Jang, Vi-\nlas Sridharan, Aaron Nygren, Yesin Ryu, Kyomin\nSohn, Taekyun Kim, and Hoeju Chung. 2021. Hbm3\nras: Enhancing resilience at scale. IEEE Computer\nArchitecture Letters, 20(2):158–161.\nMingxuan He, Choungki Song, Ilkon Kim, Chunseok\nJeong, Seho Kim, Il Park, Mithuna Thottethodi, and\nT. N. Vijaykumar. 2020. Newton: A dram-maker’s\naccelerator-in-memory (aim) architecture for ma-\nchine learning. In 2020 53rd Annual IEEE/ACM\nInternational Symposium on Microarchitecture (MI-\nCRO), pages 372–385.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding.\nMark Horowitz. 2014. Energy table for 45nm process.\nQing Jin, Jian Ren, Richard Zhuang, Sumant Hanu-\nmante, Zhengang Li, Zhiyu Chen, Yanzhi Wang,\nKaiyuan Yang, and Sergey Tulyakov. 2022. F8net:\nFixed-point 8-bit only multiplication for network\nquantization. In International Conference on Learn-\ning Representations.\nJin Hyun Kim, Shin-haeng Kang, Sukhan Lee, Hyeonsu\nKim, Woongjae Song, Yuhwan Ro, Seungwon Lee,\nDavid Wang, Hyunsung Shin, Bengseng Phuah, Ji-\nhyun Choi, Jinin So, YeonGon Cho, JoonHo Song,\nJangseok Choi, Jeonghyeon Cho, Kyomin Sohn,\nYoungsoo Sohn, Kwangil Park, and Nam Sung Kim.\n2021. Aquabolt-xl: Samsung hbm2-pim with in-\nmemory processing for ml accelerators and beyond.\nIn 2021 IEEE Hot Chips 33 Symposium (HCS), pages\n1–26.\nSe Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min\nYoo, Jin-Hwa Kim, Baeseong Park, Byeongwook\nKim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee.\n2022. AlphaTuning: Quantization-aware parameter-\nefficient adaptation of large-scale pre-trained lan-\nguage models. In Findings of the Association for\nComputational Linguistics: EMNLP 2022 , pages\n3288–3305, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th\n14735\nSymposium on Operating Systems Principles, SOSP\n’23, page 611–626, New York, NY , USA. Association\nfor Computing Machinery.\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim,\nand Eunhyeok Park. 2023a. Owq: Lessons learned\nfrom activation outliers for weight quantization in\nlarge language models.\nJanghwan Lee, Youngdeok Hwang, and Jungwook Choi.\n2023b. Finding optimal numerical format for sub-8-\nbit post-training quantization of vision transformers.\nIn ICASSP 2023 - 2023 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1–5.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, and Song Han. 2023. Awq: Activation-\naware weight quantization for llm compression and\nacceleration. arXiv.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\nRaghuraman Krishnamoorthi, and Vikas Chandra.\n2023. Llm-qat: Data-free quantization aware training\nfor large language models.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quanti-\nzation. In Proceedings of the 37th International Con-\nference on Machine Learning, ICML’20. JMLR.org.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nGunho Park, Baeseong Park, Minsub Kim, Sungjae Lee,\nJeonghoon Kim, Beomseok Kwon, Se Jung Kwon,\nByeongwook Kim, Youngjoo Lee, and Dongsoo Lee.\n2023. Lut-gemm: Quantized matrix multiplication\nbased on luts for efficient inference in large-scale\ngenerative language models.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2019. Winogrande: An adver-\nsarial winograd schema challenge at scale.\nXiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang\nWang, Swagath Venkataramani, Vijayalakshmi (Viji)\nSrinivasan, Xiaodong Cui, Wei Zhang, and Kailash\nGopalakrishnan. 2019. Hybrid 8-bit floating point\n(hfp8) training and inference for deep neural net-\nworks. In Advances in Neural Information Process-\ning Systems, volume 32. Curran Associates, Inc.\nXiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni,\nAnkur Agrawal, Xiaodong Cui, Swagath Venkatara-\nmani, Kaoutar El Maghraoui, Vijayalakshmi (Viji)\nSrinivasan, and Kailash Gopalakrishnan. 2020. Ultra-\nlow precision 4-bit training of deep neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 1796–1807. Curran Asso-\nciates, Inc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2022. Smoothquant: Accurate\nand efficient post-training quantization for large lan-\nguage models. arXiv preprint arXiv:2211.10438.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\nRepresentations.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\n14736\nA Appendix\nA.1 Experimental Details\nBaseline Setup. As comparative baselines for\nweight scaling, we employ SQ (Xiao et al.,\n2022) as the scale method for 8-bit activation\nquantization and AWQ (Lin et al., 2023) as the\nscaling method for 4-bit weight quantization. In\nterms of weight rounding, we evaluate both options\nprovided by OPTQ (Frantar et al., 2023), which\noffers additional optimization, and the standard\nnearest rounding method. As for numerical format,\nwe compare dINT with existing INT4 methods in\nterms of performance.\nTask and Models. We evaluate our proposed\napproach on various tasks, including language mod-\neling using Wikitext (Merity et al., 2016)), CSQA\n(PIQA (Bisk et al., 2019), WinoGrande (Sakaguchi\net al., 2019), and ARC easy (Clark et al., 2018)),\nas well as MMLU (Hendrycks et al., 2021)\nbenchmarks. For benchmark models, we assess a\nrange of options across OPT and LLaMA models,\nranging from 125M to 66B, which are widely used\nLLMs.\nQuantization Settings. We apply quantiza-\ntion to both the weights and activations of all\nmatrix multiplications in the Decoder layer. We\nconduct our experiments by implementing the\nquantizer within the PyTorch framework. For\nactivations, except for Value, we apply 8-bit\nquantization, while for memory-intensive compo-\nnents such as weights and Value, we utilize 4-bit\nquantization. Similar to commonly used methods\nfor LLM quantization (Dettmers et al., 2022; Yao\net al., 2022), we apply token-wise quantization for\nactivations, and output channel-wise quantization\nfor weights. For Value, we apply channel-wise\nquantization, taking into account the dimensions\nwhere partial-sum accumulation occurs when\nmultiplied by the self-attention map. We apply\nMin-Max asymmetric quantization determine the\nstep size and the zero-point for both activation and\nweight.\nCalibration Settings. During the calibra-\ntion process to find the weight scale, we follow\nthe calibration setting from the AWQ repository1.\nFor the attention operation, we adjust the cali-\n1https://github.com/mit-han-lab/llm-awq\nWikitext PPL OPT-66B LLaMA-65B\nFP16 baseline 10.09 3.53\nINT4 3,222.02 4.80\nAQAS + dINT4 + OPTQ10.32 4.13\nTable 6: Wikitext perplexity for W4A8V4 inference on\nOPT-66B and LLaMA-65B.\nbration process by modifying the objective of\nEq. 1 to minimize the distortion of the attention\noutput. We use a randomly extracted dataset from\nPile (Gao et al., 2020) for AWQ (Lin et al., 2023),\nSmoothQuant (Xiao et al., 2022), and AQAS\nmethods. When calibrating weights with OPTQ,\nwe follow the baseline calibration setting provided\nin the OPTQ repository2. We use a subset of the\nC4 dataset, randomly selecting 128 samples with a\nsequence length of 2048.\nA.2 Language modeling in >60B Models\nTable 6 To assess the effectiveness of our approach\non large models, we conduct language modeling\nexperiments on OPT-66B and LLaMA-65B, with\nthe objective of determining whether our method\nperforms well even on models with over 60 billion\nparameters. demonstrates that proposed scaling\nmethod and numerical format can significantly re-\nduce perplextiy in language modeling task.\nA.3 Few-shot MMLU Benchmarks\nThe results for the each category in the 5-shot\nMMLU benchmark for LLaMA models are dis-\nplayed in Table 7. As demonstrated in Table 7,\nAQAS exhibits higher accuracy compared to other\nscaling methods, emphasizing the importance of\nconsidering both weight and activation quantiza-\ntion. Furthermore, it’s noteworthy that the use\nof dINT, which effectively mitigates underflow,\nachieves the highest accuracy.\nA.4 Finding Scales for AQAS\nTo automatically determine the channel-wise scale\nfactor in AQAS, it is necessary to select representa-\ntive values for both activation and weight channels.\nIn SQ (Xiao et al., 2022), the maximum magni-\ntude was used as the criterion, while in AWQ (Lin\net al., 2023), the absolute mean value was used\nto explore the scale factor. As shown in Table 8,\nwe explore both cases and found that selecting the\n2https://github.com/IST-DASLab/gptq\n14737\nLLaMA 7B\nScalingOPTQA-bitsW/V-bitsHums STEM Social OtherAvg.\nBaseline 33.90 30.50 38.20 38.2035.20\n- -\nINT8 INT4\n28.65 25.75 26.13 31.1528.05✓ 26.99 24.88 25.71 30.4127.05\nSQ ✓ 27.80 29.66 27.40 33.0429.32\nAQAS ✓ 29.61 29.85 30.65 33.6230.81\n✓ dINT429.67 29.26 32.24 33.3731.00\nLLaMA 13B\nScalingOPTQA-bitsW/V-bitsHums STEM Social OtherAvg.\nBaseline 44.80 36.40 54.20 53.2047.15\n- -\nINT8 INT4\n36.96 35.16 45.56 47.1940.82✓ 40.38 34.10 48.94 49.2342.95\nSQ ✓ 40.96 34.26 48.72 49.2043.12\nAQAS ✓ 40.74 35.02 51.51 50.9644.23\n✓ dINT442.64 35.79 50.83 50.3144.73\nLLaMA 30B\nScalingOPTQA-bitsW/V-bitsHums STEM Social OtherAvg.\nBaseline 56.40 46.70 67.30 63.6058.50\n- -\nINT8 INT4\n46.00 39.40 54.50 54.5048.40✓ 50.20 42.60 61.90 59.7053.30\nSQ ✓ 49.65 43.31 59.86 59.6552.83\nAQAS ✓ 51.75 43.21 60.68 59.5353.67\n✓ dINT452.90 44.00 65.00 61.1055.50\nLLaMA 65B\nScalingOPTQA-bitsW/V-bitsHums STEM Social OtherAvg.\nBaseline 61.90 52.10 73.40 67.6063.60\n- -\nINT8 INT4\n54.10 46.40 66.90 62.5057.20✓ 56.00 47.80 67.20 63.9058.50\nSQ ✓ 57.70 47.50 67.90 64.3059.30\nAQAS ✓ 57.20 47.80 69.50 64.8059.60\n✓ dINT459.50 50.40 70.70 65.8061.40\nTable 7: MMLU accuracy on LLaMA models.\nmaximum magnitude as the representative value of-\nten yielded better performance. Similar to previous\nresearch (Lin et al., 2023), we use a grid search to\nfind the appropriate scale, and after determining the\nscale factor, we make adjustments by additionally\nclipping the weights.\nA.5 Sweep of the Special Value in dINT\nThe dINT format defines the special value cas half\nof the step size s. If we change this value, the\nmagnitude of the state representing small values\nwill differ. We conduct additional sweeps with\ndifferent power-of-two values (e.g., 0.25, 0.125)\nto observe the impact in Table 9. In most cases,\nsetting cto 0.25 times the sproves to be a good\nchoice, but in the case of the OPT-125M model, it\nshows a significant increase in perplexity. To select\na value that generally works well, we set cto be\nhalf of s.\nA.6 Ablation Study\nReducing Precision to 3 Bits. To achieve addi-\ntional memory savings, we conduct experiments\nWikitext\nAQASOPTQ OPT LLaMA125M 1.3B 2.7B 6.7B 13B 30B 7B\nBaseline 31.95 16.41 14.32 12.29 11.50 10.675.68\nMean - 37.46 18.16 15.43 13.08 12.11 11.066.72✓ 36.07 17.50 15.19 12.9912.07 11.026.67\nMax - 36.57 17.68 15.34 13.42 12.19 11.086.69✓ 35.62 17.48 15.08 12.9712.08 11.046.60\nPIQA\nAQASOPTQ OPT LLaMA125M 1.3B 2.7B 6.7B 13B 30B 7B\nBaseline 63.00 71.71 73.78 76.28 75.90 77.5878.35\nMean - 61.86 70.78 72.9175.5774.86 76.6677.42✓ 61.8171.2272.85 75.14 74.97 77.2077.37\nMax - 61.21 70.51 72.69 74.27 75.14 77.1577.26✓ 62.3071.1173.5675.0875.84 77.4876.28\nWinoGrande\nAQASOPTQ OPT LLaMA125M 1.3B 2.7B 6.7B 13B 30B 7B\nBaseline 50.28 59.51 61.01 65.43 65.11 73.0167.09\nMean - 51.54 58.88 60.14 65.5965.5167.56 64.96✓ 49.96 57.6261.64 65.8264.88 68.0364.80\nMax - 52.41 60.62 61.6465.27 64.88 67.4063.77✓ 49.72 58.17 60.14 63.77 65.2768.11 65.59\nTable 8: Comparing the performance of AQAS when\nexploring channel-wise quantization using the criteria\nof absolute mean and max values.\nc1 / s OPT LLaMA Avg.125M 1.3B 2.7B 6.7B 7B\nBaseline 31.95 16.41 14.32 12.29 5.68 16.13\n0.50 34.92 17.28 15.03 12.89 6.48 17.32\n0.25 35.84 17.22 14.96 12.83 6.28 17.43\n0.125 35.20 17.23 14.97 12.84 6.32 17.32\nTable 9: dINT4’s special value sweep, W4A8V4 in-\nference with AQAS+OPTQ. Where c1 is the positive\nspecial value, and sis the step size.\nWeightOPTQW/V-bits OPT LLaMA\nscaling 125M 2.7B 6.7B 7B\nBaseline 31.95 14.32 12.29 5.68\n- - INT3 1.7e3 4.3e4 1.2e4 94.97\ndINT3 127.94 8.7e3 55.24 10.99\nAQAS ✓ INT3 54.84 36.38 69.45 24.85\ndINT3 46.34 20.67 17.42 10.04\nTable 10: Perplexity is assessed using standard language\nmodeling on the Wikitext dataset, with activations quan-\ntized to 8 bits and weights and values to 3 bits. We em-\nploy AQAS and OPTQ and compare the performance of\nINT3 and dINT3. Notably, dINT3 considerably reduces\nperformance degradation.\nin which we retain 8-bit activation while reducing\nweight and Value precision to 3 bits. As shown\nin Table 10, as bit precision decreases and the\nimpact of underflow becomes more significant, the\neffectiveness of dINT becomes more pronounced.\n14738\nPrecisionFormatMethod OPT125M 1.3B 2.7B 6.7B 13B\nFP16 baseline 31.95 16.41 14.32 12.29 11.50\nW3A16 g=128\nINT3 RTN 58.37 195.10 499.39 39.18 29.37OPTQ41.93 18.53 15.79 13.13 12.01AWQ41.45 18.56 15.63 12.99 12.03\ndINT3RTN 54.53 21.70 24.15 15.80 13.14OPTQ39.6018.2615.52 12.9911.91AWQ39.4618.2915.47 12.9712.03\nTable 11: Performance comparison of W3A16 infer-\nence results with various state-of-the-art methods when\napplying group-wise quantization (group size: 128).\nPrecisionFormatMethod OPT125M 1.3B 2.7B 6.7B 13B\nFP16 baseline 31.95 16.41 14.32 12.29 11.50\nW4A16 g=128\nINT4 RTN 35.52 17.69 15.12 13.02 11.89OPTQ34.23 16.92 14.69 12.51 11.60AWQ33.96 16.85 14.6112.4411.60\nFP4 RTN 39.13 18.25 15.54 13.35 12.14OPTQ37.42 18.06 15.19 12.84 11.91\ndINT4RTN 34.40 17.06 14.83 12.75 11.79OPTQ34.04 16.82 14.64 12.4711.59AWQ33.66 16.78 14.56 12.4411.61\nTable 12: Performance comparison of W4A16 infer-\nence results with various state-of-the-art methods when\napplying group-wise quantization (group size: 128).\nBy solely changing the numerical format without\napplying weight scaling, we are able to signifi-\ncantly reduce the perplexity of the LLaMA-7B\nmodel from 94.97 to 10.99. This underscores the\ninfluence of underflow on model performance.\nWeight-Only Quantization Method with dINT.\ndINT, as a numerical format, can be integrated\nwith existing PTQ methods. We combine the\ndINT format with state-of-the-art PTQ methods\nfor LLMs, namely OPTQ and AWQ, and compare\ntheir performance with the integer format. As\nshown in Table 11 and Table 12, dINT outperforms\nthe traditional integer format in both 3-bit and\n4-bit quantization. This indicates that underflow\nsignificantly affects the performance of weight\nquantization in LLMs.\nOther 4-Bit Formats. To compare the perfor-\nmance of 4-bit quantization formats, we evaluate\nperformance by applying integer, floating-point,\nand dINT4 to the weights, without considering ac-\ntivation quantization. We employ a 4-bit floating-\npoint (FP4), consisting of a single sign bit and\nthree exponent bits. While alternative configura-\ntions with different exponent and mantissa bits are\navailable, we experimentally determine the neces-\nsity of a 3-bit exponent for the FP4. Additional\nModel PrecisionW4 formatWikitext PIQA MMLU\nLLaMA-7B\nFP16 baseline 5.68 78.29 35.20\nW4A16\nFP4 (1-1-2)165582.55 51.69 26.88\nFP4 (1-2-1)26.52 62.84 27.31\nFP4 (1-3-0)6.30 76.77 31.46\ndINT4 6.07 77.91 32.53\nLLaMA-13B\nFP16 baseline 5.09 78.78 47.15\nW4A16\nFP4 (1-1-2)74763.98 52.29 24.72\nFP4 (1-2-1)7.95 74.65 31.54\nFP4 (1-3-0)5.56 78.62 40.76\ndINT4 5.38 79.05 44.35\nLLaMA-30B\nFP16 baseline 4.10 80.96 58.50\nW4A16\nFP4 (1-1-2)34027.07 51.52 25.32\nFP4 (1-2-1)9.10 71.22 32.05\nFP4 (1-3-0)4.57 79.71 53.50\ndINT4 4.36 80.41 55.87\nTable 13: Experiments on various configurations of 4-\nbit floating-point: FP4 (1-e-m) represents floating-point\nformat with a 1-bit sign bit, e-bit exponent, and m-bit\nmantissa. We conduct experiments on Wikitext PPL,\nPIQA accuracy, and MMLU average accuracy. Among\nFP4 configurations, a 3-bit exponent exhibits the best\nperformance, while dINT surpassing it.\nPrecisionFormatOPTQ OPT\n125M 1.3B 2.7B 6.7B 13B\nFP16 baseline 31.95 16.41 14.32 12.29 11.50\nW4A16\nINT4 - 43.15 29.90 19.70 14.18 12.89\n✓ 36.29 17.68 15.15 12.88 11.73\nFP4 - 42.17 18.52 16.01 13.38 12.33\n✓ 37.52 18.31 15.39 13.18 11.89\ndINT4 - 37.16 18.10 15.53 13.75 12.06\n✓ 35.10 17.30 14.94 12.65 11.68\nTable 14: Comparing the performance of various for-\nmats in weight-only quantization for the language mod-\neling task, using both dINT and OPTQ together shows\nthe best performance.\nFigure 6: The variation in the absolute max values of\nweights and activations when applying weight scaling\nin LLaMA-7B.\ndetails can be found in Table 13. As shown in\nTable 14, FP4 achieves some performance improve-\nment compared to uniform quantization due to its\nwider dynamic range. However, dINT4 outper-\nforms the other two formats by effectively repre-\nsenting a wide range of values with uniform inter-\nvals while accurately representing small values. It\ndemonstrates better performance and good compat-\nibility with existing optimization techniques such\nas OPTQ.\n14739",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.8686478137969971
    },
    {
      "name": "Computer science",
      "score": 0.6477718353271484
    },
    {
      "name": "Arithmetic underflow",
      "score": 0.5640478134155273
    },
    {
      "name": "Arithmetic",
      "score": 0.5469451546669006
    },
    {
      "name": "Language model",
      "score": 0.5030414462089539
    },
    {
      "name": "Vector quantization",
      "score": 0.4870010018348694
    },
    {
      "name": "Computation",
      "score": 0.47065725922584534
    },
    {
      "name": "Algorithm",
      "score": 0.39698055386543274
    },
    {
      "name": "Computer engineering",
      "score": 0.38486406207084656
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33430033922195435
    },
    {
      "name": "Speech recognition",
      "score": 0.31217706203460693
    },
    {
      "name": "Mathematics",
      "score": 0.23203936219215393
    },
    {
      "name": "Programming language",
      "score": 0.20880606770515442
    }
  ]
}