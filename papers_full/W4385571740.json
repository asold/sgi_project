{
  "title": "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies",
  "url": "https://openalex.org/W4385571740",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2130773608",
      "name": "Shiyue Zhang",
      "affiliations": [
        "University of North Carolina Health Care",
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2130569026",
      "name": "Shijie Wu",
      "affiliations": [
        "University of North Carolina Health Care",
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2007975086",
      "name": "Ozan Irsoy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131496537",
      "name": "Steven Lu",
      "affiliations": [
        "Johns Hopkins University",
        "University of North Carolina Health Care",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2070810387",
      "name": "Mohit Bansal",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A2023626662",
      "name": "Mark Dredze",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory",
        "University of North Carolina Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A1857804699",
      "name": "David Rosenberg",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University",
        "University of North Carolina Health Care"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4229005866",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W4307535256",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2970692082",
    "https://openalex.org/W4317897818",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W1503398984",
    "https://openalex.org/W4310299640",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4385571048",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W3133644679",
    "https://openalex.org/W2099057450",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4297801368",
    "https://openalex.org/W2899852118",
    "https://openalex.org/W3154046074",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4313447172",
    "https://openalex.org/W3125356501",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W3118026775",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W2257317309",
    "https://openalex.org/W2133564696"
  ],
  "abstract": "Shiyue Zhang, Shijie Wu, Ozan Irsoy, Steven Lu, Mohit Bansal, Mark Dredze, David Rosenberg. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9027–9050\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMIXCE: Training Autoregressive Language Models\nby Mixing Forward and Reverse Cross-Entropies\nShiyue Zhang♠∗ Shijie Wu♡ Ozan ˙Irsoy♡\nSteven Lu♡ Mohit Bansal♠ Mark Dredze♡♣ David Rosenberg♡\n♡Bloomberg ♠UNC Chapel Hill ♣Johns Hopkins University\nAbstract\nAutoregressive language models are trained by\nminimizing the cross-entropy of the model dis-\ntribution Qθ relative to the data distributionP –\nthat is, minimizing the forward cross-entropy,\nwhich is equivalent to maximum likelihood es-\ntimation (MLE). We have observed that models\ntrained in this way may “over-generalize”, in\nthe sense that they produce non-human-like\ntext. Moreover, we believe that reverse cross-\nentropy, i.e., the cross-entropy of P relative to\nQθ, is a better reflection of how a human would\nevaluate text generated by a model. Hence,\nwe propose learning with MIXCE, an objec-\ntive that mixes the forward and reverse cross-\nentropies. We evaluate models trained with this\nobjective on synthetic data settings (where P is\nknown) and real data, and show that the result-\ning models yield better generated text without\ncomplex decoding strategies.\nhttps://github.com/bloomberg/\nmixce-acl2023\n1 Introduction\nRapid advances in pre-trained large-scale autore-\ngressive language models (LMs) have dramati-\ncally improved the performance of a variety of\ntasks (Radford et al., 2019; Brown et al., 2020;\nZhang et al., 2022; Chowdhery et al., 2022). How-\never, these systems still struggle in many open-\nended generation settings, where they are asked\nto produce a long text following a short prompt.\nIn these cases, we seek systems that generate sen-\nsical, coherent, fluent, and engaging, or in short,\nhuman-like text (Pillutla et al., 2022).\nDifferent decoding strategies to generate such\ntext from pretrained LMs suffer from different de-\ngeneration problems. Unbiased sampling1 usually\n∗Work done during an internship at Bloomberg.\n1Unbiased sampling is vanilla random sampling, i.e., sam-\npling with temperature=1.0. It is also called ancestral sam-\npling (Eikema and Aziz, 2020) or pure sampling (Holtzman\nP P\nQ\nQ\n-logP(x)  \n \n-logQ(x)  \n \nReverse Cross-Entropy Forward Cross-Entropy\nx ~ Q x ~ P\nFigure 1: MIXCE combines two complementary driving\nforces: reverse CE helps narrow the model distribution\nQθ down when it is broader than data distribution P,\nwhile forward CE helps broaden Qθ out when it is nar-\nrower than P.2\nresults in incoherent and nonsensical text, while\ngreedy and beam searches often get stuck in repeti-\ntion loops (Holtzman et al., 2020). These observa-\ntions suggest that the learned LM distribution Qθ\nstill differs substantially from the human LM distri-\nbution P. A possible reason is that the autoregres-\nsive modeling ofQθ gives a non-zero probability to\nevery possible sequence of tokens, while many se-\nquences are impossible under P. Nevertheless, we\nstill hope that Qθ(x) is as small as possible when\nP(x) = 0. To this end, maximum likelihood esti-\nmation (MLE), i.e., minimizing the cross-entropy\n(CE) −Ex∼P[log Qθ(x)], is the most widely used\nobjective to train Qθ(x) using sequences sampled\nfrom P. In an idealized setting, with unlimited\ntraining data and model capacity, as well as a per-\nfect optimizer, fitting Qθ with MLE will learn a\ndistribution as close to P as we like. However, in\npractice, we only have finite and noisy data.\nWe argue that the MLE objective only weakly\npenalizes generations x from Qθ that are “bad”,\net al., 2020). We call it unbiased sampling because it allows\nunbiased exploration of the model distribution.\n2Note that log P(x) is infinite when P(x) = 0. But in\npractice, we use log P(x) =∑\ntlog(P(xt|x<t)+ϵ) to avoid\nlog 0and ϵ= 1e−30.\n9027\nin the sense that P(x) = 0. When Qθ puts a\nsmall amount of probability mass onto P(x) = 0\nspace, MLE cannot sufficiently discourage this\nbehavior (see Figure 3 in Appendix C). More-\nover, minimizing forward CE, −Ex∼P[log Qθ(x)],\nis equivalent to minimizing the forward KL di-\nvergence between P and Qθ, i.e., KL (P||Qθ) =\nEx∼P[log P(x)/Qθ(x)]. Forward KL has a zero-\navoiding property – avoiding Qθ(x) = 0 when\nP(x) ̸= 0(Murphy, 2012). Therefore, if there is\nnoise in the data, Qθ will try to cover the noise as\nwell, which leads the model to over generalize, in\nthe sense of putting non-trivial probability mass\nover P(x) = 0generations (Huszár, 2015; Theis\net al., 2016; Ott et al., 2018; Kang and Hashimoto,\n2020). As a result, we observe samples from the\nmodel deviating from human-like text. A common\nstrategy is to modify the decoding method, e.g.,\ntop-k, top-p, typical, contrastive (Fan et al., 2018;\nHoltzman et al., 2020; Meister et al., 2022; Li et al.,\n2022) samplings, to tailor the model distribution\nQθ in a post-hoc manner to avoid unwanted gener-\nations. In contrast, our approach differs: how can\nwe obtain a better Qθ to obviate the need for these\nsampling strategies?\nWe propose a novel training objective for autore-\ngressive LMs –MIXCE that Mixes the forward and\nreverse Cross-Entropies: −η·Ex∼P[log Qθ(x)] −\n(1 −η) ·Ex∼Qθ[log P(x)]. MIXCE can be under-\nstood in two ways. First, we want model genera-\ntions to be high-quality as well as diverse. Reverse\ncross-entropy reflects how we conduct human eval-\nuations, sampling from the model Qθ and evaluat-\ning it by the human P, where the focus is text qual-\nity. Forward cross-entropy emphasizes the diver-\nsity of model generations (Hashimoto et al., 2019).\nSecond, MIXCE works similarly to a mixture of the\nforward and reverse KL divergences. The reverse\nKL divergence (KL(Qθ||P)) is zero-forcing – forc-\ning Qθ(x) = 0when P(x) = 0– and thus more\nstrongly penalizes generating non-human-like sam-\nples compared to MLE. Overall, MIXCE combines\ntwo complementary driving forces to better fit Qθ\nto P (Figure 1). We elaborate on these interpreta-\ntions in § 3.1.\nUnfortunately, optimizing reverse cross-entropy\nis intractable because we do not know P. Hence,\nwe propose an approximation of the reverse cross-\nentropy (see § 3.2), which ends up being a self-\nreinforced loss function that encourages the model\nto produce generations in which it is already con-\nfident. This loss function has the same computa-\ntional complexity as forward cross-entropy, making\nMIXCE easy to implement and as fast as MLE.\nWe demonstrate the effectiveness ofMIXCE in\nboth a synthetic setting, where the “human” distri-\nbution P is known, as well as a real setting. For the\nsynthetic case, we evaluate six learning objectives:\nMIXCE, MIXCE∗(MIXCE without approxima-\ntion), forward KL (=MLE), reverse KL, the mix-\nture of two KL divergences, and Jensen–Shannon\n(JS) divergence. We show that MIXCE∗ works\nslightly worse than the mixture of KLs while out-\nperforming other objectives, and MIXCE works\nworse than MIXCE∗ but generally outperforms\nMLE. In real settings, we finetune GPT-2 (Rad-\nford et al., 2019) of different sizes on three English\ntext domains using MIXCE or MLE. Our results\nshow that, compared to MLE, unbiased sampling\nfrom MIXCE-finetuned models produces text that\nhas diversity (Meister et al., 2022) closer to that\nof human text, has higher Coherence (Su et al.,\n2022), has higher Mauve (Pillutla et al., 2021),\nand is preferred by humans. When using top- p\nsampling (Holtzman et al., 2020) and carefully tun-\ning p, generations from MLE-finetuned models are\nsimilar to those generated from MIXCE-finetuned\nmodels. Nonetheless, MIXCE models have tuned\npvalues closer to 1, implying a less noisy model\ndistribution. In addition, we modify the original\nMauve to make it more robust to spurious features\n(e.g., text length), under which MIXCE still im-\nproves over MLE when using unbiased sampling.\n2 Background and Related Work\n2.1 Autoregressive Language Modeling\nLanguage generation is mostly based on the au-\ntoregressive language modeling methodology. The\ngeneration of one word is conditioned on previ-\nously generated words, Qθ(xt|x<t), and the final\nprobability of the sequencexis the product of prob-\nabilities of each step, Qθ(x) = ∏\ntQθ(xt|x<t).\nEarly works build n-gram neural LMs (Bengio\net al., 2000) and then RNN-based LMs (Mikolov\net al., 2010), and now Transformers (Vaswani et al.,\n2017) have become the dominant architecture. Lan-\nguage generation models have either a decoder-\nonly (Mikolov et al., 2010) or an encoder-decoder\narchitecture (Sutskever et al., 2014; Bahdanau et al.,\n2015). In this work, we focus on decoder-only\nLMs. In recent years, many large-scale pre-trained\ndecoder-only LMs have been introduced (Radford\n9028\net al., 2019; Brown et al., 2020; Zhang et al., 2022;\nChowdhery et al., 2022). They can be finetuned for\ndownstream tasks and even perform surprisingly\nwell in a zero-shot or few-shot manner. Despite the\nimpressive performance, language degeneration is\none of the key issues that remain to be solved.\n2.2 Language Degeneration\nAccording to Holtzman et al. (2020), language de-\ngeneration refers to output text that is bland, in-\ncoherent, or gets stuck in repetitive loops . It is\nwidely observed in open-ended generations from\npretrained LMs. Two commonly observed pat-\nterns of degeneration are the incoherent text from\nunbiased sampling and the repetitive text from\ngreedy or beam search. Degeneration also appears\nin sequence-to-sequence generation tasks but in a\nslightly different form (Stahlberg and Byrne, 2019).\nThere is no agreement on what causes degen-\neration. Ott et al. (2018) attribute it to data noise\nand the smooth class of model functions. It is\ninherent in the model’s structure to have support\neverywhere, in particular, because all probabilities\nare produced by softmax, which is strictly posi-\ntive. Therefore, Hewitt et al. (2022) assume that an\nLM distribution is the true data distribution plus a\nuniform-like smoothing distribution. Based on the\nobservation that human-like text has a large but not\ntoo large likelihood under the learned LM distribu-\ntion (Zhang et al., 2021), a lot of works propose em-\npirically useful decoding methods beyond unbiased\nsampling and greedy/beam search (Fan et al., 2018;\nHoltzman et al., 2020; Eikema and Aziz, 2020;\nBasu et al., 2021; Meister et al., 2022; Li et al.,\n2022; Hewitt et al., 2022; Su et al., 2022; Krishna\net al., 2022). One of these approaches is the canoni-\ncal top-p(or nucleus) sampling method (Holtzman\net al., 2020), which samples from top tokens that\ntake up pproportion (e.g., 95%) of the probability\nmass at each decoding step. Even though these\ndecoding methods work impressively well, they\nare post-hoc fixes rather than learning the LM ac-\ncurately in the first place. Therefore, some other\nworks criticize the MLE training objective and pro-\npose alternative loss functions.\n2.3 Objectives Beyond MLE\nUnlikelihood training (Welleck et al., 2020; Li\net al., 2020) was proposed to penalize repetition\n(or any undesirable phenomenon) explicitly during\ntraining. The idea is to minimize the likelihood\nof a set of negative tokens at each generation step\nduring training. The selection of negative tokens is\npre-defined, e.g., tokens that appear often in the pre-\nvious context. MIXCE shares the same goal with\nunlikelihood training – matching the human LM\ndistribution, but provides a more general approach\nwithout targeting any specific problem.\nSimilar to our motivation, Kang and Hashimoto\n(2020) think that the zero-avoiding property of\nMLE makes the model sensitive to dataset noise.\nTo cover these noisy examples, the model has to put\nnon-trivial probability mass on the P(x) = 0area.\nTo combat this problem, they propose a loss trunca-\ntion method that drops high-loss (low-likelihood)\nexamples during training time.\nPang and He (2021) want to address the mis-\nmatch of learning objective and human evaluation\n(likelihood vs. quality) and introduce the GOLD al-\ngorithm to approximate reverse cross-entropy. Our\napproximation is similar to theirs but has a different\nderivation process (see § 3.2). Moreover, GOLD\nis evaluated on controlled generation tasks (e.g.,\nsummarization and translation) in which the goal\nis to generate one high-quality text for each input,\nand diversity is not so important. In contrast, if we\ntrain the LM only with reverse CE till convergence,\nthe model will deterministically produce the most\nlikely text for each prompt, which is undesirable\nfor an LM. Therefore, mixing forward and reverse\nCEs is necessary.\nThe idea of MIXCE is also relevant to\nGANs (Goodfellow et al., 2014). GANs opti-\nmize the Jensen–Shannon (JS) divergence between\nmodel and data distributions. Essentially, JS diver-\ngence is also for balancing the two driving forces\nof forward and reverse KL divergences (Huszár,\n2015), and it has been successfully used for evaluat-\ning LM-generated text (Pillutla et al., 2021). How-\never, probably due to the discrete nature of text,\nGANs have not been well applied to LM training.\nCaccia et al. (2020) show that previous language\nGANs often give up diversity for quality.\nAnother related work is Popov and Kudinov\n(2018), which finetunes LMs with the sum of the\nforward cross-entropy loss and reverse KL diver-\ngence. They train a discriminator to estimate re-\nverse KL, similar to a GAN. On the other hand, we\ndirectly approximate reverse cross-entropy in our\nobjective function, without training an additional\ndiscriminator.\nConcurrently, with the same motivation as ours,\nJi et al. (2023) propose to replace MLE with min-\n9029\nimization of the total variation distance (TVD)\n(Van Handel, 2014) between data and model distri-\nbutions. Notably, their final approximation of TVD,\nwhich they call TaiLr, is equivalent to forward\ncross-entropy when the hyperparameter γ = 0\nand equals our approximated reverse cross-entropy\nwhen γ = 1.\n3 Methodology\n3.1 M IXCE\nOur MIXCE learning objective for training LMs\nis the combination of forward and reverse cross-\nentropies, written as\n−η·Ex∼P[log Qθ(x)]−(1−η)·Ex∼Qθ[log P(x)]\n(1)\nwhere η is the mixing ratio. When η = 1, it be-\ncomes the normal MLE objective; and whenη= 0,\nit is the reverse cross-entropy only.\nThe MIXCE loss can be understood in two ways.\nFirst, reverse and forward cross-entropy (CE) em-\nphasize quality and diversity respectively. The re-\nverse CE, −Ex∼Qθ[log P(x)], focuses on quality\nbecause it resembles how we conduct human eval-\nuations – sampling from the model Qθ and evaluat-\ning it by the human P. In human evaluations, the\nfocus is more on the quality of the model-generated\ntext. So, it is possible that a model always gener-\nates the same few high-quality texts, but still gets\nhigh human evaluation scores. This is similar to\nthe mode collapse problem of GANs. The forward\nCE, −Ex∼P[log Qθ(x)], instead focuses more on\ndiversity because it needs any sample from P to\nhave a non-trivial probability underQθ (Hashimoto\net al., 2019). Note that it does not mean forward\nCE has zero effect on quality, rather, the model\nlikelihood Qθ(x) only loosely correlates with the\nhuman-perceived quality of x(Zhang et al., 2021).\nSecond, we hypothesize that MIXCE works sim-\nilarly to a mixture of forward and reverse KL di-\nvergences, which we will show empirically in our\nsynthetic experiments (§ 4.1). On the one hand,\nminimizing forward KL is equivalent to optimizing\nforward CE. On the other hand, reverse KL diver-\ngence, Ex∼Qθ[log Qθ(x)\nP(x) ], has two parts: reverse\nCE and negative entropy of Qθ, Ex∼Qθ[log Qθ(x)].\nReverse CE is minimized when the model deter-\nministically outputs the most likely example, i.e.,\nQθ(x) = δ(the most likely xunder P). Instead,\nminimizing the negative entropy (maximizing the\nentropy) of the model encourages it to be as un-\ncertain as possible, i.e., having a large support and\nuniform distribution. This entropy term counter-\nacts the narrowing-down effect of reverse CE. As\ndiscussed above, forward CE pushes the Qdistri-\nbution to fully cover the support of P. In this case,\nforward CE can also help counteract the narrowing-\ndown effect of reverse CE, i.e., the maximizing en-\ntropy term becomes less important when forward\nCE is present. Hence, we think it is reasonable to\ndrop it from reverse KL.\nOverall, MIXCE combines two complementary\ntraining signals, as shown in Figure 1. Reverse CE\nprevents the model distribution from being broader\nthan the data distribution, while forward CE is more\nhelpful for preventing the model distribution from\nbeing narrower than the data distribution. Although\nforward CE also has non-zero loss when the model\ndistribution is too wide, its loss magnitude is much\nsmaller than what reverse CE provides (see Ap-\npendix C for more discussion). When data is clean,\ntwo CEs work jointly to help learn the data distribu-\ntion better. When data is noisy, the mixing ratio η\nallows us to trade-off between emphasizing a good\ncoverage of the data and putting more weight on\nthe actually high-quality sequences.\n3.2 Optimization of Reverse CE\nOptimizing MIXCE is non-trivial. The obstacle\nis to minimize the reverse CE, −Ex∼Qθ[log P(x)]\nwith respect to θ. To this end, we need to know\nP and to have a differentiable sampling operation\nfrom Qθ. In our synthetic experiments (§ 4.1), we\nuse a distribution P of our own construction and\nuse Gumbel-Softmax (Jang et al., 2017; Maddi-\nson et al., 2017) to make the sampling operation\ndifferentiable.\nHowever, in a real setting, we do not know P.\nTo deal with this, we take the following steps to\nderive an approximated reverse cross-entropy (we\nomit the negative sign for simplicity):\n∇θEx∼Qθ[log P(x)] (2)\n≈∇θEx∼Qθ[P(x)] (3)\n=\n∑\nx\n∇θQθ(x)P(x) (4)\n=\n∑\nx\nQθ(x)∇θlog Qθ(x)P(x) (5)\n=\n∑\nx\nP(x)Qθ(x)∇θlog Qθ(x) (6)\n=Ex∼P[Qθ(x)∇θlog Qθ(x)] (7)\n9030\n=Ex∼P[\nT∏\nt=1\nQθ(xt|x<t)\nT∑\nt=1\n∇θlog Qθ(xt|x<t)]\n(8)\n≈Ex∼P[\nT∑\nt=1\nQθ(xt|x<t)∇θlog Qθ(xt|x<t)] (9)\nFirst, from (2) to (3), we substitute expected\nlog-likelihood by expected accuracy. Irsoy (2019)\nshows that expected accuracy is a comparable or\nbetter alternative loss function to cross-entropy for\nclassification tasks. Then, following the Policy Gra-\ndient theorem (Williams, 1992; Sutton et al., 1999),\nwe get (4) and (5), where we view model Qθ as the\npolicy and P(x) as the reward we want to optimize\nfor the whole sequence. Next, we switch from the\nexpectation of Qθ to the expectation of P (from\n(5) to (6) and (7)), so that we can use the offline\nsamples from P (data samples in the training set)\ninstead of online sampling from Qθ. We unfold\nQθ(x), which results in (8). Until this point, theo-\nretically, we are already able to optimize the model\nusing Equation (8) without knowing P. However,\nthe product of Qθ(xt|x<t) has a very high vari-\nance, and in practice, it underflows whenT is large.\nTherefore, we apply a final rough approximation\nthat leads to (9).\nEquations (8) and (9) are apparently not equiva-\nlent to each other. Nonetheless, they have similar\neffects. Intuitively, in (8), we weigh the gradients of\neach sequence differently based on their sequence-\nlevel probabilities, Qθ(x); in other words, it pro-\nmotes high-likelihood sequences. Similarly, (9)\nweighs gradients at each step by Qθ(xt|x<t), i.e.,\npromoting high-likelihood tokens at each step. So\nessentially, they both encourage the model to pro-\nduce generations in which it is already confident.\nWe call it a self-reinforced objective. To further\nillustrate why self-reinforcement makes sense, we\nconduct an analysis using GPT-2 (Radford et al.,\n2019). Please refer to Appendix B for a detailed\ndiscussion. In short, we show that MLE-pretrained\nGPT-2 on average assigns a higher probability to\nhuman text than to text sampled from the model.\nTherefore, when we promote high-probability se-\nquences or tokens, it is like “pushing” the model\ndistribution toward the human distribution. But,\nwe need to avoid overly “pushing” it to the ex-\ntremely high-probability region where repetitive\ngreedy search outputs locate.\nNote that our approximation of reverse cross-\nentropy is relevant to the method proposed by Pang\nand He (2021), though we have a different deriva-\ntion process from theirs. Please see Appendix A\nfor a detailed comparison.\nFinally, combining forward CE and Equation (9),\nour approximated MIXCE objective is to maximize\nEx∼P[\nT∑\nt=1\n(η+(1 −η)·Qθ(xt|·))∇θlog Qθ(xt|·)],\n(10)\nwhere Qθ(xt|·) is short for Qθ(xt|x<t). This loss\nfunction has the same computational complexity\nas forward CE (MLE). Since Qθ(xt|x<t) is strictly\nlower than 1 (it is around 0.017 to 0.13 when using\nGPT-2), the gradient from approximated reverse\nCE is smaller than that from forward CE. Therefore,\nit is important to tune η to balance the effects of\ntwo CEs.\n4 Experiments\n4.1 Synthetic Experiments\nWe first conduct experiments in a synthetic ideal\nsetting, where we know P, to show the effective-\nness of mixing two cross-entropies with or without\napproximation. Moreover, during evaluation, we\ncan directly compare the learned model parameters\nagainst the ground truth parameters of P.\nDefine the “human” LM P. We start by defin-\ning P as a bi-gram LM. Bi-gram means that the\nprediction of the next token only depends on the im-\nmediately previous token, i.e., P(xt|xt−1). There-\nfore, P is determined by a transition matrix among\nwords M ∈ RV×V (V=vocabulary size) and a\nstart token probability distribution π ∈RV, i.e.,\nstochastic finite-state automata. The last token in\nthe vocabulary is the end-of-sequence (EOS) token.\nFor simplicity, we initialize π as a uniform distri-\nbution. To initialize M, we use two methods. The\nfirst is random initialization. We sample categori-\ncal distributions from a Dirichlet (α=0.5) prior to\ninitialize each row of M. However, one remain-\ning problem is that P has support everywhere. To\nhave P = 0areas, we randomly assign 0s to a cer-\ntain percent of values in each row of M and then\nre-normalize to sum to 1.3 We test 3 percentages:\n10%, 50%, and 90%. The second is initialization\nusing real data. We sample 5000 pieces of text\nfrom WebText (Radford et al., 2019), count the oc-\ncurrence of bigrams, and then use the occurrence to\n3When we assign 0s, we make sure every token has non-\nzero transition probability to EOS.\n9031\ninitialize M. In this case, there are naturally 0s in\nM, and the larger the vocabulary size is, the sparser\nM is. No matter which initialization is used, we\nreserve the last row of M for EOS and it has all\n0s, i.e., will not transit to any token. We set the\nvocabulary size V=20, 50, 100, 500, or 1000.4\nLearn an LM Qθ. We implement model Qθ as\na simple neural bigram LM. Given the word em-\nbedding ei−1 of the previous token xi−1, the next\ntoken is predicted via a simple neural network f:\nhi−1 = Dropout(ReLU(W1ei−1 + b1)),\nQ(xi|xi−1) =Softmax(W2hi−1 + b2),\nwhere W1 ∈ Rd×d (d is the hidden dimension\nsize), b1 ∈Rd, W2 ∈Rd×V, and b2 ∈RV are\nmodel parameters. After training this model, the\nlearned transition matrix can be obtained by M′=\nf(E), E is the word embedding matrix.\nSynthetic data. We sample sequences from P.\nWe set the max sequence length as 500. We sample\n50K and 5K sequences as the training and valida-\ntion set, respectively. There is no test set because\nwe directly compare the learned transition matrix\nM′to the gold M during evaluation.\nMetrics. (1) avg. js : we compute the JS diver-\ngence between each row (except the last row) ofM′\nand the corresponding row in M, and then average\nacross rows. This metric evaluates the overall diver-\ngence of M′from M, and equals 0 iff M′= M;\n(2) avg. 0s: we get the probabilities from M′from\npositions where the corresponding gold probabili-\nties are 0 inM, and take their average. IfM′= M,\navg. 0s = 0, but vice versa is not true.\nObjectives. (1) Forward KL , KL (P||Qθ) =\nEx∼P[log P(x)/Qθ(x)], which is equivalent\nto MLE; (2) Reverse KL , KL (Qθ||P) =\nEx∼Qθ(x)[log Qθ(x)/P(x)]; (3) Mixture of two\nKLs, η · KL(P||Qθ) + (1 - η) · KL(Qθ||P);\n(4) JS, we use a general definition of JS diver-\ngence (Huszár, 2015), η ·KL(P||M) + (1 - η)\n·KL(Qθ||M), where M=η ·P + (1 - η) ·Qθ;5\n(5) Oracle mixture of cross-entropies (MIXCE∗),\nwhere we use the known P. (6) Approximated\n4Our defined bi-gram LMs are always tight, i.e., do not\n“leak” probability mass onto infinite sequences because we\nmake sure that all accessible tokens also have non-zero paths\nto other tokens. Please refer to Du et al. (2022) for the proof.\n5When η = 0 .5, it is the same as the objective of\nGAN (Goodfellow et al., 2014). But instead of using GAN’s\nmin-max loss, we directly optimize JS because we know P.\nRandom (50%) WebText\nV ocab Objective avg. js avg. 0s avg. js avg. 0s\nGold 0.0 0.0 0.0 0.0\n20 For. KL 7.40e-4 1.44e-4 9.93e-4 1.79e-4\nRev. KL 1.36e-1 7.42e-6 3.93e-3 1.95e-6\nMix KLs 4.89e-4 5.15e-5 9.91e-4 1.11e-5\nJS 2.14e-1 4.88e-5 1.12e-2 5.84e-6\nMIXCE* 8.12e-4 1.05e-4 1.36e-3 1.19e-4\nMIXCE 7.02e-4 1.25e-4 1.00e-3 1.79-4\n50 For. KL 6.47e-3 5.65e-4 4.30e-3 4.77e-4\nRev. KL 4.29e-1 1.53e-3 3.48e-2 5.30e-5\nMix KLs 4.45e-3 2.80e-4 3.91e-3 2.83e-4\nJS 4.74e-1 1.40e-3 9.23e-3 2.48e-5\nMIXCE* 4.49e-3 3.72e-4 3.94e-3 2.75e-4\nMIXCE 6.47e-3 5.64e-4 4.29e-3 4.77e-4\n100 For. KL 3.56e-2 1.44e-3 9.70e-3 3.10e-4\nRev. KL 5.57e-1 3.62e-4 1.00e-1 4.04e-5\nMix KLs 2.74e-2 2.10e-4 9.19e-3 1.84e-4\nJS 5.53e-1 9.69e-4 1.73e-1 5.56e-4\nMIXCE* 2.85e-2 9.16e-4 9.61e-3 1.87e-4\nMIXCE 3.56e-2 1.41e-3 9.69e-3 3.16e-6\n500 For. KL 2.39e-1 1.49e-3 4.60e-2 1.78e-4\nRev. KL 6.78e-1 2.76e-6 3.05e-1 1.68e-5\nMix KLs 2.32e-1 8.60e-4 4.27e-2 1.33e-4\nJS 5.34e-1 7.19e-4 2.78e-1 3.84e-5\nMIXCE* 2.34e-1 1.38e-3 4.23e-2 1.29e-4\nMIXCE 2.35e-1 1.46e-3 4.53e-2 1.64e-4\n1000 For. KL 2.93e-1 8.80e-4 8.10e-2 1.50e-4\nRev. KL 6.85e-1 1.21e-6 3.30e-1 6.26e-6\nMix KLs 2.91e-1 8.57e-4 7.50e-2 1.17e-4\nJS 4.59e-1 5.97e-4 3.02e-1 1.93e-5\nMIXCE* 2.92e-1 8.58e-4 7.44e-2 1.14e-4\nMIXCE 2.92e-1 8.76e-4 7.94e-2 1.42e-4\nTable 1: Synthetic experimental results. Random (50%)\nrandomly initializes M and sets 50% of the probabilities\nto 0. WebText means initializing M by the bigram\noccurrence in the WebText data. Gold refers to the\nresults when M′=M. avg. js is our main metric, which\nrepresents the average JS divergence between M and\nM′ (please see the definition of avg. 0s in text). Each\nnumber is a 5-seed average, and Table 7 shows the 95%\nconfidence intervals of some experiments.\nmixture of cross-entropies (MIXCE), where we\nassume P is unknown. Except for Forward KL\nand MIXCE, the other four objectives all need\nto sample from Qθ and require gradients to pass\nthrough this sampling operation. To this end, we\nuse Gumbel-Softmax (Jang et al., 2017; Maddison\net al., 2017) to make sampling differentiable.\nModel selection. During training, we check the\nvalidation loss (the value of the objective function)\nafter every epoch and only save the best checkpoint\nthat has the lowest validation loss. For objectives\nwith η, we choose the best ηbased on the avg. js\nresult on the validation set. We report a 5-seed\naverage for each experiment. The search space of\nηis [0.99, 0.9, 0.5, 0.1, 0.01]. Selected best ηs are\nreported in Table 11 in the Appendix.\n9032\nWikiText WebText WritingPrompts\nModel Size Objective ppl div mauve coh ppl div mauve coh ppl div mauve coh\nHuman - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473\nSmall MLE 26.98 0.91 0.67 0.556 21.45 0.87 0.90 0.555 28.45 0.87 0.85 0.397\nMIXCE 35.04 0.87 0.93 0.567 21.69 0.85 0.92 0.565 28.79 0.86 0.89 0.403\nMedium MLE 20.43 0.90 0.73 0.573 15.92 0.87 0.88 0.560 22.72 0.88 0.89 0.414\nMIXCE 25.92 0.88 0.95 0.584 16.51 0.83 0.93 0.585 23.04 0.86 0.91 0.419\nLarge MLE 18.24 0.90 0.75 0.567 14.13 0.87 0.81 0.570 21.95 0.87 0.87 0.425\nMIXCE 23.44 0.88 0.95 0.578 14.66 0.82 0.94 0.592 21.04 0.86 0.94 0.429\nTable 2: Unbiased sampling results of models finetuned by MLE or MIXCE on three datasets. For all metrics, the\ncloser to the human scores the better. Bold numbers are the ones that are closer to human scores in each setting.\nEach number is a 3-run average.\nWikiText WebText WritingPrompts\nModel Size Objective bestp div mauve coh bestp div mauve coh bestp div mauve coh\nHuman - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473\nSmall MLE 0.85 0.89 0.93 0.584 0.93 0.84 0.94 0.580 0.97 0.86 0.90 0.410\nMIXCE 0.99 0.87 0.95 0.568 0.99 0.84 0.93 0.571 0.99 0.85 0.90 0.407\nMedium MLE 0.85 0.88 0.95 0.602 0.93 0.85 0.95 0.592 0.97 0.86 0.92 0.428\nMIXCE 0.99 0.87 0.96 0.590 0.99 0.81 0.93 0.594 0.99 0.85 0.92 0.427\nLarge MLE 0.87 0.89 0.96 0.594 0.95 0.84 0.87 0.593 0.99 0.86 0.89 0.430\nMIXCE 0.99 0.87 0.97 0.580 0.99 0.81 0.94 0.601 0.99 0.86 0.94 0.435\nTable 3: Top-psampling results of the same models as Table 2. Since changing the decoding method will not affect\nperplexity, we report the selected best pinstead.\nResults. Table 1 (and Table 6 in the Appendix)\nshows the results of our synthetic experiments.\nAcross 4 kinds of initialization of M and 5 vo-\ncabulary sizes, we observe some common patterns.\nFirst, the mixture of two KLs often gets the best\navg. js compared to other objectives, and MIXCE∗\nusually comes second. This supports our expec-\ntation that the mixture of two cross-entropies ap-\nproximates the mixture of two KLs (§ 3.1), as\nwell as demonstrates that combining two KLs or\nCEs can help learn the data distribution more ac-\ncurately compared to MLE. Second, the approxi-\nmated MIXCE usually under-performs MIXCE∗\nbut outperforms forward KL (MLE). Third, reverse\nKL generally works best for the avg. 0s metric, due\nto its property of zero-forcing – forcing Qθ(x) = 0\nwhen P(x) = 0. Lastly, JS divergence oftentimes\nworks similarly to reverse KL, which is consistent\nwith the observation made by Caccia et al. (2020)\n– language GANs trade off diversity for quality.\n4.2 GPT-2 Experiments\nNext, we test MIXCE in a real setting where we\ndo not know P, but we have finite samples from\nP. We use GPT-2 (Radford et al., 2019) as the\nLM Qθ. Though GPT-2 models are already pre-\ntrained by MLE, for simplicity, we use different\nobjectives to finetune it. We test GPT-2 in 3 sizes:\nsmall (24M), medium (355M), and large (774M).\nSee more implementation details in Appendix G.\nReal data. We use English text data from 3 do-\nmains: (1) WikiText (Merity et al., 2017): text from\nWikipedia; (2) WebText (Radford et al., 2019): text\nfrom the Web. It was used for pretraining GPT-2;\nand (3) WritingPrompts (Fan et al., 2018): text\nfrom the writing prompts forum of Reddit. We\nsample from each of these 3 datasets to form our\ntraining, development, and test sets. By default, our\ntraining/development/test set contains 50K/5K/5K\nexamples. Please find more details about these\ndatasets in Appendix G.\nMetrics. (1) Perplexity (ppl) is defined as\ne− 1\nN∗T\n∑\nN\n∑\nT logeQθ(xt|x<t), where N is the num-\nber of examples and T is the sequence length. Per-\nplexity is not necessarily correlated with human\nperceived quality (Zhang et al., 2021). (2) Diver-\nsity (div): following Meister et al. (2022), we de-\nfine n-gram diversity as the average fraction of\nunique vs. total n-grams for n ∈{1, 2, 3, 4}\nin each piece of text. (3) Mauve (Pillutla et al.,\n2021) compares model-generated text against hu-\nman text via a KL divergence curve and is the state-\nof-the-art metric for open-ended text generation.\nWe use Mauve as our primary metric. (4) Coher-\n9033\nence (coh) (Su et al., 2022) computes the cosine\nsimilarity between the embedding of prompt and\nthe embedding of continuation, and embeddings\nare from SimCSE (Gao et al., 2021). All metrics\nare the closer to human scores the better.\nObjectives. Since we have no access to P, we\ncan only implement two out of the six objectives\nwe test in the synthetic setting: (1) MLE, which is\nequal to forward CE or forward KL; (2) MIXCE,\nthe approximated mixture of cross-entropies.\nDecoding. We use unbiased sampling (see foot-\nnote 1) as our primary decoding method as it allows\nus to explore the learned distribution in an unbi-\nased way (Eikema and Aziz, 2020). Additionally,\nwe test top-psampling (Holtzman et al., 2020) to\ncheck if MIXCE is complementary to advanced\ndecoding methods, and we carefully tune pon the\ndevelopment set. For each text, we take the first 50\ntokens (by GPT-2 tokenizer) as the prompt and set\nthe max generation length as 512.\nModel selection. We finetune the model for 5\nepochs on the training set and save the best check-\npoint with the lowest dev loss. We select the best\nmixing ratio ηand the best pbased on the Mauve\nscore on the dev set. The search space of η is\n[0.99, 0.9, 0.7, 0.5, 0.3, 0.1, 0.01, 0.0] and that of\npis [0.85, 0.87, 0.89, 0.91, 0.93, 0.95, 0.97, 0.99].\nSelected best ηs are reported in Table 12 in the\nAppendix. Best ps are reported in Table 3. Metric\nscores are reported on the test set and are 3-run\naverages because sampling is stochastic.\nResults. Table 2 shows unbiased sampling results\nof models in different sizes and finetuned with dif-\nferent objectives on three datasets. As you can\nsee, MIXCE-finetuned models usually get worse\nperplexity but consistently better diversity, mauve,\nand coherence, compared to MLE-finetuned mod-\nels. Table 3 shows top- p sampling results from\nthe same models as Table 2. Since perplexity will\nnot change as the decoding method changes, we\ninstead report the selected best pin this table. It\ncan be seen that after carefully applying top-psam-\npling, MIXCE-finetuned models work on par with\nMLE-finetuned models for diversity, mauve, and\ncoherence. Nonetheless, the best p for MIXCE\nmodels is always 0.99, while MLE models have\nsmaller and more diverse ps. This indicates that\nMIXCE leads to a less noisy model distribution.\nWhich is better?\nDataset MIXCE MLE Same\nWikiText 135* 85 95\nWebText 139* 79 97\nWritingPrompts 111 119 85\nTable 4: Human evaluation results. The star (*) means\nsignificantly6 better (p<0.01).\nHuman evaluation. Besides automatic metrics,\nwe also conduct a human evaluation. Following Kr-\nishna et al. (2022), we conduct blind A/B test-\ning. We randomly sample 105 examples from each\ndataset. For each example, we ask humans to read\ntwo generations from MLE and MIXCE-finetuned\nGPT-2 large models, respectively, and the order of\nshowing these two generations is random. We use\nunbiased sampling to get the generations. Then,\nwe ask them to judge which one is better (or they\nare the same) and justify their preference, based on\nfluency, coherence, informativeness, and whether\nit is sensical. We conduct this evaluation on Ama-\nzon Mechanical Turk and collect 3 responses for\neach example. Please refer to Appendix F for more\ndetails and examples. The final results are shown\nin Table 4. As you can observe, MIXCE-finetuned\nmodels significantly outperform MLE-finetuned\nmodels on both WikiText and WebText domains,\nwhile the two methods perform similarly on Writ-\ningPrompts. It is also worth noting that, compared\nto the results shown in Table 2, none of the 4 au-\ntomatic metrics share the same trend with human\nevaluation.\n4.3 Robustness & Analysis\nVarying training data sizes. We test 3 other\ntraining data sizes: 10K, 25K, and 100K using\nGPT-2 small. Table 5 in the Appendix contains the\nresults, and it shares the same story trend as Table 2:\nMIXCE-finetuned models get worse perplexity but\nin general work better than MLE-finetuned models\nfor diversity, mauve, and coherence.\nVaryingηand max generation length. To exam-\nine how the mixing ratio ηand the max generation\nlength affect the performance, we show the mauve\nscore curves on the dev set in Figure 4. The x-axis\nis the mixing ratio ηfrom 0 to 1 ( MIXCE=MLE\nwhen η = 1), and the y-axis is the mauve score\nwith different max generation lengths (128, 320,\n6The significance test is conducted following the bootstrap\ntest setup (Efron and Tibshirani, 1994).\n9034\nand 512). First, reasonable performances are usu-\nally observed when η≥0.1, and only training the\nmodels with approximated reverse CE (i.e., η= 0)\nleads to degeneration. Second, the advantage of\nMIXCE is more prominent when the max genera-\ntion length is longer.\nControlled Mauve. The max generation length\nis not the actual text length because when sampling\nfrom the model, EOS can be generated at any step.\nWe find that the actual text length can affect the\nmauve computation. Even if we truncate all texts\nto the same length, the incompleteness caused by\ntruncation can be another confounding factor. Both\ntext length and text completeness are irrelevant to\ntext quality but can be used by mauve to distinguish\nmodel generations from human texts. Therefore, to\neliminate the influence of these confounding fac-\ntors, we propose a controlled mauve (or c-mauve)\ncomputation approach. Concretely, for human texts\nand model generations, we randomly sample 10K\nL-length text fragments from each of these two\nsets. Lis the number of tokens. Then, we compute\nthe mauve between these two sets of fragments.\nTable 8 shows the results. As you can see, c-mauve\nscores are in general very high ( ≥0.90), which\nmay indicate that, after controlling the confounding\nfactors, the ability of mauve to distinguish model\ntext from human text has been weakened. MIXCE\nstill gets better performance than MLE in most\ncases. Besides, we also compute controlled coher-\nence in the same fashion, and MIXCE retains its\nadvantage. Please refer to Appendix D.4 for more\ndetails about controlled Mauve and Coherence.\n5 Conclusion\nWe propose a novel training objective,MIXCE, for\nautoregressive language modeling. MIXCE com-\nbines forward and reverse cross-entropies, which\ncan be viewed as combining two complementary\ndriving forces for better fitting the model distribu-\ntion to the data distribution. We demonstrate the\nsuperiority of MIXCE over MLE in both synthetic\nand real settings via both automatic and human\nevaluations. In the future, MIXCE can be poten-\ntially used for pretraining language models.\nAcknowledgments\nWe thank anonymous reviewers for their valuable\ncomments. We thank Xiang Zhou for the help-\nful discussions. This work was supported by a\nBloomberg Data Science Ph.D. Fellowship.\nLimitations\nOne apparent disadvantage of MIXCE is the mix-\ning ratio η. As shown in Table 12 and Figure 4, the\nbest ηchanges as the experimental setting changes.\nIt may be because we use mauve as the model se-\nlection criteria or because different datasets have\ndifferent noise levels. In general, we do not have a\ngood answer to which ηshould be used. The ideal\nsolution is to select ηbased on the performance of\nthe development set like what we did. However, in\npretraining settings, it is too expensive to search\nover multiple ηs. Therefore, how to find a univer-\nsal η or how to determine η automatically is an\nimportant problem to resolve before MIXCE can\nbe reliably used for pretraining.\nAs we mentioned in § 1, language degenera-\ntion of open-ended generation shows two distinct\npatterns: the non-sensical text from unbiased sam-\npling and the repetition loops from greedy search.\nThough MIXCE helps improve the performance of\nsampling, we still see repetition loops when using\ngreedy search.\nEthical Considerations\nAs the OpenAI team pointed out, GPT-2 does not\ndistinguish fact from fiction, so it can not support\nuse cases that require the generated text to be true.\nAdditionally, GPT-2 reflect the biases inherent to\nthe systems they were trained on, so it can not be\ndeployed into systems that interact with humans\nunless the deployers first carry out a study of bi-\nases relevant to the intended use case. Though\nour MIXCE-finetuned GPT-2 gets improved per-\nformance with respect to the metrics we used, the\nabove statement still holds. At this point, we are\nnot sure whether MIXCE can help improve fac-\ntuality or lead to less biased generations, but we\nare sure that the generations still have non-factual\ncontent and biases.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nSourya Basu, Govardana Sachitanandam Ramachan-\ndran, Nitish Shirish Keskar, and Lav R. Varshney.\n2021. {MIROSTAT}: A {neural} {text} {decoding}\n{algorithm} {that} {directly} {controls} {perplexity}.\n9035\nIn International Conference on Learning Representa-\ntions.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems,\nvolume 13. MIT Press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems , NIPS’20,\nRed Hook, NY , USA. Curran Associates Inc.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin. 2020.\nLanguage gans falling short. In International Confer-\nence on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nLi Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara\nMeister, Jason Eisner, and Ryan Cotterell. 2022. A\nmeasure-theoretic characterization of tight language\nmodels. arXiv preprint arXiv:2212.10502.\nBradley Efron and Robert J Tibshirani. 1994. An intro-\nduction to the bootstrap. CRC press.\nBryan Eikema and Wilker Aziz. 2020. Is MAP decoding\nall you need? the inadequacy of the mode in neural\nmachine translation. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4506–4520, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nIan J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron C\nCourville, and Yoshua Bengio. 2014. Generative\nadversarial nets. In NIPS.\nTatsunori B. Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation for\nnatural language generation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1689–1701, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJohn Hewitt, Christopher D. Manning, and Percy Liang.\n2022. Truncation sampling as language model\ndesmoothing. In Findings of the Conference on\nEmpirical Methods in Natural Language Processing\n(Findings of EMNLP).\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nFerenc Huszár. 2015. How (not) to train your generative\nmodel: Scheduled sampling, likelihood, adversary?\narXiv preprint arXiv:1511.05101.\nOzan Irsoy. 2019. On expected accuracy. arXiv preprint\narXiv:1905.00448.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-\ncal reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nHaozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang,\nand Minlie Huang. 2023. Tailoring language gener-\nation models under total variation distance. In The\nEleventh International Conference on Learning Rep-\nresentations.\nDaniel Kang and Tatsunori B. Hashimoto. 2020. Im-\nproved natural language generation via loss trunca-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n718–731, Online. Association for Computational Lin-\nguistics.\nKalpesh Krishna, Yapei Chang, John Wieting, and Mo-\nhit Iyyer. 2022. Rankgen: Improving text generation\nwith large ranking models. In Empirical Methods in\nNatural Language Processing.\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck,\nY-Lan Boureau, Kyunghyun Cho, and Jason Weston.\n2020. Don’t say that! making inconsistent dialogue\nunlikely with unlikelihood training. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4715–4728, Online.\nAssociation for Computational Linguistics.\n9036\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,\nJason Eisner, Tatsunori Hashimoto, Luke Zettle-\nmoyer, and Mike Lewis. 2022. Contrastive decoding:\nOpen-ended text generation as optimization.\nChris J. Maddison, Andriy Mnih, and Yee Whye Teh.\n2017. The concrete distribution: A continuous re-\nlaxation of discrete random variables. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Locally typical sampling. Transac-\ntions of the Association for Computational Linguis-\ntics, abs/2202.00666.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations.\nTomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-\nnock`y, and Sanjeev Khudanpur. 2010. Recurrent neu-\nral network based language model. In Interspeech,\nvolume 2, pages 1045–1048. Makuhari.\nKevin P Murphy. 2012. Machine learning: a probabilis-\ntic perspective. MIT press.\nMyle Ott, Michael Auli, David Grangier, and\nMarc’Aurelio Ranzato. 2018. Analyzing uncertainty\nin neural machine translation. In Proceedings of the\n35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learning\nResearch, pages 3956–3965. PMLR.\nRichard Yuanzhe Pang and He He. 2021. Text genera-\ntion by learning from demonstrations. In ICLR.\nKrishna Pillutla, Lang Liu, John Thickstun, Sean\nWelleck, Swabha Swayamdipta, Rowan Zellers, Se-\nwoong Oh, Yejin Choi, and Zaid Harchaoui. 2022.\nMauve scores for generative models: Theory and\npractice. arXiv preprint arXiv:2212.14578.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34:4816–4828.\nVadim Popov and Mikhail Kudinov. 2018. Fine-\ntuning of language models with discriminator. arXiv\npreprint arXiv:1811.04623.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nFelix Stahlberg and Bill Byrne. 2019. On NMT search\nerrors and model errors: Cat got your tongue? In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3356–\n3362, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. In Advances\nin Neural Information Processing Systems.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nRichard S Sutton, David McAllester, Satinder Singh,\nand Yishay Mansour. 1999. Policy gradient methods\nfor reinforcement learning with function approxima-\ntion. Advances in neural information processing\nsystems, 12.\nL Theis, A van den Oord, and M Bethge. 2016. A\nnote on the evaluation of generative models. In In-\nternational Conference on Learning Representations\n(ICLR 2016), pages 1–10.\nRamon Van Handel. 2014. Probability in high dimen-\nsion. Technical report, PRINCETON UNIV NJ.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3):229–256.\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and\nArvind Neelakantan. 2021. Trading off diversity and\nquality in natural language generation. In Proceed-\nings of the Workshop on Human Evaluation of NLP\nSystems (HumEval), pages 25–33, Online. Associa-\ntion for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nAppendix\nA Connection to Pang and He (2021)\nIn Section 3.2, we introduce an approximation of\nthe reverse cross-entropy (CE) objective. Similarly,\n9037\n500 1000 1500 2000 2500\nsequence-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000number of examples\nwikitext\nhuman\nsample\ngreedy\n0 1000 2000 3000\nsequence-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000\nwebtext\nhuman\nsample\ngreedy\n0 1000 2000 3000\nsequence-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000\nwritingPrompts\nhuman\nsample\ngreedy\n1 2 3 4 5\ntoken-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000number of examples\nwikitext\nhuman\nsample\ngreedy\n0 2 4 6\ntoken-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000\nwebtext\nhuman\nsample\ngreedy\n0 2 4 6\ntoken-level neg. log-likelihood\n0\n1000\n2000\n3000\n4000\n5000\nwritingPrompts\nhuman\nsample\ngreedy\nFigure 2: The histograms of sequence-level and token-level negative log-likelihoods of human texts and model\ngenerations from GPT-2 large.\nPang and He (2021) also propose to approximate\nreverse CE, and the resulting GOLD algorithm is\nsimilar to our Equation 9. Here, we would like to\nclarify the difference and connection.\nThe following equation is the start policy gradi-\nent equation used by Pang and He (2021).\nEτ∼πθ[\n∑\nt\n∇θlog πθ(at|st) ˆQ(st,at)]\nThey used different notations from ours. πθ is the\nsame as our Qθ, i.e., πθ(at|st) is the same as our\nQθ(xt|x<t). ˆQis the accumulated future reward\nfrom timestamp t, ∑T\nt′=tγt′−trt′, γ is the decay\nfactor and rt′ is the reward for each step. We will\ndiscuss ˆQin detail later.\nThen, they apply importance sampling to sample\nfrom a different behavioral policy πb. Since they\nalso use examples from the training set, their πb is\nthe same as our human (or data) distribution P.\nEτ∼πb[\n∑\nt\nwt∇θlog πθ(at|st) ˆQ(st,at)]\nwt is the importance weight. They use a per-action\napproximation: wt ≈πθ(at|st)\nπb(at|st) , which is similar to\nhow we get Equation 9 from Equation 8.\nSince πb is unknown, they assume a uniform\ndistribution: πb ≈1/N(N is the number of train-\ning examples). Hence, their final approximated\ngradient is:\nEτ∼πb[\n∑\nt\nπθ(at|st)∇θlog πθ(at|st) ˆQ(st,at)]\nThey define rt′ and ˆQin three ways. The first is\ncalled δ-reward, i.e., ˆQ = 1. In this case, their\nfinal gradient is exactly the same as our Equation 9.\nHowever, as you can see, we take a different path\nof derivation. Instead of using this δ-reward, our\nˆQis the sequence-level reward P(x). The reward\nP(x) nicely helps us to switch from the expectation\nof Qθ to the expectation of P (from Equation 5\nto Equation 7). Therefore, without assuming a\nuniform distribution of πb, our πb is just P.\nWhen using the other two rewards, they also\nneed to know P. To address this, they use an MLE-\npretrained model as a proxy of P.\nOverall, we introduce a different derivation ap-\nproach for approximating reverse CE. Moreover, as\nwe mentioned in § 2.3, Pang and He (2021) focused\non improving controlled generation tasks where the\nfocus is on the quality of the text, while we focus\n9038\nWikiText WebText WritingPrompts\nData Size Objective ppl div mauve coh ppl div mauve coh ppl div mauve coh\nHuman - 0.89 1.0 0.628 - 0.84 1.0 0.633 - 0.85 1.0 0.473\n10K MLE 29.23 0.91 0.60 0.537 22.03 0.88 0.82 0.542 30.40 0.88 0.74 0.385\nMIXCE 36.70 0.88 0.93 0.546 22.79 0.83 0.86 0.562 30.65 0.87 0.81 0.395\n25K MLE 27.90 0.91 0.68 0.545 21.75 0.88 0.86 0.547 29.37 0.88 0.79 0.394\nMIXCE 35.73 0.88 0.94 0.562 21.97 0.85 0.88 0.561 29.67 0.86 0.86 0.401\n100K MLE 25.93 0.90 0.69 0.559 21.31 0.87 0.90 0.556 27.63 0.87 0.88 0.401\nMIXCE 34.13 0.87 0.93 0.575 21.58 0.85 0.92 0.566 28.01 0.85 0.90 0.409\nTable 5: Unbiased sampling results of GPT-2 small models finetuned by MLE or MIXCE on three datasets of\ndifferent training data sizes. All metrics are the closer to the human scores the better. Bold numbers are the ones\nthat are closer to human scores in each setting.\non open-ended generations where quality and diver-\nsity are both important. Therefore, we mix reverse\nCE with forward CE to form our MIXCE learning\nobjective.\nB Intuition behind the Self-reinforced\nObjective\nTo further illustrate why this self-reinforced objec-\ntive (Equation (8) or (9)) makes sense and their\nshortcomings, we conduct an analysis using GPT-2\nlarge (Radford et al., 2019). We first sample 5000\npieces of text from WikiText, WebText, and Writ-\ningPrompts, respectively, and we call them human\ntexts. Then, using the first 50 tokens of each hu-\nman text as a prompt, we get 5000 sampling and\ngreedy search generations from pretrained GPT-2\nlarge (max generation length = 512). Next, we use\nthe same model to score human texts and model\ngenerations and get the sequence-level and token-\nlevel negative log-likelihoods. Figure 2 shows the\nhistograms of these negative log-likelihoods.\nIn Figure 2, we take the human text histogram\n(in blue) as a proxy of human distribution and\nthe sampling text histogram (in red) as a proxy of\nmodel distribution. As you can see, the support of\nmodel distribution usually contains the support of\nhuman distribution. It supports our previous claim\nthat MLE-trained models tend to over-generalize.\nMeanwhile, at both the sequence and the token\nlevels, the model on average assigns a higher prob-\nability to human text than to text sampled from\nthe model. Therefore, when we promote high-\nprobability sequences or tokens, it is equivalent\nto pushing the model distribution toward the hu-\nman distribution. However, we need to avoid overly\npushing it to the extremely high-probability region\nwhere greedy search outputs locate (in yellow) be-\ncause they are known to be poor-quality and repeti-\nReverse Cross-Entropy Forward Cross-Entropy\nP\nQ\nx ~ P\n-logQ(x)  \n \nP\nQ\n-logP(x)  \n \nx ~ Q\ntmp\nbyryuer\nDecember 2022\n1 Introduction\nMixCEs = \u0000 ⌘ ⇤ E x ⇠ P [log Q ✓ ( x )] \u0000 (1 \u0000 ⌘ ) ⇤ E x ⇠ Q ✓\n[log P ( x )]\n\u0000 E x ⇠ P [log Q ✓ ( x )]\n\u0000 E x ⇠ Q ✓\n[log P ( x )]\n1\ntmp\nbyryuer\nDecember 2022\n1 Introduction\nMixCEs = \u0000 ⌘ ⇤ E x ⇠ P [log Q ✓ ( x )] \u0000 (1 \u0000 ⌘ ) ⇤ E x ⇠ Q ✓\n[log P ( x )]\n\u0000 E x ⇠ P [log Q ✓ ( x )]\n\u0000 E x ⇠ Q ✓\n[log P ( x )]\n1\nFigure 3: Forward CE only weakly penalizes the model\nQθ when it puts a small amount of probability mass\nonto P(x) = 0space. And the loss magnitude is much\nsmaller than what we will get from reverse CE.\ntive. Also, as shown in the figure, when promoting\nhigh-probability sequences, even if we overdo it, it\nwill still be within the support of human distribu-\ntion. In contrast, when promoting high-probability\ntokens, it can go outside the support of the human\ndistribution, which is the drawback of Equation (9)\ncompared to Equation (8).\nLastly, if we train the model only with the self-\nreinforced objective till convergence, it is inevitable\nto end up with a model that can only output greedy\nsearch generations. Hence, we need to combine it\nwith the forward cross-entropy.\nC Loss Magnitude\nAs shown in Figure 1, we use reverse cross-entropy\n(CE) to provide a driving force for narrowing the\nmodel distribution down when it is broader than the\ndata distribution. And forward CE is to broaden\nthe model distribution up. However, it does not\nmean forward CE does not have the opposite drive\nforce because forward CE is minimized if and only\nif Qθ(x) =P(x). However, as shown in Figure 3,\nthe loss magnitude is greatly smaller than the loss\nmagnitude we get from reverse CE.\n9039\nmixing ratio \n0.2\n0.4\n0.6\n0.8\n1.0mauve(max length=128)\nwikitext\nsmall\nmedium\nlarge\nmixing ratio \n0.2\n0.4\n0.6\n0.8\n1.0\nwebtext\nsmall\nmedium\nlarge\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nwritingPrompts\nsmall\nmedium\nlarge\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mauve(max length=320)\n small\nmedium\nlarge\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\nsmall\nmedium\nlarge\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nsmall\nmedium\nlarge\n0.0 0.2 0.4 0.6 0.8 1.0\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0mauve(max length=512)\n small\nmedium\nlarge\n0.0 0.2 0.4 0.6 0.8 1.0\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\nsmall\nmedium\nlarge\n0.0 0.2 0.4 0.6 0.8 1.0\nmixing ratio \n0.0\n0.2\n0.4\n0.6\n0.8\nsmall\nmedium\nlarge\nFigure 4: The mauve scores obtained by MIXCE-finetuned GPT-2 models on development sets with different max\ngeneration lengths and different η. Note that when η= 1, MIXCE is equivalent to MLE. The x-axis is the mixing\nratio η, and the y-axis refers to mauve scores with different max generation lengths. The 3 lines in each subplot show\nthe results of GPT-2 models in different sizes. The 3 subplots in each row are the results of 3 datasets respectively.\nUnbiased sampling is used as the decoding method. Each dot is the average of 3 runs of sampling and the error bar\nshows the standard deviation of 3 runs.\nD Additional Results\nD.1 Additional synthethic experiments\nTable 6 shows the results of additional synthetic ex-\nperiments besides Table 1 in the main paper. Here,\nthe goal transition matrixM is randomly initialized\nwith 10% and 90% zero probabilities.\nAs the magnitudes of both avg. js and avg. 0s\nare fairly small, we examine the 95% confidence\nintervals under one synthetic experimental setting –\ninitializing the transition matrix M by the bigram\noccurrence in the WebText data and setting vocab-\nulary size as 1000. Table 7 contains the results.\nWe can see that 95% confidence intervals are small\nenough to maintain the trend of the results.\nD.2 Varying training data sizes\nTable 5 shows the results of using different training\ndata sizes in the real-data setting.\nD.3 Varying ηand max generation length\nFigure 4 illustrates the curves of mauve scores on\nthe development sets.\nD.4 Controlled Mauve and Coherence\nWe find that the actual length of the text is a con-\nfounding factor of mauve computation. For ex-\nample, when we compute mauve between a set of\ntexts and the same set with an extra new line to-\nken after each text (or the same set with the last\nk tokens being truncated), the score will be lower\n9040\nRandom (10%) Random (90%)\nV ocab Objective avg. js avg. 0s avg. js avg. 0s\nGold 0.0 0.0 0.0 0.0\n20 For. KL 3.65e-4 1.80e-4 7.56e-4 9.10e-5\nRev. KL 3.41e-3 5.56e-6 1.87e-1 1.54e-6\nMix KLs 3.11e-4 7.11e-5 4.01e-4 2.67e-5\nJS 5.68e-3 1.17e-5 2.14e-1 5.24e-4\nMIXCE* 4.92e-4 1.59e-4 4.87e-4 2.95e-5\nMIXCE 3.31e-4 1.57e-4 7.08e-4 8.49e-5\n50 For. KL 6.01e-3 1.21e-3 2.18e-3 8.90e-5\nRev. KL 2.03e-2 2.01e-5 4.11e-1 4.55e-6\nMix KLs 4.65e-3 1.29e-4 1.54e-3 3.41e-5\nJS 1.03e-1 9.03-5 4.24e-1 1.25e-5\nMIXCE* 5.20e-3 6.84e-4 1.48e-3 2.70e-5\nMIXCE 5.96e-3 1.20e-3 2.03e-3 7.70e-5\n100 For. KL 3.34e-2 2.49e-3 6.98e-3 1.49e-4\nRev. KL 2.30e-1 1.79e-3 5.30e-1 6.25e-6\nMix KLs 2.98e-2 4.66e-4 5.04e-3 6.34e-5\nJS 2.38e-1 1.06e-3 5.18e-1 1.32e-3\nMIXCE* 3.10e-2 1.73e-3 5.12e-3 6.00e-5\nMIXCE 3.29e-2 2.44e-3 7.01e-3 1.50e-5\n500 For. KL 1.56e-1 1.57e-3 1.93e-1 8.45e-4\nRev. KL 2.94e-1 9.91e-4 6.49e-1 2.33e-6\nMix KLs 1.55e-1 1.45e-3 1.70e-1 6.83e-4\nJS 2.95e-1 9.78e-4 5.75e-1 1.35e-3\nMIXCE* 1.55e-1 1.45e-3 1.69e-1 6.71e-4\nMIXCE 1.55e-1 1.56e-3 1.88e-1 6.28e-4\n1000 For. KL 1.83e-1 8.95e-4 3.65e-1 7.31e-4\nRev. KL 2.86e-1 6.12e-4 6.68e-1 3.88e-6\nMix KLs 1.80e-1 8.64e-4 3.50e-1 6.86e-4\nJS 2.88e-1 6.11e-4 5.80e-1 7.73e-4\nMIXCE* 1.83e-1 8.64e-4 3.50e-1 6.84e-4\nMIXCE 1.83e-1 8.92e-4 3.48e-1 6.71e-4\nTable 6: The results of the other two synthetic exper-\niments. Random (10%) and Random (90%) both use\nrandom initialization for M, and 10% and 90% prob-\nabilities in M are 0 respectively. Gold refers to the\nresults when M′=M. Each value is a 5-seed average.\nthan 0.01. Though you may think truncating all\ntexts to the same length can resolve this problem,\nwe find that the incompleteness caused by trunca-\ntion can also be a confounding factor. For instance,\nkeeping human texts intact, we truncate texts gen-\nerated by two systems by their shorter lengths (i.e.,\nfor each example, we truncate text1 and text2 by\nmin_length(text1, text2)). Then, the system whose\ntexts get truncated less will get a greatly larger\nmauve score than the other system. Therefore, to\neliminate the influence of these two confounding\nfactors, we propose a controlled mauve computa-\ntion approach. Concretely, for the set of human\ntexts Th and the set of model-generated texts Tm,\nwe randomly sample 10K L-length text fragments\nfrom each of these two sets. Lis the number of\ntokens in each text fragment. After that, we com-\npute the mauve between these two sets of 10K text\nfragments. We denote this controlled mauve as\nWebText\nV ocab Objective avg. js avg. 0s\n1000 For. KL 8.10e-2±2.45e-4 1.50e-4 ±5.58e-7\nMIXCE* 7.44e-2±2.46e-4 1.14e-4±6.15e-7\nMIXCE 7.94e-2±2.15e-4 1.42e-4 ±5.05e-7\nTable 7: Synthetic experimental results with 95% confi-\ndence intervals. WebText means initializingM by the\nbigram occurrence in the WebText data.\nc-mauveL.\nFh,L = {fi\nh,L}10K\ni=1 ,fi\nh,L ∼Th\nFm,L = {fi\nm,L}10K\ni=1 ,fi\nm,L ∼Tm\nc-mauveL = mauve(Fh,L,Fm,L)\nTo sample each fragment, we first randomly sample\na text ti from the set, and then randomly select a\nstart token s (as long as there are more than L\ntokens from sto the end of ti), then the fragment is\nti[s: s+L]. Finally, Table 8 shows the results. We\nset L = 100, 200, and 300, except that we could\nnot get 10K 200-token fragments from WikiText\nbecause its texts are shorter.\nThe Coherence score (Su et al., 2022) computes\nthe cosine similarity between the prompt and the\ncontinuation. We suspect that the length of the\ncontinuation may affect the score. Therefore, fol-\nlowing the same idea of controlled mauve, we also\nsample 10K fragments of the same length from\nthe set of texts for evaluation and compute coher-\nence on the fragments. And for each fragment,\nwe take the first 50 tokens as the prompt and the\nrest as the continuation. Table 9 shows the results.\nAs you can observe, under this controlled setting,\nMIXCE-finetuned models generally achieve better\ncoherence over MLE-finetuned models.\nD.5 Text length of model generations\nThough by default we set the max generation length\nas 512, the actual text length can vary as the EOS\ntoken can be sampled at any time step. Therefore,\nwe list the average text length of the human text\nand GPT2-large generations in Table 10. We ob-\nserve that model generations are always shorter\nthan human text. Compared to MLE, our MIXCE-\nfinetuend model produces shorter text on Wiki-\nText while producing longer text on the other two\ndatasets. We suspect that the shorter length of\nMIXCE on WikiText is due to the small mixing\nratio (0.1) chosen based on mauve (see Table 12).\nHowever, we do not think shorter text length leaves\n9041\nWikiText WebText WritingPrompts\nModel Size Objective c-mauve100 c-mauve100 c-mauve200 c-mauve300 c-mauve100 c-mauve200 c-mauve300\nHuman 0.97 0.96 0.96 0.96 0.96 0.96 0.96\nSmall MLE 0.92 0.93 0.92 0.90 0.94 0.94 0.92\nMIXCE 0.92 0.94 0.94 0.93 0.95 0.94 0.94\nmedium MLE 0.94 0.93 0.91 0.90 0.94 0.94 0.93\nMIXCE 0.93 0.95 0.94 0.94 0.95 0.94 0.94\nLarge MLE 0.93 0.93 0.93 0.91 0.94 0.94 0.93\nMIXCE 0.93 0.94 0.94 0.93 0.95 0.95 0.95\nTable 8: Controlled mauve results. Unbiased sampling is used as the decoding method, i.e., using the same model\ngenerations as Table 2. Human scores are not 1 because sampling 10K fragments twice result in two different sets.\nEach number is a 3-run average.\nWikiText WebText WritingPrompts\nModel Size c-coh100 c-coh100 c-coh200 c-coh300 c-coh100 c-coh200 c-coh300\nHuman 0.570 0.521 0.583 0.600 0.412 0.470 0.481\nSmall MLE 0.504 0.444 0.515 0.535 0.350 0.412 0.429\nMIXCE 0.508 0.458 0.524 0.545 0.363 0.422 0.437\nMedium MLE 0.518 0.446 0.515 0.535 0.355 0.415 0.432\nMIXCE 0.527 0.484 0.546 0.565 0.362 0.425 0.437\nLarge MLE 0.521 0.449 0.515 0.536 0.372 0.431 0.447\nMIXCE 0.522 0.469 0.531 0.569 0.369 0.434 0.450\nTable 9: Controlled coherence results. Unbiased sampling is used as the decoding method, i.e., using the same\nmodel generations as Table 2. Each number is a 3-run average.\nWikiText WebText WritingPrompts\nModel Size Objective avg. len avg. len avg. len\nHuman 124.5 304.5 332.5\nLarge MLE 114.8 284.2 325.8\nMIXCE 89.0 298.9 326.4\nTable 10: Unbiased sampling text lengths of models\nfinetuned by MLE or MIXCE on three datasets. Length\nis computed by simply splitting text by whitespaces.\nto better mauve, as shown by the other two datasets\nand discussed in D.4.\nE Best ηs\nTable 11 has the best ηs for synthetic experiments.\nTable 12 contains the best ηs selected for GPT-2\nexperiments.\nF Human Evaluation Details\nWe conduct A/B testing (or pairwise comparison)\nto compare generations from two models. As\nshown in Figure 5, in each job, we give the eval-\nuator two text paragraphs (in random order) that\nshare the same beginning part (the prompt) but\nhave different continuations. Then, they need to\nchoose which one they think is better (or non-\ndistinguishable). To avoid random selections, they\nare also asked to provide a justification for their\nchoice. We find this justification not only gives\nus additional explanations of their choices but also\nhelps us easily identify bad workers, because bad\nworkers tend to use one single justification or sev-\neral repeated justifications.\nWe instruct them by defining a good text para-\ngraph as being:\n• Fluent: Should have no obviously ungram-\nmatical sentences, missing components, etc.\nthat make the text difficult to read.\n• Coherent: Should stay on topic with the\nprompt and build from sentence to sentence\nto a coherent body of information.\n• Informative: Should have diverse and inter-\nesting content.\n• Sensical: Should generally make sense.\nSince short text has little information and long\ntext is difficult to read, we only use paragraphs with\n5 to 8 sentences for evaluation. If a paragraph has\nmore than 8 sentences, we truncate it to 8 sentences.\nAnd we remove paragraphs with less than 400 or\nmore than 2000 characters. Besides, to eliminate\nthe influence of length difference, we do not select\nexamples whose length difference between two\n9042\nModel section is based on avg. js\nRandom (50%) WebText Random (10%) Random (90%)\nV ocab Objective best η best η best η best η\n20 Mix KLs 0.99 0.9 0.99 0.99\nJS 0.9 0.9 0.9 0.9\nMIXCE* 0.99 0.99 0.99 0.99\nMIXCE 0.9 0.99 0.99 0.99\n50 Mix KLs 0.99 0.99 0.9 0.99\nJS 0.01 0.99 0.9 0.9\nMIXCE* 0.99 0.99 0.99 0.99\nMIXCE 0.99 0.99 0.99 0.9\n100 Mix KLs 0.9 0.99 0.9 0.99\nJS 0.01 0.99 0.99 0.01\nMIXCE* 0.99 0.99 0.99 0.99\nMIXCE 0.5 0.9 0.5 0.99\n500 Mix KLs 0.9 0.99 0.99 0.99\nJS 0.99 0.99 0.99 0.99\nMIXCE* 0.99 0.99 0.99 0.99\nMIXCE 0.1 0.5 0.1 0.1\n1000 Mix KLs 0.99 0.99 0.99 0.99\nJS 0.99 0.99 0.99 0.99\nMIXCE* 0.99 0.99 0.99 0.99\nMIXCE 0.1 0.5 0.1 0.1\nTable 11: The selected best ηof synthetic experiments reported in Table 1 and Table 6. The model section is based\non avg. js.\nModel section is based on mauve (max length=512) on dev set\nWikiText WebText WritingPrompts\nModel Size Objective best η best η best η\nSmall M IXCE 0.1 0.5 0.5\nMedium M IXCE 0.1 0.3 0.5\nLarge M IXCE 0.1 0.3 0.7\nTable 12: The selected best ηof GPT-2 experiments reported in Table 2. The model section is based on mauve (max\nlength=512) on the dev set.\nparagraphs is more than 1 sentence or more than\n200 characters.\nWe conduct this evaluation on Amazon Mechan-\nical Turk. We only allow workers, who are located\nin the US, have a Masters Qualification,7 have an\napproval rate larger than 97%, and have more than\n10000 HITs approved, to do our tasks. In addition,\nwe first ran a testing batch, then manually checked\nthe results, and selected 44 qualified workers to\ncontinue doing the rest of our tasks.\nFor each of the 3 datasets, we sampled 105 ex-\namples and collected 3 responses per example. In\ntotal, we received 945 human evaluations. We pay\nworkers $1 per response, and it takes around 5 min-\nutes to finish one response, i.e., the hourly rate is\naround $12.\n7https://www.mturk.com/worker/help\nTable 13 shows that inter-annotator agreements.\nFigure 6-11 are 6 randomly sampled examples from\nhuman evaluation results, 2 examples per dataset.\nG Reproducibility\nIn our GPT-2 experiments, we use English text data\nfrom 3 domains: (1) WikiText (Merity et al., 2017):\ntext from Wikipedia, and we use wikitext-103-raw-\nv1 from Hugging Face. 8 Its license is Creative\nCommons Attribution-ShareAlike License (CC BY-\nSA 4.0). (2) WebText (Radford et al., 2019): text\nfrom the Web. It was used for pretraining GPT-2.\nThe full WebText is not available but they released\na subset on Github9. The GitHub repository con-\n8https://huggingface.co/datasets/\nwikitext\n9https://github.com/openai/\ngpt-2-output-dataset\n9043\nDataset all agree 2 agree no agreement\nWikiText 24% 59% 17%\nWebText 24% 52% 24%\nWritingPrompts 20% 70% 10%\nTable 13: Inter-annotator agreement. The numbers are the portions of examples that have a 3-annotator agreement\n(all agree), a 2-annotator agreement (2 agree), or no agreement. E.g., 24% of examples used in human evaluation\nfor WikiText have an agreement among 3 annotators.\ntains an MIT license, and they did not specify the li-\ncense of the data. But they indicated in the readme:\n“We look forward to the research produced using\nthis data!” (3) WritingPrompts (Fan et al., 2018)10:\ntext from the writing prompts forum of Reddit. Its\nGitHub repository also contains an MIT license\nwithout specification of the data license. However,\nWritingPrompts has been used by many other re-\nsearch works, e.g., Pillutla et al. (2021). We use\ntheir official dev and test sets as much as possible.\nIf they have fewer than 5K examples, we sample\nfrom their official training set to make up the rest.\nAll of our experiments were conducted on\nNVIDIA Tesla V100 32G GPUs. We use a sin-\ngle GPU to run each experiment and change the\nbatch size to fit models of different sizes. When\nfine-tuning GPT-2 small using a single GPU with\nMLE or MIXCE, it took less than 1 hour to finish 5\nepochs on 50K WikiText training data and took less\nthan 2 hours to finish 5 epochs on 50K WebText or\nWringPrompts training data.\nWe implemented our GPT-2 based models based\non the GPT-2 modeling code from Hugging Face\nTransformers11. For training and evaluation, we\nmodified the example script of causal language\nmodel training12. We used the default optimizer,\nlearning rate, scheduler, etc. in that script. But\nwe set the maximum training epochs as 5 and\nchanged the batch size and gradient accumulation\nsteps based on the model size to fit it in one 32G-\nmemory GPU.\n10https://github.com/facebookresearch/\nfairseq/tree/main/examples/stories\n11https://github.com/huggingface/\ntransformers/blob/main/src/transformers/\nmodels/gpt2/modeling_gpt2.py\n12https://github.com/huggingface/\ntransformers/blob/main/examples/pytorch/\nlanguage-modeling/run_clm_no_trainer.py\n9044\nFigure 5: Human evaluation interface and a random example from our collected human annotations.\n9045\nParagraph1 (MLE):\nWithin minutes of issuing the signal and turning his flagship HMS Queen Charlotte, Howe’s plan began to falter.\nMany of the British captains had either misunderstood or ignored the signal and were hanging back in the original line.\nOther ships were still struggling with damage to their ships and other equipment caused by the storm, so that Australia’s\nwar effort was threatened. In response to the storm-ravaged Australian distress call on 12 March, Howe ordered his ship\nHMS Sun Babies as flagship of rowing party V HMNZS Platypus. He assigned elevensacks from the ship to the crew for a\nrescue, and placed much of the more aged of his crew and stockmen on sick leave on the advice of Admiral Rear-Admiral\nJohn Forbes.\nParagraph2 (MIXCE):\nWithin minutes of issuing the signal and turning his flagship HMS Queen Charlotte, Howe´s plan began to falter. Many\nof the British captains had either misunderstood or ignored the signal and were hanging back in the original line.\nOther ships were still struggling with damage to their yards and were standing still. Another officer, Lieutenant Colonel\nStrother, privately stated that the British ship captains had \"obsessed [ed] the whole thing over with the consciousness of the\nimpossibility of victory\". Commander Howe was eager to proclaim victory, but it was not until the Chief of the General Staff,\nSir Charles Ogilvy, finally appeared on the scene that he decided to make a decision.\nChoice: paragraph2\nJustification: Paragraph 1 has details that do not flow with each other as if Howe was involved with two ships at the same\ntime. The beginning of the paragraph mentions Howe’s flagship HMS Queen Charlotte and later on his flagship HMS\nSun Babies. It is not clear about the role of Australians whether there were Australians in the crew or they were helping\nAustralian ships. In paragraph 2, there is no mention of another nation other than just British. It has details of different ranks\nof commands. Moreover the paragraph focus on failure and uncertainty that flow to the rest of the paragraph.\nChoice: paragraph2\nJustification: Paragraph 2 seems to make more sense and is more coherent than Paragraph 1. #2 has a narrative flow that can\nbe read from start to finish, giving the reader a sense of what is happening. The quote is the weak point, but as a quote, can be\ntaken as it is given here. On the other hand, Paragraph 1 can also be read from start to finish, but makes less sense, and seems\nless coherent.\nChoice: paragraph2\nJustification: Paragraph 2 is more coherent, and it wins hands down in terms of making sense. Paragraph 1 has Howe sending\nmen on leave in the middle of a battle and also has him assigning \"elevensacks\" to the crew, which isn’t even a real word.\nFigure 6: WikiText example 1.\nParagraph1 (MLE):\nIn imitation of the Greeks, Nero built a number of gymnasiums and theatres. Enormous gladiatorial shows were\nalso held. Nero also established the quinquennial Neronia. The festival included games, poetry, and theater. For\nthe recesses of the Calendrical Mysterium, the city quinquenored, with the city many times superseded.[ndx] Nero also\nFestina-Illuminatae in honour of the doomed Emperor Caeser and his son Severus, the founders of the Flavian dynasty, who\nwere slain in that period. In Britain, Nero erected the Attic theatre.\nParagraph2 (MIXCE):\nIn imitation of the Greeks, Nero built a number of gymnasiums and theatres. Enormous gladiatorial shows were also\nheld. Nero also established the quinquennial Neronia. The festival included games, poetry, and theater. In order to\ncombat the influence of the Venetian party, the local aristocracy discouraged the shooting of arrows at the official spectators.\nNero’s penchant for shooting arrows became a local insult.\nChoice: paragraph2\nJustification: The first one is hard to understand. What does \" the city quinquenored, with the city many times super-\nseded.[ndx]\" mean.? It does not make sense. The second one is easy to understand.\nChoice: paragraph2\nJustification: I had to read Paragraph 1 a few times before it made sense, but Paragraph 2 flowed nicely and logically without\neffort.\nChoice: paragraph2\nJustification: 2 takes a bit of a stretch but it can at least follow some plausible logical order. 1 just abruptly starts talking\nabout geopolitics which doesn’t make sense in that context.\nFigure 7: WikiText example 2.\n9046\nParagraph1 (MLE):\nTwo years ago, Sandrine Jourdan walked up a friend’s Sunshine Coast driveway and simply disappeared. The\nCaboolture woman has not accessed her bank accounts, used her phone or wished any of her three children a happy\nbirthday since then. She insists she hasn’t moved house, childcare is still open and she can speak to her family now. \"I owe\nthem no money.\" Jourdan said. Jourdan was last heard from around February this year and her family does not know where\nshe is. One of her family members told me Jourdan was hers but to leave Jourdan alone would be like asking the sick to go to\nthe doctor. However, the woman, who is in her 50s, said Jourdan was extremely important to her family and they didn’t want\nto send her any money. They saw her daily and had almost no tensions of any kind to handle\nParagraph2 (MIXCE):\nTwo years ago, Sandrine Jourdan walked up a friend’s Sunshine Coast driveway and simply disappeared. The\nCaboolture woman has not accessed her bank accounts, used her phone or wished any of her three children a happy\nbirthday since then. But she has no one to blame but herself. The welfare office at Taree hostel was \"terminating her\naccommodation on a whim\" last Spring, she says. Discarded money The hostel is not Charter and cannot employ staff.\nDesperate for a fresh start, Sandrine tried to access her bank accounts and managed to access her daughter’s allowance, the\nbequests of her parents, a benefit card and money in savings. She has not seen her family since. To compound her plight, she\nhas lost her job on the condominium project near the town of Machalung where she was staying\nChoice: non-distinguishable\nJustification: Both are bad and neither follow the prompt. Paragraph 1 isn’t coherent. She is on good terms with her family,\nthen not, then back again. Paragraph 2 also has contradictions, and the sentence \"Discarded money The hostel is not Charter\nand cannot employ staff. \" seems randomly inserted.\nChoice: non-distinguishable\nJustification: Neither follows the prompt in the least. It is about a woman who disappears, and both go on to have quotes and\ninformation about the missing woman as if she were still interacting with others. There are incomplete sentences in 2 which\nare not present in 1, but I still can’t say 1 is better because it’s so off prompt.\nChoice: paragraph1\nJustification: Paragraph 2 has some punctuation and grammar errors, for example \"Discarded money The hostel is not\nCharter and cannot employ staff.\"\nFigure 8: WebText example 1.\nParagraph1 (MLE):\nAnother Phantom is leaving the building. The musical, which is the longest running show in Broadway history,\nannounced Monday that the Phantom, James Barbour, will leave the production on Dec. 23, 2017, after a close to\nthree-year run. A premature announcement about the closure came shortly after the show was first announced at the 2016\nTony Awards. A representative for JBL had told The Hollywood Reporter’s Hayden Dingman last year that the Broadway\nappearance would likely wrap but that there were still plans to continue performing on the stage. The event marked the end of\nthe run of ’The Phantom’ which reached a two-decade high in the Broadway season. The show had run a whopping 129\nshows on Broadway since its inception in 1995. After 11 seasons, Ice Cube’s ’Once’, which had been announced 14 months\nprior, was a box office success\nParagraph2 (MIXCE):\nAnother Phantom is leaving the building. The musical, which is the longest running show in Broadway history,\nannounced Monday that the Phantom, James Barbour, will leave the production on Dec. 23, 2017, after a close to\nthree-year run. \"Moving on to a new chapter with a new partner is one of the hardest things we’ve ever done as a show,\"\nsaid producer Scott Ploog. \"Joining forces with our fearless leader James was a huge statement to us.\" After singing and\ndancing its way through the Fort Worth Convention Center in 2011, the show was working on a script for its sixth season.\nThe Phantom surprised the show’s production staff with a call from James, who was the project’s conductor for the years it\nwas running. \"After hearing the love he had for the show, I knew whether I wanted to take a job as indispensable leader of the\ncast or what, I had to decide,\" said Expedition leader Cary Joji. \"He was just engaging, embracing and completely out of his\nways, but he was relentless\nChoice: paragraph2\nJustification: Paragraph 2 feels more personal with more quotes from the people that worked on the show.\nChoice: paragraph2\nJustification: Both do well on fluency. For coherence, I don’t think either one forms a totally coherent body of information,\nbut Paragraph 2 sticks to the topic a touch better. I think they’re roughly equal in terms of diverse and interesting content. As\nusual, making sense is the metric each is worst at. I believe they are indistinguishably terrible in that regard. So Paragraph 2\nwins, just barely, via being slightly more coherent.\nChoice: paragraph2\nJustification: The first paragraph seems to be a mix of unrelated items, at first discussing a performer leaving the cast of\nPhantom and then talking about the end of the production and then concluding with \"After 11 seasons, Ice Cube’s ’Once’,\nwhich had been announced 14 months prior, was a box office success\", which seems unrelated to Phantom. The second\nparagraph seemed little off also, but had somewhat more coherence. The first paragraph also lacked a couple comma, by rule,\nbut I didn’t factor that in very much.\nFigure 9: WebText example 2.\n9047\nParagraph1 (MLE):\n[WP] There is no such thing as “dating ” anymore. Just like in Cinderella, As soon as you touch the hand of“ the one”\nyou have an insatiable urge to kiss. Sam sighed. Despite her best efforts to hide it, the bruises and cuts beneath her blouse\nwere pretty apparent from her sitting down. Her heart pierced so many ribs that buckles broke, and chips torn the flesh of her\nribs, arm and leg. And they weren’t just sitting there - it was pouring out down the side of the castle. When they arrived back\nat the villa after a short- but exhausting trip from the train station, they were determined to hug and kiss, just as well as they\ncould in dark alleys. Unfortunately, the darkness blinded them from view, so the light was flowing with the pouring rain,\nsending sparks from their inexpensive outfits\nParagraph2 (MIXCE):\n[WP] There is no such thing as “dating ” anymore. Just like in Cinderella, As soon as you touch the hand of“ the\none” you have an insatiable urge to kiss. Sam sighed. Despite her best efforts to reverse the trend, her hand seemed to\ninstinctively take the hand of the lone participant. He smiled, as Sam closed her eyes. \"Today’s the day you know, Yattar,\"\nhe insisted confidentially. \"Today’s the day you know the thrill of kissing!\" Yattar nodded, with conviction. \"Anyone the\npurveyors of-\" \"Remind me to spice everything up,\" she dismissed, this time making eye contact with the man her mouth had\ncaught them\nChoice: paragraph2\nJustification: Assuming that \"Yattar\" is a pet name/different name for \"Sam\", 2 is much more cohesive and fluent than 1. It\nis easy to follow the story and the story makes more sense than 1. The \"pouring out down the side of the castle\" in 1 makes\nno sense, and other content is pretty incoherent as well.\nChoice: paragraph1\nJustification: Paragraph 1 is a more concrete story of love, while 2 feels more disjointed.\nChoice: paragraph2\nJustification: Paragraph 2 is more cohesive, sticking to the concept introduced in the first sentence and describing the initial\nmeeting of a couple. Para 1 rather jarringly adds a description of injuries and a castle setting.\nFigure 10: WritingPrompts example 1.\nParagraph1 (MLE):\n[CC] I wrote these stories recently here on WP, and would like to share them with my friends; however, I feel like they\ncould be tuned up a bit. Any thoughts on how to make improvements is greatly appreciated. First off, there are a lot of\ninconsistencies in their sentences, it’s good to have a prompt where you can color all the immersion steps individually so you\ncan judge meaningful sequences when looking at them in context. 2nd, while I did my best to adhere to game mechanics in\nthe story, they were probably made largely to draw parallels between the reader and protagonist. Seth Vargo Hughes is an\nunfortunately short person himself (7 weeks a year usa is somewhere between 8 to 10 years long. I amn’t entirely sure what\nhis killer skills are now, or about what he can do), so he may get caught with some pretty nasty tools and weapons, and at the\nend of the story, he’s not gonna give the guy who rescued him the satisfaction of just putting his fist through his friend’s chest\nto make his statement true. But that happens. 2nd, 2 other things that PWP failed to properly convert into told story\nParagraph2 (MIXCE):\n[CC] I wrote these stories recently here on WP, and would like to share them with my friends; however, I feel like they\ncould be tuned up a bit. Any thoughts on how to make improvements is greatly appreciated. First off, there’s a ton of\nimage and word overlap on this story, which leads to a lot of writing and visibility issues. There definitely should have been\ndone more body alignment, with the characters looking like more anthropomorphic animal animals more often. Additionally,\nthis idea if poorly worded, as the phrase \"spider\" was spelled multiple times throughout the story and is such a terribly named\nword. It should be the appropriate sound, and become \"f*cking spider\"! It also doesn’t help that you are writing in third\nperson, which means that you could have brought in third person pronouns, such as \"She\" or \"I\" (though you may want to\nchange some of the wording, such as \"She\"). The last thing I would change about this story is the different lengthy, regular\nsentences that end so often. One of the most common techniques used for lengthening stories is italicizing, wherein you\nsubstitute a word with an italicized sound.\nChoice: paragraph2\nJustification: Paragraph 1 has several grammatical flaws. It also begins to ramble in places. Paragraph 2 includes pertinent\ndetails and completes the main subject.\nChoice: paragraph1\nJustification: Both are truly terrible on every metric. Paragraph 2 is so problematic on fluency that Paragraph 1 wins despite\nusing the non-word \"amn´t.\" As far as coherence and information goes, they are equally dreadful, and neither makes any sense\nwhatsoever.\nChoice: paragraph2\nJustification: 1 deviates halfway through the prompt and starts talking about a different subject matter almost seemlessly. It\nalmost makes sense if you don’t read it very closely.\nFigure 11: WritingPrompts example 2.\n9048\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe limitation section on page 9\n□\u0013 A2. Did you discuss any potential risks of your work?\nThe ethical consideration section on page 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nabstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4.2, Appendix F , and Appendix G\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.2, Appendix F , and Appendix G\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix G\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix G\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe used public datasets and our data collection does not introduce identiﬁcations or offensive\ncontent.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.2, Appendix F , and Appendix G\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.2, Appendix F , and Appendix G\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Appendix G\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9049\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix G\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4 and Appendix D\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix G\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4.2 and Appendix F\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 4.2 and Appendix F\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 4.2 and Appendix F\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix F\n□\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nAppendix F , by a Bloomberg legal team\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nAppendix F\n9050",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.7443947792053223
    },
    {
      "name": "Zhàng",
      "score": 0.5614961385726929
    },
    {
      "name": "Computational linguistics",
      "score": 0.5292738676071167
    },
    {
      "name": "Computer science",
      "score": 0.4725605845451355
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4299297332763672
    },
    {
      "name": "Mathematical economics",
      "score": 0.4201710820198059
    },
    {
      "name": "Econometrics",
      "score": 0.419197678565979
    },
    {
      "name": "Natural language processing",
      "score": 0.3408701419830322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3386595845222473
    },
    {
      "name": "Applied mathematics",
      "score": 0.3251182436943054
    },
    {
      "name": "Mathematics",
      "score": 0.30940771102905273
    },
    {
      "name": "History",
      "score": 0.17396005988121033
    },
    {
      "name": "Thermodynamics",
      "score": 0.09439659118652344
    },
    {
      "name": "Physics",
      "score": 0.08253693580627441
    },
    {
      "name": "China",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}