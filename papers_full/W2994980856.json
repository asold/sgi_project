{
  "title": "FlauBERT: Unsupervised Language Model Pre-training for French",
  "url": "https://openalex.org/W2994980856",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2151311963",
      "name": "Hang Le",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A2606294960",
      "name": "Loïc Vial",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A2784273412",
      "name": "Jibril Frej",
      "affiliations": [
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2954872716",
      "name": "Vincent Segonne",
      "affiliations": [
        "Délégation Paris 7",
        "Université Paris Cité"
      ]
    },
    {
      "id": "https://openalex.org/A2512618028",
      "name": "Maximin Coavoux",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A2304643320",
      "name": "Benjamin Lecouteux",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A208304746",
      "name": "Alexandre Allauzen",
      "affiliations": [
        "Université Paris Sciences et Lettres"
      ]
    },
    {
      "id": "https://openalex.org/A1835867671",
      "name": "Benoît Crabbé",
      "affiliations": [
        "Délégation Paris 7",
        "Université Paris Cité"
      ]
    },
    {
      "id": "https://openalex.org/A2237966808",
      "name": "Laurent Besacier",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2777519208",
      "name": "Didier Schwab",
      "affiliations": [
        "Laboratoire d'Informatique de Grenoble",
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W11511616",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W338621447",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W1021951279",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2989268779",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2953109491",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2251529656",
    "https://openalex.org/W2436001372",
    "https://openalex.org/W1865928303",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2541443726",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2171068337",
    "https://openalex.org/W2791272817",
    "https://openalex.org/W2954203547",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2143391265",
    "https://openalex.org/W2102153514",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2970558573",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2065157922",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2976444281",
    "https://openalex.org/W2150337636",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3158775312",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2250653840"
  ],
  "abstract": "Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP.",
  "full_text": "arXiv:1912.05372v4  [cs.CL]  12 Mar 2020\nFlauBERT: Unsupervised Language Model Pre-training for Fr ench\nHang Le 1 Lo¨ ıc Vial1 Jibril Frej 1 Vincent Segonne 2 Maximin Coavoux 1\nBenjamin Lecouteux 1 Alexandre Allauzen 3 Benoˆ ıt Crabb´e2 Laurent Besacier 1 Didier Schwab 1\n1 Univ . Grenoble Alpes, CNRS, LIG 2 Universit´ e Paris Diderot 3 E.S.P .C.I, CNRS LAMSADE, PSL Research University\n{thi-phuong-hang.le, loic.vial, jibril.frej}@univ-grenoble-alpes.fr\n{maximin.coavoux, didier.schwab, benjamin.lecouteux, laurent.besacier}@univ-grenoble-alpes.fr\n{vincent.segonne@etu, bcrabbe@linguist}.univ-paris-diderot.fr, alexandre.allauzen@espci.fr\nAbstract\nLanguage models have become a key step to achieve state-of-t he art results in many different Natural Language Processin g (NLP)\ntasks. Leveraging the huge amount of unlabeled texts nowada ys available, they provide an efﬁcient way to pre-train cont inuous word\nrepresentations that can be ﬁne-tuned for a downstream task , along with their contextualization at the sentence level. This has been\nwidely demonstrated for English using contextualized repr esentations (Dai and Le, 2015; Peters et al., 2018; Howard an d Ruder, 2018;\nRadford et al., 2018; Devlin et al., 2019; Y ang et al., 2019b) . In this paper, we introduce and share FlauBERT, a model lear ned on\na very large and heterogeneous French corpus. Models of diff erent sizes are trained using the new CNRS (French National C entre\nfor Scientiﬁc Research) Jean Zay supercomputer. W e apply our French language models to diver se NLP tasks (text classiﬁcation,\nparaphrasing, natural language inference, parsing, word s ense disambiguation) and show that most of the time they outp erform other\npre-training approaches. Different versions of FlauBERT a s well as a uniﬁed evaluation protocol for the downstream tas ks, called FLUE\n(French Language Understanding Evaluation), are shared to the research community for further reproducible experimen ts in French NLP .\nKeywords: FlauBERT, FLUE, BERT , Transformer, French, language model , pre-training, NLP benchmark, text classiﬁcation,\nparsing, word sense disambiguation, natural language infe rence, paraphrase.\n1. Introduction\nA recent game-changing contribution in Natural Language\nProcessing (NLP) was the introduction of deep unsu-\npervised language representations pre-trained using only\nplain text corpora. Previous word embedding pre-training\napproaches, such as word2vec (Mikolov et al., 2013) or\nGloV e (Pennington et al., 2014), learn a single vector for\neach wordform. By contrast, these new models are trained\nto produce contextual embeddings : the output representa-\ntion depends on the entire input sequence ( e.g. each to-\nken instance has a vector representation that depends on its\nleft and right context). Initially based on recurrent neura l\nnetworks (Dai and Le, 2015; Ramachandran et al., 2017;\nHoward and Ruder, 2018; Peters et al., 2018), these mod-\nels quickly converged towards the use of the Transformer\n(V aswani et al., 2017), such as GPT (Radford et al., 2018),\nBER T (Devlin et al., 2019), XLNet (Y ang et al., 2019b),\nRoBER T a (Liu et al., 2019). Using these pre-trained mod-\nels in a transfer learning fashion has shown to yield strik-\ning improvements across a wide range of NLP tasks. One\ncan easily build state-of-the-art NLP systems thanks to\nthe publicly available pre-trained weights, saving time,\nenergy, and resources. As a consequence, unsupervised\nlanguage model pre-training has become a de facto stan-\ndard in NLP . This has been, however, mostly demon-\nstrated for English even though multi-lingual or cross-\nlingual variants are also available, taking into account mo re\nthan a hundred languages in a single model: mBER T\n(Devlin et al., 2019), XLM (Lample and Conneau, 2019),\nXLM-R (Conneau et al., 2019).\nIn this paper, we describe our methodology to build\nFlauBER T – French Language Understanding via\nBidirectional Encoder Representations from Transformers,\na French BER T 1 model that outperforms multi-\nlingual/cross-lingual models in several downstream\nNLP tasks, under similar conﬁgurations. FlauBER T\nrelies on freely available datasets and is made publicly\navailable in different versions. 2 For further reproducible\nexperiments, we also provide the complete processing and\ntraining pipeline as well as a general benchmark for evalu-\nating French NLP systems. This evaluation setup is similar\nto the popular GLUE benchmark (W ang et al., 2018),\nand is named FLUE (French Language Understanding\nEvaluation).\n2. Related W ork\n2.1. Pre-trained Language Models\nSelf-supervised3 pre-training on unlabeled text data was\nﬁrst proposed in the task of neural language modeling\n(Bengio et al., 2003; Collobert and W eston, 2008), where\nit was shown that a neural network trained to pre-\ndict next word from prior words can learn useful em-\nbedding representations, called word embeddings (each\nword is represented by a ﬁxed vector). These rep-\nresentations were shown to play an important role in\nNLP , yielding state-of-the-art performance on multiple\n1 W e learned of a similar project that resulted in a publicatio n\non arXiv (Martin et al., 2019). However, we believe that thes e two\nworks on French language models are complementary since the\nNLP tasks we addressed are different, as are the training cor pora\nand preprocessing pipelines. W e also point out that our mode ls\nwere trained using the CNRS (French National Centre for Scie n-\ntiﬁc Research) public research computational infrastruct ure and\ndid not receive any assistance from a private stakeholder.\n2 https://github.com/getalp/Flaubert\n3 Self-supervised learning is a special case of unsupervised\nlearning where unlabeled data is used as a supervision signal.\ntasks (Collobert et al., 2011), especially after the intro-\nduction of word2vec (Mikolov et al., 2013) and GloV e\n(Pennington et al., 2014), efﬁcient and effective algorith ms\nfor learning word embeddings.\nA major limitation of word embeddings is that a word can\nonly have a single representation, even if it can have mul-\ntiple meanings ( e.g. depending on the context). There-\nfore, recent works have introduced a paradigm shift from\ncontext-free word embeddings to contextual embeddings :\nthe output representation is a function of the entire input\nsequence, which allows encoding complex, high-level syn-\ntactic and semantic characteristics of words or sentences.\nThis line of research was started by Dai and Le (2015)\nwho proposed pre-training representations via either an\nencoder-decoder language model or a sequence autoen-\ncoder. Ramachandran et al. (2017) 4 showed that this\napproach can be applied to pre-training sequence-to-\nsequence models (Sutskever et al., 2014). These models,\nhowever, require a signiﬁcant amount of in-domain data\nfor the pre-training tasks. Peters et al. (2018, ELMo)\nand Howard and Ruder (2018, ULMFiT) were the ﬁrst\nto demonstrate that leveraging huge general-domain text\ncorpora in pre-training can lead to substantial improve-\nments on downstream tasks. Both methods employ LSTM\n(Hochreiter and Schmidhuber, 1997) language models, but\nULMFiT utilizes a regular multi-layer architecture, while\nELMo adopts a bidirectional LSTM to build the ﬁnal\nembedding for each input token from the concatenation of\nthe left-to-right and right-to-left representations. Ano ther\nfundamental difference lies in how each model can be\ntuned to different downstream tasks: ELMo delivers\ndifferent word vectors that can be interpolated, whereas\nULMFiT enables robust ﬁne-tuning of the whole network\nw .r .t. the downstream tasks. The ability of ﬁne-tuning was\nshown to signiﬁcantly boost the performance, and thus\nthis approach has been further developed in the recent\nworks such as MultiFiT (Eisenschlos et al., 2019) or most\nprominently Transformer-based (V aswani et al., 2017)\narchitectures: GPT (Radford et al., 2018), BER T\n(Devlin et al., 2019), XLNet (Y ang et al., 2019b), XLM\n(Lample and Conneau, 2019), RoBER T a (Liu et al., 2019),\nALBER T (Lan et al., 2019), T5 (Raffel et al., 2019). These\nmethods have one after the other established new state-of-\nthe-art results on various NLP benchmarks, such as GLUE\n(W ang et al., 2018) or SQuAD (Rajpurkar et al., 2018),\nsurpassing previous methods by a large margin.\n2.2. Pre-trained Language Models Beyond\nEnglish\nGiven the impact of pre-trained language models on NLP\ndownstream tasks in English, several works have recently\nreleased pre-trained models for other languages. For\ninstance, ELMo exists for Portuguese, Japanese, German\nand Basque, 5 while BER T and variants were speciﬁcally\ntrained for simpliﬁed and traditional Chinese 8 and Ger-\n4 It should be noted that learning contextual embeddings was\nalso proposed in (McCann et al., 2017), but in a supervised fash-\nion as they used annotated machine translation data.\n5 https://allennlp.org/elmo\nman.6 A Portuguese version of MultiFiT is also available. 7\nRecently, more monolingual BER T -based models have\nbeen released, such as for Arabic (Antoun et al., 2020),\nDutch (de Vries et al., 2019; Delobelle et al., 2020),\nFinnish (V irtanen et al., 2019), Italian\n(Polignano et al., 2019), Portuguese (Souza et al., 2019),\nRussian (Kuratov and Arkhipov, 2019), Span-\nish (Ca ˜ nete et al., 2020), and V ietnamese\n(Nguyen and Nguyen, 2020). For French, besides\npre-trained language models using ULMFiT and MultiFiT\nconﬁgurations, 7 CamemBER T (Martin et al., 2019) is a\nFrench BER T model concurrent to our work.\nAnother trend considers one model estimated for sev-\neral languages with a shared vocabulary. The release\nof multilingual BER T for 104 languages pioneered this\napproach.8 A recent extension of this work leverages\nparallel data to build a cross-lingual pre-trained ver-\nsion of LASER (Artetxe and Schwenk, 2019) for 93 lan-\nguages, XLM (Lample and Conneau, 2019) and XLM-R\n(Conneau et al., 2019) for 100 languages.\n2.3. Evaluation Protocol for French NLP T asks\nThe existence of a multi-task evaluation benchmark such\nas GLUE (W ang et al., 2018) for English is highly beneﬁ-\ncial to facilitate research in the language of interest. The\nGLUE benchmark has become a prominent framework to\nevaluate the performance of NLP models in English. The\nrecent contributions based on pre-trained language models\nhave led to remarkable performance across a wide range\nof Natural Language Understanding (NLU) tasks. The\nauthors of GLUE have therefore introduced SuperGLUE\n(W ang et al., 2019a): a new benchmark built on the princi-\nples of GLUE, including more challenging and diverse set\nof tasks. A Chinese version of GLUE 9 is also developed\nto evaluate model performance in Chinese NLP tasks. As\nof now , we have not learned of any such benchmark for\nFrench.\n3. Building FlauBERT\nIn this section, we describe the training corpus, the text\npreprocessing pipeline, the model architecture and traini ng\nconﬁgurations to build FlauBER T BASE and FlauBER TLARGE .\n3.1. T raining Data\nData collection Our French text corpus consists of 24\nsub-corpora gathered from different sources, covering di-\nverse topics and writing styles, ranging from formal and\nwell-written text ( e.g. Wikipedia and books) 10 to random\ntext crawled from the Internet ( e.g. Common Crawl). 11\nThe data were collected from three main sources: (1)\nmonolingual data for French provided in WMT19 shared\ntasks (Li et al., 2019, 4 sub-corpora); (2) French text cor-\npora offered in the OPUS collection (Tiedemann, 2012, 8\n6 https://deepset.ai/german-bert\n7 https://github.com/piegu/language-models\n8 https://github.com/google-research/bert\n9 https://github.com/chineseGLUE/chineseGLUE\n10 http://www.gutenberg.org\n11 http://data.statmt.org/ngrams/deduped2017\nBER TBASE RoBER T aBASE CamemBER T FlauBER TBASE /FlauBER TLARGE\nLanguage English English French French\nTraining data 13 GB 160 GB 138 GB † 71 GB ‡\nPre-training objectives NSP and MLM MLM MLM MLM\nT otal parameters 110 M 125 M 110 M 138 M/ 373 M\nT okenizer W ordPiece 30K BPE 50K SentencePiece 32K BPE 50K\nMasking strategy Static + Sub-word masking Dynamic + Sub-word masking Dynamic + Whole-word masking Dynamic + Sub-word masking\n†, ‡: 282 GB, 270 GB before ﬁltering/cleaning.\nT able 1: Comparison between FlauBER T and previous work.\nsub-corpora); and (3) datasets available in the Wikimedia\nprojects (Meta, 2019, 8 sub-corpora).\nW e used the WikiExtractor tool 12 to extract the text from\nWikipedia. For the other sub-corpora, we either used our\nown tool to extract the text or download them directly from\ntheir websites. The total size of the uncompressed text be-\nfore preprocessing is 270 GB. More details can be found in\nAppendix A.1.\nData preprocessing For all sub-corpora, we ﬁltered\nout very short sentences as well as repetitive and non-\nmeaningful content such as telephone/fax numbers, email\naddresses, etc. For Common Crawl, which is our largest\nsub-corpus with 215 GB of raw text, we applied aggressive\ncleaning to reduce its size to 43.4 GB. All the data were\nUnicode-normalized in a consistent way before being to-\nkenized using Moses tokenizer (Koehn et al., 2007). The\nresulting training corpus is 71 GB in size.\nOur code for downloading and preprocessing data is made\npublicly available. 13\n3.2. Models and T raining Conﬁgurations\nModel architecture FlauBER T has the same model\narchitecture as BER T (Devlin et al., 2019), which\nconsists of a multi-layer bidirectional Transformer\n(V aswani et al., 2017). Following Devlin et al. (2019), we\npropose two model sizes:\n• FlauBER TBASE : L = 12, H = 768, A = 12,\n• FlauBER TLARGE : L = 24, H = 1024, A = 16,\nwhere L, H and A respectively denote the number of Trans-\nformer blocks, the hidden size, and the number of self-\nattention heads. As Transformer has become quite stan-\ndard, we refer to V aswani et al. (2017) for further details.\nT raining objective and optimization Pre-training of the\noriginal BER T (Devlin et al., 2019) consists of two super-\nvised tasks: (1) a masked language model (MLM) that\nlearns to predict randomly masked tokens; and (2) a next\nsentence prediction (NSP) task in which the model learns\nto predict whether B is the actual next sentence that follows\nA, given a pair of input sentences A,B.\nDevlin et al. (2019) observed that removing NSP signiﬁ-\ncantly hurts performance on some downstream tasks. How-\never, the opposite was shown in later studies, including\nY ang et al. (2019b, XLNet), Lample and Conneau (2019,\n12 https://github.com/attardi/wikiextractor\n13 https://github.com/getalp/Flaubert\nXLM), and Liu et al. (2019, RoBER T a). 14 Therefore, we\nonly employed the MLM objective in FlauBER T.\nT o optimize this objective function, we followed\nLiu et al. (2019) and used the Adam optimizer\n(Kingma and Ba, 2014) with the following parameters:\n• FlauBER TBASE : warmup steps of 24k, peak learning\nrate of 6e−4, β1 = 0.9, β2 = 0.98, ǫ = 1e−6 and\nweight decay of 0.01.\n• FlauBER TLARGE : warmup steps of 30k, peak learning\nrate of 3e−4, β1 = 0.9, β2 = 0.98, ǫ = 1e−6 and\nweight decay of 0.01.\nT raining FlauBERT LARGE Training very deep\nTransformers is known to be susceptible to instabil-\nity (W ang et al., 2019b; Nguyen and Salazar, 2019;\nXu et al., 2019; Fan et al., 2019). Not surprisingly, we also\nobserved this difﬁculty when training FlauBER T LARGE\nusing the same conﬁgurations as BER T LARGE and\nRoBER T aLARGE , where divergence happened at an early\nstage.\nSeveral methods have been proposed to tackle this is-\nsue. For example, in an updated implementation of\nthe Transformer (V aswani et al., 2018), layer normaliza-\ntion is applied before each attention layer by default,\nrather than after each residual block as in the original im-\nplementation (V aswani et al., 2017). These conﬁgurations\nare called pre-norm and post-norm, respectively. It was\nobserved by V aswani et al. (2018), and again conﬁrmed\nby later works e.g. (W ang et al., 2019b; Xu et al., 2019;\nNguyen and Salazar, 2019), that pre-norm helps stabilize\ntraining. Recently, a regularization technique called\nstochastic depths (Huang et al., 2016) has been demon-\nstrated to be very effective for training deep Transform-\ners, by e.g. Pham et al. (2019) and Fan et al. (2019) who\nsuccessfully trained architectures of more than 40 layers.\nThe idea is to randomly drop a number of (attention) layers\nat each training step. Other techniques are also available\nsuch as progressive training (Gong et al., 2019), or improv-\ning initialization (Zhang et al., 2019a; Xu et al., 2019) and\nnormalization (Nguyen and Salazar, 2019).\nFor training FlauBER T LARGE , we employed pre-norm atten-\ntion and stochastic depths for their simplicity. W e found\nthat these two techniques were sufﬁcient for successful\ntraining. W e set the rate of layer dropping to 0.2 in all the\nexperiments.\n14 Liu et al. (2019) hypothesized that the original BERT imple-\nmentation may only have removed the loss term while still ret ain-\ning a bad input format, resulting in performance degradatio n.\nOther training details A vocabulary of 50K sub-word\nunits is built using the Byte Pair Encoding (BPE) al-\ngorithm (Sennrich et al., 2016). The only difference be-\ntween our work and RoBER T a is that the training\ndata are preprocessed and tokenized using a basic tok-\nenizer for French (Koehn et al., 2007, Moses), as in XLM\n(Lample and Conneau, 2019), before the application of\nBPE. W e use fastBPE,15 a very efﬁcient implementation to\nextract the BPE units and encode the corpora.\nFlauBER TBASE is trained on 32 GPUs Nvidia V100 in 410\nhours and FlauBER T LARGE is trained on 128 GPUs in 390\nhours, both with the effective batch size of 8192 sequences.\nFinally, we summarize the differences between FlauBER T\nand BER T, RoBER T a, CamemBER T in T able 1.\n4. FLUE\nIn this section, we compile a set of existing French lan-\nguage tasks to form an evaluation benchmark for French\nNLP that we called FLUE (French Language Understand-\ning Evaluation). W e select the datasets from different do-\nmains, level of difﬁculty, degree of formality, and amount\nof training samples. Three out of six tasks (T ext Classiﬁ-\ncation, Paraphrase, Natural Language Inference) are from\ncross-lingual datasets since we also aim to provide results\nfrom a monolingual pre-trained model to facilitate future\nstudies of cross-lingual models, which have been drawing\nmuch of research interest recently.\nT able 2 gives an overview of the datasets, including their\ndomains and training/development/test splits. The detail s\nare presented in the next subsections.\nDataset Domain Train Dev T est\nCLS-FR\nBooks\nProduct reviews\n2 000 - 2 000\nDVD 1 999 - 2 000\nMusic 1 998 - 2 000\nP A WS-X-FR General domain 49 401 1 992 1 985\nXNLI-FR Diverse genres 392 702 2 490 5 010\nFrench Treebank Daily newspaper 14 759 1 235 2 541\nFrenchSemEval Diverse genres 55 206 - 3 199\nNoun Sense Disambiguation Diverse genres 818 262 - 1 445\nT able 2: Descriptions of the datasets included in our FLUE\nbenchmark.\n4.1. T ext Classiﬁcation\nCLS The Cross Lingual Sentiment CLS\n(Prettenhofer and Stein, 2010) dataset consists of Ama-\nzon reviews for three product categories: books, DVD,\nand music in four languages: English, French, Ger-\nman, and Japanese. Each sample contains a review text\nand the associated rating from 1 to 5 stars. Following\nBlitzer et al. (2006) and Prettenhofer and Stein (2010),\nratings with 3 stars are removed. Positive reviews have\nratings higher than 3 and negative reviews are those rated\nlower than 3. There is one train and test set for each\nproduct category. The train and test sets are balanced,\nincluding around 1 000 positive and 1 000 negative reviews\nfor a total of 2 000 reviews in each dataset. W e take the\n15 https://github.com/glample/fastBPE\nFrench portion to create the binary text classiﬁcation task\nin FLUE and report the accuracy on the test set.\n4.2. Paraphrasing\nP A WS-X The Cross-lingual Adversarial Dataset for Para-\nphrase Identiﬁcation P A WS-X (Y ang et al., 2019a) is the\nextension of the Paraphrase Adversaries from W ord Scram-\nbling P A WS (Zhang et al., 2019b) for English to six other\nlanguages: French, Spanish, German, Chinese, Japanese\nand Korean. P A WS composes English paraphrase identiﬁ-\ncation pairs from Wikipedia and Quora in which two sen-\ntences in a pair have high lexical overlap ratio, generated b y\nLM-based word scrambling and back translation followed\nby human judgement. The paraphrasing task is to iden-\ntify whether the sentences in these pairs are semantically\nequivalent or not. Similar to previous approaches to cre-\nate multilingual corpora, Y ang et al. (2019a) used machine\ntranslation to create the training set for each target langu age\nin P A WS-X from the English training set in P A WS. The de-\nvelopment and test sets for each language are translated by\nhuman translators. W e take the related datasets for French\nto perform the paraphrasing task and report the accuracy on\nthe test set.\n4.3. Natural Language Inference\nXNLI The Cross-lingual NLI (XNLI) corpus\n(Conneau et al., 2018) extends the development and\ntest sets of the Multi-Genre Natural Language Inference\ncorpus (Williams et al., 2018, MultiNLI) to 15 languages.\nThe development and test sets for each language consist\nof 7 500 human-annotated examples, making up a total\nof 112 500 sentence pairs annotated with the labels en-\ntailment, contradiction, or neutral. Each sentence pair\nincludes a premise ( p) and a hypothesis ( h). The Natural\nLanguage Inference (NLI) task, also known as recognizing\ntextual entailment (R TE), is to determine whether p entails,\ncontradicts or neither entails nor contradicts h. W e take the\nFrench part of the XNLI corpus to form the development\nand test sets for the NLI task in FLUE. The train set is\nobtained from the machine translated version to French\nprovided in XNLI. Following Conneau et al. (2018), we\nreport the test accuracy.\n4.4. Parsing and Part-of-Speech T agging\nSyntactic parsing consists in assigning a tree structure to a\nsentence in natural language. W e perform parsing on the\nFrench Treebank (Abeill´ e et al., 2003), a collection of sen -\ntences extracted from French daily newspaper Le Monde,\nand manually annotated with both constituency and depen-\ndency syntactic trees and part-of-speech tags. Speciﬁcall y,\nwe use the version of the corpus instantiated for the SPMRL\n2013 shared task and described by Seddah et al. (2013).\nThis version is provided with a standard split representing\n14 759 sentences for the training corpus, and respectively\n1 235 and 2 541 sentences for the development and evalua-\ntion sets.\n4.5. W ord Sense Disambiguation T asks\nW ord Sense Disambiguation (WSD) is a classiﬁcation task\nwhich aims to predict the sense of words in a given\ncontext according to a speciﬁc sense inventory. W e\nused two French WSD tasks: the FrenchSemEval task\n(Segonne et al., 2019), which targets verbs only, and a\nmodiﬁed version of the French part of the Multilingual\nWSD task of SemEval 2013 (Navigli et al., 2013), which\ntargets nouns.\nV erb Sense Disambiguation W e made experiments of\nsense disambiguation focused on French verbs using\nFrenchSemEval (Segonne et al., 2019, FSE), an evaluation\ndataset in which verb occurrences were manually sense an-\nnotated with the sense inventory of Wiktionary, a collabora -\ntively edited open-source dictionary. FSE includes both th e\nevaluation data and the sense inventory. The evaluation dat a\nconsists of 3 199 manual annotations among a selection of\n66 verbs which makes roughly 50 sense annotated occur-\nrences per verb. The sense inventory provided in FSE is\na Wiktionary dump (04-20-2018) openly available via Db-\nnary (S´ erasset, 2012). For a given sense of a target key, the\nsense inventory offers a deﬁnition along with one or more\nexamples. For this task, we considered the examples of the\nsense inventory as training examples and tested our model\non the evaluation dataset.\nNoun Sense Disambiguation W e propose a new chal-\nlenging task for the WSD of French, based on the\nFrench part of the Multilingual WSD task of Se-\nmEval 2013 (Navigli et al., 2013), which targets nouns\nonly. W e adapted the task to use the W ordNet\n3.0 sense inventory (Miller, 1995) instead of BabelNet\n(Navigli and Ponzetto, 2010), by converting the sense keys\nto W ordNet 3.0 if a mapping exists in BabelNet, and re-\nmoving them otherwise.\nThe result of the conversion process is an evaluation corpus\ncomposed of 306 sentences and 1 445 French nouns anno-\ntated with W ordNet sense keys, and manually veriﬁed.\nFor the training data, we followed the method pro-\nposed by Hadj Salah (2018), and translated the SemCor\n(Miller et al., 1993) and the W ordNet Gloss Corpus 16 into\nFrench, using the best English-French Machine Translation\nsystem of the fairseq toolkit17 (Ott et al., 2019). Finally, we\naligned the W ordNet sense annotation from the source En-\nglish words to the the translated French words, using the\nalignment provided by the MT system.\nW e rely on W ordNet sense keys instead of the original Ba-\nbelNet annotations for the following two reasons. First,\nW ordNet is a resource that is entirely manually veriﬁed,\nand widely used in WSD research (Navigli, 2009). Sec-\nond, there is already a large quantity of sense annotated dat a\nbased on the sense inventory of W ordNet (V ial et al., 2018)\nthat we can use for the training of our system.\nW e publicly release 18 both our training data and the evalu-\nation data in the UFSAC format (V ial et al., 2018).\n5. Experiments and Results\nIn this section, we present FlauBER T ﬁne-tuning results\non the FLUE benchmark. W e compare the performance\n16 The set of W ordNet glosses semi-automatically sense anno-\ntated which is released as part of W ordNet since version 3.0.\n17 https://github.com/pytorch/fairseq\n18 https://zenodo.org/record/3549806\nof FlauBER T with Multilingual BER T (Devlin et al., 2019,\nmBER T) and CamemBER T (Martin et al., 2019) on all\ntasks. In addition, for each task we also include the best\nnon-BER T model for comparison. W e made use of the\nopen source libraries (Lample and Conneau, 2019, XLM)\nand (W olf et al., 2019, Transformers) in some of the exper-\niments.\n5.1. T ext Classiﬁcation\nModel description W e followed the standard ﬁne-tuning\nprocess of BER T (Devlin et al., 2019). The input is a de-\ngenerate text- ∅ pair. The classiﬁcation head is composed\nof the following layers, in order: dropout, linear, tanh ac-\ntivation, dropout, and linear. The output dimensions of the\nlinear layers are respectively equal to the hidden size of th e\nTransformer and the number of classes (which is 2 in this\ncase as the task is binary classiﬁcation). The dropout rate\nwas set to 0.1.\nW e trained for 30 epochs using a batch size of 16 while per-\nforming a grid search over 4 different learning rates: 1e−5,\n5e−5, 1e−6, and 5e−6. A random split of 20% of the train-\ning data was used as validation set, and the best performing\nmodel on this set was then chosen for evaluation on the test\nset.\nModel Books DVD Music\nMultiFiT† 91.25 89.55 93.40\nmBER T† 86.15 86.90 86.65\nCamemBER T 92.30 93.00 94.85\nFlauBER TBASE 93.10 92.45 94.10\nFlauBER TLARGE 95.00 94.10 95.85\n† Results reported in (Eisenschlos et al., 2019).\nT able 3: Accuracy on the CLS dataset for French.\nResults T able 3 presents the ﬁnal accuracy on the test set\nfor each model. The results highlight the importance of\na monolingual French model for text classiﬁcation: both\nCamemBER T and FlauBER T outperform mBER T by a\nlarge margin. FlauBER T BASE performs moderately better\nthan CamemBER T in the books dataset, while its results on\nthe two remaining datasets of DVD and music are lower\nthan those of CamemBER T . FlauBER T LARGE achieves the\nbest results in all categories.\n5.2. Paraphrasing\nModel description The setup for this task is almost iden-\ntical to the previous one, except that: (1) the input sequenc e\nis now a pair of sentences A,B; and (2) the hyper-parameter\nsearch is performed on the development data set ( i.e. no val-\nidation split is needed).\nResults The ﬁnal accuracy for each model is reported in\nT able 4. One can observe that the monolingual French mod-\nels perform only slightly better than the multilingual mode l\nmBER T , which could be attributed to the characteristics of\nthe P A WS-X dataset. Containing samples with high lexical\noverlap ratio, this dataset has been proved to be an effectiv e\nmeasure of model sensitivity to word order and syntactic\nstructure (Y ang et al., 2019a). A multilingual model such\nas mBER T , therefore, could capture these features as well\nas a monolingual model.\nModel Accuracy\nESIM† (Chen et al., 2017) 66.20\nmBER T† 89.30\nCamemBER T 90.14\nFlauBER TBASE 89.49\nFlauBER TLARGE 89.34\n† Results reported in (Y ang et al., 2019a).\nT able 4: Results on the French P A WS-X dataset.\n5.3. Natural Language Inference\nModel description As this task was also considered in\n(Martin et al., 2019, CamemBER T), for a fair comparison,\nhere we replicate the same experimental setup. Similar to\nparaphrasing, the model input of this task is also a pair\nof sentences. The classiﬁcation head, however, consists of\nonly one dropout layer followed by one linear layer.\nResults W e report the ﬁnal accuracy for each model in\nT able 5. The results conﬁrm the superiority of the French\nmodels compared to the multilingual model mBER T on\nthis task. FlauBER T LARGE performs moderately bet-\nter than CamemBER T . Both of them clearly outperform\nXLM-RBASE , while cannot surpass XLM-R LARGE .\nModel Accuracy\nXLM-RLARGE\n† 85.2\nXLM-RBASE\n† 80.1\nmBER T‡ 76.9\nCamemBER T ‡ 81.2\nFlauBER TBASE 80.6\nFlauBER TLARGE 83.4\n† Results reported in (Conneau et al., 2019).\n‡ Results reported in (Martin et al., 2019).\nT able 5: Results on the French XNLI dataset.\n5.4. Constituency Parsing and POS T agging\nModel description W e use the parser described by\nKitaev and Klein (2018) and Kitaev et al. (2019). It is an\nopenly available 19 chart parser based on a self-attentive en-\ncoder. W e compare (i) a model without any pre-trained\nparameters, (ii) a model that additionally uses and ﬁne-\ntunes fastT ext20 pre-trained embeddings, (iii) models based\non pre-trained language models: mBER T , CamemBER T ,\nand FlauBER T. W e use the default hyperparameters from\nKitaev and Klein (2018) for the ﬁrst two settings and the\nhyperparameters from Kitaev et al. (2019) when using pre-\ntrained language models, except for FlauBER T LARGE . For\nthis last model, we use a different learning rate (0.00001),\nbatch size (8) and ignore training sentences longer than 100\n19 https://github.com/nikitakit/self-attentive-parser\n20 https://fasttext.cc/\ntokens, due to memory limitation. W e jointly perform part-\nof-speech (POS) tagging based on the same input as the\nparser, in a multitask setting. For each setting we perform\ntraining 3 times with different random seeds and select best\nmodel according to development F-score.\nFor ﬁnal evaluation, we use the evaluation tool provided\nby the SPMRL shared task organizers 21 and report labelled\nF-score, the standard metric for constituency parsing eval -\nuation, as well as POS tagging accuracy.\nModel Dev T est\nF1 POS F 1 POS\nBest published (Kitaev et al., 2019) 87.42\nNo pre-training 84.31 97.6 83.85 97.5\nfastT ext pre-trained embeddings 84.09 97.6 83.64 97.7\nmBER T 87.25 98.1 87.52 98.1\nCamemBER T (Martin et al., 2019) 88.53 98.1 88.39 98.2\nFlauBER TBASE 88.95 98.2 89.05 98.1\nFlauBER TLARGE 89.08 98.2 88.63 98.2\nEnsemble: FlauBER T BASE + CamemBER T 89.32 89.28\nT able 6: Constituency parsing and POS tagging results.\nResults W e report constituency parsing results in T a-\nble 6. Without pre-training, we replicate the result from\nKitaev and Klein (2018). FastT ext pre-trained embeddings\ndo not bring improvement over this already strong model.\nWhen using pre-trained language models, we observe that\nCamemBER T , with its language-speciﬁc training improves\nover mBER T by 0.9 absolute F 1. FlauBER T BASE outper-\nforms CamemBER T by 0.7 absolute F 1 on the test set and\nobtains the best published results on the task for a single\nmodel. Regarding POS tagging, all large-scale pre-trained\nlanguage models obtain similar results (98.1-98.2), and ou t-\nperform models without pre-training or with fastT ext em-\nbeddings (97.5-97.7). FlauBER T LARGE provides a marginal\nimprovement on the development set, and fails to reach\nFlauBER TBASE results on the test set.\nIn order to assess whether FlauBER T and CamemBER T are\ncomplementary for this task, we evaluate an ensemble of\nboth models (last line in T able 6). The ensemble model\nimproves by 0.4 absolute F 1 over FlauBER T on the devel-\nopment set and 0.2 on the test set, obtaining the highest re-\nsult for the task. This result suggests that both pre-traine d\nlanguage models are complementary and have their own\nstrengths and weaknesses.\n5.5. Dependency parsing\nModel W e use our own reimplementation of the parsing\nmodel of Dozat and Manning (2016) with maximum span-\nning tree decoding adapted to handle several input sources\nsuch as BER T representations. The model does not perform\npart of speech tagging but uses the predicted tags provided\nby the SPMRL shared task organizers.\nOur word representations are a concatenation of word\nembeddings and tag embeddings learned together with\nthe model parameters on the French Treebank data\nitself, and at most one of (fastT ext, CamemBER T ,\nFlauBER TBASE , FlauBER T BASE , mBER T) word vector. As\n21 http://pauillac.inria.fr/∼ seddah/evalb spmrl2013.tar.gz\nDozat and Manning (2016), we use word and tag dropout\n(d = 0.5) on word and tag embeddings but without dropout\non BER T representations. W e performed a fairly com-\nprehensive grid search on hyperparameters for each model\ntested.\nModel UAS LAS\nBest published (Constant et al., 2013) 89.19 85.86\nNo pre-training 88.92 85.11\nfastT ext pre-training 86.32 82.04\nmBER T 89.50 85.86\nCamemBER T 91.37 88.13\nFlauBER TBASE 91.56 88.35\nFlauBER TLARGE 91.61 88.47\nT able 7: Dependency parsing results.\nResults The results are reported in T able 7. The best\npublished results in this shared task (Constant et al., 2013 )\nwere involving an ensemble of parsers with additional re-\nsources for modelling multi word expressions (MWE), typ-\nical of the French treebank annotations. The monolin-\ngual French BER T models (CamemBER T , FlauBER T) per-\nform better and set the new state of the art on this dataset\nwith a single parser and without speciﬁc modelling for\nMWEs. One can observe that both FlauBER T models\nperform marginally better than CamemBER T , while all of\nthem outperform mBER T by a large margin.\n5.6. W ord Sense Disambiguation\nV erb Sense Disambiguation Disambiguation was per-\nformed with the same WSD supervised method used by\nSegonne et al. (2019). First we compute sense vector rep-\nresentations from examples found in the Wiktionary sense\ninventory: given a sense s and its corresponding examples,\nwe compute the vector representation of s by averaging\nthe vector representations of its examples. Then, we tag\neach test instance with the sense whose representation is\nthe closest based on cosine similarity. W e used the contex-\ntual embeddings output by FlauBER T as vector represen-\ntations for any given instance (from the sense inventory or\nthe test data) of a target word. W e proceeded the same way\nwith mBER T and CamemBER T for comparison. W e also\ncompared our model with a simpler context vector repre-\nsentation called averaged word embeddings (A WE) which\nconsists in representing context of target word by averagin g\nits surrounding words in a given window size. W e experi-\nmented A WE using fastT ext word embeddings with a win-\ndow of size 5. W e report results in T able 8. BER T -based\nmodels set the new state of the art on this task, with the best\nresults achieved by CamemBER T and FlauBER T LARGE .\nNoun Sense Disambiguation W e implemented a neu-\nral classiﬁer similar to the classiﬁer presented by\nV ial et al. (2019). This classiﬁer forwards the output of a\npre-trained language model to a stack of 6 trained Trans-\nformer encoder layers and predicts the synset of every in-\nput words through softmax. The only difference between\nour model and V ial et al. (2019) is that we chose the same\nModel F1\nfastT ext 34.90\nmBER T 49.83\nCamemBER T 50.02\nFlauBER TBASE 43.92\nFlauBER TLARGE 50.48\nT able 8: F1 scores (%) on the V erb Disambiguation T ask.\nhyper-parameter as FlauBER T BASE for the dff and the num-\nber of attention heads of the Transformer layers (more pre-\ncisely, dff = 3072and A = 12).\nModel Single Ensemble\nMean Std\nNo pre-training 45.73 ±1.91 50.03\nfastT ext 44.90 ±1.24 49.41\nmBER T 53.03 ±1.22 56.47\nCamemBER T 52.06 ±1.25 56.06\nFlauBER TBASE 51.24 ±1.33 54.74\nFlauBER TLARGE 53.53 ±1.36 57.85\nT able 9: F1 scores (%) on the Noun Disambiguation T ask.\nAt prediction time, we take the synset ID which has the\nmaximum value along the softmax layer (no ﬁlter on the\nlemma of the target is performed). W e trained 8 models for\nevery experiment, and we report the mean results, and the\nstandard deviation of the individual models, and also the\nresult of an ensemble of models, which averages the out-\nput of the softmax layer. Finally, we compared FlauBER T\nwith CamemBER T , mBER T , fastT ext and with no input\nembeddings. W e report the results in T able 9. On this\ntask and with these settings, we ﬁrst observe an advantage\nfor mBER T over both CamemBER T and FlauBER T BASE .\nW e think that it might be due to the fact that the train-\ning corpora we used are machine translated from English\nto French, so the multilingual nature of mBER T makes it\nprobably more ﬁtted for the task. Comparing CamemBER T\nto FlauBER T BASE , we see a small improvement in the for-\nmer model, and we think that this might be due to the dif-\nference in the sizes of pre-training corpora. Finally, with\nour FlauBER T LARGE model, we obtain the best scores on\nthe task, achieving more than 1 point above mBER T .\n6. Conclusion\nW e present and release FlauBER T, a pre-trained lan-\nguage model for French. FlauBER T was trained on a\nmultiple-source corpus and achieved state-of-the-art re-\nsults on a number of French NLP tasks, surpassing multi-\nlingual/cross-lingual models. FlauBER T is competitive\nwith CamemBER T (Martin et al., 2019) – another pre-\ntrained language model for French – despite being trained\non almost twice as fewer text data. In order to make the\npipeline entirely reproducible, we not only release prepro -\ncessing and training scripts, together with FlauBER T, but\nalso provide a general benchmark for evaluating French\nNLP systems (FLUE). FlauBER T is also now supported by\nHugging Face’s transformers library.22\n7. Acknowledgements\nThis work beneﬁted from the ‘Grand Challenge Jean\nZay’ program and was also partially supported by\nMIAI@Grenoble-Alpes (ANR-19-P3IA-0003).\nW e thank Guillaume Lample and Alexis Conneau for their\nactive technical support on using the XLM code.\nA Appendix\nA.1 Details on our French text corpus\nT able 10 presents the statistics of all sub-corpora in our\ntraining corpus. W e give the description of each sub-corpus\nbelow .\nDatasets from WMT19 shared tasks W e used four cor-\npora provided in the WMT19 shared task (Li et al., 2019). 23\n• Common Crawl includes text crawled from billions of\npages in the internet.\n• News Crawl contains crawled news collected from\n2007 to 2018.\n• EuroP arlcomposes text extracted from the proceed-\nings of the European Parliament.\n• News Commentary consists of text from news-\ncommentary crawl.\nDatasets from OPUS OPUS24 is a growing resource\nof freely accessible monolingual and parallel corpora\n(Tiedemann, 2012). W e collected the following French\nmonolingual datasets from OPUS.\n• OpenSubtitles comprises translated movies and TV\nsubtitles.\n• EU Bookshop includes publications from the Euro-\npean institutions.\n• MultiUN composes documents from the United Na-\ntions.\n• GIGA consists of newswire text and is made available\nin WMT10 shared task. 25\n• DGT contains translation memories provided by the\nJoint Research Center.\n• Global V oices encompasses news stories from the\nwebsite Global V oices.\n• TED T alksincludes subtitles from TED talks videos. 26\n• Euconst consists of text from the European constitu-\ntion.\n22 https://huggingface.co/transformers/\n23 http://www.statmt.org/wmt19/translation-task.html\n24 http://opus.nlpl.eu\n25 https://www.statmt.org/wmt10/\n26 https://www.ted.com\nWikimedia database This includes Wikipedia, Wik-\ntionary, Wikiversity, etc. The content is built collaboratively\nby volunteers around the world. 27\n• W ikipedia is a free online encyclopedia including\nhigh-quality text covering a wide range of topics.\n• W ikisourceincludes source texts in the public domain.\n• W ikinewscontains free-content news.\n• W iktionary is an open-source dictionary of words,\nphrases etc.\n• W ikiversitycomposes learning resources and learning\nprojects or research.\n• W ikibooksincludes open-content books.\n• W ikiquoteconsists of sourced quotations from notable\npeople and creative works.\n• W ikivoyageincludes information about travelling.\nProject Gutenberg This popular dataset contains free\nebooks of different genres which are mostly the world’s\nolder classic works of literature for which copyright has ex -\npired.\nEnronSent This dataset is provided by (Styler, 2011) and\nis a part of the Enron Email Dataset, 28 a massive dataset\ncontaining 500K messages from senior management exec-\nutives at the Enron Corporation.\nPCT This sub-corpus contains patent documents col-\nlected and maintained internally by the GET ALP 29 team.\nLe Monde This is also collected and maintained inter-\nnally by the GET ALP team, consisting of articles from Le\nMonde30 collected from 1987 to 2003.\nBibliographical References\nAbeill´ e, A., Cl´ ement, L., and T oussenel, F ., (2003). Build-\ning a T reebank for French , pages 165–187. Springer\nNetherlands, Dordrecht.\nAntoun, W ., Baly, F ., and Hajj, H. (2020). Arabert:\nTransformer-based model for arabic language under-\nstanding. arXiv preprint arXiv:2003.00104.\nArtetxe, M. and Schwenk, H. (2019). Massively multi-\nlingual sentence embeddings for zero-shot cross-lingual\ntransfer and beyond. T ransactions of the Association for\nComputational Linguistics, 7:597–610.\nBengio, Y ., Ducharme, R., V incent, P ., and Jauvin, C.\n(2003). A neural probabilistic language model. Journal\nof machine learning research, 3(Feb):1137–1155.\nBlitzer, J., McDonald, R., and Pereira, F . (2006). Domain\nadaptation with structural correspondence learning. In\nProceedings of the 2006 conference on empirical meth-\nods in natural language processing, pages 120–128. As-\nsociation for Computational Linguistics.\n27 https://dumps.wikimedia.org/other/cirrussearch/current/\n28 https://www.cs.cmu.edu/∼ enron/\n29 http://lig-getalp.imag.fr/en/home/\n30 https://www.lemonde.fr\nDataset Post-processed text size Number of T okens (Moses) N umber of Sentences\nCommonCrawl (Buck et al., 2014) 43.4 GB 7.85 B 293.37 M\nNewsCrawl (Li et al., 2019) 9.2 GB 1.69 B 63.05 M\nWikipedia (Meta, 2019) 4.2 GB 750.76 M 31.00 M\nWikisource (Meta, 2019) 2.4 GB 458.85 M 27.05 M\nEU Bookshop (Skadins et al., 2014) 2.3 GB 389.40 M 13.18 M\nMultiUN (Eisele and Chen, 2010) 2.3 GB 384.42 M 10.66 M\nGIGA (Tiedemann, 2012) 2.0 GB 353.33 M 10.65 M\nPCT 1.2 GB 197.48 M 7.13 M\nProject Gutenberg 1.1 GB 219.73 M 8.23 M\nOpenSubtitles (Lison and Tiedemann, 2016) 1.1 GB 218.85 M 13 .98 M\nLe Monde 664 MB 122.97 M 4.79 M\nDGT (Tiedemann, 2012) 311 MB 53.31 M 1.73 M\nEuroParl (Koehn, 2005) 292 MB 50.44 M 1.64 M\nEnronSent (Styler, 2011) 73 MB 13.72 M 662.31 K\nNewsCommentary (Li et al., 2019) 61 MB 13.40 M 341.29 K\nWiktionary (Meta, 2019) 52 MB 9.68 M 474.08 K\nGlobal V oices (Tiedemann, 2012) 44 MB 7.88 M 297.38 K\nWikinews (Meta, 2019) 21 MB 3.93 M 174.88 K\nTED T alks (Tiedemann, 2012) 15 MB 2.92 M 129.31 K\nWikiversity (Meta, 2019) 10 MB 1.70 M 64.60 K\nWikibooks (Meta, 2019) 9 MB 1.67 M 65.19 K\nWikiquote (Meta, 2019) 5 MB 866.22 K 42.27 K\nWikivoyage (Meta, 2019) 3 MB 500.64 K 23.36 K\nEUconst (Tiedemann, 2012) 889 KB 148.47 K 4.70 K\nT otal 71 GB 12.79 B 488.78 M\nT able 10: Statistics of sub-corpora after cleaning and pre- processing an initial corpus of 270 GB, ranked in the decreas ing\norder of post-processed text size.\nBuck, C., Heaﬁeld, K., and van Ooyen, B. (2014). N-gram\ncounts and language models from the common crawl. In\nProceedings of the Language Resources and Evaluation\nConference, Reykjavik, Iceland, May.\nCa ˜ nete, J., Chaperon, G., Fuentes, R., and P ˜A c⃝ rez, J.\n(2020). Spanish pre-trained bert model and evaluation\ndata. In to appear in PML4DC at ICLR 2020.\nChen, Q., Zhu, X., Ling, Z.-H., W ei, S., Jiang, H., and\nInkpen, D. (2017). Enhanced lstm for natural language\ninference. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (V olume\n1: Long P apers), pages 1657–1668.\nCollobert, R. and W eston, J. (2008). A uniﬁed architecture\nfor natural language processing: Deep neural networks\nwith multitask learning. In Proceedings of the 25th in-\nternational conference on Machine learning, pages 160–\n167. ACM.\nCollobert, R., W eston, J., Bottou, L., Karlen, M.,\nKavukcuoglu, K., and Kuksa, P . (2011). Natural lan-\nguage processing (almost) from scratch. Journal of ma-\nchine learning research, 12(Aug):2493–2537.\nConneau, A., Rinott, R., Lample, G., Williams, A., Bow-\nman, S., Schwenk, H., and Stoyanov, V . (2018). Xnli:\nEvaluating cross-lingual sentence representations. In\nProceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2475–2485.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V .,\nW enzek, G., Guzm ´ an, F ., Grave, E., Ott, M., Zettle-\nmoyer, L., and Stoyanov, V . (2019). Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nConstant, M., Candito, M., and Seddah, D. (2013). The\nligm-alpage architecture for the spmrl 2013 shared task:\nMultiword expression analysis and dependency parsing.\nIn Proceedings of the EMNLP W orkshop on Statistical\nP arsing of Morphologically Rich Languages (SPMRL\n2013).\nDai, A. M. and Le, Q. V . (2015). Semi-supervised se-\nquence learning. In Advances in neural information pro-\ncessing systems, pages 3079–3087.\nde Vries, W ., van Cranenburgh, A., Bisazza, A., Caselli, T .,\nvan Noord, G., and Nissim, M. (2019). Bertje: A dutch\nbert model. arXiv preprint arXiv:1912.09582.\nDelobelle, P ., Winters, T ., and Berendt, B. (2020). Rob-\nbert: a dutch roberta-based language model. arXiv\npreprint arXiv:2001.06286.\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K.\n(2019). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage T echnologies, V olume 1 (Long and Short P a-\npers), pages 4171–4186.\nDozat, T . and Manning, C. D. (2016). Deep biafﬁne atten-\ntion for neural dependency parsing. In ICLR.\nEisele, A. and Chen, Y . (2010). Multiun: A multilingual\ncorpus from united nation documents. In Proceedings of\nthe Seventh conference on International Language Re-\nsources and Evaluation (LREC’10) .\nEisenschlos, J., Ruder, S., Czapla, P ., Kardas, M., Gug-\nger, S., and Howard, J. (2019). Multiﬁt: Efﬁcient multi-\nlingual language model ﬁne-tuning. In Proceedings of\nthe 2019 conference on empirical methods in natural lan-\nguage processing (EMNLP), pages 1532–1543.\nFan, A., Grave, E., and Joulin, A. (2019). Reducing trans-\nformer depth on demand with structured dropout. In In-\nternational Conference on Learning Representations.\nGong, L., He, D., Li, Z., Qin, T ., W ang, L., and Liu, T .\n(2019). Efﬁcient training of bert by progressively stack-\ning. In International Conference on Machine Learning ,\npages 2337–2346.\nHadj Salah, M. (2018). Arabic word sense disambigua-\ntion for and by machine translation . Theses, Universit´ e\nGrenoble Alpes ; Universit´ e de Sfax (Tunisie). Facult´ e\ndes Sciences ´ economiques et de gestion, December.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-\nterm memory. Neural computation, 9(8):1735–1780.\nHoward, J. and Ruder, S. (2018). Universal language\nmodel ﬁne-tuning for text classiﬁcation. In Proceedings\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics (V olume 1: Long P apers), pages\n328–339.\nHuang, G., Sun, Y ., Liu, Z., Sedra, D., and W einberger,\nK. Q. (2016). Deep networks with stochastic depth. In\nEuropean conference on computer vision , pages 646–\n661. Springer.\nKingma, D. P . and Ba, J. (2014). Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKitaev, N. and Klein, D. (2018). Constituency parsing with\na self-attentive encoder. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational Lin-\nguistics (V olume 1: Long P apers) , pages 2676–2686,\nMelbourne, Australia, July. Association for Computa-\ntional Linguistics.\nKitaev, N., Cao, S., and Klein, D. (2019). Multilingual\nconstituency parsing with self-attention and pre-trainin g.\nIn Proceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3499–3505,\nFlorence, Italy, July. Association for Computational Lin-\nguistics.\nKoehn, P ., Hoang, H., Birch, A., Callison-Burch, C., Fed-\nerico, M., Bertoldi, N., Cowan, B., Shen, W ., Moran,\nC., Zens, R., et al. (2007). Moses: Open source toolkit\nfor statistical machine translation. In Proceedings of the\n45th annual meeting of the association for computational\nlinguistics companion volume proceedings of the demo\nand poster sessions, pages 177–180.\nKoehn, P . (2005). Europarl: A parallel corpus for statis-\ntical machine translation. Machine T ranslation Summit,\n2005, pages 79–86.\nKuratov, Y . and Arkhipov, M. (2019). Adaptation of deep\nbidirectional multilingual transformers for russian lan-\nguage. arXiv preprint arXiv:1905.07213.\nLample, G. and Conneau, A. (2019). Cross-lingual lan-\nguage model pretraining. In Advances in neural infor-\nmation processing systems.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,\nP ., and Soricut, R. (2019). Albert: A lite bert for self-\nsupervised learning of language representations. arXiv\npreprint arXiv:1909.11942.\nLi, X., Michel, P ., Anastasopoulos, A., Belinkov, Y ., Dur-\nrani, N., Firat, O., Koehn, P ., Neubig, G., Pino, J., and\nSajjad, H. (2019). Findings of the ﬁrst shared task on\nmachine translation robustness. WMT 2019, page 91.\nLison, P . and Tiedemann, J. (2016). Opensubtitles2015:\nExtracting large parallel corpora from movie and tv sub-\ntitles. In International Conference on Language Re-\nsources and Evaluation.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\n(2019). Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMartin, L., Muller, B., Ortiz Su ´ arez, P . J., Dupont, Y ., Ro-\nmary, L., V illemonte de la Clergerie, ´E., Seddah, D., and\nSagot, B. (2019). CamemBER T: a T asty French Lan-\nguage Model. arXiv e-prints , page arXiv:1911.03894,\nNov.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\n(2017). Learned in translation: Contextualized word\nvectors. In Advances in Neural Information Processing\nSystems, pages 6294–6305.\nMeta. (2019). Data dumps — meta, discussion about wiki-\nmedia projects.\nMikolov, T ., Sutskever, I., Chen, K., Corrado, G., and\nDean, J. (2013). Distributed representations of words\nand phrases and their compositionality. In Proceedings\nof the 26th International Conference on Neural Infor-\nmation Processing Systems - V olume 2, NIPS’13, pages\n3111–3119, USA. Curran Associates Inc.\nMiller, G. A., Leacock, C., T engi, R., and Bunker, R. T .\n(1993). A semantic concordance. In Proceedings of the\nworkshop on Human Language T echnology , HL T ’93,\npages 303–308, Stroudsburg, P A, USA. Association for\nComputational Linguistics.\nMiller, G. A. (1995). W ordnet: a lexical database for en-\nglish. Communications of the ACM, 38(11):39–41.\nNavigli, R. and Ponzetto, S. P . (2010). Babelnet: Build-\ning a very large multilingual semantic network. In Pro-\nceedings of the 48th annual meeting of the association\nfor computational linguistics , pages 216–225. Associa-\ntion for Computational Linguistics.\nNavigli, R., Jurgens, D., and V annella, D. (2013).\nSemEval-2013 T ask 12: Multilingual W ord Sense Dis-\nambiguation. In Second Joint Conference on Lexical and\nComputational Semantics (*SEM), V olume 2: Proceed-\nings of the Seventh International W orkshop on Semantic\nEvaluation (SemEval 2013), pages 222–231.\nNavigli, R. (2009). W ord sense disambiguation: A survey.\nACM Computing Surveys, 41(2):10:1–10:69, feb.\nNguyen, D. Q. and Nguyen, A. T . (2020). Phobert: Pre-\ntrained language models for vietnamese. arXiv preprint\narXiv:2003.00744 .\nNguyen, T . Q. and Salazar, J. (2019). Transformers with-\nout tears: Improving the normalization of self-attention.\narXiv preprint arXiv:1910.05895.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N.,\nGrangier, D., and Auli, M. (2019). fairseq: A fast, ex-\ntensible toolkit for sequence modeling. In Proceedings\nof NAACL-HLT 2019: Demonstrations.\nPennington, J., Socher, R., and Manning, C. D. (2014).\nGlove: Global vectors for word representation. In In\nEMNLP.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. (2018). Deep contextu-\nalized word representations. In Proceedings of NAACL-\nHLT, pages 2227–2237.\nPham, N.-Q., Nguyen, T .-S., Niehues, J., Muller, M.,\nand W aibel, A. (2019). V ery deep self-attention net-\nworks for end-to-end speech recognition. arXiv preprint\narXiv:1904.13377 .\nPolignano, M., Basile, P ., de Gemmis, M., Semeraro, G.,\nand Basile, V . (2019). AlBER T o: Italian BER T Lan-\nguage Understanding Model for NLP Challenging T asks\nBased on T weets. In Proceedings of the Sixth Ital-\nian Conference on Computational Linguistics (CLiC-it\n2019), volume 2481. CEUR.\nPrettenhofer, P . and Stein, B. (2010). Cross-language text\nclassiﬁcation using structural correspondence learning.\nIn Proceedings of the 48th annual meeting of the asso-\nciation for computational linguistics, pages 1118–1127.\nRadford, A., Narasimhan, K., Salimans, T ., and Sutskever,\nI. (2018). Improving language understanding by gener-\native pre-training. T echnical report, OpenAI.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W ., and Liu, P . J. (2019). Ex-\nploring the limits of transfer learning with a uniﬁed text-\nto-text transformer. arXiv preprint arXiv:1910.10683.\nRajpurkar, P ., Jia, R., and Liang, P . (2018). Know what you\ndon’t know: Unanswerable questions for squad. arXiv\npreprint arXiv:1806.03822.\nRamachandran, P ., Liu, P ., and Le, Q. (2017). Unsuper-\nvised pretraining for sequence to sequence learning. In\nProceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 383–391.\nSeddah, D., Tsarfaty, R., K ¨ ubler, S., Candito, M., Choi,\nJ. D., Farkas, R., Foster, J., Goenaga, I., Gojenola Gal-\nletebeitia, K., Goldberg, Y ., Green, S., Habash, N.,\nKuhlmann, M., Maier, W ., Nivre, J., Przepi ´ orkowski, A.,\nRoth, R., Seeker, W ., V ersley, Y ., V incze, V ., W oli ´ nski,\nM., Wr ´ oblewska, A., and V illemonte de la Clergerie,\nE. (2013). Overview of the SPMRL 2013 shared task:\nA cross-framework evaluation of parsing morpholog-\nically rich languages. In Proceedings of the F ourth\nW orkshop on Statistical P arsing of Morphologically-\nRich Languages , pages 146–182, Seattle, W ashington,\nUSA, October. Association for Computational Linguis-\ntics.\nSegonne, V ., Candito, M., and Crabb ´ e, B. (2019). Using\nwiktionary as a resource for wsd: the case of french\nverbs. In Proceedings of the 13th International Confer-\nence on Computational Semantics-Long P apers , pages\n259–270.\nSennrich, R., Haddow , B., and Birch, A. (2016). Neural\nmachine translation of rare words with subword units.\nIn Proceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics (V olume 1: Long\nP apers), pages 1715–1725.\nS´ erasset, G. (2012). Dbnary: Wiktionary as a lmf based\nmultilingual rdf network. In Language Resources and\nEvaluation Conference, LREC 2012.\nSkadins, R., Tiedemann, J., Rozis, R., and Deksne, D.\n(2014). Billions of parallel words for free: Building and\nusing the EU bookshop corpus. In Proceedings of the\nNinth International Conference on Language Resources\nand Evaluation, LREC 2014, Reykjavik, Iceland, May\n26-31, 2014, pages 1850–1855.\nSouza, F ., Nogueira, R., and Lotufo, R. (2019). Portuguese\nnamed entity recognition using bert-crf. arXiv preprint\narXiv:1909.10649 .\nStyler, W . (2011). The enronsent corpus. T echnical Report\n01-2011. University of Colorado at Boulder Institute of\nCognitive Science, Boulder , CO.\nSutskever, I., V inyals, O., and Le, Q. V . (2014). Sequence\nto sequence learning with neural networks. In Advances\nin neural information processing systems , pages 3104–\n3112.\nTiedemann, J. (2012). Parallel data, tools and interfaces\nin opus. In Nicoletta Calzolari (Conference Chair),\net al., editors, Proceedings of the Eight International\nConference on Language Resources and Evaluation\n(LREC’12) , Istanbul, Turkey, may. European Language\nResources Association (ELRA).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).\nAttention is all you need. In Advances in neural infor-\nmation processing systems, pages 5998–6008.\nV aswani, A., Bengio, S., Brevdo, E., Chollet, F ., Gomez,\nA., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N.,\nParmar, N., et al. (2018). T ensor2tensor for neural ma-\nchine translation. In Proceedings of the 13th Conference\nof the Association for Machine T ranslation in the Amer-\nicas (V olume 1: Research P apers), pages 193–199.\nV ial, L., Lecouteux, B., and Schwab, D. (2018). UF-\nSAC: Uniﬁcation of Sense Annotated Corpora and\nT ools. In Language Resources and Evaluation Confer-\nence (LREC), Miyazaki, Japan, May.\nV ial, L., Lecouteux, B., and Schwab, D. (2019). Sense\nV ocabulary Compression through the Semantic Knowl-\nedge of W ordNet for Neural W ord Sense Disambigua-\ntion. In Proceedings of the 10th Global W ordnet Confer-\nence, Wroclaw , Poland.\nV irtanen, A., Kanerva, J., Ilo, R., Luoma, J., Luotolahti, J .,\nSalakoski, T ., Ginter, F ., and Pyysalo, S. (2019). Mul-\ntilingual is not enough: Bert for ﬁnnish. arXiv preprint\narXiv:1912.07076 .\nW ang, A., Singh, A., Michael, J., Hill, F ., Levy, O.,\nand Bowman, S. (2018). GLUE: A multi-task bench-\nmark and analysis platform for natural language under-\nstanding. In Proceedings of the 2018 EMNLP W ork-\nshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 353–355, Brussels, Belgium,\nNovember. Association for Computational Linguistics.\nW ang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F ., Levy, O., and Bowman, S. R.\n(2019a). Superglue: A stickier benchmark for general-\npurpose language understanding systems. arXiv preprint\narXiv:1905.00537 .\nW ang, Q., Li, B., Xiao, T ., Zhu, J., Li, C., W ong, D. F ., and\nChao, L. S. (2019b). Learning deep transformer models\nfor machine translation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational Lin-\nguistics, pages 1810–1822.\nWilliams, A., Nangia, N., and Bowman, S. (2018). A\nbroad-coverage challenge corpus for sentence under-\nstanding through inference. In Proceedings of the 2018\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long P apers), pages 1112–\n1122.\nW olf, T ., Debut, L., Sanh, V ., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P ., Rault, T ., Louf, R., Funtow-\nicz, M., and Brew , J. (2019). Huggingface’s transform-\ners: State-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nXu, H., Liu, Q., van Genabith, J., and Zhang, J. (2019).\nWhy deep transformers are difﬁcult to converge? from\ncomputation order to lipschitz restricted parameter ini-\ntialization. arXiv preprint arXiv:1911.03179.\nY ang, Y ., Zhang, Y ., T ar, C., and Baldridge, J. (2019a).\nPaws-x: A cross-lingual adversarial dataset for para-\nphrase identiﬁcation. arXiv preprint arXiv:1908.11828.\nY ang, Z., Dai, Z., Y ang, Y ., Carbonell, J., Salakhutdinov,\nR., and Le, Q. V . (2019b). Xlnet: Generalized autore-\ngressive pretraining for language understanding. In Ad-\nvances in neural information processing systems.\nZhang, H., Dauphin, Y . N., and Ma, T . (2019a). Fixup\ninitialization: Residual learning without normalization .\narXiv preprint arXiv:1901.09321.\nZhang, Y ., Baldridge, J., and He, L. (2019b). Paws: Para-\nphrase adversaries from word scrambling. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language T echnologies, V olume 1 (Long\nand Short P apers), pages 1298–1308.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7827115058898926
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5932539701461792
    },
    {
      "name": "Natural language processing",
      "score": 0.5771355628967285
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5659105181694031
    },
    {
      "name": "Language model",
      "score": 0.4380275011062622
    },
    {
      "name": "Geography",
      "score": 0.04123157262802124
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210114212",
      "name": "Laboratoire de Linguistique Formelle",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210115485",
      "name": "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur",
      "country": "FR"
    }
  ],
  "cited_by": 61
}