{
  "title": "Language Model Augmented Relevance Score",
  "url": "https://openalex.org/W3166624321",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5023339675",
      "name": "Ruibo Liu",
      "affiliations": [
        "Dartmouth Hospital",
        "Dartmouth College"
      ]
    },
    {
      "id": "https://openalex.org/A5078446912",
      "name": "Jason Wei",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5035399743",
      "name": "Soroush Vosoughi",
      "affiliations": [
        "Dartmouth Hospital",
        "Dartmouth College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2140054881",
    "https://openalex.org/W3037142412",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953287808",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2947771965",
    "https://openalex.org/W2962979564",
    "https://openalex.org/W3103753836",
    "https://openalex.org/W3024131638",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W2964178377",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3103450644",
    "https://openalex.org/W2896780650",
    "https://openalex.org/W2970561469",
    "https://openalex.org/W2968888988",
    "https://openalex.org/W3095901788",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W3094040156",
    "https://openalex.org/W3099766584",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W2892280852",
    "https://openalex.org/W3100258764",
    "https://openalex.org/W2979702391",
    "https://openalex.org/W1854214752",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2925000324",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W2945760033",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3098757391",
    "https://openalex.org/W3119775721",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2963963993",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3158733382",
    "https://openalex.org/W2963674921",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3085932930",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2951813108",
    "https://openalex.org/W2947120937",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W4394651747",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1776713878",
    "https://openalex.org/W2970986500",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2888812214",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2512848817",
    "https://openalex.org/W3173898364",
    "https://openalex.org/W2584220694",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3099872554",
    "https://openalex.org/W2952523122",
    "https://openalex.org/W2963466651",
    "https://openalex.org/W2108325777",
    "https://openalex.org/W2964042428"
  ],
  "abstract": "Although automated metrics are commonly used to evaluate NLG systems, they\\noften correlate poorly with human judgements. Newer metrics such as BERTScore\\nhave addressed many weaknesses in prior metrics such as BLEU and ROUGE, which\\nrely on n-gram matching. These newer methods, however, are still limited in\\nthat they do not consider the generation context, so they cannot properly\\nreward generated text that is correct but deviates from the given reference.\\n In this paper, we propose Language Model Augmented Relevance Score (MARS), a\\nnew context-aware metric for NLG evaluation. MARS leverages off-the-shelf\\nlanguage models, guided by reinforcement learning, to create augmented\\nreferences that consider both the generation context and available human\\nreferences, which are then used as additional references to score generated\\ntext. Compared with seven existing metrics in three common NLG tasks, MARS not\\nonly achieves higher correlation with human reference judgements, but also\\ndifferentiates well-formed candidates from adversarial samples to a larger\\ndegree.\\n",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 6677–6690\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n6677\nLanguage Model Augmented Relevance Score\nRuibo Liu1 Jason Wei2 Soroush Vosoughi1\n1Dartmouth College 2Google AI Language\nruibo.liu.gr@dartmouth.edu jasonwei@google.com\nsoroush.vosoughi@dartmouth.edu\nAbstract\nAlthough automated metrics are commonly\nused to evaluate NLG systems, they often cor-\nrelate poorly with human judgements. Newer\nmetrics such as BERTScore have addressed\nmany weaknesses in prior metrics such as\nBLEU and ROUGE, which rely on =-gram\nmatching. These newer methods, however, are\nstill limited in that they do not consider the\ngeneration context, so they cannot properly re-\nward generated text that is correct but deviates\nfrom the given reference.\nIn this paper, we propose Language M odel\nAugmented Relevance Score (MARS), a new\ncontext-aware metric for NLG evaluation.\nMARS leverages off-the-shelf language mod-\nels, guided by reinforcement learning, to cre-\nate augmented references that consider both\nthe generation context and available human\nreferences, which are then used as additional\nreferences to score generated text. Compared\nwith seven existing metrics in three common\nNLG tasks, MARS not only achieves higher\ncorrelation with human reference judgements,\nbut also differentiates well-formed candidates\nfrom adversarial samples to a larger degree.\n1 Introduction\nAutomated metrics such as BLEU (Papineni et al.,\n2002) and ROUGE (Lin, 2004) are popular meth-\nods for evaluating natural language generation\n(NLG) systems. Compared with human evalua-\ntion, they are cheaper and faster, and accordingly,\nthey often serve as essential metrics for benchmark-\ning the performance of NLG models (Novikova\net al., 2017). Despite their widespread use, how-\never, these automated metrics often poorly correlate\nwith ratings given by human judges, particularly\nfor datasets in which only a single human reference\nexists (Gupta et al., 2019; Novikova et al., 2017).\nMoreover, these automated metrics only capture\nContext\nExsiting Metrics: PPL /BLEU / ROUGE / BERT-Score / etc.\nOurs: MARS\nHuman Reference Candidate\nLM\nLM Augmented References\nFigure 1: Existing metrics compare the candidate with\nthe human reference but ignore context. MARS (our\nmethod) augments the human reference while consider-\ning the context, which allows it to provide evaluation\nscores that correlate highly with human references.\nsimilarities between generated sentences and refer-\nence candidates, crucially ignoring provided con-\ntexts that are relevant for evaluating the answer in\ncontextual NLG tasks, such as story generation,\nnews summarization, and question-answering (Tao\net al., 2018; Nema and Khapra, 2018).\nTable 1 shows a story generation 1 example\nthat exempliﬁes some weaknesses of several com-\nmon metrics. Perplexity (PPL) (Brown et al.,\n1992) successfully detects ungrammatical sen-\ntences, but it fails to distinguish legitimate novel\ncontinuations and copy-and-pasted ones. Rely-\ning on surface-level =-gram matching, BLEU-1\nand ROUGE-L2 cannot detect reordering effec-\ntively, and wrongly score the well-formed candi-\ndate lower than its retrieval-based adversarial ex-\nample. BERTScore (Zhang et al., 2019) leverages\ncontextual embeddings from BERT (Devlin et al.,\n2019), thus mitigating the above challenges, but\nstill does not fairly evaluate candidates that cor-\nrectly align with the context but happen to differ\n1The ROC story generation task asks systems to generate\na legitimate ending for a four-sentence story.\n2L stands for longest common sequence matching.\n6678\nContext.Wendy was driving down the road. She heard her car making a noise. She pulled over to examine the problem.\nThere was nothing but oil all on the road from her car.\nHuman Reference.She called for help and waited to get her car ﬁxed.PPL BLEU-1 ROUGE-L BERTScore MARS\nCandidate.Her fears were conﬁrmed when her engine was smoking.75.58 0.223 0.182 0.338 0.574\nReorder.her conﬁrmed engine fears Her when was were smoking. 405.600.223 0.182 0.265 0.352\nRetrieve.She heard her car making a noise. 63.93 0.337 0.400 0.406 0.448\nTable 1: In this story generation example, MARS is the only metric that gives the well-formed candidate a higher\nscore than two adversarial examples. The human rating of the candidate averaged over 20 judgements is 5.05 out of\n6.00. Two adversarial examples are generated by Reordering the tokens of the candidate (as weak NLG systems\nwhose generation is not readable) and Retrieveing a sentence from the context (as systems with no generation\nability). We boxed the cases where the adversarial example does not score lower than the well-formed candidate.\nfrom the provided reference example. In our exam-\nple, the candidate “... her engine was smoking” is\nreasonable but deviates from the human reference,\nand so BERTScore rates it relatively low (0.338 out\nof 1.0), thus correlating poorly with human rating,\nwhich was high (5.05 out of 6.00).\nTo address the above issues, prior studies have\nproposed a number of promising remedies. One\nline of work has proposed to combine human rat-\nings with automated metrics (Durmus et al., 2020;\nChaganty et al., 2018, inter alia). For instance, in\nHUSE score, Hashimoto et al. (2019) leverages the\ndifferences between perplexity and human judge-\nments to consider both quality and diversity of\ngenerated text. Another line has proposed train-\ning separate neural models to aid automated met-\nrics (Mehri and Eskenazi, 2020; Yuma et al., 2020,\ninter alia). For instance, BLEURT (Sellam et al.,\n2020) ﬁne-tunes BERT (Devlin et al., 2019) on syn-\nthetic reference-candidate pairs for machine trans-\nlation. These methods, however, are often limited\nin practical use, because the high-cost human rat-\nings are not always available for every dataset, and\nthe data- or system-speciﬁc training is not easily\nextended to other domains (Zhang et al., 2019), and\ncan even bias the evaluation (Freitag et al., 2020b).\nIn this paper, we present MARS (Language\nModel Augmented Relevance Score), a new NLG\nevaluation metric that requires neither supervision\nfrom human ratings nor additional training on spe-\nciﬁc domains. As shown in Figure 1, instead of\ncomparing candidates only with human written ref-\nerences, as many prior metrics do, MARS uses a\nmixture of both human and augmented references.\nSpeciﬁcally, MARS masks tokens in the reference\nto create templates, and then uses the context and\ntemplates to generate augmented references by in-\nﬁlling the masked parts with an LM guided by rein-\nforcement learning. The augmented references thus\nincorporate information from both the context and\nthe human reference, and are enriched with lexical\nand syntactic diversity, facilitating fairer evaluation\nof candidates. Finally, we compute the score as\na weighted average of the similarity between the\ncandidate and the set of augmented references in\nthe contextual embedding space.\nThe advantages of MARS are three-fold. First,\nMARS correlates highly with human judgements.\nWe apply MARS to three diverse NLG tasks, and\ndemonstrate that, compared with seven popular\nNLG metrics, MARS better correlates with hu-\nman judgements and is robust against adversarial\nattacks. Second, MARS is context-aware. Un-\nlike existing metrics that only consider the given\nhuman reference, we use a constrained NLG ap-\nproach to incorporate the generation context into\naugmented references, thus alleviating bias against\ndiverse candidates. Third, MARS is easy to deploy\nand extend. Built on off-the-shelf LMs, MARS\nrequires neither human supervision nor additional\ntraining for speciﬁc domains, and can therefore\nserve as a general-purpose metric for a broad range\nof NLG applications, as we will demonstrate for\nthree common NLG tasks: story generation, news\nsummarization, and question-answering.\n2 Approach\nMARS comprises three steps. First, we mask out\nnon-important tokens from the human reference to\nproduce templates for augmentation ( §2.1). Sec-\nond, we guide off-the-shelf LMs to generate refer-\nence augmentation on these templates via a rein-\nforced self-planning algorithm (§2.2). Finally, we\ncompute a weighted average score that reﬂects the\noverall similarity between the candidate and the set\nof augmented references (§2.3).\n6679\n2.1 Human Reference Token Masking\nThe ﬁrst step in MARS is to take in the given hu-\nman reference and generate templates—masked\nversions of the human reference—which can then\nbe used to generate augmented references. Our\nmasking procedure can be viewed as a reversed\nprocess of prior insertion- and template-based gen-\neration approaches (Zhang et al., 2020; Miao et al.,\n2019); whereas these generation approaches start\nwith templates of important tokens and then ﬁll\nin the details to generate complete sentences, our\nmasking procedure starts with the complete sen-\ntence (i.e., the human reference) and then masks\nout unimportant tokens to generate templates. To\nbetter explain our masking procedure, we introduce\ntwo concepts, mask priority and mask cost:\nMask Priority. We compute a mask priority E8\nfor each token G8, which captures the priority of\nmasking G8, where non-important words should\nreceive higher priority. We compute E8 as a func-\ntion of two things: the inverse document frequency\n(IDF) of G8, and the part-of-speech (POS) of G8:\nE8 = U(POS [G8])\nIDF(G8,-) , (1)\nwhere Uis a function that assigns a weight to each\nPOS tag.3 Common tokens across the corpus -\n(e.g., stop words, with low IDF) will receive high\nmask priority. Tokens responsible for description\ndetails will also be assigned high mask priority\nbased on their part-of-speech (e.g., adjectives are\nmainly used for details and so they are given higher\npriority of being masked).\nMask Cost. For each token G8, we also compute\na mask cost F8. Tokens that appear in both con-\ntext and human reference should have high mask-\ning cost as they are deemed context-carrying. We\nuse the longest common sequence (LCS) match-\ning between the context and the human reference\nto identify these context-carrying tokens. In our\nexperiments, we set the F8 of these tokens to 10\nand the default F8 of all other tokens to 1. We use\n_to denote the ratio of tokens to be masked in a\nsentence of # tokens, and deﬁne ,max =_·# as\nthe maximum cost allowed.\n3Uvaries for each task. Empirically, we ﬁnd that it works\nwell to assign adjectives, adverbs, and nouns higher weights\nthan other parts-of-speech. For our setting, we assign weights\nof 4, 3, 2 to the above three types.\nDP-based Token Masking. Now that for each\ntoken we have a mask priority and a mask cost,\nwe aim to choose a set of tokens to mask with the\nhighest possible sum of priorities for which the\nsum of mask costs is not greater than ,max. Given\na function q(G8)={1,0}where 1 means token G8\nis masked and 0 means it remains, the objective of\ntoken masking can be expressed as follows:\nmax\n#Õ\n8=1\nE8 ·q(G8),\ns.t.\n#Õ\n8=1\nF8 ·q(G8)≤ ,max .\n(2)\nSuch a goal is actually a NP-complete combina-\ntorial optimization problem, called the Knapsack\nproblem (Pisinger, 1995), which we solve using\ndynamic-programming (DP). In general, the mask-\ning strategy aggressively harvests tokens of high\nmask priority while keeping the cost of masked to-\nkens from exceeding the mask cost limitation,max.\nThe detailed DP algorithm for solving this problem\nis shown in Appendix A.\n2.2 Self-planning Cloze Augmentation\nAfter creating the templates described in §2.1, we\nproduce augmented reference examples based on\nboth the templates as well as the generation context.\nThis procedure can be seen as a mixture of hard-\nand soft-constrained NLG, where the template to-\nkens pre-exist with some blanks, and the system,\nconditioned on the context, aims to ﬁll in the blanks.\nWe henceforth refer this process of creating aug-\nmented references as cloze4 augmentation.\nBackground. Masked Language Models (MLM)\nsuch as RoBERTa (Liu et al., 2019) and BERT (De-\nvlin et al., 2019) are trained to predict masked\ntokens within sentences, and thus are able to do\ncloze augmentation off-the-shelf. However, with-\nout architecture-level modiﬁcation, MLMs are only\nable to inﬁll a pre-determined number of missing\ntokens (Zhu et al., 2019). This is especially prob-\nlematic since—if they are directly used to augment\nreferences—all the augmented references will have\nthe same number of tokens as that of the original\nhuman reference. We believe this unnecessarily\nconstrains augmentation diversity, and thus con-\nsider it as a Naive method in our evaluations (§4).\n4A cloze test (Taylor, 1953) is a language test where a\nportion of language is removed and the participant is asked to\nreplace the missing language item.\n6680\nI really like the show performed at the Theatre!\nI enjoy every minute of the show at the Theatre!\nI [blk] [blk] the show [blk] [blk] the Theatre!\n(a) Naive Cloze Augmentation: Masked LM \n(b) Self-planning Cloze Augmentation: Autoregressive LM \nI enjoy the show only performed at the Theatre!\nContext\nContext\nBi-dir ectional A ttention\nUni-dir ectional A ttention Reinfor ced Self-planning\n+\n+\nI [blk] [blk] the show [blk] [blk] the Theatre!\nFigure 2: Compared with the Naive method, our rein-\nforced self-planning approach inﬁlls blanks with ([blk])\nvarying-length tokens while considering both past and\nfuture tokens, which promote diversity and coherence\nrespectively. The context is concatenated to the begin-\nning of the reference template.\nAutoregressive Language Models (ALM) such\nas GPT-2 (Radford et al., 2019), on the other hand,\nare trained to predict current step token given past\ntokens. They can generate sequences of varying\nlengths, but they cannot inﬁll missing tokens within\nsentences effectively since they do not consider\nfuture context. To enable ALMs to inﬁll blanks\nof unspeciﬁed length, prior work has proposed\neither retraining a new LM from scratch (Shen\net al., 2020) or ﬁne-tuning on specially prepared\ndata (Donahue et al., 2020), which are costly and\nnot easy to extend to new NLG tasks. As shown\nin Figure 2, we take a reinforcement learning (RL)\napproach that uses future words after the blank to\nguide current step inﬁlling generation. Since such\nRL guidance only relies on the tokens within its\nown to-be-inﬁlled template, we call it reinforced\nself-planning. Our method combines the advan-\ntages of both MLMs and ALMs, requiring neither\nre-training nor collecting new data, and thus is eas-\nier to extend to other off-the-shelf LMs.\nReinforced Self-planning. At each decoding\nstep during generation, a vanilla ALM will pick\nthe token GC that has the highest probability by\napplying an argmax over the softmax output of hid-\nden states. We add a self-planning stage between\nthe argmax and softmax function. Following the\nRL framework, we deﬁne the state at step Cas the\ngenerated sequences before C (i.e., BC =G<C ), and\nthe action at step C as the C-th output token (i.e.,\n0C = GC). We take the softmax output of the last\nhidden states (with parameter \\) as the policy c\\ ,\nsince it is the probability of picking tokenGC (action\n0C) given the state BC =G<C . Similarly, we denote\nthe policy after reinforced self-planning as c\\3 .\nTypically, the RL objective is to maximize the\nexpectation of total reward \u001f, summed over )steps\non the trajectory ginduced by c\\ :\n\u001f(\\)=Eg∼c\\\n[ )Õ\nC=0\nWCAC\n]\n, (3)\nwhere W ∈(0,1]is the discounting factor, and Ais\nthe single-step reward. In text generation, however,\nsuch a reward deﬁnition requires sampling over the\nfuture generated sequence to estimate current step\nreward (Gong et al., 2019), which may cause the\npolicy to end in zero reward region because of high\nvariance of the gradient (Pang and He, 2021). Since\nwe guide the generation in every step of decoding,\nwe derive the C-th step policy gradient ▽\\ \u001fC (\\)as:\nEC\ng∼c\\\n[\nnC▽\\ log c\\ (0C |BC)·A(G3\nC )\n]\n, (4)\nwith importance sampling weight nC to stabilize the\noptimization (Munos et al., 2016), which is:\nnC = c\\3 (0C |BC)\nc\\ (0C |BC) .\nIf we denote a certain token in future context\nas F ∈{Ffuture}, single-step self-planning reward\nA(G3\nC )can be approximated by the cosine similarity\nbetween C-th step hidden state and the embedded\nvector of Fby the LM embedding layers, which is\nA(G3\nC )=\nÕ\nF ∈Ffuture\nlog(softmax(ℎ\\3\n<C )· emb(F)).\n(5)\nGiven all above deﬁnitions, at C-th step, we up-\ndate c\\ towards the self-planned c\\3 as:\n\\3 ←\\+[\n:Õ\n8=1\n▽\\ \u001fC (\\3/b)\n∥▽\\ \u001fC (\\3/b)∥, (6)\nwhere [is the learning rate andbis the temperature\nparameter to control the stochastic sampling dur-\ning token decoding (Keskar et al., 2019). After :\niterations of reinforced self-planning, the updated\npolicy c\\3 should produce tokens approaching the\nfuture context in embedding space, since future\ncontext contributes to the calculation of reward A\n(Eq. 5).5 More details about how we handle edge\ncases during reinforced self-planning are presented\nin Appendix B.\n5In our setting, [, band :are 0.02, 1.3, and 3 respectively.\n6681\n2.3 Computing Contextual Similarity\nAfter generating augmented reference sentences,\nthe ﬁnal MARS score is computed as a weighted\naverage of the similarity between the candidate and\neach reference in the augmentation set (including\nthe original human reference). One way to ob-\ntain similarity scores is using BERTScore (Zhang\net al., 2019), but BERTScore requires training on\nexternal resources to make its outputs more read-\nable. Therefore, in order to keep all the resources\nused by MARS off-the-shelf, we utilize Sentence-\nBERT (Reimers and Gurevych, 2019), which uses\nthe mean of all token embeddings in a sentence as\nthe overall sentence-level encoding. As the sen-\ntence encoder, we use RoBERTa-large (Liu et al.,\n2019), a common choice in the literature (Zhang\net al., 2019; Reimers and Gurevych, 2020). As\nshown in Eq. 7, we then compute MARS score as\nthe average of the cosine similarities weighted us-\ning a geometric progression with a common ratio\n@ ∈(0,1]and a scale factor (start value) 0≠0:\nMARS =\n#_Õ\n8=1\n0@8−1 candT ·ref8−1\n∥cand∥T ∥ref8−1 ∥\ns.t.\n#_Õ\n8=1\n0@8−1 =1 ,\n(7)\nwhere the candidate encoding is cand, the reference\nencodings are ref8 (8is the index of the augmented\nreference under a certain_, and ref0 marks the zero-\nmask human reference), and #_is the number of\nmasking ratios we use in §2.1. Different @values,\nas deﬁned by the geometric progression, determine\nhow much weight each reference contributes. By\ndefault, Eq. 7 assigns the largest weight to the hu-\nman reference since it is the gold standard.\n3 Tasks & Datasets\nWe evaluated MARS and compared it with several\npopular NLG metrics on the following three tasks:\nStory Generation. We use the ROC stories\ndataset6 for story generation, which requires candi-\ndate NLG systems to generate coherent endings to\nfour-sentence stories (Mostafazadeh et al., 2016).\nThe dataset consists of 96,198 examples of par-\ntially written stories; we take the human-rated\nsubset ( #=300) released by HUSE (Hashimoto\net al., 2019), which contains continuances by (1)\n6https://cs.rochester.edu/nlp/rocstories/\nAvg.\n|Cntx.|\nAvg.\n|H Ref.| Ω # data\n(# HR / data) U\nROC 34.38 8.37 4.1 300 (20) 0.64\nNewsroom772.21 34.70 22.3 540 (3) 0.71\nMOCHA 161.92 4.69 34.5 450 (5) 0.82\nTable 2: Statistics of the three datasets with human rat-\nings used in this work. Avg. |Cntx.|and |H Ref.|: the\naveraged number of tokens in contexts and human ref-\nerences. Ω: the ratio of the previous two terms (lower\nΩ can indicate a more open-ended task). # HR: the\nnumber of Human Ratings. U: Krippendorff’s alpha\ncoefﬁcient to measure inter-annotator agreement.\nan industry-level system based on Apache Solr7,\nand (2) an Open-NMT model with global atten-\ntion (McCann et al., 2017).\nNews Summarization. For the news summariza-\ntion task, we use the Newsroom summary dataset.8\nThis dataset contains 1.3 million articles from 38\nmajor publications (Grusky et al., 2018) and we use\nthe subset with human ratings (#=540) released by\nthe authors.9 This dataset contains outputs from\nsummarization models: (1) TextRank: a sentence-\nlevel summarization system inspired by Google\nPageRank (Page et al., 1999), (2) aSeq2Seq model\nwith attention (Rush et al., 2015), and (3) Pointer-\nN: a pointer-based neural model (See et al., 2017)\ntrained on Newsroom dataset.\nQuestion Answering. For question answering,\nwe use the MOCHA dataset, 10 which includes\nhuman ratings on outputs of ﬁve models trained\non six QA datasets (Chen et al., 2020). We con-\nsider a distributionally-balanced subset ( #=450)\nof these outputs from three systems: (1) ﬁne-\ntuned GPT-2 (Radford et al., 2019), (2) a Back-\nTranslation model (Sennrich et al., 2016), and (3)\na MHPG model (Bauer et al., 2018) trained on Nar-\nrativeQA (Koˇcisk`y et al., 2018) and MCScript (Os-\ntermann et al., 2018) datasets.\nThe detailed statistics of these three datasets\nwe used for this work are shown in Table 2. For\npre-processing, we removed hashtags and urls in\nthe text, but leave punctuation and stop words,\nwhich can affect LCS matching when computing\nmask costs. For all tasks, we use GPT-2 (large,\nwith 774M parameters) as the language model for\n7https://lucene.apache.org/solr\n8http://lil.nlp.cornell.edu/newsroom/\n9The subset includes human ratings on four perspectives:\ncoherence, ﬂuency, informative and relevance. We compute\nthe average of the four scores as an overall human rating.\n10https://allennlp.org/mocha\n6682\nROC Story Generation\nΩ= 4.1\nNewsroom Summarization\nΩ= 22.7\nMOCHA Question Answering\nΩ= 34.5\nExisting Metrics Solr Open-NMT TextRank Seq2Seq Pointer-N GPT-2 Back-Tran MHPG\nBLEU-1 0.198 0.104 0.224 0.268 0.115 0.328 0.061 0.318\nMETEOR 0.180 0.116 0.288 0.235 0.256 0.466 0.179 0.409\nROUGE-L 0.118 0.195 0.041 -0.133 0.065 0.468 0.056 0.247\nSent. Mover Sim. 0.020 0.015 0.112 0.099 0.177 0.510 0.166 0.610\nMoverScore 0.181 0.391 0.075 0.337 0.212 0.535 0.190 0.592\nBERTScore 0.245 0.386 0.154 0.302 0.181 0.444 0.274 0.458\nPerplexity -0.104 -0.073 -0.385 0.011 -0.035 0.014 -0.051 -0.128\nMARS(default) 0.476 0.397 0.372 0.336 0.329 0.526 0.644 0.741\n- w/o. self-plan. 0.313 0.212 0.290 0.245 0.314 0.477 0.631 0.709\n- w/o. context+ 0.360 0.334 0.107 0.160 -0.009 0.134 0.222 0.303\n- w/o. both 0.276 0.183 -0.163 0.149 -0.057 -0.092 0.121 0.299\nNaive (MLM) 0.449 0.197 0.201 0.324 0.114 0.443 0.307 0.540\nTable 3: Pearson’s A correlations with human judgements for MARS and seven existing metrics across system\noutputs for three generation tasks. BLEU-1 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and\nROUGE-L (Lin and Och, 2004) use=-gram matching. Sentence Mover’s Similarity (Clark et al., 2019) and Mover-\nScore (Zhao et al., 2019) measure similarity using earth mover’s distance. BERTScore (Zhang et al., 2019) lever-\nages contextual embeddings from pre-trained LMs. As an ablation, we remove self-planning guidance, context,\nand both. Naive uses RoBERTa-large for reference augmentation (see§2.2). Ωis deﬁned as in Table 2.\nMARS, and RoBERTa-large for the Naive method.\nFor the newsroom dataset, some news articles were\nlonger than the max sequence length of 1024 BPE,\nand so we cut off the tail end of these examples.\nWith a single RTX-2080 GPU, cloze augmentation\nwith _= {0 (human ref.), 20%, 40%, 60%, 80%}\ntakes 0.8 seconds on average per reference, amount-\ning to a total augmentation time of 17, 45, and 32\nminutes for the ROC, Newsroom and MOCHA\ntasks respectively. We show how we pick the mask-\ning ratios for different tasks in §4.3.\n4 Evaluation\n4.1 MARS Better Correlates With Humans\nAs automated metrics are only helpful if they cor-\nrelate sufﬁciently with human judgements, in this\nsection we examine how MARS correlates with\nhuman judgements compared with prior metrics.\nSystem-level Correlation. Table 3 shows the\ncorrelations between human judgements and au-\ntomated metrics for MARS and seven other unsu-\npervised metrics, across all NLG systems studied\nin our three tasks. Compared with the other metrics,\nMARS achieves the highest correlation with human\njudgements for ﬁve of the seven systems (and com-\nparable with the top in the other two systems), mak-\ning considerable improvements over the next-best\nmetric for many of the NLG systems (e.g., 0.370 ↑\nfor Back-Translation, and 0.231 ↑for Solr). We\nalso notice that MARS has greater improvements\non more open-ended tasks (e.g., story generation,\nwhich has low Ω), which corroborates MARS’s\noriginal objective of judging diverse candidates\nmore fairly. As for the baselines, =-gram matching\nmetrics such as BLEU correlate poorly with human\nratings on such open-ended tasks; BERTScore per-\nforms better on short candidates and high-Ωtasks\n(e.g., QA); and perplexity, as expected, correlates\nweakly with human ratings. The Naive method,\nwhich uses multiple augmented references of the\nsame length, improves over BERTScore, which\nonly uses the original reference.\nAblation Study. As shown in the lower rows of\nTable 3, we see that the performance of MARS\ndrops substantially when the crucial components\nare removed. Speciﬁcally, removing self-planning\nhurts performance more for tasks with longer refer-\nences (e.g., story generation) since self-planning is\nmore helpful when there are more blanks to in-ﬁll,\nand removing context hurts performance more in\ntasks that are less open-ended (highΩ, such as QA)\nbecause there is no adequate input for a reasonable\naugmentation. We take these ablation study re-\nsults as evidence that the techniques we propose in\nMARS are crucial for improving correlation with\nhuman judgements.\nTask-level Correlation Visualization. To visu-\nalize the correlation between automated metrics\n6683\nROC Story Generation Newsroom SummarizationMOCHA Question Answering\nExisting MetricsReorder (Δ) Retrieve (Δ) ref. Reorder (Δ) Retrieve (Δ) ref. Reorder (Δ) Retrieve (Δ) ref.\nBLEU-1 (=) 0 ▼0.015 0.137 (=) 0 ▼0.144 0.176 (=) 0 ▼0.424 0.344\nMETEOR ▼0.041 ▼0.031 0.094 ▼0.132 ▼0.142 0.244 ▼0.012 ▼0.379 0.412\nROUGE-L ▼0.131 ▼0.123 0.194 ▲0.011 ▼0.035 0.036 ▼0.032 ▼0.363 0.336\nSent. Mover Sim. ▼0.024 ▼0.062 0.019 ▼0.153 ▼0.161 0.136 ▼0.232 ▼0.161 0.515\nMoverScore ▼0.131 ▼0.123 0.276 ▲0.011 ▼0.135 0.236 ▲0.027 ▼0.495 0.500\nBERTScore ▼0.109 ▼0.127 0.337 ▼0.112 ▼0.026 0.344 ▼0.101 ▼0.461 0.462\nPerplexity ▼0.113 ▲0.170 -0.089 ▼0.298 ▲0.008 0.234 ▼0.035 ▲0.026 -0.032\nMARS\nw/. RoBERTa Emb.▼0.125 ▼0.191 0.459 ▼0.117 ▼0.198 0.423 ▼0.092 ▼0.504 0.667\nw/. GloVe Emb. ▼0.087 ▼0.177 0.363 ▼0.052 ▼0.149 0.409 ▼0.085 ▼0.426 0.602\nNaive (MLM) ▼0.149 ▼0.156 0.350 ▼0.112 ▼0.190 0.314 ▼0.098 ▼0.247 0.639\nTable 4: We test robustness of MARS and seven other automated metrics under attacks from adversarial samples\ngenerated by following two attack strategies: (1) Reorder: randomly reorders 50% of tokens in the candidates; (2)\nRetrieve: randomly retrieves a sentence from the context as a candidate. ref.: correlation of original candidates\nwith human judgements. If a metric scores adversarial samples equal to (=) or higher ( ▲) than ref., we consider\nsuch metrics not robust under attacks. Robust systems should assign decreased scores (▼) compared to ref.\nBERTScore (r = 0.46)\nHuman Judgement\nGPT-2\nBack-Tran\nMHPG\n0.00 0.25 0.50 0.75 1.00\n1\n2\n3\n4\n5\nGPT-2\nBack-Tran\nMHPG\n0.00 0.25 0.50 0.75 1.00\n1\n2\n3\n4\n5\nMARS (r = 0.67)\nHuman Judgement\nFigure 3: Correlation between BERTScore (left) and\nMARS (right) with human judgements for MOCHA\nQA. The G-axis is the automated metric score and H-\naxis is the human judgement. Points in different col-\nors represent generation outputs of three NLG systems:\nGPT-2 (red circles), Back-Translation (green triangles),\nand MHPG (blue squares).\nand human judgements, we consider the MOCHA\nQA task as an example and plot the correlations of\nBERTScore (left) and MARS (right) with human\njudgements. As shown in Figure 3, compared with\nMARS, BERTScore has more candidates in the\nupper-left corner of the plot (i.e., low BERTScore\nbut high human judgement). Many of these are\ngenerated by GPT-2 and MHPG, which, based on\nmanual examination, tend to provide more details\nin the answer than the human reference. For in-\nstance, given a context about shopping, one ques-\ntion is “Did they need to buy any meat?” . The\nhuman reference answer is simply “Yes, they did. ”,\nbut GPT-2 returns “Yes, they bought chicken and\na roast. ”, which is more detailed, even containing\nitem names derived from the context. Whereas\nBERTScore cannot evaluate such cases where the\ngenerated candidate is over-described with respect\nto the human reference, MARS uses augmented ref-\nerences enriched with information from the context\nto provide a fairer judgement.\n4.2 Is MARS robust?\nGood evaluation metrics ought to also be able to de-\ntect adversarial examples by assigning them lower\nscores than well-formed candidates. As shown in\nTable 4, uni-gram matching BLEU-1 cannot de-\ntect reordered sequences, while ROUGE-L scores\nreordered sequence higher occasionally if token-\nswapping leads to more LCS. Sentence Mover’s\nSimilarity combines word and sentence embed-\ndings and thus is more capable of recognizing re-\nordered samples than MoverScore. Perplexity can\ndetect reordered examples effectively, but is unable\nto detect retrieved sentences, as they are usually\nwell-formed. MARS, on the other hand, has the\nbest robustness against adversarial samples, possi-\nbly because multiple context-infused augmented\nreferences help MARS detect adversarial samples\nmore reliably. We also study the effects of contex-\ntual embeddings we use in §2.3—when switching\nto GloVe embeddings (Pennington et al., 2014),\nwhich are not contextual, MARS is less able to\ndetect adversarial samples, especially reordered\nones. The Naive method, which by default uses\nRoBERTa embedding, achieves comparable robust-\nness as MARS but its task-level correlations with\nhumans (ref.) are generally lower than MARS, po-\ntentially because its ﬁxed-length cloze generation\nlimits the diversity of augmented references.\n6684\nROC Story Generation\n{_}max 0% (ref.) 20% 40% 60% 80%\nPearson’sA 0.411 0.432 0.444 0.459 0.452\nAvg.f - 0.027 0.046 0.055 0.059\nNewsroom Summarization\n{_}max 0% (ref.) 20% 40% 60% 80%\nPearson’sA 0.395 0.407 0.416 0.423 0.411\nAvg.f - 0.061 0.062 0.063 0.068\nMOCHA Question Answering\n{_}max 0% (ref.) 20% 40% 60% 80%\nPearson’sA 0.658 0.667 0.649 0.603 0.584\nAvg.f - 0.074 0.104 0.117 0.125\nTable 5: Evaluating correlation with human judge-\nments for various max masking ratios ( _max) used in\nMARS. 0% masking ( ref.) means only the human ref-\nerence was used to score candidates. We also show\nthe averaged standard deviation of the cosine similar-\nities between the candidate and augmented references\nacross all samples.\n4.3 Choosing Masking Ratios for MARS\nThe masking ratios for MARS are set using the hy-\nperparameter {_}max, which corresponds to MARS\nusing masking ratios from 0% to {_}max in in-\ncrements of 20%, e.g., {_}max = 40% indicates\n_ ∈ {0%,20%,40%}. In preliminary experi-\nments, we observed that {_}max varied for differ-\nent datasets. Thus, for our three generation tasks,\nwe evaluate MARS performance given different\n{_}max, as shown in Table 5. We ﬁnd that tasks that\nwere more open-ended (low Ω; e.g., story genera-\ntion) beneﬁted from higher {_}max, which created a\nmore diverse set of augmented references, whereas\ntasks that were less open-ended (high Ω; e.g., QA)\nworked better with lower {_}max, which kept the\naugmented references more similar to the original.\n4.4 Error Analysis\nWe analyzed cases where MARS score substan-\ntially differed from human judgements. From test\nset outputs, we found that errors could often be cat-\negorized into one of three types (shown in Table 6):\n(1) Out of Vocabulary errors, often induced by\nunknown tokens in the candidates, (2) Confusion\nerrors, where candidates are simply copied from\ncontext, and (3) Inference errors, where the candi-\ndates are further inferences of the context based on\ncommonsense knowledge. In these cases, human\nannotators tended to assign higher scores, whereas,\nMARS over-penalized them.\nError Example\nOOV\n(ROC)\nContext: ...waltz dance at wedding...\nGold: All the guests gasped\nwhen they saw the couples’ skill!\nCandidate: All the guests gasped\nwhen they saw the UNK UNK\nHuman: 0.392 MARS: 0.198\nConfusion\n(Newsroom)\nContext: ...bidding on a neighborhood...\nGold: A neighborhood named\nfor its former orchards inspires loyalty\nand bidding wars.\nCandidate: Living there cherrydale lies\nnorth of interstate... (a sentence extracted\nfrom Context)\nHuman: 0.700 MARS: 0.399\nInference\n(MOCHA)\nContext: ...washing cloths...\nQ: Why did they do the laundry?\nGold: To clean their clothes\nCandidate: Because they were dirty.\nHuman: 0.400 MARS: 0.083\nTable 6: Error analysis of MARS. We investigated three\ntypical types of errors within the samples which re-\nceived large differences between the MARS score and\nhuman ratings. Gold: human written references.\n5 Human Judgement\nWe conducted human evaluation on Amazon Me-\nchanical Turk (MTurk) to further study the quality\nof MARS augmentation. In total 150 participants\nwere randomly assigned to evaluate the three tasks.\nParticipants (61.3% male and 38.7% female) were\nall from the United States and above 18 years old,\nwith an average age of 34.7 years old. Each partici-\npant was paid 75 cents for completing 14 questions\nin each questionnaire (average completion time per\nquestionnaire was about 5.11 minutes).\nResults We conducted paired sample C-tests to\nexamine how much the augmentation samples re-\nsemble the original human references regarding\nrelevance to context and readability. As shown in\nTable 7, in terms of relevance to context, MARS\nhad no statistically signiﬁcant difference compared\nwith original human references in Newsroom and\nMOCHA datasets, but was rated as even more rel-\nevant to the generation context than the human\nreference in the ROC dataset (MARS Mean = 5.07\n>Human Ref. Mean = 4.95), possibly because re-\ninforced self-planning guided the augmentation to\nbe more related to the context. In terms of readabil-\n6685\nROC Newsroom MOCHA\nOri. Naive MARS Ori. Naive MARS Ori. Naive MARS\nRelevance Mean 4.95 4.81 5.07 4.62 4.50 4.61 5.16 4.61 4.97\np - .00* .04* - .05 .95 - .00* .10\nReadability Mean 5.67 5.53 5.40 4.54 4.31 4.59 5.41 5.23 5.33\np - .11 .05 - .12 .41 - .16 .29\nOverall Mean 5.69 5.31 5.42 4.87 4.57 4.75 4.62 4.44 4.68\np - .12 .30 - .10 .22 - .07 .10\nTable 7: Human evaluation results on Relevance (to context), Readability, and Overall quality of MARS and\nNaive augmentation method. All results are compared with the original human reference (Ori.). Text was scored\non a scale from 1-7. ?value describes the signiﬁcance of difference. (* corresponds to ? <0.05, ** to ? <0.01\nand *** to ? <0.001.)\nity, both MARS and Naive were rated lower than\nthe original but not signiﬁcantly; we take this as a\ncompromise of cloze style augmentation. No sta-\ntistically signiﬁcant differences were seen between\nthe original and MARS augmentation in overall\nratings across the three tasks. These results further\nconﬁrm that augmented examples from MARS are\nof similar quality to the original human references.\n6 Related Metrics\nUnsupervised Metrics. In addition to the met-\nrics we directly compared with previously, other\nunsupervised metrics have also been proposed.\nTER (Snover et al., 2006), CharacTer (Wang\net al., 2016), and chrF (Popovi ´c, 2017) focus on\ncharacter-level overlaps instead of =-gram match-\ning. Similar to BERTScore, YiSi (Lo, 2019) and\nBERTr (Mathur et al., 2019) leverage pre-trained\ncontextual embeddings to better capture similarity.\nΔBLEU (Galley et al., 2015) adds human anno-\ntated sentences as negative references. Bawden\net al. (2020) ﬁnd the gain from multiple references\ncan be limited by inherent weaknesses in BLEU.\nWe considered lessons from many of the above\nworks while designing MARS.\nLearned Metrics. Compared with unsupervised\nmetrics, learned metrics collect human supervi-\nsions (Freitag et al., 2020a; Chaganty et al., 2018)\nor train on specially prepared data of a certain do-\nmain (Sellam et al., 2020; Rei et al., 2020). Other\napproaches train on related tasks and use these mod-\nels as metrics for the original task (Goodrich et al.,\n2019; Eyal et al., 2019). Whereas learned metrics\nmay have limited applicability on tasks where no\nsuch resources are available, MARS fully exploits\nthe few-shot learning abilities of off-the-shelf LMs\nand therefore does not require additional training.\nTask-speciﬁc Metrics. Finally, many metrics\nhave been proposed for task-speciﬁc evaluation,\nsuch as LEIC (Cui et al., 2018) and CIDEr (Vedan-\ntam et al., 2015) for image captioning, PAR-\nENT (Dhingra et al., 2019) for table-to-text, and\nEASSE (Alva-Manchego et al., 2019) for sentence\nsimpliﬁcation. MARS, with some modiﬁcations,\ncan potentially be extended to these tasks.\n7 Limitations\nMARS can be limited by the LM that it uses—\nfor instance, the total length of context + refer-\nence/candidate is limited by the max sequence\nlength of the LM used. Additionally, our work\nhas focused on English, and MARS may require\nnon-trivial modiﬁcations to handle cases where the\ncontext and reference/candidate are in different lan-\nguages, such as machine translation. Future work,\ncould potentially extend MARS to these scenarios\nusing multi-lingual sequence-to-sequence models\nsuch as multilingual-T5 (Xue et al., 2020). We also\nanalyzed errors and found that MARS sometimes\nunder-scores candidates that contained unknown to-\nkens or were copied directly from the context (see\nAppendix C for examples and further analysis).\n8 Conclusion\nWe have proposed MARS, a context-aware and\neasy-to-deploy NLG metric built upon an off-the-\nshelf language model (GPT-2). On three contextual\nNLG tasks, we show that MARS better correlates\nwith human judgements compared with seven other\nunsupervised metrics. Requiring neither costly hu-\nman supervision nor additional training, MARS\ncan be applied to a broad range of NLG tasks.\n6686\nEthical Considerations\nThe goal of MARS is to aid the evaluation of NLG\nmodels, and hence we draw attention to several eth-\nical considerations. First, the augmented references\nof MARS can be affected by certain biases from the\nLM it is based on (e.g., GPT-2) (Liu et al., 2021),\nthough those biases may be partially mitigated by\nthe relatively narrow scope ofcloze completion and\nby generations being guided by given context and\nhuman references. Second, MARS facilitates eval-\nuation and therefore development of NLG models,\nfor which a major ethical consideration is that they\ncan mimic target properties in training data that\nare undesirable. This is especially true of models\ntrained on non-contemporary data that does not rep-\nresent current norms and practices. These biases\ncan lead to ethical concerns if users or deployers of\nmodels are not aware of these issues or do not ac-\ncount for them. More generally, NLG models can\nalso be used in malicious ways such as to generate\nfake news or spam, which we strongly discourage.\nFinally, our experiments and analysis are done in\nEnglish, and therefore we do not claim that our\nﬁndings will generalize across all languages, al-\nthough our framework has potential to be extended\nto other languages with necessary modiﬁcations.\nReferences\nFernando Alva-Manchego, Louis Martin, Carolina\nScarton, and Lucia Specia. 2019. EASSE: Easier au-\ntomatic sentence simpliﬁcation evaluation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP): System Demonstra-\ntions, pages 49–54, Hong Kong, China. Association\nfor Computational Linguistics.\nLisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.\nCommonsense for generative multi-hop question an-\nswering tasks. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4220–4230.\nRachel Bawden, Biao Zhang, Lisa Yankovskaya, An-\ndre T ¨attar, and Matt Post. 2020. A study in im-\nproving BLEU reference coverage with diverse au-\ntomatic paraphrasing. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 918–932, Online. Association for Computa-\ntional Linguistics.\nPeter F Brown, Stephen A Della Pietra, Vincent J\nDella Pietra, Jennifer C Lai, and Robert L Mercer.\n1992. An estimate of an upper bound for the entropy\nof english. Computational Linguistics, 18(1):31–40.\nArun Chaganty, Stephen Mussmann, and Percy Liang.\n2018. The price of debiasing automatic metrics in\nnatural language evalaution. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 643–653, Melbourne, Australia. Association\nfor Computational Linguistics.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2020. Mocha: A dataset for train-\ning and evaluating generative reading comprehen-\nsion metrics. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 6521–6532.\nElizabeth Clark, Asli Celikyilmaz, and Noah A. Smith.\n2019. Sentence mover’s similarity: Automatic eval-\nuation for multi-sentence texts. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2748–2760, Florence,\nItaly. Association for Computational Linguistics.\nYin Cui, Guandao Yang, Andreas Veit, Xun Huang,\nand Serge Belongie. 2018. Learning to evaluate im-\nage captioning. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 5804–5812.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh,\nMing-Wei Chang, Dipanjan Das, and William Co-\nhen. 2019. Handling divergent reference texts when\nevaluating table-to-text generation. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4884–4895, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to ﬁll in the blanks. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2492–\n2501, Online. Association for Computational Lin-\nguistics.\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A\nquestion answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5055–\n5070, Online. Association for Computational Lin-\nguistics.\nMatan Eyal, Tal Baumel, and Michael Elhadad. 2019.\nQuestion answering as an automatic evaluation met-\nric for news article summarization. In Proceed-\nings of the 2019 Conference of the North American\n6687\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 3938–3948, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMarkus Freitag, George Foster, David Grangier, and\nColin Cherry. 2020a. Human-paraphrased refer-\nences improve neural machine translation. In Pro-\nceedings of the Fifth Conference on Machine Trans-\nlation, pages 1183–1192, Online. Association for\nComputational Linguistics.\nMarkus Freitag, David Grangier, and Isaac Caswell.\n2020b. BLEU might be guilty but references are not\ninnocent. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 61–71, Online. Association for\nComputational Linguistics.\nMichel Galley, Chris Brockett, Alessandro Sordoni,\nYangfeng Ji, Michael Auli, Chris Quirk, Mar-\ngaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015.\ndeltaBLEU: A discriminative metric for generation\ntasks with intrinsically diverse targets. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers) , pages 445–450,\nBeijing, China. Association for Computational Lin-\nguistics.\nHongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong,\nand Wen-mei Hwu. 2019. Reinforcement learn-\ning based text style transfer without parallel train-\ning corpus. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 3168–3180, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBen Goodrich, Mohammad Ahmad Saleh, Peter Liu,\nand Vinay Rao. 2019. Assessing the factual accu-\nracy of text generation.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nPrakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy\nPavel, Maxine Eskenazi, and Jeffrey P Bigham.\n2019. Investigating evaluation of open-domain di-\nalogue systems with human generated multiple ref-\nerences. In Proceedings of the 20th Annual SIG-\ndial Meeting on Discourse and Dialogue, pages 379–\n391.\nTatsunori Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation for\nnatural language generation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1689–1701, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nTom´aˇs Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, G´abor Melis, and\nEdward Grefenstette. 2018. The narrativeqa reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317–328.\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An\nautomatic metric for MT evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation, pages 228–231, Prague, Czech Repub-\nlic. Association for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In Proceedings of the 42nd Annual Meet-\ning of the Association for Computational Linguistics\n(ACL-04), pages 605–612, Barcelona, Spain.\nRuibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu,\nLili Wang, and Soroush V osoughi. 2021. Mitigating\npolitical bias in language models through reinforced\ncalibration. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChi-kiu Lo. 2019. YiSi - a uniﬁed semantic MT quality\nevaluation and estimation metric for languages with\ndifferent levels of available resources. In Proceed-\nings of the Fourth Conference on Machine Transla-\ntion (Volume 2: Shared Task Papers, Day 1) , pages\n507–513, Florence, Italy. Association for Computa-\ntional Linguistics.\nNitika Mathur, Timothy Baldwin, and Trevor Cohn.\n2019. Putting evaluation in context: Contextual\nembeddings improve machine translation evaluation.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2799–2808, Florence, Italy. Association for Compu-\ntational Linguistics.\n6688\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nShikib Mehri and Maxine Eskenazi. 2020. USR: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 681–707, Online. Association for\nComputational Linguistics.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei\nLi. 2019. Cgmh: Constrained sentence generation\nby metropolis-hastings sampling. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 6834–6842.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nR´emi Munos, Tom Stepleton, Anna Harutyunyan, and\nMarc Bellemare. 2016. Safe and efﬁcient off-policy\nreinforcement learning. In Advances in Neural In-\nformation Processing Systems, pages 1054–1062.\nPreksha Nema and Mitesh M. Khapra. 2018. Towards a\nbetter metric for evaluating question generation sys-\ntems. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3950–3959, Brussels, Belgium. Association\nfor Computational Linguistics.\nJekaterina Novikova, Ond ˇrej Du ˇsek, Amanda Cer-\ncas Curry, and Verena Rieser. 2017. Why we need\nnew evaluation metrics for NLG. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing , pages 2241–2252,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nSimon Ostermann, Ashutosh Modi, Michael Roth, Ste-\nfan Thater, and Manfred Pinkal. 2018. Mcscript:\nA novel dataset for assessing machine comprehen-\nsion using script knowledge. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nLawrence Page, Sergey Brin, Rajeev Motwani, and\nTerry Winograd. 1999. The pagerank citation rank-\ning: Bringing order to the web. Technical report,\nStanford InfoLab.\nRichard Yuanzhe Pang and He He. 2021. Text gen-\neration by learning from off-policy demonstrations.\nInternational Conference on Learning Representa-\ntions (ICLR 21’).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nDavid Pisinger. 1995. Algorithms for knapsack prob-\nlems.\nMaja Popovi ´c. 2017. chrF++: words helping charac-\nter n-grams. In Proceedings of the Second Con-\nference on Machine Translation , pages 612–618,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 2685–2702, Online. Associa-\ntion for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083.\nThibault Sellam, Dipanjan Das, and Ankur Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7881–7892, Online. Association for Computa-\ntional Linguistics.\n6689\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96.\nTianxiao Shen, Victor Quach, Regina Barzilay, and\nTommi Jaakkola. 2020. Blank language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5186–5198, Online. Association for Computa-\ntional Linguistics.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study of\ntranslation edit rate with targeted human annotation.\nIn Proceedings of association for machine transla-\ntion in the Americas, volume 200. Cambridge, MA.\nChongyang Tao, Lili Mou, Dongyan Zhao, and Rui\nYan. 2018. Ruber: An unsupervised method for au-\ntomatic evaluation of open-domain dialog systems.\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 32.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism quarterly,\n30(4):415–433.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566–4575.\nWeiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl,\nand Hermann Ney. 2016. CharacTer: Translation\nedit rate on character level. In Proceedings of the\nFirst Conference on Machine Translation: Volume\n2, Shared Task Papers, pages 505–510, Berlin, Ger-\nmany. Association for Computational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. arXiv preprint arXiv:2010.11934.\nTsuta Yuma, Naoki Yoshinaga, and Masashi Toyoda.\n2020. uBLEU: Uncertainty-aware automatic evalua-\ntion method for open-domain dialogue systems. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics: Student Re-\nsearch Workshop, pages 199–206, Online. Associa-\ntion for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\nYizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe\nGan, Chris Brockett, and Bill Dolan. 2020.\nPOINTER: Constrained progressive text generation\nvia insertion-based generative pre-training. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 8649–8670, Online. Association for Compu-\ntational Linguistics.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nWanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text\ninﬁlling. arXiv preprint arXiv:1901.00158.\n6690\nAppendix A: DP-based Token Masking\nAlgorithm\nAs part of Eq.1 in the main paper, we deﬁne the\nIDF score given tokenG8 and a corpus -containing\n\"documents as:\nIDF(G8,-)=−log 1\n\"\n\"Õ\n9=1\n[G8 ∈-9],\nwhere [·]is the indicator function. We present our\nDP-based masking algorithm in Algorithm 1:\nAlgorithm 1: DP-based Token Masking\nInput: Human reference {G8}#\n8=1, masking\nratio _, and task-speciﬁc factor U.\nCompute E8 for each G8 with U(Eq. 1);\nCompute F8 depending on LCS for each G8;\nInit DP-table )[#+1][,max +1]with all 0;\nfor 8=1,2,...,# do\nfor 9 =1,2,...,, max do\nif 9−F8−1 <0 then\n)[8][9]=)[8−1][9];\nRecord masking choice q(G8);\nelse\n)[8][9]=max()[8−1][9],\n)[8−1][9−F8−1]+ E8−1);\nRecord masking choice q(G8);\nend\nend\nend\n{q(G8)#\n8=1}← backtracking via records;\nreturn best masking strategy {q(G8)#\n8=1};\nAppendix B: Generate, Judge, and Revise\nAlgorithm\nThe complete procedure for augmenting human ref-\nerences is presented in Algorithm 2. For a given\ntemplate, we ﬁrst group the tokens into a block-\nby-block form with blank blocks ( [B]) and text\nblocks ([T]). Then, we generate varying lengths\nof tokens, iteratively concatenating them with next\ntext block, and judging them based on PPL, and ﬁ-\nnally revising current generations accordingly. We\nuse the language modeling ability of LM to check\nthe perplexity of the current sequence, and set a\nhyper-parameter f to control the maximum ex-\ntended generation (for a lower PPL).\nDepending on whether there is a subsequent\ntext block, the generation will switch between two\nAlgorithm 2: Generate, Judge and Revise\nInput: Template {q(G8)}#\n8=1, max guess f,\nand LM perplexity checker PPL.\nGroup {q(G8)}#\n8=1 into [B] and [T];\nInit ﬁnal output B;\nforeach block do\n8←0;\nInit priority queue @, buffer B′;\nif [T] then\nAppend [T] to B;\nelse if [B] then\nwhile 8 <f+|[B]|do\nif next is [T] then\nF←self-planning gen.;\nelse\nF←open-ended gen.;\nend\nB′←B+F;\nRecord (PPL(B′+ [T]), B′) in @;\n8←8+1;\nend\nB←B+lowest PPL B′pop from @;\nend\nend\nreturn augmented reference B;\nmodes: self-planning generation (if there is future\ncontext) and open-ended generation (otherwise).\nWe use a priority queue to store each step genera-\ntion and its corresponding PPL for quick revisions\nafterwards.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8486925363540649
    },
    {
      "name": "Metric (unit)",
      "score": 0.708541214466095
    },
    {
      "name": "Relevance (law)",
      "score": 0.7053075432777405
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6722286939620972
    },
    {
      "name": "Language model",
      "score": 0.6492819786071777
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6249093413352966
    },
    {
      "name": "Matching (statistics)",
      "score": 0.6248733401298523
    },
    {
      "name": "Natural language processing",
      "score": 0.5901505351066589
    },
    {
      "name": "Natural language generation",
      "score": 0.5653643608093262
    },
    {
      "name": "Machine learning",
      "score": 0.49654632806777954
    },
    {
      "name": "Information retrieval",
      "score": 0.4201483428478241
    },
    {
      "name": "Natural language",
      "score": 0.20187485218048096
    },
    {
      "name": "Statistics",
      "score": 0.0874384343624115
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166639",
      "name": "Dartmouth Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I107672454",
      "name": "Dartmouth College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}