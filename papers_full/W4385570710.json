{
  "title": "Knowledge Graph-augmented Language Models for Complex Question Answering",
  "url": "https://openalex.org/W4385570710",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2178900758",
      "name": "Priyanka Sen",
      "affiliations": [
        "Amazon (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A701476892",
      "name": "Sandeep Mavadia",
      "affiliations": [
        "Amazon (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2097703981",
      "name": "Amir Saffari",
      "affiliations": [
        "Amazon (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4287816895",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W2963796939",
    "https://openalex.org/W4302306557",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W3199020415",
    "https://openalex.org/W4300506197",
    "https://openalex.org/W4295189643",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3045295485",
    "https://openalex.org/W3204877056",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "Large language models have shown impressive abilities to reason over input text, however, they are prone to hallucinations. On the other hand, end-to-end knowledge graph question answering (KGQA) models output responses grounded in facts, but they still struggle with complex reasoning, such as comparison or ordinal questions. In this paper, we propose a new method for complex question answering where we combine a knowledge graph retriever based on an end-to-end KGQA model with a language model that reasons over the retrieved facts to return an answer. We observe that augmenting language model prompts with retrieved KG facts improves performance over using a language model alone by an average of 83%. In particular, we see improvements on complex questions requiring count, intersection, or multi-hop reasoning operations.",
  "full_text": "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023), pages 1–8\nJune 13, 2023 ©2023 Association for Computational Linguistics\nKnowledge Graph-augmented Language Models for Complex Question\nAnswering\nPriyanka Sen\nAmazon Alexa AI\nCambridge, UK\nsepriyan@amazon.com\nSandeep Mavadia\nAmazon Alexa AI\nCambridge, UK\nsmavadia@amazon.com\nAmir Saffari\nAmazon Alexa AI\nCambridge, UK\namsafari@amazon.com\nAbstract\nLarge language models have shown impres-\nsive abilities to reason over input text, however,\nthey are prone to hallucinations. On the other\nhand, end-to-end knowledge graph question\nanswering (KGQA) models output responses\ngrounded in facts, but they still struggle with\ncomplex reasoning, such as comparison or or-\ndinal questions. In this paper, we propose a\nnew method for complex question answering\nwhere we combine a knowledge graph retriever\nbased on an end-to-end KGQA model with a\nlanguage model that reasons over the retrieved\nfacts to return an answer. We observe that\naugmenting language model prompts with re-\ntrieved KG facts improves performance over\nusing a language model alone by an average\nof 83%. In particular, we see improvements\non complex questions requiring count, intersec-\ntion, or multi-hop reasoning operations.\n1 Introduction\nLarge language models (LMs) have shown great\npromise in a variety of NLP tasks, including\nquestion-answering (QA) (Zhang et al., 2022; Sanh\net al., 2022; Wei et al., 2022a). As language models\nscale, they achieve impressive results on standard\nQA benchmarks, such as SQuAD (Raffel et al.,\n2020), and can answer questions either few-shot or\nzero-shot using only knowledge stored within the\nmodel parameters (Roberts et al., 2020). Language\nmodels have also been shown to solve complex rea-\nsoning tasks by outputting step-by-step instructions\nfrom question to answer (Creswell et al., 2022; Wei\net al., 2022b). Despite these successes, language\nmodels are still prone to hallucinations and can re-\nturn answers that are incorrect, out-of-date, and not\ngrounded in verified knowledge sources, making\nthem an unsafe choice for a factual question an-\nswering service. Additionally, step-by-step reason-\ning can be computationally expensive as it requires\nmultiple calls to a language model.\nAlternatively, knowledge graph-based question\nanswering (KGQA) models (Chakraborty et al.,\n2019; Fu et al., 2020) are trained to traverse knowl-\nedge graph (KG) facts to return answers to ques-\ntions. These models are faithful and grounded to\nfacts stored in a KG. However KGQA models are\noften limited in the types of reasoning that they can\nperform. Most end-to-end KGQA models can per-\nform relation following for single or multiple hops\n(Cohen et al., 2020), and some models have been\ntrained for set intersection (Sen et al., 2021), union,\nor difference (Sun et al., 2020), however expanding\nto general reasoning capabilities remains an open\nchallenge. KGQA models are also restricted to us-\ning facts stored in a knowledge graph and can not\nleverage common world knowledge.\nIn this paper, we propose a novel method for\ncomplex question answering using a KGQA model\nretriever with a language model reasoner. Our ap-\nproach harnesses both the ability to traverse over\nverified facts with a KGQA model, and the ability\nto reason over text with an LM.\nFor our KGQA retriever, we train an end-to-\nend KGQA model based on ReifKB (Cohen et al.,\n2020) and the Rigel family of models (Saffari et al.,\n2021; Sen et al., 2021). We use this model to re-\nturn a weighted set of facts from the knowledge\ngraph that could be useful for answering a given\nquestion. We then prompt an LM with the question\nand the top-k facts retrieved, in a zero-shot setting,\nand the language model returns a natural language\nanswer. In our experiments over four QA datasets,\nwe show that our approach can outperform using\nan LM alone by an average of 83%.\n2 Related Works\nRecent work on using language models for rea-\nsoning tasks include Kojima et al. (2023), where\nthe authors prompt the models to output the steps\nto the answer in addition to the final result. Other\nmethods have also tried to solve questions by break-\n1\nFigure 1: Architecture for our model setup. Question\nentities are shown as blue nodes in the knowledge graph\ndiagrams. See section 3.1 for details.\ning them down into intermediate steps: Wei et al.\n(2022b) prompts the language model with similar\nexamples where an answer is formed step-wise be-\nfore providing the requested answer. Creswell et al.\n(2022) fine-tune several models with the task of\nchoosing relevant knowledge until a satisfactory\nanswer is reached. Although these methods im-\nprove language model performance, it can be costly\nto fine-tune a large language model or to pass an\ninput through a language model multiple times, es-\npecially at runtime. In this work, we instead build\na lighter weight retriever model to collect relevant\nfacts followed by a single call to a language model.\nDifferent data sources have also been used for re-\ntrieval: Lazaridou et al. (2022) uses Google search.\nKang et al. (2022) also retrieves facts from a knowl-\nedge graph but requires fine-tuning a language\nmodel, which can be expensive for the larger mod-\nels. Recently, Baek et al. (2023) used a similarity\nmetric between KG facts and questions to retrieve\nrelevant facts to add to the prompt of a language\nmodel. However Baek et al. (2023) found that sim-\nilarity alone is not always enough to find relevant\nfacts for complex question. In this work we present\na more sophisticated model for identifying relevant\nfacts with a KGQA model.\n3 Method\nWe propose a new method for question answer-\ning using a KGQA model to retrieve facts from a\nknowledge graph, and a language model to reason\nover the question and facts to return an answer.\n3.1 Model description\nAs shown in Figure 1, our model has three main\ncomponents:\n1. A sequence-to-sequence KGQA model\n(which we refer to as RIGEL based on Saffari\net al. (2021); Sen et al. (2021)) for predicting\na distribution over which relations to follow\nin a knowledge graph.\n2. A differentiable knowledge graph (DKG),\nwhere the KG is stored in three linear maps\nfrom left-entities, relations, and right-entities\nto triples respectively, represented as sparse\nbinary matrices (Cohen et al., 2020).\n3. A language model for interpreting the ques-\ntions and reasoning over facts provided from\nthe KG by the two previous components.\nWe do not yet integrate entity resolution, so ques-\ntion entities are provided from the datasets.\nThere are three steps to running the model in\ninference. First, Rigel is used to estimate a distribu-\ntion over relations for each hop using a sequence-\nto-sequence architecture. We initialize the encoder\nusing RoBERTa-base (Liu et al., 2019). The de-\ncoder predicts a distribution over relations in the\nknowledge graph. This decoding step is performed\nfor up to M hops (in our experiments, M = 2).\nSecond, the question entities and the distribution\nover relation are used to extract weighted triples\nfrom the knowledge graph. We represent the ques-\ntion entities as a one-hot vector in entity-space and\nmap this to a vector of triples using the left-entity to\ntriple sparse matrix in the DKG. Similarly, for rela-\ntions, we use the vector over relations predicted by\nthe Rigel model and map this to a vector of triples\nusing the relation to triple matrix in the DKG. Fi-\nnally we take the Hadamard (element-wise) prod-\nuct to extract a weighted vector of triples. For the\nsecond hop, we map the triple vector back to en-\ntities using the right-entity to triple matrix in the\nDKG and repeat the process above.\nWe retain only the top- k triples for each hop\n(in our experiments k = 10) and convert them into\nnatural language using the names of the entities\nand relations as stored within the KG. We also in-\nclude inverse triples within our DKG where the re-\nlations are prefixed with “<inv>-” when converted\nto natural language, e.g. “(Paris, <inv>-capital-of,\nFrance)”. We also include literal values for num-\nbers, strings and dates in our DKG as right entities.\nFinally, we run inference in a zero-shot setting\nwith a pretrained language model. We compose\na prompt following the template: “ Given the fol-\nlowing context: \"{context}\" Answer the question:\n2\n{question}. Answer:” where the context is the set\nof filtered triples formatted as “(left entity, relation,\nright entity), . . . , (left entity, relation, right entity)”.\nWe input this prompt into a language model to out-\nput an answer.\n3.2 Training\nOf the three components outlined in section 3.1,\nwe only train the Rigel KGQA model. The DKG\nis instantiated from a static dump of the Wikidata\n(Vrandeˇci´c and Krötzsch, 2014) knowledge graph,\nand the language models in our experiments are\nnot fine-tuned.\nRigel is trained using the train and dev sets of\nKGQA datasets annotated with natural language\nquestions, question entities, and answer entities. As\ndescribed in section 3.1 we estimate a distribution\nover entities for each hop. During training, we\nalso jointly learn an attention mechanism which is\nconditioned on the question embedding to predict\nhow much to weigh answer entities returned for\neach hop. We use a binary cross-entropy objective\nto allow for multiple answer entities. For more\ninformation on training the Rigel model see Sen\net al. (2021). We leave end-to-end training of both\nthe KGQA model and the LM to future work.\n4 Experimental Setup\n4.1 Datasets\nWe use four KGQA datasets in our experiments\nwith Wikidata as the knowledge graph. For datasets\nbuilt using FreeBase, we link entities to Wikidata\nusing the FreeBase IDWikidata property. The train\n/ dev sets for each dataset are used to train the Rigel\nmodel, and all results are reported on the test sets.\n• WebQuestions (Berant et al., 2013) is an\nEnglish question-answering dataset of 4,737\nquestions (2,792 train, 306 dev, 1,639 test)\noriginally built on FreeBase. WebQuestions\nincludes questions requiring multiple hops\nand set intersections.\n• ComplexWebQuestions (Talmor and Berant,\n2018) is an extended version of WebQuestions\nwith 34,689 questions (27,649 train, 3,509\ndev: since the test set is not public, we use\nthe dev set for testing) in English requiring\ncomplex operations, such as multiple hops\nand temporal constraints.\n• Mintaka (Sen et al., 2022) is a complex ques-\ntion answering dataset of 20,000 questions\n(14,000 train, 2,000 dev, 4,000 test) linked to\nWikidata and using complex operations such\nas comparisons and set operations. We use the\nEnglish subset.\n• LC-QuAD is a dataset of 30,000 synthetically\ngenerated English questions and SPARQL\nparses. We use the subset with SPARQL\nparses that return a valid answer from Wiki-\ndata (20,438 train, 5,230 test). These ques-\ntions include complex operations such as\nmulti-hop and count.\n4.2 Language Models\nWe evaluate our method using language models\nfrom four families of models:\n• Flan-T5 (Chung et al., 2022) models are an\nextension of T5 encoder-decoder models that\nhave been instruction tuned on a large set\nof instructions that were automatically gen-\nerated using existing datasets and templates.\nWe use the Flan-T5 Small (80M parameters),\nXL (3B), and XXL (11B) models.\n• T0 (Sanh et al., 2022) models are encoder-\ndecoder models that are trained on a variety\nof prompts, which are automatically built from\nsupervised datasets using templates. We use\nthe T0 (11B) and T0 3B (3B) models.\n• OPT (Zhang et al., 2022) models are large,\nopen-source, decoder-only models that have\nbeen trained to roughly match the perfor-\nmance of GPT-3 models. We use the 13B\nparameter version of OPT.\n• AlexaTM (Soltan et al., 2022) is a 20 billion\nparameter encoder-decoder model trained on\npublicly available data in multiple languages.\n4.3 Training Specifications\nWe train each of our Rigel models on a single\nNVIDIA Tesla V100 GPU for 40,000 steps. We\nrun inference over our language models using four\nTesla V100 GPUs and distribute across GPUs using\nHugging Face Accelerate (Gugger et al., 2022).\n4.4 Evaluation metric\nOur datasets provide answer entities (e.g., Wiki-\ndata Q-codes). To evaluate the performance of the\nLM’s natural language output, we test if any of the\nprovided answer entity names or their aliases as\n3\nFlan-T5 T0\nDataset Experiment Rigel S XL XXL 3B 11B OPT ATM\nWebQ No Knowledge – 16.29 40.15 45.15 29.10 34.05 26.85 38.56\nRandom Facts – 21.90 28.49 39.96 28.07 36.30 48.02 41.98\nRigel Facts 48.9 45.52 55.58 59.79 53.33 55.64 57.60 55.40\n% improvement – 179% 38% 32% 83% 63% 115% 44%\nCWQ No Knowledge – 9.63 28.79 31.00 20.26 24.98 18.19 27.20\nRandom Facts – 14.69 23.96 31.15 20.86 26.85 28.13 29.41\nRigel Facts 29.21 25.55 36.09 40.38 32.54 36.35 32.54 35.72\n% improvement – 165% 25% 30% 61% 46% 79% 31%\nMintaka No Knowledge – 12.65 24.63 30.15 21.13 30.08 38.53 28.48\nRandom Facts – 12.20 20.98 33.63 21.33 30.40 42.20 33.00\nRigel Facts 21.7 20.58 33.50 37.90 29.28 33.35 40.03 35.60\n% improvement – 63% 36% 26% 39% 11% 4% 25%\nLC-QuAD No Knowledge – 3.90 8.15 3.82 8.32 9.31 8.75 11.79\nRandom Facts – 8.40 8.91 11.24 9.71 10.65 12.65 12.81\nRigel Facts 27.86 15.65 22.82 9.41 20.54 22.32 20.93 22.30\n% improvement – 301% 180% 146% 147% 140% 139% 89%\nTable 1: Results by language model and dataset over two baselines (No Knowledge and Random Facts) and our\nproposed method, Rigel Facts. % improvement shows the percentage improvement over No Knowledge to Rigel\nFacts. Rigel shows the baseline of using Rigel alone with no language model.\nstored in Wikidata exist within the LM output. We\nalso lower case text in the prediction and remove\npunctuation, articles, and extra white space.\n5 Results\nWe evaluate our method on four complex question-\nanswering datasets using seven language models.\nWe compare against two baselines.\n• No Knowledge: we provide the question with\nno additional context. The prompt is “ Ques-\ntion: {question} Answer:”.\n• Random Facts: we provide k random facts (k\n= 10) sampled uniformly over all facts reach-\nable in one hop from the question entities.\nThe results are reported in Table 1, with the %\nimprovement showing the percentage of improve-\nment from the No Knowledge baseline to our pro-\nposed Rigel Facts method. We also report scores\nfor the Rigel model alone in the Rigel column.\nThese results show that in almost all cases, a lan-\nguage model using facts retrieved from our Rigel\nmodel outperforms No Knowledge and Random\nFacts. For smaller models such as Flan-T5, using\nRigel facts improves performance by up to 300%.\nLarger models, such as AlexaTM, start with higher\nbaselines using no knowledge, but still see an aver-\nage of 47% improvement across datasets. Excep-\ntions are OPT on Mintaka and Flan-T5 XXL on\nLC-QuAD, where random facts outperform Rigel.\nWe observe that in many of the questions where\naugmenting with random facts performs better than\nRigel facts, neither provide useful information. In-\nterestingly, however, random facts still encourage\nthe LM to output the correct answer.\nThe No Knowledge results show that models can\nanswer questions without additional facts. Larger\nmodels with no facts can even outperform smaller\nmodels with Rigel facts, for example, AlexaTM vs.\nFlan-T5 on Mintaka (28.48 vs. 20.58). Neverthe-\nless, it is promising to see smaller models become\nmore competitive with the help of a retriever.\nThe use of random facts shows mixed results.\nRandom facts rarely outperform Rigel, but com-\npared to the No Knowledge baseline, random facts\ncan sometimes help, as seen across models on Com-\nplexWebQuestions and LC-QuAD. In other cases,\n4\nFlan-T5 T0\nQuestion Type Experiment S XL XXL 3B 11B OPT ATM Average\nComparative No Facts 48.50 63.00 64.00 49.25 59.75 58.50 57.25 56.75\nRandom Facts 36.00 61.25 62.50 44.50 57.75 60.00 63.00 55.00\nRigel Facts 42.75 64.25 65.50 42.00 55.50 54.75 60.00 55.12\nCount No Facts 16.75 26.25 33.00 21.25 25.00 25.00 40.75 27.56\nRandom Facts 17.25 17.75 28.50 27.25 28.75 51.00 43.50 30.57\nRigel Facts 23.75 27.25 31.25 40.75 32.00 49.00 48.75 36.47\nDifference No Facts 4.25 16.75 19.50 17.00 21.00 20.00 28.25 19.28\nRandom Facts 6.75 11.75 20.50 11.25 15.50 29.00 21.75 16.64\nRigel Facts 15.50 17.50 24.00 14.25 17.00 28.75 27.25 20.44\nGeneric No Facts 2.12 16.50 24.25 18.12 28.75 48.38 37.50 27.12\nRandom Facts 11.62 20.50 35.75 19.62 30.88 50.00 43.62 30.29\nRigel Facts 20.50 34.25 41.12 30.88 36.75 47.12 45.50 37.61\nIntersection No Facts 1.75 20.00 28.50 22.50 35.25 54.00 42.50 31.47\nRandom Facts 8.50 22.50 37.00 18.00 31.25 51.00 40.50 29.82\nRigel Facts 16.00 40.25 44.75 35.00 41.00 49.50 45.75 39.81\nMulti-hop No Facts 3.00 7.25 12.75 8.25 13.25 20.00 13.25 12.44\nRandom Facts 2.75 9.50 18.25 6.50 14.25 24.00 15.75 13.00\nRigel Facts 13.50 22.25 27.75 20.50 21.25 25.75 21.00 22.69\nOrdinal No Facts 1.50 9.50 16.50 10.00 15.50 27.75 20.25 15.44\nRandom Facts 6.75 9.50 17.75 9.50 18.00 29.75 20.75 16.00\nRigel Facts 12.00 16.50 23.75 17.25 18.00 24.25 24.25 19.84\nSuperlative No Facts 1.25 11.75 16.00 16.00 19.25 28.75 21.50 17.12\nRandom Facts 6.00 12.00 21.75 12.25 17.50 28.25 23.75 17.36\nRigel Facts 10.50 18.75 21.75 16.75 14.00 25.00 24.00 19.34\nYes/No No Facts 45.25 59.00 62.75 49.00 63.25 54.00 37.00 53.16\nRandom Facts 14.75 24.50 58.25 44.50 60.50 49.25 51.25 43.29\nRigel Facts 30.75 59.75 58.50 44.50 62.50 49.25 51.25 52.12\nAverage No Facts 13.82 25.56 30.81 23.49 31.22 37.38 33.14 28.93\nRandom Facts 12.26 21.03 33.36 21.49 30.49 41.36 35.99 28.00\nRigel Facts 20.58 33.42 37.60 29.10 33.11 39.26 38.64 33.72\nTable 2: A breakdown of results by the different complexity types in the Mintaka dataset\nrandom facts can hurt performance, as seen in Flan-\nT5 XL on WebQuestions (from 40.15 to 28.49)\nand Mintaka (from 24.63 to 20.98). This can be\nattributed to random facts adding in distractors that\nsome models are more susceptible to. For exam-\nple, given the QA pair “ Q: Where does Princess\nLeia live? A: Alderaan”, if the random facts in-\nclude “Leia Organa place of birth Polis Massa”,\nthe model can incorrectly answer Polis Massa.\nWe also show results in Table 2 as a breakdown\nof performance by complexity type on the Mintaka\ndataset. On average, we see that Rigel facts help\nacross complexity types. The highest gains are\nin Count, Intersection, and Multi-hop questions.\nThese are also the areas that a model like Rigel,\nwhich traverses a knowledge graph by following\nrelations, is best suited for. Finding facts for com-\nparatives or yes/no questions are less reliable since\nthe training signal can be weak and there can be\nmultiple paths that spuriously lead to the correct\nanswer. For example, to answer Who is older, The\nWeeknd or Drake?, there are several ways to get to\n5\nQuestion\nHow many countries were in the Central Powers alliance in World War I?\nRandom Facts Rigel Facts\n· Central Powers has part Austria-Hungary · Central Powers has part Ottoman Empire\n· Central Powers Commons category Central Powers · Central Powers has part Kingdom of Bulgaria\n· Central Powers participant in World War I · Central Powers has part German Empire\n· Central Powers instance of military alliance · Central Powers has part Austria-Hungary\nPredictions Model No Knowledge Random Facts Rigel Facts\nFlan-T5 XXL 6 ✗ 2 ✗ 4 ✓\nQuestion\nWhere did the author of Pet Sematary go to college?\nRandom Facts Rigel Facts\n· Pet Sematary author Stephen King · Stephen King education Lisbon High School\n· Pet Sematary follows Christine · Stephen King education University of Maine\n· Pet Sematary language of work or name English · Pet Sematary author Stephen King\n· Pet Sematary publisher Doubleday · Pet Sematary notable work Stephen King\nPredictions Model No Knowledge Random Facts Rigel Facts\nT0 University of Michigan ✗ Dartmouth College ✗ University of Maine ✓\nQuestion\nWhat was the first book in the Lord of the Ring’s series?\nRandom Facts Rigel Facts\n· Lord of the Rings characters Gandalf · Fellowship of the Ring follows The Hobbit\n· Lord of the Rings characters Elrond · Two Towers follows Fellowship of the Ring\n· Lord of the Rings translator Maria Skibniewska · Return of the King follows Two Towers\n· Lord of the Rings nominated for Prometheus Award · Appendices follows Return of the King\nPredictions Model No Knowledge Random Facts Rigel Facts\nT0 Fellowship of the Ring ✓ Fellowship of the Ring ✓ The Hobbit ✗\nTable 3: Examples of questions and model predictions. For simplicity, we only show the top four facts. In\nPredictions, No Knowledge is only given the question. Random and Rigel Facts are given the question and the\nrespective facts. The correct answer is indicated with a ✓. Incorrect answers are indicated with a ✗.\nthe answer entity Drake without following a date\nof birthrelation and performing a comparison. In\nfuture work, we plan to explore different ways to\ntrain the Rigel model to provide a better training\nsignal of which facts will be useful to the LM.\nFinally, in Table 3, we provide examples. The\nfirst example is a count question, where the LM\nseems to count the entities Rigel returns get to\nthe correct answer. The second example is of a\nmulti-hop question. Of note is that Rigel’s top\nfact is about a high school, but the LM is able to\nrecover and return a college instead. The third\nexample is an ordinal question. Since the Rigel\nfacts do not specify which books are part of the\nseries, the model returns an incorrect answer by\ntrying to stay faithful to the facts given. Relying\non facts given rather than facts in the parameters\ncan be a desirable trait for an LM, however this\nexample highlights that more work needs to be\ndone on improving the KGQA retriever.\n6 Conclusion\nIn this paper, we show how facts from a KGQA\nbased retriever can be combined with a language\nmodel to help answer complex questions. Our re-\nsults show improvements over calling a language\nmodel directly over four datasets, and in particular\non complexity types such as multi-hop and count\nquestions. We present our method as a promising\nway to leverage a knowledge graph, which con-\n6\ntains verified and up-to-date facts, with a single\ncall to a language model. In future work, we plan\nto improve performance across more complexity\ntypes and aim to explore ways to update the train-\ning of our KGQA retriever with feedback from the\nlanguage model.\n7 Limitations\nWe present a method for question answering using\na KGQA retriever and a language model reasoner.\nLimitations of our method include a lack of an\nintegrated entity resolution system when training\nour KGQA model: we instead rely on annotated\nentities from the datasets. While our KGQA archi-\ntecture is robust to new entities added at test time,\nit does require retraining when new relations are\nadded to the KG or if a different target KG is used.\nAdditionally, our results are based on training and\nevaluating on one dataset at a time; training on a\nmix of datasets could lead to better generalization,\nhowever this is not tested.\nReferences\nJinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023.\nKnowledge-augmented language model prompting\nwith knowledge graphs for zero-shot question an-\nswering. In Proceedings of the First Workshop on\nMatching, Toronto, Canada. Association for Compu-\ntational Linguistics.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533–1544.\nNilesh Chakraborty, Denis Lukovnikov, Gaurav Ma-\nheshwari, Priyansh Trivedi, Jens Lehmann, and Asja\nFischer. 2019. Introduction to neural network based\napproaches for question answering over knowledge\ngraphs. arXiv preprint arXiv:1907.09361.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nWilliam W. Cohen, Haitian Sun, R. Alex Hofer, and\nMatthew Siegler. 2020. Scalable neural methods for\nreasoning with a symbolic knowledge base. In Inter-\nnational Conference on Learning Representations.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nBin Fu, Yunqi Qiu, Chengguang Tang, Yang Li,\nHaiyang Yu, and Jian Sun. 2020. A survey on\ncomplex question answering over knowledge base:\nRecent advances and challenges. arXiv preprint\narXiv:2007.13069.\nSylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp\nSchmid, Zachary Mueller, and Sourab Mangrulkar.\n2022. Accelerate: Training and inference at scale\nmade simple, efficient and adaptable. https://\ngithub.com/huggingface/accelerate.\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and\nSung Ju Hwang. 2022. Knowledge-consistent di-\nalogue generation with knowledge graphs. In ICML\n2022 Workshop on Knowledge Retrieval and Lan-\nguage Models.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\narXiv preprint arXiv:2203.05115.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Empirical Methods in\nNatural Language Processing (EMNLP).\nAmir Saffari, Armin Oliya, Priyanka Sen, and Tom Ay-\noola. 2021. End-to-end entity resolution and question\nanswering using differentiable knowledge graphs.\nIn Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n4193–4200, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\n7\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nPriyanka Sen, Alham Fikri Aji, and Amir Saffari.\n2022. Mintaka: A complex, natural, and multilin-\ngual dataset for end-to-end question answering. In\nProceedings of the 29th International Conference\non Computational Linguistics, pages 1604–1619,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nPriyanka Sen, Armin Oliya, and Amir Saffari. 2021.\nExpanding end-to-end question answering on differ-\nentiable knowledge graphs with intersection. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 8805–\n8812, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGer-\nald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\nPeris, Stephen Rawls, Andy Rosenbaum, Anna\nRumshisky, et al. 2022. Alexatm 20b: Few-shot\nlearning using a large-scale multilingual seq2seq\nmodel. arXiv preprint arXiv:2208.01448.\nHaitian Sun, Andrew Arnold, Tania Bedrax Weiss, Fer-\nnando Pereira, and William W. Cohen. 2020. Faithful\nembeddings for knowledge base queries. Advances\nin Neural Information Processing Systems, 33.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\narXiv preprint arXiv:1803.06643.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\n8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7451769709587097
    },
    {
      "name": "Question answering",
      "score": 0.7101155519485474
    },
    {
      "name": "Language model",
      "score": 0.6506177186965942
    },
    {
      "name": "Graph",
      "score": 0.5326523184776306
    },
    {
      "name": "Knowledge graph",
      "score": 0.5226002335548401
    },
    {
      "name": "Natural language processing",
      "score": 0.5211912989616394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49152883887290955
    },
    {
      "name": "Intersection (aeronautics)",
      "score": 0.4541844129562378
    },
    {
      "name": "Theoretical computer science",
      "score": 0.24773728847503662
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210123934",
      "name": "Amazon (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 17
}