{
  "title": "Reducing Sequence Length by Predicting Edit Spans with Large Language Models",
  "url": "https://openalex.org/W4389523685",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2000996583",
      "name": "Masahiro Kaneko",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2030501650",
      "name": "Naoaki Okazaki",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963045354",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4288289156",
    "https://openalex.org/W4287725215",
    "https://openalex.org/W3104814493",
    "https://openalex.org/W4385574295",
    "https://openalex.org/W3037162118",
    "https://openalex.org/W4287762561",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W4361019538",
    "https://openalex.org/W2153013403",
    "https://openalex.org/W4287779338",
    "https://openalex.org/W2109802560",
    "https://openalex.org/W4321854628",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3197133407",
    "https://openalex.org/W4293361313",
    "https://openalex.org/W3040809437",
    "https://openalex.org/W2877825950",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385573126",
    "https://openalex.org/W3102273025",
    "https://openalex.org/W4366115126",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W4362655849",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3034639488",
    "https://openalex.org/W4367369802",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3114463723",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W3175441946",
    "https://openalex.org/W4285227169",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3102692503",
    "https://openalex.org/W3035010485",
    "https://openalex.org/W4285600138",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4389521054",
    "https://openalex.org/W2470595162",
    "https://openalex.org/W2891760559",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4385571182",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4221161867",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3099852471",
    "https://openalex.org/W3153637831",
    "https://openalex.org/W4288826774",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2605243085",
    "https://openalex.org/W2554764206",
    "https://openalex.org/W4385572879",
    "https://openalex.org/W4287855135",
    "https://openalex.org/W3125498921",
    "https://openalex.org/W4386973859",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3156246620",
    "https://openalex.org/W4385571208",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W3176400576",
    "https://openalex.org/W2970429618",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10017–10029\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nReducing Sequence Length by Predicting Edit Spans with Large Language\nModels\nMasahiro Kaneko1,2 Naoaki Okazaki2\n1MBZUAI\n2Tokyo Institute of Technology\nMasahiro.Kaneko@mbzuai.ac.ae okazaki@c.titech.ac.jp\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable performance in various\ntasks and gained significant attention. LLMs\nare also used for local sequence transduction\ntasks, including grammatical error correction\n(GEC) and formality style transfer, where most\ntokens in a source text are kept unchanged.\nHowever, the models that generate all target\ntokens in such tasks have a tendency to sim-\nply copy the input text as is, without mak-\ning needed changes, because the difference\nbetween input and output texts is minimal in\nthe training data. This is also inefficient be-\ncause the computational cost grows quadrat-\nically with the target sequence length with\nTransformer. This paper proposes predicting\nedit spans for the source text for local sequence\ntransduction tasks. Representing an edit span\nwith a position of the source text and corrected\ntokens, we can reduce the length of the target\nsequence and the computational cost for infer-\nence. We apply instruction tuning for LLMs\non the supervision data of edit spans. Experi-\nments show that the proposed method achieves\ncomparable performance to the baseline in four\ntasks, paraphrasing, formality style transfer,\nGEC, and text simplification, despite reducing\nthe length of the target text by as small as 21%.\nFurthermore, we report that the task-specific\nfine-tuning with the proposed method achieved\nstate-of-the-art performance in the four tasks.\n1 Introduction\nLarge Language Models (LLMs), including Chat-\nGPT1 and Bard2, have exhibited exceptional per-\nformance across a range of natural language pro-\ncessing (NLP) tasks and amassed a significant user\nbase (Brown et al., 2020; Chowdhery et al., 2022;\nOpenAI, 2023). As performance gains are brought\nfrom the increases in model size (Kaplan et al.,\n2020; Wei et al., 2022; Zhao et al., 2023), LLMs\n1https://chat.openai.com/\n2https://bard.google.com/\nare becoming larger and larger. However, the com-\nputational cost of inference is a severe bottleneck\nof many practical applications, especially when the\nnumber of parameters in an LLM is massive (Ben-\nder et al., 2021; Kraus et al., 2023).\nMeanwhile, LLMs are also used for local se-\nquence transduction tasks, such as paraphrasing,\nformality style transfer, Grammatical Error Cor-\nrection (GEC), and simplification (Kaneko et al.,\n2022; Reif et al., 2022; Wu et al., 2023a; Wang\net al., 2022; Kaneko and Okazaki, 2023), where\nonly a small portion of the source text is edited.\nMost tokens in a source text are kept unchanged in\nthese tasks. For example, the source text, “Many\nyears ago, the situationis different, ”and the target\ntext, “Many years ago, the situationwas different, ”\nof the GEC task mostly share the common tokens\nexcept for the underlined tokens (is and was).\nExisting methods of downstream tasks do not\nmake use of the characteristics of local sequence\ntransduction (Reif et al., 2022; Wu et al., 2023a;\nWang et al., 2022), simply generating all target\ntokens. In this paper, we hypothesize that this treat-\nment is disadvantageous in achieving high perfor-\nmance in terms of task accuracy and computational\ntime. More specifically, it is inefficient to generate\nunchanged tokens (e.g. Many, years, ago, the, sit-\nuation, different) in the previous example because\nthe model must copy many source tokens only to\nincrease the length of the target sequence.\nThis study proposes to predict a set of edit spans,\nwhich represent the changed parts of the target text\nrelative to the source tokens. Omitting unedited\ntokens that occupy most of the target text, we can\nreduce the length of the target text and the infer-\nence time for local sequence transduction tasks.\nFigure 1 shows the process of creating a set of edit\nspans from source and target texts in GEC. First,\nwe align tokens in the source and target texts to\nextract the edit locations and tokens and convert\nthem into a set of edit spans. In the example shown\n10017\nFigure 1: Inference of instruction tuned LLMs using edit spans. LLMs take instruction text and source text as input\nand output only the positions and tokens for rewriting. Rule-based conversion applies the outputted positions and\ntokens of the rewriting to the source text and produces the plaintext output.\nin Figure 1, the edit spans (1, 1, “the”), (8, 9, “have\nbeen”), (12, 13, “”) are created from the source\ntext “Through thousands of years, most Chinese\nscholars are greatly affected bythe Confucianism. ”\nand the target text“Through the thousands of years,\nmost Chinese scholarshave beengreatly affected\nby Confucianism. ”. LLMs are fine-tuned using\npairs of source text and edit spans with the instruc-\ntions.\nWe conducted experiments on four local se-\nquence transduction tasks: paraphrasing, formality\nstyle transfer, GEC, and simplification. The pro-\nposed method achieved comparable performance to\nthe baseline that directly outputs the target text. In\nthese tasks, the proposed method could reduce the\nsequence length on the target side by 32% on aver-\nage and by as small as 21% in GEC. Furthermore,\nthe proposed method with task-specific fine-tuning\nachieved state-of-the-art (SoTA) performance in\nthe four tasks.\n2 Edit Spans\n2.1 Edit Span Extraction\nTo extract the editing locations and results of the\nsource and target texts, we calculate the alignment\nbetween the tokens in each text. We use linguis-\ntical alignment, which incorporates linguistic in-\nformation, to perform the alignment (Felice et al.,\n2016). Linguistical alignment is a method based\non the Damerau-Levenshtein algorithm that aligns\ntokens by considering not only the distance be-\ntween tokens but also the match of their lemma,\npart-of-speech, and character features, weighted\naccordingly. Taking into account the linguistic in-\nformation of tokens, linguistical alignment is more\naccurate compared to alignment methods that only\nuse surface information. Furthermore, linguistic\nalignment merges token alignments using recursive\nrules to create alignments for multiple tokens, such\nas “have been”in Figure 1.\nTo indicate the edit position identified by the\nalignment, a 0 is assigned before the first token\nof the source text, and an index is sequentially\nassigned to the space after each token. When the\nlength of the source text is N, N is assigned after\nthe last token. The edit span is represented by the\ntuple of the start position of the source text, the\nend position of the source text, and the token result\nafter being edited.\nThere are three types of edit operations: insert,\nreplace, and delete; we explain them using the ex-\nample in Figure 1. The tuple (1, 1, “the”) represents\nthe operation to insert “the”. In an insertion oper-\nation, both the start and end positions are set to\nthe same position where a token is inserted in the\nsource. The tuple stands for inserting “ the” be-\ntween the tokens located at the 1st position. The\ntuple (8, 9, “have been”) presents the operation to\nreplace “are” with “have been”. By specifying the\n8th and 9th positions of the source text, this tuple\ntargets the “are” and rewrites them as “have been”.\nThe tuple (12, 13, “”) represents the operation to\ndelete “the”. It points “the” by specifying the 12th\nand 13th positions in the source text. Because the\ntarget token after this edit operation is empty, this\ntuple corresponds to removing “the”.\n2.2 Instruction Tuning with Edit Spans\nInstruction tuning fine-tunes LLMs by using nat-\nural language instructions describing a task (Wei\n10018\net al., 2021). Compared to the conventional fine-\ntuning that specializes the model for a specific task,\ninstruction tuning aims for generalization to var-\nious tasks by training LLMs to respond well to\nmany kinds of instructions. Therefore, instruction\ntuning is used for training many LLMs in an open-\nended setting (Ouyang et al., 2022; Chung et al.,\n2022; Wang et al., 2022; Wu et al., 2023b). We\nuse the description of local sequence transduction\ntasks as instructions to perform instruction tuning\nof LLMs. We provide the LLMs with instructions\nand source text, and train the LLMs to generate\nedit spans. When there are multiple edits, they are\nconcatenated with commas like “1 1 the, 8 9 have\nbeen, 12 13”. When no editing is required in the\nsource text, “None” is given as the gold text.\nRecent LLMs are expected to have the ability to\nhandle unknown tasks and various tasks, to achieve\ngenerality. It is important that learning through\nedit spans does not degrade the performance of\ntasks other than local sequence transduction tasks.\nTherefore, we add edit span data to the existing\ntraining data for instruction tuning, which includes\nvarious tasks, and fine-tune LLMs.\n2.3 Conversion from Edit Spans to Output\nText\nTo convert the edit spans output by LLMs into\nplaintext, we use a rule-based approach. If LLMs\ngenerate “None”, we use the source text as the\nfinal output text. Otherwise, we split the edit spans\nby commas and extract the edits. From each edit,\nwe extract the starting position, ending position,\nand edited result. If LLMs generate edits in an\nincorrect format that do not include start or end\npositions or edits where the start or end positions\nexceed the source text range, we ignore them. To\nensure that the token indices do not shift, we apply\nthe edits to the source text in descending order of\nstarting positions. This conversion is implemented\nby simple rules with a minimal computational cost.\n3 Experiment Setting\n3.1 Local Sequence Transduction Taskes\nWe conducted experiments on local sequence trans-\nduction tasks such as GEC, paraphrasing, formality\nstyle transfer, and simplification.\nGEC We used NUCLE as the training data,\nCoNLL2013 (Ng et al., 2013) as the development\ndata, and CoNLL2014 (Ng et al., 2014) as the evalu-\nation data. The dataset is comprised of essays com-\nposed by college students from the National Uni-\nversity of Singapore, covering a broad spectrum of\nsubjects, including environmental pollution, health-\ncare, and more. We used the M2 score (Dahlmeier\nand Ng, 2012) as the evaluation metric. For GEC,\nwe provide the instruction text “Rewrite the input\ntext into grammatically correct text. ”.\nParaphrasing Quora published a dataset that in-\ncludes more than 400K lines of potential question\nduplicate pairs3. Of these pairs, 150K question\npairs were labeled as paraphrases. Only those la-\nbeled paraphrase question pairs are used as training,\ndevelopment, and test sets. We used BLEU-4 (Pap-\nineni et al., 2002), ROUGE-1, and ROUGE-2 (Lin,\n2004) to evaluate LLMs, following previous re-\nsearch (Kumar et al., 2020; Meng et al., 2021; Li\net al., 2022). For paraphrasing, we provide the\ninstruction text “Rewrite the input text into para-\nphrased text. ”\nStyle transfer We used FST benchmark Gram-\nmarly Yahoo Answers Corpus (GYAFC) (Rao\nand Tetreault, 2018) for formality style transfer.\nGY AFC is a plain corpus that contains pairs of in-\nformal and formal sentences conveying the same\nmeaning. It covers domains such as Entertainment\n& Music (E&M) and Family & Relationship (F&R).\nWe utilized the corpus BLEU in NLTK (Bird and\nLoper, 2004) as described in Chawla and Yang\n(2020). For formality style transfer, we provide the\ninstruction text “Rewrite the input text into formal\ntext. ”\nSimplification We used WikiSmall4 (Zhu et al.,\n2010; Zhang and Lapata, 2017) as the training\ndata and ASSET (Alva-Manchego et al., 2020) and\nTurkCorpus (Xu et al., 2016) as the evaluation data.\nWe used SARI (Xu et al., 2016) to evaluate LLMs,\nwhich compares the generated text with the target\ntext and calculates the average F1 score for addi-\ntion, keep, and deletion operations. For text simpli-\nfication, we provide the instruction text “Rewrite\nthe input text into simpler text. ”\n3.2 Open-ended Tasks\nThe rules of edit spans differ from the rules in the\nraw text, which could potentially have a negative\nimpact on the performance of tasks other than local\n3https://www.kaggle.com/c/\nquora-question-pairs\n4https://github.com/XingxingZhang/dress\n10019\nsequence transduction. By combining open-ended\ninstruction tuning data and edit spans instruction\ntuning data, we can train LLMs and investigate\ntheir impact on other tasks as well.\nWe utilize the databricks-dolly-15k dataset5 by\nrandomly dividing it into 13K for training, 1K for\ndevelopment, and 1K for evaluation. databricks-\ndolly-15k is a publicly available dataset consist-\ning of instructional records created by numerous\nDatabricks employees. It covers various behavioral\ncategories described in InstructGPT (Ouyang et al.,\n2022), such as brainstorming, classification, closed\nQA, generation, information extraction, open QA,\nand summarization. We sampled 3K instances for\neach of the tasks: GEC, paraphrasing, style trans-\nfer, and simplification, resulting in a total of 12K\ninstruction instances. We fine-tuned LLMs using a\ncombined dataset of all these instructions, totaling\n25K instances.\nWe used BERTScore6 (Zhang et al., 2019) as\nour evaluation metric. BERTScore is an evalua-\ntion method that measures the similarity between\ngenerated text and target text using contextual em-\nbeddings from pre-trained models. We utilized\nRoBERTa (Liu et al., 2019) (roberta-large7) as the\nBERTScore models.\n3.3 Instruction Tuning Settings\nWe used the following four LLMs for our ex-\nperiments: MPT (mpt-7b) 8 (Team, 2023), OPT\n(opt-6.7b)9 (Zhang et al., 2022), LLaMA (llama-\n7b)10 (Touvron et al., 2023), and BLOOM (bloom-\n7b1)11 (Scao et al., 2022).\nWe used the code for instruction tuning from\nStanford Alpaca (Taori et al., 2023) code12 for in-\nstruction tuning. We set the number of epochs to\n3 and used a batch size of 32. The learning rate\nwas set to 2e-5, with a warmup rate of 0.03, and we\nemployed a cosine learning rate schedule. These\nhyperparameters were determined following Stan-\nford Alpaca. We report the average results of three\nmodels trained with different seeds for instruction\ntuning. We used four nodes, each containing eight\n5https://huggingface.co/datasets/\ndatabricks/databricks-dolly-15k/viewer/\ndatabricks--databricks-dolly-15k\n6https://github.com/Tiiiger/bert_score\n7https://huggingface.co/roberta-large\n8https://huggingface.co/mosaicml/mpt-7b\n9https://huggingface.co/facebook/opt-6.7b\n10https://github.com/facebookresearch/llama\n11https://huggingface.co/bigscience/bloom-7b1\n12https://github.com/tatsu-lab/stanford_alpaca\nNVIDIA A100 GPUs. We used the code13 for lin-\nguistical alignment provided by Felice et al. (2016).\nBaselines We compare the results of the pro-\nposed method with the results of LLMs fine-tuned\nfor instruction tuning using the target text as the\nground truth instead of using edit spans. This com-\nparison examines whether edit spans can reduce\ncomputational costs during inference without com-\npromising performance.\n4 Experiment\n4.1 Performance on Local Sequence\nTransduction Tasks\nTo demonstrate the contribution of edit spans to\nperformance improvement, we first compare the\nbaseline performance with fine-tuned data using\nplain text. Table 1 shows the results of performance\ncomparison between the baseline and the proposed\nmethod in the GEC, paraphrasing, style transfer,\nand simplification tasks. Out of 32 cases, perfor-\nmance improvement was observed in 19 cases, and\nedit spans contributed to the performance enhance-\nment. Furthermore, it can be observed that the\nLLaMA trained with edit spans achieves the high-\nest performance in most cases.\n4.2 Reducing Text Length\nWe examine how much the fine-tuning of LLMs\nwith edit span data reduced the length of the output\ntext. Figure 2 shows the ratio of output text length\nto target text length when fine-tuned with plain\ndata and edit span data, respectively, on the devel-\nopment data for each task. The proposed method\nsuccessfully compresses the output text across all\ntasks, independent of the model used; it achieves\ntext compression in the range of 21% in the most\ncompressed cases and 41% even in the least com-\npressed cases. In GEC, there are cases where gram-\nmatically correct text is provided as source text. In\nsuch cases, the model does not need to make any\nrevisions and can simply output “None”, resulting\nin significant compression in GEC.\n4.3 Performance on Open-ended Task\nIn open-ended tasks, the target texts are written in\nplain text, while edit spans introduce significant dif-\nferences in text formatting. This misalignment in\ntext representation may potentially impact the per-\nformance of open-ended tasks. Therefore, we aim\n13https://github.com/chrisjbryant/errant\n10020\nGEC Paraphrasing Style transfer Simplification\nPlain\nMPT 68.0 37.9/66.5/47.1 78.9/81.2 46.3/41.1\nOPT 65.7 35.2/63.2/45.4 75.0/77.2 43.7/40.5\nLLaMA 68.2 39.3/69.0/47.2 79.5/81.0 48.0/41.9\nBLOOM 66.4 37.0/66.4/46.1 78.2/79.9 45.0/41.0\nEdit spans\nMPT 68.5 38.2/66.7/47.1 78.2/81.3 46.6/41.3\nOPT 66.2 34.1/61.2/43.9 75.6 /77.9 43.9/40.3\nLLaMA 69.1 39.0/69.2/47.6 79.3/81.2 48.3/42.0\nBLOOM 65.8 37.2 /66.1/46.3 78.0/80.3 44.8/40.7\nTable 1: The performance of four LLMs fine-tuned with edit spans and plain data instructions on four local sequence\ntransduction tasks. The bold values indicate the highest performance for each task. The underlined values indicate\nwhen edit spans exceed the baseline.\n(a) MPT\n (b) OPT\n(c) LLaMA\n (d) BLOOM\nFigure 2: The ratio of output text length to target text length when MPT, OPT, LLaMA, and BLOOM are fine-tuned\nwith plain data and edit span data, respectively.\nto demonstrate that edit spans do not significantly\ndegrade the performance of open-ended tasks.\nTable 2 shows the scores for each LLM when\nusing RoBERTa as BERTScore models on the 1K\nsubset of the databricks-dolly-15k dataset, which\nwas divided for evaluation. This indicates that the\nproposed method achieves efficient computational\ncost during inference without significantly sacrific-\ning open-ended task performance.\nTo maintain performance in open-ended tasks,\nthe proposed method combines data from both lo-\ncal sequence transduction tasks and open-ended\ntasks. To demonstrate the effectiveness of com-\nbining open-ended task data, we also investigate\nthe open-ended task performance of instruction-\ntuned LLMs when solely trained on local sequence\ntransduction task data.\nTable 3 demonstrates the performance difference\non the 1K split of the databricks-dolly-15k dataset,\nevaluating LLMs trained on both open-ended task\nand local sequence transduction task data versus\nLLMs trained solely on local sequence transduction\ntask data. The performance decreases when not us-\ning open-ended task data for training, both in terms\nof plain text and edit spans. This is likely because\nopen-ended task data consists of plain text, while\nedit spans include totally different text formats,\nleading to a larger disparity in task requirements.\n10021\nBERTScore\nPlain\nMPT 81.5\nOPT 79.3\nLLaMA 81.8\nBLOOM 79.9\nEdit span\nMPT 81.0\nOPT 78.6\nLLaMA 81.3\nBLOOM 79.5\nTable 2: Scores using BERTScore on the databricks-\ndolly-15k dataset, which was divided for evaluation.\nBERTScore diff.\nPlain\nMPT -5.2\nOPT -5.7\nLLaMA -4.4\nBLOOM -6.2\nEdit span\nMPT -8.1\nOPT -8.6\nLLaMA -6.9\nBLOOM -7.6\nTable 3: The performance difference between instruc-\ntion tuned LLMs using local sequence transduction task\nand open-ended task datasets, and instruction tuned\nLLMs using only local sequence transduction task\ndatasets.\n4.4 The Accuracy of Edits Generated by the\nLLMs\nEven if the edit span text is different, there are\ncases where the text is transformed by the rule,\nand the text matches. For example, in GEC, the\nmodel is given the input “This technology could\nalso be seen as invasion of human privacy. ”and\nthe model outputs “7 9 invading”. In this case,\neven with the alternate edit span text “7 8 invading,\n8 9”, the conversion based on the rules would re-\nsult in the same output text. However, this would\nincrease the sentence length by the index, creating\nroom for improvement in terms of computational\ncost. Therefore, we investigate how well edit span\nof the model matches the results using linguistic\nalignment.\nFirst, we convert the edit spans generated by the\nmodel to plain text using rules. From the converted\nplain text and the source text, we create edit spans\nusing linguistic alignment and calculate the per-\ncentage of agreement with the edit spans output\nby the model. Only when the start position s, end\nposition e, and the edit token r all match exactly is\nit considered a correct answer.\nTable 4 shows the percentage of agreement be-\ntween the edit spans output by the LLMs and the\nedit spans extracted by the linguistic alignment in\nthe development data for each task. The proposed\nmethod achieves more than 90% agreement in 13\nout of 16 settings. This indicates that LLMs are\nable to learn the extraction rules for linguistical\nalignment through instruction tuning.\n4.5 Task-specific Fine-tuning\nIn the previous experiments, LLMs were trained\nby combining data from the four local sequence\ntransduction tasks and the open-ended task. To\nexplore the maximum potential performance of the\nproposed method, we fine-tune LLMs with task-\nspecific focus using edit span data. We fine-tune\nLLMs for each task using all available training data.\nIn this case, we specialize LLMs for specific tasks\nwithout the need for instruction texts. Therefore,\nwe trained the LLMs by providing only the source\ntexts as input.\nWe trained LLaMA, which showed the high-\nest performance in the local sequence transduction\ntasks. We set the number of epochs to 2 and used a\nbatch size of 32. The learning rate was set to 1e-5,\nwith a warmup rate of 0.03, and we employed a\ncosine learning rate schedule. Following the explo-\nration method described in Section 3.3, we deter-\nmined the hyperparameters for our experiments.\nTable 5 shows the results of performance com-\nparison with existing studies on GEC, paraphras-\ning, style transfer, and simplification tasks. The\nproposed method outperforms existing studies by\n1.8 points in GEC, 0.9, 1.2, and 2.3 points in para-\nphrasing, 1.9 and 1.3 points in style transfer, and\n1.2 and 0.7 points in simplification tasks, respec-\ntively. Thus, the proposed method achieves the\nSoTA performance in all tasks. From these results,\nit can be concluded that edit spans are an effective\nmethod, even in task-specific fine-tuning scenarios.\n4.6 Example of LLMs Output Using Edit\nSpans\nTable 6 shows the output in CoNLL2013 for\nLLaMA using edit span and LLaMA outputting\nplain text. The normal model outputting plain text\noutputs 23 tokens, while the model using edit span\noutputs only 3 tokens. The output of the model\nusing the edit span is a much shorter sequence than\nthe original model that outputs plain text. Fur-\nthermore, LLaMA, which outputs in plain text, is\nunable to correct a grammatical error. In a local se-\nquence transduction task, most tokens in the source\n10022\nGEC Paraphrasing Style transfer Simplification\nMPT 96.6 95.0 89.2 94.7\nOPT 93.3 91.9 88.8 92.7\nLLaMA 99.0 96.2 92.6 95.4\nBLOOM 94.2 92.5 89.4 93.5\nTable 4: Agreement between edit spans generated by LLMs and edit spans extracted by linguistic alignment.\nGEC\n(Kaneko et al., 2020) 65.2\n(Omelianchuk et al., 2020) 66.5\n(Qorib et al., 2022) 69.5\nEdit span 71.3\n(a) M2 scores on the CoNLL2014 dataset.\nParaphrasing\n(Kumar et al., 2020) 38.0/68.1/45.7\n(Meng et al., 2021) 26.8/65.0/38.5\n(Li et al., 2022) 39.3/70.8/48.3\nEdit span 41.2/72.0/50.6\n(b) BLEU-4, ROUGE-1, and ROUGE-2 scores on\nthe Quora dataset.\nStyle transfer\n(Chawla and Yang, 2020) 76.2/79.9\n(Lai et al., 2021) 76.5/79.3\n(Liu et al., 2022) 78.8/81.4\nEdit span 80.7/82.7\n(c) NLTK BLEU scores on the E&M and F&R\ndatasets.\nSimplification\n(Martin et al., 2020) 40.1/41.4\n(Martin et al., 2022) 44.2/42.6\n(Feng et al., 2023a) 47.9/41.8\nEdit span 49.1/43.5\n(d) SARI scores on ASSET and TurkCorpus\ndatasets.\nTable 5: Performance comparison with previous studies on GEC, paraphrasing, style transfer, and simplification\ntasks.\ntext and target text are common, and the model\ntends to learn just to copy the input tokens (Rastogi\net al., 2016). Contrarily, our model that uses edit\nspans outputs only the edited parts. Thus simply\ncopying the input is not an issue for our model.\n5 Related Work\n5.1 Efficient LLMs\nMost of the methods for achieving efficient LLMs\ninvolve improving the memory complexity of self-\nattention mechanisms or enhancing the overall effi-\nciency of the Transformer architecture (Tay et al.,\n2022; Loem et al., 2022). In the initial stages, the\nmodifications made to self-attention focused on\nreducing the computational complexity by intro-\nducing sparsity in the attention matrix. This was\naccomplished by restricting the attention’s scope\nto predetermined patterns, such as local windows\nand fixed stride block patterns (Liu et al., 2018;\nQiu et al., 2020; Beltagy et al., 2020). A natural\nextension to the blockwise method is to connect\nthese blocks via recurrence. Dai et al. (2019) in-\ntroduced a mechanism of segment-level recurrence\nthat establishes connections among multiple seg-\nments and blocks.\nAn expansion upon fixed, predetermined pat-\nterns is the utilization of learnable patterns. Mod-\nels that incorporate learnable patterns aim to ac-\nquire the access pattern through data-driven meth-\nods. One crucial aspect of learning patterns is to\nestablish a concept of token relevance and subse-\nquently assign tokens to buckets or clusters (Vyas\net al., 2020; Wang et al., 2021; Kitaev et al., 2020;\nTay et al., 2020; Roy et al., 2021).\nAnother approach is to utilize a trainable side\nmemory module capable of accessing multiple\ntokens simultaneously (Sukhbaatar et al., 2019;\nAinslie et al., 2020; Beltagy et al., 2020). A preva-\nlent example is the global neural memory, which\ncan access the entire sequence. The global tokens\nfunction as a type of model memory, learning to\ngather information from the input sequence tokens.\nAnother method to enhance efficiency is by\nutilizing low-rank approximations of the self-\nattention matrix to improve computational perfor-\nmance (Wang et al., 2020), and to view the atten-\ntion mechanism through kernelization (Choroman-\nski et al., 2020; Peng et al., 2021). Sparse models\nselectively activate a fraction of the parameters, re-\nsulting in an improved parameter to FLOPs ratio in\ngeneral (Fedus et al., 2022).\nAs a way to reduce the length of the text, Cheng\net al. (2023) proposed including multiple examples\n10023\nSource text Since we do not to bring cash to pay for the transportation fee , enormous time has been saved\nfor everybody .\nTarget text Since we do not need to bring cash to pay for the transportation fee , enormous time has been saved\nfor everybody .\nTarget edit span 4 4 need\nPlain Since we do not to bring cash to pay for the transportation fee , enormous time has been saved\nfor everybody .\nSystem edit span 4 4 need\nTable 6: Outputs in plain text and edit span formats respectively by LLaMA in the CoNLL2013.\nin one prompt and inferring in parallel.\nThese techniques, unlike our research, do not al-\nter the writing style of the target text, and edit spans\ncan be used in conjunction with these methods.\n5.2 Edit-based Model\nSince the question of necessarily using the seq2seq\nmodel for local sequence transduction tasks was\nraised (Rastogi et al., 2016; Schnober et al., 2016),\nvarious edit-based models have been proposed.\nGuu et al. (2018) proposed a language model that\ninitially selects a prototype sentence from the train-\ning dataset and subsequently modifies it to create\na new sentence. Ribeiro et al. (2018) introduced\na method for representing general string transduc-\ntion problems as sequence labeling. Koide et al.\n(2018) proposed the model implemented to ana-\nlyze the evolution of biological sequences driven\nby substitution, insertion, and deletion edit opera-\ntions, achieving improved accuracy on protein sec-\nondary structure prediction. Awasthi et al. (2019)\npresented a parallel iterative edit model reducing\ndecoding time for local sequence transduction tasks.\nGu et al. (2019) developed the Levenshtein Trans-\nformer, a non-autoregressive model using edit oper-\nations. (Mallinson et al., 2020) introduced FELIX,\nan adaptable text-editing approach for the genera-\ntion that aims to leverage the advantages of decod-\ning with bi-directional contexts and self-supervised\npretraining to the fullest extent. (Xu and Carpuat,\n2021) presented an Edit-Based Transformer with\nRepositioning, which enhances sequence genera-\ntion flexibility by seamlessly incorporating user-\nspecified preferences in output lexical choice. Reid\nand Neubig (2022) proposed the modeling of edit-\ning processes, encompassing the iterative gener-\nation of sequences as a whole. They establish a\nconceptual framework to explain the probability\nof multi-step edits and outline neural models capa-\nble of learning a generative model of sequences by\nleveraging these multi-step edits.\nHowever, these methods have different architec-\ntures from LLMs. Therefore, it is not easy to apply\nthem to LLMs, unlike our method, which can train\nmodels by simply changing the output text.\n5.3 LLMs for Local Sequence Transduction\nTasks\nIn GEC, the model based on GPT-3 achieves state-\nof-the-art in unsupervised settings (Loem et al.,\n2023). Fang et al. (2023) showed that ChatGPT\ncorrects input text very fluently. Yamashita et al.\n(2020); Rothe et al. (2021); Sun et al. (2022) pro-\nposed a method for multilingual GEC using multi-\nlingual LLMs. Feng et al. (2023b) investigated the\nperformance of few-shot and zero-shot of GPT3\nand ChatGPT in the simplification. Anschütz et al.\n(2023) used LLMs for German simplification and\nfound them to be effective in languages with little\nparallel data. (Witteveen and Andrews, 2019) ver-\nified the performance of GPT-2 (Radford et al.,\n2019) in paraphrasing. Wahle et al. (2022) in-\nvestigated the utilization of T5 and GPT3 in gen-\nerating machine-generated paraphrases for scien-\ntific articles sourced from arXiv, student theses,\nand Wikipedia. Reif et al. (2022) introduced a\nmethod based on GPT-3 that solely relies on nat-\nural language instruction and does not necessitate\nmodel fine-tuning or exemplars in the desired style.\nMalmi et al. (2020) proposed a method of using\nLLMs for style transfer where no parallel data is\navailable. On the other hand, these studies did not\ntarget the efficiency of LLMs based on the edit.\n6 Conclusion\nIn this study, we proposed to predict a set of edit\nspans, which represent the changed parts of the\ntarget text relative to the source tokens. We showed\nour method omits unedited tokens that occupy most\nof the target text and reduces the length of the tar-\nget text and the inference time for local sequence\ntransduction tasks. Moreover, we reported that in-\nstruction tuning with the proposed method achieves\n10024\nstate-of-the-art performance in the four tasks.\nLimitations\nIn our preliminary experiments, even high-\nperformance LLMs such as GPT-3 (Brown et al.,\n2020) and ChatGPT (OpenAI, 2023) could not gen-\nerate edit spans with zero-shot and few-shot. In\nparticular, indexes could not be generated correctly.\nTherefore, it is a future work to apply the proposed\nmethod to zero-shot and few-shot. Moreover, the\nuse of edit span is not necessarily effective for tasks,\nsuch as machine translation and dialogue, other\nthan the local sequence transduction task, where\nmany tokens in the source and target texts are not\ncommon.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 268–284, Online. Association\nfor Computational Linguistics.\nFernando Alva-Manchego, Louis Martin, Antoine Bor-\ndes, Carolina Scarton, Benoît Sagot, and Lucia Spe-\ncia. 2020. ASSET: A dataset for tuning and evalua-\ntion of sentence simplification models with multiple\nrewriting transformations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4668–4679, Online. Association\nfor Computational Linguistics.\nMiriam Anschütz, Joshua Oehms, Thomas Wimmer,\nBartłomiej Jezierski, and Georg Groh. 2023. Lan-\nguage models for german text simplification: Over-\ncoming parallel data scarcity through style-specific\npre-training. arXiv preprint arXiv:2305.12908.\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,\nSabyasachi Ghosh, and Vihari Piratla. 2019. Par-\nallel iterative edit models for local sequence trans-\nduction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4260–4270, Hong Kong, China. Association for Com-\nputational Linguistics.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?. In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nSteven Bird and Edward Loper. 2004. NLTK: The natu-\nral language toolkit. In Proceedings of the ACL In-\nteractive Poster and Demonstration Sessions, pages\n214–217, Barcelona, Spain. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nKunal Chawla and Diyi Yang. 2020. Semi-supervised\nformality style transfer using language model dis-\ncriminator and mutual information maximization. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 2340–2354, Online.\nAssociation for Computational Linguistics.\nZhoujun Cheng, Jungo Kasai, and Tao Yu. 2023. Batch\nprompting: Efficient inference with large language\nmodel apis. arXiv preprint arXiv:2301.08721.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\nPeter Hawkins, Jared Davis, David Belanger, Lucy\nColwell, et al. 2020. Masked language modeling for\nproteins via linearly scalable long-context transform-\ners. arXiv preprint arXiv:2006.03555.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better\nevaluation for grammatical error correction. In Pro-\nceedings of the 2012 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n568–572, Montréal, Canada. Association for Compu-\ntational Linguistics.\n10025\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nTao Fang, Shu Yang, Kaixin Lan, Derek F Wong, Jin-\npeng Hu, Lidia S Chao, and Yue Zhang. 2023. Is\nchatgpt a highly fluent grammatical error correction\nsystem? a comprehensive evaluation. arXiv preprint\narXiv:2304.01746.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parame-\nter models with simple and efficient sparsity. The\nJournal of Machine Learning Research, 23(1):5232–\n5270.\nMariano Felice, Christopher Bryant, and Ted Briscoe.\n2016. Automatic extraction of learner errors in ESL\nsentences using linguistically enhanced alignments.\nIn Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 825–835, Osaka, Japan. The\nCOLING 2016 Organizing Committee.\nYutao Feng, Jipeng Qiang, Yun Li, Yunhao Yuan, and\nYi Zhu. 2023a. Sentence simplification via large\nlanguage models. arXiv preprint arXiv:2302.11957.\nYutao Feng, Jipeng Qiang, Yun Li, Yunhao Yuan, and\nYi Zhu. 2023b. Sentence simplification via large\nlanguage models. ArXiv, abs/2302.11957.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Lev-\nenshtein transformer. Advances in Neural Informa-\ntion Processing Systems, 32.\nKelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,\nand Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association\nfor Computational Linguistics, 6:437–450.\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun\nSuzuki, and Kentaro Inui. 2020. Encoder-decoder\nmodels can benefit from pre-trained masked language\nmodels in grammatical error correction. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4248–4254, On-\nline. Association for Computational Linguistics.\nMasahiro Kaneko and Naoaki Okazaki. 2023. Con-\ntrolled generation with prompt insertion for natural\nlanguage explanations in grammatical error correc-\ntion. arXiv preprint arXiv:2309.11439.\nMasahiro Kaneko, Sho Takase, Ayana Niwa, and Naoaki\nOkazaki. 2022. Interpretability for language learners\nusing example-based grammatical error correction.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 7176–7187, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451.\nSatoshi Koide, Keisuke Kawano, and Takuro Kutsuna.\n2018. Neural edit operations for biological se-\nquences. Advances in Neural Information Processing\nSystems, 31.\nMathias Kraus, Julia Anna Bingler, Markus Leip-\npold, Tobias Schimanski, Chiara Colesanti Senni,\nDominik Stammbach, Saeid Ashraf Vaghefi, and\nNicolas Webersinke. 2023. Enhancing large lan-\nguage models with climate resources. arXiv preprint\narXiv:2304.00116.\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli,\nand Partha Talukdar. 2020. Syntax-Guided Con-\ntrolled Generation of Paraphrases. Transactions of\nthe Association for Computational Linguistics, 8:330–\n345.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim. 2021.\nThank you BART! rewarding pre-trained models im-\nproves formality style transfer. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 484–494, Online. Asso-\nciation for Computational Linguistics.\nZhigen Li, Yanmeng Wang, Rizhao Fan, Ye Wang, Jian-\nfeng Li, and Shaojun Wang. 2022. Learning to adapt\nto low-resource paraphrase generation. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 1014–1022,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAo Liu, An Wang, and Naoaki Okazaki. 2022. Semi-\nsupervised formality style transfer with consistency\ntraining. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4689–4701, Dublin,\nIreland. Association for Computational Linguistics.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by\nsummarizing long sequences. arXiv preprint\narXiv:1801.10198.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\n10026\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMengsay Loem, Masahiro Kaneko, Sho Takase, and\nNaoaki Okazaki. 2023. Exploring effectiveness of\ngpt-3 in grammatical error correction: A study on per-\nformance and controllability in prompt-based meth-\nods. arXiv preprint arXiv:2305.18156.\nMengsay Loem, Sho Takase, Masahiro Kaneko, and\nNaoaki Okazaki. 2022. Are neighbors enough?\nmulti-head neural n-gram can be alternative to self-\nattention. arXiv preprint arXiv:2207.13354.\nJonathan Mallinson, Aliaksei Severyn, Eric Malmi, and\nGuillermo Garrido. 2020. FELIX: Flexible text edit-\ning through tagging and insertion. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1244–1255, Online. Association for\nComputational Linguistics.\nEric Malmi, Aliaksei Severyn, and Sascha Rothe. 2020.\nUnsupervised text style transfer with padded masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8671–8680, Online. As-\nsociation for Computational Linguistics.\nLouis Martin, Éric de la Clergerie, Benoît Sagot, and\nAntoine Bordes. 2020. Controllable sentence sim-\nplification. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 4689–\n4698, Marseille, France. European Language Re-\nsources Association.\nLouis Martin, Angela Fan, Éric de la Clergerie, Antoine\nBordes, and Benoît Sagot. 2022. MUSS: Multilin-\ngual unsupervised sentence simplification by mining\nparaphrases. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n1651–1664, Marseille, France. European Language\nResources Association.\nYuxian Meng, Xiang Ao, Qing He, Xiaofei Sun,\nQinghong Han, Fei Wu, Chun Fan, and Jiwei Li.\n2021. ConRPG: Paraphrase generation using con-\ntexts as regularizer. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2551–2562, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task\non grammatical error correction. In Proceedings of\nthe Eighteenth Conference on Computational Natu-\nral Language Learning: Shared Task, pages 1–14,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian\nHadiwinoto, and Joel Tetreault. 2013. The CoNLL-\n2013 shared task on grammatical error correction.\nIn Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning: Shared\nTask, pages 1–12, Sofia, Bulgaria. Association for\nComputational Linguistics.\nKostiantyn Omelianchuk, Vitaliy Atrasevych, Artem\nChernodub, and Oleksandr Skurzhanskyi. 2020.\nGECToR – grammatical error correction: Tag, not\nrewrite. In Proceedings of the Fifteenth Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 163–170, Seattle, W A, USA→\nOnline. Association for Computational Linguistics.\nOpenAI. 2023. Introducing ChatGPT.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A Smith, and Lingpeng Kong.\n2021. Random feature attention. arXiv preprint\narXiv:2103.02143.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise self-\nattention for long document understanding. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 2555–2565, Online. Association\nfor Computational Linguistics.\nMuhammad Qorib, Seung-Hoon Na, and Hwee Tou\nNg. 2022. Frustratingly easy system combination\nfor grammatical error correction. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1964–1974,\nSeattle, United States. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam,\nmay I introduce the GY AFC dataset: Corpus, bench-\nmarks and metrics for formality style transfer. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 129–140, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nPushpendre Rastogi, Ryan Cotterell, and Jason Eisner.\n2016. Weighting finite-state transductions with neu-\nral context. In Proceedings of the 2016 Conference\n10027\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 623–633, San Diego, California.\nAssociation for Computational Linguistics.\nMachel Reid and Graham Neubig. 2022. Learning to\nmodel editing processes. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\npages 3822–3832, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen,\nChris Callison-Burch, and Jason Wei. 2022. A recipe\nfor arbitrary text style transfer with large language\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 837–848, Dublin,\nIreland. Association for Computational Linguistics.\nJoana Ribeiro, Shashi Narayan, Shay B. Cohen, and\nXavier Carreras. 2018. Local string transduction as\nsequence labeling. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 1360–1371, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nSascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-\ntian Krause, and Aliaksei Severyn. 2021. A simple\nrecipe for multilingual grammatical error correction.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 702–707,\nOnline. Association for Computational Linguistics.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics, 9:53–\n68.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nCarsten Schnober, Steffen Eger, Erik-Lân Do Dinh, and\nIryna Gurevych. 2016. Still not there? comparing\ntraditional sequence-to-sequence models to encoder-\ndecoder neural networks on monotone string trans-\nlation tasks. In Proceedings of COLING 2016, the\n26th International Conference on Computational Lin-\nguistics: Technical Papers, pages 1703–1714, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume Lam-\nple, Herve Jegou, and Armand Joulin. 2019. Aug-\nmenting self-attention with persistent memory. arXiv\npreprint arXiv:1907.01470.\nXin Sun, Tao Ge, Shuming Ma, Jingjing Li, Furu Wei,\nand Houfeng Wang. 2022. A unified strategy for\nmultilingual grammatical error correction with pre-\ntrained cross-lingual language model. arXiv preprint\narXiv:2201.10707.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020. Sparse sinkhorn attention.\nIn International Conference on Machine Learning,\npages 9438–9447. PMLR.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComputing Surveys, 55(6):1–28.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, ly usable llms.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nApoorv Vyas, Angelos Katharopoulos, and François\nFleuret. 2020. Fast transformers with clustered at-\ntention. Advances in Neural Information Processing\nSystems, 33:21665–21674.\nJan Philip Wahle, Terry Ruas, Frederic Kirstein, and\nBela Gipp. 2022. How large language models are\ntransforming machine-paraphrase plagiarism. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 952–963,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nShuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun\nChen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing\nLiu. 2021. Cluster-former: Clustering-based sparse\ntransformer for question answering. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 3958–3968, Online. Association\nfor Computational Linguistics.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\n10028\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nSam Witteveen and Martin Andrews. 2019. Paraphras-\ning with large language models. In Proceedings of\nthe 3rd Workshop on Neural Generation and Trans-\nlation, pages 215–220, Hong Kong. Association for\nComputational Linguistics.\nHaoran Wu, Wenxuan Wang, Yuxuan Wan, Wenx-\niang Jiao, and Michael Lyu. 2023a. Chatgpt\nor grammarly? evaluating chatgpt on grammat-\nical error correction benchmark. arXiv preprint\narXiv:2303.13648.\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muham-\nmad Abdul-Mageed, and Alham Fikri Aji. 2023b.\nLamini-lm: A diverse herd of distilled mod-\nels from large-scale instructions. arXiv preprint\narXiv:2304.14402.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,\nand Chris Callison-Burch. 2016. Optimizing sta-\ntistical machine translation for text simplification.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nWeijia Xu and Marine Carpuat. 2021. Editor: An edit-\nbased transformer with repositioning for neural ma-\nchine translation with soft lexical constraints. Trans-\nactions of the Association for Computational Linguis-\ntics, 9:311–328.\nIkumi Yamashita, Satoru Katsumata, Masahiro Kaneko,\nAizhan Imankulova, and Mamoru Komachi. 2020.\nCross-lingual transfer learning for grammatical er-\nror correction. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 4704–4715, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nXingxing Zhang and Mirella Lapata. 2017. Sentence\nsimplification with deep reinforcement learning. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 584–\n594, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simplification. In Proceedings of the\n23rd International Conference on Computational Lin-\nguistics (Coling 2010), pages 1353–1361, Beijing,\nChina. Coling 2010 Organizing Committee.\n10029",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7735270857810974
    },
    {
      "name": "Formality",
      "score": 0.7285965085029602
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5474940538406372
    },
    {
      "name": "Transformer",
      "score": 0.5371848344802856
    },
    {
      "name": "Inference",
      "score": 0.5331931114196777
    },
    {
      "name": "Task (project management)",
      "score": 0.4982879161834717
    },
    {
      "name": "Machine translation",
      "score": 0.48993057012557983
    },
    {
      "name": "Natural language processing",
      "score": 0.48886173963546753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46635711193084717
    },
    {
      "name": "Speech recognition",
      "score": 0.44799134135246277
    },
    {
      "name": "Edit distance",
      "score": 0.42688193917274475
    },
    {
      "name": "Language model",
      "score": 0.41275736689567566
    },
    {
      "name": "Linguistics",
      "score": 0.17103064060211182
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}