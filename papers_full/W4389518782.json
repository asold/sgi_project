{
    "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
    "url": "https://openalex.org/W4389518782",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2597999739",
            "name": "Haoyan Yang",
            "affiliations": [
                "New York University",
                "Ping An (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2097743687",
            "name": "Zhitao Li",
            "affiliations": [
                "Ping An (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2106380920",
            "name": "Yong Zhang",
            "affiliations": [
                "Ping An (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2170577868",
            "name": "Jianzong Wang",
            "affiliations": [
                "Ping An (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1988505109",
            "name": "Ning Cheng",
            "affiliations": [
                "Ping An (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1996079675",
            "name": "Ming Li",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A2047540789",
            "name": "Jing Xiao",
            "affiliations": [
                "Ping An (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1191599655",
        "https://openalex.org/W4287674181",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2736601468",
        "https://openalex.org/W4303443398",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3046423960",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4389518671",
        "https://openalex.org/W4226059645",
        "https://openalex.org/W4321392589",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4385968090",
        "https://openalex.org/W3201255524",
        "https://openalex.org/W3155807546",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4320813768",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4366735744",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W3034999214"
    ],
    "abstract": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generators formulate the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA’s effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5364–5375\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nPRCA: Fitting Black-Box Large Language Models for Retrieval Question\nAnswering via Pluggable Reward-Driven Contextual Adapter\nHaoyan Yang1,2†, Zhitao Li1, Yong Zhang1, Jianzong Wang1∗,\nNing Cheng1, Ming Li1,3, Jing Xiao1\n1Ping An Technology (Shenzhen) Co., Ltd., China\n2New York University 3University of Maryland\njzwang@188.com\nAbstract\nThe Retrieval Question Answering (ReQA)\ntask employs the retrieval-augmented frame-\nwork, composed of a retriever and generator.\nThe generator formulates the answer based on\nthe documents retrieved by the retriever. Incor-\nporating Large Language Models (LLMs) as\ngenerators is beneficial due to their advanced\nQA capabilities, but they are typically too large\nto be fine-tuned with budget constraints while\nsome of them are only accessible via APIs. To\ntackle this issue and further improve ReQA per-\nformance, we propose a trainable Pluggable\nReward-Driven Contextual Adapter (PRCA),\nkeeping the generator as a black box. Po-\nsitioned between the retriever and generator\nin a Pluggable manner, PRCA refines the re-\ntrieved information by operating in a token-\nautoregressive strategy via maximizing rewards\nof the reinforcement learning phase. Our ex-\nperiments validate PRCA’s effectiveness in en-\nhancing ReQA performance on three datasets\nby up to 20% improvement to fit black-box\nLLMs into existing frameworks, demonstrating\nits considerable potential in the LLMs era.\n1 Introduction\nRetrieval Question Answering (ReQA) tasks in-\nvolve generating appropriate answers to given ques-\ntions, utilizing relevant contextual documents. To\nachieve this, retrieval augmentation is employed\n(Chen et al., 2017; Pan et al., 2019; Izacard and\nGrave, 2021), and comprised of two key compo-\nnents: a retriever and a generator. The retriever’s\nrole is to retrieve relevant documents from a large\ncorpus in response to the question, while the gener-\nator uses this contextual information to formulate\naccurate answers. Such systems alleviate the prob-\nlem of hallucinations (Shuster et al., 2021), thereby\nenhancing the overall accuracy of the output.\n†The work was done when the first author was doing\ninternship at Ping An Technology (Shenzhen) Co., Ltd., China.\n∗Corresponding author: Jianzong Wang.\nCurrent Paradigm \nPRCA \nPRCA-based Paradigm \nQuery \nRetriever \nTop K \nDocuments \nQuery \n+\nGenerator \nBlack-Box \nModel \nCorpus \nGenerator \nWhite-Box \nModel \nQuery \nRetriever Corpus \nTop K \nDocuments \nQuery \n+\nFigure 1: A comparison between two paradigms for\ninformation retrieval and generation. The upper section\nshowcases the traditional method where a query is pro-\ncessed by a retriever that scans a corpus to fetch the\nTop-K documents and then fed to a white-box genera-\ntor. The lower section introduces our proposed PRCA\nmethod, which processes extracted Top-K documents\nfrom the retriever before feeding them to black-box\ngenerator to achieve better performance for in-domain\ntasks.\nRecent advances in Large Language Models\n(LLMs) such as the generative pre-trained trans-\nformer (GPT) series (Brown et al., 2020; Ouyang\net al., 2022; OpenAI, 2023) have demonstrated\nremarkable potential, notably in their zero-shot\nand few-shot abilities within the realm of QA\ntasks. Owing to these capabilities, LLMs are ex-\ncellent choices as generators within the retrieval-\naugmented framework. However, due to the vast\nparameters of LLMs, fine-tuning them becomes\nexceedingly difficult within a limited computation\nbudget. Furthermore, certain LLMs such as GPT-4\n(OpenAI, 2023) are closed-source, making it im-\npossible to fine-tune them. To achieve optimal\nresults on specific datasets, fine-tuning retrieval-\naugmented models becomes necessary (Guu et al.,\n2020; Lewis et al., 2020b; An et al., 2021). Previ-\nous attempts to integrate LLMs into the retrieval-\naugmented framework have met with partial suc-\n5364\ncess but also come with limitations. (Shi et al.,\n2023) utilized the logits from the final layer of the\nLLMs when calculating the loss function, which\nmay not be available to certain powerful LLMs that\nserved via APIs. (Ma et al., 2023) involved fre-\nquently invoking pricy LLMs and overlooked the\nimpact of the input token length on the accuracy\nand effectiveness of the system.\nTo overcome these hurdles, we propose a train-\nable Pluggable Reward-driven Context Adapter\n(PRCA) that enables one to fine-tune the adapter\ninstead of LLMs under the retrieval-augmented\nframework on specific datasets and achieve\nhigher performance. Furthermore, PRCA distills\nthe retrieved documents information guided by\nrewards from the generator through reinforcement\nlearning. The distillation of retrieval information\nthrough PRCA reduces the length of text input to\nthe generator and constructs a context of superior\nquality, which mitigates the hallucination issues\nduring the answer generation. As shown in Figure\n1, PRCA is placed between the retriever and the\ngenerator, forming a PRCA-based Paradigm where\nboth the generator and the retriever remain frozen.\nIn general, the introduction of the PRCA-based\nparadigm brings the following advantages:\nBlack-box LLMs Integration With the use\nof PRCA, LLMs can be treated as a black box\nintegrated into the retrieval-augmented framework,\neliminating the need for resource-intensive fine-\ntuning and restrictions on closed-nature models.\nRobustness PRCA serves as a pluggable\nadapter that is compatible with various retrievers\nand generators because PRCA-based paradigm\nkeeps both the generator and retriever frozen.\nEfficiency The PRCA-based paradigm en-\nsures the efficiency of the framework by reducing\nthe text length inputted into the generator and can\nadapt to different retrieval corpus.\n2 Related Work\n2.1 The Potential of LLMs as Black-Box\nModels\nLLMs have demonstrated remarkable capabilities\nin downstream QA tasks, even in scenarios with\nlimited or no training data (Wei et al., 2022). This\nemergence capability enables them to efficiently\ntackle such tasks, making them potential candidates\nfor black-box models in inference. Furthermore,\nthe non-open-source nature and large parameter\nsize of these models further contribute to their in-\nclination towards being perceived as black boxes.\nOn one hand, LLMs like GPT-4 (OpenAI, 2023)\nand PaLM (Scao et al., 2023) have showcased im-\npressive performance in QA tasks. However, their\nclosed source nature restricts access to these mod-\nels, making API-based utilization the only feasi-\nble option, thereby categorizing them as black-box\nmodels.\nOn the other hand, training LLMs, exemplified\nby models like Bloom (Scao et al., 2022) and GLM-\n130B (Zeng et al., 2023), impose substantial com-\nputational demands. Specifically, training Bloom\ntook 3.5 months using 384 NVIDIA A100 80GB\nGPUs. Similarly, GLM-130B requires a two-month\ntraining period on a cluster of 96 DGX-A100 GPU\nservers. These resource requirements make it ex-\ntremely challenging for the majority of researchers\nto deploy these models. Moreover, LLMs exhibit\nrapid development speeds. For instance, from\nLLaMA (Touvron et al., 2023) to Alpaca (Taori\net al., 2023) and now Vicuna (Peng et al., 2023),\nthe iterations are completed within a month. It\nis evident that the speed of training models lags\nbehind the pace of model iterations. Consequen-\ntially, tuning small-size adapters for any sequence-\nto-sequence LLMs on downstream tasks could be\na simpler and more efficient approach.\n2.2 Retrieval-Augmented Framework\nVarious retrieval augmented ideas have been pro-\ngressively developed and applied to improve the\nperformance in the ReQA task.\nIn the initial stage of research, independent\nstatistical similarity-base retrievers like TF-IDF\n(Sparck Jones, 1972) and BM25 (Robertson and\nZaragoza, 2009) were used as fundamental retrieval\nengines. They helped in extracting the most rel-\nevant documents from the corpus for QA tasks\n(Chen et al., 2017; Izacard and Grave, 2021).\nThe concept of vectorization was subsequently\nintroduced, where both questions and documents\nwere represented as vectors, and vector similar-\nity became a critical parameter for retrieval. This\nparadigm shift was led by methods such as dense\nretrieval, as embodied by DPR (Karpukhin et al.,\n2020). Models based on contrastive learning like\nSimCSE (Gao et al., 2021) and Contriver (Izac-\nard et al., 2022a), along with sentence-level se-\n5365\nmantic models such as Sentence-BERT (Reimers\nand Gurevych, 2019), represented this era. These\nmethods can be seen as pre-trained retrievers that\nboosted the effectiveness of the ReQA task.\nFurther development led to the fusion of retrieval\nand generation components within the ReQA\nframeworks. This was implemented in systems\nlike REALM (Guu et al., 2020) and RAG (Lewis\net al., 2020b), where retrievers were co-trained with\ngenerators, further refining the performance in the\nReQA task.\nRecently, advanced approaches like Atlas (Izac-\nard et al., 2022b) and RETRO (Borgeaud et al.,\n2022) have been introduced which could achieve\nperformance comparable to large-scale models like\nPalm (Chowdhery et al., 2022) and GPT3 (Brown\net al., 2020) with significantly fewer parameters.\n3 Methodology\n3.1 Two-Stage Training for PRCA\nPRCA is designed to take sequences composed of\nthe given query and the Top-K relevant documents\nretrieved by the retriever. The purpose of PRCA\nis to distill this collection of results, presenting\na concise and effective context to the generator,\nwhile keeping both the retriever and the genera-\ntor frozen. This PRCA-based paradigm introduces\ntwo challenges: the effectiveness of the retrieval\ncannot be directly evaluated due to its heavy depen-\ndence on the responses generated by the genera-\ntor, and learning the mapping relationship between\nthe generator’s outputs and the input sequence via\nbackpropagation is obstructed due to the black-box\ngenerator. To tackle these issues, we propose a two-\nstage training strategy for PRCA, as illustrated in\nFigure 2. In the contextual stage, supervised learn-\ning is employed to train PRCA, encouraging it to\noutput context-rich extractions from the input text.\nDuring the reward-driven stage, the generator is\ntreated as a reward model. The difference between\nthe generated answer and the ground truth serves as\na reward signal to further train PRCA. This process\neffectively optimizes the information distillation\nto be more beneficial for the generator to answer\naccurately.\n3.2 Contextual Extraction Stage\nIn the contextual extraction stage, we train PRCA\nto extract textual information. Given an input\ntext Sinput, PRCA generates an output sequence\nCextracted, representing the context derived from the\nReward \nGenerator \nReward Model \nPRCA \nTop-K \nDocuments \nQuery \nOptimal \nContext \nQuery \n++\nPRCA \nExtracted Context Input Text \nContextual Extraction Stage \nReward-Driven Stage \nFigure 2: An illustration of the two-stage sequential\ntraining process for the PRCA. In the first “Contex-\ntual Extraction Stage”, PRCA module is pre-trained on\ndomain abstractive summarization tasks. The second\n“Reward-Driven Stage”, demonstrates the interaction be-\ntween retrieved Top-K documents and the PRCA. Here,\nthe PRCA refines the query using both the documents\nand the original query, producing an optimal context.\nThis context is processed by a generator to obtain a\nreward, signifying the quality and relevance of the con-\ntext, with the feedback loop aiding in further refining\nthe model’s output and performance.\ninput text. The objective of the training process is\nto minimize the discrepancy between Cextracted and\nthe ground truth context Ctruth and the loss function\nis demonstrated as follows:\nmin\nθ\nL(θ) =−1\nN\nN∑\ni=1\nC(i)\ntruth log(fPRCA(S(i)\ninput; θ))\n(1)\nwhere θrepresents the parameters of PRCA\nIn the context extraction stage, PRCA is ini-\ntialized from a BART-Large model pre-trained on\nCNN Daily Mail dataset (Lewis et al., 2020a).\n3.3 Reward-Driven Stage\nIn the reward-driven stage, the objective is to align\nthe extracted context Cextracted from the previous\nstage with the downstream generator, ensuring that\nthe text distilled by PRCA serves effectively to\nguide the generator’s answering. Given the black-\nbox nature of the generator, a direct update of\nPRCA is not feasible. Therefore, we resort to rein-\nforcement learning to optimize PRCA’s parameters.\nSpecifically, the generator offers rewards to guide\nthe update of PRCA’s parameter, targeting to im-\nprove answer quality. The reward is based on the\nROUGE-L score between the generated answer O\n5366\nand the ground truth O∗. Meanwhile, it’s vital that\nPRCA retains its skill of information extraction\nfrom long texts, as learned in the contextual extrac-\ntion stage. Our objective is twofold: maximizing\ngenerator’s reward and maintaining similarity be-\ntween updated and original parameters of PRCA\nafter contextual extraction training. Catering to the\nreward-driven training where policy actions ma-\nnipulate sequence tokens, policy optimization, par-\nticularly via Proximal Policy Optimization (PPO)\n(Schulman et al., 2017; Stiennon et al., 2020), is\nthe preferred method. However, when employing a\nblack-box generator as a reward model, we identify\ncertain limitations of using PPO.\nIn (2), we present the PPO’s objective function\nJ(θ). This function strives to optimize the advan-\ntage, a value derived from the Generalized Advan-\ntage Estimation (GAE) (Schulman et al., 2016).\nThe GAE leverages both γ and λas discounting\nfactors, adjusting the estimated advantage based\non the temporal difference δV\nt+l, as depicted in (3).\nHere, Et[min(rt(θ) ·AGAE\nt ,clip(rt(θ),1 −ϵ,1 +\nϵ) ·AGAE\nt )] captures the expected advantage. The\nclip function serves to prevent excessive policy\nupdates by constraining the policy update step, en-\nsuring stability in the learning process. The term\nβ(V(st) −Rt)2 is a squared-error term between\nV(st) and Rt. This term seeks to minimize the\ndifference between the predicted and actual value,\nensuring accurate value predictions. However, the\ncritic network V is usually initialized to have the\nsame parameter as the reward model (Yao et al.,\n2023; Fazzie et al., 2023), which is inapplicable\nwhen the reward models are black-boxed. Addition-\nally, the APIs from vendors usually have limited\namount of return parameters which may cause the\ncomputation of Rt impossible.\nmax\nθ\nJ(θ) =Et[min(rt(θ) ·AGAE\nt ,\nclip(rt(θ),1 −ϵ,1 +ϵ) ·AGAE\nt )]\n−β(V(st) −Rt)2 (2)\nwhere rt(θ) = πθ(at|st)\nπθori(at|st) is the ratio of the up-\ndated policy πθ to the original policy πθori ; at\nrepresents the action (the next token);st is the state\n(the sequence of previous tokens); ϵ is the clip-\nping parameter; V is a critic network; V(st) is the\npredicted value of state st; β is a coefficient that\nweights the squared-error term; Rt is the expected\nreturn at time t.\nAGAE(γ,λ)\nt =\nT∑\nl=0\n(γλ)lδV\nt+l (3)\nwhere δV\nt+l = Rt+l + γV(st+l+1) −V(st+l);\nγ and λ as discounting and GAE parameters\nrespectively.\nTo tackle this issue, we introduce a strategy to\nestimate Rt. In the PRCA, when the token ⟨EOS⟩\nis generated, we can obtain the reward REOS by\ncomparing the generated answer against the ground\ntruth. We consider it an accumulation of the reward\nRt achieved at each time step t for the generated to-\nken. As for Rt, it serves as a target in J(θ) to train\nthe critic network V(s) for fitting, symbolizing the\naverage reward of the current action, thereby as-\nsessing the advantage of the current policy. For\neach token, the greater the probability of genera-\ntion, the more important this token is perceived by\nthe current policy, so we consider its contribution\nto the total reward to be greater. Therefore, we\nregard the probability of generating each token as\nthe weight of REOS, and the representation of Rt\nis given by the following:\nRt = REOS ∗ eπθ(at|st)\n∑K\nt=1 eπθ(at|st) (4)\nREOS = ROUGE-L(O,O∗)\n−β·DKL(πθ||πθori) (5)\nROUGE-L = LCS(X,Y )\nmax(|X|,|Y|) (6)\nwhere K is the number of tokens in one gener-\nated context, LCS(X,Y ) denotes the length of the\nlongest common subsequence between sequence\nX and sequence Y, and |X|and |Y|denote the\nlengths of sequences X and Y, respectively.\nThis method mitigates the challenges associated\nwith calculating Rt when interpreting the black-\nbox generator as a reward model. A substantial\nadvantage it confers is the requirement of invok-\ning the reward model only once for each context\ngeneration. Compared to the original PPO that\nemploys the reward model for every token com-\nputation, our approach reduces the reward model\nusage to 1\nK, which is cost-effective especially when\nusing LLMs as generators.\n5367\nTable 1: Overview of the data quantities used for train-\ning and testing across three benchmark datasets.\nDataset Train / Test # of Q # of C # of A\nSQuAD Train 87.6k 18.9k 87.6k\nTest 10.6k 2.1k 10.6k\nHotpotQA Train 90.4k 483.5k 90.4k\nTest 7.4k 66.5k 7.4k\nTopiQCQA Train 45.5k 45.5k 45.5k\nTest 2.5k 2.5k 2.5k\n4 Experimental Setup\n4.1 Datasets\nWe performed our experiments on three QA\ndatasets: SQuAD (Rajpurkar et al., 2016), Hot-\npotQA (Yang et al., 2018) and TopiOCQA (Ad-\nlakha et al., 2022). The complexity of three datasets\nincreases sequentially: SQuAD is a dataset that\nmatches questions, documents, and answers in a\none-to-one manner. HotpotQA is a multi-hop QA\ndataset, requiring the synthesis of correct answers\nfrom multiple documents. TopiOCQA is a conver-\nsational QA dataset with topic switching.\nTo align these datasets with our ReQA task, we\nreconstructed all three datasets into the form of\n(Q,C,A ), where Qand Adenote the question and\nanswer pair, and Crepresents a corpus composed\nof all the documents in the dataset respectively. In\nTable 1, we present the number of questions and\nanswers employed in the PRCA training and testing\nphases for every dataset. Additionally, we provide\nthe quantity of documents contained within each\nrespective corpus.\n4.2 Baseline Retrievers and Generators\nWe conducted experiments with five different\nretrievers, specifically BM25 (Robertson and\nZaragoza, 2009), SentenceBert (Reimers and\nGurevych, 2019), DPR (Karpukhin et al., 2020),\nSimCSE (Gao et al., 2021), and Contriver (Izac-\nard et al., 2022a). We also utilized five generators\nwhich are T5-large (Raffel et al., 2020), Phoenix-\n7B (Chen et al., 2023), Vicuna-7B (Peng et al.,\n2023), ChatGLM (Du et al., 2022) and GPT-3.5 1\nto assess the effectiveness of PRCA. Note that both\nthe retrievers and generators remain frozen through\nthe experiment.\nBy pairing every retriever with each generator,\n1Our experiments were conducted with the default version\nof GPT-3.5-turbo and GPT-4 between May and June 2023 via\nhttps://openai.com.\nTable 2: Hyperparameters settings used in the experi-\nments.\nHyperparameters Value\nLearning rate 5 × 10−5\nBatch size 1/2/4\nNum beams 3\nTemperature 1\nEarly Stopping True\nTopk 0.0\nTopp 1.0\nwe established a total of seventy-five baseline con-\nfigurations on three datasets. For each configura-\ntion, we evaluated the performance with and with-\nout the application of PRCA and the difference\nserves as an indicator of the effectiveness of our\nproposed approach.\n4.3 GPT-4 Assessment\nNotably, we used GPT-4 for evaluation rather than\ntraditional metrics like F1 and BLEU, as these met-\nrics often misjudged semantically similar sentences.\nLLMs often output longer textual explanations for\nanswers, even when the correct answer might be\na word or two. Despite attempts to constrain an-\nswer lengths, the results weren’t ideal. We then\nevaluated predictions using both manual methods\nand GPT-4 against golden answers. GPT-4’s evalu-\nations showed correctness rates of 96%, 93%, and\n92% across three datasets, demonstrating its relia-\nbility and alignment with human judgment.\nSpecifically, the template for GPT-4 assessment\nis shown as follows. Finally, the accuracy rate\nof answering “Yes” is counted as the evaluation\nmetric.\nTemplate for GPT-4 Assessment\nPrompt: You are now an intelligent assess-\nment assistant. Based on the question and\nthe golden answer, judge whether the pre-\ndicted answer correctly answers the question\nand give only a Yes or No.\nQuestion:\nGolden Answer:\nPredicted Answer:\nExpected Output: Yes / No\n4.4 Hyperparameter Configurations\nTo achieve optimal results in our PRCA training,\ncareful selection of hyperparameters is pivotal. The\n5368\nTable 3: Comparative results of performance for different retriever and generator combinations in the presence and\nabsence of PRCA integration. The results are based on the evaluation using three benchmark datasets: SQuAD,\nHotpotQA, and TopiOCQA, and focus on the selection of the Top-5 most relevant documents.\nRetriever Generator SQuAD HotpotQA TopiOCQA\nBM25\nT5 0.74-0.03 0.35+0.01 0.27+0.08\nPhoenix 0.61+0.02 0.31+0.09 0.25+0.03\nVicuna 0.59+0.09 0.19+0.13 0.23+0.10\nChatGLM 0.67+0.03 0.36+0.04 0.35+0.03\nGPT-3.5 0.75+0.02 0.48+0.06 0.44+0.04\nSentenceBert\nT5 0.48-0.06 0.20+0.05 0.28+0.05\nPhoenix 0.42+0.04 0.13+0.10 0.26+0.08\nVicuna 0.36+0.09 0.22+0.03 0.23+0.05\nChatGLM 0.57+0.04 0.16+0.08 0.28+0.04\nGPT-3.5 0.6+0.02 0.34+0.03 0.47+0.03\nDPR\nT5 0.57+0 0.23+0.02 0.20+0.09\nPhoenix 0.56+0.01 0.15+0.09 0.15+0.16\nVicuna 0.42+0.06 0.16+0.11 0.15+0.14\nChatGLM 0.53+0.0 0.16+0.04 0.31+0.07\nGPT-3.5 0.69+0.04 0.41+0.02 0.34+0.06\nSimSCE\nT5 0.75+0.01 0.28+0.02 0.18+0.09\nPhoenix 0.67+0.02 0.17+0.10 0.17+0.13\nVicuna 0.47+0.06 0.19+0.06 0.10+0.20\nChatGLM 0.75+0.05 0.17+0.05 0.21+0.06\nGPT-3.5 0.77+0.04 0.37+0.05 0.31+0.06\nContriver\nT5 0.80-0.08 0.35+0.03 0.18+0.11\nPhoenix 0.69+0.02 0.10+0.11 0.16+0.18\nVicuna 0.58+0.08 0.17+0.12 0.14+0.19\nChatGLM 0.71+0.05 0.13+0.09 0.23+0.05\nGPT-3.5 0.80+0.02 0.37+0.05 0.30+0.08\n‘+’ indicates an improvement in performance metrics upon the incorporation of PRCA. The color coding provides a visual\nrepresentation of the effect: Green signifies a positive enhancement in performance, while Red indicates a decrement.\nconfiguration settings employed in our experiment\nare stated in Table 2.\n5 Results and Analysis\n5.1 Overall Performance\nAs delineated in Table 3, among the seventy-five\nconfigurations, our experimental results suggest\nthat the inclusion of PRCA improves performance\nin seventy-one configurations. On average, we ob-\nserve an enhancement of 3%, 6%, and 9% on the\nSQuAD, HotpotQA, and TopiOCQA datasets, re-\nspectively. This demonstrates that PRCA possesses\nrobustness and can enhance the performance of dif-\nferent combinations of retrievers and generators\non the ReQA task. As illustrated in Figure 3, the\nimprovements rendered by PRCA to the generators\nare significant across all three datasets. Particularly\non the TopiOCQA dataset, the average improve-\nment for generator Vicuna across five different re-\ntrievers reaches 14%. Notably, when SimSCE is\nthe retriever, the enhancement offered by PRCA is\n20%.\nIn Figure 3, we notice that the improvement to\ngenerator performance by PRCA across the three\ndatasets is incremental, while the original perfor-\nmance of the generators across the three datasets\nis decremental without PRCA, correlating directly\nwith the complexity of the datasets. This is be-\ncause when faced with more complex issues, such\nas multi-hop questions in HotpotQA and topic tran-\nsitions in multi-turn QA in TopiOCQA, PRCA re-\nserves and integrates critical information which is\nbeneficial for generators from the retrieved doc-\numents. This attribute of PRCA alleviate issues\nwhere generators struggle with lengthy texts, fail-\ning to answer questions correctly or producing hal-\nlucinations, thus enhancing performance.\nHowever, the inclusion of PRCA has a negative\neffect on the performance of the generator T5 on\nthe SQuAD dataset. This is because the SQuAD\ndataset is relatively simple, where the answer often\ndirectly corresponds to a phrase in the text. As an\nencoder-decoder architecture model, T5 tends to\nextract answers directly rather than infer in-depth\nbased on the context. Therefore, without infor-\nmation distillation by PRCA from the retrieved\n5369\nFigure 3: Comparison of performance of different gen-\nerators (T5, Phoenix, Vicuna, ChatGLM, and GPT-3.5)\non three benchmark datasets: SQuAD, HotpotQA, and\nTopicOCQA. The horizontal axis represents the GPT-4\nassessment accuracy. Bars depict the performance levels\nof each generator, with green and red arrows indicating\nthe enhanced or diminished effects due to PRCA inte-\ngration, respectively.\ndocuments, T5 performs well because its features\nfit well in handling this dataset, capable of directly\nextracting answers from the context. But under the\neffect of PRCA, the structure of the text might be\naltered, and T5’s direct answer extraction may lead\nto some errors, thereby reducing performance.\nWhile in a few configurations, the characteristics\nof PRCA may have negative effects, for the vast\nmajority of configurations, our experiments vali-\ndate that under PRCA-based paradigm, PRCA can\neffectively enhance the performance in the ReQA\ntask, demonstrating robustness.\n5.2 Efficiency of PRCA\nPRCA represents an effective approach for\nenhancing the performance of the ReQA task\nwithout significantly increasing computational\ndemand. Its efficiency is manifested in optimizing\nparameters to achieve superior results and in\nsimplifying input text, thereby aiding generators in\nmanaging complex text.\nParameter Efficiency Figure 4 portrays a\ncomparative analysis between the generators,\nwhich gain the maximum improvements with\nPRCA, and the GPT-3.5 model which operates\nwithout PRCA, across 3 datasets. PRCA boasts\nroughly 0.4 billion parameters, the most signif-\nicantly improved generators encompass about\n7 billion parameters on average, while GPT-3.5\nhas approximately 1.75 trillion parameters. As\ndemonstrated in Figure 4, with a marginal\nSQuAD HotpotQA T opicOCQA\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nGPT4 Assessment Accuracy\n+12.0 %\n+27.1%\n+64.5%\nBaselines\nGPT3.5\nPositive  Effect \nof PRCA\nFigure 4: Performance comparison between PRCA-\nenhanced baseline models and GPT-3.5 across SQuAD,\nHotpotQA, and TopicOCQA. Light and dark blue bars\nrepresent baseline and GPT-3.5 performance, while\nstriped green indicates PRCA’s improvement.\nTable 4: PRCA inference speed test results.\nDataset Precision GPU Batch Size Inference Speed\n(token/s)\nPRCA float32 A100 1 126\nPRCA float32 A100 2 231\nPRCA float32 A100 4 492\nparameter increment, the performance of these\ngenerators improved by 12.0%, 27.1%, and 64.5%\nrespectively. Hence, PRCA has great potential to\nbe an efficient way to boost the performance of\nReQA task while keeping computational resources\nconsumption acceptable. During the inference\nprocess, a fully-trained PRCA will perform only\nstandard forward propagation and hence introduce\nlimited impact on inference latency. Our inference\nlatency test on SQUAD was reported in Table 4.\nThis low latency ensures that the system maintains\na smooth process without significant delays after\nintegrating PRCA, underscoring the high efficiency\nof PRCA in boosting system performance.\nInput Simplification As illustrated in Figure 5,\nwe analyzed the relationship between reward and\ntoken count during reward-driven stage for a QA\npair in the HotpotQA dataset, with and without\nPRCA. There’s a discernible difference in the re-\nward trajectories with and without PRCA. Both\nreward curves ascend with the increase in token\n5370\nFigure 5: A depiction of reward trajectories over in-\ncreasing token counts during the reward-driven stage\nfor a QA pair within the HotpotQA dataset. Distinct\nlines represent rewards achieved with and without the\nimplementation of PRCA, underscoring PRCA’s ability\nto extract more concise and high-quality text.\ncount, but the gradient of ascent with PRCA is no-\nticeably steeper. This implies that when PRCA is\nin action, the generator reaches its optimal perfor-\nmance with a significantly reduced token count.\nUnder the influence of PRCA, the generator can\nderive the correct answer with approximately four\ntimes fewer tokens. This indicates that PRCA can\ndistill the retrieved text while ensuring the qual-\nity of the generated answer. This simplification\nprocess filters out redundant information, thereby\npromoting the generator to extract answers more\naccurately using a more streamlined context. More-\nover, the reduction in token count enables the gen-\nerator to process text faster and produce outputs\nmore promptly. Overall, PRCA’s efficiency in infor-\nmation distillation greatly bolsters the generator’s\ncapacity to manage and interpret complex text.\n5.3 Impact of Top-K Selection\nWe conducted parameter sensitivity experiments\nto observe the performance of PRCA when the\nnumber of retrieved relevant documents changes.\nThe results presented in Figure 6 show that on the\nSQuAD dataset, both the performance with and\nwithout PRCA improve as the number of retrieved\ndocuments increases, while the addition of PRCA\nconsistently provides a positive effect across dif-\nferent Top-K values. Since the dataset is relatively\nsimple, with the increased likelihood of the correct\nanswer being included in the retrieved documents,\nboth trends exhibit an upward trajectory.\nIn contrast, without the implementation of\nPRCA, there is a noticeable drop in performance on\nthe HotpotQA and TopiOCQA datasets when more\n5 8 10 13 15 18 20\nT op-K Retrieved Documents\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT4 Assessment Accuracy\nSQuAD\nHotpotQA\nT opiOCQA\nSQuAD-PRCA\nHotpotQA-PRCA\nT opiOCQA-PRCA\nFigure 6: Comparison of performance with and with-\nout PRCA with the different number of retrieved docu-\nments.\ndocuments are added. This decline is attributed\nto the model’s diminishing capability to generate\naccurate answers to complex questions due to the\nrise in distracting information and the onset of hal-\nlucination problems. However, by implementing\nPRCA, these adverse effects are systematically al-\nleviated, which not only reduces the onset of hallu-\ncinations but also enhances the generator’s ability\nto handle complex queries amidst distractions.\nIn general, at different Top-K values, PRCA\ndemonstrates positive effects across all three\ndatasets, thereby illustrating the universal appli-\ncability of PRCA regardless of the quantity of re-\ntrieved documents.\n5.4 Case Study\nWhen answering the form of Mersenne primes\nproblem, the retrieved text contains two distinct\nsources of information. One directly specifies the\nform as 2p-1, accurately reflecting the nature of\nMersenne primes. The other source misguidedly\nintroduces “factorial primes” as an answer. With-\nout PRCA’s intervention, this diversion leads the\ngenerator astray, resulting in an erroneous answer\nof “factorial primes”. However, when PRCA is\nengaged, it sifts through the information, priori-\ntizing the accurate context. This refined context\nextraction steers the generator towards the correct\nanswer.\n5371\nCase Study without and with PRCA\nQuestion:\nOf what form do Mersenne primes take?\nGolden Answer: 2p-1\nPart of Retrieved Documents: [Golden\nAnswer Source] Mersenne primes are\nprime numbers that are of the form ::::2p-1,\nwhere p is an arbitrary prime. [Pre-\ndicted Answer Source] The Sieve of Er-\natosthenes, attributed to Eratosthenes, is a\nsimple method to compute primes, although\nthe large primes found today with comput-\ners are not generated this way. are prime.\nPrime numbers of this form are known as\n:::::::factorial:::::::primes.\nPredicted Answer without PRCA: Facto-\nrial primes\nContext through PRCA: Mersenne primes\nare prime numbers that are of the form 2p-1,\nwhere p is an arbitrary prime. The Lu-\ncas–Lehmer test is particularly fast for num-\nbers of this form, so many of the largest\nprimes found today are Mersenne primes.\nPredicted Answer with PRCA: 2p-1\nNote: “–” denotes key information\nrelevant to the question, “ ~” represents\npredicted answers.\n5.5 Ablation Study of PRCA\nWe assessed the impact of PRCA on three datasets\nusing the configurations from section 5.2, which\nshowed maximum improvements. The evaluation\nis conducted with and without the reward-driven\nstage to observe the impact of PRCA on the per-\nformance. As illustrated in Figure 7, without the\nreward-driven training stage, the effect of PRCA on\nthe entire configuration becomes adverse because\nPRCA merely simplifies the text without discerning\nwhich information is beneficial for the generator\nto answer questions, resulting in the omission of\nuseful text. In contrast, once the training process\nincorporates the reward-driven stage, the quality of\nthe context becomes directly aligned with reward\nvalues, assisting PRCA in more effectively distill-\ning pertinent information. Therefore, the reward-\ndriven stage is vital, allowing PRCA to retain key\ndetails while simplifying text, enhancing its overall\neffect.\n−0.05 0.00 0.05 0.10 0.15 0.20\nGPT4 Assessment Accuracy\nSQuAD\nHotpotQA\nT op cOCQA\n0.09\n0.13\n0.2\n-0.05\n-0.08\n-0.04\nEffect of PRCA  W th the \nReward-Dr ven T ra n ng Stage\nEffect of PRCA  W thout the \nReward-Dr ven T ra n ng Stage\nFigure 7: An illustration showcasing the impact of the\nreward-driven stage on PRCA’s performance.\n6 Conclusion\nIn conclusion, this research successfully introduces\na PRCA-based paradigm for ReQA tasks, tack-\nling the inherent challenges of fine-tuning LLMs\nin the retrieval-enhancement framework, especially\ngiven their vast parameter size and closed-source\nnatures. PRCA innovatively distills retrieved docu-\nments via generator rewards, leading to a marked\nimprovement in the ReQA task’s performance. Ex-\nperimental outcomes consistently demonstrate the\nrobustness and effectiveness of PRCA when paired\nwith various retrievers and generators, indicating\nits potential to be widely deployed as an adapter on\nthe ReQA task.\nLimitations\nWhile PRCA has shown effectiveness in improving\nReQA task performance, it has limitations, includ-\ning dependency on generators, convergence issues,\nand limited integration with retrievers. The reward\nduring reinforcement learning training is derived\nfrom the generator, requiring PRCA retraining with\ndifferent generators, which can be time-consuming.\nPRCA may also experience difficulties converging\nin a single training session, which impacts the sta-\nbility and consistency of its performance. Lastly,\nPRCA’s operation as a pluggable adapter limits its\nability to train jointly with retrievers, which means\nif the retrieval quality is not up to par, PRCA’s\neffectiveness could be compromised.\nAcknowledgement\nSupported by the Key Research and Develop-\nment Program of Guangdong Province (grant No.\n2021B0101400003) and Corresponding author is\nJianzong Wang (jzwang@188.com).\n5372\nReferences\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-\nman, Harm de Vries, and Siva Reddy. 2022. Top-\niOCQA: Open-domain conversational question an-\nswering with topic switching. Transactions of the\nAssociation for Computational Linguistics, 10:468–\n483.\nChenxin An, Ming Zhong, Zhichao Geng, Jianqiang\nYang, and Xipeng Qiu. 2021. Retrievalsum: A re-\ntrieval enhanced framework for abstractive summa-\nrization. CoRR, abs/2109.07943.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 2206–2240.\nPMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan\nWang, Fei Yu, Guiming Chen, Hongbo Zhang,\nJuhao Liang, Chen Zhang, Zhiyi Zhang, et al. 2023.\nPhoenix: Democratizing chatgpt across languages.\narXiv preprint arXiv:2304.10453.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nFazzie, FrankLeeeee, BlueRum, ver217, ofey404, Wen-\nhao Chen, Zangwei Zheng, and Xue Fuzhao. 2023.\nColossalchat. https://github.com/hpcaitech/\nColossalAI/tree/main/applications/Chat.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022a. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research, 08:1–21.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022b. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6769–6781, Online. As-\nsociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\n5373\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2302.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nXiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng\nJi, Claire Cardie, and Dong Yu. 2019. Improving\nquestion answering with external knowledge. In Pro-\nceedings of the 2nd Workshop on Machine Reading\nfor Question Answering, pages 27–37, Hong Kong,\nChina. Association for Computational Linguistics.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing, pages 3982–3992, Hong\nKong, China. Association for Computational Linguis-\ntics.\nStephen Robertson and Hugo Zaragoza. 2009. The prob-\nabilistic relevance framework: Bm25 and beyond.\nFoundations and Trends® in Information Retrieval,\n3(4):333–389.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2023. Palm 2 technical report.\nArXiv, abs/2305.10403.\nJohn Schulman, Philipp Moritz, Sergey Levine,\nMichael I. Jordan, and Pieter Abbeel. 2016. High-\ndimensional continuous control using generalized\nadvantage estimation. In The Fourth International\nConference on Learning Representations.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nKaren Sparck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of Documentation, 28(1):11–21.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\n5374\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research, 08:1–30. Survey\nCertification.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji\nRuwase, Samyam Rajbhandari, Xiaoxia Wu, Am-\nmar Ahmad Awan, Jeff Rasley, Minjia Zhang,\nConglong Li, Connor Holmes, Zhongzhu Zhou,\nMichael Wyatt, Molly Smith, Lev Kurilenko, Heyang\nQin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon\nSong, and Yuxiong He. 2023. DeepSpeed-Chat:\nEasy, Fast and Affordable RLHF Training of\nChatGPT-like Models at All Scales. arXiv preprint\narXiv:2308.01320.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGLM-130b: An open bilingual pre-trained model. In\nThe Eleventh International Conference on Learning\nRepresentations.\n5375"
}