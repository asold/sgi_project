{
    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
    "url": "https://openalex.org/W4409363038",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2108452071",
            "name": "Yutao Zhu",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2302520222",
            "name": "Zhaoheng Huang",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2330149811",
            "name": "Zhicheng Dou",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A3212238123",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2108452071",
            "name": "Yutao Zhu",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2302520222",
            "name": "Zhaoheng Huang",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2330149811",
            "name": "Zhicheng Dou",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A3212238123",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2252136820",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W6838393215",
        "https://openalex.org/W2953271402",
        "https://openalex.org/W6803096969",
        "https://openalex.org/W6853251322",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3205789812",
        "https://openalex.org/W6779857854",
        "https://openalex.org/W2987283559",
        "https://openalex.org/W3083410900",
        "https://openalex.org/W3101823922",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W4362656079",
        "https://openalex.org/W6810242208",
        "https://openalex.org/W4398795713",
        "https://openalex.org/W4391988146",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W3208581628",
        "https://openalex.org/W3020908159",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W6793601707",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W6856709431",
        "https://openalex.org/W3015697211",
        "https://openalex.org/W6853859572",
        "https://openalex.org/W3205270560",
        "https://openalex.org/W6792279967",
        "https://openalex.org/W6849760658",
        "https://openalex.org/W2558203065",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3083182073",
        "https://openalex.org/W4304192721",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W6718053083",
        "https://openalex.org/W6849392780",
        "https://openalex.org/W2888296173",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W4391987988",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3190126809",
        "https://openalex.org/W6859925366",
        "https://openalex.org/W4310923309",
        "https://openalex.org/W4388717437",
        "https://openalex.org/W4386794805",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W4387355694",
        "https://openalex.org/W4361229539",
        "https://openalex.org/W4386501849",
        "https://openalex.org/W4385889719",
        "https://openalex.org/W4390897662"
    ],
    "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
    "full_text": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens\nfor Retrieval-Augmented Large Language Models\nYutao Zhu, Zhaoheng Huang, Zhicheng Dou*, Ji-Rong Wen\nGaoling School of Artificial Intelligence, Renmin University of China\nyutaozhu94@gmail.com, {huangzh, dou, jrwen}@ruc.edu.cn\nAbstract\nRetrieval-augmented generation (RAG) is a promising way to\nimprove large language models (LLMs) for generating more\nfactual, accurate, and up-to-date content. Existing methods\neither optimize prompts to guide LLMs in leveraging re-\ntrieved information or directly fine-tune LLMs to adapt to\nRAG scenarios. Although fine-tuning can yield better perfor-\nmance, it often compromises the LLMs‚Äô general generation\ncapabilities by modifying their parameters. This limitation\nposes challenges in practical applications, especially when\nLLMs are already deployed, as parameter adjustments may\naffect their original functionality. To address this, we propose\na novel method that involves learning scalable and pluggable\nvirtual tokens for RAG. By maintaining the LLMs‚Äô origi-\nnal parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs‚Äô\nperformance but also preserves their general generation ca-\npabilities. Furthermore, we design several training strategies\nto improve the scalability, flexibility, and generalizability of\nour method. Comprehensive experiments across 12 question-\nanswering tasks demonstrate the superiority of our approach.\nIntroduction\nLarge language models (LLMs) have achieved remarkable\nperformance across various natural language processing\ntasks (Brown et al. 2020; OpenAI 2023; Touvron et al.\n2023). Despite their extensive parameters enabling them\nto learn rich knowledge during pre-training, LLMs may\nstill generate hallucinated, outdated, or inaccurate content,\nespecially in scenarios requiring long-tail knowledge (Ji\net al. 2023; Zhang et al. 2023b). To address this problem,\nretrieval-augmented generation (RAG) has emerged as a piv-\notal strategy. By explicitly decoupling knowledge retrieval\nfrom the backbone LLMs, such architectures have achieved\nmore accurate and reliable content generation and shown\nparticularly enhanced performance on knowledge-intensive\ntasks such as open-domain question answering (Petroni et al.\n2021; Tan et al. 2024; Jin et al. 2024b).\nExisting efforts in RAG development can be roughly cat-\negorized into two groups (as illustrated in Figure 1). The\nfirst group leverages the in-context learning capabilities of\n*Corresponding author.\nCopyright ¬© 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nLLMs by incorporating retrieved information into the input\nalong with appropriate prompts (Shi et al. 2023; Ram et al.\n2023). This allows for straightforward application to anyoff-\nthe-shelf LLM without tuning its parameters. However, its\neffectiveness largely depends on the human experience in\ncrafting effective prompts and the LLM‚Äôs ability to interpret\nthese prompts. The second group focuses on training LLMs\nto enhance their performance in RAG scenarios. This train-\ning might involve either end-to-end pre-training (Guu et al.\n2020; Borgeaud et al. 2022) or fine-tuning (Lin et al. 2023;\nWang et al. 2023) for specific tasks. These approaches can\noften lead to better performance, but they require significant\ncomputational resources. Recently, parameter-efficient fine-\ntuning techniques, such as LoRA (Hu et al. 2022), have been\nwidely studied, significantly reducing training costs. These\nmethods can optimize the LLMs‚Äô parameters for RAG, but\nunfortunately compromise the model‚Äôs general abilities in\nnon-RAG scenarios, such as commonsense reasoning and\nin-context learning. All these limitations prevent their appli-\ncation to LLMs already operational in real-world settings.\nTherefore, a critical research problem arises:Is it possible\nto enhance LLMs‚Äô performance under RAG scenarios while\npreserving their general generation capabilities?To achieve\nthis, we introduce a novel, lightweight tuning method named\nSPRING, which learns S\ncalable and Pluggable viRtual to-\nkens for retrI eval-augmeNted Generation. Our basic idea\nis to add trainable virtual tokens to help LLMs learn RAG\nproblems. Through fine-tuning, these virtual tokens effec-\ntively enhance the LLM‚Äôs capability to understand retrieved\ninformation and its correlation with user inputs. Importantly,\nas the LLM‚Äôs original parameters are frozen, its general gen-\neration abilities are preserved without any loss. During in-\nference, when retrieval is triggered, these trained virtual to-\nkens can be simply added to the prompt, which includes both\nthe retrieved results and user input, thereby significantly en-\nhancing performance. Moreover, we employ a scalable train-\ning approach, allowing the number of virtual tokens to be\nadjusted according to the needs of the inference scenario.\nVarious training strategies have been implemented to further\nimprove the generalizability of our method, ensuring robust-\nness regardless of the number of the retrieved results.\nIn experiments, SPRING is trained with the base and\ninstruction fine-tuned versions of Mistral-7b, LLaMA-2-\n7b, and LLaMA-2-13b models and evaluated on 12 com-\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n26166\nLLM\nAdded tokens\n(1) Prompt-based(2) Tuning-based(3) SPRING\nLLM\n LLM\nRetrieved resultsInput\nRAGPromptTuningSPRINGPerformance comparison\nGeneral\nRAG:General:LLM\nPromptTuningSPRING\nRAG\nFigure 1: Illustration of existing methods for RAG and our proposed method. Our method can improve LLMs‚Äô performance in\nRAG scenarios by incorporating trainable virtual tokens, and these tokens can be removed to preserve the general generation\nabilities in non-RAG scenarios.\nmonly used QA datasets, covering both in-domain and out-\nof-domain scenarios. The experimental results demonstrate\nthat SPRING not only effectively improves the RAG perfor-\nmance of LLMs but also successfully preserves their gen-\neral generation capabilities. Overall, the SPRING method\nexhibits four main characteristics:\n‚Ä¢ Lightweight yet effective. Instead of updating the full\nparameters of the LLMs, we opt to freeze the pre-trained\nmodels and only learn the embeddings for the added vir-\ntual tokens. For example, adding 50 tokens to the Mistral-\n7b model introduces only 0.2M parameters in total. Despite\nthese minimal parameters, SPRING improves the average\nEM and F1 scores by more than 43% and 17% across 12\nQA datasets, respectively.\n‚Ä¢ Scalable. With our proposed scalable training approach,\nSPRING can be effective with any number of virtual tokens\n(k ‚àà [1, 50] in our experiments). Remarkably, even just one\ntoken can substantially improve the LLMs‚Äô performance in\nRAG scenarios.\n‚Ä¢ Pluggable. Owing to its lightweight design, SPRING\ncan be applied in a plug-and-play manner. When retrieval is\ntriggered, simply adding the virtual tokens can lead to bet-\nter performance. In non-RAG scenarios, the virtual tokens\nare not added so the LLMs‚Äô original capabilities can be well\npreserved. This characteristic is crucial for LLMs that have\nalready been deployed for practical use.\n‚Ä¢ Generalizable. Our robust training strategies ensure\nthat SPRING is adaptable to different retrievers and various\nnumbers of retrieved results. Consequently, there is no need\nto retrain SPRING with each update to the retrieval system,\nenhancing its practicality and efficiency.\nRelated Work\nRetrieval-Augmented Generation Compared to standard\ntext generation, retrieval-augmented generation (RAG) in-\ncorporates a retrieval module that accesses external knowl-\nedge to enhance generation quality (Lewis et al. 2020; Guu\net al. 2020; Zhu et al. 2023; Jin et al. 2024a). The main-\nstream RAG follows a ‚Äúretrieve-then-read‚Äù paradigm, where\nthe retrieval module provides external knowledge as addi-\ntional context, which is then read by generation models to\nproduce the final output (Shi et al. 2023; Ram et al. 2023;\nBorgeaud et al. 2022; Lin et al. 2023; Zhu et al. 2024).\nTo optimize the use of external knowledge, some methods\nfocus on crafting effective prompts that guide the utiliza-\ntion of retrieved information (Shi et al. 2023; Ram et al.\n2023). These prompt-based methods are applicable to any\nLLM without tuning its parameters. However, they depend\nheavily on skillful prompt writing and the LLMs‚Äô abil-\nity to understand instructions. In contrast, other studies at-\ntempts to directly train the model to better use the retrieved\nknowledge. For example, REALM (Guu et al. 2020) and\nRETRO (Borgeaud et al. 2022) incorporate retrieval in end-\nto-end retrieval-augmented pre-training. RA-DIT (Lin et al.\n2023) employs fine-tuning to enhance LLMs‚Äô retrieval un-\nderstanding. These tuning-based methods often yield bet-\nter performance than prompt-based methods by optimizing\nmodel parameters for RAG. However, they may compromise\nthe LLMs‚Äô general capabilities, particularly in non-retrieval\nscenarios. Different from existing methods, we design a new\nlightweight tuning method for RAG. It is a plug-and-play\nmodule that enhances RAG performance using trainable vir-\ntual tokens, which can be removed in non-RAG scenarios to\npreserve the LLMs‚Äô general generation abilities.\nParameter-Efficient Fine-Tuning The paradigms of\n‚Äúpre-training then fine-tuning‚Äù have demonstrated efficacy\nacross various natural language (Devlin et al. 2019; Raffel\net al. 2020; Radford et al. 2019) and vision tasks (He et al.\n2020; Chen et al. 2020). The common fine-tuning process\ninvolves tuning all parameters of a model, which is com-\nputational intensive, especially for LLMs. To address this,\nparameter-efficient fine-tuning (PEFT) (Mangrulkar et al.\n2022) approaches have been developed. These approaches\nfreeze most of the pre-trained models‚Äô parameters, yet still\nmanage to achieve comparable performance on downstream\ntasks. PEFT has been widely studied (Wan et al. 2023), and\ntypical methods including adapter-based tuning (Houlsby\net al. 2019; Lin, Madotto, and Fung 2020; Chen et al. 2023),\nlow-rank adaptation (LoRA) (Hu et al. 2022; Dettmers et al.\n2023), and prompt tuning (Li and Liang 2021; Lester, Al-\nRfou, and Constant 2021; Liu et al. 2021b,a). Adapter-based\ntuning inserts lightweight modules into a model‚Äôs existing\nlayers and have been extended to various domains (Gao\net al. 2024; Hu et al. 2023; Zhang et al. 2023a). LoRA (Hu\net al. 2022) introduces trainable low-rank matrices that ad-\njust the model‚Äôs weight updates, achieving promising fine-\ntuning performance on LLMs (Hu et al. 2023). Prompt\ntuning incorporates a series of trainable prompt tokens to\nLLMs. These tokens can be inserted either to the input layer\nonly (Lester, Al-Rfou, and Constant 2021; Liu et al. 2021b)\n26167\nor to all of the intermediate layers (Li and Liang 2021; Liu\net al. 2021a). In this paper, we proposes a novel prompt tun-\ning method, SPRING, specifically designed for RAG sce-\nnarios. Our method introduces virtual tokens between re-\ntrieved results and the input, exploiting the auto-regressive\ngeneration paradigm to improve the model‚Äôs ability to uti-\nlize retrieved information. Additionally, it is designed to\nbe scalable and pluggable, thus broadening its application\nscope while preserving the original generative capabilities\nof LLMs.\nMethodology\nTo take advantage of both the flexibility of prompt-based\nmethods and the efficacy of fine-tuning-based methods, we\npropose SPRING to learn scalable and pluggable virtual to-\nkens for retrieval-augmented generation (RAG).\nProblem Formulation\nLanguage models are designed to calculate the probability\ndistribution over sequences of natural language texts. Auto-\nregressive models are commonly used for this through next-\ntoken prediction:\npLM =\nmY\ni=1\npŒ∏(xi|x<i), (1)\nwhere x<i denotes the sequence of tokens preceding xi at\neach step, and Œ∏ represents the parameters of the model. For\nRAG, a retrieval corpusD and a retriever M are introduced.\nThen, the generation process is conditioned on bothx<i and\nthe retrieved results R = MD(x<i) as:\npRAG =\nmY\ni=1\npŒ∏(xi|R; x<i), (2)\npRAG-QA =\nmY\ni=1\npŒ∏(ai|R; Q; a<i). (3)\nNote that here x<i serves as the query for retrieval. In\nquestion-answering (QA) tasks, x<i is usually the question\nQ, and the learning objective is to generate the right answer\nA = {ai}m\ni=1. The retriever can yield multiple passages,\nwhich can be concatenated as a long text sequence using\nproper separator such as ‚Äú\\n\\n‚Äù. For brevity, this formula-\ntion directly concatenates the retrieved results R with the\nquestion Q, omitting more complex prompt designs. Hence-\nforth, we will use the notations in QA tasks as our evaluation\nis performed on them.\nScalable and Pluggable Virtual Tokens for RAG\nOur SPRING method, shown in the left side of Figure 2,\nintroduces trainable virtual tokens into the input to opti-\nmize LLMs for RAG scenarios. Specifically, following the\nnotation in Equation (3), we add n trainable tokens T =\n[t1, t2, ¬∑¬∑¬∑ , tn] between the retrieved results R and the input\nQ. The generation process can then be described as:\npSPRING =\nmY\ni=1\npŒ∏,Œ¥(ai|R; [t1, t2, ¬∑¬∑¬∑ , tn]; Q; a<i),\nwhere Œ¥ ‚àà Rn√ód represents the added parameters of the\ntrainable tokens (i.e., their embeddings), and d is the em-\nbedding size of the LLM. Œ∏ denotes the parameters of the\nbackbone LLM, which arefrozen during training. Given that\n|Œ¥| ‚â™ |Œ∏|, our method is highly efficient for training. For\nexample, with the Mistral-7b model (where d = 4, 096),\nwhen n = 50 tokens are added, we only add and train\n50 √ó 4, 096 = 0.2M parameters, approximately 0.003% of\nthe full model.\nImportantly, we place the virtual tokensT between the re-\ntrieved results R and the question Q for two main reasons:\n(1) In the auto-regressive generation paradigm, positioning\nthe tokens after the retrieved results allows them to attend to\nthis information, thereby aiding the model‚Äôs comprehension.\n(2) Recent studies have indicated that LLMs are particularly\nsensitive to the end of an input (Liu et al. 2023). By consis-\ntently placing these virtual tokens before the question across\nall test samples, we aim to mitigate any potential adverse\neffects on the understanding of the question.\nScalable In practical developments, LLMs are often con-\nstrained by their maximum input lengths, limiting the num-\nber of tokens available for retrieval augmentation (especially\nwhen the retrieved results are very long). Therefore, it is de-\nsired to design a mechanism so that any number of virtual\ntokens can be used in the inference to improve RAG perfor-\nmance. To achieve this, we propose an optimization strategy\nworking like a ‚Äúspring‚Äù (as shown in Figure 2). Specifically,\nfor a given sample{R, Q, A} with the total number of added\ntokens n, we randomly select a number k(k ‚â§ n) and uti-\nlize the first k virtual tokens t1:k to construct the training ex-\nample as [R; t1, t2, ¬∑¬∑¬∑ , tk; Q]. This method allows for the\nflexible optimization of any number of virtual tokens. Con-\nsequently, the number of virtual tokens incorporated during\ninference can be arbitrarily selected based on the require-\nments of the application. The effectiveness of this strategy\nand its comparison with other methods are further discussed\nin our experiments.\nPluggable Due to its designed structure, our method pro-\nvides considerable flexibility in application. Practically, if\nuser input is assessed to require external knowledge, our vir-\ntual tokens can be simply appended after the retrieval results\nand then fed, along with the user input, into the LLM for\ngeneration. In contrast, if the user input does not necessitate\nretrieval, it can be processed directly by the LLM. As our ap-\nproach does not adjust the original parameters of the LLM,\nit preserves the model‚Äôs inherent capabilities. This feature\nis particularly important for industry or business since ex-\nisting LLMs may have already been deployed for multiple\npurposes; our method enhances the retrieval understanding\ncapabilities of these models without compromising their ex-\nisting functionalities.\nInference We illustrate the instructions for using our\nSPRING in the right side of Figure 2. After training, the\nembeddings of the added tokens have been optimized for\nRAG, but these tokens do not correspond to any existing to-\nkens in the vocabulary. To make them easy to use, we can\nadd some special tokens (e.g., [r1], ¬∑¬∑¬∑ , [r50]) to the\n26168\nPassages ùë°1 Question\nùë°1 ùë°2Passages Question\nPassages Questionùë°1 ùë°2 ùë°3 ùë°4\nAdded ùëõ Trainable Tokens for RAG\nunused\nunused\nSequence = [ùëÖ;ùë°1,ùë°2;ùëÑ]\nSequence = [ùëÖ;ùë°1,ùë°2,ùë°3;ùëÑ]\nSequence = [ùëÖ;ùë°1,‚ãØ,ùë°5;ùëÑ]\nScalable\nunused\n0 <s>\n1 </s>\n‚Ä¶ ‚Ä¶\n31998 „Çº\n31999 Ê¢¶\n32000 [r1]\n32001 [r2]\n‚Ä¶ ‚Ä¶\n32048 [r49]\n32049 [r50]\n(1) Add special tokens and \nmerge embeddings\nOriginal vocab Added tokens\n+\nLLM\nRetriever\nPassages R\nQR [r1] [rk]‚ãØ[ ]\n(2) Retrieve, append tokens, \nthen generate\nQuestion\nTraining Inference\nR Q\nFigure 2: Illustration of SPRING. Only the embeddings of the addedn tokens are trainable during fine-tuning. The added tokens\nare scalable where any first k(k ‚â§ n) tokens can be used in inference.\nvocabulary and initialize their embeddings with the trained\nembeddings. Then, during inference, after obtaining the re-\ntrieved results R, we can add any number of these special\ntokens (e.g., [r1] ¬∑¬∑¬∑ [rk]) after R and input them with\nthe question Q to the LLMs for generation.\nWe refer to our method as SPRING due to its scalable\nand pluggable nature, making it particularly well-suited for\nenhancing existing LLMs that have already been deployed.\nAdditionally, it effectively bridges the gap between retrieved\nresults and user input, significantly improving the LLMs‚Äô ca-\npabilities in understanding the retrieved external knowledge.\nExperiment\nDatasets and Retrievers\nWe conduct experiments on twelve commonly\nused question-answering datasets, including Trivi-\naQA (TQA) (Joshi et al. 2017), Natural Questions\n(NQ) (Kwiatkowski et al. 2019), HotpotQA (HQA) (Yang\net al. 2018), SQuAD 1.1 (Rajpurkar et al. 2016), Web\nQuestions (WebQ) (Berant et al. 2013), 2WikiMulti-\nHopQA (2Wiki) (Ho et al. 2020), CoQA (Reddy, Chen,\nand Manning 2019), MS MARCO (Nguyen et al. 2016),\nPopQA (Mallen et al. 2023), Fermi (Kalyan et al. 2021),\nMusique (Trivedi et al. 2022), and Bamboogle (Press et al.\n2023). These datasets are publicly available at HuggingFace\nor their official websites. To evaluate the generalizability\nof the methods, we select PopQA, Fermi, Musique, and\nBamboogle as held-out datasets. We mix the training set\nof all remaining datasets for training. For all datasets, we\nprioritize the use of test sets for evaluation purposes. In\ncases where the test set is not available, we utilize the\ndevelopment set instead. It is worth noting that, though\nsome datasets have provided golden reference passages\nfor the answer, we do not use them in our experiment but\nuse the passages retrieved from the retrieval sets in both\ntraining and inference stages, which aligns with practical\napplications. Exact match (EM) and F1 score are employed\nas evaluation metrics.\nFor the retrieval sets, we follow previous studies (Yoran\net al. 2023) and use the combination of Wikipedia and MS\nMARCO datasets as the retrieval corpus. Wikipedia con-\ntains high-quality human knowledge, which is helpful for\nmany knowledge-intensive tasks. MS MARCO contains a\nlarge amount of web pages, which can provide information\nnecessary for curating some natural language questions. We\nuse the datasets that have already been preprossed into pas-\nsages and released on HuggingFace.1 The Wikipedia set has\n21M passages, while the MS MARCO set has 8M passages.\nWe use E5-large (Wang et al. 2022) as the main\nretriever in our experiments. The impact of other re-\ntrievers, i.e., BM25 (Robertson and Zaragoza 2009),\nBGE-base (Xiao et al. 2023), and E5-base, is studied in\nour further analysis. Among these retrievers,BM25 is a non-\nneural sparse retrieval algorithm, while others are neural-\nbased dense retrievers. In general, dense retrievers perform\nbetter on several benchmarks (Muennighoff et al. 2023).\nBaseline Methods\nWe consider both the base and instruction fine-tuned ver-\nsions of Mistral-7b, LLaMA-2-7b, and LLaMA-2-13b as the\nbackbone models, and compare our SPRING with the fol-\nlowing baselines.\n‚Ä¢ Concat: This method directly concatenates the retrieval\nresults and the question for evaluation.\n‚Ä¢ Prompt: This method uses a manually-crafted prompt\nto indicate the use of retrieval information.\n‚Ä¢ Prefix-tuning (Li and Liang 2021): This method uses\nprefix-tuning to fine-tune the backbone models. To make a\nfair comparison with our method, we add 50 prefix tokens\nfor training.\n‚Ä¢ LoRA (Hu et al. 2022): This method uses LoRA to fine-\ntune the backbone models. We use the hyperparameters sug-\ngested by the LLaMA‚Äôs official guidance. 2 To further vali-\ndates the effectiveness of our SPRING on models that have\nalready been optimized for RAG, we also train our method\nbased on the LoRA checkpoint, and denote this variant as\nSPRING+.\n1Wikipedia passages: https://huggingface.co/datasets/Tevatron/\nwikipedia-nq-corpus. MS MARCO passages: https://huggingface.\nco/datasets/Tevatron/msmarco-passage-corpus.\n2LLaMA Recipes, https://github.com/meta-llama/llama-\nrecipes/blob/main/src/llama recipes/configs/peft.py\n26169\nwith Retrieval without Retrieval\nDataset Metric Concat Prompt Prefix LoRA SPRING SPRING + Concat Prompt Prefix LoRA SPRING SPRING +\nTuning Parameters 0 0 0.2M 4M 0.2M 4.2M 0 0 0.2M 4M 0.2M 4.2M\nTrivia QA EM 0.00 57.79 11.74 62.76 65.71 63.89 0.01 39.90 0.00 0.03 46.56 43.37\nF1 65.60 80.33 59.97 85.44 85.26 85.94 63.96 69.72 28.30 34.91 74.48 74.89\nNQ EM 0.00 28.99 13.04 47.95 42.35 49.78 0.00 13.36 0.00 0.00 18.80 25.54\nF1 41.77 58.72 38.22 74.15 70.73 75.22 43.74 48.63 17.74 29.10 55.75 60.82\nHQA EM 0.00 26.36 5.79 39.95 35.26 41.14 0.00 17.07 0.00 0.03 20.15 23.38\nF1 44.91 56.15 42.59 68.93 65.44 69.95 47.54 49.50 17.45 27.57 54.79 57.99\nSQuAD EM 0.00 23.92 7.19 35.71 33.67 35.98 0.00 8.61 0.00 0.00 12.71 13.90\nF1 43.05 57.66 39.75 68.05 66.99 68.41 43.32 46.81 21.88 27.51 53.58 54.93\nWebQ EM 0.00 17.53 4.44 43.65 31.84 48.10 0.00 14.79 0.00 0.00 24.95 28.81\nF1 37.46 52.10 31.55 71.99 64.78 73.88 44.34 50.60 20.14 32.36 59.83 62.95\n2Wiki EM 0.00 22.64 4.38 35.93 31.80 37.12 0.00 23.45 0.00 0.01 24.62 28.60\nF1 47.77 55.58 41.82 63.85 62.03 64.60 52.83 53.55 21.14 37.09 56.83 59.12\nCoQA EM 0.00 8.20 1.56 12.89 13.28 13.87 0.00 8.59 0.00 0.00 9.96 12.50\nF1 27.98 36.72 20.02 41.19 42.41 42.04 32.97 36.58 13.15 18.99 39.96 41.36\nMS MARCO EM 0.00 5.73 0.60 8.13 6.57 8.27 0.00 2.56 0.00 0.01 2.09 3.24\nF1 56.44 53.81 50.56 54.81 53.48 55.90 49.50 47.75 47.44 52.44 51.41 49.84\nPopQA* EM 0.00 39.79 10.02 47.15 48.71 46.98 0.00 16.05 0.00 0.00 20.25 18.70\nF1 56.49 68.26 44.54 73.12 73.90 73.29 53.61 54.85 20.39 25.09 58.32 58.05\nFermi* EM 0.00 0.06 0.00 0.12 0.18 0.24 0.00 0.06 0.06 0.00 0.06 0.19\nF1 21.66 18.16 12.65 29.63 31.15 30.67 25.51 17.84 20.33 25.42 29.28 30.37\nMusique* EM 4.01 4.05 0.04 10.86 8.80 12.58 0.00 1.93 0.66 0.17 3.64 4.43\nF1 38.79 38.64 20.91 52.65 48.74 53.65 42.30 36.18 35.22 45.31 44.93 47.70\nBamboogle* EM 12.80 12.80 0.00 24.22 22.66 28.13 0.00 4.69 3.20 0.00 12.00 12.80\nF1 45.81 48.13 13.67 59.05 56.95 62.00 42.61 42.24 36.22 45.14 47.69 46.89\nAverage EM 1.40 19.80 4.90 30.78 28.40 32.17 0.00 12.59 0.33 0.02 16.32 17.96\nF1 43.98 51.30 34.69 61.91 60.15 62.96 45.19 46.19 24.95 33.41 52.24 53.74\nTable 1: Evaluation results of different methods on twelve QA datasets. The retriever is E5-large model, and the number of\nretrieved passages is set as three. The number of virtual tokens used in SPRING is set as 50. ‚àóPopQA, Fermi, Musique, and\nBamboogle are invisible during training. ‚ÄúPrefix‚Äù stands for prefix-tuning, and ‚ÄúSPRING +‚Äù is trained based on the LoRA‚Äôs\ncheckpoint. The best results are in bold.\nImplementation Details\nWe use PyTorch (Paszke et al. 2019) and Huggingface Ac-\ncelerate library to implement all methods. The learning rate\nis set as 1e-4 with a warm-up ratio of 0.1. All methods are\ntrained for three epochs, with a training batch size of 256.\nWe use eight NVIDIA A800 GPUs for training. Training\nour SPRING for Mistral-7b models consumes around 2.2\nhours per epoch. The embeddings of the virtual tokens are\ninitialized by the embeddings of the prompt: ‚ÄúAccording to\nthe previous relevant passages, please answer the following\nquestion. Only return the answer without any other words.‚Äù\nFollowing the settings of prefix-tuning, if the number of\ntokens required exceeds those available in the prompt, the\nprompt is repeated to complete the initialization; if fewer\nare needed, the prompt is truncated accordingly. Addition-\nally, we experiment with random initialization of tokens\nbut observe that its performance is slightly worse than that\nachieved through prompt-based initialization. Our code is\navailable at https://github.com/DaoD/SPRING.\nExperimental Results\nWe fine-tune the prefix-tuning, LoRA, and SPRING meth-\nods on RAG tasks, and then evaluate their performance\nin scenarios both with (RAG) and without (non-RAG) re-\ntrieval. For SPRING, we use k = 50virtual tokens for in-\nference by default, and the impact of token quantity k is\ndiscussed in later. The experimental results are shown in\nTable 1. To save space, we only show the results based on\nMistral-7b-instruct.\nWe can observe: (1) It is evident that SPRING signifi-\ncantly improves the RAG performance of the original LLM\nwith manually-crafted prompts (the average EM and F1\nscores are improved by 43.4% and 17.3%, respectively). It\noutperforms LoRA on certain datasets, such as TriviaQA\nand CoQA. Given that SPRING involves only 0.2M train-\nable parameters, these results demonstrate its remarkable ef-\nficiency and effectiveness. (2) While LoRA achieves slightly\nbetter performance on some datasets, it adjusts the LLMs‚Äô\noriginal parameters, which adversely impact their perfor-\nmance in non-RAG scenarios‚Äîa significant drop has been\nobserved, even far worse than the original models. This chal-\nlenge also extends to other general generation tasks, which\nwill be discussed in the next section. (3) In non-RAG evalu-\nation, only SPRING and SPRING+ demonstrate better per-\nformance than the Prompt method. This indicates that even\nin the absence of retrieved results, adding virtual tokens is\n26170\nDataset n-shot LoRA SPRING Diff\nBoolQ 0 79.30 82.97 3.67\nCommonsenseQA 0 55.45 63.80 8.35\nCommonsenseQA 4 59.87 67.07 7.20\nGSM8K 8 17.33 31.89 14.56\nMMLU 0 51.30 53.62 2.32\nMMLU 5 48.76 54.96 6.20\nTable 2: Performance comparison on other datasets.\nstill beneficial. We speculate that beyond simply utilizing\nretrieved results, virtual tokens can help the LLM under-\nstand the task goal and format (e.g., the task is question-\nanswering rather than text continuation). (4) Based on the\nLoRA‚Äôs checkpoint, SPRING + achieves the best perfor-\nmance on most datasets. Additionally, all backbone mod-\nels show improvements with SPRING. These findings verify\nthe versatility and flexibility of our approach, confirming its\nsuitability for enhancing various LLMs in RAG scenarios.\n(5) Using manually-crafted prompts is effective for improv-\ning LLMs‚Äô performance on RAG tasks. However, this im-\nprovement is limited as no training is involved. (6) SPRING\nachieves robust performance on the held-out datasets, val-\nidating the good generalizability of our method. (7) Inter-\nestingly, prefix-tuning cannot perform well for RAG, high-\nlighting that the insertion position of the virtual tokens in\nSPRING is both reasonable and effective.\nFurther Analysis\nWe further conduct a series of experiments to investigate\nthe impact of different settings in SPRING. All the fol-\nlowing experiments are conducted based on fine-tuning the\nMistral-7b-instruct model.\nPerformance on Other Tasks To examine the impact of\ndifferent fine-tuning methods on the inherent capabilities of\nLLMs, we evaluate the performance of models fine-tuned\nby LoRA and SPRING on several other (non-RAG) tasks.\nThese tasks are commonly used to evaluate LLMs‚Äô reason-\ning, mathematical abilities, and world knowledge, including\nBoolQ (Clark et al. 2019), CommonsenseQA (Talmor et al.\n2019), GSM8K (Cobbe et al. 2021), and MMLU (Hendrycks\net al. 2021). The experimental results are shown in Table 2.3\nFrom the results, we can observe: (1) Thanks to the plug-\nand-play design of our method, SPRING can revert to to\nthe original LLMs by not using virtual tokens. Therefore, it\nsuccessfully preserves the original capabilities of the LLMs.\nIn contrast, LoRA, which adjusts the model‚Äôs parameters\nfor RAG tasks, inevitably compromises the model‚Äôs perfor-\nmance on other tasks. (2) A noticeable decline is observed\nin the few-shot evaluation, reflecting a decrease in the in-\ncontext learning abilities of LLMs. This decline may stem\nfrom the fact that RAG fine-tuning does not incorporate in-\ncontext learning capabilities. Besides, fine-tuning for RAG\n3We notice that our results are quite different from those of-\nficially reported, which we attribute to the impact of different\nprompts.\n29.97 31.92 32.87 33.08 33.20 33.87 34.49 34.35 34.42 34.35\n61.27 62.77 63.42 63.96 64.14 64.51 65.01 64.86 64.92 65.00\n15.39 13.37 13.97 15.13 16.04 17.86\n24.73\n30.15 31.69 33.78\n48.26 46.48 47.10 48.11 48.82 50.81\n57.60\n61.25 62.08 63.29\n0\n10\n20\n30\n40\n50\n60\n70\n0 1 2 3 4 5 10 20 30 40 50\n# Virtual Tokens\nSPRING-EM SPRING-F1 Fix 50-EM\nFix 50-F1 Prompt-EM Prompt-F1\nFigure 3: Average performance on nine QA datasets with\nvarious numbers of virtual tokens.\nPrompt SPRING\nRetriever EM F1 EM F1\nBM25 21.23 54.94 30.94 62.73\nBGE-base 23.07 56.12 31.81 63.46\nE5-base 24.38 56.84 33.34 64.49\nE5-large 25.66 57.70 34.35 65.00\nAverage 23.58 56.40 32.61 63.92\nVariance 2.69 1.02 1.75 0.78\nTable 3: Average performance on nine QA datasets with dif-\nferent retrievers.\ntasks may lead the model to overfit to specific task formats\n(prompts), thereby impairing its general generation abilities.\nImpact of Token Quantity In SPRING, we design a scal-\nable training approach that enables to use arbitrary numbers\nof virtual tokens in inference. To validate its effectiveness,\nwe test the performance of our method with various num-\nbers of virtual tokens and compare it with a variant model\ntrained with a fixed number of tokens (k = 50). The ex-\nperimental results are illustrated in Figure 3. In general, we\ncan observe that the performance of SPRING increases with\nmore virtual tokens used. Surprisingly, SPRING can signif-\nicantly enhance LLMs‚Äô performance in RAG scenarios with\njust a single token, which is very encouraging.4 In compari-\nson, training with a fixed number of tokens limits the flexi-\nbility of SPRING, making it can only be used with the same\nnumber of tokens in inference (i.e., k = 50).\nEffects of Different Retrievers In our experiments,\nSPRING is fine-tuned using passages retrieved by\nE5-large. To investigate its effectiveness with other\nretrievers, we conduct an experiment by testing its perfor-\nmance with passages retrieved by BM25, BGE-base, and\n4This varies across different LLMs.\n26171\n20.01\n31.57 33.33 34.35 34.71 34.00\n56.11\n63.14 64.38 65.00 65.29 64.92\n16.04\n24.34 25.06 25.66 26.07 26.02\n50.89\n57.21 57.57 57.70 57.92 57.85\n10\n20\n30\n40\n50\n60\n70\n0 1 2 3 4 5\n# Retrieved Results\nSPRING-EM SPRING-F1 Prompt-EM Prompt-F1\nFigure 4: Average performance on nine QA datasets with\ndifferent number of retrieved passages.\nE5-large. The results are presented in Table 3. First,\nSPRING achieves consistent improvement over the original\nmodel using manually crafted prompt, thereby confirming\nthe generalizability of our approach. Second, compared\nto the original model, the performance gap (variance)\namong different retrievers becomes smaller, highlighting\nSPRING‚Äôs robustness to variations in retrievers. Finally,\neven fine-tuned with a superior retriever (i.e., E5-large),\nSPRING maintains strong performance well with less\neffective retrievers (such as BM25). This indicates that\nour method can effectively adapt to varying quality of\nretrieved results. Hence, there is no necessity to retrain the\nvirtual tokens with each update of retrievers in practical\napplications, significantly enhancing its applicability.\nInfluence of Retrieved Passages During the fine-tuning\nof SPRING, we construct training samples by randomly se-\nlecting the top-m (m ‚àà [1, 5]) retrieved passages. This aims\nto enhance SPRING‚Äôs adaptability by ensuring it can oper-\nate effectively with varying numbers of retrieved passages in\nreal-world scenarios. To evaluate the effect of this training\nstrategy, we test the SPRING‚Äôs performance across a range\nfrom zero to five passages. Figure 4 illustrates the results.\nWe can find that SPRING‚Äôs performance gradually improves\nas more retrieved passages are used (m = 0‚Üí 4), suggest-\ning that more retrieved passages contribute valuable knowl-\nedge for question answering. However, the performance\npeaks at four passages and declines when more passages are\nadded. This decrease could be attributed to noise accumula-\ntion within the retrieved knowledge, a phenomenon also re-\nported in recent studies (Yoran et al. 2023). Despite this, the\nuse of retrieved passages still results in performance gains\ncompared to scenarios without retrieval (m = 0), highlight-\ning again the benefits of RAG.\nCross-Dataset Generalizability Inspired by previous\nstudies in multi-task learning (Raffel et al. 2020; Khashabi\net al. 2020), we mix eight QA datasets for training as they\nTraining ‚Üí TQA NQ Mix\nTest ‚Üì EM F1 EM F1 EM F1\nTQA 62.80 84.65 65.51 84.73 65.71 85.26\nNQ 32.19 64.98 45.26 72.68 42.35 70.73\nHQA 22.97 56.95 28.11 59.45 35.26 65.44\nSQuAD 25.24 61.39 30.81 64.32 33.67 66.99\nWebQ 26.03 62.40 34.13 67.30 31.84 64.78\n2Wiki 17.32 54.41 25.43 58.21 31.80 62.03\nCoQA 6.05 36.61 7.62 38.54 13.28 42.41\nMARCO 3.26 31.88 3.88 33.33 6.57 53.48\nPopQA 44.07 72.50 48.28 74.07 48.71 73.90\nAverage 26.66 58.42 32.11 61.40 34.35 65.00\nTable 4: Performance comparison between training on a spe-\ncific dataset or a mixture of all datasets.\nrequire similar LLM capabilities (e.g., reasoning). To study\nthe impact of this strategy, we conduct experiments by train-\ning SPRING on each dataset individually and then testing\nits performance on the others. Table 4 shows partial results.\nAs indicated, training on a mixed dataset generally enhances\nperformance on most datasets, thereby validating the bene-\nfits of multi-task learning. While training on a single dataset,\nsuch as NQ, may yield superior results on its specific test set,\nsuch improvements often fail to generalize to other datasets.\nNotably, training solely on NQ may negatively impact per-\nformance on MS MARCO, where the original LLM using\na prompt could outperform it. These findings inspire us to\ncarefully consider the interaction between different datasets\nwhen applying our method in future applications.\nConclusion\nIn this paper, we introduced scalable and pluggable vir-\ntual tokens for retrieval-augmented large language models.\nOur method, SPRING, serves as a parameter-efficient fine-\ntuning approach that significantly enhances RAG perfor-\nmance with the addition of only 0.2M trainable parame-\nters. More importantly, the plug-and-play nature of our ap-\nproach successfully preserves the performance of LLMs on\nnon-RAG tasks, while its scalable training strategy broad-\nens the method‚Äôs applicational flexibility. Through extensive\nexperiments across various datasets, we have demonstrated\nthe effectiveness, generalizability, flexibility, and high effi-\nciency of our method. We believe that our research will fos-\nter further integration of information retrieval and LLMs,\nand advance the development of other parameter-efficient\nfine-tuning technologies for LLMs.\nAcknowledgments\nThis work was supported by National Natural Science\nFoundation of China (Grant No. 62402497 and 62272467),\nBeijing Natural Science Foundation L233008, and Bei-\njing Municipal Science and Technology Project No.\nZ231100010323009. The work was partially done at the En-\ngineering Research Center of Next-Generation Intelligent\nSearch and Recommendation, MOE.\n26172\nReferences\nBerant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Se-\nmantic Parsing on Freebase from Question-Answer Pairs. In\nEMNLP, 1533‚Äì1544.\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.;\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\nSifre, L. 2022. Improving Language Models by Retrieving\nfrom Trillions of Tokens. In ICML, 2206‚Äì2240.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. In NeurIPS.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. E.\n2020. A Simple Framework for Contrastive Learning of Vi-\nsual Representations. In ICML, 1597‚Äì1607.\nChen, Z.; Duan, Y .; Wang, W.; He, J.; Lu, T.; Dai, J.; and\nQiao, Y . 2023. Vision Transformer Adapter for Dense Pre-\ndictions. In ICLR.\nClark, C.; Lee, K.; Chang, M.; Kwiatkowski, T.; Collins, M.;\nand Toutanova, K. 2019. BoolQ: Exploring the Surprising\nDifficulty of Natural Yes/No Questions. In NAACL-HLT,\n2924‚Äì2936.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems. CoRR.\nDettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,\nL. 2023. QLoRA: Efficient Finetuning of Quantized LLMs.\nIn NeurIPS.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT, 4171‚Äì4186.\nGao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y .;\nLi, H.; and Qiao, Y . 2024. CLIP-Adapter: Better Vision-\nLanguage Models with Feature Adapters. Int. J. Comput.\nVis., (2): 581‚Äì595.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.\n2020. Retrieval Augmented Language Model Pre-Training.\nIn ICML, 3929‚Äì3938.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. B. 2020.\nMomentum Contrast for Unsupervised Visual Representa-\ntion Learning. In CVPR, 9726‚Äì9735.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Mul-\ntitask Language Understanding. In ICLR.\nHo, X.; Nguyen, A. D.; Sugawara, S.; and Aizawa, A. 2020.\nConstructing A Multi-hop QA Dataset for Comprehensive\nEvaluation of Reasoning Steps. In COLING, 6609‚Äì6625.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nde Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-Efficient Transfer Learning for NLP. In\nICML, 2790‚Äì2799.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adap-\ntation of Large Language Models. In ICLR.\nHu, Z.; Wang, L.; Lan, Y .; Xu, W.; Lim, E.; Bing, L.; Xu, X.;\nPoria, S.; and Lee, R. K. 2023. LLM-Adapters: An Adapter\nFamily for Parameter-Efficient Fine-Tuning of Large Lan-\nguage Models. In EMNLP, 5254‚Äì5276.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;\nBang, Y .; Madotto, A.; and Fung, P. 2023. Survey of Hal-\nlucination in Natural Language Generation. ACM Comput.\nSurv., (12): 248:1‚Äì248:38.\nJin, J.; Zhu, Y .; Yang, X.; Zhang, C.; and Dou, Z. 2024a.\nFlashRAG: A Modular Toolkit for Efficient Retrieval-\nAugmented Generation Research. CoRR.\nJin, J.; Zhu, Y .; Zhou, Y .; and Dou, Z. 2024b. BIDER:\nBridging Knowledge Inconsistency for Efficient Retrieval-\nAugmented LLMs via Key Supporting Evidence. CoRR.\nJoshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017.\nTriviaQA: A Large Scale Distantly Supervised Challenge\nDataset for Reading Comprehension. In ACL, 1601‚Äì1611.\nKalyan, A.; Kumar, A.; Chandrasekaran, A.; Sabharwal, A.;\nand Clark, P. 2021. How much coffee was consumed during\nEMNLP 2019? Fermi Problems: A New Reasoning Chal-\nlenge for AI. In EMNLP, 7318‚Äì7328.\nKhashabi, D.; Min, S.; Khot, T.; Sabharwal, A.; Tafjord, O.;\nClark, P.; and Hajishirzi, H. 2020. UnifiedQA: Crossing For-\nmat Boundaries With a Single QA System. In Findings of\nEMNLP, 1896‚Äì1907.\nKwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;\nParikh, A. P.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\nJ.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.;\nDai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019. Nat-\nural Questions: a Benchmark for Question Answering Re-\nsearch. Trans. Assoc. Comput. Linguistics, 452‚Äì466.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power\nof Scale for Parameter-Efficient Prompt Tuning. InEMNLP,\n3045‚Äì3059.\nLewis, P. S. H.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,\nV .; Goyal, N.; K¬®uttler, H.; Lewis, M.; Yih, W.; Rockt¬®aschel,\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In\nNeurIPS.\nLi, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing\nContinuous Prompts for Generation. In ACL, 4582‚Äì4597.\nLin, X. V .; Chen, X.; Chen, M.; Shi, W.; Lomeli, M.; James,\nR.; Rodriguez, P.; Kahn, J.; Szilvasy, G.; Lewis, M.; Zettle-\nmoyer, L.; and Yih, S. 2023. RA-DIT: Retrieval-Augmented\nDual Instruction Tuning. CoRR.\nLin, Z.; Madotto, A.; and Fung, P. 2020. Exploring Versatile\nGenerative Language Model Via Parameter-Efficient Trans-\nfer Learning. In Findings of EMNLP, 441‚Äì459.\n26173\nLiu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua,\nM.; Petroni, F.; and Liang, P. 2023. Lost in the Middle: How\nLanguage Models Use Long Contexts. CoRR.\nLiu, X.; Ji, K.; Fu, Y .; Du, Z.; Yang, Z.; and Tang, J. 2021a.\nP-Tuning v2: Prompt Tuning Can Be Comparable to Fine-\ntuning Universally Across Scales and Tasks. CoRR.\nLiu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang, Z.; and\nTang, J. 2021b. GPT Understands, Too. CoRR.\nMallen, A.; Asai, A.; Zhong, V .; Das, R.; Khashabi, D.;\nand Hajishirzi, H. 2023. When Not to Trust Language\nModels: Investigating Effectiveness of Parametric and Non-\nParametric Memories. In ACL, 9802‚Äì9822.\nMangrulkar, S.; Gugger, S.; Debut, L.; Belkada, Y .; Paul,\nS.; and Bossan, B. 2022. PEFT: State-of-the-art Parameter-\nEfficient Fine-Tuning methods.\nMuennighoff, N.; Tazi, N.; Magne, L.; and Reimers, N.\n2023. MTEB: Massive Text Embedding Benchmark. In\nEACL, 2006‚Äì2029.\nNguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.;\nMajumder, R.; and Deng, L. 2016. MS MARCO: A Human\nGenerated MAchine Reading COmprehension Dataset. In\nNeurIPS.\nOpenAI. 2023. GPT-4 Technical Report. CoRR.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; K ¬®opf, A.; Yang, E. Z.; DeVito, Z.; Rai-\nson, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang,\nL.; Bai, J.; and Chintala, S. 2019. PyTorch: An Impera-\ntive Style, High-Performance Deep Learning Library. In\nNeurIPS, 8024‚Äì8035.\nPetroni, F.; Piktus, A.; Fan, A.; Lewis, P. S. H.; Yazdani,\nM.; Cao, N. D.; Thorne, J.; Jernite, Y .; Karpukhin, V .; Mail-\nlard, J.; Plachouras, V .; Rockt ¬®aschel, T.; and Riedel, S.\n2021. KILT: a Benchmark for Knowledge Intensive Lan-\nguage Tasks. In NAACL-HLT, 2523‚Äì2544.\nPress, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N. A.;\nand Lewis, M. 2023. Measuring and Narrowing the Compo-\nsitionality Gap in Language Models. InFindings of EMNLP,\n5687‚Äì5711.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res., 140:1‚Äì140:67.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension\nof Text. In EMNLP, 2383‚Äì2392.\nRam, O.; Levine, Y .; Dalmedigos, I.; Muhlgay, D.; Shashua,\nA.; Leyton-Brown, K.; and Shoham, Y . 2023. In-Context\nRetrieval-Augmented Language Models. CoRR.\nReddy, S.; Chen, D.; and Manning, C. D. 2019. CoQA: A\nConversational Question Answering Challenge. Trans. As-\nsoc. Comput. Linguistics, 249‚Äì266.\nRobertson, S. E.; and Zaragoza, H. 2009. The Probabilistic\nRelevance Framework: BM25 and Beyond. Found. Trends\nInf. Retr., (4): 333‚Äì389.\nShi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.; Lewis,\nM.; Zettlemoyer, L.; and Yih, W. 2023. REPLUG: Retrieval-\nAugmented Black-Box Language Models. CoRR.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting\nCommonsense Knowledge. In NAACL-HLT, 4149‚Äì4158.\nTan, J.; Dou, Z.; Zhu, Y .; Guo, P.; Fang, K.; and Wen, J.\n2024. Small Models, Big Insights: Leveraging Slim Proxy\nModels To Decide When and What to Retrieve for LLMs.\nCoRR.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; Azhar,\nF.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G.\n2023. LLaMA: Open and Efficient Foundation Language\nModels. CoRR.\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2022. MuSiQue: Multihop Questions via Single-hop\nQuestion Composition. Trans. Assoc. Comput. Linguistics,\n539‚Äì554.\nWan, Z.; Wang, X.; Liu, C.; Alam, S.; Zheng, Y .; Liu, J.;\nQu, Z.; Yan, S.; Zhu, Y .; Zhang, Q.; Chowdhury, M.; and\nZhang, M. 2023. Efficient Large Language Models: A Sur-\nvey. CoRR.\nWang, L.; Yang, N.; Huang, X.; Jiao, B.; Yang, L.; Jiang,\nD.; Majumder, R.; and Wei, F. 2022. Text Embeddings by\nWeakly-Supervised Contrastive Pre-training. CoRR.\nWang, Z.; Araki, J.; Jiang, Z.; Parvez, M. R.; and Neubig, G.\n2023. Learning to Filter Context for Retrieval-Augmented\nGeneration. CoRR.\nXiao, S.; Liu, Z.; Zhang, P.; and Muennighof, N. 2023.\nC-Pack: Packaged Resources To Advance General Chinese\nEmbedding. CoRR.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018. HotpotQA:\nA Dataset for Diverse, Explainable Multi-hop Question An-\nswering. In EMNLP, 2369‚Äì2380.\nYoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2023. Mak-\ning Retrieval-Augmented Language Models Robust to Irrel-\nevant Context. CoRR.\nZhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,\nH.; Gao, P.; and Qiao, Y . 2023a. LLaMA-Adapter: Efficient\nFine-tuning of Language Models with Zero-init Attention.\nCoRR.\nZhang, Y .; Li, Y .; Cui, L.; Cai, D.; Liu, L.; Fu, T.; Huang, X.;\nZhao, E.; Zhang, Y .; Chen, Y .; Wang, L.; Luu, A. T.; Bi, W.;\nShi, F.; and Shi, S. 2023b. Siren‚Äôs Song in the AI Ocean: A\nSurvey on Hallucination in Large Language Models. CoRR.\nZhu, Y .; Yuan, H.; Wang, S.; Liu, J.; Liu, W.; Deng, C.; Dou,\nZ.; and Wen, J. 2023. Large Language Models for Informa-\ntion Retrieval: A Survey. CoRR.\nZhu, Y .; Zhang, P.; Zhang, C.; Chen, Y .; Xie, B.; Dou, Z.;\nLiu, Z.; and Wen, J. 2024. INTERS: Unlocking the Power of\nLarge Language Models in Search with Instruction Tuning.\nCoRR.\n26174"
}